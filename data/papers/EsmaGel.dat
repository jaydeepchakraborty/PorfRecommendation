This article was downloaded by: [149.169.145.169] On: 09 June 2017, At: 22:54
Publisher: Institute for Operations Research and the Management Sciences (INFORMS)
INFORMS is located in Maryland, USA

Operations Research
Publication details, including instructions for authors and subscription information:
http://pubsonline.informs.org

Quantitative Comparison of Approximate Solution Sets
for Multicriteria Optimization Problems with Weighted
Tchebycheff Preference Function
Bilge Bozkurt, John W. Fowler, Esma S. Gel, Bosun Kim, Murat Köksalan, Jyrki Wallenius,

To cite this article:
Bilge Bozkurt, John W. Fowler, Esma S. Gel, Bosun Kim, Murat Köksalan, Jyrki Wallenius, (2010) Quantitative Comparison
of Approximate Solution Sets for Multicriteria Optimization Problems with Weighted Tchebycheff Preference Function.
Operations Research 58(3):650-659. https://doi.org/10.1287/opre.1090.0766
Full terms and conditions of use: http://pubsonline.informs.org/page/terms-and-conditions
This article may be used only for the purposes of research, teaching, and/or private study. Commercial use
or systematic downloading (by robots or other automatic processes) is prohibited without explicit Publisher
approval, unless otherwise noted. For more information, contact permissions@informs.org.
The Publisher does not warrant or guarantee the article’s accuracy, completeness, merchantability, fitness
for a particular purpose, or non-infringement. Descriptions of, or references to, products or publications, or
inclusion of an advertisement in this article, neither constitutes nor implies a guarantee, endorsement, or
support of claims made of that product, publication, or service.
Copyright © 2010, INFORMS
Please scroll down for article—it is on subsequent pages

INFORMS is the largest professional society in the world for professionals in the fields of operations research, management
science, and analytics.
For more information on INFORMS, its publications, membership, or meetings visit http://www.informs.org

OPERATIONS RESEARCH

informs

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

Vol. 58, No. 3, May–June 2010, pp. 650–659
issn 0030-364X  eissn 1526-5463  10  5803  0650

®

doi 10.1287/opre.1090.0766
© 2010 INFORMS

Quantitative Comparison of Approximate Solution
Sets for Multicriteria Optimization Problems with
Weighted Tchebycheff Preference Function
Bilge Bozkurt
Department of Industrial Engineering, Middle East Technical University, 06531 Ankara, Turkey, bilge.bozkurt2002@yahoo.com

John W. Fowler, Esma S. Gel, Bosun Kim
School of Computing, Informatics, and Decision Systems Engineering, Arizona State University,
Tempe, Arizona 85287 {john.fowler@asu.edu, esma.gel@asu.edu, bosun.kim@asu.edu}

Murat Köksalan
Department of Industrial Engineering, Middle East Technical University, 06531 Ankara, Turkey, koksalan@ie.metu.edu.tr

Jyrki Wallenius
Department of Business Technology, Helsinki School of Economics, FIN-00100 Helsinki, Finland, jyrki.wallenius@hse.ﬁ

We consider the problem of evaluating the quality of solution sets generated by heuristics for multiple-objective combinatorial optimization problems. We extend previous research on the integrated preference functional (IPF), which assigns
a scalar value to a given discrete set of nondominated points so that the weighted Tchebycheff function can be used as
the underlying implicit value function. This extension is useful because modeling the decision maker’s value function
with the weighted Tchebycheff function reﬂects the impact of unsupported points when evaluating sets of nondominated
points. We present an exact calculation method for the IPF measure in this case for an arbitrary number of criteria. We
show that every nondominated point has its optimal weight interval for the weighted Tchebycheff function. Accordingly,
all nondominated points, and not only the supported points in a set, contribute to the value of the IPF measure when
using the weighted Tchebycheff function. Two- and three-criteria numerical examples illustrate the desirable properties of
the weighted Tchebycheff function, providing a richer measure than the original IPF based on a convex combination of
objectives.
Subject classiﬁcations: multiple objective combinatorial optimization; Tchebycheff function; multiple criteria
metaheuristics.
Area of review: Optimization.
History: Received July 2008; revision received April 2009; accepted June 2009. Published online in Articles in Advance
January 28, 2010.

1. Introduction

is sensitive to the reference point, and it is not straightforward to ﬁnd a good reference point, especially when
there are more than two objectives. Furthermore, the hypervolume indicator may be biased toward solutions having
certain properties. Lizarraga et al. (2008) compare several
measures, including the hypervolume indicator and the socalled epsilon indicator. They ﬁnd that the hypervolume
indicator performs well in general but has handicaps in certain problems. The other indicators performed worse. Sayin
(2000) considers the continuous solution space problems
and proposes to assess the goodness of a discrete representation of the efﬁcient set by three different measures.
The integrated preference functional (IPF) is a set functional that assigns numerical values (i.e., scalars) to given
discrete sets of nondominated points for MOCO problems.
It tries to capture all aspects that deﬁne the “closeness”
of a solution set to the efﬁcient frontier in a single measure. Essentially, the IPF measure can be perceived as

The solution of multiple-objective combinatorial optimization (MOCO) problems often requires the use of heuristics
or metaheuristics such as evolutionary algorithms, simulated annealing, and tabu search. Because the efﬁcient frontiers for these problems are commonly not available, the
performance evaluation of these algorithms requires the
comparison of the solution sets (consisting of the nondominated solutions) generated by the different heuristics.
There are several performance measures that have been
developed to test how well a set of solutions represent the
efﬁcient frontier. Some of these procedures capture only
some portion of the performance we may be interested in,
and some are not very suitable for more than two criteria
(see Deb 2001, pp. 306–324). The so-called hypervolume
indicator is commonly considered to be among the best
indicators. It measures the area dominated by a solution
set up to a reference point. The magnitude of the indicator
650

Bozkurt et al.: Quantitative Comparison of Approximate Solution Sets

651

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

Operations Research 58(3), pp. 650–659, © 2010 INFORMS

representing the “expected utility” that a particular solution
set carries for a decision maker.
Carlyle et al. (2003) introduced the IPF measure for
bicriteria optimization problems. In that paper, the form of
the decision maker’s implicit value function is assumed to
be known and equal to a convex combination of objectives
with unknown weights. Although the weights are unknown,
partial information in the form of probability distributions
of the weights is assumed to be available. Although Carlyle
et al. (2003) assumes that the weight of the ﬁrst objective
is distributed uniformly between 0 and 1, the paper outlines
other generalizations with respect to the weight density distributions. The authors present a derivation of the measure,
as well as a computationally efﬁcient method for the exact
calculation of the measure, along with numerical results
to show the effectiveness of the measure in evaluating the
performance of different heuristics.
Several extensions to the work in Carlyle et al. (2003)
have been published. Fowler et al. (2005) made use of the
IPF in the comparison of two competing bicriteria genetic
algorithms for a scheduling problem. Kim et al. (2006)
developed an extension of the IPF for general k-objective
optimization problems. We provide here a brief review of
the IPF measure for the reader’s convenience. Further details
can be found in Carlyle et al. (2003) and Kim et al. (2006).
Consider a set of nondominated solutions represented in
the objective function space with vectors, z ∈ Z ⊂ Rm , and
a parameterized family of implicit value functions gz ,
where a given value of the weight vector  ∈ A ⊂ Rm
produces a speciﬁc value function to be optimized. For
any given , there exists at least one optimal point in
the solution set Z. Let zg  denote the point in set Z
that optimizes gz  (i.e., in the case of minimization,
gzg    gz  for all z ∈ Z). zg  is piecewise
constant over the weight vector domain, A. Given a weight
density function A → R+ that represents the probability
distribution of the (unknown)
weight vector,  ∈ A, and sat
isﬁes the condition that ∈A h d = 1, the IPF value of
the solution set Z, where IPF: Z ⊂ Rm → R+ is deﬁned as

IPFZ =
hgzg   d
∈A

As indicated in Carlyle et al. (2003), the assumption that
the decision maker’s value function can be represented as
a convex combination of objectives implies that only supported points contribute to the IPF measure. In general, this
is a serious limitation when the decision maker’s implicit
value function is nonlinear, and hence, some unsupported
points in the set of nondominated solutions may, in fact,
be preferable. In the extreme case, there could be problems where there are very few supported efﬁcient solutions
and the vast majority of the efﬁcient solutions could be
unsupported. A good way to consider the impact of unsupported points when evaluating sets of nondominated points
is to use the weighted Tchebycheff function to represent the
decision maker’s value function. The weighted Tchebycheff

function corresponds to the weighted Lp metric in Equation (1) when p = :

minimize
i∈I



1/p
pj zij

p
− z∗∗
j 



(1)

j∈J

where I = 1 2     n	 denotes the index set of solutions,
J = 1 2     m	 denotes the index set of objectives,
j

is the weight of objective j with j  0 and j∈J j = 1,
zij is the jth objective function value of the ith solution,
and z∗∗
j is the utopian value of the jth objective. Note that
this point can be found by minimizing the jth objective
by itself. When p = 1, the metric corresponds to the convex combination of objective functions for z∗∗
j = 0. When
p = , the weighted L metric (Tchebycheff function) is
equivalent to
minimize
maxj zij − z∗∗
j 	 
i∈I

j∈J

(2)

The weighted Tchebycheff function has been widely
used in multiobjective optimization problems because it can
be used to generate all nondominated points, supported as
well as unsupported ones (Wierzbicki 1980, 1983; Steuer
and Choo 1983; Korhonen and Wallenius 1988; Steuer et al.
1993). When the weighted Tchebycheff function is used,
every nondominated point in a set has a weight interval
over which it is optimal. For a detailed review of the properties of the weighted Tchebycheff function, the reader is
referred to Steuer (1986, pp. 440–446).
In this paper, we present the calculation of the IPF measure for the weighted Tchebycheff function. If we assume
that all weights are equally likely to occur (i.e., a uniform weight density function, h = 1 for all  ∈ A),
the IPF essentially measures how well the solution set at
hand represents the efﬁcient frontier. If, on the other hand,
we restrict the weight space in accordance with some partial information on the decision maker’s preferences, the
IPF measures how well the solution set at hand represents the preferred portions of the efﬁcient frontier. These
two aspects differentiate the Tchebycheff-based IPF from
other measures. It considers all nondominated points (supported or unsupported) with appropriate weights of the
Tchebycheff function, and it can incorporate the preferences of a decision maker to favor the solutions in the
desirable regions of the solution space.
The bicriteria case has special characteristics that make it
computationally much simpler than the general case. Some
initial developments of this case were presented in Daskin
et al. (2003), which demonstrates the use of IPF in evaluating different settings for a genetic algorithm designed
to ﬁnd the trade-off between the demand-weighted average distance and the maximum distance between a demand
node and the nearest facility (i.e., P -median and P -center
objectives) in a facility location problem. The paper shows
that IPF is capable of highlighting important differences
between trade-off curves.

Bozkurt et al.: Quantitative Comparison of Approximate Solution Sets

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

652

Operations Research 58(3), pp. 650–659, © 2010 INFORMS

We ﬁrst present the bicriteria case and outline the exact
method to calculate the IPF measure in §2. We provide some properties of the IPF measure in §3 and give
numerical examples to develop the reader’s intuition in §4.
We then generalize the procedure for the general case in
§5, and demonstrate it using a numerical example with
three criteria in §6. We provide our concluding remarks
in §7.

Step 4 describes the integration of the scalar function over
the decomposed optimal weight intervals.
Step 1. Sort all points in the objective space in increasing order of zi1 , and index them such as z1  z2      zn . Then
(not considering the weakly nondominated solutions) the
inequalities in (4) hold because no point in the set dominates another point in the set. Hence,
z11 < z21 < · · · < zn1

2. Calculation of the IPF Measure in
the Bicriteria Case

ib zi1 = 1 − ib zi2 

ib =

where  denotes the weight of the ﬁrst objective function
with 0    1.
As noted in Carlyle et al. (2003) and Kim et al. (2006),
the IPF method consists of two subproblems: (i) ﬁnding the
optimal weight interval for each nondominated point, and
(ii) integrating the scalar value function over the decomposed optimal weight intervals. The method of ﬁnding optimal weight intervals is explained in Steps 1 to 3 below.

Figure 1.

(4)

(5)

In Figure 1(a) there is a point in the objective space where
the dotted line represents a break-even weight, ib , at which
Equation (5) is satisﬁed. As can be seen from Figure 1(b),
the break-even weight can be obtained for all points, zi ,
i ∈ I as follows:

(3)

i∈I

z12 > z22 > · · · > zn2 

Step 2. Obtain the break-even weight, ib , for each point
i ∈ I, where ib satisﬁes Equation (5):

Consider nondominated points, zi = zi1  zi2 , indexed by i ∈
I = 1 2     n	, in a solution set Z ⊂ R2 . Suppose that
two objectives (i.e., J = 1 2	 are to be minimized simultaneously; the ideal point, z∗∗ , for the sake of simplicity, is
0 0; and all objective function values, zij  0, for all i ∈ I
and j ∈ J . Then, the weighted Tchebycheff function can be
represented as
minimize
maxzi1  1 − zi2 	

and

zi2

zi1 + zi2

(6)

Using these break-even weights, maxzi1  1 − zi2 	
for each nondominated point can be obtained as in
Equation (7):
 i
  ib 
z1 
i
i
(7)
maxz1  1 − z2 	 =
1 − zi2    ib 

An illustration of ﬁnding an optimal interval of weights with weighted Tchebycheff function for the bicriteria
case.

z2

(a)

(b)
z2
b1

b1
z1

z1

L1 = U2

b2
2

z

z1

Level set of weighted Tchebycheff function

z1

Break-even weight  at which z1 = (1 –)z2 → b =

Boundary weight at which two adjacent points have the same value of the weighted Tchebycheff function:
z1
max{z1,1 (1– )z21 } = max{z12, (1– }z22} → L1 =  2 = 2 2 1
U
z1 + z2
(L1 is the lower bound of optimal weight interval for z1, and U2 is upper bound of optimal weight interval for z2)

z2
z1 + z2

Bozkurt et al.: Quantitative Comparison of Approximate Solution Sets

653

Operations Research 58(3), pp. 650–659, © 2010 INFORMS

By Theorem 1, the inequalities for break-even weights
given in (8) always hold:

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

1b > 2b > · · · > nb 

(8)

Theorem 1. Consider n nondominated points, z1 
z2      zn , and their corresponding break-even weights,
ib  i = 1 2     n. Then, the inequality 1b > 2b > · · · > nb
always holds.
Proof. (By contradiction) Consider two arbitrary adjacent points, zi and zi+1 . We refer to two points as adjacent if they are the closest to each other in terms of the
Euclidian distance. Assume that the inequality ib  i+1
b
for the corresponding break-even weights holds. Then,
=
by Equation (6), we have ib = zi2 /zi1 + zi2  and i+1
b
i+1
i+1
i
i
i
/z
+
z
.
Then,
by
assumption,
z
/z
+
z

zi+1
2
1
2
2
1
2 
i+1
i+1
i+1 i
i i+1
/z
+
z
,
which
gives
z
z

z
z
.
However,
zi+1
2
1
2
2
2 1
1
this last inequality cannot hold because zi and zi+1 , by convention in Step 1, are adjacent nondominated points, which
> zi1 and zi2 > zi+1
implies that zi+1
1
2 . Hence, the inequality
1
2
b > b > · · · > nb always holds. 
Step 3. Obtain an upper bound, iU , and a lower bound,
of the optimal weight interval for each nondominated
point sequentially as follows:

iL

1U = 1
1L = 2U =


zi2
i+1
z1 + zi2

Proof. When  = 1, the optimal point is z1 because
mini∈I zi1 	 = z11 by (4). Hence, the upper bound of the
optimal weight interval for z1 is 1U = 1. In fact, the point z1
is optimal for 1b    1, as can be seen from Figure 1(b).
For 1L    1b , the optimal point can be obtained from
the following equations:
min
maxzi1  1 − zi2 	
i∈I

= min1 − z12  z21  z31      zn1 	
= min1 − z12  z21 	

(10)

From (10), both points z1 and z2 have the same objective
value at  = z12 /z21 + z12  = 1L = 2U . In Figure1(b), the
line between z1 and z2 stands for the weight where two
adjacent points have the same composite objective function
value. Consider the case when  > 1L . For example, let
 = z12 /z21 + z12  + , where  is a small positive number.
We can show that 1 − z12 < z21 as follows:
	

1 − z12 = 1 −

z12
z21 + z12

iL = i+1
U =

(4) The union of the optimal weight intervals covers
n−1
the entire weight interval. That is, 
nL  nU  ∪ 
n−1
L  U 
1
1
∪ · · · ∪ 
L  U  = 
0 1.
(5) The unique optimal weight intervals associated with
the nondominated points are disjoint.

(9)



nL = 0
According to Theorems 1 and 2 (below), “the unique optimal weight interval” over which a nondominated point
is optimal for the weighted Tchebycheff function can be
determined by only considering adjacent nondominated
points sequentially.
Theorem 2. The optimal weight intervals associated with
the nondominated points for the weighted Tchebycheff function have the following properties:
(1) There exists at least one point, zi ∈ Z, that is optimal
for the weighted Tchebycheff function within the optimal
weight interval, 
iL  iU 
(2) There exists one and only one point, zi ∈ Z, that is
optimal for the weighted Tchebycheff function within the
unique optimal weight interval, iL  iU .
(3) Two adjacent nondominated points zi and zi+1 (zi−1
and zi  have the same value of the weighted Tchebycheff
function at the boundary of the optimal weight interval,
i
iL iU . Hence, i+1
U = L for all i = 1 2     n − 1, and
1
n
U = 1 and L = 0.



z12
z 1 z2
+

z12 = 2 2 1 1 − z12
1
2
z1 + z2
z1 + z2


1 2
1
z 2 z1
z2
2
< 2
+ z1 = 2
+  z21 = z21
z1 + z12
z1 + z12

because z21 > 0 and z12 > 0 by deﬁnition. Hence, the optimal point is still z1 . However, when  < 1L = 2U , the
optimal point is z2 . This can easily be shown using the
same argument above by letting  = z12 /z21 + z12  − . We
note that 2U  2b always holds because z12 /z21 + z12  
z22 /z21 + z12 .
This sequential procedure to obtain the weight interval
that renders each nondominated point optimal can easily
be generalized for the pair consisting of solutions i and
i + 1. Note that when  = 0, the optimal point is zn because
mini∈I 
1 − zi2  = 1 − zn2 by (4).
In the above statement of the theorem, note the difference between the optimal weight interval and the unique
optimal weight interval. The unique optimal weight intervals, iL  iU  i ∈ I, for which there exists one and only
one optimal point, are obviously disjoint by the above
argument. At each of the boundaries, iL = i+1
U  i =
1 2     n − 1, of the optimal weight intervals 
iL  iU ,
i ∈ I, two points, zi and zi+1 , are optimal, and the union
of the optimal weight intervals has to be equal to 
0 1.
Hence, Theorem 2 holds. 
Step 4. Integrate the weighted Tchebycheff function
over the decomposed optimal weight interval from Step 3

Bozkurt et al.: Quantitative Comparison of Approximate Solution Sets

654

Operations Research 58(3), pp. 650–659, © 2010 INFORMS

Proof. The proof follows because h  0 and u z 
u y for all  ∈ W 
 . 

for each point in set Z as follows:
IPFZ =
=



1
0


 

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

i∈I

Lemma 2. Let W i Z = 
 u zi   u zk  for all k 	=
i i k = Iz 	. Let Qi Z be any other decomposition of the
weight
space such thatW i Z 	= Qi Z, and where the condition i∈Iz W i Z = i∈Iz Qi Z = W is satisﬁed. Then,

hmin
maxzi1 1−zi2 	d
i∈I

ib

iL

+



h1−zi2 d
iU

ib


hzi1 d 

(11)

3. Properties of the IPF Measure
In this section, we present some desirable properties of the
IPF measure with the Tchebycheff function. The results are
analogous for the case of convex combination of objectives
discussed in Carlyle et al. (2003).
Corollary 1. Adding a new nondominated solution z to
the set Z can never increase the IPF value of the set
Z ∪ z	. Hence, IPF(Z is monotonically nonincreasing
over increasing sequences of sets of solutions.
Proof. See Carlyle et al. (2003).



From Corollary 1, the IPF can be used to trace solution
quality improvements. Let the set of all individual optima,
ZIO , be known in advance, and let IPF(ZIO  denote the IPF
for this set. By adding a nondominated solution, z to the
set ZIO , the IPF of the set ZIO ∪ z	 will be monotonically
nonincreasing by Corollary 1. By using this property, the
convergence of an approximation can be estimated (i.e.,
the decrease in the IPF value due to adding solutions goes
to 0). Furthermore, as suggested in Daskin et al. (2003), the
net decrement in the IPF due to adding a new solution to
the set ZIO provides information about the solution quality
improvement.
Deﬁnition. Set Pareto Dominance Relation (Carlyle et al.
2003). Let Ap and B p be approximate sets of nondominated solutions generated by competing heuristics, and
Ap ∪ B p = C. Let C p denote the set of nondominated solutions from the set C If C p = Ap and Ap 	= B p , then set
Ap dominates set B p in the set Pareto dominance sense and
vice versa.
We introduce some deﬁnitions and notations to develop
further properties of the IPF measure. Let u zi  =
maxj∈J j zij − z∗∗
j 	 denote the weighted Tchebycheff
distance of zi from the ideal point z∗∗ . Let W =


j j = 1 j  0 for all j ∈ J 	 denote the overall
weight space and Z = zi  i ∈ Iz 	 denote a set of solutions
that do not dominate each other, where zi ∈ m .
Lemma 1. Let W 
 ⊆ W and z y ∈ m such that zj  yj for
all j ∈ J and zj < yj for at least one j (i.e., z dominates y).
Then,

∈W 


hu z d 


∈W 


hu y d

IPFZ =





i∈Iz ∈W Z





i∈Iz ∈Q Z

hu zi  d
hu zi  d = IPF
 Z

Proof. By deﬁnition, u zi   u zk  for all k 	= i for
 ∈ W i Z. Hence, each term is multiplied by the smallest value, u zi , in each weight region, W i Z, in ﬁnding
IPFZ. IPFZ  IPF
 Z because this property does not
hold for IPF
 Z. 
Theorem 3. Let Z = zi  i ∈ Iz 	 and Y = y k  k ∈ Iy 	
such that for each y k ∈ Y there exists at least one
zi ∈ Z satisfying zij  yji for all j ∈ J and zij < yji for
at least one j. Let W k Y  = 
 u y k   u y t  for all t 	=
k k t ∈ Iy 	 be a decomposition of the weight space where

k
i
k
k∈Iy W Y  = W . Let Si = k
 z dominates y for some
i




k ∈ Iy 	 for some z ∈ Z , where Z ⊆ Z. Arrange the contents
 of Si such that St ∩ Sq = 
 for all t 	= q t q ∈ Iz , and
i∈Iz
 Si = Iy . Then,
IPFZ  IPFZ 
   IPF
 Z 
 

=
hu zi  d
k
i∈Z k∈Si ∈W Y 





i∈Z 


k∈Si

∈W k Y 

hu y k  d = IPFY 

Proof. The contents of Si can be arranged as stated in the
theorem by arbitrarily assigning each y k to the index set,
Si , of only one of the zi s that dominate it. By Lemma 1,


hu zi  d 
hu y k  d
∈W k Y 

∈W k Y 

for each k. When we sum up over all terms, the inequality
still holds. IPFZ 
   IPF
 Z 
  by Lemma 2 and IPFZ 
IPFZ 
  by Corollary 1. 
Theorem 3 establishes an important desirable property of
the IPF measure that when a set of solutions, Y , is dominated by another set of solutions, Z, in the set Pareto dominance sense, then IPF(Z is not larger than IPF(Y  when
the Tchebycheff function is used. This desirable property is
only shared by the hypervolume indicator in the literature
to the best of our knowledge.
Corollary 2. The IPF of the efﬁcient frontier is always
less than or equal to the IPF of any approximation of the
efﬁcient frontier.

Bozkurt et al.: Quantitative Comparison of Approximate Solution Sets

655

Operations Research 58(3), pp. 650–659, © 2010 INFORMS

Proof. The proof directly follows from Theorem 3.



When the efﬁcient frontier is known, its IPF value can
be used as a benchmark against which the IPF values of
the approximation sets can be compared. When the efﬁcient
frontier is not known, the IPF values of the approximation
sets can be compared against each other.

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

Optimal value of the weighted Tchebycheff
function as a function of .

Figure 2.

min[max{z1i, (1 – )z2i}]
i∈I

0.20

0.15

4. Numerical Examples for
the Bicriteria Case

0.10

In this section, two numerical examples are provided to
demonstrate the effectiveness of the IPF measure with the
Tchebycheff function in comparing solution sets. Example 1 illustrates the calculation of the exact IPF measure
with the weighted Tchebycheff function for a bicriteria case. Example 2 contrasts the IPF calculations with
a convex combination of objectives and the weighted
Tchebycheff function in assigning scalar values to two
given sets of nondominated points. Interesting results
emerge. In both cases, a uniform weight density function,
h, is used for simplicity.
Example 1. Consider an example with eight nondominated points in a given set Z = z1  z2      z8 	 for a bicriteria optimization problem. See Table 1 and “Set 1” in
Figure 3.
Step 1. As can be seen from Table 1, the points are
sorted by increasing order of objective function 1 (i.e., zi1
values for solutions i = 1 2     8.
Steps 2 to 4. In Table 2, ib , iL , iU , and IPF(zi  for
each nondominated point are shown. We provide all IPF
calculations to ﬁve signiﬁcant digits for the interested readers who may want to reproduce the results and test their
calculations. The IPF value for the given set of nondominated points is the sum of all decomposed IPF values
for all nondominated points as in Equation (11). Hence,
IPF(Z = 012048. The closed area from weight 0 to 1
under the piecewise-linear line in Figure 2 is the IPF(Z of
the given set.
Table 2 and Figure 2 show that: (1) one and only one
point is optimal for the weighted Tchebycheff function
within the optimal weight interval (in Figure 2 each optimal point is marked over the corresponding optimal weight
interval); (2) two adjacent nondominated points have the
same value of the weighted Tchebycheff function at the
boundary of the optimal weight interval; (3) the union of
the optimal weight intervals covers the entire weight interval [0, 1]; and (4) the optimal weight intervals associated
with the nondominated points are disjoint.
Table 1.
z1
zi1
zi2

A set of nondominated points for Example 1.
z2

z3

z4

z5

z6

z7

z8

0000 0142 0228 0257 0285 0400 0485 0771
1000 0561 0540 0439 0252 0222 0166 0000

0.05
z8

z7

z6

z5

z4 z3

z2

z1

0.00
0.0

0.1

0.3

0.2
0.177

0.4

0.314
0.387

0.5

0.6

0.7
0.8
0.711

0.606
0.678

0.9
0.876

1.0



Example 2. In Figure 3, two sets of nondominated points
are compared. Set 1 consists of eight points from Example 1, and Set 2 consists of four supported points among the
eight points of Set 1. When the two sets are compared using
the IPF with a convex combination of objectives, the IPF
values of the sets are clearly identical (0.18587) because
both sets have the same supported points. However, the IPF
with the weighted Tchebycheff function for Set 1 and Set 2
are 0.12048 and 0.12810, respectively. Recall that a lower
IPF value is better in minimization problems, and hence,
the IPF measure with the Tchebycheff function indicates a
preference for Set 1 over Set 2, as expected.

5. Calculating IPF for Problems with
More Than Two Criteria
We now generalize the procedure for more than two criteria.
The decomposition of the optimal weight space for more
than two criteria brings additional difﬁculties. For each solution, we need to identify the weight region for which each
criterion determines its weighted Tchebycheff distance from
the ideal point. We then need to compare each solution with
the other solutions to determine the weight region for which
Table 2.

Results of IPF(Z calculations for Example 1.

i

iL

ib

iU

1
2
3
4
5
6
7
8

0876
0711
0678
0606
0387
0314
0177
0000

1000
0798
0703
0631
0469
0357
0255
0000

1000
0876
0711
0678
0606
0387
0314
0177



iL    ib ib    iU
 i
z1 d
1 − zi2 d

000769
001198
000418
000419
001182
000634
001015
000000
IPF(Z) = 0.12048

000000
000927
000129
000791
002099
000446
000814
001208

IPFzi 	
000769
002125
000547
001209
003281
001081
001829
001208

Bozkurt et al.: Quantitative Comparison of Approximate Solution Sets

656

Operations Research 58(3), pp. 650–659, © 2010 INFORMS

Figure 3.

Comparison of two sets of nondominated
points.

1.200
Set 1
Set 2

0.800

z2

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

1.000

Remark. Different Wti sets, t ∈ J , are disjoint except for
possible common boundaries.
Recall that u zi  denotes the weighted Tchebycheff distance of zi from z∗∗ . We next present a property that helps
identify some subsets of the weight space for which zk is
at least as close to the ideal point as zi is.
Theorem 5. For  ∈ Wjk ,
(a) if zkj  zij , then u zk   u zi ;
(b) if zkj < zij , then u zk  < u zi  for  ∈ Wjk .

0.600
0.400

Proof. By deﬁnition, u zk  = j zkj
Because zkj  zij ,

0.200
0.000
0.000

0.200

0.400

0.600

0.800

1.000

for  ∈ Wjk .

u zk  = j zkj  j zij  maxj zij 	 = u zi 
j

z1

this solution is closest to the ideal point. We show that this
weight region is made up of the union of smaller convex
weight regions, and hence is not necessarily convex. In this
section, we introduce the necessary notation and develop
the theory to characterize the weight regions corresponding to each of the solutions. We also identify simplifying
properties that help characterize these weight regions. We
demonstrate the decomposition of the weight space on a
three-criteria problem.
Dell and Karwan (1990) developed an interactive multiple
criteria decision making procedure locally approximating the decision maker’s utility function with a weighted
Tchebycheff function. Based on pairwise preferences of the
decision maker, they restrict the weight space. Because it is
not known which criterion is at maximum weighted distance
from the ideal point, they obtain weight spaces conditional
upon different criteria. This leads to a number of disjoint
convex weight regions that might contain the weights that
represent the decision maker’s preferences.
In our case, we need to identify the weight space
for which each nondominated alternative is at minimum
Tchebycheff distance from an ideal point z∗∗ . Let alterby m criteria (i.e.,
native zi = zi1  zi2      zim  be deﬁned

J = 1 2     m	. Let W = 
 j j = 1 j  0 for all
j ∈ J 	, and let Wti denote the weight set for which criterion t
determines the weighted Tchebycheff distance of solution i
from z∗∗ .
Theorem 4. Wti = 
 t zit  = maxj∈J j zij 	  ∈ W 	 is a
convex set.
Proof. Wti is deﬁned by the following three inequalities:
t zit  j zij

j = 1

for all j ∈ J 

j

j  0

for all j ∈ J 

which forms a polyhedral set.



The proof for the strict inequality case (given in part (b) of
the theorem) is similar. 
Let J ki = j
 zkj  zij 	 denote the index set of criteria
for which zk is at least as good as zi . Then, J ki ∪ J ik = J .
Note that j belongs to both J ki and J ik if zkj = zij . If
j  J ki , then j ∈ J ik , but the reverse is not necessarily true.
Theorem 5 implies that u zk   u zi  for  ∈ Wjk for
each j ∈ J ki , and that u zi   u zk  for  ∈ Wji for each
j∈ J ik . Therefore, we know that u zk   u zi  for  ∈
k
k
i
ki
and
j∈J ki Wj . We need to partition Wj ∩ Wr for j  J
ik
r J .
Let Wjrki = 
 u zk   u zi   ∈ W   ∈ Wjk ∩ Wri }
for some zk , zi , and each j  J ki and r  J ik . We know
that u zk  = j zkj and u zi  = r zir for  ∈ Wjk ∩
Wri . Hence, we have Wjrki = 
 j zkj  r zir ,  ∈ W ,
 ∈ Wjk ∩ Wri 	. In words, Wjrki denotes the weight set for
which the weighted Tchebycheff distance from z∗∗ of solution k is smaller than that of solution i, where the weighted
Tchebycheff distances of solutions k and i are determined
by criteria j and r, respectively.
Theorem 6. Wjrki is convex for any zk , zi , j  J ki , and
r  J ik .
Proof. It directly follows because Wjrki is an intersection of
polyhedral sets. 
Theorem 7. Sets Wjrki , zk 	= zi , j  J ki , and r  J ik are
disjoint except for possible common boundaries.
Proof. Because Wjk and Wri are disjoint except for common boundaries, so are their intersections Wjk ∩ Wri . Each
Wjk ∩ Wri is partitioned into two subsets, Wjrki and Wjrik ,
by the half spaces j zkj  r zir and j zkj  r zir , respectively. Hence, each disjoint Wjk ∩ Wri is partitioned into two
disjoint subsets that have a common boundary. 
We have established all possible cases for which
u zk   u zi . We can now combine these and deﬁne
k
i
ki
the
for
 set of values
 whichk u zki   u z  as W =
ki
 jJ ki  rJ ik Wjr 	 ∪  j∈J ki Wj 	. W denotes the weight set
for which the weighted Tchebycheff distance from z∗∗ of

Bozkurt et al.: Quantitative Comparison of Approximate Solution Sets

657

solution k is smaller than that of solution i. Note that W ki
is not necessarily convex because it is a union of convex
sets. To ﬁnd the overall weight set, Wk , for which u zk  
u zi  for all i 	= k, we ﬁnd W k = i	=k W ki . W k denotes
the weight set for which the weighted Tchebycheff distance
from z∗∗ of solution k is smaller than that of all other solutions. Then, we can calculate the IPF measure of zk as


IPFzk  =
· · · hj zkj dm−1    d1  (12)
j∈J

Figure 4.

Partition of the weight space between criteria
for alternative 2.

∈W k ∩Wjk

where we substitute 1 −

m−1
j=1

2

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

Operations Research 58(3), pp. 650–659, © 2010 INFORMS

j for m .

6. Numerical Example for
the Three-Criteria Case
Example 3. Consider a three-criteria example with uniform weight density function h and four nondominated
alternatives in the solution set, as shown in Table 3. Each
criterion is to be minimized.
∗∗
∗∗
∗∗
i
Let z∗∗ = z∗∗
1  z2  z3 , where zj = mini zj 	 −  j =
1 2 3, for a small  > 0 (say, for example,  = 01) (see
i
∗∗
i
Steuer 1986, p. 420). We have z∗∗
j = 0 and zj − zj = zj for
all i j. Then, for each alternative i, the set of weights Wji
for which criterion j determines its Tchebycheff distance
from z∗∗ can be calculated as follows:

1

W33 = 
 521 +212  21 211 +322  21  ∈ W 	

Again, we use u zi  to denote the weighted Tchebycheff
distance of zi from z∗∗ . The weight space for which zk
is at minimum weighted Tchebycheff distance from z∗∗
can be represented by W k = 
 u zk   u zi  i 	= k
 ∈ W 	 = 
 maxj j zkj   maxj j zij  i 	= k  ∈ W 	.
Using Wji  maxj j zij  can be identiﬁed.
To compare u z2  with u z1 , we observe that
whenever u z1  = maxj j z1j  = 1 z11  or u z1  =
maxj j z1j  = 3 z13 , we have u z1   u z2 
because z21 > z11  z23 > z13 . Similarly, whenever u z2  =
maxj j z2j  = 2 z12 , we have u z2   u z1  because
z12 > z22 . Utilizing the notation of the previous section,
12
we have u z2   u z1  for
J 21 = 2	
 and J = 1 3	, 
W 21 =  jJ 21  rJ 12 Wjr21 	 ∪  j∈J 21 Wj2 	 We can calculate
21
= 
 211  202   ∈ W   ∈ W12 ∩ W21 	 and
W12
21
W32 = 
 111 − 1 − 2   202   ∈ W   ∈
W32 ∩ W21 	. After eliminating the redundant constraints,
21
= 
 211 − 202  0 211 − 162  0,
we have W12
21
= 
 111 +
321 + 112  11  ∈ W 	 and W32
312  11 321 + 112  11 111 + 272  11
 ∈ W 	. Because W22 = 
 211 − 162  0 111 +
272  11  ∈ W 	, we obtain u z2   u z1  for
 ∈ W 21 , where

W14 = 
 411 −012  0 721 +312  31  ∈ W 	

21
21
W 21 = W12
∪ W32
∪ W22

W11 = 
 011  202  011  011−1 −2   ∈ W 	
= 
 011 −202  0 021 +012  01  ∈ W 	
W21 = 

011  202  202  011−1 −2   ∈ W 	

= 
 011 −202  0 011 +212  01  ∈ W 	
W31 = 

021 +012  01 011 +212  01  ∈ W 	

W12 = 
 211 −162  0 321 +112  11  ∈ W 	
W22 = 
 211 −162  0 111 +272  11  ∈ W 	
W32 = 
 321 +112  11 111 +272  11  ∈ W 	
W13 = 
 311 −112  0 521 +212  21  ∈ W 	
W23 = 
 311 −112  0 211 +322  21  ∈ W 	

W24 = 
 411 −012  0 311 +322  31  ∈ W 	
W34 = 

721 +312  31 311 +322  31  ∈ W 	

We demonstrate the regions

Table 3.

= 
 211 −202  0 321 +112  11  ∈ W 	

Wj2

for j = 1 2 3 in Figure 4.

∪
 111 + 312  11 321
+ 112  11  ∈ W 	
We have u z2   u z3  for  ∈ W 23 , where W 23 can be
obtained in a similar manner by substituting all the inequalities implied by the above sets and simplifying:

Nondominated solutions for Example 3.

i

1

2

3

4

zi1
zi2
zi3

0.1
2.0
0.1

2.1
1.6
1.1

3.1
1.1
2.1

4.1
0.1
3.1

W 23 = 
 211 +372  21 521 +212  21  ∈ W 	
∪
 311 −162  0 521 +212  21  ∈ W 	
In a similar manner, we can divide the weight space into
four regions as shown in Figure 5. Notice that the regions,

Bozkurt et al.: Quantitative Comparison of Approximate Solution Sets

658

Operations Research 58(3), pp. 650–659, © 2010 INFORMS

Partition of the weight space between
alternatives.

Figure 5.

+



025258  11−321 /11

023688

11−111 /31

111 − 1 − 2  d1 d2

2

Calculating the ﬁrst and second components in a similar manner, we obtain IPF(z2  = 008217. When we make
similar calculations for z1 , z3 , and z4 , we obtain IPF(z1  =
010736, IPF(z3  = 004587, and IPF(z4  = 002268.

7. Conclusions

1
W i , are not convex in general as we mentioned before. By
overlaying Figure 4 on Figure 5, we can divide the W 2
region into three parts as shown in Figure 6. In part j,
u z2  is determined by the jth criterion, i.e., u z2  = j z2j
for j = 1 2 3. Similarly, we can divide each W i so that we
know the set of weights, W i ∩ Wji , for which u zi  = j zij
for j = 1 2 3. Then,


IPFzi  =
1 zi1 d2 d1 +
2 zi2 d2 d1
W i ∩W1i

+



W i ∩W3i

W i ∩W2i

1 − 1 − 2 zi3 d2 d1 

Speciﬁcally, utilizing Figures 4 and 5, the third component of IPF(z2  can be calculated as

1 − 1 − 2 zi3 d2 d1
W i ∩W3i

=


0

Figure 6.

023688  11−111 /27
11−111 /31

111 − 1 − 2  d1 d2

Partition of the W 2 region.

2

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

= 001302

1

In this paper, we have extended the previous research on
the IPF, a set functional assigning a scalar value to a
given discrete set of nondominated points. An exact calculation method for the IPF measure was developed using the
weighted Tchebycheff function as the underlying implicit
value function. The main step of the IPF calculation is
to ﬁnd optimal weight intervals for each nondominated
point. We show that every nondominated point has its optimal weight interval for the weighted Tchebycheff function.
Accordingly, all nondominated points, and not only the supported points in a set, contribute to the value of the IPF
measure when using the weighted Tchebycheff function.
Two- and three-criteria numerical examples illustrate the
desirable properties of the weighted Tchebycheff function,
providing a richer measure than the original IPF based on
a convex combination of objectives.
The calculation of the IPF measure becomes computationally more demanding as the number of criteria increases. Although we completely deﬁne the optimal
weight space for each nondominated solution, the integration over this space is not straightforward for more
than three criteria. Similar observations were made in Kim
et al. (2006), which includes a reasonable exact method
of obtaining IPF for k-objective problems when the decision maker’s preference function can be modeled by a
convex combination of the objectives. One of the main
observations on the computational burden of computing
IPF measures exactly is that CPU time increases roughly
exponentially with the number of objectives, and more than
90% of the time is consumed in computing the volumes of
k-convex polytopes (i.e., integration step).
In addition to an exact method, Kim et al. (2006)
presents computational results from experimentation with a
Monte Carlo approximation method. Guidelines on various
parameters of the approximation algorithm were provided
in Kim et al. (2006). The results show that such approximation methods can be much more practical to compute
IPF values with little error.
We expect such results to be similar in the case of preference functions of Tchebycheff functional form. Although
it is beyond the scope of this paper, numerical experimentation with such approximation methods is a worthwhile
area of future research.

Bozkurt et al.: Quantitative Comparison of Approximate Solution Sets
Operations Research 58(3), pp. 650–659, © 2010 INFORMS

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

Acknowledgments
Jyrki Wallenius acknowledges the ﬁnancial support of the
Academy of Finland, as well as the Foundation of Economic Education and Wihuri Foundation. Esma S. Gel
and John W. Fowler acknowledge the ﬁnancial support
of the National Science Foundation under contract DMII0121815. The authors thank Banu Tuna Lokman for her
help with some of the ﬁgures and W. Matthew Carlyle for
his help in the early part of this research.
References
Carlyle, W. M., J. W. Fowler, E. S. Gel, B. Kim. 2003. Quantitative
comparison of approximate solution sets for bi-criteria optimization
problems. Decision Sci. 34(1) 63–82.
Daskin, M. S., W. M. Carlyle, E. S. Gel, J. W. Fowler. 2003. Experiments
with an integrated preference function for assessing a genetic algorithm for the center-median location tradeoff. Proc. 2003 NSF Design
and Manufacturing Grantees Conf., Birmingham, AL, January 4–8.
National Science Foundation, Washington, DC, DMI-0121815/11.
Deb, K. 2001. Multi-Objective Optimization Using Evolutionary Algorithms. Wiley, Chichester, UK.
Dell, R. F., M. H. Karwan. 1990. An interactive MCDM weight space
reduction method utilizing a Tchebycheff utility function. Naval Res.
Logist. 37 263–277.
Fowler, J. W., B. Kim, W. M. Carlyle, E. S. Gel, S.-M. Horng. 2005.
Evaluating solution sets of a posteriori solution techniques for

659
bi-criteria combinatorial optimization problems. J. Scheduling 8(1)
75–96.
Kim, B., E. S. Gel, J. W. Fowler, W. M. Carlyle, J. Wallenius. 2006.
Evaluation of nondominated solution sets for k-objective optimization
problems: An exact method and approximations. Eur. J. Oper. Res.
173 565–582.
Korhonen, P., J. Wallenius. 1988. A pareto race. Naval Res. Logist. 35
615–623.
Lizarraga, G., A. Hernandez, S. Botello. 2008. A set of test cases for
performance measures in multiobjective optimization. A. Gelbukh,
E. F. Morales, eds. MICAI 2008: Advances in Artiﬁcial Intelligence.
Springer, Berlin-Heidelberg, 429–439.
Sayin, S. 2000. Measuring the quality of discrete representations of efﬁcient sets in multiple objective mathematical programming. Math.
Programming 87 543–560.
Steuer, R. E. 1986. Multiple Criteria Optimization: Theory, Computation
and Application. Wiley, New York.
Steuer, R. E., E. V. Choo. 1983. An interactive weighted Tchebycheff
procedure for multiple objective programming. Math. Programming
26 326–344.
Steuer, R. E., J. Silverman, A. W. Whisman. 1993. A combined Tchebycheff/aspiration criterion vector interactive multiobjective programming procedure. Management Sci. 39(10) 1255–1260.
Wierzbicki, A. P. 1980. The use of reference objectives in multiobjective optimization. G. Fandel, T. Gal, eds. Multiple Criteria Decision
Making: Theory and Applications, Vol. 177. Lecture Notes. Springer
Verlag, Berlin-Heidelberg, 468–486.
Wierzbicki, A. P. 1983. A mathematical basis for satisﬁcing decision making. Math. Model. 3 391–405.

European Journal of Operational Research 180 (2007) 1411–1426
www.elsevier.com/locate/ejor

O.R. Applications

A framework for evaluating remote diagnostics
investment decisions for semiconductor equipment suppliers
Cem Vardar, Esma S. Gel *, John W. Fowler
Department of Industrial Engineering, Arizona State University, P.O. Box 5906, Tempe, AZ 85287-5906, United States
Received 14 August 2005; accepted 14 May 2006
Available online 4 August 2006

Abstract
With advances in information technology, service activities for expensive equipment used in semiconductor manufacturing can be performed from a remote location. This capability is called remote diagnostics (RD). Currently, there are
intense development eﬀorts in the semiconductor industry for implementing RD in wafer fabrication facilities to reduce
maintenance and capital costs and improve productivity. In this paper, we develop a queueing-location model to analyze
the capacity and location problem of after sales service providers, considering the eﬀects of RD technology. Our model
optimizes the location, capacity and the type of service centers while taking congestion eﬀects into consideration. We solve
this model using a simulation optimization approach in which we use a genetic algorithm to search the solution space. We
demonstrate how our methodology can be used in strategic investment planning regarding the adoption of RD technology
and service center siting through a realistic case study.
 2006 Elsevier B.V. All rights reserved.
Keywords: Location; Simulation; Genetic algorithms; Remote diagnostics in ﬁeld service systems; Maintenance of semiconductor
equipment

1. Introduction
Semiconductor wafer fabrication is one of the
most complex manufacturing processes in the world.
Thus, the process requires very complex and expensive equipment. Keeping the equipment in working
condition is not a trivial task. Integrated circuit manufacturers spend millions of dollars on equipment
maintenance each year. Despite sophisticated maintenance strategies, various types of equipment fail or
*
Corresponding author. Tel.: +1 480 965 2906; fax: +1 480 965
8692.
E-mail address: esma.gel@asu.edu (E.S. Gel).

degrade in performance from time to time. In these
situations, outside service may be needed. This
‘‘after sales’’ service is often purchased in terms of
service agreements from the equipment manufacturers or other service providers. The terms of these
contracts include the response times, repair times,
and penalties that the service provider will pay in
case the requirements of the agreement are not met.
With recent advances in communication capabilities, it is now technologically possible to perform
some levels of equipment service remotely by the
service providers. That is, a service engineer can
log into the computers that control a tool via the
internet/intranet, etc., and perform the service on

0377-2217/$ - see front matter  2006 Elsevier B.V. All rights reserved.
doi:10.1016/j.ejor.2006.05.020

1412

C. Vardar et al. / European Journal of Operational Research 180 (2007) 1411–1426

the tool without physically being where the tool is.
This capability is often referred to as remote
diagnostics (RD) or e-diagnostics by industry
practitioners.
Currently, development eﬀorts are continuing by
both service providers and integrated circuit manufacturers to make use of this technology in order to
decrease equipment downtime and maintenance
costs. The possible beneﬁts of this new technology
continue to be thoroughly debated by the semiconductor industry, although the emphasis of these discussions has been mostly on developing enabling
information technologies, protocols, industry-wide
standards, etc. While leading integrated circuit manufacturers and service providers agree that the logistical and ﬁnancial beneﬁts of RD can be substantial
(Raman and Pillai, 2001; Sako et al., 2003), almost
no eﬀort has been spent on determining cost-eﬀective strategies to implement RD technology and
integrate it with the existing equipment maintenance
infrastructure.
Our objective in this research is to approach the
use of RD in a more strategic manner by including
a logistics perspective to address the problems that
might be encountered in maintenance resource planning (with respect to capital, personnel and equipment), contract negotiating, maintenance policy
improvement, etc. As RD technology takes its
expected place in the semiconductor industry, it is
expected to cause changes in current maintenance
and ﬁeld service practices. A mathematical capacity
planning model would be a useful tool for decision
makers of both service providers and integrated circuit manufacturers to evaluate the investment decision for RD technology and get ready for the
resulting changes in ﬁeld service systems.
In this paper, we present a queueing-location
model for a service provider for integrated circuit
manufacturing equipment. The queueing process
we are interested in is the queueing of expensive
equipment, waiting for service. Using this model,
we quantify the multiple tradeoﬀs between RD technology investments, personnel and traveling costs
and service levels to minimize the expected total cost
of a service provider subject to requirements of its
current service contracts. The queueing system we
consider involves modeling complexities including
general service times and a ﬁnite calling population.
Thus, we cannot adequately estimate the ﬁnancial
impact (i.e., penalties due to delays in service or
repair of equipment) of diﬀerent solution alternatives in closed form. We deal with this complexity

by employing a simulation optimization approach
in which we use a genetic algorithm as the search
strategy.
The main contributions of this paper are a model
to evaluate the beneﬁts of RD technology and a
realistic case study that shows how industry practitioners can use the approach and interpret the
results to make eﬀective capacity and location decisions. We use a simulation optimization approach
to heuristically solve this model. The solution of
our model will help decision makers in the semiconductor industry understand the structure of the ﬁeld
service support system of the industry and will provide insights about beneﬁts of RD technology with a
broader vision. The model is also expected to be
helpful in decision making concerning RD investments and strategic planning for integrated circuit
manufacturers, in addition to equipment manufacturers and service providers.
The rest of the paper is organized as follows. In
Section 2, we review the literature related to RD
and ﬁeld service system design. In Section 3, we
present the service provider’s model and we outline
our solution methodology in Section 4. We present
a small but realistic case study in Section 5. Finally,
we ﬁnish with conclusions and promising areas of
future research on the subject.
2. Literature survey
In this section, we review the literature related to
RD. The section consists of two parts. In the ﬁrst
part, we review the publications that directly recognize the diﬀerences between service systems with RD
and traditional ﬁeld service systems. Most of these
publications introduce the technology and predict
potential beneﬁts from it without any modeling
eﬀort. In the second part, we review papers that
model ﬁeld service systems or similar systems in a
much broader sense.
2.1. Remote diagnostics as a vision
RD is a popular subject in semiconductor industry publications. There are numerous articles in
trade magazines such as Semiconductor International and Future Fab International that analyze
the opportunities that RD present and obstacles in
its implementation. Wohlwend (2001) presents RD
as an integral part of an e-factory vision that is being
developed by semiconductor industry organizations
such as International SEMATECH and JAITA

C. Vardar et al. / European Journal of Operational Research 180 (2007) 1411–1426

(Japan Electronics & Information Industry Association) to reduce overall costs and improve capabilities of semiconductor manufacturing companies.
Perez (2001) and Hanssmann (2001) discuss how
tool monitoring by RD can both enhance equipment lifecycle management and reduce equipment
maintenance costs. Bloss and Pillai (2001) and
Raman and Pillai (2001) also identify RD as a great
opportunity for e-manufacturing and point out the
obstacles that needs to be overcome in implementing it. Recently, Sako et al. (2003) present some realized beneﬁts of RD technology such as reduced
mean time to repair and reduced maintenance management costs. Overall, there is a consensus among
industry practitioners that RD technology will be
a useful tool in improving current maintenance
strategies.
Lee (1998) is the ﬁrst refereed journal article that
mentions RD as an option in ﬁeld service activities.
The author refers to RD and maintenance as ‘‘teleservice engineering,’’ and presents several research
opportunities in information technology on issues
such as diagnostic system standardization, techniques for collaborative maintenance and diagnostics, self maintenance and availability. Agnihotri
et al. (2002) present a conceptual model for assessing the role and inﬂuence of technology in creating
an eﬀective ﬁeld service organization. The authors
claim that using available technologies, equal
emphasis should be given to customer relations,
having well-trained and motivated personnel and
the actual encounter between the customer and the
service provider.
In addition to reduction in maintenance costs,
RD can enhance equipment eﬃciency by continuously monitoring important equipment and using
the data generated in making maintenance decisions. Sloan and Shanthikumar (2002) present a
model that uses in-line equipment condition and
yield information for optimizing maintenance
scheduling and dispatching of lots. They assume
that the equipment condition and yield data is monitored continuously. Pool (2001) reports a similar
continuous remote monitoring practice that General
Electric uses to improve aircraft maintenance. In
this system, several performance indicators of aircraft are monitored during ﬂight. If an indicator is
out of control limits and service is required,
arrangements for the maintenance (i.e., service engineers, spare parts) are made before the ﬂight arrives
so that the next ﬂight of the aircraft is not delayed.
These examples are just some of the ways service

1413

providers can use the data obtained through RD
monitoring capabilities to make better service
decisions.
2.2. Field service system design
Field service systems are designed to meet the service needs of equipment or products after they are
sold. Hence, service is usually performed at the customer’s site. There are two main problems intertwined in the design of ﬁeld service systems. The
ﬁrst problem involves locating the regional service
sites in ‘good’ locations, while the second problem
considers capacity planning in these regional sites.
Both problems have been studied extensively in
the literature. In maintenance and service capacity
planning, researchers have developed models for
analyzing and/or optimizing the maintenance crew
capacity, spare parts inventory, number of spare
machines, etc., to minimize the total cost of the system. In this section, we ﬁrst review studies that deal
only with the capacity planning decision and after
that we give examples of studies that combine both
problems. Most of the studies in the second group
come from a diﬀerent application area, which is
location and capacity planning of emergency service
systems.
Waller (1994) is one of the ﬁrst papers that model
ﬁeld service systems as a closed queueing network
for staﬃng decisions. Spare parts management is
also included to a certain extent in the model. In this
model, when a tool goes down and needs service, it
enters a queue for a ﬁeld engineer. After it is serviced, with probability p it returns to the up state
and with probability (1  p) it joins a diﬀerent
queue, which models waiting for a spare part. After
a random amount of time in the spare parts queue,
it returns to the ﬁeld engineer’s queue. The author
states that each spare parts inventory policy results
in a diﬀerent probability, p. Although this paper is a
good modeling paper, it does not attempt to optimize the staﬃng levels or the inventory policy for
spare parts.
Papadopoulos (1996) extended Waller (1994) by
considering multiple customer classes with diﬀerent
priorities and implementing the methodology on a
real life case study. To solve for the average waiting
time of customers, the author used priority mean
value analysis (Shalev-Oren et al., 1984) and showed
that the solution methodology was eﬃcient and produced more realistic results than the one presented
in Waller (1994). For demonstration, the paper

1414

C. Vardar et al. / European Journal of Operational Research 180 (2007) 1411–1426

presents a real life dataset obtained from an after
sales service department of a computer manufacturer, and discusses how accurately the results from
both models replicate real life data.
The two papers summarized above analyze the
problem from the point of view of the service provider. Kennedy (1993) analyzed the service problem
from the point of view of a company that purchases
maintenance services. For some of the maintenance
service needs, the company has the option of either
using in-house personnel or outsourcing it from a
service provider. The author constructed a linear
programming model to decide which types and what
percentage of service will be provided in-house versus outside (by a contractor) so that total costs are
minimized. The model includes constraints on the
available man-hours for each type of service personnel. In the second part of the paper, the author
extends this model to incorporate a ﬁxed cost for
performing in-house maintenance and learning
eﬀects for the in-house maintenance staﬀ. These
extensions produce a nonlinear integer model. No
real life applications of the problem or solution
methodology were mentioned in the paper.
Another thread in the ﬁeld service literature is
optimizing the capacity of an in-house maintenance
team and number of stand-by units so that the total
cost of maintenance staﬀ, cost of owning the standby units and cost of interruptions in the process is
minimized. Catuneanu et al. (1990), Lin et al.
(1994, 2000, 2001) and Zeng and Zhang (1997) are
examples that model these kinds of systems. They
diﬀer from each other by various assumptions such
as the modeling of the service system, usage of cold
and warm stand-by units, and choice of solution
methodology according to the resulting model.
The basic modeling methodology used in all these
papers is similar. They model the system by using
a closed queueing network but their solution methods diﬀer depending on the structure of the system
modeled. They either derive closed form expressions
for the system performance and use calculus methods, or use techniques like enumeration, genetic
algorithms, and simulated annealing to optimize a
system performance measure.
Jayaraman and Srivastava (1995) is a good example that considers both the siting of the service centers and the capacity to be allocated at each service
center. The authors study a generic system with
multiple facilities and multiple types of equipment.
Their model seeks to locate a given number of facilities in a network and allocation of diﬀerent levels of

equipment to these facilities such that the expected
coverage of demand is maximized. They recognize
the fact that due to the random demand for service,
even when a site is ‘‘covered,’’ equipment needed for
the service may not be available at all times. They
model this unavailability by using a probability of
service equipment being available and assume that
this probability is the same for all the equipment
at all the sites, which may not be a realistic assumption, since this probability is expected to be lower
for service sites that cover more sites than other service sites. The resulting model is a mixed integer
programming (MIP) model. They solve 30 node
problems using an oﬀ-the-shelf MIP solver. They
present extensive results from 30 node problem
instances and analyze the results generated by the
MIP.
We have not come across other signiﬁcant studies
that model ﬁeld service systems with both capacity
and location decisions per se, but there are numerous studies in siting and capacity planning of emergency vehicle systems such as ambulance services or
ﬁreﬁghting services. Brotcorne et al. (2003) provides
an excellent review on the subject. In this paper, the
authors classify the research in this ﬁeld into two
main classes.
(1) Deterministic models: These are mainly set
covering models in which either covered
demand is maximized with limited capacity,
or the capacity needed is minimized, subject
to all demand being covered. These models
do not consider the congestion eﬀects in service sites.
(2) Stochastic static models: In these models, congestion caused by queueing eﬀects of stochastic arrivals and service times in the service
sites are represented explicitly. In the earlier
studies, congestion is modeled mainly by
limiting the workload at service sites either
by constraints or by a penalty function in
the objective function.
In addition to these static models, dynamic models that relocate ambulances from site to site according to system status are also reviewed in Brotcorne
et al. (2003). For a more general review of research
that belongs to the ﬁrst category above, refer to
Owen and Daskin (1998) which reviews the strategic
facility location modeling literature.
Since congestion eﬀects need to be explicitly
modeled for the service system that we consider in

C. Vardar et al. / European Journal of Operational Research 180 (2007) 1411–1426

this paper, studies that belong to the second category are more relevant to our work. Desrochers
and Marcotte (1995) is a good example in which
congestion is modeled by a nonlinear penalty function in the objective function. For a certain form of
penalty function, the authors prove that the objective function is convex, and solve the problem using
a column generation technique.
Marianov and ReVelle (1995) is the ﬁrst study to
incorporate a queueing model into a set covering
problem. The most important contribution of this
paper to existing models is that the time (or distance) between the service site and demand point
is modeled as a random variable. The earlier models
were not capable of accounting for stochasticity in
distance or travel time. Marianov and Serra (2001)
extend Marianov and ReVelle (1995) by modeling
a hierarchical service structure.
Although there are signiﬁcant similarities
between emergency service systems and ﬁeld service
systems, the size of the population from which
demand occurs is quite diﬀerent. In emergency system models, assuming that the demand arrivals
come from an inﬁnite calling population is reasonable since the demand comes from a neighborhood
or a town which has thousands of residents or
houses. However, for a ﬁeld service system (and particularly for semiconductor manufacturing facilities), an inﬁnite calling population is not a realistic
assumption. This type of a service system requires
a closed queueing network, which is much harder
to deal with than open queueing networks of emergency service systems.
Kochel et al. (2003), which investigates optimal
capacity decisions for a car rental company, is a
good example where an inﬁnite calling population
assumption is not reasonable. The capacity decisions
considered in this system include how many vehicles
to own, how many vehicles to allocate to diﬀerent
branches and what kind of relocation policy should
be used to relocate the empty vehicles between
branches. In the ﬁrst part of the paper the authors
simplify the decision alternatives by ignoring the
relocation option. This reduces the model to a
product form queueing network which can be analyzed with mean value analysis and optimized with
a greedy algorithm. When relocation of vehicles is
allowed in the network, the structure of the queueing
network is corrupted and it is no longer a product
form queueing network. At this point the authors
choose to use simulation optimization to ﬁnd the
optimal relocation policy parameters and optimal

1415

vehicle ﬂeet size. They use a simulation model as a
black box to calculate the performance of diﬀerent
solution alternatives and a genetic algorithm to
search through the search space.
In this paper, we develop a model for capacity/
location planning of ﬁeld service system of an original equipment manufacturer for the semiconductor
industry. In our model, the maintenance system is
modeled as a closed queueing network and traveling
of the service engineers to customer’s site is modeled
explicitly. Currently, no such detailed documented
method exists for ﬁeld service capacity and location
planning in semiconductor manufacturing industry.
We will use an evolutionary heuristic and simulation optimization methods to solve the resulting
capacitated location problem. We also demonstrate
how the model can be utilized for strategic decision
support through a realistic case study.
3. Service provider’s problem
We call a company that sells maintenance service
contracts to integrated circuit manufacturers a service provider. Service providers are usually also the
original equipment manufacturers that design and
manufacture high technology equipment used in
semiconductor manufacturing. Original equipment
manufacturers typically sell service contracts for
the equipment they manufacture. The terms of these
contracts include the required response times, repair
times, and penalties that service provider will pay
when the requirements in the agreement are not met.
We consider the problem of a service provider
who wants to design the minimum expected total
cost ﬁeld service system that satisﬁes the requirements of its contracts. With the RD technology, service providers have the option of investing in RD
technology and opening RD-enabled service centers
instead of traditional service centers. By having RDenabled service centers a service provider can resolve
some types of service requests without the need to
travel to the site of the customer requesting service.
Hence, a service provider needs to make decisions
on the following strategic and tactical issues:
1. What type of regional service centers (remote or
traditional) should be opened.
2. Where to open regional service centers.
3. How to assign customers to regional service
centers.
4. How many service engineers of diﬀerent levels to
employ at each regional service center.

1416

C. Vardar et al. / European Journal of Operational Research 180 (2007) 1411–1426

3.1. Assumptions of the model
• The service provider’s ﬁeld service system is a
network consisting of diﬀerent types of personnel. We further assume that each service request
is handled through a prespeciﬁed protocol, and
the routing probabilities through the ﬁeld service
network are known.
• The service provider can invest in RD technology
and open RD-enabled regional service centers.
With this investment, the service provider gains
the capability of servicing a proportion of incoming service requests remotely without the need to
travel to the customer site, which results in
shorter service times. For traditional service centers, we assume that mean service time depends
on the distance between the customer and the service center. In addition, we assume that a travel
cost is incurred for each trip to the customer’s site.
• There are a ﬁnite number of service contracts
that the service provider needs to service, resulting in a ﬁnite calling population of tools requiring service. For each customer location there is
a contract for all the tools in that location.
• The service provider pays a penalty to the customer based on the average number of down
tools during a year in a contract.
• Each service contract is assigned to a service center
and there is no state dependent cooperation
between service centers. This assumption is valid
for the current ﬁeld service systems without RD
technology due to geographical and business process constraints. Cooperation between service centers with remote diagnostic technology might
bring some beneﬁts such as higher availability of
service personnel due to pooling of the resources.
However currently there is no information about
the policies for governing how such cooperation
between the service centers would work. Thus we
have assumed no cooperation in our current model.
• Each piece of equipment has an exponential time
to failure distribution and repair times follow a
triangular distribution.
• Service engineers can only be employed at service
centers that are open and in operation (i.e., costs
of opening and maintaining the service center
must be incurred).
3.2. The mathematical programming model
In this section, a general formulation for the service provider’s ﬁeld service system design problem is

presented. The objective of the model is to minimize
the costs of establishing and operating a ﬁeld service
system while all the requirements of the service contracts are met. The following summarize the notation used in the model.
Sets
I
J
L
M

set of service contracts
set of possible service center locations
set of service engineer levels
set of service center types {1: traditional; 2:
RD-enabled}

Model parameters
T
ﬁxed cost of technology investment for RD,
discounted to per year cost assuming a
5-year lifetime
Fjm
ﬁxed cost of opening a service center at site
j of type m, discounted to per year cost
assuming a 5-year lifetime, j 2 J, m 2 M
PCjlm yearly cost of employing a service engineer
of level l at site j at a service center of type
m, j 2 J, l 2 L, m 2 M
DCi
penalty paid for one year of tool downtime
in contract i, i 2 I
TCij
cost for a service engineer to travel from
service center j to the location of contract
i, i 2 I
Qi
number of tools in contract i, i 2 I
Rijm
vector of queueing network parameters for
a contract i getting service from a service
center of type m at site j, such as
ki
rate of request for service of contract i, i 2 I
lijm
average repair time for a contract i from
service center j of type m, j 2 J, i 2 I, m 2 M
pil1 l2
transfer probabilities for contract i in service network from l1 to l2, i 2 I, l1, l2 2 L
P
proportion of service requests that can be
serviced remotely from a remote diagnostics capable service center
Decision variables
y :¼ 1 if the investment for RD technology is
made, and 0 otherwise,
xjm :¼ 1 if a service center is opened at site j of
type m, and 0 otherwise for all j 2 J, m 2 M,
zijm :¼ 1 if contract i is assigned to service center j
that is of type m, 0 otherwise, for all i 2 I, j 2 J,
m 2 M,
sejlm :¼ number of service engineers employed in
service center j at level l at a service center of type
m, j 2 J, l 2 L, m 2 M,

C. Vardar et al. / European Journal of Operational Research 180 (2007) 1411–1426

The problem of the service provider can then be
written as follows:
XX
Min
Ty þ
F jm xjm
j2J m2M

þ

XXX
j2J

þ

X

PC jlm sejlm

l2L m2M

DC i fi ðy;x;z;se; Q;R;P Þ

i2I

þ

XX
i2I

TC ij gij ðy;x;z;se; Q;R;P Þ

j2J

ð1Þ
subject to: Kxjm P sejlm for all j 2 J ; l 2 L; and m 2 M;
ð2Þ
y P xj2 for all j 2 J ;

ð3Þ

zijm 6 xjm for all i 2 I; j 2 J and m 2 M;
XX

ð4Þ
zijm ¼ 1 for all i 2 I;

ð5Þ

m2M j2J

seilm integer; y;xjm ;zijm 2 f0;1g;
for all i 2 I; j 2 J ; m 2 M; l 2 L:

The objective function (1) consists of a sum of
ﬁve terms and represents the expected value of total
annual cost of the service provider. The ﬁrst term is
the cost of investing in RD technology and the second term is the total ﬁxed costs incurred for opening
service centers. The third term in the objective function is the total personnel costs and the fourth term
is the total penalty cost paid for down tools. Function fi(Æ) represents the time weighted average number of down tools in contract i, and DCi is the
penalty cost paid for one year of tool downtime in
contract i. Average number of down tools (ﬁ(Æ)) is
a time-weighted average which takes the length of
downtime into account. The ﬁfth term is the total
traveling costs incurred by service engineers while
traveling to customer’s location for service. Function gij(Æ) represents the average number of total
round trips per year made from service center j to
the location of contract i.
The constraints of the model closely resemble
those in traditional location models. Constraint set
(2) ensures that no service engineer is employed at
a service center that is not open, where K represents
a big enough number to ensure that the constraint
works. Constraint set (3) ensures that RD-enabled
service centers cannot be opened without incurring
investment cost for RD. Constraint set (4) ensures
that a contact cannot be assigned to a site unless that

1417

site is opened. Finally, constraint set (5) ensures that
each contact is assigned to exactly one service center.
Although this model represents the problem of a
service provider suﬃciently, note that this is not a
regular mixed integer programming model that
can be solved by a general integer programming
solver. This is due to the fact that, in the fourth
and ﬁfth terms in the objective function (1) we
embed a queueing system into the model to take
congestion eﬀects into account. No closed form
representations of functions fi(Æ) and gij(Æ) exist.
However, functions fi(Æ) and gij(Æ) can be estimated
by using a simulation model developed for this ﬁeld
service system under a given solution alternative.
There are numerous location models in the literature that consider congestion eﬀects along with the
location of facilities and allocation of demand to
these facilities decisions. For a comprehensive
review of location models considering congestion
eﬀects see Section 4.1 of Owen and Daskin (1998).
Since most facility location allocation problems
are NP-complete, considering congestion eﬀects
while making location decisions results in either
intractable or oversimpliﬁed models. In the above
presented form, our model cannot be solved with
traditional mathematical solution techniques either.
In the next section, we present our solution methodology which deals with this problem.
4. Solution methodology
Since some terms in the objective function of our
model cannot be represented adequately by closed
form expressions, we cannot solve the problem with
traditional mathematical programming techniques.
To solve this model, we use a simulation optimization framework, similar to the approach used in
Kochel et al. (2003), where the performance measures fi(Æ) and gij(Æ) are evaluated by a simulation
model and a genetic algorithm is used to search
the solution space.
Simulation modeling has traditionally been used
for descriptive modeling of complex stochastic systems. Recently, researchers have begun to study
how this technique could also be used as a prescriptive modeling technique. For a tutorial on simulation optimization, see Section 12.6 in Law and
Kelton (2000). Fu (1994) and more recently Fu
(2001) provide excellent reviews on simulation optimization research.
The early research in the ﬁeld of simulation optimization was focused on methods such as stochastic

1418

C. Vardar et al. / European Journal of Operational Research 180 (2007) 1411–1426

approximation and gradient-based approaches.
Although these methods work well with continuous
decisions variables, they cannot handle binary and
integer variables in their pure form. (See Gerencsér
(1999) for using gradient based methods for models
with discrete variables.) For these types of problems, researchers have started using metaheuristics
such as genetic algorithms and simulated annealing,
which are commonly used in the ﬁeld of combinatorial optimization.
The main advantage of using metaheuristics as
the search strategy in simulation optimization is
their ability to function reasonably in noisy environments. This enables straightforward application of
these methods to simulation optimization by using
average of a ﬁxed number of simulation replications
as the deterministic evaluation of solutions. This
approach works well particularly when the variability of the simulation outputs between replications is
fairly low. Ahmed et al. (1997), Paul and Chanev
(1998) and Kochel et al. (2003) apply these types of
techniques to diﬀerent types of simulation optimization problems and report success. However, if the
variability of the simulation outputs between replications is not low, this straightforward approach
of making a ﬁxed number of simulation replications
fails to adequately guide the heuristic search. Studies
like Ahmed and Alkhamis (2002), Boesel et al.
(2003a,b) and Rosen and Harmonosky (2005)
develop methodologies to use in heuristics for simulation optimization while taking advantage of the
special structures of random outputs of simulation
runs, which leads to making a diﬀerent number of
replications for diﬀerent solutions to improve solution times and quality. These techniques outperform
the more straightforward implementations in terms
of solution speed and quality when the variability
of output statistics between replications is large.
In this study, we observed that for smaller pilot
problems the variability of output statistics between
diﬀerent replications is not large enough to signiﬁcantly inﬂuence the performance of the evolutionary
algorithm. Thus, we have used the straightforward
approach of making a ﬁxed number of replications
for each alternative during the GA search in the case
study presented in the next section. Our criteria for
good enough number of replications was: a 95%
conﬁdence interval constructed on the total
expected cost should not be larger than 1% of the
mean. For the case study described in the following
section we made pilot runs on 1000 random solution
alternatives and observed that 10 replications

resulted in the desired precision for all of these alternatives. While using this straightforward approach
we are bound to make some errors in comparison
of close alternatives during the genetic algorithm
search but this does not signiﬁcantly aﬀect the progress of the algorithm towards better solutions.
After the genetic algorithm run is completed we
use the ‘‘screen, restart and select’’ procedure
described in Boesel et al. (2003b) on all solutions
visited during the genetic algorithm search for
selecting the best solution. This procedure ensures
that the solution we have picked is in fact the best
solution among visited solutions with a given conﬁdence level and an indiﬀerence zone guarantee. Our
general solution framework is shown in Fig. 1.
We start by generating a set of random alternatives and evaluating these alternatives through the
simulation model. This set makes up the ﬁrst generation of the genetic algorithm (GA). The GA generates
new solutions using standard mutation and crossover
techniques and evaluates the objective functions for
these solutions with the simulation model.
In solving the case study presented in the next
section, we use a population size of 30, and at each
iteration, we generate 10 new solutions with mutation and 20 new solutions with crossover using randomly selected solutions from the current
generation and generate a candidate set consisting
of 60 solutions (i.e., 30 from the current generation
and 30 from the crossover and mutation) that will
go through the selection operation. We copy the
best solution from the candidate set into the next
generation and use tournament selection as our
selection mechanism to select the remaining members of the next generation from the candidate set.
We make a total of 100 iterations and employed elitism by never eliminating the observed best solution
in a generation between the iterations. We have set
these parameters after pilot experimentation with
the model to increase the convergence rate. Our
chromosome structure consists of the following
parts.
• A single binary ﬁeld for representing if RD
investment is made or not (y).
• A binary ﬁeld for each possible site representing
if that site is open or closed (xjm).
• A numeric ﬁeld for each contract representing the
assignment between the contracts and service
centers (in place of zijm).
• A numeric ﬁeld for each level of service engineer
at each possible site (sejlm).

C. Vardar et al. / European Journal of Operational Research 180 (2007) 1411–1426

1419

Initial alternative
generator

Evaluate
solutions

Generate new
solutions
Simulation model of
the field service
system

Evaluate new
solutions

Solution
evaluator

Make Selection

Report10 best
solutions

GA Solver

Local neighborhood
optimization by
enumeration

Screen and Select from
Boesel et al. (2003b)

Report best
solution

Fig. 1. General solution framework.

For example, the structure of a sample chromosome for a problem instance with 7 contracts, 4
possible sites, and with 2 levels of service engineer
(this is also the conﬁguration in the case study presented in the next section) the GA can be seen in
Table 1.
Due to the assignment variables, infeasible solutions can be generated after crossover or mutation.
For example, after a crossover, a contract may be
assigned to a service center which is not open. In
such cases, the algorithm assigns the contract with
the infeasible assignment to one of the open service
centers randomly.
After the GA is run, it reports out the 10 best
solutions found during the entire GA run. The algorithm performs a local enumerative neighborhood
search around these 10 solutions to further improve
objective values. In the local neighborhood search,
we keep the location, type and capacity constant
and try all possible contract assignments. This step
was added after we have observed that GA’s mutations and crossovers were often not successful in
ﬁnding the best assignments.

Table 1
Chromosome structure
Chromosome ﬁelds

Chromosome value

RD investment made of not?
Possible site open or not
Site 1
Site 2
Site 3
Site 4
Assignment of contracts
Cont. 1
Cont. 2
Cont. 3
Cont. 4
Cont. 5
Cont. 6
Cont. 7
Number of SEs
Site 1
Level 1
Level 2
Site 2
Level 1
Level 2
Site 3
Level 1
Level 2
Site 4
Level 1
Level 2

1
1
0
1
0
1
3
1
1
3
1
3
2
1
3
2
1
2
3
4

We keep the objective function value evaluations
of all the solutions that the algorithm visits during

1420

C. Vardar et al. / European Journal of Operational Research 180 (2007) 1411–1426

electronic or software problems. All personnel
who perform onsite and oﬀsite service are assumed
to be employed by the service provider.
When a tool requires service, an onsite FST initially responds to the service request and diagnoses
the problem. If the problem can be resolved by a
FST, the FST proceeds to repair the tool. Otherwise, he forwards the problem to a fellow FST. If
the FST diagnoses the problem as an AE problem,
the service request is forwarded to an onsite AE.
When an onsite FST or AE cannot resolve a service
request, they elevate the service request to be
responded by the regional service center assigned
to this speciﬁc fab. The same structure of FSTs
and AEs are also present in regional service centers.
The queueing network structure used in the case
study can be seen in Fig. 2. In Fig. 2, the box on the
left represents the fab and the one on the right represents the regional service center assigned to this
fab. The descriptions of the nodes in the ﬁgure are
given below.

the GA and local search at the last step in a list. We
use the ‘screen, restart and select’ described in Boesel
et al. (2003b) on all solutions in the list for selecting
the best solution. We used 95% as the conﬁdence
level and $10,000 as the indiﬀerence zone parameter
for our case study.
5. Case study from the semiconductor industry
In this section, we analyze the situation for a
hypothetical service provider that sells manufacturing equipment and service contracts for those tools
to wafer fabrication facilities (fabs). This case study
was co-developed by industry experts to make it
representative of the real service providers in the
semiconductor supplier industry. The purpose of
the case study is to demonstrate how the model
developed in this paper can be used to support strategic decision making on ﬁeld service systems and
RD technology.
The ﬁeld service systems for fab tools usually
consist of two levels of support; onsite and oﬀsite.
Onsite service is performed by personnel dedicated
to a fab. They are available to that speciﬁc fab at
all times, and do not respond to service requests
from other customers of the service provider. Oﬀsite
personnel work in a regional service center and
respond to service requests from multiple customers
or fabs (i.e., diﬀerent contracts in our model). In
general, there are two types of service personnel, differing by the type of problems they handle: ﬁeld service technicians (FST) and application engineers
(AE). FSTs generally handle requests that are due
to mechanical problems and AEs are generally
responsible for more complex problems, such as

• UP: This node represents a group of tools that
the service provider is responsible for providing
service (i.e., a contract).
• FSTf: This node represents onsite FSTs.
• AEf: This node represents onsite AEs.
• FSTr: This node represents FSTs in the regional
service center assigned to the fab.
• AEr: This node represents AEs in the regional
service center assigned to the fab.
The arcs represent the ﬂow of the service requests
between the nodes and the values on the arcs give
the probability of a service request taking that path
after it leaves a node. For example, the ﬂow leaving

.9

.80

.1
.85

1

UP

FSTf

FSTf

FSTr
.1

.1

.15

AE f

.85

AEr
.15

1

FAB

Service center

Fig. 2. The queueing network and routing probabilities.

C. Vardar et al. / European Journal of Operational Research 180 (2007) 1411–1426

ments of fabs to the service centers. The list of the
fab locations, mean times to failure per tool and
the number of tools data can be seen in Table 2.
We assume an exponential distribution for the time
to failure, although any other distribution could be
incorporated.
The mean service times for onsite service is 0.5
hours for FST’s diagnosis, 2.5 hours for FST service
and 4.8 hours for AE service for all fabs. The average service time of oﬀsite service depends on the
proximity of the assigned service center to the customer site due to the need of travel of a service engineer. Mean service time data for oﬀsite service is
provided in Table 3. Note that the mean service time
for remote service does not depend on the location
of the service center and the fab. We assume a symmetrical triangular distribution for both onsite and
oﬀsite service times with ±30% oﬀset from the mean
for the minimum and maximum values of the triangular distribution.

the ‘UP’ node represents service request generation
(i.e., a tool is down), the ﬂow between FSTs and
AEs represent forwarding of the service request
between diﬀerent types of personnel and the ﬂow
going back into the ‘UP’ node represents a resolved
service request (i.e., the tool is up again). Note that
multiple fabs can be assigned to the same service
center.
Vardar et al. (2004) studied a similar maintenance network structure with a simpliﬁed case problem which involved one fab site and two possible
service center locations. For getting exact results
for the performance measures the authors used
analytical queueing methods which required many
simplifying assumptions. These assumptions include
exponential service times and separate service personnel for RD and traditional service. In addition
the model can handle only a single fab. For validation purposes we have duplicated this network
model in our simulation and genetic algorithm
framework. Our simulation model and genetic algorithm were able to generate the same results as complete enumeration with the analytical queueing
model approach.
The service provider in our case study has contracts with seven fabs with ten tools at each fab,
as in Fig. 3. The service provider has four candidate
sites for regional service centers in diﬀerent geographies. In Fig. 3, the candidate service center sites are
in the middle and the 7 contracted fabs are on the
top and bottom of the ﬁgure. Arcs between fabs
and service centers represent the possible assign-

.85

FST

FST

UP

.1
AEf

AEf

FSTr

California

.1

.85

Germany
AEr

.8
.85

.8
.85

1

FST
f

FST

.85

1

FST
f

.1

UP

FST

.85

Germany

AEf

FST
f

.1

FST

UP

f

.15

Service
provider’s
Taiwan regional
locations

.1

AEr

f

Texas

FSTr

.1

.8

UP

AEf

.15

FAB2

f

AEf

.1

FAB2

1

Fabs

FST
f

AEr

.1

UP

.1

FAB2

FST

FST
f

FST

.85

Arizona

f

.85

1

FST

.15

.8

.15

Oregon

FSTr

FST

10
10
10
10
10
10
10

.85

AEr

UP

40
40
40
40
40
40
40

.85

FSTr

.85

Arizona
California
Oregon
Texas
Germany
Japan
Taiwan

f

.15

1

Number of tools

f

f

.1

Mean time to fail (hour)

.8
.85

1

f

UP

Fab location

.8

.8
1

Table 2
List of fab locations and tool failure data

California

Arizona

1421

.1

f

.15

AEf

.15

.85

Taiwan

.85

Japan

Fig. 3. Basic structure of the case study.

AEf

Fabs

1422

C. Vardar et al. / European Journal of Operational Research 180 (2007) 1411–1426

Table 3
Mean service time for oﬀsite service
Level of service

Fab location (i)

Mean service times (in hours)
If service engineer has to travel (m = 0)
Service center location (j)

If service performed remotely (m = 1)
Service center location (j)

Arizona

California

Taiwan

Germany

Arizona

California

Taiwan

Germany

FST service time (l = 0)

Arizona
California
Oregon
Texas
Germany
Japan
Taiwan

6
12
12
12
18
24
24

12
6
12
12
18
24
24

24
24
24
24
18
9
6

18
18
18
18
6
18
18

3
3
3
3
3
3
3

3
3
3
3
3
3
3

3
3
3
3
3
3
3

3
3
3
3
3
3
3

AE service time (l = 1)

Arizona
California
Oregon
Texas
Germany
Japan
Taiwan

8
14
14
14
20
26
26

14
8
14
14
20
26
26

26
26
26
26
20
11
8

20
20
20
20
8
20
20

4.8
4.8
4.8
4.8
4.8
4.8
4.8

4.8
4.8
4.8
4.8
4.8
4.8
4.8

4.8
4.8
4.8
4.8
4.8
4.8
4.8

4.8
4.8
4.8
4.8
4.8
4.8
4.8

The simulation model for the case study was
developed in C++ and has the ﬂexibility to model
the alternatives generated by the genetic algorithm.
When the genetic algorithm needs to evaluate an
alternative a new simulation model instance is created for that alternative and 10 replications of
60,000 simulation hour runs are performed.
Table 4 shows the travel cost incurred with each
round trip of a service engineer from service center
to the customer and back. The annual personnel cost
is $50,000 and $70,000 per year for FSTs and AEs,
respectively. The yearly ﬁxed site opening and operating cost is $100,000 for all of the four possible sites.
The penalty cost for down tools is $150,000 per tool
per year (i.e., if on the average a tool is down 20% of
the time during a year service provider pays a penalty cost of (20%)($150,000) = $30,000).
The only remaining cost parameter that needs to
be included is the ﬁxed cost of RD technology, T,
which enables the service provider to use RD technology to handle service requests remotely. This
cost measure highly depends on the eﬀectiveness of
RD technology (which reﬂects itself through P,
the probability of remote service) and is not easy
to estimate. We assume that the higher the investment made for RD, the higher the probability of
resolving a service request remotely. For this reason, we ﬁxed this cost to zero, parameterized P
and solved the problem instance multiple times with
diﬀerent values of P, starting with P = 0 up to P = 1
with a step size of 0.1. Note that, P = 0 represents

Table 4
Travel costs
Fab location

Service center location (j)
Arizona

California

Taiwan

Germany

Arizona
California
Oregon
Texas
Germany
Japan
Taiwan

$0
$300
$300
$300
$600
$1000
$1000

$200
$0
$200
$200
$1000
$1000
$1000

$1000
$1000
$1000
$1000
$600
$300
$200

$600
$600
$600
$600
$0
$600
$600

the extreme case in which none of the service
requests can be handled remotely. Similarly, P = 1
represents the extreme case where all service
requests can be handled remotely. P values between
0 and 1 represent diﬀerent levels of RD technology
eﬀectiveness between the two extremes.
For example, when we solved the problem for
P = 0.5 case, in the best solution found by the algorithm, service centers at California and Taiwan were
opened with remote capability and a total of 6 oﬀsite and 19 onsite service engineers were employed.
Arizona, California, Oregon and Texas were
assigned to the service center at California and the
remaining fabs were assigned to the service center
in Taiwan. The expected total cost for this scenario
came out to be $3450 K. In a similar fashion, we
solved the problem for 11 diﬀerent P values.
Since we have ﬁxed the RD technology investment cost to zero in all cases, the resulting objective

C. Vardar et al. / European Journal of Operational Research 180 (2007) 1411–1426

that the investment justiﬁed in RD technology
increases linearly at the beginning and increases
more steeply for values of P > 0.7.
To understand the steep increase in the value of
RD technology that we observe in Fig. 4, we present
a breakdown of the total cost (see Eq. (1)) into its ﬁve
components in Fig. 5. The terms that the diﬀerent
cost components correspond to in the objective function are given in the legend of the graph. We also separated personnel costs (term 3 in objective function)
into onsite and oﬀsite personnel costs. Observe that,
as the eﬀectiveness of RD increases (i.e., an RD technology with higher P), the traveling and service penalty costs decrease in general, whereas site opening
and oﬀsite service engineer costs stay the same until
P = 0.7 and then decrease. At this point, the eﬀectiveness of RD technology causes the optimal solution to
change from opening two service centers to one

function values of diﬀerent cases are comparable.
Note that as P increases, the objective function
value is bound to decrease since there is no added
RD investment cost and since with RD, some portion of the service requests will be resolved more
quickly and without any travel cost. We take
P = 0 case as our base case, and the cost diﬀerence
between the base case and the P > 0 cases give the
value of RD technology investment for that particular RD eﬀectiveness level. For example, the
expected total costs for P = 0 and P = 0.5 are
$3.638 K and $3.245 K respectively. The cost diﬀerence between two cases is the maximum amount the
service provider should be willing to pay for the RD
technology capability at P = 0.5 level.
Fig. 4 shows the plot of the total cost for the best
solutions found by the algorithm and the value of
the RD technology for each instance of P. Observe

Investment justified and total cost for different levels of RD
technology

4000

1200

3000
800

2500
2000

600

1500
400
1000
200

Total Cost (in $1,000)

Investment (in $1,000)

3500
1000

500
0

0
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

P (level of RD)
RD investment justified

Total Cost

Fig. 4. Total cost and yearly investment justiﬁed in RD technology.

Different Components of Costs
1600
Site Opening
(term 2)

cost (in $1,000)

1400
1200

Offsite SE
(term 3)

1000

Onsite SE
(term 3)

800
600

Penalty
(term 4)

400

Traveling
(term 5)

200
0
0

1423

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

p (level of remote diagnostics)
Fig. 5. Breakdown of total cost.

1

1424

C. Vardar et al. / European Journal of Operational Research 180 (2007) 1411–1426

service center and from ﬁve oﬀsite service engineers to
four oﬀsite service engineers. Also, observe that this
causes a slight increase in travel costs due to the fact
that service engineers have to travel to farther fabs
from only one location worldwide. This structural
change in the best solution causes the value of RD
to increase more steeply.
In the scenario described above we assumed that
there is a requirement for at least one onsite FST
and one onsite AE, which is the state of the art in
current wafer fabs. However, RD technology might
enable service providers to change the current structure of their systems. One possible scenario is relaxing the constraint of having at least one onsite AE in
the fab. This scenario makes sense with RD technology since AEs mainly deal with electronic and software problems, which are easier to handle remotely.
To analyze this case we ran our model without the
constraint of having at least one onsite AE in the
fab and compared the total system cost of two

diﬀerent scenarios. A plot of this comparison can
be seen Fig. 6.
When the RD eﬀectiveness is low (P < 0.5) the
relaxation in the assumption does not provide any
cost savings. As the eﬀectiveness of RD (P)
increases the system starts to enjoy cost savings
due to the pooling of the service requests at the service centers instead of having individual onsite AEs.
As another scenario we consider increased travel
costs which might be a realistic scenario due to
increasing fuel prices. We run our original model
with 50% increased travel costs. To make a fair
comparison for the value of RD technology investment in both cases we state the investment justiﬁed
for each level of RD eﬀectiveness (P) in terms of
total expected cost of the system with no RD
(P = 0) in respective scenarios. As expected RD
technology gets more valuable when travel costs
constitute a larger ratio of the total cost (see
Fig. 7). Another thing to note is the increase in

Total Cost compared - Curent vs No AE in fab required case
3800
Investment (in $1,000)

3600
3400
3200
3000
2800
2600
2400
2200
2000
0

0.1

0.2

0.3

0.4
0.5
0.6
P (level of RD)

current

0.7

0.8

0.9

1

No AE in fab required

Fig. 6. Current vs. No onsite AE required case compared.

Investment justified in terms of% of the total expected cost with
no RD technology

% of total expected cost

40%
35%
30%
25%
20%
15%
10%

original

5%

50% increased travel costs

0%
0.1

0.2

0.3
0.4
0.5
0.6
0.7
P (Level of remote diagnostics)

0.8

0.9

Fig. 7. 50% increased traveling costs compared with the original case.

1

C. Vardar et al. / European Journal of Operational Research 180 (2007) 1411–1426

value in terms of % of total cost is steeper for lower
levels of remote diagnostics eﬀectiveness and levels
oﬀ at high levels of remote diagnostics eﬀectiveness.
This also makes sense because at lower levels of P
travel is required more frequently, thus travel costs
are a bigger factor on objective function.
This type of scenario analysis can help decision
makers in planning strategic investments in RD
technology and shape the direction of the current
research and development eﬀorts towards the most
promising directions.

1425

speed up the algorithm. The ﬁrst one is to improve
the selection mechanism used by the genetic algorithm by considering the variability in the observed
performance measures between diﬀerent replications of the simulation. The second approach that
is worthwhile to study is using crude analytical
approximations or short simulation runs to preevaluate each solution alternative and run the full,
detailed simulation model for only the more promising alternatives. With these improvements, we
expect that large sized problems can be solved eﬃciently with our solution procedures.

6. Conclusions and future research
Acknowledgements
In this paper, we presented a ﬁeld service system
optimization model speciﬁcally developed for analyzing the eﬀects of RD technology in the semiconductor manufacturing industry. Our model
optimizes the location, type and capacity of the
regional ﬁeld service centers to minimize the total
expected personnel, facility location, RD technology investment, response time penalties and traveling costs.
The mathematical model presented cannot be
solved using traditional solution techniques due to
the lack of closed form expressions for the relevant
performance measures of the system. In order to
address this problem, we developed a simulation
model for evaluating the values of these performance
measures and used these evaluations in an evolutionary heuristic for optimization. Our solution approach
provides satisfactory solutions in a reasonable time
for small and medium sized problems.
In addition, we also presented a realistic case study
from the industry to demonstrate how our model can
be used to evaluate strategic investment decisions
about RD technology, site selection and capacity
investments. This type of analysis can greatly assist
service providers in making strategic decisions about
their service network and evaluating potential investments in RD technology. In addition, integrated circuit manufacturers can utilize this model to estimate
the expected number of up tools in a wafer fabrication facility which can be used in negotiating service
agreements and strategic capacity planning.
The solution approach used in this study can lead
to fairly long run times. For example the solution of
each instance in the case study took 4–6 hours on a
Pentium M 1.6 GHz computer with 480 MB of
RAM. For solving larger models and to enable
more extensive what if analysis, we are planning to
improve the solution procedure in two aspects to

Authors would like to thank Devadas Pillai and
Anant Raman from Intel Corporation for their help
in understanding remote diagnostics technology and
building the case study.
References
Agnihotri, S., Sivasubramaniam, N., Simmons, D., 2002. Leveraging technology to improve ﬁeld service. International
Journal of Service Industry Management 13 (1), 47–68.
Ahmed, M.A., Alkhamis, T.M., 2002. Simulation-based optimization using simulated annealing with ranking and selection.
Computers and Operations Research 29, 387–402.
Ahmed, M.A., Alkhamis, T.M., Hasan, M., 1997. Optimizing
discrete stochastic systems using simulated annealing and
simulation. Computers and Industrial Engineering 32 (4),
823–836.
Bloss, D., Pillai, D., 2001. E Manufacturing opportunities in
semiconductor manufacturing. Semiconductor International
(7), 9–12.
Boesel, J., Nelson, B.L., Ishii, N., 2003a. A framework for
simulation optimization. IIE Transactions 35, 221–235.
Boesel, J., Nelson, B.L., Kim, S.H., 2003b. Using ranking and
selection to ‘clean-up’ after simulation optimization. Operations Research 51, 814–825.
Brotcorne, L., Laporte, G., Semet, F., 2003. Ambulance location
and relocation problems. European Journal of Operational
Research 147, 451–463.
Catuneanu, V.M., Moldovan, M.C., Popentiu, F., Albeanu, G.,
1990. Maintenance philosophy using queueing multilevel
stochastic service system. Microelectronic and Reliability 30
(5), 897–901.
Desrochers, M., Marcotte, P., 1995. The congested facility
location problem. Location Science 3 (1), 9–23.
Fu, M.C., 1994. Optimization via simulation: A review. Annals of
Operations Research 53, 199–247.
Fu, M.C., 2001. Optimization for simulation: Theory and
practice. INFORMS Journal on Computing 14 (3), 192–215.
Gerencsér, L., 1999. Optimization over discrete sets via SPSA. In:
Proceedings of the IEEE Conference on Decision and
Control, pp. 1791–1795.
Hanssmann, M., 2001. E-diagnostics: A winning strategy. Semiconductor Magazine 2 (11), 19–22.

1426

C. Vardar et al. / European Journal of Operational Research 180 (2007) 1411–1426

Jayaraman, V., Srivastava, R., 1995. A service logistics model
for simultaneous siting of facilities and multiple levels of
equipment. Computers and Operations Research 22 (2),
191–204.
Kennedy, W.J., 1993. Modeling in-house vs. contract maintenance, with ﬁxed costs and learning eﬀects. International
Journal of Production Economics 32, 277–283.
Kochel, P., Kunze, S., Nielander, U., 2003. Optimal control of a
distributed service system with moving resources: Application
to the ﬂeet sizing allocation problem. International Journal of
Production Economics 81–82, 443–459.
Law, A.M., Kelton, W.D., 2000. Simulation Modeling and
Analysis, third ed. McGraw-Hill, New York.
Lee, J., 1998. Teleservice engineering in manufacturing: Challenges and opportunities. International Journal of Machine
Tools and Manufacture 38, 901–910.
Lin, C., Madu, C.N., Chien, T.W., Kuei, C.H., 1994. Queueing
models for optimizing system availability of a ﬂexible
manufacturing system. Journal of the Operational Research
Society 45 (10), 1141–1155.
Lin, C., Yeh, J.M., Ding, J.R., 2000. A genetic algorithm for
solving a maintenance model in a FMS. International Journal
of Systems Science 31 (4), 449–456.
Lin, C., Ding, J.R., Yeh, J.M., 2001. A closed queueing
maintenance network for automated manufacturing systems.
Journal of the Operational Research Society 52, 1121–1129.
Marianov, V., ReVelle, C., 1995. The queueing Maximal Availability Location Problem: A model for siting of emergency
vehicles. European Journal of Operational Research 93,
110–120.
Marianov, V., Serra, D., 2001. Hierarchical location-allocation
models for congested systems. European Journal of Operational Research 135, 195–208.
Owen, S.H., Daskin, M.S., 1998. Strategic facility location: A
Review. European Journal of Operational Research 111,
423–447.
Papadopoulos, H., 1996. A ﬁeld service support system using a
queueing network model and the priority MVA algorithm,
Omega. International Journal of Management Science 24 (2),
195–203.

Paul, R.J., Chanev, T.S., 1998. Simulation optimization using a
genetic algorithm. Simulation Practice and Theory 6, 601–
611.
Perez, S., 2001. Leveraging e-commerce opportunities to enhance
equipment lifecycle management. Future-Fab International
(11), 32–37.
Pool, R., 2001. If It Ain’t Broke, Fix It. Technology Review
(September), 19–26.
Raman, A., Pillai, D., 2001. E-Diagnostics: A secure and cost
eﬀective method to increase factory equipment productivity.
In: Proceedings of the International Conference on Modeling
and Analysis of Semiconductor Manufacturing, Phoenix, AZ,
pp. 52–57.
Rosen, S.L., Harmonosky, C.M., 2005. An improved simulated
annealing simulation optimization method for discrete
parameter stochastic systems. Computers & Operations
Research 166 (2), 343–358.
Sako, S., Yamamoto, H., Kondo, H., Arimae, J., 2003. Ediagnostics technology for supporting e-manufacturing. Hitachi Review 52 (3), 171–175.
Shalev-Oren, S., Seidmann, A., Schweitzer, P.J., 1984. Analysis
of ﬂexible manufacturing systems with priority scheduling:
PMVA. In: Proceedings of the First ORSA/TIMS Conference
on Flexible Manufacturing Systems, pp. 135–141.
Sloan, T.W., Shanthikumar, J.G., 2002. Using in-line equipment
condition and yield information for maintenance scheduling
and dispatching in semiconductor wafer fabs. IIE Transaction
34, 191–209.
Vardar, C., Gel, S.E., Fowler, W.J., 2004. Designing a ﬁeld
service system for semiconductor manufacturing systems for
remote diagnostics ERA, CD-ROM. In: IERC Conference
Proceedings, Houston, TX USA, 15–19 May 2004.
Waller, A., 1994. A queueing network model for ﬁeld service
support systems. Omega, International Journal of Management Science 22 (1), 35–40.
Wohlwend, H., 2001. An E-factory vision: Building on
e-diagnostics. Future-Fab International (11), 24–30.
Zeng, A.Z., Zhang, T., 1997. A queueing model for designing an
optimal three dimensional maintenance ﬂoat system. Computers and Operations Research 24 (1), 85–95.

Proceedings of ?he 2002 Winrer Simulation Conference
E. Yuceson, C:H. Chen, J. L. Snowdon, and J. M. Charnes, eds.

PARAMETERIZATION OF FAST AND ACCURATE
SIMULATIONS FOR COMPLEX SUPPLY NETWORKS
Brett Marc Duarte
John W. Fowler
Kraig Knutson
Esma Gel
Dan Shunk
Department of Industrial Engineering
PO Box 875906
Arizona State University
Tempe, AZ 85287-5906, U.S.A.

ABSTRACT

1999). The first dimension is functional integration, involving decisions about purchasing, manufacturing and distribution activities within the company and between the company
and its suppliers and customers. The second dimension is
geographical integration of these functions across physical
facilities located in one or several continents. The third dimension is integration of strategic, tactical and operational
supply chain decisions. Supply Chains (SC) as described by
Stevens (1989) is: “A system whose constituent parts include material suppliers, production fac
services and customers linked together via the feed forward
flow of materials and the feedback flow of information”.
With globalization of the market, optimization of supply
chains becomes more and more important. Synchronizing
this complex supply chain network, and making it respond to
demand fluctuation is not trivial, hut how well companies’
react to rapidly changing customer demands becomes a very
important factor in their ability to dominate their markets.
Typically, a supply chain is a multi-echelon system where
each “node” in the supply chain may have several disparate
suppliers. A simple supply chain in the semiconductor industry that was adapted from Godding and Kempf (2001) is
shown in Figure 1.

More efficient and effective control of supply networks is
conservatively worth billions of dollars to the world economy. Adopting an approach by which the basic disciplines
of Industrial Engineering, Control Engineering, System
Simulation and Business Re-Engineering are integrated
into one comprehensive system has been known to produce
impressive results. This paper discusses a modular approach to develop a discrete event simulation model that
has the appropriate level of abstraction to capture the inherent complexities that exist in a supply chain and is yet
simple, fast and produces results of high fidelity. It discusses a method to parameterize each module by finetuning a few parameters to make it represent an entire factory, a warehouse or a transportation link.

1

INTRODUCTION

High value products that quickly become obsolete! A vast
manufacturing network! Rapidly declining prices! A demanding customer base! The supply chain challenges facing the semiconductor industry are complex and difficult!
In today’s globally competitive business world, the network of companies that band together to create an end
product or service are not restricted to a sub-continent. The
world is moving from single enterprise mass production to
multiple enterprise customizations. Why? The strengths
strategic alliances have to offer, which include higher margins, shorter development cycles, higher quality, lower
overall costs, and the ability to meet demand on a single. customer basis. The key to gaining this competitive advantage is integrating decisions across the supply network.
across geographically dispersed facilities, and across time.
The essence of Supply Chain Management is integrated
planning, which has three important dimensions (Shirodkar

c..
P
!“.I:

-

5

L!a,
-&+
AD,,

f.biS.*,

_I

, - ,

!

-

“.I;.

3

.

I;

-.

-

!“,S.

&

,-,i

rUIt“l

1082

&-.
““I

,”.I

.~

e.*>

Figure 1: Simple Model of a Supply Chain

1327

Duane, Fowler, Knutson, Gel, and Shunk
One major obstacle in creating a seamless supply
chain is uncertainty. In order to deal with this issue, it is
imperative to identify and understand the cause of uncertainty and determine how it affects other activities up and
down the supply chain. The complexity described above
causes the semiconductor industry to experience erratic
changes in demand and this makes it difficult to decipher
the true demand from normal fluctuations, (Shirdokar
1999). What often appears as small random ripple variations in sales at the market place are amplified dramatically
at each level in the chain, so that upstream companies or
facilities experience the classical "boom-bust" effect,
(Towill 1996). In particular the variance in orders tends to
be larger than that of sales and this distortion tends to increase upstream. (Lee er a1 1997) describes this phenomenon termed the "bullwhip" effect and attributes its cause to
demand forecast updating, order batching, price fluctuations, and rationing and shortage gaming.
Computer simulation, because it can be applied to operational problems that are too difficult to model and solve
analytically, is an especially effective tool to help analyze
supply chain logistical issues. Currently, tools for understanding uncertainty are limited to traditional mathematical
formulas that do not account for variability. However,
simulation is one of the best means for analyzing supply
chains because of its capability to handle variability, (Towill 1996). Obviously, experimenting with an actual supply
chain could be detrimental, as the profit at risk is prohibitively high. Useful results have been obtained by adopting
an approach in which the basic disciplines of industrial engineering, control engineering, system simulation and
Business Reengineering are integrated into one comprehensive system, (Forrester 1961).

2

SII"LATIO*

""NllA,.,

Ik..,

Figure 2: Simulation Accuracy Versus Run Time

3

THE BASIC ATOMIC MODULE

T o model the material flow in the physical system a module was developed by Shirodkar (1999) that can be used to
represent a factory, a transportation link, or a warehouse.
This hybrid module is made up of three sub-modules: a capacity sub-module, a delay sub-module and a yield submodule as shown in the Figure 3.

-+
' T I

'

T I The time a lot gpends in the ',e,
T2 +The processing time of the lot.
T3 3 Total cycle time of the lot
Figure 3: The Basic Atomic Module (Adapted
from Shirodkar 1999)
The production units that arrive at the capacity submodule sit in a queue. A sample is then drawn from a
probability distribution, which represents the capacity of
the module. This occurs at a predetermined time interval
based on the chosen sampling rate; we use once a day in
our experimentation. The value of capacity drawn from the
distribution is then compared to the number of lots sitting
in the queue and the lesser of the two is picked and that
number of lots are released from the capacity sub-module
into the delay sub-module. The delay sub-module has an
infinite number of servers and each lot that enters this
module is allotted a processing time from a user specified
probability distribution. The queue in the capacity module
thus represents the time spent waiting in front of the capacity for it to become available and the delay sub-module
represents the time spent in processing once the capacity
has become available. The lots that finish processing proceed to the "yield sub-module" where the good lots m
separated from the defective ones.

PROBLEM STATEMENT

Traditionally, simulation models used in supply chains
have either been detailed discrete event simulation (DES)
models that track every individual lot that is processed at
every workstation or high-level, continuous simulation
models that do not track each lot but consider the gross
output and cycle time performance of each factory in the
chain. The first approach produces results that are very accurate but it generally takes a long time to build the model
and the execution time of such a model is often extremely
slow. Building models of the second type is generally
much easier and their execution time is much faster, but
the data produced is often far from accurate. Little work
has been done to combine these approaches to develop a
model that has an appropriate level of abstraction to capture the inherent complexities that exist in a supply chain
and is yet simple, fast and produces results of high fidelity.
The schematic diagram shown in Figure 2, illustrates the
objective.

1328

Duane, Fowler, Knutson, Gel, and Shunk
4

PARAMETERIZATION OF MODULE

Among the various sources of error that make the output
from a simulation less valid, is the modeling the “wrong” distribution for various input quantities, for example the arrival
times of jobs to a job shop or the service time of machines. A
commonly encountered problem in simulation modeling is
the specification of a suitable input distribution for the observed data. The data is a specific realization of some underlying distribution that can be regarded as the ‘ h e ” distribution, (Shankar and Kelton 1999). A prevalent practice is to
approximate this ‘ h e ” distribution with a standard family,
for example an exponential distribution or a uniform distribution. In many situations, this approximation may not adequately represent the observed data, and may introduce significant error in the input that may adversely affect the
validity of the output. In general there are three methods of
specifying an input distribution, (Shankar and Kelton 1999).
1.

2.

3.

eterize the atomic module is shown in Figure 4. In this a p
proach, we use data taken from the real factory or from a
detailed discrete event simulation (DES) of a factory to develop the capacity and cycle time parameters. Determining
the capacity distribution is relatively simple. Since the predetermined sampling rate at the capacity sub-module is
chosen to be once a day, the capacity parameters are determined by fitting the daily throughput of a simulation of
a fully loaded factory to an empirical distribution or by fitting the daily throughput of the real factory divided by the
utilization of the bottleneck to an empirical distribution.

Figure 4: Parameterization Methodology

Use a “standard” parametric distribution: These
include distributions such as uniform, exponential,
weibull.
Use an empirical distribution: Here the observed
data itself is used in some way to come up with a
distribution function. Empirical distributions have
flexibility, which is much desired.
Use a Flexible parametric family: Such a parametric family supplies a flexible distribution function
that is an approximation of the true distribution
function. This alternative can be viewed as a compromise between the first two approaches and is
both generalizable and flexible.

Matching the cycle time is not as straightforward because the cycle time distribution depends on the loading of
the factory. It is important to use the cycle times of individual lots in ascertaining the distribution at each capacity
loading. By using individual cycle times the reduction in
variance caused by the averaging affect of lots coupled into
daily or weekly time buckets is eliminated.
Figure 5 shows the (sanitized, for confidentiality) distribution of cycle times for lots coming out of a real Intel
factory. Figure 6 shows a similar cycle time distribution of
lots coming out of the detailed discrete event simulation
(DES) model. The model used was dataset #I from the
MASM Lab at Arizona State University (www. eas.asu.
edu/-masmlab). The dataset was modified to consist of
a single product with a release rate of 12 lotslday, which
corresponds to a factory loading of 97%, each lot containing 48 wafers. The modified model produces no scrap and
has 83 tool groups with 265 tools. The model also has 32
operator groups with 90 operators. The detailed simulation
was run for 200 days with 10 replicates and the first 65
days of each replicate was truncated. The remaining lot
cycle times were combined into one file and data from
10,000 lots were used to plot the histogram.

Output measures of performance can indeed be sensitive to the particular input distribution. Using standard
two-parameter distributions for which only the first two
moments are captured in many cases is not sufficient,
unless the system is running at relatively high traffic intensity. It may be necessary to use at least five moments for
systems with low traffic intensity, (Gross and Juttijudatta
1997), even though the lower order moments are the ones
that actually dominate.
It is apparent that the problem of input distribution selection is inherent to simulation modeling. A point in favor
of the empirical distribution is that their performance is
consistent. This cannot be said about the standard distributions whose performance quality depends more critically
upon the underlying true distribution. This robustness of an
approximating method is an important issue in input distribution specification.
Results from the previous research inferred that the
model produces data that is qualitatively correct. The next
step would therefore be to develop an approach by which
the model could produce throughput and cycle time data
that is (nearly) quantitatively correct and thus consistent
with data from a real factory. The approach used to param-

Figure 5: Total Cycle Time of Lots From
a Real Factory

1329

Duane, Fowler, Knurson, Gel, and Shurtk
results of both simulations. Therefore, subtracting the additional queue time ‘TI’ would render results that are very accurate, as we would expect since the resulting cycle time is
simply a sample from the actual cycle time distribution.

Figure 6: Total Cycle Time of the Detailed DES
The cycle time distribution in Figure 5 and Figure 6
above look fairly similar. Note that the average cycle time
of the DES model is 34.65 days. A simulation was then run
using our atomic module (coded in EXTEND”). The capacity distribution for this model was obtained from the
100% factory loading detailed DES and the cycle time distribution was obtained from the 97% factory loading detailed DES by fitting the cycle time of the 10,000 lots to an
empirical distribution. Figure 7 shows the cycle time distribution from this simulation run.

Figure 8: Average Cycle Time versus Capacity Loading
Estimating this additional queue time ‘TI’ can be either done on an individual entity basis or by evaluating the
average queue time at a particular capacity loading using
analytical methods. The latter approach proves beneficial
later in the section, as a graph of the average queue time
versus the capacity loading is a good characteristic approximation of what the cycle time curve would look like
qualitatively.

Analytical Approach to
Estimate Queue Time

4.1

The model under consideration can be thought of as an inventory system that has a deterministic supply but a random demand. It is assumed that any demand that cannot be
satisfied for the day is lost.

Figure 7: Total Cycle Time from Our
Model

Y.

Once again the distribution looks similar’to that of the
DES. The average cycle time however is 40.87 days, which
is higher that that of the DES and is attributed to the additional time the lots spend at the capacity sub-module. The
breakdown of the overall cycle time of a lot through the
atomic module is shown in Figure 3. The value ‘TI’, which
is the amount of time a lot spends in the queue, is determined by the capacity sub-module and will be negligible
when the factory is lightly loaded, but will increase as the
factory loading increases. ‘T2‘ for each lot is simply a
sample from the given cycle time distribution and does not
depend on the factory load. ‘T3’ is the total cycle time of a
lot and is the sum of ‘TI’ and ‘T2’. Matching the cycle
time distribution of the detailed simulation with that from
our module for a given loading by running a detailed simulation for each loading of interest is easy.
A simple experiment shown in Figure 8 show the results
obtained by using this approach. As mentioned above the
difference in cycle times between the two simulations is
purely due to the time a lot spends at the capacity submodule ‘TI”. As the system is more heavily loaded this
queue time increases and so also the difference between the

X,
a

The demand on day n
The inventory available to satisfy the demand on day n.
Deterministic start rate (Lotdday), of the
lots that arrive at the beginning of the day
and are available to satisfy the demand for
the day.

The stochastic process (X, n 2 0) possesses the
Markovian property which states that if the present state of
the system is known, the future of the system is independent of its past, (Kulkami 1995). Stated another way, the
present state of the system contains all the relevant information needed to predict the future in a probabilistic sense..
Hence the stochastic process (X, n 201 can be modeled as
a Discrete-Time Markov Chain (DTMC). The steady state
distribution of the process can be found by solving for the
following set of equations.

1330

Duane, Fowler, Knurson, Gel. and Shunk
where lim

. ... .......
... ....,. ,.
._
........
..............
._.- ._......
........
."......,..."
._._._
..........
..............
..............
..............
..............
........
....
......
........
._
.
._
..
............
.....
............
..
........
....
.............

ei= R ~ is,the long-run average fraction of

time that the system stays at state j and Py represents the
transitional probability of moving from state 'i' to state
as shown in Figure 9 and each state is defined as the inventory left at the end of the day.

.
I

Y

Y

Y

.
I
.I.IY
Y

Y
.
I

Y.I.

Y

Y

"-1

... ... .,. ...
... ... ... ... . .
.. .. ,. ..
.. .. ... ...
.-.
......
.......
. -.
........
._
........
._
._
._
..
-.".
..
.
.
I
........
....
.......
....,,
....._
."._
. .....
".,,"

"

"+DD

Y

"

.... ..

Y . .

".,
I).

.I

\

.

Figure 11: Transitional Probability Matrix
(TPM) with Finite State Space
Figure 9: Markov Chain with Transition
Probabilities
The Transitional Probability Matrix (TPM) that represents this Markov chain is set up by evaluating the probability of a having a certain number of lots waiting in inventory at the end of the day, after the capacity of the
system has been set for that day
The example shown in Figure 10 is for a start rate of 5
lotdday. Poo would therefore be the transition probability
of having zero lots at the end of a day on which five lots
entered the system with an initial inventory of zero. This
would occur if the capacity for the day were greater than or
equal to 5. The probability of achieving this based on the
capacity distribution is 0.85. Similarly, Pol is the transition
probability of having one lot at the end of a day on which
five lots entered the system with an initial inventory of
zero. This would occur if the capacity for the day were
four. The probability of achieving this based on the capacity distribution is 0.03. Similarly the rest of the probabilities in the transitional probability matrix (TPM) can be
evaluated. The TPM for the model is that of an irreducible
Markov chain with infinite state space. The matrix is symmetrical with an upper and lower triangle of zeros.
A quick way of solving this matrix is by approximating the TPM with a finite state space. We truncated the
TPM as shown in Figure 11, and then solved it using the
Grassmann, Taksar and Heyman (GTH) algorithm (Grassmann et al 1985).
The GTH Algorithm is a state reduction algorithm.
Recursively, a Markov chain with one state less is consuuctttd from the previous one. The algorithm begins with
the n row and column and performs a series of iteration
and computation, working its way up the matrix. In the

process it calculates the steady state probability vector
n'=
with low relative error. The GTH algorithm was coded in Matlab. The expected queue length (not
including jobs in processing) 'LY'can then be calculated as
L , =

n

j

'

j

j=o

From Little's Law we have:

L

=,x*w,

Where h denotes the start rate and W, denotes the waiting
time in the queue. Therefore for a given start rate 'L',the
waiting time in queue W,can easily be calculated.
One question that arises is how big should the truncated transitional probability matrix be? Naturally we expect that the bigger the matrix, the more accurate the result
will be. Figure 12 show the trade-off between the size of
the matrix and the average queue time obtained. The experiment was carried out using the matrix corresponding to
a 97% loading.

Figure 12: Trade-off between the
Size of the Matrix and the Average
Queue Time for a 97% Capacity
Loading
Results obtained by using the analytical method of estimating queue time were also compared to that of the
simulation to check the validity of the method. The comparison is shown in Table 1.
Therefore, if we can estimate the additional queue
time analytically, it is relatively easy to match the cycle
time distribution of the detailed simulation with that from
our module for given start rates by running a detailed simu-

Figure IO: Transitional Probability Matrix
(TPM) with infinite state space

1331
I

2

Duane, Fowler, Knurson, Gel. and Shirnk
asymptote corresponds to a lightly loaded factory where
the cycle time is almost equal to the raw processing time.
The second asymptote represents a heavily loaded factory
where the traffic intensity approaches the capacity of the
system. The cycle time for such loadings approaches infinity due to the ever-increasing queue.
As a next step, to achieve the cycle time at various
loadings, we linearly interpolated the average cycle times
or each segment of the curve based on the load percent.
Developing linear equations to represent each segment requires two reference points for each equation. The question
is how do we pick the reference points? A graph of the average queue time versus the capacity loading is a good
characteristic approximation of what the cycle time curve
would look like qualitatively. Since obtaining the average
queue time using analytical methods is efficient, we use the
queue time versus capacity loading graph to choose which
capacity loadings to tun the DES to best represent each cycle time segment. The mean and standard deviation of the
cycle time at each reference point is noted and linear equations for both parameters are set.

Table I: Comparison of the Queue Time Achieved
from the Analytical Approach to that of the Simulatinn R i m

lation for each loading of interest, using empirical distributions for the capacity and delay and subtracting the additional queue. However, our goal is to specify a small numher of capacity and cycle time parameters that will give
reasonable estimates of cycle time distributions over a
range of factory loadings.
As indicated earlier, using a single cycle time distribution to statistically match data at different capacities would
be ideal. In order to see how well a single cycle time distribution would work, the module was tun using the empirical cycle time distribution that was built using data that
corresponded to a 40% capacity load of the detailed DES.
It was assumed that the effect of queuing is insignificant at
this loading. The capacity distribution supplied to the
module was the throughput distribution from a detailed
DES at 100% capacity load. The module was then run at
different start rates (40%. 8l%, 90%. 97%) to check if the
output matched that of the detailed DES. The results of this
experiment are shown in Figure 13. Notice that the average
cycle time from our module significantly underestimated
the average cycle time from the detailed DES for all loadings. The same experiment was repeated using the 80% cycle time distribution of the detailed DES as the delay distribution in our module. As shown in Figure 13, using this
distribution led to a significant overestimate of average cycle time for a lightly loaded factory and a significant underestimate for a heavily loaded factory.
The cycle time characteristic curve of a system, with
no batching policies, can be represented by a monotonically increasing curve (Fowler and Park 2001). For these
systems, the cycle time curve can be broken up into three
principal segments, two asymptotes and a knee. The first

!

L.W

Figure 1 4 Choosing the Three Segments
Based on the assumption of Rose (1999).the cycle time
distributions at higher capacity loadings can be assumed to
be normally distributed. Using the equations for the mean
and the standard deviation we can set the parameters of'the
normal distribution. Intuitively the average cycle time using
our module will still he overestimated due to the fact that a
non-linear curve has been replaced by a linear one for the
purpose of estimation and due to the additional queue time
'TI' in the model. We propose to use the analytical method
to estimate 'TI' and subtract this estimate from the cycle
time of each lot so as to eliminate the later problem.
To verify this approach, the simulation model was run
for a period of 3400 days at different start rates. An initial
bias of 5000 lots were considered and eliminated from the
statistics. Eight replicates were performed at each capacity
loading. The results obtained are shown in Figure 15 and in
Table 2.
Figure 16 is a plot of the average daily cycle time versus the elapsed time for the system run at 89% capacity
loading which corresponds to a start rate of 11 lotdday.
Data from day 400 through day 3400 has been plotted. The
average cycle time for the DES is 25.20 days with a standard deviation of 1.78 days while our model has a mean
cycle time of 25.58 days with a standard deviation of 1.51

I

Figure 13: Cycle Time Versus Capacity
Loading Using a Single Delay Distribution

1332

Duane, Fowler, Knutson, Gel, and Shunk

10

so

.o
5.p.m"L...l"

/ '

so

100,

__*--

...10

,
/

!I ,I
!I

/ I
IO
,111

so

I

Figure 15: Cycle Time Versus Capacity
Loading

Figure 17: Effect of a Deterministic/Stochastic Cycle Time Distribution

Table 2: Statistical Comparison of DES with

,

.
I
0."

.1~,~I.1I..~l.l,,.",~",o

.,.,

--,

Figure 18: Effect the Delay Distribution
h& on the Throughput
trates the extent to which this "cross-jumping" of lots effect the variance in throughput. When the delay distribution is deterministic, the standard deviation of the throughput for our model matches that of the DES, however, as the
width of the cycle time distribution increases, the standard
deviation of the throughput decreases till it eventually
reaches a steady state.

-"RI
81-m
I
I I
Figure 1 6 Cycle Timemhroughput Versus Elapsed
Time at 89.1% Capacity Loading

I

5

days. Similar experiments were run to validate the model
at different capacity loadings.
As far as throughput goes, the first 400 days of data
has been truncated and data for the next 6M) days has been
plotted. Notice that while the average throughput of our
model is consistent with that of the DES, it does not have
as much variability. The average throughput for the DES is
11.03 unitslday while that for our model is 11.02 unitdday.
The standard deviation for the DES is 6.27 unitslday while
that for OUT model, however, is 3.36 unitdday. This is attributed to the interaction between the capacity distribution
and the delay distribution.
The schematic diagrams in Figure 17 illustrate the effect of the interaction between the capacity and delay distribution. The system is analogous to a conveyor on which
the delay distribution sprays lots. When the delay distribution is deterministic, the lots that enter the delay submodule fall into the same time bucket and the variability in
throughput is preserved. For this example the processing
time is 4 days. With the advance of the time clock the lots
move one day closer to completion as a result the throughput at the end of days four, five, six, seven and eight would
be five, two, zero, five, and six.
However, variability in the delay distribution causes
lots to jump into different time buckets and in the process
reduces the variability in the throughput. Figure 18 illus-

EXECUTION TIMES

As far as accuracy goes, sufficient evidence has been put
forth to illustrate the credibility of our model, speed on the
other hand is a critical issue. Figure 19 is a plot of the
simulation run time for the DES compared to our model.
The model was run at different capacity loadings and the
simulation run time was recorded. The experiments were
run on a Pentium II, 333 MHz machine. The results show
that our model is much faster than the DES.

When modeling complex, supply networks, which
consists of several manufacturing, assembly and distribution facilities, the speed of our model would be even more
apparent. With its low run time and accuracy the model
should be a useful tool.

1333

Duarfe, Fowler, Knutson. Gel, and Shunk

6

REFERENCES

CONCLUSIONS

In manufacturing, common performance measures used to
evaluate a system are Cycle Time (CT), Throughput (TH)
and Work in Process (WIP). Changes to operating policies
can he evaluated by examining the impact on these three
performance metrics. Due to the complexities of manufacturing systems in the semiconductor industry, a simulationbased approach becomes a viable choice.
As stated earlier, detailed discrete event simulators
(DES) track each individual lot that is processed at every
workstation. As a result such models produce results that are
very accurate but they generally take a long time to execute.
Our model on the other hand aims at having the right level
of abstraction to capture the inherent complexities that exist
in a supply chain and yet is simple, fast and produces results
of high fidelity. By means of a simple model, we intend to
foster a basic understanding of the behavior of manufacturing units. If the simple modeling approach mimics the full
factory accurately, then these models can be used to model
complex supply networks. As far as accuracy goes, sufficient evidence has been put forth to prove its credibility.
Speed on the other hand is a critical issue. Run-time experiments carried out on a Pentium LI 333 MHz machine
show that our model is much faster than the detailed discrete
event simulator (DES) when modeling a single manufacturing unit. It is believed that the speed of the model would he
even more impressive when modeling a complex supply
network consisting of multiple factories, assembly facilities,
transportation centers and component warehouses.
Currently the model is set up to accommodate one
generalized product family, however an important next
step would he to accommodate multiple product groups.
This would lead to a more intuitive understanding of factory dynamics based on product prioritization coupled with
various dispatching policies. Future research in this area
would he aimed at attaining output parameters. namely cycle time and throughput that are statically indistinguishable
from data obtained from a real factory. The present model
produces results that are very encouraging, however, interaction between the capacity and delay distribution tends to
squeeze the variability in the throughput.
Each module can further be embellished to make it
look more like a factory, a transportation link or a component warehouse. Yield loss can be incorporated into the
model to give it a more realistic flavor.
As the capacity loading of the system increases the effect of auto-correlation in cycle time becomes more apparent. Future work in this area could entail comparing several correlation scenarios with respect to their ability to
mimic real factory data.

Forrester, J.W.,1961 Industrial Dynamics. MIT Press,
Cambridge, MA.
Fowler, W. J., Park, S., 2001, Efficient Cycle TimeThroughput Curve Generation Using Fixed Sample
Size Procedure. International Journal of Production
Research, Vol. 39, No.12,2595-2613.
Godding, G., Kempf, K., August 11-14, 2001, A Modular,
Scalable Approach To Modeling And Analysis Of
Semiconductor Manufacturing Supply Chains. Proceedings of IVSlMPOI/POMS 2001, GuarujUSP-Brazila.
Grassmann, W. K., Taksar, M. I., Heyman, D. P.,1985,
Regenerative Analysis and Steady State distributions
for Markov Chains. Operations Research, Vol. 33,
No. 5, 1107-1116.
Gross, DJuttijudatta, M., Dec.1997, Sensitivity Of Output
Performance Measures To Input Distributions in
Queuing Simulation Models. Winter Simulation
Conference, pp 296-302.
Hopp. W. J., Spearman, M. L, 2001, Fuctory Physics. Second Edition.
Ingalls, R., Kasales, C., Dec.1999, CSCAT: The Compaq
Supply Chain Analysis Tool. Winter Simulation
Conference, Vol. 1, pp 1201-1206.
Jain, S., Lim, C.C., Gan, B.P., Low, Y.K., Dec 1999, Criticality Of Detailed Modeling In Semiconductor Supply
Chain Simulation. Winter Simulation Conference,
Vol.1, pp 888-896.
Kempf , K., Knutson. K., Fowler J. W., Armbmster, B.,
Duane, B. M., Babu, P., April 24-25, 2001, Fast And
Accurate Simulations Of Physical Flow In Demand
Networks. Proceeding of International Conference on
Semiconductor Manufacturing Operational Modelling
an Simulation, Seattle. WA, pp 11 1-1 16.
Kitagawa, T., Maruta, T., Ikkai, Y., Komoda, N., Aug 2426, 2000, A Description Language Based On Multifunctional Modeling And A Supply Chain Simulation
Tool. 4'* IEEE International Workshop, pp 71-78.
Kulkarni, G. V.,1995, Modelling And Annlysis Of Stochastic Systems. 1995 Edition.
Law, A. M., Kelton, D. W.,1991, Simulation Modelling
And Analysis. Second Edition.
Lee, H. L., Padmanabhan, V., and Whang, S., April 1997,
Information Distortion In A Supply Chain: .The Bullwhip Effect. Management Science, Vol. 43, No. 4, pp

546-558.
Leemis, L., Dec 2000, Input Modeling. Winter Simulation
Conference, Vol. I , pp 17-25.
Maltz, A. B., Grenoble, W. I., Rogers, D. S., Baseman, R.
S . , Grey, W. and Katircioglu, K .K.,Lessons From The
Semiconductor Industry. Retrieved March 10, 2001
from the World Wide Web: http://www.rnan
ufacturing.net/scl/lessons/james.htrnl

ACKNOWLEDGMENTS
This research has been partially supported by a grant from
Intel Corporation.

1334

D u a m , Fowler, Knurson, Gel, and Shunk
Maruta, T., Ikkai, Y., Komoda, N., May 1999, Simulation
Tool Of Supply Chain Model With Various Structure
And Decision Making Processes. ThIEEE Conference,
Vol. 2 , pp 1443-1449.
Maskell, B., 2001, The Age Of Agile Manufacturing. Supply Chain Management: An International Journal,
Vol. 6, ISSN 1359-8546.
Ramberg, J. S., Dudewicz, E. J., Tadikamalla, P. R.,
Mykytka, E. F., May1978,
A Probability Distribution And Its Uses In Fitting Data. ASQC Chemical Division Technical Conference.
Rose, 0..Jan 1999, Estimation Of The Cycle Time Distribution Of A Wafer Fab By A Simple Simulation
Model. In Proceedings of rhe SMOMS ‘99 (1999
WMC), pp. 133-138.
Schunk, D., Dec 2000, Using Simulation To Analysis Supply Chains. Winrer Simularion Conference, Vol. 2, pp
1095-1100.
Shankar, A., Kelton, W., Dec.1999, Emperical Input Distributions: An Alternative To Standard Input Distributions In Simulation Modeling. Winrer Simulation Conference, pp 978-985.
Shirodkar, S., Dec.1999, A Modular Approach For Modeling And Simulating Semiconductor Supply Chains.
Masters thesis at Arizona State University.
Stevens, J., 1989, Integrating The Supply Chain. International Journal of Physical Distribution & Material
Managemenr, Vol. 19, pp 3-8.
Towill, D., 1996, Industrial Dynamics Modeling Of Supply
Chains. International Journal Of Physical Disrriburion
And Logistics Managemenr, Vol. 26, No.2, pp 23-42.
Towill, D. R., 1995, Time Compression And Supply Chain
Dynamics. Logistics Internarional, Sterling publicarions, London, pp 43-7.
Turner, S., Gan, P., 2000, Adapting A Supply Chain Simulation For HLA. 4Ih IEEE Internarional Workshop, pp
71-78, Aug. 24-26.

at Advanced Micro Devices. His research interests include
modeling, analysis, and control of semiconductor manufacturing systems. Dr. Fowler is the co-director of the Modeling and Analysis of Semiconductor Manufacturing Laboratory at ASU. The lab has had research contracts with NSF,
SRC, SEMATECH, Infineon Technologies, Intel, Motorola, ST Microelectronics, and Tefen, Ltd. He is a member of ASEE, IIE, IEEE, INFORMS, POMS, and SCS. His
emailaddress is <john.fowler@asu.edu>

KRAIG KNUTSON is an assistant professor in the Del E.
Webb School of Construction at Arizona State University.
He holds a bachelor’s and master’s degree in construction
and a Ph.D. in industrial engineering from Arizona State
University. His research interests are related to the design,
simulation and optimization of manufacturing systems and
construction processes. He is a member of IIE, INFORMS,
AACE, AIC and ASCE.
ESMA GEL is currently Assistant Professor of Industrial
Engineering at Arizona State University. Her research interests are stochastic modeling and control of manufacturing systems and her current work is on agile workforce
policies in various production environments. She is a
member of INFORMS, IIE and ASEE. She completed her
Ph.D. studies in 1999, at the Department of Industrial Engineering and Management Sciences of Northwestern University where she also received her M.S. degree in 1996.
She earned her B.S. degree in Industrial Engineering from
Om Dogu Technical University, Ankara, Turkey and was
awarded the Walter P. Murphy Fellowship by Northwestern University for graduate study in 1994.

DAN SHUNK is a Full Professor of Industrial Engineering
at Arizona State University and former Director of the
CIM Systems Research Center. He is currently pursuing
research into global new product development, modelbased enterprises and global supply chain. His latest book
is Inteerated Process Desien and Development. an Irwin
publication. Dr. Shunk studied at Purdue where he received
his Ph.D. in Industrial Engineering in 1976. He is cofounder of the USAF Integrated Computer Aided Manufacturing (ICAM) Program where he launched such industry standards as IDEF and IGES, former manager of Industrial Engineering at Rockwell, former manager of
manufacturing systems at International Harvester, and
former VP-GM of the multi-million dollar Integrated Systems Division of GCA Corporation. Dr. Shunk has served
on the Board of Advisors of CASA of the Society of
Manufacturing Engineers, and chaired CASA in 1993. He
helped Motorola conceive Motorola University and has
served on their faculty since 1984. He is on the Editorial
Board of the Aeilitv and Global ComDetition Journal and
the International Journal of Flexible Automation and Integrated Manufacturing, He is an active member of the Inter-

AUTHOR BIOGRAPHIES
BRETT MARC DUARTE received his Master’s degree
in Industrial Engineering from Arizona State University in
May 2002. He has a specialization in manufacture of
semiconductors, and his interests lie in simulation and
modeling, with an emphasis on supply chain management
and integration. His email address is <Brett.
Duarte@asu.edu>

JOHN W. FOWLER is an Associate Professor in the Industrial Engineering Department at Arizona State University. Prior to his current position, he was a Senior Member
of Technical Staff in the Modeling, CAD, and Statistical
Methods Division of SEMATECH. He received his Ph.D.
in Industrial Engineering from Texas A&M University and
spent the last 1.5 years of his doctoral studies as an intern

1335

Duarte, Fowler, Knurson, Gel, and Shunk
national Federation of Information Processors (IF'IP) Committee 5.3 on CIM. He is a senior member of SME and
IIE. He won the 1996 SME International Award for
Education, the 1999 and 1991 Industrial Engineering Faculty of the Year award, the 1989 SME Region VI1 Educator of the Year award, chaired AutoFact in 1985, and won
the 1982 SME Outstanding Young Engineer award. For the
year 2000 he has been nominated as the US Alternate to
the Intelligent Manufacturing Systems project.

1336

European Journal of Operational Research 173 (2006) 565–582
www.elsevier.com/locate/ejor

Discrete Optimization

Evaluation of nondominated solution sets
for k-objective optimization problems: An exact method
and approximations
B. Kim a, E.S. Gel

a,*

, J.W. Fowler a, W.M. Carlyle b, J. Wallenius

c

a

Arizona State University, P.O. Box 875906, Tempe, AZ 85287-5906, United States
b
Naval Postgraduate School, Monterey, CA 93943, United States
c
Helsinki School of Economics, Runeberginkatu 14-16, Helsinki FIN-00100, Finland
Received 19 September 2003; accepted 10 January 2005
Available online 16 March 2005

Abstract
Integrated Preference Functional (IPF) is a set functional that, given a discrete set of points for a multiple objective
optimization problem, assigns a numerical value to that point set. This value provides a quantitative measure for comparing diﬀerent sets of points generated by solution procedures for diﬃcult multiple objective optimization problems.
We introduced the IPF for bi-criteria optimization problems in [Carlyle, W.M., Fowler, J.W., Gel, E., Kim, B., 2003.
Quantitative comparison of approximate solution sets for bi-criteria optimization problems. Decision Sciences 34 (1),
63–82]. As indicated in that paper, the computational eﬀort to obtain IPF is negligible for bi-criteria problems. For
three or more objective function cases, however, the exact calculation of IPF is computationally demanding, since this
requires k (P3) dimensional integration.
In this paper, we suggest a theoretical framework for obtaining IPF for k (P3) objectives. The exact method includes
solving two main sub-problems: (1) ﬁnding the optimality region of weights for all potentially optimal points, and (2)
computing volumes of k dimensional convex polytopes. Several diﬀerent algorithms for both sub-problems can be
found in the literature. We use existing methods from computational geometry (i.e., triangulation and convex hull algorithms) to develop a reasonable exact method for obtaining IPF. We have also experimented with a Monte Carlo
approximation method and compared the results to those with the exact IPF method.
 2005 Elsevier B.V. All rights reserved.
Keywords: Multiple objective programming; Combinatorial optimization; Metaheuristics

*

Corresponding author. Tel.: +1 480 965 2906; fax: +1 480 965 8692.
E-mail addresses: bosun.kim@asu.edu (B. Kim), esma.gel@asu.edu (E.S. Gel), john.fowler@asu.edu (J.W. Fowler), mcarlyle@
nps.navy.mil (W.M. Carlyle), walleniu@hkkk.ﬁ (J. Wallenius).
0377-2217/$ - see front matter  2005 Elsevier B.V. All rights reserved.
doi:10.1016/j.ejor.2005.01.029

566

B. Kim et al. / European Journal of Operational Research 173 (2006) 565–582

1. Introduction
Many multiple objective a posteriori heuristics
have been developed and successfully applied for
solving real world multiple objective optimization
problems (Aksoy et al., 1996; Coello, 1999; Czyzak
and Jaszkiewicz, 1998; Viana and De Sousa, 2000;
Zitzler, 1999). These a posteriori heuristics provide
a set of nondominated solutions for the decision
makerÕs evaluation rather than a single ﬁnal solution. One of the key issues is how to evaluate the
quality of approximate solution sets generated by
diﬀerent heuristics or diﬀerent parameter settings
for a heuristic (Coello, 1999). For this purpose,
various measures have been suggested in the literature (a detailed review can be found in Carlyle
et al., 2003). A measure called Integrated Preference Functional (IPF) for two objective function
problems was recently developed in Carlyle et al.
(2003) and applied in the comparison of two competing bi-criteria genetic algorithms for a parallel
machine scheduling problem (Fowler et al.,
2005). As shown in these two papers and Kim et
al. (2001), the IPF possesses many good qualities
and provides a robust, quantitative measure for
comparing diﬀerent solution sets for diﬃcult bicriteria optimization problems.
In this paper, we extend the exact IPF method
presented in Carlyle et al. (2003) to three or more
objectives, when the implicit value function is a
convex combination of objectives and the weight
density function has a uniform distribution. This
extension is important because many diﬃcult multiple objective optimization problems have more
than two objectives. We have also conducted a
computational experiment to test the performance
of the exact IPF method for three, four, and ﬁve
objective functions and compared it against
approximation methods. Furthermore, we discuss
two research topics in numerically approximating
IPF.
The organization of the rest of the paper is as
follows. In Section 2, we review the IPF measure
and illustrate it for the case of two objectives. In
Section 3, an exact IPF method is described for
the case of three or more objectives. In Section 4,
a numerical example to illustrate the steps of the
exact method is provided. Experimental results

on both exact and approximate methods are discussed in Section 5. Section 6 provides concluding
remarks and suggestions for future research. In
Appendix A we present the basic terminology,
and state (and prove where appropriate) several
theorems related to the IPF measure. We note that
the computer code used for the experimentation in
this paper is available upon request from the
authors.

2. Review of IPF with an illustration to the
bi-criteria case
A review of the IPF measure is provided for the
readerÕs convenience. Further details can be found
in Carlyle et al. (2003).
Consider a set of ﬁnite nondominated solution
vectors (x 2 X) in Rn, the corresponding objective
function vector z = f(x) in Rk, and a parameterized family of implicit value functions g(z; a),
where a given value of the parameter vector a in
its domain A produces a speciﬁc scalar-valued
value function to be optimized. Note that throughout the paper, we consider minimization problems.
Then, there is at least one optimal point in set Z
(which refers to the set of nondominated solutions
to be evaluated) for any ﬁxed value of a. However,
there are clearly some points that are not optimal
for any value of a in its domain A. In Fig. 1(a)
there are ﬁve points in a two objective minimization problem. If g(z; a) is an implicit convex combination of objectives, any particular weight (e.g.,
a = 0.5) leads to at least one supported point
(e.g., z3). Clearly points z2 and z4 are not optimal
(nonsupported points) for any value of a in its
domain A.
For a given g, deﬁne a function zg : A ! Z that
maps parameter values to a corresponding optimal
point. This function zg(a) is clearly piecewise
constant over A. The function ag(z), with z 2 Z,
deﬁnes a partition of A into the sets over which
zg is constant (optimality region of weights for z):
[
[
A¼
ag ðzÞ ¼
Az
z2Z

z2Z

for z1 5 z2 2 Z, where Az1 and Az2 have at most
one value in common in the two objective case.

B. Kim et al. / European Journal of Operational Research 173 (2006) 565–582

IPFðZÞ ¼

Z

hðaÞgðzg ðaÞ; aÞ da

a2A

¼

"
X Z
z2Z

¼
(a)

Objective 1

(c)

(b)

X

567

#
hðaÞgðz; aÞ da

a2z1
g ðzÞ

ð2Þ

IPFðzÞ:

z2Z

Objective 1

(d)

Fig. 1. Illustration of IPF for two objective functions case—(a)
the optimal solution for a1 = 0.5 (i.e., z3), (b) range of weights
for which z3 is optimal, (c) optimality regions of weights for
each supported point, (d) IPF for a nondominated solution set.

In general, Az1 \ Az2 is the boundary between the
two regions for which z1 and z2 are optimal. Potentially optimal points in a set are deﬁned as points
which have the optimality region of weights (Az)
for a given g and A. Fig. 1(b) shows that there is
a range of weights for which any particular supported point is optimal. Any one of these weights
will give an objective vector in the cone indicated
in the ﬁgure. In Fig. 1(c), we show the range of
weights for which diﬀerent supported points are
optimal. In particular, for the case shown in Fig.
1(b), Fig. 1(c) shows that point z3 will be optimal
for weights between 13 and 34. It is well known that,
for any supported point, this weight range is a single interval.
Given a weight density
(preference) function
R
h : A ! Rþ such that a2A hðaÞ da ¼ 1, the IPF(Z)
is
Z
IPFðZÞ 
hðaÞgðzg ðaÞ; aÞ da;
ð1Þ
a2A

which maps sets of points to real numbers. Because zg is piecewise constant, the integral in (1)
can be decomposed into the portions of the domain z1
g ðzÞ corresponding to each element z 2 Z:

The weight density function h(a) assigns diﬀerent
weights to the diﬀerent objectives, and the IPF(Z)
then provides a general measure of ‘‘expected value’’ of a set of points, given the decision makerÕs
implicit value function. The second form of Eq.
(2) indicates that we only need to be able to evaluate integrals involving a. In Fig. 1, if h(a) has a
uniform distribution, Fig. 1(d) shows that the
IPF for the given set of nondominated points in
Fig. 1(a) is the area of the bounded region in the
weight (a)-value function (g) space.
As shown in Carlyle et al. (2003), the computational complexity of the exact IPF method for two
objective functions case is trivial. However, the
main diﬃculties with this method for k P 3 objective functions cases include computing the optimality region of weights Az over which z is
optimal for g(z; a), and k dimensional (number
of objectives) integration of a linear or nonlinear
function of a over the region Az. The functions
integrated are linear or nonlinear depending on
the types of value functions or weight density functions assumed.
Due to these diﬃculties, a numerical approximation method was presented in Hansen and Jaszkiewicz (1998). Assume IPFN(Z) denotes the
approximation of IPF(Z). Then IPFN(Z) is obtained as follows:
IPFðZÞ  IPFN ðZÞ ¼

N
1 X
gðz; ai Þ;
N i¼1

ð3Þ

Pk
where gðz; ai Þ ¼ minz2Z f i¼1 ai zi g, ai is the ith random weight vector (ai 2 Rk), k is the number of
objectives, and N is the number of weight vectors
used. In Fig. 1, if a discrete weight a is selected randomly from an assumed h(a), g(z; a) can be easily
obtained by comparing g(z; a) values for the ﬁve
solution points in Fig. 1(a). Then the minimum

568

B. Kim et al. / European Journal of Operational Research 173 (2006) 565–582

g(z; a) is a point on the value line in Fig. 1(d). If a
suﬃcient number of weights (N) are selected, the
IPF can be approximated from Eq. (3). In the
approximation method, however, the proper number of weight vectors (and computational time) required for estimating the IPF within a speciﬁed
margin of error needs to be determined. As we
are interested in the k (P3) objective functions
case, a random weight vector generation method
in a k  1 simplex (i.e., for three objective function
case, random weight vectors are generated from a
triangle) is also needed.

3. Exact IPF method for k > 3 objectives
Consider n nondominated points in a set Z in
Rk. Then
• If the value function g(z; a) is a convex combination of objective functions and h(a) follows
a uniform distribution, then the optimality
region of weights for supported point z is a
(k  1)-convex polytope in Rk1 as can be seen
from Theorem 2 in Appendix A.
• The value surface g(z; a), over the optimality
region of weights, is clearly a linear function
of a by the assumptions we have made. Hence,
for a supported point z, integration of the value
function over the optimality region of weights
(IPF(z)) gives the volume of a k-convex
polytope.
• As can be seen in the ﬁnal equation in (2), the
computation of IPF(Z) for a set of nondominated points, Z, is the summation of the volumes of k-convex polytopes, IPF(zs), for all
supported points.
• The k dimensional integration in Eq. (2)
becomes a series of volume computations of
k-convex polytopes.
Two main sub-problems related to (multiple
objective) linear optimization and computational
geometry need to be solved sequentially to obtain
IPF(Z). These sub-problems and steps are listed
in Fig. 2. Since there are several alternative algorithms for solving each of these sub-problems,
there can be many alternatives to construct an

algorithm to calculate IPF(Z). We simply chose
one easy to implement variation among them, instead of investigating the best combinations of
the available algorithms.
Step-1
The ﬁrst step to obtain IPF(Z) is to ﬁnd all supported and adjacent supported points from a given
set of nondominated points. Supported points are
only potentially optimal, when the value function
is a convex combination of objectives and the optimality regions of weights for supported points can
be obtained using adjacent supported points.
Three methods can be found in the literature for
solving this sub-problem.
First, Zionts and Wallenius (1976, 1980) suggested a method to enumerate all supported and
adjacent supported points in their study of identifying eﬃcient vectors. They showed that ﬁve diﬀerent
problems of ﬁnding eﬃcient vectors are related and
a common method based on the simplex method
of linear programming was provided. The algorithm of ﬁnding all adjacent supported points was
applied as a component of a multiple criteria method for choosing among discrete alternatives
(Zionts, 1981).
Second, Insua and French (1991) and Proll
et al. (1993) suggested two methods, (1) a mathematical programming formulation and (2) a
system of inequalities, to ﬁnd all supported and
adjacent supported points among a given set of
nondominated points in the study of sensitivity
analysis of multiple criteria decision making. They
noted that identifying all supported points and
adjacent supported points is likely to be computationally demanding.
The third method is to use a convex-hull algorithm (Barber et al., 1996) from the ﬁeld of computational geometry. The convex hull of a set of
points Z is the intersection of all half-spaces that
contain Z. The input is n points in k dimensions
(V-representation) and the output is all facets of
the convex-hull. There are many diﬀerent convex-hull algorithms, including gift-wrapping, reverse search, quick hull, incremental algorithm,
etc. (Barber et al., 1996). All extreme points and
adjacent extreme points of an input set of points
can be easily found by using a convex-hull algo-

B. Kim et al. / European Journal of Operational Research 173 (2006) 565–582

569

Fig. 2. Steps for the suggested exact IPF method.

rithm. The output (facets) of a convex-hull algorithm, however, includes noneﬃcient facets. Noneﬃcient facets can be identiﬁed based on the
result that a facet is eﬃcient, if none of the convex
combinations of the facet deﬁning points is dominated by any other point. Hence, if a convex combination of facet deﬁning points is dominated by
any other point in the set, then the facet is not eﬃcient. By eliminating noneﬃcient facets, all supported and adjacent supported points can be
obtained.
Step-2
Once all supported and adjacent supported
points have been obtained, the optimality region
of weights for zs is deﬁned as a system of linear
inequalities as in (4). This corresponds to the Hrepresentation of a k-convex polytope. Clearly
the system in (4) may include redundant inequalities. Since all known exact volume computation
methods require a minimal H- or V-representation of a convex polytope (Bueler et al., 2000),
eliminating redundant inequalities in such systems is necessary. Given a system of linear
inequalities, redundant inequalities can be identiﬁed by using a method suggested by Zionts and
Wallenius (1980). The system of linear inequalities (12) in Appendix A can be represented as
follows:
Aa 6 b;

ð4Þ

where
2

3

z11  z1k

z12  z1k

   z1ðk1Þ  z1k

6 z21  z2k
6
6
6 
6
6 zp1  zpk
6
6
1
A¼6
6
6 1
6
6
6
0
6
..
6
4
.

z22  z2k

1
..
.

   z2ðk1Þ  z2k 7
7
7
7
7
   zpðk1Þ  zpk 7
7
7
7;

1
7
7

0
7
7
7

0
7
..
7
5
.

0
2


3

0

2

a1

3

6 a 7
6 2 7
a¼6
7;
4  5
ak1

zp2  zpk
1
0

1

z1k
6 z2k 7
7
6
7
6
6  7
7
6
6 zpk 7
7
6
7
6
7;
6
1
b¼6
7
6 0 7
7
6
7
6
6 0 7
7
6
6 .. 7
4 . 5
0

and zij represents the jth objective value of the ith
point.
Step-3
One dimension (an axis for the value function)
needs to be added to the optimality region of

570

B. Kim et al. / European Journal of Operational Research 173 (2006) 565–582

weights obtained in Step-2 to construct a k-convex
polytope representing IPF(zs). The value surface
g(zs; a) over the optimality region of weights for
zs, is linear, since g(z; a) is assumed to be a convex
combination of objective functions and h(a) follows
a uniform distribution. Hence, the polytope formed
in the optimality region of weights and the value
function space is convex. Thus, k dimensional integration to obtain IPF(Z) in Eq. (2) becomes a series
of exact volume computations of k-convex polytopes corresponding to each supported point zs.
From a system of linear inequalities as in (4), let
a = (aT, g) and g = dTa + b 0 . Then g 6 dTa + b 0 .
Hence a k-convex polytope for a supported point
can be constructed as a system of linear inequalities as in (5).
ð5Þ

:

For example, consider the k objective function
case. Assume a supported point vector zp =
(zp1, zp2, . . ., zpk) is optimal for a weight region
Aa 6 b. Then g(zp; a) 6 (zp1  zpk)a1 + (zp2 zpk)
a2 +P
   + (zp(k1)  zpk)a(k1) + zpk. Since ak ¼
1  ai , where i = 1, 2, . . . , k  1, dT = (zp1 
zpk, zp2  zpk, . . . , zp(k1)  zpk), a = (a1, a2, . . . ,
ak1, g), and b 0 = zpk. Fig. 3 illustrates convex
polytopes corresponding to supported points. As
can be seen from Eq. (2), the volume of a convex
polytope corresponding to zs represents IPF(zs).
Step-4
Algorithms for exact volume computation
decompose a given convex polytope into simplices,
and they all rely, explicitly or implicitly, on the
volume formula of a simplice (Bueler et al.,
2000). A k-convex polytope P can be divided into
m simplices in Rk by a decomposition method.
Then the volume of the k-convex polytope is the
sum of the volume of m simplices as in Eq. (6):
m
X
VolðiÞ;
VolðP Þ ¼
i¼1

 i
 v11 vi12

 vi
vi
1

VolðiÞ ¼ det  21 22
...
k!

 1
1


vi1k 
. . . vi2k 
;


...: 1 
...

ð6Þ

gα(z)
IPF(z2)

IPF(z1)

IPF(z4)

α1

α2

IPF(z3)

Fig. 3. Illustration of convex polytopes for supported points in
the weight-value function space.

where Vol(P) stands for the volume of a k-convex
polytope P, Vol(i) stands for the volume of the ith
simplice of polytope P, and vijl represents the lth
coordinate of the jth point of the ith simplice.
As can be seen from Bueler et al. (2000), there
exist many diﬀerent volume computation algorithms based on the decomposition method of a
given convex polytope, such as Delaunay triangulation, boundary triangulation, Cohen & HickeyÕs
triangulation, LasserreÕs recursive algorithm, LawrenceÕs volume formula, etc. Table 1 summarizes
the volume computation methods and their required input data structures based on Bueler et
al. (2000). In our problem, the k-convex polytope
is a simple polytope and represented with facets
(H-representation) as can be seen in Fig. 3. LasserreÕs method seems to be eﬃcient according to
the experimental results of Bueler et al. (2000).
Step-5
Step-5 is self-explanatory.
3.1. Remarks on the implementation of the
suggested exact IPF method
The exact IPF method suggested in this paper
was coded using Matlab (student version 6.1). To
ﬁnd all supported points and their adjacent supported points, a Matlab function Ôconvhulln( )Õ
was used. The Ôconvhulln( )Õ function uses the
Qhull algorithm (Barber et al., 1996). Input for

B. Kim et al. / European Journal of Operational Research 173 (2006) 565–582

571

Table 1
Volume computation methods of convex k-polytopes
Class of volume computation methods

Method

Required input data

Suggested use

Triangulation

Boundary triangulation
Delaunay triangulation
Cohen & Hickey triangulation

V-representation
V-representation
V- & H-representation

Simplicial polytope
(small n to m ratio)

Sign decomposition

LasserreÕs method
LawrenceÕs method

H-representation
V- & H-representation

Simple polytope
(large n to m ratio)

Note. A d-polytope is simple, if each vertex is contained in exactly d facets. A d-polytope is simplicial, if each facet contains exactly d
vertexes (http://www.ifor.math.ethz.ch/~fukuda/polyfaq.html).

the Ôconvhulln( )Õ function is n nondominated
points in Rk. Output of the function is m facets
of the convex hull deﬁned by the input data points.
Among the m facets, noneﬃcient facets are eliminated. Then, all supported and adjacent supported
points are obtained by using eﬃcient facets.
The optimality region of weights for each supported point was identiﬁed by using adjacent supported points. The resulting system of linear
inequalities may include redundant constraints.
Rather than eliminating them to obtain the minimal H-representation, a vertex enumeration algorithm based on Steuer (1986) was used to convert
the H-representation into a V-representation, since
the volume computation method we used requires
V-representation.1 This conversion method does
not require minimal H-representation. Through
the vertex enumeration algorithm, all extreme
points (vi) in the weight space can be obtained as
follows:
P k1 ¼ fv1 ; v2 ; . . . ; vn g:

ð7Þ

Then the k-convex polytope for zs can be represented by the following Ôlift-upÕ step. Assume there
are l extreme points in the V-representation of the
optimality region of weights for zs, then we need 2l
extreme points to represent a k-convex polytope
for zs:
P k ¼ ½ðv1 ; 0Þ; ðv2 ; 0Þ; . . . ; ðvl ; 0Þ; ðv1 ; gðx; v1 ÞÞ;
ðv2 ; gðx; v2 ÞÞ; . . . ; ðvl ; gðx; vl ÞÞ:

ð8Þ

1
For large systems of linear inequalities the conversion from
H-representation to V-representation may be time consuming.
In such cases, one could use the redundancy elimination
algorithms.

To calculate the volumes of k-convex polytopes a
Matlab function Ôdelaunayn( )Õ was used. The Ôdelaunayn( )Õ function uses Delaunay triangulation
algorithm (Barber et al., 1996; Bueler et al.,
2000) to decompose each k-convex polytope into
m simplices. The input of the Ôdelaunayn( )Õ function is n vertexes of a k-polytope and the output
m simplices in Rk. Then the volume of each of
the k-simplices is obtained using Eq. (6).

4. A numerical example
A numerical example of calculating IPF(Z) for
a set of nondominated points is provided to illustrate the calculation procedures and to show the
input and output data structures throughout the
steps of the method.
Consider a three objective function case and assume that there are six points in set Z: z1(1, 2, 3),
z2(2, 5, 1), z3(3, 2, 2), z4(4, 1, 5), z5(5, 3, 1),
z6(2, 4, 2) in the objective space. Notice that these
six points are all nondominated.
[Step-1] Enumerating all supported points and
their adjacent supported points.
Recall that we only need to consider the supported points since we consider a convex combination of the objectives as the value function. All
extreme points and adjacent extreme points are
found using a convex hull algorithm. In this example, eight facets f1, f2, . . . , f8 deﬁne the convex hull
of the six points in the objective function space.
This facet information is given in Table 2.

572

B. Kim et al. / European Journal of Operational Research 173 (2006) 565–582

For example, facet f1 in Table 2 consists of three
points z2, z4 and z5 (see Fig. 4 for a planar graph of
the facets). Among the eight facets, three facets, f1,
f5 and f7 are determined as noneﬃcient, since a
convex combination of the facet deﬁning points
is dominated by another point in the set. From
the ﬁve eﬃcient facets, supported points are easily
found by checking all points, which form eﬃcient
facets. Any input point that is not included in
the set of eﬃcient facet deﬁning points, is not a
supported point. In this example, all six points
are supported points.
All adjacent supported points for each supported point are found by simply enumerating eﬃcient facet-deﬁning points. Since the convex-hull
algorithm used here employs a triangulation method
in deﬁning facets, all facets are k  1 simplices.
Table 3 shows all adjacent supported points for
each supported point. For example, adjacent supported points of z1 are z2, z3, z4, and z6.
[Step-2] Deﬁning the optimality regions of weights
for supported points.
For a supported point z1 the H-representation
of the optimality region of weights is deﬁned with
seven linear inequalities: 3a1  5a2 6 2, 3a1 
a2 6  1,  a1 + 3a2 6 2,  2a1 + a2 6  1, a1 +
a2 6 1,  a1 6 0, and a2 6 0. This is not a minimal H-representation of a convex polytope, since
it includes redundant inequalities. This H-representation, however, can be converted into a V-representation by using an extreme point enumeration
algorithm. For z1 the minimal V-representation of

Facets

Facet-deﬁning points

Eﬃciency

f1
f2
f3
f4
f5
f6
f7
f8

z4
z3
z2
z4
z3
z2
z6
z4

Noneﬃcient
Eﬃcient
Eﬃcient
Eﬃcient
Noneﬃcient
Eﬃcient
Noneﬃcient
Eﬃcient

z5
z1
z5
z1
z5
z1
z1
z2

z6
z4

z5

z1

z3

Fig. 4. Planar graph of all facets deﬁning a convex hull of the
input data.

Table 3
Supported points and adjacent supported points
Points

Adjacent supported points

z1
z2
z3
z4
z5
z6

z2
z1
z1
z1
z2
z1

z3
z3
z2
z2
z3
z2

z4
z4
z4
z3

z6
z5
z5
z6

z6

z4

the optimality region of weights is: v1(0.25, 0.25),
v2(0.1, 0.7), v3(0.667, 0), v4(0.25, 0.75), v5(1, 0).
[Step-3] Constructing a k-convex polytope for a
supported point zs in the weight-value function
space.
For z1, 10 points deﬁne the convex polytope:
v1(0.25, 0.25, 0), v2(0.1, 0.7, 0), v3(0.667, 0, 0),
v4(0.25, 0.75, 0), v5(1, 0, 0), v6(0.25, 0.25, 2.25),
v7(0.1, 0.7, 2.1), v8(0.667, 0, 1.667), v9(0.25, 0.75,
1.75), v10(1, 0, 1) (see Fig. 5).
[Step-4] Computing the volume of the k-convex
polytope formed in Step-3.

Table 2
Output of the convex hull algorithm

z2
z2
z3
z3
z4
z6
z4
z6

z2

For z1 the polytope in Fig. 5 is decomposed into
10 simplices via the Delaunay triangulation
method as shown in Table 4. The volume of each
simplice is obtained by Eq. (7). Then, the IPF(z1)
is the sum of the volumes of the 10 simplices.
For z1, the sum of the 10 tetrahedrons is 0.457,
and the volume of the k  1 simplex is 0.5 (triangle). Hence, IPF(z1) is 0.914 (0.457/0.5).

B. Kim et al. / European Journal of Operational Research 173 (2006) 565–582
g(x;α)
v6

v7

v8

v9

v10
v3
v1

v2

v5

α1

573

the set. We continue randomly generating points
and eliminating the dominated ones until the
set reaches a prespeciﬁed size. We note that this
method tends to generate a relatively large number
of points near the corner, but still provides a fair
basis for comparing the performance of the
approximate computation method to that of the
exact algorithm discussed above.
5.1. Computational eﬃciency of the exact IPF
method

v4

α2

Fig. 5. k-Convex polytope for supported point z1.

[Step-5] Finally, IPF(Z) is obtained via summing
up IPF(zi), i = 1, 2, . . . , 6.

5. Computational tests
In this section, the computational eﬃciency of
the exact IPF method is analyzed and compared
to that of the approximation methods. To generate
a number of appropriate solution sets with a desired number of extreme points quickly, we
adopted the following simplistic approach.
Throughout this experimentation, the points in
the solution sets were randomly generated from
the uniform distribution U(0, 1) for each objective,
and only the nondominated points are included in

Table 4
Decomposition of k-convex polytope for z1
Simplices

Simplice-deﬁning points

s1
s2
s3
s4
s5
s6
s7
s8
s9
s10

v9
v4
v4
v8
v8
v7
v7
v3
v3
v3

v1
v1
v9
v9
v9
v9
v9
v4
v4
v8

v10
v10
v10
v1
v6
v1
v6
v1
v5
v1

v2
v2
v2
v10
v1
v2
v1
v10
v10
v10

We investigated the relationship between CPU
time of the algorithm as a function of the number
of nondominated points (or supported points) in a
set and the number of objectives. Fig. 6 shows the
experimental results. In Fig. 6, 20 diﬀerent sets of
nondominated points for three, four, and ﬁve
objective functions were tested. As can be seen in
Fig. 6, the CPU time increases roughly linearly
as the number of nondominated points (or supported points) increases. However, the CPU time
increases roughly exponentially as the number of
objective functions increases. Accordingly, a
numerical approximation (Monte Carlo) method
seems to be reasonable for problems with four or
more objective functions.
To analyze the CPU time of the exact method, the
CPU time was divided into two parts. As discussed
in Section 3, the suggested algorithm can be decomposed into two main sub-problems—determination
of the optimality regions of weights and a series of
volume computations of convex polytopes. Time
checkpoints were inserted in the method to obtain
the CPU time for obtaining the optimality region
of weights (CPU-ORW) and the CPU time for volume computations (CPU-VC) of convex polytopes.
The total CPU time is equal to the sum of CPUORW and CPU-VC.
Fig. 7 shows the CPU time for computing both
the optimality regions of weights and volumes of
convex polytopes for three, four, and ﬁve objective
function cases. We have found that the CPU time
for computing the optimality regions of weights is
at most 10% of total CPU time in the case of three
objectives. Furthermore, it is less than 4% in the
cases of four and ﬁve objectives. From Fig. 7, we
can see that the computational time to obtain

574

B. Kim et al. / European Journal of Operational Research 173 (2006) 565–582
200
180

3 objectives

160

4 objectives

CPU time (sec.)

140

5 objectives
120
100
80
60
40
20
0
10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200
(10) (14) (19) (20) (23) (30) (40) (42) (47) (49) (51) (55) (63) (64) (74) (79) (80) (86) (89) (91)

Number of non-dominated points in set Z
(number) represents supported points in the corresponding set
Fig. 6. CPU time versus number of nondominated points (supported points) and number of objectives.

optimality regions of weights increases linearly
with the number of supported points. However,
computational time to calculate volumes of convex
polytopes seems to increase exponentially with the
number of supported points.

the number of objectives and the number of supported points increase. In such cases, using an
approximation of IPF is reasonable. Two research
questions related to the approximation methods
are investigated and discussed.

5.2. Computational testing of numerical
approximations of IPF

5.2.1. Random weight vector generation methods
To use the numerical approximation method,
a method of generating random weight vectors
ai which follow the assumed distribution h(a),
is needed. Also ai need to be sampled in the

As can be seen in Section 5.1, the exact IPF
method becomes computationally burdensome as

4 Objectives

CPU time (sec.)

3 Objectives

5 Objectives

4.5

16

200

4

14

180
160

3.5

12

3
2.5
2

120

8

100
80

1.5

6

1

4

0.5

2

0

140

10

Number of supported points

volume
computation

60
40
20

0

5 8 11 18 27 39 61 69 81

optimal weight
region

0

5 10 14 17 23 32 42 47 59 73

Number of supported points

8 13 20 27 32 39 54 67

Number of supported points

Fig. 7. CPU time for computing both optimality regions of weights and volumes of convex polytopes.

B. Kim et al. / European Journal of Operational Research 173 (2006) 565–582

k  1 simplex, since the sum of the weights is assumed to be 1. Three methods to generate random weight vectors uniformly in a k-simplex
could be found in the literature. Rubinstein
(1982) provided two methods of generating uniformly distributed random weight vectors on
the simplex. Jaszkiewicz (2000) provided a method for the same purpose. For the convenience of
the reader, the three methods are brieﬂy reviewed below.

[Step-3]

• Method-2: Rubinstein-2
[Step-1] Generate k  1 random variates ui from
U(0, 1).
[Step-2] Sort ui in increasing order: u(1),
u(2), . . . , u(k1).
[Step-3] Then
a1 = u(1), a2 = u(2)  u(1), . . . ,
ak1 = u(k1)  u(k1), ak = 1  u(k1).

[Step-1] Generate k  1 random variates ui from
U(0, 1).
[Step-2] Generate random numbers ai from Eq.
(9):

4 Objectives

Mean CPU Time (Sec)

Apprx-1
Apprx-2
Apprx-3

0.2
0.15
0.1
0.05
0
300

500

700

If i = k, then stop.

5.2.2. Number of weight vectors and
approximation error
From the fundamental theorem of Monte Carlo
estimation of high dimensional integration,
IPFN(Z) converges to IPF(Z) as N approaches
inﬁnity (Smith, 1984). Large numbers of weight
vectors will reduce the variance of the mean estimate but will also increase the CPU time. However,
to apply numerical estimation methods in practice,
an appropriate number of weight vectors to estimate IPFN(Z) needs to be determined. To do this,

• Method-3: Jaszkiewicz

3 Objectives

900

Number of Weight Vectors

5 Objectives

0.45
0.4
0.35
0.3
0.25

0.45
0.4
0.35
0.3
0.25

0.2
0.15
0.1
0.05
0

0.2
0.15
0.1
0.05
0
100

ð9Þ

All three random weight vector generation
methods were tested. Apprx-1 represents IPF
approximation using RubinsteinÕs ﬁrst method,
Apprx-2 represents RubinsteinÕs second method,
and Apprx-3 stands for JaszkiewiczÕs method.
Fig. 8 shows the CPU time comparison results
for Apprx-1, Apprx-2, and Apprx-3. For three,
four, and ﬁve objectives, a set of 46, 123, and
169 nondominated points were tested respectively.
For each number of weight vectors, 30 replicates
were performed. The CPU time in Fig. 8 is the
average CPU time for a given number of weight
vectors.
As can be seen from Fig. 8, Apprx-2 is the best
from the average CPU time point of view, even
though the diﬀerence appears negligible.

[Step-1] Generate k random variates ui from
U(0, 1),
ai = ln(ui)
(exponential
mean =
1).
Pk
[Step-2] Tot ¼ i¼1 ai .
ai
[Step-3] ai ¼ Tot
.

100

k1

j¼1

• Method-1: Rubinstein-1

0.45
0.4
0.35
0.3
0.25

pﬃﬃﬃﬃﬃ
u1 ; . . . ; ai
!
i1
X
pﬃﬃﬃﬃ
1
aj ð1  ki ui Þ; . . .

a1 ¼ 1 
¼

575

300

500

700

900

Number of Weight Vectors

100

300

500

700

900

Number of Weight Vectors

Fig. 8. Average CPU time for three weight vector generation methods over 30 replicates.

576

B. Kim et al. / European Journal of Operational Research 173 (2006) 565–582

the trade-oﬀ between the time to estimate IPFN(Z)
and the estimation error (i.e., error = IPF(Z) 
IPFN(Z)) needs to be considered.
To show the convergence of Apprx-1, Apprx-2,
and Apprx-3 as the number of weight vectors (N)
increases, three sets including 150 nondominated
points (number of supported points for each set
was 72, 69, 65) for three, four, and ﬁve objective
functions were tested. Common random numbers
were used for the three methods. For a single replication, the Error Ratio for i weight vectors is
calculated as follows:

increases, more replicates are likely to be within
the speciﬁed error ratio.
From Fig. 10, it appears that when 5000 weight
vectors are used in estimating IPF, the chance
that the estimation error ratio is within 0.01 is
about 0.9.
Fig. 11 shows the relationship between speciﬁed
error ratios and the number of weight vectors for
cases where 95 replicates out of 100 are within
the speciﬁed error ratios. Fig. 11 shows the
trade-oﬀ between time and error of estimation.
From Fig. 11, if an estimation of IPF within 0.01
error ratio is needed for the case of ﬁve objectives,
then 4000 weight vectors are needed with 0.05 signiﬁcance level.
To generalize the conjecture made in Figs. 10
and 11, 45 sets of randomly generated nondominated points, 15 for three, 15 for four, and 15 for
ﬁve objectives, were tested. For each data set, all
three approximation methods, Apprx-1, Apprx-2,
and Apprx-3, were tested. Two designated error
ratios: 0.05 and 0.01, were speciﬁed. Then, the
number of weight vectors for cases where 95 replicates out of 100 are within the speciﬁed error ratios
were calculated. Tables 5–7 show the results for
three, four, and ﬁve objective functions.
First of all we do not see any signiﬁcant diﬀerences in the convergence rate among Apprx-1,
Apprx-2, and Apprx-3 (all pairwise t-statistics
are less than 2). For the exact method, the trend
of the CPU time as a function of the number of
objectives and the number of nondominated
points, is obvious. However, a similar trend cannot

Error Ratio for i weight vectors ðERi Þ
IPFðZÞ  IPFi ðZÞ
;
ð10Þ
IPFðZÞ
Pi
where IPFi ðZÞ ¼ 1i j¼1 gðz; aj Þ.
In Fig. 9, we see that the Error Ratios of all
three methods converge to 0 as the number of
weight vectors increases for three, four, and ﬁve
objective cases. The convergence rate for three
objective cases appears low compared to the four
or ﬁve objective cases.
To determine a robust estimate of the appropriate (suﬃcient) number of weight vectors to use in
estimating IPF, 100 replicates were performed for
each number of weight vectors. Then the number
of replicates for which the error ratio is within
0.01 was counted. Fig. 10 shows the results of
Apprx-2 for three sets including 100 nondominated points for three, four, and ﬁve objectives.
We can see that as the number of weight vectors
¼

4 Objectives

3 Objectives

500

1000

1500

Number of Weight Vectors

500

1000

1500

Number of Weight Vectors

5 Objectives

500

1000

1500

Number of Weight Vectors

Fig. 9. Convergence of error ratio as a function of the number of weight vectors—single run.

B. Kim et al. / European Journal of Operational Research 173 (2006) 565–582

577

100

Number of Replicates

90
80
3 Objectives

70

4 Objectives

60

5 Objectives

50
40
30
20
10
0
1

181

551

1127

1888

2536

3312

4076

4782

5567

6289

6957

Number of Weight Vectors

Number of weight vectors

Fig. 10. Number of replicates among 100 replicates where error ratio is within 0.01 as a function of the number of weight vectors.

6000
5000

3 Objectives

4000

4 Objectives

3000

5 Objectives

be found for the approximation methods. We cannot detect any relationship between the required
number of weight vectors to estimate IPF within
a speciﬁed error ratio with 0.05 signiﬁcance level,
and the number of objectives or the number of
nondominated points (or supported points).
As demonstrated in Fig. 11, the required number of weight vectors is clearly aﬀected by the speciﬁed error ratio. In our experiments, when a 5%
error ratio can be tolerated, at most 250 weight
vectors are needed with 5% signiﬁcance level for
any number of objectives, any number of nondominated points, and any random weight vector

2000
1000
0
0.01

0.02

0.04

0.06

0.08

0.1

Error Ratio

Fig. 11. Number of weight vectors as a function of error ratio
for cases when 95% of the observations are within a speciﬁed
error ratio.

Table 5
Number of weight vectors needed to obtain the designated error ratio 95% of the time—three objectives
3 objectives

Number of weight vectors
5% error ratio

1% error ratio

NNP

NSP

Apprx-1

Apprx-2

Apprx-3

Apprx-1

Apprx-2

Apprx-3

20
30
40
50
60
70
80
90
100
110
120
130
140
150
160

6
17
23
35
45
51
56
57
58
65
70
75
84
84
101

173
161
145
164
188
109
118
179
147
126
198
162
200
131
115

142
104
100
134
161
146
127
160
112
161
230
206
174
151
110

176
136
173
154
179
123
139
141
167
139
196
167
227
124
111

3636
4189
4019
3868
3127
3013
4100
2378
4448
2439
5856
3099
4633
4692
2330

4057
4708
4289
3227
4181
3739
3403
2486
3417
2384
4197
4420
3767
4290
2820

3275
3669
4241
3475
3895
3077
3702
2968
3931
2484
4923
3639
4383
4212
2509

NNP: Number of Nondominated Points, NSP: Number of Supported Points.

578

B. Kim et al. / European Journal of Operational Research 173 (2006) 565–582

Table 6
Number of weight vectors needed to obtain the designated error ratio 95% of the time—four objectives
4 objectives

Number of weight vectors
5% error ratio

1% error ratio

NNP

NSP

Apprx-1

Apprx-2

Apprx-3

Apprx-1

Apprx-2

Apprx-3

20
30
40
50
60
70
80
90
100
110
120
130
140
150
160

7
14
28
36
39
40
41
45
47
54
58
63
68
70
77

93
107
103
137
114
144
186
216
160
83
212
64
125
123
147

99
106
103
158
110
89
169
201
142
114
208
126
118
123
125

67
92
106
142
146
96
178
176
159
81
185
81
111
100
122

2292
1879
2464
4302
3233
2201
3521
3957
3530
2901
3528
2358
2935
3828
2578

1948
2294
2207
3388
2426
2800
5853
4059
2765
2168
4277
2167
3209
4871
3462

2229
2110
2073
3762
3766
2489
3632
3406
3397
3088
3941
2333
3473
3120
2554

NNP: Number of Nondominated Points, NSP: Number of Supported Points.

Table 7
Number of weight vectors needed to obtain the designated error ratio 95% of the time—ﬁve objectives
5 objectives

Number of weight vectors
5% error ratio

1% error ratio

NPP

NSP

Apprx-1

Apprx-2

Apprx-3

Apprx-1

Apprx-2

Apprx-3

20
30
40
50
60
70
80
90
100
110
120
130
140
150
160

9
16
20
28
34
30
39
46
52
62
67
75
69
82
79

119
96
124
149
65
60
110
88
103
113
128
85
94
129
58

129
76
138
138
109
63
95
55
108
90
162
88
125
159
66

128
93
70
132
74
71
101
104
106
104
136
80
99
112
80

3704
2444
2249
2575
2204
1294
2552
2556
2750
3455
3831
2395
2793
2483
2627

2766
1550
2105
2669
3032
1504
2268
2536
2841
2005
5090
1954
3022
3235
2692

3598
2521
2796
2527
1795
1435
2901
2245
2875
2550
3915
2430
2568
2542
2344

NNP: Number of Nondominated Points, NSP: Number of Supported Points.

generation method. When a 1% error ratio is required, at most 6000 weight vectors are needed
with 5% signiﬁcance level. Finally one interesting
ﬁnding is that, as the number of objectives increases, the number of required weight vectors
seems to decrease as can be seen across the three
tables.

6. Conclusions and future research
Many multiple criteria a posteriori heuristics
have appeared in the literature to solve diﬃcult
multiple criteria combinatorial optimization problems. Accordingly, a robust and eﬃcient measure
is needed to compare alternative heuristics and to

B. Kim et al. / European Journal of Operational Research 173 (2006) 565–582

tune parameters employed in them. For this purpose, the Integrated Preference Functional (IPF)
for bi-criteria optimization problems was suggested in Carlyle et al. (2003). The computational
eﬀort of obtaining IPF for the bi-criteria case is
trivial. However, the computational complexity
increases exponentially for three or more objective
function cases.
In this paper, we have developed a theoretical
procedure to obtain an exact IPF measure for k
(P3) objective functions, when the value function
is implicitly a convex combination of objectives
and the weight density function follows a uniform
distribution. This procedure includes two main
sub-problems: (1) obtaining the optimality region
of weights for all supported points and (2) volume
computation of k-convex polytopes. Several diﬀerent algorithms have been presented to solve each
sub-problem. Accordingly there seems to be many
alternative methods to obtain an exact IPF. We
suggest an exact method, which is easy to
implement.
Experimental results show that the CPU time of
the exact IPF method increases roughly linearly as
the number of nondominated points (and supported points) increases and roughly exponentially
as the number of objectives increases. Also most of
the CPU time (more than 90%) to calculate the
IPF is consumed in computing the volumes of kconvex polytopes.
Since the computational time to compute the
volume of convex polytopes increases exponentially with the number of objectives, Monte Carlo
approximation methods were tested for k P 3.
Experimental results show that three random
weight vector generation methods on a k-simplex
are not signiﬁcantly diﬀerent in the convergence
rate. We observe that the number of weight vectors
needed increases exponentially as one desires a
lower approximation error. We also found that
the convergence rate of the approximation methods is neither dependent upon the number of nondominated points nor the number of objectives.
Based on our experimental results, we recommend
that about 250 random weight vectors be used to
estimate the IPF of a set within a 5% error ratio
and about 6000 random weight vectors be used
to estimate it within 1% error ratio. This experi-

579

mental result can be used as a guideline for practical use of the numerical approximation.
In this work we assumed that the decision makerÕs value function is a convex combination of
the objectives. This implies that nonsupported
solutions do not contribute to the IPF measure.
One area of future work we would like to pursue
involves the consideration of other more general
forms of value functions, such as the Tchebycheﬀ
function. Carlyle et al. (2003) brieﬂy discusses the
Tchebycheﬀ function in this context. We present
an exact method for calculating the IPF value with
the Tchebycheﬀ function for two objectives in Kim
et al. (2004), which is available from the authors
upon request. Unfortunately, extending this
method to more than two objectives has proven
to be more challenging for several reasons (e.g.,
the optimality region of weight for a potentially
optimal solution may not be continuous) and is
subject of ongoing work.
We believe that two other extensions to the current work will prove to be fruitful. The ﬁrst is to
incorporate the decision makerÕs partial preference
information into the IPF calculation. Such partial
preference information could be modeled by identifying a most preferred region in the objective
space or specifying constraints on the weights.
The second is to verify whether the recommendation about the number of weight vectors we provided is still applicable for other cases, such as
nonconvex value functions (e.g., Tchebycheﬀ)
and nonuniform weight functions (e.g., Triangular
distributed).

Acknowledgment
The authors would like to acknowledge the support of the National Science Foundation under
contract DMII-0121815.

Appendix A. Some deﬁnitions and proofs
We only consider multiple objective minimization problems, where all objective functions, without loss of generality, are assumed to be
minimized.

580

B. Kim et al. / European Journal of Operational Research 173 (2006) 565–582

Deﬁnition (Facet (H-) representation of a k-convex
polytope). A convex polytope P is a bounded
subset of Rk, which is an intersection of a ﬁnite
number of half spaces; P = {x j Ax 6 b}.

From Corollary 1, a supported point from a given set of nondominated points Z, can be obtained
by solving the following system of linear
inequalities:
aT ðzi  zj Þ 6 0;

i 6¼ j; zi ; zj 2 Z;

Deﬁnition (Vertex (V-) representation of a k-convex polytope). A convex polytope P is the convex
hull conv(V) of a ﬁnite set V = {v1, v2, . . . , vn} of
points in Rk; P = conv(V).

where zi is a supported point if and only if (11) is
consistent, otherwise zi is not a supported point.

Deﬁnition (A supported (efficient extreme) point).
A supported point zs in a set of nondominated
points Z, corresponds to a point not dominated
by a convex combination of other points in
set Z.

Theorem 2. An optimality region of weights for a
supported point in Rk is a k  1 convex polytope in
weight space, if the value function g(z; a) is a convex
combination of z and the weight density function
h(a) follows a uniform distribution.

Deﬁnition (An adjacent supported point). Two
supported points are adjacent, if they are adjacent
on the convex hull of set Z and if none of their
convex combinations are dominated by any other
point in set Z.

Proof. Consider
problem:

Deﬁnition (Optimality region of weights). An optimality region of weights for a supported point zs is
a region in weight space, within which one and
only one point is optimal for a given convex combination of objectives. On the boundaries of such
regions, a supported point zs and its adjacent
extreme points are optimal (with the same objective function value).
TheoremP1. If there exists an a 2 fa 2 Rk j ai P 0
8i and
ai ¼ 1g such that x minimizes
the LP:
P
minfaT x j Ax 6 b; ai P 0 8i and
ai ¼ 1g, then
x 2 X is a supported point.
Proof. The proof of Theorem 1 can be found in
Steuer (1986). h

ai P 0

the

following

optimization

min

gðz; aÞ ¼ aT z

s:t:

z 2 Z ðZ is a set of nondominated pointsÞ
X
0 6 a 6 1;
a ¼ 1:

Assume that for supported points z1 and z2 their
optimality region of weights is not convex in
weight space as in Fig. 12. Assume z1 is optimal
within the bounded region and z2 is optimal outside the boundary region in Fig. 12. Consider three
weight vectors, a1, a2, and a3, which lie on the
same half space. By deﬁnition of optimality, the
following three strict inequalities can be obtained:
(1) aT1 z1 < aT1 z2 , (2) aT2 z1 < aT2 z2 , and (3) aT3 z1 >
aT3 z2 . Also a3 can be represented as a convex combination of a1 and a2 like a3 = wa1 + (1  w)a2,
where 0 6 w 6 1. Hence, the strict inequality (3)
can be rewritten as (4) waT1 z1 þ ð1  wÞaT2 z1 >
waT1 z2 þ ð1  wÞaT2 z2 by substituting for a3. However, the strict inequality (4) cannot be satisﬁed,

Corollory P
1. If there exists an a 2 fa 2 Rk j ai P
0 8i and
ai ¼ 1g such that z minimizes the LP:
min{aTz j z 2 Z, Z is a set of all extreme points in
Rk}, then z 2 Z is a supported point.
Proof. In Theorem 1, the feasible region of the LP
is an H-representation. This feasible region can be
converted into a V-representation through an
extreme point enumeration algorithm (Steuer,
1986). Hence, Corollary 1 holds. h

ð11Þ

8i;

z2
α3

α1
z1

α2
Non-convex optimality region of
weights for supported point z1

Fig. 12. Nonconvex optimality region of weights.

B. Kim et al. / European Journal of Operational Research 173 (2006) 565–582
f3(x)

f3(x)

581

α2
1

z3

z2

z3

z4
f1(x)

f1(x)

z2

z4
z1

f2(x)

(a) objective space
A set of non-dominated points

f2(x)

z1

(b) objective space
A set of supported points

1

α1

(c) weight space
Optimality regions of weights
for supported points

Fig. 13. Illustration of optimality regions of weights for supported points.

since w P 0, (1  w) P 0, aT1 z1 < aT1 z2 from (1),
and aT2 z1 < aT2 z2 from (2). Thus, the optimality region of weights is convex. Since a convex combination of z is assumed, the optimality region of
weights is a bounded region on a k  1 simplex
(k  1 convex polytope). h
Theorem 3. The optimality region of weights for a
supported point zs is defined by a system of linear
inequalities as in (4), if the value function g(z; a) is
a convex combination of z and the weight density
function h(a) follows a uniform distribution:
aT ðzs  zj Þ 6 0;
k
X
ai ¼ 1;

s 6¼ j; zs 2 Z s ; zj 2 Z as ;
ð12Þ

i¼1

ai P 0

8i;

where Zs is a set of supported points and Zas is a set
of adjacent supported points of zs.
Proof. From the deﬁnition of an adjacent supported point, Theorem 3 can easily be proved.
The system of linear inequalities in (12) is clearly
consistent, since zs is a supported point. h
Fig. 13 illustrates the optimality regions of
weights in weight space for a given set of nondominated points (represented in objective space) for
three objectives. In Fig. 13, (a) represents a set of
nondominated points in objective space, (b) represents four supported points and all their adjacent
supported points in objective space, and (c) represents the optimality regions of weights for each
supported point in weight space.

References
Aksoy, Y., Butler, T.W., Minor, E.D., 1996. Comparative
studies in interactive multiple objective mathematical programming. European Journal of Operational Research 89,
408–422.
Barber, C.B., Dobskin, D.P., Huhdanpaa, H., 1996. The
quickhull algorithm for convex hulls. ACM Transactions
on Mathematical Software 22 (4), 469–483.
Bueler, B., Enge, A., Fukuda, K., 2000. Exact volume computation for polytopes: A practical study, in: Gil Kalai, Gunter
M. Ziegler (Eds.), Polytopes Combinatorics and Computation, vol. 29 of DMV Seminar, Birkhauser Verlag, Basel,
pp. 131–154.
Carlyle, W.M., Fowler, J.W., Gel, E., Kim, B., 2003. Quantitative comparison of approximate solution sets for
bi-criteria optimization problems. Decision Sciences 34 (1),
63–82.
Coello, C.A., 1999. An updated survey of evolutionary multiobjective optimization techniques: State of the art and
future trends. 1999 Congress on Evolutionary Computation, IEEE Service Center, Washington, DC, pp. 3–13.
Czyzak, P., Jaszkiewicz, A., 1998. Pareto simulated annealing—
A metaheuristic technique for multiple objective combinatorial optimization. Journal of Multi-Criteria Decision
Analysis 7, 34–47.
Fowler, J.W., Kim, B., Carlyle, W.M., Gel, E., Horng, S.-M.,
2005. Evaluating a posteriori solution techniques for bicriteria parallel machine scheduling problems. Journal of
Scheduling 8 (1), 75–96.
Hansen, P., Jaszkiewicz, A., 1998. Evaluating the quality of
approximations to the nondominated set. Technical Report,
IMM-REP-(1998)-7, Institute of Mathematical Modeling,
Technical University of Denmark, Lyngby.
Insua, D.R., French, S., 1991. A framework for sensitivity
analysis in discrete multi-objective decision-making. European Journal of Operational Research 54, 176–190.
Jaszkiewicz, A., 2000. On the computational eﬀectiveness of
multiple objective metaheuristics. In: Proceedings of the
Fourth International Conference on Multi-Objective Programming and Goal Programming (MOPGPÕ00): Theory &
Applications. Springer-Verlag, Berlin, Heidelberg.

582

B. Kim et al. / European Journal of Operational Research 173 (2006) 565–582

Kim, B., Fowler, J.W., Gel, E.S., Carlyle, W.M., Wallenius, J.,
2004. Evaluation of solution sets for MOCO problems with
Tchebycheﬀ function. Technical Report, Department of
Industrial Engineering, Arizona State University, Tempe,
AZ.
Kim, B., Gel, E.S., Carlyle, W.M., Fowler, J.W., 2001. A new
technique to compare algorithms for bi-criteria combinatorial optimization problems, MCDM in the New Millenium.
In: Koksalan, M., Zionts, S. (Eds.), Lecture Notes in
Economics and Mathematical Systems, vol. 507. SpringerVerlag, pp. 113–123.
Proll, L.G., Insua, D.R., Salhi, A., 1993. Mathematical
programming and the sensitivity of multi-criteria decisions.
Annals of Operations Research 43, 109–122.
Rubinstein, R.Y., 1982. Generating random vectors uniformly
distributed inside and on the surface of diﬀerent regions.
European Journal of Operational Research 10, 205–209.
Smith, R.L., 1984. Eﬃcient Monte Carlo procedures for
generating points uniformly distributed over bounded
regions. Operations Research 32 (6), 1296–1307.

Steuer, R.E., 1986. Multiple Criteria Optimization: Theory,
Computation, and Application. Wiley, New York.
Viana, A., De Sousa, J.P., 2000. De Using metaheuristics in
multi-objective resource constrained project scheduling.
European Journal of Operational Research 120, 359–
374.
Zionts, S., 1981. A multiple criteria method for choosing among
discrete alternatives. European Journal of Operational
Research 7, 143–147.
Zionts, S., Wallenius, J., 1976. An interactive programming
method for solving the multiple criteria problem. Management Science 22 (3), 652–663.
Zionts, S., Wallenius, J., 1980. Identifying eﬃcient vectors:
Some theory and computational results. Operations
Research 28 (3), 785–793.
Zitzler, E., 1999. Evolutionary algorithms for multiobjective
optimization: Methods and applications. PhD Thesis,
Swiss Federal Institute of Technology (ETH), Zurich,
Switzerland.

Journal of Scheduling 8: 75–96, 2005.
© 2005 Springer Science + Business Media, Inc. Printed in the Netherlands.

EVALUATING SOLUTION SETS OF A POSTERIORI
SOLUTION TECHNIQUES FOR BI-CRITERIA
COMBINATORIAL OPTIMIZATION PROBLEMS
JOHN W. FOWLER1 , BOSUN KIM1 , W. MATTHEW CARLYLE2 ,
ESMA SENTURK GEL1 , AND SHWU-MIN HORNG3
1 Industrial

Engineering, Arizona State University, Tempe AZ 85287-5906, U.S.A.
2 Naval Postgraduate School
3 National Taipei University of Technology

ABSTRACT
The quality of an approximate solution for combinatorial optimization problems with a single objective can
be evaluated relatively easily. However, this becomes more difﬁcult when there are multiple objectives. One
potential approach to solving multiple criteria combinatorial optimization problems when at least one of the
single objective problems is NP-complete, is to use an a posteriori method that approximates the efﬁcient
frontier. A common difﬁculty in this type of approach, however, is evaluating the quality of approximate
solutions, since sets of multiple solutions should be evaluated and compared. This necessitates the use of a
comparison measure that is robust and accurate. Furthermore, a robust measure plays an important role in
metaheuristic optimization for “tuning” various parameters for evolutionary algorithms, simulated annealing,
etc., which are frequently employed for multiple criteria combinatorial optimization problems. In this paper,
the performance of a new measure, which we call Integrated Convex Preference (ICP) is compared to that of
other measures appearing in the literature through numerical experiments—speciﬁcally, we use two a posteriori
solution techniques based on genetic algorithms for a bi-criteria parallel machine scheduling problem and
evaluate their performance (in terms of solution quality) using different measures. Experimental results
show that the ICP measure evaluates the solution quality of approximations robustly (i.e., similar to visual
comparison results) while other alternative measures can misjudge the solution quality. We note that the ICP
measure can be applied to other non-scheduling multiple objective combinatorial optimization problems, as
well.
KEY WORDS: multiple criteria combinatorial optimization, comparison measures, parallel machine scheduling

1. INTRODUCTION
Multiple criteria combinatorial optimization problems have received relatively little attention compared to single criterion combinatorial optimization problems in the literature. However, multiple
objective optimization has become more important in today’s competitive environment where continuous improvement along all fronts is essential to business success. Manufacturing operations
must constantly choose a good alternative considering conﬂicting criteria such as maximizing
throughput and minimizing cycle time, or minimizing WIP (work in process) inventory and maximizing on time delivery. As such, job scheduling is an area that requires optimization of conﬂicting
performance criteria for most manufacturers. A scheduling solution that minimizes makespan will
Correspondence to: John W. Fowler. E-mail: john.fowler@asu.edu

76

J. W. FOWLER ET AL.

not necessarily minimize the total weighted tardiness. Using methods to solve these types of problems expeditiously can be a key to maintaining a company’s proﬁtability or market share.
Many single criterion scheduling problems are NP-hard due to their inherent combinatorial
nature and complicated problem structures (Pinedo, 1995). When multiple criteria are considered,
these problems become even more difﬁcult to solve optimally. In fact, for conﬂicting objectives, it is
hard to even say what “optimal” means. As Fry, Armstrong, and Lewis (1989) pointed out, general
combinatorial optimization techniques such as branch and bound and dynamic programming
methods have limitations when applied to practical problems (e.g., number of jobs is more than 20
or 30). Thus, an approximate solution approach is a proper way to attack these real sized multiple
criteria scheduling problems.
Until the early 90’s, multiple criteria scheduling research was almost exclusively focused on single machine problems (Fry, Armstrong, and Lewis, 1989; De, Ghosh, and Wells, 1992; Lee and
Vairaktarakis, 1993; Nelson, Sarin, and Daniels, 1986). From the middle of the 90’s, research has
been extended to more complicated multiple criteria scheduling problems such as parallel machine scheduling problems with sequence dependent setups (Cochran, Horng, and Fowler, 2003;
Serifoglu and Ulusoy, 1999; Tuzikov, Makhaniok, and Manner, 1998) and ﬂow shop scheduling problems with sequence dependent setups (Marett and Wright, 1996; Murata, Ishibuchi, and
Tanaka, 1996). In these cases, each single objective problem is NP-hard (Pinedo, 1995). Hence the
multiple criteria problems are clearly NP-hard. Many approximate algorithms such as genetic algorithms, simulated annealing, tabu search, and ﬁltered beam search are introduced in the literature
(Cunha, Oliveira, and Covas, 1997; Fonseca and Fleming, 1993; Loughlin and Ranjithan, 1997;
Louis and Rawlins, 1993; Schaffer, 1985) to solve multiple criteria combinatorial optimization
problems, including scheduling problems. The majority of these algorithms take an a posteriori
approach. A posteriori approaches attempt to generate exact or approximate efﬁcient solutions.
The decision maker then selects the most preferred solution from the efﬁcient solutions. This kind
of solution approach is well suited to complicated scheduling problems. Hence research to develop
more effective and robust algorithms for multiple criteria scheduling problems is needed.
In this case, however, evaluating the solution quality of competing algorithms or algorithms with
different parameter sets, is not easy, since the “solution”, which is an approximation of the efﬁcient
frontier is a set of near Pareto-optimal solutions (also called non-dominated solutions, efﬁcient
solutions) in the objective space. When the set of all true non-dominated solutions (i.e., efﬁcient
frontier) can be generated by competing algorithms, their performance can be compared by the
computational effort needed to solve the same problem instance as is done in the single objective
case. However, computational effort is not sufﬁcient to compare competing algorithms when the
true set of Pareto-optimal solutions cannot be obtained in a reasonable amount of time. This is
the case for most multiple criteria scheduling problems. To compare the performance of heuristic
algorithms, one approach is to run competing algorithms for the same amount of computational
effort (CPU time or number of evaluations) and then compare the quality of solutions. Another
approach is to run each algorithm to its own stopping criteria and then compare both solution
quality and computational effort (Schaffer, 1985). In both cases, robust and efﬁcient methods to
compare the quality of sets of near Pareto-optimal solutions are required. We note that equating
the CPU time of different algorithms can be impacted by different coding methods and data
structures; thus this may not be a perfect comparison.
However, there seems to be no generally accepted measure(s) in the multiple criteria optimization
literature, as pointed out in Carlyle et al. (2003). This is primarily due to the difﬁculties in comparing
the various geometric features of sets of near Pareto-optimal solutions. These difﬁculties include

EVALUATING SOLUTION SETS OF A POSTERIORI SOLUTION TECHNIQUES

77

the fact that a non-identical number of non-dominated solutions are generated by heuristics and
the fact that heuristics generate non-identical tail points (extreme solutions for each objective).
Often, only visual comparison of alternative sets of Pareto-optimal solutions is employed in bicriteria optimization problems. However, visual comparison is not efﬁcient, since typically many
experiments are required to verify the effectiveness and robustness of heuristic algorithms. Hence,
a new measure called Integrated Convex Preference (ICP), was proposed in Carlyle et al. (2003) to
evaluate the quality of sets of Pareto-optimal solutions efﬁciently. Carlyle et al. (2003) provides a
more detailed description of the theoretical development and properties of ICP. In this paper, the
robustness and efﬁciency of the ICP measure for comparing a posteriori solution techniques are
examined through extensive experiments using a bi-criteria parallel machine scheduling problem.
These results are followed by a discussion on the properties of ICP based on the analysis of the
experimental results.
The ICP has some similarities with Data Envelopment Analysis (DEA). Both methods assume
weighted sum objective functions (outputs), do not determine a speciﬁc weight for each objective
(output) a priori, and consider all envelopment points as efﬁcient. Thus both methods require an
efﬁcient way to ﬁnd all envelopment points from a given set of non-dominated solutions (Decision
Making Units). However DEA and ICP have different purposes (i.e., DEA measures the relative
efﬁciency of decision making units (DMUs) in the presence of multiple inputs and outputs and
sets targets for inefﬁcient DMUs), and thus need different methods or algorithms. For example,
a method to ﬁnd the optimal weight region for each envelopment point in weight space and
integration over the optimal weight region are essential for ICP to evaluate the solution set quality
(the decision maker’s expected value for the set). However, these calculations are not needed in
DEA.
In the next section, the literature concerning solution techniques for multiple criteria scheduling
problems and the measures used to compare sets of Pareto-optimal solutions are reviewed. Then,
a summary of the ICP measure is presented, which is followed by the experimental scheme used to
compare alternative heuristics. The results are then discussed in the experimental results section
and ICP properties are discussed. Finally, conclusions and future research topics are provided.
2. LITERATURE REVIEW
In this section, two topics are reviewed. First, solution approaches for multiple objective optimization problems are reviewed, with particular emphasis on methods for multiple objective
scheduling problems. This is followed by the measures appearing in the literature to compare the
solution quality of sets of Pareto-optimal solutions.
2.1. Solution approaches for multiple objective optimization problems
The majority of solution approaches for multiple criteria scheduling problems that have appeared in the literature can be divided into two categories (Fry, Armstrong, and Lewis, 1989). The
ﬁrst category is ‘a priori’ solution approaches, which assume that the decision maker’s preference
information, such as a priority, weight, or goal (target) for each criterion, can be obtained before
the solution procedure starts. A good example of an objective priority method is found in Lee and
Vairaktarakis (1993), which dealt with bi-criteria single machine scheduling problems. The authors
assumed that the priority of each objective is given and that the second objective is optimized subject to the constraint that the ﬁrst criterion meets its minimum value. The paper provides the proof

78

J. W. FOWLER ET AL.

of NP completeness or provides polynomial time algorithms for almost all pairwise combinations
of performance criteria considered in the literature. Applications of an objective weighting method
can be found in Serifoglu and Ulusoy (1999) and Marett and Wright (1996). Serifoglu and Ulusoy
(1999) suggested a genetic algorithm to solve the problem of Qm |s j k | w E  E j + wT  Tj . They
consider two types of machine groups, identical and proportional. In Marett and Wright (1996),
compare a tabu search method to a simulated annealing method to solve the problem of F3 |prmu,
s j k | (1) Cmax , (2) total setup cost, (3) total holding time (job waiting), (4) total late time (machine
idle), for 30 jobs. They assume that the weight of each objective is given, and the objective function
is linear.
The second category is a posteriori solution approaches where the decision maker’s preference is
not considered in advance of the solution methods. The decision maker eventually selects the single
best solution. Nelson, Sarin, and Daniels (1986) suggested an optimal algorithm (non-polynomial
time) for a single machine with the following pairs of performance measures; (1) mean ﬂow time
and number of tardy jobs; (2) number of tardy jobs and maximal tardiness; and (3) mean ﬂow time
and maximal tardiness. De, Ghosh, and Wells (1992) suggested an approximate algorithm (ﬁltered
beam search) to generate a set of efﬁcient extreme solutions for a single machine problem with
mean and variance of completion times criteria. They assumed that the scalar objective function
is a convex combination of objectives and the weight of each objective is unknown. Due to the
convex combination of objectives assumption, only the efﬁcient extreme points in the objective
space are obtained. Tuzikov, Makhaniok, and Manner (1998) suggested an optimal polynomial
time algorithm to generate a set of non-dominated solutions for Qm | p j = p, r j | (1) ϕ j (c j (S)),
(2) max{ψ j (c j (S))}, where ϕ j (c j (S)) and ψ j (c j (S)) are regular (non decreasing) cost functions
for the completion time of job j . The problem is to schedule jobs with identical processing times
on a uniform processor.
Genetic algorithms are often applied to multiple criteria optimization problems to generate a set
of near Pareto-optimal solutions in a reasonable amount of computational effort. Schaffer (1985)
proposed the Vector Evaluated Genetic Algorithm (VEGA) method to ﬁnd a set of near Paretooptimal solutions for general multiple objective problems. In this method, a population is divided
into disjointed sub-populations and each sub-population is optimized with respect to one of the
objectives. This method, by the nature of its ‘disjointing approach (vector optimization)’, tends to
form the extreme solutions of the approximate efﬁcient frontier since its search is unidirectional.
The lack of a combined search in the Pareto-optimal solution set will naturally restrict the decisionmaker’s choices. Murata, Ishibuchi, and Tanaka (1996) proposed the Multi-Objective Genetic
Algorithm (MOGA). MOGA selects individuals for a crossover operation, based on a weighted
sum of linear objective functions with variable weights, which are not constant but are randomly
speciﬁed for each generation. With these variable weights, MOGA searches in various directions.
The method generally produces more diverse Pareto-optimal solutions, enabling the decision maker
a broader choice of solutions. They applied
MOGA

 to solve a multiple criteria ﬂow shop scheduling
problem, F10 |prmu| (1) Cmax , (2)
w j Tj , (3)
w j C j , for 20 jobs and compared the solution
quality of a set of near Pareto-optimal solutions generated by MOGA to that generated by VEGA.
They showed that MOGA generates a better approximate efﬁcient frontier than VEGA by visual
comparison.
Cochran, Horng, and Fowler (2003), proposed the hybridized Multi-Population Genetic Algorithm (MPGA) to solve multiple criteria
scheduling problems with sequence
 parallel machine

dependent setups, P5 |s j k , r j | (1) Cmax , (2) w j Tj , (3) w j C j , for 100 jobs. In their study, a genetic
algorithm is hybridized with dispatching rules. The GA is used to assign jobs to machines, and

EVALUATING SOLUTION SETS OF A POSTERIORI SOLUTION TECHNIQUES

79

dispatching rules such as setup avoidance and apparent tardiness cost with setups rules (Pinedo,
1995), are used to schedule the individual machines. The method consists of two stages. In the ﬁrst
stage, multiple objectives are combined as the multiplication of the relative objective functions.
In the second stage, the solutions of the ﬁrst stage are rearranged and divided into several subpopulations, which are the initial populations of the second stage. Each sub-population evolves
separately (similar to the VEGA approach). They also sought to ﬁnd the best time to change
between the two stages (called the turning point). MPGA outperformed MOGA when comparing
them by the number of Pareto-optimal solutions and the number of combined Pareto-optimal
solution measures, which will be deﬁned in the next section (Cochran, Horng, and Fowler, 2003).
2.2. Comparison methods used in the literature
The methods to compare the solution quality of approximate algorithms used in the literature belong to four main groups. The ﬁrst group involves the visual comparison of the sets of
non-dominated solutions using graphical displays of the solution points in the objective space
(Murata, Ishibuchi, and Tanaka, 1996; Cieniawski, Eheart, and Ranjithan, 1995). Although this
method can compare solution sets of various shapes effectively, it can only be used for bi-criteria
optimization problems and it is especially inefﬁcient when a large number of numerical experiments
(or replications) need to be performed. Also, such a method clearly cannot be embedded into an
algorithm that automatically selects the best heuristic from among many alternative procedures
or the best set of heuristic parameters from among a set of possible choices.
The second group of measures focuses on geometric features of the solution sets plotted in the
objective space. Measures in this group include (1) length and area measures (De, Ghosh, and
Wells, 1992) and (2) distance measures (Czyzak and Jaszkiewicz, 1998; Viana and Sousa, 2000).
The length and area measures proposed by De, Ghosh, and Wells (1992) can only handle problems
with two objectives and are not applicable when the solution set contains only one or two points.
Although the distance measures evaluate the approximate solution set on the characteristics of
diversity (coverage), uniformity and closeness deﬁned in Carlyle et al. (2003) to the true set of
Pareto-optimal solutions, they are not applicable when the true set of solutions is not available,
which is often the case in scheduling problems.
The third group of comparison techniques uses the cardinality of the set of Pareto-optimal
solutions. The measures in this group are (1) the number of Pareto-optimal solutions and (2)
the number of combined Pareto-optimal solutions (Cochran, Horng, and Fowler, 2003; Schaffer,
1985; Hyun, Kim, and Kim, 1998). To obtain the number of combined Pareto-optimal solutions,
all non-dominated solutions are compared together with respect to the Pareto-optimal criterion. If
any solution is dominated, then it is discarded. After that the number of non-dominated solutions
found by each algorithm is counted. While the number of generated solutions is important, it
certainly cannot determine the solution quality quantitatively.
The fourth group uses the value (utility) function of the decision maker to obtain a scalar
value of a set of Pareto-optimal solutions. Daniels (1992) suggested two measures, maximum
and average approximation error (ε) of the discrete approximation from true efﬁcient solutions
under the assumption of a linear weighted sum utility (value) function. These measures and the
ICP measure suggested in Carlyle et al. (2003) are similar in the sense that both methods utilize an
assumption on the decision maker’s value (unknown) function in evaluating the solution quality of
multiple objective heuristics. However, Daniels’ methods evaluate the solution quality of heuristics
based on the known discrete true efﬁcient frontier. Hence, they are similar to geometric measures

80

J. W. FOWLER ET AL.

in the sense that they are not applicable when the true set of solutions is not available, whereas
ICP is designed for such cases. Hansen and Jaszkiewicz (1998) suggested three types of measures –
probability difference, expected value difference, and relative ratio. The probability difference (R1)
measure is obtained from the cumulative probability that Set-1 gives a better solution than Set-2,
and vice versa. The expected value difference (R2) measure is similar to the difference between
ICP values. The relative ratio (R3) measure is the same as the average approximation error (ε)
in Daniels (1992). They assumed that p(u) is the probability that utility function, u, is held by
the decision maker. The authors also note that these three methods can be applied whether true
efﬁcient solutions are known or not. However they did not provide exact calculation methods
for the three measures due to the difﬁculty of the high dimensional integration of smooth or
non-smooth functions. Instead, the approximate method of sampling a set of utility functions
according to its distribution is suggested in Hansen and Jaszkiewicz (1998). In comparison, ICP is
an exact method to obtain a scalar value of a set of near Pareto-optimal solutions when a convex
combination of objective functions is assumed.
3. INTEGRATED CONVEX PREFERENCE (ICP)
The basic concept of ICP begins with the fact that the most preferred single solution among
feasible solutions would be eventually selected by the decision maker(s), regardless of the employed
solution approach: a priori, interactive, or a posteriori. To select the most preferred solution among
the feasible solutions, the decision maker applies his/her value function. Hence, we seek to use
this value function approach in comparing the quality of sets of near Pareto-optimal solutions.
ICP is, in short, the expected value or utility of a set of Pareto-optimal solutions for an assumed
value function. The exact extraction of the value function of the decision maker is difﬁcult and
this is a research area itself. Hence, the most frequently used value function, a convex combination
(weighted sum) of objective functions, is assumed in this paper to obtain ICP for a set of Paretooptimal solutions although other types of value functions can be incorporated.
3.1. Integrated convex preference (ICP)
Consider a bi-criteria minimization problem. When the decision maker’s preference can be
represented as a convex combination of linear objective functions with varying weights, then an
optimal solution x∗ is selected by min j ∈J {w f1 j + (1 − w) f2 j }, where J is a set of non-dominated
solutions, fi j is the i th objective value of the j th non-dominated solution, and w is in the interval
(0, 1). In Figure 1, for example, ﬁve efﬁcient extreme points (lower-envelope points) are candidates
for an optimal solution among the nine Pareto-optimal points. If the decision maker’s preference
(i.e., weight) for objective 1, w, equals 0, then the non-dominated point p1 is the optimal solution
since the decision maker only considers objective 2. If w equals 0.5 (both objectives are equally
important), then p2 will be the optimal solution. If w equals 1.0 then p3 is the optimal solution.
As shown in Figure 1, the weight interval for which an efﬁcient extreme point p2 is optimal can be
obtained by a polar cone. This polar cone can be generated by the orthogonal vectors of the faces,
which contain p2 . Then, ICP is obtained by Eq. (1).

ICP =
0

1

min{w f1 j + (1 − w) f2 j } dw.
j ∈J

(1)

81

EVALUATING SOLUTION SETS OF A POSTERIORI SOLUTION TECHNIQUES

Figure 1. Optimal weight range for each efﬁcient extreme solution in the objective space

The ﬁrst step to obtain ICP is to ﬁnd all efﬁcient extreme points from a given set of Paretooptimal solutions. This is the well-known convex hull problem (i.e., ﬁnd all extreme points among
a set of given points). A pseudo code for a convex hull algorithm in bi-criteria problems is provided
below.

r Sort the ( f1 , f2 ) points in increasing order of f2 value;
r
r
r
r
r
r

/* Finding an adjacent extreme point sequentially */
Let starting point ( f1 0 , f2 0 ) be the ﬁrst point in the sort list;
Do until the last point in the sort list is selected as an adjacent point.
Calculate the slope between the starting point ( f1 0 , f2 0 ) and all the remaining points;
Assign an adjacent point, which has the minimum slope to be the adjacent extreme point;
Delete the non-extreme points between the starting point and the adjacent extreme point
Assign adjacent extreme point to be the new start point ( f1 0 , f2 0 );

The second step is to calculate the weight intervals within which each extreme point is an optimal
solution for a convex combination of the objective functions. There are several methods to obtain
these optimal weight intervals (details can be found in Carlyle et al. (2003)). One of the efﬁcient
ways for the bi-criteria case is provided here, which can also be applied to more than two objective
cases. Assume n efﬁcient extreme solutions f•i , i = 1, 2, . . . , n are obtained from Step 1. Then a
system of linear inequalities can be generated for each efﬁcient extreme solution as Eq. (2).
{wf 1i + (1 − w) f2i } − {wf 1k + (1 − w) f2k } ≤ 0, i = k, k ∈ K,

(2)

where K is the set of adjacent efﬁcient extreme points of f•i .
The system of inequalities above is derived from the fact that if a solution f•i is an optimal
solution for a convex combination of objectives, then there should be a weight w for which w f1i +
(1 − w) f2i is less than or equal to that of all other solutions. In a bi-criteria optimization problem,
every efﬁcient extreme point has two adjacent extreme points except for the two tail points. Thus,
two linear inequalities can be generated from these two adjacent extreme points. The two linear
inequalities give the lower and upper bounds on the optimal weight interval. The two tail points

82

J. W. FOWLER ET AL.

have one adjacent extreme point, which gives a bound on the optimal weight interval. And the
other bound is 0 or 1 since w is assumed to belong to (0, 1). Then the optimal solution f ∗ in the
objective space can be decomposed as a function of w as in Eq. (3).


(w f11 + (1 − w) f21 ), 0 ≤ w ≤ w1
 (w f + (1 − w) f ), w ≤ w ≤ w 
12
22
1
2 

f ∗ (w) = 
(3)
,
······

(w f1n + (1 − w) f2n ),

wn−1 ≤ w ≤ 1

where wi means the breakpoint for the range of w for which extreme solution i is optimal.
Finally, ICP for a given set of Pareto-optimal solutions can be obtained by Eq. (4).
 1
ICP =
f ∗ (w) dw
0


=

w1


(w f11 + (1 − w) f21 ) dw +

0

+··· +



1

wn−1

w2

w1

(w f12 + (1 − w) f22 ) dw

(w f1n + (1 − w) f2n ) dw

(4)

3.2. Integrated convex preference with triangular weight function (ICP T)
The decision maker(s) typically does not know his or her weight value for each objective precisely, but is able to specify some relations between weights. In comparing sets of Pareto-optimal
soluitions, one practical assumption is that the decision maker wants to give more weight to solutions that are well compromised (good for both criteria: elbow solutions) than to solutions that
are only good for one objective (tail solutions). This consideration can be modeled as a weight
density function such as a triangular weight function. The ICP described earlier can be considered
to have a uniform weight function. As shown in Figure 2, a set of Pareto-optimal solutions which
has better solutions in the elbow area and worse solutions in the tail areas, may be preferred by a

Figure 2. Comparison of two sets of Pareto-optimal solutions by ICP with a uniform weight function and a triangular
weight function

EVALUATING SOLUTION SETS OF A POSTERIORI SOLUTION TECHNIQUES

83

decision maker over a set of Pareto-optimal solutions which has worse solutions in the elbow area
and better solutions in both tail areas. In Figure 2, the circle shaped solutions have a lower ICP
(i.e. are preferred) when compared to ICP with a uniform weight function. On the other hand, the
rectangular shaped set of Pareto-optimal solutions has a lower ICP when compared to ICP with
a triangular weight function. A detailed calculation procedure of ICP T can be found in Carlyle
et al. (2003).
3.3. Integrated convex preference with scaled objective values (ICP * S)
As stated before, the ICP uses a blended value function (unknown) to represent the preference
of the decision maker. When objectives are incommensurable like number of tardy jobs and total
completion time, it is hard to interpret a blended objective value. Also, when the difference between
the ranges of each objective value is so large that one objective value overwhelms the other objective
value, proper scaling is clearly needed. In our study, the range of total weighted tardiness is much
larger than that of makespan. As Schenkerman (1990) suggested, minimum and maximum values
in sets of Pareto-optimal solutions are used in scaling each objective value as shown in (5). The
same scaling method is employed in De, Ghosh, and Wells (1992) to compare sets of Pareto-optimal
solutions with area and length measures:

 	


	
( f2i − mini ∈I ( f2i ))
{ f1i − mini ∈I ( f1i )}
(g1i , g2i ) =
,
,
(5)
{maxi ∈I ( f1i ) − mini ∈I ( f1i )}
(maxi ∈I ( f2i ) − mini ∈I ( f2i ))
where ( f1i , f2i ) are the non-scaled objective values of non-dominated solution i , (g1i , g2i ) are the
scaled objective values of non-dominated solution i , and mini ∈I ( f ji ) (maxi ∈I ( f ji )) = minimum
(maximum) f•i among all f•i ’s in competitive sets of Pareto-optimal solutions
4. EXPERIMENTAL SCHEME
To test the ICP measure, a multiple criteria parallel machine scheduling problem studied by
Cochran, Horng, and Fowler (2003) is used. Extensive experimental results for the solution quality
of two competing a posteriori solution techniques are reported. A summary of the experimental
scheme is presented for the convenience of readers, even though details about the experiments can
be found in Cochran, Horng, and Fowler (2003).
4.1. Test problem description
A parallel machine scheduling problem with sequence dependent setups is considered. A setup
is required if the next job on the same machine has a different family. 100 jobs with 4 different
families are scheduled on ﬁve identical machines. Two objectives are optimized simultaneously.
The ﬁrst objective is the makespan, deﬁned as max {C1 , C2 , . . . , Cn }, where C j is the completion
time
n of job j . The second objective is the total weighted tardiness (TWT), deﬁned as TWT =
j − d j } and d j is the due date of job j . Thus, the problem can
j =1 w j Tj , where Tj = max {0, C
w j Tj , with 100 jobs. As shown in Table 1, four factors are
be represented as P5 |s j k , r j |Cmax ,
used to generate the 100 jobs. A total of 36 problem instance sets can be generated using the four
factors. Ten problem instances are generated randomly in each set, resulting in 360 test problem
instances. All problem instances are solved 10 times due to the inherent randomness of the genetic

84

J. W. FOWLER ET AL.

Table 1. Four factors and levels to generate 36 (22 × 32 ) problem instance sets
Factors
Range of weights

Range of due dates

Ratio ( p̄/s̄)

WIP status

Levels

Description

1 (Narrow)

U (1, 10)

2 (Wide)

U (1, 20)

1 (Narrow)

Ready time + U (−1, 2) × total process time.

2 (Wide)

Ready time + U (−2, 4) × total process time.

1 (High)

50/10, p = 50 + U(−9, 9), s = U(6, 14).

2 (Moderate)

30/30, p = 30 + U(−9, 9), s = U(18, 42).

3 (Low)

10/50, p = 10 + U(−9, 9), s = U(30, 70).

1 (High)

All jobs are ready at time 0

2 (Moderate)

50% of jobs are ready at time 0
and the others are ready at time U(0, 720)

3 (Low)

All jobs are ready at time U(0, 720)

Notes : U (a, b): Discrete random number generated from uniform distribution.
p̄: Average process time.
s̄: Average setup times.
Average setup times of job j : (1/4) × (si k ), where si k is the setup time from a job of family i to a job
of family k, i is the family of job j , and k = 1, 2, 3, 4.
Due dates are determined after ready times, processing times and setup times have been generated.

algorithms. Two genetic algorithms, MOGA (Murata, Ishibuchi, and Tanaka, 1996) and MPGA
(Cochran, Horng, and Fowler, 2003), described in the literature review section are tested for all
problem instances.
4.2. Parameter settings for each algorithm
In Cochran, Horng, and Fowler (2003), preliminary experiments were performed to ﬁnd the best
parameter settings for both genetic algorithms, since the performance of each genetic algorithm
is dependent on the parameter settings used. We use the same parameter settings as in Cochran,
Horng, and Fowler (2003), listed as follows:
r Crossover probability: 0.6
r Mutation probability: 0.01
r Population size: 20
r Elitism: three elite solutions are selected from the tentative set of non-dominated solutions
r Stopping criteria: 5000 generations
For MPGA, the turning criterion is set at the 2000th generation. After the turning criterion
has been reached, the population is divided into three sub-populations, one for each of the two
objectives and one for the combined objective function.
4.3. Measures to compare MPGA with MOGA
To provide evidence on which measure gives reasonable and robust comparison results for
sets of near Pareto-optimal solutions, the measures in the literature and the different types of

EVALUATING SOLUTION SETS OF A POSTERIORI SOLUTION TECHNIQUES

85

ICP measures need to be tested. However, the geometrical comparison methods outlined in the
literature review section (De, Ghosh, and Wells, 1992; Czyzak and Jaszkiewicz, 1998; Viana and
Sousa, 2000; Daniels, 1992) are not applicable because there exist no efﬁcient algorithms to generate
the true set of Pareto-optimal solutions for the scheduling problem considered. Thus, the methods
we experiment with are restricted to the following four: (1) visual comparison, (2) the number
of Pareto-optimal solutions (# of POS), (3) number of combined Pareto-optimal solutions (# of
CPOS), and (4) ICP. In ICP measures, four types of ICP measures—uniform weight function with
scaling (ICP U S), triangular weight function with scaling (ICP T S), uniform weight function
without scaling (ICP U), and triangular weight function without scaling (ICP T) are considered.
There are three types of comparison methods using the ICP. The ﬁrst one is to compare the ICP
values directly. This method is useful when several sets of Pareto-optimal solutions (several heuristics) need to be compared simultaneously. A set of solutions with the minimum ICP value can then
be considered as the best set of solutions among the alternatives. Also, when parameter optimization (tuning) is performed through an experimental design and response surface optimization, ICP
values can be used as a response value. The second method is the ICP difference between two sets
of Pareto-optimal solutions (two heuristics). When pairwise comparison is needed, ICP difference
can be used to determine which set of Pareto-optimal solutions has better solution quality, by the
sign of the difference between ICP values. The magnitude of ICP difference represents the quantitative difference between two solution sets. A third method of comparison is the ratio of ICP
ICP(A)
values (e.g., ICP(A)−ICP(R)
or ICP(R)
, where R is a reference set and A is an approximate set). When
ICP(R)
a reference set of solutions such as a set of true Pareto-optimal solutions is known, the ratio of
ICP can provide useful information about the solution quality of heuristics based on the solution
quality of the reference set. In this paper, two heuristics are compared and no reference sets are
available, hence, ICP difference is used.

5. DISCUSSION OF EXPERIMENTAL RESULTS
Table 2 contains the experimental results of four types of ICP measures and two types of cardinality measures for 36 problem instance sets. The ‘Problem instance set’ column represents the
combination of levels of four factors. For example, ‘1111’ means that level ‘1’ is used to generate
100 jobs for all four factors in Table 1. Values in the four ‘ICP *’ columns represent the number
of wins of MPGA (over MOGA) out of 100 comparisons (10 randomly generated problem instances * 10 replicates) in using the corresponding ICP measure. In the ‘# Pareto-optimal’ column,
the average number of Pareto-optimal solutions generated by MPGA and MOGA are shown respectively. In the ‘# Combined Pareto’ column, the average combined number of Pareto-optimal
solutions generated by MPGA and MOGA are shown. In the ‘Total’ row, values in the four ‘ICP’
columns indicate the number of wins of MPGA out of 3,600 comparisons and values in the last
four columns indicate the sum of the average number of Pareto-optimal and combined solutions
for MPGA and MOGA, respectively. Finally, in the ‘Ratio’ row, values are the ratio of the number
of wins of MPGA out of the total number of comparisons.
The ﬁrst thing to notice in Table 2 is that all ratio values are greater than or equal to 0.5. This
means that MPGA outperforms MOGA in overall performance. This result is consistent with
the results in Cochran, Horng, and Fowler (2003). However, comparison results (total number of
wins) are much different depending on the measure used. When the number of Pareto-optimal
solutions is used, MPGA wins in all 36 problem instance sets. When ICP U, ICP T, and combined

86

J. W. FOWLER ET AL.

Table 2. Number of wins (0.5 for a tie) of MPGA out of 100 comparisons in each 36 problem instance set
# Pareto-optimal

Problem
instance set

ICP U

ICP U S

1111

62

50

1112

67

1113

81

1121

ICP T

# Combined Pareto

ICP T S

MPGA

MOGA

MPGA

MOGA

62

47

11.9

9.4

6.6

5.2

61

67

57

12.0

9.5

8.2

4.2

77

81

75

10.7

8.4

7.9

2.5

55

47

55

43

11.7

8.8

6.0

5.4

1122

65

56.5

65

56.5

5.7

4.6

3.5

1.9

1123

66

62

66

60

7.0

5.4

4.4

2.3

1131

52

46

52

42

12.6

8.6

5.8

5.6

1132

44

42

44

41

1.8

1.2

0.6

0.7

1133

58

44

58

43

2.9

1.9

1.2

1.1

1211

59

53

60

53

12.0

9.7

7.3

4.7

1212

70

62

70

62

14.1

9.5

9.5

4.0

1213

84

69

84

66

10.8

8.9

7.9

3.0

1221

57

45

57

39

11.4

9.2

6.0

5.4

1222

73

60

73

57

6.7

4.8

3.9

2.0

1223

71

55

71

53

7.9

5.3

5.0

2.5

1231

52

49

52

45

12.5

9.5

5.7

6.1

1232

54

49

54

47

2.1

1.3

0.8

0.7

1233

48

38

50

35

4.3

1.9

1.5

1.3

2111

58

53

58

52

12.2

8.9

6.6

4.8

2112

76

71

76

71

13.3

9.4

9.7

3.7

2113

72

62

72

60

11.5

8.8

7.1

3.7

2121

64

54

64

52

11.9

8.7

6.7

4.5

2122

74

62.5

74

61.5

5.7

4.8

3.7

1.4

2123

63

55

63

52

8.1

5.4

4.5

2.7

2131

55

41

55

39

12.6

9.3

6.0

6.0

2132

51

47

51

47

1.6

1.3

0.7

0.7

2133

62

44

63

43

3.3

2.0

1.3

1.1

2211

53

51

54

46

12.4

9.3

6.9

4.9

2212

82

73

82

69

13.8

9.4

10.7

3.2

2213

80

74

80

73

11.9

9.2

8.9

3.0

2221

59

49

59

43

11.7

8.6

6.5

4.7

2222

73

65

73

64

7.5

5.2

5.0

1.8

2223

60

49

60

48

8.5

5.6

4.7

2.9

2231

45

45

45

45

12.9

9.6

6.1

6.0

2232

45

41

46

39

1.9

1.3

0.7

0.8

2233

59.5

39.5

59.5

39.5

4.2

2.0

1.8

1.0

Total

2249.5

1941.5

2255.5

1865.5

323.1

236.7

189.4

Ratio

0.62

0.54

0.63

0.52

1.00

115.5
0.89

EVALUATING SOLUTION SETS OF A POSTERIORI SOLUTION TECHNIQUES

87

Pareto-optimal solutions are used, MPGA wins 32 times out of 36 problem instance sets. When
ICP U S is used, however, MPGA only wins 19 times out of 36 comparisons. MPGA and MOGA
are tied (each wins 18 times) when ICP T S is used.
To verify the effectiveness of the six measures used in evaluating the quality of sets of near Paretooptimal solutions with various shapes, a detailed investigation for randomly selected problem
instances is performed. Problem instance set 2212 is randomly selected among the 36 sets. Two
problem instances, the best one for MPGA and MOGA, are selected in that problem instance set
to analyze the performance of the measures considered.
At ﬁrst, visual comparison is performed as a baseline due to the lack of a standard measure(s).
Then, the performances of all other numerical measures are compared to the visual comparison
results. To avoid the subjectivity involved in visual comparison, the set Pareto dominance relation
(see Deﬁnition 1 in the next section) is used. If solutions in Set-1 dominate all of the solutions in Set2 visually (clear cases), then Set-1 is judged to be a winner, and vice versa. If Set-1 and Set-2 cross
each other (not-clear cases), then visual comparison will not decide the winner. Visual comparisons
of sets of near Pareto-optimal solutions generated by MPGA and MOGA for problem instance
set 2212 are shown in Figures 3 and 4. Visual comparison results by graph, four ICP measures,
and two cardinality measures for Figures 3 and 4 are provided in Tables 3 and 4, respectively.
In Tables 3 and 4, the visual comparison columns document the visual comparison results. In
‘clear cases’, the name of the algorithm that is decided as a winner is represented and a ‘−’ represents
the ‘not-clear’ cases. Numbers in the four ‘ICP *’ columns represent the ICP difference between
two sets of Pareto-optimal solutions by MOGA and MPGA. A positive value means that MPGA
is evaluated as a winner, and vice versa for a negative value. In the cardinality number measure

Figure 3. Visual comparison of MPGA with MOGA—best case for MPGA in problem instance set 2212 (‘o’ represents
solutions of MPGA and ‘−’ represents solutions of MOGA. X-axis is makespan from 1,000 to 1,300 time units and each
grid line is 100 time units. Y-axis is total weighted tardiness from 200,000 to 350,000 time units and each grid line is 30,000
time units)

88

J. W. FOWLER ET AL.

Figure 4. Visual comparison of MPGA with MOGA—best case for MOGA in problem instance set 2212. (‘o’ represents
solutions of MPGA and ‘−’ represents solutions of MOGA. X-axis is makespan. Y-axis is total weighted tardiness)

columns, the number of Pareto-optimal solutions and the combined number of Pareto-optimal
solutions are represented.
By visual comparison of the 10 graphs in Figure 3, we can see that MPGA generates better
Pareto-optimal solutions, except in Replication 9. MPGA and MOGA generate similar solutions
for the makespan objective, but MPGA outperforms MOGA for the total weighted tardiness
objective. In Figure 4, MPGA wins four times in visual comparison. In both ﬁgures, the objective
value range difference between makespan and total weighted tardiness is signiﬁcant. For example,
the makespan objective values range from 1,000 to 1,300, and the total weighted tardiness objective
values range from 200,000 to 350,000 in Figure 3. Hence, the makespan objective is overwhelmed
by total weighted tardiness in non-scaled ICP measures (ICP U and ICP T). In this case, the scaled
ICP can provide more reasonable comparison results.
An analysis of Tables 3 and 4 shows that the number of Pareto-optimal solutions cannot be
used alone since it does not reﬂect the overall solution quality of the set. Consider, for example,
Replicate-6 in Table 4. MPGA would be preferred over MOGA if the number of Pareto-optimal
solutions is used (21 vs. 13). However, from a visual comparison (see Replicate-6 in Figure 4)
MOGA appears to be better than MPGA in this case, even though both Pareto-fronts cross each
other.
The number of combined Pareto-optimal solutions provided the same results that visual comparison did in all the clear cases, which shows that this measure works pretty well when the difference
in the solution qualities of competitive algorithms is fairly large. However, this measure does not
consider the location of Pareto-optimal solutions in the not-clear cases. For example, in Replicate-2
in Figure 4, MPGA has better solutions in both tail areas and MOGA has better solutions in the
elbow areas. MPGA would be preferred over MOGA if this measure was used (14 vs. 8), which
shows that although this measure is more effective than the number of Pareto-optimal solutions,
it still has shortcomings.

89

EVALUATING SOLUTION SETS OF A POSTERIORI SOLUTION TECHNIQUES

Table 3. ICP difference between MPGA and MOGA in best case for MPGA in problem instance set 2212
Difference between
MOGA and MPGA
Replicate

ICP U S

ICP T

ICP T S

# Pareto-optimal

# Combined Pareto

MPGA

MOGA

MPGA

MOGA

Visual comparison

ICP U

1

MPGA

12,293

0.065

24,660

0.059

22

9

22

1

2

MPGA

9,091

0.086

18,211

0.093

14

10

14

0

3

MPGA

14,317

0.067

28,626

0.058

14

9

12

2

4

MPGA

16,479

0.120

32,949

0.122

12

9

8

1

5

MPGA

8,278

0.031

16,534

0.017

18

10

18

1

6

MPGA

17,477

0.136

34,974

0.143

12

8

12

0

7

MPGA

9,878

0.063

19,812

0.064

22

10

22

2

8

MPGA

10,337

0.026

20,691

0.015

19

7

18

2

9

–

5,756

−0.001

11,579

−0.013

17

6

9

5

10

MPGA

3,709

0.039

7,434

0.037

15

13

14

1

Note : Value = ICP(MOGA)—ICP(MPGA).
‘−‘ in visual comparison column means that it is difﬁcult to judge whether MPGA is winner or not by visual
comparison. This occurs when competing sets of Pareto-optimal solutions cross each other.
Table 4. ICP difference between MPGA and MOGA in best case for MOGA in problem instance set 2212
Difference between
MOGA and MPGA
Replicate Visual comparison

ICP U

ICP U S

ICP T

# Pareto-optimal # Combined Pareto
ICP T S MPGA MOGA MPGA

MOGA

1

MPGA

11,385

0.031

22,895

0.015

20

7

19

2

2

–

4,231

0.005

8,573

−0.004

16

10

14

8

3

MPGA

4

–

5
6
7

MPGA

14,416

0.097

28,902

8

–

−5,339

−0.042

−10,632

9

MPGA

10

–

1,188

0.009

2,411

0.007

13

7

12

1

−20,110

−0.145

−40,242

−0.143

10

12

4

11

–

2,827

−0.009

5,674

−0.016

14

10

6

9

–

−6,946

−0.068

−13,822

−0.070

21

13

7

10

0.091

13

10

13

0

−0.040

16

8

8

8

5,784

0.071

11,588

0.074

10

9

10

0

−4,838

−0.026

−9,784

−0.021

9

8

2

6

Note : Value = ICP(MOGA)—ICP(MPGA).

In all the clear cases, the four ICP difference measures provided the same results that a visual
comparison did (all four measures have the same sign). If all four measures have the same sign, one
set of solutions has better solutions in the elbow area and in both tail areas, or one set of solutions
has better solutions in the elbow area and signiﬁcantly better solution in one tail area. And the
ICP difference is larger, relatively, than that of the not-clear cases (where all four ICP differences
do not have the same signs). Consider, for example, Replicate-3 in Table 4. The ICP U difference is

90

J. W. FOWLER ET AL.

1,188, the least difference among 10 replicates, but all 4 signs of ICP difference are positive. Hence
it can be interpreted that MPGA generates a better set of Pareto-optimal solutions for both the
tail and elbow areas, even though the difference is very small.
However, in not so clear cases, the four ICP measures provide different comparison results
(provide different signs) for the same two sets of Pareto-optimal solutions. Consider, for example,
Replicate-9 in Figure 3. Even though the two sets of solutions are very close and cross each
other, MGPA has better solutions for the total weighted tardiness objective and MOGA has
better solutions in the elbow area. Both sets of solutions have similar solutions for the makespan
objective. As can be seen in Table 3, the comparison result by ICP U (ICP T) is that MPGA
generates better solutions than MOGA. On the other hand, the comparison result by ICP U S
(ICP T S) is reversed. This is due to the objective value range difference between the two objectives
as indicated earlier. Hence, positive ICP U and negative ICP U S can be interpreted as MPGA has
better solutions for the TWT objective and MOGA has better solutions for the makespan objective
or in the elbow area. And negative ICP T S implies that MPGA has worse solutions in the elbow
area. Thus, it appears that MOGA has better solutions in the elbow area (at least). For Replicate10 in Table 4, all four ICP differences are negative, even though this is one of the not-clear cases
due to the crossing of sets of Pareto-optimal solutions. This implies that MOGA generates better
Pareto-optimal solutions for the TWT objective and for the elbow area. For Replicate-2 in Table 4,
ICP U, ICP U S, and ICP T are positive, but ICP T S is negative. It can thus be interpreted that
MPGA has better solutions in almost all the weight ranges, but has worse solutions in the elbow
area. And we can see that ICP T is not sensitive enough to give some useful information in our
experiments. The signs of all 20 ICP T differences are the same as the signs of the ICP U differences
as shown in Tables 3 and 4. This is due to the large difference in objective value range between the
two objectives.
The magnitude of ICP difference also provides useful information to interpret the difference
of solution qualities of competing sets of solutions. For example, the maximum ICP U difference
between MPGA and MOGA is 17,477 in Table 3, which implies the maximum difference of the
solution quality of MPGA and MOGA occurs in Replicate-6 (also the maximum difference of
ICP U S is 0.136 in Replicate-6). The minimum ICP U difference is 3,709 in Replicate-10 among
clear cases. And the minimum ICP U S difference is - 0.001 in Replicate-9. This implies that the
difference of the solution quality of MPGA and MOGA is very small in both cases.
To summarize, the ICP measure gave comparison results that were the closest to the method of
visual comparison, which is taken as the baseline comparison method in this study. For replications
with a clear winner, ICP yielded the same preferences as visual comparison and for the not-clear
replications the ICP values for the two sets were close. These interpretations are consistent with
visual comparison.
Based on the analyses above, we can interpret the entire experimental results in Table 2 again.
MPGA outperformed (i.e., had better values for all four ICP measures) MOGA 18 times out of
the 36 problem instance sets. On the other hand, MOGA outperformed MPGA 4 times out of the
36 problem instance sets (1132, 1233, 2231, and 2232). In the remaining 14 problem instance sets
(e.g., 1121), neither algorithm generates a dominant set of solutions.
The four MOGA wins occur when the process time/setup time ratio is ‘3’ (low). This implies that
the MPGA and MOGA performance depend on the factor levels. To perform the factor analysis,
Table 5 is derived from Table 2. As shown in Table 5, MPGA wins across all weight range and due
date range factor levels. However, when the level of the process-setup ratio is ‘3’ and WIP ratio
factor is ‘1’, it cannot be said that MPGA outperforms MOGA.

91

EVALUATING SOLUTION SETS OF A POSTERIORI SOLUTION TECHNIQUES

Table 5. Number of wins of MPGA by levels of four factors
Factor
Weight range
Due date range
Process- setup ratio

WIP ratio

Level

ICP U

Win

ICP U S

Win

ICP T

Win

ICP T S

Win

1

1118

1

965.5

1

1121

1

921.5

1

2

1131.5

1

976

1

1134.5

1

944

1

1

1125

1

975

1

1126

1

942

1

2

1124.5

1

966.5

1

1129.5

1

923.5

1

1

844

1

756

1

846

1

731

1

2

780

1

660

1

780

1

629

1

3

625.5

1

525.5

0

629.5

1

505.5

0

1

671

1

583

0

673

1

546

0

2

774

1

690

1

775

1

672

1

3

804.5

1

668.5

1

807.5

1

647.5

1

Note : In ‘win’ columns, ‘1’ means MPGA wins over 900 times out of 1800 comparisons in the weight range
factor and the due date range factor rows and MPGA wins over 600 times out of 1200 comparisons in the
process-setup time ratio factor and the WIP status factor rows.

For further analysis of the relation between the performance of MPGA and process-setup and
WIP ratio factors, Table 6 is derived from Table 2. As shown in Table 6, when the level of processsetup ratio is ‘3’(low), or when the level of WIP ratio is ‘1’, it is difﬁcult to say which one generates a
better Pareto front. MOGA outperformed MPGA when the level of process-setup ratio is ‘3’(low)
and the level of WIP ratio is ‘2’(moderate). In all other cases, MPGA outperforms MOGA.
Because of the randomness of the genetic algorithms, it is not easy to understand the exact
reasons why the performance of the algorithms varies for different problem instances. However,
one of the reasons is due to the algorithmic characteristics of MPGA. In MPGA, the population is
divided after the speciﬁed turning criteria. Then each subpopulation evolves for the improvement
of the objective assigned to it. This is one of the reasons that MPGA outperformed MOGA in

Table 6. Number of wins of MPGA by levels for sensitive factors
Process setup
time ratio

WIP
ratio

ICP U

Win

ICP U S

Win

ICP T

Win

ICP T S

Win

1

1

232

1

207

1

234

1

198

0

1

2

295

1

267

1

295

1

259

1

1

3

317

1

282

1

317

1

274

1

2

1

235

1

195

0

235

1

177

0

2

2

285

1

244

1

285

1

239

1

2

3

260

1

221

1

260

1

213

1

3

1

204

1

181

0

204

1

171

0

3

2

194

0

179

0

195

0

174

0

3

3

227.5

1

165.5

0

230.5

1

160.5

0

Note : In ‘win’ columns, ‘1’ means MPGA wins over 200 times out of 400 comparisons.

92

J. W. FOWLER ET AL.

most cases. However, once an objective reaches the optimal or near optimal solution before the
termination of the algorithm, then the subpopulation assigned to that objective has little chance
to improve the solutions.
As shown in Table 1, when the level of process time and setup time ratio is 3(low), the process
times of 100 jobs are randomly generated from U(1, 19) and the setup times are from U(30, 70). In
this case, the makespan objective is much more dependent on the setup times than process times.
As stated before, there are four families and ﬁve identical machines. Thus a near optimal schedule
for the makespan objective can be obtained easily by assigning the jobs with the same family
to the same machine. Once a job sequence that satisﬁes this approximately is determined by the
genetic algorithm (crossover or mutation operation), then not much room remains for improving
the solutions. It is more likely that the makespan objective reaches a (near) optimal solution within
a relatively few generations in level 3 than level 1 or level 2 of the process time and setup time ratio.
In the similar way, when the level of WIP status is 1(high), the release time (r j ) of all jobs are zero.
In this case, the makespan objective without release times is easier to solve than the one with non
zero release times.
6. ICP PROPERTIES
Several useful ICP properties can be derived by the following deﬁnitions and the experimental
results.
Definition 1 (Set Pareto dominance relation). Assume sets of Pareto-optimal solutions (Paretofronts) A and B are not empty and A ∪ B = C. The set of Pareto-optimal (non-dominated)
solutions from set C is D. If D ≡ A (or D ≡ B), then A(B) dominates B(A) in a set Pareto
dominance relation. Thus, if the number of combined Pareto-optimal solutions from one solution
set is 0, then the solution set is dominated by the other solution set according to the set Pareto
dominance relation. In this case, all four ICP difference values have the same sign as can be seen in
Property 1. If D = A (or D = B) and D ⊃ A (or D ⊃ B), then the two sets have a ‘Non Set Pareto
dominance relation’. When two sets belong to this relation, these sets can be classiﬁed according
to the following Deﬁnition 2.
Definition 2 (Cross relation). Assume there are two sets of Pareto optimal solutions A and B
which are not empty and include a ﬁnite number of solutions. Let the efﬁcient frontier of a set be
a set of lines drawn between two adjacent points in that set and a line from each tail point directed
to each objective. If the efﬁcient frontiers of the two sets cross each other, then the two sets have a
‘cross relation’. If the efﬁcient frontiers of the two sets do not cross each other, then the two sets
have a ‘non cross relation’.
According to Deﬁnitions 1 and 2 above, the sets of Pareto optimal solutions can be categorized
into three cases; (1) set Pareto dominance relation, (2) non set Pareto dominance relation with
non cross relation, and (3) non set Pareto dominance with cross relation. These three cases are
illustrated in Figure 5. When two sets have the set Pareto dominance relation, then the two sets
also have a non-cross relation as can be seen in Figure 5(a). When two sets have the non set Pareto
dominance relation, the two sets can have a cross or non-cross relation as can be seen in Figure 5(b)
and (c) respectively.

EVALUATING SOLUTION SETS OF A POSTERIORI SOLUTION TECHNIQUES

93

Figure 5. ‘Set Pareto Dominance’ and ‘Cross’ Relation of two efﬁcient frontiers

The following properties show that these cases can be estimated and hence, evaluated as such
by numerical ICP measures.
Property 1. In minimization problems, if a set of Pareto-optimal solutions B is dominated by a set
of Pareto-optimal solutions A in a set Pareto dominance relation, then ICP(A) is always less than or
equal to ICP(B) regardless of scaling of the objective value or the types of weight density functions
used.
Proof. Recall that ICP is proportional to the sum of f ∗ (w) for every weight w (0,1). If set A
dominates set B in a set Pareto dominance relation, then f ∗ (w) of set A is always less than or equal
to f ∗ (w) of set B for every weight. Hence the sum of f ∗ (w) of set A is always less than or equal to
the sum of f ∗ (w) of set B for all weights. Let the step length of w go to 0 (w → 0), i.e. the number
of values of w goes to inﬁnity. Then the sum of f ∗ (w) for all weights within (0, 1) converges to the
integration of f ∗ (w) over (0, 1). Therefore Property 1 holds. We note that the reverse clearly does
not always hold.

Corollary 1. In minimization problems, if two sets A and B have a non set Pareto dominance
with non cross relation, then ICP(A) is always less than or equal to ICP(B ) regardless of scaling the
objective values or the types of weight density function used.
Proof. From the proof of Property 1, it can be easily proved.



Corollary 1 may be a weak point of the ICP measures, since well-compromised non-extreme
solutions may actually be preferable to good solutions for only one objective. However, this seems
not to have occurred in practical cases as can be seen in Figures 3 and 4. From Property 1 and
Corollary 1 above, if two sets have either a set Pareto dominance relation or non set Pareto
dominance with non cross relation, then all four ICP differences have the same sign. In our
experiments, this can be detected in Replication 1 through 8 and 10 in Figure 3 (Table 3), and
Replications 1, 3, 7, and 9 in Figure 4 (Table 4).
By visual comparison, the cross relation and location (elbow or tail) of two sets can be easily
detected. Using several ICP measures, the cross relation and location can be detected numerically
as can be seen in the Propertys 2 and 3. This will be helpful to interpret the solution quality of sets
of Pareto optimal solutions in a computerized framework.

94

J. W. FOWLER ET AL.

Property 2. If the ICP U and ICP U S differences have different signs, then the two Pareto fronts
cross each other. But the reverse does not always hold.
Proof. If set A is better in ICP U but set B is better in ICP U S then, from the proof of
Property 1, there is at least one point (in the scaled objective space) in set B at which f *(w) is the
lowest for a certain weight w. In the same way, there is at least one point (in the objective space)
in set A at which f *(w) is the lowest for a certain weight w. Thus, if ICP U and ICP U S have
different signs, then two sets have a cross relation.

In our experiments, Property 2 can be detected in Replicate-9 in Table 3, Replicate-5 in Table 4
(see corresponding graphs from Figures 3 and 4).
Property 3. If the signs of ICP U (ICP U S) and ICP T (ICP U T) differences are different, then
the competing Pareto fronts cross each other. More precisely the crossing occurs in the elbow area
rather than the tail area. But the reverse does not always hold.
Proof. Property 3 can be easily proved from the deﬁnition of ICP U (ICP U S) and ICP T
(ICP U T). In our experiments, Property 3 can be seen in Replicate-2 in Table 4 and Figure 4. 
7. CONCLUSIONS AND FUTURE RESEARCH
An a posteriori solution approach is one of the practical ways to attack multiple criteria combinatorial optimization problems. Approximate solution techniques will generally be more appropriate
to solve these problems rather than exact methods due to the complexity inherent in these problems. In developing and applying such heuristics to solve practical problems, robust and efﬁcient
measures play an important role in (1) evaluating the quality of sets of Pareto-optimal solutions
and comparing competing algorithms robustly, (2) optimizing parameters through experimental
design and response surface optimization when heuristics which have a stochastic nature are employed, and (3) determining the stopping criteria of heuristic algorithms based on the approximate
convergence of the solution quality. For these purposes, the Integrated Convex Preference (ICP)
family of functions was suggested and the performance of four ICP measures and two cardinality
measures were tested to verify the appropriateness of the measures in evaluating the quality of sets
of Pareto-optimal solutions.
Through the experiments for a multiple criteria parallel machine scheduling problem, we were
able to show that the two cardinality measures can misjudge the quality of near Pareto-optimal
solutions. Also, we saw that there can be large objective value range differences between objective
values (e.g. makespan and total weighted tardiness) in a set of Pareto-optimal solutions. This range
difference can lead to misleading results when non-scaled ICP (ICP U, ICP T) measures are used.
Scaled ICP measures (ICP U S or ICP T S) give more robust comparison results in such cases
and speciﬁcally ICP T S can be used to check the solution quality in the elbow area.
Experimental results show that MPGA outperformed MOGA in overall performance for the 36
problem instance sets. However, we found that the solution quality of the algorithms is dependent
on the problem instance through the comparison results of the scaled ICP measures. MOGA works
better than MPGA when one objective can be optimized much easier than the other objective. This
is because MPGA may waste a sub-population that was assigned to improve an already optimized
objective function.

EVALUATING SOLUTION SETS OF A POSTERIORI SOLUTION TECHNIQUES

95

ICP measures use only efﬁcient extreme points among a set of Pareto-optimal solutions due
to the assumption that the decision maker’s value function is a convex combination of objective
functions. Hence, ICP has a limitation in comparing sets of Pareto-optimal solutions, which include
all the same efﬁcient extreme solutions and different non-supported solutions. In this case, different
types of value functions can be considered to overcome this limitation. For example, if a weighted
Tchebycheff metric is assumed as the decision maker’s value function, then all of the Paretooptimal solutions in a set will be considered in evaluating the solution quality of sets of near
Pareto-optimal solutions (Carlyle et al., 2003; Miettinen, 1999). However, this case seems to occur
rarely in comparing approximate algorithms. We observed no such case in our experiments. Also,
considering the fact that a set of efﬁcient extreme solutions provides the boundary information
of a set of Pareto-optimal solutions, it can be concluded that ICP delivers a good representative
scalar value of a set of non-dominated solutions from a geometric point of view. The experimental
results in this paper support this conclusion.
Further research is needed on three topics. The ﬁrst is to extend the ICP measure for three or
more criteria cases and for non-convex value functions such as the weighted Tchebycheff metric.
To extend the ICP for three or more objectives under the assumption of a convex value function,
we need to develop an efﬁcient convex-hull algorithm for three or higher dimensions by utilizing
the fact that input points are a set of non-dominated solutions and an efﬁcient method to integrate
over the disjoint polytope regions of the parameter space. The weighted Tchebycheff metric as
discussed above, provides consideration of non-dominated points that are not extreme solutions.
Hence, we will develop a version of ICP that uses the weighted Tchebycheff metric as the decision
maker’s value function.
The second area for future research is to develop more robust a posteriori solution techniques
or metaheuristics using the ICP measure. The ICP for a set of near Pareto-optimal solutions will
be a response value of a parameter optimization procedure. ICP will also be used to determine
when to stop an algorithm by considering the convergence of solution quality.
The third future research topic is to embed ICP and its extensions into a computational framework that considers multiple algorithms for attacking complex multiple criteria combinatorial
optimization problems. When applied over the entire range of solutions as in this paper, ICP can
identify which procedure or algorithm is the best overall. Alternatively, ICP can be applied over
certain sub-regions of the Pareto-optimal solution set (e.g., for solutions where the makespan is
less than 200 time units or for solutions which may be optimal within some weight interval (0.4,
0.6), and etc.). And it can identify which of two or more algorithms (or parameter settings for
a single algorithm) is best for problems in that region of objective space. Also, the calculations
for ICP provide an optimal weight interval (or region) for each Pareto-optimal solution in the
parameter space, which will suggest the robustness of the solutions. This can help to discover the
decision maker’s weight density function in an interactive manner.
SUMMARY
We present the use of a new measure called Integrated Convex Preference (ICP) to determine the solution quality of approximate solution algorithms for multiple objective combinatorial optimization problems. The performance of ICP is compared to that of other
measures appearing in the literature by comparing the solution sets generated by two approximate solution techniques (genetic algorithms) for a bi-criteria parallel machine scheduling
problem.

96

J. W. FOWLER ET AL.

REFERENCES
Carlyle, W. M., J. W. Fowler, E. Gel, and B. Kim, “Evaluating the quality of approximate solution sets for multiple objective
optimization problems,” Decision Sciences, 34(1), 63–82 (2003).
Cieniawski, S. E., J. W. Eheart, and S. Ranjithan, “Using genetic algorithms to solve a multi-objective groundwater
monitoring problem,” Water Resources Research, 31(2), 399–409 (1995).
Cochran, J. K., S.-M. Horng, and J. W. Fowler, “A multi-population genetic algorithm to solve multi-objective scheduling
problems for parallel machines,” Computers & Operations Research, 30(7), 1087–1102 (2003).
Cunha, A. G., P. Oliveira, and J. A. Covas, “Use of genetic algorithms in multicriteria optimization to solve industrial
problems,” in Proceeding of International Conference on Genetic Algorithms, 1997, pp. 682–688.
Czyzak, P. and A. Jaszkiewicz, “Pareto simulated annealing—A metaheuristic technique for multiple-objective combinatorial optimization,” Journal of Multi-Criteria Decision Analysis, 7, 34–47 (1998).
Daniels, R. L. in “Analytical evaluation of multi-criteria heuristics,” Management Science, 38(4), 501–513 (1992).
De, P., J. B. Ghosh, and C. E. Wells, “Heuristic estimation of the efﬁcient frontier for a bi-criteria scheduling problem,”
Decision Sciences, 23, 596–609 (1992).
Fonseca, C. M. and P. J. Fleming, “Genetic algorithms for multiobjective optimization: Formulation, discussion and
generalization,” Proceeding of International Conference on Genetic Algorithms, 1993, pp. 416–423.
Fry, T. D., R. D. Armstrong, and H. Lewis, “A framework for single machine multiple objective sequencing research,”
Omega International Journal of Management Science, 17(6), 594–607 (1989).
Hansen, P. and A. Jaszkiewiecz, “Evaluating the quality of approximations to the non-dominated set. Institute of mathematical modeling,” Technical University of Denmark, Lyngby. Technical Report 1998, IMM-REP-1998-7.
Hyun, C. J., Y. Kim, and Y. K. Kim, “A genetic algorithm for multiple objective sequencing problems in mixed model
assembly lines,” Computers & Operations Research, 25(7/8), 675–690 (1998).
Lee, C.-Y. and G. L. Vairaktarakis, “Complexity of single machine hierarchical scheduling: A survey in complexity in
numerical optimization,” P. M. Pardalos (ed.), World Scientiﬁc Publishing, 1993, pp. 269–298.
Loughlin, D. H. and S. Ranjithan, “The neighborhood constraint method: A genetic algorithm-based multiobjective
optimization technique,” in Proceeding of International Conference on Genetic Algorithms, 1997, pp. 666–673.
Louis, S. J. and G. J. E. Rawlins, “Pareto-optimality, GA-easiness and deception,” in Proceeding of International Conference
on Genetic Algorithms, 1993, pp. 118–123.
Marett, R. and M. Wright, “A comparison of neighborhood search techniques for multi-objective combinatorial problems,”
Computers & Operations Research, 23(5), 465–483 (1996).
Miettinen, K. M. Nonlinear Multiobjective Optimization, Kluwer Academic, Boston, 1999.
Murata, T., H. Ishibuchi, and H. Tanaka, “Multi-objective genetic algorithm and its application to ﬂowshop scheduling,”
Computers Industrial Engineering, 30(4), 957–968 (1996).
Nelson, R. T., R. K. Sarin, and R. L. Daniels, “Scheduling with multiple performance measures: The one-machine case,”
Management Science, 32(4), 464–479 (1986).
Pinedo, M., Scheduling: Theory, Algorithms, and Systems, Prentice Hall, New Jersey, 1995.
Schaffer, J. D., “Multiple objective optimization with vector evaluated genetic algorithms,” in Proceeding of the First
International Conference on Genetic Algorithms, 1985, pp. 93–100.
Schenkerman, S., “Relative objective functions in multiobjective decision support models,” Decision Sciences, 21, 727–737
(1990).
Serifoglu, F. S. and G. Ulusoy, “Parallel machine scheduling with earliness and tardiness penalties,” Computers & Operations
Research, 26, 773–787 (1999).
Tuzikov, A., M. Makhaniok, and R. Manner, “Bicriteria scheduling of identical processing time jobs by uniform processors,”
Computers & Operations Research, 25(1), 31–35 (1998).
Viana, A. and J. P. Sousa, “Using metaheuristics in multi-objective resource constrained project scheduling,” European
Journal of Operational Research, 120, 359–374 (2000).

Computers & Operations Research 32 (2005) 1147 – 1164

www.elsevier.com/locate/dsw

Web server QoS models: applying scheduling rules from
production planning
Nong Ye∗ , Esma S. Gel, Xueping Li, Toni Farley, Ying-Cheng Lai
Department of Industrial Engineering, Arizona State University, P.O. Box 875906, Tempe, AZ 85287, USA

Abstract
Most web servers, in practical use, use a queuing policy based on the Best E8ort model, which employs
the 9rst-in-9rst-out (FIFO) scheduling rule to prioritize web requests in a single queue. This model does
not provide Quality of Service (QoS). In the Di8erentiated Services (Di8Serv) model, separate queues are
introduced to di8erentiate QoS for separate web requests with di8erent priorities. This paper presents web
server QoS models that use a single queue, along with scheduling rules from production planning in the
manufacturing domain, to di8erentiate QoS for classes of web service requests with di8erent priorities. These
scheduling rules are Weighted Shortest Processing Time (WSPT), Apparent Tardiness Cost (ATC), and Earliest
Due Date. We conduct simulation experiments and compare the QoS performance of these scheduling rules
with the FIFO scheme used in the basic Best E8ort model with only one queue, and the basic Di8Serv model
with two separate queues. Simulation results demonstrate better QoS performance using WSPT and ATC,
especially when requested services exceed the capacity of a web server.
? 2003 Elsevier Ltd. All rights reserved.
Keywords: Web server; Quality of Service (QoS); Scheduling rules; Simulation model

1. Introduction
The World Wide Web has become one of the most popular and important applications on the
Internet. A web server receives numerous requests for its services, which it cannot handle at the same
time, and will typically use a bu8er or queue to store incoming requests awaiting service. Requests
in the queue are typically stored in order of arrival. The web server will take the request at the front
of the queue, and service it 9rst. This is an example of 9rst-in-9rst-out (FIFO) scheduling. Most
existing web servers provide services based on the Best E8ort model, using FIFO scheduling.
∗

Corresponding author. Tel.: +1-480-965-7812; fax: +1-480-965-8692.
E-mail address: nongye@asu.edu (N. Ye).

0305-0548/$ - see front matter ? 2003 Elsevier Ltd. All rights reserved.
doi:10.1016/j.cor.2003.10.001

1148

N. Ye et al. / Computers & Operations Research 32 (2005) 1147 – 1164

Quality of Service (QoS) has three main attributes: timeliness, precision, and accuracy [1,2]. A
service request can be considered a process with an input and output. Timeliness measures how fast
an output is produced for a given input. Precision measures the quantity, and accuracy measures
the quality, of the output. We often see such measures as delay, response time, and jitter used for
the timelines attribute; throughput, bandwidth, and loss rate (e.g., packet drop rate) used for the
precision attribute; and error rate used for the accuracy attribute. Tradeo8s often need to be made
among these attributes.
The Best E8ort model of a web server does not provide QoS because the completion time of
a request depends on how many requests are already in the queue, and thus cannot be predicted
for timeliness assurance. Furthermore, in any set of web requests, some may have a high priority,
while others have a low priority. Bhatti and Friedrich [3] discuss classifying web requests into high,
medium, and low priorities using such information as IP addresses and requested web sites. The
Best E8ort model does not support the di8erentiation of requests based on their priority. Instead,
requests are treated “fairly” based on their arrival time.
The lack of QoS in the Internet today makes it a highly vulnerable system in its current role of
supporting critical operations in many sectors of our society. QoS has been considered as a “must”
for the next-generation Internet [4,5]. Existing work on QoS for the Internet falls into two general
frameworks: Integrated Services (IntServ) and Di8erentiated Services (Di8Serv), both of which are
concerned mostly with timeliness assurance.
In the IntServ framework, applications are placed in two classes based on timeliness requirements:
real-time (hard or strict) and elastic (soft) [6–11]. IntServ aims to provide per Low QoS by reserving
bandwidth along a source-destination path to assure timeliness of data delivery. Many scheduling
rules and admission control strategies have been proposed in the IntServ framework to assure a
bound on end-to-end timeliness [12–15]. Because it requires that every hop on the end-to-end path
maintain the state of all bandwidth reservations for each Internet connection, IntServ is not scalable. Furthermore, a bound on end-to-end timeliness usually depends on the number of hops on an
end-to-end path, which is diMcult to predict, implying that there can exist no absolute bound on
end-to-end timeliness.
The Di8Serv framework marks data at the edges of a network with two classes of priority:
premium and best e8ort, which, respectively, have a high and low priority for being serviced [16–18].
In a router or host, two separate queues with di8erent capacities are used, one for each class of
data. Because the vast majority of data is classi9ed as Best e8ort, the capacity of the premium
queue is usually much smaller. Data in the premium queue is served 9rst, whereas data in the Best
E8ort queue is served only when the premium queue is empty. Lu et al. [19] use a feedback control
method to address the relative delay or jitter in a web server for assuring QoS. Some studies use load
balancing to address QoS [20–22]. Still other QoS methods based on Di8Serv have been explored
[23–25].
Research in providing QoS on a web server takes di8erent approaches. Ferrari [26] investigates
the e8ect of aggregation on performance using Priority Queuing (PQ) and Weighted Fair Queuing
(WFQ) scheduling algorithms. Chen and Mohapatra [27] exploit the dependence among session-based
requests and propose a dynamic weighted fair sharing (DWFS) scheduling algorithm to control
overloads in web servers.
We investigate Internet QoS by providing QoS on web servers. We use only one queue for all web
requests, and an advanced scheduling rule to di8erentiate services and improve overall QoS. Many

N. Ye et al. / Computers & Operations Research 32 (2005) 1147 – 1164

1149

advanced scheduling rules have been developed for production planning in the manufacturing domain
[28]. If we consider a web server as a production system, those scheduling rules can be adopted.
In this study, we investigate the application of three scheduling rules from production planning.
These scheduling rules are: Weighted Shortest Processing Time (WSPT), Apparent Tardiness Cost
(ATC), and Earliest Due Date (EDD). EDD is also known as Earliest Deadline First (EDF) [29].
We compare the QoS performance of these rules with that of FIFO in the basic Best E8ort model
with only one queue, and that of a basic Di8Serv model for a web server with two separate queues.
The rest of this paper is organized as follows: First, we de9ne the QoS models of a web server
using WSPT, ATC, and EDD, along with basic Best E8ort and Di8Serv models. Next, we describe
the simulation of these web server models using a commercial simulation tool for computer networks
(OPNET), and the simulation experiments conducted under various data conditions to discover the
QoS performance of the models. Finally, we present the simulation results and summarize our
9ndings.

2. Web server QoS models
We introduce high level frameworks for web server QoS models to provide QoS at the application
layer. The units queued in these frameworks are considered as complete web requests. Similar to
real-time applications where jitter and delay are critical to performance, timeliness is also important
to web requests, especially for some commercial web sites where a late response may cause clients
to turn to rival web sites. With the increasing development of e-commerce, it is signi9cant to provide
timeliness on web sites to attract users. Loss of users means loss of pro9t. Our study focuses on the
server side of a web site. Any incoming request is treated as a new request, regardless of whether
it is a resubmitted, previously dropped request, or an entirely new request.
QoS requirements on the Internet primarily come from end users. A web server provides services
to end users, who can specify their QoS requirements for these services. Web requests on the Internet
today do not come with any indication of their priority or other QoS requirements, such as expected
delay. We anticipate that in the future web requests will be accompanied by such data in order to
allow for QoS assurance on the Internet. In this study, we assume that each web request comes with
the following information:
•
•
•
•

Priority
Requested data (from which we derive its size)
Due date (required completion time of request)
Arrival time (determined at time of arrival).

In our study it is assumed that it is not entirely up to the web client to set the priority. Clients
cannot set priorities without web server authentication. Otherwise, everyone, including malicious
users, will give themselves the highest priority. Clients’ access history, user name and password, IP
address, and current status (e.g., a customer with a full shopping cart or with a purchasing history
gains a higher weight than a customer without a purchasing history or a full cart) can be used to
prioritize the requests. Web QoS allows the incoming requests to be categorized as high, medium,
or low priority based on IP address, requested URL, and so on [5].

1150

N. Ye et al. / Computers & Operations Research 32 (2005) 1147 – 1164

Fig. 1. QoS model of a sequential web server.

The complete round trip time, or delay, of a web request includes time waiting in the queue,
9nding and retrieving requested data, and sending it to the web client. In this study, for each web
request, we ignore the time spent 9nding and retrieving the requested data, and consider only the
waiting time in the queue and the time sending requested data to the requesting web client.
In this paper, we model a web server as a sequential single server, which means that the web
server processes incoming requests one at a time as shown in Fig. 1. This concept is di8erent from
the traditional multiprocessor or mutlithreaded web server model. We argue that this is reasonable
in terms of web server QoS. For example, if bandwidth were the performance bottleneck under
scrutiny, then we would argue that high priority requests should not share bandwidth with lower
priority requests, thereby requiring a multiprocessor or multithreaded solution.
In our study, we are addressing scheduling issues relating to the web server itself. Such solutions
could be scalable to a multiprocessor or multithreaded web server, when applied at each level of
redundancy. In this section we present the models used in our experiments.
2.1. The basic Best E;ort model
The basic Best E8ort model of a web server consists of three elements, as shown in Fig 1:
incoming web requests, a queue to sort and keep incoming requests before they are extracted for
servicing, and a server to process incoming requests and provide web services. The basic best-e8ort
model uses only the arrival time to sort incoming requests in the queue using the FIFO rule. A
request is dropped by the web server if it has been in the queue for longer than some “timeout”
threshold (for example, 90 s).
The Internet and most corporate intranets are built using the IP protocol, which is a connectionless
protocol that provides no guarantees of service time or the relative ordering of packets. For the Best
E8ort web servers in such networks, there is no admission control scheme. Hence, if client requests
are placed into the web server queue faster than they are removed for processing, congestion occurs,
resulting in delayed requests and requests dropped due to the server’s timeout threshold. Thus, the
Internet can only provide a single level of service; that of Best E8ort.

N. Ye et al. / Computers & Operations Research 32 (2005) 1147 – 1164

1151

Fig. 2. Basic Di8Serv model.

2.2. The basic Di;Serv model
Under the Di8Serv policy, requests are categorized into priority classes and placed in separate
queues. The server always processes the higher priority queue before serving any of the lower priority
queues. Di8Serv architectures generally de9ne two types of classi9cation: (1) the behavior aggregate
classi<er, which selects packets according to the Di8serv codepoint (DSCP) value stored in the IP
header on ingress, and (2) the multi<eld classi<er, which uses a more general set of classi9cation
conditions like IP header 9eld values and source address.
To implement a Di8Serv model, we classify the incoming requests into two categories: high
and low (best e8ort) priority, based on their assigned weights. Fig. 2 shows the two queues used
for these two service classes, both of which are serviced in a FIFO manner. In this model, the
best e8ort queue will only be serviced when there are no requests waiting in the high priority
queue. Again, as in the Best E8ort model, there is no admission control in the basic Di8Serv
model.
2.3. WSPT
A number of scheduling rules have been developed in the manufacturing domain to schedule a
set of jobs on a single machine, given that all of the jobs are available at time=0. The WSPT
scheduling rule is one such rule that schedules a set of jobs by decreasing order using a formula
which includes the priority weight and processing time of the job. The completion time of a job
is de9ned as the time elapsed between time=0 and the time that the machine 9nishes processing
the job.
It is shown that the WSPT scheduling rule minimizes the weighted completion time for a set
of jobs [28]. A similar technique is used in operating systems for scheduling processes on a single
CPU. This scheduling algorithm, called shortest job 9rst (SJF), is “provably optimal” [30]. However,
the SJF algorithm does not take priorities into account. We investigate the application of WSPT to
scheduling in a QoS model of a web server because WSPT incorporates the weight factor, thereby
allowing the di8erentiation of web requests with various priority weights. The WSPT rule also
incorporates the processing time of a job, to minimize the sum of the completion times for a set
of jobs. Hence, the application of the WSPT rule to scheduling in a QoS model of a web server
enables us to minimize the delays of web requests.
Using the WSPT rule, we schedule incoming web requests in a web server queue in decreasing
order by their priority values. The priority value of a web request j is determined by wj =pj , where

1152

N. Ye et al. / Computers & Operations Research 32 (2005) 1147 – 1164

wj is the given priority weight for the request j, and pj is its service time, which is calculated from
the size of the requested data, dj , as follows:
pj =

dj
:
s

(1)

In Eq. (1), s is the service rate or data transmission rate (the amount of data that can be transmitted
per time unit) of the web server. Note that the higher the weight and the shorter the service time
of a web request, the higher its priority value will be.
When a new request arrives at the web server, its priority value is computed, and the request
is then inserted into the queue according to this value. Before a new request is inserted into the
queue, we 9rst make an admission control decision based on its expected completion and due dates.
Deferring an incoming request at the very beginning of the transaction, rather than in the middle,
is a desirable scheme for an overloaded web server. First of all, it avoids further frustration on the
client side by refusing to accept requests for which it cannot satisfy the respective QoS requirements
(e.g., limits on cycle time. lateness and tardiness, etc.). Secondly, it keeps the queue levels relatively
stable, resulting in less variable output.
QoS requirements are determined by a web request’s QoS factor. We de9ne the QoS factor of an
incoming request as:
Qj = Dj − Tj − Wj :

(2)

In Eq. (2), Dj is the due date of the web request j; Tj is its arrival time, and Wj is its predicted
waiting time (i.e., the sum of the processing times of the requests ahead of request j). According
to our assumed admission control scheme, if Qj is less than zero, request j will be rejected.
2.4. ATC
The ATC rule combines WSPT and the Minimum Slack (MS) 9rst rule [28]. MS is a dynamic
dispatching rule that orders jobs in increasing order of slack, where the slack of job j at time t is
de9ned as max{dj − pj − t; 0}, where dj denotes the due date of job j and pj denotes its processing
time as in Eq. (1).
Under the ATC rule, jobs are scheduled one at a time; that is, every time the machine becomes
free, a ranking index is computed for each remaining job. The job with the highest-ranking index
is then selected to be processed next. The index is de9ned as


wj
max{dj − pj − t; 0}
Ij (t) =
:
(3)
exp −
pj
k pS
In Eq. (3), k is a scaling parameter that can be determined empirically, and pS represents the average
processing time of the remaining jobs in the queue at time t. We can see that if k is big enough,
the ATC rule will reduce to WSPT, since the ATC index Ij (t) → wj =pj as k → ∞.
In this paper, we use the following equation in place of Eq. (3) to index incoming requests using
the ATC rule:


wj
max{dj − t; 0}
∗
Ij (t) =
:
(4)
exp −
pj
k pS

N. Ye et al. / Computers & Operations Research 32 (2005) 1147 – 1164

1153

In Eq. (4), the processing time pj is eliminated because the slack of the request equals its due date
minus its waiting time, which we previously de9ned as the time elapsed between the time a request
comes into the web server and the time it gets processed.
2.5. EDD
Sequencing in increasing order by due date, EDD, minimizes the maximum lateness of a set of
jobs with given due dates and processing times [28]. While we know that EDD is optimal for the
case in which all jobs to be scheduled are available at time zero, it is harder to 9nd an optimal
policy for the case in which jobs are released in di8erent points in time, such is the case in a web
server. This is due to the fact that when preemption of jobs is not allowed, the optimal schedule is
not necessarily a non-idling schedule.
Having noted this point, we adopt a static EDD rule in our web server model by placing the
incoming requests into the queue according to their due date. Requests with an earlier due date are
placed in the front of the queue, and processed before requests with a later due date.
3. QoS measures
We de9ne four QoS measures in this paper: number of dropped jobs per unit time (drop rate),
average waiting time in the system, average lateness and throughput.
Under the basic Best E8ort and Di8Serv policies, request drops only occur when the request’s
waiting time in the queue reaches the TIMEOUT threshold. Under the WSTP, ATC, and EDD
policies, request drops may happen at admission control, as well as when the waiting time exceeds the
due date while requests are waiting in the queue. Waiting time is used to measure the responsiveness
of the web server. Lateness represents the gap between the waiting and due dates, and can be negative
or positive. A negative lateness indicates that a request completed before its due date, and a positive
lateness indicates that a request was tardy. Lateness depicts how well the due date requirement of
the request is met.
4. Simulation model and experiments
We simulate a web server under the policies discussed above using the OPNET Modeler 8.1.A
simulation environment. The simulation experiments were conducted on a Micron PC with a single
Pentium4 1:9 GHz CPU and 512 MB of RAM running on the Windows 2000 operating system. Fig. 3
depicts the simulation model based on the web server system presented in Fig. 1. The three generator
modules generate web requests with di8erent priority weights. The forwarder module forwards the
requests to the queue, where the admission control scheme is implemented. The requests are then
placed into the queue based on the scheduling rule currently in use. The sink module destroys the
requests after the web server processes them.
We de9ne two 9elds in the data packet format: weight and due date. We assume that the requested
data follows a Pareto distribution with shape parameter 65536 and scale parameter 1.4 [31,32]. Thus
∗
1:4
the mean of the requested data is 65536
=229376 bits, about 28 K bytes. We calculate the processing
1:4−1

1154

N. Ye et al. / Computers & Operations Research 32 (2005) 1147 – 1164

Fig. 3. The topology of the QoS web server simulation.

time by dividing the requested data by a constant, deterministic service rate. For example, if the
requested data is 6000 bytes and the service rate of the server is 240,000 bytes per second, the
processing time for this request is assumed to be 0:025 s.
The weight 9eld is a 4-bit integer that contains the weight information of the request. The weight
value is recorded in the weight 9eld of a request packet when the request is generated. There are
multiple choices available to select the proper weight value [3]. For simplicity, we only de9ne
three priority classes. In Fig. 3, generators 1, 2, and 3 generate requests with weights 1, 5 and 10,
respectively, corresponding to low, medium, and high priority requests.
The due date 9eld in the packets shows the time a request is due. If an application cannot receive
a response as required, its QoS requirements cannot be met. Of course, whether or not an application
can get its response on time depends not only on how fast the web server processes the request,
but also on the Internet transmission delay. How to guarantee a lower bound delay between routers
is beyond the scope of this paper. Hence, we focus on the due date, by which the web server is
required to have processed a request, and assume that the client will receive its response once the
server sends it. We use the same distribution to model the randomness of the due dates for all
classes of requests because we assume that they require the same URLs. We also assume that the
due dates are normally distributed with a mean of 2 s and a standard deviation of 0:2 s.
In our experiments, we use two traMc conditions: overwhelming and light. We use an exponential
distribution to model randomness in the arrival of requests to the web server. For the overwhelming
traMc condition, generators 1 and 2 generate requests at a rate of 40 requests per second, while
generator 3 generates requests at a rate of 20 requests per second. Hence, the total traMc generated
is equal to (40 + 40 + 20)∗ 229376 bits per second. We set the queue service rate at 12,697,600 bits
per second, about T1 speed of 1:55 Mbps, which is around 55.36% of the generated web traMc. The
server can process about 55 requests per second.
In the light traMc case, the arrival rate for low and medium priority requests are reduced to 16
requests per second, while the arrival rate for the high priority requests is at a rate of 8 requests

N. Ye et al. / Computers & Operations Research 32 (2005) 1147 – 1164

1155

Table 1
Simulation parameters
TraMc case

Class

Weight

Request interarrival time

Due date (s)

Document size (bits)

Overwhelming

LOW
MEDIUM
HIGH

1
5
10

Exponential(0.025)
Exponential(0.025)
Exponential(0.05)

Normal(2,0.2)
Normal(2,0.2)
Normal(2,0.2)

Pareto (65536; 1:4)
Pareto (65536; 1:4)
Pareto (65536; 1:4)

Light

LOW
MEDIUM
HIGH

1
5
10

Exponential(0.0625)
Exponential(0.0625)
Exponential(0.125)

Normal(2,0.2)
Normal(2,0.2)
Normal(2,0.2)

Pareto (65536; 1:4)
Pareto (65536; 1:4)
Pareto (65536; 1:4)

JOBS
WAITING
FREE

ARRIVAL

NO JOB
WAITING

FINISH

ARRIVAL

START

BUSY

NOT FINISH

JOBS
WAITING

DONE

IDLE

DEFAULT

Fig. 4. FSM for sequential web server.

per second. Hence, the total traMc generated reduces to only 72% of the traMc the web server
can handle. In both overwhelming and light traMc cases, we use 100 for the scaling parameters
in the ATC policy. We set the simulation duration to 4000 s. The parameters for our simulation
experiments are given in Table 1.
We use a Finite State Machine (FSM) to simulate processing in a web server. The model actually
includes two FSMs. The ARRIVAL state itself is an FSM in which requests come into the queue
based on QoS rules. The second FSM includes the three states: START, DONE, and IDLE. The
FSMs are shown in Fig. 4.
We describe the processing handled in each state in the FSM shown in Fig. 4 for each of the
web server models in our investigation:
ARRIVAL state. Best E;ort model: When a request comes in, it enters the ARRIVAL state, and
will be placed into the queue using the FIFO rule.
Basic Di;Serv model: After a request enters the ARRIVAL state, we get the priority ‘weight’
value from the packet and place it into the respective queue.
WSPT model: First we execute the admission control algorithm and drop any requests with a
QoS factor less than zero. For new requests, we calculate their priority according to equation (1),
and insert the request into the queue based on its priority.
ATC model: As an extension of WSPT, we also implement the Admission Control Scheme at the
very beginning of the ARRIVAL state. For a new incoming request, we calculate its index using Eq.
(4) as its priority for queue placement. We assign k = 100; the scaling parameter in our simulation.

1156

N. Ye et al. / Computers & Operations Research 32 (2005) 1147 – 1164

EDD model: We perform the Admission Control Scheme when the process enters the ARRIVAL
state. We extract the due date information of a new request from the packet’s due date 9eld. We
use its reciprocal as the priority and insert the request into the queue accordingly.
START state. When the server is not busy, a process enters the START state. In this state, we
will remove a request from the head of the queue and schedule it based on its processing time. The
FSM will then stay in the START state until the current request is processed.
DONE state. After a request is processed, the FSM enters the DONE state, and the server becomes
free again. If there are no jobs waiting for processing, the FSM will go to IDLE, otherwise, the
FSM will go back to START to process the waiting requests.
IDLE state. The server is free at the IDLE state. When there are no requests waiting in the
queue, the FSM remains in the IDLE state. When a request comes into the ARRIVAL state and is
inserted into the queue, the FSM will transition from IDLE to START.

5. Experimental results
In this section, we provide detailed results on our simulation experiments for the heavy traMc and
light traMc cases and provide a discussion of our results for each of the QoS performance metrics.
5.1. Overwhelming tra@c case
We 9rst present the results of the overwhelming traMc scenario. Due to the “bursty” nature of web
traMc, it is not surprising that a web server can become overwhelmed when one minute previous
its workload was quite small. A malicious attack that sends a high volume of requests to a web
server is another source of overwhelming traMc. We argue that even under the overwhelming traMc
condition, our QoS enabled web server can still provide QoS.
The simulation results for this section are given in Table 2. In our tabulated results, STD stands
for Standard Deviation which is calculated over time from 600 to 4000 s (a time interval in which
the system is in a steady state). BE stands for Best E8ort policy and DS stands for basic Di8Serv
policy. The number in each cell can also be considered a percentage of requests since the columns
of each row total the actual number of requests.
5.1.1. Waiting time
The waiting time performance of the Best E8ort, basic Di8Serv, WSPT, ATC, and EDD policies
are shown in Fig. 5. We take all processed requests into account and calculate the waiting time in
the steady state window.
We observe that the overall performance is dramatically enhanced under the ATC, EDD, and
WSPT policies, as shown in Table 2. These policies employ an admission control scheme. The
admission control scheme discards requests whose QoS requirements cannot be met before they are
even placed in the queue, which results in a smaller average number of requests waiting in the queue
and consequently a shorter waiting time. The average number of requests waiting in the queue is
about 25 under WSPT policy, and 67 under EDD policy. Under Best E8ort and basic Di8Serv, the
average number of waiting requests is about 8989 and 7279, respectively.

N. Ye et al. / Computers & Operations Research 32 (2005) 1147 – 1164

1157

Table 2
Experimental results of the overwhelming traMc case
Priority class

BE

DS

WSPT

ATC

EDD

Waiting time of requests
All
Mean
STD
High
Mean
STD
Medium
Mean
STD
Low
Mean
STD

89.96725
0.004698
89.96703
0.005094
89.96749
0.00474
89.96762
0.004737

56.39645
7.965471
3.776491
10.2084
89.95819
0.007214
89.958
0.006525

0.100961
0.012236
0.021563
0.003994
0.039224
0.005459
0.231369
0.029947

0.104629
0.013271
0.022253
0.00413
0.040503
0.005846
0.240477
0.032699

0.2742334
0.0301226
3.776491
10.2084
0.273149
0.03198
0.274335
0.032978

45.34254
11.2525
9.001148
2.279903
18.24159
4.625595
18.10011
4.4637

46.44422
12.06754
0.066765
0.612841
23.28894
6.022406
23.08864
5.869901

10.44669
1.174351
0.180685
0.067248
1.001089
0.170591
9.265216
1.061894

10.63292
1.191997
0.199215
0.075172
1.046383
0.17572
9.387631
1.07052

46.50868
14.39948
9.21321
2.850334
18.75024
5.782265
18.54573
5.851304

Lateness of requests
All
Mean
STD
High
Mean
STD
Medium
Mean
STD
Low
Mean
STD

87.964869
0.0124608
87.96774
0.023457
87.96471
0.01794
87.96591
0.016003

54.39429
7.964984
0.4149
10.20868
87.95174
0.022036
87.96192
0.028651

−1.87864
0.210136
−1.95503
0.219513
−1.93868
0.217253
−1.77454
0.022221

−1.8751
0.2099
−1.95416
0.219381
−1.93742
0.217093
−1.76515
0.024757

−1.45155
0.046376
−1.4516044
0.051434
−1.45239
0.047151
−1.4509
0.049007

Throughput of requests
All
Mean
STD
High
Mean
STD
Medium
Mean
STD
Low
Mean
STD

12696420
2968060
2311544
562505.6
5015363
1321620
5369555
2870234

12696350
4493832
4241935
1302286
3963317
1529590
4491137
2748046

12697660
7463.187
3396161
186548.3
6083328
165117.2
3218229
158414.8

12697573
5814.585
3373713
178843.3
6073930
162480
3249982
164200.2

12697636
3727158
2290618
670560.8
5023274
1967933
5383762
3589084

Requests dropped
All
High
Medium
Low

Mean
STD
Mean
STD
Mean
STD
Mean
STD

The WSPT and ATC policies also contribute to the stabilized waiting time as shown in Fig. 5.
We notice that the variance of the overall Waiting Time under basic Di8Serv policy is quite large,
which can also be seen from Table 2.

N. Ye et al. / Computers & Operations Research 32 (2005) 1147 – 1164

Second

1158

100
90
80
70
60
50
40
30
20
10
0
600

BE
DS

1600

2600

3600

Time (Second)

0.5

Second

0.4
0.3

WSPT
ATC

0.2

EDD

0.1
0
600

1600

2600

3600

Time (Second)

Fig. 5. Overwhelming traMc case: overall waiting time of requests.

120

Requests

100

BE

80

DS

60

WSPT

40

ATC
EDD

20
0
600

1600

2600

3600

Time (Second)
25

Requests

20
BE

15

DS

10

WSPT
ATC

5
0
600
-5

EDD
1600

2600

3600

Time (Second)

Fig. 6. Overwhelming traMc case: overall drop of all (above) and high priority (below) requests.

5.1.2. Drop rate
Fig. 6 shows the overall drop rate of all requests and high priority requests. We observe that
the overall dropped requests using Best E8ort, basic Di8Serv, and EDD is about 45 requests per
second as shown in Table 2. However, only about 10 requests are dropped per second under ATC

N. Ye et al. / Computers & Operations Research 32 (2005) 1147 – 1164

1159

14000000
12000000

Bits

10000000
8000000

Mean

6000000

Std

4000000
2000000
0
BE

DS

WSPT

ATC

EDD

4500000
4000000
3500000

Bits

3000000
2500000

Mean

2000000

Std

1500000
1000000
500000
0
BE

DS

WSPT

ATC

EDD

Fig. 7. Overwhelming traMc case: throughput of all (above) and high priority (below) requests.

and WSPT; 22% of Best E8ort. This is because the WSPT and ATC policies take processing time
into account and discriminate against requests with a long processing time.
In the high priority class of requests, using the basic Di8Serv policy has the lowest drop rate
because the requests in the low priority queue can only be processed when the high priority queue
is empty. We can see from the lower chart in Fig. 6 that some high priority requests may still
be dropped. This can happen when the timeout threshold is reached, for example, when a high
priority request comes into the system while a request that asked for a large document is being
processed.
5.1.3. Lateness
From Table 2, we note that ATC, WSPT and EDD can meet the Lateness QoS requirement. On
the other hand, Best E8ort and basic Di8Serv cannot. In Di8Serv, most of the High priority requests
can meet the Lateness QoS requirement (negative Lateness), however, the Lateness of some requests
can be over 60 s (STD).
5.1.4. Throughput
The mean throughput of all requests is about 12,697,600 bits per second, the service rate of the
server, because the web server is overwhelmed (see Table 2). We 9nd that di8erences in standard
deviation of the throughput between the 9ve policies are quite large as shown in Fig. 7.
We observe from the lower chart in Fig. 7 that with High priority class requests, the basic
Di8Serv policy yields the highest throughput among the 9ve models and EDD has the lowest. The
more requests that are dropped, the less the throughput yield, and vice versa.

1160

N. Ye et al. / Computers & Operations Research 32 (2005) 1147 – 1164

For Medium priority requests, WSPT has the highest throughput, about 21% more than Best
E8ort and 53% more than basic Di8Serv. For Low priority requests, WSPT and ATC have the
lowest throughput and standard deviation (see Table 2). Best E8ort and EDD policies have the
highest throughput. However, the standard deviations are quite large.
5.1.5. Requests waiting in queue
The average number of requests waiting in the queue under each of the policies is shown in Table
2. There are only 25 requests waiting in the queue under WSPT and ATC policies, while EDD has
about 67 requests. Best E8ort and basic Di8Serv have about 8989 and 7279 requests waiting in the
queue, respectively.
5.2. Light tra@c case
We now present the results of the light traMc scenario. The simulation results for this section are
given in Table 3.
5.2.1. Waiting time
The overall waiting time of the ATC, Best E8ort, basic Di8Serv, EDD and WSPT policies is
shown in Fig. 8. WSPT and ATC have similar performance. The overall waiting time for Best E8ort
and basic Di8Serv are more than 15 times longer than the WSPT and ATC policies.
From Table 3 and Fig. 8, we can see that the deviation of waiting times of the Best E8ort
and basic Di8Serv policies are very large, about 5.87 for Best E8ort and 5.49 for basic Di8Serv.
Congestion can happen under Best E8ort and basic Di8Serv models at some points, leading to longer
waiting times. A request may need to wait more than 40 s before getting service.
5.2.2. Drop rate
Table 3 also includes the drop rate information. There are no requests dropped under the Best
E8ort and Basic Di8Serv policies. There is a tradeo8 between waiting time and requests dropped.
There are a small amount of requests dropped under ATC, WSPT and EDD policies, due to the
admission control scheme, however a shorter waiting time is gained.
5.2.3. Lateness
We 9nd that on the average there is no violation of due date requirements in the Light TraMc
scenario under each of the policies because the mean lateness, as shown in Table 3, is negative.
But we should keep in mind that, as shown in Fig. 8, the waiting time of some requests can exceed
40 s and thus the lateness can be positive.
5.2.4. Throughput
Since the incoming traMc and service rates are the same under all policies, they produce almost
the same level of throughput in the light traMc scenario because the system is stable and able to
handle the incoming traMc. The throughput of the Best E8ort and Basic Di8Serv policies are a little
higher than that of the other policies because no requests are dropped, resulting in a higher arrival
(and hence, throughput) rate.

N. Ye et al. / Computers & Operations Research 32 (2005) 1147 – 1164

1161

Table 3
Experimental results of the light traMc case
Priority class

BE

DS

WSPT

ATC

EDD

Waiting time of requests
All
Mean
STD
High
Mean
STD
Medium
Mean
STD
Low
Mean
STD

1.530703
5.878742
1.529657
5.887196
1.530479
5.875447
1.531522
5.878587

1.493246
5.493077
0.354413
1.552743
1.825389
6.939911
1.827767
6.948821

0.092163
0.04996
0.068523
0.03778
0.078798
0.044733
0.117916
0.065669

0.090235
0.046535
0.069562
0.036143
0.079292
0.042569
0.112029
0.059874

0.124289
0.062083
0.126305
0.063747
0.121852
0.056926
0.125791
0.070296

0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0

1.352074
4.215406
0.217108
0.777847
0.50021
1.694942
0.635015
1.746721

1.441486
4.232963
0.232718
0.781258
0.53609
1.698867
0.672957
1.756716

1.659334
4.426103
0.312424
0.828979
0.676685
1.787868
0.67051
1.810628

Lateness of requests
All
Mean
STD
High
Mean
STD
Medium
Mean
STD
Low
Mean
STD

−0.47009
5.881148
−0.47449
5.889278
−0.46995
5.877842
−0.46819
5.881193

−0.50748
5.495734
−1.65009
1.557602
−0.17499
6.941919
−0.17192
6.951396

−1.91247
0.047722
−1.93752
0.046556
−1.92551
0.044774
−1.88658
0.063165

−1.91452
0.04479
−1.9364
0.045589
−1.92505
0.042893
−1.89277
0.058128

−1.87361
0.068402
−1.87521
0.07022
−1.87535
0.065856
−1.87127
0.076792

Throughput of requests
All
Mean
STD
High
Mean
STD
Medium
Mean
STD
Low
Mean
STD

8995437
1564300
1713773
360182.2
3598025
883359.1
3683673
1390708

9022972
1543513
1719019
333671.4
3609039
913816
3694949
1345147

8599323
1198582
1651044
315147.4
3460339
747808.3
3487975
1327450

8592518
1187294
1644867
314850.4
3463858
744651.2
3483828
1326310

8624697
1203524
1636118
317698.7
3459718
746697.3
3528896
1347935

Requests dropped
All
High
Medium
Low

Mean
STD
Mean
STD
Mean
STD
Mean
STD

5.2.5. Requests waiting in queue
There are about 3.6 requests waiting in the queue under the WSPT and ATC policies, 5.3 requests
under EDD, 61.5 requests under basic Di8Serv and 62.2 requests under Best E8ort. The smaller
number of waiting requests in the queue contributes to the shorter waiting time.

N. Ye et al. / Computers & Operations Research 32 (2005) 1147 – 1164

Second

1162

45
40
35
30
25
20
15
10
5
0
600

BE
DS

1100

1600

2100

2600

3100

3600

Second

Time (Second)

0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
600

WSPT
ATC
EDD

1600

2600

3600

Time (Second)

Fig. 8. Light traMc case: overall waiting time.

6. Conclusions and future work
In this paper, we demonstrate how to model a sequential web server as a single machine and apply
WSPT, ATC, and EDD queuing disciplines to di8erentiate the services, and thus provide QoS. We
compare these policies with the Best E8ort and basic Di8Serv policies.
We propose that most web servers can be modeled as a Best E8ort Model using the FIFO queuing
discipline. We then verify that the Best E8ort Model cannot provide quality of service, especially
in regard to waiting time. Furthermore, our results show that with the Basic Di8Serv Model, the
performance of high priority requests cannot be guaranteed because the requests cannot meet the
Lateness QoS requirement. Performance using other classes of requests is no better than the Best
E8ort policy can provide. As we can see from our experimental results, the Basic Di8Serv policy
cannot provide QoS for a web server.
We introduce an Admission Control Scheme, which contributes to a tremendous improvement in
performance. Thanks to the Admission Control Scheme, the overall waiting time of WSPT, ATC,
and EDD Models are much shorter than the overall waiting time of Best E8ort and Basic Di8Serv
Models. The waiting time of Best E8ort policy is about 900 times longer than that of WSPT in
the overwhelming traMc case. In the light traMc case, the waiting time of WSPT is also about 15
times shorter than that of Best E8ort. It reveals that the Admission Control Scheme is e8ective to
maintain timeliness for an overwhelmed web server.
We have shown that WSPT and ATC dispatching rules can be used to provide di8erentiated
services. From the simulation results, the performance of ATC is quite similar to that of WSPT
when the scaling parameter k is set to 100. We also create other scenarios of ATC with di8erent
scaling parameters and 9nd that there is no signi9cant di8erence if the scaling parameter k is larger
than 10 in our simulation.

N. Ye et al. / Computers & Operations Research 32 (2005) 1147 – 1164

1163

We can also safely conclude that our QoS models not only provide good performance when the
server is overloaded but also work well under a light traMc condition. In a light traMc scenario,
the QoS models provide better waiting time for high priority requests at the cost of a very small
amount of dropped lower priority requests.
However, as Pinedo [28] stated, real-world scheduling problems are di8erent from the mathematical
models in academia. For example, WSPT is a static rule which is not time dependent. It assumes
that there are n jobs to be scheduled and the problem is solved after the n jobs are scheduled. For a
web server, requests are submitted by clients continuously. WSPT may not be the optimal scheduling
rule to gain the minimum total weighted completion time. Another important aspect is that stochastic
models usually use special distributions which may not closely represent the behaviors of the real
system. Here, we use a requested document size divided by service rate to decide the processing
time of a request. Request size follows the Pareto distribution. For a web server, the processing time
of a request may also be inLuenced by the load and con9guration of the web server.
In spite of these small di8erences, scheduling rules in manufacturing can provide valuable
insights into scheduling problems in an information infrastructure. From the results of our research,
we surmise that some manufacturing scheduling rules may be used to develop a framework for
QoS enabled web servers. To implement our web server QoS models, other information about web
requests, such as hostname, port, etc. along with the requested links, are also required. We suggest
that further work might be done to investigate network costs and delays on implementing models
based on these frameworks.
Acknowledgements
This work is sponsored by the Air Force Research Laboratory—Rome (AFRL-Rome) under grant
number F30602-01-1-0510, and the Department of Defense (DoD) and the Air Force OMce of
Scienti9c Research (AFOSR) under grant number F49620-01-1-0317. The US government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright
annotation thereon. The views and conclusions contained herein are those of the authors and should
not be interpreted as necessarily representing the oMcial policies or endorsements, either express or
implied, of, AFRL-Rome, DoD, AFOSR, or the US Government.
References
[1] Lawrence TF. The quality of service model and high assurance. In Proceedings of the IEEE High Assurance Systems
Engineering Workshop, Washington, DC, 1997.
[2] Ye N. QoS-Centric stateful resource management in information systems. Information Systems Frontiers
2002;4(2):149–60.
[3] Bhatti N, Friedrich R. Web server support for Tiered services. IEEE Network 2000;13(5):64–71.
[4] Strnadl C. At your service: QoS for the Internet. IEEE Multimedia 2002;9(1):93–5.
[5] Bhatti N, Bouch A, Kuchinsky A. Integrating user-perceived quality into web server design. Computer Networks
2000;33(1– 6):1–16.
[6] Lzzo P. Gigbit networks: standards and schemes for next-generation networking. New York: Wiley; 2000.
p. 177–233 [chapter 6].
[7] Braden R, Clark D, Shenkar S. Integrated services in the internet architecture: an overview, RFC 1633, IETF, 1994.

1164

N. Ye et al. / Computers & Operations Research 32 (2005) 1147 – 1164

[8] Braden R, Zhang L, Berson S, Herzog S, Jamin, S. Resource reservation protocol (RSVP) version 1, functional
speci9cation, RFC 2205, IETF, 1997.
[9] Wroklawski J. The use of RSVP with IETF integrated services, RFC 2210, 1997.
[10] Wroklawski J. Speci9cation of the controlled-load network element service, RFC 2211, 1997.
[11] Shenker S, Partridge C, Guerin R. Speci9cation of guaranteed quality of service, RFC 2212, 1997.
[12] Almeida J, Dabu M, Manikutty A, Cao P. Providing di8erentiated levels of services in web content hosting. First
Workshop on Internet Server Performance, Madison, WI, 1998.
[13] Cherkasova L, Phaal P. Session-based admission control: a mechanism for peak load management of commercial
web sites. IEEE Transactions on Computers 2002;13(6):669–85.
[14] Li K, Jamin S. A measurement-based admission-controlled web server. Proceedings of the IEEE INFCOM 2000,
2000.
[15] Zhang H. Service disciplines for guaranteed performance service in packet-switching networks. Proceedings of the
IEEE, 1995;83(10):1374 –96.
[16] Chandra S, Ellis CS, Vahdat A. Di8erentiated multimedia web services using Quality Aware Transcoding, INFOCOM,
2000.
[17] Nichols K, Blake S, Baker F, Black D. De9nition of the di8erentiated services 9eld (DS Field) in the Ipv4 and
Ipv6 headers, RFC 2474, 1998.
[18] Nichols K, Jacobson V, Zhang L. A two-bit di8erentiated services architecture for the Internet, 1999.
ftp://ftp.ee.lbl.gov/papers/dsarch.pdf.
[19] Lu C, Abdelzaher TF, Stankovic JA, Son SH. A feedback control approach for guaranteeing relative delays in web
servers. IEEE Real-Time Technology and Application Symposium (RTAS’ 2001), Taipei, Taiwan, 2001.
[20] Conti M, Gregori E, Panzieri F. Load distribution among replicated web servers: a Qos-based approach. Second
Workshop on Internet Server Performance in conjunction with ACM SIGMETRICS 99/FCRC, Atlanta, GA, 1999.
[21] Engelschall RS. Load balancing your web site, Practical Approaches for Distributing HTTP traMc, 1998.
www.webtechniques.com/archives/1998/engelschall.
[22] Shan Z, Lin C, Marinescu DC, Yang Y. Modeling and performance analysis of QoS-aware load balancing of
web-server clusters. Computer Networks 2002;40(2):235–56.
[23] Rhee, Yoon-Jung, Hyun, Eun-Sil, Kim, Tai-Yun. Connection management for QoS service on the Web. Journal of
Network and Computer Applications 2002;25(1):57– 68.
[24] Abdelzaher TF, Shin KG, Bhatti N. Performance guarantees for web server end-systems: a control-theoretical
approach. IEEE Transactions on Parallel and Distributed Systems 2002;13(1):80–96.
[25] Striegel A, Manimaran G. Packet scheduling with delay and loss di8erentiation. Computer Communications
2002;25(1):21–31.
[26] Ferrari T. End-to-end performance analysis with traMc aggregation. Computer Networks 2000;34(6):905–14.
[27] Chen H, Mohapatra P. Overload control in QoS-aware web servers. Computer Networks 2003;42(1):119–33.
[28] Pinedo M. Scheduling theory, algorithms, and systems. Englewood Cli8s, NJ: Prentice-Hall; 1995.
[29] Guerin R, Peris V. Quality-of-service in packet networks: basic mechanisms and directions. Computer Networks
1999;31(3):169–89.
[30] Silberschatz A, Galvin PB, Gagne G. Operating systems concepts, 6th ed. New York: Wiley; 2003. p. 158– 61
[chapter 6].
[31] David von S. CRC Standard curves and surfaces. Boca Raton, FL: CRC Press; 1993. p. 252–3.
[32] Arlitt MF, Williamson CL. Web server workload characterization: the search for invariants. In ACMSIGMETRICS
Performance Evaluation Review 1996:126 –37.

European Journal of Operational Research 227 (2013) 503–514

Contents lists available at SciVerse ScienceDirect

European Journal of Operational Research
journal homepage: www.elsevier.com/locate/ejor

Innovative Applications of O.R.

Optimizing specimen collection for processing in clinical testing laboratories
E. Yücel a, F.S. Salman a, E.S. Gel b,⇑, E.L. Örmeci a, A. Gel c
a

College of Engineering, Koç University, Istanbul, Turkey
School of Computing, Informatics and Decision Systems Engineering, Arizona State University, 699 S. Mill Avenue, Tempe, AZ 85281, United States
c
Alpemi Consulting LLC, Phoenix, AZ, United States
b

a r t i c l e

i n f o

Article history:
Received 28 June 2011
Accepted 26 October 2012
Available online 12 November 2012
Keywords:
OR in health services
Transportation
Logistics of clinical testing laboratories

a b s t r a c t
We study the logistics of specimen collection for a clinical testing laboratory that serves sites dispersed in
an urban area. The specimens that accumulate at the customer sites throughout the working day are
transported to the laboratory for processing. The problem is to construct and schedule a series of tours
to collect the accumulated specimens from the sites throughout the day. Two hierarchical objectives
are considered: (i) maximizing the amount of specimens processed by the next morning, and (ii) minimizing the daily transportation cost. We show that the problem is NP-hard and formulate a linear Mixed
Integer Programming (MIP) model to solve the bicriteria problem in two levels. We characterize properties of optimal solutions and develop a heuristic approach based on solving the MIP model with additional constraints that seeks for feasible solutions with speciﬁc characteristics. To evaluate the
performance of this approach, we provide an upper bounding scheme on the daily processed amount,
and develop two relaxed MIP models to generate lower bounds on the daily transportation cost. The
effectiveness of the proposed solution approach is evaluated using realistic problem instances. Insights
on key problem parameters and their effects on the solutions are extracted by further experiments.
 2012 Elsevier B.V. All rights reserved.

1. Introduction
Clinical laboratory testing is an essential element in the delivery
of healthcare services. Healthcare professionals use laboratory
tests to assist in the detection, diagnosis, monitoring and treatment of diseases and other medical conditions. Clinical testing is
performed on bodily ﬂuids, such as blood and urine, and is usually
outsourced to specialized clinical testing companies by hospitals
and healthcare professionals. These companies analyze the specimens on special-purpose medical testing equipment, and compile
the results for each patient in a report, which is then sent back
to the patient’s healthcare provider.
It is estimated that lab testing has an impact on over 70% of
medical decisions (Knowledge Source Inc., 2008). It is essential that
clinical laboratories provide error-free processing and reporting, as
well as reliable service with short turn-around times of test results.
Most clinical testing companies have strict agreements on turnaround time, which is generally required to be less than one business day, and sometimes as short as a few hours for urgent requisitions. Achieving a high level of customer service can be possible
only if the collection and delivery of specimens to the laboratory is
planned and executed efﬁciently. Additionally, clinical testing
companies cannot prioritize or refuse to serve customers based
⇑ Corresponding author. Tel.: +1 480 965 2906; fax: +1 480 965 8692.
E-mail address: esma.gel@asu.edu (E.S. Gel).
0377-2217/$ - see front matter  2012 Elsevier B.V. All rights reserved.
http://dx.doi.org/10.1016/j.ejor.2012.10.044

on proﬁt concerns to maintain market competitiveness and due
to ethical obligations.
Clinical testing companies are faced with the following daily
operations. Clinical specimens are obtained from patients at various sites, such as physician ofﬁces, hospitals, and patient service
centers operated by the companies. The specimens are then transferred to a central processing facility to be analyzed. The collection
of the specimens from the sites is achieved by a ﬂeet of vehicles
that visit each site a number of times every day. The sites are
grouped into several regions, each of which is serviced by one of
the vehicles. The drivers visit the sites according to a predetermined sequence, except for situations when an urgent requisition
arises. Once the specimens get to the processing facility, the requisition orders are logged into a database system for tracking and
billing of the orders. The specimens are then processed on
highly-automated testing equipment that runs at a predetermined
rate. Finally, the analysis results are compiled in a report.
Clinical testing companies are under increasing pressure to provide service at lower cost, and hence, they constantly seek ways to
reduce their operational costs, even though their ﬁrst priority is
still to provide reliable and accurate results with a fast turn-around
time. An important determinant of the operational costs is the
effective utilization of the high-tech processing equipment and
skilled labor, which in turn is affected by the way the specimens
are transported from the sites to the laboratory. In particular, the
arrival rate of specimens to the laboratory has to match (or exceed)

504

E. Yücel et al. / European Journal of Operational Research 227 (2013) 503–514

the processing rate so that starvation of the processing resources
can be avoided.
We consider the problem of transporting specimens from a
number of geographically-dispersed sites to the processing facility
of a clinical testing company. The fundamental decisions to be
optimized can be represented by the schedule (i.e., timing) of visits
to each site throughout the day. This involves designing tours to be
conducted by a vehicle throughout the day, which requires the
determination of both the sequence of sites to visit and their timings. This problem is signiﬁcantly different from a routing problem
where the only decision of interest is the sequence in which the
sites will be visited by each vehicle. Another major difference is
that we consider the number of specimens processed as a prioritized objective, in addition to the transportation cost, since collection strategies that only minimize transportation cost may result in
excessive idle times at the processing facility. On the other hand,
keeping the processing unit busy may dictate the use of more frequent, and possibly more costly tours. Hence, transportation decisions should be made with the consideration of the input that they
provide to the processing system.
In this paper, we deﬁne a single-vehicle deterministic version of
this problem as the Collection for Processing Problem (CfPP). Speciﬁcally, we deﬁne CfPP as the problem of designing tours to match
collected workload with the processing capacity at the processing
facility. CfPP involves two hierarchical objectives. The ﬁrst-level
objective is to maximize the daily processed amount, whereas
the second-level objective is the minimization of transportation
costs, which are assumed to be directly proportional to the distance traveled by the vehicle. In particular, we seek a solution that
minimizes the transportation costs while maintaining the maximized daily processed amount obtained by optimizing only with
respect to the ﬁrst level objective.
Clinical specimen collection was ﬁrst addressed by McDonald
(1972) in a case study, where the objective was to minimize the total traveling time. The problem is formulated as a Vehicle Routing
Problem (VRP), and a heuristic algorithm is proposed for its solution. Revere (2004) also discusses a case study on a business process re-engineering project for a laboratory courier service. The
goal is to minimize both the laboratory courier and stafﬁng costs.
The problem is divided into two sub-problems, each dedicated to
one of the two objectives. For the objective of minimizing laboratory courier cost, a traveling salesman model is used. An integer
programming formulation is developed to minimize stafﬁng costs.
Differently from our study, neither of these studies considers the
accumulation of specimens over time.
A related problem appears in the domain of blood collection.
CfPP has two key common features with blood collection problems studied before: (i) units to be collected accumulate over
time and (ii) there is a deadline for the completion of processing.
Operational efﬁciency of the blood collection process has been
studied since the 1970s (e.g., Dumas and Rabinowitz, 1977; Or,
1976), albeit, without vehicle routing. Yi and Scheller-Wolf
(2003) is the ﬁrst paper that incorporates vehicle routing aspects
into this problem, through a real life application at the American
Red Cross. Recently, Doerner et al. (2008) studied logistics of the
blood program in the Austrian Red Cross as a vehicle routing
problem with multiple interdependent time windows for each
customer. Although these studies consider accumulation over
time in the sites, none of them considers the impact of ﬁnite
processing capacity.
CfPP is a routing problem with multiple tours, accumulation
over time, a ﬁnite processing capacity, a processing deadline, and
multiple prioritized objectives. To the best of our knowledge, ours
is the ﬁrst study that addresses this problem, although the following well-studied routing problems possess similar characteristics
that deserve to be discussed.

The Team Orienteering Problem (TOP) aims to route a number
of vehicles through a set of nodes each of which contains a ﬁxed
reward (i.e., no accumulation by time), in order to maximize the
total collected reward while ensuring that all vehicles return to
the pre-determined nodes within a given time limit. Two exact
algorithms have been proposed for the TOP; one that uses column
generation (Butt and Ryan, 1999) and another one that uses a
branch-and-price algorithm (Boussier et al., 2007), to solve smallto moderate-sized instances. A survey of heuristic methods is provided in Vansteenwegen et al. (2009).
The Inventory Routing Problem (IRP) is concerned with the distribution of products from a single facility to a set of nodes to satisfy customer demand over a given planning horizon. The objective
of IRP is to minimize the operating costs, which consists of transportation and inventory holding costs (Moin and Salhi, 2007). Differently from the IRP, in the CfPP, accumulated items do not incur
holding costs and the primary objective is to maximize the daily
processed amount. Both IRP and CfPP combine a temporal element
(node visit times) with a spatial element (vehicle routing), but
there is no consideration of processing in IRP.
Inbound logistics problems arising in just-in-time or lean production systems are concerned with coordinating the material inﬂow with the production rate, similar to the CfPP. In a recent study,
Ohlmann et al. (2008) consider vehicle routing with time-windows
to collect components from suppliers in a lean production system.
The objective is to minimize the total traveling cost subject to
inventory limitations.
While the above listed family of problems have similarities to
CfPP, we can summarize the basic differences of CfPP from these
problems as follows: (i) The main objective is to maximize the processed amount by a deadline. Operational costs are of secondary
concern since a case of unprocessed items will have serious consequences, e.g., the laboratory may lose the business of the corresponding site. (ii) The processing rate at the processing facility is
explicitly modeled. (iii) The workload at each site accumulates
over time, so that the collected amounts depend on the node visit
times. (iv) The tours have to be designed simultaneously due to the
signiﬁcant dependency between the tours. In every tour, the return
time of the vehicle to the processing facility, as well as the collected amount affect the processed amount up to the return time
of the vehicle from the next tour. Furthermore, due to accumulation, the amount collected from a node in a tour depends on the
last visit time of that node.
In this paper, we prove that CfPP is NP-hard, and introduce a
Mixed Integer Programming (MIP) formulation for the problem.
We characterize some properties of optimal solutions and identify
rules to eliminate feasible solutions that are likely to be suboptimal. Based on these results, we develop a heuristic approach that
solves the MIP model with additional constraints that reduce the
solution space. To evaluate the performance of this approach, we
provide methods to bound the two objectives. For the challenging
task of generating strong lower bounds on the daily transportation
cost for a given daily processed specimen level, we derive valid
inequalities and develop two relaxed MIP models that rely on these
inequalities and an alternative ﬂow formulation. We conduct computational experiments on realistic test instances to demonstrate
the effectiveness of our approach. We also extract insights on key
problem parameters and their effects on the solutions by further
experiments.
The remainder of this paper is organized as follows. In Section 2,
we provide the problem deﬁnition and some propositions. Section 3
derives an upper bound on the daily processed amount. In Section 4, we provide an MIP formulation, and describe two relaxations of the MIP to generate lower bounds on daily
transportation cost for a given daily processed amount in Section 5.
Section 6 presents our approach to ﬁnd a feasible solution from the

E. Yücel et al. / European Journal of Operational Research 227 (2013) 503–514

MIP formulation. Results from our computational analysis on realistic data are given in Section 7. Finally, Section 8 summarizes the
main contributions and suggests possible extensions.
2. Problem description and notation
In this section, we provide a formal description of the CfPP.
We assume that items accumulate between time 0 and time
se, and processing can be performed at the processing facility between time 0 and time sf. The problem is deﬁned on a directed
graph G = (N+, A), where N+ = {0, 1, 2, . . . , n} is the node set and
A = {(i, j)j i, j 2 N+} is the arc set. Each arc (i, j) is associated with
non-negative travel time tij and distance dij. We assume that
both the distances and the travel times are metric. Node 0 represents the processing facility, and N = N+n{0} is the set of sites.
Items accumulate at each site i 2 N at a constant and known rate
of ki units per unit time. Without loss of generality, we label the
sites so that k1 6 k2 6    6kn. Items are processed at the processing facility at a constant and known rate of l units per unit
time. We let Ii(t) represent the amount of items at site i at time
t, where i 2 N, and I0(t) denote the amount of items waiting in
the queue or undergoing processing at the processing facility
at time t.
We consider the decisions regarding a single vehicle dedicated to
serve the node set, N+. The vehicle is assumed to have sufﬁciently
large capacity to transfer the daily volume of all sites, which is a realistic assumption for clinical laboratory logistics. We assume that the
vehicle is positioned at the processing facility at time 0.
Appendix A summarizes all of the notation used throughout the
paper.
Let us denote the set of feasible solutions for a CfPP instance by
G. For a solution g 2 G, let P represent the daily processed amount,
and C represent the daily transportation cost, which is directly proportional to the distance traveled. The objective of CfPP is ﬁrst to
maximize the daily processed amount over the set G, and then to
select a solution with minimum transportation cost among the
ones with maximum daily processed amount. We call such solutions optimal.
For a CfPP instance, the daily processing capacity of the processing unit, i.e., the maximum amount of items that can be processed
between times 0 and sf, is equal to sfl. The daily accumulated
amount, i.e., the amount of items accumulated at all sites in one
P
day, is equal to se i2N ki , all of which must be transported to the
processing facility.
Deﬁnition 1. The workload level, a, for a CfPP instance is deﬁned as
the ratio of the daily accumulated amount to the daily processing
capacity. That is,

a¼

se

P

i2N ki

sf l

:

ð1Þ

In systems with heavy workload (i.e., large a), it may not be
possible to process all the daily volume by sf. On the other hand,
when the workload is relatively low (i.e., small a), there may be
many solutions that can process all of the daily volume on time,
in which case minimization of the daily transportation cost gains
importance.
Let us consider a small illustrative example. Suppose that

se = 540 and sf = 1200 minutes, the processing rate is l = 100 items
per hour and there are four sites with accumulation rates of k1 = 7,
k2 = 30, k3 = 50, and k4 = 106 items per hour. Symmetric distances
are given as d01 = 36, d02 = 24, d03 = 15, d04 = 25, d12 = 15, d13 = 22,
d14 = 15, d23 = 18, d24 = 10, and d34 = 11 kilometer. For simplicity,
we take tij = dij assuming that the speed of the vehicle is 1 kilometer per minute. In this CfPP instance, the daily volume accumulated

505

P

in all sites is se i2N ki ¼ 1737, whereas the daily processing capacity is sfl = 2000. Therefore, a = 1737/2000 = 0.869. This workload
level indicates that it may be possible to process all items depending on the transportation decisions.
In systems where all of the daily volume cannot be processed on
time, the unprocessed items that remain at the end of processing
(i.e., at time sf) may be backlogged to be processed in the next
day, i.e., I0(0) > 0 for the next day. In this paper, as we solve a single-day problem, we assume I0(0) = 0. Section 7.2.5 analyzes a multi-day problem with I0(0) > 0.
A solution for a CfPP instance has two components. The ﬁrst component is the sequence of nodes to be visited throughout the day. In
other words, a solution can be represented as a concatenation of
tours, where each tour starts and ends at the processing facility
and visits a subset of sites in N. Since the vehicle can wait at a node,
the second component of a CfPP solution provides the waiting times
at the visited nodes. Hence, the travel times between nodes and
waiting times at the nodes determine the node visit times. In order
for a solution to be feasible, it should visit all sites after time se to collect all the remaining accumulated items, and should be completed
by time sf. We note that the vehicle is allowed to be touring at time
se. For our example, if the tours performed by the vehicle throughout
the day are 0-3-4-1-0, 0-4-2-0, 0-3-0, and 0-4-1-2-0, then the corresponding sequence is 0-3-4-1-0-4-2-0-3-0-4-1-2-0. There might be
more than one feasible solution for this sequence. One such solution
is 0-3-4-1-0(153)-4-2-0(251)-3-0-4-1-2-0, where the numbers in
parentheses denote the waiting times in minutes at the corresponding nodes. The vehicle waits only at the processing facility in this
solution.
An optimal solution for our example is 0(74.7)-4-3-0(142.1)-4-30(221.2)-3-4-1-2-0, with P = 1737 and C = 182. In this solution, all of
the daily accumulated amount is processed. The timeline representation of the solution and the amount of unprocessed items at the
processing facility (i.e., I0(t)) in time interval [0, sf] are illustrated
in Fig. 1. We see that the processor remains idle during the ﬁrst tour
and for a short interval during the second tour.
We next investigate the problem difﬁculty and certain properties of optimal solutions.
Proposition 2.1. CfPP is NP-hard. (A proof via reduction from the
Traveling Salesman Problem (TSP), which is shown to be NP-hard
(Golden et al., 1987) is given in Appendix C available as part of the
Online Supplement.)
The set of feasible solutions, G, may be extremely large even for
problems with limited number of nodes (e.g., n P 10). In order to
simplify the search for optimal solutions in a large feasible solution
space, we investigate the structure of optimal solutions. The following two propositions allow us to concentrate on certain types
of solutions.
Proposition 2.2. There exists an optimal solution in which the vehicle
does not wait at any collection site. (Proof is provided in Appendix D1
available as part of the Online Supplement.)
Proposition 2.3. There exists an optimal solution in which sites are
not visited more than once in any tour. (Proof is provided in Appendix
D2 available as part of the Online Supplement.)
Since any solution that does not satisfy these properties can be
converted to one with these properties without any loss of performance, we search only for such solutions.
3. An upper bound on daily processed amount
We next provide a strong upper bound for the daily processed
amount. We use this upper bound to analyze the performance of

506

E. Yücel et al. / European Journal of Operational Research 227 (2013) 503–514

Fig. 1. The timeline representation of an optimal solution for the example.

our solution approach with respect to the ﬁrst-level objective of
maximizing daily processed amount.
The daily processed amount, P, cannot exceed the daily accumulated amount or the daily processing capacity. Therefore,
 P

min se i2N ki ; sf l provides an upper bound for P. However, this
bound may be quite loose, since it does not consider the time
required for the transportation of items from the sites to the processing facility. In order to obtain a tighter bound, we relax the
constraint on the number of vehicles and deﬁne UBP as the processed amount until sf.
When there is no limit on the number of vehicles available, a
vehicle can be dispatched to each site every time unit. Then, the
ﬁrst delivery of items to the processing facility occurs at time
mini2N{t0i + ti0}. If I0(0) = 0, then the processing facility will be
idle until time mini2N{t0i + ti0}. Since the accumulation of items
at the sites ends at time se, the latest arrival of items to the
processing facility occurs at time t = se + maxi2N{ti0} from the
farthest site. We give a pseudocode to calculate UBP in
Algorithm 1.

Table 1
The relation between UBP and UB0P for the example when a = 0.8, 1, and 1.2.
P
a
lsf
UBP
UB0P
i2N ki se

Algorithm 1. UBP Algorithm

4. The mathematical model

Input: N+, tij, ki
Output: UBP
1: Set UBP
0, I0
0.
2: for t 2 {1, 2, . . ., sf} do
3: Set c
0. // c denotes the amount of items that arrive at
time t.
4: for i 2 N do
5:
if t = t0i + ti0 then
6:
Set c
c + t0i  ki. // The ﬁrst item arrival from node i.
7:
else if t0i + ti0 < t < se + ti0 then
8:
Set c
c + ki.
9:
else
10:
// Do nothing: no item arrival from node i.
11:
end if
12: end for
13: Set P
min{I0, l}. // Calculate the amount of processed
items during time t.
14: Set I0
I0 + c  P. // Update the amount of unprocessed
items in the processing facility.
15: Set UBP
UBP + P.
16: end for

0.8
0.869
1
1.2

1737
1737
1737
1737

2171
2000
1737
1448

1737
1737
1737
1448

1737
1737
1692
1410

We can demonstrate how UBP improves the trivial bound
 P

UB0P ¼ min se i2N ki ; sf l using the previously discussed example,
where a = 0.869. We also analyze the same system for a values of
0.8, 1, and 1.2 by changing l accordingly (see Table 1). According to
the results, UBP improves UB0P around 3% when a P 1, even for this
small problem instance.

In this section, we develop an MIP model that determines the
number of tours to be performed by the vehicle until time sf, as
well as the start and end times of the tours throughout the day.
It also selects the sites to be visited and their sequence in each
tour. The number of tours that the vehicle can perform between
times 0 and sf is, naturally, bounded. Accordingly, we assume
that the vehicle performs at most j1 tours during [0, se) and at
most j2 tours during [se, sf], where j1 and j2 are parameters
to be determined by the decision makers. It is sufﬁcient to visit
a site just once after se, as accumulation stops at this time.
Hence, a natural bound on j2 is j2 6 n. However, the actual
number of tours is determined by the model, since we allow
empty tours in the formulation.
To calculate the accumulated amount at each node, we keep
track of the last visit time of each node. Moreover, we need to
know whether a node is visited before time se or not since accumulation stops at se. The model ensures that all accumulated items are
collected by the vehicle. The model allows the vehicle to wait only
at the processing facility before se due to Proposition 2.2, and does
not allow visiting a site more than once in a tour due to Proposition
2.3. We deﬁne the parameters, decision variables and constraints
as follows.

507

E. Yücel et al. / European Journal of Operational Research 227 (2013) 503–514

Parameters
j1
Maximum number of tours that can be performed
during [0, se)
j2
Maximum number of tours that can be performed
during [se, sf]
Decision variables
Xijk
Binary variable indicating if node j is visited
immediately after node i in tour k ((i, j) 2 A,
k = 1, 2, . . . , j1 + j2). If tour k is an empty tour, then
X00k = 1 and X0ik = 0, Xi0k = 0, Xijk = 0, "i, j 2 N
Yik
Binary variable indicating if node i is visited in tour k
(i 2 N, k = 1, 2, . . . , j1 + j2)
Tik
visit time of node i at tour k (i 2 N+,
k = 1, 2, . . . , j1 + j2). If node i is not visited in tour k, it
denotes the last visit time of node i before tour k (i.e.,
Tik = Ti, k1). T0k denotes the starting time of tour k
Rk
Return time of the vehicle from tour k to processing
facility (k = 1, . . . , j1 + j2); R0 = 0
Eik
collected amount from site i in tour k (i 2 N,
k = 1, 2, . . . , j1 + j2)
Sk
Total collected amount in tour k (k = 1, 2, . . . , j1 + j2)
Lik
Auxiliary variable used to calculate Eik
(i 2 N, k = 1, 2, . . . , j1 + j2)
Wk
Waiting time of the vehicle at the processing facility
at the beginning of tour k (k = 1, 2, . . . , j1 + j2)
Qk
Amount of unprocessed items at the processing
facility at the end of tour k (k = 0, 1, 2, . . . , j1 + j2). Q0
is a parameter which represents the number of
unprocessed items at the beginning of the day, i.e.,
Q0 = I0(0)
Uk
amount of processed items between Rk and Rk+1
(k = 0, 1, 2, . . . , j1 + j2  1)
U j1 þj2 amount of processed items between Rj1 þj2 and sf

8i 2 N; k ¼ 1; 2; . . . ; j1 þ j2 ;
8i 2 N; k ¼ 1; 2; . . . ; j1 þ j2 ;
Li0 ¼ 0; 8i 2 N;
Eik ¼ ki ðLik  Li;k1 Þ; 8i 2 N; k ¼ 1; 2; . . . ; j1 þ j2 ;

Lik 6 T ik ;

ð5aÞ

Lik 6 se ;

ð5bÞ

jX
1 þj2
k¼1

Sk ¼

Eik ¼ ki se ;
X
Eik ;

8i 2 N;

ð5cÞ
ð5dÞ
ð5eÞ

8k ¼ 1; 2; . . . ; j1 þ j2 ;

ð5fÞ

i2N

U k 6 lðRkþ1  Rk Þ;

8k ¼ 0; 1; . . . ; j1 þ j2  1;

U j1 þj2 6 lðsf  Rj1 þj2 Þ;
Uk 6 Q k ;

8k ¼ 0; 1; . . . ; j1 þ j2 ;

Q k ¼ Q k1  U k1 þ Sk ;

ð6aÞ
ð6bÞ
ð6cÞ

8k ¼ 1; 2; . . . ; j1 þ j2 ;

X ijk ; Y ik 2 f0; 1g; 8i; j 2 Nþ ; k ¼ 1; 2; . . . ; j1 þ j2 ;

ð7Þ
ð8Þ

8i 2 N þ ;

T ik ; Rk ; Eik ; Sk ; Lik ; W k ; Q k ; U k P 0;
k ¼ 1; 2; . . . ; j1 þ j2 :

ð9Þ

Using these constraints, we deﬁne two models. Since the ﬁrst level
objective is to maximize the daily processed amount, model M1 is
solved ﬁrst:

M1 : Maximize

P¼

jX
1 þj2

Uk

k¼1

subject to Constraints

ð2aÞ  ð9Þ:

The solution of M1 provides the optimal value of P, denoted by P⁄,
which is then used as a lower bound for the daily processed amount
in model M2. Hence, model M2 minimizes the daily transportation
cost subject to this additional constraint:

M2 : Minimize

C¼

jX
1 þj2

X

dij X ijk

k¼1 ði;jÞ2E

Constraints

X

X 0jk ¼ 1;

8k ¼ 1; 2; . . . ; j1 þ j2 ;

ð2aÞ

j2Nþ

X

subject to Constraints ð2aÞ  ð9Þ
jX
1 þj2
U k P P :

ð10Þ

k¼1

X j0k ¼ 1;

8k ¼ 1; 2; . . . ; j1 þ j2 ;

ð2bÞ

j2Nþ

X

X ijk ¼ Y ik ;

8i 2 N; k ¼ 1; 2; . . . ; j1 þ j2 ;

ð2cÞ

j2Nþ ;j–i

X

X

X ijk 

j2Nþ ;j–i

X jik ¼ 0;

8i 2 N; k ¼ 1; 2; . . . ; j1 þ j2 ;

ð2dÞ

j2Nþ ;j–i

Rk ¼ Rk1 þ W k þ

X

tij X ijk ;

8k ¼ 1; 2; . . . ; j1 þ j2 ;

ð3aÞ

ði;jÞ2A

T 0k ¼ Rk1 þ W k ;

8k ¼ 1; 2; . . . ; j1 þ j2 ;

ð3bÞ

Rj1 þj2 6 sf ;

ð3cÞ

T 0;j1 6 se ;

ð3dÞ

T 0;j1 þ1 P se ;

ð3eÞ

T ik þ t ij  T jk 6 Mð1  X ijk Þ;

8ði; jÞ 2 A; k ¼ 1; 2; . . . ; j1 þ j2 ; ð4aÞ

T ik þ t ij  T jk P Mð1  X ijk Þ; 8ði; jÞ 2 A; k ¼ 1; 2; . . . ; j1 þ j2 ; ð4bÞ
T i;kþ1  T ik 6 MY i;kþ1 ;
T i;kþ1 P T ik ;

8i 2 Nþ ; k ¼ 1; 2; . . . ; j1 þ j2  1;

8i 2 Nþ ; k ¼ 1; 2; . . . ; j1 þ j2  1;

ð4cÞ
ð4dÞ

Constraints (2a) and (2b) require that each tour begins and ends
at the processing facility. The outgoing degree of any node i must
be equal to Yik through Constraint (2c). Constraint (2d) implies that
the ﬂow is balanced at each node.
Constraints (3a)–(3e) deﬁne the relation between tour return
times, waiting times, and tour start times. Constraints (3a) and
(3b) calculate the return time and the starting time of a tour,
respectively. Constraint (3c) guarantees that the last tour should
be completed by sf and Constraints (3d), (3e) limit the number of
tours before se by j1.
Constraints (4a)–(4d) deﬁne the node visit times. If the vehicle
visits node j immediately after node i in tour k, Constraints (4a) and
(4b) together restrict the vehicle to be at node j at time Tik + tij.
These constraints also eliminate subtours. Constraints (4c) and
(4d) ensure that if node i is not visited in tour k, Tik equals the last
visit time before that tour.
Next, we deﬁne Constraints (5a)–(5e) to calculate the collected
amounts from the visited nodes. Since the accumulation at the sites
ends at time se, ki(min{Tik, se}  min{Ti, k1, se}) gives the collected
amount from node i in tour k. Note that, if node i is not visited in tour
k, then Tik = Ti, k1, and hence, ki(min{Tik, se}  min{Ti, k1, se})
is zero. Since this is nonlinear, we introduce a set of linear constraints that does not calculate the exact value of the collected
amount from a visited node, but guarantees that the total amount
collected from a node by the visit time cannot be greater than the
accumulated amount up to that time. First, we deﬁne auxiliary

508

E. Yücel et al. / European Journal of Operational Research 227 (2013) 503–514

variables, Lik’s, which cannot be larger than se and Tik through Constraints (5a), (5b). Note that, Li0 is set to zero by Constraint (5c). Then,
we set k(Lik  Li, k1) as the collected amount from node i in tour k
through Constraint (5d). Constraint (5e) ensures that all of the items
accumulated at the nodes are collected. Finally, Constraint (5f) calculates the collected amount in each tour.
The amount of processed items between the return times of two
consecutive tours, Uk, cannot exceed the amount of unprocessed
items, Qk, and processing capacity, l(Rk+1  Rk). This condition is
satisﬁed via Constraints (6a)–(6c). Constraint (7) balances the
queue size at the processing facility. Constraint (8) deﬁnes the binary variables in a tour, while Constraint (9) dictates nonnegativity
of all variables.
Our computational experiments showed that both M1 and M2 can
be solved to optimality only for problems with low workload levels
(e.g., a 6 0.8) and limited number of nodes (e.g., n 6 10), but verifying optimality takes extensive computation time. In particular, M2 is
signiﬁcantly more difﬁcult than M1. To strengthen model M2, we
generate a valid inequality using a lower bound for the daily processed amount, P⁄. In order to process at least P⁄ items, the vehicle
should complete the ﬁrst tour by time sf  P⁄/l. That is, R1 6 sf  P⁄/
l. As the vehicle transports S1 items to the processing facility at the
end of tour 1, the processor can process at most S1 items up to
R2. Therefore, the second tour cannot be completed later than
sf  (P⁄  S1)/l so that the remaining P⁄  S1 items can be processed
in (P⁄  S1)/l time units. That is, R2 6 sf  P⁄/l + S1/l. Continuing
like this, we obtain the following valid inequalities.
Proposition 4.1. The following inequalities are valid for model M2:

Rk 6 sf  P =l þ

k1
X
Sp =l;

8k ¼ 1; 2; . . . ; j1 þ j2 :

ð11Þ

p¼1

(Proof is provided in Appendix D.3, which is available as part of the
Online Supplement.)
Proposition 4.1 is used in relaxations of model M2 to generate
lower bounds. In addition, we tested the inequalities on a set of instances provided in Section 7 and observed that it provides a slight
improvement (of around 1–2%) on the best found objective value of
M2 at the end of a four-hour (i.e., 14,400 seconds) run time limit.
Hence, we included these inequalities in all of our computational
tests.
5. Lower bounds on daily transportation cost
In this section, we bound the daily transportation cost, C, under
the condition that at least P⁄ items should be processed by time sf.
We present two relaxations of model M2. The corresponding MIP
formulations are solved with a run time limit and the best found
lower bound is kept. The highest bound is used to evaluate the performance of our solution approach in terms of daily transportation
cost.
In model M2, having Constraint (10) narrows the set of feasible
solutions dramatically. Especially when the workload level is high,
there may be few solutions that achieve P⁄. Thus, the branch and
bound algorithm has difﬁculty in ﬁnding a candidate feasible solution. To overcome this, we relax Constraint (10) and replace it by
the valid inequalities given in Constraint (11). As a result, the relaxed model cannot guarantee that P⁄ items are processed by time
sf. Another reason for the difﬁculty of solving M2 is due to the bigM constraints, i.e., Constraints (4a) and (4b). We eliminate these
constraints, which calculate the exact values of Tik variables and
prevent subtours. Instead, we utilize a single commodity ﬂow formulation and add the following constraints to the MIP:

T ik 6 Rk  t i0 ;

8i 2 N; k ¼ 1; 2; . . . ; j1 þ j2 ;

ð12Þ

X

X

Gijk 

j2N þ ;j–i

Gjik ¼ ki ðLik  Li;k1 Þ;

8i 2 N;

j2N þ ;j–i

k ¼ 1; 2; . . . ; j1 þ j2 ;
X
ki0 se ; 8ði; jÞ 2 E;

ð13aÞ

Gijk 6 X ijk

i0 2N

k ¼ 1; 2; . . . ; j1 þ j2 ;
Gijk 6 Sk ;
Gijk P 0;

8i 2 N; 8j 2 Nþ ; k ¼ 1; 2; . . . ; j1 þ j2 ;
8ði; jÞ 2 E; k ¼ 1; 2; . . . ; j1 þ j2 ;

ð13bÞ
ð13cÞ
ð13dÞ

where Gijk denotes the amount of items carried by the vehicle while
traveling from node i to node j in tour k (i, j 2 N+ and
k = 1, 2, . . . , j1 + j2).
Constraint (12) puts a bound on node visit time variables Tik. We
utilize Constraints (13a)–(13d) for subtour elimination. For this
purpose, single commodity ﬂow variables Gijk are deﬁned akin to
the Gavish-Graves formulation for the TSP (Gavish and Graves,
1978). Constraint (13a) balances the ﬂow at each node i 2 N. Note
that these constraints do not calculate the collected amount exactly. The ﬂow between two nodes in a tour can be positive only
if the arc between these nodes is traveled in the corresponding tour
by Constraint (13b), and the amount of the ﬂow is bounded by the
amount collected in that tour by Constraint (13c). Constraint (13d)
is the nonnegativity constraint for the ﬂow variables. This MIP constitutes our ﬁrst relaxation of M2.
We obtain a second relaxation by excluding the calculations related to the processed amount between consecutive tours from the
ﬁrst relaxation. We eliminate the decision variables Uk and Qk, as
well as the related Constraints (6a)–(6c) and (7).
The computational complexity of the relaxations is further reduced by concentrating on the decisions regarding the part of the
problem before se by setting j2 = 0 and adding the minimum transportation cost incurred to visit all sites after se to the objective value in the relaxed models. We note that the minimum
transportation cost incurred to visit all sites after se can be found
by solving a mathematical model, which differs from a TSP model
by only its objective function. Since the vehicle might be on a tour
at time se, this model aims to ﬁnd a minimum cost Hamiltonian
path ending at node 0 rather than a TSP cycle. Therefore, the objecP
P
0
0
tive function of this model should be
i;j2N þ xij dij 
i2Nþ x0j d0j ,
0
where xij is the binary variable indicating if node j is visited immediately after node i in the TSP path/tour.
Our experiments show that despite signiﬁcant improvements in
the progress of the branch-and-bound algorithm, even these relaxations are difﬁcult to solve to optimality in reasonable time. Hence,
we put a time limit for their solution and record the best found
lower bound. These bounds bring 74% improvement on the average
over the best lower bound obtained by solving M2 within its run
time limit.

6. Solution approach
In this section, we propose two approaches that eliminate feasible solutions that are likely to be inferior without signiﬁcantly
compromising from optimality. Both approaches utilize the proposed models with some modiﬁcations. The ﬁrst approach is based
on restricting the vehicle to complete at most j1 tours before se
and to perform only one tour during (se, sf) that starts after se
and visits all sites (Section 6.1). In the second approach, for each
tour, a set of sites with low accumulation rates are identiﬁed so
that the vehicle cannot visit these sites in the corresponding tour
(Section 6.2). Although these approaches can be used individually,
their joint use has generated better results for speciﬁed run time
limits in our experiments (Section 7.2.1).

509

E. Yücel et al. / European Journal of Operational Research 227 (2013) 503–514

6.1. Searching for solutions with a ﬁnal TSP tour
To cope with the computational difﬁculty of the proposed models, we ﬁrst restrict the vehicle to perform only one tour that visits
all nodes after se with duration s and cost h. Since items do not
accumulate after se, the node sequence of the ﬁnal tour does not
affect the collected amount, and so, it is independent of the accumulated amounts at sites. Therefore, routing and scheduling decisions before and after time se become independent. Hence, we ﬁrst
solve a TSP with the objective of minimizing the total traveling
time (not distances) to visit all nodes. This constitutes the last tour.
We exclude the visiting variables of the last tour from models M1
and M2, but in order to calculate the collected and processed
amounts we still keep the remaining variables corresponding to
the last tour. We generate models M T1 and M T2 from M1 and M2, correspondingly, according to the following.
 j2 = 1.
 Constraints (2a)–(2d), (3a), (3b), (4a), (4b), and (8) are deﬁned
for k = 1, 2, . . . , j1 rather than k = 1, 2, . . . , j1 + j2.
 Constraints (3c)–(3e) are replaced with

Rj1 þ1 P se þ s;

ð14aÞ

Rj1 6 se ;

ð14bÞ

where Constraint (14a) guarantees that the start time of the ﬁnal
TSP tour is greater than or equal to se and Constraint (14b) ensures
that at most j1 tours are completed before se.
 Variables Lik, "i 2 N, k = 1, 2, . . . , j1 + 1, are removed so that Constraints (5a), (5b) are deleted and Constraints (5c) and (5d) are
replaced with

T i0 ¼ 0;

8i 2 N;

ð15aÞ

8i 2 N; k ¼ 1; 2; . . . ; j1 ;
6 ki ðse  T i;j1 Þ; 8i 2 N;

Eik 6 ki ðT ik  T i;k1 Þ;

ð15bÞ

Ei;j1 þ1

ð15cÞ

where Constraints (15b) and (15c) do not calculate the exact value
of the collected amount from a visited node, but guarantees that the
total amount collected from a node by its visit time cannot exceed
the accumulated amount after its previous visit.
After solving M T2 , the cost of the ﬁnal TSP tour, h, is added to the
traveling cost.
6.2. Node ﬁltering heuristic
In this section, we propose a node ﬁltering heuristic by adding
constraints to models M1 and M2. These constraints ensure that
nodes with low accumulation rates are not allowed to be visited
in each tour. By ﬁltering a node, we defer collecting its accumulated amount to a subsequent tour. In order to identify the nodes
to be ﬁltered in a tour, we consider the remaining processing
capacity and workload. We adjust the remaining processing capacity by assuming that the deferred amount will be processed by sf. If
this remaining capacity is sufﬁcient to achieve the upper bound on
P, we allow ﬁltering this node.
Speciﬁcally, to decide on which nodes to ﬁlter in tour k, we consider the following quantities. At the return time of tour k  1 (i.e.,
Pk2
Rk1), the total amount of items processed is
r¼1 U r . Therefore,
Pk2
UBP  r¼1 U r is an upper bound on the processed amount in the
time interval (Rk1, sf], for any k 6 j1. Moreover, the remaining
processing capacity is (sf  Rk1)l, while the total accumulation
amount in site i is kise. Then, the adjusted remaining capacity is
(sf  Rk1)l  seki. If the following inequality holds,

ðsf  Rk1 Þl  se ki P UBP 

k2
X
Ur ;
r¼1

ð16Þ

then we still have enough capacity to attain the upper bound even if
site i is not visited in tour k. Hence, site i can be ﬁltered out in tour k.
Recall that the sites are indexed in non-decreasing order of the
accumulation rate. We can generalize the above inequality to a
Node Filtering Rule in order to ﬁnd a threshold value, m⁄, such that
the ﬁrst m⁄ nodes are ﬁltered out in tour k.
Deﬁnition 2 (Node Filtering Rule). The vehicle is not allowed to
visit sites i1 ; i2 ; . . . ; im in tour k, where m⁄ is the largest index such
that

ðsf  Rk1 Þl  se

m
k2
X
X
kil P UBP 
Ur :

ð17Þ

r¼1

l¼1

We incorporate the node ﬁltering approach to models M1 and
M2, and obtain models M F1 and M F2 by modifying them as follows.
P
We deﬁne Km ¼ m
i¼1 ki and a new decision variable, Zmk.
Z mk : binary variable indicating if every node i with i 6 m is filtered out in tour k
ð1 6 m 6 n; k ¼ 1; 2;. . .; j1 Þ:

Next, we add two additional constraints to the formulation:

Y ik 6 1  Z mk ;

8i 2 N; 1 6 m 6 n; i 6 m; k ¼ 1; 2; . . . ; j1 ; ð18aÞ

MZ mk P ðsf  Rk1 Þl  UBP þ

k2
X
U r  s e Km ;

1 6 m 6 n;

r¼1

k ¼ 1; 2; . . . ; j1 :

ð18bÞ

Constraint (18a) guarantees that if Zmk = 1, then a site i, where
i 6 m, is ﬁltered out in tour k = 1, 2, . . . , j1. Constraint (18b) determines the value of m⁄ for tour k = 1, 2, . . . , j1 as deﬁned in the Node
Filtering Rule.
For the illustrative example, the node ﬁltering approach generates an optimal solution of 0(74.7)-4-3-0(142.1)-4-3-0(221.2)-3-41-2-0, with P = 1737 and C = 182. According to the Constraints
(18a) and (18b), node 1 is ﬁltered out in the tours before se.
Although node 2 is not ﬁltered out, the models decide not to visit
node 2 before se.
7. Computational experiments
In this section, we analyze the characteristics and performance
of the solutions obtained by the proposed solution approach using
numerical experiments with realistic data instances. This section
starts with a description of our CfPP instances and the performance
metrics. General results on the performance of our solution approach and detailed analysis on solution characteristics follow. Finally, managerial insights are provided.
7.1. Data instances and performance metrics
Using a real-life data set that we have obtained from a clinical
testing laboratory that we have been collaborating with, we constructed 12 problem instance sets. Each instance set is deﬁned
on a different complete graph, G = (N+, E), with nodes that correspond to the clients of the clinical laboratory served by a single
vehicle in a speciﬁed region. The number of nodes in N+ varies between 10 and 18. The accumulation rate ki for each node i 2 N is
determined according to empirical data such that the majority of
the volume comes from a small portion of the clients. The distance
dij for each arc (i, j) 2 E is the shortest path distance on the roads
obtained from Google Maps. The traveling time, tij, for each arc
(i, j) 2 E is taken as tij = dij/t + sj, where t denotes the constant speed
of the vehicle (miles per min) and sj denotes the service time at
node j (in minutes). The service time, sj, represents the amount
of time that the driver spends at node j to pick up the accumulated

510

E. Yücel et al. / European Journal of Operational Research 227 (2013) 503–514

items. We randomly generated the sj values from a uniform distribution deﬁned on the interval [2, 5] if kj 6 20 (items per hour), and
on the interval [6, 9] otherwise, based on empirical data.
For each problem instance set, we generated three workload
levels (a = 0.8, 1.0, 1.2) by varying l. The value of l for the correP
sponding instance is deﬁned using the equality se i ki ¼ alsf . As
a result, there are 36 data instances in total.
Typically, the sites open at 8 a.m. and close at 7 p.m. Processing
at the facility generally ends at 5 a.m. Thus, se is set to 660 minutes,
and sf is to 1260 minutes, considering minutes as the time unit.
The initial queue level at the processing facility is set to zero for
each instance. The data sets can be accessed at http://home.ku.edu.tr/eyucel/Research/DatasetCfPP.zip.
The computational experiments were performed on a Xeon
E5520 @ 2.27 gegahertz processing unit with 48 gigabytes of memory. GAMS 23.3 was used to create the models. CPLEX 12.2 was employed to solve them with the following options turned on:
threads 0; parallelmode 1. This setting enables CPLEX to run
as a multi-threaded application and to distribute the computing
load to as many as eight logical cores of the Core i7 Quad-core processor. The run time limit for each M1 model was set to two hours
(7200 seconds), while each M2 model was run for 4 hours
(14,400 seconds). A run time limit of 4 hours (14,400 seconds)
was set for each relaxation proposed in Section 5. In all models,
j1 was set to 10 as it was observed to be large enough.
7.2. Experimental results
In this section, we ﬁrst compare the solutions found by the proposed models in terms of the daily processed amount and the daily
transportation cost for a set of instances with different workload
levels. Next, we report the solutions of the best performing model
with respect to the deviations from the upper bound presented in
Section 3 on the daily processed amount and the best lower bound
obtained from the two relaxations given in Section 5 on the daily
transportation cost. In addition, we investigate the effect of the
workload level and different accumulation rate patterns on the
solutions and the problem difﬁculty. Finally, we analyze the properties of the best found solutions to derive some managerial
insights.
7.2.1. Comparison of the proposed models
By determining the appropriate sites to be ﬁltered out in each
tour, the node ﬁltering approach and the assumption of a ﬁnal
TSP tour narrow the search space, and thus, improve the best found
solution by the branch-and-bound process within the given time
limit. In order to demonstrate its effectiveness, for all of the 36 instances we solve the models that seek four different types of solutions: (i) with a ﬁnal TSP tour and the node ﬁltering constraints
TF
(referred to as M TF
1 and M 2 ), (ii) without a ﬁnal TSP tour and with
node ﬁltering constraints (M F1 and M F2 ), (iii) with a ﬁnal TSP tour
and without node ﬁltering constraints (M T1 and M T2 ), and (iv) without a ﬁnal TSP tour and node ﬁltering constraints (M1 and M2). Table 2 provides the best found solution values at the end of a given
computation time limit (two hours for model M1 and four hours for
model M2) averaged among all instances corresponding to each
workload level. We note that Table 2 reports the results of
TF
F
F
MT1 ; M T2 ; M TF
1 , and M 2 with j1 = 10, and M 1 ; M 2 ; M 1 and M2 with
(j1, j2) = (10, 3) as these values perform the best in our experiments with different settings of j1 and j2.
Table 2 shows that all models are equally effective in ﬁnding the
maximal daily processed amount, P. However, in terms of the daily
transportation cost, C; M TF
2 clearly performs the best. Based on
these observations demonstrating the effectiveness of the node ﬁltering approach with a ﬁnal TSP tour, the remaining experiments
TF
are performed using models M TF
1 and M 2 . We refer to the heuristic

Table 2
The best found solution values of the models averaged over the 12 instance sets.

a
0.8
1
1.2
Avg.

M TF
1
Avg. P

M TF
2
Avg. C

M T1
Avg. P

M T2
Avg. C

M F1
Avg. P

MF2
Avg. C

M1

M2

Avg. P

Avg. C

5375
5188
4342
4968

183
356
315
285

5375
5188
4342
4968

212
384
334
310

5375
5187
4342
4968

227
410
350
329

5375
5186
4338
4966

213
381
335
310

approach based on solving these two models within the given time
limit as the TF heuristic hereafter.
7.2.2. Test results and performance of the TF heuristic
We now evaluate the performance of solutions obtained by the
TF heuristic using the upper bound on the daily processed amount
generated by the procedure in Section 3 and the lower bound on
the daily transportation cost generated from the MIP relaxations
in Section 5 with j1 = 10. For each instance, we report the following results.
P
C
K

v
f

DA
DU
GapP
GapC

Daily processed amount of the best found solution by
the TF heuristic
Daily transportation cost of the best found solution by
the TF heuristic
Number of tours performed up to time se for the best
found solution
Percentage of sites that are visited, averaged over K
P
tours for the best found solution (i.e., 100 Kk¼1 v k =n,
where vk is the number of sites visited in tour k)
Percentage of sites that are ﬁltered, averaged over K
P
tours for the best found solution (i.e., 100 Kk¼1 mk =n,
where the ﬁrst mk sites are ﬁltered out in tour k)
Daily achievement percentage, calculated as
 P

100P= se i2N ki
Daily capacity utilization percentage, calculated as
100P/(l sf)
Percentage gap between P and UBP, calculated as
100(UBP  P)/P
Percentage gap between C and LBC, where LBC is the
best bound found by CPLEX for the two relaxations at
the end of the given run time limit (i.e., 100(C  LBC)/
LBC)

The experimental results are reported in Table B.1 in Appendix
B and the aggregate results for all instances are provided in Table 3.
We see from Table 3 that the gap between the daily processed
amounts of the best found solution by the TF heuristic and the inﬁnite vehicle relaxation solution is 1.07% on the average, while the
maximum gap is 3.59%. This shows that this solution approach is
very effective in terms of maximizing the daily processed amount.
It also indicates that the inﬁnite vehicle relaxation provides tight
upper bounds on the test instances. The optimality gap for the daily transportation cost, GapC, is 18.06% on the average, while the
maximum gap is 29.52%. These relatively large gaps may be attributed to several factors: (1) the weakness of the lower bounds, (2)
the possibility that the best found solutions may be far from the
optimal, (3) inherent computational difﬁculty of the problem. Similar to researchers studying challenging routing problems such as
Table 3
Aggregate test results.

Avg.
Min.
Max.

K

v

6.78
3
10

14.9
7.14
25

(%)

f (%)

DA (%)

DU (%)

GapP (%)

GapC (%)

53.9
28.79
80.95

91.51
69.63
100.00

91.58
80.00
97.48

1.07
0.00
3.59

18.06
4.22
29.52

511

E. Yücel et al. / European Journal of Operational Research 227 (2013) 503–514

daily processed amount and the daily transportation cost are the
highest for a = 1.0, which generally represents a difﬁcult case.

Table 4
Average results over each workload level.

a

Avg.
P

Avg.
C

Avg.
K

Avg.
v (%)

Avg.
f (%)

Avg.
DA
(%)

Avg.
DU
(%)

GapP
(%)

Avg.
GapC
(%)

0.8
1.0
1.2

5375
5188
4342

183
356
315

3.50
8.92
7.75

15.48
15.59
15.15

58.52
51.46
50.29

100
96.51
80.77

80
96.51
96.93

0
1.84
1.45

6.88
25.60
23.79

IRP and TOP (Moin and Salhi, 2007; Archetti et al., 2007), we have
observed that ﬁnding a good lower bound for CfPP is very difﬁcult.
Although our problem relaxations given in Section 5 provide substantial improvement in achieving stronger bounds, there is still
room for further improvement.

7.2.3. Effect of workload level
In order to investigate the effect of the workload level on the
solutions and the problem difﬁculty, we aggregate in Table 4 the
results by workload level and provide averaged values corresponding to the same workload level. We see that the vehicle performs
less number of tours when a = 0.8, since a signiﬁcant portion of
the total accumulation can be processed after the ﬁnal TSP tour.
When a P 1.0, the vehicle performs more tours to prevent idleness
of the processing unit during the time interval (0, se]. Correspondingly, the daily transportation cost becomes higher. When a increases to 1.2, the vehicle starts to perform less number of tours
compared to the a = 1 case, since it collects a smaller volume up
to time se as processing the total accumulation is not possible. In
the best found solutions, independent of the workload level, the
vehicle visits about 15% of the sites in each tour on the average,
implying that it is generally sufﬁcient to visit only a small subset
of the nodes to collect ‘‘enough’’ items. In addition, at least 50%
of the sites are ﬁltered out on the average in each tour without
much loss from optimality. For small a, more nodes can be ﬁltered
out. For all instances with a = 0.8, we see that 80% of daily processing capacity utilization is sufﬁcient to process all of the daily accumulated amount. For the instances with a P 1, although all of the
daily accumulation cannot be processed, 96.72% of the daily processing capacity is utilized on the average. As expected, the average
DA values in Table 4 show that the percentage of items processed
on time decreases as the workload increases. On the other hand,
the processing capacity utilization increases in the workload level.
The gaps reported for the best found solutions in terms of both the

7.2.4. Effect of accumulation rate pattern
The accumulation rate pattern of the sites is an inﬂuential problem characteristic in CfPP since nodes are selectively visited during
the tours before se. To evaluate the performance of the TF heuristic
in settings with different accumulation rate patterns, we extend
our tests to new data sets. We pick the ﬁrst instance set in
Table B.1, and in addition to the original pattern (denoted by O),
we generate new instance sets by distributing the total daily accumulation randomly in four patterns: (i) Pareto Random (PR), where
20% of sites contribute to 80% of the whole accumulation; (ii) Pareto Closer (PC), where 20% of sites that are closest to the processing
facility accumulate 80% of the whole accumulation; (iii) Pareto Distant (PD), where the closest 80 percent of sites contribute to only
20% of the whole accumulation; and (iv) Identical (I), where all
accumulation rates are identical. In all of the patterns, the sum of
the accumulation rates are equal to that of the original one. For
the selected problem instance, the geographical locations of the
sites (nodes 1–14) and the processing facility (node 0) are illustrated in Fig. E1 in Appendix E, which is available as part of the Online Supplement. The accumulation rates, the traveling times to
processing facility, and the service times for each site for pattern
O are also provided in Table E.2 in Appendix E. In Fig. E1, the sites
are colored such that darker ones have a higher accumulation rate.
The experimental results reported in Table 5 indicate that the
performance of the TF heuristic, measured by average GapP and
GapC, decreases in the order of PC, PR, PD, and I. Since the accumulation rates are identical for all sites in pattern I, the ﬁltering rule
can ﬁlter out only a small percentage of sites (around 10%); thus,
the accelerating effect of the node ﬁltering approach on the
branch-and-bound algorithm deteriorates. In PC, which resembles
the original pattern observed at the clinical laboratory we have
been collaborating with, our approach shows the best performance
by ﬁltering nearly 50% of the sites. Although the percentage of ﬁltered sites are also nearly 50% in PD, the solution quality gets
worse especially in traveling cost (GapC = 30%). While the ﬁltered
node percentage is only 34% in pattern PR, our solution approach
still provides comparable average gaps (1.3% for the daily processed amount and 28.53% for the daily transportation cost).
In conclusion, we observe that the ﬁltering approach is effective
in the Pareto cases, which are probably the most realistic. The extreme case with identical accumulation rates, which is unlikely to
be seen in practice, results in relatively poor performance.

Table 5
Test results for the selected problem instance with different accumulation rate patterns.
Pattern

a

P

C

K

v

PC

0.8
1.0
1.2
Avg.

10,493
10,315
8612
9807

154
308
249
237

3
10
9
7.33

PR

0.8
1.0
1.2
Avg.

10,493
10,192
8511
9732

169
381
305
285

PD

0.8
1.0
1.2
Avg.

10,493
10,035
8391
9640

I

0.8
1.0
1.2
Avg.

10,493
9891
8323
9569

f (%)

DA (%)

DU (%)

GapP (%)

GapC (%)

9.52
25
22.2
18.92

59.67
53.00
39.51
50.72

100.00
98.30
82.07
93.46

80.00
98.30
98.48
92.26

0.00
0.85
0.66
0.50

7.69
26.75
10.67
15.04

3
8
9
6.67

16.7
31.3
38.1
28.67

28.57
50.00
25.40
34.66

100.00
97.13
81.11
92.75

80.00
97.13
97.33
91.49

0.00
1.99
1.90
1.30

6.96
44.87
33.77
28.53

223
419
367
336

4
8
6
6.00

16.1
31.3
39.3
28.87

65.75
59.25
25.07
50.02

100.00
95.63
79.97
91.87

80.00
95.63
95.96
90.53

0.00
2.56
2.31
1.62

5.19
48.58
37.45
30.41

303
511
451
422

3
9
10
7.33

85.7
23
26.4
45.05

7.14
11.11
11.43
9.89

100.00
94.26
79.32
91.19

80.00
94.26
95.18
89.81

0.00
4.69
3.78
2.82

33.48
99.88
90.30
74.55

(%)

512

E. Yücel et al. / European Journal of Operational Research 227 (2013) 503–514

Table 6
Best found solution values of the models for ﬁve sequential days for the selected instance set.
P
a
lsf
1st day
2nd day
i2N ki se

0.8
1
1.2

10,493
10,493
10,493

13,117
10,493
8744

P

Q0

P

Q0

P

Q0

P

Q0

P

10,493
10,185
8507

0
308
1986

10,493
10,493
8744

0
308
3735

10,493
10,493
8744

0
308
5484

10,493
10,493
8744

0
308
7233

10,493
10,493
8744

Table 7
Test results for the selected problem instance for varying workload levels.
C

K

v

f (%)

DA (%)

DU
(%)

GapP
(%)

GapC
(%)

80.95
44.64
59.82
63.27
63.27
55.95
69.05

100.00
100.00
97.06
88.34
81.07
74.92
69.63

80.00
90.00
97.06
97.17
97.29
97.40
97.48

0.00
0.00
1.23
1.16
1.07
1.01
0.96

6.88
7.07
15.05
13.86
14.06
6.00
4.96

(%)
0.8
0.9
1.0
1.1
1.2
1.3
1.4

10,493
10,493
10,185
9269
8507
7862
7307

171
212
321
304
284
265
254

4
5
9
8
8
9
7

7.14
10.7
10.7
11.2
11.2
9.52
8.33

5th day

0
0
0

7.2.6. Managerial insights
All of the test results up to this point show that both the solution
quality and the characteristics of the solution change as the workload level changes. In this section, we further investigate the real-life
implications of this factor by additional tests with the aim of providing some generalizable insights. We pick, again, the ﬁrst instance set
in Table B.1 and solve it for a number of different a values varying between 0.8 and 1.4 (obtained by decreasing the processing rate l
while keeping the daily accumulated amount constant).
According to the results presented in Table 7, we observe that
all of the daily accumulation can be processed when a < 1.0. As
the workload level increases, the percentage of the daily accumulation that can be processed on time decreases, whereas the processing capacity utilization increases. The gap between the daily
processed amount of the solution and the corresponding inﬁnite
vehicle relaxation solution is the largest when a = 1.0 and it decreases as the workload exceeds 1.
When a < 1.0, idle times at the processing unit are tolerable
during (0,se] since most of the items can be processed after the ﬁnal TSP tour. Since a smaller number of tours is sufﬁcient, the best
found daily transportation costs are low. When a = 0.8, the prob-

P

4th day

Q0

7.2.5. Effect of positive initial workload
As we see in Table B.1, for some instances, all of the daily accumulated amount cannot be processed by time sf. In those cases, the
unprocessed items that remain at time sf, which is equal to
P
i2N ki se  P, may constitute the initial workload for the next
day. These items can be processed while the vehicle is performing
its ﬁrst tour, and this reduces the idle time.
In this section, we solve a ﬁve-day problem for the week days,
and analyze the effect of positive initial workload on the daily processed amount, P, for each day. For the ﬁrst day, we solve the models with Q0 = 0. For the following days, we solve the models with Q0
set to the unprocessed amount of the previous day, i.e.,
P
i2N ki se  P. We assume that the same accumulation rates are observed every day and items remaining from the previous day are
processed ﬁrst. Table 6 reports Q0 and P values for the best found
solutions of each day for the ﬁrst instance set in Table B.1 with
a = 0.8, 1, and 1.2.
According to the results in Table 6, when a P 1, allowing initial
backlog increases the utilization of the processing facility. In the
case of a = 1, the system becomes stable after the ﬁrst day. In the
case of a = 1.2, although the processing capacity is fully utilized
after the ﬁrst day, the initial workload increases day by day. This
also illustrates the effect of the workload parameter on the system
performance when backlogging is possible.

a

3rd day

Table 8
The best found solutions for the selected instance for varying workload levels.

a

Best found solution

Processor idle
time (min)

Vehicle idle
time (min)

0.8

0(226)-14-0(161.42)-10-0(61.58)14-0(57)-TSP-0
0(100)-14-0(85.35)-12-0(16.43)-1413-90(19.28)-14-0(64.94)-TSP-0

252

506

126

465

1.0

0-13-14-0(1.2)-14-0(7.47)-140(11.2)-10-12-0-9-13-140(101.18)-14-0(14.53)-12-0(29.04)14-0-TSP-0

37

294.6

1.1

0(0.66)-8-9-14-0-14-0(12.28)-1012-0-14-0(33.9)-140(63.1)-13-14-0(181.35)-14-0-TSP-0

35.7

291.3

1.2

0(4.15)-9-14-0-14-0(1.4)-13-140(58.71)-10-12-0(25.16)-140(180.81)-10-0(0.03)-14-0-TSP-0

34.2

270.3

1.3

0(2.84)-9-14-0-14-0(4.6)-13-140(68.17)-12-0(23.2)-140(181)-14-0-TSP-0

32.8

279.8

1.4

0(1.71)-9-14-0-14-0(17.91)-140(64.55)-12-0(14.51)-140(175.72)-14-0-TSP-0

31.7

274.4

0.9

lem becomes relatively easy as around 81% of nodes can be ﬁltered
without causing any signiﬁcant loss from optimality. As a increases
to 0.9, it is more difﬁcult to identify the nodes to be ﬁltered
(f ¼ 44:64%). In addition, the percentage of nodes visited in a tour
increases from 7.14% to 10.7%, causing the transportation cost to
increase by 24%. In case of a = 1.0, the problem becomes signiﬁcantly more difﬁcult since the number of tours required almost
doubles. The vehicle visits the same percentage of nodes in a tour
but has to perform more frequent tours to ensure that the processing center is well-supplied with the items to be processed. The processing rate decreases as a increases, so that it requires more time
to process the same amount of items. As a result, the best found
daily transportation cost decreases in a.
When a P 1.0, capacity utilization remains constant due to
routing restrictions. For a = 1.4, both M1 and M2 can be solved to
near-optimality since ﬁltering becomes more effective while both
 decrease.
K and v
The best found solutions by the TF heuristic for each workload
level are provided in Table 8, where the numbers in parentheses
denote the amount of waiting times at the corresponding nodes.
The TSP tour performed after time se is 8-6-2-7-14-5-11-13-4-93-12-10-1 with a cost of 108. According to these solutions, before
the ﬁrst tour a longer waiting time can be tolerated when a < 1,
since all of the daily accumulation can be processed easily on time.
For larger workload levels, the waiting time before the ﬁrst tour is
smaller. For all workload levels, it is better to visit closer sites with
high accumulation rates (i.e., nodes 12, 13, and 14) before se. In
none of the solutions, nodes 1–7, which have the lowest accumulation rates, are visited before se. For these nodes, the ratio of the
accumulation rate to the total time to service a node and return
to the processing facility (given in Table E2) is less than 0.01.

513

E. Yücel et al. / European Journal of Operational Research 227 (2013) 503–514

In Table 8, we report the idle time of the processing facility
throughout the day and the idle time of the vehicle during the
working day (i.e., between times 0 and se). As the workload level
increases, the idle times of the processor decrease, while the vehicle stays almost at the same level of idleness for a P 1.

G
g
P(t)

8. Conclusions

UBP

We considered the clinical specimen collection problem under
the objectives of maximizing the daily processed amount as a ﬁrst
priority and minimizing the daily transportation costs as a second
priority. We deﬁned the Collection for Processing Problem (CfPP) as
the problem of designing vehicle routes throughout the day to
match the collected workload with the processing capacity. We
showed that CfPP is NP-hard and formulated an MIP model for
its solution. We observed that since the solution space is extremely
large even for small problem sizes, the MIP model strengthened
with valid inequalities cannot ﬁnd good solutions within reasonable time. Therefore, we proposed a heuristic approach that incorporates additional constraints to the MIP model with the goal of
eliminating solutions that are likely to be suboptimal. We also presented a procedure to calculate an upper bound on the daily processed amount and two relaxed MIP models to generate lower
bounds on the daily transportation cost.
We showed through computational tests that our approach
ﬁnds solutions that are very close to optimal in terms of daily processed amount. Furthermore, we show that transportation costs of
these solutions are in the vicinity of 13% to optimality on the
average.
We identiﬁed the workload level (i.e., a) as a key indicator
parameter for problem difﬁculty. For small (a < 1) and large
(a P 1.2) workload levels, the problem becomes easier. When a
is close to 1, the vehicle should perform frequent, and more carefully designed tours in order to feed the processor continuously.
In such cases, a high service level can be achieved at the expense
of a higher transportation cost. Another indicator of problem difﬁculty is the pattern of the accumulation at the sites. When the pattern is Pareto, as observed at the clinical laboratory we have been
collaborating with, visiting sites with low accumulation rates can
be ruled out without signiﬁcantly compromising from optimality.
Future work includes developing heuristic algorithms to solve
larger CfPP instances. Additional considerations to be incorporated
to the formulation includes multiple vehicles and stochastic accumulation of specimens at the sites.

LBC

C(t)

Rk
Wk
Ii(t)
Qk

Eik

Sk
Uk

U j1 þj2

Appendix B. Results
Table B.1.

Table B.1
Test results.
Instance n

se
sf
tij
dij
ki

l
j1
j2
a

Number of sites
Time of day when sites close
Time of day when processing at the processing
facility ends
Traveling time from node i to node j, where i, j 2 N+
Distance from node i to node j, where i, j 2 N+
Accumulation rate at site i per unit time, where i 2 N
Processing rate per unit time of the processing facility
Maximum number of tours that can be performed
until time se
Maximum number of tours that can be performed
between time se and sf
Workload level, the ratio of daily accumulation to

P 
se
ki
daily processing capacity, a ¼ sf li2N

a

P

C

K

v
(%)

1

Appendix A. Notation

n

Set of feasible solutions for a CfPP instance
A solution in G
Amount of items processed between time 0 and t,
where t 2 ð0; sf ; PðtÞ 2 Rþ
Transportation cost between time 0 and t, where
t 2 ð0; sf ; CðtÞ 2 Rþ
Upper bound for the daily processed amount,
UBP 2 Rþ
Lower bound for the daily transportation cost,
LBC 2 Rþ
Return time of tour k, where k = 1, 2, . . . , j1 + j2 and
Rk 2 Rþ
Amount of waiting time before tour k, where
k = 1, 2, . . . , j1 + j2 and W k 2 Rþ
Amount of items at node i at time t, where i 2 N+, t 2
(0, sf] and Ii ðtÞ 2 Rþ
Amount of unprocessed items at the end of tour k,
where k = 1, 2, . . . , j1 + j2
and Q k 2 Rþ ðQ k ¼ I0 ðRk ÞÞ
Collected amount from node i in tour k, where i 2 N,
k = 1, 2, . . . , j1 + j2
and Eik 2 Rþ
Total collected amount in tour k, where
k = 1, 2, . . . , j1 + j2 and Sk 2 Rþ
Amount of items processed between tour k’s end time
and tour (k + 1)’s end time,
where k = 1, 2, . . . , j1 + j2  1 and U k 2 Rþ
(Uk = P(Rk+1)  P(Rk))
Amount of items processed between tour (j1 + j2)’s
end time and sf, U j1 þj2 2 Rþ .

14 0.8 10,493 171 4
1.0 10,185 321 9
1.2 8507
284 8

f

DA (%) DU
(%)

GapP GapC
(%)
(%)

7.14 80.95 100.00 80.00 0.00
10.7 59.82 97.06 97.06 1.23
11.2 63.27 81.07 97.29 1.07

6.88
15.05
14.06

2

9 0.8 4786
1.0 4658
1.2 3886

143 4
328 9
260 9

14.8 74.07 100.00 80.00 0.00
23.6 68.06 97.33 97.33 1.21
16.7 43.06 81.21 97.45 1.16

6.72
25.67
27.45

3

12 0.8 4522
1.0 4389
1.2 3665

173 4
352 10
306 8

11.1 33.33 100.00 80.00 0.00
13
48.15 97.05 97.05 1.27
13.1 34.52 81.04 97.25 1.13

8.13
24.82
18.15

4

13 0.8 4509
1.0 4361
1.2 3651

218 3
415 10
362 8

23.1 42.31 100.00 80.00 0.00
13.7 51.28 96.71 96.71 1.56
15.4 53.85 80.97 97.16 1.16

7.92
28.88
26.13

5

14 0.8 4540
1.0 4362
1.2 3668

215 3
379 8
351 8

17.9 42.86 100.00 80.00 0.00
16.3 37.76 96.07 96.07 2.30
13.3 56.12 80.80 96.96 1.38

11.40
27.61
29.52

6

11 0.8 4928
1.0 4746
1.2 3983

169 3
334 7
320 8

18.2 72.73 100.00 80.00 0.00
21.2 50.00 96.30 96.30 1.93
14.3 71.43 80.82 96.98 1.23

4.97
27.97
28.00

7

11 0.8 4807

170 3

22.7 59.09 100.00 80.00 0.00

6.92

(continued on next page)

514

E. Yücel et al. / European Journal of Operational Research 227 (2013) 503–514

References

Table B.1 (continued)
Instance n

a

P

C

K

v
(%)

f

DA (%) DU
(%)

GapP GapC
(%)
(%)

1.0 4643
1.2 3873

344 9
290 7

18.2 55.68
19.7 43.94

96.58 96.58 1.46
80.56 96.67 1.39

26.94
25.00

8

12 0.8 4455
1.0 4301
1.2 3601

173 3
351 9
312 7

16.7 75.00 100.00 80.00 0.00
16.7 52.08 96.55 96.55 1.74
16.7 75.00 80.83 97.00 1.36

6.13
27.64
26.83

9

11 0.8 4763
1.0 4613
1.2 3849

150 3
317 8
270 7

18.2 54.55 100.00 80.00 0.00
18.2 41.56 96.86 96.86 1.48
13.6 28.79 80.82 96.98 1.42

6.38
20.99
9.31

10

16 0.8 5798
1.0 5592
1.2 4666

216 4
428 10
378 6

10.4 52.08 100.00 80.00 0.00
11.8 66.67 96.44 96.44 1.92
25
50.00 80.47 96.56 1.85

6.93
28.92
28.14

11

17 0.8 6214
1.0 5906
1.2 4967

229 4
387 8
357 9

13.7 50.98 100.00 80.00 0.00
13.4 41.18 95.05 95.05 3.59
9.56 42.65 79.94 95.92 2.71

6.02
27.30
27.50

12

14 0.8 4687
1.0 4502
1.2 3784

173 4
311 10
291 8

11.9 64.29 100.00 80.00 0.00
10.3 45.24 96.05 96.05 2.38
13.3 40.82 80.74 96.89 1.57

4.22
25.40
25.43

6.78 14.9 53.9
91.51 91.58 1.07
3
7.14 28.79 69.63 80.00 0.00
10
25
80.95 100.00 97.48 3.59

18.06
4.22
29.52

Avg.
Min.
Max.

Appendix C. Supplementary material
Supplementary data associated with this article can be found, in
the online version, at http://dx.doi.org/10.1016/j.ejor.2012.10.044.

Archetti, C., Hertz, A., Speranza, M., 2007. Metaheuristics for the team orienteering
problem. Journal of Heuristics 13, 46–76.
Boussier, S., Feillet, D., Gendreau, M., 2007. An exact algorithm for team orienteering
problems. 4OR 5, 211–230.
Butt, S., Ryan, D., 1999. An optimal solution procedure for the multiple tour
maximum collection problem using column generation. Computers and
Operations Research 26, 427–441.
Doerner, K.F., Gronalt, M., Hartl, R.F., Kiechle, G., Reimann, M., 2008. Exact and
heuristic algorithms for the vehicle routing problem with multiple
interdependent time windows. Computers and Operations Research 35,
3034–3048.
Dumas, M.B., Rabinowitz, M., 1977. Policies for reducing blood wastage in hospital
blood banks. Management Science 23, 1124–1132.
Gavish, B., Graves, S.C., 1978. The Travelling Salesman Problem and Related
Problems. Working Paper GR-078-78, Operations Research Center,
Massachusetts Institute of Technology.
Golden, B.L., Levy, L., Vohra, R., 1987. The orienteering problem. Naval Research
Logistics 34, 307–318.
Knowledge Source Inc., 2008. Clinical Laboratory Testing Market Overview.
McDonald, J.J., 1972. Vehicle scheduling – a case study. Operational Research
Quarterly 1970–1977 23, 433–444.
Moin, N.H., Salhi, S., 2007. Inventory routing problems: a logistical overview.
Journal of the Operational Research Society 58, 1185–1194.
Ohlmann, J.W., Fry, M.J., Thomas, B.W., 2008. Route design for lean production
systems. Transportation Science 42 (3), 352–370.
Or, I., 1976. Traveling Salesman-Type Combinatorial Problems and Their Relation to
the Logistics of Regional Blood Banking. PhD thesis, Evanston, IL: Northwestern
University.
Revere, L., 2004. Re-engineering proves effective for reducing courier costs. Business
Process Management Journal 10, 400–414.
Vansteenwegen, P., Souffriau, W., Berghe, G.V., Oudheusden, D.V., 2009.
Metaheuristics for tourist trip planning. In: Metaheuristics in the Service
Industry, vol. 624 of Lecture Notes in Economics and Mathematical Systems, pp.
15–31.
Yi, J., Scheller-Wolf, A., 2003. Vehicle routing with time windows and timedependent rewards: a problem of american red cross. Manufacturing and
Service Operations Management 5, 73–77.

This article was downloaded by: [149.169.145.169] On: 09 June 2017, At: 22:54
Publisher: Institute for Operations Research and the Management Sciences (INFORMS)
INFORMS is located in Maryland, USA

INFORMS Journal on Computing
Publication details, including instructions for authors and subscription information:
http://pubsonline.informs.org

Expected Tardiness Computations in Multiclass Priority M/
M/c Queues
A. Baykal Hafızoğlu, Esma S. Gel, Pınar Keskinocak,
To cite this article:
A. Baykal Hafızoğlu, Esma S. Gel, Pınar Keskinocak, (2013) Expected Tardiness Computations in Multiclass Priority M/M/c
Queues. INFORMS Journal on Computing 25(2):364-376. https://doi.org/10.1287/ijoc.1120.0507
Full terms and conditions of use: http://pubsonline.informs.org/page/terms-and-conditions
This article may be used only for the purposes of research, teaching, and/or private study. Commercial use
or systematic downloading (by robots or other automatic processes) is prohibited without explicit Publisher
approval, unless otherwise noted. For more information, contact permissions@informs.org.
The Publisher does not warrant or guarantee the article’s accuracy, completeness, merchantability, fitness
for a particular purpose, or non-infringement. Descriptions of, or references to, products or publications, or
inclusion of an advertisement in this article, neither constitutes nor implies a guarantee, endorsement, or
support of claims made of that product, publication, or service.
Copyright © 2013, INFORMS
Please scroll down for article—it is on subsequent pages

INFORMS is the largest professional society in the world for professionals in the fields of operations research, management
science, and analytics.
For more information on INFORMS, its publications, membership, or meetings visit http://www.informs.org

INFORMS Journal on Computing

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

Vol. 25, No. 2, Spring 2013, pp. 364–376
ISSN 1091-9856 (print)  ISSN 1526-5528 (online)

http://dx.doi.org/10.1287/ijoc.1120.0507
© 2013 INFORMS

Expected Tardiness Computations in
Multiclass Priority M/M/c Queues
A. Baykal Hafızoğlu, Esma S. Gel
School of Computing, Informatics and Decision Systems Engineering, Arizona State University, Tempe, Arizona 85287
{baykal@asu.edu, esma.gel@asu.edu}

Pınar Keskinocak
School of Industrial and Systems Engineering, Georgia Institute of Technology, Atlanta, Georgia 30332,
pinar@isye.gatech.edu

W

e discuss the evaluation of expected tardiness of an order at the time of arrival in an M/M/c queuing
system with N priority classes, considering both nonpreemptive and preemptive service disciplines. Upon
arrival, a customer order is quoted a lead time of d, and placed in the queue according to the priority class of
the customer. Orders within the same priority class are processed on a first-come, first-served basis. We derive
the Laplace transforms of the expected tardiness of the order given the quoted lead time, priority class of the
order, and system status. For the special case of single priority class, the Laplace transform can be inverted
into a closed-form expression. For the case with multiple priority classes, a closed-form expression cannot be
obtained, hence, we develop three customized numerical inverse Laplace transformation algorithms. Two of
these algorithms provide upper and lower bounds for the expected tardiness under a simple condition on
system parameters. Using this property, we obtain error bounds for our customized algorithms; such bounds
are not available for general purpose numerical inversion algorithms in the literature. Next, we develop a
novel methodology to compare the precision of general purpose numerical inversion algorithms and analyze
the performances of three algorithms from the literature. Finally, we provide a recommendation scheme given
computational time and error tolerances of the decision maker. The methods developed in this paper for the
accurate estimation of expected tardiness establish an important step toward developing due date quotation
policies in a multiclass queue, contributing to the due date quotation literature that has been largely focused on
single-class queues.
Key words: due date quotation; scheduling; priority queues; Laplace transforms
History: Accepted by Winfried Grassmann, Area Editor for Computational Probability and Analysis; received
May 2011; revised October 2011, January 2012; accepted February 2012. Published online in Articles in
Advance June 6, 2012.

1.

Introduction

defined as the positive difference between the completion time of the order and the promised due date. For
example, metal pipe producer Merle Blanc stipulates
a contract, where suppliers pay a penalty amounting
to 1% of the shipment value for each week the product is delayed (Merleblanc.de 2012).
While quoting a due date, companies need the
ability to estimate the tardiness of an order, and in
turn, evaluate the late delivery penalties that can be
incurred. Hence, obtaining sufficiently accurate tardiness estimates plays an important role in quoting due
dates, which in turn impacts the ability to attract customer orders and maintain profitability.
Due date management literature partially addresses
the issue of expected tardiness computation in the
context of dynamic quotation models. The due date
to quote to an arriving customer is often based on
the system status, where the expected tardiness is crucial in the determination of the optimal due date.
Research to date has considered relatively simple

In many make-to-order environments, due date quotation is a key aspect of business transactions. The due
date quoted by the manufacturer impacts whether
or not the customer decides to do business (i.e.,
place an order) with that manufacturer. For example, National Bicycle Industrial Co., a sports bicycle
producer in Japan, emphasizes their ability to offer
short due dates proudly in their slogan: “We can
deliver a custom-made bicycle to you within two
weeks” (Lean-manufacturing-japan.com 2012). Once
an order is placed, if the actual completion time of the
order exceeds the manufacturer’s promised due date,
there may be monetary penalties as well as negative implications for future business. Savasaneril et al.
(2010) mention several examples, where companies
in various industries pay huge penalties due to late
deliveries.
The late delivery penalty often (linearly) increases
in the tardiness of the order, where tardiness is
364

Hafızoğlu, Gel, and Keskinocak: ETA Computations

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

INFORMS Journal on Computing 25(2), pp. 364–376, © 2013 INFORMS

M/M/1 queuing models with a single customer class
(Savasaneril et al. 2010, Feng et al. 2011), or multiple
customer classes with first-come, first-served (FCFS)
sequencing (Duenyas 1995). In practice, suppliers may
face demand from various customer classes and prioritize the orders based on different criteria, such as
the type of contract between the supplier and the customer, or whether the order comes from a long-term
versus a one-time customer. For example, Motorola
offers a three-day repair time with a bronze service
plan, where the regular warranty guarantees ten-day
repair time (Motorola 2012). Given that prioritization
of orders is a common practice in the presence of
multiple customer classes, extension of dynamic due
date quotation research to multiple customer classes
requires the computation of expected tardiness of
orders from different priority classes.
The expected tardiness can be calculated using the
distribution of the time in system (often referred to as
cycle time or sojourn time in references, and denoted
as TIS henceforth) of the order, and a simple conditioning argument. When the distribution of the TIS is
known explicitly as, for example, in the case of a single priority class, the derivation of the expected tardiness is straightforward. In contrast, when there are
multiple priority classes, a high priority order arrival
may push a low priority order to the back of the
queue, leading to an increase in the TIS for the low
priority order. The derivation of the TIS distribution is
not trivial in this case, and requires the use of Laplace
transforms, which do not necessarily yield a closedform TIS distribution (Wein 1991, Zeltyn et al. 2009).
In this paper, we discuss the computation of
expected tardiness of an order at the time of arrival
for a given due date (denoted as ETA henceforth) in
an M/M/c queuing system. ETA is computed given
(i) the quoted lead time, d (i.e., the difference between
the due date and the time of arrival); (ii) the state of
the queuing system at the time of arrival; and (iii) the
class of the arriving order. We present our results for
the cases of preemptive and nonpreemptive service
and discuss a special case with a single priority class.
As also argued by Zeltyn et al. (2009), the extension
to general service times (i.e., M/G/c queues) requires
more complex analysis, which is beyond the scope of
this paper.
More specifically, we formulate the Laplace transform of ETA, and strive to obtain the inverse transforms. For the trivial single-class case, we obtain a
closed-form expression for ETA. For the multipleclass case, however, Laplace transforms cannot be
inverted into a closed-form expression, necessitating the implementation of numerical inverse Laplace
transformation (NILT) methods. Even though there
is an abundance of general-purpose NILT algorithms
(denoted as GP-NILTA henceforth) in the literature,

365
the existing studies do not provide error bounds for
the noninvertible Laplace transforms such as those
in our case. The lack of error bounds may result in
significant errors in late delivery penalty estimation
(LPE), which may lead to erroneous due date policies.
Consequently, obtaining error bounds for the NILT
methods is crucial for due date management.
For the multiple classes case with noninvertible
Laplace transforms, we develop three customized
NILT algorithms (C-NILTA), which we refer to as
trapezoidal (Z), midpoint (M), and hybrid (H) algorithms, and show that Z overestimates and M underestimates ETA under a simple condition. This result
allows us to provide lower and upper bounds for
ETA, and to obtain worst-case error bounds for Z,
M, and H. Using a computational analysis, we test
the precision of worst-case bounds, and observe that
precision increases with computational time. Noting
that high computational times may be undesirable
for decision makers, we do the following: (i) test the
performance of three fast and prominent GP-NILTAs
presented in Abate and Whitt (2006), which provide a unification of the state-of-the-art GP-NILTAs,
(ii) develop a recommendation scheme on the usage
of NILT methods, and (iii) illustrate the recommendations using an example.
GP-NILTAs are first developed in the late 1960s
by the seminal studies of Gaver (1966), Dubner and
Abate (1968), Zakian (1969), and Stehfest (1970). Various variants of these algorithms have been developed since then. The readers are referred to Abate and
Whitt (1992) for an extensive survey. The performance
of GP-NILTAs has also been studied extensively,
where Abate and Valko (2004), Abate and Whitt
(2006), Avdis and Whitt (2007), and Hassanzadeh and
Pooladi-Darvish (2007) present some recent examples. The common performance evaluation method
in these papers is the comparison of algorithm results with the exact results of Laplace transforms for
which the closed-form inverses are readily available.
For example, Abate and Whitt (2006) test the Laplace
transforms in Equations (46) and (48) in their paper, whose inverse transforms are given in Equations (47) and (49), respectively. On the other hand,
Hassanzadeh and Pooladi-Darvish (2007) study some
noninvertible Laplace transforms, and compare the
results of GP-NILTAs with each other. As also stated
by Avdis and Whitt (2007), GP-NILTAs are not
needed for the invertible Laplace transforms, but
indispensable for noninvertible ones. In this paper,
we present an approach to evaluate the performance
of GP-NILTAs in the context of expected tardiness
estimation in a multiclass queue, which involves noninvertible Laplace transforms.
Although results on the distributions of waiting time in the queue/system are abundant in the

Hafızoğlu, Gel, and Keskinocak: ETA Computations

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

366
literature, to the best of our knowledge, this is the
first study addressing the expected tardiness computation problem in a multiclass queuing setting.
Earlier studies typically focus on the derivation of
queuing performance criteria, e.g., TIS and waiting
time distributions in priority queues. Davis (1966)
derives the TIS distribution in a nonpreemptive priority M/M/c queue using conditioning arguments
and Laplace transforms. Using a similar approach,
Heyman and Sobel (2004) derive first passage time
distributions in an M/M/1 queue. Similar to Heyman
and Sobel (2004), we define the TIS of orders in §4.
Following the approach of Davis (1966), Segal (1970)
derives the Laplace transforms of TIS distributions in
an M/M/c queue with preemptive service for a service rate of  = 1. Unfortunately, the  = 1 case does
not provide a generalizable result for arbitrary values of the service rate, and hence, we revisit those
results to provide a more general expression in §5.
More recently, Zeltyn et al. (2009) discuss a special
case with K priority classes, where P of the highest
priority classes (P < K) can preempt the lower priority orders in an M/M/c queue. The authors derive
the Laplace-Stieltjes transforms of the TIS and waiting time distributions. There are some other studies
considering the more general M/G/c case (Paterok
and Ettl 1994, Stanford and Drekic 2000, Jagerman
and Melamed 2003). However, none of these studies
address the ETA computations.
The early studies on due date quotation focus on
the comparison of due date setting and scheduling
rules using simulation analysis, with objectives or
constraints on the expected (weighted) tardiness or
the number of tardy jobs (Baker and Bertrand 1981,
Bertrand 1983, Bookbinder and Noor 1985, Wein 1991,
Hunsucker and Shah 1992). The reader is referred
to Keskinocak and Tayur (2004) for an extensive
literature survey on due date quotation. Wein (1991)
proposes a due date quotation rule with the goal of
minimizing the weighted average of due date quotes,
subject to an upper bound on the expected tardiness.
Similar to our study, the author considers prioritization under multiple customer classes, and implements
Laplace transformation to compute the expected TIS
of the low priority customer class in an example with
two customer classes. More recently, due date quotation studies have focused on the derivation of optimal
policies (with the goal of maximizing profits subject
to late delivery penalties) assuming that the probability of the purchase is a function of the quoted due
date to the customer (Duenyas 1995, Duenyas and
Hopp 1995, Savasaneril et al. 2010, Feng et al. 2011).
Extension of these studies from FCFS settings to more
realistic prioritized multiclass customer models necessitates the computation of ETA, which is the focus of
this study.

INFORMS Journal on Computing 25(2), pp. 364–376, © 2013 INFORMS

In addition to being the first approach to compute
ETA in the literature, our study has three major contributions in the field of NILT. First, we develop error
bounds for the inverse Laplace transformation for our
problem; such error bounds have not been obtained
in the literature, even for a restricted class of transforms, according to Abate and Valko (2004). Second,
because the exact solutions for noninvertible Laplace
transforms are not known, one cannot measure how
good an algorithm performs in terms of solution
quality. The error bounds we developed present a
novel methodology to measure the quality of approximations obtained by GP-NILTAs using noninvertible
Laplace transforms. Third, using our recommendation scheme, our C-NILTAs provide approximations
within desired precision requirements, which is not
necessarily achieved by GP-NILTAs.
The remainder of the paper is organized as follows. In §2, we formulate the Laplace transform of
the ETA. In §3, we discuss the case of single priority
class, and provide a closed-form solution for the ETA.
The Laplace transforms of the ETA for the nonpreemptive and preemptive service cases are derived, in
§§4 and 5, respectively. The upper and lower bounding algorithms for ETA are presented at the end of §4
for the nonpreemptive case, and modified to handle
the preemptive case at the end of §5. We conduct a
computational analysis to compare the performances
of NILT algorithms in §6, where we also develop
recommendations on the use of appropriate NILT
algorithms illustrating the benefits of error bounds
obtained using the approach presented in this paper.
We conclude with a discussion on the contributions
of the paper in §7.

2.

Methodology

We consider an M/M/c queuing system with N prioritized customer classes, where the arrival rate is
i for class i ∈ 811 0 0 0 1 N 9, and the service rate is 
for each class. Without loss of generality, we assume
that class k has higher priority than class m where
k < m ≤ N , k ∈ 811 0 0 0 1 N − 19, and customer orders
from the same priority class are sequenced on a FCFS
basis. We assume that there are no upper bounds
on the number of orders in the
PNsystem, and the stability condition is met, i.e.,
i=1 i < c. Let Vi 4t5
and Qi 4t5, i ∈ 811 0 0 0 1 N 9, denote the number of class i
orders in the system P
and in the queue, respectively,
at time t. Let V0 4t5 = Ni=1 Vi 4t5 denote the total number of orders in the system at time t. The state of
the system at time t is defined by the vector Z4t5 =
4V 4t51 Q4t55, where V 4t5 = 4V1 4t51 V2 4t51 0 0 0 1 VN 4t55,
and Q4t5 = 4Q1 4t51 Q2 4t51 0 0 0 1 QN 4t55. 8Z4t51 t ∈ T 9
defines a stochastic process, and can be modeled as
a continuous time Markov chain. Our formulations
provide expressions for the expected tardiness of an

Hafızoğlu, Gel, and Keskinocak: ETA Computations

367

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

INFORMS Journal on Computing 25(2), pp. 364–376, © 2013 INFORMS

order that arrives at time t and is quoted a lead time
of d. For simplicity, we drop t from the notation below
and refer to the order for which the expected tardiness is evaluated as the “marked” order. Let Tz1 i 4d5
denote the ETA of a class i marked order if d is the
quoted lead time and z is the system state upon the
arrival of the order. Equation (1) provides the expression for ETA:
Z 
Tz1 i 4d5 =
4x − d5fz1 i 4x5 dx = Sz1 i − d + z1 i 4d51

Using the convolution property,
˜ f˜z1 i 4s50
˜z1 i 4s5 = 4s5

The Laplace transform of 4x5 is equal to 1/s 2 .
By plugging in Equation (3), we reach the desired
equation. 
Using Equation (1) and Proposition (1), we have

d

Tz1 i 4d5 = Sz1 i − d

d ≥ 01 (1)
where fz1 i 4 · 5 denotes the probability density function
(pdf) of the TIS, Sz1 i denotes the expected TIS, and
z1 i 4d5 denotes the expected earliness of the marked
order at the time of arrival, where earliness is defined
as the positive difference between the due date and
completion time of the marked order. By definition,
Z 
Sz1 i =
xfz1 i 4x5 dx1 and
0
(2)
Z d
z1 i 4d5 = 4d − x5fz1 i 4x5 dx1 d ≥ 00
0

We note that Tz1 i 4d5 depends on both the system
state and the class of the marked order but it is computed for a given d that may or may not depend on
the system state and the class of the marked order. For
example, a manufacturer may develop a class dependent due date quotation scheme, or may quote the
same due date to all customers. For a given d, the
value of ETA is the same in both cases.
We evaluate z1 i 4d5 and Sz1 i by Laplace transforms.
Following the notation of Heyman and Sobel (2004),
we denote the Laplace transform of the function
f 4 · 5 as
Z 
f˜4s5 =
e−sx f 4x5 dx1 s ∈ 1
0

where  denotes the set of complex numbers.
Remark 1. The convolution property of Laplace
transforms (Doetsch 1974, Theorem 10.1) implies,
Z t
a4t5 =
b4x5c4t − x5 dx ⇐⇒ ã4s5 = b̃4s5c̃4s50

(3)

+ L−1
d

 ˜

fz1 i 4s5
1
s2

d ≥ 01

(4)

where L−1
d 8 · 9 denotes the inverse Laplace transformation operation evaluated at point d ≥ 0. Using the
properties of Laplace transforms we obtain
Sz1 i = −


d f˜z1 i 4s5 
 0
ds s=0

Hence, Equation (4) requires the derivation of f˜z1 i 4s5
as well as the inverse transformation of ˜z1 i 4s5 =
f˜z1 i 4s5/s 2 . We begin the analysis with the special case
of N = 1, i.e., one customer class.

3.

Special Case: Single Customer
Class

Because orders within each class are ordered in FCFS
manner, N = 1 case is equivalent to the well-known
FCFS sequencing of all customer orders. For this special case, v0 (the number of orders in the system
upon arrival of the marked order) becomes the only
necessary information for the ETA evaluation. Thus,
z is replaced with v0 . The index i is dropped from
the notation, i.e., we aim to compute Tv0 4d5, which
depends on whether all the servers are busy upon
arrival of the marked order. Hence, there are two
cases to consider.
Case I: v0 ≤ c − 1. There is at least one server available at the time of the marked order’s arrival, and the
processing of the marked order can start immediately.
Hence, the TIS of the marked order is distributed
exponentially with rate . Using Equation (1), we get

0

Using Laplace transforms and the convolution property we prove the following.
Proposition 1.
f˜z1 i 4s5
1 s ∈ 0
˜z1 i 4s5 =
s2
Proof. Note that z1 i 4d5 is the convolution of two
functions by Equation (2). Letting 4x5 = x, x ∈ , the
convolution can be expressed as
z1 i 4d5 = 4 ∗ fz1 i 54d5 =

Z
0

d

4d − x5fz1 i 4x5 dx1

d ≥ 00

Tv0 4d5 =

Z
d



4x − d5e−x dx =

e−d
1


d ≥ 00

(5)

Case II: v0 > c − 1. Because all servers are busy, the
marked order has to wait for the completion of v0 −
c + 1 orders before its processing can start. Thus, the
TIS of the marked order is a random variable that is
equal to the sum of the waiting time in the queue
(WTQ) and the processing time (PT). WTQ can be
expressed as the sum of v0 − c + 1 exponentially distributed random variables with rate c, and hence, is
a gamma random variable with parameters v0 − c + 1

Hafızoğlu, Gel, and Keskinocak: ETA Computations

368

INFORMS Journal on Computing 25(2), pp. 364–376, © 2013 INFORMS

and c. PT is an exponential random variable with
rate , and its pdf is denoted by h 4 · 5. Then,

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

h 4x5 = e−x 1 x ≥ 01 and

h̃ 4s5 =
1 s ∈ 0
s+

(6)

From this, the pdf of the TIS, fv0 4 · 5, is the convolution
of h 4 · 5 and 4v0 − c + 15-fold convolution of hc 4 · 5,
implying that the Laplace transform of the fv0 4 · 5 is
equal to
f˜v0 4s5 = h̃ 4s54h̃c 4s55v0 −c+1

v0 −c+1

c
1
=
s +  s + c

s ∈ 0

even if an order of higher priority joins the queue.
In §5, we consider the preemptive service case, where
a higher priority order can preempt the processing of
a lower priority order.
Similar to single-class case, the TIS of the marked
order can be divided into two durations: (i) the time
elapsed until the order starts processing, i.e., the
WTQ, and (ii) PT. Any high priority order arrival during the WTQ pushes the marked order back in the
queue and increases its TIS. For a given marked order,
from class i, one needs to consider two parameters for
the evaluation of the ETA: (i) the number of orders in
the system, v0 , and (ii) the number of orders in the
queue ahead of the marked order, q̄i , which can be
calculated from state variables qj as

Using Proposition (1), we get the Laplace transform
of v0 4d5 as

v0 −c+1

c
1
˜v0 4s5 = 2
s 4s + 5 s + c

s ∈ 0

(7)

In Theorem 1, we derive Tv0 4d5 using ˜v0 4s5.
Theorem 1.
Tv0 4d5
 −d v0
X
e
v +1−k


1
if c = 11
4d5k 0




k!

k=0



 −d 
v0 −c+1


c
e
e−dc 4dc5v0 −c+1



+


c4v0 −c5!
  c −1



=
v0 −c
X 4d5k k 1

v0 −c +1
−dc

 +e
−d +
c


k!

c

k=0





v0 −c+1 



4c −15k
c


−
1 if c ≥ 20



c −1


q̄i =

where i denotes the class of the marked order. To find
Tv0 1 q̄i 1 i 4d5, similar to §3, we consider two cases based
on server availability at the time of the marked
order’s arrival.
Case I: v0 ≤ c − 1. This condition implies that q̄i = 0,
and the order can start processing immediately, i.e.,
the WTQ of the marked order is equal to zero. Using
Equation (5), we get

4.

Nonpreemptive Service Discipline

In this section, we consider N ≥ 2 customer classes,
indexed in decreasing priority. Orders are sequenced
on a FCFS basis within each class, and preemption
of service is not allowed. That is, once the processing of an order is started, it cannot be interrupted

e−d
1


d ≥ 00

Case II: v0 > c − 1. The marked order is behind q̄i
orders in the queue. Thus, the parameter, v0 , is redundant and is dropped from the notation. Researchers
have focused on the use of Laplace transforms to
derive the TIS distribution in this case (Davis 1966,
Segal 1970, Wein 1991, Zeltyn et al. 2009). Let ¯ i
denote the total arrival rate of customer orders with
higher priority than class i. Then,

Proof. Proofs of theorems are provided in the
online supplement available at http://dx.doi.org/
10.1287/ijoc.1120.0507.
This result provides a closed-form solution for the
expected tardiness of an order that observes v0 orders
in the system upon arrival in an M/M/c queue with
FCFS service discipline when the quoted lead time is
d. We next discuss the derivations for systems with
multiple customer classes, under the nonpreemptive
and preemptive service assumptions.

qj 1

j=1

Tv0 1 01 i 4d5 =

(8)

i
X

¯ i =

i−1
X

j 1

i ∈ 821 0 0 0 1 N 91

j=1

where ¯ 1 = 0. We can obtain Tq̄1 1 1 4d5 by plugging c + q̄1
for v0 in Equation (8). Hence, we focus on the calculations for lower priority classes. Because ¯ i and q̄i
uniquely define priority class of the order, we also
drop i from the notation, and derive ˜q̄i 4s5 for i ∈
821 0 0 0 1 N 9 in Theorem 2.
Theorem 2. For i ∈ 821 0 0 0 1 N 9,

4g̃ ¯ 4s55q̄i +1 1
˜q̄i 4s5 = 2
s 4s + 5 i 1 c

s ∈ 1

(9)

where
g̃¯ i 1 c 4s5 =

q
s + ¯ i +c− 4s + ¯ i +c52 −4¯ i c
2¯ i

1

s ∈ 0

Hafızoğlu, Gel, and Keskinocak: ETA Computations

369

Using ˜q̄i 4s5, one can employ any GP-NILTA to
numerically approximate q̄i 4d5, although it is not
clear how well any of these algorithms would perform in this setting. To obtain error bounds and a testing methodology for the GP-NILTAs, we develop two
C-NILTAs approximating j 4d5 for j ∈ 801 11 0 0 0 1 q̄i 9
using an iterative approach. The main idea is to write
the Laplace transform in a product form, where each
component of the product is inverse transformable as
in Equation (9), which for q̄i = 0 gives
˜0 4s5 =


g̃ ¯ 4s51
+ 5 i 1 c

s 2 4s

and,
˜j 4s5 = ˜0 4s54g̃¯ i 1 c 4s55j

(10)

for all i ∈ 821 0 0 0 1 N 9 and j ∈ 801 1 0 0 0 1 q̄i 9. Equation (10)
for customer class i can be written recursively as
˜j 4s5 = ˜j−1 4s5g̃¯ i 1 c 4s51

j ∈ 801 11 0 0 0 1 q̄i 9 1

(11)

where ˜−1 4s5 = /4s 2 4s + 55. Equation (11) provides a
product form of the Laplace transform with inverse
transformable terms. Then, using the convolution
property, we can write j 4 · 5 as a convolution of j−1 4 · 5
and g¯ i 1 c 4 · 5 as
j 4d5 =

Z
0

d

j−1 4d −x5g¯ i 1 c 4x5 dx1

j ∈ 801110001 q̄i 91 (12)

−x

where −1 4x5 = x − 1/ + e /. Thus, using Equation (12), q̄i 4d5 can be evaluated by recursively calculating j 4d5 for j ∈ 801 11 0 0 0 q̄i 9. Using the results of
Heyman and Sobel (2004) we get,
g¯ i 1 c 4x5 = L−1
x 8g̃¯ i 1 c 4s59
 √c
 q
¯
= e−4i +c5x I1 2x ¯ i c q 1
x ¯ i

x ≥ 01 (13)

where I1 4 · 5 is the Bessel function of imaginary argument and first order (Heyman and Sobel 2004, p. 90).
For example, 0 4d5 can be evaluated by the expression

Z d
1 e−4d−x5
0 4d5 =
d−x− +


0
 q
 √c
−4¯ i +c5x
¯
·e
I1 2x i c q dx1 d ≥ 00
x ¯ i
We next approximate j 4d5 functions for j ∈
801 11 0 0 0 1 q̄i 9 using numerical integration and Equation (12),

and
d/w−1

j 4d5 = lim w
w→0

k=0





w
w
g¯ i 1 c kw +
1
j−1 d − kw −
2
2
d ≥ 01 j ∈ 801 11 0 0 0 1 q̄i 91

(15)

where Equations (14) and (15) follow the trapezoidal and midpoint rules, respectively, (Davis and
Rabinowitz 1984, Chapter 2). Using these two rules,
we develop two C-NILTAs (pseudocodes are given in
the online supplement): (i) trapezoidal (Z) and, (ii) midpoint (M) to inverse transform ˜j 4s5, whose approximations are denoted as Zwj 4d5 and Mwj 4d5, given the
numerical integration parameter w. We next show
that Zwj 4d5 and Mwj 4d5, respectively, provide an upper
and lower bound for j 4d5 under simple and mild conditions on ¯i and c.
Theorem 3. Mwj 4d5 ≤ j 4d5 ≤ Zwj 4d5 hold for d ≥ 0,
j = 01 11 0 0 0, and w ≤ d, when p
the following
p two conditions
hold: (i) ¯i + c ≥ 24¯i c + 4 ¯i c5/42 ¯i c − 155 and
(ii) ¯i c ≥ 41 .
Theorem 3 conditions are used to show the convexity of g¯ i 1 c 4 · 5, which is a sufficient condition for
the proof of Theorem 3. In Corollary 1, we provide a
simple condition that implies Theorem 3 conditions.
Corollary 1. Theorem
¯i c ≥ 4.

3

conditions

hold

when

Corollary 1 is simpler condition, albeit more restrictive. Figure 1 plots the parameter space in terms of
¯i and c and graphically shows that Theorem 3 conditions hold everywhere except in the small shaded
region.
Recall that the region ¯i ≥ c is not considered
due to the stability condition. Corollary 1 condition
implies that the manufacturer’s production rate times
the total arrival rate of the customers with priority

12

9

i = c
6

3

i = c
0

X
w d/w−1
j−1 4d − kw5g¯ i 1 c 4kw5
w→0 2
k=0

0

j 4d5 = lim

3

6

9

12

c
Figure 1

+ j−1 4d − kw − w5g¯ i 1 c 4kw + w51
d ≥ 01 j ∈ 801 11 0 0 0 1 q̄i 91

X

i

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

INFORMS Journal on Computing 25(2), pp. 364–376, © 2013 INFORMS

(14)

Representation of the Parameter Space of Theorem 3
Conditions, Which Hold Everywhere Except in the Small
Shaded Region

Hafızoğlu, Gel, and Keskinocak: ETA Computations

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

370

INFORMS Journal on Computing 25(2), pp. 364–376, © 2013 INFORMS

(H) approximates j 4d5 by the value Hwj 4d5 and the
pseudocode of the algorithm is given in the online
supplement.

higher than i exceeds four. For example, a manufacturer with a production capacity of four or more per
day, and receiving an average of one or more orders
per day from the highest priority customer class satisfies this condition (when the lead times are quoted
in daily or weekly time units, as is common practice in industry). Hence, the condition is not restrictive
in practice. Under the cases where Theorem 3 conditions do not hold, we may still have convexity of
j−1 4d − x5g¯ i 1 c 4x5, which is a sufficient condition for
Theorem 3 to hold.
Although numerical integration is commonly used
in Laplace transformation methods (Abate and Whitt
1992), M and Z are the first NILT algorithms that
implement numerical integration methods in a recursive manner as given in Equations (14) and (15).
In addition, to the best of our knowledge, they
are the first algorithms providing lower and upper
bounds for the inverse Laplace transform operation. The obtained lower and upper bounds enable a
novel methodology in the comparison of GP-NILTAs.
That is, any approximation value less than Mwj 4d5
is definitely a worse approximation than Mwj 4d5.
Similarly, any approximation higher than Zwj 4d5
is a worse approximation than Zwj 4d5. Using these
bounds, we compute worst-case error bounds for
C-NILTAs in §6. One can achieve tighter bounds
(smaller Zwj 4d5 − Mwj 4d5 values) by using lower values
of w, at the expense of increased computational times
due to the increased number of steps in numerical
integration.
While developing our C-NILTAs, we made use
of the product form of two Laplace transforms
(as shown in Equation (10)) to obtain the closedform convolution integral. Hence, given any Laplace
transform that can be inverted into a closed-form
inversion integral, one can implement numerical
integration and develop C-NILTAs with the same
structure. For example, such closed-form inversion
integrals are provided by Duffy (1993), who presents
five Laplace transform examples with corresponding
inverse transforms provided as integral equations.
To improve on the accuracy provided by the GPNILTAs, we also present a hybrid C-NILTA, which
applies trapezoidal/midpoint rule for odd/even j
values for j ∈ 801 11 0 0 0 1 q̄i 9. The hybrid algorithm

Figure 2

Corollary 2. If Theorem 3 conditions hold, then
Mwj 4d5 ≤ Hwj 4d5 ≤ Zwj 4d5 for d ≥ 0, j = 01 11 0 0 0 1 and
w ≤ d.
Thus, when Theorem 3 conditions hold, H always
generates an approximation within the bounds given
by Z and M. Note that Hwj 4d5 may or may not be a
better approximation than Mwj 4d5, Zwj 4d5, or any value
obtained by any GP-NILTAs. However, it is within the
derived worst-case bounds that are discussed in §6.
We next discuss the evaluation of Sq̄i . Wein (1991)
derives the expected TIS for class-dependent service
rates and a single server. We evaluate the expected
TIS of the marked order by modifying the necessary
parameters, e.g., service rates, as follows:
Sq̄i =

q̄i + 1
c − ¯ i

+

1
0


(16)

Finally, Tq̄i 4d5 can be evaluated using Z, M, H, or any
of GP-NILTA, and Equations (1) and (16).

5.

Preemptive Priority Discipline

We now consider the case where an arriving order
of higher priority preempts the lowest priority inprocess order, which, upon being preempted, joins the
beginning of the queue pushing back all the orders
in the queue by one position. In this case, any class i
P
order is inserted behind v̄i = ij=1 vj orders (including
the orders in process), pushing back the remaining
P
v0 − ij=1 vj orders by one position. Our goal is to evaluate Tv̄i 1 i 4d5. In Figure 2, we illustrate a three-server
case, where the numbers inside and outside the circles denote the priority class and position of the order,
respectively. If a class 2 order arrives when the state
is as depicted in Figure 2, it is inserted to the second
position, pushing all the lower priority orders back by
one position, and the processing of the class 4 order
in the third position is preempted.
Similar to §4, the highest priority orders are placed
ahead of the other orders in the queue, and sequenced
on a FCFS basis. Thus, the ETA for the highest priority

2

3

4

4

5

5

6

7

7

8

9

1

2

3

4

5

6

7

8

9

10

11

Position of Orders Under Preemptive Discipline

Hafızoğlu, Gel, and Keskinocak: ETA Computations

371

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

INFORMS Journal on Computing 25(2), pp. 364–376, © 2013 INFORMS

class, Tv̄1 1 1 4d5, can be evaluated by plugging v1 for
v0 in Equation (8). For the lower priority classes,
i ∈ 821 0 0 0 1 N 9, similar to the nonpreemptive case, we
have two alternatives to consider: (i) the marked
order starts processing immediately (v̄i ≤ c − 1), and
(ii) the marked order joins the queue (v̄i > c − 1). As in
the nonpreemptive case, we drop i from the notation,
and derive f˜v̄i 4s5.
Case I: v̄i > c − 1. If this condition holds, then the
marked order joins the queue by taking the position v̄i + 1. The marked order is pushed back by the
arrival of any higher priority arrival with rate ¯ i , and
moves one position forward with rate c, as long as
its processing has not started yet. Thus, the Laplace
transforms for v̄i > c − 1 are obtained in Equation (17),
similar to the nonpreemption case given in Equation (10):
f˜v̄i 4s5 = f˜c−1 4s54g̃¯ i 1 c 4s55v̄i −c+1 1

s ∈ 0

(17)

Case II: v̄i ≤ c − 1. Laplace transforms for this case
are derived in Theorem 4.
Theorem 4. For v̄i ≤ c − 1, i ∈ 821 0 0 0 1 N 9,
f˜v̄i 4s5 =

¯ i f˜v̄i +1 4s5 + v̄i f˜v̄i −1 4s5 + 
1
s + ¯ i + 4v̄i + 15

s ∈ 0

(18)

s ∈ 0

By setting v̄i = c − 1, using Equation (17) and algebraic
operations we get
4c − 15f˜c−2 4s5 + 
f˜c−1 4s5 =
s + ¯ i + c − ¯ i g̃¯ 1 c 4s5
i

= g̃¯ i 1 c 4s544c − 15f˜c−2 4s5 + 151

s ∈ 0

(19)

Consequently, having c unknowns and c equations
by Theorem 4, Laplace transforms are solvable. The
analysis simplifies vastly in the single server case, i.e.,
c = 1, which is discussed below.
Single Server Case. Setting c = 1 in Equation (19)
and by algebraic manipulations one gets for i ∈
821 31 0 0 0 1 N 9
f˜v̄i 4s5 = 4g̃¯ i 1  4s55v̄i +1 1

and
(20)

v̄i +1

˜v̄i 4s5 =

4g̃¯ i 1  4s55
s2

s ∈ 0

2
−1 4x5 = L−1
x 81/s 9 = x0

Moreover,
Sv̄i =

v̄i + 1
0
 − ¯ i

We note that, results of Theorem 3 and Corollary 1
hold for this case as well.
We derive the Laplace transforms and discuss the
implementation of Z, M, H for c = 2 and c = 3 in the
online supplement. The useful property of Theorem 3
may or may not hold for the multiple servers case
because of the extensive complexity of the Laplace
transforms in these cases. In the next section, we test
the performances of Z, M, H, as well as the three
prominent GP-NILTAs for the cases where we can
obtain worst-case error bounds using Theorem 3, i.e.,
nonpreemptive and preemptive single-server cases.

6.

Letting  = 1 in Equation (18), one reaches the
result of Segal (1970). However, the result in Theorem 4 cannot be derived by rescaling the time
variable, i.e., setting ¯ i / for ¯ i in Segal (1970).
To prove Theorem 4, we use Laplace-Stieltjes transforms, as presented in the online supplement.
By setting v̄i = 0, the middle term in the numerator
in Equation (18) disappears, which gives
¯ f˜ 4s5 + 
1
f˜0 4s5 = i 1
s + ¯ i + 

Thus, the recursive form in Equation (12) is valid
for inversion of ˜v̄i 4s5 given in Equation (20) with a
modification on −1 4x5 as

Computational Analysis

In this section, we provide a computational study to
(i) analyze the precision of approximations obtained
by M, Z, and H, and (ii) assess the performance of
three prominent GP-NILTAs, namely, Gaver-Stehfest
(G), Euler (E), and Talbot (T). Abate and Whitt (2006)
provide a general framework for GP-NILTAs and
reduce the GP-NILTAs in the literature into these
three main algorithms. We then provide a practical
recommendation scheme for the inverse transformation of ˜j 4s5. To evaluate the error precision, we assess
the number of significant digits obtained by M, Z, and
H in Equation (21) (see Equation (34) of Abate and
Whitt 2006):

 w
 Xj 4d5−j 4d5 
1 X ∈ 8M1Z1H90 (21)
P 4Xjw 54d5 = −log

j 4d5
Because the exact values of j 4d5 are not known,
we define the following approximation for the number of significant digits obtained by Z, M, and H:
 w

 Zj 4d5 − Mwj 4d5 
w
0
j 4d5 = − log
(22)

Mwj 4d5
Note that, if Theorem 3 conditions holds, we have
wj 4d5 ≤ P 4Xjw 54d5, X ∈ 8M1 Z1 H9, i.e., wj 4d5 is a lower
bound for the number of significant digits produced
by Z, M, and H, and hence, represents the worstcase error bounds for the three C-NILTAs developed
in this paper. The same condition applies to any
GP-NILTA only when the obtained approximation
value is within the interval 6Mwj 4d51 Zwj 4d57.
GP-NILTAs generally require the setting of several parameters as in the case of the Euler algorithm

Hafızoğlu, Gel, and Keskinocak: ETA Computations

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

372

INFORMS Journal on Computing 25(2), pp. 364–376, © 2013 INFORMS

(Abate et al. 1999), which necessitates the selection
of four different parameters. Abate and Whitt (2006)
overcome this burden by reducing the number of
parameters in G, E, and T to a single parameter called
M, which is common to all of the three algorithms. The
authors recommend that M should be set to 101R,
107R, and 107R to obtain R significant digits in
G, E, and T, respectively (see sections 4, 5, and 6 in
Abate and Whitt 2006). We denote the required parameter setting to obtain R significant digits by MX 4R5,
X ∈ 8G1 E1 T9, i.e., MG 4R5 = 101R, ME 4R5 = 107R,
MT 4R5 = 107R. The approximations obtained using
these parameter settings are denoted as XjR 4d5.
Our testing methodology works as follows. For
each instance, we first generate the approximations
Zwj 4d5, Mwj 4d5, and compute wj 4d5, for four levels of
w ∈ 8000051 000011 0000051 0000019; ten levels of d ∈ D =
811 21 0 0 0 1 109; and ten levels of j ∈ J = 801 11 0 0 0 1 99,
by running Z and M with q̄i = 9, and d¯ = 10. We then
generate approximations GRj 4d5, ERj 4d5, and TRj 4d5 for
d ∈ D and j ∈ J , and R ∈ 851 79. Finally, for each 4w1 R5
pair in 8000051 000011 0000051 0000019 × 851 79, we count
Table 1

the number of instances in which the approximations GRj 4d5, ERj 4d5, and TRj 4d5 are within the interval
6Mwj 4d51 Zwj 4d57, over all d ∈ D and j ∈ J .
We use Visual C++ 6.0 compiler that allows a
system precision of 18 digits, which is sufficient for
our test bed. The maximum value of R is chosen
as 7, because the system precision requirement for G
exceeds 18 for R > 8 (Abate and Whitt 2006). We select
a minimum value of w = 000001 to avoid excessively
long computational times.
We test eight instances under the nonpreemptive
case with c = 1,  = 5, ¯ i ∈ 811 21 31 49, and c = 3,
 = 5, ¯ i ∈ 831 61 91 129, and four instances under the
preemptive case with c = 1,  = 5, ¯ i ∈ 811 21 31 49,
to test one representative case for single and multiple servers under changing traffic intensity. Note that
all of the instances comply with Theorem 3 conditions and the stability condition. The results for the
nonpreemptive and preemptive cases are presented
in Tables 1 and 2, respectively. For each instance-w
pair represented in a row, we report (i) the average,

Performances of NILT Algorithms Under Nonpreemptive Case
wj 4d5

Zwj 4d5 − Mwj 4d5

No. of gen. within 6Mwj 4d51 Zwj 4d57

CPU time (sec.)

w

c



¯ i

Avg.

Min.

Max.

Avg.

Min.

Max.

0.005

1
1
1
1
3
3
3
3

5
5
5
5
5
5
5
5

1
2
3
4
3
6
9
12

1073
1055
2033
3029
1000
0094
0081
1005

−1052
−1049
0043
2004
−0078
−1020
−1024
−0058

5047
5081
6020
6071
1097
2048
3003
3070

2.E−03
2.E−03
2.E−03
1.E−03
2.E−02
2.E−02
2.E−02
2.E−02

5.E−06
3.E−06
2.E−06
1.E−06
7.E−04
8.E−04
8.E−04
5.E−04

0.001

1
1
1
1
3
3
3
3

5
5
5
5
5
5
5
5

1
2
3
4
3
6
9
12

2036
2056
3073
4069
1039
1048
1029
2049

−1011
−1025
1087
3044
−1058
−1028
−2030
1016

6087
7021
7060
8011
3037
3088
4043
5010

8.E−05
8.E−05
8.E−05
7.E−05
9.E−04
1.E−03
1.E−03
1.E−03

0.0005

1
1
1
1
3
3
3
3

5
5
5
5
5
5
5
5

1
2
3
4
3
6
9
12

2057
3018
4033
5029
1084
1079
1061
3010

−3054
0009
2048
4004
−1021
−0090
−1036
1077

7047
7081
8020
8071
3097
4048
5003
5070

0.0001

1
1
1
1
3
3
3
3

5
5
5
5
5
5
5
5

1
2
3
4
3
6
9
12

3038
4058
5073
6069
2056
2036
2053
4049

−0092
1059
3087
5044
−1006
−1049
−0063
3017

8087
9021
9060
10011
5037
5088
6043
7010

M

Z

G(5)

E(5)

T(5)

G(7)

E(7)

T(7)

7.E−03
7.E−03
7.E−03
5.E−03
7.E−02
8.E−02
9.E−02
8.E−02

0021
0021
0021
0021
0021
0021
0021
0021

0017
0017
0017
0017
0017
0017
0017
0017

59
74
90
90
100
99
99
100

100
100
100
100
100
100
100
100

92
94
94
95
100
100
100
100

90
95
98
99
100
100
100
100

100
100
100
100
100
100
100
100

91
95
94
95
100
100
100
100

2.E−07
1.E−07
1.E−07
7.E−08
2.E−05
3.E−05
3.E−05
2.E−05

2.E−04
3.E−04
2.E−04
2.E−04
3.E−03
3.E−03
3.E−03
3.E−03

5037
5037
5036
5036
5036
5035
5037
5035

4039
4037
4039
4039
4039
4039
4039
4039

8
23
39
54
82
77
88
95

87
89
97
98
100
100
100
100

86
91
92
93
100
100
100
100

39
58
82
91
93
99
99
100

100
100
100
100
100
100
100
100

85
92
93
93
100
100
100
100

2.E−05
2.E−05
2.E−05
1.E−05
2.E−04
2.E−04
2.E−04
2.E−04

5.E−08
4.E−08
3.E−08
1.E−08
7.E−06
8.E−06
8.E−06
5.E−06

7.E−05
8.E−05
7.E−05
5.E−05
7.E−04
8.E−04
9.E−04
8.E−04

21043
21040
21038
21038
21038
21038
21041
21043

17050
17051
17051
17051
17051
17053
17051
17054

1
3
16
24
61
59
60
83

34
40
39
44
100
100
100
100

79
90
91
91
99
100
100
100

12
34
48
71
83
86
97
98

100
100
100
100
100
100
100
100

85
91
91
93
100
100
100
100

8.E−07
8.E−07
8.E−07
7.E−07
9.E−06
1.E−05
1.E−05
1.E−05

2.E−09
1.E−09
1.E−09
7.E−10
2.E−07
3.E−07
3.E−07
2.E−07

2.E−06
3.E−06
2.E−06
2.E−06
3.E−05
3.E−05
3.E−05
3.E−05

540003
538073
538079
538074
538091
539006
539004
539022

435062
435059
435061
435064
435065
435063
435064
435071

0
0
1
0
12
12
2
18

0
0
0
0
0
0
0
0

44
52
71
84
90
95
100
100

0
0
4
13
34
19
51
70

86
88
97
98
100
100
100
100

82
87
89
90
98
100
100
100

Hafızoğlu, Gel, and Keskinocak: ETA Computations

373

INFORMS Journal on Computing 25(2), pp. 364–376, © 2013 INFORMS

Table 2

Performances of NILT Algorithms Under Preemptive Case

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

wj 4d5

Zwj 4d5 − Mwj 4d5

w

c



¯ i

Avg.

Min.

Max.

Avg.

Min.

Max.

0.005

1
1
1
1

5
5
5
5

1
2
3
4

2027
2036
2064
3029

1042
1041
1065
2030

5011
5046
5086
6038

2.E−03
2.E−03
2.E−03
2.E−03

1.E−05
9.E−06
6.E−06
4.E−06

7.E−03
8.E−03
7.E−03
6.E−03

0021
0021
0023
0021

0.001

1
1
1
1

5
5
5
5

1
2
3
4

3067
3076
4004
4069

2082
2081
3006
3070

6051
6086
7026
7078

9.E−05
9.E−05
9.E−05
8.E−05

5.E−07
3.E−07
2.E−07
2.E−07

3.E−04
3.E−04
3.E−04
2.E−04

0.0005

1
1
1
1

5
5
5
5

1
2
3
4

4027
4036
4064
5029

3043
3041
3066
4030

7011
7046
7086
8038

2.E−05
2.E−05
2.E−05
2.E−05

1.E−07
9.E−08
6.E−08
4.E−08

0.0001

1
1
1
1

5
5
5
5

1
2
3
4

5067
5076
6004
6069

4082
4081
5006
5070

8051
8086
9026
9078

9.E−07
9.E−07
9.E−07
8.E−07

5.E−09
3.E−09
2.E−09
2.E−09

minimum, and maximum wj 4d5 over all d ∈ D, j ∈ J ;
(ii) the average, minimum and maximum Zwj1 i 4d5 −
Mwj1 i 4d5, over all d ∈ D, j ∈ J ; (iii) total CPU times of
Z and M; and (iv) the number of GRj 4d5, ERj 4d5, and
TRj 4d5 that are within 6Mwj1 i 4d51 Zwj1 i 4d57 over all d ∈ D,
j ∈ J (i.e., out of 100 instances), for R ∈ 851 79, where R
is indicated inside the parenthesis as G4R5, E4R5, and
T4R5. The average, minimum, and maximum values
of wj1 i 4d5 are evaluated only for nonnegative values of
Mwj1 i 4d5 because of the logarithm restriction in Equation (22). We also omit the CPU times of G, E, and
T because they are negligibly small. Experiments are
run on a Intel Core 2 Quad CPU PC with processors running at 2.66 GHz and 4 GB memory under
Windows 7.
Observation 1. Up to eight significant digit precision
is obtained in around 20 seconds by Z, M, and H, whose
performances significantly improve at a cost of higher computational times.
Because Z, M, and H provide approximations that
are guaranteed to be within the bounds, we conclude
that our three proposed C-NILTAs are the best NILT
methods in terms of solution quality. From Tables 1
and 2, when c = 1, up to nine significant digits are
attained by either of Z, M, and H, under preemptive
and nonpreemptive cases. One also observes that the
increase in the number of servers negatively impacts
the performance of the algorithms by reducing wj 4d5.
For example, when w = 000005, the average wj 4d5
ranges from 2.57 to 5.29 when c = 1, and from 1.61
to 3.10 when c = 3. The traffic intensity, on the other
hand, has a positive impact on the performance of
the algorithms, as observed under most instances in
Tables 1 and 2. Comparison of preemptive and nonpreemptive cases reveals that slightly higher wj 4d5

No. of gen. within 6Mwj 4d51 Zwj 4d57

CPU time (sec.)
Mid.

Trap.

G(5)

E(5)

T(5)

G(7)

E(7)

T(7)

0017
0017
0018
0017

71
85
92
94

100
100
100
100

92
93
95
95

96
97
100
100

100
100
100
100

91
93
95
95

5038
5036
5036
5036

4039
4039
4038
4039

18
29
48
61

89
96
98
99

90
92
91
92

56
71
93
94

100
100
100
100

88
91
92
93

7.E−05
8.E−05
7.E−05
6.E−05

21044
21043
21043
21042

17054
17054
17054
17054

0
9
17
29

38
42
43
50

83
91
90
91

28
48
73
90

100
100
100
100

84
89
91
92

3.E−06
3.E−06
3.E−06
2.E−06

545022
544021
544013
544072

437012
437003
436096
437009

0
2
0
0

0
0
0
0

49
59
85
88

1
2
3
19

88
89
98
99

80
85
88
90

values are obtained for the preemptive case. This is
due to the lower ETA values under the preemptive
case resulting in lower Mwj 4d5 values, whereas Zwj 4d5−
Mwj 4d5 differences do not differ significantly under
preemptive and nonpreemptive cases. Moreover, we
observe that lower w values lead to an increase in
both CPU times and wj 4d5.
Observation 2. E outperforms G and T.
Results of Observation 2 are seen on the right-most
six columns of Tables 1 and 2, with some exceptions
for R = 5, w ∈ 80000051 0000019, where T performs
better. Using the parameter setting ME 475, E generates approximations 100% within the bounds until
the maximums of wj 4d5 exceed 8.5 (see the results
for w = 000001). Similarly, under the nonpreemptive
case, E fails to generate approximations 100% within
the bounds only when wj 4d5 maximums reach 8.87
and 6.87, respectively, for the settings with ME 475,
and ME 455 (see the first row for w = 000001, and
the fifth row for w = 00001 in Table 1, respectively).
We conclude that E approximations are reliable for
R ≥ wj 4d5 − 1, when the parameter setting ME 4R5
is used.
In some instancesG performs surprisingly poorly.
For example, consider the nonpreemptive case results
in Table 1 for w = 00001. When it is executed with
the parameter setting MG 455, 92% of the approximations are out of the bounds, where the average of
wj 4d5 is only 2.36. This may occur because of two
main reasons: (i) the significant digits produced by Z
and M are higher than five, (recall that wj 4d5 gives a
lower bound), (ii) the parameter setting MG 455 gives
fewer significant digits than five, probably even less
than 2.36. The performances of E and T for the same
instance, which are 87%, and 88%, respectively, imply

Hafızoğlu, Gel, and Keskinocak: ETA Computations

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

374

INFORMS Journal on Computing 25(2), pp. 364–376, © 2013 INFORMS

that case (i) is not possible. Hence, we conclude that
the precision analysis of Abate and Whitt (2006) does
not hold for this particular instance.
In summary, Z, M, and H perform best in terms of
solution quality, whereas E, G, and T run faster but
at the expense of lower solution quality. Although we
cannot determine which one of Z, M, and H gives
better solution quality on the average, we recommend
the use of H, which is the hybrid version of the other
two algorithms. The algorithmic structures reveal that
under the same settings, H runs longer than Z and
shorter than M. On the other hand, E is the best NILT
algorithm when fast solutions are needed, and the
precision requirements are relaxed.

The first two cases identify the conditions, where H is
recommended because of its superior solution quality.
In the third case, because of tight computational time
restrictions, E is recommended with a guarantee that
the estimation is within the error tolerances. Therefore, the presented recommendation scheme ensures
that the selected NILT algorithm gives approximations within LPE error tolerance and acceptable computation times.
As discussed in §4, for the cases where the sufficient
conditions for Mwj 4d5 ≤ j 4d5 ≤ Zwj 4d5 do not hold, the
bounds are still likely to be valid. Hence, we suggest the use of the above recommendation scheme in
general.

6.1. NILT Algorithm Recommendations
In this section, we obtain an upper bound on the
expected LPE error, and develop a recommendation
scheme for the use of an appropriate NILT algorithm.
Consider the moment when a class i customer arrives
to the system when there are j orders in the queue.
Assuming that the customer is quoted a lead time
of d, and the late delivery penalty cost per unit time
is k dollars per unit time, the expected late delivery
penalties to be incurred can be expressed as kTj1 i 4d5.
Using Theorem 3 and Equation (1), kTj1 i 4d5 can be
bounded as


k Sz1i −d +Mwj1i 4d5 ≤ kTj1i 4d5 ≤ k Sz1i −d +Zwj1i 4d5 1 (23)

6.2. An Example
Consider a make-to-order manufacturer, who promises to pay k dollars to its customers per each day the
product is delivered late, and wants to analyze the
impact of lead time quotes on late delivery penalty
costs. Assume that there is a single server working
at a rate of five per day; three priority classes with
arrival rates of 1 = 2, 2 = 1, and 3 = 1 per day;
preemption is not allowed; and the manufacturer considers only ten lead time options as D̄ = 811 21 0 0 0 1 109
days, and quotes due dates to newly arriving customers when there are less than 10 orders in the system, i.e., J¯ = 801 11 0 0 0 1 99.
In Table 3, we present the maximum LPE errors and
computational times obtained by using H with w ∈
8000051 000011 0000051 0000019 over all j ∈ J¯, d ∈ D̄, and
i ∈ 821 39, for k ∈ 811000$/day1 101000$/day9. We note
that the LPE for priority class 1 can be obtained with
zero error using Theorem 1, and ¯2 = 1 = 2, ¯3 = 1 +
2 = 3.
Using the information from Table 3 and the recommendation scheme in §6.1, we develop NILT algorithm recommendations for three levels of K̄ ∈ 8$10, $1,
$0019, and C̄ ∈ 810 sec., 1 min., 10 min.9, for two delay penalty levels, k ∈ 8$11000/day, $101000/day9 in
Tables 4 and 5, respectively. We select the smallest w
in case there are multiple w ∈ W1 ∩ W2 .

where we include the priority class index i back into
the notation to avoid confusion. Using Equation (23),
kTj1 i 4d5 can be estimated with a maximum error of
k4Zwj1 i 4d5 − Mwj1 i 4d55. Let C4Hwj1 i 4d55 denote the computational time of H for approximating j1 i 4d5, within
the LPE error tolerance, K̄, and maximum allowable
computational time for NILT of the manufacturer, C̄,
when w ∈ W1 and w ∈ W2 , respectively. That is,
n
o
W1 ∈ w2 max k4Zwj1 i 4d5 − Mwj1 i 4d55 ≤ K̄ 1 and
d∈D̄1 j∈J¯1 i∈I¯

n
W2 ∈ w2

max

d∈D̄1 j∈J¯1 i∈I¯

o
C4Hwj1 i 4d55 ≤ C̄ 1

where D̄, J¯, and I¯ denote the set of d, j, and i values, respectively, under consideration for due date
quotation. The NILT recommendation scheme is as
follows:
• If there are no computational time restrictions,
implement H with setting w ∈ W1 ;
• if there are computational time restrictions, and
W1 ∩ W2 6= , implement H with setting w ∈ W1 ∩ W2 ;
• if there are computational time restrictions, and
W1 ∩ W2 = , find a w ∗ ∈ W1 . Then, implement E with
∗
∗
setting R such that ERj1 i 4d5 ∈ 6Mwj1 i 4d51 Zwj1 i 4d57 for all
¯
d ∈ D̄, j ∈ J¯, and i ∈ I.

Table 3

Maximum Expected Lateness Penalty Cost Estimation Errors
and CPU Times Obtained by H
Priority class 2

k ($)
1,000/day

Priority class 3

w

Max.
LPE ($)

Max. CPU time

Max.
LPE ($)

Max. CPU time

00005
00001
000005
000001

7
003
0008
00003

Less than 1 sec.
Around 5 sec.
Around 20 sec.
Around 8 min.

7
002
0007
00002

Less than 1 sec.
Around 5 sec.
Around 20 sec.
Around 8 min.

70
3
008
0003

Less than 1 sec.
Around 5 sec.
Around 20 sec.
Around 8 min.

70
2
007
0002

Less than 1 sec.
Around 5 sec.
Around 20 sec.
Around 8 min.

10,000/day 00005
00001
000005
000001

Hafızoğlu, Gel, and Keskinocak: ETA Computations

375

INFORMS Journal on Computing 25(2), pp. 364–376, © 2013 INFORMS

Table 4

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

K̄ ($)

NILT Recommendations for k = $11000/day
C̄

w within W1

w within W2

Recommendation

10
10
10

10 min.
1 min.
10 sec.

{0.005, 0.001, 0.0005, 0.0001}
{0.005, 0.001, 0.0005, 0.0001}
{0.005, 0.001, 0.0005, 0.0001}

{0.005, 0.001, 0.0005, 0.0001}
{0.005, 0.001, 0.0005}
{0.005, 0.001}

H with w = 000001
H with w = 000005
H with w = 00001

1
1
1

10 min.
1 min.
10 sec.

{0.001, 0.0005, 0.0001}
{0.001, 0.0005, 0.0001}
{0.001, 0.0005, 0.0001}

{0.005, 0.001, 0.0005, 0.0001}
{0.005, 0.001, 0.0005}
{0.005, 0.001}

H with w = 000001
H with w = 000005
H with w = 00001

0.1
0.1
0.1

10 min.
1 min.
10 sec.

{0.0005, 0.0001}
{0.0005, 0.0001}
{0.0005, 0.0001}

{0.005, 0.001, 0.0005, 0.0001}
{0.005, 0.001, 0.0005}
{0.005, 0.001}

H with w = 000001
H with w = 000005
E with R = 7

numerical inverse transformation algorithms that run
faster. We finally develop a recommendation scheme
on the selection of the numerical inverse transformation algorithm given an error tolerance and allowable
computational time, and illustrate the recommendation scheme on an example.
Our study is unique in a couple of aspects.
First, to the best of our knowledge, it is the first
study addressing the evaluation of ETA in a multiclass queuing environment. Second, it presents three
numerical inverse Laplace transformation algorithms
with worst-case error bounds. Third, it provides a
novel methodology for performance evaluation of
numerical inverse Laplace transformation methods.
Fourth, the Laplace transforms for the preemptive priority settings are rederived to generalize the previous
results for  = 1 to any service rate.
Our research is expected to contribute to the
dynamic due date quotation literature to extend those
studies to multiserver and multiclass cases. A worthwhile extension to our study is the consideration
of class-dependent service rates, i.e., i for class i.
This case is more complex because the total service
rate depends on the class of orders being processed,
and the class of the higher priority arrival joining
in front of the order affects the ETA. Extensions to
more general M/G/c or G/G/c models are more difficult to handle, however, approximations based on the
results outlined in this paper may provide plausible
solutions.

Note that when k = $101000/day and K̄ = $001, only
w = 000001 gives results within error tolerances, but
it does not satisfy the computational time restriction
when C̄ ≤ 1 min; hence, the use of E is recommended.
∗
∗
Because, the condition ERj1 i 4d5 ∈ 6Mwj1 i 4d51 Zwj1 i 4d57 does
¯ R ∈ 851 79, and w ∗ =
not hold for d ∈ D̄, j ∈ J¯, i ∈ I,
000001, E should be executed with a parameter setting
giving better solution quality, i.e., R > 7.

7.

Conclusions

In this study, we discuss the computation of the
expected tardiness of an order as a function of the system state at the moment of the order’s arrival in an
M/M/c queue with N priority classes. The study is
motivated by the need for tardiness computations in
dynamic due date quotation models considering multiple priority classes. We first formulate the Laplace
transform of expected tardiness and discuss the
numerical inverse Laplace transformation methodologies to approximate the expected tardiness. We
develop two customized numerical inverse Laplace
transformation algorithms giving upper and lower
bounds for the expected tardiness, and another hybrid
algorithm that is guaranteed to give approximations well inside the bounds. Using the two bounds,
we obtain worst-case error bounds for the developed algorithms, and test them using a computational study. Noting that these customized algorithms
may require long computational times, we also test
the performance of three prominent general purpose
Table 5
K̄ 4$5

NILT Recommendations for k = $101000/day
C̄

w within W1

w within W2

Recommendation

10
10
10

10 min.
1 min.
10 sec.

{0.001, 0.0005, 0.0001}
{0.001, 0.0005, 0.0001}
{0.001, 0.0005, 0.0001}

{0.005, 0.001, 0.0005, 0.0001}
{0.005, 0.001, 0.0005}
{0.005, 0.001}

H with w = 000001
H with w = 000005
H with w = 00001

1
1
1

10 min.
1 min.
10 sec.

{0.0005, 0.0001}
{0.0005, 0.0001}
{0.0005, 0.0001}

{0.005, 0.001, 0.0005, 0.0001}
{0.005, 0.001, 0.0005}
{0.005, 0.001}

H with w = 000001
H with w = 000005
E with R = 7

0.1
0.1
0.1

10 min.
1 min.
10 sec.

{0.0001}
{0.0001}
{0.0001}

{0.005, 0.001, 0.0005, 0.0001}
{0.005, 0.001, 0.0005}
{0.005, 0.001}

H with w = 000001
E with R > 7
E with R > 7

376
Electronic Companion
An electronic companion to this paper is available as
part of the online version at http://dx.doi.org/10.1287/
ijoc.1120.0507.

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

Acknowledgments
The authors would like to thank the associate editor and
two anonymous referees whose comments greatly improved
the paper. This research has been supported in part by the
National Science Foundation [Grant DMI-0621012].

References
Abate GL, Choudhury J, Whitt W (1999) An introduction to
numerical inversion and its applications to probability models. Computational Probability (Kluwer Academic Publishers,
Norwell, MA).
Abate J, Valko PP (2004) Multi-precision Laplace transform
inversion. Internat. J. Numerical Methods Engrg. 60(5):979–993.
Abate J, Whitt W (1992) The Fourier-series method for inverting transforms of probability distributions. Queueing Systems
10(1–2):5–88.
Abate J, Whitt W (2006) A unified framework for numerically
inverting Laplace transforms. INFORMS J. Comput. 18(4):
408–421.
Avdis E, Whitt W (2007) Power algorithms for inverting Laplace
transforms. INFORMS J. Comput. 19(3):341–355.
Baker KR, Bertrand JWM (1981) A comparison of due-date
selection-rules. AIIE Trans. 13(2):123–131.
Bertrand JWM (1983) The effect of workload dependent due-dates
on job shop performance. Management Sci. 29(7):799–816.
Bookbinder JH, Noor AI (1985) Setting job-shop due-dates with
service-level constraints. J. Oper. Res. Soc. 36(11):1017–1026.
Davis PJ, Rabinowitz P (1984) Methods of Numerical Integration
(Academic Press Inc., Orlando, FL).
Davis RH (1966) Waiting-time distribution of a multiserver priority
queuing system. Oper. Res. 14(1):133–136.
Doetsch G (1974) Introduction to the Theory and Application of the
Laplace Transforms (Springer-Verlag, New York).
Dubner H, Abate J (1968) Numerical inversion of Laplace transforms by relating them to finite Fourier cosine transform.
J. ACM 15(1):115–123.
Duenyas I (1995) Single facility due-date setting with multiple customer classes. Management Sci. 41(4):608–619.
Duenyas I, Hopp WJ (1995) Quoting customer lead times.
Management Sci. 41(1):43–57.

Hafızoğlu, Gel, and Keskinocak: ETA Computations
INFORMS Journal on Computing 25(2), pp. 364–376, © 2013 INFORMS

Duffy DG (1993) On the numerical inversion of Laplace transforms:
Comparison of three new methods on characteristic problems
from applications. ACM Trans. Math. Software 19(3):333–359.
Feng J, Liu L, Liu X (2011) An optimal policy for joint dynamic
price and leadtime quotation. Oper. Res. 59(6):1523–1527.
Gaver DP (1966) Observing stochastic processes and approximate
transform inversion. Oper. Res. 14(3):444–459.
Hassanzadeh H, Pooladi-Darvish M (2007) Comparison of different numerical Laplace inversion methods for engineering
applications. Appl. Math. Comput. 189(2):1966–1981.
Heyman DP, Sobel MJ (2004) Stochastic Models in Operations
Research, Vol. 1. (Dover, Mineola, NY).
Hunsucker JL, Shah JR (1992) Performance of priority rules in a due
date flow-shop. Omega-Internat. J. Management Sci. 20(1):73–89.
Jagerman DL, Melamed B (2003) Models and approximations
for call center design. Methodology Comput. Appl. Probab.
5(2):159–181.
Keskinocak P, Tayur S (2004) Due date management policies.
Handbook of Quantitative Supply Chain Analysis: Modeling in the
eBusiness Era (Kluwer Academic Publishers, Norwell, MA).
Lean-manufacturing-japan.com (2012) MTO (make to order).
Retrieved February 18, http://www.lean-manufacturing-japan
.com/scm-terminology/mto-make-to-order.html.
Merleblanc.de (2012) General purchasing terms and conditions of
the company Merle Blanc UG & Co.KG. Retrieved February 18,
http://merleblanc.de/en/ekb.html.
Motorola (2012) 3 year bronze service plan overview. Retrieved February 18, http://www.dcrs.com/pics/db/document_library/
000077.pdf.
Paterok M, Ettl M (1994) Sojourn time and waiting time distributions for M/GI/1 queues with preemption-distance priorities.
Oper. Res. 42(6):1146–1161.
Savasaneril S, Griffin PM, Keskinocak P (2010) Dynamic lead-time
quotation for an M/M/1 base-stock inventory queue. Oper. Res.
58(2):383–395.
Segal M (1970) A multiserver system with preemptive priorities.
Oper. Res. 18(2):316–323.
Stanford DA, Drekic S (2000) Interdeparture time distributions
in Sigma M-i(i)/G(i)/1 priority queues. Queueing Systems
36(1–3):1–21.
Stehfest H (1970) Numerical inversion of Laplace transforms.
Comm. ACM 13(1):47–49.
Wein LM (1991) Due-date setting and priority sequencing in a multiclass M/G/1 queue. Management Sci. 37(7):834–850.
Zakian V (1969) Numerical inversion of Laplace transform.
Electronics Lett. 5(6):120–121.
Zeltyn S, Feldman Z, Wasserkrug S (2009) Waiting and sojourn
times in a multi-server queue with mixed priorities. Queueing
Systems 61(4):305–328.

Available online at www.sciencedirect.com

European Journal of Operational Research 190 (2008) 724–740
www.elsevier.com/locate/ejor

O.R. Applications

Heuristics for workforce planning with worker diﬀerences
John W. Fowler *, Pornsarun Wirojanagud, Esma S. Gel
Department of Industrial Engineering, Arizona State University, Tempe, AZ 85287, United States
Received 16 August 2006; accepted 20 June 2007
Available online 4 July 2007

Abstract
This study considers decisions in workforce management assuming individual workers are inherently diﬀerent as measured by general cognitive ability (GCA). A mixed integer programming (MIP) model that determines diﬀerent staﬃng
decisions (i.e., hire, cross-train, and ﬁre) in order to minimize workforce related costs over multiple periods is described.
Solving the MIP for a large problem instance size is computationally burdensome. In this paper, two linear programming
(LP) based heuristics and a solution space partition approach are presented to reduce the computational time. A genetic
algorithm was also implemented as an alternative method to obtain better solutions and for comparison to the heuristics
proposed. The heuristics were applied to realistic manufacturing systems with a large number of machine groups. Experimental results shows that performance of the LP based heuristics performance are surprisingly good and indicate that the
heuristics can solve large problem instances eﬀectively with reasonable computational eﬀort.
Ó 2007 Elsevier B.V. All rights reserved.
Keywords: Workforce planning; Cross-training; Heuristics; Genetic algorithms

1. Introduction
Workforce ﬂexibility is an eﬀective buﬀer against various forms of variability in manufacturing systems. It
can be achieved in several ways including the use of temporary workers, overtime, and cross-trained workers.
In this paper, we focus on workforce ﬂexibility obtained by cross-training. Cross-training enables companies
to utilize workers more eﬀectively. Moreover, cross-training is also known to have other beneﬁts such as
improved communication, increased worker learning, and higher worker morale. Cross-training, however,
can be very expensive due to training costs, increased worker salaries, and loss of labor or eﬃciency during
the period of training. Thus, cross-training should be implemented after careful consideration of various alternatives and their costs.
In Wirojanagud et al. (2007), a workforce planning model that incorporates individual worker diﬀerences in
ability to learn new skills and perform tasks was presented. The model is a mixed integer programming (MIP)
model that allows a number of diﬀerent staﬃng decisions (i.e., hire, cross-train, and ﬁre) in order to minimize

*

Corresponding author. Tel.: +1 480 965 3727.
E-mail address: john.fowler@asu.edu (J.W. Fowler).

0377-2217/$ - see front matter Ó 2007 Elsevier B.V. All rights reserved.
doi:10.1016/j.ejor.2007.06.038

J.W. Fowler et al. / European Journal of Operational Research 190 (2008) 724–740

725

workforce related and missed production costs. General cognitive ability (GCA), deﬁned as the ability to learn
or process information (Hunter, 1986), was used to model individual diﬀerences in the eﬃcacy of cross-training and productivity. Further details on GCA can be found in Hunter (1986), Thorndike (1986) and Schmidt
and Hunter (1998).
The numerical experiments in Wirojanagud et al. (2007) using realistic manufacturing data showed that
when the problem instance size is large (e.g., 25 machine groups), the optimal solution could not be obtained
within a reasonable time. A solution space partition approach (referred to as the ‘‘decomposition approach’’
in Wirojanagud et al., 2007), which makes use of groupings of skills for cross-training, was proposed in order
to reduce the problem instance size. However, the computational time for the solution space partition
approach can still be quite large (approximately 10 hours) and is dominated by larger subgroups. In addition,
solving the problem by linear programming (LP) relaxation can take a relatively long computational time
(e.g., approximately 25 minutes). Thus, algorithms such as branch and bound that requires solving the LP
relaxation many times are not appropriate for solving large problem instances. The objective of this paper
is to develop simple and eﬀective heuristics that reduce the computational time required to solve workforce
planning problems of realistic size.
Wee (1999), Walsh (2000) and Wirojanagud et al. (2007) have shown that the major motivations for crosstraining, in deterministic planning models, are due to varying demand and fractional worker requirements.
The two heuristics presented in this paper are based on the latter observation. Based on the LP relaxation solutions, the heuristics attempt to transform the fractional solutions to feasible solutions by either rounding up or
rounding down to the next integer value. Rounding up creates overstaﬃng while rounding down creates
missed production. Therefore, the heuristics decide whether to round up or round down the solutions and
attempts to cross-train the excess number of workers resulting from rounding up to cover the demand that
is not fulﬁlled from rounding down.
In addition to the two LP based heuristics proposed, a genetic algorithm (GA) is implemented. GAs are
commonly used for solving combinatorial optimization problems and often yield optimal or near optimal
solutions (Goldberg, 1989). GAs have been applied to solve many production and operations management
problems (Aytug et al., 2003). In this paper, a GA is used as a benchmark for the proposed heuristics and also
as an alternative method that practitioners could use to obtain better solutions. Furthermore, as mentioned
above, the computation time for the solution space partition approach is dictated by larger subgroups. In this
paper, we illustrate that the LP based heuristics proposed can be used to solve the entire problem as well as
applied to subproblems in the solution space partition approach.
The contributions of this paper include: (1) two LP based heuristics for a workforce planning model that
considers worker diﬀerences, (2) a genetic algorithm for solving the model, and (3) a case study comparing the
performance of the heuristics and a solution space partition approach developed in Wirojanagud et al. (2007).
In the next section, we review the relevant literature, which is followed by the model description in Section
3. Section 4 includes a description of the two heuristics proposed and a description of the genetic algorithm.
Experimental results and insights are presented in Section 5. The ﬁrst experiment utilized a dataset from a
semiconductor manufacturing system with a large number of machine groups and worker skills. The second
set of experiments investigates the performance of the heuristics by using randomly generated datasets. We
summarize the conclusions of our study and suggest future research in the last section.
2. Literature review
In this section, we cover the literature on workforce planning and cross-training. Many quantitative models
including optimization, heuristics, and simulation have been developed in order to address worker cross-training problems. In the optimization area, several researchers utilized integer programming (IP) models to determine worker assignments, workforce scheduling, and optimal training policies. For example, Ebeling and Lee
(1994) developed an IP model to make cost-eﬀective cross-training assignments. Suer (1996) developed an MIP
that generates alternative operator levels and an IP to ﬁnd the optimal operator assignments to the cells. Billionnet (1999) formulated the problem of scheduling a workforce assignment with diﬀerent levels of worker
qualiﬁcations in order to minimize labor costs. Norman et al. (2002) proposed an MIP model for assigning
workers to manufacturing cells in order to maximize the proﬁt. There is considerable other research in this

726

J.W. Fowler et al. / European Journal of Operational Research 190 (2008) 724–740

ﬁeld, some of the more relevant examples are Stewart et al. (1994), Brusco and Johns (1998), Brusco et al.
(1998) and Campbell (1999).
The papers discussed above focused on using IP to ﬁnd the optimal solution. However, as the model complexity increases, it is often impossible to solve realistic IP workforce planning models within a reasonable
amount of time. Several researchers have developed heuristic algorithms for solving these problems. Vairaktarakis and Winch (1999) formulated and developed heuristic algorithms to solve the problem of scheduling
production with cross-training in a paced mixed-model assembly system. The heuristics are based on solving
a lower bound and using branch and bound. Nembhard (2001) described a greedy heuristic approach based on
individual learning rate for the improvement of productivity in organizations through targeted assignment of
workers to tasks. Askin and Huang (2001) studied several heuristics (i.e., greedy, beam search, and simulated
annealing) for a multiobjective optimization model for the formation of worker teams and a cross-training
plan for cellular manufacturing. Results showed that the beam search heuristic is preferable to simulated
annealing when memory and computation time are limited. Campbell and Diaby (2002) proposed an assignment heuristic for allocation of cross-trained workers.
Loucks and Jacobs (1991) developed a heuristic for worker assignments in order to minimize overstaﬃng.
The heuristic used the correction approach on a relaxation of the problem, i.e., the problem is solved with a
constraint relaxed and the heuristic ﬁxes the violation. The correction approach was also used by Vairaktarakis et al. (2002) to develop heuristic algorithms for minimizing workforce size and the number of production
cycles for a paced job shop and by French and Wilson (2002) to develop heuristics for the multi level generalized assignment problem. Vairaktarakis et al. (2002) showed that the heuristics have near-optimal performance. In our research, we also use the correction approach on a relaxation in developing the heuristics.
While the three papers discussed above relaxed the upper bound constraints (e.g., capacity constraints) in their
models, we relax the integrality constraints.
There are several LP based heuristics for labor scheduling. LP based heuristic solutions are obtained by
solving the LP and then modifying the solution to eliminate the fractional values. Morris and Showalter
(1983) used a round down/build up approach in developing heuristics for shift, days-oﬀ, and tour scheduling
problems. Tour scheduling problems are problems which address both shift and days-oﬀ scheduling. The solutions were on average 0.46% from the bound. Using a similar approach, Showalter and Mabert (1987) proposed heuristics for full-/part-time tour scheduling. They showed that the heuristic provided optimal or
near-optimal solutions. Henderson and Berry (1976) developed an LP round up heuristic for telephone operator scheduling. Other LP based research includes work by Keith (1979), Bechtold et al. (1991) and Wullink
et al. (2004).
This paper presents two LP based heuristics and a genetic algorithm (discussed in Section 4) in order to
solve the workforce planning problem. The heuristics are compared to the solution space partition approach
of Wirojanagud et al. (2007). The model description follows in the next section.
3. Model description
The MIP model used in this paper is taken from Wirojanagud et al. (2007). It is described in this section for
completeness. In the model, the skill set of a worker refers to the set of machine groups that he/she can operate. For example, an empty skill set refers to a newly hired worker with no skills. Skill sets with one skill refer
to a specialized worker who can only operate one machine group, and skill sets with two or more skills correspond to those of cross-trained workers.
The parameters of the model are deﬁned below. Note that in the following, a refers to skill sets before training and b refers to skill sets after training, where a  b. In addition, we refer to a worker with a GCA level of
g 2 f1; . . . ; Gg as a g-level worker.
M
S
A
T
G

number of skills
set of all non-empty skill subsets, hence |S| = 2M  1
available working time per worker in hours per time period
number of time periods
number of GCA levels

J.W. Fowler et al. / European Journal of Operational Research 190 (2008) 724–740

htag
ctabg
stag
t
fag
t
pi
cgi
kag
dabg
xti

727

hiring cost for a g-level worker with skill set a in period t
cost of training a g-level worker from skill set a to skill set b in period t
salary (in dollars/month) for a g-level worker with skill set a in period t
penalty cost for ﬁring a g-level worker with skill set a in period t
penalty cost for allowing one unit of missed production for skill i in period t
working productivity for a g-level worker in skill i, where 0 6 cgi 6 1
training productivity for a newly hired g-level worker with skill set a, where 0 6 kag 6 1
cross-training productivity for a g-level worker who is being trained from skill set a to skill set b where
0 6 dabg 6 1
demand for skill i in period t (in worker hours per time period)

The decision variables are deﬁned as follows:
U tag
X tabg
ntag
W tag
Y tagi
Z ti

number
number
number
number
number
number

of
of
of
of
of
of

g-level workers with skill set a hired in period t, integer
g-level workers to be trained from skill set a to skill set b in period t, integer
g-level workers with skill set a in period t (bookkeeping variable)
g-level workers with skill set a ﬁred in period t, integer
g-level workers with skill set a assigned to skill i in period t, where i 2 a  f1; 2; . . . ; Mg
hours of unsatisﬁed skill i requirement in period t

The problem can be formulated as an MIP as follows:
"
!
#
T
G
M
X
X
X
XX
X
X
X
t1
t
t
t
t
t
t
t
t t
min
hag U ag þ
ðcabg X abg Þ þ
sag nag þ
fag W ag þ
pi Z i
t¼1

"
subject to

A

a2S

g¼1

G
X
X

a2S

b2S ab

#

cgi Y tagi

þ Z ti ¼ xti

a2S

8i 2 f1; . . . ; Mg;

ð1Þ

i¼1

8t 2 T ;

ð2Þ

g¼1 a:i2a2S

X

t1
X tabg þ W tag 6 nag

b:ab
t1
nag
þ kag U tag þ

X

8a; b 2 S;

dmag X tmag 

m:ma

ntag

¼

t1
nag

þ U tag þ

8t 2 T ;

X tabg  W tag P

m:ma

X tmag 

X

X

Y tagi

ð3Þ
8a; b 2 S;

8g 2 G;

8t 2 T ;

i2a

b:ab

X

U tag ; X tabg ; W tag P 0;

X

8g 2 G;

ð4Þ
X tabg  W tag

8a; b 2 S;

8g 2 G;

8t 2 T ;

ð5Þ

b:ab

integer; ntag ; Z ti ; Y tagi P 0:

ð6Þ

The bookkeeping variable, ntag , is used to update and adjust the number of workers in each skill set according to the cross-training decisions in each period. The objective function (1) minimizes total costs over T periods. The total cost consists of hiring, training, salary, and ﬁring costs as well as missed production costs for the
M skills over the periods of the planning horizon. For the costs, we assume that: (i) training costs are determined based on the GCA level as well as the skills that the worker is trained for, and (ii) salary and ﬁring costs
are determined based on the GCA level as well as the worker’s skills.
Constraint set (2) ensures that the total available worker hours plus missed production is equal to the number of hours required for each skill. Constraint set (3) limits the number of workers to be cross-trained from
skill set a to skill set b in period t and the number of g-level workers with skill set a ﬁred in period t to the
number of workers with skill set a at the end of period t  1. Constraint set (4) ensures that there are no more
assignments to skills in a than the available g-level workers with skill set a. Constraint set (5) is the set of
worker balance equations. It updates the number of workers with skill set a for period t. Finally, constraint
set (6) represents the non-negativity and integrality restrictions on the decision variables.
Wirojanagud et al. (2007) provides additional details on the formulation and also indicates that the number
of decision variables increases geometrically with the number of skills considered. Since there is no special

728

J.W. Fowler et al. / European Journal of Operational Research 190 (2008) 724–740

structure to exploit, the time to solve this problem to optimality increases rapidly as M increases. Therefore, in
the next section we discuss heuristics developed for this problem.
4. Proposed heuristics
The two heuristics proposed are linear programming (LP) based heuristics. They transform the fractional LP
solution into a feasible solution by increasing (decreasing) hiring and cross-training (ﬁring) decision variables to
the next higher (lower) integer value creating overstaﬃng or decreasing (increasing) them to the next lower
(higher) integer value creating missed production. The approach also attempts to cross-train the excess number
of workers resulting from rounding up in order to cover demand for worker levels rounded down. In addition,
we also used a genetic algorithm (GA) and the solution space partition approach presented in Wirojanagud et al.
(2007) to obtain solutions, for the purpose of evaluating the performance of the LPH1 and LPH2 heuristics.
4.1. LP Heuristic 1 (LPH1)
The LPH1 heuristic begins by solving the LP relaxation for the problem. Then, starting at period 1, the
fractional values of the decision variables are sorted in descending order, and the variable with the highest
fractional value is selected ﬁrst. The highest fractional value is selected in order to minimize the extra hours
from rounding up to an integer value. There are two options for the selected variable, rounding up or rounding down to an integer value resulting in hiring/overstaﬃng or missed production, respectively. The hiring,
cross-training, and missed production costs are calculated for rounding up and rounding down, respectively.
The option that gives the minimum cost is selected for this variable. If missed production is selected, the variable is rounded down, and the next variable in the fractional value list is selected for consideration. If hiring is
selected, then the extra hours from rounding up is calculated. The other variables not in consideration are temporarily rounded down to the closest integer value, which results in missed production. The hours required for
each skill are calculated. The extra hours are compared to the skill with the minimum hours required; if there
are enough extra hours then the worker is cross-trained to satisfy the hours required. When all the time
required for each skill in the ﬁrst period is satisﬁed, the logic is repeated for periods 2 to T. Note that the solution at the end of period t is used as the initial number of workers for period t + 1, and the LP is not rerun for
periods 2 to T. The following summarizes the procedure for LPH1.
LP Heuristic 1
Stage 1
Step 0. Let t = 1
Step 1. Solve the LP relaxation
Step 2. Let L = set of variable values for a given solution from LP relaxation, L ¼ fU tag ; X tabg ; W tag g.
Rank the fractional value of the variables, U tag ; X tabg and W tag , in descending order.
t
t
Step 3. Select U tag ; X tabg or W tag with the highest fractional value, U t
ag ; X abg or W ag .
t
t
t
Delete U ag ; X abg or W ag from L.
t
t
t
Step 4. Round up U t
ag ; X abg or W ag . Calculate the worker cost, wcag , for hiring, cross-training or ﬁring (e.g.,
t
t
t
the worker cost for hiring is wcag ¼ hag U ag Þ.
t
t
t
t t
Step 5. Round down U t
ag ; X abg or W ag . Calculate the missed production cost, mci ¼ p i Z i .
t
t
Step 6. Select round up or round down from min fwcag ; mci g.
If round up, then
t
t
t
Calculate the extra hours from rounding the variable
PGup,Phei ¼ hai t xi
t
t
(where hai is the workers hour available, hai ¼ A½ g¼1 a:i2a2S cgi Y agi )
Go to step 7.
If round down, then
Go to step 3.
Step 7. Round down all variables in L; U tag ; X tabg and W tag .
Calculate the remaining hours required for all skills, hrti ¼ xti  hati .
Rank hrti in ascending order.

J.W. Fowler et al. / European Journal of Operational Research 190 (2008) 724–740

729

Step 8. Check if cross-training is possible, starting from the skill that has the minimum requirement.
t
If het
i  hr i > 0, then
Cross-train the worker and update the variables.
Go to step 9.
Step 9. If demand for all the skills are fulﬁlled, hrti ¼ 0 for all skills i, then
Go to step 11.
Step 10. If L = / and demand for any skill is not fulﬁlled, hrti 6¼ 0, then
For all skills i; . . . ; M which hrti > 0
– Hire workers, U tag , by selecting the one with the minimum productivity/cost ratio. Calculate
t
t
worker cost, wct
ag ¼ hag U ag .
t t
– Allow missed production. Calculate missed production cost, mct
i ¼ pi Z i .
t
Select min fwct
ag ; mci g.
Go to step 11.
Else
Go to step 3.
Stage 2
Step 11. Let t = t + 1
Step 12. Calculate the extra hours, heti ¼ hati  xti , and hours required for all skills, hrti ¼ xti  hati .
Rank heti and hrti in the ascending order.
Let E = set of skills with positive heti
Step 13. Check if cross-training is possible, starting from the skill that has minimum requirement.
t
If het
i  hr i > 0, then
Cross-train the worker and update the variables.
Go to step 14.
Step 14. If demand for all the skills are fulﬁlled, hrti ¼ 0 for all skills i, then
Go to step 16.
Step 15. If E = / and demand for any skill is not fulﬁlled, hrti 6¼ 0, then
For all skills i; . . . ; M which hrti > 0
– Hire workers, U tag , by selecting the one with the minimum productivity/cost ratio. Calculate
t
t
worker cost, wct
ag ¼ hag U ag
t t
– Allow missed production. Calculate missed production cost, mct
i ¼ pi Z i .
t
t
Select minfwcag ; mci g
Go to step 16.
Else
Go to step 13.
Step 16. Let t = t + 1
If t < T, then
Go to step 12.
Else
Stop
In LPH1, the heuristic solves for the results by considering one period at a time. In the next section, we
propose an improved heuristic that considers other periods in solving for the results.

4.2. LP Heuristic 2 (LPH2)
The procedure for LPH2 is similar to LPH1. The main diﬀerence between the two heuristics is that in
LPH1, the LP relaxation is solved only once in the ﬁrst period (it considers all periods but only uses the results
for the ﬁrst period) while in the LPH2, the LP relaxation is solved at the beginning of every period. This generally improves the solution because solving the LP at the beginning of each period looks forward to the

730

J.W. Fowler et al. / European Journal of Operational Research 190 (2008) 724–740

remaining periods. In each period after the ﬁrst, the worker proﬁle from the previous period is used as the
initial number of workers for the LP. The procedure for LPH2 is as follows.
LP Heuristic 2
Step 0. Let t = 1
Step 1. Solve LP relaxation for periods t to T
Step 2. Let L = set of variable values for a given solution from LP relaxation, L ¼ fU tag ; X tabg ; W tag g.
Rank the fractional value of the variables, U tag ; X tabg and W tag , in descending order.
t
t
Step 3. Select U tag ; X tabg or W tag with the highest fractional value, U t
ag ; X abg or W ag .
t
t
t
Delete U ag ; X abg or W ag from L.
t
t
t
Step 4. Round up U t
ag ; X abg or W ag . Calculate the worker cost, wcag , for hiring, cross-training or ﬁring (e.g.,
t
t
t
the worker cost for hiring is wcag ¼ hag U ag Þ.
t
t
t
t t
Step 5. Round down U t
ag ; X abg or W ag . Calculate the missed production cost, mci ¼ p i Z i .
t
t
Step 6. Select round up or round down from minfwcag ; mci g.
If round up, then
t
t
Calculate the extra hours from rounding the variable up, het
i ¼ hai  xi .
Go to step 7.
If round down, then
Go to step 3.
Step 7. Round down all variables in L; U tag ; X tabg and W tag .
Calculate the hours required for all skills, hrti ¼ xti  hati .
Rank hrti in ascending order.
Step 8. Check if cross-training is possible, starting from the skill that has the minimum requirement.
t
If het
i  hr i > 0, then
Cross-train the worker and update the variables.
Go to step 9.
Step 9. If demand for all the skills are fulﬁlled, hrti ¼0 for all skills i, then
Go to step 11.
Step 10. If L = / and demand for any skill is not fulﬁlled, hrti 6¼ 0, then
For all skills i; . . . ; M which hrti > 0
– Hire workers, U tag , by selecting the one with the minimum productivity/cost ratio.
t
t
Calculate worker cost, wct
ag ¼ hag U ag .
t t
– Allow missed production. Calculate missed production cost, mct
i ¼ pi Z i .
t
t
Select minfwcag ; mci g.
Go to step 11.
Else
Go to step 3.
Step 11. Let t = t + 1
If t < T, then
Go to step 1.
Else
Stop
In order to evaluate the performance of the two LP based proposed heuristics described above, a GA was
implemented to solve the problem. The GA parameters and steps are discussed in the next section.

4.3. Genetic algorithm (GA)
A Genetic algorithm (GA) is a metaheuristic technique that has been used for solving combinatorial optimization problems and has been shown to provide near optimal or optimal solutions. An introduction to GA,
its properties, and its applications can be found in Goldberg (1989) and Man et al. (1996). Dimopoulus and
Zalzala (2000) conducted a survey for the use of GA’s in manufacturing. Several researchers have used a GA

J.W. Fowler et al. / European Journal of Operational Research 190 (2008) 724–740

731

in assignment and scheduling problems. An extensive review and classiﬁcation of GA applications in production and operations management problems is provided by Aytug et al. (2003). We implemented a GA as
another method to solve the workforce planning problem and for the purpose of evaluating the performance
of the LPH1 and LPH2 heuristics since it is not possible to obtain exact solutions for some of the large test
instances we use. The basic steps for the GA are shown in Fig. 1, and the GA components are described in
more detail below.
– Chromosome encoding. The chromosome represents the decision variables, U tag ; X tabg , and W tag . It is
expressed as vectors of positive integers. The value of each gene in the chromosome represents the number
of g-level workers hired, cross-trained, or ﬁred in each period t. The hiring and ﬁring variables are ordered
by the workers’ skill set a, GCA level g, and period t, respectively. The cross-training variables are ordered
by the workers’ skill set before cross-training, a, skill set after (possible) cross-training, b (where a  b),
GCA level g, and period t, respectively. The length of the chromosome depends on the number of skills,
GCA levels, and periods. As an example, the encoding scheme for a three skills, two-GCA level, and
one-period problem is
h
U 1f1g1 ; U 1f2g1 ; U 1f3g1 ; U 1f1;2g1 ; U 1f1;3g1 ; U 1f2;3g1 ; U 1f1;2;3g1 ; U 1f1g2 ; U 1f2g2 ; U 1f3g2 ; U 1f1;2g2 ; U 1f1;3g2 ; U 1f2;3g2 ; U 1f1;2;3g2 ;
X 1f1gf1;2g1 ; X 1f1gf1;3g1 ; X 1f1gf1;2;3g1 ; X 1f2gf1;2g1 ; X 1f2gf2;3g1 ; X 1f2gf1;2;3g1 ; X 1f3gf1;3g1 ; X 1f3gf2;3g1 ; X 1f3gf1;2;3g1 ; X 1f1;2gf1;2;3g1 ;
X 1f1;3gf1;2;3g1 ; X 1f2;3gf1;2;3g1 ; X 1f1gf1;2g2 ; X 1f1gf1;3g2 ; X 1f1gf1;2;3g2 ; X 1f2gf1;2g2 ; X 1f2gf2;3g2 ; X 1f2gf1;2;3g2 ; X 1f3gf1;3g2 ; X 1f3gf2;3g2 ;
X 1f3gf1;2;3g2; X 1f1;2gf1;2;3g2 ; X 1f1;3gf1;2;3g2 ; X 1f2;3gf1;2;3g2 ; W 1f1g1 ; W 1f2g1 ; W 1f3g1 ; W 1f1;2g1 ; W 1f1;3g1 ; W 1f2;3g1 ; W 1f1;2;3g1 ; W 1f1g2 ;
i
W 1f2g2 ; W 1f3g2 ; W 1f1;2g2 ; W 1f1;3g2 ; W 1f2;3g2 ; W 1f1;2;3g2 :
– Initialization. The initial population consists of the solutions from LPH1, LPH2, the solution space partition approach, and randomly generated chromosomes.
– Fitness evaluation. Fitness of a solution is calculated as the value of the objective function.
– Selecting candidates. The parents are selected based on their ﬁtness. The methodology used for selecting the
parents is roulette wheel selection (Goldberg, 1989). The chromosomes in the population will be placed in
the roulette wheel according to their normalized ﬁtness. Chromosomes with larger ﬁtness have a better
chance of being selected because they occupy more space in the roulette wheel. A random number is generated and a chromosome is picked. Since this is a minimization problem, the ﬁtness is transformed such
that the smaller total cost will have larger ﬁtness. Therefore, the ﬁtness fj of chromosome j is equal to1/total
costj, where total costj is from (1).
– Reproduction. In the experiments, we generate a child solution by ﬁrst applying one point crossover to
selected parents (Goldberg, 1989). Two parent chromosomes are selected randomly according to their ﬁtness and the crossover point is randomly chosen. Mutation is applied to the new oﬀspring after crossover.

Step 0.
Step 1.
Step 2.
Step 3.
3.1.
3.2.
3.3.
3.4.
3.5.
Step 4.

Generate initial population of N chromosomes
Check feasibility and repair the chromosomes that are infeasible
Evaluate the fitness of each chromosome in the population
Create new population by the following steps until the new population is complete
Selection: Select two parent chromosomes according to their fitness
Crossover: Crossover the parents to produce new offspring. If no crossover was performed,
then the offspring are copies of the parents.
Mutation: A gene is randomly selected and a new offspring is produced.
Elitism: Select the best chromosome to be a member of the next generation
Insert new offspring in a new population
Check the stopping condition
If the stopping condition is satisfied, then
Stop and return best solution in current population
Else
Go to step 1
Fig. 1. Steps for the genetic algorithm.

732

J.W. Fowler et al. / European Journal of Operational Research 190 (2008) 724–740

It is performed by subtracting one worker from the current value of the gene selected. Elitism is also used in
the reproduction, the best chromosome is always carried over to the next generation.
– Feasibility. Crossover and/or mutation might generate infeasible solutions. In our model, the infeasibility is
due to elements in constraint set (3). To ensure feasibility, the oﬀspring is repaired by subtracting the difference between the number of workers cross-trained from each skill set a to skill set b in period t for chromosome j and the initial number of workers with skill set a at the beginning of period t for each infeasible
variable.
– Stopping criteria. The algorithm terminates after a ﬁxed number of function evaluations, which equals to
the number of generations times the population size or when the best solution in the population has not
decreased in the last 50 generations.

4.3.1. Parameter settings for the GA
The values for the GA parameters are selected based on preliminary experiments. Three parameters, crossover probability, mutation probability, and population size were studied. A 23 factorial design was used to
determine the best parameter values. The total number of function evaluations was set to 25,000. The ranges
for each parameter values are selected based on common practice in the GA literature (e.g., Goldberg, 1989).
The values for each parameter and the results from the four-skills model (problem instance 1 in Section 5.2.1)
are shown in Table 1. Note that the ‘Total cost’ column is the average total cost (objective function value) of
10 runs with diﬀerent random number seeds.
From the experiments, the crossover probability of 0.8, the mutation rate of 0.1, and the population size of
50 chromosomes (resulting in 500 generations) provides the best solution. Therefore, these values are used as
the GA parameter settings for the experiments in Section 5.
4.4. Solution space partition approach (SSP)
In this section, we brieﬂy explain the solution space partition approach (SSP) which was referred to as the
‘‘decomposition approach’’ in Wirojanagud et al. (2007). SSP reduces the problem size by making use of the
fact that cross-training workers within subgroup of machines is likely to be almost as eﬀective as cross-training
workers among all machines in a factory. Machines groups could be clustered by task similarity, process/
logical ﬂow, physical machine layout, or organization boundaries. Further details about machine groups
and partitioning schemes are discussed in Wirojanagud et al. (2007). The steps for SSP are as follow.
Step 1.
Step 2.
Step 3.
Step 4.

Divide the skills into subgroups,
Allow cross-training only within the subgroups,
Find the optimal solution for each subgroup,
Combine the optimal solution from each subgroup to obtain the solution for the whole problem.

In the next section, the heuristics presented are applied to case studies in order to evaluate the performance
of the heuristics.
Table 1
GA parameter settings and results from problem instance 1 (Section 5.2.1)
Crossover probability

Mutation probability

Population size

Total cost ($)

0.6
0.6
0.6
0.6
0.8
0.8
0.8
0.8

0.1
0.1
0.2
0.2
0.1
0.1
0.2
0.2

50
100
50
100
50
100
50
100

799,307
774,261
748,648
779,470
727,773
731,727
753,013
728,761

J.W. Fowler et al. / European Journal of Operational Research 190 (2008) 724–740

733

5. Experimental results
The heuristics developed in Section 4 were tested using two sets of experiments. The ﬁrst experiment (Section 5.1) applies the heuristic to a case study in semiconductor manufacturing for the purpose of comparing
the heuristic results to those of the solution space partition approach proposed in Wirojanagud et al. (2007).
The second set of experiments (Section 5.2) investigates the performance of the heuristics with randomly generated problem instances. Two models were studied in this experiment: a four-skill, three-GCA level, threeperiod model and a 25-skill, three-GCA level, six-period model.
5.1. Case study from semiconductor manufacturing
The proposed heuristics, LPH1 and LPH2, were applied to the data from a case study presented in Wirojanagud et al. (2007). The dataset consists of two product ﬂows with 25 diﬀerent machine groups (skills). The
25 skills were divided into groups according to location (i.e., area) in the factory and organizational boundaries. A three-period and three-GCA level (i.e., G = 3) model was considered in this study. GCA levels 1, 2,
and 3 represent the highest, medium, and lowest GCA levels, respectively. Further details for this model and
dataset are described in Wirojanagud et al. (2007).
The solution for the LP relaxation for the model is presented as a comparison basis. We also present the
solution obtained by (1) rounding all variables up (LPRU), (2) rounding all variables down (LPRD), (3)
rounding each variable to the closest integer (LPRC) from the LP relaxation solution. Rounding the LP relaxation solution represents a simple heuristic for obtaining an integer solution. We compare these simple heuristics based on the rounding of the LP relaxation and the GA to the SSP, LPH1, and LPH2 solutions. The
total cost (both workforce costs and missed production costs) and computation times for all methodologies
are shown in Table 2.
In the case of LPRU, the result is infeasible due to violations in constraint set (3) that the number of workers cross-trained from skill set a should not exceed the number of workers in skill set a in the beginning of
period t  1. Rounding up creates overstaﬃng, therefore, no missed production should occur. LPRU* is
the case where the infeasibility from rounding up is repaired. In this case, missed production can occur because
of the variable that is adjusted to avoid infeasibility. The solution for LPRU* is 14.32% higher than the LP
relaxation result. For the case of LPRD, the result shows a 268.30% diﬀerence from LP relaxation solution:
this is mainly due to the missed production costs. In the last one of the simple LP relaxation heuristics, LPRC,
the result is 61.15% diﬀerent from the LP relaxation solution. In this case, there are signiﬁcant costs from both
overstaﬃng and missed production.
The total cost for LPH1 and LPH2 are 4.90% higher than the LP relaxation solution, which represents a
lower bound. The total cost for the SSP approach is 4.18% higher than the LP relaxation solution. It can also
Table 2
Experimental results for the case study from semiconductor manufacturing
Methodology

Workforce
cost ($)

Missed production
cost ($)

Total cost
($)

%

Time
(seconds)

LP relaxation (LP)
LP relaxation-round up (LPRU)
LP relaxation-round up* (LPRU*)
LP relaxation-round down (LPRD)
LP relaxation-round to closest integer (LPRC)
LPH1
LPH2
Solution space partition (SSP) approach
SSP using LPH1
SSP using LPH2
Genetic Algorithm (GA)

658,138
–
749,841
549,775
677,462
690,355
690,355
685,644
689,280
689,280
684,967

0
–
2562
1,874,140
383,095
0
0
0
715
715
0

658,138
Infeasible
752,403
2,423,915
1,060,557
690,355
690,355
685,644
689,995
689,995
684,967

–
–
14.32
268.30
61.15
4.90
4.90
4.18
4.84
4.84
4.08

1512.9
1801.5
1846.2
1806.5
1794.0
5632.4
6001.1
37,472.6
3.7
3.8
39,461.4

Note. Percentage column is the percentage above LP relaxation.
LPRU* is the case where the infeasibility from rounding up is repaired.

734

J.W. Fowler et al. / European Journal of Operational Research 190 (2008) 724–740

be seen that the performance of the LPH1, LPH2, and SSP approach are quite a bit better than the simple
rounding LP relaxation heuristic solutions. Though the solutions from the LPH1, LPH2, and SSP approach
are similar, the computational times for the LPH1 and LPH2 are signiﬁcantly less than the time for the SSP
approach. This is because in the SSP approach, each subgroup is solved optimally. The diﬀerence between the
SSP approach and the LPH1 and LPH2 solutions is that in LPH1 and LPH2, workers can be cross-trained for
any of the 25 skills while in the SSP approach workers can be cross-trained only within the subgroup. However, the SSP approach provides the optimal IP solution within the subgroups. Also, as expected, LPH2 has a
higher computation time than LPH1. This is because in LPH2, the LP is solved for each period while in LPH1,
the LP is solved only once in the ﬁrst period. We note that in this example, LPH1 and LPH2 provide the same
solution. We do not expect this in general.
The SSP approach was also solved by using LPH1 and LPH2 to solve the decomposed set of workers. The
solutions for both heuristics were equal but again LPH2 took slightly more computation time. The total cost
for this approach is about 4.84% higher than the LP relaxation solution. The solution is only slightly higher
than the original SSP. However, the computation time is signiﬁcantly less than the original SSP approach. The
GA provided the best result, approximately 4% higher than the LP relaxation solution but it took the longest
computational time. It can also be seen that, the SSP approach and the GA had similar results and computational time.
The results show that hiring and cross-training of workers varied between all GCA levels depending on the
demand structure, productivity, and cost. In general, the LP results illustrated that all workers are hired with
one skill, and some are cross-trained to two skills. For the integer programming models in the SSP approach,
it can be seen that if the required worker hours are high, specialized workers are hired. If the required worker
hours are low, cross-trained workers are hired. Cross-training of workers also occurs if there are large diﬀerences in the demand from period to period for one or more machine group.
For LPH1 and LPH2, since the initial result is based on the LP relaxation, all workers are hired with one
skill. Cross-training of workers occurs in the skills where the required hours are low. These workers are crosstrained from the skills that are overstaﬀed, which results from rounding up the fractional solutions. Note that
if the initial settings for the experiment are modiﬁed, the results will be diﬀerent.
5.2. Randomly generated datasets
In this section, LPH1 and LPH2 were tested rigorously on two models: a four-skill, three-GCA level,
three-period model and a 25-skills, three-GCA level, six-period model where the problem instances (20 for
the four-skill model and 10 for the 25-skill model) were randomly generated. GCA level 1, 2, and 3 represent
the highest, medium, and lowest GCA levels, respectively.
5.2.1. Four skills model
In the ﬁrst experiment set, a four-skills, three-GCA-level, and three-period model was studied. This is a
problem instance in which the optimal solution to the MIP can be obtained within a reasonable time. Twenty
problem instances were randomly generated varying the initial number of workers, demand, productivity factors, and costs. The ranges of the values for the parameters are shown in Table 3. The notation U ½x; y indicates that for a given problem instance, a value is generated from a continuous uniform distribution between x
and y. The hiring, cross-training, salary, and ﬁring costs are assumed to be higher for higher GCA level workers. The details of these assumptions are discussed in Wirojanagud et al. (2007). Cross-training costs decrease
by a random value distributed by U[0.1,0.4] as time increases because when a worker is cross-trained later in
the time horizon, their new skill is used for a shorter amount of time. The hiring cost is set to be the sum of the
salary and the training cost for a given skill set. Salary, ﬁring, and missed production costs are assumed to be
equal for all periods.
For each problem instance, the integer programming model, LP relaxation, LPH1, and LPH2 are solved.
The following simple LP relaxation heuristics results are also included: (1) LPRU (2) LPRD, and (3) LPRC.
The optimal cost and the percentage diﬀerence from optimal for all methodologies are shown in Table 4.
In the case of LPRU, which results in overstaﬃng, the diﬀerence from optimality is 0.57% to 2.95% with a
mean of 1.60%. In this case, infeasible results might occur due to the violation in constraint set (3), the number

J.W. Fowler et al. / European Journal of Operational Research 190 (2008) 724–740

735

Table 3
Ranges of initial parameter settings for the experiments
Parameters

Values

Number of skills
Number of GCA levels
Number of periods

4
3
3

Hiring productivity

k1g

k2g

k3g

GCA level 1
GCA level 2
GCA level 3

U½0:8; 0:95
k11  ðk11  U½0:1; 0:4Þ
k12  ðk12  U½0:1; 0:4Þ

k11  ðk11  U ½0:1; 0:4Þ
k21  ðk21  U ½0:1; 0:4Þ
k22  ðk22  U ½0:1; 0:4Þ

k21  ðk21  U½0:1; 0:4Þ
k31  ðk31  U½0:1; 0:4Þ
k32  ðk32  U½0:1; 0:4Þ

Training productivity

dj1j;j2j;g

dj1j;j3j;g

dj2j;j3j;g

GCA level 1
GCA level 2
GCA level 3

U½0:85; 0:95
d121  ðd121  U½0:1; 0:4Þ
d122  ðd122  U½0:1; 0:4Þ

d121  ðd121  U ½0:1; 0:4Þ
d131  ðd131  U ½0:1; 0:4Þ
d132  ðd132  U ½0:1; 0:4Þ

U ½0:9; 0:95
d231  ðd231  U ½0:1; 0:4Þ
d232  ðd232  U ½0:1; 0:4Þ

Worker productivity

cgi

GCA level 1
GCA level 2
GCA level 3

U½0:8; 1
c1i  ðc1i  U ½0:1; 0:4Þ
c2i  ðc2i  U ½0:1; 0:4Þ

Hiring costs

h1g

h2g

h3g

GCA level 1
GCA level 2
GCA level 3

U½2035; 3045
h11  ðh11  U½0:1; 0:4Þ
h12  ðh12  U½0:1; 0:4Þ

h11 þ ðh11  U ½0:1; 0:4Þ
h21  ðh21  U ½0:1; 0:4Þ
h22  ðh22  U ½0:1; 0:4Þ

h21 þ ðh21  U½0:1; 0:4Þ
h31  ðh31  U½0:1; 0:4Þ
h32  ðh32  U½0:1; 0:4Þ

Cross-training costs

cj1j;j2j;g

cj1j;j3j;g

cj2j;j3j;g

GCA level 1
GCA level 2
GCA level 3

U½20; 30
c121  ðc121  U ½0:1; 0:4Þ
c122  ðc122  U ½0:1; 0:4Þ

c121 þ ðc121  U½0:1; 0:4Þ
c121  ðc121  U½0:1; 0:4Þ
c122  ðc122  U½0:1; 0:4Þ

U½10; 20
c231  ðc231  U ½0:1; 0:4Þ
c232  ðc232  U ½0:1; 0:4Þ

Salary

s1g

s2g

s3g

GCA level 1
GCA level 2
GCA level 3

U½2000; 3000
s11  ðs11  U½0:1; 0:4Þ
s12  ðs12  U½0:1; 0:4Þ

s11 þ ðs11  U ½0:1; 0:4Þ
s21  ðs21  U ½0:1; 0:4Þ
s22  ðs22  U ½0:1; 0:4Þ

s21 þ ðs21  U½0:1; 0:4Þ
s31  ðs31  U½0:1; 0:4Þ
s32  ðs32  U½0:1; 0:4Þ

Firing costs

f1g

f2g

f3g

GCA level 1
GCA level 2
GCA level 3
Missed production costs
Demand

U½3000; 4000
f11  ðf11  U ½0:1; 0:4Þ
f12  ðf12  U ½0:1; 0:4Þ
U½50; 100
U½5; 000; 20; 000

f11 þ ðf11  U½0:1; 0:4Þ
f21  ðf21  U½0:1; 0:4Þ
f22  ðf22  U½0:1; 0:4Þ

f21 þ ðf21  U ½0:1; 0:4Þ
f31  ðf31  U ½0:1; 0:4Þ
f32  ðf32  U ½0:1; 0:4Þ

of workers cross-trained and the number of workers ﬁred from skill set a does not exceed the number of workers in skill set a in the beginning of period t  1. In LPRD, the results show 4.01–49.30% diﬀerence with a
mean of 17.83% from the optimal solution. This is due to the missed production costs; if the unit missed production costs were lower, the percentage diﬀerence might have decreased. In LPRC, the results are 0.23–
10.50% diﬀerent with a mean of 4.20% from the optimal solution. In this case, the costs are from both missed
production cost and overstaﬃng. There is also the possibility of infeasible results.
The total cost for LPH1 and LPH2 are 0–0.97% with a mean of 0.36%, and 0–0.73% with a mean of 0.29%
higher than the optimal solution, respectively. LPH2 performs slightly better than LPH1 because of solving
the LP relaxation at the beginning of each period, and the computation time are approximately the same.
The performance of LPH1 and LPH2 are considerably better than rounding the simple LP relaxation heuristic
solutions. This is reasonable since the heuristics utilizes rounding of the LP solutions more eﬀectively than the
simple LP relaxation rounding. The total costs for the GA are 0–0.27% with the mean of 0.04% higher than
the optimal solution. The performance of GA is better than LPH1 and LPH2. However, the computation time
is signiﬁcantly longer (approximately 11 minutes compared to 0.22 and 0.28 seconds) than that for the

722,778
1,222,308
750,575
862,058
763,622
873,600
673,258
970,766
1,028,676
1,048,771
1,064,098
686,193
683,426
988,591
823,777
807,064
813,154
1,015,281
1,029,471
864,014
884,574
673,258
1,222,308
153,800

0.01

725,850
1,230,328
754,098
868,806
770,176
877,004
678,150
976,521
1,033,961
1,054,985
1,073,675
688,771
684,864
994,359
826,191
815,940
817,356
1,019,368
1,036,430
867,857
889,735
678,150
1,230,328
155,128

48.5

LP

Total
cost ($)

IP

Total
cost ($)

744,387
1,241,538
776,367
889,505
776,727
897,159
684,801
990,325
1,053,963
1,061,000
1,087,008
698,103
693,902
1,014,117
Infeasible
826,125
830,982
1,043,057
1,046,906
879,860
907,149
684,801
1,241,538
159,599
0.04

0.42
0.65
0.47
0.78
0.85
0.39
0.72
0.59
0.51
0.59
0.89
0.37
0.21
0.58
0.29
1.09
0.51
0.40
0.67
0.44
0.57
1.09
0.21
0.22

99.98

LPRU

Total
cost ($)

%

99.92

2.55
0.91
2.95
2.38
0.85
2.30
0.98
1.41
1.93
0.57
1.24
1.35
1.32
1.99
–
1.25
1.67
2.32
1.01
1.38
1.60
0.57
2.95
0.66

%
4
4
4
5
4
4
4
5
4
4
4
4
4
5
–
4
5
5
4
4
4.26
4
5
0.45

Rank

Note. The percentage column is the percentage diﬀerence from the optimal solution.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
Average
Minimum
Maximum
Standard
deviation
Average
Computation
Time
(seconds)

Instance

Table 4
Experimental results for four-skills, three-GCA level, and three-period model
LPRD

0.03

894,472
1,349,519
856,323
988,904
1,149,848
1,133,138
847,083
1,097,487
1,152,773
1,186,585
1,130,404
968,872
712,335
1,095,195
1,017,918
1,021,686
892,216
1,097,544
1,095,887
1,089,431
1,038,881
712,335
1,349,519
145,811

Total
Cost ($)

99.94

23.23
9.69
13.56
13.82
49.30
29.21
24.91
12.39
11.49
12.47
5.28
40.67
4.01
10.14
23.21
25.22
9.16
7.67
5.74
25.53
17.83
4.01
49.30
12.13

%
6
6
6
6
6
6
6
6
6
6
6
6
5
6
5
6
6
6
6
6
5.90
5
6
0.31

Rank

LPRC

0.03

782,021
1,267,024
833,270
879,655
797,918
940,534
703,236
982,326
1,100,672
1,095,541
1,094,026
760,019
712,336
1,001,195
857,626
852,470
825,964
1,021,739
1,087,859
907,150
925,129
703,236
1,267,024
152,060

Total
cost ($)

99.94

7.74
2.98
10.50
1.25
3.60
7.24
3.70
0.59
6.45
3.84
1.90
10.34
4.01
0.69
3.80
4.48
1.05
0.23
4.96
4.53
4.20
0.23
10.50
3.01

%
5
5
5
4
5
5
5
4
5
5
5
5
5
4
4
5
4
4
5
5
4.70
4
5
0.47

Rank

LPH1

0.22

731,102
1,231,859
759,722
874,425
770,231
880,492
678,494
982,326
1,034,773
1,054,985
1,076,429
692,067
688,192
996,743
832,198
819,169
817,515
1,019,896
1,039,049
876,318
892,799
678,494
1,231,859
154,578

Total
cost ($)

99.55

0.72
0.12
0.75
0.65
0.01
0.40
0.05
0.59
0.08
0
0.26
0.48
0.49
0.24
0.73
0.40
0.02
0.05
0.25
0.97
0.36
0
0.97
0.30

%
2
2
3
3
3
3
2
2
2
1
2
3
2
2
2
2
2
2
2
3
2.25
1
3
0.55

Rank

LPH2

0.28

731,102
1,231,859
758,320
872,317
770,182
878,653
678,543
982,326
1,035,651
1,054,985
1,076,429
688,787
688,192
996,743
832,198
819,169
817,515
1,019,896
1,039,049
873,298
892,261
678,543
1,231,859
154,948

Total
cost ($)

99.42

0.72
0.12
0.56
0.40
0.001
0.19
0.06
0.59
0.16
0
0.26
0.002
0.49
0.24
0.73
0.40
0.02
0.05
0.25
0.63
0.29
0
0.73
0.25

%

2
2
2
2
2
2
3
2
3
1
2
2
2
2
2
2
2
2
2
2
2.05
1
3
0.39

Rank

GA

644.31

727,773
1,230,333
755,001
869,119
770,176
877,125
678,150
976,953
1,033,961
1,054,985
1,073,740
688,776
685,366
994,500
826,631
816,244
817,356
1,019,368
1,036,430
868,643
890,032
678,150
1,230,333
154,941

Total
cost ($)

1,228.47

0.27
0
0.12
0.04
0
0.01
0
0.04
0
0
0.01
0
0.07
0.01
0.05
0.04
0
0
0
0.09
0.04
0
0.27
0.06

%

1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
0

Rank

736
J.W. Fowler et al. / European Journal of Operational Research 190 (2008) 724–740

J.W. Fowler et al. / European Journal of Operational Research 190 (2008) 724–740

737

heuristics. From this experiment, it can be seen that LPH1 and LPH2 produce near-optimal solutions and
reduces computation time signiﬁcantly.
In addition, the GA was run without initializing the population with the solutions from LPH1, LPH2, and
SSP. The total cost for these experiments are 0–0.37% with a mean of 0.11% higher than the optimal solution
and the computation time is on average 15 minutes. It can be seen that when the population includes initial
solutions from LPH1, LPH2, and SSP, the GA converges quicker. The computation time improved by 26.67%
(15 minutes compared to 11 minutes) and results improved approximately 0.09%.
In order to show the robustness of the parameter values on the solution for each approach, the solutions
from all six algorithms, LPRU, LPRD, LPRC, LPH1, LPH2, and GA were ranked according their percentage
diﬀerence from the IP results, that is, the algorithm that yields the smallest percentage diﬀerence from the IP
solution is ranked 1 and so on. The ranking shows that the GA performed best (ranked 1) and LPRD performed worst for all 20 problem instances. LPH1 and LPH2 ranked 3 and 2, respectively or equally at 2
for most problem instances. LPRU and LPRC ranked 4 and 5, respectively, for approximately 75% of the
problem instances. It can be seen that the algorithms’ performance are robust to the range of parameter values
in our experimentation.
From observation, one parameter that signiﬁcantly aﬀects the solution is the demand variation between
periods. In LHP1 and LPH2, for problem instances with low percentage diﬀerence from optimal, the demand
variations between periods are low. When the demand variation between periods is high, the percentage difference from the optimal for these heuristics tends to be relatively higher. This results from the initial LP solutions. For all approaches except the GA, the quality of solution depends on the initial LP results. If the initial
LP and IP results are close, the quality of the solution after using LPH1 or LPH2 is better than if the gap of
results is large. We observed that when the demand variation is high, the initial LP solution is quite diﬀerent
from the IP solution, resulting in a relatively lower solution quality.
5.2.2. Twenty-ﬁve skills model
In this model, we applied the heuristics to larger problem instances. The model studied was the 25-skills and
three-GCA level semiconductor manufacturing case study presented in Section 5.1. However, to test the performance of the heuristics rigorously, the problem has been extended to six periods and the demand is varied
for each of the ten problem instances used in testing. The costs and productivity factors are the same as those
used in Section 5.1. For each problem instance, the LP relaxation, LPH1, LPH2, and GA are solved. The simple LP relaxation heuristics results are also included: (1) LPRU, (2) LPRD, and (3) LPRC. The total cost and
the percentage diﬀerence from the LP relaxation, solution of which represents a lower bound, for all methodologies are shown in Table 5.
From the experiment, LPRU* represents the solution for which the LP solutions were rounded up to
repair infeasibility. The results are approximately 4–33% with the mean of 13% higher than the LP
relaxation solutions. The results for LPRD are approximately 16–211% with the mean of 107% higher
than the LP relaxation solutions. This high percentage diﬀerence is due to the missed production costs.
In the case for LPRC, the results are approximately 4–77% with the mean of 25% higher than the LP relaxation
solutions. The costs for LPRC are from missed production and overstaﬃng. LPRU* performs the best among
the three simple heuristics. Note that the results are likely to be diﬀerent if the initial settings are modiﬁed.
The total cost for LPH1 and LPH2 are on average 2.33% and 2.09% higher than the LP relaxation solution,
respectively. LPH2 performs slightly better than LPH1 because of solving the LP relaxation at the beginning
of each period. Since the LP relaxation solution represents a lower bound, the heuristics solutions provide on
average a suboptimality less than about 2.50%. In the experiments in Section 5.2.1, the computation time for
LPH1 and LPH2 are approximately the same. However, in this section, the computation time for LPH2 is
signiﬁcantly higher than LPH1. This is because for LPH2, we have to solve the LP at the beginning of every
period and the experiments were for six periods while in the previous section, the experiments were for three
periods. Since the suboptimal percentage between LPH1 and LPH2 is not much diﬀerent, in this experiment,
LPH1 is more preferable than LPH2 because of the computation time.
LPH1 and LPH2 performance is signiﬁcantly better than rounding the simple LP relaxation heuristic solutions. The computation time for LPH1 increased slightly from the computation time for the simple LP relaxation heuristics compared to the improvement of the solutions.

738

LPRU*

Instance

LP
Total cost ($)

Total cost ($)

%

Total cost ($)

%

Total cost ($)

%

Total cost ($)

1
2
3
4
5
6
7
8
9
10
Average
Minimum
Maximum

9,219,895
4,486,119
6,918,697
4,342,797
6,157,700
3,129,843
4,419,779
5,638,475
5,537,585
2,483,187
5,233,408
2,483,187
9,219,895

10,624,736
4,791,780
7,546,580
5,808,273
7,521,400
3,483,744
4,598,775
6,228,333
6,211,293
2,687,526
5,950,244
2,687,526
10,624,736

15.24
6.81
9.08
33.75
22.15
11.31
4.05
10.46
12.17
8.23
13.32
4.05
33.75

10,758,070
12,143,313
11,449,675
5,721,118
7,381,563
9,750,818
13,396,800
9,161,700
10,222,860
7,460,383
9,744,630
5,721,118
13,396,800

16.68
170.69
65.49
31.74
19.88
211.54
203.11
62.49
84.61
200.44
106.67
16.68
211.54

9,649,873
5,920,756
7,678,021
5,038,942
6,836,036
4,378,968
4,962,638
6,933,494
6,552,800
4,398,643
6,235,017
4,378,968
9,649,873

4.66
31.98
10.97
16.03
11.02
39.91
12.28
22.97
18.33
77.14
24.53
4.66
77.14

9,409,486
4,572,483
7,239,339
4,415,664
6,216,232
3,241,844
4,511,050
5,749,957
5,739,262
2,503,198
5,359,851
2,503,198
9,409,486

2.06
1.93
4.63
1.68
0.95
3.58
2.07
1.98
3.64
0.81
2.33
0.81
4.63

9,396,919
4,570,195
7,211,644
4,409,102
6,212,844
3,227,239
4,497,620
5,740,968
5,710,353
2,498,662
5,347,555
2,498,662
9,396,919

1.92
1.87
4.23
1.53
0.90
3.11
1.76
1.82
3.12
0.62
2.09
0.62
4.23

9,331,531
4,528,865
7,121,085
4,395,992
6,195,462
3,206,980
4,471,242
5,704,768
5,658,931
2,496,194
5,311,105
2,496,194
9,331,531

1.21
0.95
2.93
1.22
0.61
2.46
1.16
1.18
2.19
0.52
1.44
0.52
2.93

7.92

8.10

2.23

8.06

1.74

8.06

1.79

9.33

17.87

14.96

88.86

59.26

648.26

Average
computation
time (hours)

LPRD

Note. Percentage column is the percentage above LP relaxation.

LPRC

LPH1

LPH2
%

Total cost ($)

GA
%

Total cost ($)

%

J.W. Fowler et al. / European Journal of Operational Research 190 (2008) 724–740

Table 5
Experimental results for 25-skills, three-GCA level, and six-period model

J.W. Fowler et al. / European Journal of Operational Research 190 (2008) 724–740

739

For example, on average, LPH1 solutions is 82.51% better than LPRU* while the computation time
increased only 15.18%.The GA provides the best results, approximately 0.5–3% with the mean of 1.4% higher
than the LP relaxation solutions but the computation time is the longest.
The results show that hiring and cross-training of workers are varied between all GCA levels depending on
the demand structure, productivity, and cost. The LP results illustrated that all workers are hired with one skill,
and some are cross-trained to two skills. For LPH, since the initial result is based on the LP relaxation, all the
workers are hired with one skill. Cross-training of workers occurs in the skills that required hours are low.
These workers are cross-trained from the skills that are overstaﬀed, which result from rounding up the fractional solutions. Note that, if the initial settings for the experiments are modiﬁed, the results will be diﬀerent.
6. Conclusions
In this paper, an integer programming model that included the consideration of workers’ cognitive abilities
was described. The results from the model makes staﬃng decisions on when, where, and whom to hire, crosstrain, or ﬁre and the amount of missed production. In Wirojanagud et al. (2007), numerical experiments using
realistic manufacturing data showed that as problem instance size increases, the optimal solution cannot be
obtained within a reasonable computational time. A solution space partition approach, referred to as the
‘‘decomposition approach’’, that produced a near optimal solution was also presented. However, as seen in
some problem instances, the computational time is still long.
Two LP based heuristics (LPH1 and LPH2) was proposed for solving this problem. They adjust the fractional LP solutions into a feasible solution by either rounding up (creating overstaﬃng) or rounding down
(creating missed production). A genetic algorithm (GA) was also implemented as an alternative method to
solve the problem and to compare the performance of the proposed heuristics.
The heuristics were applied to a case study from semiconductor manufacturing system with a large number
of machine groups and was compared to the solution space partition approach presented in Wirojanagud et al.
(2007). Both the solution space partition approach and LP based heuristics produced feasible solutions that
are within 5% of the LP relaxation results within a reasonable computational time. The combination of the
solution space partition approach and the LP based heuristics reduces computation time signiﬁcantly.
In addition, the heuristics were applied to models with randomly generated data in order to examine the
performance of the heuristics. Results showed that the LP based heuristics produce near-optimal solutions
within a reasonable computational time. In all the experiments, the GA provided the best results but took
the longest computation times. Additional study on the GA parameters in order to reduce computational time
could be conducted.
Further research could be on performing a design of experiment to study the problem parameters eﬀect on
the solution. Also, developing heuristics using other approaches than the LP based correction approach as
used in this research. Moreover, the model and results discussed in this paper are applied to only one type
of manufacturing problem. Therefore, the heuristics should be applied to other manufacturing problems.
In this research, we consider only cross-training as a source for workforce ﬂexibility. An extension could
be to develop models and heuristics for problems that consider other sources of ﬂexibility, such as the use
of temporary workers or overtime.
References
Askin, R.G., Huang, Y., 2001. Forming eﬀective worker teams for cellular manufacturing. International Journal of Production Research
39 (11), 2431–2451.
Aytug, H., Khouja, M., Vergara, F.E., 2003. Use of genetic algorithms to solve production and operations management problems: A
review. International Journal of Production Research 41 (17), 3955–4009.
Bechtold, S.E., Brusco, M.J., Showalter, M.J., 1991. A comparative evaluation of labor tour scheduling methods. Decision Sciences 22,
683–699.
Billionnet, A., 1999. Integer programming to schedule a hierarchical workforce with variable demands. European Journal of Operational
Research 114, 105–114.
Brusco, M.J., Johns, T.R., 1998. Staﬃng a multiskilled workforce with vary levels of productivity: An analysis of cross-training policies.
Decision Sciences 29 (2), 499–515.

740

J.W. Fowler et al. / European Journal of Operational Research 190 (2008) 724–740

Brusco, M.J., Johns, T.R., Reed, J.H., 1998. Cross-utilization of a two-skilled workforce. International Journal of Operations &
Production Management 18 (6), 555–564.
Campbell, G.M., 1999. Cross-utilization of workers whose capabilities diﬀer. Management Science 45 (5), 722–732.
Campbell, G.M., Diaby, M., 2002. Development and evaluation of an assignment heuristic for allocating cross-trained workers. European
Journal of Operational Research 138, 9–20.
Dimopoulus, C., Zalzala, A.M.S., 2000. Recent developments in evolutionary computation for manufacturing optimization: Problems,
solutions, and comparisons. IEEE Transactions on Evolutionary Computation 4 (2), 93–113.
Ebeling, A.C., Lee, C.Y., 1994. Cross training eﬀectiveness and proﬁtability. International Journal of Production Research 32 (12), 2843–
2859.
French, A.P., Wilson, J.M., 2002. Heuristic solution methods for the multilevel generalized assignment problem. Journal of Heuristics 8,
143–153.
Goldberg, D.E., 1989. Genetic Algorithm in Search, Optimization, and Machine Learning. Addison-Wesley, Reading, MA.
Henderson, W.B., Berry, W.L., 1976. Heuristic methods for telephone operator shift scheduling: An experimental analysis. Management
Science 22 (12), 1372–1980.
Hunter, J.E., 1986. Cognitive ability, cognitive aptitudes, job knowledge, and job performance. Journal of Vocational Behavior 29, 340–
362.
Keith, E.G., 1979. Operator scheduling. AIIE Transactions 11 (1), 37–41.
Loucks, J.S., Jacobs, F.R., 1991. Tour scheduling and task assignment of a heterogeneous work force: A heuristic approach. Decision
Sciences 22, 719–738.
Man, K.F., Tang, K.S., Kwong, S., 1996. Genetic algorithms: Concepts and applications. IEEE Transactions on Industrial Electronics 43
(5), 519–533.
Morris, J.G., Showalter, M.J., 1983. Simple approaches to shift, days-of and tour scheduling problems. Management Science 29 (8), 942–
950.
Nembhard, D.A., 2001. Heuristic approach for assigning workers to task based on individual learning rates. International Journal of
Production Research 39 (8), 1968–1995.
Norman, B.A., Tharmmaphornphilas, W., Needy, K.L., Bidanda, B., Warner, R.C., 2002. Worker assignment in cellular manufacturing
considering technical and human skills. International Journal of Production Research 40 (6), 1479–1492.
Schmidt, F.L., Hunter, J.E., 1998. The validity and utility of selection methods in personnel psychology: Practical and theoretical
implications of 85 years of research ﬁndings. Psychological Bulletin 124 (2), 262–274.
Showalter, M.J., Mabert, M.J., 1987. An evaluation of a full-/part-time tout scheduling methodology. International Journal of Operations
& Production Management 8 (7), 54–71.
Stewart, B.D., Webster, D.B., Ahmad, S., Matson, J.O., 1994. Mathematical models for developing a ﬂexible workforce. International
Journal of Production Economics 36, 243–254.
Suer, G.A., 1996. Optimal operator assignment and cell loading in labor intensive manufacturing cells. Computers and Industrial
Engineering 31 (2), 155–158.
Thorndike, R.L., 1986. The role of general ability in prediction. Journal of Vocational Behavior 29, 332–339.
Vairaktarakis, G.L., Winch, J.K., 1999. Worker cross-training in paced assembly lines. Manufacturing & Service Operations Management
1 (2), 112–131.
Vairaktarakis, G.L., Cai, X., Lee, C., 2002. Workforce planning in synchronous production systems. European Journal of Operational
Research 136, 551–572.
Walsh, M.A., 2000. Operator cross training in wafer fabrication under uncertainty: A stochastic programming approach. Masters Thesis,
Department of Industrial Engineering, Arizona State University, Tempe, AZ.
Wee, C.L. 1999. Modeling operators in wafer fab. Masters Thesis, Department of Industrial Engineering, Arizona State University,
Tempe, AZ.
Wirojanagud, P., Gel, E.S., Fowler, J.W., Cardy, R.L., 2007. Modeling inherent worker diﬀerence for workforce planning. International
Journal of Production Research 45 (3), 525–553.
Wullink, G., Gademann, A.J.R.M., Hans, E.W., Van Harten, A., 2004. Scenario-based approach for ﬂexible resource loading under
uncertainty. International Journal of Production Research 42 (24), 5079–5098.

European Journal of Operational Research 206 (2010) 417–425

Contents lists available at ScienceDirect

European Journal of Operational Research
journal homepage: www.elsevier.com/locate/ejor

Decision Support

Interactive evolutionary multi-objective optimization for quasi-concave
preference functions
John W. Fowler a, Esma S. Gel a, Murat M. Köksalan b, Pekka Korhonen c, Jon L. Marquis a, Jyrki Wallenius c,*
a

Arizona State University, P.O. Box 875906, Tempe, AZ 85287-5906, USA
Middle East Technical University, 06531 Ankara, Turkey
c
Aalto University School of Economics, P.O. Box 21210, 00076 AALTO, Finland
b

a r t i c l e

i n f o

Article history:
Received 7 August 2009
Accepted 19 February 2010
Available online 4 March 2010
Keywords:
Interactive optimization
Multi-objective optimization
Evolutionary optimization
Knapsack problem

a b s t r a c t
We present a new hybrid approach to interactive evolutionary multi-objective optimization that uses a
partial preference order to act as the ﬁtness function in a customized genetic algorithm. We periodically
send solutions to the decision maker (DM) for her evaluation and use the resulting preference information to form preference cones consisting of inferior solutions. The cones allow us to implicitly rank solutions that the DM has not considered. This technique avoids assuming an exact form for the preference
function, but does assume that the preference function is quasi-concave. This paper describes the genetic
algorithm and demonstrates its performance on the multi-objective knapsack problem.
Ó 2010 Elsevier B.V. All rights reserved.

1. Introduction
The ﬁeld of combinatorial optimization, with its variety of NPhard problems, has turned to heuristics to provide nearly optimal
solutions to previously intractable problems. Frequently, however,
a DM is faced with a combinatorial optimization problem with several different objectives. These multi-objective combinatorial optimization (MOCO) problems are even more difﬁcult to solve
optimally since they involve both NP-hard problems and multiple
objectives.
While it is sometimes possible to create a single objective problem by combining the objective functions into a single preference
function, the way a DM combines conﬂicting objectives is often difﬁcult to capture. In many cases, the DM cannot quantify how the
objectives should be combined into a preference function that
can then be optimized using single objective optimization techniques. This complication is often severe enough to make a priori
approaches impractical. For a more complete discussion of the
advantages and shortcomings of these approaches, the interested
reader is referred to Dyer et al. (1992) and Wallenius et al. (2008).
An alternative approach is to generate the set of Pareto optimal
solutions (or approximate Pareto optimal solutions), and present
them to the DM. The DM then selects a solution a posteriori. Unfortunately, this set is difﬁcult to visualize for problems having three
or more objectives, and is very time consuming to generate since it
* Corresponding author.
E-mail address: Jyrki.Wallenius@hse.ﬁ (J. Wallenius).
0377-2217/$ - see front matter Ó 2010 Elsevier B.V. All rights reserved.
doi:10.1016/j.ejor.2010.02.027

may be comprised of thousands of solutions, even in the case of linear constraints and objectives (Kondakci et al., 1996).
Interactive decision making addresses many of the challenges
associated with a priori and a posteriori approaches. It combines
the process of obtaining information from the DM with the process
of generating solutions to the problem. Seminal papers in the area
include Geoffrion et al. (1972) and Zionts and Wallenius (1976).
Since these approaches generate solutions that the DM must evaluate in order to guide the progression of the algorithm, research in
this area is concerned with both the generation and the representation of solutions. There are software packages that perform both
activities. See Caballero et al. (2002) and Poles et al. (2006) for a
comprehensive list of software descriptions. For more information
on generating solutions and representing them to a DM, the interested reader is referred to Miettinen (1999).
Evolutionary optimization and computing has emerged as a
new ﬁeld with strong ties to Multiple Criteria Decision Making/
Multiattribute Utility Theory (Deb, 2001). The ﬁrst evolutionary
multi-objective optimization algorithm is due to Schaffer (1984).
However, it was not until about 10 years later that three working
evolutionary algorithms were suggested almost at the same time:
MOGA by Fonseca and Fleming (1993), NSGA by Srinivas and Deb
(1994), and NPGA by Horn et al. (1995). The main thrust in all these
algorithms was to generate an approximation of the Pareto optimal
frontier. An interesting paper is Fonseca and Fleming (1998), who
demonstrate the need for preference articulation in cases where
many objectives lead to a non-dominated set too large to effectively sample. A survey of methods developed before 2000

418

J.W. Fowler et al. / European Journal of Operational Research 206 (2010) 417–425

attempting to handle user’s preferences is provided by Coello
Coello (2000). More recent attempts to incorporate user’s preferences into a multi-objective evolutionary framework include
Cvetkovic and Parmee (2002), Phelps and Köksalan (2003), Branke
and Deb (2004), Deb et al. (2005), Hanne (2005), Kamalian et al.
(2004), Molina et al. (2009), and Parmee et al. (2001). Cvetkovic
and Parmee (2002) assign weights for the objectives and additionally require a minimum level for dominance. NSGA-II (Deb et al.,
2002) elaborates on two methods; the guided dominance principle,
and biased crowding distance to incorporate vague user preferences. The method in Deb et al. (2005) is based on the idea of using
reference direction projections as part of the ﬁtness function.
Hanne (2005) discusses interactive decision support based on evolutionary principles. Parmee et al. (2001) and Kamalian et al.
(2004) discuss interactive evolutionary systems for multi-objective
design. Molina et al. (2009) develop a reference point-based method where they favor solutions that dominate a reference point or
that are dominated by this reference point over all other solutions.
Phelps and Köksalan (2003) develop and demonstrate an interactive genetic algorithm on multi-objective knapsack and minimum
spanning tree problems. Finally, Köksalan and Phelps (2007) developed a multi-objective evolutionary algorithm to concentrate on a
desired part of the efﬁcient frontier using partial information on
the preferences of the DM.
To the best of our knowledge, Phelps and Köksalan (2003)’s and
our approach are the only approaches that guarantee correct partial orders of the populations provided that the DM’s preferences
are consistent with the assumed utility function forms. Phelps
and Köksalan (2003) use a linear utility function and make corrections to the partial order whenever the DM’s expressed preferences
are not consistent with such a function. We assume a more general
quasi-concave function that is considered to represent human
preferences well. To our knowledge, this is the only evolutionary
algorithm that incorporates the properties of an implicit quasiconcave utility function into the algorithm. Utilizing the theory
developed for quasi-concave functions we guarantee to produce
partial orders that are consistent with preferences derived from
such functions.
Our research uses a similar interactive genetic algorithm framework to that of Phelps and Köksalan (2003), and we use a similar
experimental framework to test our algorithm. They estimate
and use a linear utility function to order the population. They make
corrections to this ordering when the DM’s expressed preferences
are in conﬂict with this order. However, our technique is more general since it is designed to support any quasi-concave function of
the objectives. Incorporating DM preferences in the metaheuristic
via DM interaction guides the search to the most preferred region
of the solution space. This helps avoid both the computational burden of generating the full efﬁcient frontier and the problems inherent in attempting to quantify the DM’s preference function. More
speciﬁcally, we propose an evolutionary metaheuristic that evaluates (i.e., partially rank orders) solutions using the convex preference cones developed in Korhonen et al. (1984). These cones are
generated based on pairwise comparisons of solutions, making it
possible to preference order solutions that the DM has never directly evaluated and provide a stronger ordering than previous
dominance-based techniques. The preference order is used to evaluate the ﬁtness of the population members. As in Phelps and
Köksalan (2003), we assume that the individual objectives are
known, and that the user’s preference function is unknown.
The results of our method are evaluated in a manner similar to
that of Phelps and Köksalan (2003). We compare our results to the
best solution found by the genetic algorithm per the DM’s true
preferences to assess the ability of our algorithm to correctly select
the DM’s most preferred solution. Due to the difﬁculty of accurately solving large problems to optimality, we present a compar-

ison of our result to the optimal linear programming (LP)
relaxation of our problem instances. We report the average value
of these metrics over several runs of each tested conﬁguration of
the algorithm.
In the following sections, we detail the development of the evolutionary metaheuristic and present computational results for the
multi-objective 0–1 knapsack problem (MOKP). Without loss of
generality, we assume that each objective is to be maximized.
2. Steps of the evolutionary metaheuristic
While all genetic algorithms share certain common features
(Michalewicz, 1996), they must be adapted to the speciﬁc problem
at hand in order to provide good solutions efﬁciently (Phelps and
Köksalan, 2003). In this case, the algorithm must generate the population of solutions, obtain preference information from the DM,
and partially order the population members, based on the preference information in order to ﬁnd which members of the population
to select for breeding and which to replace. The standard functions
of our genetic algorithm, such as the random number generator,
are based on GAlib (Wall, 1995).
The ﬁrst problem to consider in a genetic algorithm is how to
represent the solution. For the multi-objective knapsack, the simplest approach is to create a boolean vector of length J, where J is
the number of candidate items for inclusion in the knapsack. Since
the items can be indexed from 1 to J, the jth entry in the vector will
be set to one if the jth item is to be included. It remains zero
otherwise.
A high level overview of the steps of the evolutionary metaheuristic is given below.
1. Create the population
2. Send a sample, S0 , of the population to the DM
3. Preference order the population using feasibility, Pareto dominance, and preference cone dominance (most preferred to least
preferred)
4. Replace the last (worst) r% of the population
5. Select parents
(a) Probabilistically select the ﬁrst parent based on preference
(b) Randomly select the second parent
6. Breed the parents and mutate the offspring
(a) Breed using a crossover operator
(b) With probability pm , mutate the offspring
7. Update the population by adding the new population members
(a) 23 r% are bred using the crossover operator
(b) The rest are generated randomly
8. Update the best solution and repeat for the required number of
generations
The following subsections describe each step in detail.
2.1. Create the population
Creating the initial population is known to be important to how
quickly a genetic algorithm reaches a good solution (Phelps and
Köksalan, 2003). Genetic algorithms provide better solutions when
the initial solutions are Pareto optimal, or nearly so, and have a
reasonable amount of diversity in the population. For this reason,
the initial population is composed of two groups: a ‘‘random”
group and an ‘‘efﬁcient” group. The random group consists of solutions whose chromosomes are generated randomly with an equal
probability of all possible values. This group composes 1/3 of the

J.W. Fowler et al. / European Journal of Operational Research 206 (2010) 417–425

initial population, and the solutions are not constrained to be feasible. Instead, infeasible solutions will be penalized, and thus tend
to be replaced as the algorithm progresses. The primary purpose of
this group is to provide genetic diversity. The efﬁcient group is
based on assigning weights to the objectives to produce a linear
combination of the objectives. This linear combination acts as a
single objective, so a greedy heuristic is then applied to the problem to produce a good solution. The weights are randomly generated and normalized to sum to unity. This process ﬁlls the
remaining 2/3 of the population with solutions near the efﬁcient
frontier of the multi-objective problem with a linear objective
function, and constitutes a means of providing a good initial population. This generation technique is more favorable to linear preference functions since it uses a linear combination of the
objectives, but it has also shown to be effective for Chebyshev preference functions. This suggests that providing nearly efﬁcient solutions is a good starting point for a variety of preference functions.

2.2. Send a sample of the population to the DM
Once an initial population has been created, the DM is asked to
evaluate a set S0 of jS0 j members of the population, and to specify
the best and worst in the set. (Due to human cognition limits explained in Miller (1956), jS0 j is restricted to be strictly less than
eight.) The best and worst solutions in the set (called sample) are
used to create convex preference cones in the objective space of
the problem, which are used to deﬁne inferior solutions. The precise method of using the cones is explained in the next section.
Theorem 1 of Korhonen et al. (1984) explains the role of the cones
in ranking solutions. It states, ‘‘Assume a quasi-concave and nondecreasing function f ðxÞ deﬁned in a p-dimensional Euclidean space
Rp . Consider distinct points xi 2 Rp ; i ¼ 1; . . . ; M and any point
x 2 Rp and assume that f ðxk Þ < f ðxi Þ; i – k. Then, if  P 0 in the
following linear programming problem:

Max
Subject to :

;
M
X

li ðxk  xi Þ   P x  xk ;

ð1Þ

i¼1;i–k

li P 0 8i;
it follows that f ðxk Þ P f ðx Þ”. Given that xk is the vertex of the cone
and x is shown to be less preferred than that point, we will refer to
the point x as being cone dominated.
Sending a sample of six solutions to the DM, and using the DM
provided preference information, it is possible to construct ﬁve 2point convex preference cones by observing that the best member
is a point and each of the other ﬁve members of the set is a vertex.
We also form a single 6-point preference cone with the worst point
in the sample at the vertex. This process produces ﬁve unique preference cones per call to the DM since the 2-point cone formed between the best and the worst is redundant to the 6-point cone.
The convex cones are formed from the objective function values
of the population members selected for evaluation (and exist in
objective space), hence they are independent of the population in
that the information in the cones is not dependent on the continuing existence of the population members that created them. The
information they contain is dependent solely upon the true preference function. Due to this characteristic, all preference cones are
retained throughout all the genetic algorithm’s generations,
whether or not the members of the population from which they
were derived are still surviving. Note that these cones are valid
for the underlying preference function regardless of which solutions are included in the cones as proven in Korhonen et al. (1984).

419

2.3. Preference order the population
Placing the population in preference order is divided into two
steps. First, the algorithm selects members of the population for
DM evaluation so the algorithm can elicit preference information.
Second, the algorithm uses the preference information to sort the
population. These two steps are explained in the sections below.
2.3.1. Selecting members for DM Evaluation
The members selected for review by the DM will impact the
power of the cones created as well as the regions of the objective
space that fall under the cone. For this reason, we select solutions
to send to the DM from the set of nearly feasible non-dominated
solutions. First, we add solutions that cannot be ranked using the
already existing preference cones. If this sample is too small, we
randomly select members from the set of feasible non-dominated
solutions to supplement the sample. Including these additional
members provides an opportunity to create cones powerful enough to provide information about new solutions. Pseudo code
for this algorithm is provided below. Note that a better solution
at the vertex of a cone results in a more powerful cone, so not sending dominated solutions to the DM actually improves the ability of
the algorithm to evaluate solutions.
for all members of the population
if population[0] preferred to population[i] by cone or direct
information
else
sample = sample + population[i]
end if
end for
while size(sample) < 6
index = (int) rand( ) * number of feasible non-dominated
solutions + 1
sample = sample + population[index]
end while
The selected members are sent to the DM, who tells us the best
and worst of the sample. The objective values of the worst solution
deﬁne the coordinates of the vertex, and the other solutions deﬁne
the coordinates of the other points in the cone. Note that the best
solution from the most recent DM interaction is retained independent of the population as we discuss in Section 2.8.
2.3.2. Preference order the population
Any time new convex preference cones or population members
are formed, the next step is to put the population into preference
order. (This occurs immediately after the ﬁrst DM interaction,
hence it is explained here.) The most preferred solution is placed
ﬁrst, the next best solution is placed second, and so on. This is a
partial ordering of the population since we do not know the
DM’s true preference function and, therefore, may not always
know whether or not one solution is preferred to another. The
three steps used to efﬁciently sort the population are listed below
and illustrated in Fig. 1. Note that 1, 2, 3 in Fig. 1 implies a partial
order and is not representative of a ﬁxed number of solutions in
the cluster.
1. Sort by feasibility (F indicates feasible, I indicates infeasible) (if
applicable)
2. Sort by Pareto dominance (ND indicates not dominated, D indicates dominated)
3. Sort with cone dominance and information obtained from DM
interaction
The ﬁrst step in creating the partial order is to divide the population into feasible (F) and infeasible (I) clusters in problems where

420

J.W. Fowler et al. / European Journal of Operational Research 206 (2010) 417–425

Fig. 1. Population member clustering diagram. Key: F = feasible solution, I = infeasible solution, ND = non-dominated solution, D = dominated solution, 1 2 3 = partial
order.

infeasible population members can be generated (Deb et al., 2002).
Alternatively, the genetic encoding may be designed not to permit
the creation of infeasible population members or a repair operator
may be used. Whether using a repair operator or penalizing infeasibility is more preferred depends on the problem being investigated and the genetic representation of the solution in the
algorithm. If infeasible solutions cannot be created, sorting by feasibility is skipped and we proceed directly to considering Pareto
dominance.
The second sort is conducted on Pareto dominance. Since the
true preference function of the objectives and the set of efﬁcient
solutions are unknown, this sorting checks whether or not another
member of the population is at least as good in all objectives and
better at least in one objective as the population member in question. If this condition is met, the member is considered dominated
(D), and that member is placed in a less preferred cluster than a
non-dominated (ND) member (Deb et al., 2002).
For computational efﬁciency, the ﬁnal sorting is performed only
on the most preferred cluster, the feasible non-dominated members. In this sort, the top cluster is placed in preference order using
the convex preference cones and DM preference information elicited in step 2. Speciﬁcally, if solution i is under cone K and solution
j is within cone K’s polyhedron (spanned by points 4, 5, and 6 in
Fig. 2), solution j will appear before solution i in the partial order.
In terms of the cones, this means that, if a solution is under a cone
(like point 1 in Fig. 2), it will appear after the vertex of the cone
(point 4 in Fig. 2), which will appear after all the solutions in the
cone’s polyhedron (like point 2 in Fig. 2). This is repeated for all
the cones generated. The DM preference information is applied
by placing solution j before solution i if j is within the cone’s polyhedron and i is the vertex of the cone. All this follows from the quasi-concavity of the underlying value function. (Note that while any
point in the cone’s polyhedron is preferred to the vertex, the point
designated as the best in the cone is not necessarily preferred to

every other point in the polyhedron since we allow nonlinear preference functions. In relation to Fig. 2, imagine that the upper two
white points are equally preferred by the DM, but that a point directly in between them is more preferred. This would occur if the
isopreference lines in the objective space were upward-opening
parabolas.)
The above steps are conducted for each member of the population, and a simple interchange sort exchanges the position of two
members of the population if the earlier one is found to be less preferred than the later one. In this way, the best solution (and those
members of the population that cannot be ranked against it) tends
to rise to the top of the population. Feasible non-dominated solutions that are less preferred than other members of the population
will come next, and the dominated and infeasible clusters of solutions appear last.
The preference cone (the shaded region in Fig. 2) is constructed
from two binary comparisons: solution 5 is preferred to solution 4,
and solution 6 is preferred to solution 4. As shown in Korhonen
et al. (1984), it follows from the quasi-concavity of the underlying
preference function that the shaded cone (and naturally all solutions which lie ‘‘under” the cone) contains solutions that are inferior to solution 4, as well as to solutions that lie in the cone’s
polyhedron (solutions spanned by 4, 5, and 6.)
To perform the above ordering, ﬁrst note that any population
member can have four possible locations relative to a convex preference cone. It can be: (1) under the cone, (2) in the cone’s polyhedron, (3) and (7) outside the cone, or (4) the cone’s vertex. These
possible locations are shown for a 3-point cone in Fig. 2, where
the white solutions deﬁne the cone and the black solutions are
points being compared to the cone. Finding a member’s location
relative to a cone requires solving two linear programs. The formulation to determine if a member falls under a cone is presented in
Korhonen et al. (1984), and is given above in Eq. (1). The formulation to determine if the member falls within the polyhedron follows directly from Lemma 1 of Korhonen et al. (1984), and is
given in Eq. (2). Essentially, the linear program (LP) checks to see
if a solution’s objective values can be written as a convex combination of the objective values of the points that make up the cone. If
the above condition is true (i.e., if there is a feasible solution to Eq.
(2)), the solution lies within the cone’s polyhedron.
Let X j denote the jth solution, zi ðX j Þ denote the value of the ith
objective of the jth solution, and K denote the set of points generating the cone. If a feasible solution exists to the following LP, the
solution X j ; j R K falls within the polyhedron of the cone.

Min

0;

Subject to : zj ðX j Þ ¼
X

X

lk zi ðX k Þ;

k2K

lk ¼ 1;

ð2Þ

k2K

lk P 0 for all k:
2.4. Replace the worst r% of the population
Since the population is already clustered into four sets and ordered from most preferred to least preferred, the last members
of the last set in population are the worst. We replace the worst
ranked 20% of the population every generation by removing them
from the end of the population listing.
2.5. Select parents

Fig. 2. Possible locations of a solution with respect to a cone.

In order to replace the less ﬁt members of the population, new
members are bred from two parents. The ﬁrst parent is selected
probabilistically from the population. The probability of selection

J.W. Fowler et al. / European Journal of Operational Research 206 (2010) 417–425

is given in Eq. (3) below. The second parent is selected randomly
(with equal probability) from the population.
Let w be the population size, and let i be the index of the solution in the (partially ordered) population. (Note that a smaller i implies higher preference, and that the ﬁrst solution is stored with
index 0.) Then the probability of selection is given by

wi
PðselectÞ ¼ Pw
:
j¼0 ðw  jÞ

ð3Þ

2.6. Breed the parents and mutate the offspring
A simple uniform crossover operator is used to combine the
chromosomes of the two parents (Michalewicz, 1996). We set
the crossover operator such that there is a 50% probability that
the child’s ith chromosome will come from the ﬁrst parent (independent of any other chromosome selections.) Again, there is no
feasibility check imposed on the offspring. The clustering and subsequent replacement of the worst members is used to remove
infeasible solutions from the population. Approximately 2/3 of
the total replacements are made in this way.
In order to preserve diversity in the population, the remaining
population members selected for replacement are replaced with
immigration (Ahuja et al., 2000). These immigrant solutions are
generated by the same technique used to place random members
in the initial population.
Regardless of whether the new members were generated by
breeding or randomly, we set the mutation probability of the
new member to 90% as in Phelps and Köksalan (2003). The mutation exchanges two randomly selected chromosomes in the new
population member. The high mutation probability combined with
the relatively gentle mutation operation slightly alters new population members and prevents premature convergence of the population. Since the best solution is maintained independently from
the population as discussed in Section 2.8, the high mutation probability will not adversely affect solutions already in the population.
2.7. Update the population
Once breeding is complete, the new members are added to the
population, their objective functions are calculated, and the partial
order is updated. From here, the algorithm repeats from either step
2 or step 3 in order to keep the number of calls to the DM at a reasonable level. The best method (that is, frequency and pattern) of
allocating calls to the DM is an ongoing research topic.
2.8. Update the best solution
The best solution (deﬁned as the ﬁrst member of the population
sorted using information from the cones) is selected at the end of
each generation. This solution is stored independent of the population, and is called the incumbent. Each time the population is updated, the new best solution is compared to the incumbent. The
incumbent solution is replaced with the new best solution after
each iteration, unless the new best solution is dominated by the
incumbent. This replacement condition is imposed as a sanity
check to ensure superior solutions are not discarded.
In the ﬁnal generation, the ﬁrst four members in the partial order, the current incumbent solution, and the best solution from the
most recent DM interaction are sent to the DM as a ﬁnal six-member sample. The best of this sample is the reported solution.
3. Experimentation
Several design and implementation issues arose while developing this genetic algorithm. Like most genetic algorithms, we

421

needed to decide on the best population size, the number of generations to run, and several other similar issues. We addressed this
parameter tuning problem using an experimental design
framework.
For all experiments, the DM was replaced with a ‘function robot’
that would evaluate the sample of solutions sent the DM and return the best and the worst of the sample based on randomly generated preference function coefﬁcients. This allowed us to
experiment on the algorithm without issues of DM fatigue or
inconsistency. It also reduces the computation time of the algorithm to less than a minute per problem instance.
3.1. Problem
For testing purposes and to allow us to benchmark our results,
we applied this algorithm to the multi-objective 0–1 knapsack
problem, as in Phelps and Köksalan (2003). Since that approach
is the ﬁrst interactive approach for evolutionary multi-objective
optimization (EMO) and since we build upon their approach we
decided to use the same problems. Our results are based on replicated instances of different randomly generated problems. There
are other test problems available for the knapsack problems in
the literature. These are mostly two-objective problems. We consider problems with three and four-objectives in addition to twoobjectives. This was an important part of our tests, since a major
problem with existing EMO algorithms has been the fact that they
have largely been developed for 2-objective cases. This problem
can be formulated as

Max

J
X

ckj dj ;

k ¼ 1; . . . ; n;

j¼1

Subject to :

J
X

aj dj 6 b;

ð4Þ

j¼1

dj 2 f0; 1g;
where b is the knapsack capacity, J is the number of candidate items
for the knapsack, aj is the volume of the jth item, dj is a boolean
indicating if the jth item is included in the knapsack, n is the number of objectives, and ckj is the value of the jth item to the kth objective. Selecting a research portfolio can be regarded as a knapsack
problem where the DM must decide whether or not to fund each
project. The goal is to maximize the value of the portfolio subject
to a budget constraint. In general, this means that the algorithm
is faced with a set of items that have a speciﬁed value to each objective and a volume. The objective is to maximize the total value of
the items included in the knapsack without exceeding the volume
constraint. The 0–1 variant of the knapsack means that an item
can either be included once (1) or not at all (0). The modiﬁcations
to the algorithm that are made speciﬁcally to address the 0–1 knapsack problem within the framework described in Section 2 are explained below. The random generation of problem instances and
the experimental inputs are explained in Section 3.2.
For the knapsack problem, we elected to allow infeasible members to be generated and added to the population rather than to
employ a repair operator. To promote genetic diversity, the volume
constraint of the knapsack is inﬂated by a factor / in the early generations. This temporary relaxation allows nearly feasible solutions
(referred to as quasi-feasible) to be bred, in the hope that they will
eventually turn into good feasible solutions. This practice must be
stopped before the population converges to an infeasible solution,
hence the relaxation of the volume constraint is progressively
tightened to the actual volume constraint over the ﬁrst 10
generations.
Since we wish to breed the quasi-feasible solutions described
above, they must not be sorted into the infeasible class described

422

J.W. Fowler et al. / European Journal of Operational Research 206 (2010) 417–425

in Section 2.3. Instead, the ﬁrst sorting step splits the population
into quasi-feasible (QF) (more preferred) and infeasible (I) (less
preferred) clusters. Let V max denote the knapsack volume constraint, V member the volume of the population member, and g the
number of generations that have passed so far. If
V member 6 V max ð1 þ /ðgÞÞ, the member is declared quasi-feasible.
Otherwise, it is declared infeasible. The function /ðgÞ, which calculates the amount of allowable volume constraint violation at generation g, is given by



/
g
/ðgÞ ¼ max /initial  initial
;0 :
10

ð5Þ

In this implementation, we set /initial to 0.3. Note that the
denominator is the generation at which the volume constraint is
fully imposed. The remaining two sorting steps proceed exactly
as described in Section 2.3.
3.2. Experimental inputs
Each of the runs in the set of designed experiments is a multiobjective knapsack problem instance. For each run, the experimenter provides the information necessary to set up the problem.
These input values uniquely determine the full course of the run
(because the random number seed is an input). If the same inputs
are repeated for two runs, the results will be identical. The input
variables (and their investigated ranges) are:
 Preference function type (categorical: linear or Chebyshev)
 Population size (50, 70, and 90)
 Number of candidate items for the knapsack (problem size:
100–200 items)
 Knapsack volume constraint as a function of the sum of the sizes
of the candidate items (0.5)
 Mutation probability (90%)
 Number of objectives (2, 3, and 4)
 Number of generations (20–500)
 Number of calls to the DM (0–10 calls total)
 Random number seed (randomly selected)
A few of these inputs require some discussion. First, we used
two different preference functions, linear and Chebyshev, because
they represent the bounds of a wide set of indifference curves. The
linear preference function has linear indifference curves (in the
objective space) while the Chebyshev function has indifference
curves that contain a 90-degree bend. A wide set of common indifference curves falls between these two curves, suggesting that a
technique that works for both extremes can be reasonably inferred
to work for the curves that fall in between.
Section 2.7 mentions that the best method of calling the DM is
an open research topic. For the purposes of this experiment, we set
a number of DM calls, and distribute them evenly over the generations. The number of calls allows us to estimate DM effort required to achieve a level of performance with this algorithm.
Many of the other values are chosen to allow easy comparison
to the results in Phelps and Köksalan (2003), including the volume
constraint being 50% of the volume of the knapsack, the mutation
probability, and the number of objectives that are being
considered.
Given the inputs listed above, the speciﬁc parameters of each
problem instance are created randomly in a fashion similar to that
used in Phelps and Köksalan (2003). The three types of random
parameters are: (i) the value of each item to each objective function if the object is included in the knapsack; (ii) the volume of
each item; and (iii) the coefﬁcients used in the preference function
(shown in Eqs. (6) and (7) as a). The objective function coefﬁcients

and volume of each candidate item are uniformly distributed integers between 60 and 100. The random coefﬁcients of each objective in the preference function are random numbers normalized
such that the sum of all the coefﬁcients is unity. Eqs. (6) and (7)
give the general form of the linear and Chebyshev preference functions, respectively.
Let ai be the normalized random coefﬁcient of the ith objective
function, zi ðXÞ be the value of the ith objective of solution X; Bi be
the ideal value of the ith objective, n be the number of objectives, Z
be the value of the preference function, and S be the set of feasible
solutions. Then Z is found by (6) for the linear preference function,
and by (7) for the Chebyshev preference function.

Max

Z¼

n
X

ai zi ðX i Þ

ð6Þ

i¼1

Subject to : X 2 S
Min
Z ¼ Maxi ai ðzi ðX i Þ  Bi Þ
Subjectto :

ð7Þ

X2S

If the underlying preference function is known, it is possible
to rewrite the multi-objective knapsack problem as a single
objective problem where the overall objective function is the
preference function of the objectives. If the problem’s set of
non-dominated solutions is known, it is then possible to evaluate the preference function at each vector of objective function
values corresponding to points in the non-dominated set. The
best value is the optimal solution to the problem. (As pointed
out above, the underlying preference function and the full efﬁcient frontier are hard to ﬁnd in practice, but are useful for testing our approach.)
It is worth noting that efﬁcient solutions to the single objective
problem described above tend to be very close to the optimal solution. For one problem instance with 200 items in the knapsack and
two objectives, the full efﬁcient frontier was generated and both
linear and Chebyshev preference functions were applied with a
range of coefﬁcients. The maximum difference in preference function value between the best and worst efﬁcient solution was less
than 4% and the median deviation was 0.5% of the total value of
the preference function. Considering these results, we expect the
percentage difference between the best found and the output solution to be small in all cases since we only consider solutions that
are non-dominated with respect to the genetic algorithm’s
population.
3.3. Experimental evaluation criteria
Since multi-objective knapsack problem instances are inherently difﬁcult to solve, comparing the genetic algorithm’s solution
to the optimal solution is impractical. Instead, we establish a set of
outputs that will allow comparison of the solutions without having
to optimally solve instances of NP-hard problems. Let zbad be the
preference function value of a bad solution, zout be the preference
function value of the reported output solution, zbest be the preference function value of the best solution in the algorithm population (when searching with knowledge of the true preference
function), and zlp be the value of the preference function of the
LP relaxation of the knapsack problem. The evaluation criteria
are listed below.
 The percent deviation from the best found solution is given by

zbest  zout
zbest  zbad
and referred to as output versus best found,
 Percent deviation from the LP relaxation of the knapsack problem is given by

423

J.W. Fowler et al. / European Journal of Operational Research 206 (2010) 417–425

zlp  zout
zlp  zbad

Table 2
Linear preference function results.
Generations

and referred to as output versus LP relaxation,
 The preference function value of the reported solution (that
would be output to the DM), zout .
A measure of the deviation between the output solution and the
best found with perfect information provides a measure of the
algorithm’s ability to sort the solutions generated by the genetic
algorithm. (As noted earlier, we retain the best solution in the population with knowledge of the true preference function, but use
that information only to create this performance measure.) The
percent deviation from the LP relaxation provides a comparison between the algorithm’s output and an easily generated bound on the
optimal solution. Finally, the value of the solution’s preference
function is useful for comparing different options in the algorithm
(such as the number of generations or the number of calls to the
DM) for a given problem instance.

0 Calls

3 Calls

6 Calls

11 Calls

Output versus best found
30
2
30
3
30
4

2.4%
3.3%
13.6%

0.0%
2.1%
3.0%

0.0%
0.0%
0.6%

0.0%
0.0%
0.0%

Output versus LP relaxation
30
2
30
3
30
4

17.0%
12.8%
22.7%

15.1%
11.6%
13.2%

15.1%
9.8%
11.1%

15.1%
9.8%
10.5%

8993
8958
8942

8993
8966
8968

8993
8966
8972

Average output preference function value
30
2
8964
30
3
8878
30
4
8849

Table 3
Chebyshev preference function results.
Generations

3.4. Experimental results
We designed experiments to determine which factors signiﬁcantly affect the difference between the reported solution and
the best solution found using the true preference function. The results (averaged over 10 replications) are shown in Table 1 for both
two and four-objective function cases. It is clear that raising the
number of objectives decreases the average performance of the
algorithm (all other factors remaining the same). It is also clear
that even for the modest number of generations in these runs,
there are no effects for the two-objective case, since the algorithm
returned the best found regardless of the factor levels.
A more complete experimental table containing averages from
10 replications of each input combination is shown below for both
linear and Chebyshev preference functions. Tables 2 and 3 show
the intuitive result that the deviation from the best found decreases with more calls to the DM using a population size of 70
and a volume fraction of 0.5. There are two main reasons for this.
First, the algorithm is better able to sort the solutions in the population, and second, the algorithm is better guided so the population
contains more preferred solutions. It also shows that the deviation
from the best found increases with increased problem complexity
(as measured by number of objectives) for both preference
functions.

Table 1
Genetic algorithm factor screening experiment with a linear preference function.
Population
size

50
90
50
90
50
90
50
90
50
90
50
90
50
90
50
90

Objects

100
100
200
200
100
100
200
200
100
100
200
200
100
100
200
200

Volume
fraction

0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5

Number of
DM calls

15
5
5
15
5
15
15
5
5
15
15
5
15
5
5
15

Deviation from best found
2 Objective
cases

4 Objective
cases (%)

0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

0.11
0.00
0.28
0.00
0.00
0.26
0.00
0.72
0.00
0.00
0.00
0.08
0.00
0.11
0.00
0.00

Objectives

0 Calls

3 Calls

6 Calls

11 Calls

Output versus best found
30
2
30
3
30
4

Objectives

6.1%
21.1%
36.0%

0.0%
18.5%
10.9%

0.0%
0.0%
1.4%

0.0%
0.0%
0.0%

Output versus LP relaxation
30
2
30
3
30
4

23.3%
26.0%
43.4%

18.7%
23.6%
21.1%

18.7%
5.7%
12.7%

18.7%
5.7%
11.5%

4848
3330
2719

4848
3258
2680

4848
3258
2677

Average output preference function value
30
2
4904
30
3
3429
30
4
2830

Zero deviation from the best found can be created by increasing
the effort expended in the four-objective case. This is a fortunate
result, since our response (with its minimum value of zero) violates
the model assumptions for a standard designed experiment even
after the response is transformed using the Box–Cox method
(Myers and Montgomery, 2002).
More interestingly, we can state that the performance of the
algorithm is affected by problem complexity. We infer that similar
effects would result from applying the technique to other problem
types. For this reason, we recommend running at least 50 generations and terminating the algorithm when the DM is satisﬁed with
the solution presented rather than setting a ﬁxed budget of interactions. Our experiments have shown that satisfactory performance can be achieved using a reasonable number of DM
interactions.
The performance of the algorithm appears to be robust to variations in population size and the number of generations executed
between calls to the DM within the ranges presented in this section. Since the computer only requires a few seconds between
DM calls for the ranges considered, the number of generations between calls is far less relevant to algorithm performance than the
total number of DM calls. Both the dependence on problem complexity and the robustness to a reasonable range of genetic algorithm parameters is consistent with the conclusions presented in
Phelps and Köksalan (2003) for a similar application. The granularity in the number of DM calls required is adequate to show the
trends in the data, and the problem complexity dependence reinforces the conclusion that DM satisfaction is a better stopping criterion than a count of the number of DM calls.
We believe the most compelling conclusion in this data is the
rapid convergence of the output solution to the best found solution. The proposed framework can be applied to genetic algorithms
that have been tuned to speciﬁc problems. Since GA tuning

424

J.W. Fowler et al. / European Journal of Operational Research 206 (2010) 417–425

methods for speciﬁc problems is widely available in the literature,
we did not attempt to tune this algorithm to the types of knapsack
problem instances being evaluated. Hence the top part of Tables 2
and 3 is more important than the comparison to the LP relaxations.
The results show that we can do as well as any genetic algorithm.
The rest is up to tuning the GA.
3.5. Effectiveness of the convex preference cones
An important question that remains is an analysis of the usefulness of the cones when sorting solutions.
There are three obvious approaches to answering this question.
Since the population is sorted using a simple exchange sort, it is
logical to ask what percentage of these exchanges require information from the cones. For the cases presented above, on average, 67%
of the exchanges required cone information beyond what is provided directly by the DM.
The second approach is to determine how much difference there
is in the output solution if the cone evaluation is turned off. While
there are few conclusive results available from this method, it is
interesting to note that the output solutions were uniformly worse
for this algorithm than the algorithm that uses the cones when
three calls are made to the DM. These differences tend to diminish
once six calls are made, but these results suggest that the cones help
the algorithm make better use of sparse information.
Finally, we can ask how many calls would be required to fully
sort the population, and how this perfect sort would affect the
solution the algorithm returns. (Note that we still sort the population based on feasibility and dominance as outlined above and the
DM is only called to sort the feasible non-dominated population
members.) The maximum improvement over the approach using
convex cones is a 2% increase in the preference function, and the
minimum number of DM calls required in the approach is over
8000. The average number of calls required is nearly 40,000,
whereas our algorithm only requires six to 10 calls to achieve similar performance.
All of the evaluation techniques show that the cones play a major role in the functioning of the genetic algorithm. In the ﬁrst evaluation, the cones are shown to be important to sorting the
population. The second evaluation technique shows that the addition of the cones improves the output solution in cases with fewer
DM interactions (and having fewer interactions is preferred in
interactive algorithms). The ﬁnal approach shows that even a perfect sorting of the population provides very little additional information over that provided by the cones, and the amount of DM
effort required to achieve these results is staggering.
4. Conclusions and future work
We have developed and tested a new evolutionary approach to
interactive multi-objective optimization by showing how convex
preference cones can be used to sort solutions to multi-objective
combinatorial optimization problems. In turn, this guided search
reduces the efﬁcient solutions to the region of the solution space
most preferred by the DM. This guided search can also reduce
the number of efﬁcient solutions that must be generated for the
DM to evaluate. Applying this technique requires only that the
underlying preference function be quasi-concave for the convex
preference cones to be valid.
The computational experiments show it is possible to obtain
solutions with a reasonable number of DM interactions that are
very near to or equal to the best found by a similar algorithm that
is operating with perfect knowledge of the user’s preference function. We also present a reasonable range of parameters for the genetic algorithm to achieve these results. Clearly, if a more complex

problem instance must be solved, additional effort should be
expended.
Most existing preference-based multi-objective evolutionary
algorithms are ad hoc in the sense that they are not backed with
theory in converging to the preferred regions. Many use a reference
point or a reference direction to guide the algorithm. We believe
that our approach will open up a new research direction in multi-objective evolutionary research. We expect other researchers
to pursue developing approaches that formally use the properties
of the DM’s underlying preferences. The idea of convex preference
cones derived from quasi-concave utility functions have been
widely used in the multi-criteria decision making literature. They
can be incorporated into other multi-objective evolutionary algorithms and other heuristic search techniques as well. The mechanisms and details of such approaches and testing their
performances await future research.
There are several other future research ideas suggested by our
results. Investigating the effect of DM inconsistencies on the algorithm is a logical next step. Also, using sequential sampling to select points to go to the DM in order to maximize the information
provided by each cone can improve the information provided by
each interaction. Finally, altering the sample size sent to the DM
has the potential to improve the power of the cones and reduce
DM burden.
References
Ahuja, Ravindra, Orlin, James, Tiwari, Ashish, 2000. A greedy genetic algorithm for
the quadratic assignment problem. Computers and Operations Research 27,
917–934.
Branke, J., Deb, Kalyanmoy, 2004. Integrating user preferences into evolutionary
multi-objective optimization. KanGal Report, Indian Institute of Technology,
Kanpur, India.
Caballero, R., Luque, M., Molina, M., Ruiz, F., 2002. Promoin: An interactive system
for multiobjective programming. International Journal of Information
Technology and Decision Making 1, 635–656.
Coello Coello, Carlos A., 2000. Handling preferences in evolutionary multiobjective
optimization: A survey. In: Proceedings of the 2000 Congress on Evolutionary
Computation, pp. 30–37.
Cvetkovic, Dragan, Parmee, Ian C., 2002. Preferences and their application in
evolutionary multiobjective optimization. IEEE Transactions on Evolutionary
Computation 6, 42–57.
Deb, Kalyanmoy, 2001. Multi-Objective Optimization Using Evolutionary
Algorithms. Wiley, Chichester.
Deb, Kalyanmoy, Pratap, Amrit, Agarwal, Sameer, Meyarivan, T., 2002. A fast and
elitist multiobjective genetic algorithm: Nsga-ii. IEEE Transactions on
Evolutionary Computation 6, 182–197.
Deb, Kalyanmoy, Sundar, J., Uday, B.R.N., 2005. Reference point based multiobjective optimization using evolutionary algorithms. KanGal Report, Indian
Institute of Technology, Kanpur, India.
Dyer, James, Fishburn, Peter, Steuer, Ralph, Wallenius, Jyrki, Zionts, Stanley, 1992.
Multiple criteria decision making, multiattribute utility theory: The next ten
years. Management Science 38, 645–654.
Fonseca, C.M., Fleming, P.J., 1993. Genetic algorithms for multiobjective
optimization: Formulation, discussion, and generalization. In: Proceedings of
the Fifth International Conference on Genetic Algorithms, pp. 416–423.
Fonseca, C.M., Fleming, P.J., 1998. Multiobjective optimization and multiple
constraint handling with evolutionary algorithms. Part II: Application
example. IEEE Transactions on Systems, Man and Cybernetics, Part A: Systems
and Humans 28, 38–47.
Geoffrion, A.M., Dyer, J.S., Feinberg, A., 1972. An interactive approach for
multicriterion optimization, with an application to the operation of an
academic department. Management Science 19, 683–694.
Hanne, T., 2005. Interactive decision support based on multiobjective evolutionary
algorithms. Operations Research Proceedings (GOR), 761–766.
Horn, J., Nafploitis, N., Goldberg, D., 1995. A niched pareto genetic algorithm for
multi-objective optimization. In: Proceedings of the First IEEE Conference on
Evolutionary Computation, pp. 82–87.
Kamalian, R., Takagi, H., Agogino, A., 2004. Optimized design of MEMS by
evolutionary multi-objective optimization with interactive evolutionary
computation. In: Genetic and Evolutionary Computation Conference (GECCO),
pp. 1030–1041.
Köksalan, Murat, Phelps, Selcen (Pamuk), 2007. An evolutionary metaheuristic for
approximating preference-nondominated solutions. INFORMS Journal on
Computing 19, 291–301.
Kondakci, S., Azizoglu, M., Köksalan, M., 1996. Note: Bicriteria scheduling
minimizing ﬂowtime and maximum tardiness. Naval Research Logistics 43,
929–936.

J.W. Fowler et al. / European Journal of Operational Research 206 (2010) 417–425
Korhonen, Pekka, Wallenius, Jyrki, Zionts, Stanley, 1984. Solving the discrete
multiple criteria problem using convex cones. Management Science 30, 1336–
1345.
Michalewicz, Zbigniew, 1996. Genetic Algorithms + Data Structures = Evolution
Algorithms, third ed. Springer, Berlin, Heidelberg, and New York.
Miettinen, Kaisa, 1999. Nonlinear Multiobjective Optimization. Kluwer Academic
Publishers., Boston, London, and Dordrecht.
Miller, George A., 1956. The magical number seven, plus or minus two: Some limits
on our capacity for processing information. The Psychological Review 63, 81–
97.
Molina, J., Santana, L.V., Hernandez-Diaz, A.G., Coello Coello, C.A., Caballero, R.,
2009. G-dominance: Reference point based dominance for multiobjective
metaheuristics. European Journal of Operational Research 197, 685–692.
Myers, Raymond, Montgomery, Douglas, 2002. Response Surface Methodology:
Process and Product Optimization Using Designed Experiments, second ed.
Wiley, New York.
Parmee, I.C., Cvetkovic, D., Bonham, C.R., Packahm, I.S., 2001. Introducing prototype
interactive evolutionary systems for ill-deﬁned multi-objective design
environments. Advances in Engineering Software 32, 429–441.

425

Phelps, Selcen (Pamuk), Köksalan, Murat, 2003. An interactive evolutionary
metaheuristic for multiobjective combinatorial optimization. Management
Science 49, 1726–1738.
Poles, S., Vassileva, M., Sasaki, D., 2006. Multiobjective optimization software. In:
Branke, J., Deb, K., Miettinen, K., Slowinski, R. (Eds.), Published as a Chapter in
Springer State-of-the-Art Survey LNCS 5252 Multiobjective Optimization:
Interactive and Evolutionary Approaches.
Schaffer, J.D., 1984. Some experiments in machine learning using vector evaluated
genetic algorithms. Ph.D. Thesis, Vanderbilt University, Nashville, Tennessee.
Srinivas, N., Deb, K., 1994. Multi-objective function optimization using nondominated sorting genetic algorithms. Evolutionary Computation Journal 2,
221–248.
Wall, Matthew, 1995. Massachusetts Institute of Technology’s GAlib Library.
<http://web.mit.edu/galib/www/GAlib.html> (accessed 15.02.06).
Wallenius, J., Dyer, J.S., Fishburn, P.C., Steuer, R.E., Zionts, S., Deb, K., 2008. Multiple
criteria decision making/multiattribute utility theory: Recent accomplishments
and what lies ahead. Management Science 54, 1336–1349.
Zionts, S., Wallenius, J., 1976. An interactive programming method for solving the
multiple criteria problem. Management Science 22, 652–663.

Optim Lett (2013) 7:1627–1642
DOI 10.1007/s11590-012-0578-1
ORIGINAL PAPER

A constant-factor approximation algorithm
for multi-vehicle collection for processing problem
E. Yücel · F. S. Salman · E. L. Örmeci · E. S. Gel

Received: 29 November 2011 / Accepted: 5 October 2012 / Published online: 26 October 2012
© Springer-Verlag Berlin Heidelberg 2012

Abstract We define the multiple-vehicle collection for processing problem (mCfPP)
as a vehicle routing and scheduling problem in which items that accumulate at customer
sites over time should be transferred by a series of tours to a processing facility. We
show that this problem with the makespan objective (mCfPP(Cmax )) is NP-hard using
an approximation preserving reduction from a two-stage, hybrid flowshop scheduling
problem. We develop a polynomial-time, constant-factor approximation algorithm to
solve mCfPP(Cmax ). The problem with a single site is analyzed as a special case with
two purposes. First, we identify the minimum number of vehicles required to achieve
a lower bound on the makespan, and second, we characterize the optimal makespan
when a single vehicle is utilized.
Keywords Approximation algorithm · Vehicle routing and scheduling ·
Makespan · Collection

E. Yücel (B) · F. S. Salman · E. L. Örmeci
College of Engineering, Koç University, Istanbul, Turkey
e-mail: edatyucel@gmail.com
F. S. Salman
e-mail: ssalman@ku.edu.tr
E. L. Örmeci
e-mail: lormeci@ku.edu.tr
E. S. Gel
School of Computing, Informatics, and Decision Systems Engineering,
Arizona State University, Tempe, AZ, USA
e-mail: esma.gel@asu.edu

123

1628

E. Yücel et al.

1 Introduction
In the delivery of health care services, important diagnostic and therapeutic decisions
are often based on the results of clinical tests. Clinical testing laboratories aim to
achieve timely and error free processing of the tests by means of coordinated collection, transportation and processing of the specimens obtained from the patients.
Clinical testing companies that service a geographical region need to transport the
specimens that accumulate at customer sites (such as physician offices, hospitals, and
patient service centers of the clinical testing company) throughout the working day
to a processing facility. The specimens are picked up from the sites and transferred to
the facility by a fleet of vehicles. As specimens arrive, they are processed on a highlyautomated testing equipment that runs at a predetermined rate. Each site may be visited
several times in a day to feed the processor. The processing of all specimens should be
completed as early as possible, considering that the timely completion of the results is
critical for the detection, diagnosis, monitoring and treatment of diseases. The problem is to construct and schedule a series of tours to collect the accumulated specimens
from the sites, in order to minimize the time when all specimens are processed.
This problem involves the routing of the vehicles through the sites over time. However, it has some basic differences from the well-known routing problems, such as the
traveling salesman problem (TSP), team orienteering problem (TOP), vehicle routing problem (VRP), inventory routing problem (IRP), and their variants studied so far.
First, the problem has a scheduling aspect. Since the specimens accumulate at the sites
over time, the timing of the visits affects the collected amounts from the sites. Therefore, not only the sequence of sites to be visited by each vehicle, but also their timing
should be determined in this problem. Second, the transferred items are input to the
processor, so that the transportation decisions also affect the idleness of the processor,
which in turn affects the completion time of the processing of all specimens. Third, the
vehicle tours have to be designed simultaneously due to the significant dependency
between them. In every tour, the return time of the tour to the processing facility, as
well as the collected amount affect the processed amount until the return time of the
next tour. In the literature, [5] and [6] address clinical specimen collection, but do not
consider the accumulation of specimens over time.
The single vehicle version of the problem was defined as the collection for processing problem (CfPP) by [8] with two hierarchical objectives: maximizing the processed
amount by the next morning as a first priority, and minimizing the transportation cost of
the single vehicle as a second priority. It was shown that the CfPP with these objectives
is NP-hard and a mixed integer programming (MIP) formulation was given for CfPP.
Due to the computational difficulty of solving the MIP formulation, the authors developed a heuristic solution approach, which is based on solving the MIP with additional
constraints. Computational experiments showed that this solution approach provides
effective solutions in terms of both objectives for CfPP instances having 10–18 sites
and realistic parameters.
In this paper, we study the multiple-vehicle version of the problem, which is referred
to as mCfPP. A number of alternative objectives are considered for the mCfPP: minimizing the completion time of the jobs, i.e., makespan, minimizing the total idle time
of the processor, and maximizing the processed amount by a due date, such as the next

123

A constant-factor approximation algorithm for multi-vehicle collection for processing problem

1629

morning. We show that minimizing the makespan is sufficient to optimize the other
objectives as well, and hence, we focus on mCfPP with the makespan objective. We
prove that the problem is NP-hard through an approximation preserving reduction from
a two-stage, hybrid flowshop scheduling problem. We analyze two special cases of the
problem: single site, multi-vehicle and single site, single vehicle. For the first one, we
identify the necessary number of vehicles to achieve a lower bound on the makespan,
and for the second one, we characterize the optimal makespan under different workload settings. With insights obtained from these cases, we develop a constant-factor
approximation algorithm for the general multiple-site multiple-vehicle case. The algorithm forms disjoint clusters of the sites and assigns a vehicle to each cluster. Each
cluster is traversed by a tour that is repeated over time.
The remainder of the paper is organized as follows. Section 2 defines the problem,
and investigates alternative objectives and computational difficulty. Section 3 analyzes the problem with a single site. Section 4 presents the proposed approximation
algorithm. Section 5 concludes the paper.
2 Problem description
The problem is defined on an undirected graph G = (N + , E) with node set
N + = {0, 1, 2, . . . , n} and edge set E = {(i, j) | i, j ∈ N + }. Node 0 represents
the processing facility where m identical and uncapacitated vehicles are stationed.
N = N + \{0} represents the set of sites from which items should be collected for
processing. Note that the items represent specimens and the processing facility corresponds to the laboratory in the clinical testing context. Items accumulate at each site
i ∈ N at a constant and known rate of λi units per unit time between time 0 and time
τe . Here, τe corresponds to the ending time of accumulation at the sites. All the accumulated items should be processed at the processing facility, which has a constant and
known processing rate of μ items per unit time. The items are assumed to be identical,
requiring equal amount of processing time, without loss of generality. The accumulated items should be collected and transferred to a centralized processing facility via
a set of tours performed by the m vehicles. Each edge (i, j) ∈ E is associated with a
non-negative travel time ti j . The traveling times are assumed to be metric (implying
that the triangle inequality holds). The collection and processing can start at time 0,
and a vehicle may perform more than one tour. Since accumulation of items ends at
time τe , a feasible solution should visit all sites at least once after time τe to collect
any accumulated items remaining.
We define a measure ofthe total workload at the processing facility as
α = i∈N λi /μ. We let D = i∈N λi τe denote the total accumulated amount during
(0, τe ] and Q(t) the number of items at the processing facility (either waiting in the
queue or undergoing processing) at time t. We set Q(0) = q0 .
We consider three different objectives for the problem:
– Minimizing the completion time of the processing of all the accumulated items
(i.e., makespan), which is denoted by Cmax .
– Minimizing the total idle time of the processor until all the accumulated items are
processed, i.e., until Cmax , which is denoted by I.

123

1630

E. Yücel et al.

– Maximizing the processed amount by a due date τ f , (where τ f is sufficiently large
to visit all sites after τe ), which is denoted by Pτ f . (This objective is equivalent to
minimizing the total number of tardy items.)
As the items are identical, their processing order does not affect any of these objectives. Therefore, for these objectives, the problem is to construct and schedule a set of
tours for each vehicle. Each tour starts and ends at node 0 and includes visits to a subset
of sites in N . We denote the problem with objective A, where A ∈ {Cmax , I, Pτ f }, as
mCfPP(A). Propositions 1 and 2 show that mCfPP(I ) and mCfPP(Cmax ) are equivalent,
and an optimal solution to mCfPP(Cmax ) is also optimal for mCfPP(Pτ f ), respectively.
Therefore, we focus on mCfPP(Cmax ) in the remainder of the paper.
Proposition 1 The problems mCfPP(I ) and mCfPP(Cmax ) are equivalent.
Proof We first note that both problems have the same feasible set. Given a feasible
solution, we have:
Cmax = I + D/μ,

(1)

where D/μ is the total processing time. Since D and μ are constant parameters of
the problem, minimization of the total idle time is equivalent to minimization of the
makespan.


Proposition 2 Any optimal solution of mCfPP(Cmax ) is also optimal for mCfPP(Pτ f )
but any optimal solution of mCfPP(Pτ f ) is not necessarily optimal for mCfPP(Cmax ).
Proof Suppose that we have optimal solutions to mCfPP(Cmax ) and mCfPP(Pτ f )
denoted by s C and s P , respectively. There are two possible cases based on whether
s C can process all the total accumulated amount within the given due date:
(i) Cmax (s C ) ≤ τ f meaning that Pτ f (s C ) = D, and
(ii) Cmax (s C ) > τ f meaning that Pτ f (s C ) < D.
In the case Cmax (s C ) ≤ τ f , clearly, s C is optimal for mCfPP(Pτ f ) as no items are tardy.
Let us consider the case Cmax (s C ) > τ f . Assume s C is not optimal for mCfPP(Pτ f ).
Then, Pτ f (s C ) < Pτ f (s P ) and the amount of unprocessed items by time τ f in solution
s C is strictly larger than that amount in s P . Therefore, at time τ f , it requires more time
to process the remaining unprocessed items in solution s C , which contradicts with the
optimality of s C for mCfPP(Cmax ).
To show that s P is not necessarily optimal for mCfPP(Cmax ), let us consider the
case Cmax (s C ) ≤ τ f . Then, Pτ f (s C ) = Pτ f (s P ) = D. However, s P might process
some items in the time interval (Cmax (s C ), τ f ] so that the makespan of s P is strictly
larger than that of s C .
It is worthwhile to note here that, if τ f , the processing due date, is not late enough
to transfer all the accumulated amount at the sites, the solution that minimizes Cmax
might not necessarily provide an optimal solution to mCfPP(Pτ f ). In order to illustrate
such a case, let us consider a single vehicle and two sites instance, where t01 = 1,
t02 = 2, λ1 = λ2 = μ = 1, τe = 2, and τ f = 3. For this example, a solution

123

A constant-factor approximation algorithm for multi-vehicle collection for processing problem

1631

which minimizes Cmax performs a tour that visits site 2 at time 2 and a second tour

= 8. However, this solution has Pτ f = 0.
that visits site 1 at time 5, and has Cmax
On the other hand, another solution that performs the first tour to visit site 1 at time
1 has Pτ f = 1. mCfPP instances with early τ f values might offer some potentially
interesting tradeoffs and remains as an ample opportunity for future research.


2.1 NP-hardness of mCfPP(Cmax )
In order to prove that mCfPP(Cmax ) is NP-hard, we use an approximation preserving
reduction from an extended version of the two-stage, hybrid flowshop scheduling
problem with m parallel machines (processors, the words are used interchangeably)
in the first stage and a single machine in the second stage. This scheduling problem
is referred to as F H 2, (Pm (1) , 1(2) )||Cmax , according to the notation used by [1]. In
F H 2, (Pm (1) , 1(2) )||Cmax , there are n jobs to be processed in two stages, where each
job i should be processed first in stage 1 with processing time pi , and then in stage 2
with processing time qi . The objective is to minimize the makespan.
We next define an extended version of F H 2, (Pm (1) , 1(2) )||Cmax , denoted by
F H 2, (Pm (1) , 1(2) ), (β → γ )||Cmax , where β denotes the stage with batch processors, γ denotes the stage with a single discrete processor, and → denotes the flowshop
configuration. We will show that F H 2, (Pm (1) , 1(2) ), (β → γ )||Cmax reduces to
mCfPP(Cmax ). This problem extends F H 2, (Pm (1) , 1(2) )||Cmax by allowing batch
availability and batch setup times in all processors of stage 1. A constant setup time,
s, is incurred whenever a batch is formed on any processor in stage 1. Transfer of jobs
to the second stage is possible only when processing of all jobs belonging to the same
batch is completed in the first stage. The problem is to determine the optimal batch
composition and to schedule the jobs in two stages so that the makespan is minimized.
Lemma 1 F H 2, (Pm (1) , 1(2) ), (β → γ )||Cmax is NP-hard.
Proof Although the two machine flow shop problem with the objective of minimizing makespan, (i.e., F2||Cmax ) is polynomially solvable, its extended version
with batch availability and batch setup times in the first stage (i.e., F2(β →
γ )||Cmax ) was shown to be NP-hard [2]. F2(β → γ )||Cmax is a special case of
F H 2, (Pm (1) , 1(2) ), (β → γ )||Cmax since it has a single processor in the first stage.
Therefore, F H 2, (Pm (1) , 1(2) ), (β → γ )||Cmax is NP-hard.


The NP-hardness of mCfPP(Cmax ) is implied by the following proposition.
Lemma 2 There is an approximation preserving, polynomial-time reduction from
F H 2, (Pm (1) , 1(2) ), (β → γ )||Cmax to mCfPP(Cmax ).
Proof First, note that m vehicles of an mCfPP(Cmax ) instance correspond to
m machines of the first stage and the processing facility of the mCfPP(Cmax )
instance corresponds to the single machine in the second stage in an instance of
F H 2, (Pm (1) , 1(2) ), (β → γ )||Cmax , which we refer to as the scheduling problem,
or SP, in short from here on.

123

1632

E. Yücel et al.

Given an instance I1 of the SP with processing times pi and qi for all i ∈
{1, 2, . . . , n}, and s, we construct an instance I2 of the mCfPP(Cmax ) consisting of n
sites (where N + = {0, 1, . . . , n} and N = N + \ {0}) as follows. For each site i ∈ N ,
we set ti j = t ji = ( pi + p j )/2, for all j ∈ N \{i}, ti0 = t0i = (s + pi )/2, and
λi = qi /τe . For each i ∈ N + , we set tii = 0. We also set μ = 1 and τe = s/2 so
that the accumulation ends at the earliest possible visiting time of any site. Clearly,
the construction of mCfPP(Cmax ) instance takes polynomial time.
 . We conSuppose that there exists a feasible SP solution to I1 with makespan Cmax
struct an mCfPP(Cmax ) solution to I2 as follows. As vehicles correspond to machines
in the SP instance, the tours of a vehicle correspond to the batches scheduled on the
corresponding machine in the first stage. For each tour of a vehicle, the sites are traveled in the order of jobs scheduled in the corresponding batch. Then, the duration of
a tour is equal to the processing time of the batch plus the setup time by construction.
Furthermore, the amount collected by a tour becomes available for processing at the
end of the tour. As each job is scheduled in the first stage in the SP solution, all sites
are visited by a vehicle. In addition, as τe = s/2 and t0i ≥ s/2 for all i ∈ N , all the
accumulated amount at each site i, i.e., λi τe = qi , is collected by a visit. Then, the
processing time of the collected amount from a site is equal to the processing time
of the corresponding job in the second stage. Since all jobs must be processed in the
second stage, all the accumulated amount is processed. Therefore, we obtain a feasible
 .
solution for instance I2 of the mCfPP(Cmax ) with makespan Cmax
Next, let us consider a feasible mCfPP(Cmax ) solution to the constructed instance
 . We obtain an SP solution to I as follows. For each vehicle,
I2 with makespan Cmax
1
we assign a distinct machine in stage 1. Then, we form a batch from each tour of
the vehicle. We schedule each batch in the order of the tours and schedule the jobs
in the batch according to the order of the sites visited in the tour. Processing time of
the batch will be equal to the duration of the tour. Also, note that each job must be
scheduled once since each site is visited once. As all items should be processed by the
processing facility, all jobs are processed in the second stage. Since there is no batch
processing in the second stage, the scheduling of the jobs in the second stage might
be done according to the FCFS (first-come, first-served) order. Therefore, we obtain
 .
a feasible solution for instance I1 of the SP with makespan Cmax

As a result, an optimal solution to I2 with makespan Cmax yields an optimal solution


to I1 with the same makespan value.
Lemmas 1 and 2 lead to Proposition 3.
Proposition 3 mCfPP(Cmax ) (or mCfPP(I ), equivalently) is NP-hard.
2.2 Preliminaries
 , follows from Equation (1) as
A simple lower bound on the optimal makespan, Cmax

Cmax ≥ D/μ = ατe . This bound shows how makespan increases with the workload

≥ τe since the accumulation
parameter. When α < 1, we can tighten the bound as Cmax

at sites ends at time τe . Cmax ≥ τe can be further improved by using the fact that in
a feasible solution, the sites should be visited after τe to collect the remaining items.

123

A constant-factor approximation algorithm for multi-vehicle collection for processing problem

1633

Fig. 1 Idle time between return times of two consecutive tours. The figures a and b show cases without
idleness and with idleness, respectively

Then, the smallest time to transfer all items to the processing facility is τe +maxi∈N ti0 .
Thus,

≥ max{ατe , τe + max ti0 }.
Cmax
i∈N

(2)

The makespan may increase further by the idleness of the processor. Next, we
analyze the conditions under which the processing facility remains idle in [0, Cmax ].
Suppose that we have a feasible solution with κ tours. We index the tours in the order of
their arrival time to the processing facility. Hence, tour k ∈ {1, 2, . . . , κ} corresponds
to the kth arriving tour. Rk is the return time of tour k to the processing facility. Note
that, by definition, Rk−1 ≤ Rk . Figure 1 illustrates the two possible cases based on
whether idleness occurs between two consecutive tour return times, Rk and Rk+1 .
Let us define R0 = 0 and Rκ+1 = Cmax for ease of notation, and Ik as the idle time
in the time interval (Rk , Rk+1 ] for k = 0, 1, . . . , κ. Then, Ik depends on (i) the queue
level at Rk , i.e., Q(Rk ), and (ii) Rk+1 − Rk . If Q(Rk ) ≥ μ(Rk+1 − Rk ), then the
processor does not remain idle during (Rk , Rk+1 ] as in Fig. 1. Otherwise, as in Fig. 1,
the processor idles for Ik = (Rk+1 − Rk ) − Q(Rk )/μ time units.
If Q(0) = 0, then the processor is idle during (0, R1 ]. That is, I0 = R1 . Since all
items should be delivered by time Rκ , the processor does not idle in (Rκ , Cmax ], i.e.,
Iκ = 0. Therefore, minimizing the idleness of the processor in (0, Rκ ] is sufficient to
minimize the total idle time.
3 Special case: mCfPP with a single site
In this section, we analyze the special case with a single collection site. In Sect. 3.1,
we determine the necessary and sufficient number of vehicles to achieve the lower

 .
and in Sect. 3.2, allowing only a single vehicle, we characterize Cmax
bound on Cmax
As Cmax = I + ατe , we minimize either Cmax or I during the analysis.
Let us consider the case where the set N consists of a single site with accumulation
rate λ and traveling time t to and from the processing facility. We assume that τe > 2t;
that is, at least one tour can be completed by time τe , and the processor queue is initially

123

1634

E. Yücel et al.

empty, i.e., Q(0) = 0. In addition, we assume that at least one item accumulates and
can be processed in t time units, i.e., λ ≥ 1/t and μ ≥ 1/t, where t ≥ 1. In this
setting, α = λ/μ.

≥ ατe can be improved based on the assumption
For this setting, the bound of Cmax
that the processor queue is initially empty. Since the processor is certainly idle in the

≥ 2t+ατe .
time interval (0, R1 ] and duration of a trip to the site is 2t, we can write Cmax
As a result, we get

Cmax
≥ max{2t + ατe , τe + t}.

(3)

3.1 Multiple vehicles
In this section, we determine the necessary and sufficient number of vehicles, denoted

given in Eq. (3). For this purpose, we
by m  , to achieve the lower bound on Cmax
differentiate between two cases depending on the workload parameter, α < 1 and
α ≥ 1. In each case, we will construct the tours consecutively, starting from the first
tour, and identify the required number of vehicles for each case.
Case 1 (α ≥ 1): When α ≥ 1, Eq. (3) gives 2t + ατe ≥ τe + t + α, so that


= 2t + ατe . In a solution that achieves Cmax
= 2t + ατe , the first tour should
Cmax
be completed at time R1 = 2t and the processor should not idle after R1 . Now,
setting R1 = 2t, let us construct the consecutive tours to avoid idleness. Then, tour
k = 2, . . . , κ is completed at time Rk = Rk−1 + α k−1
 t. We set the number of tours,
i.e., κ, to the minimum integer that satisfies 2t + κk=1 α k t ≥ τe + t in order to
guarantee that there are no unnecessary tours and tour κ visits the site after τe . Let
Ak be the collected amount in tour k. Since A1 = λt, and Ak = λ(Rk − Rk−1 ) for

= 2t + ατe is achieved,
k = 2, . . . , κ − 1, the processor never idles after R1 , i.e., Cmax
in this solution. Figure 2 shows the timeline representation of the constructed tours.
Note that, as α ≥ 1, Rk − Rk−1 gets larger with k.
Now let us determine the number of vehicles required in this solution. When α ≥
2, Rk − Rk−1 = α k−1 t is greater than or equal to a tour duration, i.e., Rk − Rk−1 ≥ 2t
for all k so that a single vehicle is sufficient to perform all tours in order to achieve

= 2t + ατe , i.e., m  = 1. Note that the vehicle waits to match the required
Cmax
Rk values.
When 1 ≤ α < 2, a single vehicle is not sufficient to prevent idleness after R1 = 2t
since the earliest return time of the second tour is R1 + 2t = 4t in any solution, and
the collected amount by the first tour, which is equal to λt, is completely depleted at
R1 + λt/μ = 2t + αt, which is strictly smaller than 4t. We now demonstrate that two
vehicles are sufficient. As R3 − R1 = αt + α 2 t ≥ 2t and Rk − Rk−1 gets larger as k
gets larger, the odd tours in Fig. 2 can be performed by one vehicle and even tours

Fig. 2 Timeline representation of tours constructed for Case 1 (α ≥ 1)

123

A constant-factor approximation algorithm for multi-vehicle collection for processing problem

1635

can be performed by another vehicle. Therefore, when 1 ≤ α < 2, two vehicles are

= 2t + ατe .
necessary and sufficient (that is, m  = 2) to achieve Cmax

Case 2 (α < 1): When α < 1, we set Cmax to the larger of 2t + ατe and τe + t + α

− ατe . Similar to Case 1, we
based on the values of α, τe , and t. Then, I  = Cmax

can construct a solution with R1 = I and prevent idleness after R1 . Now, contrary
to Case 1, as α < 1, Rk − Rk−1 gets smaller as k gets larger. Therefore, the required
number of vehicles, m, in this solution, is equal to the number of tours performed
during (Rκ − 2t, Rκ ] as a vehicle tour requires 2t time units. Before analyzing the
value of m, we should note that m gives only an upper bound on m  differently from
Case 1 where it provides the exact value of m  . The reason is that there might be
another optimal solution with a smaller number of vehicles, which has R1 < I  and
allows a total idle time of I  − R1 after R1 .
Let us find an upper bound on the number of tours performed in interval (Rκ −
2t, Rκ ], denoted by m, which is also an upper bound on m  . As we assume that
there is at least one time unit between consecutive visits of the site, the first tour of
(Rκ − 2t, Rκ ], say tour k  , collects at least λ items. It requires λ/μ = α time units to
process this amount. Therefore, tour k  + 1, must return before Rk  + α. It collects at
most αλ items, which requires a processing time of αλ/μ = α 2 time units. Therefore,
2
tour k  + 2, must return before
mRk + 1k + α . Continuing like this, we find that m is the
largest integer that satisfies k=1 α ≤ 2t.
We summarize the results of this section in the next proposition.
Proposition 4 If m  is the number of vehicles necessary and sufficient to achieve

= max{2t + ατe , τe + t + α}, then,
Cmax

m =

⎧
1,
⎪
⎪
⎨ 2,

if α ≥ 2;
if 1 ≤ α < 2;

≤ m, where m is the largest
⎪
⎪

⎩
k
integer that satisfies m
k=1 α < 2t, if 0 < α < 1.

3.2 Single vehicle

when there is only one vehicle.
Now let us analyze Cmax
Case 1 (α ≥ 2): According to Proposition 4, if α ≥ 2, a single vehicle is sufficient

= 2t + ατe .
to achieve the lower bound, i.e., Cmax
Case 2 (1 ≤ α < 2): If 1 ≤ α < 2, we can construct a feasible solution in
which the vehicle performs κ tours where tour k = 1, . . . , κ is completed at time
Rk = 2tk + w, where w ≥ 0 is the waiting time of the vehicle at the processing
facility before starting the first tour. We will determine the optimal level of w in order
to minimize the idleness. In this solution, the first tour collects A1 = λ(t + w) items
and each tour k = 2, . . . , κ − 1 collects Ak = 2λt items, where Rk − Rk−1 = 2t,
for k = 1, . . . , κ. Since α ≥ 1 and Q(Rk ) ≥ Ak = 2λt ≥ μ(Rk+1 − Rk ) = 2μt,
the processor never stays idle after R2 as in the case of Fig. 1a. Now let us analyze
the amount of idle time in (0, R2 ], which is equal to I1 + I2 . As I1 = R1 = 2t + w and
I2 = max{0, R2 − R1 − A1 /μ} = max{0, 2t −λ(t + w)/μ} = max{0, 2t −α(t + w)},

123

1636

E. Yücel et al.

I1 + I2 = max{2t + w, t (4 − α) + w(1 − α)}. I1 + I2 is minimized when w =

= ατe + (1 + 2/α)t.
t (2/α − 1). Setting w = t (2/α − 1) gives a makespan of Cmax
Case 3 (α < 1): When α < 1, let us construct a feasible CfPP solution, in which
the vehicle performs κ = 	(τe + t)/(2t)
 tours, where tour k = 1, . . . , κ is completed
at time 2tk + w and tour κ visits the site exactly at time τe . That is, Rκ = τe + t and
w = τe + t − 2tκ is the waiting time of the vehicle at the processing facility before
starting the first tour. Note that, w < 2t since κ = 	(τe + t)/(2t)
. In this solution, the
first tour collects A1 = λ(t + w) items and each tour k = 2, . . . , κ collects Ak = 2λt
items and satisfies Rk+1 − Rk = 2t.
In order to calculate the makespan of this solution, let us analyze whether all items
collected by the first κ − 1 tours have been processed by the end of tour κ, i.e.,
Rκ = τe + t. Let us assume that in case of an arrival of new items, these items are
processed first, that is, newly arriving items have higher priority than the items in the
processor queue, if any. Note that this does not affect the makespan but simplifies the
following analysis. For the first tour, A1 = λ(t + w) = λ(τe + 2t (1 − κ)). Then, all
items collected in the first tour can be processed by the time R2 , if A1 ≤ 2μt, which
requires α ≤ 1/(τe /2t + 1 − κ). Otherwise, we have A1 − 2μt items waiting in the
queue at R2 . For tours k = 2, . . . , κ − 1, as Ak /μ < Rk+1 − Rk , all items collected in
a tour are processed by the end of the next tour by our assumption. In addition, there is
an excess processing capacity of μ(Rk+1 − Rk ) − Ak = 2t (μ − λ), which can be used
to process some of the items collected by the first tour. Therefore, the amount that is
waiting in the queue at R2 can be processed by Rκ , if A1 − 2μt ≤ 2t (κ − 2)(μ − λ),
which requires α ≤ (κ − 1)/(τe /2t − 1). Then, if α ≤ (κ − 1)/(τe /2t − 1), this
solution has a makespan of Cmax = Rκ + Aκ /μ = τe + (1 + 2α)t. In order to see
the optimality of this solution, let us compare it with a solution that visits the site at
τe + x, where x > 0. In this solution, the last tour is completed at τe + x + t and
collects at least max{1, λ(2t − x)} items. It takes at least max{1/μ, α(2t − x)} time
units to process that amount. Therefore, this solution has a makespan of τe + x + t +
max{1/μ, α(2t − x)}, which is strictly larger. Therefore, the constructed solution is
optimal for the case α ≤ (κ − 1)/(τe /2t − 1) where α < 1.

since
For (κ − 1)/(τe /2t − 1) < α < 1, we provide an upper bound on Cmax

finding Cmax is cumbersome. Let us analyze the solution constructed in the previous
paragraphs for this case. Note that, A1 < λ3t since w < 2t. As μ2t of A1 is processed
during (R1 , R2 ] and some of the remaining unprocessed items collected in the first tour
can be processed up to Rκ , the amount waiting in the queue at Rκ is strictly less than

< Rκ +(5λt −2μt)/μ =
Aκ +(A1 −μ2t) = 5λt −2μt. As Rκ = τe +t, we get Cmax
τe + (5α − 1)t. Same as the case with α ≤ (κ − 1)/(τe /2t − 1), a solution cannot have
a makespan smaller than Rκ + Aκ /μ, which is greater than or equal to τe + (1 + 2α)t.

< τe + (5α − 1)t.
Thus, τe + (1 + 2α)t ≤ Cmax
Proposition 5 summarizes the analysis.
Proposition 5 When there is a single vehicle, if
(i)
(ii)
(iii)
(iv)


α ≥ 2, then Cmax
= 2t + ατe ;

= ατe + (1 + 2/α)t;
1 ≤ α < 2, then Cmax

= τe + (1 + 2α)t;
α < 1 with α ≤ (κ − 1)/(τe /2t − 1), Cmax

< τe + (5α − 1)t.
α < 1 with α > (κ − 1)/(τe /2t − 1), τe + (1 + 2α)t ≤ Cmax

123

A constant-factor approximation algorithm for multi-vehicle collection for processing problem

1637

From the results in Propositions 4 and 5, we see that utilizing a single vehicle does
not worsen the makespan significantly, except for the rare case in which the processing
facility is far from the single site. For α ≥ 2, a single vehicle is sufficient to obtain
the optimal makespan. For α < 2, the difference between the optimal makespans of
single and multi-vehicle cases is at most t, when 1 ≤ α < 2, and at most 3t, when
α < 1, where t is the traveling time between the single site and the processing facility.
Therefore, using a single vehicle increases the optimal makespan by at most 3t time
units.
We utilize the results of Propositions 4 and 5 in developing a heuristic solution
approach for the general case with multiple sites and multiple vehicles in the next
section.
4 A constant-factor approximation algorithm
In this section, we develop a polynomial time algorithm for mCfPP(Cmax ) that produces provably good solutions for every instance. The algorithm is composed of two
phases. In the first phase, called Clustering, the set of sites is partitioned into m disjoint
subsets where m is the number of vehicles. The clusters are formed with the aim of
minimizing the maximum duration of the shortest Hamiltonian tour within the clusters. We utilize an existing algorithm for this problem. Then, each vehicle is assigned
to exactly one cluster, and performs tours to transfer the accumulated workload of the
sites in the cluster to the processing facility. In the second phase, called Scheduling,
the touring schedule of each vehicle is determined. We mimic the tour construction
of Sect. 3.2 in this phase by performing identical tours. We refer to this algorithm as
cluster-schedule (CS).
An algorithm is said to be a δ-approximation algorithm (or have a δ approximation
ratio) for a minimization problem P, if for any instance of P, it yields a feasible
solution whose objective function value is at most δ times the optimum value. For
NP-hard problems, developing polynomial time approximation algorithms is a widely
accepted approach to cope with computational difficulty while providing worst-case
performance guarantees [7]. In Sect. 4.3, we prove that our proposed algorithm has
a constant approximation ratio.
4.1 Clustering
The aim of the clustering phase is to form m disjoint clusters of sites such that the
longest traveling time within the clusters is minimized. We define the traveling time
within a cluster as the shortest traveling time of a tour that starts and ends at node 0
and visits each site in the cluster exactly once.
The clustering problem we want to solve is equivalent to the m-Traveling Salesman
Problem (m-TSP, m ≥ 2) defined by [4], and also referred to as Minmax m-TSP by [3].
In Minmax m-TSP, an undirected complete graph is given with a distinguished initial
node 0 and a symmetric matrix of nonnegative integer costs (distances or travel times)
associated with the edges. There are m identical vehicles based at node 0. A solution
consists of m vehicle tours starting and ending at node 0 such that every node is visited

123

1638

E. Yücel et al.

by one vehicle. The objective is to minimize the maximum cost of the m tours. [4] prove
that the problem is NP-hard for m ≥ 2 and provide two approximation algorithms
for metric Minmax m-TSP. In the clustering phase, we use the first heuristic of [4],
which runs in polynomial time and is the best approximation algorithm proposed so
far having an approximation ratio of two. In general, any approximation algorithm for
Minmax m-TSP, with performance ratio δ mT S P , can be utilized.
Given an instance of mCfPP, we run the two-approximation algorithm given in
[4] for the metric completion of G. Let N (v) ⊆ N represent the set of sites in
cluster v ∈ {1, . . . , m}, and let A(v) ⊆ E represent the set of edges between the
nodes in N (v) ∪ {0}. In this approximate Minmax m-TSP solution, let D(v) represent
the traveling time of the tour that starts and ends at node 0, and visits each site in
N (v), and
let 
(v) represent the total accumulation rate of the sites in N (v), that is,
, where Dm is the optimal

(v) = j∈N (v) λ j . Then, maxv=1,...,m D(v) ≤ δ mT S P Dm
value of the Minmax m-TSP problem for the associated mCfPP instance.

4.2 Scheduling
In the scheduling phase, we assign a vehicle to each cluster and then construct and
schedule the tours of each vehicle independently. Construction of the tours of each
vehicle is inspired by the single vehicle, single site case provided in Sect. 3.2. In
Sect. 3.2, we showed that a single vehicle performing subsequent tours without waiting between them prevents the idleness of the processor after the second tour when
α ≥ 1, and minimizes the unprocessed amount at time τe when α < 1. Based on this
observation, the scheduling phase makes each vehicle perform tours without waiting
except that the vehicle might wait at the processing facility before the first tour. Each
tour of a vehicle is the same as the tour of the corresponding vehicle in the Minmax
m−TSP solution.
For each cluster v ∈ {1, . . . , m}, starting at time w(v) ≥ 0, the vehicle performs
κ(v) identical tours, each with traveling time D(v). Clearly, tour k = 1, . . . , κ(v) of
cluster v is completed at time Rk (v) = D(v)k+w(v), that is, Rk+1 (v)−Rk (v) = D(v).
We set κ(v) to be the minimum integer that satisfies D(v)κ(v) > τe and w(v) =
τe − D(v)(κ(v) − 1) so that the starting time of tour κ(v) is τe and w(v) < D(v). Note
that if D(v) > τe , κ(v) = 1 and the vehicle waits at the processing facility until τe ,
i.e., w(v) = τe . Figure 3 illustrates a time line representation of the vehicle assigned to
cluster v in an mCfPP solution output by the CS algorithm. Let X k (v) be the collected

Fig. 3 Graphical time representation of the vehicle assigned to cluster v in an mCfPP solution found by
the CS algorithm

123

A constant-factor approximation algorithm for multi-vehicle collection for processing problem

1639

amount in tour k in such a solution. Note that X 1 (v) depends on the visit time of each
site in the first tour. As Fig. 3 illustrates, the time between the consecutive visits to a
site is exactly D(v) time units. All of the accumulated amount at the sites in cluster v,
which is equal to 
(v)τe = 
(v)(D(v)(κ(v) − 1) + w(v)), is collected through κ(v)
tours. These facts lead us to Lemma 3, which will be used later in the performance
analysis.
Lemma 3 For cluster v,
(1) X k (v) = 
(v)D(v) for all k = 2, . . . , κ(v) − 1, and
(2) X 1 (v) + X κ(v) (v) = 
(v)(D(v) + w(v)).
4.3 Performance analysis
 , and then we
We first provide a lower bound for the optimal makespan, i.e., Cmax
derive the approximation ratio of our algorithm.

Proposition 6 When there are m vehicles,


≥ max{ατe , τe + Dm
/2},
Cmax

(4)

 is the optimal value of the corresponding Minmax m-TSP problem.
where Dm

Proof As in Eq. (2), clearly, Cmax
≥ ατe . Now, we derive the other bound in Eq. 4.
Let T be the traveling time after τe in an optimal solution to mCfPP(Cmax ) and let
P(v) be the corresponding partial tour of vehicle v after τe . Note that all sites should
be visited after τe in any feasible solution. However, the last tours of the vehicles may
start before τe . We bound T using a solution to Minmax m-TSP.
Let us first obtain a feasible Minmax m-TSP solution from P(v). P(v) starts either
at a node in N (v) ∪ {0} or at a point on an edge in A(v), visits a subset of sites in
N (v), and returns to node 0. If P(v) starts at node 0, we keep it in the Minmax m-TSP
solution. Otherwise, we delete the part of P(v) up to its first node (say, node i), if
any, and add edge (0, i) to this path. By this procedure, we obtain a Minmax m-TSP
 is the optimal value
solution with objective value of at most T + maxi∈N t0i . Since Dm


of the Minmax m-TSP, T + maxi∈N t0i ≥ Dm . Then, T ≥ Dm − maxi∈N t0i .
 /2. Therefore, T ≥ D  /2. Then,
Since we have metric ti j , maxi∈N t0i ≤ Dm
m




Cmax ≥ max{ατe , τe + Dm /2}.

Theorem 1 There exists a (δ mT S P +3/2)-approximation algorithm for mCfPP(Cmax ),
where δ mT S P is the approximation ratio for Minmax m-TSP.
Proof We consider instances with Q 0 = 0 in the analysis, since otherwise, having an
initial queue decreases the idle time of the processor, and this assumption does not
improve Cmax . In order to facilitatethe analysis of the CS algorithm, let us allocate
a processing rate μ(v) = μ
(v)/ i∈N λi = 
(v)/α to each cluster v to have an
equal workload level, α, in all clusters. That is,

(v)/μ(v) = α, for all v = 1, . . . , m.

(5)

123

1640

E. Yücel et al.

In this way, we decompose the problem into m single vehicle problems with processing
rate μ(v) for each problem v. Let us consider a solution output by the CS algorithm.
We next characterize the total idle time, I (v), and the makespan, Cmax (v), for each v,
for the following three possible cases. Note that Cmax (v) = I (v) + 
(v)τe /μ(v) =
I (v) + ατe by Eqs. 1 and 5. We drop the index v from Rk (v) in the following and use
Rk for Rk (v) for simplicity.
– Case 1 (κ(v) = 1): Note that in this case, D(v) > τe and the vehicle that is assigned
to the cluster v does not perform a tour before τe . Therefore, the vehicle transfers
all of the accumulated amount in cluster v, i.e., 
(v)τe , to the processing facility
at time Rκ(v) = τe + D(v). Due to Eq. 5, it takes 
(v)τe /μ(v) = ατe time units to
process this amount. Then, Cmax (v) = Rκ(v) + ατe = τe + D(v) + ατe . Therefore,
Cmax (v) = C1 + C2 + C3 , where
 /2, C = D(v) − D  /2, and C = ατ .
C1 = τe + Dm
2
3
e
m
Now, we provide bounds for C1 , C2 , C3 .
 .
– According to Proposition 6, C1 , C3 ≤ Cmax
 . Then, C ≤
– Due to the clustering phase of the algorithm, D(v) ≤ δ mT S P Dm
2

mT
S
P

−1/2). As Cmax cannot be less than the time required to visit all sites
Dm (δ
 ≤ C  . Then, C ≤ C  (δ mT S P − 1/2).
through m vehicles, we have Dm
2
max
max

Therefore, we have Cmax (v) ≤ Cmax (δ mT S P + 3/2), for κ(v) = 1.
– Case 2 (κ(v) > 1 and α ≥ 1): If there is only one tour before τe , i.e., κ(v) = 2,
the processor is idle up to the return time of the first tour, R1 = w(v) + D(v).
Otherwise, i.e., if κ(v) ≥ 3, the processor is idle up to the return time of the first
tour, R1 = w(v) + D(v), and idleness might occur during the second tour with a
duration of max{0, D(v) − X 1 (v)/μ(v)}. In this case, the processor never stays
idle during any other tour k 
= 1, 2, since Q(Rk−1 ) ≥ 
(v)D(v) = αμ(v)D(v) ≥
μ(v)(Rk − Rk−1 ) for k 
= 1, 2 due to Eq. 5. Then, for both cases, the total idle
time of the processor is I (v) ≤ w(v) + D(v) + max{0, D(v) − X 1 (v)/μ(v)}.
Therefore,

Cmax (v) ≤ C1 + C2 + C3 , where
 /2, and C = max{0, D(v)−X (v)/μ(v)}−
C1 = ατe , C2 = w(v)+D(v)+Dm
3
1

Dm /2.
Now, we provide bounds for C1 , C2 , C3 .
 .
– According to Proposition 6, C1 ≤ Cmax
– As κ(v) > 1, there is at least one tour before τe (see Fig. 3). Hence, w(v) +
 /2 ≤ C  , due to Proposition 6.
D(v) ≤ τe . Therefore, C2 ≤ τe + Dm
max
 /2 < 0. Otherwise, C ≤
– If D(v) − X 1 (v)/μ(v) < 0, then C3 = −Dm
3

D(v)−Dm /2 since X 1 (v)/μ(v) ≥ 0. Then, similar to the case κ(v) = 1, C3 ≤
 (δ mT S P − 1/2) ≤ C  (δ mT S P − 1/2).
Dm
max
 (δ mT S P + 3/2), for κ(v) > 1 and α ≥ 1.
Then, we have Cmax (v) ≤ Cmax
– Case 3 (κ(v) > 1 and α < 1): Idleness might occur during every tour, since

(v)D(v) ≤ αμ(v)D(v) ≤ μ(v)(Rk+1 − Rk ) due to Eq. 5. Therefore, at time

123

A constant-factor approximation algorithm for multi-vehicle collection for processing problem

1641

Rκ(v) = τe + D(v), the processor queue has at most 
(v)(D(v) + 2w(v)) − X 1 (v)
items to be processed, since 
(v)(D(v) + w(v)) − X 1 (v) of them are collected
in tour κ(v) and 
(v)w(v) of them might remain unprocessed from the previous
tours due to w(v) time units of waiting before the first tour. Then, Cmax (v) ≤
Rκ(v) + (
(v)(D(v) + 2w(v)) − X 1 (v))/μ(v), leading to
Cmax (v) ≤ C1 + C2 + C3 , where
 /2, C = D(v) − D  /2, and C = α(D(v) + w(v)) − (X (v) −
C1 = τe + Dm
2
3
1
m

(v)w(v))/μ(v).
Now, we provide bounds for C1 , C2 , C3 .
 .
– According to Proposition 6, C1 ≤ Cmax
 /2 ≤ D  (δ mT S P − 1/2) ≤
– Similar to the case κ(v) = 1, C2 = D(v) − Dm
m

mT
S
P
− 1/2).
Cmax (δ
– Since R1 = w(v)+D(v), X 1 (v) ≥ 
(v)w(v). Then, (X 1 (v)−
(v)w(v))/μ(v)
≥ 0. Therefore, C3 ≤ α(D(v) + w(v)). As κ(v) > 1, w(v) + D(v) ≤ τe

similar to the case κ(v) = 1. Then, C3 ≤ ατe ≤ Cmax
due to Proposition 6.

mT
S
P
+ 3/2), for κ(v) > 1 and α < 1.
As a result, we have Cmax (v) ≤ Cmax (δ

When we consider all three cases, we obtain Cmax = maxv=1,...,m Cmax (v) ≤
 (δ mT S P + 3/2).


Cmax
Corollary 1 If we use the two-approximation algorithm of [4] for Minmax m-TSP
and set δ mT S P = 2, we obtain a 3.5-approximation algorithm. Hence, mCfPP(Cmax )
admits a constant-factor algorithm.
Since we have an approximation preserving reduction in the proof of Lemma 2,
we note that the approximation ratio in Theorem 1 is also valid for the scheduling problem of Sect. 2.1. Hence, we deduce the following corollary as a side result.
Corollary 2 The scheduling problem, F H 2, (Pm (1) , 1(2) ), (β → γ )||Cmax , is
(δ mT S P + 3/2)-approximable, where δ mT S P is the approximation ratio for Minmax
m-TSP.
5 Conclusions
In this paper, we define the CfPP with multiple vehicles and the makespan objective
(mCfPP(Cmax )). We prove that the problem is NP-hard by a reduction from a twostage, hybrid flowshop scheduling problem. We analyze the special case with a single
site to find the number of vehicles necessary to achieve the minimum makespan and
identify the minimum makespan with a single vehicle. Using the insights obtained from
these results, we develop a heuristic called cluster-schedule (CS). We provide the first
approximation result for mCfPP(Cmax ) and prove that a constant-factor approximation
exists for this NP-hard problem.
Acknowledgments The authors would like to thank the anonymous reviewers for their valuable comments
and suggestions to improve the quality of the paper.

123

1642

E. Yücel et al.

References
1. Ahmadi, J.H., Ahmadi, R.H., Dasu, S., Tang, C.S.: Batching and scheduling jobs on batch and discrete
processors. Oper. Res. 40, 750–763 (1992)
2. Cheng, T.C.E., Wang, G.: Batching and scheduling to minimize the makespan in the two-machine
flowshop. IIE Trans. 30, 447–453 (1998)
3. Franca, P.M., Gendreau, M., Laporte, G., Müller, F.M.: The m-traveling salesman problem with minmax
objective. Transp. Sci. 29, 267–275 (1995)
4. Frederickson, G.N., Hecht, M.S., Kim, C.E.: Approximation algorithms for some routing problems.
SIAM J. Comput. 7, 178–193 (1978)
5. McDonald, J.J.: Vehicle scheduling—a case study. Oper. Res. Q. (1970–1977) 23, 433–444 (1972)
6. Revere, L.: Re-engineering proves effective for reducing courier costs. Bus. Process Manag. J.
10, 400–414 (2004)
7. Williamson, D.P., Shmoys, D.B.: The Design of Approximation Algorithms, Page Numbers. Cambridge
University Press, New York (2010)
8. Yücel, E., Salman F.S., Gel E.S., Örmeci, E.L., Gel, A.: Optimizing specimen collection for processing
in clinical testing laboratories. Eur. J. Oper. Res. (2012, accepted)

123

This article was downloaded by: [149.169.145.169] On: 09 June 2017, At: 22:53
Publisher: Institute for Operations Research and the Management Sciences (INFORMS)
INFORMS is located in Maryland, USA

Operations Research
Publication details, including instructions for authors and subscription information:
http://pubsonline.informs.org

Price and Lead Time Quotation for Contract and Spot
Customers
A. Baykal Hafızoğlu, Esma S. Gel, Pınar Keskinocak

To cite this article:
A. Baykal Hafızoğlu, Esma S. Gel, Pınar Keskinocak (2016) Price and Lead Time Quotation for Contract and Spot Customers.
Operations Research 64(2):406-415. https://doi.org/10.1287/opre.2016.1481
Full terms and conditions of use: http://pubsonline.informs.org/page/terms-and-conditions
This article may be used only for the purposes of research, teaching, and/or private study. Commercial use
or systematic downloading (by robots or other automatic processes) is prohibited without explicit Publisher
approval, unless otherwise noted. For more information, contact permissions@informs.org.
The Publisher does not warrant or guarantee the article’s accuracy, completeness, merchantability, fitness
for a particular purpose, or non-infringement. Descriptions of, or references to, products or publications, or
inclusion of an advertisement in this article, neither constitutes nor implies a guarantee, endorsement, or
support of claims made of that product, publication, or service.
Copyright © 2016, INFORMS
Please scroll down for article—it is on subsequent pages

INFORMS is the largest professional society in the world for professionals in the fields of operations research, management
science, and analytics.
For more information on INFORMS, its publications, membership, or meetings visit http://www.informs.org

OPERATIONS RESEARCH
Vol. 64, No. 2, March–April 2016, pp. 406–415
ISSN 0030-364X (print)  ISSN 1526-5463 (online)

http://dx.doi.org/10.1287/opre.2016.1481
© 2016 INFORMS

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:53 . For personal use only, all rights reserved.

Price and Lead Time Quotation for
Contract and Spot Customers
A. Baykal Hafızoğlu, Esma S. Gel
School of Computing, Informatics and Decision Systems Engineering, Arizona State University, Tempe, Arizona 85287
{baykal@asu.edu, esma.gel@asu.edu}

Pınar Keskinocak
School of Industrial and Systems Engineering, Georgia Institute of Technology, Atlanta, Georgia 30332, pinar@isye.gatech.edu

We study price and lead time quotation decisions in a make-to-order system with two customer classes: (1) contract customers
whose orders are practically always accepted and fulfilled based on a contract price and lead time agreed on at the beginning
of the time horizon, and (2) spot purchasers who arrive over time and are quoted a price and lead time pair dynamically. The
objective is to maximize the long-run expected average profit per unit time, where profit from a customer is defined as
revenues minus lateness penalties incurred because of lead time violations. We model the dynamic quotation problem of the
spot purchasers as an infinite horizon Markov decision process, given a fixed price and lead time for contract customers. We
analyze the impact of customer preferences (e.g., price and lead time sensitivity) on the optimal price and lead time decisions
for spot purchasers and characterize the optimal policy. We explore the benefits of dynamic quotation compared to the use of
fixed price and lead times, and provide recommendations for firms. Finally, we analyze the optimal contract terms given the
dynamic quotation strategy for spot purchasers and discuss the profit improvements offered by the optimal mix of spot and
contract customers.
Keywords: due date management; pricing; Markov decision processes.
Subject classifications: dynamic programming/optimal control: applications; inventory/production: policies: lead time;
inventory/production: policies: pricing; inventory/production: stochastic models; queues: priority.
Area of review: Operations and Supply Chains.
History: Received November 2013; revisions received December 2014, August 2015; accepted December 2015. Published
online in Articles in Advance April 4, 2016.

1. Introduction

customers and reserving capacity for future spot purchasers.
Favorable contracts can attract contract customers and
increase capacity utilization. For example, Wencor, an aircraft
parts distributor, offers lower prices and shorter lead times to
contract customers to ensure some level of long-term business security (Wencor.com 2012). However, this leaves less
flexible capacity to be used for potentially more profitable
spot purchasers. Hence, offering the right contract terms
and spot market deals is crucial for achieving a desirable
customer mix and profitability in the long run.
In this paper, we address a make-to-order manufacturer’s
(or service provider’s) capacity reservation trade-off by considering two main aspects of contracting and spot purchasing:
price and lead time. The prices and lead times of contract
customers are set at the beginning of the planning horizon,
and all orders from contract customers must be accepted and
prioritized for processing. For spot purchasers, the company
has the option of rejecting orders; hence, we propose the use
of a dynamic price and lead time quotation policy (DPLQ)
dependent on the system conditions (i.e., congestion). Previous studies on price and/or lead time quotations investigate
the impact of customer preferences (e.g., price/lead time
sensitivity) on the optimal decisions using relatively simple
models where price and lead time quotes are not dynamic

Contracts formalize short- and long-term transactions in
supply chains and cultivate relationships while providing
suppliers with a partial view of future demand and buyers
with some guarantee of capacity availability and price
stability. Noncontractual spot purchases do not imply any
future business guarantee, but they do provide flexibility
to the suppliers to sell their excess capacity or inventory,
and they allow buyers to better meet their demand. Often,
companies prefer to buy and/or sell through a mix of
long-term contractual agreements and spot purchases to
benefit from both channels. For example, Hewlett Packard,
one of the largest memory part buyers, has developed a
procurement portfolio to meet 90% of its demand using
long-term contracts; the remaining 10% is met from the spot
market (Feng and Pang 2010). The global steel manufacturer
ArcelorMittal sells 80% of its steel in the spot market and
20% to customers with contracts (Haksoz and Kadam 2009).
There is a similar trend with contractual agreements in
the metal products sector, with a 64%/36% split between
contracts and spot customers (Stundza 2007).
In capacitated manufacturing/service systems, a critical trade-off is between committing capacity to contract
406

Hafizoglu, Gel, and Keskinocak: Price and Lead Time Quotation

407

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:53 . For personal use only, all rights reserved.

Operations Research 64(2), pp. 406–415, © 2016 INFORMS

and demand functions are linear (Easton and Moodie 1999,
Ray and Jewkes 2004, Wu et al. 2012). We generalize these
models considering a more general demand function and
dynamic quotation.
For given contract terms, our theoretical analyses indicate
that the optimal spot purchaser quotes depend heavily on
price and lead time sensitivity of spot purchasers, and
the probability of completing an order on time follows a
newsvendor-like critical ratio. When spot purchasers are
highly sensitive to lead time (price) changes, it is optimal to
quote zero lead times (low prices) and modify the price (lead
time) quotes according to the marginal profit obtained from
a spot purchaser. Noting that DPLQ policies might require
frequent price and/or lead time changes with potentially
undesirable effects, we compare DPLQ policies to fixed
price/lead time quotation policies. Although computational
results show that DPLQ policies often lead to significant
profit increases, dynamic pricing combined with fixed lead
time quotes may perform satisfactorily when spot purchasers
are lead time sensitive and the demand is linear in lead time.
Our analysis of the optimal contract terms shows that
achieving the optimal spot-contract mix boosts profits when
the contract customer population is willing to pay a wide
range of possible prices; by contrast, when this price range is
tight, simple policies that allocate all the capacity to contract
customers or spot purchasers may perform reasonably well.
Using our theoretical analysis, we develop a fast algorithm
to approximate the optimal contract terms.
The paper is organized as follows. We review the literature
in §2. In §3, we model and analyze the dynamic price and
lead time quotation problem for spot purchasers. In §4, we
discuss the optimal contract customer/spot purchaser mix
and the benefits of achieving the optimal mix. We summarize
our findings and conclude in §5.

2. Literature Review
There has been a significant amount of work on dynamic
pricing and lead time quotation separately (see Keskinocak
and Tayur 2004 and Elmaghraby and Keskinocak 2003
for reviews in due date management and dynamic pricing
literature, respectively). The literature on joint dynamic price
and lead time quotation is relatively scarce (see Çelik and
Maglaras 2008, Feng et al. 2011).
Recent work on dynamic pricing in make-to-order systems
focuses on admission control models with the option of
accepting or rejecting price sensitive customers (Yoon and
Lewis 2004, Aktaran-Kalayci and Ayhan 2009, Çil et al.
2009, Afeche and Ata 2013). Most of the studies in this
stream employ Markov decision process (MDP) models and
focus on developing structural properties of optimal policies,
such as the monotonicity of price quotes.
As stated in Savasaneril et al. (2010), a remarkable portion
of the due date management literature focuses on sequencing and due date setting decisions assuming all arriving
orders must be served (Bookbinder and Noor 1985, Wein

1991, Spearman and Zhang 1999). Some recent studies
employ admission control using due date decisions (Duenyas
and Hopp 1995; Duenyas 1995; Keskinocak et al. 2001;
Charnsirisakskul et al. 2004, 2006; Ata 2006; Kapuscinski
and Tayur 2007; Ata and Olsen 2009; Savasaneril et al.
2010). Among these studies, Keskinocak et al. (2001),
Charnsirisakskul et al. (2004, 2006), and Kapuscinski and
Tayur (2007) assume deterministic processing times; Ata
(2006) and Ata and Olsen (2009) employ heavy-traffic
approximations to overcome the complexity of MDP formulations. Duenyas and Hopp (1995) study a dynamic lead
time quotation problem in a make-to-order system as an
M/M/1 queue using an MDP model. Using a similar MDP
model, Duenyas (1995) considers multiple customer classes,
where all orders are assumed to be sequenced in first-come,
first-served (FCFS) order. Similar to our setting, but focusing
only on lead time decisions, Keskinocak et al. (2001) and
Kapuscinski and Tayur (2007) consider two customer classes,
where the high-priority customer either brings more revenue
or incurs a higher delay penalty. Carr and Lovejoy (2000)
consider two prioritized customer classes, where the demand
of the higher (lower)-priority customer class is served earlier
(may not be fulfilled).
Joint price and lead time decisions, where at least one of
these decisions are constant over time, are considered by
Palaka et al. (1998), Easton and Moodie (1999), ElHafsi
(2000), Plambeck (2004), Ray and Jewkes (2004), Chaharsooghi et al. (2011), Zhao et al. (2012), and Wu et al. (2012).
Fixed price and lead time quotation under decentralized
supply chains is considered by Liu et al. (2007), Pekgun
et al. (2008), Hua et al. (2010), and Xiao et al. (2010).
Çelik and Maglaras (2008) study DPLQ under heavytraffic assumptions. They also discuss the effects of lead
time flexibility, expediting (under a high cost), and dynamic
pricing. Feng et al. (2011) address DPLQ in a make-toorder system using a GI/M/1 queuing model and an MDP
formulation. In contrast to Feng et al. (2011), we consider
two customer classes with different contractual rights to
processing prioritization, discuss the impact of customer
preferences on the optimal policy, and quantify the benefits
of DPLQ versus fixed price and lead time policies, as well
as optimizing the contract-spot customer mix.

3. Dynamic Quotation Model for
Spot Purchasers
We consider a make-to-order system with two customer
classes, each arriving according to a Poisson process with
rate k , where k = C and k = S denote contract customers
and spot purchasers, respectively. We refer to contract
customer and spot purchaser orders as C-order and S-order,
respectively. The service times for both customer classes are
independent and identically distributed exponential random
variables with rate . Hence, the system is modeled as an
M/M/1 queue with two customer classes. At the beginning
of the time horizon, the company sets the price 4pC ¾ 05

Hafizoglu, Gel, and Keskinocak: Price and Lead Time Quotation

408

Operations Research 64(2), pp. 406–415, © 2016 INFORMS

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:53 . For personal use only, all rights reserved.

and the lead time 4lC ¾ 05 for C-orders, which are always
accepted and fulfilled. When an S-order arrives, a lead
time 4lS ¾ 05 and a price 4pS ¾ 05 are quoted dynamically,
based on the system state. The spot purchaser accepts the
quote with probability f S 4pS 1 lS 5, which we refer to as the
acceptance probability function.
Assumption 1. (i) f S 4pS 1 lS 5 is continuous, nonincreasing, twice differentiable, concave in pS and lS , and
¡ 2 f S 4pS 1 lS 5/¡pS ¡lS ¶ 0.
(ii) There exists a nonnegative lower bound on price,
S
pmin
, and lS ¾ 0, such that f S 4pS 1 lS 5 remains constant
S
S
S
for pS ¶ pmin
. That is, f S 4p1 l5 = f S 4pmin
1 l5 for p < pmin
S
S
S
and l ¾ 0. Furthermore, f 4pmin 1 05 = 1. Note that pmin
represents the minimum of the willingness to pay of the
individuals in the spot purchaser population and is referred
to as the “accept-all price.”
(iii) Given a lead time quote lS , there exists an upper
S
bound on price, denoted by p̄max
4lS 5, such that any price
quote above this upper bound is rejected by spot purchasers.
S
S
That is, p̄max
4lS 5 = min8p ∈ <+ 2 f S 4p1 lS 5 = 09. p̄max
4lS 5 is
S
S
nonincreasing in l , and its highest value is denoted as pmax
;
S
S
S
i.e., pmax = p̄max 405. pmax is referred to as the “reject-all
price.”
(iv) Given a price quote pS , there exists an upper bound
S
on lead time, denoted by l̄max
4pS 5, such that any lead time
quote above this upper bound is rejected by spot purchasers.
S
S
That is, l̄max
4pS 5 = min8l ∈ <+ 2 f S 4pS 1 l5 = 09. l̄max
4pS 5 is
S
non-increasing in p , and its highest value is denoted as
S
S
S
S
lmax
; i.e., lmax
= l̄max
4pmin
5.
Assumption 1(i) defines the general properties of the
acceptance probability function, such as monotonicity and
concavity in price and lead time, which are well accepted in
the literature. The assumption that ¡ 2 f S 4pS 1 lS 5/¡pS ¡lS ¶ 0
fortunately holds for a wide range of order acceptance functions. For example, consider an additive form,
Figure 1.

f S 4pS 1 lS 5 = 1 − p 4pS 5 − l 4lS 5, and a multiplicative form,
f S 4pS 1 lS 5 = 1 − p 4pS 5n l 4lS 5m , where p 4·5 and l 4·5
are nondecreasing and nonnegative functions of price and
lead time, and m ¾ 1, n ¾ 1. Since ¡p 4pS 5/¡pS ¾ 0, and
¡l 4lS 5/¡lS ¾ 0, the condition ¡ 2 f S 4pS 1 lS 5/¡pS ¡lS ¶ 0 holds
true for both multiplicative and additive forms.
An f S 4pS 1 lS 5 that is linear in both pS and lS is illustrated
in Figures 1(a) and 1(b) in three and two dimensions,
respectively. In the remainder of the paper, we use the
2-dimensional representation.
The goal in DPLQ is to dynamically quote prices and lead
times to spot purchasers with the objective of maximizing
the long-run average expected profit per unit time, where
profit from a customer is equal to the revenue minus late
delivery penalties. Using the properties of f S 4pS 1 lS 5, we
first restrict the optimal price and lead times into a finite
region in Observation 1.
Observation 1. There exists at least one optimal solution
to DPLQ in  S , where

	
S
S
S
 S = 4p1l5 ∈ <2 2 pmin
¶ p ¶ p̄max
4l51 0 ¶ l ¶ l̄max
4p5 0 (1)
All proofs are provided in the e-companion (available as supplemental material at http://dx.doi.org/10.1287/
opre.2016.1481) of the paper.
In Figure 1(b),  S is shown as the area under the
S
f 4pS 1 lS 5 = 0 line.
The state of the system at time t is defined by the vector
X4t5 = 4I4t51 J 4t51 K4t55 ∈ S, where I4t5 and J 4t5 denote
the number of spot purchasers and contract customers in
the system, respectively, and K4t5 denotes the class of the
customer order currently being served. We assume a finite
buffer of size N (due to the need to limit the state space
for numerical analysis); i.e., none of the incoming orders
are accepted when there are N orders in the system. While

(Color online) Illustration of the acceptance probability function.
f ( pS, lS)
pS
1
S
pmax

f ( pS, l S) = 0
0

S (l)
pmax

S
pmin

0

lS

p

S
lmax
S
pmin
S
pmax

0

S
lmax
( p)

l

S
lmax

f S( pS, l S) = 1

pS
(a) Three-dimensional representation

(b) Two-dimensional representation

lS

Hafizoglu, Gel, and Keskinocak: Price and Lead Time Quotation

409

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:53 . For personal use only, all rights reserved.

Operations Research 64(2), pp. 406–415, © 2016 INFORMS

this implies the possible rejection of a C-order, we use a
sufficiently high buffer size in the numerical analysis, such
that the probability of customer rejection due to a full buffer
is negligibly small.
Note that 8X4t51 t ∈ T 9 defines a stochastic process and can
be modeled as a continuous time Markov chain. We drop t
from the notation and simply use 4i1 j1 k5 to denote the state
(note that the price and lead time quotations of the customers
already in system do not affect the current state). The state
space S can be denoted as S = S1 ∪ S2 ∪ S3 ∪ 8401 01 059,
where S1 = 84i1 j1 k52 i + j ¶ N 1 i ¾ 11 j ¾ 11 k ∈ 8C1 S99,
S2 = 84i1 01 S52 1 ¶ i ¶ N 9, S3 = 8401 j1 C52 1 ¶ j ¶ N 9,
and S = N 2 + N + 1. Note that S2 and S3 denote the
states where there are C- and S-orders in the system,
respectively, and 401 01 05 corresponds to the state when
there are no orders in the system.
We assume that C-orders are always prioritized and
processed ahead of the S-orders, and preemption is not
allowed. Orders within the same class follow the FCFS
sequence. Hence, at any time t ¾ 0, when the state is 4i1 j1 k5
with i + j > 0,
(i) An incoming S-order is placed to position i + j + 1
(Figure 2).
(ii) An incoming C-order is placed to position j + 1, if
there is a C-order in process (Figure 2(a)); otherwise, it is
placed to position j + 2 (Figure 2(b)).
Most of the due date management studies, which use
MDP formulations similar to ours, consider a single class of
customers with FCFS ordering (Duenyas 1995, Duenyas and
Hopp 1995, Savasaneril et al. 2010, Feng et al. 2011). To
the best of our knowledge, this is the first study considering
prioritization of a particular customer class in the dynamic
price and lead time quotation literature.
If the service of an order is completed after the quoted
due date, there is a late delivery penalty, which increases
linearly with the tardiness duration at rate  k , k ∈ 8C1 S9. Let
LCj1 k 4lC 5 denote the expected late delivery penalty incurred
for a C-order that had a promised a lead time lC and joins
the queue when there are j C-orders in the system and a
class k order is in process. We have
LCj1 c 4lC 5 =  C
LCj1 s 4lC 5 =  C

Z



lC
Z 
lC

C
4t − lC 5gj+1
4t5 dt1

and
(2)

4t − l

C

C
5gj+2
4t5 dt1

where gjC 4t5 denotes the probability density function (pdf) of
the time-in-system of the jth order in the system, i.e., pdf of
Gamma distribution with parameters j and .
Once the quote is accepted by the spot purchaser, the
S-order joins the queue according to the above protocol.
When the state is 4i1 j1 k5, the arriving S -order waits for
at least i + j + 1 order(s) for service completion. Time-insystem for an S-order possibly increases due to C-orders

arriving before the S-order starts processing. The expected
late delivery penalty of an S-order that arrives when the
system state is 4i1 j1 ·5 can be expressed as
LSi+j 4lS 5 =  S

Z



lS

S
4t − lS 5gi+j+1
4t5 dt1

(3)

for a lead time quote of lS . In Equation (3), giS 4t5 denotes
the pdf of the time-in-system of an S-order that is placed
in the i-th position on arrival. We note that the computation of LCj1 c 4lC 51 LCj1 s 4lC 5 and LSi+j 4lS 5 requires specialized
algorithms. For the computations discussed in the paper,
we use the computational algorithms provided in Hafizoglu
et al. (2013).
The DPLQ problem is formulated as an infinite-horizon
MDP with the long-run average expected profit per unit
time criteria. Any time a state transition happens, the firm
determines the quote 4pS 1 lS 5 ∈  S for the next spot purchaser.
Although the decision epochs are all event occurrences (i.e.,
spot purchaser arrivals, contract customer arrivals, and service
completion), in practice the quote is offered only when a spot
purchaser arrives. The continuous time model is converted
to an equivalent discrete time model using a uniformization
rate of  = S + C + . The Bellman’s equation, referred to
as Dyna, is presented in Equations (4)–(6).
Dyna2

∗
vDyna



+ h∗i1 j1 k = max i1 j1 k 4pS 1 lS 51
4pS 1 lS 5∈ S

(4)

where
i1j1k 4pS 1lS 5
 S
 +C ∗



hi1j1k + h̄∗i1j1k 1 for i +j = N 1









S




4f S 4pS 1lS 54pS −LSi+j 4lS 5+h∗i+11j1k 5









+41−f S 4pS 1lS 55h∗i1j1k 5








C


+ 4pC −LCj1k 4lC 5+h∗i1j+11k 5+ h̄∗i1j1k 1






=
for 4i1j1k5 6= 40101051 i +j < N 1





S


  4f S 4pS 1lS 54pS −LS 4lS 5+h∗ 5


0
1101S








+41−f S 4pS 1lS 55h∗01010 5






C C




+
4p −LC01C 4lC 5+h∗0111C 5+ h∗01010 1









for 4i1j1k5 = 40101050

(5)

∗
In Equation (5), vDyna
is the optimal expected average
profit per unit time, h∗i1 j1 k is the relative value of starting in

Hafizoglu, Gel, and Keskinocak: Price and Lead Time Quotation

410
Figure 2.

Operations Research 64(2), pp. 406–415, © 2016 INFORMS

(Color online) Illustration of the sequencing policy, where the numbers 1̄1 2̄1 0 0 0 denote the position of the order.
S

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:53 . For personal use only, all rights reserved.

C

C

C

C

S

S

S

S

C

C

S

S

–
1

–
2

–
3

–
4

–
5

–
1

–
2

–
3

–
4

–
5

(a) Case (i). i = 3, j = 2, k = C

(b) Case (ii). i = 3, j = 2, k = S

state 4i1 j1 k5 under an optimal policy, and h̄∗i1 j1 k is defined as


h∗i1 j−11 C if i ¾ 01 j ¾ 21 k = C1






h∗
if i ¾ 11 j = 11 k = C1

 i1 01 S
∗
h̄i1 j1 k = h∗i−11 j1 C if i ¾ 11 j ¾ 11 k = S1
(6)




h∗i−11 01 S if i ¾ 21 j = 01 k = S1




 h∗
if 4i1 j1 k5 ∈ 8411 01 S51 401 11 C590
01 01 0
At state 4i1 j1 k5, the spot purchaser accepts the quote with
probability f S 4pS 1 lS 5, bringing an expected profit of pS −
LSi+j 4lS 5 and changing the state to 4i + 11 j1 k5. The quote is
rejected with probability 1 − f S 4pS 1 lS 5 and no change occurs.
When a C-order arrives, it is immediately put into the system,
transition occurs to state 4i1 j + 11 k5, and an expected profit
of pC − LCj1 k 4lC 5 is obtained. When the processing of an
order is completed, the system state is decreased by one (as
explained by Equation (6)), depending on the class of the
order currently being processed.
3.1. Characterization of the Optimal Policy
In price and lead time quotation, there are three factors that
pull the decision maker in different directions. On one hand,
short lead times and/or low prices attract more customers to
the system, depending on the customers’ price and lead
time sensitivity. On the other hand, the congestion should be
kept low by quoting higher prices and lead times to avoid
paying high delay penalties. Finally, one should consider the
profitability of an order and the immediate and future impact
of its admission on the system.
We characterize the optimal solution to Dyna considering
(i) the critical ratio of the price/lead time sensitivity of spot
purchasers to the unit delay penalty, and (ii) the profitability
of an S-order. The critical ratio of spot purchasers who are
quoted 4p1 l5 is defined as S 4p1 l5/ S , where
S 4p1 l5 =

¡f S 4p1 l5/¡l
0
¡f S 4p1 l5/¡p

S

(7)

A high (low) S 4·1 ·5 value indicates that a spot purchaser
is more sensitive to lead time (price) changes and encourages
the decision maker to quote shorter lead times to earn the
S-order. Conversely, a high value of  S motivates higher lead
time quotes to mitigate delay penalties. Hence, the critical

ratio captures the trade-off between attracting spot purchasers
and paying delay penalties. We call spot purchasers price
S
sensitive if S 4pmin
1 05/ S ¶ 1, and lead time sensitive
otherwise.
The profitability of an S-order, denoted by çi1 j1 k 4p1 l5,
is defined as the long-run marginal profit earned from an
S-order, which arrives when the system state is 4i1 j1 k5 and
is quoted a price and lead time pair of 4p1 l5. That is,
çi1 j1 k 4p1 l5 = p − LSi+j 4l5 − ãh∗i1 j1 k 1

(8)

where
ãh∗i1 j1 k

=

 ∗
∗

hi1 j1 k − hi+11 j1 k

for 4i1 j1 k5 6= 401 01 051
and i + j < N 1
(9)




for 4i1 j1 k5 = 401 01 050

h∗01 01 0 − h∗11 01 S

Note that p − LSi+j 4l5 is the immediate expected profit
earned and ãh∗i1 j1 k is the expected monetary burden brought
on by one additional spot purchaser if the quote 4p1 l5 is
accepted in state 4i1 j1 k5. To facilitate the analysis, we
define Tl and Tp as follows:
Tl : Lateness penalty savings by lead time increase from
S
the quote 4pmin
1 05
Tl = −

S
S
¡f S 4pmin
1 05/¡l

0

(10)

Tp : Revenue increase by price increase from the quote
S
4pmin
1 05
1
Tp = − S S
0
(11)
¡f 4pmin 1 05/¡p
The values Tl and Tp denote the decrease in expected
lateness penalty and increase in revenue, respectively, per
unit decrease in the acceptance probability of the quote, by
lead time and price increases, respectively.
We next define four profitability levels at a given state
4i1 j1 k5, by comparing çi1 j1 k 4p1 l5 with Tl and Tp . Profitability is:
S
• High when max8Tl 1 Tp 9 ¶ çi1 j1 k 4pmin
1 05.
S
• Medium when min8Tl 1 Tp 9 < çi1 j1 k 4pmin
1 05 ¶
max8Tl 1 Tp 9.
S
• Low when çi1 j1 k 4pmin
1 05 ¶ min8Tl 1 Tp 9 and there exists
a 4p1 l5 satisfying çi1 j1 k 4p1 l5 > 0.
• Zero when çi1 j1 k 4p1 l5 ¶ 0, for all 4p1 l5.

Hafizoglu, Gel, and Keskinocak: Price and Lead Time Quotation

411

Operations Research 64(2), pp. 406–415, © 2016 INFORMS

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:53 . For personal use only, all rights reserved.

In Section EC.1 of the e-companion, we demonstrate the
profitability levels using an example. In Theorem 1, we
characterize the optimal policy structure.
Theorem 1. An optimal policy for Dyna has the following
structure. If the profitability level is
S
High: 4pi1∗ j1 k 1 li1∗ j1 k 5 = 4pmin
1 05.
Medium: If the spot purchasers are lead time sensitive,
S
then li1∗ j1 k = 0 and pi1∗ j1 k > pmin
. If the spot purchasers are
∗
S
price sensitive, then pi1 j1 k = pmin
and li1∗ j1 k > 0.
∗
∗
Low: 4pi1 j1 k 1 li1 j1 k 5 satisfies 0 < f S 4pi1∗ j1 k 1 li1∗ j1 k 5 < 1.
Zero: 4pi1∗ j1 k 1 li1∗ j1 k 5 satisfies f S 4pi1∗ j1 k 1 li1∗ j1 k 5 = 0.
S
Under the high profitability level, offering 4pmin
1 05 is
more profitable than quoting a higher price or lead time;
S
hence, the firm offers 4pmin
1 05 to attract spot purchasers.
As profitability decreases, when spot purchasers are price
sensitive, increasing the lead time quote rather than the price
is preferable to mitigate the due date violation risk. If spot
purchasers are lead time sensitive, increasing the price quote
is preferable for the firm to gain more revenue from S-orders
while keeping the congestion under control.
S
Theorem 1 presents cases where pi1∗ j1 k = pmin
and/or
∗
li1 j1 k = 0, using the profitability level, which requires Dyna
to be solved. The simple conditions given in Theorem 2, in
S
contrast, allow us to determine pi1∗ j1 k = pmin
and/or li1∗ j1 k = 0
without solving Dyna explicitly.

Theorem 2. 4pi1∗ j1 k 1 li1∗ j1 k 5 ∈ i1S j , where
(i) if S 4p1 l5/ S > 1 for all 4p1 l5 ∈  S , then li1∗ j1 k = 0;
i.e.,

	
S
S
i1S j = 4p1 05 ∈ <2 2 pmin
¶ p ¶ pmax
0
(12)
S
(ii) if S 4p1 l5/ S ¶ ḠSi+j+1 4lmax
5 for all 4p1 l5 ∈  S , then
S
= pmin ; i.e.,
 S
	
S
i1S j = 4pmin
1 l5 ∈ <2 2 0 ¶ l ¶ lmax
0
(13)

pi1∗ j1 k

(iii) otherwise
 S
	
S
i1S j = 4pmin
1 l5 ∈ <2 2 0 ¶ l ¶ lmax

	
S
S
∪ 4p1 05 ∈ <2 2 pmin
¶ p ¶ pmax

	
∪ 4p1 l5 ∈  S 2 S 4p1 l5/ S = ḠSi+j+1 4l5 1

(14)
(15)

Rl
where ḠSi 4l5 = 1 − 0 giS 4t5 dt denotes the probability that
the i-th S-order is not completed on time if the quoted lead
time is l.
When f S 4p1 l5 is additive, in Theorem 3, we obtain a
simpler optimal policy structure for Part (iii) of Theorem 2.
Theorem 3. If f S 4p1 l5 is additive and there exists some
S
4p1 l5 ∈  S satisfying ḠSi+j+1 4lmax
5 ¶ S 4p1 l5/ S , then the
following hold:
(i) If spot purchasers are lead time sensitive, then li1∗ j1 k = 0
or S 4pi1∗ j1 k 1 li1∗ j1 k 5/ S = ḠSi+j+1 4li1∗ j1 k 5.
S
(ii) If spot purchasers are price sensitive, then pi1∗ j1 k = pmin
S
∗
∗
S
S
∗
or  4pi1 j1 k 1 li1 j1 k 5/ = Ḡi+j+1 4li1 j1 k 5.

S
S
1 05/¡l = ¡f S 4pmin
1 05/¡p = 0, then
(iii) If ¡f S 4pmin
∗
∗
S
S
∗
 4pi1 j1 k 1 li1 j1 k 5/ = Ḡi+j+1 4li1 j1 k 5.
S

We illustrate Theorems 2 and 3 in the examples provided
in the e-companion.
Theorems 2 and 3 are beneficial in three aspects. First, they
reveal that joint DPLQ is not always necessary. When spot
purchasers are highly lead time sensitive (i.e., S 4p1 l5/ S > 1
for all 4p1 l5 ∈  S as in Theorem 2(i)), firms should quote
zero lead times, and focus only on dynamic pricing. When
customers are highly price sensitive (i.e., S 4p1 l5/ S ¶
S
ḠSi+j+1 4lmax
) for all 4p1 l5 ∈  S , as in Theorem 2(ii)), however,
firms should offer the accept-all price and focus only on
dynamic lead time quotation. Hence, when the conditions in
Theorem 2(i) or (ii) hold, the firm can avoid the burden of
joint dynamic price/lead time quotation.
Second, when spot purchasers are not highly price or lead
time sensitive, Theorems 2 and 3 show that the optimal
solution often follows a newsvendor-like policy, where
the optimal solution satisfies 1 − S 4pi1∗ j1 k 1 li1∗ j1 k 5/ S = 1 −
ḠSi+j+1 4li1∗ j1 k 5 = GSi+j+1 4li1∗ j1 k 5 (Part (iii) of Theorem 2). Note
that GSi+j+1 4li1∗ j1 k 5 denotes the probability that the S-order is
met on time when an optimal lead time is quoted. Hence,
1 − S 4pi1∗ j1 k 1 li1∗ j1 k 5/ S is analogous to the well-known
newsvendor critical ratio, which determines the proportion
of demand to be met.
Third, Theorems 2 and 3 show that an optimal solution
can be found in i1S j , reducing the action space and expediting
our solution algorithms. We evaluate the computational time
savings using a numerical study. All numerical analysis
settings in this paper are presented in Section EC.6. We use a
relative value iteration algorithm (Bertsekas 2001) and solve
the problems with regular action space,  S , and the reduced
action space, i1S j for 4i1 j1 k5 ∈ S. Our analyses, conducted
using 256 instances, reveal that the action space reduction
offers minimum, average, and maximum computational
time savings of 30.75%, 74.20%, and 96.92%, respectively.
Hence, we use the reduced action spaces to solve Dyna in
the remainder of our computational analysis.
3.2. Performance of Dynamic Price and
Lead Time Quotation
We perform an extensive computational study (see EC.2 for
details) to evaluate the optimal long-run average expected
profit per unit time improvements obtained by DPLQ for
spot purchasers versus the use of fixed price and/or lead time
quotation policy, which quotes the same price (lead time)
for all 4i1 j1 k5 ∈ S. We test (i) 22 levels of break-even delay
from 0.5 to 100, (ii) three levels of arrival rates, (iii) linearity
versus concavity of f S 4p1 l5, (iv) additive versus nonadditive
f S 4p1 l5, (v) three levels of accept-all price, and (vi) four
levels of price/lead time sensitivity of spot purchasers, giving
a total of 19,872 instances.
S
We define “break-even delay,” pmin
/ S , which represents
the maximum amount of time that the delivery of an S-order
can be delayed such that the firm earns a nonnegative

Hafizoglu, Gel, and Keskinocak: Price and Lead Time Quotation

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:53 . For personal use only, all rights reserved.

412

Operations Research 64(2), pp. 406–415, © 2016 INFORMS

profit by serving that S-order at the accept-all price. A firm
can estimate break-even delay depending on the business
environment and the terms and conditions in customer
agreements. For example, a firm selling to Samsung pays
a penalty in the amount of 1% of the order value for
every week an order is delayed (Samsung 2012); thus, the
break-even delay is 100 weeks if the agreed price is the
accept-all.
The main observations from the computational study are
as follows:
• Using Dyna versus a fixed price and lead time could lead
to profitability versus being out of business, especially when
the break-even delay is “quite small” (i.e., the accept-all
price is relatively small compared to the lateness penalty).
• The benefits of Dyna are most prominent when (i) the
break-even delay and hence the profit margin decreases,
and (ii) when spot purchasers are lead time sensitive, and
the acceptance probability function is strictly concave in
lead time.

4. Contracting Model
In this section, we discuss the price and lead time decisions
for contract customers and, implicitly, the choice of the
optimal contract-spot mix to maximize the optimal long-run
average expected profit per unit time. We model the contract
customer arrival rate, C 4pC 1 lC 5, as a function of the contract
terms pC and lC , assuming that it incorporates all the factors
customers consider while signing contracts, such as supply
uncertainty, future spot price/lead time quotes, future status
of their business, etc. We denote the contract terms by 4p1 l5
for simplicity. The properties of C 4p1 l5 are outlined in
Assumption 2, which is analogous to Assumption 1, which
defines the acceptance probability of a spot purchaser.

0 ¶ 4p1 l5 ¶ 1. The independent spot purchasers arrive
with a rate of SI . Consequently, given the contract terms of
4p1 l5, the total arrival rates of spot purchasers and contract
customers are S 4p1 l5 = SI + 4p1 l54Cmax − C 4p1 l55 and
C 4p1 l5, respectively. The arrival process is depicted in
Figure 3.
The optimal contract terms, 4p∗ 1 l∗ 5, can be determined
by solving the problem
Cont2

∗
v∗ = max vDyna
4p1 l51
4p1 l5∈ C

(16)

∗
where v∗ = vDyna
4p∗ 1 l∗ 5, C∗ = C 4p∗ 1 l∗ 5, S∗ = S 4p∗ 1 l∗ 5,
C
and the set  , which is analogous to  S in Equation (14), is
defined as follows:

	
C
C
C
 C = 4p1 l5 ∈ <2 2 pmin
¶ p ¶ p̄max
4l51 0 ¶ l ¶ l̄max
4p5 0

4.1. Characterization of the Optimal Policy
We first focus on structural properties of Cont. Our preliminary computational analysis indicates that while 4pi1∗ j1 k 1 li1∗ j1 k 5
is significantly affected by C , the changes in 4p1 l5 do not
impact 4pi1∗ j1 k 1 li1∗ j1 k 5 significantly, for a given C . That is,
4p1 l5 ∈  C 4˜ C 5 result in similar 4pi1∗ j1 k 1 li1∗ j1 k 5 values, where

	
 C 4˜ C 5 = 4p1 l5 ∈  C 2 C 4p1 l5 = ˜ C 1 ˜ C ∈ 601 Cmax 70
In Figure 4, we show  C 4˜ C 5 for ˜ C = 801 0021 0041
0061 0089 in a sample instance. In this case, because C 4p1 l5
is linear in p and l, all 4p1 l5 ∈ ˜ C lie on a line.
We next define åC , the set of all ˜ C such that the optimal
spot purchaser quotes remain the same across all elements
of the set  C 4˜ C 5 and for all 4i1 j1 k5 ∈ S. That is,

åC = ˜ C ∈ 601Cmax 72 4pi1∗ j1 k 4p1 1l1 51li1∗ j1 k 4p1 1l1 55
= 4pi1∗ j1 k 4p2 1l2 51li1∗ j1 k 4p2 1l2 551

C

Assumption 2. (i)  4p1 l5 is continuous, nonincreasing,
twice differentiable, and concave in p and l for 4p1 l5 ∈  C .
(ii) There exists a maximum potential contract customer
arrival rate of Cmax .
C
(iii) There exist nonnegative lower bounds pmin
and 0
such that decreasing p and l below these bounds does not
C
change C 4p1 l5. We have C 4pmin
1 05 = Cmax .
(iv) Given a lead time quote of l, there exists an upper
C
bound on price, denoted by p̄max
4l5, such that any unit
contract price above this upper bound is definitely rejected
C
by a contract customer. p̄max
4l5 is nonincreasing in l, and its
C
C
highest value is denoted as pmax
= p̄max
405.
(v) Given a price quote p, there exists an upper bound
C
on lead time, denoted by l̄max
4p5, such that any lead time
C
above this upper bound is definitely rejected. l̄max
4p5 is
nonincreasing in p, and its highest value is denoted as
C
C
C
lmax
= l̄max
4pmin
5.
Contract terms 4p1 l5 result in a demand rate of C 4p1 l5
from contract customers. Among the remaining Cmax −
C 4p1 l5 contract customers, 4p1 l5 proportion joins the spot
purchaser stream, and 1 − 4p1 l5 proportion is lost, where

	
for all 4p1 1l1 514p2 1l2 5 ∈  C 4˜ C 51 and 4i1j1k5 ∈ S 0
For example, if 004 ∈ åC , then for any given state 4i1 j1 k5
∈ S, 4pi1∗ j1 k 1 li1∗ j1 k 5 remain unchanged for all 4p1 l5 ∈  C 40045
(see Figure 4). This means that the optimal spot purchaser
quotes are the same for all possible contract terms for a given
arrival rate. In Theorem 4, we derive 4p∗ 4˜ C 51 l∗ 4˜ C 55, which
∗
maximizes vDyna
4p1 l5 on  C 4˜ C 5, given that ˜ C ∈ åC , and
use this result to develop an algorithm to calculate the optimal
contract terms. We define C 4·1 ·5, the relative price/lead
time sensitivity for contract customers, similar to S 4·1 ·5
defined in (7), as C 4p1 l5 = 4¡C 4p1 l5/¡l5/4¡C 4p1 l5/¡p5.
Theorem 4. Let ˜ C ∈ åC . Then,
(i) If C 4p1 l5/ C ¾ 1, for all 4p1 l5 ∈  C 4˜ C 5, then
∗ ˜C
l 4 5 = 0
P
C
(ii) If C 4p1 l5/ C ¶ 4i1 j1 k5∈S i1∗ j1 k ḠCi+j+1 4lmax
41 −
˜C /C 55, for all 4p1 l5 ∈  C 4˜ C 5, then p∗ 4˜ C 5 = pC
max
min
(iii) Otherwise,
4p∗ 4˜ C 51 l∗ 4˜ C 55 satisfies C 4p∗ 4˜ C 51
P
l∗ 4˜ C 55/ C = 4i1 j1 k5∈S i1∗ j1 k ḠCi+j+1 4l∗ 4˜ C 55,
where i1∗ j1 k is the limiting probability of state 4i1 j1 k5 under
the optimal dynamic quotation policy.

Hafizoglu, Gel, and Keskinocak: Price and Lead Time Quotation

413

Operations Research 64(2), pp. 406–415, © 2016 INFORMS

Figure 3.

(Color online) Arrival processes of contract customers and spot purchasers.
)NITIAL CONTRACT CUSTOMER STREAM
#MAX

#ONTRACT CUSTOMER STREAM
#

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:53 . For personal use only, all rights reserved.

 P L 	 OFFER

#MAX n #

,OST CUSTOMER STREAM
#
 n 	MAX 
n #	

# n #	
MAX 

)NITIAL SPOT PURCHASER STREAM
3)

Cases (i) (contract customers are highly lead-time sensitive)
and (ii) (contract customers are highly price sensitive) indicate
the two conditions where 4p∗ 4˜ C 51 l∗ 4˜ C 55 is located on the
upper left (e.g., (21,0) on  C 40025) and lower right (e.g., (0,3)
on  C 40025) extreme points of 4˜ C 55 in Figure 4, respectively.
In contrast, Case (iii) helps us find where 4p∗ 4˜ C 51 l∗ 4˜ C 55 is
located in between these two extreme points.
The structure of the optimal contract terms is analogous
to that of the optimal spot purchaser quotes, where the
decisions are dependent on the critical ratio of price/lead
time sensitivity of the customers and the delay penalties.
Recalling that 4pi1∗ j1 k 1 li1∗ j1 k 5 depends on the newsvendor-like
ratio 1 − S 4·1 ·5/ S (see Theorem 2), we observe a similar
structure for 4p∗ 4˜ C 51 l∗ 4˜ C 55 in Case (iii) of Theorem 4. The
P
term 4i1 j1 k5∈S i1∗ j1 k ḠCi+j+1 4l∗ 4˜ C 55 gives the probability
that a C-order is not met on time, given the lead time
quote l∗ 4˜ C 5 (due to Poisson arrivals see time averages).

Figure 4.

(Color online) Illustration of  C 4˜ C 5 for ˜ C =
0, 0.2, 0.4, 0.6, and 0.8 in an example where
C
C
C
C 4p1 l5 = 00841 − 4p − pmin
5/4pmax
− pmin
5−
C
C
S
C
S
l/lmax 5, pmin = pmin = 15, pmax = pmax = 23,
C
S
lmax
= lmax
= 4,  S =  C = 105 and Cmax = 008.
p
C(0)
23

C(0.2)
C(0.4)

21

C(0.6)
C(0.8)

19

17

15

0

1

2

3

4

l

3POT PURCHASER STREAM
3)   #MAX n #	

Thus, 1 − C 4p∗ 4C 51 l∗ 4C 55/ C is the probability that the
C-order is met on time.
4.2. An Efficient Algorithm to Compute Contract
Terms
One straightforward way to solve Cont is to discretize
C
C
C
6pmin
1 pmax
7 and 601 lmax
7 into M equal intervals (e.g., see
Savasaneril et al. 2010). However, this discretization algorithm requires Dyna to be solved at least M 2 /2 times,
resulting in extensive computational times when high precision is required, i.e., when M is high. Hence, we develop
(i) an arrival rate search algorithm that provides reliable
results in less computational time, and (ii) rules to reduce
the action space  C to reduce solution times. We observe
that the arrival rate search algorithm performs better than the
discretization algorithm in more than half of the instances,
and reduces the computational time by 73% on the average.
The details of the computational study are provided in
Section EC.3 of the e-companion.
4.3. Benefits of Offering Optimal Contract Terms
To analyze the potential benefits of offering the optimal
contract terms in comparison to simple contracting strategies,
we consider the following three schemes: (i) the optimal policy (MIX), where 4p1 l5 = 4p∗ 1 l∗ 5 and 4C 1 S 5 = 4C∗ 1 S∗ 5,
(ii) maximal contract customer policy (MCC), where the firm
C
offers 4p1 l5 = 4pmin
1 05 to maximize the contract customer
arrival rate, i.e., 4C 1 S 5 = 4Cmax 1 SI 5, and (iii) maximal
spot purchaser policy (MSP), where the firm offers 4p1 l5 =
C
4pmax
1 05 to maximize the spot purchaser arrival rate, i.e.,
C
C
4 1 S 5 = 401 41 − 4pmax
1 055Cmax + SI 5. We conduct an
extensive computational study (see Section EC.4 of the
e-companion for details) to analyze the benefits of offering
the optimal contract terms. Our main findings are as follows:
• The optimal policy improves profits significantly in
the majority of the cases when the contract customers are
C
C
diverse in their willingness to pay (i.e., pmax
/pmin
∈ 821 39).

Hafizoglu, Gel, and Keskinocak: Price and Lead Time Quotation

414

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:53 . For personal use only, all rights reserved.

• MSP is often optimal when the break-even delay is low
and customers are willing to pay more for spot purchases
C
S
(i.e., pmin
/pmin
< 1). MCC is often optimal when contract
customers are less diverse in their willingness to pay, the
break-even delay is high, and customers are willing to pay
C
S
more for contracts (i.e., pmin
/pmin
> 1).

5. Conclusions and Discussion
We consider the price and lead time quotation problem of a
make-to-order manufacturer or service provider that faces
a mix of contract customers and spot purchasers arriving
dynamically over time. Contract customers quoted a price
and lead time at the beginning of the time horizon. We model
the dynamic price and lead time quotation for spot purchasers
as a Markov decision process over an infinite horizon,
with the objective of maximizing the long-run expected
average profit per unit time. We show that the optimal price
and lead time quotation policy is heavily affected by the
price/lead time sensitivity of the spot purchasers and the
penalty for missed due dates. When spot purchasers are
highly price sensitive, it is optimal to quote a sufficiently
small fixed price while dynamically changing the lead time,
depending on the profitability of the spot purchaser. In
contrast, if spot purchasers are highly lead time sensitive,
offering zero lead times and dynamic pricing is optimal.
Our extensive numerical study indicates that when spot
purchasers are not highly price or lead time sensitive, a
dynamic price and lead time quotation often brings significant
improvements above dynamic lead time quotation (pricing)
and fixed price (fixed lead time) versus a policy where at
least one of these decisions is fixed for lead time (price)
sensitive spot purchasers. Using these results, we develop a
recommendation scheme as to when to offer a joint dynamic
price and lead time policy.
We next focus on the selection of the optimal price and
lead time terms for contract customers to maximize long-run
expected average profits. We first identify cases where the
optimal contracting terms follow a newsvendor-like policy,
where the probability that the customer order is met on
time balances the trade-off between price and lead time
sensitivity of the customer and the delay penalty. We develop
an algorithm (via action space reduction) that is fast and
produces near-optimal results. We finally analyze the benefits
of achieving the optimal mix of contract customers and spot
purchasers and develop managerial insights as to when firms
should strive for the optimal mix versus settling for serving
only one type of customer.
Most of our results depend on the properties of the customer population (e.g., price/lead time sensitivity, willingnessto pay-diversity), which are characterized by the acceptance
probability functions. Hence, accurate estimation of customers’ acceptance probabilities is important. One possible
estimation methodology is statistical curve fitting (such as
linear regression), which is commonly used in demand forecasting literature to analyze the demand-price relationships
(see McGuigan et al. 2011 Ch. 4), utilizing data from earlier
transactions and interviews with customers.

Operations Research 64(2), pp. 406–415, © 2016 INFORMS

There are several avenues for future research. First, if the
customers are strategic, acceptance probability functions
might depend on historical decisions of prices and lead times.
For example, spot purchasers may be more (less) likely
to accept short lead times if their quoted lead times were
(not) met in the past. Similarly, quoting low (high) prices
frequently may decrease (increase) the spot purchasers’
probability of accepting high (low) prices in the future.
Hence, the firm may consider the future impact of the quotes
if the customers are strategic. Second, contract customers
may have information on the historical spot purchaser quotes,
and they may accept or reject the contract term offers by
explicitly considering their expectations on future spot prices
and lead times. Third, there may be multiple customer classes
with different service times and arrival patterns. Fourth,
scheduling decisions may consider the quoted lead times or
simultaneously consider (and dynamically change) along
with price and lead time decisions. Incorporating any of
these additional features would significantly complicate the
model and analysis and is left for future work.
Supplemental Material
Supplemental material to this paper is available at http://dx.doi
.org/10.1287/opre.2016.1481.

Acknowledgments
The authors would like to thank the associate editor and two
anonymous referees whose comments greatly improved the paper.
This research has been supported in part by the National Science
Foundation [Grant DMI-0621012].

References
Afeche P, Ata B (2013) Bayesian dynamic pricing in queueing systems
with unknown delay cost characteristics. Manufacturing Service Oper.
Management 15(2):292–304.
Aktaran-Kalayci T, Ayhan H (2009) Sensitivity of optimal prices to
system parameters in a steady-state service facility. Eur. J. Oper. Res.
193(1):120–128.
Ata B (2006) Dynamic control of a multiclass queue with thin arrival
streams. Oper. Res. 54(5):876–892.
Ata B, Olsen TL (2009) Near-optimal dynamic lead-time quotation and
scheduling under convex-concave customer delay costs. Oper. Res.
57(3):753–768.
Bertsekas DP (2001) Dynamic Programming and Optimal Control, Vol. 2
(Athena Scientific, Belmont, MA) 02178–9998.
Bookbinder JH, Noor AI (1985) Setting job-shop due-dates with service-level
constraints. J. Oper. Res. Soc. 36(11):1017–1026.
Carr S, Lovejoy W (2000) The inverse newsvendor problem: Choosing an
optimal demand portfolio for capacitated resources. Management Sci.
46(7):912–927.
Çelik S, Maglaras C (2008) Dynamic pricing and lead-time quotation for a
multiclass make-to-order queue. Management Sci. 54(6):1132–1146.
Chaharsooghi SK, Honarvar M, Modarres M, Kamalabadi IN (2011)
Developing a two-stage stochastic programming model of the price
and lead-time decision problem in the multi-class make-to-order firm.
Comput. Indust. Engrg. 61(4):1086–1097.
Charnsirisakskul K, Griffin PM, Keskinocak P (2004) Order selection and
scheduling with leadtime flexibility. IIE Trans. 36(7):697–707.
Charnsirisakskul K, Griffin PM, Keskinocak P (2006) Pricing and scheduling
decisions with leadtime flexibility. Eur. J. Oper. Res. 171(1):153–169.
Çil EB, Örmeci EL, Karaesmen F (2009) Effects of system parameters on
the optimal policy structure in a class of queueing control problems.
Queueing Systems 61(4):273–304.

Hafizoglu, Gel, and Keskinocak: Price and Lead Time Quotation

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:53 . For personal use only, all rights reserved.

Operations Research 64(2), pp. 406–415, © 2016 INFORMS

Duenyas I (1995) Single facility due-date setting with multiple customer
classes. Management Sci. 41(4):608–619.
Duenyas I, Hopp WJ (1995) Quoting customer lead times. Management Sci.
41(1):43–57.
Easton FF, Moodie DR (1999) Pricing and lead time decisions for make-toorder firms with contingent orders. Eur. J. Oper. Res. 116(2):305–318.
ElHafsi M (2000) An operational decision model for lead-time and price
quotation in congested manufacturing systems. Eur. J. Oper. Res.
126(2):355–370.
Elmaghraby W, Keskinocak P (2003) Dynamic pricing in the presence of
inventory considerations: Research overview, current practices, and
future directions. Management Sci. 49(10):1287–1309.
Feng JJ, Liu LM, Liu XM (2011) An optimal policy for joint dynamic
price and lead-time quotation. Oper. Res. 59(6):1523–1527.
Feng YY, Pang Z (2010) Dynamic coordination of production planning and
sales admission control in the presence of a spot market. Naval Res.
Logist. 57(4):309–329.
Hafizoglu AB, Gel ES, Keskinocak P (2013) Expected tardiness computations in multiclass priority M/M/C queues. INFORMS J. Comput.
25(2):364–376.
Haksöz C, Kadam A (2009) Supply portfolio risk. J. Oper. Risk 4(1):59–77.
Hua GW, Wang SY, Cheng TCE (2010) Price and lead time decisions in
dual-channel supply chains. Eur. J. Oper. Res. 205(1):113–126.
Kapuscinski R, Tayur S (2007) Reliable due-date setting in a capacitated
MTO system with two customer classes. Oper. Res. 55(1):56–74.
Keskinocak P, Tayur S (2004) Due date management policies. Handbook of
Quantitative Supply Chain Analysis: Modeling in the eBusiness Era
(Kluwer Academic Publisher, New York), 485–554.
Keskinocak P, Ravi R, Tayur S (2001) Scheduling and reliable lead-time
quotation for orders with availability intervals and lead-time sensitive
revenues. Management Sci. 47(2):264–279.
Liu L, Parlar M, Zhu SX (2007) Pricing and lead time decisions in
decentralized supply chains. Management Sci. 53(5):713–725.
McGuigan JR, Moyer RC, Harris FH (2011) Managerial Economics:
Applications, Strategy and Tactics (Cengage Learning, Stamford, CT).
Palaka K, Erlebacher S, Kropp DH (1998) Lead-time setting, capacity
utilization, and pricing decisions under lead-time dependent demand.
IIE Trans. 30(2):151–163.
Pekgun P, Griffin PM, Keskinocak P (2008) Coordination of marketing and
production for price and leadtime decisions. IIE Trans. 40(1):12–30.
Plambeck EL (2004) Optimal leadtime differentiation via diffusion approximations. Oper. Res. 52(2):213–228.
Ray S, Jewkes EM (2004) Customer lead time management when both
demand and price are lead time sensitive. Eur. J. Oper. Res. 153(3):
769–781.
Samsung (2012) General terms and conditions of purchase Samsung
C&T Deutschland GmbH. Accessed September 29, 2013, http://
www.samsungdeutschland.de.
Savasaneril S, Griffin PM, Keskinocak P (2010) Dynamic lead-time quotation
for an M/M/1 base-stock inventory queue. Oper. Res. 58(2):383–395.

415
Spearman ML, Zhang RQ (1999) Optimal lead time policies. Management
Sci. 45(2):290–295.
Stundza T (2007) Metals buyers prefer contracts, indexes more than
ever (forecasts and trends of metal industries). Purchasing (July).
Accessed September 29, 2013, http://www.highbeam.com/doc/1G1166615581.html.
Wein LM (1991) Due-date setting and priority sequencing in a multiclass
M/G/1 queue. Management Sci. 37(7):834–850.
Wencor.com (2012) Wencor contracts. Accessed September 29, 2013, http://
www.wencor.com/products/contracts.html.
Wu ZP, Kazaz B, Webster S, Yang KK (2012) Ordering, pricing, and leadtime quotation under lead-time and demand uncertainty. Production
Oper. Management 21(3):576–589.
Xiao TJ, Jin JA, Chen GH, Shi J, Xie MQ (2010) Ordering, wholesale
pricing and lead-time decisions in a three-stage supply chain under
demand uncertainty. Comput. Indust. Engrg. 59(4):840–852.
Yoon S, Lewis ME (2004) Optimal pricing and admission control in a
queueing system with periodically varying parameters. Queueing
Systems 47(3):177–199.
Zhao XY, Stecke KE, Prasad A (2012) Lead time and price quotation mode
selection: Uniform or differentiated? Production Oper. Management
21(1):177–193.

Esma S. Gel is an associate professor of Industrial Engineering
at the School of Computing, Informatics and Decision Systems
Engineering at Arizona State University. Her research focuses
on the design, control, and management of operations in various
settings, with emphasis on manufacturing and supply chain systems,
business and logistics processes, and health care systems. Her work
has been funded by the National Science Foundation, as well as
several industrial partners such as Intel and the Mayo Clinic. She
is an active member of INFORMS, IIE, and ASEE.
A. Baykal Hafizoğlu is a supply chain optimization-operations
research professional working for LogicBlox-Predictix as an optimization scientist. He specializes in modeling and solution of
complex supply chain and pricing problems. He received his
Ph.D. in Industrial Engineering from Arizona State University.
His research interests include supply chain management, revenue
management, and health care systems.
Pinar Keskinocak is the William W. George Chair and Professor
and cofounder and codirector of the Center for Health and Humanitarian Systems in the Stewart School of Industrial Engineering
at Georgia Tech. Previously, she worked at the IBM T.J. Watson
Research Center. Her research focuses on the applications of
operations research and management science with societal impact,
particularly health and humanitarian applications, supply chain
management, and logistics/transportation.

Proceedings of the 2012 Winter Simulation Conference
C. Laroque, J. Himmelspach, R. Pasupathy, O. Rose, and A. M. Uhrmacher, eds.

COMPARISON OF AMBULANCE DIVERSION POLICIES VIA SIMULATION
Adrian Ramirez-Nafarrate
Instituto Tecnologico Autonomo de Mexico
Rio Hondo 1
Mexico City, 01080, MEXICO

A. Baykal Hafizoglu
Esma S. Gel
John W. Fowler
Arizona State University
699 S. Mill Avenue
Tempe, AZ 85281, USA

ABSTRACT
Ambulance diversion (AD) is often used by emergency departments (EDs) to relieve congestion. When an
ED is on diversion status, the ED requests ambulances to bypass the facility; therefore ambulance patients
are transported to another ED. This paper studies the effect of AD policies on the average waiting time
of patients. The AD policies analyzed include (i) a policy that initiates diversion when all the beds are
occupied; (ii) a policy obtained by using a Markov Decision Process (MDP) formulation, and (iii) a policy
that does not allow diverting at all. The analysis is based on an ED that comprises two treatment areas.
The diverted patients are assumed to be transported to a neighboring ED, whose average waiting time is
known. The results show significant improvement in the average waiting time spent by patients in the ED
with the policy obtained by MDP formulation. In addition, other heuristics are identified to work well
compared with not diverting at all.
1

INTRODUCTION

Overcrowding of Emergency Departments (EDs) is a problem present in many countries around the world.
This problem has been highlighted in the United States (US) in the last decade and is very likely to continue
since the arrivals of patients has an increasing trend in recent years (Associated Press 2006; Centers for
Disease Control and Prevention 2010).
There are several problems caused by overcrowding of EDs, one of the most important is the long
waiting time before a patient is treated. Several papers have studied this problem using queuing formulations
and simulation models to re-design patient flow, identify bottlenecks and increase the throughput of EDs
(McConnell et al. 2005; Cochran and Roche 2009). Another alternative to relieve congestion is diverting
ambulances to less crowded facilities (United States General Accounting Office 2003; United States General
Accounting Office 2009). This paper compares different ambulance diversion (AD) policies using simulation.
AD policies refer to the rules or guidelines that must be met in order to trigger the diversion status.
While on diversion, an ED requests that all ambulances bypass the facility and transport the patients to
another ED. Practitioners consider AD as an inefficient and risky method; therefore, they recommend to
avoid AD. Papers from the medical perspective have the objective of designing AD policies that minimize
diversion (Vilke et al. 2004; Asamoah et al. 2008; Patel et al. 2006). On the other hand, prohibiting
AD might lead to increase in waiting time and stressing the operations of EDs (Massachusetts Nurses
Association 2009).
Few papers have analyzed the impact of AD policies using analytical methods. For example, papers
by Deo and Gurvich (2011) and Ramirez et al. (2011) analyzed the impact of AD policies on a network of
EDs, the former using game theory and queuing formulations, and the later using discrete-event simulation.
In addition, Hagtvedt et al. (2009) also studied the diversion decisions using agent-based simulation. These
papers suggest the existence of an agent that coordinates AD decisions on a network of EDs. Furthermore,

978-1-4673-4781-5/12/$31.00 ©2012 IEEE

Ramirez-Nafarrate, Hafizoglu, Gel, and Fowler
these papers analyzed AD heuristics with a threshold structure on the ED occupancy, but they do not
explore the optimality of the policies.
In this paper, we propose an AD policy obtained by a Markov Decision Process (MDP) formulation,
and compare the average waiting time of patients using that policy with that of simple policies that are
likely to be used in practice. The remaining sections of the paper are organized as follows: Section 2
describes the simulation model used in the experimentation, Section 3 introduces the AD policies used in
the experimentation and presents the formulation of the MDP model, Section 4 presents the results and
the comparison of the effectiveness of different AD policies and finally Section 5 presents the concluding
remarks.
2

SIMULATION MODEL OF AN ED

2.1 Overview of the Patient Flow
Figure 1 shows an overview of the patient flow in the simulation model. The model consists of two arrival
streams: ambulance arrivals and walk-in arrivals. We assume non-homogeneous Poisson process for the
arrivals given that the total arrivals to the ED exhibit a pattern seen in several places around the US (Centers
for Disease Control and Prevention 2008; Green 2006; Cochran and Roche 2009).

Figure 1: Overview of the patient flow in the simulation model.
Arriving patients are classified in one out of two severity levels: most critical patients belong to level
1 and less emergent patients belong to level 2. There are two treatment areas: A1 and A2 where patients
with level 1 and level 2 are treated, respectively. This structure is similar to layouts of many EDs across the
US, where a fast-track area is included (Cochran and Roche 2009). Furthermore, severity level 1 represents
patients with immediate and emergent need of care, and severity level 2 includes patients with urgent and
semi/non urgent needs. We assume that ambulance patients are classified as level 1 with probability pA1
and level 2 with probability 1 − pA1 . Similarly, walk-in patients are classified as level 1 with probability

Ramirez-Nafarrate, Hafizoglu, Gel, and Fowler
A
W
W
pW
1 and level 2 with probability 1 − p1 . We refer to the tuple (p1 , p1 ) as the patient mix. Patients are
served according to the FCFS discipline in each treatment area. The number of beds where patients receive
treatment is c1 and c2 for areas A1 and A2, respectively. The only resource modeled in the simulation is
the bed. The doctors, nurses and lab equipment are discarded from the model because the CDC found that
other resources have a low impact on the diversion decisions (Centers for Disease Control and Prevention
2006a).
Once a bed is assigned to a patient, the length of stay is divided in treatment time and boarding time.
The treatment time in an ED bed is assumed to have a lognormal distribution, which is identified as a
reasonable assumption to model treatment times (Hoot et al. 2008). After ending treatment, the patient
with severity level i might require to be admitted to an inpatient unit of the hospital with probability qi . If
the patient requires admission, then the patient remains in the ED bed for an additional amount of time,
which is referred to as boarding time. While a patient is boarding in the ED, he/she blocks access to that
bed in the ED to other patients.
The AD policies require to observe if the conditions to divert a patients are met given the current state
of the system upon an ambulance arrival. If the diversion condition is satisfied, then the ambulance patient
is assumed to be transported to a neighboring ED, where the waiting time is a random variable X. Similar to
the arrival pattern observed for the ED, it is reasonable to assume that if a patient is diverted, the expected
waiting time in a neighboring ED also changes during a day. Therefore, we change the parameters of the
distribution of X throughout the day.
The simulation model is built in Arena (Kelton et al. 2010). Pilot runs are used to determine a warm-up
period of two months and the replication length after warm-up is set to one year. Thirty replications for each
policy and scenario are executed, obtaining an average relative precision of 2.82% using 95% confidence
intervals on the average patient waiting time. In addition, common random numbers are applied in order
to reduce the noise when comparing the policies (Banks et al. 2010).

2.2 Input Data for the Simulation Model
In order to build a realistic model, we use information from published papers and reports from US agencies
to obtain the input data. The input requirements are set as follows:
•

•

•

•

A
Severity mix (pA1 , pW
1 ): It is reasonable to assume that p1 is a value close to 1. For this
A
W
experimentation, we set p1 = 0.9 and p1 ∈ {0.3, 0.5}. If the severity mix is (0.9, 0.3), then the
area A2 is congested, meaning that utilization of beds in A2 is higher than in A1. If the severity
mix is (0.9, 0.5), then the congested area is A1.
Number of beds: We assume that c1 = 15 beds and c2 = 5 beds. These values are close to the
average number of standard treatment spaces and other treatment spaces found by the CDC in the
US (Centers for Disease Control and Prevention 2006b).
Arrival rates: Ambulance and walk-in arrival rates are assumed to follow non-homogeneous
Poisson processes. Green (2006) finds reasonable to use a Poisson process to model arrivals to EDs.
Cochran and Roche (2009) presents multiplicative indices for the seasonality of arrivals to an ED
throughout the day. In order to mimic that pattern, we first find λ W such that the utilization due to
walk-in arrivals is 90% for the peak hour between 7pm and 8pm. Then, we scale the arrivals across
the day using the multiplicative indices. We find λ A assuming that ambulance arrivals represent
15% of all the arrivals to the ED. This percentage is close to the average found by Centers for
Disease Control and Prevention (2010). The resulting arrival pattern can be observed in Figure 2
for a severity mix of (0.9, 0.3).
Treatment time: We assume that the treatment time in the ED is lognormally distributed with
mean of 240 minutes and 60 minutes for patients with levels 1 and 2, respectively. The standard
deviation of the distribution was adjusted to obtain the coefficients of variations found in Cochran
and Roche (2009), which are 0.72 and 0.102 for treatments in A1 and A2, respectively.

Ramirez-Nafarrate, Hafizoglu, Gel, and Fowler
•

•

Boarding time: We assume that q1 = 0.24 and q2 = 0.045, which imply that 12.1% of all the
ED patients require admission to an inpatient unit. This percentage is the same found by Singer
et al. (2011) and very similar to the findings in United States General Accounting Office (2003).
Furthermore, based on Singer et al. (2011) we assume that boarding time is uniformly distributed in
between [0,2], [2,6], [6,12] and [12,24] hours with probabilities 0.5022, 0.3705, 0.0763 and 0.051,
respectively. This scheme produces an average waiting time of 3.58 hours, which is very similar
to the averages found in other references (United States General Accounting Office 2003).
Waiting time in the neighboring hospital, X: We define three settings for the distribution of X.
In all the settings, X follows a triangular distribution with parameters as shown in Table 1. In
addition, the values of the parameters also depend on the traffic in the ED under study. The traffic
is classified as low, medium and high as also shown in Figure 2. The purpose of this scheme is
to have a positive correlation between the traffic intensity in the ED under study and the expected
waiting time in the neighboring ED. Given the behavior of arrivals observed in Figure 2, it is very
likely that the traffic intensity follows the same pattern for neighboring EDs.

Figure 2: Mean arrival rates to the ED for a severity mix of (0.9, 0.3).
Table 1: Settings of X used in the simulation model.
Traffic in main ED
Low
Medium
High

Parameters of
Setting 1
(5, 15, 25)
(10, 30, 50)
(15, 45, 75)

Triangular Distribution (mins)
Setting 2
Setting 3
(5, 15, 25)
(10, 30, 50)
(15, 45, 75)
(25, 75, 125)
(25, 75, 125) (40, 120, 200)

Ramirez-Nafarrate, Hafizoglu, Gel, and Fowler
3

AD POLICIES EVALUATED

The simulation model described in Section 2 is used to compare several AD policies, including one obtained
by using a Markov Decision Process model and other simple policies found in the literature, some of them
applied in practice. The complete list of AD policies is the following:
1. MDP Policy (MDP). AD policy obtained by an MDP model over a simplified version of the ED
presented in Section 2.
2. Full Beds in A1 (FB A1). This policy diverts patients to a neighboring hospital when all the beds
in A1 are occupied.
3. 14 Beds in A1 (14 A1). This policy diverts patients when there are 14 beds occupied in A1, which
implies that there is only 1 bed available in A1.
4. 13 Beds in A1 (13 A1). This policy diverts patients when there are 13 beds occupied in A1, which
implies that there are 2 beds available in A1.
5. 12 Beds in A1 (12 A1). This policy diverts patients when there are 12 beds occupied in A1, which
implies that there are 3 beds available in A1.
6. Full Beds in A1 or in A2 (FB A1/A2). This policy diverts patients when all the beds in A1 or in
A2 are occupied.
7. Full Beds (FB). This policy diverts patients when all the beds in A1 and in A2 are occupied.
8. Myopic policy (Myopic): This policy diverts an arriving ambulance only if the expected waiting time
for the current ambulance patient at the neighboring hospital is smaller than the expected waiting
time if he/she is accepted. Thus, under the myopic policy, the ambulance is diverted only when
pA1 W1D + (1 − pA1 )W2D ≤ pA1 W1 (n1 ) + (1 − pA1 )W2 (n2 ). Note that this heuristic evaluates W1 (n1 ) and
W2 (n2 ) under the assumption that length of stay of a patient in the ED is exponentially distributed.
9. No AD Policy (No AD). This policy never diverts patients.
3.1 MDP Model Formulation
A simplified model of the ED presented in Section 2 is used to obtain an AD policy via MDP. For analytical
tractability, the model assumes two arrival streams following homogeneous Poisson processes: ambulance
arrivals with rate λ A and walk-in arrivals with rate λ W . Similar to the simulation model, ambulance patients
are classified as level 1 with probability pA1 and level 2 with probability 1 − pA1 . Walk-in patients are
W
classified as level 1 with probability pW
1 and level 2 with probability 1 − p1 . Patients with level 1 receive
treatment in A1 where there are c1 beds, and patients with level 2 receive treatment in A2 where there are
c2 beds. After waiting for an ED bed in the corresponding area, the patient stays in the ED for an amount
of time referred to as length of stay, which is the sum of treatment time and boarding time. For tractability
purposes of the MDP model, we assume that the length of stay in the ED is exponentially distributed with
rates µ1 and µ2 for areas A1 and A2, respectively. After the stay in the ED, the patients are discharged.
The state of the system is represented by the total number of patients in A1 and A2, denoted as the
tuple (n1 , n2 ). Hence, the state space is given by S = {(n1 , n2 ) : n1 ≥ 0, n2 ≥ 0}. The objective of the MDP
model is to obtain a state-dependent policy to divert patients to a neighboring hospital that minimizes the
long-run average expected waiting time over an infinite horizon.
Let Wi (ni ), for i ∈ {1, 2}, denote the expected waiting time of an arriving patient with level i given
that there are ni patients in Ai upon his arrival. Considering that the average length of stay in Ai is 1/ci µi ,
we have that
(
ni −ci +1
ci µi , if ni ≥ ci ,
(1)
Wi (ni ) =
0,
if ni < ci .
The MDP model also assumes that if an ambulance is diverted, the patient is transported to a neighboring
hospital to receive appropriate treatment. The waiting time in the neighboring hospital is modeled as a

Ramirez-Nafarrate, Hafizoglu, Gel, and Fowler
random variable X with a probability density function of f (x). Let WiD be the expected waiting time of a
diverted patient with level i for i ∈ {1, 2}, we have that
WiD =

Z ∞

x f (x)dx,

(2)

0

The continuous-time MDP model is converted to an equivalent discrete time model using uniformization
with rate ν = λ A + λ W + c1 µ1 + c2 µ2 . Let v∗ be the optimal long-run average expected waiting time and
h∗ (n1 , n2 ) denote the optimal relative effect of starting in state (n1 , n2 ). The Bellman equation is given as
v∗

λ W pW
λ W (1 − pW
λW + λ A
1
1 )
+ h∗ (n1 , n2 ) =
[W1 (n1 ) + h∗ (n1 + 1, n2 )] +
[W2 (n2 ) + h∗ (n1 , n2 + 1)]
ν
ν
ν
ce1 µ1 ∗
ce2 µ2 ∗
+
h (n1 − 1, n2 ) +
h (n1 , n2 − 1)
ν
ν
n λ A pA
λ A (1 − pA1 ) D
1
+ min
[W1D + h∗ (n1 , n2 )] +
[W2 + h∗ (n1 , n2 )],
ν
ν
o
λ A pA1
λ A (1 − pA1 )
[W1 (n1 ) + h∗ (n1 + 1, n2 )] +
[W2 (n2 ) + h∗ (n1 , n2 + 1)]
ν
ν


W
A
λ + λ + ce1 µ1 + ce2 µ2 ∗
h (n1 , n2 ),
(3)
+ 1−
ν

where

(
ni if ni ≤ ci ,
cei =
ci , if ni > ci ,

for i ∈ {1, 2}.
The first two terms on the right hand side of Equation 3 refer to walk-in arrivals of patients with
severity level 1 and 2, respectively. The third and fourth term refer to discharge of patients from A1
and A2, respectively. The terms inside the minimum expression refers to the potential actions upon an
arriving ambulance. The first term inside the minimum statement represents the average waiting time if
an ambulance arriving patient is diverted to a neighboring hospital; while the second term represents the
average waiting time if the patient is accepted to the ED. The last term is a self loop due to uniformization.
A working paper presents theoretical properties of the optimal policy (Ramirez-Nafarrate et al. 2012).
The optimal diversion policy is characterized by a monotonic threshold curve as illustrated in Figure 3.
Above the threshold curve, the optimal action is to divert the patients; below the threshold curve, the
optimal action is to accept the patients.
The AD policy based on the MDP model is obtained using the relative value iteration algorithm coded
in C++ (Puterman 2005). The input parameters for the MDP model are obtained from the assumptions
made in Section 2 in the following way:
•
•

•

The arrival rates λ W and λ A are computed as the average arrival rates across the day.
The length of stay assumes that the component regarding treatment time has rates of 0.25 and 1, for
A1 and A2, respectively, as assumed in the simulation model. In addition, as assumed also in the
simulation model, we considered that the average boarding time is 3.58 hours and the probabilities
of admissions are 0.24 and 0.045 for patients with level 1 and level 2, respectively. Hence, the
length of stay is assumed to be exponentially distributed with rates of 0.20 and 0.86 in A1 and A2,
respectively.
Regarding the waiting time in the neighboring ED, X is assumed to have a triangular distribution with
parameters resulting from the average of the values of parameters across the day. This assumption
resulted from a pilot study evaluating different options.

Ramirez-Nafarrate, Hafizoglu, Gel, and Fowler

Figure 3: Illustration of the optimal diversion policy.
4

RESULTS

The results obtained during the experimentation show that not diverting at all, which is the No AD policy,
produces average waiting time per patient of 1048.27 ± 215.11 minutes when the congested area is A1; and
28.85 ± 0.64 minutes when the congested area is A2. Note that the model does not include features such
as patients leaving without being seen or time-dependent service rates, which may yield smaller average
waiting times. Figure 4 presents the confidence intervals for settings 1 and 3 of X, given in Table 1. The
results for setting 2 are not shown in this table because they fall somewhere between the results from
settings 1 and 3.
The results presented in Figure 4 show that the policy obtained by the MDP model is, at least, as good
as other heuristics and in some cases it is significantly better. Since the MDP policy is obtained through a
model that is a simplified version of the actual simulation mode, it does not guarantee to be the optimal
policy. However, the performance of the MDP is superior to other heuristics. On the other hand, most
of the heuristics perform significantly better than No AD, contradicting recommendations of the medical
community. However, the benefits of ambulance diversion is subject to several conditions, including health
conditions of ambulance patients, distance and traffic conditions for traveling to another ED.
The AD policies based on available capacity in A1 (i.e. FB A1, 14 A1, 13 A1, and 12 A1) also
perform reasonable well compared with the MDP policy. This is due to the fact that the threshold policy,
as illustrated in Figure 3, usually recommends saving few beds in A1 before going on diversion, especially
if the congested area is A1 and the expected waiting time in the neighboring hospital is relatively small.
As the expected waiting time in the neighboring hospital increases and congestion is observed in A2, the
threshold policy recommends to observe full occupancy in A1 before diverting. In addition, the threshold
policy usually allows some queueing in A2. This aspect also explains why the policy FB A1/A2 performs
better if the congested area is A1 than if it is A2. If the congested area is A2, the policy FB A1/A2 diverts
patients unnecessarily, increasing the average waiting time per patient. In a similar way, the policy FB
performs much worse than the other policies if congested area is A1, because this policy delays going on
diversion significantly. The Myopic policy performs significantly worse than the MDP, which implies that
a good AD policy must observe the impact of the current decision on future patients.
Another important result to highlight is the magnitude of the improvement on the average waiting time
using AD, depending on which the congested area is. Table 2 shows the relative performance of the AD
policies compared with the performance obtained by the policy obtained through the MDP formulation.
It it is clear how AD improves the performance regarding the average waiting time per patient when the
congested area is A1. This is due to the fact that most ambulance patients have severity level 1; therefore,
AD is more effective handling congestion when A1 has a high utilization. In addition, AD policies that
perform as good as the suggested by the MDP model can be identified when congested area is A2.

Ramirez-Nafarrate, Hafizoglu, Gel, and Fowler

(a) Setting 1, Congested area: A1

(b) Setting 1, Congested area: A2

(c) Setting 3, Congested area: A1

(d) Setting 3, Congested area: A2

Figure 4: 95% confidence intervals on ETP.
A factor that is critical for obtaining an effective policy via MDP is to identify which the congested
area is. The congested area depends not only on the arrival rate, but also on the severity mix. A wrong
analysis of the congestion in the ED may lead to a highly ineffective policy. For instance, observe the
relative performance of the FB policy. This policy performs almost as good as the MDP policy when
congested area is A2, but if congested area is A1, then the performance of the policy is very poor.
In summary, the results show that the AD policy obtained using the MDP model performs better than
other policies in many situations. However, given that the objective function in the MDP model does not
penalize diverting ambulances, the MDP policy might require to spend a large fraction of time on diversion
in order to minimize the average waiting time per patient. The fraction of time spent on diversion status
is an important performance measure used by EDs. Table 3 shows the average fraction of time spent on
diversion for the MDP policy.
Note that the average fraction of time spent on diversion does not depend on the setting for all the
policies but for the MDP. This happens because the values of the parameters of the distribution of X affect
the optimal policy; whereas for the other policies, the settings do not affect the time spent on diversion.

Ramirez-Nafarrate, Hafizoglu, Gel, and Fowler
Table 2: Relative performance of AD policies compared with MDP policy (%). Bold numbers show
statistical significance using 95% confidence level.
Policy
FB A1
14 A1
13 A1
12 A1
FB A1/A2
FB
Myopic
No AD

Setting 1
A1
A2
65.99
4.18
49.01
5.37
39.78
2.32
27.65
1.08
54.24
2.35
551.21
8.56
67.64
6.03
4608.41 15.00

Setting 2
A1
A2
45.41
-1.36
31.86
0.17
24.87
-2.13
15.55
-2.51
36.69
3.98
446.12
2.55
69.18
2.13
3827.98 7.86

Setting 3
A1
A2
23.36
-1.50
13.40
0.63
8.77
-0.79
2.41
-0.01
17.76
14.57
334.42
2.05
67.80
5.19
2999.75
6.17

It can be observed that the MDP policy may spend a high fraction of time on diversion, especially if the
congested area is A1. However, as the waiting time in the neighboring hospital decreases, the percentage
of time spent on diversion also decreases. Furthermore, the policies prescribed by the MDP seem to be on
diversion less time than the other policies when the area A2 is congested. When the area A1 is congested,
the MDP policy spends more time on diversion for low and moderate values of X.
Table 3: Average fraction of time spent on diversion.
Policy
MDP
FB A1
14 A1
13 A1
12 A1
FB A1/A2
FB

Setting 1
A1
A2
0.9863 0.1590
0.5915 0.1351
0.6286 0.1790
0.6727 0.2296
0.7132 0.2941
0.3227 0.2353
0.7732 0.2480

Setting 2
A1
A2
0.7824 0.0944
0.5915 0.1351
0.6286 0.1790
0.6727 0.2296
0.7132 0.2941
0.3227 0.2353
0.7732 0.2480

Setting 3
A1
A2
0.6032 0.0477
0.5915 0.1351
0.6286 0.1790
0.6727 0.2296
0.7132 0.2941
0.3227 0.2353
0.7732 0.2480

In order to reduce the fraction of time on diversion and at the same time reduce the average waiting time
per patient, EDs must address the problem of capacity. The lack of capacity might be an issue upstream
(in the ED) and/or downstream (in the inpatient units). In any case, a strategic plan is recommended in
order to design a system (the whole hospital) that is capable of satisfying the demand in a reasonable time.
In order to observe the impact of capacity, Figure 5 shows the average fraction of time spent on diversion
and the average waiting time per patient varying the number of beds in A1 (c1 ). This analysis is conducted
for Setting 2 of X according to Table 1 when congested area is A1.
Results presented in Figure 5 show that a small change in the capacity of the ED for our model makes
a significant impact in both performance measures. Therefore, for a given desired performance, an analyst
could obtain the appropriate capacity given a limited budget. In addition, the analysis should be extended
to inpatient units to reduce the impact of boarding patients in the congestion of the ED.
The policy obtained by the MDP model is characterized by a single threshold that recommends when
to accept or divert patients. The single threshold may cause that the ED changes the diversion status very
frequently. However, in practice, when EDs go on diversion they stay on that status for some amount of
time. In order to ensure that the ED does not go on and off diversion often, the Bellman’s equation shown
in 3 should include a penalization for changing the diversion status.

Ramirez-Nafarrate, Hafizoglu, Gel, and Fowler

Figure 5: Average fraction of time on diversion vs average waiting time per patient for different number
of beds in A1 and considering the MDP prescribed policy.
5

CONCLUSIONS

We compare the performance regarding average waiting time per patient of several AD policies and a
simulation model with realistic input data is used to mimic the patterns and behaviors of several EDs around
the US. The AD policies included in the study comprise several simple heuristics and a policy obtained
by a Markov Decision Process model of a simplified version of the ED.
The results show that the policy prescribed by the MDP model performs significantly better than most
of the heuristics. In particular, the MDP policy performs better when the emergent care area of the ED
is more congested than the fast-track care area. Furthermore, most of the AD policies show a significant
improvement on the average waiting time than not diverting at all. However, the results show than an
intelligent design of the AD policies may lead to effective results. In order to identify the appropriate AD
policies, it is required to identify which the congested area of the ED is. In addition, the model assumes that
the waiting time in the neighboring hospital is known. Therefore, information sharing amongst hospitals
that serve a common area is needed to ensure that the AD policies work effectively.
Nevertheless, the MDP formulation proposed in this paper does not penalize diverting ambulances,
which causes that a large fraction of time might be spent on diversion. In order to improve the timeliness
and accessibility of the emergency care system, measured with the average waiting time per patient and
fraction of time on diversion, respectively, decision makers must address the issue of capacity. The analysis
of capacity should include not only the number of resources required in the ED, but also in the inpatient
units.
Future research about this topic includes redesigning the MDP formulation to include a penalization
for being on diversion. In addition, other MDP objective functions will be explored to obtain an MDP
policy that does not allow going on and off of diversion frequently, as it may happen with a single AD
threshold. Other models that include the dynamics of neighboring hospitals will also be explored.
REFERENCES
Asamoah, O. K., S. J. Weiss, A. A. Ernst, M. Richards, and D. P. Sklar. 2008. “A novel diversion protocol
dramatically reduces diversion hours”. American Journal of Emergency Medicine 26 (6): 670–675.
Associated Press 2006. “Report:
ER care in U.S. at ’breaking point”’. Available at:
“http://www.msnbc.msn.com/id/13320317/ns/health-health care/t/report-er-care-us-breaking-point/”.

Ramirez-Nafarrate, Hafizoglu, Gel, and Fowler
Banks, J., J. Carson II, B. Nelson, and D. Nicol. 2010. Discrete-Event System Simulation. Upper Saddle
River, NJ.: Pearson Education, Inc.
Centers for Disease Control and Prevention 2006a. “Advanced Data from Vital and Health Statistics Saffing,
Capacity, and Ambulance Diversion in Emergency Departments: United States, 2003-04”. Available
at: “http://www.cdc.gov/nchs/data/ad/ad376.pdf”.
Centers for Disease Control and Prevention 2006b. “National Hospital Ambulatory Medical Care Survey:
2004 Emergency Department Summary”. Available at: “http://www.cdc.gov/nchs/data/ad/ad372.pdf”.
Centers for Disease Control and Prevention 2008. “National Hospital Ambulatory Medical Care Survey: 2006
Emergency Department Summary”. Available at: “http://www.cdc.gov/nchs/data/nhsr/nhsr007.pdf”.
Centers for Disease Control and Prevention 2010. “National Hospital Ambulatory Medical Care Survey: 2007
Emergency Department Summary”. Available at: “http://www.cdc.gov/nchs/data/nhsr/nhsr026.pdf”.
Cochran, J. K., and K. T. Roche. 2009. “A multi-class queuing network analysis methodology for improving
hospital emergency department performance”. Computers & Operations Research 36 (5): 1497–1512.
Deo, S., and I. Gurvich. 2011. “Centralized vs. Decentralized Ambulance Diversion: A Network Perspective”.
Management Science 57 (7): 1300–1319.
Green, L. 2006. “Queueing analysis in healthcare”. Patient Flow: Reducing delay in healthcare delivery:281–
307.
Hagtvedt, R., P. Griffin, P. Keskinocak, M. Ferguson, and F. Jones. 2009, December. “Cooperative strategies
to reduce ambulance diversion”. In Proceedings of the 2009 Winter Simulation Conference, edited by
M. D. Rossetti, R. R. Hill, B. Johansson, A. Dunkin, and R. G. Ingalls, 1861–1874. Piscataway, New
Jersey: Institute of Electrical and Electronics Engineers, Inc.
Hoot, N., L. LeBlanc, I. Jones, S. Levin, C. Zhou, C. Gadd, and D. Aronsky. 2008. “Forecasting emergency
department crowding: A discrete event simulation”. Annals of Emergency Medicine 52 (2): 116–125.
Kelton, W., R. Sadowski, and N. Swets. 2010. Simulation with Arena. Columbus, OH.: McGraw-Hill.
Massachusetts Nurses Association 2009. “State’s no diversion policy is putting strain on Massachusetts
hospitals”.
McConnell, K. J., C. F. Richards, M. Daya, S. L. Bernell, C. C. Weathers, and R. A. Lowe. 2005. “Effect
of increased ICU capacity on emergency department length of stay and ambulance diversion”. Annals
of Emergency Medicine 45 (5): 471–478.
Patel, P. B., R. W. Derlet, D. R. Vinson, M. Williams, and J. Wills. 2006. “Ambulance diversion reduction:
the Sacramento solution”. American Journal of Emergency Medicine 24 (2): 206–213.
Puterman, M. 2005. Markov Decision Processes Discrete Stochastic Dynamic Programming. Hoboken, NJ:
Wiley.
Ramirez, A., J. Fowler, and T. Wu. 2011, December. “Design of centralized ambulance diversion policies
using simulation-optimization”. In Proceedings of the 2011 Winter Simulation Conference, edited by
S. Jain, R. R. Creasey, J. Himmelspach, K. P. White, and M. Fu, 1251–1262. Piscataway, New Jersey:
Institute of Electrical and Electronics Engineers, Inc.
Ramirez-Nafarrate, A., A. B. Hafizoglu, E. S. Gel, and J. W. Fowler. 2012. “Optimal Ambulance Diversion
Control Policies”. Working Paper.
Singer, A. J., H. C. Thode, P. Viccellio, and J. M. Pines. 2011. “The Association Between Length of
Emergency Department Boarding and Mortality”. Academic Emergency Medicine 18 (12): 1324–1329.
United States General Accounting Office 2003, March. “Hospital Emergency Departments:
Crowded Conditions Vary among Hospitals and Communities”. Available at:
“http://www.gao.gov/new.items/d03460.pdf”.
United States General Accounting Office 2009, April. “Hospital Emergency Departments: Crowding
Continues to Occur, and Some Patients Wait Longer than Recommended Time Frames”. Available at:
“http://www.gao.gov/products/GAO-09-347”.

Ramirez-Nafarrate, Hafizoglu, Gel, and Fowler
Vilke, G. M., E. M. Castillo, M. A. Metz, L. U. Ray, P. A. Murrin, R. Lev, and T. C. Chan. 2004.
“Community trial to decrease ambulance diversion hours: The San Diego County Patient Destination
Trial”. Annals of Emergency Medicine 44 (4): 295–303.
AUTHOR BIOGRAPHIES
ADRIAN RAMIREZ-NAFARRATE is a Professor in the Department of Industrial & Operations Engineering at Instituto Tecnologico Autonomo de Mexico. His research interests include modeling, simulation
and analysis of healthcare delivery systems. He received a PhD degree in Industrial Engineering from
Arizona State University, an MS in Manufacturing Systems at ITESM and a BS in Industrial Engineering
at Universidad de Sonora. His email address is adrian.ramirez@itam.mx.
A. BAYKAL HAFIZOGLU earned his Ph.D. from the Department of Industrial Engineering at Arizona
State University. His research focuses on stochastic modeling and optimal control of manufacturing and
service systems. He earned his B.S. and M.S. degrees from Department of Industrial Engineering at Middle
East Technical University in 2005 and 2007, respectively. His email address is baykal@asu.edu.
ESMA S. GEL is Graduate Program Chair and Professor of the Industrial Engineering program at the
School of Computing, Informatics and Decision Systems Engineering at Arizona State University. Her
re-search interests include applied probability, stochastic processes, queuing theory and stochastic modeling.
Her email address is esma.gel@asu.edu.
JOHN W. FOWLER is Chair and Professor of the W.P. Carey Supply Chain Management Department at
Arizona State University. His research interests include modeling, analysis, and control of manufacturing
and service systems. He is a Fellow of the Institute of Industrial Engineers and is the SCS representative on
the Board of Directors of the Winter Simulation Conference. He is an Area Editor of Transactions of the
Society for Computer Simulation International, an Associate Editor of IEEE Transactions on Semiconductor
Manufacturing, and Editor of IIE Transactions on Healthcare Systems Engineering. His email address is
john.fowler@asu.edu.

European Journal of Operational Research 236 (2014) 298–312

Contents lists available at ScienceDirect

European Journal of Operational Research
journal homepage: www.elsevier.com/locate/ejor

Innovative Applications of O.R.

Optimal control policies for ambulance diversion
Adrian Ramirez-Nafarrate a, A. Baykal Haﬁzoglu b, Esma S. Gel b,⇑, John W. Fowler b
a
b

Department of Industrial and Operations Engineering, Instituto Tecnologico Autonomo de Mexico, Mexico, DF 01080, Mexico
School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ 85287, United States

a r t i c l e

i n f o

Article history:
Received 19 November 2012
Accepted 16 November 2013
Available online 26 November 2013
Keywords:
Ambulance diversion
Emergency departments
Markov decision processes

a b s t r a c t
Ambulance diversion (AD) is used by emergency departments (EDs) to relieve congestion by requesting
ambulances to bypass the ED and transport patients to another facility. We study optimal AD control policies using a Markov Decision Process (MDP) formulation that minimizes the average time that patients
wait beyond their recommended safety time threshold. The model assumes that patients can be treated
in one of two treatment areas and that the distribution of the time to start treatment at the neighboring
facility is known. Assuming Poisson arrivals and exponential times for the length of stay in the ED, we
show that the optimal AD policy follows a threshold structure, and explore the behavior of optimal policies under different scenarios. We analyze the value of information on the time to start treatment in the
neighboring hospital, and show that optimal policies depend strongly on the congestion experienced by
the other facility. Simulation is used to compare the performance of the proposed MDP model to that of
simple heuristics under more realistic assumptions. Results indicate that the MDP model performs significantly better than the tested heuristics under most cases. Finally, we discuss practical issues related to
the implementation of the policies prescribed by the MDP.
Ó 2013 Elsevier B.V. All rights reserved.

1. Introduction
Media and academic papers have highlighted the prevalence of
the overcrowding problem in emergency departments (EDs) in the
United States (US) during recent years (Associated Press, 2006).
One of the major negative impacts of congestion in EDs is the long
time that patients have to wait before starting to receive treatment, resulting in seriously adverse events, including death (CNN
U.S., 2008; News, 2010). The risk of such adverse events increases
when the condition of the patient is severe and when waiting
times extend beyond a Recommended Safety Time Threshold
(RSTT), which is used by the Center for Disease Control and Prevention (CDC) based on patient severity (which is assessed by various
indicators of the health condition of the patient such as vital signs
and stability), and the amount of resources required. Reports from
the United States General Accounting Ofﬁce (2009) have highlighted the poor performance of EDs around the US where patients
have to wait beyond their RSTT.
Researchers and managers of EDs have explored different strategies to reduce congestion and avoid potential implications of long
wait times. Hoot and Aronsky (2008) cite ambulance diversion as
one of the main approaches hospitals use for demand management
in EDs. In particular, hospitals, at times of excessive congestion,
may divert ambulances to other neighboring hospitals by request⇑ Corresponding author. Tel.: +1 480 965 2906; fax: +1 480 965 2751.
E-mail address: esma.gel@asu.edu (E.S. Gel).
http://dx.doi.org/10.1016/j.ejor.2013.11.018
0377-2217/Ó 2013 Elsevier B.V. All rights reserved.

ing emergency medical services to bypass their facilities. This
strategy is implemented relatively frequently in US hospitals.
According to the United States General Accounting Ofﬁce, in
2003, 25% or more of the hospitals in several US metropolitan areas
were on diversion more than 10% of the time. During 2006, 27.3%
of hospitals reported going on diversion, and the average number
of hours on diversion during that year were 473 hours (United
States General Accounting Ofﬁce, 2009).
Although EDs often divert ambulances to tackle overcrowding,
this approach can have negative consequences when AD policies
are not properly designed. For instance, Yankovic, Glied, Green,
and Grams (2010) and Shen and Hsia (2011) indicate that AD
might increase mortality among patients with acute myocardial
infarction that are transported by ambulance. Consequently, AD
decisions should consider various factors such as the current congestion at the ED, severity of the patients, and the status of neighboring hospitals. For example, if a neighboring hospital is relatively
near and currently less crowded, then it is more likely that an
arriving patient in an ambulance can start receiving appropriate
treatment earlier if he/she is diverted from an overcrowded facility. On the other hand, while ambulances can be diverted, EDs do
not have control over walk-in arrivals, who, by law, have to be accepted and treated. Therefore, while on diversion, EDs still accept
walk-in patients; these patients also contribute to congestion.
Empirical studies on AD focus on the effectiveness of various AD
approaches, and provide some managerial recommendations to re-

A. Ramirez-Nafarrate et al. / European Journal of Operational Research 236 (2014) 298–312

duce or minimize AD, such as restricting the number of hours spent
on diversion by hospitals serving a speciﬁc geographic region. The
implementation of these guidelines have resulted in signiﬁcant decreases in the number of hours on AD in the regions of study; this
includes San Diego and Sacramento, California (Asamoah, Weiss,
Ernst, Richards, & Sklar, 2008; Patel, Derlet, Vinson, Williams, &
Wills, 2006; Vilke et al., 2004). Unfortunately, these studies do
not discuss the effect of avoiding AD on other performance measures, such as the average patient waiting time.
Analytical studies on AD, on the other hand, suggest that appropriate policies could signiﬁcantly improve the logistical performance of an emergency care system. For example, Deo and
Gurvich (2011) modeled the decisions of two EDs using game theoretic approaches with the objective of minimizing the average patient waiting time for each hospital in a system with two EDs. The
authors found that a centralized design of diversion policies is Pareto-improving compared to a decentralized strategy, which leads
to a defensive equilibrium. The authors also proposed a threshold-type AD policy, but they did not explore the optimality of this
type of control policy. Using similar approaches, Hagtvedt, Grifﬁn,
Keskinocak, Ferguson, and Jones (2009) analyzed AD, and pointed
out the need for a central agent that coordinates AD. Ramirez, Fowler, and Wu (2011) presented a simulation model of an emergency
care delivery system to analyze the effectiveness of diversion and
destination policies. They evaluated the use of an effective combination of diversion–destination policies as an ambulance ﬂow control mechanism in order to reduce the average time spent in
activities with inappropriate level of care, which includes transportation to ED, waiting, and boarding in the ED.
In this paper, we are interested in studying effective ambulance
diversion control policies that optimize the time to patients’ access
to the needed healthcare (at either the considered hospital or at
the neighboring hospitals), and address the following research
questions: (i) Can optimal AD policies signiﬁcantly increase the
safety of patients by minimizing the time that patients wait beyond their RSTT? (ii) What is the structure of optimal AD policies?
(iii) What are the impacts of patient trafﬁc and severity mix on
optimal AD decisions? (iv) What is the value of information of
knowing severity level of ambulance patients, and about the time
to start treatment in the neighboring hospital(s) on the performance of the optimal policy? and (v) How do policies applied in
practice perform compared to optimal AD policies?
In particular, we explore optimal AD policies for EDs with two
treatment areas: an emergent care area and a fast-track treatment
area. This is a layout commonly found in EDs around the world
(Combs, Chapman, & Bushby, 2006, 2007). We study AD policies
characterized by a two-dimensional diversion trigger that accounts
for crowding in both of these treatment areas. This structure allows
us to explore the impact of various parameters (such as those pertaining to the frequency and severity of patients arriving by ambulance) on optimal ambulance diversion decisions, and study
various inherent dependencies in such systems. To the best of
our knowledge, this is the ﬁrst paper that takes into account patient severity levels, regardless of arrival mode (i.e., walk-in or by
ambulance), for constructing AD policies.
Even though admission control methods are commonly used in
various manufacturing and service systems, the AD literature has
not considered the use of such methods in the control of ambulance arrivals to date. Early studies on admission control typically
focus on the control of a single customer class using M=M=1 queueing models (see Stidham (1985) for a survey). More recently, studies consider control of several demand classes requiring different
levels of service. Ha (1997) discusses an inventory control problem
of N demand classes that incur different lost sales costs when customers are not admitted into the system. Similar to our setting,
Carr and Duenyas (2000) discuss two demand classes, where one

299

of the classes is always accepted into the system (similar to the
walk-ins in our model), and the company has an option to reject
the arrivals from the other class (similar to the ambulance arrivals
in our model). Gupta and Wang (2007) consider one contracted demand class whose orders are always accepted, and one transactional demand class whose orders can be rejected. Similarly, Feng
and Pang (2010) consider a long-term contract market whose orders are always accepted, and the spot market whose orders may
be subject to rejection. In the recent work of Chen, Chen, Parlar,
and Xiao (2011), the authors discuss the admission control problem of the orders coming from an online retailer. All of above discussed studies control demand using accept/reject decisions,
similar to our accept/divert decisions.
In particular, this paper contributes to the existing literature on
AD by presenting a Markov Decision Process (MDP) formulation to
obtain optimal AD control policies for a hospital. The objective is to
minimize the long-run average expected tardiness per patient,
where tardiness is deﬁned as the length of time that a patient waits
beyond his/her RSTT, before starting to receive treatment. Hence,
we assume that the hospital sets the ambulance diversion policy
to optimize the patient welfare (rather than a proﬁt or cost measure) by considering the patient’s options of being treated at their
facility versus at a neighboring hospital. We then explore the effectiveness of the obtained policies with respect to other performance
measures of interest to the hospital management, such as the average fraction of time on diversion.
Assuming Poisson arrivals, exponential length of stay in the ED
and two patient severity levels, we analyze the structure of optimal
policies using both theoretical and computational analysis. Using
theoretical analysis, we show that the optimal diversion policy
can be characterized by a threshold curve. Using computational
analysis, we further study the structure of optimal AD policies by
observing the impact of (i) patient arrival rates, (ii) the severity
mix of the patient population, and (iii) the ‘‘amount’’ of available
information on the time to start treatment at the neighboring hospital(s). We next present a simulation study, where various modeling assumptions are relaxed to represent more realistic scenarios,
and compare the performance of optimal policies with that of
other simpler policies used in practice, such as not diverting at
all and diverting only when there are no available beds. Computational analysis veriﬁes the superior performance of the optimal
policies obtained using our MDP model. In addition, a simple policy
that diverts ambulances when there are no available beds for critical patients is shown to yield somewhat satisfactory results in certain cases. We ﬁnally discuss possible drawbacks of our approach
in practice, and provide some recommendations to overcome
these.
This study has two main contributions to the health-care literature. First, to the best of our knowledge, it is the ﬁrst paper discussing optimal control of AD using an MDP formulation. Second,
it considers a novel objective that minimizes the time that patients
wait beyond a RSTT before starting to receive treatment. Although
the AD literature includes various studies that discuss the minimization of time spent in ED, this objective does not take into account
the severity of more critical patients, whose treatment delays may
result in death. Since the RSTT depends on the severity level of patients, our objective considers the safety of the patient as a performance measure for AD policies, which is a signiﬁcant measure to
evaluate the effectiveness of AD policies according to Asplin
(2003). In addition, because our objective is in time units, it does
not require any cost parameterizations that have plagued the previous studies.
The remaining sections of the paper are organized as follows.
Section 2 introduces the model. Section 3 presents some properties
of an optimal AD policy. Section 4 analyzes the impact of the level
of information on the time to start treatment in a neighboring hos-

300

A. Ramirez-Nafarrate et al. / European Journal of Operational Research 236 (2014) 298–312

pital(s) and the severity level of ambulance patients. Section 5 presents a simulation model to compare the policy prescribed by the
MDP formulation with some simple policies used in real-life settings. In Section 6, we analyze issues related to the practical implementation of the AD policies prescribed by the MDP. Finally,
Section 7 presents some conclusions and future extensions.

2. Model formulation
We consider an ED with two major arrival streams, each following a Poisson process, which has been discussed as a reasonable approach to model arrivals to EDs (Green, 2006): (i) ambulance
arrivals with rate kA and (ii) walk-ins with rate kW . Arriving patients can have one of two types of severity levels: level 1 represents the critical patients, and level 2 are less emergent cases.
The use of two severity levels is generally well-justiﬁed, since
although most Emergency Severity Indices (ESI) consider three to
ﬁve severity levels, the majority of patients can be grouped under
two major categories in terms of the required treatment resources
and priority (Saghaﬁan, Hopp, Van Oyen, Desmond, & Kronick,
2012). One group includes patients with an immediate and emergent need for emergency care, and the other group includes patients with urgent and semi/non-urgent needs.
The treatment times for the critical (i.e., level 1) patients can be
signiﬁcantly different than the treatment times for the less emergent (i.e., level 2) patients. To protect level 2 patients from waiting
behind level 1 patients, most EDs are designed with a fast-track
area, which primarily serves level 2 patients. While it is possible
to ﬁnd hospitals without this two-area design, this design is relatively well-established in practice (Cochran & Roche, 2009; Saghaﬁan et al., 2012; Welch, 2009), and supported by evidence on its
efﬁciency from queueing systems research (Hu & Benjafaar,
2009; Saghaﬁan et al., 2012). Hence, in this study, we assume that
the ED has two treatment areas dedicated to each severity level: A1
(emergent care), which treats patients of severity level 1, and A2
(fast-track), which treats patients of severity level 2. However,
the model presented in this paper can be modiﬁed to accommodate a system with a single pooled area that serves all customers,
as will be discussed later.
We assume that each arriving ambulance patient is level 1 with
probability pA1 , and level 2 with probability 1  pA1 . Similarly, walkin patients are level 1 with probability pW
1 , and level 2 with probA
W
ability 1  pW
1 . The parameters p1 and p1 can be estimated through
the use of historical data. We refer to the tuple (pA1 ; pW
1 ) as the
‘‘severity mix.’’ Upon admission to the ED, patients are ﬁrst identiﬁed as level 1 or level 2, and they are served in the order of their
arrival at the corresponding area (i.e., A1 or A2). If all beds in the
appropriate area are occupied, then an arriving patient waits in a
ﬁrst-come-ﬁrst-served (FCFS) queue corresponding to his/her
treatment area. The number of staffed beds is denoted by c1 for
A1 and c2 for A2.

Once a patient accesses a bed in the corresponding area, the patient remains in the bed for some amount of time referred to as
‘‘length of stay.’’ The length of stay considered in this paper may include activities such as treatment (bedside assessment, interactions with the physician, tests, and delivery of medications) and
discharge process, as well as a boarding time due to the crowding
in inpatient units. We assume that the length of stay of a level i patient is a random variable distributed exponentially with rate li for
i 2 f1; 2g. While the exponential distribution may not be a very
good ﬁt to represent the total length of stay, it is commonly used
in the literature due to its analytical tractability (see, for example,
Deo & Gurvich (2011)). In Section 5, we relax this assumption and
develop a simulation model using more realistic distributions such
as the lognormal distribution.
The state of the system can be represented by the tuple
ðn1 ðtÞ; n2 ðtÞÞ, where n1 ðtÞ and n2 ðtÞ represent the number of patients in the system with severity level 1 and level 2 at time t,
respectively. We drop t from the notation and denote the state
space as S ¼ fðn1 ; n2 Þ : n1 P 0; n2 P 0g. The ﬂow of patients in
the ED is depicted in Fig. 1.
To ensure the existence of a solution for our MDP model, we assume that the total arrival rate is less than the total departure rate
in each treatment area (Bertsekas, 2001), that is,
W
pA1 kA þ pW
< c 1 l1 ;
1 k

W
and ð1  pA1 ÞkA þ ð1  pW
< c 2 l2 :
1 Þk

In our computational analysis, we limit the total number of patients
in the system to eliminate the need for a stability condition as given
in Eq. (1). Therefore, this condition is relaxed in our computational
analysis.
The objective is to ﬁnd a state-dependent ambulance diversion
policy that minimizes the long-run average expected tardiness per
patient (denoted as ETP henceforth) over an inﬁnite horizon. Different from the traditional settings, we refer to tardiness as the
non-negative difference between the total waiting time of the patient in ED and the RSTT of the patient (denoted as di for severity
level i 2 f1; 2g). We assume that d1 < d2 ; therefore, the difference
in the RSTT provides a ‘‘natural’’ weight that severely penalizes
the objective as the waiting time of critical patients increases.
Our theoretical results also hold for the commonly used objective
of ‘‘minimization of total waiting time in ED,’’ as we discuss later.
We let T i ðni Þ; i 2 f1; 2g denote the expected tardiness of an
arriving patient with level i given that there are ni patients with
severity level i in the system upon his/her arrival. We have

T i ðni Þ ¼

Z

1

di

ðx  di Þfi;ni ðxÞdx;

ð2Þ

where fi;ni ðÞ denotes the probability density function (pdf) of the
waiting time of a level i patient that observes ni level i patients in
the system upon his/her arrival. If ni < ci , we have T i ðni Þ ¼ 0. For
ni P ci , since length of stay in the ED is exponential with rate
ci li ; f i;ni ðÞ is the pdf of the Gamma distribution with parameters
ni  ci þ 1 and ci li . The expression T i ðni Þ for ni P ci is evaluated
using Laplace transforms in Theorem 1 of Haﬁzoglu, Gel, and Keskinocak (2013). The closed-form expression for T i ðni Þ is provided in
Eq. (3).

8
nX
i c i
>
k
< edi ci li
ðdi ci li Þ ni cik!þ1k ; if ni P ci ;
ci li
T i ðni Þ ¼
k¼0
>
:
0;
if ni < ci :

Fig. 1. System representation.

ð1Þ

ð3Þ

When an ambulance is diverted, the patient is sent to a neighboring hospital for treatment. To reﬂect the impact of diverting
the ambulance on the patient’s time to treatment, we model the
time to start treatment at the neighboring hospital as a random
variable, X i for the patient severity level of i, and calculate the patient’s tardiness value based on this random variable, if a diversion

301

A. Ramirez-Nafarrate et al. / European Journal of Operational Research 236 (2014) 298–312

decision is made. Note that X i would include any additional time
that may be incurred to travel to a further-away facility, as well
as the waiting time inside the ED of that hospital.
Ideally, the distribution of X i should be a function of the current
status (e.g., congestion) of the neighboring hospital that the patient
is diverted to, or at least dependent on the recent instances of
diversions. Due to the fact that such considerations would make
the problem analytically intractable, our model makes the simplifying assumption that X i is stationary over time, which is valid for
cases in which the neighboring hospital is a relatively big facility,
or the hospital uses some intelligent diversion such that patients
are diverted to multiple hospitals in the region, so that no one hospital is adversely affected by the congestion due to diversion of patients. In Section 5, we use simulation to explore the impact of
nonstationary X i over the course of a day to account for the increased trafﬁc during certain parts of the day, which is likely to result in more diversions to the neighboring hospital from the
hospital under consideration. While this is not a direct method of
accounting for the effect of events like ‘‘multiple diverted patients
within a short period of time,’’ it is a method to obtain some insights since repeated diversions that are likely to have an effect
on X i are likely to occur during periods of high trafﬁc.
We note here that the issue of how ambulance diversion decisions made by one hospital locally impacts the performance at another one is a question that has attracted attention in the
literature. Deo and Gurvich (2011), for example, show that, noncooperative diversion decisions between hospitals may indeed lead
to a defensive equilibrium, in which hospitals have to avoid diverting patients when all of them get overcrowded and the beneﬁts of
regional resource pooling are null.
The expected tardiness of a diverted patient with level i is denoted as T Di for i 2 f1; 2g, and evaluated as in the following
equation:

T Di ¼

Z

1

ðx  di Þfi ðxÞdx;

ð4Þ

di

where fi ðxÞ is the pdf of X i for level i patient, and can be different for
level 1 and level 2 patients.
We assume that the ambulance crew communicates with the
ED to learn if the patient can be taken to the hospital or not. Then,
the decision maker in the ED chooses to divert or accept the ambulance depending on the current state of the system. This assumption is in line with the diversion guidelines formulated by the
American College of Emergency Physicians stating that diversion
criteria must be based only on hospital capacity and not on ﬁnancial decisions (American College of Emergency Physicians, 1999).
In addition, it is assumed that the severity of the patient is not
known at the time that the diversion decision is made, although
this assumption will be relaxed and studied later on.
The continuous-time MDP model is converted to an equivalent
discrete time model using uniformization with rate
m ¼ kA þ kW þ c1 l1 þ c2 l2 . Let v  denote the optimal ETP and

h ðn1 ; n2 Þ denote the optimal relative effect of starting in state
ðn1 ; n2 Þ. The Bellman’s equation, for each ðn1 ; n2 Þ is given as

v

kW þ kA

m



þ h ðn1 ;n2 Þ ¼

kW p W
1

þ
þ



½T 1 ðn1 Þ þ h ðn1 þ 1;n2 Þ

m

kW ð1  pW
1 Þ

m
e
c 1 l1



h ðn1  1; n2 Þ þ

m (

þ min
þ



½T 2 ðn2 Þ þ h ðn1 ; n2 þ 1Þ

kA pA1

m

kA ð1  pA1 Þ

m

e
c 2 l2

m



h ðn1 ; n2  1Þ



½T D1 þ h ðn1 ; n2 Þ


½T D2 þ h ðn1 ; n2 Þ;

kA pA1

m

½T 1 ðn1 Þ



þ h ðn1 þ 1; n2 Þ þ
þ 1

kA ð1  pA1 Þ

m

)

kW þ kA þ e
c 1 l1 þ e
c 2 l2

m



½T 2 ðn2 Þ þ h ðn1 ;n2 þ 1Þ
!


h ðn1 ; n2 Þ;

ð5Þ

where

e
ci ¼



ni if ni 6 ci ;
ci ; if ni > ci ;

for i 2 f1; 2g.
The ﬁrst two terms on the right hand side of Eq. (5) refer to the
walk-in patients with severity level 1 and level 2, respectively. The
third and fourth terms represent the departure events, which decrease the number of patients in A1 or A2 by one, depending on
the severity level of the departing patient. The ﬁrst term inside
the minimum statement represents the average tardiness if an
arriving ambulance patient is diverted, whereas the second term
represents the average tardiness if the ambulance patient is accepted to the hospital. The last term corresponds to a self loop
due to uniformization.
We note that the objective function can be changed easily to
minimizing the weighted average expected tardiness per patient
in the long run by adding weights to the tardiness expressions.
These weights may depend on the severity level, with the weight
given to the tardiness of level 1 patients being greater than the
weight given to the tardiness of level 2 patients.
Having two treatment areas is a common and suggested practice (Cochran & Roche, 2009; Combs et al., 2006, Combs, Chapman,
& Bushby, 2007; Pennsylvania Patient Safety Advisory, 2010; Sanchez, Smally, Grant, & Jacobs, 2006; Victorian Government Department of Human Services, 2008), but certainly it is not universal.
Modeling an ED with a single pool of beds with prioritized high
severity patients requires three modiﬁcations on our model. First,
we would need to deﬁne a total number of staffed beds c, which
would be equal to c1 þ c2 in our current model. Second, since the
length of stay of patients differ by their criticality, we need to
add one more state variable into our model to keep the number
of level 1 (or 2) patients that are currently being treated. Accordingly, the optimality equations would be changed. Third, the FCFS
assumption will no longer be valid in this case, since some sort of
prioritization of level 1 patients would be more acceptable. While
the computation of expected tardiness would be much more complicated, it is still doable using Laplace transforms, similarly to the
approach in Haﬁzoglu et al. (2013), which consider arrivals from
two classes of customers with a prioritized class. The only modiﬁcation to that derivation would be to consider differential service
rates for level 1 and level 2 customers.
A plausible alternative to the two extreme cases of completely
separated and pooled treatment areas is to implement intelligent
sharing of resources between the two physically separate treatment areas, as necessary. Such approaches have been studied and
termed as ‘‘virtual streaming’’ by Saghaﬁan et al. (2012). Our models do not consider such resource sharing protocols between the
two treatment areas for the sake of tractability. However, we note
that the use of virtual streaming strategies could only improve the
performance of the system (since not sharing is a viable option),
and it is possible to construct a mapping between the actual state
of the system (i.e., the number of patients of different severity levels in each of the treatment areas) and an effective state (i.e., equivalent numbers of severity level 1 patients in area A1 and level 2
patients in area A2), which could be used to determine the ambulance diversion decision. We expect such an approach to provide
an effective heuristic.

302

A. Ramirez-Nafarrate et al. / European Journal of Operational Research 236 (2014) 298–312

Other resources found in EDs such as physicians, nurses, and
medical equipment are not included directly in the model. However, we note that the treatment time distributions that we assume
in our decision and simulation models are selected so that the
availability of these resources are considered. For example, the
treatment time for the patient includes all interactions by the physician as well as the time that the patient spends waiting for his/
her physician in the ED bed. Similarly, we consider boarding as a
possibility due to the unavailability of beds in the inpatient units,
and while we do not explicitly model the inpatient units, we consider representative values for the boarding time as part of the
treatment time. This aspect is considered and analyzed in Sections
3.2 and 5.
3. Properties of optimal diversion policies
In this section, we derive some properties of the optimal solution to the Bellman’s equation given in (5), and analyze the impact
of parameters on the optimal solution using computational analysis in Sections 3.1 and 3.2, respectively.
3.1. Theoretical analysis
In Theorem 1, we show that the optimal diversion policy is
characterized by a monotonic threshold curve. This result also justiﬁes the common use of threshold-type policies used previously in
the AD literature (Deo & Gurvich, 2011; Hagtvedt et al., 2009).
Theorem 1. There exists a threshold curve Dðn1 Þ, such that it is
optimal to divert incoming ambulances when n2 > Dðn1 Þ, and accept
them whenever n2 6 Dðn1 Þ. Furthermore, Dðn1 Þ is non-increasing in
n1 .
Proof. The proof is included in the Online Supplement. h
In Fig. 2, we illustrate Theorem 1, where Dðn1 Þ is shown by the
representative non-increasing curve. It is optimal to divert an
ambulance if n2 > Dðn1 Þ, that is, if the state is located above the
curve. In words, the state space above the curve denotes the cases
where the ETP added from accepting incoming ambulance patients
(i.e., second term within minimization in Eq. (5)) is greater than
the ETP added from diverting them (i.e., ﬁrst term within minimization in Eq. (5)).
Note that one can obtain the same result in Theorem 1 for the
objective of ‘‘minimization of total waiting time in ED,’’ since the
patients’ waiting time, similarly to our objective, T i ðni Þ, is nondecreasing in the number of patients in the ED. The same result
also holds for Theorems 2 and 3.
We next discuss the impact of the magnitude of X i on the
thresholds in Theorem 2 and Corollary 1. Noting that T Di is a function of X i , in Theorem 2, we show that the increase in T Di pushes the

Fig. 2. Illustration of Theorem 1.

threshold curve in the upward direction, and results in higher optimal ETP’s.
D
Theorem 2. Dðn1 Þ is non-decreasing in T D
1 and T 2 .

Proof. The proof is omitted in the interest of saving space, and can
be obtained from the authors. h
Theorem 2 states that increases in T Di shift the threshold curve
‘‘up.’’ In Corollary 1, we state a similar result with respect to X i
using stochastic order relations.
Corollary 1. Let X i and X i be two random variables where X i Pst X i for
i ¼ 1; 2 (i.e., X i is stochastically larger than X i ). Furthermore, let Dðn1 Þ
and Dðn1 Þ be the threshold curves and optimal ETP obtained by solving
the problems with X i ¼ X i and X i ¼ X i , respectively. Then, we have
Dðn1 Þ P Dðn1 Þ.

Proof. The proof is included in the Online Supplement. h
Given two random variables X i and X i with F X i ðaÞ 6 F X i ðaÞ for all
a, where FðÞ denotes the corresponding cumulative distribution
function, we obtain higher threshold levels and optimal ETP’s for
X. For example, let X i and X i be uniform and triangular random
variables with parameters ½15; 35 and ½15; 25; 30 minutes for
i ¼ 1; 2, respectively. One can simply show that X i Pst X i , and hence,
obtain Dðn1 Þ P Dðn1 Þ using Corollary 1.
Similar to Theorem 1, 2 and Corollary 1 offer computational
time improvements. Given the threshold curve Dðn1 Þ (Dðn1 Þ), and
the inequality Dðn1 Þ P Dðn1 Þ obtained from Theorem 2 and Corollary 1, one knows that optimal decision is on-diversion (off-diversion) for all ðn1 ; n2 Þ, where n2 P Dðn1 Þ (n2 6 Dðn1 Þ) in the problem
solved with X ¼ X i (X ¼ X i ).
3.2. Computational analysis
In this section, we explore the behavior of the optimal policy
using computational analysis. Our objective in these computational studies is to obtain some general insights on the effectiveness of the different policies that we consider. For this purpose,
we used a number of sources from the literature, as well as data
and information obtained from our partners at a local hospital to
determine the below stated experimental parameters. The insights
obtained on this section are applicable to a large variety of EDs
sizes. Accordingly, in all of the experiments, we ﬁx the values of
c1 ; c2 ; l1 ; l2 ; d1 and d2 as indicated in Table 1, and vary the values
of the other parameters to test the system behavior under various
scenarios.
Number of staffed beds, c1 and c2 : 15 and 5 are the closest values
to the average number of beds in treatment spaces in real-life EDs
(an average of 14.6 beds in standard treatment spaces and 5 beds
for other treatment spaces) (Centers for Disease Control & Prevention, 2006). According to this report, 57.5% of EDs in Metropolitan
Statistical Areas (MSA) and 97.8% of EDs in Non-MSA have less than
20 standard treatment spaces and 49.9% of EDs in MSA and 87.9%
of EDs in Non-MSA have less than 5 other treatment spaces. Furthermore, the percentages of EDs with between 10 and 19 standard
treatment spaces are 36.1% for MSA and 12.1% for Non-MSA. Therefore, proportionally, the size of the ED chosen for experimentation
is representative for many EDs around the US.
Service rates l1 and l2 : To set l1 and l2 , we estimate two time
values: (i) actual treatment time and (ii) boarding time of patients
in ED. The treatment time denotes the actual time that the patient
spends on an ED bed receiving treatment. Boarding time, on the
other hand, denotes the additional time that a patient spends on
the ED bed waiting for an open bed in an inpatient unit. Hence,

A. Ramirez-Nafarrate et al. / European Journal of Operational Research 236 (2014) 298–312

hand, we obtain kW ¼ 5:63, UDWA1 ¼ 75% and UDWA2 ¼ 56:25%,
which implies that the congested area is A1. Furthermore, for any
combination of trafﬁc intensity and severity mix, the value of pW
1
determines the congested area in the ED. Hence, if pW
1 ¼ 0:3, then
the congested area is A2; whereas if pW
1 ¼ 0:5, then the congested
area is A1.
The proportion of ambulance arrivals was ﬁxed to be 15% of all
kA
arrivals, i.e., we set kA þk
¼ 0:15. This value is very close to the naW
tional average of the percentage of ambulance arrivals to EDs in the
United States, which is 15.5% (Centers for Disease Control & Prevention, 2007).

Table 1
Factor levels used in the computational analysis.
Fixed factors
c1
c2

d1
d2

15 beds
5 beds
0.20 patient/hour
0.86 patient/hour
15 minutes
90 minutes

Variable factors
Trafﬁc
pA1

2{Low, Medium, High}
2{0.8, 0.9}

pW
1
E½X 1  ¼ E½X 2 

2{15, 45, 75, 90, 105 and 120 minutes}

l1
l2

2{0.3, 0.5}

boarding delays the time to start treatment in the ED. Several
sources have identiﬁed boarding patients as a cause of congestion
in EDs (Asplin, 2003; Centers for Disease Control & Prevention,
2006; McConnell et al., 2005; United States General Accounting Ofﬁce, 2003).
The average actual treatment times are set to 4 and 1 hours for
emergent and fast-track care, respectively. The ﬁrst value is close
to the average treatment time for immediate and emergent patients, given in several sources (Centers for Disease Control & Prevention, 2004; Cochran & Roche, 2009; Hoot et al., 2008). The fasttrack treatment rate is close to the value observed in Cochran and
Roche (2009) for semiurgent and nonurgent patients.
In order to deﬁne the boarding time, we used data from Singer,
Thode, Viccellio, and Pines (2011) to assume an average boarding
time of 3.58 hours, which is a value that matches the ﬁndings by
the United States General Accounting Ofﬁce (2003). Furthermore,
using the ﬁndings of Singer et al. (2011), we assumed that 24% of
patients with level 1 and 4.5% of patients with level 2 require
transfer to an inpatient unit after ending treatment in the ED. This
implies that 12.1% of all ED patients are transferred, which is the
same value found by Singer et al. (2011) and very similar to the
average percentage of admitted patients from the ED according
to the CDC (Centers for Disease Control & Prevention, 2006), which
is 12.8%. Using this information, the average boarding time is chosen to be 0.86 hours and 0.16 hours for levels 1 and 2, respectively,
which gives a total of 4.86 and 1.16 hours service time for level 1
and level 2 patients, respectively. Thus, we set l1 ¼ 0:20 and
l2 ¼ 0:86 patients per hour.
RSTT, d1 and d2 : d1 is set to 15 minutes, which corresponds to
the second most emergent level in the ESI; this category is usually
referred as ‘‘less than 15 minutes.’’ d2 , on the other hand, is set to
90 minutes, which corresponds to an average of the third and
fourth ESI indices, usually referred as urgent (‘‘1 hour’’) and
semi-urgent (‘‘2 hours’’) (Centers for Disease Control & Prevention,
2004).
Arrival rates kW ; kA , and severity level probabilities pA1 and pW
1 : We
consider low, medium and high levels of trafﬁc, which is quantiﬁed
by the Utilization Due to Walk-in Arrivals (UDWA) (note that walkin arrivals cannot be controlled using AD).

UDWA1 ¼

W
pW
1 k
;
c 1 l1

303

and UDWA2 ¼

W
ð1  pW
1 Þk
;
c 2 l2

Distribution of X i : In all of our analysis in this section, we choose
X i to be deterministic, with the tested values shown in Table 1. The
impact of randomness of X i is analyzed in Section 4. We note that
choosing X i values lower than 15 and higher than 120 minutes result in unrealistic optimal policies such as always diverting or
never diverting (recall that d1 ¼ 15 and d2 ¼ 90 minutes). We also
conduct a preliminary computational analysis using the three settings of k 2 f0:5; 1; 2g, where E½X 1  ¼ kE½X 2 , while E½X 2  is changed
as given in Table 1. We observe that the value of k does not affect
the threshold curves. Hence we assume X 1 and X 2 to be independent and identically distributed in all our analysis. Furthermore,
we use X to denote both X 1 and X 2 in the remainder.
We used 500 as the upper limit for the number of patients in the
system in order to implement the relative value iteration algorithm. Such an upper limit also allows us to relax the stability condition given in the expressions in Eq. (1). This upper limit is large
enough to approximate the inﬁnite capacity assumed in Section 2
while ensuring a reasonable execution time of the relative value
iteration algorithm.
In Figs. 3–5, we analyze the impact of (pA1 ; pW
1 ), trafﬁc intensity
and X, respectively. In all of the ﬁgures, we observe non-increasing
threshold curves, whose existence was proven in Theorem 1. Fig. 3
presents the threshold curves for four different values of (pA1 ; pW
1 )
under medium trafﬁc and a deterministic value of X ¼ 45 minutes.
As shown in Fig. 3, the congested area determines the shape of
the threshold curve. It is expected that most ambulance patients
will be critical patients; therefore, if the congested area is A2, then
the optimal policy initiates diversion only when all of the beds in
A1 are occupied for low values of n2 . However, if the congested
area is A1, then the optimal policy initiates diversion much earlier

ð6Þ

where UDWA1 and UDWA2 denote the Utilization Due to Walk-in
Arrivals in A1 and A2, respectively. We use maxfUDWA1 ;
UDWA2 g ¼ 60%; 75% and 90% to model the low, medium and high
levels of trafﬁc, respectively. The area with the highest utilization is
referred to as the ‘‘congested area’’. For example, when trafﬁc level
is medium and pW
1 ¼ 0:3, we ﬁnd kW ¼ 5:36, which gives
UDWA1 ¼ 43% and UDWA2 ¼ 75%, indicating that the congested
area is A2. When trafﬁc level is medium and pW
1 ¼ 0:5, on the other

Fig. 3. Illustration of thresholds for pA1 2 f0:8; 0:9g and pW
1 2 f0:3; 0:5g, under
medium trafﬁc, and a deterministic time to start treatment in the other hospital (X)
of 45 minutes.

304

A. Ramirez-Nafarrate et al. / European Journal of Operational Research 236 (2014) 298–312

Fig. 4. Illustration of thresholds for changing trafﬁc levels.

Fig. 5. Illustration of thresholds for changing levels of the time to start treatment in the neighboring hospital (X) for medium trafﬁc intensity.

in order to save beds in the congested area for future demand (possibly walk-ins). Hence, AD is an effective mechanism to alleviate
congestion from A1, but it is not as effective when the congested
area is A2. In the next sections, we present statistics that show signiﬁcant reductions on ETP when the congested area is A1. From a
practical point of view, saving beds for future demand might not
be acceptable by healthcare administrators, especially in the case
of emergent care beds in the ED. Nevertheless, in the case of an
emergency situation that affects a large proportion of the population (e.g., an earthquake or terrorist attack), an ED located in the
affected zone might divert ambulances to other hospitals and save
resources for the walk-in arrivals of injured people.
In addition, the effect of the difference in RSTT values and treatment rates can be seen in Fig. 3. Any optimal policy plotted in this
ﬁgure allows a queue in area A2 before diverting ambulances; that
is, the thresholds observed in n2 are greater than 5, which is the value set for c2 . The size of the queue allowed in A2 before diverting
ambulances is smaller if that area is the congested one. On the
other hand, queueing is not allowed in the emergent care area
A1; that is, the thresholds observed in n1 are smaller than or equal
to 15, which is the value set for c1 .
Figs. 4(a) and (b) demonstrate how threshold curves change
with trafﬁc intensity when X ¼ 45 minutes and congested areas
are A1 and A2, respectively.
In general, the higher the utilization in the congested area is, the
lower the threshold to initiate diversion. However, changes in the
thresholds are more evident if the congested area is A1. That is, if
the congested area is A1, AD policies might initiate diversion even

when there are plenty of beds available in A1. Since patients arriving by ambulance are more likely to be critical patients, the optimal policy changes signiﬁcantly in n1 in order to manage the
trafﬁc. For example, when the congested area is A1 and there is
high trafﬁc intensity (90% UDWA), the optimal policy diverts all
the time. For medium trafﬁc, the optimal policy accepts some patients, but it saves almost half of the emergent care beds for future
demand. For low trafﬁc, the optimal policy saves only few beds in
A1 before diverting ambulances. On the other hand, if the congested area is A2, the threshold in n1 is also around the value of
c1 for low values of n2 , and the thresholds in n2 allows patients
waiting in A2.
We ﬁnally discuss the impact of the magnitude of X on the
thresholds in Figs. 5(a) and (b).
As observed in Fig. 5, the threshold curves increase in X. The result is due to the fact that T D1 and T D2 increase in deterministic X,
which implies that Dðn1 Þ is non-decreasing in X from Theorem 2.
For a deterministic value of X = 15 minutes, the optimal policy diverts all the time, regardless of which area is the congested one.
This is due to the fact that d2 > d1 ¼ 15 minutes, and hence, the
policy diverts all of the ambulance patients with a guarantee of
zero tardiness. However, as the time to be seen in another hospital
increases, the optimal policy increases the threshold that initiates
diversion. If the congested area is A1, the optimal policy might save
several beds in A1 for future demand when X has a moderate value.
But as X increases beyond d2 , the optimal policy approaches initiating diversion only under full occupancy in A1. If the congested
area is A2 as shown in Fig. 5(b), the threshold in n1 increases with

305

A. Ramirez-Nafarrate et al. / European Journal of Operational Research 236 (2014) 298–312

increases in X, and might even allow a small queue, depending on
the trafﬁc intensity.
4. Value of information in ambulance diversion
In this section, we discuss the value of two types of information
in ambulance diversion: (i) the status of neighboring hospitals and
(ii) the severity of the ambulance patient. We use the computational settings provided in Section 3.2.
4.1. Status of neighboring hospitals
The model presented in Section 2 assumes that the hospital under study has some information about the time to start treatment
in a neighboring hospital if patients are diverted. This time is related to multiple state variables of the neighboring hospital, such
as crowding conditions, stafﬁng, availability of lab and equipment,
and even trafﬁc conditions. The level of information on these variables determine the accuracy of the estimation of X proposed in the
model.
In this section, we analyze the value of information on the time
to start treatment in the neighboring hospital. We assume that the
decision maker at the ED knows E½X, however, does not have exact
knowledge about the distribution of X, and evaluates the threshold
curves using a possibly incorrect X distribution. We consider three
different values of E½X 2 f15; 45; 75g, and conduct a test where the
distribution of X used in the calculation of T Di in Eq. (4) is either
deterministic (D), uniform distributed (U) or triangular distributed
(T), as shown in Table 2. For each combination of distribution and
expected value, there is one instance that has larger variability
than the other. These cases are referred to as small (S) and large
(L) variability cases.
Note that the value of information on the distribution of X is signiﬁcant in the case where ED evaluates a wrong threshold curve,
which may lead to signiﬁcant increase in ETP. On the other hand,
if ED happens to obtain and use the right threshold curve (even
assuming a wrong distribution of X) there is no value of knowing
the exact distribution of X. Thus, we focus on the threshold curves
obtained using the above distributions, and analyze the cases
where the an incorrect threshold curve obtained. In Fig. 6, we depict the threshold curves under medium trafﬁc for the tuple
ðDistribution; E½X; VariabilityÞ, where Distribution 2 fD, U,
Tg; E½X 2 f15; 45; 75g and Variability 2 fS, Lg.
One clearly observes that the distribution and variability of X
typically have a relatively small impact on threshold curves, since
threshold curves with the same E½X generally overlap each other

Table 2
Properties of X used in the computational analysis.
E½X
(minutes)

Distribution

Range
(minutes)

15

Deterministic
U(10, 20)
U(5, 25)
T(10, 15, 20)
T(5, 15,v25)

–
10
20
10
20

45

Deterministic
U(30, 60)
U(15, 75)
T(30, 45, 60)
T(15, 45, 75)

75

Deterministic
U(50, 100)
U(25, 125)
T(50, 75, 100)
T(25, 75, 125)

Std. dev.
(minutes)

CV

Variability

–
2.8868
5.7735
2.0412
4.0825

–
0.1925
0.3849
0.1361
0.2722

–
S
L
S
L

–
30
60
30
60

–
8.6603
17.3205
6.1237
12.2474

–
0.1925
0.3849
0.1361
0.2722

–
S
L
S
L

–
50
100
50
100

–
14.4338
28.8675
10.2062
20.4124

–
0.1925
0.3849
0.1361
0.2722

–
S
L
S
L

Fig. 6. Illustration of threshold curves with the tested X distributions where the
congested area is A1 and trafﬁc is medium.

Table 3
Percentage of increase in ETP using a non-optimal policy for medium trafﬁc, the
congested area in A1 and E½X ¼ 75.
True distribution of X

Det
(T, S)
(T, L)
(U, S)
(U, L)

Assumed distribution of X
Det

(T, S)

(T, L)

(U, S)

(U, L)

–
0.00
0.00
0.00
0.0004

0.00
–
0.00
0.00
0.0004

0.01
0.00
–
0.00
0.0004

0.00
0.00
0.00
–
0.0004

0.0009
0.0004
0.00
0.0004
–

for most of the threshold curve. In particular, for E½X ¼ 45 minutes,
we obtain the same threshold curves regardless of the distribution
and the variability of X. In addition, under some cases, we observe a
small upward shift in the threshold curve when the variability increases; this is observed particularly when E½X ¼ 75 minutes.
These results can be attributed to Theorem 2, which indicates that
a change in the distribution and variability of X may change the value of T D1 and T D2 (see Eq. (4)), resulting in a shift of the threshold
curves. For example, when E½X ¼ 45 minutes, in all cases we have
T D1 ¼ 60 and T D2 ¼ 0 minutes giving the same threshold curves in all
cases. On the other hand, when E½X ¼ 75 minutes, we have
T D2 ¼ 1:002, and T D2 ¼ 6:12 minutes, for the uniform distribution
with small and large variability, respectively, resulting in an upward shift of the threshold curve for the uniform distribution with
large variability given in Fig. 6.
As observed in Fig. 6, the value of information is not signiﬁcant
when E½X ¼ 15 and 45 minutes. We next analyze the value of
information when E½X ¼ 75, assuming that the actual distribution
of X is Deterministic, U(50, 100), U(25, 125), T(50, 75, 100), or
T(25, 75, 125), whereas ED considers one of the other four when
computing the diversion policy. We test for all 20 possible cases,
and indicate the percentage ETP increases in Table 3.
As observed in Table 3, wrong information on the distribution of
X results in quite an insigniﬁcant increase in ETP, where the highest observed increase is 0.0009%. This analysis is also conducted for
low and high trafﬁc levels, E½X 2 f15; 45; 75g minutes, and also
considering the settings where the congested area is A2. Hence,
the total number of experiments are 360. These results show that
the wrong estimation of X distribution result in an highest ETP increase of only 0.23 minutes. Hence, we conclude that knowing the
exact distribution of X does not improve ETP values signiﬁcantly, as
long as ED knows the expected value of X. However, it is worthwhile to note that the distribution of X may deﬁnitely have a significant impact on other performance measures. For instance, a highly
variable distribution of X may produce a relative high fraction of
patient waiting beyond their RSTT.

306

A. Ramirez-Nafarrate et al. / European Journal of Operational Research 236 (2014) 298–312

Table 4
Percentage of improvement on the ETP in minutes for different levels of trafﬁc and different distributions of X when the severity of ambulance patient is known before the arrival.
Tr.

C.A.

Low

E½X ¼ 15 minutes

A1
A2
A1
A2
A1
A2

Medium
High

E½X ¼ 75 minutes

Det.

Tria.

Det.

Tria.

–

S

L

S

Unif.
L

–

S

L

S

L

0.00
0.00
0.00
0.00
0.00
0.00

0.00
8.10
0.00
1.03
0.00
0.00

0.00
10.68
0.00
2.69
0.00
0.03

0.00
9.66
0.00
1.91
0.00
0.01

0.01
12.26
0.00
4.07
0.00
0.10

0.12
15.71
0.06
18.54
0.00
10.17

0.09
15.09
0.06
18.46
0.00
10.17

0.28
11.20
0.18
17.72
0.01
10.13

0.13
13.44
0.07
18.25
0.00
10.16

0.55
9.34
0.42
16.78
0.06
10.08

4.2. Severity of the ambulance patient
In Section 2, we assume that the severity level of patients transported by ambulance is not known in advance to their arrival to the
ED. In this section, we assume that the ED has information about
the severity level of the patient upon arrival by ambulance. The
optimality equation for the new model is obtained by replacing
the minimization term in Eq. (5) with the expression in Eq. (7).

min

(
kA pA1

m



½T D1 þ h ðn1 ;n2 Þ þ



þ h ðn1 þ 1;n2 Þ þ
þ
þ

kA ð1  pA1 Þ

m
kA ð1  pA1 Þ

m

kA ð1  pA1 Þ

kA ð1  pA1 Þ

m

m



½T 2 ðn2 Þ þ h ðn1 ;n2 þ 1Þ;



½T 2 ðn2 Þ þ h ðn1 ;n2 þ 1Þ;
)

½T D2 þ h ðn1 ;n2 Þ



½T D2 þ h ðn1 ;n2 Þ;

:

kA pA1

m

kA pA1

m

kA pA1

m

½T 1 ðn1 Þ

Unif.

the congestion level of A1 only. Hence, the model presented in this
section can manage trafﬁc in both areas more effectively.
One insight from this section is that the assessment and communication of patient severity level may provide signiﬁcant beneﬁts when there is a high level of congestion in A2. This might be the
case during the ﬂu season where more people might attend EDs
looking for treatment. Similarly, the change of health insurance
status may lead to increased visits to EDs, some of them being
non-urgent (Ginde, Lowe, & Wiler, 2012). On the other hand, in
hospitals where a signiﬁcant fraction of patients are level 1, however, communication of patient severity level does not bring significant ETP improvements, and hence, can be skipped.



½T D1 þ h ðn1 ;n2 Þ

5. Simulation of ambulance diversion policies



½T 1 ðn1 Þ þ h ðn1 þ 1;n2 Þ
ð7Þ

Note that, unlike (5), where there are two possible actions, we
consider four actions in (7). The four terms inside of the minimization term correspond to (i) divert all the patients, (ii) do no divert
any patient, (iii) divert only patients with severity level 1, and (iv)
divert only patients with severity level 2, respectively. This model
allows us to determine diversion decisions based on the knowledge
of the severity level of patient in addition to the current status of
ED. The model assumes that diversion decisions are made while
patient is being transported by ambulance. Therefore, we assume
that if a patient is not diverted, the state of the system does not
change prior to his arrival.
We conduct a computational study based on the parameters deﬁned for the experimentation in Section 4.1. We computed the percentage of improvement on the ETP when severity level of the
ambulance patient is known in advance to his/her arrival (model
in Eq. (7)) compared to unknown severity level (model in Eq.
(5)). The results are presented in Table 4.
Similar to observations in Section 4.1, percentage of improvement increases as the variability of the distribution of X decreases
and as the E½X increases. However, the most important observation from this table is that the improvement on the ETP when
ambulance patient severity level is known is signiﬁcantly higher
when the congested area is A2. On the other hand, when the congested area is A1, the knowledge of the ambulance patient severity
level is almost irrelevant.
This is due to the fact that most ambulance patients are level 1;
therefore, ambulance diversion decisions in both models manage
the trafﬁc of A1 in an effective way. However, when the congested
area is A2, the model with perfect information on the severity level
of ambulance patients is able to make better decisions for those patients. Thus, the model in Eq. (7) allows diverting level 2 patients
only when that decision can improve the performance of the system based on the congestion level of A2; while the model in Eq.
(5) tends to establish the decision thresholds based primarily on

The MDP proposed in this paper assumes stationary arrival
rates and exponential length of stay in the ED. However, there is
evidence that these assumptions do not sufﬁciently represent
real-life settings. In this section, we relax these assumptions and
explore the impact of more realistic assumptions that are commonly observed in EDs across the US, using a discrete-event simulation model. In addition, the AD policy prescribed by the MDP
formulation is compared with the following simple AD heuristics.
(1) Full Beds in A1 (FB A1): Since most of the ambulance arrivals
are critical patients, this policy diverts when all of the beds
in area A1 are occupied (i.e., when n1 P c1 ).
(2) H Beds Occupied in A1 (H A1): This policy initiates diversion
when H beds in A1 are occupied. We test this heuristic with
H values of 12, 13 and 14.
(3) Full Beds in A1 or in A2 (FB A1/A2): This policy diverts an
arriving ambulance when there is at least one area with all
beds occupied (i.e., when n1 P c1 or n2 P c2 ).
(4) Full Beds (FB): This policy diverts an arriving ambulance
only when all of the beds in the ED (both A1 and A2) are
occupied (i.e., when n1 P c1 and n2 P c2 ).
(5) Myopic policy (Myopic): This policy diverts an arriving
ambulance only if the expected tardiness for the current
ambulance patient at the neighboring hospital is smaller
than the expected tardiness if he/she is accepted. Thus,
under the myopic policy, the ambulance is diverted only
when pA1 T D1 þ ð1  pA1 ÞT D2 6 pA1 T 1 ðn1 Þ þ ð1  pA1 ÞT 2 ðn2 Þ.
(6) No AD policy (No AD): This policy does not divert patients at
any time.
We modify the MDP settings used in Section 3.2 as follows:
5.1. Non-stationary arrival rate
Several sources have identiﬁed a pattern in the ED arrivals
across the US (Centers for Disease Control & Prevention, 2006;
Cochran & Roche, 2009; Green, 2006). This pattern exhibits low

307

A. Ramirez-Nafarrate et al. / European Journal of Operational Research 236 (2014) 298–312

Fig. 7. Arrival multiplicative indices, adopted from Cochran and Roche (2009).

trafﬁc between 1 am and 8 am approximately. Then, the rate of
arrivals increase between 8 am and 10 am, and remain at a high level between 10 am and 11 pm. Then, a decline in the rate of the
arrivals is observed between 11 pm and 1 am. In order to consider
this pattern in our simulation model, we adopt the arrival rate pattern used by Cochran and Roche (2009). The authors present an
hourly multiplicative index that indicates the trafﬁc intensity compared to the average arrival rate. Fig. 7 is taken from Cochran and
Roche (2009), and it shows the change in the arrival multiplicative
index throughout the day.
In order to mimic the arrival pattern of Cochran and Roche
(2009), we set arrival rates as follows. We ﬁrst ﬁnd a kW such that
maxfUDWA1 ; UDWA2 g ¼ 90%, and set this kW as the walk-in arrival rate for the highest peak hour, which is from 7 pm to 8 pm with
a multiplicative index of 1.45. For example, for the setting
W
(pA1 ; pW
¼ 6:75, which gives the arrival
1 ) = (0.9, 0.5), we obtain k
rate used for 7–8 pm in the simulation model. We next scale the
walk-in arrival rates using the multiplicative indices to obtain
the arrival rates for every hour during the day. For example, for
the setting (pA1 ; pW
1 ) = (0.9, 0.5), the arrival rate between 1am and
2am, which has a multiplicative index of 0.6, is chosen as
0:6
 6:75 ¼ 2:793. Then, the hourly ambulance arrival rates are
1:45
calculated such that they represent 15% of the total arrivals to
the ED (Centers for Disease Control & Prevention, 2007).
Similar to settings in Section 3.2, we use two different severity
mixes with (pA1 ; pW
1 ) as (0.9, 0.5) and (0.9, 0.3), which make A1 and
A2 the congested areas, respectively. Furthermore, low, medium
and high trafﬁc levels denote the trafﬁc levels at hours where the
multiplicative index are in between [0.4, 0.75], [0.75, 1.1], and
[1.1, 1.45], respectively (see Fig. 7).
5.2. Distribution of X
The arrival pattern observed in most EDs across the United
States suggest that trafﬁc in neighboring hospitals are positively
correlated (United States General Accounting Ofﬁce, 2003). Therefore, it is very likely that if an ED experiences high trafﬁc, a neighboring hospital also is experiencing high trafﬁc, increasing the
waiting times of the diverted patients. To model this fact, we assume that the parameters for the distribution of X change depending on the level of trafﬁc intensity at the ED under study. We
consider three settings for the parameters of X, where Settings 1,
2, and 3 denote the cases with low, medium and high correlation
between the trafﬁc levels of the ED under study and the neighboring hospitals. X is assumed to have a triangular distribution with
E½X having minimum and maximum values of 15 and 120 minutes,
respectively. The parameter settings are presented in Table 5. The
coefﬁcient of variation of X is set to 0.2722 in all cases for consis-

Table 5
Settings of X used in simulation.
Trafﬁc in main ED

Low
Medium
High

Parameters of triangular distribution (minutes)
Setting 1

Setting 2

Setting 3

(5, 15, 25)
(10, 30, 50)
(15, 45, 75)

(5, 15, 25)
(15, 45, 75)
(25, 75, 125)

(10, 30, 50)
(25, 75, 125)
(40, 120, 200)

tency, because we use a minimum of 5 minutes for the value of X
(note that a value less than 5 minutes is quite unrealistic).
5.3. Treatment time
The treatment times in areas A1 and A2 are assumed to be lognormally distributed, which is one of the distributions identiﬁed in
Hoot et al. (2008) to represent treatment times in healthcare. The
expected treatment times used in the simulation model remain
240 minutes and 60 minutes for patients treated in areas A1 and
A2, respectively. The standard deviation was adjusted to match
the coefﬁcient of variation of treatment times found in Cochran
and Roche (2009). Therefore, the standard deviation of treatment
in A1 was set to 173.88 minutes, yielding a coefﬁcient of variation
of 0.72; and the standard deviation of treatment in A2 was set to
6.12 minutes, yielding a coefﬁcient of variation of 0.102.
5.4. Boarding time
In the simulation model, a boarding patient does not release the
ED bed before an additional amount of time referred to as boarding
time. Unlike Section 3.2, where we use an average boarding time,
the boarding time in the simulation model is assumed to be uniformly distributed in between [0, 2], [2, 6], [6, 12], and
[12, 24] hours with probabilities 0.5022, 0.3705, 0.0763 and
0.051, respectively. Furthermore, we use the same boarding probabilities used in Section 3.2 for each severity level. Hence, the average boarding time produced is 3.58 hours.
5.5. Settings for the MDP model
We also use the simulation model in order to test the performance of the policy prescribed by the MDP. In our preliminary
analysis, we consider stationary arrival rates of the overall daily
average, and test four MDP strategies differentiated with respect
to the parameters of the distribution of X: (i) the parameters of
the distribution of X during high trafﬁc hours; (ii) the parameters
during medium trafﬁc hours; (iii) the parameters during low trafﬁc
hours; and (iv) the overall daily average of the parameters of the

308

A. Ramirez-Nafarrate et al. / European Journal of Operational Research 236 (2014) 298–312

distribution of X. The preliminary experiments show that the MDP
solved with the overall daily average rate for the parameters of X
outperforms all others. Hence, the overall daily average is used to
derive the optimal AD threshold by the MDP model. For each simulation setting, we solve the MDP model ﬁrst, and then the obtained AD control policy is implemented in the simulation model
to estimate its performance.
The ED modeled in this section is representative of a smallersized ED with high congestion during peak times, which is the
most important factor for going on diversion. There are other elements that could be observed in EDs that are not included in the
model: patients leaving without treatment and virtual capacity.
The former refers to patients that decide to leave the hospital without initiating treatment because of their long wait and the latter
refers to the use of other types of resources, such as chairs or beds
allocated in hallways, to deliver treatment to patients during high
congestion episodes. These two elements may help reducing congestion on the ED. Therefore, the ETP and diversion statistics
shown in the results might be overestimated. However, the insights provided about the performance of the diversion policies
compared with other policies remain valid since the effect of
including the two aspects would not beneﬁt a particular AD policy.
Each policy with all possible combinations of severity mix and
setting for X were modeled in simulation models developed using
Arena (Kelton, Sadowski, & Swets, 2010). Pilot runs were used to
determine a warm-up period of two months, replication length
of one year and 30 replications in order to capture the performance
of the system in steady state and estimate the average tardiness
per patient using 95% conﬁdence intervals with an average relative
precision of 3.76%. In addition, common random numbers were
used to reduce noise when comparing alternative AD policies
(Banks, Carson, Nelson, & Nicol, 2010).
The results obtained during the experimentation show that not
diverting at all, which is the No AD policy, produces average tardiness per patient of 1036.19 ± 215.10 minutes when the congested
area is A1; and 10.43 ± 0.42 minutes when the congested area is
A2. Fig. 8 presents the conﬁdence intervals for setting 3 of X, given
in Table 5. The results for settings 1 and 2 show similar patterns.
When the congested area is A1, the average tardiness per patient
obtained by the MDP policy are 13.87, 18.06 and 24.89 minutes
for settings 1, 2 and 3, respectively. When the congested area is
A2, the average tardiness per patient obtained by the MDP policy
are 6.87, 8.63 and 9.02 minutes for settings 1, 2 and 3, respectively.
Even though the simulation model includes several relaxations
that invalidate the optimality of the policy suggested by the MDP,
the policy prescribed performs consistently well in all scenarios
compared with other heuristics. The policies based on the occupancy at A1 (i.e., FB A1, 14 A1, 13 A1 and 12 A1) also perform rea-

sonable well when A1 is the congested area because they take
advantage of the fact that most ambulance patients are critical,
and hence, the policy tries to avoid queuing in the emergent care
area. On the other hand, policy FB does not work well in comparison to the other policies when the congested area is A1 because it
delays diversion until all of the beds, including those in A2, are
occupied.
The policy that diverts when at least one area has all of the beds
occupied (i.e., FB A1/A2) is the only one that performs worse than
not diverting at all. This occurs when the congested area is A2 and
using setting 3. This result indicates that this policy initiates diversion when if is not needed and might not take advantage of the fact
that patients in A2 have a high RSTT. In general, most of the policies perform signiﬁcantly better than No AD. Not diverting patients
can produce a high average tardiness per patient, especially if the
congested area is A1, where the critical patients are treated. For
this case, some of the AD policies, including the one suggested by
the MDP, can reduce the average tardiness by several hours, which
could make a signiﬁcant difference in terms of the mortality rate in
critical patients. Therefore, these results suggest that intelligent
design of AD policies can reduce the time to deliver appropriate
treatment to patients, even if the time to start treatment in a
neighboring hospital is relatively large.
Table 6 presents the relative performance of the heuristics taking the policy prescribed by the MDP as a basis, and shows if there
is a statistically signiﬁcant difference between the performances of
the policies. The table shows that if the congested area is A1, the
policy obtained via MDP performs signiﬁcantly better than all of
the other heuristics. Therefore, regardless of the fact that the
MDP proposed in Section 2 includes several assumptions that
may not be completely justiﬁable, it still provides an effective policy that could reduce important risks in emergent patients’ health
in real life systems. In addition, regarding the policies based on
occupancy level of A1, the policies that initiate diversion so that
they reserve a few beds for future walk-in demand improve performance as the time to start treatment in the neighboring hospital
decreases. However, when the time to start treatment at the neighboring hospital increases, these policies lose effectiveness because
many patients could be diverted to a more crowded ED unnecessarily. Therefore, reserving a few beds in A1 may result in good performance if a signiﬁcant fraction of walk-ins are level 1 patients
and if the time to start treatment in the neighboring hospital is relatively small.
On the other hand, if the congested area is A2, the MDP policy
performs signiﬁcantly better than most of the other policies, when
the time to start treatment in another hospital is relatively small.
However, if the time to start treatment in another hospital is relatively large, the policy prescribed by the MDP performs better than

Fig. 8. 95% Conﬁdence intervals on ETP.

A. Ramirez-Nafarrate et al. / European Journal of Operational Research 236 (2014) 298–312
Table 6
Relative performance of heuristics compared with MDP (%). Values in italics show
statistical signiﬁcance using 95% conﬁdence level.
Heuristic

FB A1
14A1
13A1
12A1
FB A1/A2
FB
Myopic
No AD

Setting 1

Setting 2

Setting 3

A1

A2

A1

A2

A1

A2

94.33
69.81
57.41
39.87
78.53
834.89
81.73
7114.99

9.70
14.02
5.60
4.66
10.68
21.17
10.92
43.30

59.60
41.39
32.73
20.17
48.76
627.86
50.06
5488.03

4.83
0.01
5.54
4.15
15.92
4.29
3.38
20.88

27.99
15.59
10.38
2.45
21.70
437.87
21.33
3994.83

4.89
1.53
1.31
3.41
46.27
2.96
2.80
15.63

FB A1/A2 and No AD, but there is no signiﬁcant difference from
other policies. Therefore, it seems that the MDP model is a good
option when the congested area is A1, but other heuristics that
are simpler to implement might be preferred if the congested area
is A2.
Another important aspect to highlight from Table 6 is the large
difference in the relative performance when comparing the columns that deﬁne the congested area for the same setting. For
example, the heuristic FB performs 834.89% worse than the threshold prescribed by the MDP under setting 1 and the congested area
is A1; but the same heuristic performs only 21.17% worse than the
MDP under the same setting and the congested area is A2. Therefore, the knowledge of the severity mix that deﬁnes the congested
area is a key parameter that determines the effectiveness of an AD
policy.
The results show that effective design of AD policies can decrease the ETP signiﬁcantly, even if the neighboring hospital is
far away or crowded like in the case of setting 3. However, inappropriate heuristics can lead to worse performance than not diverting
at all, as in the case of policy FB A1/A2 and setting 3 with the congested area in A2. In addition, the simulation model conﬁrms that
ambulance diversion is more likely to have a signiﬁcant impact if
the congested area is A1.

6. Insights on implementation of AD policies prescribed by the
MDP
Typically, AD is implemented such that the ED maintains the
diversion status for a predetermined period in which ambulances
are diverted to other hospitals. In contrast, the optimal AD control
policies prescribed by our MDP model comprise a single threshold
that determine accepting or diverting individual ambulance arriv-

309

als. Hence, there may be some downsides of this approach if implemented in practice: (i) EDs could go on and off diversion very often,
increasing the cost of communicating with Emergency Medical
Services (EMS); (ii) an ambulance could be rejected and another
could be accepted within a short time frame, which may seem to
be unethical to practitioners; (iii) the AD policy produced by the
MDP formulation requires continuous monitoring of the state of
the system; and (iv) the frequent changes on diversion status
may confuse EMS and could bypass the ED unnecessarily, which
would increase the loss of potential admits.
In order to overcome these issues in this section, we present a
new heuristic, MDPs, that requires the diversion status to last for
at least a predetermined duration of s. This policy implements
the threshold prescribed by the MDP to determine when to initiate
the diversion status. Once the ED goes on diversion by exceeding
the threshold, the ED maintains the diversion status for the next
s time units. After s time units, the state of the system is evaluated.
If the state of the system is above the threshold curve according to
the prescribed MDP policy, then the diversion status is maintained
for another s time units. Otherwise, the ED removes the diversion
status.
The remainder of the section analyzes this family of policies for
the case where the congested area is A1 because AD is more effective in this scenario. In addition, setting 2 of Table 5 was chosen for
analysis because it implies moderate values for the parameters of
X; however, similar observations have been made for other settings
in our extended set of results. Fig. 9 shows the ETP and the average
number of diversion episodes per day for the MDP policy and the
MDPs policies, with s 2 f30; 60; 90; 120g (in minutes).
In general, ETP resulting from the MDPs policies are greater
than that of the policy prescribed by the MDP. The difference is
small, but signiﬁcant. However, the differences in the average tardiness of the MDPs policies is not signiﬁcant among them, which
could mean that the value of s has a low impact on the ETP obtained. On the other hand, the MDPs policies reduce the average
number of diversion episodes per day signiﬁcantly, as observed
in Fig. 9(b), and hence, they may avoid ethical problems related
to admission control in emergency care.
Decision makers in practice have the objective of providing
timely care to patients requiring emergency care, as well as minimizing the duration of the diversion episodes and the fraction of
time spent on diversion. The diversion episode length refers to
the duration of the diversion status every time that the ED goes
on diversion. Since the formulation presented in this paper does
not penalize being in the diversion status, the policies prescribed
by the MDP may result in long diversion durations (particularly
when treatment times at the other hospital are short). In Fig. 10,
we present the average fraction of time spent on diversion and

Fig. 9. Performance of the MDP prescribed policy and MDPs policies in terms of tardiness and number of diversion episodes.

310

A. Ramirez-Nafarrate et al. / European Journal of Operational Research 236 (2014) 298–312

Fig. 10. Performances of MDP and MDPs in terms of fraction of time on diversion and diversion episode length for c1 ¼ 15 beds.

Table 7
Sensitivity analysis for different number of beds in A1 and considering the MDP prescribed policy.
Number of beds in A1

Avg. tardiness/patient (minutes)
Avg. fraction of time on div.

12

13

14

15

16

17

18

137.87
1.00

48.78
1.00

27.27
0.996

18.54
0.782

14.56
0.510

11.46
0.359

10.31
0.086

the average diversion episode length for the MDP and MDPs policies, using again setting 2 for the distribution of X and congested
area in A1.
We observe that an increase of s results in more undesirable
outcomes in terms of both metrics. Furthermore, the fraction of
time on diversion of the MDP policy is signiﬁcantly higher than
the values observed in real settings (less than 20% according to
United States General Accounting Ofﬁce (2003)). Therefore, the
proposed MDP policy improves the performance of EDs in terms
of ETP, however, it increases the fraction of time on diversion
and the average diversion episode lengths, which may be undesirable for the ED under study and for other neighboring EDs because
diverted patients might cause congestion in the other facilities.
After spending s time units on diversion, an ED might keep the
diversion status for other s time units if the state of the ED still exceeds the diversion threshold. For example, when the congested
area is A1 and for setting 2, the average number of successive
diversion episodes is 12.10, 7.58, 5.92 and 5.98 for MDP30,
MDP60, MDP90 and MDP120, respectively. In general, the number
of successive diversion episodes decreases as s increases, which
might be convenient to determine a value of s that avoids keeping
the diversion status for several periods; however, this assumption
reduces ﬂexibility to change diversion decisions.
In order to ﬁnd long-term solutions that improve performance
in both metrics, decision makers must address the root cause of
the problem, which may well be insufﬁcient capacity to provide
care either at the ED or downstream at the inpatient units of the
healthcare delivery system. We brieﬂy analyze the impact of
capacity of the ED in Table 7, which presents a sensitivity analysis
varying the number of beds in A1 and observing how the optimal
average tardiness per patient and the fraction of time on diversion
change (under the optimal control policy) for setting 2.
Capacity has a signiﬁcant impact on the performance of the ED.
Adding beds to A1 reduces the optimal average tardiness per patient, which approaches to zero as the number of beds becomes
sufﬁcient to serve the demand. In addition, adding beds reduces
signiﬁcantly the fraction of time on diversion under the optimal
control policy. The MDP model may prescribe being always on
diversion when the walk-in demand exceeds the capacity during

the peak time. For example, under setting 2 and assuming
c1 ¼ 12 beds, the best average tardiness per patient that the ED under study can obtain is 137 minutes, but this implies being always
on diversion, and accepting only walk-ins. In order to reduce both
metrics simultaneously, the ED would need to invest further in
emergent care beds. For example, assuming 18 beds in A1, the
ED could achieve an average tardiness per patient of 10.31 minutes
while being on diversion only 8% of the time. If inpatient units are
bottlenecks that constrain the ﬂow of patients out of the ED, causing congestion in A1 and/or A2, then the increase of capacity at the
inpatient units would have a similar impact on the performance
measures as observed in Table 7. Hence, strategic planning of
EDs must consider the capacity of the EDs and inpatient units to
estimate the size and amount of resources required to reach a desired or acceptable level of tardiness and fraction of time on
diversion.

7. Conclusions
This paper presents an MDP model to determine the AD policy
that minimizes the long-run average tardiness per patient for a single ED. We deﬁne tardiness as the amount of time that the patients
wait beyond the recommended safety time threshold. The model
considers two treatment areas, differentiated by patient severity,
and assumes availability of (some) information on the time to start
treatment in a neighboring hospital if an ambulance patient is
diverted.
The structural properties of the model indicate the existence of
a threshold curve ‘‘above’’ which one should divert ambulances.
This threshold curve is sensitive to trafﬁc intensity, severity mix
and expected time to start treatment at the neighboring hospital.
Based on analytical results and computational experiments shown
in this paper, it is observed that the threshold curve is non-increasing in trafﬁc intensity and non-decreasing in the expected tardiness experienced if patients are diverted to a neighboring
hospital. We analyzed the value of information and found that
knowledge of the severity of an ambulance patient is not signiﬁcant unless the congested area in the ED is A2. In addition, we

A. Ramirez-Nafarrate et al. / European Journal of Operational Research 236 (2014) 298–312

found that the variability regarding the time to start treatment in a
neighboring hospital has a relatively small impact on the deﬁnition
of the threshold and on the optimal value.
Even though the MDP includes assumptions that might not be
realistic in real-life settings, the optimal AD policy performs consistently well under the incorporation of time-dependent arrival
patterns, non-exponential distributions for the length of stay and
addition of boarding time. In particular, the policy prescribed by
the MDP performs signiﬁcantly better than all of the other heuristic policies, including not diverting at all, when the congested area
is A1. Therefore, intelligent use of MDP could contribute to increased patient safety by minimizing the time that they wait beyond their recommended safety time threshold.
Our MDP formulation does not take into account some performance metrics considered by EDs such as the fraction of time on
diversion and the average diversion episode length. Hence, the proposed MDP policy may prescribe policies that may result in relatively long diversion episodes, and relatively frequent diversions.
A sensitivity analysis made on the number of beds in A1 shows that
capacity should be addressed in a strategic manner in order to have
signiﬁcant improvements in both, timeliness (measured by, e.g.,
average tardiness per patient) and in accessibility to emergency
care (measured by, e.g., fraction of time on diversion).
The paper can be extended in several directions. First, one can
assign a ﬁxed cost for changing diversion decisions (e.g., from on
to off and from off to on) in order to avoid frequent diversion decision changes. This proposed model requires another dimension in
the state space to keep track of current diversion status, and generalizes our MDPs heuristic by providing an optimal time length
that the diversion decisions should not be changed. A second
worthwhile extension is to consider more general settings and policies for how resources are prioritized among patients in the emergency department. We have indicated above that various virtual
streaming policies toward that end could be considered, and a
study on how such policies may reduce the use of ambulance
diversion would be very informative for emergency departments
considering such operational design options. Last but not least,
an important but not very straightforward extension is to consider
the status of neighboring hospitals (e.g., number of patients) in the
state deﬁnition, and provide optimal diversion decisions also considering the congestion in neighboring hospitals. Such approaches
could also involve the consideration of the beneﬁts of centralized
decision making across a number of hospitals, rather than localized
decision making by each hospital.

Appendix A. Supplementary material
Supplementary data associated with this article can be found, in
the online version, at http://dx.doi.org/10.1016/j.ejor.2013.11.018.

References
American College of Emergency Physicians (1999). Guidelines for ambulance
diversion.
Asamoah, O. K., Weiss, S. J., Ernst, A. A., Richards, M., & Sklar, D. P. (2008). A novel
diversion protocol dramatically reduces diversion hours. American Journal of
Emergency Medicine, 26, 670–675.
Asplin, B. R. (2003). Does ambulance diversion matter? Annals of Emergency
Medicine, 41, 477–480.
Associated Press (2006). Report: ER care in U.S. at ‘breaking point’. <http://
www.msnbc.msn.com/id/13320317/ns/health-health_care/t/report-er-care-usbreaking-point/>.
Banks, J., Carson, J., II, Nelson, B., & Nicol, D. (2010). Discrete-event system simulation.
Upper Saddle River, NJ: Pearson Education, Inc..
Bertsekas, D. P. (2001). Dynamic programming and optimal control (Vol. 2). Belmont,
Mass: Athena Scientiﬁc. 02178-9998.
Carr, S., & Duenyas, I. (2000). Optimal admission control and sequencing in a maketo-stock/make-to-order production system. Operations Research, 48, 709–720.

311

Centers for Disease Control and Prevention (2006). National hospital ambulatory
medical care survey: 2004 emergency department summary. <http://
www.cdc.gov/nchs/data/ad/ad372.pdf>.
Centers for Disease Control and Prevention (2006). Advanced data from vital and
health statistics safﬁng, capacity, and ambulance diversion in emergency
departments: United States, 2003–2004. <http://www.cdc.gov/nchs/data/ad/
ad376.pdf>.
Centers for Disease Control and Prevention (2008). National hospital ambulatory
medical care survey: 2006 emergency department summary. <http://
www.cdc.gov/nchs/data/nhsr/nhsr007.pdf>.
Centers for Disease Control and Prevention (2010a). National hospital ambulatory
medical care survey: 2007 emergency department summary. <http://
www.cdc.gov/nchs/data/nhsr/nhsr026.pdf>.
Chen, J. A., Chen, Y. H., Parlar, M., & Xiao, Y. B. (2011). Optimal inventory and
admission policies for drop-shipping retailers serving in-store and online
customers. IIE Transactions, 43, 332–347.
CNN U.S. (2008). Tape shows woman dying on waiting room ﬂoor. <http://
articles.cnn.com/2008-07-01/us/waiting.room.death_1_hospital-staff-hospitalemployee-kings-county-hospital-center?_s=PM:US>.
Cochran, J. K., & Roche, K. T. (2009). A multi-class queuing network analysis
methodology for improving hospital emergency department performance.
Computers and Operations Research, 36, 1497–1512.
Combs, S., Chapman, R., & Bushby, A. (2006). Fast track: One hospital’s journey.
Accident and Emergency Nursing, 14, 197–203.
Combs, S., Chapman, R., & Bushby, A. (2007). Evaluation of fast track. Accident and
Emergency Nursing, 15, 40–47.
Deo, S., & Gurvich, I. (2011). Centralized vs. decentralized ambulance diversion: A
network perspective. Management Science, 57, 1300–1319.
Feng, Y. Y., & Pang, Z. (2010). Dynamic coordination of production planning and
sales admission control in the presence of a spot market. Naval Research
Logistics, 57, 309–329.
Ginde, A., Lowe, R., & Wiler, J. (2012). Health insurance status change and
emergency department use among us adults. Archives of Internal Medicine,
172, 642–647.
Green, L. (2006). Queueing analysis in healthcare. Patient Flow: Reducing Delay in
Healthcare Delivery, 281–307.
Gupta, D., & Wang, L. (2007). Capacity management for contract manufacturing.
Operations Research, 55, 367–377.
Ha, A. Y. (1997). Inventory rationing in a make-to-stock production system with
several demand classes and lost sales. Management Science, 43, 1093–1103.
Haﬁzoglu, A. B., Gel, E. S., & Keskinocak, P. (2013). Expected tardiness computations
in multi class priority M/M/c queues. INFORMS Journal on Computing, 25,
364–376.
Hagtvedt, R., Grifﬁn, P., Keskinocak, P., Ferguson, M., & Jones, F. (2009). Cooperative
strategies to reduce ambulance diversion. In Winter simulation conference
(pp. 1861–1874). Piscataway, New Jersey: Institute of Electrical and Electronics
Engineers, Inc..
Hoot, N. R., & Aronsky, D. (2008). Systematic review of emergency department
crowding: Causes, effects, and solutions. Annals of Emergency Medicine, 52,
126–136.
Hoot, N., LeBlanc, L., Jones, I., Levin, S., Zhou, C., Gadd, C., et al. (2008). Forecasting
emergency department crowding: A discrete event simulation. Annals of
Emergency Medicine, 52, 116–125.
Hu, B., & Benjafaar, S. (2009). Partitioning of servers in queueing systems during
rush hour. Manufacturing and Service Operations Management, 11, 416–428.
Kelton, W., Sadowski, R., & Swets, N. (2010). Simulation with ARENA. Columbus, OH:
McGraw-Hill.
McConnell, K. J., Richards, C. F., Daya, M., Bernell, S. L., Weathers, C. C., &
Lowe, R. A. (2005). Effect of increased icu capacity on emergency
department length of stay and ambulance diversion. Annals of Emergency
Medicine, 45, 471–478.
News, K. (2010). Parents of woman who died after waiting in ER sue hospital. <http://
www.kval.com/news/local/98377384.html>.
Patel, P. B., Derlet, R. W., Vinson, D. R., Williams, M., & Wills, J. (2006). Ambulance
diversion reduction: The Sacramento solution. American Journal of Emergency
Medicine, 24, 206–213.
Pennsylvania Patient Safety Advisory (2010). Managing patient access and ﬂow in the
emergency
department
to
improve
patient
safety.
<http://
patientsafetyauthority.org/ADVISORIES/AdvisoryLibrary/-2010/dec7(4)/
documents/123.pdf>.
Ramirez, A., Fowler, J., & Wu, T. (2011). Design of centralized ambulance diversion
policies using simulation–optimization. In Winter simulation conference
(pp. 1251–1262). Piscataway, New Jersey: Institute of Electrical and
Electronics Engineers, Inc..
Saghaﬁan, S., Hopp, W. J., Van Oyen, M. P., Desmond, J. S., & Kronick, S. L. (2012).
Patient streaming as a mechanism for improving responsiveness in emergency
departments. Operations Research, 60, 1080–1097.
Sanchez, M., Smally, A. J., Grant, R. J., & Jacobs, L. M. (2006). Effects of a fast-track
area on emergency department performance. The Journal of Emergency Medicine,
31, 117–120.
Shen, Y. C., & Hsia, R. Y. (2011). Association between ambulance diversion and
survival among patients with acute myocardial infarction. JAMA Journal of the
American Medical Association, 305, 2440–2447.
Singer, A. J., Thode, H. C., Viccellio, P., & Pines, J. M. (2011). The association between
length of emergency department boarding and mortality. Academic Emergency
Medicine, 18, 1324–1329.

312

A. Ramirez-Nafarrate et al. / European Journal of Operational Research 236 (2014) 298–312

Stidham, S. (1985). Optimal control of admission to a queuing system. IEEE
Transactions on Automatic Control, 30, 705–713.
United States General Accounting Ofﬁce (2003). Hospital emergency departments:
Crowded conditions vary among hospitals and communities. <http://
www.gao.gov/new.items/d03460.pdf>.
United States General Accounting Ofﬁce (2009). Hospital emergency departments:
Crowding continues to occur, and some patients wait longer than recommended
time frames. <http://www.gao.gov/products/GAO-09-347>.
Victorian Government Department of Human Services (2008). Streaming care: Fast
track services in hospital emergency departments. <http://www.health.vic.gov.au/
emergency/streamingcare0109.pdf>.

Vilke, G. M., Castillo, E. M., Metz, M. A., Ray, L. U., Murrin, P. A., Lev, R., et al.
(2004). Community trial to decrease ambulance diversion hours: The San
Diego County patient destination trial. Annals of Emergency Medicine, 44,
295–303.
Welch, S. J. (2009). Patient segmentation: Redesigning now. Emergency Medicine
News, 31.
Yankovic, N., Glied, S., Green, L. V., & Grams, M. (2010). The impact of ambulance
diversion on heart attack deaths. Inquiry-the Journal of Health Care Organization
Provision and Financing, 47, 81–91.

This article was downloaded by: [149.169.145.169] On: 09 June 2017, At: 22:54
Publisher: Institute for Operations Research and the Management Sciences (INFORMS)
INFORMS is located in Maryland, USA

Interfaces
Publication details, including instructions for authors and subscription information:
http://pubsonline.informs.org

A Decision-Making Framework for Project Portfolio
Planning at Intel Corporation
Siddhartha Sampath, Esma S. Gel, John W. Fowler, Karl G. Kempf

To cite this article:
Siddhartha Sampath, Esma S. Gel, John W. Fowler, Karl G. Kempf (2015) A Decision-Making Framework for Project Portfolio
Planning at Intel Corporation. Interfaces 45(5):391-408. https://doi.org/10.1287/inte.2015.0809
Full terms and conditions of use: http://pubsonline.informs.org/page/terms-and-conditions
This article may be used only for the purposes of research, teaching, and/or private study. Commercial use
or systematic downloading (by robots or other automatic processes) is prohibited without explicit Publisher
approval, unless otherwise noted. For more information, contact permissions@informs.org.
The Publisher does not warrant or guarantee the article’s accuracy, completeness, merchantability, fitness
for a particular purpose, or non-infringement. Descriptions of, or references to, products or publications, or
inclusion of an advertisement in this article, neither constitutes nor implies a guarantee, endorsement, or
support of claims made of that product, publication, or service.
Copyright © 2015, INFORMS
Please scroll down for article—it is on subsequent pages

INFORMS is the largest professional society in the world for professionals in the fields of operations research, management
science, and analytics.
For more information on INFORMS, its publications, membership, or meetings visit http://www.informs.org

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

Vol. 45, No. 5, September–October 2015, pp. 391–408
ISSN 0092-2102 (print)  ISSN 1526-551X (online)

http://dx.doi.org/10.1287/inte.2015.0809
© 2015 INFORMS

A Decision-Making Framework for Project
Portfolio Planning at Intel Corporation
Siddhartha Sampath

Decision Engineering Group, Intel Corporation, Chandler, Arizona 85226, siddhartha.sampath@intel.com

Esma S. Gel

School of Computing, Informatics, and Decision Systems Engineering, Ira A. Fulton Schools of Engineering,
Arizona State University, Tempe, Arizona 85281, esma.gel@asu.edu

John W. Fowler

Supply Chain Management Department, W. P. Carey School of Business, Arizona State University, Tempe, Arizona 85287,
john.fowler@asu.edu

Karl G. Kempf

Decision Engineering Group, Intel Corporation, Chandler, Arizona 85226, karl.g.kempf@intel.com

The work we describe addresses the problem of deciding between project-funding opportunities under budget
and headcount constraints. Although the projects lead to products that yield revenue in the market, complex
interactions between the projects and products make the selection of a portfolio difficult. Furthermore, the
senior managers in the company have a wealth of business intuition that can inform the required decisions. We
combine modeling, simulation, and optimization techniques to provide a set of the best portfolios possible from
the proposed projects and resulting products. We also provide a rich set of analysis and visualization tools for
the decision makers to use in exploring the suggested portfolios and applying their intuition to make the final
selection. The resulting interplay between analytics and intuition produces better business solutions through a
more focused and effective debate in a shorter time than previously achieved.
Keywords: analytics; binary integer linear program; decision support; elimination by aspects; intuition;
portfolio management; practice of OR; simulation.
History: This paper was refereed.

F

unding reviews occur regularly in companies
offering a variety of products in a number
of markets. At any point in time, many productdevelopment projects are in various stages of execution, ranging from those in their initial ramp-up
stages to those nearing new-product launch; all require continued funding. Other projects are new and
seeking startup funding. In innovative companies, the
number of requests usually exceeds the budget available to fund these requests. Management has to periodically make decisions on the effective allocation of
limited budgets to achieve corporate goals, including
profit maximization.
Making these decisions is inherently difficult because of the combinatorial complexity resulting from
the number of projects, products, and markets involved and their extensive interrelationships. For

example, one project may support many products.
Other projects may depend on each other for intermediate deliverables. One product may be released
into multiple markets, and one market may receive
many different products. Product offerings can affect
each other synergistically or cannibalistically in the
marketplace. These interactions result in a single
project or product having different costs and (or)
benefits depending on the other projects or products
the company decides to undertake. Funding (or not
funding) any specific development project will have
far-reaching consequences for products in the marketplace. We define the result of this strategic selection
of funding opportunities that advance the company’s
stated goals as project portfolio planning.
Portfolio selection is further complicated by the
business reality of dealing with multiple objectives
391

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

392

Sampath et al.: Decision-Making Framework for Project Portfolio Planning at Intel

(e.g., revenue, sales volume, investment efficiency),
many participants, and the long-lasting effects of decisions. Various projects compete for limited resources
with their respective champions’ advocating supporting strategies and agendas. Some of these competing
ideas are quantifiable, whereas others are based on
qualitative intuition. What constitutes a good portfolio depends on goals that align with an organization’s overall vision and resources. A successful
portfolio selection framework must, therefore, allow
for open and transparent comparison of both tangible
and intangible objectives, as well as project and product relationships, to a committee of decision makers
to enable the committee members to ultimately select
a good portfolio.
With hundreds of innovative products in hundreds
of competitive markets across the globe, Intel Corporation faces project portfolio planning decisions on
a regular cadence. With 2014 revenues of $55.9 billion, Intel is a world leader in silicon innovation.
Intel product divisions compete in a wide variety
of markets, including desktop and laptop computing, servers, networking, communications, entertainment, and embedded computing. In addition, Intel
Research operates on an innovation treadmill that
regularly produces new product technologies, including enhanced graphics, security, novel sensors, inputoutput modalities, and connectivity advancements.
Considering the diversity and dynamics of both markets and technologies, effectively solving project portfolio planning problems at Intel is vitally important.
We have designed and implemented a decision support system that addresses the combinatorial complexities encountered in this process, while making
use of the business intuition that senior managers
have gained through years of experience in developing and marketing Intel products. We use modeling,
simulation, and multicriteria optimization to analyze
high-quality data to produce a relatively small set
of project portfolio recommendations that maximize
net present value (NPV), while respecting the available budget and project relationships. Conversely, we
provide a rich set of analysis and visualization tools
and easy-to-use what-if capabilities to enable senior
decision makers to apply their intuition and unique
perspectives to further evaluate our suggestions and
select the best portfolio.

Interfaces 45(5), pp. 391–408, © 2015 INFORMS

We have found that managing the interaction
between analytics and intuition is a very effective
method to produce superior project portfolios from
the perspective of NPV and efficient use of budgets.
All members of the decision-making team are able to
reach decisions with transparency in less time than
they previously required. Analytics directs decision
makers to the best portfolios in the combinatorial
search space. Intuition helps decision makers compare
the best portfolios against each other beyond the optimization metrics. The analytical tools support what-if
scenarios stimulated by intuition and drive the convergence to a single project portfolio. We developed
the new framework described in this paper with the
goals of producing the best analytics and enabling the
refinement of the results with the decision makers’
business intuition. Analytics informs intuition, and
intuition informs analytics; the result is superior business results.

The Planning Framework
We begin to describe our framework by providing a
definition of key entities and terms (Table 1).
We include three types of portfolio units (Table 2)
and six types of relationships between those portfolio
units (Table 3).
In the example in Figure 1, the ovals represent
projects, including three engineering projects (ENG-1,
ENG-2, ENG-3) and one marketing project (MKTG). Execution of the engineering projects results in two products represented by boxes (PROD-A and PROD-B).
ENG-1 is required to realize PROD-A and ENG-2 is required to realize PROD-B, as the double arrows show.
Portfolio unit
Relationship
Neighborhood
Scenario
Decision unit
Valuation
Portfolio

Any developmental venture that a company decides to
fund and develop moving forward.
A link between two portfolio units that affects the value
of one or both of them.
All portfolio units that affect the value of a specific
portfolio unit through a relationship.
A unique combination of all portfolio units in a
neighborhood that affect a portfolio unit.
A particular version of a portfolio unit as determined by
a specific scenario.
The end result of assigning values to all metrics of a
particular decision unit.
A collection of portfolio units.

Table 1: Our framework uses a variety of entities and terms that we define
here.

Sampath et al.: Decision-Making Framework for Project Portfolio Planning at Intel

393

Interfaces 45(5), pp. 391–408, © 2015 INFORMS

Project

A modeling construct that stores the engineering cost
information of a given venture. Projects have positive
spending and negative NPV.
A modeling construct that stores the manufacturing cost
information of a certain physical product or venture.
Products have zero spending and zero NPV.
A modeling construct that stores the volume and average
selling price information of a specific product in a specific
market. Prospects have zero spending and positive NPV.

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

Product

Prospect

Table 2: Our framework uses three types of portfolio units.

Required
Optional
AND
Soft OR
Hard OR
Influence

When one portfolio unit is required by another to be funded
and developed.
When one portfolio unit affects the value of another but is not
required.
When two or more portfolio units must all be funded or not
funded at all.
When two or more portfolio units are mutually exclusive and
picking one is optional.
When two or more portfolio units are mutually exclusive but
one must be picked.
When two or more Prospects influence market impacts
(synergistic, cannibalistic).

Table 3: Our framework uses six types of relationships between portfolio
units.

The single arrows show that ENG-3 is optional for
both products, although inclusion of ENG-3 enriches
both products. Prospects represented by diamond
shapes (C1 and C2) connect products to a market represented by hexagons (MARKET-X). Although
they are not portfolio units, markets are a construct
that stores information for associated prospects concerning the total available market and share of the

MARKET-X
MKTG
C1

C2

PROD-B

PROD-A

ENG-1

ENG-3

ENG-2

Figure 1: A simple example of a portfolio problem shows entities (projects
in ovals, products in boxes, prospects in diamonds, markets in hexagons) and relationships (required with double arrows, optional with single
arrows, influence with dashed arrow).

available market we believe we can capture. The
dashed arrow between prospects indicates that there
may be synergism (at least one of the respective products nets more value in that market than it would if
done without the other) or cannibalism (at least one of
the products nets a lesser value than it would if done
without the other) between products. The dotted line
indicates the neighborhood of prospect C1 including
all portfolio units connected to that prospect.
Because making a decision regarding the various
prospects to bring to a particular market uniquely
determines which products to manufacture, and
because we define products to have no NPV or
spending (these values are incorporated into corresponding projects or prospects), we typically do not
include products in our optimization formulations or
directly in the calculation of the statistics of portfolios. However, we do indirectly use the information
contained in markets and products to determine various statistics associated with prospects and projects
as described in The Simulation Phase section. We next
explain in detail the manner in which some portfolio
units affect the value of other portfolio units.
Within the neighborhood of a particular prospect,
considering the portfolio units included and the relationships between them, there are one or more decision units. In Table 4, all eight decision units in the
neighborhood of Prospect C1 contain PROD-A since
connecting that product to the market is the role
Discounted NPV
Decision unit

Change

1. C1: PROD-A + ENG-1
+ ENG-3 + C2 + MKTG
2. C1: PROD-A + ENG-1
+ ENG-3 + C2
(−MKTG)
3. C1: PROD-A + ENG-1
(−C2)
+ ENG-3 + MKTG
4. C1: PROD-A + ENG-1
(−ENG-3)
+ C2 + MKTG
5. C1: PROD-A + ENG-1
(−C2, −MKTG)
+ ENG-3
6. C1: PROD-A + ENG-1
(−ENG-3, −MKTG)
+ C2
7. C1: PROD-A + ENG-1
(−ENG-3, −C2)
+ MKTG
8. C1: PROD-A + ENG-1
(−ENG-3, −C2, −MKTG)

Cash flow
166
125
75
55
42
23
59
30

Table 4: We describe the eight decision units in the neighborhood of
Prospect C1 (Figure 1).

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

394

Sampath et al.: Decision-Making Framework for Project Portfolio Planning at Intel

of C1. They all contain ENG-1 as well, since ENG-1
is required to generate PROD-A. Conversely, ENG-3,
C2, and MKTG are optional individually and in combinations, in generating the different decision units.
Next, we need to value the NPV of the decision
units generated. In our framework, the NPV is a
function of a number of independent and correlated
variables, including revenue, volume, average selling
price (ASP), cost of goods sold, manufacturing costs,
product margins, total engineering costs, taxes, and
depreciation for various assets, and the number of
years until the portfolio unit will be implemented.
The NPV is the result of a sequence of operations
that may involve at each stage the estimation of one
or more of these variables. The information needed
in the calculation of the prospect’s NPV is associated
with the various prospects, products, and projects that
uniquely determine that decision unit.
For example, to calculate the NPV of the first decision unit of C1 (C1: PROD-A + ENG1 + ENG3 + C2 +
MKTG), we first calculate the revenue using the volume that PROD-A is expected to sell in Market-X and
its ASP. We then calculate the cost of goods sold using
the unit manufacturing cost associated with PROD-A
and the volume to be manufactured. We subtract the
taxes, depreciation, and engineering costs of the various projects involved in executing C1 and the cost of
goods sold from the revenue to obtain the free cash
flows. The sum of the discounted cash flows over the
time horizon of the planning window yields the NPV
of C1.
The NPVs of the projects are calculated in the same
way; however, they do not have other portfolio units
in their neighborhood in this simple example. Moreover, they only have engineering costs associated with
them and no volume, unit manufacturing costs, or tax
information. It is clear that a slight change to the value
of any one of these variables, which could be caused
by the presence or absence of another portfolio unit,
can propagate through the sequence of operations necessary to ultimately calculate the NPV. A small change
in a single variable early on in the procedure could
thus potentially and significantly affect the final result.
In Table 4, the NPV of the first decision unit of C1
(C1: PROD-A + ENG1 + ENG3 + C2 + MKTG) is $166.
In the second decision unit (C1: PROD-A + ENG1 +
ENG3 + C2), deciding not to execute the MKTG

Interfaces 45(5), pp. 391–408, © 2015 INFORMS

project causes the value of the NPV of C1 to drop to
$125. We explain this by assuming that the marketing campaign MKTG has a positive effect on the volume of goods sold of PROD-A in Market-X. Dropping
MKTG thus causes a drop in the NPV of prospect C1.
We see that the NPV of the third decision unit of C1
(C1: PROD-A + ENG1 + ENG3 + MKTG) reduces to
$75 when we decide not to execute C2 and, consequently, not to introduce PROD-B into Market-X. This
is because PROD-B positively affects the demand of
PROD-A sold in Market-X and, therefore, the volume
of goods sold of C1. An example of this would be
the case of a chipset and microprocessor sold together
in the same market. If the chipset and microprocessor have complimentary features, then eliminating the
chipset from the market would cause consumers to
demand less of the microprocessor. Conversely, dropping a product could have the opposite effect. Consumers faced with fewer choices could buy more of
the existing choice, thus increasing the volume sold
of the remaining prospects.
Deciding not to execute the ENG3 project in the
fourth decision unit of C1 (C1: PROD-A + ENG1 +
C2 + MKTG) causes the value of the NPV to drop
to $55. This occurs because the ENG3 project reduces
the unit manufacturing cost of P1, and dropping it
causes a decrease in the NPV of C1. Dropping both
C2 and MKTG in the fifth decision unit has an even
more debilitating effect on the NPV of C1, causing it
to drop to $42. We can similarly explain the drop in
the NPV value for the last three decision units of C1.
In this way, we define decision units for all portfolio
units. Given its rich neighborhood, C1 has eight decision units; see Table 4. Eng-1, Eng-3, and MKTG have
only one decision unit each because they do not have
any other units in their neighborhood. Their value
is, thus, not affected by the inclusion or exclusion of
other portfolio units (in that portfolio). In our simple
example, we would next generate and value all the
decision units in the neighborhood of C2. This would
set the stage to select the optimal portfolio of decision units to maximize Portfolio NPV, constrained by
the budget available. For the more complex problem
of project portfolio planning at Intel, which involves
hundreds of projects, we balance intuition and analytics with a four-phase process.

Sampath et al.: Decision-Making Framework for Project Portfolio Planning at Intel

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

Interfaces 45(5), pp. 391–408, © 2015 INFORMS

The first phase is the unique and challenging mapping phase in which analysts collect data and relationships about new and existing projects and products,
including ranges of values where appropriate. We
then identify hundreds of portfolio units by employing an optimization algorithm that generates only
valid decision units and explicitly expresses each.
The innovative simulation phase is the second
phase. Now that we have identified and explicitly
expressed each decision unit, we need to assign a
quantitative value to objective and constraint or resource metrics to perform the subsequent optimization. Because many of these variables have some level
of uncertainty associated with them, we run a Monte
Carlo simulation and average the results to obtain the
required metrics.
The third phase is optimization, which has been previously approached in different ways as reported in
the literature. Archer and Ghasemzadeh (1996) and
Krishnan and Ulrich (2001) present reviews of various project portfolio selection techniques and note that
when projects have many interdependencies, project
selection is typically best realized using integer optimization techniques. These techniques address the
quantifiable issues and, when used as the foundation
for a decision support system, can incorporate qualitative issues based on business-user intuition into the
decision-making process (Liberatore and Titus 1983).
Our optimization approach relies heavily on including
both quantitative and qualitative aspects necessary for
adoption by the Intel business community. Stummer
and Heidenberger (2003) present a technique to optimize a portfolio with many project interdependencies
using a three-stage approach to minimize the number
of projects that enter the optimization stage prior to an
interactive phase with decision makers. Our approach
includes all projects in the optimization stage, and
does not prematurely discard opportunities that seem
poor, but may actually be part of an optimal contribution. Dickinson et al. (2001) use a dependency matrix
to model project interdependencies and couple it with
a nonlinear integer programming model to optimize
project selection, but note that obtaining accurate estimates for the required matrix can be difficult and often
inaccurate as a corporate exercise. Mathieu and Gibson (1993) use a methodology for large-scale project

395

portfolio planning based on cluster analysis to manage risk in project portfolio management, but note low
acceptance by planning practitioners. Our approach
models the business by first visually mapping all the
projects involved, collecting from analysts estimates
for the various metrics associated with them, explicitly simulating various scenarios, and finally converting this nonlinear knapsack problem into an integer
program (IP).
The decision phase is the fourth phase; in it, senior
business managers, who use their intuition to select
a portfolio that achieves company goals, further evaluate the results of the optimization phase. A variety
of visualizations, novel and innovative incrementalvalue reports, and waterfall diagrams generated by
additional optimization techniques are available to
help them. This phase includes executing what-if scenarios, which may lead to more analytics and further pruning of solutions. The elimination-by-aspects
method, which we employ as a decision rule, was proposed by Tversky (1972) and discussed in Gigerenzer
and Selten (2002).
In this paper, we present an extensive framework
that details modeling the problem in a manner that
makes it intuitive to present to both analysts and
decision makers. We divide this framework into a
mapping section in which we detail an algorithm
that recursively solves a binary-integer linear program (BILP) to define the various decision alternatives from which to select, a simulation section in
which we assign each decision alternative relative
numeric attributes, an optimization section in which
we detail an algorithm that recursively solves a BILP
to generate an efficient frontier of feasible portfolios
that maximize the NPV, while minimizing budget
consumption, and a what-if section in which we detail
various algorithms we employ to generate additional
portfolios that decision makers may consider interesting to explore. In the optimization section, we also
present a novel binary-integer fractional linear programming model to maximize the ratio of conflicting
objectives and constraints using cuts, as detailed in
Nemhauser and Wolsey (1988). Figure 2 details the
entire process flow.

Sampath et al.: Decision-Making Framework for Project Portfolio Planning at Intel

396

Interfaces 45(5), pp. 391–408, © 2015 INFORMS

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

%NTER WITH PORTFOLIO UNITS RELATIONSHIPS AND DATA

-APPING PHASE

3IMULATION PHASE

Decision PHASE

/PTIMIZATION PHASE

%XIT WITH THE SELECTED PROJECT PORTFOLIO
Figure 2: The graphic shows the details of the process flow in our fourphase decision-making framework.

The Mapping Phase
Analysts typically work on their individual responsibilities for a few weeks before meeting together
to map out the company’s existing portfolio units
and identify all new portfolio units. Assumptions
are stated and viewpoints are shared until analysts
agree on all portfolio units, their relationships with
each other, and all alternatives. Then, quantitative
estimates on various portfolio unit metrics, such as
spending, headcount, expected sales volume, ASP,
time for demand to ramp-up, years at peak, and
any other metrics that are important to the decisionmaking process, are calculated. Analysts enter a low,
base, and high estimate for each variable, corresponding to the 10th percentile, median, and 90th percentile
values. They also record variable correlations and log
annotations capturing their assumptions.
Identifying all possible different decision units is
often difficult because a single portfolio unit may
interact with many others. Identifying all decision
units becomes arduous for even moderately connected portfolio units, because each optional relationship doubles the number of decision units. We
define the valuations for hundreds of portfolio units
by employing an optimization algorithm that generates only valid decision units and determines distinct
valuations for each. For a given portfolio unit surrounded by many neighborhood items, we wish to
discover all possible feasible combinations of these
portfolio units, thus determining the decision units
and then evaluating them.
We use an IP formulation to iteratively generate all
different decision units of a given portfolio unit; see
Appendices A and B. We then iterate the IP until all

valuations have been generated and no further valuations exist, as determined by finding no feasible
solutions for the IP. Each iteration of the IP includes
all cuts generated to that point. Aggregating all the
solutions from all iterations for all portfolio units, we
are able to define matrices that summarize the various
valuations and their corresponding decision units. We
use these matrices to constrain feasible combinations
of portfolio units when we optimize for various objectives as described in The Optimization Phase section.

The Simulation Phase
Now that we have identified and explicitly expressed
each decision unit, we need to assign a quantitative value to objective and constraint metrics to perform the subsequent optimization. To achieve this, we
record historical and expert input and use the data
to calculate decision-unit metrics to arrive at a valuation. The expected NPV (eNPV) of a decision unit
is derived via a financial calculation with a number
of different variables. Many of these variables may
have some uncertainty associated with them, as well
as a time dimension. We must, therefore, run a Monte
Carlo simulation to determine the eNPV and other
metrics.
Consider a simple estimate in which the profit
earned equals the number of units sold, multiplied by
the difference between the selling price and the manufacturing cost, minus the total engineering investment. Even for this simplified calculation, many of the
variables have a time dimension. Engineering costs
occur before any manufacturing costs are incurred,
and manufacturing costs are incurred before and during product sales. The volume sold in different periods will ramp up quickly, peak for a few periods,
and then gradually decline. The problem is, thus,
compounded if we wish to calculate the eNPV of a
decision unit, forcing the summation of the investment and (or) costs and (or) revenues in each period,
including the discount rate in the multiyear life cycle
of the decision unit.
Because of these complications, we use a Monte
Carlo simulation to estimate the NPV distribution.
Depending on the phase of project execution, the
engineering investment, manufacturing cost, sales
volume, and ASP will not be known with certainty

Sampath et al.: Decision-Making Framework for Project Portfolio Planning at Intel

and may best be modeled using distributions. The
distribution of overall profit is thus a convolution of
distributions that might not all belong to the same
family of distributions. The eNPV is equal to the sum
of the profits at various periods and is a random
variable itself.
Some variables may be estimated from historical
information. For example, engineering cost depends
largely on the number of engineers working on a
project over time. This can be inferred from a company’s long history of working on similar projects.
For typical projects that are underway, headcount
numbers for the remainder of the decision unit’s life
cycle can, thus, be estimated by extrapolating the
existing information until the end of the project; see
Figure 3.
Once all necessary information is at hand, simulations can be run to estimate the values for various
decision unit metrics at different stages of their life
cycle, and thus derive the appropriate valuation. It
is typical to sample the 10th percentile 30 percent of
the time, the 50th percentile 40 percent of the time,
and the 90th percentile 30 percent of the time. One
simulation run will then sample from the distributions (taking into account any correlations that may
exist between these variables and simulation runs of
other decision units) of each of these for each period
and derive a value for the profit at various periods. These time-dependent profits, when appropriately discounted and summed, will provide the eNPV
Actual number of heads used to date.
Estimated number of heads needed at future time
periods derived from an extrapolation of existing data.

Headcount

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

Interfaces 45(5), pp. 391–408, © 2015 INFORMS

50
45
40
35
30
25
20
15
10
5
0
0

2

4

6

8

10

12

Time periods
Figure 3: This example estimates future data from current data and historical experience, here for engineering headcount needed to finish a
project.

397

distribution. We typically run between 2,000 and 5,000
simulations and aggregate the results.
At this stage, the results are visualized, showing
analysts the distribution of the NPV, profit, and other
variables at various periods in case they want to
adjust some of their initial estimates. Once this cycle
has been repeated the required number of times,
and analysts are satisfied with their inputs, a quality
review team consisting of independent analysts and
senior managers reviews the data and especially the
assumptions. The review team may call for a revision
of assumptions and advocate changing the value of
other input parameters. Simulations are rerun until
the quality review board is satisfied.
We now have the quantitative metrics for the decision units that we can use as an objective to optimize
a portfolio that maximized NPV while constraining
the spending on engineering efforts in dollars, which
could represent a constraint with regard to a group’s
available budget.

The Optimization Phase
Once all the decision units have been simulated and
their eNPV, spending, and other metrics have been
determined, we run an iterative optimization algorithm to successively generate all nondominated portfolios in terms of eNPV versus next year’s spending
budget. We define a nondominated portfolio as a
combination of decision units for which no other
combination exhibits a lower spending and a higher
eNPV. Appendices A and C show the formulation
of the algorithm used to generate all nondominated
solutions. To generate the first portfolio with the
largest possible eNPV, we start by removing any
spending constraint. We then iteratively generate all
nondominated solutions by including the previous
solution as a cut and adding a spending cut to ensure
that each solution generated is nondominated. We
continue this procedure until we have generated all
nondominated portfolios.
A subset of the nondominated portfolios generated
defines an outer convex hull or the efficient frontier. We identify the portfolios on the efficient frontier
algorithm via a fast algorithm by starting with the
portfolio that has the lowest spending. We then calculate the slope to every other portfolio in the set and

Sampath et al.: Decision-Making Framework for Project Portfolio Planning at Intel

identify the portfolio that provides the highest slope
in the objective function space. We select this point
and repeat the procedure until we can identify all the
points that generate the efficient frontier and present
them for initial exploration and analysis.
Note that other metrics for optimization can be
implemented or incorporated as constraints if they are
of interest to senior business unit managers. These
may include, but are not limited to, meeting specified
revenue targets, maximizing the volume of the company’s portfolio, minimizing and (or) meeting headcount specifications, and maximizing the efficiency or
the ratio of the eNPV divided by the total spending
over all years for a portfolio.
At this point in the process, most of the original analysts who mapped the portfolio units are no
longer involved. Their role is to assemble a highquality set of inputs for the optimization. The decision
makers involved in the next phase are the senior managers of the business unit and their senior financial
analysts who have more global business intuition to
incorporate into the generated portfolios. In the next
section, we explain in detail the manner in which we
manage the incorporation of their intuition, including
additional optimization techniques that support intuitive what-if analyses in the decision-making process.

The Decision Phase
The final selection is made by comparing and contrasting various portfolios generated in the previous
step, which have been identified as desirable, and discussing the implications of implementing any portfolio on the future of the company. The analytics we
describe in the previous sections inform the business
intuition through a wide variety of reports supplied
by the system, and the intuition informs the analytics through the execution of various what-if scenarios
requested by the decision makers.
The efficient frontier visualization is an easy way
to survey the nondominated portfolios generated in
a manner that the decision makers can understand.
Figure 4 informs decision makers about the least
amount of budget they need to spend to achieve
a desired portfolio eNPV and the most eNPV they
can achieve at their current budget. If a currently
approved but relatively inefficient portfolio or future

Interfaces 45(5), pp. 391–408, © 2015 INFORMS

Portfolio NPV

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

398

Optimal portfolios
Intuitive “what-ifs”
Previous plan

Efficient
frontier

Budget

Portfolio spending
Figure 4: A visualized efficient frontier shows a variety of portfolios,
including optimal portfolios that our system generates automatically,
what-if portfolios that our system generates and that include the intuition
of senior managers, and the portfolio or plan used prior to the current
planning session.

funding strategy exists, the efficient frontier shows the
increase in value the company can potentially achieve
by switching to a lower-spending, higher-eNPV portfolio. The results of business-intuition-driven whatif analyses are initially displayed on the efficientfrontier visualization.
Next, a set of basic reports are generated to guide
the iteration between analytics and intuition. Selecting
a specific portfolio on the efficient frontier generates
the listing of all projects and products in the selected
portfolio. Selecting two portfolios generates a report
that indicates the similarities and differences between
the portfolios with respect to the projects and products included.
A variant of these reports shows a product road
map, plotting time on the X-axis and markets on the
Y -axis with bars spanning introduction to end of life
for each product in the model. Colored bars indicate
inclusion in the portfolio selected, and grey bars indicate exclusion. If products are excluded (included)
that disagree with intuition, a what-if analysis can
be run by forcing those products in (out) as a constraint in our basic IP optimization model; this forces
the entity in question to be included (excluded) in the
portfolio that resulted from solving the IP and presenting it to the decision makers for review.
Although resource-pool size and skill-set mix can
be included in our formulation as constraints, a popular option for decision makers is to get a resource
view through reports rather than optimization. Selection of a portfolio generates an estimate of the
resources required for execution; using their historical

Sampath et al.: Decision-Making Framework for Project Portfolio Planning at Intel

399

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

Interfaces 45(5), pp. 391–408, © 2015 INFORMS

knowledge of projects, the decision makers can judge
whether the required increase (decrease) in size and
change of skill sets to realize a portfolio are practical.
We have generated more sophisticated reports to
inform decision-makers’ intuition in the later stages
of portfolio selection. Here, we describe three of the
most useful reports; each requires the development
and use of further optimization models.
The first example is the budgets-efficiency report.
Optimizing overall efficiency over time is usually one
of the most important objectives for senior managers.
It is a difficult task, because spending budgets are
often hard to estimate beyond the near future, which
may comprise the next year or two; however, companies may still want to increase the amount of benefit
realized for every dollar spent over the entire planning horizon. An important consideration arises when
a portfolio that looks promising with respect to next
year’s budget contains multiyear projects that have
back-loading spending causing that attractive portfolio to exceed budget in the out years. Optimizing efficiency over many years to generate portfolios within
certain spending limits in the current year is mathematically challenging, because it requires maximizing
a ratio objective of two linear functions. This involves
converting the BILP (see Appendices A and C) into
a fractional linear program. Not only is this challenging to formulate, but the difficulty is compounded
by the binary nature of the variables. Transforming
the BILP into a suitable fractional linear program also
makes cutting off previous solutions difficult. It invalidates our previous cuts because, although the value
of the new variables still toggles between two distinct values, the higher of those values is no longer 1.

As shown in Appendices A and D, we overcome this
problem by converting the original binary integers
into a form suitable for the fractional program and
introducing additional indicator variables that enable
us to continue implementing the cuts that are critical
for our basic optimization approach (Appendices A
and C).
Another example is the incremental value report
that compares the difference in value of a particular
portfolio when a selected portfolio unit is dropped
(added) in response to an intuitive what-if analysis.
Each portfolio unit has many alternate decision units,
each of which is determined by a specific scenario and
may cost and be worth different amounts; therefore,
the value of a portfolio unit to a portfolio is not readily apparent. A portfolio unit may define scenarios for
a variety of other portfolio units. Thus, dropping a
portfolio unit from a portfolio based on business intuition may have a cascading effect on a variety of other
portfolio units and cause them to be worth a different
amount based on the relationships defined between
all the units. We explain our incremental value algorithm in Appendices A and E.
In PORTFOLIO#1 in Figure 5, when portfolio unit D
is dropped in a requested what-if analysis, this affects
the values of all other portfolio units in the portfolio. Our algorithm provides a feasible PORTFOLIO#2,
which does not contain portfolio unit D, with minimal changes to PORTFOLIO#1. As Figure 4 illustrates,
when portfolio unit D is dropped, PORTFOLIO#1 is
worth less because of the loss of revenue from portfolio unit D; however, portfolio unit B is also now worth
less because portfolio unit D contained a technology

PORTFOLIO#1 total NPV + 51,326
Drop unit D
Devalued unit B

PORTFOLIO#2 total NPV + 44,057

– 1,844

– 5,425

DELTA – 7,269

Figure 5: This typical incremental value report shows the consequence of dropping unit D and the unintended
devaluation of unit B caused by dropping unit D on portfolio NPV.

Sampath et al.: Decision-Making Framework for Project Portfolio Planning at Intel

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

400

Interfaces 45(5), pp. 391–408, © 2015 INFORMS

that would complement the sales of portfolio unit B.
Differencing the eNPV values between these portfolios gives us the incremental value for the value lost
from removing that portfolio unit from the original
portfolio, in addition to the changes in value wrought
to all the remaining portfolios units in the portfolio.
These relatively simple comparisons enable decision
makers to have meaningful discussions on the value of
various investment options or portfolio units in their
portfolios.
The waterfall report, which allows decision makers
to visualize the changes they would need to make to
migrate between any two portfolios, provides a more
sophisticated comparison. One common example is
from an existing portfolio of record that resulted from
a previous planning cycle to a new and improved
portfolio suggested in the current planning cycle. Utilizing the same formulation used to generate the
incremental value report, we iteratively add or drop
portfolio units, recording the largest negative change
at each step and designating that as the next step
in the trajectory to generate the waterfall report; see
Appendices A and F. Part of the challenge of building
a waterfall diagram is the necessity to maintain a feasible portfolio at each step. If an intermediate step is
not a feasible portfolio, with respect to satisfying all

relationship and budgetary constraints, it is not truly
representative of a migration plan from one portfolio
to another.
Figure 6 shows a typical report from executing
the waterfall algorithm to morph PORTFOLIO#3 into
PORTFOLIO#4. This report indicates that by dropping portfolio units M, N, O, P, Q, and R, and adding
portfolio units S, T, U, V, and W, we obtain a net gain
in portfolio value. Waterfall reports generated from
intuitive what-if analyses lead to important discussions among decision makers looking to choose an
optimal portfolio.
At this stage in the business process, a few valuable
portfolios have been identified, in some cases using
multiple criteria. Based on the preference of our senior
executives, we use an elimination-by-aspects method
to prune the existing solutions to choose the plan of
record. In using this method, as Tversky (1972) proposes, decision makers prioritize objectives and constraint metrics and then eliminate portfolios at each
step as they traverse the list of objectives in a lexicographical fashion. The decision makers identify and
discuss portfolio units until they select a final portfolio upon which they can all agree.
Various aspects can be chosen and visited in a
number of sequences depending on business goals;

PORTFOLIO#3 total NPV + 69,429
– 475
– 78
– 75

Drop unit N

Drop unit M

Drop unit O

Drop unit P
Drop unit Q

DELTA
+ 192

Drop unit R
+ 41

Add unit S
+ 50

Add unit T
+ 78

Add unit U
+ 119

Add unit W

Add unit V
+ 532

PORTFOLIO#4 total NPV + 69,621

Figure 6: This typical waterfall diagram shows the change in portfolio eNPV as six portfolio units are removed
and five portfolio units are added in morphing PORTFOLIO#3 into PORTFOLIO#4.

Sampath et al.: Decision-Making Framework for Project Portfolio Planning at Intel

401

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

Interfaces 45(5), pp. 391–408, © 2015 INFORMS

Portfolios close to the intersection of the efficient frontier and the budget lines.
Eliminating portfolios not having a strategic product for our best customer.
Eliminating portfolios needing unrealistic skill set morphs.
Eliminating portfolios with increasing spending over time.
Selecting the best balance across all business units.

16

11

7

4

1

Figure 7: This example illustrates pruning the set of promising portfolios aspect by aspect to get to the final
decision, specifically, by budget, then strategic product inclusion, engineering skill-set availability, spending
over time, and balance over business segments.

Figure 7 illustrates one plausible example of this process. After running our analytics process and using
our reports and what-if analysis tools, the senior managers select the 16 most promising portfolios relative
to the corporate goals they are striving to achieve.
Careful consideration of strategic products for key
customers, some of which the optimization might
have rejected because of low eNPV, reduces the set
to 11. The feasibility of finding the number of engineers required to implement the portfolio, and especially the necessary skill-set mix, trims the set to seven
members. Using the multiyear-spending what-if analysis tool to detect portfolios with unrealistic spending into the future eliminates three more portfolios.
Assuming the decision makers involved in the process represent different divisions of the business being
planned, the final selection is made based on balance
of investment and return across the divisions, strategic versus tactical, and established business versus
new ventures.

The Implementation
To implement this process, we need a suite of tools
that enables analysts and decision makers to realize
the four phases as shown in Figure 2. At Intel we
built such a suite of tools, which we called Voyager.
In the mapping section of the suite, analysts can draw
portfolio units and their relationships, and input all
relevant details. Our simulation section automatically
generates the necessary decision units and their valuations and feeds them to the optimization portion

of the suite. Here analysts and decision makers can
set up optimization parameters and proceed to investigate and identify possible portfolios. The decision
portion of the suite allows for easy navigation of the
solution space through intuition-based what-if scenarios and building reports that aid in portfolio selection.
We wrote the entire suite in C#, and all visualizations utilize the Windows Presentation Framework
(WPF) controls. For each optimization, we used IBM’s
ILOG Concert/CPLEX solver, populating the necessary data structures in C# before calling the CPLEX
solver; the solver returns the results in C# to be
processed and used as required in interactive displays or reports. The suite is deployed on a remote
server and is accessed through a client on analysts’
machines, removing most client-machine dependencies. The server’s specifications include quad-core
Intel i7 2nd generation processors, and 8GB RAM
with 100GB of disk space. For problem sizes of
approximately 200 portfolio units (which, in turn,
yielded approximately 500 decision units), it takes a
few days of coordinated planning by the business
team to enter all the requisite information into the
mapping step. It takes roughly two hours for the simulator to value a problem of this size and make the
necessary details available to the optimization engine.
The efficient frontier is then built in approximately
30 seconds. Performing what-if scenarios, such as
forcing portfolio units in or out and comparing portfolios, are fast operations that can be performed in
a few seconds. Building the incremental-value report
on a given portfolio and the waterfall report between

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

402

Sampath et al.: Decision-Making Framework for Project Portfolio Planning at Intel

two portfolios both take about 30 seconds to generate.
These short turnaround times are vital to maintain the
momentum of a decision meeting among senior Intel
decision makers.

The Business Results
The funding process that Intel had used for years
prior to deploying Voyager relied on data-driven
advocacy. Leaders of individual business segments
assembled the case for each of their projects based
on market projections for the resulting product(s) and
estimates of engineering and manufacturing costs.
Many of the projects in their business segments
were in some stage of execution; others were new.
The projects were put in a priority order based on
expected cost and return or perceived strategic importance of the resulting products. The lists from each
individual segment were passed up to the next level
of management for discussion and merged into a master list of projects for the larger business group. Dollars were allocated to projects starting at the top of the
master list until the budget was exhausted and the cut
line drawn. Although projects well above (below) the
cut line were considered in (out) of the plan of record,
debates continued for projects just below (above) the
cut line, sometimes resulting in additional budget
requests up to the next level of management.
In late 2011, the vice president of an individual
business segment in Intel’s Data Center Group (DCG)
first used our decision support tool as an aid to the
standard advocacy process. DCG’s senior vice president was intrigued by the approach and mandated
its trial use as an advocacy aid by a number of (but
not all) DCG business segments for a plan cycle in
mid-2012.
The positive feedback from this trial led to a defining experiment, including all DCG business segments,
in late 2012. The advocacy process was executed without the aid of our tool. Using the same input data,
a set of project portfolios was built using our tool
without the aid of advocacy. Direct comparison was
achieved by forcing our tool to reproduce the same
product road map that resulted from the standard
advocacy process.
The small set of project portfolios produced by our
approach exhibited an overall eNPV roughly 10 percent higher than the project portfolio produced by the

Interfaces 45(5), pp. 391–408, © 2015 INFORMS

standard advocacy process. Furthermore, because we
could directly compare portfolios in our system, we
avoided debate that might normally have included
discussions of many projects. It was relatively easy to
see that our approach identified three large projects
that were being overfunded and six smaller projects
that were being underfunded or prevented from starting in the advocacy-generated portfolio.
Studies of our reports provided decision makers
with a broader and deeper analysis than they had
previously been able to access. The use of our model
enabled them to satisfactorily address a number of
their data-related questions. An evaluation of a number of intuition-based what-if analyses demonstrated
the soundness of our recommendations. In only a few
days, the managers of the three large projects, which
we had questioned, were able to provide options
for sufficiently trimming their budgets to fund the
six smaller projects. Somewhat surprisingly, the managers who were requested to give back funding did
so willingly once they were shown the rationale
supported by the overall analysis. (They had been
engaged early in the process to supply data, but only
for their own projects.)
Based on these positive business results and the
transparency and speed with which the results were
realized, for the past two years, DCG has adopted
our approach as the principal project portfolio planning tool. Although we continue to improve datacollection activities to provide input to the system
and incrementally refine and expand the set of output
reports for the decision makers, the core mathematics have been robust, requiring only minor modifications to cope with new business scenarios. Perhaps
the most important contribution of our decision support system has been to show the highest levels
Intel management that the appropriate give-andtake interaction between analytics and intuition produces higher-quality business solutions than either
approach can produce individually. The resulting plan
is superior in terms of NPV return for the budget dollars invested; in addition, decision makers engage in
higher-quality debate (their feedback to the Voyager
team) and arrive at consensus in a much shorter time
(as measured). Given a receptive but tough-minded

Sampath et al.: Decision-Making Framework for Project Portfolio Planning at Intel

403

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

Interfaces 45(5), pp. 391–408, © 2015 INFORMS

group of decision makers, analytics can inform intuition, and intuition can inform analytics to the benefit
of the business.
Review of the DCG results at the corporate level
has exposed other product groups and their senior
management and finance personnel to the Voyager
tools and processes. The combination of improved
results and a transparent, fast process has caught
the attention of many Intel decision makers. Voyager is currently under preliminary evaluation for use
in Intel’s largest and oldest product group, the Personal Computer and Client Group (PCCG), as well
as Intel’s newest product groups, the Mobile Communications Group (MCG) and the Internet of Things
Group (IoTG).

Number of portfolio units that define the neighborhood for portfolio unit i.
Di Total number of valuations for portfolio unit i, i ∈
1     N .
pji eNPV of valuation j of portfolio unit i, i ∈ 1     N ,
j ∈ 1     Di.
sji Cost of valuation j of portfolio unit i · i ∈ 1     N ,
j ∈ 1     Di.
Bmin Minimum budget that a portfolio must exceed.
Bmax Maximum budget that a portfolio cannot exceed.
By Spending of portfolio y.
Uji Any portfolio benefit metric of valuation j of portfolio unit i, i ∈ 1     N .
Rji Any portfolio cost metric of valuation j of portfolio
unit i, i ∈ 1     N .
M Reciprocal of minji=ji 
 Rji >0 i∈1N  j∈1Dj Rji .
y a Portfolio a with respect to which we wish to determine the incremental value of various portfolio units.
Zji Any benefit or cost metric of valuation j of portfolio
unit i.
yia Portfolio unit i of portfolio a.
a
Valuation j of portfolio unit i of portfolio a.
xji


1 if portfolio unit k is included in
decision unit j of portfolio unit i
Cji k =

0 otherwise.


1 if portfolio unit k is excluded in
decision unit j of portfolio unit i
Eji k =

0 otherwise.
mi

Appendix A. Definitions of Variables Used in
the Appendices
In this appendix, we define the mathematical models
referred to in this paper. For further discussion on the mathematics behind the framework, see Sampath et al. (2016).
Parameters
Ii Set of portfolio units that are required for portfolio
unit i, i ∈ 1     N .
Ei Set of portfolio units that must be excluded when
executing portfolio unit i, i ∈ 1     N .
Mi Set of mutually exclusive portfolio units, one and
only one of which is required to choose portfolio
unit i, i ∈ 1     N .
Qi Set of mutually exclusive portfolio units, one and
only one of which may be chosen along with portfolio unit i, i ∈ 1     N .
N Total number of portfolio units.

Input : A portfolio unit, its
neighborhood and relationships

Appendix B. Generating All Possible
Decision Units (Figure B1)

Decision Variables
wji Binary variable indicating if portfolio unit j is included in the current scenario of portfolio unit i,

M 1: Maximize:

Set up model

Subject to:

Add cut to model
x ′ is the previous scenario

Yes

: Pr

Output : All possible decision units
of a given portfolio unit.

oce

Let x ′ be the solution to M 1

ss o

utp

No: Stop

ut

Call solver
Does solution x′ exist?

Figure B1: Given a portfolio unit and its neighborhood, including all relevant relationships, we use this method
to compute all possible decision units of the portfolio unit, as we use in the mapping phase of our framework.

Sampath et al.: Decision-Making Framework for Project Portfolio Planning at Intel

404

Interfaces 45(5), pp. 391–408, © 2015 INFORMS

wji =

where


1

if portfolio unit j is included in the
current scenario of portfolio unit i


0 otherwise

If x is a valid combination of neighborhood items that
clearly define a unique decision unit for the given portfolio
unit, we add the following cut to the subsequent iteration.
Both Cji k and Eji k are outputs of the mapping phase.

Appendix C. Algorithm to Generate
Nondominated Solutions (Figure C1)
Decision Variables
xji Binary variable indicating if valuation j of portfolio
unit i is included in the current portfolio,



1 if valuation j of portfolio unit i is
included in the portfolio
xji =

0 otherwise

Binary variable indicating if portfolio unit i is included
in the current portfolio,
where

1 if portfolio unit i is included
yi =
0 otherwise.
yi

Appendix D. Algorithm to Explore
Spending Durations (Figure D1)
If the original formulation defined in Appendix C had a

total of q decision variables given by q = Ni=1 Di+N and

-  -AXIMIZE

)NPUT  !LL
DECISION UNITS
AND ASSOCIATED
ATTRIBUTES

3UBJECT TO
3PENDING CONSTRAINTS

/NE VALUATION5NIT CONSTRAINT

3ETUP MODEL

)NCLUSION CONSTRAINT

%XCLUSION CONSTRAINT

!DD CUT

,ET Y BE THE SOLUTION TO - 
E

LV

3O

%XCLUDE PREVIOUS SOLUTION CUT
∈
∈

9ES

$OES Y ′  EXIST

3PENDING CUT
.O

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

here

/UTPUT  /RDERED SET OF
NONDOMINATED PORTFOLIOS

#ALCULATE CONVEX HULL OF
GENERATED PORTFOLIOS

Figure C1: Given all decision units including their associated attributes, we use this method to compute an
ordered set of all nondominated portfolios and the external convex hull of the set, as we use in the optimization
phase of our framework.

Sampath et al.: Decision-Making Framework for Project Portfolio Planning at Intel

405

Interfaces 45(5), pp. 391–408, © 2015 INFORMS

5NDER THE ASSUMPTION THAT THE FEASIBLE
REGION IS NONEMPTY AND BOUNDED

)NPUT

4RANSFORM

3UBJECT TO
7HERE

∈

3ET UP MODEL
3OLVE

$OES y′ EXIST

.O

M  -AXIMIZE
3UBJECT TO 4HE CONSTRAINTS IN M  IN !PPENDIX  AND x

S
9E

.O

∈

T

 CU

D
!D

∈
∈

S

∈
∈

∈

∈

2ECORD THE SOLUTION y′
3ET UP THE FOLLOWING CUT
∈

∈
∈

.EED MORE
SOLUTIONS

9E

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

-AXIMIZE

∈
∈

∈
∈

∈

∈

/UTPUT 3ET OF ADDITIONAL PORTFOLIOS

∈

∈

,ET y ′ BE THE SOLUTION TO M 

Figure D1: The algebraic formulation we use to explore multiple-year funding scenarios helps us to avoid picking
a portfolio that looks attractive in year 1, but that contains projects that have high spending in later years, which
would cause the portfolio to exceed budget.

the A b matrices represent all the constraints in the original optimization, with w being a q × 1 column vector representing the original decision variables, x11      xN DN  
y1      yN T , p representing a q × 1 vector with the corresponding eNPVs for variables x11      xN DN   and zero
otherwise, then the original formulation can be represented as
Maximize: pT w
subject to: Aw ≤ b

where: w ∈ 0 1q 

If we wish to generate additional portfolios to improve
the efficiency based on two metrics, (e.g., eNPV and total

spending over 10 years), each represented by q × 1 vectors
U and R (with nonzero values only for decision units and
zero for other variables), then we have the following.
Decision Variables
xji Same as Appendix C.
yi Same as Appendix C.
z
Linear variable equal to the denominator value of the
xji
objective if valuation j of portfolio unit i is included
in the portfolio and zero otherwise,
where



1 N Di

z

Rji xji
+   if xji = 1
z
xji =
i=1 j=1


0 otherwise.

Sampath et al.: Decision-Making Framework for Project Portfolio Planning at Intel

406

Interfaces 45(5), pp. 391–408, © 2015 INFORMS

Linear variable equal to the denominator value of the
objective if portfolio unit i is included in the portfolio
and zero otherwise,
where

1


 if yi = 1

 N Di
z
i=1
j=1 Rji xji +  
yiz =



0 otherwise.

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

yiz

t

Linear variable equal to the reciprocal of the sum of
N Di
z
i=1
j=1 Rji xji +  .

We achieve this mixed-integer linear formulation via the
Charnes-Cooper transformation (Charnes and Cooper 1962)
and add the integer cuts used in Figures B1, C1, and D1. We
can include any additional spending constraints and cuts to

the xji and the yi variables and generate multiple portfoz
lios, while still optimizing the ratio using the xji
and the yiz
variables, which are continuous. The spending constraints
from the original formulation are respected (Appendix C),
but solutions are maximized for the new objective. This formulation utilizes the fact that upon solution, all nonzero
z
values of the xji
and the yiz variable will be equal to t,
because the original formulation consisted entirely of binary
variables.

Appendix E. Incremental-Value Algorithm
(Figure E1)
We would change to yi = 1 to see the effects of adding
portfolio unit i’ to the current portfolio.

)NITIALIZE 

)NPUT  3TARTING PORTFOLIO  
DESIRED METRIC 

3ET
UP
MODEL

&OR EACH

3OLVE - -INIMIZE
3UBJECT TO
/UTPUT ,IST OF CHANGES IN
VALUE      FOR ALL REMAINING
UNITS IN THE PORTFOLIO WHEN
PORTFOLIO UNIT I IS DROPPED AND
THE OVERALL INCREMENTAL VALUE
OF THE UNIT  

,ET Y ′ BE THE SOLUTION TO -.

9ES

.O

3OLVE


%ND OF LIST
DO

 , FOR



, FOR

DO



2ECORD SOLUTION
UPDATE 



END , FOR

 END , FOR

Figure E1: The algebraic formulation we use to construct the list of all changes associated with the intuitionbased elimination from a portfolio of a particular unit is necessary, because eliminating any unit can have a
variety of nonobvious impacts on other units.

Sampath et al.: Decision-Making Framework for Project Portfolio Planning at Intel

407

Interfaces 45(5), pp. 391–408, © 2015 INFORMS

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

)NPUT 3TARTING PORTFOLIO
%NDING PORTFOLIO 
$ESIRED METRIC

$ETERMINE  THE LIST OF PORTFOLIO

AND
UNITS SUCH THAT
$ETERMINE V THE LIST OF PORTFOLIO

AND
UNITS SUCH THAT



/UTPUT  -IGRATION PLAN FROM
! TO " WITH A FEASIBLE
portfolio at every step, i.e., an
ORDERED SET OF PORTFOLIOS 9
.O

9ES
%ND OF LIST

$O NOTHING SINCE
NO SOLUTION EXISTS 

.O
Record the solution
Add to Y,
Update y c with y '
Update ߜ and ߙ

$ETERMINE    THE LIST OF PORTFOLIO

AND
UNITS SUCH THAT
$ETERMINE  THE LIST OF PORTFOLIO

AND
UNITS SUCH THAT

9ES

$OES Y ′
EXIST

TUP
3E DEL
MO

for t <    +    do
P D4i5
P
Solve M 5: Minimize: Ni= 1 j= 1 Zj4i5xj4i5
Subject to: M 2, unit inclusion,
exclusion
P
P constantsP
P
− i∈  yic + i∈  yic − i∈  yi = t
i∈  yi
P
 i∈  yi = 0
P
i∈  yi = 1
3OLVE
Let y 0 be the solution to M 5

Figure F1: The algebraic formulation we use to explore the step-by-step transformation of one portfolio of interest
into another is important because, when comparing two portfolios, we must understand the step-by-step addition
and subtraction of units required to convert one portfolio to the other.

Appendix F. Waterfall Algorithm (Figure F1)
The set of portfolios Y in essence describes the set of portfolios as a feasible migration from y a to y b .

References
Archer NP, Ghasemzadeh F (1996) Project portfolio selection techniques: A review and a suggested integrated approach. Internat. J. Project Management 17(4):207–216.
Charnes A, Cooper WW (1962) Programming with linear fractional
functionals. Naval Res. Logist. Quart. 9(3–4):181–186.
Dickinson MW, Thornton AC, Graves S (2001) Technology portfolio
management: Optimizing interdependent projects over multiple time periods. IEEE Trans. Engrg. Management 48(4):518–527.
Gigerenzer G, Selten R (2002) Bounded Rationality: The Adaptive Toolbox (MIT Press, Boston).
Krishnan V, Ulrich KT (2001) Product development decisions: A
review of the literature. Management Sci. 47(1):1–21.
Liberatore MJ, Titus GJ (1983) The practice of management science
in R&D project management. Management Sci. 29(8):962–974.
Mathieu RG, Gibson JE (1993) A methodology for large-scale R&D
planning based on cluster analysis. IEEE Trans. Engrg. Management 40(3):283–292.
Nemhauser GL, Wolsey LA (1988) Integer and Combinatorial Optimization, Vol. 18 (John Wiley & Sons, New York).
Sampath S, Gel E, Fowler J, Kempf K (2016) Mathematical modeling of a decision framework for project portfolio planning.
Working paper, Arizona State University, Tempe, AZ.

Stummer C, Heidenberger K (2003) Interactive R&D portfolio analysis with project interdependencies and time profiles of multiple objectives. IEEE Trans. Engrg. Management 50(2):175–183.
Tversky A (1972) Elimination by aspects: A theory of choice. Psych.
Rev. 79(4):281–299.

Siddhartha Sampath is a PhD candidate in the industrial
engineering program at the School of Computing, Informatics, and Decision Systems Engineering at Arizona State University. He is a member of the Decision Engineering team
at Intel Corporation in Chandler, Arizona. He is a member
of INFORMS.
Esma S. Gel is an associate professor of industrial engineering at the School of Computing, Informatics and Decision Systems Engineering at Arizona State University. Her
research focuses on the design, control, and management
of operations in various settings, with emphasis on manufacturing and supply chain systems, business and logistics
processes, and healthcare systems. Dr. Gel’s work has been
funded by the NSF, as well as Intel and Mayo Clinic. She is
an active member of INFORMS, IIE, and ASEE.
John W. Fowler is a professor and chair of the W. P.
Carey Department of Supply Chain Management at Arizona State University. His research interests include modeling, analysis, and control of manufacturing and service
systems. He is a fellow of the Institute of Industrial Engineers, editor of IIE Transactions on Healthcare Systems Engineering, co-editor of Journal of Simulation, an associate editor

Downloaded from informs.org by [149.169.145.169] on 09 June 2017, at 22:54 . For personal use only, all rights reserved.

408

Sampath et al.: Decision-Making Framework for Project Portfolio Planning at Intel

for IEEE Transactions on Semiconductor Manufacturing, and
former INFORMS vice president of chapters/fora.
Karl G. Kempf is a senior fellow and director of decision engineering at Intel Corporation in Chandler, Arizona,
where his responsibilities for developing decision support

Interfaces 45(5), pp. 391–408, © 2015 INFORMS

tools span from strategic product design to tactical supply
chain execution. He is a member of the National Academy
of Engineering, an INFORMS Fellow, a fellow of the IEEE,
and an adjunct in the School of Computing, Informatics and
Decision Systems Engineering at Arizona State University.

