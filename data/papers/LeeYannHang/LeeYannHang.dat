Efficient Data Race Detection for C/C++ Programs Using Dynamic Granularity
Young Wn Song and Yann-Hang Lee
Computer Science and Engineering
Arizona State University
Tempe, AZ, 85281
ywsong@asu.edu, yhlee@asu.edu

Abstract—To detect races precisely without false alarms, vector
clock based race detectors can be applied if the overhead in
time and space can be contained. This is indeed the case for the
applications developed in object-oriented programming
language where objects can be used as detection units. On the
other hand, embedded applications, often written in C/C++,
necessitate the use of fine-grained detection approaches that
lead to significant execution overhead. In this paper, we
present a dynamic granularity algorithm for vector clock
based data race detectors. The algorithm exploits the fact that
neighboring memory locations tend to be accessed together and
can share the same vector clock archiving dynamic granularity
of detection. The proposed heuristic for sharing vector clock is
simple but robust, can result in performance improvement in
time and space, and is with minimal loss in detection precision.
The algorithm is implemented on top of FastTrack and uses
Intel PIN tool for dynamic binary instrumentation.
Experimental results on benchmarks show that, on average,
the race detection tool using the dynamic granularity
algorithm is 43% faster than the FastTrack with byte
granularity and is with 60% less memory usage. Comparison
with existing industrial tools, Valgrind DRD and Intel
Inspector XE, also suggests that the proposed dynamic
granularity approach is very viable.
Keywords-Race detection; Concurrent bug; Multithreaded
programs

I.

INTRODUCTION

Most embedded applications are constructed with
multiple threads to handle concurrent events. Threads are
synchronized for proper sharing of resource and data.
Unfortunately, it is easy to misuse synchronization
operations in multithreaded programming and threads may
be vulnerable to race conditions and deadlocks. Ever
increasing demands of multi-core processors aggravate this
problem not only for embedded applications but for general
desktop applications.
A data race occurs when a memory location is accessed
concurrently by two different threads and at least one of the
accesses is a write. Data races are hard to reproduce, find,
and fix since a data race may only occur in a particular
execution of the program and the race does not necessarily
always cause observable errors in the program execution.
Over the past few years, several techniques have been
developed to detect data races. Static analysis techniques [8,
20, 26] consider all execution paths for possible data races
providing a better detection coverage than dynamic

techniques but they suffer from excessive number of false
alarms. On the other hands, dynamic techniques detect data
races when execution paths are exercised and they largely
fall into two categories: LockSet algorithms [23, 27] and
happens-before algorithms [7, 19, 21, 22, 25]. In Eraser’s
LockSet algorithm [23], data races are reported when shared
variable accesses violate a specified locking discipline, i.e.,
the variable is not protected by the same lock consistently.
For a given execution path, checking a lock discipline
enables Eraser to detect potential data races as well as ones
that actually happened in the program execution. Eraser may
report many false alarms which hinder developers’ focus on
fixing real problems. Eraser may also report false alarms by
not being able to recognize synchronization idioms, e.g., a
semaphore implementation using mutex locks.
Happens-before detectors are based on Lamport’s
happens-before relation [12] and don’t report false alarms as
the approach only checks any happens-before relation that
actually occurs during the given execution paths. Based on
the execution of a multithreaded program, a partial ordering
of memory and synchronization operations can be defined. A
pair of accesses to the same variable is concurrent when
neither of the accesses happens before the other. The
happens-before relation is realized by the use of vector clock
[5] and there are largely two happens-before detection
methods based on how shared accesses are represented and
compared for the detection.
In the first method [21, 22, 25], a segment is defined as a
code block between two successive synchronization
operations and shared memory accesses are collected in a
bitmap for each segment. In each thread a vector clock is
collected to uncover any concurrent segments of running
threads. If two concurrent segments contain shared memory
accesses, the accesses are reported as data races. This
method may incur a significant overhead in time despite of
several optimization techniques such as clock snooping and
merging segments [21, 22] as it requires set operations with
the collected shared memory accesses.
In the second method [7, 19], other than a vector clock
for each thread, each shared variable has two vector clocks
for reads and writes which record access history of the
variable by every thread. When a thread reads from a shared
variable, the write vector clock of the shared variable is
compared with the vector clock of the thread. A write-read
data race is reported if the vector clock comparison reveals
that the write and the read are not ordered by the happensbefore relation. A similar protocol can be performed for a

write access. Having vector clocks for each shared variable
can result in huge memory consumption. However, for
applications developed in object-oriented languages, the
memory requirement may be tolerable as large detection
units such as fields or objects can be used.
Data race detection for embedded applications which are
mostly written in C/C++ needs to consider fine granularity of
data access (e.g., byte). It would seem to be a better choice to
use the first happens-before detection method since having a
vector clock for every shared read or write byte may make
the detection infeasible. On the other hand, as shown in
FastTrack [7], the second method can reduce the space and
time overheads of vector clocks from O(n) (where n is the
number of threads) to nearly O(1) with no loss in detection
precision. While it may become feasible to detect races in
C/C++ programs, the overheads using the FastTrack with
fine granularity is still high for C/C++ programs, as
illustrated in our experiment results.
In this paper, we present a dynamic granularity algorithm
for vector clock based race detection. The detection
granularity starts from byte and is dynamically adjusted as
shared memory locations are accessed. A large detection
granularity is adopted when neighboring bytes have the same
vector clock. Thus, instead of multiple copies, a single copy
of vector clock is shared among these neighboring bytes.
Sharing the same vector clock among neighboring memory
locations become feasible since (1) neighboring memory
locations belonging to array or struct tend to be accessed
together, (2) data structures are often accessed together
during initialization even if they are separately protected
afterward, and (3) some groups of shared memory locations
are accessed only for one code block.
In the algorithm, a state machine is associated with a
vector clock for read or write of a memory location and the
state can be Init, Shared, Private, or Race. To minimize
analysis overhead, the sharing decision for each read or write
location is made at most twice for the lifetime of the
location. Peak memory consumption is further reduced by
temporarily sharing vector clock at the Init state. In addition,
the possibility of false alarms caused by sharing vector clock
is minimized as new sharing decision is made after the Init
state.
We have developed a race detector based on FastTrack
for C/C++ program and the dynamic granularity algorithm is
implemented on top of the FastTrack implementation. Our
experimental results on several benchmark programs show
that the race detector using our dynamic granularity provides
43% speedup and 60% less memory over the FastTrack with
byte granularity. Also we provide case studies on two
popular data race detection tools: Valgrind DRD [16, 25] and
Intel Inspector XE [11]. Our dynamic-granularity is about
2.2x and 1.4x faster than Valgrind DRD and Inspector XE,
and consumes about 2.8x less memory than Inspector XE.
The rest of the paper is organized as follows. In the
following section, a brief review of vector clock based race
detectors is presented. Section III presents the proposed
dynamic granularity algorithm. The implementation of data
race detector using dynamic granularity are explained in
Section IV. Section V shows experimental results on the tool

T0
<1, 1>
lock(s)
<1, 1 >
unlock(s)
<2, 1>

T1

Wx

Ls

<1, 1>

<0, 0>

<0, 0>

write(x)

<0, 1>
<2, 1>

lock(s)
<2, 1>
write(x)

<2, 1>

Figure 1. An example execution of DJIT+ is shown. T0 and T1 are
vector clocks of thread 0 and thread 1, respectively. Wx and Ls are
vector clocks for write x and lock s, respectively. Solid arrows show
happens-before relations and dotted arrows indicate vector clock
updates by the operations.

with FastTrack as well as comparisons with Valgrind DRD
and Intel Inspector XE. A concise survey of related works is
described in Section VI and, in Section VII, we conclude the
paper with future work.
II.

VECTOR CLOCK BASED RACE DETECTORS

A. Preliminaries
The happens-before relation [12] over the set of memory
and synchronization operations, denoted “→”, is the smallest
relation satisfying,
 Program order: If a and b are in the same thread and a
occurs before b, then a → b.
 Locking1: If a is a lock release and b is the immediately
successive lock acquire on the same lock, then a → b.
 Transitivity: If a → b and b → c, then a → c.
Two operations are concurrent if they are not ordered by
the happens-before relation. A data race occurs when two
memory operations are concurrent and at least one of them is
a write.
A vector clock [5] is a vector of logical clocks for all
threads and used to realize the happens-before relation. A
vector clock is indexed with thread ids. For instance, in
Figure 1, T1[0] gives the logical clock of thread 0 in the
thread 1’s vector clock and after the lock(s) T1[0] = 2. The
execution of a thread is logically divided into code blocks by
synchronization operations and the logical clock for the
thread is incremented in every new code block. The logical
clocks of other threads in a thread’s vector clock are updated
through synchronization operations. Similarly, the access
history of a shared memory location is recorded in vector
clocks for the location. Then, the happens-before relation for
the program execution is realized by the combination of the
vector clocks of threads and shared memory locations.
B. DJIT+
DJIT+ [19] defines the execution of a thread as a
sequence of epochs2 and a new epoch starts from every lock
1

Other synchronizations such as fork-join can be similarly defined. In this
paper, we discuss synchronizations only with locking for clarity.
2
It is defined as timeframes in the paper. But we use epochs for the
consistency of discussion.

release operation. DJIT+ detects only the first race for each
memory location. For consecutive reads of a memory
location in the same epoch, it is sufficient to check only the
first read for the detection of the first race. This property is
also true for consecutive write operations.
A thread i has a vector clock Ti in which Ti[i] (i.e., its’
own clock) is incremented upon every new epoch. A vector
clock Ls of a synchronization object s is updated when thread
i performs a lock release operation on s and is set to the
element-wise maximum of Ls and Ti. Upon a lock acquire
operation of s by thread i, Ti is updated as the element-wise
maximum of Ls and Ti. By the vector clock updates, happensbefore relations for the program execution are constructed
and a thread’s logical clock is updated. As synchronization
operations play a role in conveying a thread’s clock to other
threads, the thread’s vector clock is known to other threads.
For a memory location x, the access history of read and
write is represented with vector clocks Rx and Wx
respectively. Upon the first read of x in an epoch at thread i,
1) A write-read data race is reported if there is another
thread j whose write to x is not known to thread i,
i.e.,Wx[j] ≥ Ti[j]. If there was a synchronization from
thread j to thread i between the write and the read, then
the accesses should have been ordered and Wx[j] < Ti[j].
2) Thread i updates Rx such that, Rx[i] = Ti[i].
A similar protocol can be applied to write operations.
Figure 1 shows an example of how DJIT+ detects a data
race. In the example, the write on x in thread 0 is a data race
since Wx[1] ≥ T0[1].
By the property of checking only the first read and write
in an epoch, run-time performance can be significantly
improved. However, there still exists a huge overhead in
time and space for maintaining the vector clocks of shared
memory locations.
C. FastTrack
FastTrack [7] is based on DJIT+ and provides a
significant performance enhancement over DJIT+ with the
same detection precision as DJIT+. FastTrack exploits the
insight that, in most cases the last access of a memory
location can provide enough information for detecting data
races instead of using the full vector clock representation. An
epoch representation denotes the last access of a memory
location. If the last access was by a thread t at logical clock c,
then the epoch is denoted c@t using only two scalars. For all
writes to a memory location, the epoch representation can be
used instead of the full vector clock because all writes to the
location are totally ordered by the happens-before relation
before the first race on the location. This leads to a reduction
in time and space overhead from O(n) (where n is the
number of threads in the execution) to O(1). For read
operations, the epoch representation cannot be used all the
time since read operations can be performed without locking
(i.e., read shared). Thus, read vector clock, Rx, is replaced
with an adaptive representation which uses a full vector
clock only when the read is shared with other threads without
protection. Based on the adaptive representation, the
overhead for reads can be reduced from O(n) to nearly O(1).

III.

DYNAMIC GRANULARITY ALGORITHM

FastTrack is a fast and space-efficient race-detection tool
but it still needs to keep vector clocks for each memory
location. This is not problematic for object-oriented
programming languages since detection unit can be either a
field or an object. For C/C++ programs, it is not easy to
detect data structure boundaries (e.g., dynamically allocated
struct or array) and moreover data are often protected in fine
grained (e.g., a byte or a word). A simple way to rectify the
problem is to use a fixed granularity. However using a
granularity larger than a word would produce a large number
of false alarms and does not help reducing the overheads for
many cases as shown in our evaluation results.
In this section, we present a dynamic granularity
algorithm which enables vector clock based race detectors to
use detection granularity as large as possible with minimal
false alarms. The algorithm is described on the assumption of
using DJIT+ or FastTrack detectors; a thread’s execution is
defined as a sequence of epochs; and for consecutive
accesses of a memory location in an epoch, only the first
read and write are checked. In the description, two vector
clocks are the same when they are the same size and their
contents are of equal value, and both a vector clock and an
epoch representation are referred to as a vector clock.
The dynamic granularity is realized by sharing vector
clock with neighboring memory locations. The sharing
heuristic is based on the following observations:
1. Neighboring memory locations (e.g., array, struct) tend
to be accessed together whether the locations have data
races or not. Hence, they can have the same vector
clock.
2. At initialization, a data structure is often accessed in its
entirety, e.g., zero-out an array, even if its elements are
protected separately afterward.
3. There are groups of memory locations that are accessed
only in one epoch for the entire lifetime of the location,
e.g., dynamically allocated memory locations that are
used temporarily.
With these observations, the dynamic granularity
algorithm is realized with a vector clock state machine that is
described in the following subsection.
A. Vector Clock State Machine
For a memory location, we maintain a read location and a
write location separately. Hence, only the same access type
(read or write) of vector clocks can be shared. Let L be a
location which can be either a read or write location. When L
is accessed for the first time, a vector clock is created for it.
The sharing state of each location is maintained by a state
machine attached to its vector clock as shown in Figure 2.
The state machine basically has four states. In the first epoch
access, the vector clock is temporarily shared with its
neighbor if the neighbor has the same vector clock. When the
second epoch access begins at a location, the shared vector
clock at the location is split and new sharing decision is
made.

First Access to L

(Same VC with a neighbor
which is in Init)
and no data race on L

Init

1stEpoch

¬ (Same VC with any neighbor
which is in Init) and no data
race on L
Shared by another
location

1st EpochShared

1st EpochPrivate

Data Race

Second Epoch Access

(Same VC with a neighbor
which is in Private or Shared)
and no data race on L

2ndEpoch

¬ (Same VC with any neighbor
which is in Private or Shared)
and no data race on L

Shared by another location

Shared
No data race on L

Private
Data Race
Data Race

Data Race

No data race on L
and not shared by
another location

Race
All subsequent
accesses

Figure 2. Vector clock state machine for each read or write location.

A neighbor of L is a memory location adjacent to L that
is considered to share potentially a vector clock with L. A
location L can have two neighbors that one is located left (a
predecessor of L) and the other is located right (a successor
of L). During the first epoch, the neighbors are the nearest
predecessor and successor that have valid vector clocks. A
new access to a location L initiates a vector clock in the Init
state. This vector clock can be shared with L’s neighbors if
they have the same clock value and are in the Init state as
well. For an access to a location L in the 2nd epoch, the
neighbors are at locations L-size and L+size where size is the
data size of the access. As long as the neighbors are not in
the Init or Race state, we compare the vector clock of L with
those of its neighbors. If the vector clocks are equal, the state
is set to Shared. Otherwise, it is Private.
Each vector clock can be in one of the following 4 states:
Init: When L is accessed first time, its vector clock is
initiated and is set to this state until the next epoch access.
This Init state is intended to approximate the initialization
process. Note that elements of a data structure may be
initialized together and have the same vector clock during
the first epoch. However, starting from the 2nd epoch, the
elements may be accessed separately and have their own
private vector clocks.
While in the Init state, L can be in the 1st-Epoch-Shared
sub-state if one of the neighbors has the same vector clock
and are in the Init state. Thus, L shares temporarily its vector
clock with its neighbors during the first epoch. When there is
no neighbor that has the same vector clock as the location L,
the state of L becomes 1st-Epoch-Private. The state of L can
transition to 1st-Epoch-Shared when a new neighbor location

L′ is initiated and L′ has the same vector clock as L. The
rationale behind the temporary sharing is that there could be
many memory locations that are accessed only in one epoch.
Examples include dynamically allocated memory or groups
of memory locations in a big data structure that are used only
in one epoch. As our experimental results show, having this
Init state saves a considerable amount of memory for some
applications. Upon the next epoch access, L has its own
vector clock and state, and the new sharing decision is made
for the rest lifetime of the location L.
Shared: On the second epoch access of L, if there is no data
race on L (and no read-read conflict for a read location) and
there exists a neighbor that has the same vector clock as L
and is in either Shared or Private, the location L shares its
vector clock with the neighbor. Also, the state of L can
transition from Private to Shared when L becomes a
neighbor of another location L′ that has the same vector
clock as L.
Private: When there is no neighbor that has the same vector
clock as L, the state of L becomes Private on the second
epoch access.
Race: On a data race, the state of L becomes Race. If there
are memory locations sharing the same vector clock with L,
the sharing is terminated and each of these locations become
Race and is assigned with a private vector clock.
B. Dynamic Granularity
The dynamic granularity is achieved by sharing vector
clocks with neighboring address locations. The detection
starts with byte granularity for every location and the
granularity is increased as more neighboring locations share
the same vector clock. The vector comparison to determine
vector clock sharing can be an expensive operation.
However, following the vector clock state machine, there can
be at most two sharing decisions for the lifetime of a
memory location and it requires only O(1) time overhead in
most cases when the FastTrack algorithm is used. In fact, we
can have a significant performance enhancement by the use
of dynamic granularity since, as we change to a large
granularity, multiple accesses may be treated as the same
epoch accesses.
IV.

IMPLEMENTATION

We have implemented the FastTrack algorithm for data
race detection of C/C++ programs with fixed (byte and
word) and dynamic granularities. Intel PIN tool 2.11 [13] is
used for dynamic instrumentation of the programs.
A. Instrumentation
To trace all shared memory accesses, every data access
operation is instrumented. If an instruction accesses nonshared memory (e.g., stack), the instrumentation routine
returns immediately. Figure 3 shows pseudocode for
memory read instructions. Memory write can be similarly
described and we omit the FastTrack algorithm for clarity.
When an access is not the first read or write in an epoch,
vector clock updates and data race checking on that access
can be skipped according to the FastTrack algorithm.

void memoryRead(uint addr, uint size, uint tid)
{
if (nonSharedRead(addr) || sameEpoch(tid, addr))
return;
Location L = findReadAccess(addr);
if (!L) {
// The first access of addr
L = insertRead(addr, size);
shareFirstEpoch(L, addr, size);
// Temporary sharing
L→ state = Init;
} else if (L→state==Init) {
// Second epoch access
split(L, addr, size);
// Split for new sharing
shareSecondEpoch(L, addr, size); // New sharing decision
if (L→count>1)
L→state = Shared;
else
L→state = Private;
}
//if race found on addr, split all vectors sharing with L
// and set states of locations to Race
if (raceFound(addr))
splitAndSetRace(L, addr);
//remember access L into bitmap for this thread
//the bitmap is reset at the next epoch of this thread
insertEpochAccess(tid, addr);
}

read_address = 0xba123411
key = (read_address & 0xffffff80)
= 0xba123400
…
VC
State

…

0xba123400

128 addresses
Upon byte
granularity access
…

…
8 12 16

Hash chain entry with
32 pointers (addresses)

Checking the same epoch access can be costly since looking
up a vector clock from a global data structure requires
synchronization of threads. To reduce overhead, a per-thread
bitmap is implemented. When the first access is made in an
epoch, the access is set in the bitmap and the bitmap is reset
for every lock release operation. Because the bitmap is a
thread local data structure, checking the same epoch is more
efficient than looking up a global data structure.
The mechanism for dynamic granularity is invoked at
first two epochs for each read or write location. Thus, the
overhead can be negligible. Also it will be straightforward to
apply dynamic granularity into existing data race detection
tools.
B. Indexing Structure
To find the vector clock of each read or write location, a
chained hash table is implemented as shown in Figure 4. For
efficient sequential processing such as deleting vector clock
entries from free() and vector clock sharing process, each
hash chain entry has an indexing array which can contain up
to m pointers for vector clock entries. For a 32-bit address,
the upper address (upper 32-log2m bits of the address) is
hashed into the table to locate the corresponding hash entry.
The vector clock entry for the address is indexed using the
lower address (lower log2m bits of the address).
The size of the indexing array in the hash entry changes
according to memory access patterns, thus saving memory
on the indexing array. When a new hash entry is created, it
starts with an array of m/4 pointers since the most common
access pattern is word access. When a byte access (i.e., the
address is not word or half-word aligned) is detected, the
array is expended to have m pointers.

…

…
11 12 13

Hash chain entry with
128 pointers (addresses)

Figure 4. Top: A separate chaining hash table implementation.
Each entry can contain m addresses (shown a case m=128).
Bottom: The size of indexing array in a hash entry is changed from
m/4 to m when byte granularity access begins in the entry.

V.
Figure 3. Instrumentation code for memory read.

…
11

EVALUATION

In this section, we present the effectiveness of our
dynamic granularity algorithm. First, we show our
experimental results of the FastTrack with fixed (byte and
word) and dynamic granularities. Second, analysis results on
the state machine are presented. Lastly, performance
measures of two popular data race detection tools, Valgrind
DRD [16, 25] and Intel Inspector XE [11], are compared
with the FastTrack using dynamic granularity. All
experiments were performed on Ubuntu 12.04 with kernel
version 3.2.0 and Intel Core Duo CPU with 4 GB of RAM.
All experiments were run with 11 benchmarks: 8 from
the PARSEC-2.1 benchmark suite [1] that are implemented
with the POSIX thread library and 3 from popular
multithreaded applications: FFmpeg [4], a multimedia
encoder/decoder; pbzip2 [9], a parallel version of bzip2; and
hmmsearch [6], a sequence search tool in bioinformatics. For
input sets of the PARSEC benchmark programs, the
simsmall input set is used for raytrace while the simlarge is
used for the rest 7 programs. Input for the other three
programs is chosen to have similar run times as the PARSEC
benchmark programs.
A. Performance and Detection Precision
Table 1 shows overall experimental results of the
FastTrack with the three different granularities. “Total shared
accesses” column shows the total number of shared reads
and writes during each benchmark program run. “Max. # of
vectors in byte granularity” column indicates the maximum
number of vector clocks present for the execution of each
program in byte granularity run. These two columns,
combined with the number of threads, give us a general idea
of the instrumentation overhead in the detection.
“Slowdown” and “Memory overhead” columns report
runtime and memory overheads of each detection mechanism

as the ratios to the run time and maximum memory used in
the un-instrumented program execution. “# of Detected Data
Races” columns show the number of data races detected by
each granularity detector.
Overall Results. The results show that the dynamic
granularity detector is on average 1.43x and 1.25x faster than
the byte-granularity and the word-granularity detectors,
respectively. For memory overhead, on average the dynamic
granularity detector consumes 60% less memory than the
byte granularity detector and 23% less memory than the
word granularity detector.
For benchmarks facesim, fluidanimate, raytrace, canneal,
streamcluster, and hmmsearch, memory consumption is
neither reduced nor does detection become faster when we
switch from byte granularity to word granularity. Since the
sizes of most accesses in those benchmarks are equal to or
greater than a word, no vector clock is created for non-wordaligned locations. Thus, using word granularity does not help
to reduce the overhead of vector clock operations. However,
except for raytrace, canneal, the use of dynamic granularity
enhances the detection both in time and memory space. This
suggests the advantage of using a large granularity crossing
word boundaries. The results from ferret and pbzip2 show
improvements both in word granularity and dynamic
granularity, but the use of dynamic granularity has more
benefits than the use of word granularity. It may be strange
to see that the factor of memory overhead for dedup is 1.0
for all detectors. Note that the maximum overhead does not
always occur when the maximum memory is used in the
benchmark. In fact, dedup uses a large amount of memory
(about 2.7 GB) at the beginning of the execution when the
detection overhead is close to zero. Then, the memory usage
is gradually decreased while the peak memory overhead
from the detectors occurs afterward.
For detection precision, there are few discrepancies
among the detectors as shown in Table 1. With word
granularity, 993 data races are reported for x264 while the
dynamic granularity detector reported more data races. When

Dynamic
granularity

Byte
granularity

Word
granularity

Dynamic
granularity

288
146
248
170
49
104
2682
30
95
67
23

Word
granularity

6.1
6.7
2.0
9.5
2.2
6.5
7.7
3.8
3.0
5.7
26.6

Byte
granularity

2
11
3
3
256
3
7
5
9
6
3

# of Detected Data Races

Dynamic
granularity

Base
memory(MB)

93,930,447
83,678,104
11,220,394
3,291,927
12,550,683
7,141,372
9,208,539
2,277,958
7,195,586
8,842,583
961,831

Memory Overhead

Word
granularity

Base time
(sec)

8033
3856
2443
18
3392
359
10003
8030
5790
7239
38050

Slowdown

Byte
granularity

# of threads

facesim
ferret
fluidanimate
raytrace
x264
canneal
Dedup
streamcluster
ffmpeg
pbzip2
hmmsearch
Average

Max. # of
vectors in
byte
granularity

CESRAP

Benchmark
Program

Total shared
accesses
(million)

Table 1. Overall experimental results.

138
65
87
27
75
13
152
245
121
64
84
97

138
57
87
27
55
13
76
245
106
49
83
85

102
52
81
27
64
13
85
137
109
39
45
68

8.8
16.6
2.6
2.1
20.8
5.3
1.0
4.8
4.0
5.4
4.9
6.9

8.8
11.7
2.6
2.1
9.6
5.3
1.0
4.8
3.0
4.2
4.9
5.3

4.6
8.9
2.2
2.0
9.0
5.1
1.0
3.7
3.1
3.4
4.3
4.3

8909
2
1
13
1300
0
0
1053
1
0
1

8909
2
1
13
993
0
0
1053
9
0
1

8909
2
1
13
1313
0
0
1079
1
0
1

word granularity is used, non-word-aligned addresses are
masked to word boundary and data races for those locations
are detected as one race. That is how the word granularity
detector reported less number of data races for x264. We
carefully inspected data races from x264 found by the
dynamic granularity detector and noticed that there were 4
write locations which were sharing a vector clock with one
location having a data race. More data races from ffmpeg by
the word granularity detector and from streamcluster by the
dynamic granularity detector are found to be false alarms due
to inaccurate updates of vector clocks when large detection
granularities are used.
Memory Overhead. Table 2 shows the details of memory
overhead. For each granularity, three major overhead factors
are shown. The “Hash” column indicates the maximum
memory used for hash tables and hash entries to index vector
clocks. The “Vector clock” column gives the maximum
memory used to store vector clocks. The third column,
“Bitmap”, is the maximum memory used for bitmap data
structures for checking same epoch accesses. The overhead
is measured based on object size and is slightly
underestimated since the size of memory allocated for a data
object is usually little more than the actual size of the type.
The dynamic granularity algorithm saves a substantial
amount of memory used for vector clock allocations (as
shown in “Vector clock” columns). Another view of memory
savings on vector clocks is shown in Table 3 which lists the
maximum numbers of vector clocks during program
executions. On average, the dynamic granularity detector
uses roughly 4x and 3x less memory for vector clocks than
the byte granularity and the word granularity detectors,
respectively. Indexing costs of the byte granularity and the
dynamic granularity detectors are almost same since the use
of dynamic granularity does not save memory on indexing
vector clocks (as shown in “Hash” columns). The use of
word granularity saves memory on indexing for some
benchmark programs because the addresses are mostly wordaligned, thus using smaller indexing arrays in hash entries.

Table 2. Memory overhead of FastTrack detection with different granularities

Overhead total
(MB)

Hash(MB)

Vector Clock
(MB)

Bitmap (MB)

Overhead total
(MB)

Hash(MB)

Vector Clock
(MB)

Bitmap (MB)

Overhead total
(MB)

288
146
248
170
49
104
2682
30
95
67
23

Bitmap (MB)

facesim
ferret
fluidanimate
raytrace
x264
canneal
dedup
streamcluster
ffmpeg
pbzip2
hmmsearch
Average

Base
Memory
(MB)

513
458
132
35
77
87
212
11
37
49
12
148

1505
1573
180
53
233
176
148
37
118
141
15
380

132
66
27
15
7
52
57
6
7
17
3
35

2149
2097
339
103
317
315
417
54
162
207
30
563

514
259
132
30
33
87
147
11
18
37
12
116

1503
1072
180
53
89
176
100
36
43
89
15
305

132
57
27
15
7
52
58
6
7
17
3
35

2148
1388
338
97
129
315
305
54
68
143
30
456

517
454
132
35
77
87
214
11
37
50
13
148

273
469
74
22
44
155
88
3
28
4
1
105

131
64
27
15
7
52
56
6
7
16
3
35

921
988
233
72
128
294
358
21
72
70
17
288

Word
Granularity
(thousand)

Dynamic
Granularity
(thousand)

facesim
ferret
fluidanimate
raytrace
x264
canneal
dedup
streamcluster
ffmpeg
pbzip2
hmmsearch

Byte
Granularity
(thousand)

Max. # of vector clocks

93,930
83,678
11,220
3,291
12,550
7,141
9,208
2,277
7,195
8,842
961

93,808
52,375
11,174
3,285
4,803
7,141
6,226
2,245
2,620
5,570
950

16,991
24,689
4,590
1,319
2,019
5,812
5,474
193
1,696
265
53

Avg. sharing
count

Table 3. Maximum number of vector clocks present

Benchmark
program

Dynamic Granularity

Vector Clock
(MB)

Benchmark
program

Word Granularity

Hash(MB)

Byte Granularity

5.5
3.4
2.4
2.5
6.2
1.2
1.7
11.8
4.2
33.3
17.9

Slowdown. Speedups can be achieved in two ways by the use
of a large granularity. Firstly, in DJIT+ based race detectors
including FastTrack, the vector clock operations are
performed only for the first read and write of a shared
memory location during each epoch. The use of a large
granularity makes multiple accesses as one access. Thus,
there are more same epoch accesses enhancing the detection
performance. In Table 4, we show the percentage of same
epoch accesses along with the slowdown for each benchmark
program. The results suggest that in most cases the
performance gains from a large granularity are consistent
with the percentage of same epoch accesses. For the cases of
canneal and raytrace, as the percentages of same epoch
accesses do not vary noticeably among different
granularities, there is no performance enhancement by the
use of a large granularity.

Second, speedup comes from the reduction of vector
clock allocation and de-allocation operations. For the case of
pbzip2, the dynamic granularity detector is 1.6x faster than
the byte granularity detector while the percentages of same
epoch accesses are same. On the other hand, the average
number of locations that share a vector clocks is 33.3 under
dynamic granularity as shown in Table 3. This implies that
there will be about 33 times less vector clock creation and
deletion operations. The other interesting case is dedup. The
program has the same percentage of same epoch accesses for
both byte and dynamic granularities and the average number
of sharing vector clocks is only 1.7. However, the dynamic
granularity detector is 1.78x faster than the byte granularity
detector. The reason is that there are an excessive number of
dynamic memory locations in dedup. On average, there is
about 1.7 GB of memory allocated and de-allocated in the 11
benchmark programs whereas it is 14GB in dedup.
B. Analysis of State Machine
The sharing decision for realizing dynamic granularity is
made twice for the lifetime of a location L (read or write). In
the first epoch, L tries to share a vector clock with its
neighbors temporarily. In the second epoch, a new sharing
decision is made for the rest lifetime of L. We make a firm
sharing decision at the second epoch (after initialization of L)
since some groups of data structures can be initialized at the
same segment of code even if they are accessed separately
afterward. This design makes the sharing decision accurate.
The temporary sharing at the first epoch may save a
considerable amount of memory because there could be
groups of locations that are accessed together only once in
the same epoch and if that is the case, we do not have to keep
a separate vector clock for each of them. Notice that there is
no possibility of false alarms by the temporary sharing at the
Init state. Table 5 shows the effectiveness of this design.
Column 2 and 3 show the maximum memory used without
and with temporarily sharing at the first epoch. Column 4

Table 4. Measures of same epoch accesses

Benchmark
Program

Word
Granularity

Dynamic
Granularity

Byte
Granularity

Word
Granularity

Dynamic
Granularity

Same epoch

Byte
Granularity

Slowdown

facesim
ferret
fluidanimate
raytrace
x264
canneal
dedup
streamcluster
ffmpeg
pbzip2
hmmsearch
Average

138
65
87
27
75
13
152
245
121
64
84
97

138
57
87
27
55
13
76
245
106
49
83
85

102
52
81
27
64
13
85
137
109
39
45
68

74%
78%
89%
65%
67%
97%
85%
50%
68%
95%
83%
77%

74%
83%
89%
65%
90%
97%
93%
51%
90%
97%
83%
83%

94%
87%
94%
68%
78%
97%
85%
97%
84%
95%
98%
89%

Table 5. Comparisons of state machines with different
configurations

1317
1302
551
334
442
530
2730
111
294
225
99
721

With Init
state

2180
1808
604
348
470
550
2729
142
301
359
107
873

# of Detected
Data Races
No Init
state

facesim
ferret
fluidanimate
raytrace
x264
canneal
dedup
streamcluster
ffmpeg
pbzip2
hmmsearch
Average

Sharing at
Init

Benchmark
program

No sharing
at Init

Max.
Memory(MB)

9210
2
18529
13
1315
0
0
1079
1
2
1

8909
2
1
13
1313
0
0
1079
1
0
1

shows the number of detected data races without the Init
state, i.e., no temporary sharing and the sharing decision is
made only once in the first epoch. Comparing with column 5
in which the Init state is added, there could be many false
alarms as the consequence of improper sharing decisions
made only in the first epoch. In addition, the results suggest
that there are considerable numbers of memory locations that
are used only in one epoch.
C. Case Studies
In this section, we present experimental results on two
popular data race detection tools, DRD in Valgrind-3.8.1
[25] and Intel Inspector XE 2013 [11] update5. Also
comparison results with our dynamic granularity on
FastTrack are given. DRD, a tool for programs written with
the POSIX library, detects various errors including data
races, lock contention delays, and misuses of the POSIX

library. The race detection algorithm in DRD is based on
RecPlay [21]. Since DRD does not keep vector clocks for
each memory location in its segment comparison approach,
we expect that DRD uses less memory but is slower than
FastTrack. Intel Inspector XE is a memory and thread error
checker that is capable of detecting various errors including
data races, deadlocks, and cross-thread stack access.
Inspector XE provides a GUI with comprehensive
analysis reports, including the source code location of an
error, calling stack analyses, and suggestion for fixing any
detected errors. Likewise, DRD provides execution context
for each error as well as the location that the error occurs.
Race report from our implementation of FastTrack is not as
comprehensive as the two tools, but we provide the location
of a race along with the previous access location, thread ids,
and the race memory address. The information should be
sufficient for developers to fix the problems easily.
For Inspector XE, the command-line version was used
and only data race detection is enabled. The two tools used
byte granularity and all tools, including our dynamic
granularity version of FastTrack, traced all modules
including shared libraries. For the dynamic granularity
detector, we applied the similar suppression rules as in DRD,
e.g., suppressed data races detected from libc and ld. The
dynamic granularity detector and DRD report the first race
for each memory location while Inspector XE uses a
combination of instruction pointers and timeline when a race
occurs to distinguish races. Thus Inspector XE may report
the same accesses on a specific memory location as multiple
races or multiple accesses issued at the same instruction
points as one race. DRD and Inspector XE classify the
detected data races with execution context, but in the
experimental results we list the raw number of data races
before the classifications.
The comparison results are shown in Table 6. Both
Inspector XE and DRD exited on dedup runs with out of
memory warnings. DRD on fluidanimate and Inspector XE
on ffmpeg ran for more than 24 hours before we stopped the
analyses.
As we expected before the experiment, DRD is slower
than the FastTrack with dynamic granularity and even slower
than the FastTrack with byte granularity, but DRD consumes
less memory than the FastTrack with dynamic granularity.
The comparison results also suggest that the dynamic
granularity detector is as accurate as the other two tools. All
three tools detected the same race for hmmsearch (Inspector
XE reported the same race one more time in a different
timeline). The dynamic granularity detector and Inspector
XE reported the same races for three benchmarks ferret,
fluidanimate, and streamcluster, For raytrace, the dynamic
granularity detector and DRD reported the same races, but
DRD reported more races from pthread library which was
suppressed by the dynamic granularity detector. DRD
detected no race for ffmpeg while the dynamic granularity
detector reported one race. We manually inspected the
source code and found that it was a data race by the two
worker threads accessing a shared variable without
protection.

Table 6. Performance comparison of Valgraind DRD, Intel Inspector XE and FastTrack with dynamic granularity

Average

VI.

102
52
81
27
64
13
85
137
109
39
45

2.2
2.6
-1.9
3.2
8.2
-4.2
2.6
2.9
4.4

6.0
5.0
12.4
4.1
22.1
11.9
-17.5
-8.6
21.9

4.6
8.9
2.2
2.0
9.0
5.1
1.0
3.7
3.1
3.4
4.3

150

98

68

3.6

12.2

4.3

RELATED WORK

Aside from the 3 basic approaches for race detections, [7,
19, 21, 22, 23, 25, 27], a variety of hybrid race detectors
have been proposed in which Eraser’s Lockset algorithm is
combined with the happens-before algorithm to have better
detection coverage and to avoid false alarms. O’Callahan and
Choi [17] have proposed a race detection algorithm in which
a subset of happens-before relations is added to a Lockset
based detector. The detector is optimized by detecting
redundant event accesses and by the use of a “two phase”
approach of detailed and simple mode detections. MultiRace
[19] combines DJIT+ and Lockset algorithm and only check
the first access in each time frame. In MultiRace, the number
of detection operations is reduced based on the information
produced from LockSet and false alarms from LockSet are
filtered out by happens-before relations. ThreadSanitizer [24]
is a hybrid race detector for C++ programs that offers
tunable options to users. Its dynamic annotations allow the
detector to be aware of user defined synchronizations. Thus
the tool hides certain false alarms and benign races.
RaceTrack [27] incorporates the happens-before relation into
the LockSet algorithm and only report races caused by
concurrent accesses. One interesting idea in RaceTrack is the
use of adaptive granularity. The detection granularity starts
from object level and becomes field level when a potential
race is detected. Unfortunately, the idea, based on object
references, is not applicable to C/C++ programs.
LiteRace [14] is a sampling based race detector grounded
in the cold-region hypothesis that infrequently accessed areas
are more likely to have data races than frequently accessed
areas. Accesses to code regions of different function units are
sampled while all synchronization operations are collected.
The sampler starts at a 100% sampling rate and the sampling
rate is adaptively decreased until it reaches a lower bound.

8909
108
-16
988
0
-1067
0
0
1

Dynamic
granularity

128
87
89
17
246
41
-108
-99
64

Intel Inspector
XE

59
748
-42
143
31
-66
120
64
74

Valgrind DRD

Valgrind DRD

Dynamic
granularity

288
146
248
170
49
104
2682
30
95
67
23

Intel Inspector
XE

6.1
6.7
2.0
9.5
2.2
6.5
7.7
3.8
3.0
5.7
26.6

Base
Memory
(MB)

# of Detected Data Races

Dynamic
granularity

facesim
ferret
fluidanimate
raytrace
x264
canneal
dedup
streamcluster
ffmpeg
pbzip2
hmmsearch

Base
time
(sec)

Memory Overhead

Intel Inspector
XE

Benchmark
program

Valgrind DRD

Slowdown

31
4
7
0
218
0
-61
-0
2

8909
2
1
13
1313
0
0
1079
1
0
1

PACER [2] is another sampling based race detector that
periodically samples all threads and offers a detection rate
proportional to the sampling rate. These approaches offer
reasonable detection rate with minimal overhead, but may
miss critical data races.
As an alternative to software only race detectors, several
hardware assisted race detectors have been proposed. In
SigRace [15], data addresses are automatically encoded in
hardware address signatures and in a hardware module. The
signatures are intersected with those of other processors to
detect data races. Greatehouse et al. [10] proposed a demanddriven race detector that utilizes cache performance counters
to detect data sharing between threads. When the data
sharing is detected, a software race detector is enabled and
run until there is no more data sharing. These approaches are
efficient but require specific hardware making them
impractical.
While many researchers have focused on data race
detection algorithms for Java programs, only a few of which
have presented evaluation results for existing data race
detection tools on C/C++ programs. Aikido [18] is a
framework for shared data analysis in which sharing data is
detected using per-thread page protection technique. The
Aikido sharing detector is complementary to dynamic
granularity and is effective to remove the instrumentation
overhead of no-shared memory accesses. IFRit [3] is a
dynamic race detection algorithm for C/C++ programs based
on interference-free region which can limit the range of code
instrumentation. IFRit has been compared with FastTrack
and ThreadSanitizer [24]. Both researches applied the
PARSEC benchmark suite in their performance evaluations,
but only the simsmall input set was used and no memory
overhead was reported. Moreover, none of them made an
attempt to compare their tools with commercial grade data
race detection tools.

VII. CONCLUSIONS
In this paper, we have presented a dynamic granularity
algorithm for C/C++ programs that enables vector clock
based race detectors to adjust detection granularity. A vector
clock state machine is employed to determine when vector
clocks can be shared. The state machine also considers the
initialization patterns of data structures. Thus, possible false
alarms due to vector clock sharing can be reduced.
Our experimental results show that the dynamic
granularity detector outperforms the FastTrack with both
byte or word granularity and also outperforms two existing
data race detection tools, Valgrind DRD and Intel Inspector
XE.
We plan to extend our work by optimizing the sharing
algorithm. The current work maintains read and write vector
clocks separately. The decision of sharing read vector clocks
can be guided by the status of write vector clocks. We also
plan to enhance the vector clock state machine to
accommodate access behavior after the second epoch so that
the detection granularity can be changed more dynamically.

[12] L. Lamport. Time, clocks, and the ordering of events in a

[13]

[14]

[15]

[16]

[17]

ACKNOWLEDGMENT
This work was supported in part by the NSF I/UCRC
Center for Embedded Systems, and from NSF grant
#0856090.

[18]

REFERENCES
[1]
[2]

[3]

[4]
[5]
[6]

[7]

[8]

[9]
[10]

[11]

C. Bienia. Benchmarking Modern Multiprocessors. Ph.D.
Thesis. Princeton University, 2011.
M. D. Bond, K. E. Coons, and K. S. McKinley. PACER:
Proportional detection of data races. In Proceedings of the
ACM SIGPLAN conference on Programming language design
and implementation (PLDI), pages 255-268, 2010.
L. Effinger-Dean, B. Lucia, L. Ceze, D. Grossman, and H-J.
Boehm. IFRit: Interference-free regions for dynamic data-race
detection. In Proceedings of the ACM international
conference on Object oriented programming systems
languages and applications (OOPSLA), pages 467-484, 2012.
FFmpeg. http://www.ffmpeg.org/.
C. J. Fidge. Logical time in distributed computing systems.
IEEE Computer, 24(8):28-33, 1991.
R. D. Finn, J. Clements, and S. R. Eddy. HMMER web
server: Interactive sequence similarity searching. Nucleic
Acids Research Web Server Issue 39:W29-W37, 2011.
C. Flanagan and S. N. Freund. FastTrack: Efficient and
precise dynamic race detection. In Proceedings of the ACM
SIGPLAN conference on Programming language design and
implementation (PLDI), pages 121-133, 2009.
C. Flanagan and S. N. Freund. Type-based race detection for
Java. In Proceedings of the ACM SIGPLAN conference on
Programming language design and implementation (PLDI),
pages 219-232, 2000.
J. Gilchrist. Parallel BZIP2, http://compression.ca/pbzip2/.
J. L. Greathouse, Z. Ma, M. I. Frank, R. Peri, and T. Austin.
Demand-driven software race detection using hardware
performance counters. In Proceedings of the 38th annual
international symposium on Computer architecture (ISCA),
pages 165-176, 2011.
Intel Inspector XE 2013. http://software.intel.com/en-us/intelinspector-xe.

[19]

[20]

[21]

[22]

[23]

[24]

[25]
[26]

[27]

distributed system. Communications of the ACM, 21(7):558565, 1978.
C. Luk, R. Cohn, R. Muth, H. Patil, A. Klauser, G, Lowney,
S. Wallace, V. J. Reddi, and K. Hazelwood. Pin: Building
customized program analysis tools with dynamic
instrumentation. In Proceedings of the ACM SIGPLAN
conference on Programming language design and
implementation (PLDI), pages 190-200, 2005.
D. Marino, M. Musuvathi, and S. Narayanasamy. LiteRace:
Effective sampling for lightweight data-race detection. In
Proceedings of the ACM SIGPLAN conference on
Programming language design and implementation (PLDI),
pages 134-143, 2009.
A. Muzahid, D. Suárez, S. Qi, and J. Torrellas. SigRace:
Signature-based data race detection. In Proceedings of the
36th annual international symposium on Computer
architecture (ISCA), pages 337-348, 2009.
N. Nethercote and J. Seward. Valgrind: A framework for
heavyweight dynamic binary instrumentation. In Proceedings
of the ACM SIGPLAN conference on Programming language
design and implementation (PLDI), pages 89-100, 2007.
R. O'Callahan and J. Choi. Hybrid dynamic data race
detection. In Proceedings of the ninth ACM SIGPLAN
symposium on Principles and practice of parallel
programming (PPoPP), pages 167-178, 2003.
M. Olszewski, Q. Zhao, D. Koh, J. Ansel, and S.
Amarasinghe. Aikido: Accelerating shared data dynamic
analyses. In Proceedings of the seventeenth international
conf. on Architectural Support for Programming Languages
and Operating Systems (ASPLOS), pages 173-184, 2012.
E. Pozniansky and A. Schuster. Efficient on-the-fly data race
detection in multithreaded C++ programs. In Proceedings of
the ninth ACM SIGPLAN symp. on Principles and practice of
parallel programming (PPoPP), pages 179-190, 2003.
P. Pratikakis, J. S. Foster, and M. Hicks. LOCKSMITH:
Practical static race detection for C. ACM Transactions on
Programming Languages and Systems (TOPLAS), 33(1):1-55,
2011.
M. Ronsse and K. D. Bosschere. RecPlay: A fully integrated
practical record/replay system. ACM Transactions on
Computer Systems (TOCS), 17(2):133-152, 1999.
M. Ronsse, M. Christiaens, and K. D. Bosschere. Debugging
shared memory parallel programs using record/replay. Future
Generation Computer Systems, 19(5):679-687, 2003.
S. Savage, M. Burrows, G. Nelson, P. Sobalvarro, and T.
Anderson. Eraser: A dynamic data race detector for
multithreaded programs. ACM Transactions on Computer
Systems (TOCS), 15(4):391-411, 1997.
K. Serebryany and T. Iskhodzhanov. ThreadSanitizer: Data
race detection in practice. In Proceedings of the Workshop on
Binary Instrumentation and Applications (WBIA), pages 6271, 2009.
DRD, Valgrind-3.8.1. http://valgrind.org/.
J. W. Voung, R. Jhala, and S. Lerner. RELAY: Static race
detection on millions of lines of code. In Proceedings of the
6th joint meeting of the European software engineering
conference and the ACM SIGSOFT symposium on the
foundations of software engineering (ESEC-FSE), pages 205214, 2007.
Y. Yu, T. Rodeheffer, and W. Chen. RaceTrack: Efficient
detection of data race conditions via adaptive tracking. In
Proceedings of the twentieth ACM symposium on Operating
systems principles (SOSP), pages 221-234, 2005.

Schedulable Persistence System for Real-Time
Applications in Virtual Machine
Okehee Goh

Yann-Hang Lee

Ziad Kaakani

CSE, Arizona State University
Tempe, AZ

CSE, Arizona State University
Tempe, AZ

Honeywell International Inc.
Phoenix, AZ

ogoh@asu.edu

yhlee@asu.edu

ziad.kaakani@honeywell.com

ABSTRACT

1. INTRODUCTION

Persistence in applications saves a computation state that
can be used to facilitate system recovery upon failures. As
we begin to adopt virtual execution environments (VMs)
for mission-critical real-time embedded applications, persistence service will become an essential part of VM to ensure
high availability of the systems.
In this paper, we focus in a schedulable persistence system in VMs and show a prototype persistence system constructed on CLI’s open source platform, MONO. By employing object serialization, the system enables concurrent and
preemptible persistence operation, i.e., the task in charge of
persistence service runs concurrently with application tasks
and is a target of real-time scheduling. Thus, the execution
of application tasks can be interleaved with the operations
of persistence service, and the task timeliness can be guaranteed as the pause time caused by persistence service is
bounded. The experiment output on the prototyped system
illustrates that persistence service is appropriate for realtime applications because of its controllable pause time and
its optimized overhead.

Virtual Machines (hereafter, VMs), such as JVM[17] and
CLI (Common Language Infrastructure)[9], enable an abstract computing environment for program execution. Application programs are compiled into intermediate codes (bytecodes) to permit portability of ”write once, run everywhere.”
Besides that, applications written in Java[13] or CLI-compatible
languages are claimed secure due to type-safety and security
sandbox model [27]. The ensured safety, portability, and
reusability of OO languages make VMs attractive to embedded systems and have led to the introduction of many
VMs designed for embedded applications. One example is
Real-Time Speciﬁcation for Java (RTSJ)[4] which has been
established as a standard speciﬁcation of JVM to meet the
requirements of real-time embedded applications.
Along with the interests of employing VM in real-time
embedded systems, persistence of applications is becoming
a necessity as an approach to ensure high availability of
long-running embedded applications. Persistence in applications preserves a computation state of applications beyond
its lifetime. If a failure occurs, the systems can be recovered
by using the preserved computation state while minimizing any loss of computation. Without a support of persistence, system failures may force the systems to restart from
the beginning (cold start) or result in an inconsistent state
with which computation cannot advance. Despite of the advantages of persistence, there is a concern whether making
applications persistent may introduce an unpredictable latency that can impair the timeliness of real-time embedded
systems. For example, Monga et al. [19] shows in the experiment of persistence service using standard object serialization of .NET framework that serialization for 100 instances
of System.Int32 type takes about 100ms, which is intolerable to most real-time applications in the aspects of both
performance and pause time.
Before proceeding further, we ﬁrstly deﬁne some terminologies, frequently used in the rest of this paper. Persistence System and Persistence Service refer to a subsystem
in charge of making applications persistent, and an activity
of making applications persistent, respectively. Additionally, Persistent data and Persisted data indicate the data
that needs to be persistent among runtime data in applications, and the one that is saved through persistent service,
respectively.
Due to the time constraints that real-time applications
have, persistence service integrated to real-time systems should
not cause unpredictable blocking delays to application tasks.
To prevent unpredictable blocking delay, the service should

Categories and Subject Descriptors
C.3 [Special-purpose and application-based systems]:
Realtime and embedded systems; D.4.5 [Operating systems]: Reliability —Checkpoint/restart; D.4.7 [Operating
systems]: Organization and Design—Real-time systems and
embedded systems

General Terms
Reliability Performance

Keywords
schedulable persistence system, checkpoint/recovery, realtime applications, virtual machine, CLI

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
EMSOFT’06, October 22–25, 2006, Seoul Korea
Copyright 2006 ACM 1-59593-542-8/06/0010 ...$5.00.

195

Period of Persistence Increment

2. BACKGROUND AND RELATED WORKS

Pause of Persistence Increment

2.1 Checkpointing & Object Serialization in
VMs

Persistence Cycle

Figure 1: Scheduling Model of Schedulable Persistence System

be a schedulable object by allowing preemptivity and bounded
pause time. Thus, a Schedulable Persistence System can be
interleaved with real-time applications and both the timeliness and persistence of the real-time applications can be
guaranteed.
In this paper, we depict an approach of schedulable persistence service in VM environment. The persistence service
is taken by a separate task that runs concurrently with realtime application tasks. The service is divided into small portions (persistence increment, hereafter) to bound the pause
time caused by the service. As illustrated in Figure 1, the
scheduling model for the proposed approach is basically to
allocate CPU cycles to consecutive persistence increments
periodically. Hence, the whole persistence service becomes
preemptible after each persistence increment and its execution can be treated as a periodic task invoked in a persistence
cycle.
One of the issues raised while supporting preemptivity of
persistence service is to guarantee the consistency of persisted data. We deﬁne that a consistent persisted data is
a snapshot of all persistent data when the persistence service is triggered. With concurrent and preemptible persistence service, mutators (i.e. application tasks) are executed
concurrently with the persistence service and may update
the persistent data before the data becomes persisted completely. We address this issue by using write barrier in a
cost eﬀective manner.
The proposed approach of making persistence service schedulable along with real-time applications in VM environment
can be viewed as a part of eﬀorts to make VMs suitable
for real-time embedded systems. However, even if we consider the emerging RTSJ’s commercial products, PERC [1]
and Sun Real-Time Java System [25], there is no existing
VM that enables persistence of applications while considering the timeliness of real-time applications simultaneously.
Beyond VMs, a few previous works [16, 8] aimed at concurrent checkpointing for general real-time systems. These
approaches are to save a process memory content based on
architectural support and/or by using coarse grained write
barriers. Their limitations include non-portability of checkpoint data (process image) and/or the lack of semantics of
preserved data for alternate recovery processing.
In the following, we give a brief background on object serialization and a discussion of related works in Section 2. The
design approach for a schedulable persistence system is introduced in Section 3. In Section 4 and Section 5, we present
the details of the prototyped schedulable persistence system
and the experimental results. Finally, Section 6 draws a
simple conclusion.

196

Checkpoint based roll-back recovery is a fault-tolerant
technique to minimize the lost computation due to failures.
A checkpoint can be taken during failure-free execution and
a state of computation is saved. Upon a failure, the recovery operation uses the checkpoint to restore the computation state up to the moment that the checkpoint was taken.
Then, the computation can proceed.
Checkpoint can be done in the level of VM to save the
state of live objects which represent the computation state of
applications. Also, as suggested in [5] and [23], the applications’ execution state can be save as portable checkpoints of
thread objects. The saved state of data and thread objects
can be serialized by following VM’s standard data format
and then become readable by a VM in any architecture.
Object Serialization in Java and CLI-compatible languages
[22, 18] is a signiﬁcant functionality used for lightweight object persistence, and data marshaling/demarshaling in RMI
(Remote Method Invocation) or .NET Remoting programming. Object serialization transforms the state of objects
in memory into a sequence of bytes or other representations
suitable for transmission to ﬁle systems or communication
media. It can be a simple approach to support object persistence. According to object reachability, Persistent Objects
or Persistent Data include not only a persistent root object,
but also all reachable objects from the root object. During serialization all reachable objects from the root object
must be traced to generate persisted data. Then, deserialization restores objects from the persisted data. However,
the current approach for object serialization suﬀers several
drawbacks [10]:
• The procedure for serialization or deserialization runs
as a sequential operation of the invoking threads. Hence,
the normal thread operation can only proceed after
the serialization operation and may suﬀer a long pause
(the serialization operation cannot suspend in the middle of the procedure).
• Object serialization does not consider consistency of
persisted data. For example, if a thread serializes an
object graph (all reachable objects from a persistent
root object) while other threads mutate any objects in
the graph, the consistency of the persisted data in the
graph is not guaranteed.
Apart from the aforementioned drawbacks, the performance of the existing serialization operation in terms of time
and space is also a concern. The object serialization in both
JVM and CLI heavily uses Reﬂection mechanism of VM,
which allows managed code to retrieve information of ﬁelds,
methods, constructors for objects, classes etc. Reﬂection is
basically an interpreted operation on object meta data and
may incur a signiﬁcant overhead and performance penalty.

2.2 Efficient Object Serialization
There were several attempts to improve the performance
of Java Object Serialization in order to enhance RMI mechanisms. Haumacher et al. [20] suggested a serialization function in pure Java code. To remove the overhead in a wire
protocol, it minimizes the size of metadata in the serialized

updated in software, rather than depending on a speciﬁc
hardware. To reduce the cost of write barriers, the mirror
copy is mapped to separate physical memory pages while
checkpoint is underway. Hence, write barrier operations do
not need to be changed according to the presence of checkpoint. Employing a mirror copy can be a solution of checkpoint while concern the timeliness of applications. However,
the cost having extra memory blocks may not be tolerable
to resource constraint embedded systems, and the approach
is also limited to checkpoint memory content in OS level.
There have been a lot of research work focusing in the
scheduling issues for fault-tolerant real-time applications with
time-redundancy schemes [15, 12, 11], and [21]. Because the
time-redundancy schemes require extra time for fault detection and fault recovery, scheduling fault-tolerant real-time
applications are resorted to ﬁnding the WCET (Worst Case
Execution Time) of the applications with failure detection
and recovery, conducting schedulability study, and establishing eﬃcient scheduling algorithms in the presence of faults.

data by omitting data ﬁelds’ type information, name, etc.
To avoid any costly invocation of reﬂection mechanism in a
generic serialization operation, it supports only user-deﬁned
serialization, which requires programmers to specify methods to serialize/deserialize objects of each class. Breg et al.
[6] implemented a serialization function in native code by
using JNI. Their study shows that the overhead from Reﬂection and JNI is still high although the implementation in
native codes improves the performance to a certain extent.
In addition, a type-cache was employed in their approach
to reduce the overhead caused by repetitive invocations of
Reﬂection functions.

2.3 Orthogonal Persistent Object Systems
Making data persistent beyond the lifetime of applications may require extra programming eﬀort to save and restore data using speciﬁc programming constructs, for example, DB languages to access DBMS. Orthogonal Persistent System [2] aims at a transparent programming such
that no signiﬁcant programming eﬀort is required to handle persistent data in a database. In addition, it adopts
Lazy pointer swizzling and Incremental updates techniques
to reduce the latency due to restoring and saving the persistent objects. Lazy pointer swizzling is to update the loaded
object’s address (from persistent storage address to memory address) on demand. Furthermore, with incremental
updates, only updates, instead of the whole data objects,
are saved to a persistent storage since the latest persistence
service. Through the techniques, the orthogonal persistent
object system provides scalability on enterprise applications
that handle massive data. However, its unpredictable latency still makes the system inappropriate for real-time applications.

3. DESIGN APPROACHES
The design of the proposed persistence system is focused
on achieving three goals: to enable concurrent and preemptible persistence service with adjustable pause time while
ensuring consistency of persisted data, to make persistence
service eﬃcient for resource constrained systems, and to provide essential features necessary for fault recovery as well as
data recovery. We will limit the scope of the persistence
service to data persistence but not thread persistence for
applications. However, we claim that the characteristics of
real-time embedded applications, and inevitable initialization for external data during recovery make data persistence
the foremost concern for failure recovery.
We design a schedulable persistence system by employing
object serialization and extending the functions of VM environment. We use MONO, an open source development platform of .NET framework running on Linux platforms [28],
as an example platform to illustrate the details of our design. The beneﬁt of employing object serialization is portability because object serialization generates persisted data
with VM-aware standard format. Thus, the persisted data
can be used in both HW and time redundant fault tolerant
architectures. Serialization and Deserialization are implemented in native codes as a subsystem of CLI. This decision
is drawn to overcome the limitations of serialization provided as managed code including poor performance and the
inadequacy to be extended for preemptible and concurrent
serialization. Since we employ object serialization to achieve
persistence, we will refer serialization and serialized data interchangeably with persistence service and persisted data,
respectively.

2.4 Checkpointing for Real-Time Applications
There are few investigations on checkpointing memory
content of real-time application processes in OS level. Li
et al.[16] present a concurrent checkpointing algorithm in
which mutators are allowed to interleave with a checkpointing thread. The algorithm incurs a reduced latency by
checkpointing memory pages one at a time rather than checkpointing the whole memory space of mutators at once. The
consistency of checkpointed data is addressed by employing
a copy-on-write mechanism. The mechanism places writeprotection on memory pages when checkpoint starts. When
mutators try to update a memory page, the page is checkpointed before the update is applied. Although this approach can reduce the latency modestly, two limitations follow. Firstly, the granularity in a level of a memory page
is too coarse to satisfy the timely requirements of real-time
applications. Secondly, there is a signiﬁcant initialization
delay to enable MMU (Memory Management Unit) write
protection on the whole memory space of the application.
Cunei et al. [8] propose a concurrent checkpoint mechanism by employing a mirror copy of memory blocks. The
mirror copy works as follow: the checkpoint mechanism
maintains an auxiliary memory block as a mirror copy of
main memory. In order to have the mirror copy constantly
updated, update operations of mutators apply on both the
main memory and the mirror copy. When a checkpoint
starts, the updates on a mirror copy are suspended, and
then the data on the mirror copy is saved to a persistent
storage. Write barriers can be used to allow the mirror copy

3.1 Efficient Serialization Algorithms
The serialization operation consists of three steps, ﬁrstly
to obtain the state of objects, secondly to transform the state
into binary sequences, and then to write the serialized data
to memory, or ﬁles (persistent storage) [14]. The factors
aﬀecting the performance of serialization are:
• Reﬂection allows managed code to retrieve information of persistent objects’ ﬁelds. It is only way to obtain the state of objects in serialization implemented
in managed code, but it is very costly.

197

• The serialized data consist of metadata as well as states
of persistent objects. The metadata include type information, headers necessary to parse the states for deserialization. The excessive metadata increases IO overhead as well as the size of serialized data. The benchmark on object serialization implemented for .NET
Compact Framework [6] indicates that reducing the
size of metadata overhead helps improving the performance.

Serialized data includes not only the states of persistent
objects but also object header, which describe class information, ﬁelds’ types, ﬁelds’ names, and other header information, etc. We design a Serialization Protocol to deﬁne
the format of serialized data. The protocol minimizes the
amount of serialized data by reducing the amount of the
metadata to be saved with persisted data. This is done by,
ﬁrstly, not carrying a verbose format of metadata to specify
names and types of its ﬁelds in serialized data. Those information can be retrieved through persistent class map, which
is also generated during deserialization. Secondly, persistent classes are distinguished with unique class identiﬁcation (Class ID) and are maintained in a Persistent Class
Cache with detail information such as assembly version etc.
Serialized data of persistent objects carry only Class ID representing its class instead of verbose format of its class information. The cache is also serialized so that deserialization
can generate the same type of persistent class cache. The
serialized data is converted into binary stream, which is independent of the underlying hardware platforms.
To invoke serialization/deserialization operations, we employ a simple API of three static methods of class SPC.
AddRoot registers a persistent root object and returns a
persisted data ID. When the data needs to be persisted,
the second method SendPData is called. For deserialization, RecvPData is invoked with the ID of a persistent root
object which is used to locate the corresponding persisted
data.

• The execution performance in managed code is much
slower than native codes. To improve the performance,
Breg et al. [6] implemented serialization in native
codes using JNI. However, the overhead associated
with JNI is not negligible.
An eﬃcient serialization should avoid any reference to reﬂection mechanism and minimize the meta data information
saved in persistence storage. To further optimize the performance, we can adopt a native code implementation of
serialization service as internal functions of VM. To begin
the design, persistence class and ﬁelds should be declared.
Hence, programmers can apply their knowledge of the applications to select the class and ﬁeld of data that must be
made persistent. In C#, this can be done with additional
attributes, such as [SPersistent] and [SPersistentField]. As
shown in Listing 1 of a C# class declaration of a persistent
class with two persistent ﬁelds. Attribute in C# is associated with managed code in a form of Class Metadata, but
does not alter the managed code’s semantics. When managed code is loaded, the associated class metadata is placed
as in-memory data structure. In Java, similar to Serializable
interface, a persistence interface can be deﬁned to indicate
a class whose instances may be persistent.

Foo oKey , oValue ;
i n t nRootID1 , nRootID2 ;
// I n i t i a l i z a t i o n Phase f o r c o l d s t a r t
i f ( IsWarmRestart ()== f a l s e )
{
oKey
= new Foo ( ) ;
oValue = new Foo ( ) ;

[ SPersistent ]
c l a s s Foo
{
[ SPersistentField ]
public int i ;

// R e g i s t e r p e r s i s t e n t r o o t o b j e c t s
nRootID1 = SPC . AddRoot ( oKey ) ;
nRootID2 = SPC . AddRoot ( oValue ) ;

}
// I n i t i a l i z a t i o n Phase f o r warm r e s t a r t
else
{
oKey
= SPC . RecvPData ( nRootID1 ) ;
oValue = SPC . RecvPData ( nRootID2 ) ;
}

publ i c double d ;
[ SPersistentField ]
p u b l i c Bar o ;
}

Listing 1: Example declaration of a persistent class

// P e r i o d i c O p er a ti o n Phase
while ()
{
// Main o p e r a t i o n s
...
// S e r i a l i z e a l l p e r s i s t e n t o b j e c t g r a p h s
SPC . SendPData ( ) ;
}

To implement serialization/deserialization as internal functions in VMs, we will not be able to use reﬂection mechanism to retrieve underlying information of persistent classes/ﬁelds. Our approach is to maintain a Persistent Class
Map internally for persistent classes. The map facilitates
the introspection function of a reﬂection mechanism and
contains persistent ﬁelds’ type information and oﬀset. For
example, the map for Foo class in Listing 1 lists two entries for a ﬁeld x and a ﬁeld o. The map helps eﬃciently
locating persistent ﬁelds from persistent objects during serialization and deserialization. A persistent class and its
persistent ﬁelds are identiﬁed by accessing the persistence
attribute information in class metadata. The map can be
built when the class is loaded or when the class’ persistent
objects are serialized for the ﬁrst time.

Listing 2: An example program of using persistence
service
Listing 2 is an example of a persistent application to support warm start using the API. In many real-time control
applications, function blocks are invoked repetitively using
sensor inputs and to control actuators. The applications are
typically structured with Initialization Phase and then Periodic Operation Phase. In the initialization phase, resource

198

c l a s s Foo {
[ SPersistentField ]
public int x ;

required to execute the function blocks are initialized. Then,
in the operation phase, function blocks are invoked periodically. Once applications’ data is persisted at the end of
each period, failure recovery can be done by restarting the
applications at a new period with the latest persisted data.

[ SPersistentField ]
p u b l i c I x V e r t e x PNodes ;

3.2 Concurrent Persistence Mechanism

p u b l i c i n t XP {
get {
return x ;
}

The existing serialization in JVM or .NET is an atomic
procedure, i.e., the applications have to wait until the last
object of persistent object graph gets serialized. This may
result in a long pause on applications. To make persistence service schedulable for real-time embedded applications, we assign a separate task responsible for the service
(hereafter, SP task ) and make the task preemptible by dividing the service into small increments. As a consequence,
real-time scheduling algorithms can be applied to dispatch
urgent tasks once the SP task performs a bounded persistence service per increment and relinquishes CPU cycles to
other tasks.
We deﬁne that a consistent persisted data is a snapshot of
all persistent data when the persistence service is triggered.
With concurrent and preemptible persistence service, the
mutators may modify the persistent data before it is completely serialized. We address this issue by using a write
barrier. The write barrier traps the update operations of
mutators on persistent objects. If the persistent object yet
gets serialized, the write barrier performs serialization of the
object before the update is eﬀective on the object.
Ideally, a write barrier should be only applied to the update operations on the persistent data when persistence service is in progress. However, unless the object graph starting
from the root object is scanned, there is no straightforward
approach that can identify whether an object of persistent
class needs to be serialized. To avoid the application of write
barrier to all objects, we may take either of the following two
approaches: the ﬁrst approach is to apply write barrier on
methods that update persistent ﬁelds of a potential persistent object, and the second approach is to mark persistent
objects in advance.

}

[ SPersistentUpdate ]
set {
x = value ;
}

p u b l i c v o i d Bar ( ) {
...
XP=10;
PNodes [ i ]=Baz ;
...

}
}
public c l a s s IxVertex {
[ SPersistentField ]
p u b l i c V er tex [ ] nodes ;
p u b l i c I x V e r t e x ( V er tex [ ] param1 ) {
nodes = param1 ;
}
p u b l i c V er tex t h i s [ i n t i n d e x ] {
get {
r e t u r n nodes [ i n d e x ] ;
}

}

}

[ SPersistentUpdate ]
set {
nodes [ i n d e x ] = v a l u e ;
}

Listing 3: An example persistent class with method
annotations

3.2.1 Annotating Methods that Update Persistent
Objects

However, if the methods speciﬁed with the [SPersistentUpdate] attribute conduct update operations on nonpersistent ﬁelds as well as persistent ﬁelds, the write barrier
is unnecessarily applied to the update operations on nonpersistent ﬁelds. The solution is to use a wrapper method
designated for updating a persistent ﬁeld, and to make other
program codes call the method to update the ﬁeld. To realize that solution, we recommend to deﬁne Property for each
persistent ﬁeld, and access the ﬁeld through its corresponding property. Property in C# is an interface to provide
a direct access of a class ﬁeld through two methods: Get
method to return the value of the ﬁeld, and Set method to
write a value to the ﬁeld. Thus, specifying [SPersistentUpdate] to the property’s set method can conﬁne write barrier
to the persistent ﬁeld’s update. For example, in Listing 3, an
integer type ﬁeld x is a persistent ﬁeld and XP is a property
to the ﬁeld x. The attribute [SPersistentUpdate] on a set
method of the property XP allows to activate write barrier
on the set method while a persistence service is in progress.
If a persistent ﬁeld is array (actually a reference to an array object in CLI), the update operation on each element of
the array should trigger the write barrier. However, the set

Under this scheme, the methods that update persistent
objects are annotated (hereafter ”Annotating SP”) such that
a write barrier becomes eﬀective when the methods are invoked during a persistence service interval. Then, the write
barrier performs two operations on the object encountered:
a test operation for the serialization status of the object and
a serialization operation if it is not yet serialized. The annotation can be done by the programmer or by compiler when
a deﬁnition of persistent ﬁelds is detected. When it is done
by the programmer, the approach relies on users’ descriptions to eﬀectively restrict the range of write barrier to likely
update operations on persistent ﬁelds of persistent objects.
Because not all ﬁelds in persistent classes are persistent, applying write barrier on methods that update persistent ﬁelds
can narrow down the eﬀective range. As shown in Listing
3, the attribute [SPersistentUpdate] is used to specify the
annotated method set. This scheme can be also applied in
JVM given that annotation is supported since JDK 1.5 as
a result of JSR 175, ”a metadata facility for the Java programming language” [24].
[ SPersistent ]

199

method of the property corresponding to an array reference
does not invoke a write barrier on access to the element of
the array. A solution is to deﬁne an Indexer class for an
array in C#. This approach allows the instances of a class
or struct being indexed in the same way as arrays. Indexer
consists of get/set methods similar to properties except that
their accessors take the indices to the elements such that
the elements are guarded. In Listing 3, for example, a class
IxVertex is an Indexer class deﬁned to provide get/set methods to an access to array’s elements.
The limitation of the Annotating SP is that it may consider nonpersistent objects as persistent objects. For example, a nonpersistent object whose class happens to be a persistent class but it is not reachable from the persistent root
objects can be still protected by the write barrier. Although
it can cause unnecessary overhead, it does not violate the
consistency or integrity of persisted data because the nonpersistent objects mistakenly serialized will be deserialized
as dead objects that are unreachable via any live objects
and are the subject of garbage collection. The another limitation of this scheme is that any updates to persistent ﬁelds
must be done through properties or indexers although in
C#, properties and indexers are recommended to provide
encapsulation. However, we can also rely on compiler techniques to insert write barrier on updates to persistent ﬁelds
or deﬁne properties or indexers as shown in Listing 3.

SP Task
Serializing
Buf

Nonvolatile
RAM

WriterTask
Writing
Buf
Filesystem

Persistent Objects

PersistedData
Buffer

Persistent
Storage

Figure 2: Structure of the proposed Schedulable
Persistence System

3.3 Structure of Data Persistence
The architecture for schedulable persistence system is devised to aim at a general persistence system that can be
applied regardless the types of persistent storage. The data
persistence system consists of two tasks, SP task, and Writer
task in Figure 2. SP task traces persistent objects, serializes the state of the objects, and then writes the persisted
data into a buﬀer. On the other hand, Writer task moves
the data from the buﬀer to a persistent storage. The system
maintains two buﬀers: one buﬀer can be the SerializingBuf
to where SP task writes persisted data; and the other buﬀer
will be the WritingBuf read by Writer task to move persisted data to persistent storage. The roles of the buﬀers
are switched back and forth such that the write and read
operations are performed concurrently. Hence, a smooth
data transfer of persisted data to storage devices can be accomplished.

3.2.2 Marking Persistent Objects In Advance
To identify precisely the persistent objects, the persistence
service can mark persistent objects in advance before serializing the objects. The approach (hereafter, ”Marking SP”)
takes two phase: Marking Phase and Serialization Phase.
In marking phase, SP task traces and marks all persistent
objects in the object graph. In serialization phase, all the
marked objects get serialized. Both phases are preemptible
and are protected by a write barrier. The operations of the
write barrier is diﬀerent according to the phases. In marking
phase, the write barrier tests the serialization status of the
object encountered, and then in serialization phase, write
barrier tests the mark status of the object, and serialize the
object if it is marked and is not yet serialized. The write
barrier in making phase does not check the mark status because marking on objects is underway. It may generate some
false-serialized objects, but they may not be many because
the marking phase usually takes a short amount of time
comparing to the serialization phase.
This scheme can conﬁne the write barrier on any update
operations to persistent objects. However, the overhead is
that all update operations in marking phase conducts test
operation for the object encountered. That can be implemented with a fast path like inline codes. The study on cost
of write barrier using a fast path [3] shows that the cost
is tolerable. Unlike Annotating SP, this does not give programming burden to limit access to persistent ﬁelds through
properties or indexers.
In summary, either Annotating SP or Marking SP can
be used to distinguish persistent objects from nonpersistent
objects and to eﬃciently narrow down the eﬀective range
that the guard operation by a write barrier is applied. The
choice among two approaches might depend on the amount
of checking operations and the annotations provided by programmers.

4. IMPLEMENTATION
We implemented a prototype of schedulable persistence
system in CLI’s MONO platform version 1.1. The major effort in the implementation includes the construction of the
SP task designated for persistence service and write barrier
schemes. The SP task starts to serialize objects when SendPData API gets called as stated in Listing 2. As illustrated
in Figure 1, persistence service is performed by conducting
small increments of the service periodically. Runtime parameters are set with the targeted pause time of each persistence increment and the period of persistence increment.
During each persistence increment, the minimum work
unit of serialization is a single object; that is, the pause
time of a persistence increment can be as small as the duration taken to serialize a single persistent object. Persistent
object graphs are traced through a breadth-ﬁrst search, and
any object types including strings and array of strings are
treated as single persistent object to keep a minimum work
unit of a single object. An internal queue is employed to
buﬀer the persistent objects encountered when the object
graph is traced. During each persistence increment, depending upon the target pause time, a number of objects from
the queue are traced and serialized. To access the ﬁelds of
a persistence object, persistent class map is used to quickly
locate persistent ﬁelds within persistent objects. The map
is organized as a linked list of all persistent ﬁelds speciﬁed

200

with attribute [SPersistentField]. In our prototype implementation, the map is built while loading a persistent class
in order to avoid any delay during serialization.
To serialize persistent objects, the state of objects as well
as their reference relationship must be preserved. Each reference points to a corresponding object using an actual memory address (MID). In persistence storage, each MID must
be converted to a unique logical address (LID). We maintain the pairs of MID and LID for all persisted objects in
a hash table MID2LID. Whenever an object is encountered
during the breadth-ﬁrst search of persistent object graph,
MID2LID is looked up such that references to shared objects
can get converted by the same LID. In addition, MID2LID
table keeps the status of each object indicating whether it
has been serialized or not. The status is encoded by using MSB (most signiﬁcant bit) of LID in the table to save
memory space.
To invoke write barrier when a serialization gets started,
we consider the instructions deﬁned in Common Language
Instruction (CIL). Among the 250 instructions, only stﬂd
and stelem.{i,i1,i2,i4,i8,r4,r8,ref} are used to store a new
value in a ﬁeld of an object and in a vector element, respectively1 . To protect any update operations through write
barriers, MONO’s JIT compiler is modiﬁed such that the
two instructions are translated into CLI’s internal functions.
Each of these functions calls indirectly to a update function
through a pointer which is altered according to the presence
of concurrent persistence service. In other words, the update function with write barrier is invoked when concurrent
persistence service is underway, otherwise the one with no
write barrier is called. This approach is applied to both Annotating SP and Marking SP. However, the former modiﬁes
the implementation of stﬂd and stelem instructions when
they are a part of the methods annotated with [SPersistentUpdate]. The translated code is not altered if the stﬂd and
stelem instructions are not done by an annotated method.
On the other hand, in the latter scheme, the translated codes
for all stﬂd and stelem instructions are modiﬁed. Then, object mark is tested to trigger a write barrier.
The operation of de-serialization is completed in two phases.
In construction phase, objects are re-constructed based on
serialized data and a hash table with pairs of LID and MID
is established. In the following ﬁx-up phase, the LIDs are
ﬁxed up with corresponding memory address (MID) to recover the reference relationship between objects. Only the
objects with reference member ﬁelds are enqueued into a ﬁxup cache in construction phase so that the ﬁx-up operation
is limited to the objects with reference ﬁelds.
Unlike serialization, in our current implementation, deserialization does not work in a concurrent mode; that is,
when deserialization for recovery is requested due to an application’s failure, the recovered application’s execution can
resume once deserialization over the entire serialized data
completes. If the recovery, due to large amount of serialized
data, places a long latency beyond the application’s timely
requirements, the timeliness of the application cannot be
guaranteed even with persistence service. One of solutions
over this problem might be to resume the execution of a recovered application with a partial recovery once the partial
1
CIL deﬁnes additional store instructions such as stsﬂd,
stind.type, starg.length, and stloc. However, these store
instruction do not require a write barrier because their store
operation is not conducted on instance objects.

201

recovery initializes the application enough to resume. To allow a partial recovery, we can apply on-demand object deserialization and lazy pointer swizzling [2]: on-demand object
deserialization deserializes objects when they are referred,
and lazy pointer swizzling converts LID to a correspondent
MID when the object represented with the LID is actually
accessed. Other technique to be considered is In-Place Object Deserialization which is suggested as a method of serialization/deserialization for Java RMI (Remote Method Invocation) in [7]. The in-place object deserialization conducts
deserialization without allocation and copying of objects by
reusing a buﬀer allotted to serialized data for deserialized
object. We leave considering a partial recovery as a future
work.

5. EXPERIMENTS
Experiments in the prototype persistence system are conducted to examine the performance of the proposed design approaches. The experiment is done in a PC workstation with 1.5GHz Pentium IV processor and 256MB memory. To have a high resolution timer and preemptive kernel,
TimeSys’ Linux/Real-Time(v4.1.147)[26] is used.
The experiments basically follow a comparative study among
the performance measures of serialization and deserialization operations. MONO’s serialization library and the proposed schedulable persistence (SP) service with the Annotating SP and Marking SP write barrier schemes are tested.
We collect both the maximal and average measures after
running the experiments 40 times. If a speciﬁc measure is
common to Annotating SP and Marking SP schemes, we
simply denote it as a SP measure for the proposed schedulable persistence system. Whenever SP service and applications run concurrently in Linux threads, the SP thread is
assigned with a higher priority than application threads.

5.1 Performance of Serialization and
Deserialization
The performance of serialization and deserialization operations using MONO’s serialization library, Annotating SP,
and Marking SP, are collected by running in the stop-theworld mode. Since the time needed to complete the operations depends upon the object types as well as the number
of persisted objects, the serializations of a red-black tree of
composite objects (consisting of multiple primitive ﬁelds),
and an array of a primitive type (integer) are benchmarked.
The average response times of serialization and deserialization for the red-black tree and the array of a primitive type
are shown in Figure 3, Figure 4, Figure 5, and Figure 6
respectively.
The results in the ﬁgures clearly indicate that, in terms of
the response time of the serialization and deserialization operations, the proposed schedulable persistence system (with
either Annotating SP and Marking SP) outperforms the
MONO’s serialization library signiﬁcantly. The performance
improvement is derived from two main factors. First, the SP
serialization is implemented as a subsystem of CLI by deﬁning native functions which is much more eﬃcient than the
serialization library of managed code. The second factor is
the use of persistence class map which allows us to avoid
costly reﬂection mechanism.
Given that the experiments were run in the stop-of-theworld mode, the eﬀect of write barrier schemes is not presented in the results. However, comparing the Annotating

600

130
Serialization Library
Annotating SP
Marking SP

120

Serialization Library
Annotating SP
Marking SP

500
110

Avg. Serialization Time (ms)

Avg. Serialization Time (ms)

100
400

300

200

90

80

70

60

50
100
40

0
5000

10000

15000
Number of Persistent Objects

20000

30
5000

25000

Figure 3: Serialization of Tree

10000

15000
Array Size

20000

25000

Figure 5: Serialization of Array of Primitive type
90

600
Serialization Library
SP

Serialization Library
SP

80
500

Avg. Deserialization Time (ms)

Avg. Deserialization Time (ms)

70
400

300

200

60

50

40

30
100

20

0
5000

10000

15000
Number of Persistent Objects

20000

10
5000

25000

Figure 4: Deserialization of Tree

10000

15000
Array Size

20000

25000

Figure 6: Deserialization of Array of Primitive type
The benchmarked application mainly constructs a directed
weight graph, which contains 1326 persistent objects. The
total amount of serialized data is about 43KBytes. The application works as follows. First, the application constructs
a directed weight graph of nodes. After the construction is
complete, it triggers a persistence service. Then, the application starts to make changes on the connections of nodes
and then delete all the nodes of the graph one by one. In
other words, in preemptible mode, the SP task serializing
the graph runs concurrently with a mutator that updates
the persistent objects.
The experiment results are shown in Table 1. It indicates
that the schedulable persistence system by using both write
barrier schemes is able to control the pause time of each
persistence increment to meet the targeted bound of 500µs.
The average execution time of serialization in preemptible
mode is slightly higher that that in the stop-the-world mode.
This diﬀerence is somehow expected given additional overhead caused by context switches and controlling the pause
time of each increment.
The total execution times of the application of the Marking SP in both stop-the-world mode and preemptible mode
are higher in than that of the Annotating SP. In both write
barrier schemes, the CIL update instructions, stﬂd and stelem,
are translated into internal functions during JIT (runtime
compilation). Then, depending upon whether a serialization is in progress or not, the internal functions call actual
update functions with either a write barrier or without a

SP and Marking SP schemes, we found that Marking SP has
a 30% more overhead than Annotating SP especially for applications with many reference objects. This is mainly due
to the diﬀerence of the invocation numbers of internal functions translated from stﬂd, and stelem. More detail is given
in 5.2. Furthermore, the extra marking phase before conducting serialization in Marking SP scheme also contributes
to that. On the other hand, in this experiment, the size of
the serialized binary data in SP is not much diﬀerent from
that of serialization library because in each of the benchmarked applications there is only one persistent class and
the metadata preserved is only a small portion of serialized
data.

5.2 Controlling the Pause Time of Persistence
Increments
The aforementioned experiments illustrate the execution
time of the schedulable persistence system. To support realtime applications, we need to examine whether the proposed
schedulable persistence system has a behavior following the
scheduling model of Figure 1. The experiments conducted
are to apply the two write barrier schemes in two diﬀerent execution modes, stop-the-world mode (STW) and preemptible modes. For the preemptible mode, the targeted
pause time of each persistence increment and the period
of persistence increment are set to 500µs and 3ms, respectively.2
2
This experiment focuses on measuring the ﬂexibility of persistence increment’s granularity and the overhead due to

persistence service’s preemptivity so that these numbers,
500µs and 3ms, are arbitrarily chosen in that sense.

202

Properties

Annotating SP
STW
500µs

Marking SP
STW
500µs

Marking SP(v2)
STW
500µs

Max. pause time per increment(µs)

5171

453

6162

478

6344

478

Avg. pause time per increment(µs)

5056

401

6084

388

6257

398

Avg. persistence cycle (ms)

NA

39

NA

48

NA

48

Max. serialization time(µs)

5171

6056

6162

6758

6344

7367

Avg. serialization time(µs)

5056

5632

6084

6612

6257

6859

Max. application exec. time(ms)

300

301

374

390

302

309

Avg. application exec. time(ms)

279

282

351

366

272

284

Avg. No. of Increments

1

14

1

17

1

18

Avg. No. of WB invocations

0

1284

0

185762

0

16284

Avg. No. of WB Serializations

0

24

0

41

0

50

Table 1: Pause time and overhead of schedulable persistence system
write barrier, accordingly. The main diﬀerence between the
two schemes is the number of CIL update instructions translated into the internal functions. Annotating SP limits the
instructions to those in the methods annotated with [SPersistentUpdate] whereas all stﬂd and stelem instructions, including the ones in standard class libraries, are converted
into the internal functions in Marking SP. The overhead incurred in the execution of the internal functions results in a
longer execution time of the application in Marking SP. Furthermore, that Marking SP has a marking phase in advance
which also aﬀects the execution time for serialization.
To ﬁnd out a compromised solution for the added cost of
internal functions for write barrier, we experiment a modiﬁed Marking SP scheme in which only the stﬂd and stelem
instructions in the methods of persistence classes are translated into the internal functions. The performance is shown
in the Marking SP(v2) column of Table 5.2. This modiﬁcation reduces the number of invocations of the internal
functions signiﬁcantly and leads to a total execution time
of the applications similar to that of Annotating SP. It indicates that even though the execution time in Marking SP
is high, it can be adjusted by deﬁning the scope of the update operations to persistent ﬁelds. If persistent ﬁelds are
updated through the methods of persistent class, a write
barrier on the methods of persistent class is able to provide
a suﬃcient guard without an excessive overhead.
Besides execution times, the numbers of increments to
complete a persistence cycle and the length of a persistence
cycle are presented in Table 1. In addition, the numbers of
invocations of write barrier operations, and the numbers of
objects serialized during write barrier operations are given.
The distinct characteristics revealed on the two measures is
discussed in the following subsection 5.3.

STW

Annotating
SP

Marking
SP

Avg.No. of persistent objs.

1326

1326

1326

Avg.No. of serialized objs.

1326

1677

1326

Avg.No. of WB invocations

0

2643

240912

Avg.No. of WB serialization

0

339

13

Table 2: Checking and serialization operations of
the two write barrier schemes
persistent root object before deleting any nodes. The second
one has no persistent objects but using the same persistent
class as the ﬁrst thread. In the third thread, a diﬀerent class
is used to instantiate the objects in the graph and no persistence service is invoked. Among three threads, only the
ﬁrst one triggers a persistent service after the construction
of the graph. The application has 1326 persistent objects
out of a total of 3978 objects.
The results of the experiment is shown in Table 2. The
data indicates that Marking SP is able to precisely pinpoint
persistent objects. On the other hand, extra 351 nonpersistent objects accessed by the 2nd thread are serialized in Annotating SP. These extra objects happen to be the same class
of the persistent objects and are updated by the annotated
methods when the 1st thread requests a persistence service.
However, because Annotating SP limits the write barrier
to the annotated methods, the number of invocation to the
write barrier operations is much less than that in Marking
SP. Note that, in Annotating SP, the average number of
objects serialized by the write barrier operations, i.e. 339,
is less than the average number of the extra nonpersistent
objects serialized by the scheme. This should not happen
if nonpersistent objects can only be serialized (mistakenly)
by the write barrier operations. In fact, an object serialized
by the write barrier operations may have references to other
objects. These referenced objects must also be serialized
and are placed in the internal queue such that the SP task
can trace the object graph through the breadth-ﬁrst search.
Thus, extra nonpersistent objects may be preserved by the
SP task during the subsequent persistence increments.

5.3 Objects Serialized by Annotating SP and
Marking SP
In this experiments, we look into how the two schemes,
Annotating SP and Marking SP, eﬃciently distinguish persistent objects from nonpersistent objects. To have mixed
persistent and nonpersistent objects in heap memory, the
benchmark application consists of three threads and each
of which ﬁrstly constructs a directed graph of nodes and
then deletes all the nodes one by one. Each graph is diﬀerent in the perspective of persistence. The ﬁrst one makes
all objects in the graph persistent by making its root node a

6. CONCLUSION & FUTURE WORKS
In this paper, we aim for a schedulable persistence sys-

203

[10] H. Evans. Why Object Serialization is Inappropriate
for Providing Persistence in Java. Technical report,
Department of Computing Science, University of
Glasgow, Glasgow, 2000.
[11] S. Ghosh, R. G. Melhem, D. Mosse, and J. S. Sarma.
Fault-tolerant rate-monotonic scheduling. Real-Time
Systems, 15(2):149–181, 1998.
[12] S. Ghosh, R. Mellhem, and D. Mosse. Enhancing
real-time schedules to tolerate transient faults. In
IEEE Real-Time Systems Symposium, pages 120–129,
1995.
[13] J. Gosling, B. Joy, G. Steele, and G. Bracha. The Java
Language Speciﬁcation. Addison-Wesley, 2nd edition,
2000.
[14] M. Hericko, M. B. Juric, I. Rozman, and A. Zivkovic.
Object serialization analysis and comparison in Java
and .NET. ACM SIGPLAN Notices, 38(8):291–312,
2003.
[15] H. Lee, H. Shin, and S.-L. Min. Worst case timing
requirement of real-time tasks with time redundancy.
rtcsa, 00:410, 1999.
[16] K. Li, J. F. Naughton, and J. S. Plank. Real-time,
concurrent checkpoint for parallel programs. In Second
ACM SIGPLAN Symposium on Principles and
Practice of Parallel Programming (2nd PPOPP’90),
SIGPLAN Notices, pages 79–88, Mar. 1990.
[17] T. Lindholm and F. Yellin. The Java Virtual Machine
Speciﬁcation. Addison-Wesley, 2nd edition, 1999.
[18] Microsoft Corp. Microsoft .NET framework
binaryformatter serialization format, 2002.
[19] M. Monga and A. Scotto. A generic serializer for
mobile devices. In Proceedings of the 20th annual
ACM symposium on applied computing, Santa Fe, New
Mexico, USA, 2005.
[20] M. Philippsen and B. Haumacher. More eﬃcient
object serialization. In IPPS/SPDP Workshops, pages
718–732, 1999.
[21] S. Punnekkat, A. Burns, and R. Davis. Analysis of
checkpointing for real-time systems. Real-Time
Systems, 20(1):83–102, 2001.
[22] R. Riggs, J. Waldo, A. Wollrath, and K. Bharat.
Pickling state in the Java(TM) system. USENIX,
Computing Systems, 9(4):291–312, 1996.
[23] T. Suezawa. Persistent execution state of a java
virtual machine. In Java Grande, pages 160–167, 2000.
[24] Sun Microsystems, Inc. JSR175 : a metadata facility
for the java programming language, 2005.
http://www.jcp.org/en/jsr/detail?id=175.
[25] Sun Microsystems Inc. Sun Real-Time Java System,
2005. http://java.sun.com/j2se/realtime.
[26] TimeSys Corporation. Timesys linux/real-time user’s
guide, version 2.0, 2004.
[27] B. Venners. Inside the Java Virtual Machine.
McGraw-Hill, 1999.
[28] Ximian. MONO. http://www.go-mono.com.

tem to support object persistence for real-time applications
in VMs. The system is designed with a concurrent persistence service task to avoid any long pause delay caused
by sequential operation of serialization. To guarantee the
consistency of persistent objects due to the concurrent operations of persistence service task and application tasks,
write barrier schemes are devised. A prototype system, involving the modiﬁcation of JIT and rewriting the serialization operation, is constructed in CLI’s open source platform,
MONO. The experiments show a signiﬁcant performance
gain resulted from the native function implementation for
serialization operation and a replacement of reﬂection mechanism for class introspection. In particular, we are able to
demonstrate the bounded pause time for each persistence increment in the prototype system and the modest overhead
incurred in write barrier operations.
Based on the insight gained from this study, our future
work is to establish a cost model for the persistence service. The model is to estimate the worst case execution
time of persistence service based on number of persistence
objects, the class information of the objects, and their reference dependency. The model can then be used to schedule
persistence cycle and increments, as well as to design a suitable scheduling algorithm for application tasks subject to
recovery constraints.

7.

ADDITIONAL AUTHORS

Additional authors: Elliott Rachlin (Honeywell International Inc., email: elliott.rachlin@honeywell.com).

8.

REFERENCES

[1] Aonix North America, Inc. PERC, 2006.
http://www.aonix.com/perc.html.
[2] M. Atkinson and M. Jordan. A review of the rationale
and architectures of PJama: a durable, ﬂexible,
evolvable and scalable orthogonally persistent
programming platform. Technical Report TR-2000-90,
Sun Microsystems Laboratories and Dept. Computing
Science, Univ. Glasgow, UK, 2000.
[3] S. M. Blackburn and A. L. Hosking. Barriers: friend
or foe? In ISMM, pages 143–151, 2004.
[4] G. Bollella, J. Gosling, B. Brosgol, P. Dibble, S. Furr,
and M. Turnbull. The Real-Time Speciﬁcation for
Java. Addison-Wesley, 2000.
[5] S. Bouchenak, D. Hagimont, S. Krakowiak, N. D.
Palma, and F. Boyer. Experiences implementing
eﬃcient java thread serialization, mobility and
persistence. Softw., Pract. Exper., 34(4):355–393,
2004.
[6] F. Breg and C. D. Polychronopoulos. Java virtual
machine support for object serialization. In Java
Grande, pages 173–180, 2001.
[7] C.-C. Chang. Safe and Eﬃcient Cluster
Communication with Explicit Memory Management.
PhD thesis, Cornell University, 1999.
[8] A. Cunei and J. Vitek. A new approach to real-time
checkpointing. In Proceedings of the 2nd ACM Virtual
Machine and Execution Environments Conference
(VEE 2006), June 14-16, 2006 Ottawa, Canada.
ACM, 2006. (to appear).
[9] ECMA. Ecma-335 common language infrastructure,
2002.

204

On the Existence of Probe Effect in Multi-threaded Embedded
Programs
Young Wn Song and Yann-Hang Lee
Computer Science and Engineering
School of Computing, Informatics, and Decision Systems Engineering
Arizona State University, U.S.A.
Tempe, AZ

Abstract — Software instrumentation has been a convenient and

there is any change on program execution paths caused by the
timing perturbation.

portable approach for dynamic analysis, debugging, or profiling
of program execution. Unfortunately, instrumentation may
change the temporal behavior of multi-threaded program
execution and result in different ordering of thread operations,
which is called probe effect. While the approaches to reduce
instrumentation overhead, to enable reproducible execution, and
to enforce deterministic threading have been studied, no research
has yet answered if an instrumented execution has the same
behavior as the program execution without any instrumentation
and how the execution gets changed if there were any. In this
paper, we propose a simulation-based analysis to detect the
changes of execution event ordering that are induced by
instrumentation operations. The execution model of a program is
constructed from the trace of instrumented program execution and
is used in a simulation analysis where instrumentation overhead is
removed. As a consequence, we can infer the ordering of events
in the original program execution and verify the existence of
probe effect resulted from instrumentation.

To obtain a proper observation of multi-threaded program
execution through instrumentation, it is critical to know the effect
of timing perturbation. However, unless we adopt hardware based
monitoring, it might not be possible to know the exact execution
behavior of a program. It may be argued that a comparison of
computation results from instrumented and un-instrumented
programs can reveal any different behavior of program execution.
Note that some benign program behavior may not affect the final
computation results, for instance, a branch decision can be caused
by different conditions in a compound conditional expression. In
addition, for embedded systems, it may be tricky to manage
identical external inputs arriving at the precise instants of the
execution. Another approximation is to measure the overhead of
instrumentation, calculate the execution time by removing the
overhead, and infer the real execution. In a single thread program,
this would be feasible. However, in multi-threaded programs it is
extremely difficult to consider all thread interactions when thread
execution time is changed. Moreover, it is problematic to take into
account kernel states (e.g. run queue state) which may affect
scheduling decision.

Keywords

- multi-threaded program; event ordering;
reproducible execution; probe effect; profiling; simulation.
1

There have been several approaches to recover the
performance of parallel programs by compensating the
instrumentation overhead [4][5], but they do not examine the
ordering of the program execution. On the other hand,
deterministic replays [6][7][8] provide reproducible execution that
guarantees the same execution ordering as the one observed in a
recording operation. It is perceivable that any recording operation
should incur some instrumentation overhead since the execution of
the recording itself would have caused perturbation to the original
execution. Nonetheless no research on deterministic replay
examines the issue of any changes to the original program
execution behavior caused by the recording operation.

INTRODUCTION

In real-time embedded systems, application tasks usually run
in concurrent threads. Threads may interrelate with each other as
they share resource and data. They also interact with the external
environment to receive sensor data and to control actuators. While
the threads are running, any instrumentation to observe program
execution behavior will introduce extra overhead to the execution.
Instrumentation overhead, no matter how small it is, may intrude
and change the execution behavior of the program and,
consequently, introduce probe effect [1][2]. Hence the observed
behavior through instrumentation is not guaranteed to represent
the original program behavior.

In this paper, we propose a simulation-based analysis for
embedded software to detect any variations of event ordering
caused by instrumentation overhead. It is assumed that application
tasks are performed by concurrent threads in a priority-based
preemptive scheduling system. The analysis starts with an
instrumented program (e.g., for dynamic program analysis) from
which we want to analyze the impact of instrumentation overhead
to the program execution. The instrumented program is again
instrumented to obtain traces including thread interaction events
and the overhead of instrumentation code. Kernel’s activities as
well as external inputs are also recorded. From the traces, we
construct a simulation of program execution where the
instrumentation overhead is removed. Timing of thread events is
calculated which decides the ordering of events in the simulated

Instrumentation operations can perturb program execution in
two ways. First, the occurrence of an execution event is delayed by
the amount of time spent on running instrumented code. This can
change the time of interacting with other threads and external
environment (e.g. reading an input). Second, due to the changes of
the timing of invoking guarded resources and critical sections,
scheduling decision can be different. This, in turn, can lead to the
variations in the sequential order of accessing shared resources.
Therefore, the timing perturbation by instrumented code can result
in a different happen-before ordering of events [3] and possibly a
different program execution path from the original program. The
other related issue is that we may not be able to know whether

1

run. Then the ordering information of the original program
execution with no instrumentation overhead is projected. The
contributions of this work are:

and replays synchronization operations. In the approach, data races
can be detected by checking all shared memory references so that
the program is free of data race before record/replay operations.
Replay Debugger [18] uses a similar method as RecPlay but it
focuses on debugging techniques on embedded software. It is
integrated with GDB and can supply additional debugging
functionalities for users to control thread execution. To reduce
overhead of record and replay, speculative execution and external
deterministic replay are used in Respec [19] that is capable of
online replaying on multiprocessor systems even with data races.
Using speculative execution, the recording process can continue to
execute speculatively instead of being blocked until the
corresponding replay finishes.

1. It provides a novel way of detecting changes in event ordering
due to probe effect.
2. Accurate timing of event occurrence is simulated with the
consideration of all factors affecting the ordering of program
execution, including kernel activities, external inputs, as well
as the thread execution time.
3. It provides an analysis of simulation results for inferring the
ordering of the original program execution.
The rest of paper is organized as follows. In Section 2, a
concise survey of related works is presented. Section 3 describes
the design of the simulation analysis. The implementation details
are explained in Section 4. Section 5 shows experimental results.
The paper is concluded in Section 6.

2

Most profiling tools adopt instrumentation approaches. There
have been research efforts to reduce profiling overhead caused by
instrumentation. Froyd et al. proposed a call-path profiler based on
stack sampling [20]. The profiler, called csprof, provides an
efficient way of constructing the calling context tree without
instrumenting every procedure’s call. Zhuang et al. introduced the
adaptive busting approach [21] to build calling context tree with a
reduced overhead while preserving profiling accuracy. In their
approach, unnecessary profiling is avoided by disabling redundant
stack-walking with a history-based predictor. The profiling
overhead has been further alleviated by taking advantage of multicore systems. In shadow profiling [22], shadow processes are
periodically created for running instrumented code while the
original process is running on a different core with minimal
overhead. PiPA [23] exploits parallelism by forming a pipeline to
collect and process profiles. Application execution and profiling
operation are divided into stages that are pipelined and performed
in multiple cores. Kim et al. proposed a scalable data dependence
profiling to reduce runtime and memory overhead [24] by storing
memory references as compressed formats and using pipelining
and data level parallelism for the data dependence profiling.

RELATED WORKS

Malony et al. presented the instrumentation uncertainty
principle [4] suggesting that the accuracy of execution
performance is degraded as the degree of instrumentation
increases. Performance perturbation models were proposed to
calculate the true performance from instrumented parallel
programs. The models were further refined in [5]. In the models,
perturbations trigger a change in event execution time and event
ordering is represented by time-based and event-based
perturbation models. In the time-based model, the thread events
are independent while the event-based model considers the
dependency between events for recovering the true performance.
The dependency considered is performance degradation as arrival
time and resource state change. However, the approach assumes
the program execution is fixed no matter there is any
instrumentation overhead or not. Hence, it does not consider how
the program behavior may differ from a un-instrumented program.

Although there are research works on the overhead
compensation for performance measurement and the reduction of
instrumentation overhead for reproducible execution and profiling,
no work has been proposed to reveal the possibility of execution
deviation caused by instrumentation overhead. In this paper, the
focused analysis is to verify if the recorded or observed execution
is a true representation of the original program execution and if
any instrumentation may alter the event ordering of multi-thread
embedded programs as to cause any changes of the intended
program behavior.

When instrumentation perturbation causes different thread
interleaving, we will be concerned with the potential problems of
data race and execution non-determinism. Data races can result in
arbitrary failures and do not help increase scalability [9]. Several
efficient dynamic data race detection algorithms [8][11] have been
proposed and race detection tools [12][13] are widely used in
practice. In general, the approaches are based on the monitoring of
read/write operations to shared variables among concurrent
threads. However, the delay caused by monitoring operations and
any possible probe effect have not been addressed. Deterministic
multi-threading techniques [14] provide deterministic event
ordering for parallel program execution. In Kendo [15], a thread’s
progress is represented with a logical clock. It is a thread’s turn
when its logical clock is the global minimum and a thread can take
a lock only during its turn. In [16] and [17], thread’s shared
memory is isolated from other threads during a parallel phase.
During a serial phase, the memory updates to shared variables are
applied and locks are taken in a deterministic order. Regardless
their overheads, the approaches don’t consider any external input
events and time-based operations, and cannot be applicable to
embedded software.

3

DESIGN AND ANALYSIS

3.1 Multi-threaded Program Execution
Multi-threaded program execution can consist of a set of
thread interaction events. Such an event, as a sequence of
instructions that the program executes, defines a particular action
(e.g. a system call) to interact with other threads, internal and
external environment. Examples include synchronization, and
communication operations, and IO read/write calls. The events can
be totally ordered by the timestamps at which the events take
place. A partial order can also be defined among the events based
on logical dependencies, i.e., happened-before relation [3].

It may be argued that instrumentation can be done during
program replay if a reproducible execution can be constructed.
Instant Replay [6] is one of the earliest works that allows cyclic
debugging for parallel programs by tracing and replaying relative
order of events for each shared object in the program. RecPlay
[7][8] based on Lamport’s happens-before relations [3] records

Let’s consider multiple runs of a program and the ordering of
interaction events from each run. If the execution events happen at
different instants, the happened-before ordering of the events may
be different from one run to the other. This may lead to a change

2

global state. The other important incident is when “e happens”. It
symbolizes the result of execution, as event e, is posted and is
available to subsequent execution. Obviously, the invocation of
event functions by a thread form a sequence and an event
happened previously can causally affect any subsequent events [3].

of program execution behavior. For instance, in the sleeping
barber problem, customers may be served in different orders when
they arrive at different instants in separate runs. On the other hand,
a particular customer may find out the waiting room is full and
miss the service in one run. In the other run, if the barber cuts hair
quickly, the customer can find a seat in the waiting room and
receive the service eventually. In this case, the execution path of at
least one thread is changed that results in a different timestampbased ordering of events at thread level.

To include OS and device activities in the model, a system
thread, T0, is added. The events that occur in T0 consist of
interrupts, OS scheduling, and the arrivals and updates of device
input data. Thus, an interrupt event of T0 may set a semaphore and
wake up a waiting application thread. Also, an application thread
can read in the latest sensor data if the read operation happens after
a data update event in T0.

It is a well-known principle used in deterministic replays
[7][8] that, for two runs of a data race free program, if we supply
the same input data and have the same happened-before ordering
of events, then the two runs must result in the same behavior.
Conversely, if we observe two distinguished runs of a program,
then either inputs and/or the happened-before ordering of one run
must be different from those of the other run.

To define the occurrence instants, we adopt two types of
clock. Clock C indicates the CPU cycles used globally. Starting
from 0, it is advanced for all activities consuming CPU cycles,
including thread execution, O/S activities such as interrupt handler
and scheduler, and idle process. In addition, Ci is the thread local
clock for thread Ti and is advanced only during the execution of Ti.
Hence Ci represents the CPU time spent on the execution of Ti.
With the clocks, we define:

During an execution of a program, a happened-before ordering
of program events can be dependent upon the external inputs it
receives, including input data value and the timing that new data
arrives. If we have the same inputs, the happened-before ordering
of the program events is decided by the execution instants of
threads events on shared resources and communication. For
example, when two threads compete for a resource, the happenedbefore ordering of the locking events will be decided the instants
that the two threads issue their resource requests. The choice on
who is going to take the resource first may also depend upon the
kernel’s scheduling policy, e.g. priority based or FIFO, as well as
the kernel’s internal states, including run queue state and other
tasks running in the system including interrupt handlers. So, when
a program is instrumented, we can expect that more CPU time is
spent to execute instrumentation code and the execution time of
each thread becomes longer. This may have a ripple effect on the
instants that program events may take place, and the happenedbefore ordering of the events.

ht(f) and ht(e) as the C clock values when f is invoked and e
happens, respectively. This is the global timestamp for a call to
function f and for event e.
CTi(f) and CTi(e) as the Ci values when Ti enters fi and when ei
happens as a result of Ti’s execution, respectively.
Based on CTi(f) and CTi(e), we can compute ct(fi,k) which is
thread Ti‘s execution time between the instants that the (k-1)-th
event ei,k-1 happens and that the subsequent function invocation fi,k
is entered. Similarly, ct(ei,k) can be obtained as the processing time
from entering fi,k to posting ei,k. As shown in Figure 1, an example
execution of the interacting events performed by threads T1 and
T2 and event timestamps are depicted. In addition to the thread
local clocks for T1 and T2, thread events are aligned with the
global clock and the kernel scheduling and interrupt service events
are included.

3.2 Model of Multi-threaded Program Execution
To model multi-threaded program execution, we assume there
are n concurrent threads, Ti for i=1,…,n, in an embedded program.
The threads are data race and exception free and are scheduled
preemptively according to their priorities. The system state is the
collection of thread local states and a shared global state. The
interactions among the threads and with the external environment
are done through the operations on the global state, which are
represented by interaction events. An interaction event
(abbreviated as event), e, can be a lock/unlock, semaphore
give/take, message send/receive, or input/output operation that is
performed when a thread invokes an event function f. The notation
Ex:f→e is used to indicate that an event e is generated during the
execution of the event function f . Apparently, the resulting event
of an invocation of f depends upon the local state of the calling
thread, as well as the shared global state. For instance, a nonblocking read from a device can succeed or fail depending on the
availability of input data when the function is invoked.

C1
C2

C1

//thread T1
T1 {
//f1=sem_take(a)
//f1 -> e1
sem_take(a);
}
//priority T1 > T2

CT1(f1) CT1(e1)

CT2(f2) CT2(e2) CT2(f3) CT2(e3)

CT1(f1)

CT1(e1)

CT2(f3)

CT2(f2) CT2(e2)

C2

//thread T2
T2 {
//f2=sem_give(a)
//f2 -> e2
sem_give(a);
//f3=read(0, buf, 1)
//f3(x) -> e3
read(0, buf, 1);
}

ht(e2)

CT2(e3)

ht(e1)

C

There are two important incidents during the execution of an
event function f by a threat T. “T enters f” occurs when the
processing begins to take place globally. The entrance gives an
important timing information since it decides a logical order
between events, as well as the possible resultant event to be
generated by the function. For instance, when two threads request
a semaphore concurrently, the moments that they enter sem_wait
function provide an order of the requests and can determine which
thread can take the semaphore successfully and the consequent

T1 enters f1

T2 enters f2 e2
happens

: scheduler execution,

e1
happens

T2 enters f3

e3
happens

: interrupt execution

Figure 1. An example global/local clocks and event
execution for two interacting threads.

3

For the events generated from program thread execution and
system thread, an event graph G = (V, E) can be constructed where
V is a set of events and edge (ea, eb)  E if ea  V and eb  V and
eb is logically dependent on ea. Basically, G is a partially ordered
graph representing the "happened-before" relation among events
[3]. An example of event graphs is shown in Figure 2. In the
following sections, we use GI = (VI, EI) and GU= (VU, EU) to
represent the event graphs of the instrumented program PI and the
original program without the instrumentation PU, respectively. To
determine whether there is a probe effect caused by
instrumentation, it is assume that the initial states and the external
events, including interrupts and the arrivals of input data, are
identical in the execution of PI and PU. We will then need to
compare GI and GU or at least find a way to check whether GI
differs from GU.

Figure 3. Execution time with no overhead in simulation
analysis where CiI and CiS are thread Ti’s local clock
in PI and in simulation.
analysis, kernel resource and scheduling operations must be
incorporated to determine the event ordering.
The goal of the simulation is to generate an event graph GS to
represent the un-instrumented execution of all thread and system
events observed in the execution of PI. In the simulation, a thread
Ti will execute a sequence of function invocations and events (fi,k
ei,k) in timestamp ordering that are collected from PI. The interevent execution times of ctS(fi,k) and ctS(ei,k), are calculated by
subtracting any instrumentation overhead from ctI(fi,k) and ctI(ei,k).
The simulation is done with a global clock CS and the current
simulation time is denoted as cur-time, which is advanced based
on thread events and operating system activities. At each
simulation epoch, the ready thread with the highest priority is
chosen as the running thread. The running thread Ti can proceed
to perform its next activity ai,k, where ai,k is either fi,k or ei,k, if
there is no system event in [cur-time, cur-time+ctS(ai,k)]. The
global clock CS is then advanced to the next simulation epoch, i.e.
cur-time+ctS(ai,k). Otherwise, the running thread is preempted by
the arriving interrupt irqj at cur-time=CT0(irqj). After the uninstrumented execution time of irqj, Ti may continue its execution
or a context switch may occur if there is a thread of higher
priority waked up by the interrupt.

Figure 2. An example partial order graph G with seven
events from program execution
3.3 Simulated Program Execution
We consider the execution of instrumented and uninstrumented programs with the same input data. In an
instrumented program, extra code is inserted to record program
execution behavior that we are interested in. For instance, to
measure branch/decision coverage, instrumented code should log
the conditions used at each decision point. Furthermore, additional
instrumentation is added to obtain the trace of thread interaction
events, thread execution time, and the overhead of instrumentation
code. Thus, the event graphs GI can be constructed from the
observed event trace. However, GU of PU is not observable
directly.

As the simulation proceeds from one event to the other, the
graph GS is constructed by adding edges for logical dependence
among thread and system events. For instance, a happened-before
relation is added from a message send event to a subsequent
message receive event. Similar, an interrupt is happened before the
thread’s sem_wait event which gets completed due to a sem_post
event issued by the interrupt. In Section 4.3, the simulation
algorithm implemented in VxWorks is described.

With the event functions and the events collected from the
execution of PI, an event-driven simulation can be conducted to
analyze the execution behavior of PU. Since there is no
instrumentation in PU, any instrumentation overhead should be
removed from the thread execution time of PI. As shown in Figure
3, the execution time to be considered in the simulation analysis,
CTiS(f) and CTiS(e), can be obtained from the measured CTiI(f) and
CTiI(e) of instrumented program PI. As a consequence, if PU
invokes an identical event function and results in the same event as
in the execution of PI, the event may occur at an early instant. The
change in execution time can also vary the relative order of
program and system events. For instance, an event e of PI that is
invoked after an interrupt may happen before the same interrupt in
PU. This alteration may have a ripple effect. For instance, if the
interrupt service routine signals a ready status, the running thread
that invokes a function call to read the status in PU would not see
the ready status immediately and may be blocked until the
interrupt arrives. The example suggests that, in the simulation

3.4 Analysis of Simulation Results
Once GS is constructed, we will be interested in the execution
behavior of PU that may be inferred based on GS. Note that GS
does not always represent GU, since if the event ordering of PU is
different from that of PI (due to the instrumentation overhead),
then the execution paths of PU and PI might be different. Thus,
there might be an event e such that e  VI but e  VU. Given that
the simulation is based on the events in PI, we have e  VS. This
leads to GU ≠ GS. However, a positive result can be established in
the following theorem when GI and GS are equivalent.
Theorem 1: If and only if GS = GI, then GU = GI.
Proof: To shown the “if” part, let’s assume GU ≠ GI. Then, there
should be a thread that generates different events or experiences
different happened-before relations in the execution of PU and PI.
Let eiI(k) be the first such event of PI (in terms of the global lock

4

CI) that eiI(k)≠eiU(k), where eiX(k) represents the k-th event
performed by thread Ti of program PX. All events of PI that are
priori to eiI(k) have the identical happened-before relations as their
equivalent events in PU. For thread Ti, it must has executed the
same first (k-1) functions and generated the same (k-1) events, i.e.,
eiI(l)=eiU(l) for l=1,…,k-1. Thus, it should use the same function in
its k-th invocation, i.e. fiI(k)=fiU(k).

are no exceptions from the running applications. The priorities of
application tasks are set to be higher than the priorities of any
other system background tasks. Thus, the execution of background
tasks will not affect the analysis of probe effect. The trace data are
sent to the host at the end of the application execution to avoid any
activities including file IO during application execution.
4.2 Execution Trace and Measurements
To trace the invocation of event functions and the occurrence
of events, we adopt the instrumentation mechanism of the record
framework of Replay Debugger [18]. Since the record framework
already supports the wrappers for tracing event execution, we
added 1) the measurement code for CPU time spent for each
thread and 2) the overhead measurement for the instrumentation
code. The existing interrupt handlers for timer and keyboard are
instrumented to collect the timestamp when an interrupt arrives
and to measure the execution time of interrupt handler. In addition,
the keyboard interrupt handler is customized to directly
communicate with the keyboard driver. For our Atom-based target
processor, x86’s RDTSC (Read Time-Stamp Counter) instruction
is used to collect timestamps.

To have eiI(k)≠eiU(k), the global states of PI and PU that are
used in the processing of fiI(k) and fiU(k), should be different. This
suggests that at least one extra update is inserted before the
invocations of fiU(k) in the execution of PU. Let this update event is
performed by a different thread Tj and denoted as ejU(k’). As the
instrumentation overhead is removed in PU, this update event is
brought forward and occurs before the invocations of fiU(k) in the
execution of PU, even if ejI(k’) occurs after the invocations of fiI(k)
in the execution of PI. This change in event ordering should be
observed in the simulation analysis, i.e., ejS(k’) occurs before the
invocations of fiS(k) since the same instrumentation overhead is
deducted and the program behavior has not been changed before Ti
makes its k-th invocation. The addition of the happened-before
dependency, ejS(k’) → eiS(k), in GS results in GS ≠ GI.

Task execution time is measured in scheduler hook routines
that are invoked for every context switch. For each task, we keep
the timestamp that the task is switched in and when the task is
switched out. The difference between the current timestamp and
the switched-in timestamp is accumulated to the task CPU time.
The execution time of scheduling operation and context switching
is measured offline using two tasks, task 1 and task 2, where task2
has a higher priority than task 1. First, we let task 2 be blocked on
a semaphore and when task1 posts the semaphore, task 2 becomes
running. Then, we remove task 2 and just run the sem_post
operation by task 1. The intervals from the invocation of sem_post
to the completion of the call are measured for the two cases. The
difference is considered as the measured execution time of
scheduling operation and context switch.

For the “only if” part, the equivalency of GU and GI indicates
that thread Ti would invoke the same event function fiI(k) and
generate the same event eiI(k) as in the execution of PI, even if the
instrumentation overhead is removed. Thus, the simulated
execution of fiI(k) will generate the same event eiI(k) which has the
same happened-before relations with preceding events. This
implies GS = GI. ■
The theorem implies that, if the logical order of thread events
built in the simulation analysis is as same as the one in the
execution of the instrumented program PI, then GI is the true
representation of GU. On the other hand, if GS  GI, the simulation
failed in a sense that when the partial order begins to be different,
the execution path may also have changed too. This suggests that
the instrumented program may have started to take a different
execution path. Since the simulation uses the same execution path
as the instrumented program, the simulated execution is no longer
a representation of the un-instrumented program. However, we can
find out when the execution begins to change and how it changes.
Let ed be the very first event of GS that has a different partial order
in GI. Then, the partial graph of GS for all events priori to ed is the
true representation of the same events in the execution PU. A
follow-up investigation on the trace can be considered to find out
how the instrumentation changes the program execution.

4

4.3 Simulation Analysis Algorithm
Using the execution trace from the instrumented program
execution and the measurements of the execution environment, the
simulation is performed as shown in Figures 6 and 7. It is
governed by the invocation to event functions and the occurrence
of events and interrupts. The simulation maintains a global clock C
which advances when the running task, event function, or interrupt
service routing is executed. Note that thread T0 is used to represent
system’s external activities and runs concurrently with the
scheduled application thread. It consists of interrupt and input data
change events. The execution time of interrupt events is measured
from ISR routine, whereas the execution time for input data
change event is set to 0.

IMPLEMENTATION

4.1 Execution Environment
To implement the proposed approach of detecting probe
effect, an execution environment is set up on a single core of a 1.6
GHz Intel Atom processor running VxWorks 6.8 [25]. The
VxWorks’ priority-based preemptive scheduler is configured. Two
IRQs are available during the execution, a 60Hz timer IRQ and a
PS2 keyboard IRQ. The queuing mechanism for tasks blocked on
a semaphore is based on task priority.

When a thread is scheduled to run, the simulation clock is
adjusted to the instant of the next interrupt or the subsequent event
function call, whichever comes first. When the global clock C is
equal to the arrival time of an interrupt, the time spent on the
interrupt is added to C. If there is a thread pending for the arrival
of the interrupt, its state is changed to ready once the interrupt is
processed. Then, the highest priority ready thread is scheduled for
execution. If an event function invocation takes place, the resource
required by the function is evaluated. The call can lead a return
with error, a blocked thread, or the execution of event function. A
simulated event happens when an event function is completed.
System state may be updated (e.g., a message is de-queued) and
blocked tasks may be waked up as the consequence of the

We consider a simplified system in which three kinds of tasks
and system activities can affect the timings of event occurrences
and must be traced in the execution of instrumented program:
application tasks, interrupt handlers, and the scheduler. All
application tasks and kernel operations are run in a flat memory
space and there is no page-fault exception. We also assume there

5

1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:

enter_func(func: f, thread: T, event e)
if f is resource release function
return
if resource not available
if f is synchronous function
set T to pending
else
mark e as “fail to acquire the resource”
else
mark e as “succeed to acquire the resource”

1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:

execute_event(event: e, thread: T)
if e is resource releasing event
if any tasks pending for the resource
select a task and set it to ready
else the resource is released
eL = event that e logically depends on
VS =VS  e
ES =ES  (eL, e)
Figure 6. Sub-routines for the the simlulation algorithm

happened event. Whenever needed, the scheduler’s execution time
is added to C to simulate the scheduling operation. When there is
no ready thread, the idle process is simulated by advancing C to
the next interrupt. Note that, interrupts are accepted during task
execution and are delay if they arrive during the execution of event
functions or scheduling operation.

5

do_simulation()
C_cur = 0
// the current global clock
Ci = 0 for all Ti // Ci is the thread clock for Ti
Tr = the highest priority ready thread
next_act = the earliest action of T0 and Tr
while (true) {
=the instant of cur_act – C_cur
cur_act = next_act
C_cur is advanced to the instant of cur_act
if (cur_act == ISR completion)
set any tasks pending for the ISR to ready
else {
if (Tr ≠ null), Cr is advanced by 
if (cur_act == IRQ arrival at T0)
next_act=ISR completion
else if (cur_act==input data change)
mark data input event
else if (cur_act==function f invocation)
enter_func(f, Tr, e) // Ex:f→e
else if (cur_act==event e completion)
execute_event(e, Tr)
}
if any change in task state
Tr = the highest priority ready thread
if (cur_act ≠ IRQ arrival at T0)
next_act = the earliest action of T0 and Tr
if all events are executed
break
}

EXPERIMENTAL RESULTS

We used two multi-threaded programs, the dining
philosophers and the sleeping barber, from the LTP benchmark
suite [25] in the experiments. The dining philosophers program
has 5 philosopher threads with decreasing priorities from
philosopher 1 to philosopher 5. Each philosopher is looping from
thinking, picking up forks, and to eating. The thinking and eating
activities of philosophers 1 and 2 are implemented with blocking
reads for keyboard input. On the other hand, the thinking and
eating activities for philosophers 3, 4, and 5 are replaced with a
simulated computation. When a keyboard pressing interrupt
occurs, a philosopher (1 or 2) will be waked up and may preempt
another running philosopher. Thus, philosophers will be
differently interleaved depending on the timing of keyboard inputs
(e.g., different order of philosophers’ eating). In the sleeping
barber program, the waiting room has three available chairs and
there are one barber thread with the highest priority and 5
customer threads with decreasing priorities from customers 1 to 5.
The barber is looping from sleeping if there is no waiting
customer, and to serving a customer. A customer waits in the
waiting room and gets a haircut if a chair is available. He leaves
without a haircut if all the three chairs are occupied. The barber’s
sleep is waked up by a keyboard interrupt. As a consequence,
customers 4 and 5 (with lower priorities than customers 1 to 3)
may not get haircuts depending on how fast the barber is waked up
by keyboard interrupt.

Figure 7. The simulation algorithm

CPU TIME FOR THE DINING PHILOSOPHERS

TABLE I.

Thread
Philo.1
Philo.2
Philo.3
Philo.4
Philo.5
Average

CPU time
(cycles)
29,964,934
29,073,286
161,459,044
161,626,702
160,931,184

TABLE II.

Thread
Barber
Custo.1
Custo.2
Custo.3
Custo.4
Custo.5
Average

Both programs in the experiments have relatively short
execution times considering manually injected keyboard
operations. A simulated computation is inserted between event
functions to ensure the speed of program execution is comparative
to the rate of manual keyboard entry. As we add wrappers to event
functions and interrupts, instrumentation overhead is added to the

CPU time without
overhead(cycles)
20,042,918
20,029,304
153,060,446
153,232,244
153,228,482

Overhead
49.5%
45.1%
5.4%
5.4%
5.0%
8.6%

CPU TIME FOR THE SLEEPING BARBER

CPU time
(cycles)
7,965,912
10,065,480
10,046,790
2,568,782
1,378,142
1,615,038

CPU time without
overhead(cycles)
5,024,050
9,544,202
9,524,506
2,047,280
1,029,454
1,231,966

Overhead
58.5%
5.4%
5.4%
25.4%
33.8%
31.0%
18.4%

program execution. In addition, extra simulated computation is
inserted in thread execution to represent dynamic analysis or
profiling overheads. These instrumentation overheads are removed
in the following simulation analysis. Tables I and II show
execution time and overhead from the average of 5 executions of
each benchmark program. . In Table I, the CPU times of
6

Instrumented

290M

295M

300M

310M

305M

Philosopher 2

315M

R

SW(7)

SP(7)

35

38

39

320M

SP(5)SP(7)

Philosopher 4

SW(5)
40

36 37

Simulation
290M

295M

300M

310M

305M
R SW(7)

Philosopher 2

3839

Philosopher 4SP(5) SP(7)
3536

315M

320M

SP(7)
40

SW(5)
37

Keyboard Interrupt
R: read(), SW(s): sem_wait(s), SP(s): sem_post(s)
Number below event: total order number
Time unit: CPU cycles

Figure 8. An example execution of dinning philosopher program where GI = GS but some events are with
different timestamp ordering
philosophers 3, 4, and 5 are greater than that of philosophers 1 and
2 as simulated computations are used for the thinking and eating
activities instead of blocking reads. In Table II, customers 1 and 2
spend more CPU times than other customers as we inserted
additional simulated computations before entering the waiting
room to delay the entrances of customers 4 and 5. Thus, customers
4 and 5 would not arrive too early while the first three customers
are waiting on the three chairs, and have to leave. There are some
differences in the CPU times spent by customers 3, 4, and 5 as
customer 3 always gets a haircut while customers 4 and 5 may get
haircuts depending on input timing.

in Figure 8. Events of philosopher 4 occurred earlier than in the
instrumented program execution while philosopher 2 was still
blocking for input. Since GI ≠ GS, the simulation is no longer a
representation of the original program. We notice that the partial
graphs of GI and GS are identical until the 37-th event. Thus, for
each thread, the next event function to be invoked immediately
after the 37-th events should be identical in the instrumented
program and in the simulated execution. After the 37-th event, the
difference in the logical ordering of events is triggered by the
order of the sem_wait(7) events invoked by philosopher 2 and
philosopher 4. We manually inspected the source code and found
out that the change in the logical ordering did not result in a
change of execution path. Hence, any program dynamic analysis
based on execution path is still valid. However, in general, it will
be a very challenging task to find a change of execution path by
manual inspection of source code.

Few experiment results of the probe effect are illustrated in
Figures 8 to 10. Figures 8 and 9 are the results of two different
executions of the dining philosophers program with different
inputs and Figure 10 is based on an execution of the sleeping
barber program. In each figure, the event orderings from the
instrumented program and the simulation analysis are compared.
We only illustrate specific time frames with particular threads that
show some variations in event ordering. The horizontal lines show
the timestamps of event occurrence and arrow lines indicate the
logical dependencies between events. The number below each
event denotes the event sequence number in timestamp (total)
ordering. Figure 8 depicts a case when GI = GS but the timestamp
orderings are different. The case appears when philosopher 2 is
waiting for a keyboard input and, as soon as the input becomes
available, it preempts philosopher 4. In the simulated execution,
since thread execution time is reduced due to the removal of the
instrumentation, the events of philosopher 4 happen while
philosopher 2 is still blocked. As a result, the timestamp ordering
of events in the simulation differs from that of the instrumented
program. However, since GI = GS, the simulation is the true
representation of the original program and we can conclude that
there is no probe effect for the instrumentation overhead.

Figure 10 demonstrates a case of a change of execution path
due to instrumentation overhead in sleeping barber program. The
result shows that GI = GS up to the 27-th event. However, in the
28-th event, the sem_wait(4) is invoked much earlier in the
simulation than what happens in the instrumented program. The
resultant partial order graphs are drawn in Figure 11. The second
line is for barber thread and the last line is for customer 5. In the
simulation, the semaphore given at the 25-th event is taken by
customer 5 while in the instrumented program execution the
semaphore is taken by barber thread. In fact, in the instrumented
execution, customer 5 arrives after the barber is waked up by a
keyboard input and begins to cut hair. A chair becomes available
for the arriving customer 5. On the other hand, customer 5 arrives
before the keyboard input, suggesting the barber is still in sleep
mode. The customer should leave as he cannot find any available
chair in the waiting room, i.e., a different execution path. Since the
simulation uses the same observed event from the instrumented
program, the simulated execution cannot be correct after the 28-th
event.

Figure 9 shows a case where the logical ordering (partial) is
altered due to the instrumentation overhead. The timestamp
ordering of events in the simulation is changed in a similar way as

7

Instrumented
310M

315M
R SW(7)

Philosopher 2

320M
SP(7)

38 39

330M

325M

335M

340M

40

SW(7)

Philosopher 4

SP(7) E
4243

41

Simulation
310M

315M

R SW(7)

Philosopher 2

41 42

Philosopher 4SW(7)
38

320M

330M

325M

335M

340M

SP(7)
43

SP(7) E
39 40

Keyboard Interrupt

Figure 9. An example execution of dinning philosopher programin which GI ≠ GS (i.e., the partial orderings are
different after the 38th event)
Instrumented
23.0M

23.5M

24.0M

24.5M
R SW(4)

Barber

28 29

25.0M
SP(4) Serve the three
customers
30

Customer 5

≈

A chair available
for him

32.5M

SW(4)

SP(2)

67

68

Simulation
23.0M

23.5M

24.0M

24.5M

Barber

25.0M
R SW(4) SP(4)
31 32

Customer 5

SW(4)
28

33

SP(2) SP(4)

Three available chairs taken
A chair NOT available for him

29 30

Keyboard Interrupt

Figure 10. An example execution of sleeping barber program where GI ≠ GS after the 38th event. In the
simulation, customer5 does not get haircut while he does in the instrumented program

6

CONCLUSIONS

ACKNOWLEDGMENT

Often the most important metric in the dynamic analysis of
multithreaded programs is the overhead of instrumentation since
researchers are aware of the potential probe effect caused by the
overhead. However, to the best of our knowledge, no research has
proposed a way to detect any changes in program execution when
the programs are instrumented. In this paper, we model the
execution of multi-threaded program according to the happenedbefore ordering of global events. Using the trace of event function
invocations and OS activities, a simulation-based analysis is
presented to detect if the partial order of the happened-before
relation is altered by instrumentation. The experiments of two
simple applications running on VxWorks demonstrate how
instrumentation overhead can lead to changes in event timestamp
ordering and in the partial order of happened-before relation.

This work was supported partially by a grant from the NSF
Industry/University Cooperative Research Center (I/UCRC) on
Embedded Systems at Arizona State University.

In this paper, we only consider single core machines with
priority-based preemptive scheduling. As a further step, our
analysis can be extended for multi-core systems in which thread
migration and multiprocessor scheduling should be considered.
Also it would be interesting to attempt a hardware-assisted online
detection mechanism for potential probe effect. Then, remedy
actions, such as synchronization operations, can be inserted to
ensure deterministic event ordering

[3]

REFERENCES
[1]
[2]

[4]

[5]

8

J. Gait, “A probe effect in concurrent programs,” Software
Practice and Experience, 16(3), pp: 225-233, 1986.
D. Kranzlmüller, R. Reussner, and C. Schaubschläger.
“Monitor Overhead Measurement of MPI Applications
with SKaMPI”. Proceedings of the 6th European
PVM/MPI Users' Group Meeting on Recent Advances in
Parallel Virtual Machine and Message Passing Interface,
pp: 43-50, 1999.
L. Lamport, “Time, clocks, and the ordering of events in a
distributed system,” Communications of the ACM, 21(7),
pp: 558-565, 1978.
A.D. Malony, D.A. Reed and H.A.G. Wijshoff,
“Performance Measurement Intrusion and Perturbation
Analysis”, IEEE Transactions on Parallel and Distributed
Systems, 3(4), pp: 433-450, 1992.
F. Wolf, A.D. Malony, S. Shende and A. Morris, “TraceBased Parallel Performance Overhead Compensation”,
Proceedings of the International Conference on High

[11] C. Flanagan and S. N. Freund. “FastTrack: efficient and
precise dynamic race detection,” Proceedings of the ACM
SIGPLAN conference on Programming language design
and implementation (PLDI), pp: 121-133, 2009.
[12] DRD, Valgrind-3.8.1. http://valgrind.org/.
[13] Intel Inspector XE 2013. http://software.intel.com/enus/intel-inspector-xe.
[14] T. Bergan, J. Devietti, N. Hunt, and L. Ceze, “The
deterministic execution hammer: How well does it actually
pound nails?” The Second Workshop on Determinism and
Correctness in Parallel Programming (WODET), 2011.
[15] M. Olszewski, J. Ansel, and S. Amarasinghe, “Kendo:
Efficient Deterministic Multithreading in Software,”
Proceedings of the 14th international conference on
Architectural support for programming languages and
operating systems (ASPLOS), pp: 97-108, 2009.
[16] T. Liu, C. Curtsinger, and E. D. Berger, “DTHREADS:
Efficient Deterministic Multithreading,” The 22nd ACM
Symposium on Operating Systems Principles, pp: 327-336,
2011.
[17] Kai Lu, Xu Zhou, Tom Bergan, and Xiaoping Wang,
“Efficient Deterministic Multithreading Without Global
Barriers”, The 19th ACM SIGPLAN Symposium on
Principles and Practice of Parallel Programming, PPoPP,
pp: 287—300, 2014.
[18] Y. Lee, Y. Song, R. Girme, S. Zaveri, Y. Chen, “Replay
Debugging for Multi-threaded Embedded Software”,
Proceedings of IEEE/IFIP 8th International Conference on
Embedded and Ubiquitous Computing, pp: 15-22, 2010.
[19] D. Lee, B. Wester, K. Veeraraghavan, S. Narayanasamy,
Satish, P. Chen, et al, “Respec: efficient online
multiprocessor replay via speculation and external
determinism”, Proceedings of the fifteenth edition of
ASPLOS on Architectural support for programming
languages and operating systems, pp: 77-90, 2010.
[20] N. Froyd, J. Mellor-Crummey, and R. Fowler, “Lowoverhead call path profiling of unmodified, optimized
code”, Proceedings of the 19th annual international
conference on Supercomputing, pp: 81-90, 2005.
[21] X. Zhuang, M. Serrano, H. Cain, and J. Choi, “Accurate,
efficient, and adaptive calling context profiling”,
Proceedings of the 2006 ACM SIGPLAN conference on
Programming language design and implementation, pp:
263-271, 2006.
[22] T. Moseley, A. Shye, V. Reddi, D. Grunwald, Dirk and R.
Peri, “Shadow Profiling: Hiding Instrumentation Costs with
Parallelism”, Proceedings of the International Symposium
on Code Generation and Optimization, pp: 198-208, 2007.
[23] Q. Zhao, I Cutcutache, and W. Wong, “PiPA: Pipelined
profiling and analysis on multicore systems”, ACM
Transactions on Architecture and Code Optimization, 7(3),
pp: 13:1-13:29, 2010.
[24] M. Kim, H. Kim, and C. Luk, “SD3: A Scalable Approach
to Dynamic Data-Dependence Profiling”, Proceedings of
the 2010 43rd Annual IEEE/ACM International Symposium
on Microarchitecture, pp: 535-546, 2010.
[25] VxWorks Kernel Programmer's Guide 6.8.
[26] Linux Test Project, http://ltp.sourceforge.net/.

Figure 11. Examples of the partial order graphs from the execution of
Sleeping Barber program

Performance Computing and Communications, pp: 617628, 2005.
[6] T. J. LeBlanc and J. M. Mellor-Crummey, “Debugging
parallel programs with instant replay,” IEEE Transactions
on Computers, 36(4), pp: 471-482, 1987.
[7] K. D. Bosschere, M. Ronsse, and M. Christiaens,
“Debugging shared memory parallel programs using
record/replay,” ACM Future Generation Computer
Systems, 19(5), pp: 679-687, 2003.
[8] M. Ronsse, M. Christiaens, and K.D. Bosschere, “Cyclic
debugging using execution replay”, Proceedings of the
International Conference on Computational Science, pp:
851-860, 2001.
[9] S. V. Adve, “Data races are evil with no exceptions:
technical perspective,” Communication of the ACM, 53(11),
pp: 84, 2010.
[10] S. Savage, M. Burrows, G. Nelson, P. Sobalvarro, and T. E.
Anderson, “Eraser: A dynamic data race detector for multithreaded programs,” ACM Transactions on Computer
Systems (TOCS), 15(4), pp: 391–411, 1997.

9

Efficient Java Native Interface for Android based Mobile Devices
by
Preetham Chandrian

A Thesis Presented in Partial Fulfillment
of the Requirements for the Degree
Master of Science

Approved May 2011 by the
Graduate Supervisory Committee:
Yann-Hang Lee, Chair
Hasan Davlcu
Baoxin Li

ARIZONA STATE UNIVERSITY
August 2011

ABSTRACT
Currently Java is making its way into the embedded systems and mobile
devices like androids. The programs written in Java are compiled into machine
independent binary class byte codes. A Java Virtual Machine (JVM) executes
these classes. The Java platform additionally specifies the Java Native Interface
(JNI). JNI allows Java code that runs within a JVM to interoperate with
applications or libraries that are written in other languages and compiled to the
host CPU ISA. JNI plays an important role in embedded system as it provides a
mechanism to interact with libraries specific to the platform.
This thesis addresses the overhead incurred in the JNI due to reflection
and serialization when objects are accessed on android based mobile devices. It
provides techniques to reduce this overhead. It also provides an API to access
objects through its reference through pinning its memory location. The Android
emulator was used to evaluate the performance of these techniques and we
observed that there was 5 - 10 % performance gain in the new Java Native
Interface.

i

To My Beloved Family

ii

ACKNOWLEDGEMENTS
I would like to sincerely thank my advisor; Dr. Yann-Hang Lee, without
whose guidance, encouragement and support, this thesis would not have been
possible. I have gained a lot of knowledge about my field, improved my research
skills and otherwise, working under him. In all it has been a very satisfying and a
fulfilling experience.
I would also like to thank Dr. Hasan Davulcu and Dr. Baoxin Li for their time and
effort to help me fulfill my degree requirements; as part of my thesis committee.
I am grateful to all my friends at Arizona State University who made me feel at
home and making this stay an enjoyable experience. I am thankful to all my lab
mates, especially Yaksh Sharma, Zhou Fang and Rohit Girme who helped me
with my queries.
Finally, I am indebted to my parents and my family. They all have constantly
encouraged me and been understanding through all the tough times. I wish to
express my utmost and deepest gratitude to them for being patient, warm and
supportive.

iii

TABLE OF CONTENTS
Page
LIST OF TABLES……………………………………………………………….vii
LIST OF FIGURES……………………………………………………………..viii
CHAPTER
1 INTRODUCTION……………………………………………….........1
1.1

Motivation ............................................................................... 2

1.2

Document Outline ................................................................... 4

2 JAVA NATIVE INTERFACE (JNI) ...................................................... 6
2.1

Role of JNI .............................................................................. 6

2.2

Overview ................................................................................. 7

2.3

Native Method Arguments ..................................................... 9

2.4

The JNIEnv Interface Pointer ............................................... 10

2.5

JNI Functions ........................................................................ 11

3 ANDROID ARCHITECTURE.............................................................. 15
3.1

Application Framework ........................................................ 16

3.2

Android Runtime .................................................................. 17

3.3

Android Application Architecture ........................................ 20

4 RELATED WORK................................................................................. 21
4.1

JNI Bridge ............................................................................. 22

4.2

Interfacing Java to the Virtual Interface Architecture ......... 22

4.3

Jeannie ................................................................................... 23
iv

CHAPTER

Page
4.4

Inlining Java Native Calls At Runtime ................................ 24

4.5

Janet....................................................................................... 24

4.6

Native Code Profiling ........................................................... 25

5 DESIGN .................................................................................................. 26
5.1

Class Structure ...................................................................... 26

5.2

Hashing JNI fields ................................................................ 29

5.3

Find Class .............................................................................. 31

5.4

Get Field ................................................................................ 34

5.5

Get Method ........................................................................... 36

5.6

Pinning object ....................................................................... 38

6 IMPLEMENTATION ............................................................................ 40
6.1

Implementation of hash function.......................................... 40

6.2

Implementation of GetFieldId .............................................. 42

6.3

Implementation of GetMethodId .......................................... 43

6.4

Implementation of pinObject and unpinObject ................... 44

6.5

Implementation of FindClass ............................................... 45

7 EVALUATION ...................................................................................... 47
7.1

Android Debug Bridge ........................................................ 48

7.2

Profiling results ..................................................................... 50

8 CONCLUSION ...................................................................................... 57
9 FUTURE WORK ................................................................................... 58
v

CHAPTER

Page

REFERENCES .......................................................................................................... 59

vi

LIST OF TABLES
Table

Page

1.

JNI Function Table 1 .............................................................................12

2.

JNI Function Table 2 .............................................................................13

3.

JNI method profile.................................................................................50

4.

Heap sort running time ..........................................................................51

5.

Execution time for JetBoy .....................................................................54

6.

Running Time for JetBoy with modified JNI ........................................54

7. JNI modified method profile for
JetBoy………………………...........Error! Bookmark not defined.
8.

Reference object access time. ............................................................ 56

vii

LIST OF FIGURES
Figure

Page

1.

Overview of JNI ......................................................................................4

2.

Role of JNI ..............................................................................................6

3.

Steps writing a JNI program ....................................................................9

4.

The JNIEnv interface pointer ................................................................10

5.

Thread local JNIEnv Interface Pointer .................................................. 11

6.

Primitive array access ............................................................................14

7.

Major Components of Android Applications ........................................15

8.

Application Layer of Android ...............................................................16

9.

Application Framework of Android Libraries .......................................16

10. Libraries Layer of Android ....................................................................17
11. Layers of Android Application ..............................................................18
12. Android Runtime Layer.........................................................................19
13. Linux Kernel in Android Hardware Abstraction Layer .........................19
14.

Flow of findClass .................................................................................33

15. GetField workflow ................................................................................35
16. Get Method ............................................................................................37
17. Pinning object ........................................................................................39
18. JNI method Profiling .............................................................................51
19. Heap sort comparison graph ..................................................................52
20. Execution time of JetBoy ......................................................................53
viii

Figure

Page

21. Execution time for the Modified JNI JetBoy ........................................55
22: Execution time for Lunar Lander with Android JNI............................... 55
23: Execution time for Lunar Lander with modified JNI ... Error! Bookmark
not defined.56

ix

CHAPTER 1
INTRODUCTION

With the introduction of JavaME (Micro Edition) by Sun MicroSystem for
mobile and embedded devices Java has rapidly acquired a significant amount of
market share. Also the applications based on JavaME are portable across devices
developers favor Java over other programming languages. The problems with
using Java are following:


Standard Java libraries do not support platform specific features that are
needed by the application.



If the developers wants to use libraries written in different languages because
of the fact that these libraries are more efficient and faster than its Java
counterpart.



If the developer wants to implement time critical code in lower level
language.

Java Native Interface (JNI) addresses all these issues. It provides the following
functions:


It can create, update and inspect Java objects which include arrays.



It can call Java methods from C/C++



It can catch and raise exception from native code.



Native code can load class and request class information.

JNI has played a major role in development of application which needs to interact
1

with native code (C/C++). In recent years Java has become one of the favorite
application programming languages and is widely used by developers. This is
evident in android as well as majority of the application are written in Java. But
the fact that the operating system that android is developed on is Linux like which
requires JNI to interact with the devices. As more and more features are being
added to the mobile devices there is a need for an efficient JNI. There are papers
[1] which demonstrate the overhead incurred while making JNI calls. It provides
detailed performance benchmarks of several popular, modern, and representative
JNI implementations, pointing out their weak points and suggesting possible
solutions. There is a need to decrease these overhead in embedded systems like
android phones. Foreign-function interfaces (FFIs) such as “Jeannie” have been
developed to improve safety and productivity of JNI.
JNI are commonly used to interact with native libraries which involves
processing of chunks of data. The data transfer between the JVM address space
and that of the native address space is costly. Further the overhead occurred in
reflection and serialization of not primitive data is significant.
1.1

Motivation

Android uses JNI in its NDK (Native Development Kit), which is a toolset that
helps developers to interact with native code components in their application.
The NDK provides the following:


It provides tools and builds files that are helpful for developers to generate
native code libraries from C and C++ sources.
2



It helps developers embed native libraries into an application package file.



A set of native system headers and libraries that will be supported in all future
versions of the Android platform, starting from Android 1.5. Applications that
use native activities must be run on Android 2.3 or later.

NDKs are used to build activity, handle user inputs, use hardware sensors and
platform specific operations and the fact that they heavily rely on JNI. Making
JNI faster will not only make the application response time faster but also reduces
the use power consumption of the device. The bottle neck in JNI is the data
transfer between the JVM memory space and the native memory space. There is
also time consumed while accessing class structure and fieldIds of class attributes.
The transfer of data can be reduced if we can pin the memory address of the
object or array that we intend to manipulate. If we could cache the class structure
and the fieldIds of its attributes the over head of searching the class structure can
be reduced. As each call to the JNI is treated as new call without the information
of its previous history a lot of information is lost. If this information were
available to us we could use it and reduce the execution time of the JNI.
One of the approaches is the use of JIT (Just In Time) compilers to inline the JNI
calls. This reduces the cost of callouts to native code by reducing the over head of
stack operations that needs to be performed during the JNI calls.
The proposed solution alters the JNIEnv structure to include hash map to
store the recently accessed fieldIds and methods. It also has caches the class
structure in the JNIEnv structure. A new method is introduced to pin the address
3

of the Java object so that the same address can be used in future access this
decreases the access time and also the information can be accessed without the
use of JNI as we already know its location in the memory. To access such kind of
address the program should be a thread in the same process as that of the JVM.

Figure 1: Overview of JNI
1.2

Document Outline

The rest of the document is organized as follows.
Chapter 2, 3 provides Background information about Android, its architecture,
and JNI. It also explains some terms to better understand the document.
Chapter 4 talks about the Related Work done in the area of JNI in general. It also
mentions certain works which describe the specific mechanism and techniques
4

used to make JNI developer friendly and efficient.
Chapter 5 talks about the design and techniques that are used to make Android
JNI efficient as a whole. It mentions the detailed working of various JNI calls and
way they are made efficient.
Chapter 6 describes the implementation details.
Chapter 7 gives a various measuring techniques used for comparing the efficiency
of the new JNI aver the existing one. It concludes the thesis as a whole.
Chapter 8 presents some features and enhancements that can be done to the
existing implementation. This will further enhance the tool.

5

CHAPTER 2
Java Native Interface (JNI)

2.1 Role of JNI
When the Java platform is deployed on top of host environments, it may become
desirable or necessary to allow Java applications to work closely with native code
written in other languages. Java is slowly replacing the programs that were
written in C and C++ as they are platform independent. The Java Native Interface
is a feature that allows the programmer to take advantage of the Java virtual
machine, but still can use code written in other languages and libraries. As a part
of the Java virtual machine implementation, the JNI is a two-way interface that
allows Java applications to invoke native code and vice versa. Figure 2 illustrates
the role of the JNI.

Figure 2: Role of JNI
The JNI is designed to handle situations where you need to combine Java
applications with native code. It supports calls from native library and calls from
native application.
6



The JNI can be used to write native methods that allow Java applications to
call functions implemented that are implemented in native libraries. Java
applications call native methods in a similar way that is a simple Java method
invocation only difference is that it has the keyword native in its prototype.
But these Java calls are internally translated to calls to the native that call the
native method or library.



The JNI supports an invocation interface that allows us to call Java virtual
machine code from the native code. Native applications can link with a native
library that uses Java API, and then the native code can use the JNI APIs to
call these functions in the Java virtual machine.
2.2

Overview

The figure below illustrates the steps required write a simple application that
invokes a C function.
1. Create a class (HelloWorld.java) that declares the native method.
2. Use javac to compile the HelloWorld source file, resulting in the class file
HelloWorld.class. The javac compiler is supplied with JDK or Java 2 SDK
releases.
3. Use javah -jni to generate a C header file (HelloWorld.h) containing the
function prototype for the native method implementation. The javah tool is
provided with JDK or Java 2 SDK releases.
4. Write the C implementation (HelloWorld.c) of the native method.
5. Compile the C implementation into a native library, creating HelloWorld.dll or
7

libHelloWorld.so. Use the C compiler and linker available on the host.
Run the HelloWorld program using the Java runtime interpreter. Both the class
files (HelloWorld.class) and the native library (HelloWorld.dll or
libHelloWorld.so) are loaded at runtime.

8

Figure 3: Steps writing a JNI program
The native method in Java is declared as follows
class HellWorld {
private native void print();
:
:
}
2.3

Native Method Arguments

As discussed in the previous section, the native method implementation such as
Java_Prompt_getLine accepts two standard parameters, in addition to the
arguments declared in the native method. The first parameter, the JNIEnv
interface pointer, points to a location that contains a pointer to a function table.
Each entry in the function table points to a JNI function. Native methods always
access data structures in the Java virtual machine through one of the JNI
functions. Figure illustrates the JNIEnv interface pointer.

9

Figure 4: The JNIEnv interface pointer
The second argument differs depending on whether the native method is a static
or an instance method. The second argument to an instance native method is a
reference to the object on which the method is invoked, similar to this pointer in
C++. The second argument to a static native method is a reference to the class in
which the method is defined. Our example, Java_Prompt_getLine, implements an
instance native method. Thus the jobject parameter is a reference to the object
itself.
2.4

The JNIEnv Interface Pointer

Native code accesses virtual machine functionality by calling various functions
exported through the JNIEnv interface pointer.

10

Figure 5: Thread local JNIEnv Interface Pointer
A JNIEnv interface pointer is a pointer to thread-local data, which in turn
contains a pointer to a function table. Every interface function is at a predefined
offset in the table. The JNIEnv interface is organized like a C++ virtual function
table. Figure illustrates a set of JNIEnv interface pointers.
Functions that implement a native method receive the JNIEnv interface
pointer as their first argument. The interface pointer passed to the native methods
depends on the thread which calls it. The Java virtual machine passes the same
interface pointer to native method functions that are called from the same thread.
The calls made from different thread will have got different environment pointer.
The interface pointers are thread-local but the JNI function table are indirectly
referenced and shared among multiple threads.
As some of the platform does not provide efficient mechanism to support
thread local data access the JNIEnv refers to the thread local structure. By passing
the pointer to the thread local structure, the JNI implementation inside the virtual
machine can avoid many thread-local storage access operations that it would
otherwise have to perform if it was not referenced.
2.5

JNI Functions

The Java native interface provides various functions that can be called to access
the Java objects and methods. To access arrays it provides get<Type>ArrayRegion
.If the array contains object in Java then we need access each element
11

individually. Whenever we access the data it is copied from the Java heap to
native region. The string must be converted from Java utf-16 to UTF-8 type. It
also provides APIs to call methods in the Java form the native code. But this is
rarely used in the native code as it is costly. Following tables describes various
functions of JNI
JNI Function

Description

Since

Get<Type>ArrayRegion

Copies the contents of primi-

JDK1.1

Set<Type>ArrayRegion

tive arrays to or from a pre
allocated C buffer.

Get<Type>ArrayElements

Obtains a pointer to the contents JDK1.1

Release<Type>ArrayElements

of a primitive array. May return
a copy of the array.

GetArrayLength

Returns the number of elements JDK1.1
in the array.

New<Type>Array

Creates an array with the given

JDK1.1

length
GetPrimitiveArrayCritical

Obtains or releases a pointer to

Java 2

the contents of a primitive array. SDK1.2
Table 1: JNI Function Table 1

12

JNI Function

Description

Since

GetStringChars

Obtains or releases a pointer to the

JDK1.1

ReleaseStringChars

contents of a string in Unicode format.

GetStringUTFChars Obtains or releases a pointer to the

JDK1.1

ReleaseStringUTFC contents of a string in UTF-8 format. May
hars

return a copy of the string.

GetStringLength

Returns the number of Unicode characters

JDK1.1

in the string.
GetStringUTFLengt Returns the number of bytes needed to
h

represent a string in the UTF-8 format.

NewString

Creates a java.lang.String instance that

JDK1.1

JDK1.1

contains the same sequence of characters
as the given Unicode C string.
NewStringUTF

Creates a java.lang.String

GetStringCritical

Obtains a pointer to the contents of a string Java 2

ReleaseStringCritical in Unicode format or a copy of the string.

JDK1.1

SDK1.2

GetStringRegion

Copies the contents of a string to or from a Java 2

SetStringRegion

preallocated C buffer in the Unicode
format.

Table 2 : JNI Function Table 2
13

SDK1.2

The figure below shows how a primitive array is accessed in JNI.

Figure 6: Primitive array access

14

CHAPTER 3
Android Architecture
Android is a software stack for mobile devices that includes an operating
system, middleware and key applications. Figure shows the major components of
the Android operating system. Each section is described in more detail below.

Figure 7: Major Components of Android Applications
Basic applications like contact, email, browser settings, Bluetooth etc.,
come with the android package. All these applications are written in the Java
programming language. Many of these applications can be multi-threaded
depending upon their use and interaction. Applications can be added based upon
the user through Android Market.
15

Figure 8: Application Layer of Android

3.1

Application Framework

Application framework contains programs that manage the phone's basic
functions like resource allocation, voice applications, and window allocation for
applications, managing lifecycle of applications and keeping track of the phone's
physical location. This layer is majorly written in the Java programming
language. The API that is exposed is used by developers in their application.
There is no restriction applied while accessing this Java API in the application
developed by the programmer this helps in utilizing the features provided by the
Android.

Figure 9: Application Framework of Android Libraries
Each Android component has a set of C/C++ libraries which are used by several
System components. These are exposed to developers through the Android
application framework. Most of the CPU related tasks and peripheral devices are
16

done using native C/C++ libraries (Figure 10). Some of the core libraries are:


System C library - a tuned implementation of the standard C system library
(libc), for embedded Linux-based devices



Media Libraries - these libraries support playback and recording of many
popular audio and video formats and image files.



3D libraries - the libraries use either hardware 3D acceleration (where
available) or the included, highly optimized 3D software rasterizer.



SQLite - a powerful and lightweight relational database engine.

Figure 10: Libraries Layer of Android
As these libraries are for embedded systems they are optimized like fast pthread
implementation using 4-byte mutex rather than the 12-byte mutex (as there may
not be as many thread as compared to PC).
3.2

Android Runtime

As applications are written in Java the Android runtime environment consists of a
virtual machine. Android has its own Virtual Machine called Dalvik Virtual
17

Machine. The generated byte code in java is converted into DEX code and an
interpreter is used to convert them into assembly code. Every Android application
runs in its own process, along with its own instance of the Virtual Machine

Figure 11: Layers of Android Application
Dalvik primarily is a process virtual machine. Refer Figure below. Support
multiple virtual machines running concurrently. The Dalvik VM executes files has
the extension (.dex) format which is optimized for minimal memory footprint.
The .dex files are compiled from the .class files that are generated by Java by the
“dx”tool. The VM is register-based, and runs classes compiled by a Java language
compiler. The Dalvik VM is written in C and relies on the Linux kernel for core
OS functionality.

18

Figure 12: Android Runtime Layer
Android‟s kernel is based on Linux version 2.6 for core system services
such as security, memory management, process management, network stack, and
driver model. The kernel acts as an abstraction layer between the hardware and
the rest of the software stack. The kernel is modified so that it can cater Android
specific requirements by adding drivers etc.,. Refer Figure below.

Figure 13: Linux Kernel in Android Hardware Abstraction Layer
There is an abstraction layer present in between the Linux kernel and above
layers. This enables certain core default system applications and services to be
replaced by third party/ custom implementations. Most mobile OEMs have the
basic drivers to control their audio, video etc. Android defines this hardware
abstraction layer on top of kernel and standardizes Android„s interface. This
hardware abstraction layer exists as a user-space C/C++ library. This probably
means that Android implements a standard interface for Audio, irrespective of the
technology supported by the underlying hardware, just asking implementations of
19

features that it needs from the particular hardware.
3.3

Android Application Architecture

Android applications are written in the Java programming language. The
compiled Java code, all the necessary data and resource files of the application are
bundled by the “aapt” tool into an Android package. This file has a .apk suffix.
This file can then be used for installing the application on devices. All the code in
a single .apk file is considered to be one application.
Android application sandbox model Android uses the process separation provided
by Linux kernel as the primary means of achieving isolation against other
suspicious applications. Each application runs in its own Linux process.
Furthermore, each managed piece of code executes in a virtual machine (DVM).
As a result each application is sand- boxed from the other applications running at
any given time. All IPC is achieved via the mechanisms provided by Binder.
A second level of isolation builds upon the capability of underlying Linux to
strongly isolate data/files of one user from the other. This is achieved by
allocating a unique user-id to each installed application on a particular system.
Android starts the process when any of the application's code needs to be
executed, and shuts down the process when it's no longer needed and other
applications are in need of resources. Unlike applications on most other systems,
Android applications don't have a single entry point for everything in the
application (no main() function, for example).

20

CHAPTER 4
Related work
This chapter discusses the research that has been done on the JNI. It also reviews
projects that are relevant to this thesis. From the discussion above it is clear that if
we decrease the time required to retrieve an object in JNI and decrease the time
taken for the reflection and serialization of the data that is being transferred from
the JVM to the native space we can get a better performance and save battery life
of the device. Arrays are the main area of concern as huge amounts of data needs
to moved to and fro. Where as if the class object is huge then time is spent on
finding the attributes that we are interested in. The paper Fast Online Paper
Analysis[2] helps us understand the issues of runtime pointer analysis while using
reflection, dynamic loading of libraries etc., This helps us understand the issues
when we use some of the JNI APIs which rely on finding and loading of classes
using reflection. Walter Cazzola[3] analyses the use of reflection and its internal
working. It proposes a class named SmartMethod which transforms the calls made
by the use of reflection to direct calls that will be carried out similar to the
standard Java method invocation. SmartInvokeC[3] tool is used to generate the
stub of a class from its byte code, so to invoke a method we no longer need to use
the JNI. The call is made from a C stub that is generated. To retrieve and invoke
method faster the necessary information is hashed. Tamar et. Al[4] provides an
memory management scheme for thread local heap. This technique determines the
objects that are local and global and uses this information to avoid unnecessary
21

synchronization.
The following section describes some of the techniques.
4.1

JNI Bridge

One reason to use Java is that it can be ported easily on different platform as the
application runs inside the virtual machine. But if the application uses native call
the porting becomes difficult. That is we need to use the libraries that are specific
to the platform. This paper describes the challenges and solution so that the JVM
supports the native calls on different ISA. Here dynamic translators are used to
translate native calls based on the underlying architecture. To handle the JNI up
calls and marshaling of the data a simulated JNIEnv object in the IA-32
execution environment is used to enable 32-bit native libraries to call 64-bit
function pointer. They also use marshaling tables to map 64-bit references to 32bit references by intercepting the up calls and wrapping it with the reference and
during the down call the corresponding 32-bit reference is used. To avoid the data
movement when GetPrimitiveArrayCritical JNI API call is made the reference is
directly taken from the JVM internals. The JVM-independent implementation has
to resort to Java reflection to obtain this information.
4.2

Interfacing Java to the Virtual Interface Architecture

This paper explores the use of User-level network interface for the
communication between the Java heap and native buffer. It describes two
approaches the first approach manages the buffer between the Java heap and the
22

native space which requires the data to be copied while the second approach uses
a Java-level buffered abstraction and allocates space outside the Java heap and
this allocated space can be accessed like array in the Java. The second approach
eliminates the use of copying the data but the native garbage collector has to be
modified.
The first level of Javia (Javia-I)[5] manages the buffers used by VIA in
native code (i.e. hides them from Java) and adds a copy on the transmission and
reception paths to move the data into and out of Java arrays. Javia-I [5] can be
implemented for any Java VM or system that supports a JNI-like native interface.
(Javia-II) introduces a special buffer class that, coupled with special features in
the garbage collector, eliminates the need for the extra copies. In Javia-II [5], the
application can allocate pinned regions of memory and use these regions as Java
arrays. These arrays are genuine Java objects (i.e. can be accessed directly) but
are not affected by garbage collection as long as they need to remain accessible by
the network interface DMA. This allows the application to manage the buffer and
to send or receive Java array directly. It also describes the issues of memory
management when application creates memory outside the Java heap.
4.3

Jeannie

This paper proposes a new foreign functional interface design called Jeannie[6].
Here programmers can write both the Java code and the native code in the same
file. Jeannie compiles these files down to their respective JNI calls. This enables
static error detection across the languages and simplifies the resource
23

management. It addresses the issues of JNI being unsafe as it does not require
dynamic checks. By integrating and analyzing both Java and C together, the
compiler can produce error messages which can prevent many a maintenance
issues. The compiler is implemented using rats![6]. To access string form C in
Java conversion for UTF-8 to UTF-16 is made, and vice versa is done while
accessing strings from Java in C. The array region is still copied from the Java
heap to C memory space when access is made, the following functions are used to
gain the access _copyFromJava and _copyToJava.
4.4

Inlining Java Native Calls At Runtime

This technique inlines the native functions using JIT in java applications. The
callbacks to the JNI are transformed into their equivalent lightweight
operations[7]. IBM TR JIT[7] compiler is used as it supports multiple JVMs and
class library implementation. The control flow for the TR JIT consists of phases
for intermediate language (IL) generation, optimization and code generation. They
have enhanced the inliner so that it can synthesize the opaque call to the native
function. Then they have introduce a callback transformation mechanism that replaces expensive callbacks with compile-time constants and cheaper byte code
equivalents, while preserving the semantics of the original source language.
4.5

Janet

Janet[8] is the Java language extension which enables convenient development of
Java to native code interfaces by completely hiding the JNI layer from the user.
24

The source file is similar to ordinary Java source file except that it may contain
embedded native code (in terms of native method implementations), and the
native code can easily and directly access Java variables as Java code would. It
enables efficient direct access to Java arrays from the native side. However, when
the array is to be processed by external routine the array pointer has to be used.
Java types are converted by Janet generally to native types having the same name.
The array conversion introduces no performance reduction on platforms where
appropriate Java and native types are equivalent, but it requires allocation and
copying of the whole array in the case when they are different.
4.6

Native Code Profiling

This paper describes the technique used to profile native code that are part
of the application. Most of the profiling tool like 'hprof' do not segregate the time
spent in native code if we have this information then we can find the parts of the
code that can be improved further. The paper[2] introduces a profiling tool based
on JVM tool interface. The technique involves introduction of a wrapper methods
for the native function prototype in Java. It has the same method name and
signature as that of the native method but not a native method. This wrapper
function calls the J2N_Begin which recodes the time stamp and other profiling
information. Then it calls the native method. Upon return the wrapper calls
J2N_End is called which records the exit timestamp. Her profiling is done
statically by using a tool called ASM[2], as dynamic profiling overhead.

25

CHAPTER 5
Design
This chapter talks about the overview of the changes that are made to the existing
JNI in android. We first need to understand the class structure of a Java class
structure and how the methods and fields are represented in Java. The following
section describes these representation:
5.1

Class Structure

In Dalvik virtual machine the class can be either '.class' or '.dex' extension. We
are interested in the '.class' format as JNI API uses them. The class structure is
defined in the Object header file. Here are the fields that are of our interest:


Status- It is a structure of type ClassStatus through which we can know the
sate of initialization like initialized, ready, loaded etc.,



super- It holds the reference to its super class if any other wise NULL.



Interfaces – It is a two dimensional array which contains the list of interfaces
in this class.



DirectMethodCount and directMethods– DirectMethodCount hold the count
of the direct methods that are present in the class. Direct methods are static,
private and init methods. DirectMethod is an array which points to the
methods



VirtualMethods and VirtualMethodCount – VirtualMethodCount holds the
count of the virtual methods present in the class. VirtualMethod points to these
26

methods.


Vtable - Virtual method table (vtable) is for use by "invoke-virtual".
The vtable from the superclass is copied in, and virtual methods from this class

either replace those from the super or are appended.


sfields – These are structures of type StaticFileds and contains the type of the
field and its Jvalue.



Ifields – These contains all the instance fields of this object. Also all the
instance fileds that refer to objects are present in the beginning. Each instance
filed object has its field type and an offset which is the offset from the object
pointer.



SourceFile – This is the name of the source file.

Every method is represented by a structure called Method. The structure Method
has the following members:


clazz – This points to the class that this method belongs to.



MethodIndex – This contains the offset from either the vtable or the iftable of
its class.



Name – This is the method name.



Prototype – This is the method prototype descriptor string that contains the
return type and the argument type.



Insns – this contains the actual code for the method.



NativeFunc - This is pointer to native method. This could either be a JNI
bridge function or an actual internal native function. This can be checked by
27

performing a null check on the insns field.


JniArgInfo – This contains cached JNI argument and return type hints.



InsSize – This contains the count of the number of input argument to the this
method.



OutsSize – This contains the count of the number of return arguments of this
function.

The fields in the Dalvik virtual machine is represented by the structure Field. It
contains the following members.


Clazz - The pointer to the class it belongs to.



Name – The name of the field.



Signature – Its signature like "I", "[C", "Landroid/os/Debug;"



accessFlags – The access type.
Once the JNI has found the method that needs to be executed it transfers the

control to the DVM interpreter. The interpreter checks weather the function is
Java function or native. If the function is Java then the byte code is interpreted
into the architecture specific code.
Here is how the JNI calls are made to invoke a main method for an
application :JNI_CreateJavaVM will be called to construct a Dalvik virtual machine.The
JNI_CreateJavaVM will create everything needed to execute the .dex file, such as
dynamic memory management, thread, bytecode verifier, etc. Then JavaVM and
JNIEnv arguments will become the function interfaces to provide supported
28

functions. JNI function FindClass(JNIEnv* env, const char* name) will be called
to find the class by name. Before executing a main method in Java program, its
class is needed to be found by a specified name. GetStaticMethodID(JNIEnv*
env, jclass jclazz, const char* name, const char* sig) is called because the Java
main function must be a static method, this JNI function will be called to get the
main method ID. CallStaticVoidMethod(JNIEnv* env, jclass jclazz, jmethodID
methodID, …) This function will start to interpret the .dex file. This function will
call the Dalvik interpreter to interpret the .dex file.
5.2

Hashing JNI fields

There are several ways to store the data and use it future so the next call made to
function is faster if the programs future request can be handled based on the
previous . Hashing is the technique used here to make the JNI call efficient. As
JNI accesses fields in the Java domain it first needs to know the exact memory
location of the field. As we have seen earlier each function in the JNI API has
access to the JVM environment pointer that it is running in. This JVM
environment pointer contains the pointer to the heap and the refernces to the
object. It can also access the class structure of the classes that are loaded and that
are present in the class path or in the jar file that is included in the class path.
When we make a JNI request to access field the JNI API first checks if the class is
loaded or not. Then it access the class structure and goes through the field list. If it
finds the field that we are looking for it returns the fieldId to the caller. Now the
caller can access this field's data using this Id. This as you can see can take a long
29

time. To solve this issue hashing technique is used. The hash function design is as
follows. There are three different hashing techniques used here. The first one is to
hash and store the class structures. The second is used to store the offset and the
methodId of methods that are called through the JNI regardless of weather it is
native method or Java method. The third is for the fields that are accessed by the
JNI in its native domain.
The hashing bypasses the reflection calls that need to be made every time
when we need to access a method or field. Now due to hashing the reflection call
is made during the first access any future access will use the value obtained from
the hash table. One problem with this approach is the we will not know the size of
the hash table that we need to create. This issue is addressed by increasing the size
by a set amount when the hash table reaches its limit. To compute the hash of the
string we use the predefined function in the UtfString file.
There are always possibilities that the two stings may hash to the same key.
This is called collision. To handle this probing technique is used. In this when we
try to insert an entry into the hash table and we find that the slot in the hash table
already full then we first check if the vale is the same as the old one if so we
replace the value with new one as this is the latest value. If the value does not
match then we store it in the next available free slot. This technique is chosen
over the chaining for collision resolution as it uses the memory in an optimal way.
Due to the use of probing while inserting the value in the hash table the retrieval
takes longer. We need not address the synchronization issue as JNI API is
30

executed by a single thread and there is only one thread accessing the JVM
environment variable.
There are three hash table defined one each for to hold the class structure,
field memory location and methodIds. The following are the names of the hash
map


fieldEntryJni



refEntryTable



methEntryTable
These hash tables are updated when the call to FindClass, getMethodId and

getFieldId is called by the JNI API. These hash tables are initialized when the new
JNI environment variable is created when dvmCreateJNIEnv is called
during API invocation.
5.3

Find Class

The find class in the JNI is used to load the class given the fully qualified class
name in the JVM pointed by the JNI environment object. The find class takes in
two argument one the JNI environment object and the class name. The findClass
first make a check by calling dvmGetCurrentJNIMethod, this method check if the
current thread is executing a native method if so it returns the method by
inspecting the interp stack. Then we get the class descriptor form the class name.
The class name is surrounded by 'L' and ';'. Then we load the class from its class
loader and add the local reference in the reference table. This is a very important
step if the class does not have a reference then this class cannot be accessed in the
31

native stack. Then we return the reference to the caller who can use it to access
fields and methods.
If the class was already loaded then we could reuse this instance of the class
to get the fieldIds and methodIds. To accomplish this we store the reference of the
class in the hash table refEntryTable in the JNI environment variable. Every time
the findClass is called we check if the class already loaded by looking up in the
hash map. If not we proceed as usual and load the class. After loading the class we
add this to the hash map. If the class we are looking for is present in the hash map
we validate its reference as it may be garbage collected if the reference is valid
then we add it to the local reference table so that the class reference is not garbage
collected. Then we return this instance to the caller. This decreases the time taken
to load the class. The key that we use here is the class descriptor rather than the
class name passed. This is because the two classes can have the same name but
the descriptor are different.
The below flow diagram for the findClass method

32

Figure 14: Flow of findClass
The method findClass is used to load the applications main method hence
optimizing this will reduce the execution time of the application. As long as the
local reference of the class is present the class is not unloaded hence we can reuse
it. These are methods for the hash table access


hashcmpRefEntryTableStr



hashcmpRefEntryTable



findRefEntryTableEntry



addRefEntryTableEntry
33

5.4

Get Field

The getFieldId is used to get the instance field give the field name, its signature,
and the Java class. To access any field in the Java object we need to use this
function. It returns the fieldId in the class structure through which we can get the
offset from the object pointer. The Java class structure is obtained from the
previous findClass. As we have the reference in the local reference table of the
JNI we need to get the reference of the actual class. This is accomplished by the
function dvmDecodeIndirectRef. The we check if the class is initialized. Then we
find the filedId as follows
 First we search the class object for the given field name and signature.
 This is done by walking through the ifields of the class that is provided. If
found the filedId is returned.
 If it is not found in the class then we see if there is any super class present.
If yes then we scan through the ifields of the super class. If it is found here
then we return this fieldId.
 If the field is not found then we throw an exception.
As we can see this procedure of scanning through the list can take a lot of
time. Once we find the fieldId we can add it to the hash. But this is tricky as
multiple class have the same field name and signature. To make the key unique
there is a combination of the field name, field signature and class descriptor.
Hence this is unique. Here we also need to check if the class is loaded or not
when we return the fieldID from the hash table. If the class that the field belongs
34

is not loaded the further use of the Id will generate exception. This can be taken
care by looking into the hash table of findClass as this will take only O(1). If the
class is not loaded then we assert an exception similar to the actual flow.

Figure 15: GetField workflow
The above diagram shows the function flow.These are methods for the hash table
access


hashcmpFieldEntryTableStr



hashcmpFieldEntryTable



findFieldEntryTableEntry



addFieldEntryTableEntry

35

5.5

Get Method

The GetMethodId is used to find the methodId for a given method name,
signature and class. Similar to the GetFieldId to get the methodId we first need to
load the class by calling findClass. Form this methodId we can get the offset from
the class pointer and invoke the method. The following is performed by this
method


It first checks if the class is loaded and initialized. If not an exception is
thrown.



It then goes through the list vtable to check if the method and the signature is
present if present it checks if the method is static. If not then it returns this
methodId.



If the it is not a virtual function then it goes through the directMethods list to
check if it is present in this. If present it checks if the method is static or not. If
static then it throws exception else returns the methodId.



The method's class may not be the same as class that is supplied , but if it isn't
this must be a virtual method and the class must be a superclass Hence we
initialize the super class.

We add the method Id to the hash table represented by methEntryTable. When a
call is made first we check if the method is present in the hash table if so we
check if the class and its super class is initialized or not by checking the hash table
for the refEntryTable. If the class is not initialized we initialize it add it to the
refEntryTable and also add it to the local reference. The key used here is the class
36

descriptor, the method name and the signature. Hence this key is unique.

Figure 16: Get Method
The above digram shows the flow for the GetMethodId function. These are
methods for the hash table access


hashcmpMethEntryTableStr



hashcmpMethEntryTable



findFieldMethTableEntry
37



addFieldMethTableEntry
5.6

Pinning object

To access objects we need to go through the reflection and serialization in the JNI.
If we pin the memory location and obtain the memory location in the native space
we can access its field by knowing the offset. Pinning not only allows us access
the object but makes sure that the memory location of the object does not change.
It is also important to unpin the object once we are done using it as these can be
picked up by the garbage collector when in Java program it is assigned to a new
reference. For this two new functions are provided pinObject and unpinObject .
The pinObject method adds an entry to the global DVM pin reference table. The
pin reference table is a global table and has a limit to the number of reference it
can hold. If we cannot add it an exception is thrown. First we add this object to
the global reference table so that while pinning we can check if the address is a
valid object. We also increase the global count if it already exists and also make
sure that it is pinned only once. If it is pinned more than once then the release has
not been called.
In the unpinObject function we remove the reference from the global DVM
reference table. We also remove the global reference that we added. To access the
global DVM reference table we need to obtain mutex lock over the table. Care
should be taken while accessing the attributes an changing it as the memory that is
obtained has no restriction on it. If we override the method table or any other vital
data the object will become corrupt. Also currently only a few objects can be
38

pinned as the global table has limited count.
The figure below show the approach of pinning the object

Figure 17: Pinning object

39

CHAPTER 6
Implementation
The following section gives the implementation details of the hash functions used
and the changes made to JNI API.
6.1

Implementation of hash function

The hash function has the following functions
HashTable* dvmHashTableCreate(size_t initialSize, HashFreeFunc freeFunc)
This function is used to initialize the hash table. The free function depends
on the value that we are storing. In our approach three different functions have
been used as there are hash tables for findClass, GetFieldId and GetMethodId.
The basic operation that the free function does is that it removes the local
reference that we have added for the classes that are related to the value that we
are storing.
void* dvmHashTableLookup(HashTable* pHashTable, u4 itemHash, void* item,
HashCompareFunc cmpFunc, bool doAdd)
This function has dual purpose that is it servers the purpose of look up when
the doAdd flag is set to false and it inserts the <key, value> pair when the flag is
true. It also takes in the compare function. Our approach has used two different
compare functions one while adding entry to the hash table and the other while
retrieving the value from the hash table. Since while adding we need to compare
only the hash value of the key and the collision is taken care by probing the
compare function is simple. During the retrieval of the value we need to perform
40

checks based the data that we are retrieving. The hash is computed using the UT8
string which is the key.
void dvmHashTableLock(HashTable* pHashTable)
This function is used to lock the table so that no two threads change the data
simultaneously. Each hash table has its own mutex lock. This function grabs the
lock.
void dvmHashTableUnlock(HashTable* pHashTable)
This function is used to release the lock on the hash table once the
operations on it completed.
static bool resizeHash(HashTable* pHashTable, int newSize)
When we initialize the hash map the size of the map is set to 50 entries. But
during the program execution the size may increase. This function allows us to
resize the hash table. To perform this operation we need to remove all the
elements and reenter them into this bigger hash table. To take care of the null
values the use of tombstone constant is used. This is an expensive task as all the
values needs to be reentered into the table. To perform the resize operation the
table must be 75% full. This is the threshold value that we have used for the hash
tables.
bool dvmHashTableRemove(HashTable* pHashTable, u4 itemHash, void* item)
This function removes an item from the hash table with the given hash.
The hash function that has be used is as follows as the keys are strings and are of
UTF8 format we use the below formulae to calculate the hash

41

hash = hash *31 + value of character
This hash is computed for the whole length of the string. Here we presume
that add and look up happen more frequently than the remove. Whenever remove
function is called there should be an explicit call to the free function as well as the
remove function does not invoke it internally. If the free function is not called
then there is high chances that we will be running out of space in the local
reference table of the JNI environment. Similarly the free function also handles
the global references that we have created for the values that we store.
6.2

Implementation of GetFieldId

As discussed in the earlier section the we are using an hash map int the JNI
environment variable that is passed with the JNI API call. To insert into the hash
table the function addFieldEntryTableEntry is used. This function takes in the
hash table, the class pointer, the field name and its signature. The key is
combination of class->descriptor plus the name plus signature. This key is hashed
using the technique described previously. While adding if the slot is already
occupied the we retrieve the vale and the following check is made
fieldId->clazz->descriptor is compared with class descriptor
fieldId->name is compared with the field name
filedId->signature is compared with field signature
If all of the following matches we replace this with the new entry by
removing this value. If not probing is used to find the next empty position.
During the retrieval of the value we do the following check
42

fieldId->clazz->descriptor +
fieldId->name +
filedId->signature are concatenated in the above order and is compared to the
key which is the input.
If they are equal then we check if the local reference is valid.
If it is valid return the value else
remove the <key, value> from the hash table.
While adding the entry we check whether the class is initialized by invoking
the dvmIsClassInitializing. The same is done while retrieving the class. Also
checks are made to see if the local references are still valid otherwise we add the
local reference to this class so that no exception occur in the future when the field
values are accessed in the native domain.
6.3

Implementation of GetMethodId

The GetMethodId is a bit more complicated than the GetFieldId. Here the method
can either be a direct method or a interface method. If it is an inter face method
then we need to initialize the super class and add local reference to it. Here we are
use addMethEntryTableEntry to insert it into the hash table.
The key here is the class descriptor, method name and its prototype. Here
the prototype contains the return and the argument list. This combination will be
unique for a class.The comparison that we make while adding is the following
meth->clazz->descriptor is compared with class descriptor.
meth->name is compared with name
43

meth->prototype is compared with the sig
Comparing the prototype and signature is handled by the method
dvmCompareNameProtoAndMethod. If these match the we do not replace the
value but if the method is virtual the we check if the super class is loaded. While
retrieving the value when it is present in the hash table we check the following
check if the method object is valid
check is made to see if the class is initialized if not remove the object and throw
exception
check if the super class is initialized if not remove the object and throw exception
If everything is fine add the local reference to the classes.
If we find that the method is static then we throw an exception as static
methods cannot be accessed by the GetMethodId. Also check is made to see if
there is code to execute in the method while returning the method Id so that we do
not catch any abstract method.
6.4

Implementation of pinObject and unpinObject

The hard part of accessing an object is that we need to do find class first
then find the field offset and the get the value by invoking the get method for the
object in JNI. This is because the garbage collector may move the object however
if we pin the object we are guaranteed that the object will be in same memory
location. This is very helpful as we can access array of objects if we know the
starting location and size of the object instead of calling getObjectArrayElement.
The following is part of the code which adds the object to the global dvm
44

table.
dvmAddToReferenceTable(&gDvm.jniPinRefTable, (Object*)Obj)
This assures us that once we add this object to this reference table the garbage
collector will no longer move or reclaim this memory location. Once we have
finished we need to call the unpinObject method so that the garbage collector can
reclaim this memory and/or move the object.
6.5

Implementation of FindClass

In findClass when the class is requested we first get the class descriptor from the
method dvmNameToDescriptor.The we hash this descriptor and find out whether
this is present in the table if so we add local reference so as to make sure that this
is not garbage collected. Then return this reference. While retrieving we do the
following
class->descriptor is compared with descriptor that got through
dvmNameToDescriptor. If there is a match then we return the reference and add it
to the local reference table.
Below is the code snippet on how the comparison takes place
ClassObject* clazz = (ClassObject*) ventry;
ClassObject* clazz1 = (ClassObject*) vnewEntry;
LOGI("desc val: %s, %s ",(const char*)clazz1->descriptor,(const
char*)clazz->descriptor);
if(clazz!=NULL && clazz1!=NULL){
return strcmp((const char*)clazz1->descriptor,
45

(const char*)clazz->descriptor);
}
return 0;

46

CHAPTER 7
Evaluation
This section compares the JNI interface in the Dalvik virtual machine and the JNI
with the proposed changes. To evaluate the execution time in the JNI we record
the system clock in microseconds. This is accomplished by the following function
is used to get the CPU time.
static inline u8 getClock()
{
#if defined(HAVE_POSIX_CLOCKS)
struct timespec tm;
clock_gettime(CLOCK_THREAD_CPUTIME_ID, &tm);
if (!(tm.tv_nsec >= 0 && tm.tv_nsec < 1*1000*1000*1000)) {
LOGE("bad nsec: %ld\n", tm.tv_nsec);
dvmAbort();
}
return tm.tv_sec * 1000000LL + tm.tv_nsec / 1000;
#else
struct timeval tv;
gettimeofday(&tv, NULL);
return tv.tv_sec * 1000000LL + tv.tv_usec;
#endif
}
47

The time is logged into the system log file using the following command
LOGI(...)
This is a predefined function in the Dalvik virtual machine which internally uses
printf command to the output the sting into the system log buffer. This is then
flushed to Android Debug Bridge console. The next section gives a brief
background on Android Debug Bridge (adb).
7.1

Android Debug Bridge

The adb is a tool which allows us to manage the state of the emulator instance. It
is a client server program with following three components
 Client – This runs on the development machine. We can communicate to
the device by issuing various adb commands.
 Server – This is a background process that runs on the development
machine. This manages the communication between the clients and the
adb thread that runs on the emulator/device.
 Daemon – This is the background process that runs on the device or
emulator.
When you start an adb client, the client first checks whether there is an adb server
process already running. If there isn't, it starts the server process.
To view the logs we issue the following command once we connect the
emulator to the adb server
$ adb logcat
This logging system provides a mechanism for collecting and viewing system
48

debug output. Logs from various applications and portions of the system are
collected in a series of circular buffers, which then can be viewed and filtered by
this command. Each log message is associated with a tag and a priority. These are
the different priority associated with the log


V- verbose(lowest)



D – Debug



I – Info



W – Warning



E – Error



F – Fatal



S – Silent

To filter the log the following command is used
adb logcat tag:I AppName:D *:loglevel
This displays only the log statements that we are interested in. We also tag each of
these log messages with their corresponding processId and threadId.


Brief - Display priority/tag and PID of originating process (the default
format).



Process - Display PID only.



Tag - Display the priority/tag only.



Thread - Display process:thread and priority/tag only.



Raw - Display the raw log message, with no other metadata fields.



Time - Display the date, invocation time, priority/tag, and PID of the
49

originating process.


Long - Display all metadata fields and separate messages with a blank lines.

We have used the display option of thread in our logs. The log statements have
been put in the JNI_ENTRY and JNI_EXIT macro. These macros are invoked
each time there is a call to JNI API. In each of the JNI API we log the method
name so that we can identify the time taken in each of these calls. The next
section will explain the results and comparison of the methods that have been
changed.
7.2

Profiling results

As explained earlier we are using logging provided by the Android Operating
System and the system clock to profile the JNI calls. Here we profile the JNI calls
that we have changed. We ran the application lunar lander and recoded the time in
each of the JNI calls by logging the system time. Then we took the average of the
calls. The below table shows the time.
Time in
FindCl getMetho getFiledI getStaticF dvmCreat GetStatic
microsecond ass
dID
D
iedlID
eJNIEnv MethodI
s
D
JNI without 708.18
changes

147.84

133.18

269.4

308

80.7

JNI with
changes

117.05

114.9

198.4

281.63

82.16

606.38

Table 3: JNI method profile
This is due to the fact that the subsequent calls to these methods need not go
through the reflection of the object to get the field or class. The calls still require
50

800
Android JNI
Android JNI with changes

700
600
500
400
300
200
100
0
GetMethodId
FindClass

GetStaticFieldId
GetStaticMethodId
GetFieldId
dvmCreateJNIEnv

to initialize the object so that the reference is present int JVMs local reference.
Figure 18: JNI method Profiling
The below table gives the timings of the heap sort. First we implemented the heap
sort in Java and recorded the execution time. Then the heap sort was ran using JNI
and the sort was implemented in the native domain. Then we ran the same code
and the data on the improved JNI. The table below shows the result for the
various input size.
Array Size Dalvik Java (ms) JNI modified (ms)

Android JNI (ms)

500

34.38

30

30

1000

40

30

32.81

2000

61.45

32.77

36.53

3000

63.54

31.42

40

4000

68.82

37.64

43.07

5000

76.74

43.51

48.37

6000

85.63

50.04

55.53

Table 4: Heap sort running time
51

90
80
70
60
Dalvik Java (ms)
JNI modified (ms)
Android JNI (ms)

50
40
30
20
10
0
500

2000

4000

6000

Figure 19: Heap sort comparison graph
As we can see from the graph the execution time in the modified JNI decreases as
the input size increases. This is due to the fact that there is more data to be moved
from the Java heap to the native space.
Next we run record the application time in Java, native calls and Operating
system. Form this we can see the amount to time that is saved due to the
techniques proposed. For this we use gprof to profile the emulator. Here we need
to address the issue of scheduling that is done by the kernel when multiple
applications are running. To tackle this issue only one application is loaded into
the emulator so that all the resources are used by this application. The application
52

that we choose was jetboy. The application changes the music based on the events
that occur in the game. It uses the JET library that is provided by android to play
the music file. OpenGL is used to render the graphic contents that is used in the
game. Both these use JNI by the means of NDK library. The below graph depicts
the running time of the application with the JNI API provided by the android.
Here we do not consider the time taken by the application to start. We run the
application for 2 seconds.

Java Time
JNI Time
OS and other Time

Figure 20: Execution time of JetBoy
There were 1173 calls made to the JNI interface during the program execution .
Here we captured the time in the JNI API using the system time stamp. To find the
number of JNI calls that were made were recorded by a static variable declared in
the JNIEnv variable. Once the JNI destroy was called we printed it to the log.
Then we summed up all the count. The table below shows the execution time of
the JetBoy.

53

Environments

Time in mS

Java

1.49

JNI

0.37

OS and other

0.1

Table 5: Execution time for JetBoy
The same experiment was conducted using the modified JNI. Though the number
of calls remained almost same there was an improvement in the JNI time as the
table below shows. This is mainly due to the calls made to the findClass and
getFieldId API methods. The total number of calls that were made to the JNI were
1184
Environments

Time in mS

Java

1.48

JNI

0.31

OS and other

0.1

Table 6: Running Time for JetBoy with modified JNI

Time in
FindCl getMetho getFiledI getStaticF
microsecond ass
dID
D
iedlID
s
JNI without 638.43 206.290
changes
JNI with
changes

572.37

168.5

133.18

362.346

92.9

294.35

Table 7: JNI modified method profile for JetBoy
54

Java Time
JNI Time
OS and other Time

Figure 21: Execution time for the Modified JNI JetBoy
The same was done for Lunar Lander application. This application demonstrates
the drawing resources and animation in Android. The graph below shows the
execution time for the application with unmodified JNI.

Java
JNI
OS and others

Figure 22: Execution time for Lunar Lander with Android JNI

55

Java
JNI
OS and
others

Figure 23: Execution time for Lunar Lander with modified JNI
To analyze the time taken to access the reference object we wrote a program to
access the object which has a reference object as its member. The following were
the results for the JNI modified and unmodified JNI.
Time in micro seconds

Reference access

Android JNI

1307.742

Modified Android JNI

1215.64

Table 8: Reference object access time.
While accessing the reference object we need to have the class structure of both
the parent object and the member object. As we remove this step by caching the
object structure this computation is saved and we see that the modified JNI is
faster

56

CHAPTER 8
Conclusion
JNI has always been an important part of the Java virtual machine. It is
widely used in embedded application as they are architecture specific libraries in
native code (C/C++) which can improve the performance of the application. Our
technique reduces the over head of reflection and serialization that are used while
accessing the objects by the JNI. The pinning of objects helps the programmer to
reference the data through its memory location rather than copying the object into
the native space. There is an performance bonus of 5%-10% achieved using our
technique. As we have seen by inlining the JNI calls[7] there can be a gain in the
performance but these are not applied to the android JNI as it is new to the
market. The Janet[8] provides the programmer an easy way to integrate the JNI
and Java with type safe and static error checking.
There is not much research done in the JNI pertaining to android. This thesis
shows that JNI performance can be improved by reducing the overhead of
synchronization and by caching the class information for future use.

57

CHAPTER 9
Future Work
This thesis provides a technique that improves the running time of android
application that use JNI. The technique uses pinning of objects so that it can be
accessed through reference rather than copying the data. It also shows that if we
cache the information of the class and fields there is a an improvement in the
performance. There can be performance increase by inlining and use of JIT in the
JVM. Currently we are not looking into the stack when the transfer is being made
to the JNI and back. If we could reduce this over head then we make inexpensive
calls to the JNI and use it frequently. Also the profiling of the JNI is done using
the execution time of the application but we can gain a deeper insight once we
inspect the instruction profile in the JNI. This will provide further areas of
improvement.
As android is new to the market there is no benchmark application that can
be used to profile the platform.

58

REFERENCES
[1]

Sangchul Lee and Jae Wook Jeon “Evaluating Performance of
Android Platform Using Native C for Embedded Systems” Control
Automation and Systems (ICCAS), 2010 International Conference
pages 1160 - 1163.

[2]

Walter Binder, Jarle Hulaas and Philippe Moret “A Quantitative
Evaluation of the Contribution of Native Code to Java Workloads”
Workload Characterization, 2006 IEEE International Symposium
pages 201-209.

[3]

Walter Cazzola “SmartMethod: an Efficient Replacement for Method”
In SAC'04, pages 1305 - 1309, Nicosia,Cyprus, Mar. 2004. ACM
Press

[4]

Tamar Domani and Gal Goldshtein “Thread-Local Heaps for Java”
ISMM '02 Proceedings of the 3rd international symposium on
Memory management pages.

[5]

Chi-Chao Chang and Thorsten von Eicken “Interfacing Java to the
Virtual Interface Architecture” JAVA '99 Proceedings of the ACM
1999 conference on Java Grande pages 51-57.

[6]

Martin Hirzel and Robert Grimm “Jeannie:
Granting Java Native Interface Developers Their Wishes” OOPSLA
'07 Proceedings of the 22nd annual ACM SIGPLAN conference on
Object-oriented programming systems and applications pages 19 - 38

[7]

Levon Stepanian, Angela Demke Brown, Allan Kielstra, Gita
Koblents and Kevin Stoodley “Inlining Java Native Calls At Runtime”
In VEE '05: Proceedings of the 1st ACM/USENIX international
conference on Virtual execution environments, pages 121--131, New
York, NY, USA, 2005. ACM Press.

[8]

MarianBubak, DawidKurzyniec, and Piotr Luszczek “Creating Java to
Native Code Interfaces with Janet Extension” In Proc. SGI Users's
Conference, pp. 283--294, Oct. 2000.

[9]

J. Gosling, B. Joy, G. Steele, and G. Bracha. The Java Language
Specification. Addison-Wesley, second edition, June 2000.

[10]

S. Liang. The Java Native Interface: Programmer‟s Guide and
Specification. Addison-Wesley, June 1999.
59

[11]

Sun Microsystems.Integrating native methods into Java programs.
http://java.sun.com/docs/books/ tutorialNB/download/tutnative1dot0.zip, May 1998.

[12]

G. Tan, A. W. Appel, S. Chakradhar, A. Raghunathan, S. Ravi, and D.
Wang.” Safe Java native interface”. In Proc. 2006 IEEE International
Symposium on Secure Software Engineering, pp. 97–106, Mar. 2006.

[13]

M.Bubak, D.Kurzyniec, andP.Luszczek
“CreatingJavatonativecodeinterfaces with Janet extension”. In M.
Bubak, J. Mo scinski, and M. Noga, editors, Proceedings of the First
Worldwide SGI Users‟ Conference, pages 283–294, Cracow, Poland,
October 11-14 2000. ACC-CYFRONET.

[14]

Java Native Interface.
http://java.sun.com/j2se/1.3/docs/guide/jni/index.html.

[15]

Oracle, "Java Virtual Machine Profiler Interface (JVMPI)."

[16]

F. Y. Tim Lindholm, "The JavaTM Virtual Machine Specification,"
1999

[17]

D. Bornstein, "Dalvik VM Internals," 2008.

[18]

P. Brady, "Anatomy & Physiology of an Android," 2008.
http://www.oracle.com/technetwork/java/javame

[19]

www.wikipedia.org

[20]

Damianos Gavalas and Daphne Economou “Development Platforms
for Mobile Applications” Software, IEEE Issue Jan.-Feb. 2011
Volume 28 page 77

[21]

J. Andrews “Interfacing Java with native code – performance limits.”
http://www.str.com.au/jnibench/

60

2026

JOURNAL OF NETWORKS, VOL. 9, NO. 8, AUGUST 2014

Resource Allocation in City-wide Real-Time
Wireless Mesh Networks
Jun Xu 1, 2, Yann-Hang Lee 2, Chengcheng Guo 1, and Jianfeng Yang 1*
1. WuHan University, WuHan, HuBei, 430072, China
2. Arizona State University, Tempe, AZ 85287, USA
*Corresponding author, Email: xujunlinda@gmail.com, yhlee@asu.edu, netccg@whu.edu.cn, yjf.whu@gmail.com

Abstract—To support real-time communication, multi-radio
multi-channel city-wide WMNs (Wireless Mesh Networks)
are studied in this paper. In the studied WMNs, a WMN
router
is
equipped
with
two
communication
interfaces/radios, one is working on 2.4GHz for the client
accessing, and the other is working on 5GHz for the
backhaul communication. First, a greedy static channel
assignment algorithm for access interfaces is proposed to
minimize the maximum interference among all the access
interfaces. The proposed greedy algorithm has an
approximation factor of 2-1/m, where m is the number of
available orthogonal channels of access interfaces. To
guarantee interference-free communications for all links, a
slot allocation algorithm, combining with the channel
assignment for backhaul interfaces, is suggested. The slot
allocation algorithm is based on the fixed priorities assigned
to real-time flows. After that, the worst-case end-to-end
delay analysis of each flow under the slot and channel
allocation algorithms is given. Finally, simulation results
demonstrate the effectiveness of the delay analysis and the
channel assignment and slot allocation algorithms.
Index Terms—Channel Assignment; Slot Allocation; RealTime, Delay Analysis, Multi-Radio Multi-Channel

I.

INTRODUCTION

……
wireless
router
L2

……

© 2014 ACADEMY PUBLISHER
doi:10.4304/jnw.9.8.2026-2036

L1
r

……

Many cities have tried to deploy city-wide WIFI
networks [1]. Most of them are one-hop wireless
networks which have limited access areas. Multi-hop
wireless network can be an attractive approach in citywide networks to extend the coverage areas to some rural
or harsh environment where the deployment of cables is
difficult and non-economic.
A city-wide wireless network can provide services to
both non-real-time and real-time traffic. Unlike the nonreal-time traffic, the real-time traffic needs to be
transmitted to the destination within the specific deadline.
Thus, the resource allocation of the real-time traffic is
much more critical, which is the study point of this paper.
The real-time traffic finds lots of applications in the
daily life. For example, transmitting the environmental
surveillance message on time can avoid toxic gas leaking,
or house fire accident. Also, in the transportation
applications, the on time transmission of the car accident
or road condition messages can avoid congestion.
Considering that the periodic real-time traffic is a
general traffic pattern in surveillance, control applications,
in this paper, we focus on the resource allocation for the

periodic real-time traffic in the city-wide wireless
network. Also, the MRMC (Multi-radio Multi-channel)
technology [2] is adopted to compensate the throughput
degradation caused by the multi-hop wireless
transmissions.
The resource allocation includes the channel and slot
assignments. Although this topic is widely studied,
seldom of them consider city-wide networks. Another
factor making our work different from others is that each
wireless mesh router is equipped with two independent
wireless interfaces working on different standards. One
(access interface), with IEEE 802.11g protocol on
2.4GHz, is used for client access, and the other (backhaul
interface) is used for the communications with other
wireless mesh routers with IEEE 802.11a protocol on
5GHz. Also, each mesh router is equipped with at least
one wired interface to support the connection to gateways.
The gateways are connected to some mesh routers. The
communications between gateways are through the
Internet.
Thus, the resource allocation algorithms should
consider two different standards.
For the network deployment, we suggest the grid
topology city-wide networks. It has been shown in [3]
that grid networks are much more suitable for large scale
mesh deployment compared to random topology
networks. And the grid topology is also suggested by
Cisco [4].

city-wide
area

gateway

……
client
client
802.11g
wireless link

client
802.11a
wireless link

wired link

Figure 1. Network deployment

In Fig. 1, we illustrate the deployed network. The area
of a city can be viewed as a rectangle with the length L1

JOURNAL OF NETWORKS, VOL. 9, NO. 8, AUGUST 2014

and the width L2. And r is the maximum distance between
two mesh routers. Cisco has recommended some
reasonable distances of r in [4], one can also adjust the
distance basing on the routers one chooses. A thin dotted
line represents a wireless communication link between
two 802.11g interfaces, and a thick dotted line denotes a
wireless communication link between two 802.11a
interfaces. And a solid line represents a wired link.
The fundamental problem needed to solve when
designing a channel and slot assignment algorithm for
wireless networks is the interferences. In order to avoid
the frequency channel switching of clients, the fixed
channels are assigned to the access interfaces in the
wireless mesh routers. However, the channels for the
backhaul interfaces are assigned dynamically.
To provide real-time communications for traffic, a
deterministic end-to-end delay is important [5]. It’s
widely recognized that CSMA/CA scheme is not suitable
for the real-time applications due to the poor throughout
in multi-hop network [6] which will result in high delay
and jitter [7]. Hence, TDMA is considered in real-time
applications. It is adopted in WirelessHart [8],
ISA100.11a [9], and WIA-PA [10] standards to provide
real-time services for the applications in industry control
scenarios.
Thus, in this study, TDMA scheme is adopted, and we
assume that an active link can accomplish the
transmission of a packet in a single time slot.
Now, the following two questions come naturally:

How to assign fixed channels to the access
interfaces and how to dynamically assign
channels to the backhaul interfaces?

How to assign time slots to wireless links to
achieve real-time communications for flows?
Another problem we need to deal with is the
schedulability test of the channel and slot assignment
algorithms. Thus, an end-to-end delay analysis is needed.
Corresponding to the above questions and requirement,
our contributions include:

An approximation greedy algorithm to assign
static channels to the access interfaces of mesh
routers. The aim is to minimize the maximum
interference among all the channels. We prove
that greedy algorithm is a factor (2-1/m)
approximation algorithm, where m is the number
of available orthogonal channels for the access
interfaces.

A joint slot allocation and channel assignment
algorithm. The algorithm determines channel
assignment for backhaul links and slot allocation
for flows. The bandwidth (slots) requirements of
real-time flows are considered and the algorithm
achieves the interference-free communications.

The worst-case end-to-end delay analysis of each
real-time flow. Using this delay analysis, we can
determine whether real-time flows can meet the
deadline requirements.
The remainder of the paper is organized as follows. In
Section II, we introduce some related works on channel
assignment and slot allocation, and in Section III, we

© 2014 ACADEMY PUBLISHER

2027

present the network and interference models, and also the
problems description. The channel assignment and slot
allocation algorithms are described in Section IV. We
analyze the worst-case end-to-end delays in detail in
Section V. Numerical results from simulations are given
in Section VI to demonstrate the effectiveness of the
algorithms and the delay analysis. At last, in Section VII,
we conclude this paper.
II.

RELATED WORK

The channel assignment and scheduling algorithms in
multi-radio multi-channel wireless networks are studied
widely. Most of them focus on improving the throughput
of networks [11, 12, 13].
The fixed channel assignment algorithms are studied in
[14], with objectives of maximizing the number of
possible simultaneous transmissions, minimizing the
average size of the co-channel interference set and
minimizing the maximum size of the co-channel
interference set, are not applicable to our scenario since
for the backhaul interfaces, the channel and slot
assignments should be considered jointly.
The algorithm in [15] considers the channel
assignment and slot assignment jointly. The wireless link
interferences are concerned in the paper [15], and the
Latin squares are adopted to avoid the interferences. The
M4 algorithm doesn’t aim to improve the throughput or
reduce the communication delay directly. However, as
shown in the paper, the M4 algorithm adopting TDMA
strategy achieves higher throughput and lower average
end-to-end delay compared to CSMA/CA mechanism.
As surveyed in [16], most of the scheduling algorithms
in wireless mesh networks are short of the consideration
of the real-time flows, seldom of them look into the
MRMC technology. We ignore the detailed discussion of
those works and refer the readers to [16].
Some researches [17-19] consider flow deadlines.
However, the study in [17] is only for sink-tree topology
networks and leaky-bucket-shaped flows, which are
different from the city-wide network topology suggested
in section 1, and the periodic traffic model we study. The
CEDF (Coordinated Earliest Deadline First) scheduling
algorithm is studied in [18], the focus of the paper is the
delay bound analysis, not the scheduling algorithm itself.
In [19], the objective is to minimize the TDMA delay,
which can be viewed as the frame length in TDMA
protocol. Thus this objective is different from designing a
scheduling algorithm to provide the delay guarantee for
each flow.
As for the works related to multi-channel and real-time
flows, some works have been done, such as [20-22]
which study the WirelessHart [8] standard.
The branch and bound scheduling algorithm proposed
in [20] is an optimal scheduling algorithm for periodic
flows. However, in the worst-case situation, the timecomplexity can be very high. Thus, the authors propose a
heuristic scheduling algorithm, the C-LLF (Conflict
Aware Least Laxity First) algorithm. Channel spatial
reuse is not allowed in C-LLF algorithm, the
transmissions with the smaller conflict-aware laxities are

2028

JOURNAL OF NETWORKS, VOL. 9, NO. 8, AUGUST 2014

assigned the higher priorities. And the non-interference
transmissions with higher priorities are scheduled in each
time slots.
In [21], the authors focus on the analysis of the end-toend delay of each flow when fixed priority scheduling
algorithms are adopted. The multi-channel transmission
scheduling is modeled as the multi-processor scheduling
problem. However, this mapping works only when
channel spatial reuse is disabled, thus it’s not applicable
for the city-wide scenario where channel spatial reuse is
allowed. In the subsequent work, the worst-case end-toend delay of flows accounting for failures is analyzed
[22].
Considering that we study the periodic traffic model
and the interference is related to the flow periods. Thus
the channel and scheduling algorithms mentioned above
are not applicable.
III.

NETWORK AND INTERFERENCE MODELS

Assume that the network is represented by an
undirected graph G=(V, E), where V is the node set, and E
is the edge set. Let Vt be the mesh client set, Vr be the
mesh router set, and then we have V  Vt  Vr . Also, the
link set can be expressed as E  Et  Er , where Et is the
set of all the access links between clients and mesh
routers, and Er includes all the links between mesh
routers.
For the access interfaces of the mesh routers, the
available orthogonal channel set is Ct  {ct1 , ct2 , ctm } ,
where m is the maximum number of orthogonal channels
for the access interfaces. For the backhaul interfaces, the
available orthogonal channel set is Cr  {c1r , cr2 , , crn } ,
and n is the maximum available number of orthogonal
channels for the backhaul network. Let F={f1, f2, …, fN}
represent N periodic real-time flows. The periodic realtime flow fi (i=1, 2,…, N) can be characterized as <Peri,
Pr i, D i, i , Pi>, where Peri is the period, Pr i is the
priority, Di is the deadline, i is the starting slot of the
first packet, and Pi is the path traversed by the flow f i. Let
si be the source node and di be the destination node of
flow f i. Both the source and destination nodes are mesh
clients.
Each mesh router is equipped with one access interface
and one backhaul interface. Each wireless interface works
on the half-duplex mode. For each node, we assume the
communication range is Rc and the interference range is
RI. As in [15], the interference range is two times the
communication range, that is RI =2Rc . In the following,
we introduce the interference on the two kinds of
interfaces in detail.
A. Interferences on Access Interfaces
If the access interface of node vi is assigned channel
c( c  Ct ), we say lc,i =1, otherwise it equals to 0. And let

I ci represent the interference on that interface. Define N ic
as the set of the routers within the interference range of
node vi (including node vi) and the channel adopted by the

© 2014 ACADEMY PUBLISHER

access interfaces on these routers is c ( c  Ct ). Let xkq be
1 if flow fq comes from or goes to client k, otherwise it is
0. Let Vr j represent all the clients access to the mesh
router j ( j  Nic ), and  j represent its traffic density on
the access interface, that is,

j 

 x

f q F kVr

q
k

j

1
Perq

(1)

We define the interference on the access interface of
node vi with the channel c the summation of the traffic
density of all the access interfaces on the interference
routers. It’s expressed as follows,
I ci 



j

(2)

jNic

For an access link e  Et , let ge represent the mesh
router whose access interface enables the link e. The
channel used by this link is the channel assigned to ge.
Then the interference links of link e on channel c ( c  Ct )
can be expressed as follows.

I e  {e | e  Et , ge  N gce }

(3)

We set yec ( s) to be 1 if link e works on channel c in
slot s, otherwise the value is 0. The slots assigned to link
e should satisfy the following constraint.
yec ( s)   yec ( s)  1

(4)

eI e

The constraint (4) should be satisfied in an
interference-free network. When the number of available
orthogonal channels is not enough, interference-free
communications may not be achieved by just the channel
assignment. So the constraints should be satisfied in the
slot allocation algorithm proposed in section 4.
B. Interferences on Backhaul Interfaces
If link e=(u, v) is the current transmitting backhaul link
between the two end interfaces of nodes u and v. Its
interference links can be classified into two categories.
Let I 1(e) represent the first category links which share
at least one common interface with link e. All these links
can’t transmit in the same slots as link e no matter how
many free channels can be obtained. Link e itself belongs
to this category. In addition, let I 2(e) represent the second
category links which are connected with interfaces in the
interference range of the backhaul interface of u or v, but
don’t connect directly with node u or v. Links in this
category need to adopt different orthogonal channels if
they want to transmit in the same slots with link e.
In any slot s, the total number of orthogonal channels
used by link e and its category 2 links shouldn’t exceed
the total orthogonal channel number n. The other
condition is that link e and the links in I 1(e) shouldn’t
share the same channel in any time slot. The two
conditions are described as follows for all c and c’, where
c and c ' are two channels belonging to Cr and c  c ' .

JOURNAL OF NETWORKS, VOL. 9, NO. 8, AUGUST 2014

yec (s) 



yec ( s)  n

(5)



yec (s)  1

(6)

eI 2 ( e )

yec (s) 

2029

eI1 ( e )

If and only if inequality (4), (5) and inequality (6) are
met
simultaneously,
the
interference-free
communications can be achieved. Hence, the slot
allocation algorithm combining the channel assignment
we will describe in section 4 should meet the above three
constraints.
C. Problem Description
The problems studied are divided into two
subproblems since the access interfaces and backhaul
interfaces working on two different protocols.
Subproblem 1: Static channel assignment for the
access interfaces
Let Ct represent the collection of the channels
assigned to all the access interfaces, that is,
Ct  {c | lc ,v  1, v Vr }
Given a set of available channels Ct  {ct1 , ct2 , , ctm } ,
try to find a channel assignment algorithm to minimize
the maximum value of I ci ( vi Vr , c  Ct ). The
mathematical formulation is:
min. max. I ci
s.t. | Ct | m

l

cCt

c , vi

 1 , vi  Vr

Subproblem 2: slot allocation combing the channel
assignment for the backhaul interfaces
The interference-free communications can’t be
achieved by just considering the static channel
assignment for the access interfaces. In this problem, the
joint channel assignment for the backhaul interfaces and
slot allocation for all links are considered.
Note that, the interference model we consider for the
backhaul interfaces is protocol interference model.
However, the joint scheduling algorithm proposed in the
next section for solving subproblem 2 can be extended to
other interference models.
TABLE I.

CHANNEL ASSIGNMENT OF THE ACCESS INTERFACES

channel c with the smallest interference on the current
visited interface is selected and assigned to the interface.
The pseudocode of the greedy channel assignment
algorithm is described in TABLE I.
Now, we prove that the greedy algorithm is a factor (21/m) approximation algorithm, and the time complexity is
O(|Vr|).
Theorem 1. The greedy algorithm in TABLE I is a
factor (2-1/m) approximation algorithm.
Proof. Assume there’s an optimal algorithm  , the
maximum interference generated by the algorithm  is I*.
Then I* should be greater than the density of any single
node, that is,

I *  max iVr {i }

Let dv denote the number of mesh routers connected to
the node v. As I* is the maximum interference of the
whole network, it should be greater than the average
interference of all the nodes which locate in the
interference range of any node v including node v itself.
Thus, we have,

I* 

4

calculate I cv according to the expression (2);

5
assign the channel with the smallest I cv to the access interface
on node v

IV.

CHANNEL ASSIGNMENT AND S LOT A LLOCATION
ALGORITHMS

A. Channel Assignment on Access Interfaces
Given that the above problem is NP-hard [23], we
propose a greedy channel assignment algorithm to solve
it. The greedy algorithm behaves very simply. It visits all
the access interfaces one by one. And each time the
© 2014 ACADEMY PUBLISHER

1 dv
( i  v ); v  Vr
m i 1

(8)

Let assume that the maximum interference I g generated
by the greedy algorithm happens on the node v with
channel c*. The interference on node v should be no
smaller than I g if we assign channel c  ( c  c * ) rather
than channel c* to node v. We have,

I g  I vc  v ; c  Ct

(9)

Add both sides of the expression in (9) over all the
channels, we get,

m  Ig 

I

cCt

dv

c
v

 m   v   i  m   v

(10)

i 1

Such that,

Ig 

1 dv
(m-1)
(  i   v ) 
v
m i 1
m

(11)

Substitute expression (7) and (8) into (11), we have,

Algorithm AccessChannel( )
1 for node v Vr
2 for c  Ct

(7)

Ig  I * 

m 1
I*
m

(12)

Therefore, we get,

Ig
1
 (2  )
I*
m

(13)

Hence, the greedy algorithm in TABLE I is a factor (21/m) approximation algorithm.
In the specific situation, the access interfaces work on
the IEEE 802.11g protocol which supports 3 orthogonal
channels. The approximation factor of the greedy
algorithm is then 5/3.

2030

JOURNAL OF NETWORKS, VOL. 9, NO. 8, AUGUST 2014

Theorem 2. The time complexity of the greedy
algorithm in TABLE I is O(|Vr|).
Proof. The first iteration of the greedy algorithm in
TABLE I runs for |Vr| steps, and the second iteration runs
for |Ct| steps. The time complexity of the algorithm is
O(|Vr||Ct|), where |Ct| is a constant value whenever the
physical layer protocol is set. Thus, the time complexity
of the greedy algorithm is O(|Vr|).

from [24]. We say the real-time periodic flow set is
simply periodic if for every two flows fi and fk in the
network with Peri<Perk , Perk is an integer multiple of
Per i. The time complexity of the algorithm in TABLE II
is O(NL) when the periodic flows are simply periodic,
and is O(NLT2) with arbitrary flow periods, where N is
the total number of flows, L is the total number of links in
the network, and T is the TDMA frame length.

B. Joint Channel and Slot Allocation
In this section, we consider the channel assignment on
the backhaul network, and the slot allocation involves all
the links used by flows. Although the channels and slots
are assigned to links, it can be viewed that the channels
and slots assigned to a certain interface are the collection
of the channels and slots assigned to the links connected
to it. The slots demanded by link e during a TDMA frame
 , where F is
of length T is no less than Se    T
e
 Peri 
iFe 
the set of flow traversing through link e. It is assumed
that we have already known the parameters of the realtime flows. TABLE II shows the pseudocode of the joint
channel and slot allocation algorithm under which the
number of the assigned slots for each link e is no less
than Se.

C. An Example
As an example shown in Fig. 2, v1, v2, v3 and v4 are
mesh routers and s1, s2, d1 and d2 are terminals. There are
two flows f1 and f2. The source node of flow f1 is s1 and
the destination node is d1, the path is P1={(s1, v1 ), (v1, v 2),
(v2, d1)}, the period is 6, the start time of the first packet
is 0. For flow f2, the source node is s2, and the destination
node is d2, the path is P2={(s2, v1), (v1, v2), (v2 , v4 ), (v4,
d2)}, the period is 12, the start time of the first packet is 5.
First, we assign channels to the access interfaces of the
mesh routers. Assume for the access interfaces, we have
three orthogonal channels, channel 1, channel 2 and
channel 3. We start with node v1, assign channel 1 to it,
and then for node v2, the interference on channel 1 is
1 1 1
, the interference on channel 2 is
I1v2   
6 12 4
I 2v2  0 , and on channel 3, the interference is I 3v2  0 .
We assign the smallest channel from the least
interference channels to node v2, that is, channel 2. Then
we come to node v3, then interferences of the three
1 1 1
channels on this node are I1v3    , I 2v3  0 ,
6 12 4
v3
I 3  0 , hence we assign channel 2 to node v3. Then for
node v4, the interferences on the three channels are
1
I1v4  0 , I 2v3  , I 3v3  0 , so we assign channel 1 to
6
node v4. For the channel and slot assignment on the
interfaces for the communication between mesh routers,
let (slot, channel) represent the slot-channel pair assigned
to an interface. The TDMA frame length is
T=LCM(6,12)=12. During any interval of length T, s1
generates two packets and s2 generates one packet. It also
implies that flow f1 requires at least two slots on all the
links in path P1 and flow f2 requires at least one slot on all
the links in path P.

TABLE II.

JOINT CHANNEL AND SLOT ALLOCATION ALGORITHM

Algorithm SlotChAssign ( )
1 for flow fi  F
2 for b from 1 to ceiling(T/Peri)
3 for e  Pi
4
if e is the first link of path Pi
5
the start slot s equals to (b-1)∙Peri+ i ;
6
else
7
the start slot s equals to the slot assigned most recently to the
links before e plus 1;
8
if e is an access link
9
while expression (4) is not true
10
s←s+1;
11
assign slot s to the link e;
12 else
13
if expression (5) is TRUE
14
select the smallest channel ch which satisfies expression
(6);
15
while ch is greater than n
16
s←s+1;
17
if expression (5) is TRUE
18
select the smallest channel ch which satisfies expression
(6);
19
assign (s,ch) to link e;

The joint channel and slot allocation algorithm is based
on the priorities of flows. The flows in F are sorted with
the descending order of the priorities. It means that the
lower the flow label is, the higher the priority is, i.e., if
fi<fk , then Pri>Prk . Each link has been assigned some (slot,
channel) pairs after the algorithm “SlotChAssign”
described in TABLE II, and the interference-free
communications have been guaranteed since constraints
(4), (5) and (6) are all satisfied for each (slot, channel)
pair.
Before saying something about the time complexity of
the algorithm, we introduce the term “simply periodic”

© 2014 ACADEMY PUBLISHER

f1

s1
v1

s2
f2

d1

e1

e2

v2
e3

v3

e4

v4

d2

Figure 2. Example of channel and slot assignment

The slots and channels assigned to link according to
the algorithm in Table 2 are as follows.

JOURNAL OF NETWORKS, VOL. 9, NO. 8, AUGUST 2014

2031

For the links on f1’s path, for link (s1, v1), assign (0, 0)
and (6, 0) to it, for (v1 , v2 ), assign (1, 0), (7, 0) to it, for
(v2, d1), assign (2, 1), (8,1) to it. For the links on f2’s path,
for link (s2 , v1 ), assign (5,0) to it, for (v1, v2), because the
slot 6 has been assigned to this link for the transmission
of flow 1, so we assign (7, 0) to it, for (v 2, v4 ), assign (8, 0)
to it, for (v4 , d4), assign (9, 0) to it. The slots which are
free can be used by the non-real-time flows.
V.

WORST-CASE END-TO-END DELAY A NALYSIS

In this section, we focus on the analysis of the worstcase end-to-end delay of each flow under the proposed
channel assignment and slot allocation algorithms in
section IV.
Generally speaking, there are three kinds of delays:
conflict delay, channel contention delay, and transmission
delay. The transmission delay is equal to the path length.
We introduce the conflict delay and channel contention
delay as follows.
A. Conflict Delay
Let H i represent all flows with priorities higher than
that of flow fi, and Eia, k denote the links on the path of

In order to avoid overestimate, if a link on Pk belongs
hi 1

Eia, k , we won’t include it in Eia, k considering the

to
a2

fact that the contributions of a link will always be greater
when it acts as a 1-hop interference link than as a 2-hop
interference link.
Hence, during time duration  ia , the worst-case
channel contention delay  ia, cont on link eia is as follows.



 ia,cont   

 kHi

 ia   ia,conf   ia,cont

hi 1

Eia, k for a specific k.
a2

Let  denote the worst-case delay on the a-th
(a=1,2,…, hi) link on path Pi. The worst-case conflict
delay  ia,conf for flow fi on link eia then can be described
as,
a
i

 ia,conf 

 a 
|Eia, k |  i 

f k H i
 Perk 

(14)

B. Channel Contention Delay
The channel contention happens only in the backhaul
network. It depends on a link’s 2-hop interference range
links. For a link e in the path of flow f i, let Eia, k represent
a
i

the 2-hop interference links of link e which follows by
flow fk ( f k  H i ). Then a packet pk from flow fk can
contribute at most | Eia, k | transmissions to the channel
contention on link e.

© 2014 ACADEMY PUBLISHER

(15)

(16)

We can calculate the value of  ia by using the fixedpoint iteration algorithm. After obtaining all the worstcase delays of links on path Pi, we can get the worst-case
end-to-end delay of flow f i, that is,
hi

 i   ia

a
i

more than once in the set


|


Hence, the total delay  ia on link eia can be expressed
as,

flow fk which conflict with the a-th link eia of flow fi. All
the links belong to I 1( e ) . Let hi represent the length of
Pi.
We need to emphasize on the following two points.
 The links in set Ei1, k and Eih,ik are access links
since the first and last links are access links, and
only access links can interfere access links.
 In the backhaul network, a link on a higher
priority flow path can be 1-hop neighbor link of
two links in the same path, but a transmission on
a link can only be conflict by a packet once on
the same link. Hence, we won’t include a link

1   ia  a

 | Ei , k
n  Perk 

(17)

a 1

We
can
simplify
the
calculation
when
max{Di}<min{Peri} (i=1, 2, …, N). Under this situation,
at most one packet from a single higher priority flow
contributes to the delay of the lower priority flow. Hence,
the end-to-end channel contention delay can be expressed
as,
hi

 i ,cont   

a 1 k H i

1 a
| Ei , k |
n

(18)

For the conflict delay in the backhaul network, the
result derived in paper [21] can be used. The conflict
delay (k , i) caused by a single flow f k ( f k  Hi ) to the
flow f i is described as follows.


(k , i)  Q(k , i )   ( j (k , i)  3)

(19)

j 1

We refer the readers to the paper [21] for a more detail
explanation of the equation (19) including the meaning of
the symbols.
The expression (19) can be only used in the backhaul
network. The conflict delays on the first link and last link
should still be obtained by expression (14). And the
expression can be reduced to,

 ia,conf 

 Eia, k 
   ; a  1or hi
f k H i  Perk 

(20)

Hence, the worst-case end-to-end delay of flow fi is,

 i   i ,cont 

 ( k , i )  

kHi

1
i , conf

  ih,iconf  hi

(21)

2032

JOURNAL OF NETWORKS, VOL. 9, NO. 8, AUGUST 2014

VI.



NUMERICAL RESULTS

In this section, we study a simulated multi-hop MRMC
WMN for the Tempe city in Arizona, USA. The total area
of “Tempe City” is 40.2 square miles (104 km2) [25]. We
view “Tempe City” as a square with edge length
L=6.34miles (10.2km). The radius of the local access cell
size r is 600 feet (180m) recommended by Cisco [4]. We
specify that the distance between two mesh routers is at
most 2r , which is 848.4 feet (254.52m). Hence the
minimum number of mesh routers needed is 1600.
The topology of the city and the placement of routers
are shown in Fig. 3. Fig. 3(a) shows the boundary of the
Tempe City, Fig. 3(b) shows the wireless mesh network
deployment of the Tempe City. In Fig. 3(b), we just show
the users and mesh routers in four local areas. The
smallest circles (red ones) represent users in each local
area, and the mid-size circles (blue ones) represent the
mesh routers and the large-size circles (purple ones)
represent the gateways, and the largest size circles
represent the coverage areas of local areas. The Tempe
city can be fully covered by 1600 largest circles which
implies that the minimum number of wireless mesh
routers needed is 1600, and the minimum number of
gateways needed is 100. In the sparse areas, such as area
A and area B in Fig. 3(b), we just need to deploy one
mesh router. However, in some crowded areas, we need
to deploy more than one mesh router, such as area C and
area D in Fig. 3(b). In order to avoid congestion, the
general rule we obey is that the bandwidth required by
the users in a mesh router is no greater than the
bandwidth the mesh router can provide.

……

……

……

……

……

……

RM (Rate Monotonic). The flow rate is the
inversion of the flow period, so by RM, it implies
that the smaller the period is, the higher the
priority is.
We run each simulation 1000 times and show the link
and node utilizations, channel switch ratios, and the PMD
(Percentage of flows Meeting Deadlines) values obtained
by the analysis and simulation in the subsequent content.
A. Link and Node Utilizations
The link and node utilizations here are to show the
congestion situations of the backhaul network under the
two different period ranges 25~10 and 27~11. The utilization
on link e can be represented by
1 e
Ue  
xi
eEr , fi F Peri
and the utilization on node v can be represented by
1 v
Uv  
xi .
Per
vVr , fi F
i
The utilizations are only related to the flow paths and
periods. The routing protocol (such as the shortest path
routing protocol) will generate the same path for a flow
no matter what the priority of the flow is when the source
node, destination node and the network topology are
given. In Fig. 4, we give the maximum (Max), minimum
(Min) and average (Avg) link and node utilizations by
varying the flow number from 50 to 500 with an
increment value of 50. The Max, Min and Avg values of
the link (node) utilizations are the maximum, minimum
and average values of the 1000 runs among all the links
(nodes). Fig. 4(a) and 4(b) show that the link and node
utilizations with period range 25~10 are obviously greater
than the values with period range 27~11, which implies
that the network is much more congestion with the period
rang 25~10 than the range 27~11.
0.5

0.5
……

Max,25~10

……

……

Gateway

(a) Tempe map[26]

link utilization

0.4

……

Router

Client

Max,27~11

5~10

7~11

Min,2

Min,2

Avg,25~10

Avg,27~11

0.3
0.2

Max,25~10

Max,27~11

Min,25~10

Min,27~11

Avg,25~10

Avg,27~11

0.3
0.2
0.1

0.1

(b) WMN
0

Figure 3. Coverage of tempe city

100

200
300
number of flows

400

500

(a) Link utilization

It’s known that the distribution of traffic in a city is not
uniform. Some places may gather more traffic than others.
Hence, in the simulation, we randomly assign some
mesh routers more traffic than others. The period of each
flow is set to 2x where x is a random integer number in
the range [a, b], we also use 2a~b to represent this. And
for any flow fi  F , its period Peri should be greater than
its path length hi.
Three priority strategies are considered in the
simulations.

SPF (Shortest Path First). The shorter the path
length is, the higher the priority is.

LPF (Longest Path First). The longer the path
length is, the higher the priority is.

© 2014 ACADEMY PUBLISHER

0.4

node utilization

……

0

100

200
300
number of flows

400

500

(b) node utilization

Figure 4. Link and node utilizations

B. Channel Switch Ratio
First, we define the average channel switch ratio
“swratio” as below.
sw(e)
swratio  avg (
),
eE & S ( e )  0 | S (e) |
where sw(e) represents the channel switch time on link e.
For example, for link e, if it is assigned 5 slots
{0,3,7,9,12}, and the channels assigned to these slots are
{0,0,1,2,0}. Then the link changes channel form 0 to
1from slot 3 to slot 7, and from slot 7 to 9, it changes
channel from 1 to 2. Also from slot 9 to slot 12, it

JOURNAL OF NETWORKS, VOL. 9, NO. 8, AUGUST 2014

2033

changes channel from 2 to 0. Hence, the channel switch
time for link e is 3.
We vary the flow number from 50 to 500 with an
increment of 50, and we set the phases of all the flows to
be 0. Fig. 5(a) and 5(b) show the average channel switch
ratios over the 1000 runs and over all the links of the
whole networks. The figures show that the average switch
ratios increase with flow numbers and the values with
priority strategies SPF and LPF are much bigger than RM.
When the flow number is 500, the average channel switch
ratios with priorities SPF and LPF are about 30% under
period range 25~10 and about 23% under period range 27~11.
For RM, the values are no greater than 6% under both
period ranges. Recall the results of the maximum link and
node utilization with 500 flows in Fig. 5, the values under
period range 25~10 are about 15% and 27%, respectively.
If more flows are added into the network, the network
manager can adjust the flow periods or flow paths to
relief the link/node burden. So, we believe that the
average channel switch ratios shown in Fig. 5 are
acceptable and the channel assignment strategy on the
backhaul network won’t lead to unstable channel switch
in a network with reasonable congestion situation.
1

average channel switch ratio

average channel switch ratio

1
SPF
LPF
RM

0.8
0.6
0.4
0.2
0

100

200
300
400
number of flows

500

(a) Period (5-10)

SPF
LPF
RM

0.8
0.6
0.4
0.2
0

100

200
300
400
number of flows

500

(b) Period (7~11)

Figure 5. Channel switch situation
1
SlotChAssign
CLLF

acceptance ratio

0.8
0.6
0.4
0.2
0
10

20

30
40
number of flows

50

Figure 6. Compare our algorithm with CLLF

C. The Efficiency of the Joint Scheduling Algorithm
In order to compare our scheduling algorithm with the
state-of-the-art algorithm, we currently ignore the access
clients and compare the joint channel and slot assignment
algorithm for the backhaul network.
We compare our algorithm with the C-LLF algorithm
proposed in [20].
The algorithm proposed by us is named with
“SlotChAssign”, the fixed priorities of flows are assigned
by RM strategy. We compare the acceptance ratios of the
two algorithms. If we run the algorithm for NI instances
(in each instance, we assign different source, destination
nodes, periods, deadlines to flows), and by a specific
scheduling algorithm, only SI instances can be schedules
© 2014 ACADEMY PUBLISHER

(which means all the packets in the flows can arrive at the
destination nodes before deadlines), then the acceptance
ratio of the algorithm is SI / NI.
In the simulation, we construct a 40*40 grid topology
network, and vary the number of flows from 10 to 50
with an increment of 10 flows. The flow periods are at
the range of 25~10. The number of available orthogonal
channels is set to be 2. Each time with specified number
of flows, we run 10 instances and obtain the acceptance
ratios of the two algorithms. The results are shown in
Fig.6. It shows that the CLLF scheduling algorithm is not
suitable for the city-wide MRMC wireless networks since
the acceptance ratio is much lower than the
“SlotChAssign” algorithms. And when the number of
flows increases to 30, none of the instances are
schedulable by CLLF algorithm, while by the
“SlotChAssign” algorithm, all of the instances are
schedulable. Hence, in the city-wide wireless network,
“SlotChAssign” algorithm is much better than CLLF
algorithm.
D. The Percentage of Flows Meeting Deadlines
The Percentage of flows Meeting Deadlines (PMD) is
the number of real-time flows meeting the deadlines
divides the number of all the real-time flows in the
network.
In the experiments, we first vary the real-time flow
number Nf from 50 to 500 with the increment of 50 and
set the starting slots of all the flows to be 0 and the
deadline of each flow is equal to its period. The results in
Fig. 7 demonstrate that, with SPF and LPF priority
assignments, there are noticeable gaps of PMD values
between the simulation and the analysis results as the
network gets congested (with period range 25~10). But
with RM priority scheduling, the differences are very
small. The gaps of PMD values exist because of the
difficulty to capture the worst-case situation in the
simulation. By just observing the PMD values under SPF,
LPF and RM priority scheduling and with flow periods
range in 25~10 and 27~11 , we can conclude that the channel
and slot allocation algorithm can achieve high PMDs
even when the flow number is 500.
Then, we set the deadline of each flow to be  times
its period, and vary  from 0.1 to 1 with an increment of
0.1. From the results in Fig. 8 and 9, we know that the
PMD values increase with the increasing of  under
both period ranges. With period range 25~10, the gaps of
the PMD values between the analysis and the simulation
hardly change with the increasing of  values. With
period range 27~11, both the simulation and analysis PMD
values increase with the increasing of  values. It
suggests that it’s easier to provide the real-time guarantee
for flows with larger deadlines in a network without
much congestion. The simulations also show that the slot
and channel assignments are very effective.
Although our aim is not to discuss which priority
strategy is optimal, all the results show that the RM
priority strategy achieves higher PMD than SPF and LPF.
In fact, RM scheduling is an optimal static scheduling for

2034

JOURNAL OF NETWORKS, VOL. 9, NO. 8, AUGUST 2014

1

1

1

0.8

0.8

0.8

SPF simulation,25~10

0.4

SPF analysis,25~10

LPF simulation,25~10

0.4

7~11

0.2

RM analysis,25~10
RM simulation,27~11

0.2

LPF simulation,2

SPF analysis,2
100

RM simulation,25~10
0.4

LPF analysis,2

7~11

0

0.6

5~10

SPF simulation,27~11

0.2

0.6

PMD

PMD

PMD

0.6

RM analysis,27~11

7~11

LPF analysis,2

200
300
400
flow number, =1

0

500

100

200
300
flow number, =1

(a) SPF

400

0

500

(b) LPF

100

200
300
flow number, =1

400

500

(c) RM

Figure 7. Varying flow numbers
1

1

0.8

0.8

1

0.8

0.2

simulation,Nf=50
analysis,Nf=50
simulation,Nf=100
analysis,Nf=100
simulation,Nf=150
analysis,Nf=150

0.4

0.2

0
0.2

0.4

0.6

0.8

LPF
LPF
LPF
LPF
LPF
LPF

1

0.2

0.4

PMD

PMD

PMD

SPF
SPF
SPF
SPF
SPF
SPF

0.4

0

0.6

0.6

0.6

simulation,Nf=50
analysis,Nf=50
simulation,Nf=100
analysis,Nf=100
simulation,Nf=150
analysis,Nf=150

0.6

0.8

RM
RM
RM
RM
RM
RM

0.4

0.2

0

1

0.2

,period range:25~10

,period range:25~10

(a) SPF

0.4

simulation,Nf=50
analysis,Nf=50
simulation,Nf=100
analysis,Nf=100
simulation,Nf=150
analysis,Nf=150

0.6

0.8

1

,period range:25~10

(b) LPF

(c) RM

Figure 8. Varying flow deadlines (Period: 25~10)
1

1

1

0.8

0.8

0.8

0.4

0.2

0

0.2

0.4

simulation,Nf=50
analysis,Nf=50
simulation,Nf=100
analysis,Nf=100
simulation,Nf=150
analysis,Nf=150

0.6

0.8

LPF
LPF
LPF
LPF
LPF
LPF

PMD

PMD

0.6

SPF
SPF
SPF
SPF
SPF
SPF

0.4

0.2

0

1

,period range:27~11

0.2

0.4

0.6

simulation,Nf=50
analysis,Nf=50
simulation,Nf=100
analysis,Nf=100
simulation,Nf=150
analysis,Nf=150

0.6

0.8

PMD

0.6

0.2

1

,period range:27~11

(a) SPF

(b) LPF

RM
RM
RM
RM
RM
RM

0.4

0

0.2

0.4

simulation,Nf=50
analysis,Nf=50
simulation,Nf=100
analysis,Nf=100
simulation,Nf=150
analysis,Nf=150

0.6

0.8

1

,period range:27~11

(c) RM

Figure 9. Varying flow deadlines (Period: 27~11)

the single processor preemptive systems, but not for the
multi-processor scheduling. We can conclude that RM
priority strategy outperforms SPF and LPF priority
strategies in the channel and slot allocation algorithms in
this study.
VII. CONCLUSIONS
In the paper, we have focused on the channel
assignment and slot allocation of city-wide multi-hop
MRMC WMN. A greedy algorithm has been proposed to
assign fixed channels to the access interfaces aiming to
minimize the maximum interference. The algorithm has
an approximation factor of (2-1/m) and a time complexity
of O(|Vr|). The joint channel and slot allocation algorithm,
to assign channels to the backhaul links and slots to all
the links, has a complexity of polynomial time and can
guarantee interference-free communications. We have
also analyzed the worst-case end-to-end delay of each
flow under the slot and channel assignment algorithms.
Simulation Results show that the slot and channel
assignment algorithms won’t cause unstable channel
switch in networks with reasonable congestion condition.
And the algorithms can achieve high PMD values. Given
© 2014 ACADEMY PUBLISHER

a set of flows, we can use the worst-case end-to-end delay
analysis to test the schedulability of flows.
ACKNOWLEDGMENT
This work was supported by the National High-tech
R&D Program of China (863 Program) (Grant No.
2012AA010904), and the Science and Technology Plan
Program in Sichuan province, China (Grant No.
2013GZ0016). And our thanks to the China Scholarship
Council (CSC) for the support for the “Joint Phd
Program”.
REFERENCES
[1] http://www.muniwireless.com/2009/03/28/muniwirelesslist-of-cities-with-wifi/. Accessed by 22 July, 2013.
[2] Ashish Raniwala, Kartik Gopalan, Tzi-cker Chiueh.
Centralized Channel Assignment and Routing Algorithms
for Multi-Channel Wireless Mesh Networks. Mobile
Computing and Communications Review. 2004, 8(2) pp.
50-55.
[3] Joshua Robinson and Edward W. Knightly. A Performance
Study of Deployment Factors in Wireless Mesh Networks.
IEEE INFOCOM 2007. 2007 pp. 2054-2062.

JOURNAL OF NETWORKS, VOL. 9, NO. 8, AUGUST 2014

[4] Cisco Wireless Mesh Access Points, Design and
Deployment Guide, Release 7.0(Last revised: January 10,
2013),
http://www.cisco.com/en/US/docs/wireless/technology/me
sh/7.0/design/guide/MeshAP_70.html. Accessed by 22
July, 2013.
[5] Li Y, Chen C S, Song Y Q, et al. Real-time QoS support in
wireless sensor networks: a survey//7th IFAC International
Conference on Fieldbuses & Networks in Industrial &
Embedded Systems-FeT'2007. 2007.
[6] B. Raman and K. Chebrolu, “Design and Evaluation of a
new MAC Protocol for Long-Distance 802.11 Mesh
Networks,” in 11th Annual International Conference on
Mobile Computing and Networking paper(MOBICOM),
Aug/Sep 2005.
[7] Gabale V, Chiplunkar A, Raman B, et al. Delay-Check:
Scheduling Voice Over Multi-hop Multi-channel Wireless
Mesh Network//3rd International Conference on
Communication Systems and Networks (COMSNETS),
Bangalore, India. 2011.
[8] Jianping Song, Song Han, Aloysius K Mok, Deji Chen,
Mike Lucas, and Mark Nixon. Wirelesshart: Applying
wireless technology in real-time in-dustrial process control.
In Real-Time and Embedded Technology and Applications Symposium, 2008. RTAS'08. IEEE, pages 377386. IEEE, 2008.
[9] Mark Nixon and TX Round Rock. A comparison of
wirelesshart and isa100. 11a. white paper, July, 2012.
[10] Tang Zhong; Cheng Mengjin; Zeng Peng ; Wang Hong.
Real-time communication in WIA-PA industrial wireless
networks. ICCSIT 2010. pp. 600 – 605.
[11] Murali Kodialam, Thyaga Nandagopal. Characterizing the
Capacity Region in Multi-Radio Multi-Channel Wireless
Mesh Networks. MobiCom’05. 2005 pp. 73-87.
[12] Mansoor Alicherry, Randeep Bhatia, Li (Erran) Li. Joint
Channel Assignment and Routing for Throughput
Optimization in Multi-radio Wireless Mesh Networks.
MobiCom’05. 2005 pp. 58-62.
[13] Joint Optimal Scheduling and Routing for Maximum
Network Throughput.
[14] Hamed M. K. Alazemi1, A. Das, R. Vijaykumar, S. Roy.
Fixed channel assignment algorithm for multi-radio multichannel MESH networks. Wireless Communications And
Mobile Computing. 2008, 8 pp. 811-828.
[15] Di Wu,Shih-Hsien Yang, Lichun Bao, Chi Harold Liu.
Joint multi-radio multi-channel assignment, scheduling,
and routing in wireless mesh networks. Wireless Networks.
2014, 20(1) pp. 11-24.
[16] Vijay Gabale, Bhaskaran Raman, Partha Dutta, and
Shivkumar Kalyanraman. A Classification Framework for
Scheduling Algorithms in Wireless Mesh Networks. IEEE
Communications Surveys & Tutorials. 2013, 15(1) pp.
199-222.
[17] Link Scheduling with End-to-end Delay Constraints in
Wireless Mesh Networks
[18] Praveen Jayachandran, Matthew Andrews. Minimizing
End-to-End Delay in Wireless Networks Using a
Coordinated EDF Schedule. IEEE INFOCOM 2010. 2010
pp. 1-9.
[19] Petar Djukic and Shahrokh Valaee. Link Scheduling for
Minimum Delay in Spatial Re-use TDMA. IEEE
INFOCOM 2007. 2007 pp. 28-36.
[20] Abusayeed Saifullah, You Xu, Chenyang Lu, and Yixin
Chen. Real-Time Scheduling for WirelessHART Networks.
In RTSS 2010. 2010 pp. 150-159.
[21] Abusayeed Saifullah, You Xu, Chenyang Lu, and Yixin
Chen. End-to-End Delay Analysis for Fixed Priority

© 2014 ACADEMY PUBLISHER

2035

[22]
[23]

[24]
[25]
[26]

Scheduling in WirelessHART Networks. In RTAS 2011.
2011 pp. 13-22.
A. Saifullah, P. Tiwari, B. Li, C. Lu, and Y. Chen.
Accounting for failures in delay analysis for wirelesshart
networks. In Tech. Rep., WUCSE-2012-16. 2012.
Subramanian, A. P., Gupta, H., Das, S. R., and Cao, J..
Minimum interference channel assignment in multiradio
wireless mesh networks. Mobile Computing, IEEE
Transactions on, 2008, 7(12) pp. 1459-1473.
Jane W.S. Liu. Real-time Systems. Prentice Hall PTR
Upper Saddle River, NJ, USA. 2000, pp.115-189.
http://quickfacts.census.gov/qfd/states/04000.html.
Accessed by 22 July, 2013.
http://www.maptechnica.com/us-city-boundarymap/city/Tempe/state/AZ/cityid/473000. Accessed by 22
July, 2013.

Jun Xu received her Bachelor’s degree
in the School of Electronic Information,
WuHan University, China, in 2009. She
is currently a Ph.D. candidate in the
School of Electronic Information,
WuHan University, China. And she is
also a visiting scholar in the School of
Computing, Informatics, and Decision
Systems Engineering, Arizona State
University, USA, from Sep. 2012 to Mar. 2014. Her research
interests are in the area of wireless mesh networks, industry
control networks, and real-time and reliability communications.
Yann-Hang Lee received his Ph.D.
degree in Computer, Information, and
Control Engineering from the University
of Michigan, Ann Arbor, MI, in 1984.
He is currently a professor in the School
of Computing, Informatics, and Decision
Systems Engineering, Arizona State
University, USA. Since 2008, he has
served as the Program Chair of the
Computer Science and Engineering Program in the School. Dr.
Lee's research interests are in the areas of real-time computing,
embedded systems, software engineering, distributed systems,
and performance evaluation.
Chengcheng Guo received his Ph.D.
degree in the School of Electronic and
Information, WuHan University, China.
He received his Bachelor and Master
degree in the Computer School of
WuHan University, China. He is
currently a professor and a Ph.D.
supervisor in the School of Electronic
and Information, WuHan University,
China. His research interests are Internet and Communication
technology, wireless mesh networks, industry control networks,
and real-time and reliability communications.
Jianfeng Yang received his Bachelor’s
degree in Electronic Information in 1998,
Master's degree in Signal and
Information System in 2003 and Ph.D.
degree
in
Communication
and
Information System in 2009, all from the
school of Electronic Information,
WuHan University, China. He is

2036

Currently an Associate Professor of Wuhan University, his
research interests are in the areas of Embedded System,
Wireless Mesh Networks, Parallel Computing and Real-time

© 2014 ACADEMY PUBLISHER

JOURNAL OF NETWORKS, VOL. 9, NO. 8, AUGUST 2014

and Reliability Communications. Also, he severed as the
committee member of China Computer Federation Technical
Committee of Embedded System from 2010.

Automatic Parallelization of Simulink Models for
Multi-core Architectures
Cumhur Erkan Tuncali, Georgios Fainekos, Yann-Hang Lee
School of Computing, Informatics and Decision Systems
Arizona State University
Tempe, AZ, USA
{etuncali, fainekos, yhlee}@asu.edu
Abstract— This paper addresses the problem of parallelizing
existing single-rate Simulink models for embedded control applications on multi-core architectures considering communication
cost between blocks on different CPU cores. Utilizing the block
diagram of the Simulink model, we derive the dependency graph
between the different blocks. In order to solve the scheduling
problem, we describe a Mixed Integer Linear Programming
(MILP) formulation for optimally mapping the Simulink blocks
to different CPU cores. Since the number of variables and
constraints for MILP solver grows exponentially when model
size increases, solving this problem in a reasonable time becomes
harder. For addressing this issue, we introduce a set of techniques
for reducing the number of constraints in the MILP formulation.
By using the proposed techniques, the MILP solver finds solutions
that are closer to the optimal solution within a given time
bound. We study the scalability and efficiency of our consisting
approach with synthetic benchmarks of randomly generated directed acyclic graphs. We also use the Fault-Tolerant Fuel Control
System demo from Simulink and a Diesel engine controller from
Toyota as case studies for demonstrating applicability of our
approach to real world problems.
Keywords—Multiprocessing, embedded systems, optimization,
model based development, Simulink, task allocation.

I.

I NTRODUCTION

Model Based Development (MBD) has gained a lot of
traction in the industries that develop safety critical systems.
This is particularly true for industries that develop CyberPhysical Systems (CPS) where the software implements control algorithms for the physical system. Using MBD, system
developers and control engineers can design the control algorithms on high-fidelity models. Most importantly, they can
test and verify the system properties before having a prototype
of the system. The autocode generation facility of MBD tools
provides additional concrete benefit which helps in eliminating
programming errors.
However, currently, the autocode generation process of
commercial tools focuses on single-core systems. Namely, at
the model level, there is no automatic support for producing
code that runs on a multi-core system. This is problematic
since advanced control algorithms, e.g., Model Predictive
Control algorithms [1], are computationally demanding and
may not be executed within the limited computation budget of
a single-core embedded system. In this paper, we address this
problem at the model level. Namely, given a data flow diagram
This research was partly funded by the NSF awards CNS-1446730 and
IIP-1361926, and the NSF I/UCRC Center for Embedded Systems.

of an embedded control algorithm, the worst case execution
times of the blocks and a computation budget (deadline), can
we automatically partition the blocks onto the different cores
so that the real-time constraints are satisfied?
In particular, we focus on control models built in the
Simulink [2] MBD environment. Our goal is to produce a
framework where non-determinism in the control algorithm
is reduced or minimized to the extent possible. Especially
in safety-critical systems, scheduling in a predictable and
deterministic manner is highly important for verification and
satisfying the certification requirements that are mandated by
regulatory authorities. For example, multi-core architectures
are classified as highly complex in the 2011/6 final report
of European Aviation Safety Agency (EASA) [3] and in the
Certification Authorities Software Team position paper CAST32 Multi-core processors [4]. These classifications highlight
the difficulty of certifying safety-critical systems that are based
on multi-core architectures.
Our approach is based on keeping timing properties of
parallelized software as simple as possible. For this purpose,
we are aiming at having separate executables for each core
while Simulink blocks are allocated in each core and executed
in a predetermined order. In other words, we set the priorities
of each block inside each core.
The contributions of this paper are,
•

providing a practical solution to the Simulink model
parallelization problem,

•

improving available Mixed Integer Linear Program
(MILP) formulations in the literature for finding better
solutions within a fixed and practically feasible time
for industrial size models,

•

solving the multi-core mapping problem while considering the timing predictability of the parallelized
application for ease of verification and certification,
and

•

developing a toolbox for automating parallelization of
Simulink models to multi-core architectures.
II.

R ELATED W ORK

There is a large amount of research being done on the
optimization of scheduling multiple tasks on multi-core processors or multiple processors in the literature. In [5] Anderson

et al. propose a Pfair [6] based scheduling method for realtime scheduling on multi-core platforms where the system
has multiple tasks and task migration is allowed. For optimal
mapping of tasks to CPU cores, Yi et al. [7], Bender [8] and
Ostler et al. [9] discuss integer linear programming techniques
which constitute a base for our optimization formulation.
Cotton et al. discuss the use of mapping programs to multi
processors in [10]. Tendulkar et al. discuss the application
of SMT solvers in many-core scheduling for data parallel
applications in [11]. In [12], Feljan et al. propose heuristics
for finding a good solution for task allocation problems in a
short time instead of searching for an optimal solution.
There are studies focusing on parallelization of Simulink
models. In [13], Kumura et al. propose methods to flatten
Simulink models for parallelization without giving a detailed
description of the optimization formulation. In that work,
Simulink blocks are considered as tasks. To achieve thread
level parallelism in multi-core, Canedo et al. introduce the
concepts of strands for breaking the data dependencies in
the model. A strand is defined as a chain of blocks that are
driven by Mealy blocks [14]. The proposed method searches
for available strand split points in Simulink models and it is
heavily relying on strand characteristics in target models. In
[15], Cha et al. is focusing on automating code generation for
multi-core systems where the parallel blocks are grouped by
user-defined parallelization start and end S-functions into the
model.
There are studies on task parallelization as [9], [7], [8].
However, to apply the similar approaches, Simulink blocks
must be considered as tasks. Given that most realistic models
may consist of a significant number of blocks, either these
methods fail to find an optimal solution in a reasonable
amount of time or they rely on available loop level parallelism
or functional pipelining as described in [9]. Deng et al.
study model-based synthesis flow from Simulink models to
AUTOSAR runnables [16] and runnables to tasks on multicore architectures in [17]. The authors extend the Firing
Time Automation (FTA) [18] model to specify activations
and requested execution time at activation points. They define
modularity as a measure of number of generated runnables and
reusability as a measure of false dependencies introduced by
runnable generation. The authors use modularity, reusability
and schedulability metrics for evaluation of runnable generations. They also propose different heuristics and compare
their results with the results obtained by utilizing a simulated
annealing algorithm. Although this work is targeting a similar
problem to our target problem, they are providing experiment
results for systems with less than 50 blocks and they are not
considering inter-core communication and memory overhead.
Our work mainly differs from the other works in literature

III.

P ROBLEM D ESCRIPTION

We are addressing the problem of automatically parallelizing existing Simulink models for embedded control applications on multi-core architectures in an optimal way and in a
reasonable time.
We are focusing on single-rate, single-task embedded
control applications which are modeled in Simulink and in
which the execution order of blocks is determined only by
dependencies coming from connections between blocks. Our
target models cannot start execution of next iteration before
finishing the execution of the current iteration.
Our target platform is Qorivva MPC5675K-based evaluation board [19]. The processor is a dual-core 32-bit MCU
from Freescale targeting automotive applications. The µC/OSII from Micrium [20] is ported on the target and a library to
support Simulink code generation is devised for the platform
[21]. We handle inter-core data communications by utilizing
available shared memory and inter-core semaphores which
are used for synchronization between tasks across cores and
protecting global critical sections as described by Bulusu in
[21]. For the purpose of utilizing this approach in Simulink,
we model transmission and reception of data between different cores with two separate S-function blocks which implement inter-core transmission and reception using inter-core
semaphores and shared memory. We will refer to these Sfunction blocks as inter-core communication blocks.
A. Solution Overview
We approach the problem in five steps which are illustrated
in Fig. 1. First, creating a directed acyclic graph which
represents dependencies between blocks. Task-data graphs are
discussed in [9]. We use a similar approach using blocks
instead of tasks, worst case execution times of blocks instead
of amount of work associated with tasks and using size
of data communication between blocks. Here we will refer
to this kind of graphs as “block dependency graphs”. Our
second step in approaching the problem is finding an optimal
or near optimal mapping of blocks to different CPU cores
by formulating a Mixed-Integer Linear Program (MILP) and
solving the resulting optimization problem with off-the-shelf
MILP solvers. The third step is automatically updating the
original Simulink model by adding inter-core communication
blocks where necessary in accordance with the most optimal
solution. The next step is generating separate code for each
target core by automatically commenting out the blocks that
are not mapped to the core for which code is being generated.
Finally, we compile the generated code and deploy it on the
target platform.

by
IV.
1.

providing a complete flow for automatically parallelizing a single-rate Simulink model,

2.

incorporating the communication cost in the optimization problem,

3.

having total available shared memory constraints, and

4.

being able to handle large models with more than 100
blocks in a reasonably short time.

M ILP F ORMULATION

In this section we present our MILP formulation for the
parallelization problem. Our MILP formulation for optimal
solution is based on the formulations proposed by [7], [8]
and [9]. We introduce an extension to these formulations by
dividing the cost of communication to the transmission and
reception parts. In Subsection D, we describe our techniques
for reducing the number of constraints for allowing the MILP
solvers to find better solutions within a feasible time.

Step 1

Step 2

Step 3

Step 4

Step5

• Reading Model Block Information
• Creating Block Dependency Graph

is used in the program formulation to dominate other terms
allowing constraints to be ignored under certain conditions.

• Formulating and solving Mixed Integer Linear
Programming problem for finding optimal mapping

B. Variables

• Updating Simulink model for each core according
to the solution found by MILP solver

• Simulink code generation for each core

• Compilation and deployment onto target platform

Fig. 1. Steps of going from a single-core Simulink model to multi-core target

A. Notation and Constants
The number of CPU cores available at the target architecture is denoted by m. The set of CPU cores is defined as P =
{Pp : p ∈ [1, m]}. The number of nodes in the dependency
graph is denoted by n where each node corresponds to a block
in the flattened and merged Simulink model. Merging of blocks
is done on the flattened model as described in subsection D.
We describe the dependencies between blocks with the block
dependency graph. This is a directed acyclic graph G = (B,
E), where B = {B1 , B2 , , Bn } is the set of nodes and E is the
set of edges in G. Each node Bi corresponds to a Simulink
block with a worst case execution time wi and each edge Eik
represents a data dependency from block Bi to block Bk . The
set of leaf nodes in B, i.e., set of blocks which do not have
any output ports is denoted by L and the set of start blocks,
i.e., the set of blocks which do not have any input ports is
denoted by S. We use Z for the set of deleted connections from
the blocks that introduce delays (e.g., Unit Delay, Memory,
Integrator, etc) to successor blocks. These connections exist
in the original model, but they are deleted when forming the
directed acyclic graph for removing cycles from the model.
Such a connection is represented by Zik ∈ Z.
The size of the data transfer from block Bi to Bk in bytes
is defined as cik . When Bi and Bk are mapped on different
cores there will be a communication cost for transferring cik
bytes of data between the cores. The communication cost is
divided into transmission and receiving parts where tik denotes
the transmission part of the communication time for sending
cik bytes of data from block Bi to block Bk when they are
mapped on different cores and rik denotes the receiving part
of the communication time for sending cik bytes of data from
block Bi to block Bk when they are on different cores.
The maximum allowed execution time for one iteration of
the model on the target multi-core architecture is given by the
deadline. It is either taken as a user input or calculated as the
overall worst case execution time on a single-core architecture.
The size of a global semaphore structure in bytes is denoted
by sSize and the size of total available shared memory in
bytes is defined as totMem. Data alignment size in bytes
(word size) is denoted by aSize. A very large value (MAX)

bip : A Boolean variable indicating whether block Bi is
mapped to core Pp or not. It is defined for all Bi ∈ B and for
all Pp ∈ P . If Bi is mapped to core Pp , then bip takes value
1. If Bi is mapped to another core, then bip will takes value
0.
dik : A Boolean variable indicating whether block Bi
executes before or after Bk when both blocks are mapped to
same core. It is defined for all Bi , Bk ∈ B with i < k. If Bi
executes before Bk , then dik takes value 1 and if Bi executes
after Bk , then dik takes value 0.
si : The start time for the execution of block Bi . It is defined
for all Bi ∈ B. The lower bound for the variable si (best case
start time) is denoted by bsi . It is determined by the best case
completion time for all of the blocks from which there is a
path to Bi in G. In the best case, all of this workload before
Bi is distributed equally on all of
 The best case
P the cores.
start time of Bi is calculated as
k∈Ki wk /m where Ki =
{Bk : Bk ∈ B ∧ there exists a path f rom Bk to Bi in G}.
The upper bound for the variable si (worst case start time)
is denoted by wsi . It is determined by the best case completion time for all of the blocks to which there is a path
from Bi in G and the block Bi itself, subtracted from the
deadline. The worstPcase start
 time of Bi is calculated as
deadline − wi + k∈Yi wk /m where Yi = {Bk : Bk ∈
B ∧ there exists a path f rom Bi to Bk in G}. For all i, k
such that Bi , Bk ∈ B and for all p such that Pp ∈ P .
f : The completion time after executing all blocks. The
lower bound for variable f is 0 and the upper bound is the
deadline.
C. Objective Function and Constraints
The objective function for the optimization problem is minimizing f while the constraints for the optimization problem
are defined as follows:
1) Every block shall be assigned to a single core:
X
∀i : Bi ∈ B,
bip = 1

(1)

Pp ∈P

2) Delay introducing blocks and their first successor blocks
shall be assigned to the same core:
∀i, k : Zik ∈ Z and ∀p : Pp ∈ P, bip − bkp = 0

(2)

3) The finishing time of each leaf block shall be less than
or equal to the completion time for executing all blocks: This
constraint is serving for the purpose of being able to formulate
the objective function minimize(maxBi ∈L (si + wi ))) as
minimize(f ).
∀i : Bi ∈ L, si + wi ≤ f

(3)

4) If there is a dependency from block Bi to Bk , block Bk
shall not start execution until (i) Bi finishes execution and
transmission of its output data to its successor blocks that are
mapped on other cores (which we temporarily define as fi
below) and (ii) Bk finishes receiving all of its input data that
are sent by the blocks on other cores: Considering that Bi is
mapped to core Pp and Bk is mapped to core Pq where p can
be equal to q:
∀i, k : Bi , Bk ∈ B, Eik ∈ E, ∀p, q : Pp , Pq ∈ P,
X
fi ≤ sk −
[rlk (1 − blq )] + (2 − bip − bkq )M AX

(4)

Bl ∈B

where fi = si + wi +

P

Bl ∈B [til (1

− blp )].

5) Execution of independent blocks that are mapped to
same core cannot overlap: Considering Bi and Bk are mapped
to core Pp , we have two different constraints for this requirement.
∀i, k : i < k, Bi , Bk ∈ B, Eik ∈
/ E, ∀p : Pp ∈ P,
fi ≤ sk −

X

[rlk (1 − blp )] + (3 − bip − bkp − dik )M AX

Bl ∈B

(5)
fk ≤ si −

X

[rli (1 − blp )] + (2 − bip − bkp + dik )M AX

Bl ∈B

(6)
Where, fi = si + wi +

X

and fk = sk + wk +

XBl ∈B

[til (1 − blp )]

Bl ∈B

[tkl (1 − blp )]

Since M AX is a very large constant, (5) will be valid when
block Bi executes before Bk i.e., when dik = 1 and (6) will
be valid when block Bi executes after Bk i.e., when dik = 0.
6) Total memory needed for semaphores and communication buffers shall be less than or equal to total amount of
available shared memory:
∀i, k : Bi , Bk ∈ B, Eik ∈ E, ∀p : Pp ∈ P
"
#

l C m
X
ik
sSize+
·aSize ·|bip −bkp | < totM em
aSize
Bi ,Bk ∈B
(7)
D. Improving Solver Time
The number of variables and constraints in the MILP
formulation grows exponentially as the number of blocks in the
model increase. Consequently, the MILP solver starts failing
in finding optimal or near optimal solutions for the problem in
a reasonable time. In this section, we introduce our techniques
for addressing this issue.
We say two blocks are dependent to each other if there
exists a directed path between corresponding nodes in the DAG
representation of the model and we say that two blocks are
independent if there is no directed path between these nodes.

1) Partially ordering independent blocks: In order to reduce the execution time of a model by parallelization, the
model must preferably have a large number of blocks that
are independent to each other. If all blocks are dependent to
each other, then there can be no multi-core mapping that will
improve the execution time and, thus, the best solution will be
mapping all blocks to the same core.
Typically, in an industrial size model with a large number
of blocks, both the number of blocks that are independent to
each other and the number of blocks that are dependent to each
other becomes large. In this case, when we consider all possible combinations of execution orders (priorities) between these
independent blocks, the number of constraints introduced by
inequalities (5) and (6) becomes very large. As a consequence,
finding an optimal solution within a feasible time becomes
harder.
We address this problem by deciding the execution order
between certain independent blocks in advance. That is, before
formulating the optimization problem, we decide the values of
the dik variables for these block pairs. Since our execution
order decision is valid only when these blocks are mapped
onto the same core, this should not prevent these blocks to
be mapped on different cores and, hence, be executed in a
different order than what we specify.
Our partially ordering heuristic is based on comparing
the execution start time frames of independent blocks. The
execution start time frame of a block is defined as the time
frame between its best and worst case start time values.
The best and the worst case start time values of a block
Bi ∈ B are defined in the subsection IV-B as bsi and wsi
respectively. For all independent block pairs B
 i ∈ B and
Bk ∈ B, if (bs(i) ≤ bs(k))∧ (ws(i) < ws(k)) ∨ (bs(i) <
bs(k)) ∧ (ws(i) ≤ ws(k)) then we decide Bi to execute
before B
>
 k and set dik to 1. Else if (bs(i) ≥ bs(k))∧(ws(i)

ws(k)) ∨ (bs(i) > bs(k))∧(ws(i) ≥ ws(k)) then we decide
Bi to execute after Bk and set dik to 0.
2) Fully ordering independent blocks: Even though ordering independent blocks using the partially ordering heuristic
improves the performance, this is not enough for models with
very large number of blocks. For example we could not find
a feasible solution to models with more than 100 blocks with
this approach. For dealing with those large models we propose
deciding the execution order of all the independent blocks
when they are mapped on the same core. The logic in fully
ordering heuristic is based on comparing the midpoints of the
execution start time frames for these blocks. For independent
blocks Bi ∈ B and Bk ∈ B we decide Bi to be executed
before Bk if the average of bsi and wsi is smaller than the
average of bsk and wsk . With this approach, dik variables of
MILP formulation change to constant values. Our discussion
on the case when these blocks are mapped to different cores
in previous subsection is still valid.
3) Merging highly coupled blocks: In this heuristic we
merge blocks Bi and Bk when block Bk is the only block
connected to the output port(s) of block Bi and block Bi is
the only block connected to the input port(s) of block Bk . The
merging operation copies all incoming and outgoing edges of
Bk to Bi except the edge Eik . Then it updates wi with wi +wk
and finally deletes Bk .

4) Merging small blocks with large blocks: In this heuristic
we merge blocks Bi and Bk based on their ratio of execution
times. If block Bk is the only block connected to the output
port(s) of block Bi and the WCET of block Bi is very small
when compared to the WCET of block Bk , then block Bi is
merged into block Bk . If block Bi is the only block connected
to the output port(s) of block Bk and the WCET of block
Bk is very small when compared to the WCET of block
Bi , then block Bk is merged into block Bi . We find this
technique useful for reducing the number of blocks of concern
in a way that parallelization will be focused on blocks with
higher impact on execution time. The ratio between the worst
case execution times of the blocks for determining a merge
operation can be defined depending on how much reduction is
needed in the number of blocks.
The merging methods described above can be used for
decreasing the number of nodes in very large models where
the MILP solver can no more find a good solution. These two
techniques are also dependent on the structure of the model.
Although, in general, they assist in finding better solutions,
there can be cases where the number of nodes cannot be
reduced to an acceptable level.
V.

I MPLEMENTATION

In this section we describe the details of the implementation
of our tool in MATLAB.
Our tool accepts as an input a Simulink model that is
ready to compile as well as the desired depth of blocks
to be parallelized. It loads the model, reads specific block
information, e.g., block type, parents, etc., and all the relations
between blocks along with the width and size of the data on
the ports. For data types that are not built-in, the user input is
required to define the data size in bytes. Using this information
the model is flattened by taking blocks inside sub-systems
out of their parent blocks. The remaining blocks like input
and output ports of subsystems, emptied subsystem container
blocks and ‘Goto’ - ‘From’ pairs, which are converted to line
connections, are discarded from the set of blocks.
We represent all these dependencies in a directed graph
where a directed edge represents a data communication from
its source to its destination. Since determining Worst Case
Execution Times (WCET) is not in scope of this paper, we
assume that the WCET values for each of the blocks are
already determined. If there exists a cycle in the directed
graph, this means that there is a corresponding block in
the cycle which creates a data dependency from a previous
iteration of model execution. We will refer to these blocks
as delay introducing blocks. In these cases we break the
connection from delay introducing blocks to their successors
for transforming a directed graph to a directed acyclic graph.
Since the connection from delay introducing blocks to their
successor blocks are deleted, our MILP solution can never
introduce inter-core communication mechanism between these
blocks even if they are mapped on different cores. For dealing
with this issue we force the delay introducing blocks and their
successor blocks to be mapped on the same core in the MILP
formulation.
After all of the cycles are cleared, the blocks that are
originally inside subsystems up to the desired model depth are

merged together without introducing cycles between blocks.
An exception to this is a subsystem including a delay introducing block. In this case, the blocks inside such a subsystem
are not merged into a single block since this can cause a cycle
in the dependency graph. In such a subsystem, predecessor
blocks of a delay introducing block are only merged with other
predecessor blocks and successor blocks are only merged with
other successor blocks. In other words, a predecessor and a
successor of a delay introducing block are never merged. The
flow of the process up to this point is illustrated in the simple
model in Fig. 2. In the next step, the block dependency graph is
annotated with estimates of WCET. Fig. 4 gives an illustration
of a simple block dependency graph.
The block dependency graph and the number of CPU cores
on the target architecture are used in generating the MILP
formulation presented in Section IV. The MILP solver returns
the best solution found for mapping blocks to the available
CPU cores and the execution order between these blocks.
The solution from the MILP solver is used to add inter-core
communication blocks between the blocks which are mapped
on different CPU cores. The relevant outputs of a block which
are sending data to a block on a different core are connected
to inter-core data transmitting S-function blocks. Similarly,
corresponding inter-core data receiving S-function blocks for
each transmitter are connected to the relevant inputs of the
block which is receiving data on a different core. The intercore communication blocks are added by setting unique IDs
that set each pair of transmitting and receiving blocks to use a
dedicated inter-core semaphore and a dedicated shared memory
location.
An example of the transformation is given in Fig. 3. The
output of B1 is connected to the input of B2 in the original
model. This connection is then replaced by inter-core communication blocks. After adding all needed communication
blocks, we set the priority attributes of the blocks using the
execution start time values obtained from the optimization
solution.
As the last step, a copy of the model is created for every
CPU core. Each copy of the model corresponds to a CPU
core and the blocks which are mapped on other cores are
commented out. Code generated from each of these models
can be compiled to create separate executables for each core.
VI.

E XPERIMENTS

For studying the scalability and efficiency of our approach,
we utilize randomly generated directed acyclic graphs with
different number of nodes. We present results of these experiments in subsection VI-A and results of our case studies in subsections VI-B and VI-C. We use SCIP [22] from Achterberg
as MILP solver which is interfaced with MATLAB through
the Opti Toolbox [23] by Currie and Wilson. Experiments are
run on a 64-bit Windows 7 PC with Intel Xeon E5-2670 CPU
and 64 GB RAM.
A. Randomly Generated DAGs
For evaluating performance of our approach, we generate
DAGs in which the WCET, communication costs and connections between blocks are assigned randomly. Then we solve

Node 1

-1
Z

1
In1

Delay

Product

In2

8

1

Subtract

2

Node 5

Node 2

Node 6

4

8

4

Out1

Node 4

10

Node 9

8

Constant

1

Node 10

In1

8

8

Node 8

Node 3

Node 11

In1

1

In2

8

1

Out1

Add

Constant

32

8

Out1

Node 7

Subsystem

Node 12

2
32

In2

[Goto1]
5

Product

Constant1

Goto

Node 13

[Goto1]
From

Fig. 4.

2

3
In3

Divide

MERGED

1
In1

-1
Z

1

Add

1

Delay

Subtract

Constant

Product1

2

Out1

10

In2
Constant2

5

Product

Constant1

2

3
Divide

In3

Fig. 2.

Out2

Flattening models and merging blocks

the problem for a dual-core system with the basic MILP
formulation which is given in Section IV and with the partially
and fully ordering heuristics for deciding the execution order
of independent blocks. We set five hours (18,000 sec) as an
acceptable upper time limit for the solver run time. Here,
we present a comparison of the performance of these three
approaches in terms of the average speed-up achieved, the
average solver time and the ability to find a solution in the
given time limit. The speed-up is computed as the overall
single-core worst case execution time of the model divided
by the overall worst case execution time of the parallelized
model.
Given infinite solver time, the basic MILP formulation
is expected to find more optimal solutions than the other
approaches do for any problem size. However, when the solver
time is limited (5 hours in our experiments), it fails to find satisfactory solutions for large problems. Table I gives a comparison of the performance of the used approaches. Average speedup achieved by basic MILP formulation, partially and fully

In

Out

B1
In

Out

B1

In

Out

B2

Inter Core Sender : 2

Inter Core Sender System

Inter Core Receiver : 2

Inter Core Receiver System

Fig. 3.

Inter-core communication blocks

Block dependency graph for a simple model

Out2

In

Out
B2

ordering heuristics (respectively denoted as basic, partial and
full) and corresponding solver run-time values are presented in
the table for different problem sizes. We also present the ratio
of the solutions found over all the experiments. For a problem
size, the lines corresponding to the approaches which could
not return any solutions are discarded in the table. As it can
be seen from the results presented in Table I, as the number
of blocks in a model increases, any heuristic that (partially)
sets the execution order performs better both in terms of
solver run-time and optimality of solutions. According to our
observations, for finding an optimal mapping, the basic MILP
formulation performs best when there are less than 30 blocks.
The partially ordering heuristic performs best when there are
30 to 50 blocks. For more than 50 blocks in the model, the
fully ordering heuristic outperforms other approaches in terms
of the achieved speed-up and the ability to return a solution.
The basic MILP formulation fails to return any solution for
models with 70 or more blocks. The partially ordering heuristic
fails to return any solution for models with more than 110
blocks. Although this detail is not illustrated in Table I because
of averaging, according to our experimental results, the fully
ordering heuristic can occasionally achieve very low speedup values compared to the other approaches when there are
less than 20 blocks in the model. However, this issue is
not observed when there are large number of blocks. This
behavior is parallel to our expectations since optimization
can significantly reduce the effect of possible non-optimal
execution order decisions by trying large number of different
mapping of blocks to different cores.
In Fig. 5, we illustrate the comparison between the two
heuristics and the basic MILP formulation in terms of the
achieved speed-up over the number of nodes. The solid lines in
the plot represent how much average speed-up is achieved by
each approach. The dashed lines represent the corresponding
minimum and maximum speed-up for each approach. For very
small number of nodes, the basic MILP formulation is better
than the other approaches. However, when the number of nodes
increases, first, the partially ordering heuristic and, then, the
fully ordering heuristic perform best.
In Fig. 6, we illustrate the comparison between the two
heuristics and the basic MILP formulation in terms of the
average solver time over the number of nodes. Each line in
the graph represents the average solver time spent for each

# Nodes

C OMPARISON OF DIFFERENT APPROACHES

Average
speed-up
1.48
1.47
1.46
1.68
1.71
1.46
1.48
1.62
1.55
1.2
1.66
1.67
1.09
1.55
1.59
1.54
1.75
1.39
1.7
1.38
1.61
1.08
1.64
1.04
1.67
1.56
1.62
1.61

Approach
Basic
Partial
Full
Basic
Partial
Full
Basic
Partial
Full
Basic
Partial
Full
Basic
Partial
Full
Partial
Full
Partial
Full
Partial
Full
Partial
Full
Partial
Full
Full
Full
Full

10-15

30

40

50

60
70
80
90
100
110
130
150
170

Average
solver time
2
1
0.5
2620
1558
26
9256
2091
606
18000
12481
5174
18000
17400
11685
18000
18000
18000
18000
18000
18000
18000
18000
18000
18000
18000
18000
18000

18000

% found
Solutions
100%
100%
100%
100%
100%
100%
100%
100%
100%
100%
100%
100%
64%
100%
100%
100%
100%
100%
100%
60%
100%
50%
100%
30%
100%
100%
100%
100%

approach. As it is expected, due to the time limit given to the
solver, as the number of nodes increases, the solution times for
all approaches converge. However, the experiments on models
with less number of nodes suggests that the proposed heuristics
can improve the solver time. In the graph it can be observed
that the average solver time for proposed heuristics (as a
function of node count) is smaller than the basic formulation.
Combining the data in Fig. 5 and Fig. 6, we can see that the
fully ordering heuristic returns better solutions within shorter
solver run-time compared to the other approaches.
B. Case Study: Fault-Tolerant Fuel Control System

16000
14000
Solver Time (s)

TABLE I.

12000
10000
8000

Basic (Avg)

6000

Partial (Avg)

4000

Full (Avg)

2000

0
10 - 15

30

40

50

60

70

80

90

100

110

130

150

170

Number of Nodes

Fig. 6.

Comparison of solver time between different approaches

model has 1 input port, 1 output port and 53 blocks after
discarding the trivial blocks as described in Section V.
We performed parallelization on a completely flattened
graph. The obtained block dependency graph from this model
is presented in Fig. 7 where the blocks mapped to core 1 and
core 2 are illustrated as the nodes colored with red and blue,
respectively.
We achieved a speed-up value of 1.78 with the partially
ordering heuristic within 5 hours of solver time. A speed-up
value of 1.92 was achieved with the fully ordering heuristic.
The basic MILP solution could only achieve a speed-up value
1.19 because it was unable to find the optimum solution within
the given time limit of 5 hours. This result is parallel with the
outcomes of the experiments carried on randomly generated
DAGs.
C. Case Study: Toyota Diesel Engine Controller
We used the Diesel engine controller model from [1] as
a case study from industry. The original model contains both
controller and plant parts. The controller part of model has
1004 blocks when flattened as described in Section V, it has
7 inputs that are merged into a single input bus signal and 6
outputs that are merged into a single output bus signal. Since
the model has cycles inside the subsystems, our tool flattens
the model by searching all blocks inside subsystems, breaks

As a case study, we used the fuel rate control subsystem of
the Simulink Fault-Tolerant Fuel Control System demo. This

Node 1
24
24

Node 2
4

Node 32
4

4

2
Node 48

1.9

Node 31

Node 30
4
8

Node 28

Node 22

4

8

8

Node 40

1.8

Node 23

Node 27

8

Speed-up

Node 50

Node 26

Node 44

1.5

8

1.4

Node 51

Node 46

4

8

Node 43
8

1.3

4

4

Node 25

4

4

Node 18

8

1.1

Node 47

4

Node 34

8

Node 41

8

Node 35

8

30

40

Basic (Avg)
Full (Min)

Fig. 5.

50

60

Partial (Avg)
Basic (Max)

70
80
90
Number of Nodes

Full (Avg)
Partial (Max)

100

Basic (Min)
Full (Max)

110

130

150

Partial (Min)

Comparison of speed-up values between different approaches

170

Node 33

8 4

Node 37
4

10 - 15

Node 29
8

Node 36
4

1

Node 11

Node 4
4

Node 13

Node 19

Node 9

Node 6
4

8

4

Node 5

Node 12
1

8

Node 17
8

4

Node 45

8

4

4

Node 15

Node 52

Node 16

4

Node 14

4

Node 42

1.2

4

8

4

4

16

Node 10

8

Node 20

4

Node 49

4

1.6

Node 3
4

Node 24

16

16
8

8
8

8

1.7

Node 8
24

24

16

4

Node 21
4

4

Node 39

Node 7

4

Node 38

4

Node 53

Fig. 7. The block dependency graph and its partition onto two cores for the
fuel control system case study

the cycles as described in Section V and merges blocks inside
subsystems (when possible) without introducing new cycles.
For parallelizing this model we set the target model depth as
2. After merging deep blocks of each subsystem, the block
dependency graph is generated from the model with merged
blocks. The generated block dependency graph contains 153
nodes and a total of 184 connections between these nodes. Our
target platform for this case study is the dual-core architecture
from Freescale which is described in Section III. In our target
hardware setup we have a total of 3.8 KB shared memory
available.
For a model of this size, both the basic MILP formulation
and the partially ordering heuristic fail in finding a solution in
10 hours. However, by merging blocks of subsystems with
depth more than 2 and with our fully ordering heuristic,
our tool returned a solution to the given problem within
an average of 1.2 hours of solver time. Here the average
is taken over different sets of worst case execution time
assignments. The suggested multi-core mapping by the tool
achieves 1.44x speed-up on average. This result is parallel with
our expectations based on experiments carried on randomly
generated DAGs and illustrates applicability of our approach
to reasonably large problems in industry.
VII.

C ONCLUSION

In this paper we presented our approach for parallelizing
a single-rate Simulink model on a multi-core architecture. We
proposed a heuristic for partially deciding execution order of
independent blocks when they are mapped to the same core.
According to the experimental results with randomly generated
DAGs and our case study with the fuel system controller, this
proposed heuristic improves optimality of found solutions in
a reasonable time for a realistic size of models with around
50 to 60 blocks in our experimental environment. For models
with larger number of blocks, we proposed another heuristic
in which the execution order of all the independent blocks is
decided in advance. With this approach our tool could handle
models with larger than 150 blocks. We also presented this
heuristic together with block merging methods on a case study
from the industry where our tool reduced 1004 blocks to 153
nodes on the dependency graph by merging blocks deeper than
a specified value and solved the problem on this 153 nodes.
The results from the case study illustrate how our approach
can handle models which can contain more than 1000 blocks.

[4]

[5]

[6]

[7]

[8]

[9]

[10]

[11]

[12]

[13]

[14]

[15]

[16]
[17]

For the future work, we consider extending this work
by introducing heuristic methods for solving the optimization problem, studying multi-rate models and models with
blocks that have priority assignments. Furthermore, we plan
to incorporate worst case execution time (WCET) tools in our
framework.

[18]

R EFERENCES

[21]

M. Huang, H. Nakada, S. Polavarapu, R. Choroszucha, K. Butts, and
I. Kolmanovsky, “Towards combining nonlinear and predictive control
of diesel engines,” in American Control Conference (ACC), 2013.
IEEE, 2013, pp. 2846–2853.
[2] Simulink, version 8.5 (R2015a). Natick, Massachusetts: The MathWorks Inc., 2015.
[3] “EASA/2011/6 final report,” European Aviation Safety Agency, Tech.
Rep., 2012.

[19]
[20]

[1]

[22]
[23]

Certification Authorities Software Team, “Position paper CAST-32
multi-core processors,” Federal Aviation Administration, Tech. Rep.,
2014.
J. H. Anderson, J. M. Calandrino, and U. C. Devi, “Real-time scheduling on multicore platforms,” in Real-Time and Embedded Technology
and Applications Symposium, 2006. Proceedings of the 12th IEEE.
IEEE, 2006, pp. 179–190.
S. K. Baruah, N. K. Cohen, C. G. Plaxton, and D. A. Varvel,
“Proportionate progress: A notion of fairness in resource allocation,”
Algorithmica, vol. 15, no. 6, pp. 600–625, 1996.
Y. Yi, W. Han, X. Zhao, A. T. Erdogan, and T. Arslan, “An ILP formulation for task mapping and scheduling on multi-core architectures,” in
Design, Automation & Test in Europe Conference & Exhibition, 2009.
DATE’09. IEEE, 2009, pp. 33–38.
A. Bender, “Design of an optimal loosely coupled heterogeneous
multiprocessor system,” in European Design and Test Conference, 1996.
ED&TC 96. Proceedings. IEEE, 1996, pp. 275–281.
C. Ostler and K. S. Chatha, “An ILP formulation for system-level application mapping on network processor architectures,” in Proceedings
of the conference on Design, automation and test in Europe. EDA
Consortium, 2007, pp. 99–104.
S. Cotton, O. Maler, J. Legriel, and S. Saidi, “Multi-criteria optimization
for mapping programs to multi-processors,” in Industrial Embedded
Systems (SIES), 2011 6th IEEE International Symposium on. IEEE,
2011, pp. 9–17.
P. Tendulkar, P. Poplavko, I. Galanommatis, and O. Maler, “Many-core
scheduling of data parallel applications using SMT solvers,” in Digital
System Design (DSD), 2014 17th Euromicro Conference on. IEEE,
2014, pp. 615–622.
J. Feljan and J. Carlson, “Task allocation optimization for multicore
embedded systems,” in Software Engineering and Advanced Applications (SEAA), 2014 40th EUROMICRO Conference on. IEEE, 2014,
pp. 237–244.
T. Kumura, Y. Nakamura, N. Ishiura, Y. Takeuchi, and M. Imai, “Model
based parallelization from the simulink models and their sequential C
code,” in Proceedings of the 17th Workshop on Synthesis And System
Integration of Mixed Information Technologies (SASIMI 2012), 2012,
pp. 186–191.
A. Canedo, T. Yoshizawa, and H. Komatsu, “Automatic parallelization
of simulink applications,” in Proceedings of the 8th annual IEEE/ACM
international symposium on Code generation and optimization. ACM,
2010, pp. 151–159.
M. Cha, K. H. Kim, C. J. Lee, D. Ha, and B. S. Kim, “Deriving highperformance real-time multicore systems based on simulink applications,” in Dependable, Autonomic and Secure Computing (DASC), 2011
IEEE Ninth International Conference on. IEEE, 2011, pp. 267–274.
AUTOSAR. (2015) AUTOSAR specification. [Online]. Available:
http://www.autosar.org
P. Deng, F. Cremona, Q. Zhu, M. Di Natale, and H. Zeng, “A modelbased synthesis flow for automotive CPS,” in Proceedings of the
ACM/IEEE Sixth International Conference on Cyber-Physical Systems.
ACM, 2015, pp. 198–207.
R. Lublinerman and S. Tripakis, “Modular code generation from
triggered and timed block diagrams,” in Real-Time and Embedded
Technology and Applications Symposium, 2008. RTAS’08. IEEE. IEEE,
2008, pp. 147–158.
Freescale Semiconductor Inc. (2015) Qorivva MPC5675K. [Online].
Available: http://www.freescale.com/
Micrium
Inc.
(2015)
µC/OS-II.
[Online].
Available:
http://micrium.com/rtos/ucosii/
G. R. Bulusu, “Asymmetric multiprocessing real time operating system
on multicore platforms,” Ph.D. dissertation, Arizona State University,
2014.
T. Achterberg, “SCIP: solving constraint integer programs,” Mathematical Programming Computation, vol. 1, no. 1, pp. 1–41, 2009.
J. Currie and D. I. Wilson, “OPTI: lowering the barrier between open
source optimizers and the industrial MATLAB user,” Foundations of
computer-aided process operations, Savannah, Georgia, USA, pp. 8–
11, 2012.

Replay Debugging for Multi-threaded Embedded Software
Yann-Hang Lee1, Young Wn Song1, Rohit Girme1, Sagar Zaveri1, Yan Chen2
1

2

Computer Science and Engineering Department
Arizona State University, U.S.A.

The School of Information Science and Technology,
Xiamen University, P.R. China

{yhlee, ywsong, rgirme, szaveri}@asu.edu, yanchen@xmu.edu.cn
Abstract

It may be claimed that the existing debugging tools, such
as GDB [2], TotalView [3], and IDB [4], can be used to
debug multi-threaded programs. Although the tools include
the functions of setting up breakpoint, single stepping and
monitoring at thread level, they don‟t ensure the
reproduction of execution behavior that is required in cyclic
debugging. For instance, when a thread reaches a breakpoint
while other threads are running, we may choose the “stop the
world/all threads run” process control model in Intel‟s IDB
or the "non-stop" mode in the release of GDB 7.1 in which
only a subset of threads are suspended. However, there is no
support to coordinate the arrivals of external events or the
execution progress of running threads. It is the developers‟
responsibility to feed external signals synchronously with
thread execution during the debugging process. This may
become extremely troublesome, if not intractable, when
dealing with real-time control signals, video/audio streaming
data, and internet packets.
Due to the impediment of using cyclic debugging for
multi-threaded embedded software, engineers are forced to
look into the trace data from a failed run and to understand
thread dependency. To pinpoint any specific problems, they
manually create execution sequences to mimic the behavior
of the failed run. This tactic is certainly problematical and
time consuming. To deal with the inherent complexity of
debugging such applications, approaches have been
developed and published in last few years [5]. Out of those
mechanisms, one of the most well known approaches is
deterministic replay or trace based replay [8], [9], [12]-[28].
It is based on recording information that can be played again
in a deterministic way during replay phase. However so far
none of them have talked about the capabilities and
functionalities that replay phase can provide to users.
In this paper, a Replay Debugger based on Lamport
clocks [6] is designed for multi-threaded embedded software.
The debugger considers a multithreaded execution as a
partially ordered sequence of interactions and reproduces the
sequence in cyclic debugging process. Hence, the developers
can inspect the execution steps of each thread as well as the
interactions among threads and external world. They will not
be distracted by context switches or the physical time
instants at which events and interactions occur. As a
consequence, the debugging process can be focused in any
single thread while the execution of other threads is
coordinated precisely to reproduce identical software
behavior.
The Replay Debugger is implemented as a record/replay
framework layered between multithreaded applications and
operating system kernel. The design criteria is to minimize
any instrumentation overhead (i.e., probe effect) in record

The non-deterministic behavior of multi-threaded embedded
software makes cyclic debugging difficult. Even with the same
input data, consecutive runs may result in different executions and
reproducing the same bug is itself a challenge. Despite the fact
that several approaches have been proposed for deterministic
replay, none of them attends to the capabilities and functionalities
that replay can comprise for better debugging. This paper
introduces a practical replay mechanism for multi-threaded
embedded software. The Replay Debugger, based on Lamport
clock, offers a user controlled debugging environment in which the
program execution follows the identical partially ordered
happened-before dependency among threads and IO events as that
of the recorded run. With the order of thread synchronizations
assured, users can focus their debugging effort in the program
behavior of any threads while having a comprehension of threadlevel concurrency. Using a set of benchmark programs, experiment
results of a prototyped implementation show that, in average, the
software based approach incurs a small probe effect of 3.3% in its
record stage.
Keywords – embedded system, replay debugging, multithread, partial order, Lamport clock.

1 Introduction
Debugging has been an imperative step to ensure the
correctness of software programs. However, one of the
survey data, in a 2002 NIST report [1], suggests that an
average bug found in post-product release takes 15.3 hours
to fix. This is quite costly in terms of engineer‟s time as
well as the impact to the software products and users.
Often the very first step in debugging is to reproduce
the failed execution. In the cyclic debugging process,
breakpoints are set in the program and the application is rerun. Thus, the cause of the failure can be observed. For
sequential programs the process is pretty straightforward –
as long as each execution is deterministic and repeatable,
the cause of the observed failure can be reproduced and
identified. On the contrary, it is challenging to debug
embedded software which is often structured with multiple
threads to process concurrent IO events and application
tasks. There are significant interactions between these
threads as they need to pass control signals, application
status, intermediate computation results, and to share
resources. This reproduction of the identical execution
behavior under this software structure and with thread
interaction becomes extremely difficult since
 Different ordering of thread interactions and timing
dependency may introduce non-determinism in the
execution of concurrent threads, and
 The system interacts with, and is dependent of, an
external real-time context.

1

stage and to maximize observability and analyzability in
replay stage. The record framework is a wrapper for IO and
IPC (inter-process communication) library calls. During
recording, the framework intercepts any IPC and device IO
function calls and traces the returned code, data, and error
code into a log before returning to the application. Then, the
happened-before relationships among the execution of
multiple threads and IO events can be constructed. During
replaying, the concurrent execution can be repeated and
thread execution can be controlled individually following the
partial order of thread interactions. Moreover, to make the
Replay Debugger more practical, the proposed replay
mechanism has been incorporated into GDB with an
extended debugging interface for thread control and
observation. It provides a GUI to user that shows the threads
and various events that happened during the record phase.
The users are allowed to use all GDB commands such as
single stepping, breakpoints, tracepoints, and status views.
They can also step over the individual thread interaction
events and control the execution order of concurrent threads.
There are three contributions of our Replay Debugger:
1. Both IPC and IO events are considered during the
record and replay.
2. It provides two different debugging modes, named
Replay Minima and Replay Maxima, for users to
control over a debugging process with the
deterministic order of execution.
3. It is practical and has been incorporated into GDB
with an extended debugging interface for thread
control and observation.
The rest of the paper is organized as follows. In Section 2,
a concise survey of related works is described. Section 3
presents the design of our Replay Debugger for multithreaded embedded software. The implementation details are
explained in Section 4. In Section 5, performance evaluation
is presented, followed by a conclusion and the future work.

On software based replay mechanisms, Carver et al.
provided an approach to reproduce the rendezvous of Ada
tasks [13]. The approach can only replay concurrent program
execution events like rendezvous, but not real-time specific
events like scheduled preemptions, asynchronous interrupts
or mutual exclusion operations [14]. LeBlanc and MellorCrummey proposed a method for recording information
during run-time and using this information to reproduce the
exact behavior of the execution off-line [15]. This method,
called Instant Replay, allows cyclic debugging techniques to
be used for non-deterministic systems. It is targeted at coarse
grained operations and traces all these operations. It does not
use any technique to either reduce the size of the trace files
or limit the perturbation introduced. Replay mechanism for
message passing systems is discussed in ROLT [16]. For
shared memory computers, Netzer [17] introduced an
optimization technique based on vector clocks. As the order
of all memory accesses is traced, both synchronization and
data races will be replayed. However, no implementation
was ever proposed (of course, the overhead would be huge if
all memory accesses are traced). A more elaborate discussion
on debugging parallel programs can be found in [18].
Montesinos et al. introduced a software-hardware
interface for deterministic replay of multiprocessors [19].
This approach, called Capo, separates the responsibilities of
the hardware and software components of the replay system.
During the recording phase, the software logs all replay
sphere inputs into a Sphere Input Log, while the hardware
records the interleaving of the replay sphere threads into an
Interleaving Log. During the replay phase, the hardware
enforces the execution interleaving encoded in the
Interleaving Log, while the software provides the entries in
the Sphere Input Log back into the replay sphere and
squashes all outputs from the replay sphere. The approach
combines the performance of hardware-only schemes with
the flexibility of the software-only ones.
Park et al. [20] proposed a probabilistic replay via
execution sketching to help reproduce concurrency bugs on
multi-processors. This approach recorded only partial
execution information during the production run, and relied
on an intelligent replayer during diagnosis time to
systematically explore the unrecorded non-deterministic
space and reproduce the bug. With only partial information,
the approach has low recording overhead, but it may require
more than one coordinated replay run to reproduce a bug.
Moreover, as the approach exhaustively search the
unrecorded non-determinism space to find a bug-producing
combination, it might be difficult to reproduce an occurred
bug within an acceptable time limit.
The concept of deterministic replay was applied for
debugging real-time systems. In [21], a general-purpose
processor is dedicated to monitoring on each multiprocessor.
The monitor can observe the target processors via shared
memory. The target systems software is instrumented with
monitoring routine, by means of modifying system service
calls and interrupt service routines. Their basic assumption
about having a distributed system consisting of
multiprocessor nodes makes their approach less general.
A similar technique of deterministic replay to debug
distributed real-time systems is discussed in [22]. The
method starts by identifying and recording significant events,

2 Related Works
With respect to related work in the field of replay
debugging of concurrent and real-time systems, the main
issue with record/replay approaches is what information
should be traced during the initial record phase? On the one
hand, enough information about the execution has to be
generated for a faithful re-execution of the program. On the
other hand, the amount of information traced and the
computational overhead should be limited as much as
possible in time and in space in order to avoid any probe
effect [7].
On general topic of replay, some researches have been
relying on special hardware to reproduce interrupts and task
switches [8][9]. The software instruction counter approach
[10] records the backward branches and has been used to
identify the exact location at which an event of interest (e.g.
interrupt) occurs. The approach was adopted for log-based
recovery to force the replay of asynchronous events at the
same execution points [11]. In addition, Hill et al. present
their Flight Data Recorder (FDR) [12] as an intrusive
hardware that facilitates debugging of data races in
multiprocessor systems.

2

together with the time at which they occur, in the execution
of the sequential program. To reproduce the run-time
behavior during debugging, all inputs and outputs are
replaced with recorded values. All transfers of control,
accesses to critical regions, releases of higher priority tasks,
and preemptions by interrupts are dictated by the recorded
timestamps. A similar replay debugging using Time
machines is discussed in [23]. As the authors claim, it is the
first method for deterministic replay of single tasking and
multitasking real-time systems using standard off-the shelf
debuggers and real-time operating systems and without
instruction counters support for replay thus making it
compiler and RTOS independent.
Probably the most similar approach to the replay
mechanism of the proposed debugger is RecPlay [24][25]
which replays the synchronization operations, while
detecting data races. It is also based on Lamport‟s happensbefore relation [6]. By checking the ordering of all events
and monitoring all memory accesses, data races can be
detected for one particular program execution. Some prior
work based on Lamport clock, i.e., ordering based replay
method for shared memory programs is discussed in [26].
This mechanism lists the advantages of using Lamport clock
to generate partial ordering. It produces small trace files and
is less intrusive. Moreover, the method allows one to use a
simple compression scheme [27] which can further reduce
the trace files. Different from the RecPlay, our Replay
Debugger focuses on debugging techniques for multithreaded embedded software based on the record/replay
framework. It is integrated with GDB and supplies
debugging functionalities for users to control over a
debugging process with the deterministic order of execution.
Another approach based on Lamport clock is taken by a
race detector Eraser [28]. It goes slightly beyond the works
based on the happened-before relation. Eraser checks that a
locking discipline is used to access shared variables: for each
variable it keeps a list of locks that were hold while
accessing the variable. Each time a variable is accessed, the
list attached to the variable is intersected with the list of
locks currently held and the intersection is attached to the
variable. If this list becomes empty, the locking discipline is
violated, meaning that a data race occurred. The most
important problem with Eraser is that its practical
applicability is limited in that it can only process mutex
synchronization operations and in that the tool fails when
other synchronization primitives are built on top of these
lock operations.
The record/replay approaches have become acceptable for
debugging in practice. Recently, VMware Workstation 6.5
includes the feature of replay debugging for C/C++
developers using Microsoft Visual Studio. The tool records
program execution via VMWare‟s virtualization layer and
the replay is guaranteed to have instruction-by-instruction
identical behavior. It also includes a feature of simulating
reverse execution of the program, making it easier to pin
point the origin of a bug.
GDB record patch, available at [29], enables a reversible
debugging function in GDB. By disassembling the
instruction that will be executed, the record patch saves the
memory and register content before they are modified. The

recorded information is then used to restore the processor
state during reverse execution.
In conclusion, although there are many related works on
the record/replay of concurrent and real-time systems, there
still has no practical and convenient mechanism or tool for
users to debug the multi-threaded embedded software. In
this paper, we propose a practical Replay Debugger which
offers a user controlled debugging environment. With the
Replay Debugger, users can focus their debugging effort in
the program behavior of any threads while having a
comprehension of thread-level concurrency.

3 Design of the Replay Debugger
To facilitate execution replay, we will need to record the
execution paths and input events of the original run. Hence,
instrumentation must take place in a record phase and must
have a minimal probe effect. To replay single threaded
programs, we will need to supply identical input values read
from device drivers and system calls (e.g. timer function).
The execution can then be reproduced given the sequential
nature of the programs. On the other hand, for multithreaded programs, threads can interact with each other by
invoking inter-process communication operations. To
reproduce the execution, these invocations must take place
in the identical order as the original run. In this paper, we
assume these interaction and input events are invoked via
system calls, including inter-process communication
primitives and device drivers. Shared resources are
protected by synchronization primitives such as locks or
semaphores and there is no data race condition. If a shared
resource is not properly protected and causes a race
condition during record phase, deterministic replaying the
program will reveal the cause of errors.
3.1 Definitions
As mentioned above, for a reproducible replay, we need
to record thread interaction and input read events (or simply
“thread interaction events”) during the original thread
execution. In the following, several related definitions are
given.
Definition 1. A thread interaction event is a 4-tuple:
E = <type, Tid, Oid, t>
(1)
where type is an event type of E, e.g., read(), sem_wait(),
sem_post(), etc.; Tid is the thread invoking event E; Oid,
identifies the synchronization and communication object
which E operates on, e.g., semaphore, message queue,
mutex lock, IO file, etc.; and t is the Lamport clock
timestamp [6] of E.
Definition 2. The thread interaction events operated on the
same synchronization and communication object are
collected into an event set. The set is called object event set
and is denoted as OESid for object Oid.
Definition 3. The thread interaction events in the same
thread are collected into an event set. The set is called
thread event set and is denoted as TESid for thread Tid.
We use the happened-before relation [6] to maintain a
partial order between thread interaction events. The
happened-before relation between events a and b is denoted
by “  ”, i.e., if a happened before b, it is represented as

3

a  b. The Lamport clock [6] is used to identify events in
reconstructing a partial order graph in the replay phase. A
Lamport clock is maintained for each Oid or Tid and is equal
to the Lamport clock of the most recent event happened on
Oid or invoked by Tid. LC(a) is a function that returns a
Lamport clock timestamp taking a parameter as Ei, Oid or Tid.
In addition to the happened-before relation we define an
immediate happened-before relation, denoted by “  ”, for
two successive thread interaction events as follows:
Definition 4. For two thread interaction events Eb and Ea,
there is an immediate happened-before relation Eb  Ea, if:
1) Eb, Ea  Setid where Setid is OESid or TESid, and
2)  Ei  Setid (i ≠ b) that satisfies
LC(Ea) – LC(Ei) > LC(Ea) – LC(Eb) > 0

1: CalculateE()
2: For each event Ei =<type, Tid, Oid, t> in V,
3:
Find Ei1 such that Ei1  TEStid and Ei1  Ei
4:
Find Ei2 such that Ei2  OESoid and Ei2  Ei
5:
L = L  (Ei1, Ei)
6:
L = L  (Ei2, Ei)
Algorithm of Edge Calculation

Figure 1

1: int shared = 1;
2: void thread2(int *arg){
3:
sem_wait(&sem);
4:
shared = shared + (*arg);
5:
sem_post(&sem);
6: }
7: void thread3(int *arg){
8:
sem_wait(&sem);
9:
shared = shared * (*arg);
10: sem_post(&sem);
11: }
12: int main(void){
13:
pthread_t t2, t3;
14:
char buf[10];
15:
int arg;
16:
17:
sem_init(&sem, 0, 0);
18:
read(0, buf, 10);
19:
arg = atoi(buf);
20:
21:
pthread_create(&t2, NULL, (void *)thread2, &arg);
22:
pthread_create(&t3, NULL, (void *)thread3, &arg);
23:
sem_post(&sem);
24:
25:
pthread_join(t2, NULL);
26:
pthread_join(t3, NULL);
27:
return 0;
28: }

(2)

According to the expression (2), Eb is the last event
before Ea in the Setid.
In the subsequence discussion, a timestamp for a thread
interaction event is denoted as an instant of Lamport clock
when the thread interaction event occurs and an event is
denoted as a thread interaction event.
3.2 Record
In the record phase of Replay Debugger, the happenedbefore relations between events are captured by chains of
immediate happened-before relations. The program
execution is reproduced by following the execution orders
of events represented in the immediate happened-before
relations during the replay.
A wrapper function is provided for each event type, and
recoding of an event Ei is invoked a thread Tid and operates
on an object Oid, the computation of their timestampts
follows the Lamport clock algorithm [6] as
LC(Ei) = max(LC(Ttid), LC(Ooid)) + 1
LC(Ttid) = LC(Ei)
(3)
LC(Ooid) = LC(Ei)
Then, the 4-tuple <type, Tid, Oid, t>, as the unique
representation of Ei, is saved into a log file. If Ei is an input
read event, the input data is saved too.
Note that LC(Ooid) is initialized to 0 when the object is
created, e.g., open() for files, sem_open() for semaphores,
and mq_open() for message queues, in Linux. Also, when a
new thread is created, we add a thread creation event Ec as
the first event to the new thread and the timestamp of the
new thread is initialized to LC(Ec). If Ei is a thread exit
event of a joinable thread, Ei is added as the last event in Ttid
and LC(Tid) is kept until the join action is invoked.
After the recording stage, a partial ordering of events is
constructed using the 4-tuple event information collected.
The partial ordering is represented by a graph G = (V, L),
where V is a set of events recorded, L is a set of edges, and
each l=(E1, E2)  L denotes the immediate happened-before
relation between E1 and E2. The algorithm to calculate L is
shown in Figure 1. Note that the steps 3 and 4 can be done
easily once the events are sorted according to their
timestamps.
Figure 2 shows an example program, and the program
has few possible execution paths. Figure 3 shows one
possible program execution with a partial order graph
calculated from timestamps.

Figure 2
main()

LC: 1
LC: 2
LC: 3
LC: 4
LC: 5

RD1
TC
TC

TCX

SP1

TCX
SW1
SP1

LC: 7

EX

LC: 8

SW1
SP1

JN

LC: 10

LC: 11

thread3

SI1

LC: 6

LC: 9

thread2

An example program

JN

Figure 3

EX

LC: Lamport Clock timestamp
RD: read
SI : semaphore init
TC: thread creating
TCX: thread created
SP: semaphore post
SW: semaphore wait
JN: thread join
EX: thread exit
* Subscript represents identity of
corresponding event

A partial order graph for one possible execution path

3.3 Replay
The deterministic replay is achieved by following an
order represented in the partial order graph G. To execute an
event, we should execute all events that are happened before
it. This is done by following edges of the partial order graph
backward and executing all the events happened before the
target event.
For an event Ei  TEStid, let ProceedToEvent(Ei) be a
schedule algorithm to run the thread Tid from its current

4

program counter to the completion of an event Ei. The
algorithm can be realized by recursive calls as depicted in
Figure 4, where the Lamport clock for threads and events
are managed following the same steps in Eq. (3).

implemented by differentiating event statements that we
record from other normal statements. The corresponding
replay algorithm of the current debugging mode is
performed at the event statements. The ProceedToEvent
algorithm, shown in Figure 4, implements the Repaly
Minima mode. Other program statements that we do not
record are preceded just as normal debugging statements
without applying the algorithm.
The external IO data saved during record phase is also
replayed. This guarantees that the program execution during
replaying receives the same input data as in the recording
stage. Note that in user‟s point of view all the details about
deterministic replay are hided and he/she can concentrate on
debugging without worrying about switching threads and
feeding IO data.

1: ProceedToEvent(Ei)
2: // assume that Ei  TEStid, i.e. to be invoked by Tid
3:
4: while LC(Tid)<LC(Ei) do
5:
run thread Tid until the next event Ek
6:
find the event Ej where Ej  Ek
7:
and EiTEStid
8:
ProceedToEvent(Ej)
9:
execute Ek
10: return
Figure 4

Algorithm of event execution

4 Implemenation

Given a partially ordered happened-before graph, there
are many different execution orders. In addition to user
controlled execution ordering, Replay Debugger provides
two debugging modes: Replay Minima and Replay Maxima.
In Replay Minima mode, the events of the selected thread
will proceed first. The events of other threads will be carried
on only if they are happened before the executing event of
the selected thread. On the other hand, for Replay Maxima,
the events of all other thread can proceed first until there is
happened-before dependency upon the current execution of
the selected thread. In this case, the events of the selected
thread will be advanced in order. The two replay modes are
useful to illustrate the dependency between threads. Let‟s
assume a breakpoint BRi is set along a thread Ti ‟s execution
path. When the thread execution is suspended after reaching
the breakpoint under the replay modes:
1) Replay Minima: Only a limited subset of events in all
other threads is executed. This subset represents the
minimal amount of execution that must be done for Ti to
reach BRi. Any causes that may result in an incorrect
state status at BRi would have been triggered by the
execution of the events in the subset.
2) Replay Maxima: A maximal set of events has been
carried out as thread Ti is suspended at BRi. This set
represents how far the impact of Ti‟s current execution
can stretch to if there is no further execution in Ti. So, if
thread Ti‟s execution is bug-free up to BRi and no bugs in
other threads, then the execution of this maximal set of
events should be error-free.
Thread 1

Thread 2

Thread 3

Thread 1

Thread 2

E4
LC: 1

LC: 2
LC: 3

LC: 6

Record/Replay
Library

IPC

E1
E7

E5

E6

LC: 3

Replay Minima at E1:
E4, E3, E2, E1 are executed
in order

Figure 5

Record

Log
Files

Linux Kernel

Replay
GDB

Scheduler

The architecture of Replay Debugger

4.1 Replay Scheduling Using GDB
Thread 1

Thread 3

Thread 2

Thread 3
E1

E3
E2

E2
E6

E1

LC: 4
LC: 5

E8

IO

Figure 6

LC: 2
E2

LC: 4
LC: 5

Multi-Threads Applications

E4
LC: 1

E3

The current implementation of Replay Debugger is built
on POSIX APIs in Linux environment. The architecture of
Replay Debugger is shown in Figure 6. It includes a library
of wrapper functions. The actual system calls are substituted
with the wrapper functions during compile time. In the
record phase, the wrapper functions log information needed
in building a partial order graph including Lamport clocks
and save external input contents into files. The wrapper
functions in the replay phase read external input contents
from the files saved in the record phase. In the current
implementation, it is the GDB thread module that performs
a deterministic replay using the partial order graph built in
the record phase. A selected subset of system calls are
implemented at this stage. Further extension will include
mmap device, signal handling, time function, etc.

E7

E3
E5

E4

E8

LC: 6

Replay Maxima at E1:
E4, E3, E2, E6, E5, E1 are executed
in order

Figure 7








Thread 1 stops at E4 (bkpt)
Switch to thread 2 & resume (GDB)
Thread 2 stops at E2
Switch to thread 3 & resume (GDB)
Thread 3 stops at E1 & Single step on E1
Switch to thread 2 & single step on E2 &
resume
 Thread 2 single steps on E3
 Switch to thread 1
 Single step on E4

A scheduling example using GDB

Replay mode example

The GDB thread module is modified to schedule thread
executions according recorded partial order. On start of a
GDB session, Linux is prevented from scheduling threads

Two different debugging scenarios by Replay Minima
and Replay Maxima are shown in Figure 5. The replay is

5

using the GDB command “set scheduler-locking on” [33]
and a breakpoint is set at each event for checking happenedbefore relations. The GDB command “thread thread-num”
[33] is used for switching to a thread numbered “threadnum”. When GDB stops at an event E1, threads are
scheduled as follows,
 If there is no immediate happened-before event before
E1, then single step on E1 and resume;
 Else find E2 such that E2  E1, switch to the thread
that E2 belongs to and resume the thread.
Figure 7 shows an example of thread scheduling.

clock. Thread Information displays a table similar to the
Replay Graph in the Eclipse Console. This table shows the
different events for corresponding threads which have not
yet been executed. To call the commands added to CDT, we
contributed menu/buttons to the workbench using
commands in Eclipse, which is shown in Figure 9. A
command in Eclipse is a declarative description of a
component. The behavior of a command is defined via
handlers. Where and how should the command be included
in the UI is defined using a location Uniform Resource
Identifier.

4.2 GUI Support
We have also implemented a graphical environment for
our Replay Debugger in the form of pluggable components
called Eclipse [30] plug-ins. The current implementation is
based on Eclipse for Linux. Eclipse plug-ins are components
that provide certain types of services within the context of
the Eclipse workbench. They add to the C/C++
Development Tools (CDT) that provides a fully functional
C and C++ Integrated Development Environment (IDE) for
the Eclipse platform.
Support for our Replay Debugger functionality in
Eclipse can be categorized into three parts.
4.2.1 Support for the Record
To enable recording, applications are built using the
record library so that we can record its behavior. Our plugin provides a new project type “RecordReplay” which is
shown in Figure 8. It has two new configurations “Record”
and “Replay”. The user can avail of the Record and Replay
services in GDB by selecting this project type.
Since our plug-in is a part of Eclipse, the new project
wizard includes the project-type and configuration
definitions from our manifest files along with other manifest
files to populate the list of choices presented to the user.
Contents of the build property page for a project are created
by examining the tool-chains, tools, option categories, and
options defined for the current configuration. Therefore, if
the user wants to use the Replay Debugger functionality, the
“RecordReplay” type of project should be created from the
list of project types. After selecting the project type, the user
then selects “Record” and “Replay” configurations for his
new project.
4.2.2 Support for the Replay

Figure 8

A new project type “RecordReplay” in the plug-in

Figure 9

Menu of replay commands

4.2.3 Support for displaying threads and events
Displaying the different threads and events of an
application being debugged gives the user a clear picture of
what is going on. We have used Swing and Abstract
Window Toolkit provided by Java for our purposes.

In Eclipse framework, the C/C++ Debugger Interface
(CDI) was created by Eclipse/CDT developers, so that CDT
can access external debuggers. Similarly, the Machine
Interface (MI) [30] was created by GDB developers, so that
external applications/software can access the GDB.
For our purpose, we added MI commands to the
GDB/MI interface which are used by our code in CDT to
call any particular replay commands. So far replay
commands of Start Replay, Thread Information, Set Minima
Mode, Set Maxima Mode and Set LC Breakpoint have been
added to the CDT. Start Replay initializes the replay module
in GDB; Set Minima Mode sets minima mode for execution
of events in GDB; similarly Set Maxima Mode configures
the maxima mode for event execution in GDB. Set LC
Breakpoint initiates a breakpoint at the thread clock entered
by user. Thus, debugging can follow the order of Lamport

Figure 10 Displaying threads and events

6

TABLE II Static data of benchmark programs

The memory log obtained after recording the application
contains the information about the threads and events of the
program. Information from this log is used to draw the
display.
Figure 10 shows an example of the diagram displaying
threads and events. The display provides us with
information like number of threads, events occurred along
with their types (thread created, semaphore/mutex taken or
released etc), thread clock at which the event occurred and
so on. An additional functionality provided is each event on
the display maps to a particular line in the application
program.

Program
multi_con_pro
multi_send_rev_1
multi_send_rev_2
sem_philosopher
sem_readerwriter
sem_sleepingbarber
read_send_rev
pthread_mutex_unlock

Thread
Number
500
80
200
5
200
101
4
60

Total Event
Number
200200
3242
9486
610
1002
1208
12011
3720

TABLE III The overhead during record for the benchmark programs

5 Performace Evaluation
Program

The performance of Replay Debugger is measured on a
Linux 2.6.28 system with Intel(R) Core 2 Duo CPU T6600
@ 2.20GHz and 3GB RAM. We use PAPI Library [31] to
measure the instruction counts of actually system calls and
the wrapper calls. The results are shown in TABLE I.
“mq_send” and “mq_receive” events are measured with a
message length of 40 bytes; “read” and “fread” events are
measured with reading 1024 bytes data. The instruction
counts of wrapper calls are larger than the original calls, as
each wrapper call should do additional work: update the
Lamport clock, write log in current memory buffer, and
switch buffer and signal the “dump thread” if current buffer
is full. Moreover, the wrapper calls for “read” and “fread”
events should also create log files to save the data read, so
the instruction counts of these two wrapper calls are larger
than other wrapper functions.

multi_con_pro
multi_send_rev_1
multi_send_rev_2
sem_philosopher
sem_readerwriter
sem_sleepingbarber
read_send_rev
pthread_mutex_unlock
Average

Original
18
16
81
82
40
42
949
94
575

Record
Runtime (s)
81.460
2.831
6.392
52.414
2.008
5.503
9.939
30.142

Overhead
8.405%
9.264%
3.414%
1.162%
0.050%
0.019%
4.008%
0.040%
3.295%

6 Conclusion
In this paper we have presented a practical approach to
replay-based debugger for multithreaded embedded
software. The proposed debugger uses the Lamport‟s
happened-before relation to establish a partial order between
thread interaction and IO events. The partial order is then
applied to guide the thread execution during debugging. The
enhanced replay functionalities like Replay Minima and
Replay Maxima on events provide simple control over a
debugging process with the deterministic order of execution.
Moreover, it has been successfully integrated with GDB,
and we have also wrapped our tools for the debugger in the
form of pluggable components in Eclipse.
As a further step, we would like to improve our record
and replay library to support most kinds of thread
interaction events, asynchronous transfer (e.g. signal and
watchdog timer), and mmap devices. In addition, we would
like to enhance the functions of our GUI and the Eclipse
plug-in based on user feedbacks. The current
implementation of the proposed tool is available at
http://rts.lab.asu.edu/ReplayDebugger.

TABLE I Instruction counts of actually system calls and the wrapper calls
System Call
sem_wait
sem_post
mq_send (40 bytes)
mq_receive (40 bytes)
pthread_mutex_lock
pthread_mutex_unlock
pthread_create
read (1024 bytes)
fread (1024 bytes)

Normal
Runtime (s)
75.144
2.591
6.181
51.812
2.007
5.502
9.556
30.130

Wrapper
514
515
537
535
574
780
1688
3940
4446

TABLE II and TABLE III give an idea of the overhead
caused by Replay Debugger during record phase for
programs from the LTP benchmark suite [32]. TABLE II
shows static information of benchmark programs, including
the number of threads and the total number of interaction
events during executions with the same input data. TABLE
III shows that the overhead during the record phase for the
benchmark programs ranges from 0.019% to 9.264% with
the average of 3.295%. The reason of the low overhead is
that Replay Debugger effectively succeeds in greatly
reducing the number of synchronization operations that has
to be stored on the log file. Apparently, optimizations can
also be done to further reduce the overhead of wrapper
function and logging operation. For instance, the code to
prevent preemption during the wrapper function can be
eliminated if the wrapper is moved to the kernel.

7 Acknowledgments
This work was supported partially by a grant from the
NSF Industry/University Cooperative Research Center
(I/UCRC) on Embedded Systems at Arizona State
University.

References
NIST report, “The economic impacts of inadequate
infrastructure for software testing,” Technical report, U.S.
Department of Commerce, May 2002.
[2] R. Stallman, R. Pesch, S. Shebs, et al., “Debugging with GDB
(ninth edition, for GDB version 7.1),” Free Software
Foundation, 2010.
[1]

7

[3]
[4]
[5]
[6]

[7]
[8]

[9]

[10]

[11]

[12]

[13]

[14]

[15]

[16]

[17]

TotalView Technologies, “TotalView graphic user interface
reference guide: version 8.8,” 2010.
R. M. Albrecht, “White Paper: Intel Debugger for Linux*,”
Intel Corporation, 2009.
J. Engblom, “Debugging real-time multiprocessor systems,”
Embedded Systems Conference, Silicon Valley, 2007.
L. Lamport, “Time, clocks, and the ordering of events in a
distributed system,” Communications of the ACM, Vol.21(7),
pp: 558-565, 1978.
J. Gait, “A probe effect in concurrent programs,” Software
Practice and Experience, Vol.16(3), pp: 225-233, 1986.
H.-Y. Chen, J.P.P Tsai, K-Y. Fang, and D. Bi, “A
noninterference monitoring and replay mechanism for realtime software testing and debugging,” IEEE Transactions on
Software Engineering, Vol.16(8), pp: 897-916, 1990.
D. R. Hower, P. Montesinos, L. Ceze, M. D. Hill, and J.
Torrellas, “Two hardware-based approaches for deterministic
multiprocessor replay,” Commnications of the ACM,
Vol.52(6), pp: 93-100, 2009.
J. Mellor-Crummey and T. LeBlanc, “A software instruction
counter,” Proceedings of the Third International Conference
on Architectural Support for Programming Languages and
Operating Systems, pp:78-86, Boston, Massachusetts, Unite
State, April, 1989.
H. Slye and E.N. Elnozahy, “Supporting nondeterministic
execution in fault-tolerant systems,” Proceeding of
International Symposium on Fault-Tolerant Computing, pp:
250 - 259, Sendai, Japan, June, 1996.
M. D. Hill, M. Xu, and R. Bodik. “A „flight data recorder‟ for
enabling full-system multiprocessor deterministic replay,”
Proceedings of the 30th Annual International Symposium on
Computer Architecture, pp: 122-133, San Diego, California,
Unite States, June, 2003.
R. H. Carver, K. C Tai, and E. E. Obaid, “Debugging
concurrent ada programs by deterministic execution,” IEEE
Transactions on Software Engineering. Vol.17(1), pp: 45-63,
1991.
F. Zambonelli and R. Netzer, “An efficient logging algorithm
for incremental replay of message-passing applications,”
Proceedings of the 13th International and 10th Symposium on
Parallel and Distributed Processing, pp: 392 - 398, San Juan,
Puerto Rico, Unite States, April, 1999.
T. J. LeBlanc and J. M. Mellor-Crummey, “Debugging
parallel programs with instant replay,” IEEE Transactions on
Computers, Vol.36(4), pp: 471-482, 1987.
D. Kranzlmller and M. Ronsse, “Rolt(mp) - replay of Lamport
timestamps for message passing systems,” Proceedings of the
Sixth Euromicro Workshop on Parallel and Distributed
Processing, pp: 87-93, Madrid, Spain, January, 1998.
R. H. Netzer, “Optimal tracing and replay for debugging
shared memory parallel programs,” Proceedings of the
ACM/ONR Workshop on Parallel and Distributed Debugging,
pp: 1-11, San Diego, California, Unite States, May, 1993.

[18] J. Huselius, “Debugging parallel systems: a state of the art

[19]

[20]

[21]

[22]

[23]

[24]

[25]

[26]

[27]

[28]

[29]
[30]
[31]
[32]
[33]

8

report,” In MRTC Technical Report 63, Mlardalen University,
Department of Computer Science and Engineering,
September, 2002.
P. Montesinos, M. Hicks, S. T. King, and J. Torrellas, “Capo:
a software-hardware interface for practical deterministic
multiprocessor replay”, Proceedings of Fourteenth
International Conference on Architectural Support for
Programming Languages and Operating Systems, pp: 73-84,
Washington, DC, Unite States, March, 2009.
S. Park, Y. Zhou, W. Xiong, Z. Yin, et al., “PRES:
probabilistic replay with execution sketching on
multiprocessors”, Proceedings of SOSP’09, pp: 177-191, Big
Sky, Montana, Unite States, October, 2009.
P. Dodd and C. V. Ravishankar, “Monitoring and debugging
distributed real-time programs,” Software Practice and
Experience, Vol.22(10), pp: 863-877, 1992.
H. Hansson and Henrik Thane, “Using deterministic replay
for debugging of distributed real-time systems,” Proceedings
of 12th Euromicro Conference on Real-Time Systems, pp:
265-272, Stockholm, Sweden, June, 2000.
J. Huselius, A. Pettersson, H. Thane, and D. Sundmark,
“Replay debugging of real-time systems using time
machines,” Proceedings of the 17th International Symposium
on Parallel and Distributed Processing, Nice, France, April,
2003.
K. D. Bosschere, M. Ronsse, and M. Christiaens, “Debugging
shared memory parallel programs using record/replay,” ACM
Future Generation Computer Systems, Vol.19(5), pp: 679687, 2003.
M. Ronsse, M. Christiaens, and K.D. Bosschere, “Cyclic
debugging using execution replay”, Proceedings of the
International Conference on Computational Science, pp: 851860, San Francisco, California, Unite States, May, 2001.
K. Audenaert, L. Levrouw, and J. Van Campenhout, “A new
trace and replay system for shared memory programs based
on Lamport clocks,” Proceedings of the Second Euromicro
Workshop on Parallel and Distributed Processing, pp: 471478, Malaga, Spain, January, 1994.
L. Levrouw, M. Ronsse, and K. Bastiaens, “Efficient coding
of execution-traces of parallel programs,” Proceedings of the
ProRISC / IEEE Benelux Workshop on Circuits, Systems and
Signal Processing, pp: 251-258, Utrecht, Holland, March,
1995.
G. Nelson, P. Sobalvarro, T. Anderson, S. Savage, and M.
Burrows, “Eraser: a dynamic data race detector for
multithreaded programs,” ACM Trans. Comput. Syst. Vol.
15(4), pp: 391-411, 1997.
GDB record patch, http://sourceforge.net/projects/record/.
Eclipse, http://www.eclipse.org/.
PAPI, http://icl.cs.utk.edu/papi/.
Linux Test Project, http://ltp.sourceforge.net/.
GDB Manual,
http://sourceware.org/gdb/current/onlinedocs/gdb/.

Coverage-Based Testing on Embedded Systems
X. Wu^, J. Jenny Li*, D. Weiss* and Y. Lee^
*Avaya Labs Research, 233 Mt. Airy Rd., Basking Ridge, NJ 07920
{jjli, weiss}@research.avayalabs.com
^Department of Computer Science and Engineering, Arizona State University,
699 South Mill Avenue Tempe, AZ 85281
{xwu22, yhlee}@asu.edu
Abstract
Program

One major issue of code coverage testing is the
overhead imposed by program instrumentation, which
inserts probes into the program to monitor its
execution. In real-time systems, the overhead may alter
the program execution behavior or impact its
performance due to its strict requirement on timing.
Coverage testing is even harder on embedded systems
because of their critical and limited memory and CPU
resources. This paper describes a case study of a
coverage-based testing method for embedded system
software focusing on minimizing instrumentation
overhead. We ported a code coverage-based test tool
to an in-house embedded system, IP phone. In our
initial experiments, we found that this tool didn’t affect
the behavior of the program under test.

1. Introduction
Code coverage testing technology attempts to
quantify progress of program testing. Some academic
study on small programs showed that software
reliability improves as testing coverage increases [1].
Even though large industrial trials and theoretical study
of correlation between coverage and defect detection
are still required, intuitively, low testing coverage is
not acceptable. No developer would deliver software
with only single digit testing coverage.
We developed an automatic coverage testing tool
suite to facilitate the usage of coverage testing.
eXVantage, short for eXtreme Visual-Aid Novel
Testing and Generation, which aims to help developer
to improve testing productivity. It also helps project
managers in keeping track of each developer’s
progress. Figure 1 shows the workflow of our coverage
testing tool suite.

Second International Workshop on Automation of Software Test (AST'07)
0-7695-2971-2/07 $20.00 © 2007

Developer

Coverage
report

Automatic
Code
Inspection

Y

Analysis
Issues?
N

Program
Instrumentation

Recompile

Program
Structure
Report

Execution
Trace

Testing
Results

Automatic
Run Tests

Existing tests

Figure 1. Workflow of Automatic Unit Testing
Framework
The tool suite includes four major components: 1)
automatic code inspector, 2) program instrumentation
module which also includes program structure
recovery function, 3) automatic test execution
platform, and 4) analysis module. Each component
corresponds to one tool in the accompanying tool suite.
The component shown in the dashed box, recompile,
may not be necessary if the bytecode or object code is
used in the program structure analysis and

instrumentation. It is not part of the tool suite
component because it uses the original program
compiler without any modification or using any new
tool.
The entire framework can be invoked by a script
including five steps: code inspection, instrumentation,
compiling, test execution and analysis. The input to the
script is the directory of the programs to be tested and
analyzed and the output is testing coverage reports.
The workflow works as follows:
1)
Each program must first go through an
automatic code inspector using our stylechecker tool. If issues or problems are
discovered by the checker, they are
highlighted on the program and should be
fixed by developers immediately.
2)
After the program is clean of static problems,
it must go through a program instrumenter
that adds hooks to the program for monitoring
its execution. In the meantime, the program
structure can be recovered from the program.
3)
After the program is instrumented, the
program can then be recompiled (if the
program is the source code) or directly sent to
test execution (if the program is bytecode or
object code). We use a pattern matching
approach to select part or all of the test cases
for execution.
4)
Testing results and program structures are
used by the analysis module to generate
various reports. One typical report is program
testing code coverage report, which helps user
keep track of testing progress. If some parts of
code are not covered, a priority report will be
generated to show which part of code is to be
tested next.
The reports generated by the framework are stored
in a data base. Additional tool is included to depict
historic trend on program changes and code coverage.
Such information is useful for project planning and
process control.
One important issue of coverage testing is the
overhead of monitoring program execution, in
particular, for real-time embedded systems. The probes
inserted to the original program at step 2) can cause
test execution overhead at step 3). The step 3) is the
only step that is platform dependent. Our work on
porting the tool suite to embedded systems focuses on
step 3) in the tool suite’s workflow.
The overhead comes from two sources: CPU
requirement for probe execution and memory
requirement for storing execution traces. The focus of
this paper is on the second issue. Details of the
problem and its solutions will be described later.

Second International Workshop on Automation of Software Test (AST'07)
0-7695-2971-2/07 $20.00 © 2007

The rest of the paper is organized as follows.
Section two defines in details the porting requirements
and memory challenges. Section three describes our
solution to the problem. Section four presents our
experimental results and section five concludes that
our solution enables embedded system code coverage
testing.

2. Problem description
The target system of our case study is an embedded
system, with MIPS architecture. Due to the real time
feature of the software, porting a code coverage testing
tool to such systems requires to minimize the overhead
caused by the code coverage instrumented probes and
monitored trace data storage and transmission.
The target system is based on the VxWorks Real
Time Operating System (RTOS). VxWorks is one of
the most successful real time operating systems, which
provides multitasking, intertask communication, and
hardware interrupts handling as the three most
important features.
VxWorks has very good performance in terms of
both timing and space. Its raw context switch time
could be as low as 4 micro seconds, and the VxWorks
image we used for this case study has a size of about 4
KB.
Another issue needs to be considered in porting to
an embedded system is the memory mechanism. In the
original version of eXVantage, shared memory is the
way for trace memory buffer maintenance. The
VxWorks image in the target system does not have the
shared memory component included. Furthermore,
VxWorks itself uses a flat memory model, which
means every task could access to every single valid
memory address without any boundary, including
concurrent tasks.
eXVantage was originally developed for Linux
platform, and each process has a pointer stored in the
shared memory pointing to another piece of memory
allocated before the program starts. In this process
specific memory, the header starts with 9 bytes,
including information like process ID, buffer size, and
key value (which will be used during instrumentation).
The rest of the memory is the place where trace data
are temporarily stored for later retrieval through
network connections.
Figure 2 illustrates this memory issue in a diagram.
It shows that each process has its own block of shared
memory. This block of shared memory doesn’t need to
be shared among the original processes. But it needs to
be shared with the new eXVantage process that
handles trace transmission. Since the trace data
memories of various original processes are not shared

among each other, there is no need for a memory
locking mechanism.

Figure 2. Share memory issue of trace data
The eXVantage VxWorks run time system is
composed of three parts: PC Host, the embedded
system, and Ethernet connections. Tornado, the
VxWorks Integrated Development Environment (IDE)
runs on the PC Host, The embedded software
application program runs on the embedded system. The
PC Host and the embedded system are connected via a
serial link. Both of them also have Ethernet
connections, as shown in Figure 3.

Figure 3. The hardware setting
The program that runs on the embedded system the telephone in this system - could be divided into
three layers. The first layer contains user-defined
application tasks APP1 – APPn, VxWorks system tasks,
with the addition of a task, XV_SERVER, created by
eXVantage for memory setting and trace data
transmission; the second layer is the VxWorks library;
and the third layer is the hardware support, which
mainly includes I/O Driver and Board Support Package
(BSP). The addition of this new XV_SERVER task is
part of our solutions that will be explained in the next
section.

Second International Workshop on Automation of Software Test (AST'07)
0-7695-2971-2/07 $20.00 © 2007

3. Solutions
Our solution to reducing overhead includes several
mechanisms. First we only record the coverage
information instead of the full execution trace, thus the
number of probes and their CPU time requirements are
reduced; second each time a probe is executed we store
data in memory rather than directly writing it into files.
By this way the CPU time requirement for writing file
is eliminated; third we use binary data format for the
trace information other than readable characters so that
the memory requirement is greatly reduced.
Taken VxWorks shared memory issue into account,
we pick up the API “sysMemTop()” which returns the
first byte of memory that is neither used by the system
nor by the user. The trace memory starts from this
point to obtain blocks of memory for each original
process. Since only count information of coverage
execution is recorded, no execution sequence is
required and the total size of the memory can be predetermined. Figure 4 shows the selection of this
memory beginning point. The other arrangement is
kept the same as in Figure 2.
eXVantage runtime solution is divided into two
layers from the software point of view. The upper layer
contains all application tasks plus XV_SERVER task;
the lower layer is composed of all related modules of
eXVantage that provide supports to the upper layer
1) Application Tasks
Application tasks are created by the users and are
instrumented by eXVantage before execution. Such
instrumentation is mainly supported by the
instrumenter module of eXVantage.
2) XV_SERVER Task
After instrumentation, original application tasks are
logically organized into various connected nodes.
When any of the probes on each node is executed, a
piece of information will be written into the
corresponding buffer created and maintained by the
XV_SERVER task.
XV_SERVER task’s execution includes the
following functions:
a) Setting up the three types of buffers supported
in eXVantage, coverage (bit), profiling
(counting) and tracing(ordered); and
b) Configuring network communication in order
to send traces to trace handler program
periodically or reply to its requests.
When the system boots up, XV_SERVER task is
assigned priority 100. Periodic transmission will be
carried out every 5 seconds to send out execution
trace information.

Figure 5. eXVantage VxWorks Run Time Solution
Figure 4. Solution to shared memory issue
Currently only bit buffer has been implemented,
counting and ordered buffer will be added in the near
future; and the ordered buffer will have to be dumped
based on their used buffer size, otherwise there would
be data loss.
Dynamic transmission will be carried out upon the
receiving of requests from a remote trace handler,
which is also going to be implemented later.
The following is a more detailed description of
eXVantage run time software components and its
workflow. Figure 5 shows the workflow of eXVantage
run time solution on embedded systems which includes
four components: APPn, XV_SERVER task, Trace
Handler, and Buffer.
The following is the detailed description of the four
components.
1) APPn
APPn is the original program under test. They were
instrumented with probes that write into the
memory buffer during the execution. Every APPn
task writes execution information directly into its
own buffer when a “logic” node gets executed.
2) Buffer
The buffer temporarily stores the program execution
information. There are three types of buffer
supported by eXVantage: Bit Buffer, Counting
Buffer, and Ordered Buffer.
Bit Buffer shows if the corresponding node is
executed or not; Counting Buffer records how many
times the node has been executed; and the Ordered
Buffer saves the node execution sequence of the
whole application program. For code coverage
testing, only the first one is needed. The latter two
kinds of information have some other usage such as
compiler optimization.

Second International Workshop on Automation of Software Test (AST'07)
0-7695-2971-2/07 $20.00 © 2007

3) XV_SERVER task
XV_SERVER sends bit and counting buffer data to
Trace Handler on a remote machine every 5
seconds; sends ordered buffer data to the same place
based on its dynamic in-use buffer size, in another
word, the size of data in the buffer. Both of the two
sending operations have to guarantee no
interference to the application tasks’ execution.
In the meantime, the XV_SERVER task also
handles requests sent from the Trace Handler and
responses to them with the trace data as requested
4) Trace Handler
Trace Handler resides on a remote machine. It sends
requests to XV_SERVER task for retrieving trace
data dynamically. It also receives trace data from
XV_SERVER task passively every certain period of
time. Received trace data will be written into trace
files for further analysis as shown in Figure 1.

4. Experiments
By using a timer with resolution of 1/3600, which is
0.278 milliseconds, we obtained the time cost on
several measurements as listed below (Table 1). These
measurements show the cost of instrumentation (probe
time) and XV_SERVER task (buffer initialization and
transmission).
Several other measurements have also been
proposed and the work is in progress:
1) When combining the above time cost values
with the eXVantage overhead on the regular
non-real-time system (<1%), we have
confidence that eXVantage on the embedded
system with VxWorks RTOS could also
achieve an overhead that is at least as low as
1%. However, some more experiments are
needed to prove this.

Measurement

Time cost

Standard
deviation

Trace buffer
initialization

619.80
nanoseconds

0.30
nanoseconds

Execution time
per trace data
send

788.07
nanoseconds

1.79
nanoseconds

Execution time
per probe

115.91
nanoseconds

1.10
nanoseconds

Table 1. The overhead of instrumentation
2) Tornado provides a CPU time measurement
tool as one of its components, which could
show the CPU time spent on each of the tasks
during a specific execution. We are going to
use this tool to measure the task’s execution
time of both un- instrumented and instrumented
code, and apply statistics theory to calculate its
overhead.
3) Another way to check if our instrumentation
affects the original program is to get the
execution sequence of the program’s both uninstrumented and instrumented version, and
compare with each other. This work could be
done by another Tornado tool “WindView”.
WindView is also an instrumentation tool
which instruments VxWorks API in order to
get the execution sequence.
Initial results on the existing test cases show that the
code coverage instrumentation didn’t alter the behavior
of the original program.

5. Related work
As a method of effectively checking the quality of a
software test, also quantitatively monitoring the
progress of testing, code coverage testing has been an
active research topic for more than 10 years. M.R.Lyu
et al 0 developed a coverage analysis tool named
ATAC (Automatic Test Analysis for C), which could
evaluate test set completeness based on data flow
coverage measures and allows programmers to create
new tests intended to improve coverage by examining
code not covered. But ATAC didn’t address embedded
system’s characteristics. W.E.Wong et al [2] proposed
another solution, TestingEmbedded, on top of Χ
Suds/ATAC that works for the embedded system
environment, but up to now it is restricted to
Symbian/OMAP platform only. Y.Y.Cho et al 0

Second International Workshop on Automation of Software Test (AST'07)
0-7695-2971-2/07 $20.00 © 2007

suggested a performance evaluation system for
embedded software that it consists of code analyzer,
testing agents, data analyzer, and report viewer; code
coverage analysis is performed by the code analyzer;
report is generated from data analyzer showing the
analysis results and displayed by the report viewer.
However, they didn’t discuss how much overhead
could be caused by this system in the original
embedded software. H. Thane in his doctoral thesis 0
also discussed about the code coverage technique in
the testing phase of distributed real-time system
development, defining the complete coverage for both
single node and the whole distributed system; but
similar to 0, he didn’t take the instrumentation
overhead into consideration, which is quite an
important issue for embedded systems’ timing
behavior. In our coverage based testing technique, we
overcome the drawbacks listed above by designing the
testing environment specifically for our embedded
system
platform,
minimizing
instrumentation
overhead, and applying basic performance analysis to
verify the test overhead on the original embedded
software.

6. Conclusion
We have successfully ported a code coverage
testing tool, eXVantage, from Linux platform to an
embedded system with VxWorks RTOS. The resulted
code coverage testing runtime solution passed basic
test cases we have at hand. Currently it had been
handed to our customer for field usage.
We have also carried out some basic measurements,
of which the experimental result is pretty good.
According to the measurements after our
implementation, our solutions together could provide
us with a good enough performance for embedded
system code coverage testing. When the required
resources become available, more detailed and
comprehensive measurements will be applied and
necessary changes could be made according to the new
results.
Several future works could be valuable to consider:
1) Addition of the two other kinds of buffers,
counting and ordered buffers, which will
require more strict server task and application
tasks synchronization to prevent data loss.
2) Applying eXVantage to other software systems.
We have implemented and tested our porting
on one embedded system. More trials of the
porting are being carried out.
3) Implementing a real time system logger. The
logger works at the field during program
execution to record necessary runtime

information. The information is stored in a
remote trace file. With this trace file, exactly
the same execution can be “replayed” as many
times as users need. This can be quite helpful
for debugging and remote diagnosis.
4) Product line development
a) Language Support
Currently we have already supported
JAVA and C/C++, when there is other
programming language come out in the
future, we could also try to include it in our
eXVantage language support.
b) Platform Support
Up to now eXVantage works well on both
Linux and VxWorks. Next step we may
port it to other platforms, so that
eXVantage can be applied to a much wider
domain of applications
Besides code coverage testing, the users can also
leverage the coverage data collected from the run-time
environment to help in debugging. It can also be used
to profile the execution count to find system
performance stress points. It can also locate bugs by
finding the common parts of the failed test cases and
subtracting the parts in successful test cases. Overall,
porting code coverage testing tool to embedded
systems makes the tool’s related features usable for

Second International Workshop on Automation of Software Test (AST'07)
0-7695-2971-2/07 $20.00 © 2007

embedded systems, thus helping improve the quality of
embedded software.

7. References
[1] Michael R. Lyu, J.R Horgan, Saul London, “A coverage
analysis tool for the effectiveness of software testing”, IEEE
Transactions on Reliability, Volume 43, Issue 4, Dec. 1994,
pp. 527-535
[2] W. Eric Wong, S. Rao, J. Linn, and J. Overturf,
"Coverage Testing Embedded Software on Symbian/OMAP"

in Proceedings of the 18th International Conference on
Software Engineering and Knowledge Engineering, San
Francisco, July 2006.
[3] Yong-Yoon Cho, Jong-Bae Moon, and Young-Chul Kim,
“A system for Performance Evaluation of Embedded
Software”, in Transactions on Engineering, Computing and
Technology V1, ISSN 1305-5313, December 2004
[4] Henrik Thane, “Monitoring, Testing and Debugging of
Distributed Real-Time Systems”, Doctoral Thesis, Royal
Institute of Technology, KTH, Mechatronics Laboratory,
TRITA-MMK 2000:16, Sweden, 2000
[5] Barry W. Boehm, Software Engineering Economics,

Prentice Hall Ptr, October 22, 1981

SPDA: A Security Protocol for Data Aggregation in
Large-scale Wireless Sensor Networks
Jin Wook Lee1 Yann-Hang Lee2 and Hasan Cam2
1

Networking Lab.,
Samsung Advanced Institute of Technology, P.O. 111, Suwon, Korea 440-600
2
Department of Computer Science and Engineering,
Arizona State University, Tempe, AZ 85287-8809, USA

Abstract. In this paper we propose a new key establishment protocol enabling
any data aggregation protocol to be operated securely. This is accomplished by a
bidirectional key distribution scheme based on Forward Key Setup and Backward
Key Setup developed by using synchronized broadcast and multi-level key concept. Our protocol, called SPDA(Security Protocol for Data Aggregation) is well
suited for any data aggregation algorithms and applications. Our analysis results
prove SPDA’s efficiency meaning that its communication cost is manageable.

1

Introduction

The issue of security in sensor networks has been addressed at various levels. In order to
prevent unauthorized entities from intercepting, decrypting, or hijacking data communications, data should be encrypted with either symmetric or asymmetric keys. The keys
must be protected and managed appropriately between the base station and all sensor
nodes and must satisfy several security and functional requirements. To support secured
data aggregation in sensor networks, there must be a key management scheme between
each sensor node and its correspondent data aggregation node. Thus, the collected data
are encrypted at each sensor node and then decrypted at the data aggregation node for
aggregation processing. This paper focuses on a simple key establishment for data aggregation in sensor networks. The objective is to construct an efficient key management
for data aggregation mechanism that can give confidentiality and integrity against malicious intruders. To eliminate the security loophole open to malicious intruders, we
present a mechanism to set up pair-wise symmetric keys for data aggregation operations. The core of a Security Protocol for Data Aggregation (SPDA) mechanism is the
bidirectional key setup scheme that stochastically makes a unique symmetric key between a sensor node and a correspondent aggregation node in the sensor network.

2

Related Works

So far the issue of security in sensor networks has been addressed at various levels. Secure data aggregation schemes have been introduced in recent literatures [1] [2] [3] [4] [5].
In the papers [3] [4], the authors attempted to make data aggregation secure by designing alternate data aggregation schemes such as pattern-based data aggregation and

reference-based data aggregation. They employed a kind of group key management
scheme, so a group header becomes an aggregator to perform the aggregation algorithm. However, such schemes also have their limitations in performing specific data
aggregation algorithms and specific applications. In another work [2], Przydatek et al.
recently proposed secure information aggregation. Their scheme is also dedicated to
specific applications. Lee et al. [1] proposed a key management for data aggregation
and pointed out that the disadvantage of their scheme was that more data needed to be
passed through the key setup message and stored at each node. Worse yet, compromising a node allows an adversary to understand all data communications originating from
the node’s neighboring nodes. A similar paper to ours is proposed by Hu et al. [5], but
they don’t discuss key management issues in detail. Unlike other works, we provide
a security platform that conforms to any data aggregation scheme and any application
scenarios.

3

SPDA PROTOCOL

Our aim is to construct a key establishment mechanism which enable a node to have
a unique symmetric key agreed with the correspondent aggregator. Our idea is to allow each node to compute a key with seeds originating from both the Forward Key
and Backward Key sent by aggregators. Our design goal is to devise a simple key establishment scheme well suited to any data aggregation algorithm by using pre-defined
aggregators. Only aggregators have a data aggregation algorithm to perform and have
more security information for key generation. Throughout the paper, we use several
terms to describe the protocol as defined in Table 1.
Table 1. Glossary
Glossary

Description

FKeySetup

A protocol packet. Only the base station can generate the initial FKeySetup
packet. All the nodes start the operation by receiving a FKeySetup packet
firstly.
A protocol packet. Only the aggregators can generate their own BKeySetup
packets. All the sensor nodes create their own BKeySetup packet to send data
messages to aggregators.
Forward Level Key. A key hint of the base station.
Backward Level Key. A key hint of the aggregator.
Public key of the base station. This public key is shared by all nodes.
Secret key, partially selected bits of PubK.
Private key that the base station and aggregators share.
Combination key. This key is generated by forward and backward key setups
and is used by aggregators and sensor nodes.
Hop Count. Logical hop count from the base station
Backward Hop Count. Logical hop count from an aggregator

BKeySetup

FLK
BLK
PubK
SPK
PrvK
CK
hc
bhc

3.1

Definition of Keys and functions

Our approach to key generating functions is motivated by the need to establish a symmetric key of each node as efficient in communication cost as possible. Our idea of
key generation introduces symmetry of key with two asymmetric keys. Combining two
different keys produces a high probability of having a virtually unique key in the network if each of the two different keys is not carelessly generated. One key is propagated
from the base station to sensor nodes. The other key is propagated from aggregators to
the base station. The keys are propagated by the relaying procedure of each node. The
relayed keys should not be easily guessed, so we suggest the use of a one-way hash
function as a key relaying function. Each sensor node receives two different keys from
two different neighboring nodes by relaying keys in opposite directions.
We define and use three types of crypto keys and two types of seed keys as below:
– Crypto key
• Public Key (PubK)
• Private Key (PrvK) of the base station and aggregator
• Sub-Public Key (SPK)
• Combination Key (CK)
– Seed key
• Forward Level Key (FLK)
• Backward Level Key (BLK)
All nodes maintain the public key (PubK) of the base station. This public key offers
data confidentiality of a broadcast message during announcement at the base station.
SPK is partially selected bits of PubK (i.e. most significant 64 bits of PubK) and is used
as a secret key among all nodes. The public key processing is costly; sensor node’s
public key processing is performed only during key establishment. Once a sensor node
finishes the key establishment, PubK and SPK are no longer used for data confidentiality so this public key mechanism does not significantly affect the network performance.
We choose 64 bits for a symmetric key and 512 bits for a public key, so the size of all
symmetric keys in this work is 64 bits. Combination Key (CK) is a core cryptographic
key in this protocol; it is computed in each node to use for sensed data message confidentiality. Each CK for each sensor node has a high probability of being unique in a
network. Additionally, aggregators have the private key (PrvK) of the base station that
is pre-installed, so aggregators are said to have the same security power as the base station and can also use this to establish secure channels to other aggregators. During the
key establishment, performing Forward and Backward Key Setup enables each sensor
node to compute its CK by itself and to use it for data encryption afterwards. Forward
Level Key (FLK) and Backward Level Key (BLK) are combined to generate the CK.
The first FLK is created only by the base station, whereas the generating BLK is started
by all aggregators. We are going to explain how to generate and propagate FLK and
BLK in the next subsection.
We suggest applying two one-way hash functions and a combining function to perform the protocol. All nodes have two key-generating functions and one key-combining
function as below:
– One-way function

• Forward Key Generating Function (FFunc)
• Backward Key Generating Function (BFunc)
– Combining function
• Combination Function (CFunc)
3.2

Key Establishment

Our key establishment is done in two stages, Forward Key Setup and Backward Key
Setup. The base station starts a key establishment phase by broadcasting a message enclosing The Seed of Forward Level Key(F LK l ), where l is the level of the base station.
We now address the two key setup stages in greater detail, using Figure 1 to help explain
the Forward and Backward Key Setup protocol.

Fig. 1. Network Illustration of Forward and Backward Key Setup

Forward Key Setup Key distribution for wireless sensor networks should not ignore
scalability. In order to achieve scalability, a flooding-based broadcast is commonly used
for distributing keys. The base station starts the Forward Key Setup stage by sending a F KeySetup packet containing the commitment code of the Forward Level Key
(F LK n , where n is a big enough number and the last element of the key chain. For notation, we use superscript for hop count or level) to all adjacent nodes, which prevents
an adversary from compromising the base station. In the Forwarding Key Setup stage,
the concept of ‘level’ is important, meaning that those who have the same hop count
from the base station locate in the same level and have the same FLK. The format of
the first FKeySetup packet is shown below:
broadcast
Base Station −−−−−−→ Neighbors : F KeySetup0
EP rvK { F LK n | n | G }
| ESP K { F LK 0 | hc0 }
| M ACSP K ( F LK n | n | G )
, where G is a gap value of hop-count assigned by the base station.
On receiving the FKeySetup packet, a node starts performing the protocol. The node
naively decrypts the packet with PubK and computes the MAC of the decrypted content
with SPK. The node then verifies the packet with two procedures, MAC comparison and
Commitment key validation. Firstly, the integrity and authenticity of the packet could be
validated by comparison of the computed MAC with the received MAC; however, MAC
comparison, with the syntax verification method is not enough, since a malicious node
that steals SPK is able to forge the whole packet by changing random bits of the content
and computing corresponding MAC with SPK. In such a case, MAC comparison could
not detect this abnormality. Commitment key validation solves the problem. Provided
that hc starts with 0 and increases by one, the node applies FFunc with FLK, ‘n −
G · hc’ times to verify F LK n . Applying a gap value, G, between two consecutive hc
prevents an attacker having no G from generating the next FLK properly.
If the verification fails, the node stops the protocol; otherwise, the node becomes a
level-1 node and prepares its own FKeySetup packet based on application of FFunc for
the next Forward Level Key (F LK 1 = F F uncG (F LK 0 ), meaning G times FFunc application). After an appropriate time, all level-1 nodes transmit their FKeySetup packet
as below. Nodes that receive these kinds of packets for the first time become a part of
the network as level-2 nodes.
broadcast
Level-1 nodes −−−−−−→ Neighbors : F KeySetup1
RELAY[EP rvK { F LK n | n | G }]
| ESP K { F LK 1 | hc1 }
| RELAY[M ACSP K ( F LK n | n | G )]
, where hc1 is hc0 + 1.
As a result of completion of the Forward Key Setup stage, all nodes including aggregators have F LK l , where l is a relative hop distance from the base station.
Backward Key Setup Aggregators may start the Backward Key Setup stage right
after they receive a FKeySetup while sensor nodes wait to receive a Backward Key

Setup packet to start the Backward Key Setup stage. Backward Key Setup is performed
with unicast communication started by aggregators. Once each aggregator receives a
FKeySetup packet, it is ready to start the Backward Key Setup stage by generating the
BKeySetup packet as follows (For notation, we use subscript for node ID):
unicast
Aggregator i −−−−−→ Its parent node j : BKeySetupi
EP ubK { Seedi | bhc0i | i | hci }
| ESP K { BLKi0 | bhc0i }
| M ACSP K ( Seedi | bhc0i | i | hci )
An aggregator, i, chooses a random number as the Seed of the Backward Key, then
applies a one-way function once to make its BLKi0 . We suggest choosing an arbitrary
number for bhc instead of zero. This technique prevents an adversary who compromises
SPK from knowing the relative location of an aggregator by calculating bhc. The Seed
and bhc are encrypted with PubK and reported to an upper-level aggregator which will
use them to compute proper CKs of intermediate sensor nodes in the routing path between two aggregators. Assume that each node chooses one of its neighboring nodes as
the next hop node to reach the base station. An aggregator unicasts its BKeySetup to its
next hop node (say, node j).
On receiving this BKeySetup packet, sensor node j decrypts ESP K {BLKi0 | bhc0i }
with SPK and makes its own BKeySetupj , which contains updated bhc1j (= bhc0i + 1)
and BLKj1 (the output of BF unc(BLKi0 )) as below:
unicast

Sensor Node j −−−−−→ Its parent node k : BKeySetupj
RELAY[EP ubK { Seedi | bhc0i | i | hci }]
| ESP K { BLKj1 | bhc1j }
| RELAY[M ACSP K ( Seedi | bhc0i | i | hci )]
Only aggregators having PrvK are capable of understanding all the contents of
the BKeySetup. Whenever an aggregator receives a BKeySetup packet, it decrypts the
whole packet with PrvK and then keeps the source ID, bhc0i , and Seedi of the source
aggregator in the aggregator list.
3.3

Combination Key Generating

The main purpose of Forward and Backward Key Setup is to allow a node to be able
to have a secret key agreed with an aggregator for data message confidentiality, which
is Combination Key (CK). Once a node generates CK, it could encode a data message
to give to an aggregator lightly and securely. Now we explain how each kind of node
computes its own CK. There are two sorts of nodes in the protocol, such as aggregator
and sensor nodes. Each kind of node does apply differently CK generation functions.
A sensor node which receives FLK and BLK uses them as inputs of CFunc as CKj =
CF unc(F LKj , BLKj ). This key combination is quite unique in the network and also
can be computed with seeds of each key. All aggregators share PrvK and are able to
compute other aggregator’s CK generated with PrvK and their ID as below for aggregator i: CKi = CF unc(P rvK, i). The reason that an aggregator incorporates its ID into
CK is to differentiate CK of other aggregators. Having ID of other aggregators grants
an aggregator the CK generation of others.

3.4

Key Usage

Two different kinds of nodes build the first data message with three different message
fields usage, as below:
unicast

Node k −−−−−→ Aggregator i :
ECKk {M essage} | ESP K {k|0|k} | M ACCKk (M essage) for aggregators
or ECKk {M essage} | ESP K {k|bhck |aggrIDk } |
M ACCKk (M essage|bhck |aggrIDk ) for sensor nodes
, where aggrIDk is the source aggregator ID of bhc that node k receives.
When an aggregator receives the first data message from a node, it searches the
source ID, say k, in the aggregator list. If k is found in the aggregator list, the ID is used
to compute CKk . The aggregator figures out the generation of proper CK for sensor
nodes.
With aggrID, an aggregator can calculate the length of the path between two aggregators and also locate the message source node with bhc. Therefore, the aggregator
is able to find the relative distance of the message source node from aggregators and
calculate its FLK and BLK, as illustrated in Figure 2.

Fig. 2. Finding relative distance of a source node.

As mentioned above, CK calculation is done only once. After that, all nodes send
smaller sized messages as below:
unicast
Node k −−−−−→ Aggregator :
ECKk { M essage } | k | M ACCKk (M essage)

4

ANALYSIS

In this section, we show the performance of SPDA through approximate numerical
analysis and provide simulation results to validate our numerical analysis. We choose
communication cost as the performance metric because the number of communications
is critical in wireless sensor network. Communication cost is defined as the additional
number of communications per non-aggregator for Backward key setup. We first derive

the basic calculation of the number of nodes. During the Forward key setup, the treestructured network is formed level by level. The percentage of aggregators (denoted by
α) on each level is expected to be the same under uniform distribution. Let N (l) denote
the number of nodes on level l. Thus, the number of aggregators, Nagg (l), is α · N (l).
With a maximum level number of a network, h, the total number of nodes in a whole
network, TN , and the total number of aggregators, Tagg , can be calculated respectively
as:
TN =

h
X

N (i), Tagg = α ·

i=1

h
X

N (i).

i=1

Now we calculate the probability of connections between aggregators. To find the number of non-aggregators between two aggregators along a routing path, we note that they
are all located on different levels. In other words, all routing connections between two
nodes are between two levels. For example, the percentage of aggregators on a level
that have aggregators as their parent nodes is α times the number of aggregators on the
level (α · Nagg (l) where l is the level number).
Not all nodes receive a BLK necessary to generate CK. In the tree-structured network we target, some nodes are not destined to receive BLK because they are not located in the routing path of any aggregators. We define a tail node as a node that has no
chance of receiving any BLK from any aggregators and does not contribute the network
security. The number of tail nodes contributes directly to the protocol performance as
a whole so the number of tail nodes should be found. The number of tail nodes can be
derived like this. Intuitively, all non-aggregators on the maximum level l are tail nodes
because there is no possibility of getting them to receive a Backward key. In the same
way, some non-aggregators on level l − 1 are going to be tail nodes since they are not
selected as parents by aggregators on level l. In the same way, the number of tail nodes
on a level with h, maximum level number, can be derived as:
Ntail (l) = (1 − α)h−l (1 − α)N (l).
The total number of tail nodes in network is
Ttail =

h
X
i=1

Ntail (i) =

h
X

(1 − α)h−i (1 − α)N (i).

i=1

We suggest three scenarios for aggregator’s BLK forwarding to reduce the number of
tail nodes in a network. We categorize three different scenarios for the behavior of an
aggregator: (1) An aggregator unicasts a BLK to only its parent node(Say, Scenario I).
(2) An aggregator unicasts a BLK to all its parent level nodes, not to just the parent
node(Scenario II). (3) An aggregator unicasts a BLK to all its neighbor nodes except its
child level nodes(Scenario III).
In an ideal case, just one Backward key is sufficient for the computation of a nonaggregator’s CK. However, a node may be expected to relaying several Backward keys
to the upper level in SPDA. Note that Backward key setup communication commences
from an aggregator and ends at another aggregator. In order to count the number of

additional communications for Backward key distribution, we first find the sum of all
intermediate non-aggregators that relay the Backward key for aggregators. tn represents
the number of aggregators in the network that travel through n, the number of nonaggregators, to reach another aggregator. tn can be derived with α and Nagg as below.
t0 =

h
X

αNagg (l) + Nagg (1)

l=2

t1 =

h
X

(1 − α)αNagg (l) + (1 − α)Nagg (2)

l=3

... = ...
h
X
ti =
(1 − α)i αNagg (l) + (1 − α)i Nagg (i + 1)
l=i+2
2

(1 − α) αNagg (4), for instance, represents the case when that many aggregators
on level 4 send a Backward key; there are 2 non-aggregators relaying the key until the
key reaches an aggregator. Therefore, the total number of additional communications
necessary for Backward key setup of all non-aggregators is calculated thus:
Tf orward = p ·

h−1
X

(i · ti ) = p ·

i=0

h
X
i=0

(i · (

h−1
X

((1 − α)i α2 N (l)) + α(1 − α)i N (i + 1)).

l=i+2

, where p is the scenario factor. For scenario I, p is 1. p is going to be 0.3N and 0.7N for
scenario II and scenario III, respectively. N is the average number of neighbors. 0.3N
is the approximate average number of parent level nodes per an aggregator and 0.7N is
the approximate average number of parent and sibling level nodes per an aggregator.
In conclusion, the average number of forwarding communication per each nonaggregator, Af orward , is
Af orward =

Tf orward
.
Tnon

, where Tnon is TN − Tagg − Ttail .
Now we can calculate the communication cost if we know the network parameters,
such as the average number of neighbor nodes(N ), the average number of sensor nodes
in a level(N (l)), the number of aggregators(α), and the maximum number of levels(h).
The results of numerical analysis and simulation are shown in Figure 3(a) and 3(b). We
set the average number of neighbors, N with 10 and maximum number of level in a
network, h with 14 for numerical analysis. We see the numerical results is similar to the
simulation results.

5

CONCLUSION

In this paper we proposed SPDA, a security protocol for data aggregation, to offer a
lightweight key establishment and adaptability of any aggregation algorithms for largescale tree-structured sensor networks. The protocol provides data integrity and confidentiality by applying a secret key mechanism established with a key generation and

(a) Result from Numerical Analysis

(b) Result from Simulation

Fig. 3. Communication Cost

distribution scheme. The concept of Forward and Backward Key establishment that
plays a key role is presented. By using the protocol, a network administrator is able to
build secure networks for data aggregation. In SPDA, a sensor node generates its own
crypto-key, CK, realizing its uniqueness in a network with a high probability.
There are two main points concerning the proposed protocol. First, SPDA is totally
‘distributed’; i.e. there is no central manager for the protocol, such as a cluster-head
or group-head. Hence, it is advantageous for scalability and also adding or deleting
a sensor node is easy. Second, SPDA is independent of a data aggregation algorithm.
Any data aggregation algorithm could be used together with this protocol. Aggregators should be pre-defined before deployment, at the expense of aggregation efficiency.,
however.

References
1. Y. H. Lee, A. Deshmukh, V. Phadke and J. W. Lee, Key Management in Wireless Sensor
Networks, Proceedings of the first European Workshop, ESAS 2004.
2. B. Przydatek, D. Song and A. Perrig, SIA: Secure Inforamtion Aggregation in Sensor Networks, Proceedings of the 1st ACM International Conference on Embedded Networked Sensor Systems (SenSys 2003), 2003.
3. H. O. Sanli, S. Ozdemir and H. Cam, SRDA: Secure Reference-Based Data Aggregation
Protocol for Wireless Sensor Networks, Proceedings of IEEE VTC Fall 2004 Conference,
Sept. 26-29, 2004.
4. H. Cam, S. Ozdemir, P. Nair, D. Muthuavinashiappan and H. O. Sanli, Energy-Efficient Secure Pattern Based Data Aggregation for Wireless Sensor Networks, To appear in a special
issue of Computer Communications on Sensor Networks.
5. L. Hu and D. Evans, Secure Aggregation for Wireless Networks, Proceeding of Workshop on
Security and Assurance in Ad hoc Networks, Jan 28, 2003.

Dynamic Analysis of Embedded Software using Execution Replay
Young Wn Song and Yann-Hang Lee
Computer Science and Engineering
Arizona State University
Tempe, AZ, 85281
ywsong@asu.edu, yhlee@asu.edu

Abstract—For program optimization and debugging, dynamic
analysis tools, e.g., profiler, data race detector, are widely used.
To gather execution information, software instrumentation is
often employed for its portability and convenience. Unfortunately, instrumentation overhead may change the execution of
a program and lead to distorted analysis results, i.e., probe
effect. In embedded software which usually consists of multiple
threads and external inputs, program executions are determined by the timing of external inputs and the order of thread
executions. Hence, probe effect incurred in an analysis of embedded software will be more prominent than in desktop software. This paper presents a reliable dynamic analysis method
for embedded software using deterministic replay. The idea is
to record thread executions and I/O with minimal record overhead and to apply dynamic analysis tools in replayed execution. For this end, we have developed a record/replay framework called P-Replayer, based on Lamport’s happens-before
relation. Our experimental results show that dynamic analyses
can be managed in the replay execution enabled by P-Replayer
as if there is no instrumentation on the program.
Keywords – program dynamic analysis; probe effect;
deterministic replay; profiling, embedded software

I.

INTRODUCTION

Dynamic program analysis is widely used to collect execution information while programs are running. The approach is widely applied to aid optimization and debugging
of the programs. For the collection and analysis of program
execution, software instrumentation is inevitable unless
hardware-assisted approaches are available. For instance,
dynamic binary instrumentation tools such as INTEL PIN
[17] and Valgrind [22] are most commonly used since they
do not require source code and recompilation of program.
However, instrumentation overhead from the tools is very
high no matter how trivial the analysis is.
The instrumentation overhead can perturb the execution
of a program and lead to different execution paths, and consequently misrepresent analysis results. This is so called the
probe effect [10]. One example is the delayed executions of
input events caused by the instrumentation overhead. Consequently, arriving external inputs may be lost or the deadlines
for real-time applications may be missed. The instrumentation overhead may also lead to different order of thread operations on a shared resource and produce different execution
results.
In embedded systems, applications run with multiple
threads interacting with each other as they share resources.
The execution of threads is often triggered by external in-

puts. Thus, program behavior will depend upon the time that
input events arrive as well as the input value received. Also,
it is a common practice to use asynchronous functions to
increase response time. Hence, embedded software will be
sensitive to probe effect caused by instrumentation overhead
as the timing of input events and the order of thread execution can be easily deformed. On the other hand, the probe
effect is a less concern in desktop applications which usually
perform predefined work load (e.g., file input) with fixed
loop counts and synchronous functions. Even if there were
execution changes due to instrumentation overhead from
analysis tools, analysis results would be amortized with repeated executions of the same code, e.g., consistent data races and call-path analysis results.
In Tables 1 and 2, the probe effect is illustrated in the execution of two embedded programs. The two programs are
based on the class projects of Embedded Systems Programming in Arizona State University [5]. Each result is from the
average of 10 runs. The first program is a QT [24] application which draws lines following the inputs of mouse movement. The program consists of three threads. The first thread
receives mouse movement packets and sends them to a
POSIX message queue (MQ). The second thread receives the
input packets from the MQ and draws lines on a display device. The last thread performs a line detection algorithm with
the received mouse inputs. We collected mouse movement at
a normal sampling rate of 300 inputs per second and then fed
the inputs to the application with variant speeds. If the first
thread was delayed and was not ready to receive an input, we
counted it as a missed input. The program is instrumented
using two dynamic analysis tools, i.e., the cache simulator
[12] using PIN and our implementation of the FastTrack data
race detector [8]. The workload of the program is very light
as it only spends less than 10% of CPU time. However, the
instrumented execution may miss up to 45% of the inputs.
The impact of probe effect caused by the instrumentation is
obvious since analysis results may be misleading when the
input data are missed.
The second program shown in Table 2 is a MQ test program with six threads and two MQs. The two sender threads
send items to the first MQ and the two router threads receive
the items from the first MQ and send them to the second MQ
with timestamps. Finally, two receiver threads receive the
items from the second MQ. We used asynchronous functions
for queue operations and, if a queue is empty or full, the
thread sleeps a fixed time interval and retries. We count the
numbers of occurrences that the queues become empty or
full as a way of measuring different program behaviors. In

Table 1. QT application with mouse inputs
(% of inputs missed out of 4445 mouse movement inputs)

inputs/sec
150
300
450

Native
execution
0.0%
0.0%
0.0%

PIN
Cache
16.8%
36.1%
45.5%

Race detector
0.3%
1.2%
1.9%

Table 2. POSIX Message Queue application
(# of Queue full/# of Queue empty)

Queue
Length
5
10

Native
execution
1.3/7.5
0.5/8

PIN
Cache
8.3/191.9
2.5/146.8

Race detector
5.5/56.3
2.4/37.9

this program, there is no external environment affecting the
program execution, but the execution is determined by order
of thread executions on the shared MQs. As the results show,
instrumentation overhead from the tools has changed the
relative ordering of thread operations on the shared MQs
which, in turn, leads to different status of the message
queues.
The other concern, followed by the data shown in Tables
1 and 2, is that it will be very hard to know if there exists any
probe effect on an instrumented program execution. If we
take any serious measurement to find possible probe effect,
the measurement itself would incur instrumentation overhead
that can lead to execution divergence. Even if we know that
there were some changes in program execution, we still
would not be able to know how the changes affect the results
of the analysis.
To make dynamic analysis tools as non-intrusive as possible, hardware-based dynamic analyses can be used. Intel
Vtune Amplifier XE [13] exploits on-chip hardware for application profiling. ARM RealView Profiler [25] supports
non-intrusive profiling using dedicated hardware or simulator. However, they are specific for performance profiling and
do not support the analyses that require significant processing capabilities. Sampling based analyses [2, 9, 13, 19,
28] can also be used, but the analysis accuracy decreases
when sampling rate is reduced to limit any measurement
overhead.
In this paper, we present a dynamic program analysis
method for embedded software using deterministic replay.
The idea is to record thread executions and I/O events with a
minimal recording overhead and to apply dynamic analysis
tools on replayed executions. With the record/replay, dynamic analyses that were not feasible due to probe effect can be
performed accurately. For this end we have developed a record/replay mechanism, called P-Replayer. P-Replayer is implemented as a record/replay framework based on Lamport’s
happens-before relation [14]. The design goal is to minimize
recording overhead to prevent probe effect during recording
and to have minimal disturbance on analyzed program executions during replaying. The contributions of this paper are:
1. We present a dynamic analysis method that makes
analyses of a program feasible and faithful, and expe-

dites the debugging and optimization process for embedded programs.
2. We present a record/replay framework that can be incorporated with dynamic analysis tools.
3. We show that for thread profilers the real execution
time after removing measurement overhead can be accurately recovered with the record/replay framework.
The rest of paper is organized as follows. In the following section, the dynamic analysis method with deterministic
replay is described. Section III presents the design and implementation of P-Replayer. In section IV, the performance
evaluation of P-Replayer and the accuracy of dynamic analysis with P-Replayer are presented. A concise survey of related works is described in section V and we conclude the paper in section VI.
II.

DYNAMIC ANALYSIS WITH EXECUTION REPLAY

For dynamic analysis of a program, instrumentation
overhead is not avoidable whatever optimization techniques
are used. The overhead can result in different execution behavior. It is possible to repeat the same analysis again and
again hoping that eventually we see the true program behavior without the overhead. This repeated running is not even
feasible if the program execution depends on the timing of
external inputs.
Consider the idea of using a record/replay framework for
dynamic analysis as shown in Figure 1. First, an execution of
a program is recorded. If the overhead of recording is small
enough to avoid probe effect, we can assume that the recorded execution is as same as the original program execution.
Second, we apply dynamic analyses on the replayed execution which has the same thread executions and I/O inputs as
the recorded one. Thus, the analyzed program will be executed as if there is no probe effect.
Moreover, the analyses of a program can be expedited
with reproducible execution. When we find an unexpected
execution during testing a program, the first step will be to
locate the cause of the problem. We may need to run various
analysis tools to locate the cause. This will be very time consuming and multiple trials may be needed since it can be
hard to reproduce the execution given the significant overhead of the analysis tools. Instead, we record the program
execution during the test run. As the execution is reproducible in replayed execution, analysis runs can be performed in
the same program execution.
During a deterministic replay, measurement overhead
from profilers can also be calculated to obtain accurate execution time measurement. In thread profiling, execution
times of a program’s activities such as function execution
time can be measured for each thread. Since the measurements can incur probe effect, several execution time estimation algorithms [18, 27] have been proposed to recover the
real execution time. In the approaches, the real execution
time can be recovered with the considerations of three factors: 1) thread local measurement overhead, 2) event reordering, and 3) execution time difference due to the event reordering. As an example of the factors 2) and 3), consider the
take and give operations performed on a semaphore. Assume

Record
Test 1

Test 2

Test k

……

x
Faulty behavior
/Performance anomaly

Replay
Replay
with tool 1

Replay
with tool 2

Replay
with tool n

……

√

Debugging tools
/profilers

Bug/Bottle neck
Located
Thread
profiler

Thread profile with replay
0 sec 1 sec 2 sec …

T1
T2

Estimated results
T1
T2

0 sec 1 sec 2 sec …

……

……
Tk

Tk

Execution time estimation without overhead

Figure 1. Dynamic analyses with execution replay

that in the real execution, the semaphore is given first and is
taken afterward. Hence there is no blocking. In the instrumented execution, if the semaphore give operation had started late due to the delay of instrumentation overhead, then the
thread taking the semaphore would have been blocked. The
estimation algorithms [18, 27] can account for the blocking
time and then reorder the events. However, if there is a
change of program execution paths, then there will be no
way to recover the real execution.
To avoid the above-mentioned problem, thread profiling
can be applied in a replayed execution. Note that the execution with the profiling is deterministic as the event reordering
is handled by the replay scheduling. The overhead compensation for the reordering events is no longer needed. Therefore, as long as we can identify the overhead caused by the
replay mechanism, the total overhead from the thread profiling tool on a replayed execution is simply the sum of the
replay overhead and the thread local measurement overhead
from the profiler.
III.

P-REPLAYER

In this section, we describe a record/replay framework,
called P-Replayer. The framework is based on our Replay
Debugger [16], and is optimized and generalized for dynamic analysis tools. The framework is designed to have minimal
record and replay overheads. To enable execution replay, we
consider the happens-before relation [14] among events
which are execution instances of synchronization or IO function calls. The record/replay operations are implemented in
the wrapper functions for events. The happens-before relation over a set of events in a program’s execution is logged
during recorded execution and are used to guide the thread
execution sequence in execution replay. The happens-before
relation between two events, denoted “→”, is the smallest
relation satisfying,

 Program order: If a and b are in the same thread and
a occurs before b, then a → b.
 Locking: If a is a lock release and b is the successive
lock acquire for the same lock, then a → b.
 Communication: If a is a message send and b is the
receipt of the same message, then a → b.
 Transitivity: If a → b and b → c, then a → c.
In P-Replayer, a data race detector is included as an analysis tool and for managing execution replay in the presence
of data races. It also provides an approach for recovering real
execution time without measurement overhead of thread
profilers.
A. Record/Replay Operations
During recording operation, happens-before relations for
all events in a program execution are traced and saved into a
log file, and the same happens-before relations are to be enforced during execution replay. A general approach [26] to
obtain the happens-before relation is to use Lamport clock
[14]. In the approach, a Lamport clock is maintained for each
thread and the clock is incremented and synchronized by the
happens-before relation. A timestamp (Lamport clock value)
is attached to each recorded event. During the subsequent
replay operations, the corresponding event is delayed until
all other events having smaller clock values are executed.
This can enforce a stronger condition than necessary for replaying the partially ordered events. In our approach, we use
a global sequence number to order the events traced in recording operation. This sequence represents a total order of
events and is used to index the set of events during execution
replay.
To identify the happens-before ordering, the event log of
an event consists of the two sequence numbers for the event
itself and the event immediately before it, plus thread id, the
function type and arguments for the event. So, for an event b,
let a be the event happened before b immediately. For the
execution of the event b, the sequence numbers of both
events a and b are recorded in the event log of the event b.
For an input event, the received input is saved into the log
file as well. All logging operations are performed in a dedicated logging thread to avoid possible jitters caused by file
operations.
In a subsequent replay operation, an event table is constructed from the recorded log. The event table contains a list
of events for each thread. Using the sequence number in the
record log, events are indexed to represent the happenedbefore relations. Thus, based on the table, the next event for
each thread that is eligible for execution can be identified
and the same happens-before relations as the recorded ones
are used to schedule thread execution. The replay scheduling
is performed inside replay wrapper functions, thus the replay
operation is implemented purely in application level.
A replay wrapper function consists of three parts. Firstly,
a thread looks up the event table for the events happened
before its current event. If any of the events that should be
happened before are not executed, the thread is suspended
waiting for a conditional variable. In the second part, when a
thread can proceed with its current event, the thread carries

out the original function. If the function is for an input event,
the thread reads input value from the log file instead of actual reading from an I/O device. Lastly, the event for this
wrapper function is marked as executed and the thread wakes
up (signal) other threads waiting for the execution of this
event. Note that the total order of events based on the sequence numbering is used only for indexing events and the
replay operation follows the partial order of happens-before
relations.
B. Handling Data Races
For efficient record and replay operations, only synchronization and I/O events are considered. However, one drawback is that a replay may not be correct in the presence of
data races. A data race occurs when a shared variable is accessed by two different threads that are not ordered by any
happens-before relation and at least one of the accesses is a
write. Assume that, in the recorded execution, there is a data
race on a variable that is used to decide alternative execution
paths. Since there is no synchronization operation to enforce
any happened-before relation, the order of accessing the variable is not recorded. This implies that, if an alternate accessing order from the record one is chosen in replayed execution, the replayed program may take a different execution
path.
We use a similar approach as RecPlay [26] to handle the
presence of data races in replaying operations. RecPlay records an execution of a program as if there is no data race and
any occurrences of data races are checked during replay operation using a data race detection tool. If data races are
found, the replay operation is repeated after removing the
races. The approach is correct since a replayed execution is
correct up to the point where the first race occurs as shown in
[4]. However, most data race detection tools incur substantial
overheads. It may not be practical to fix every data race during replaying operations.
Instead, P-Replayer detects the occurrence of an unexpected event (a different event from the recorded one) during
replay process and stops the process at the location of the
unexpected event. Then, the race can be detected with a race
detection tool and fixed. The detection of an unexpected
event is done in the replay wrapper by comparing the current
events with the events in the record log. After fixing the race
that results in different execution path, the replay can be
safely used with various analysis tools including a race detection tool for detecting races that cause errors other than a
change of execution paths.
We have implemented a data race detection tool based
on FastTrack [8] and the tool is modified to be integrated
with P-Replayer. FastTrack algorithm detects data races
based on happens-before relations in a program execution.
However, the replay scheduler invokes additional locking
operations which appear as extra happens-before relations
for the FastTrack detector. As a consequence, some data
races may not be detected in the replayed execution. To correct the missed detections, we alter the FastTrack approach
to discount the synchronization operation introduced by the
replay scheduler.

C. Execution Time Estimation
In this subsection, we present an approximation approach
to estimate execution time that can account for the overheads
of replay operation and thread profilers. First, we present the
algorithm that can estimate the real execution time without
replay at program level. Second, the algorithm is refined to
estimate the real execution time at thread level without the
overheads of profilers and replay operation. The replay overhead is approximated based on the assumptions that 1)
threads run on multiple cores, 2) events are evenly distributed to each core, 3) the record overheads for all event types
are same, and 4) the number of worker threads is greater than
or equal to the number of cores.
1) Execution Time Estimation at Program Level
The estimated execution time of a program execution,
Cestimate, can be calculated by subtracting the replay overhead
Oreplay from the replayed execution time Creplay, such that,
Cestimate = Creplay – Oreplay

(1)

The replay overhead Oreplay is the sum of 1) replay initialization time Cinit, 2) extra execution time, Ce, spent in replay
wrapper functions, and 3) blocking time, Bu, by the replay
scheduling that leads to extra delay of replay execution, i.e.,
Oreplay = Cinit + Ce + Bu

(2)

The extra execution time Ce is classified into two parts.
Note that for two events a and b with a → b, the execution of
event b can be delayed until event a is executed or can be
executed with no-delay if event a has already been executed.
The two possible ways of event executions contribute to various amount of overheads, thus we account them differently.
Let nnd and nd be the numbers of events of no-delay and delayed execution, respectively. Let cnd and cd be the execution
times for no-delay and delayed events, respectively. Then, Ce
can be expressed as,
Ce = nnd*cnd + nd*cd

(3)

Threads can be blocked by the replay scheduling through
global locking and by delayed events. The blocking of
threads leads to additional execution time only when the
number of ready threads becomes less than the number of
cores, i.e., when the cores are underutilized. Let ng be the
number of occurrences that threads are blocked by the global
locking and let bg be the average delay caused by each global
locking. Also, let bd be the total execution delay caused by
the delayed events due to the replay scheduling at the end of
a replay execution. Then, Bu can be expressed as,
Bu = ng*bg + bd

(4)

Combining Equation (3) and (4) into Equation (2) gives,
Oreplay = Cinit + nnd*cnd + nd*cd + ng*bg + bd (5)
2) Overhead Measurements
The replay initialization time Cinit, and the counter values,
nnd, nd, and ng, can be measured during a replay execution.
The blocking delay, bd, is calculated using the algorithm in
Figure 2 based on the utilization of the cores. On the other
hand, the per event overheads, cnd, cd, and bg, are hard to

n = number of runnable threads
M = number of cores

//At start of event delay in thread T
ts-T = get_timestamp();

//At start of every delayed event
if ( n==M ) //start measurement
ts = get_timestamp();
else if (n < M)
tmp = get_timestamp();
bd += (tmp- ts )*((M-n)/M);
ts = tmp;
n--;

//At end of event delay in T
te-T = get_timestamp();
bT += (te-T - ts-T);

//At end of every delayed event
if ( n < M )
tmp = get_timestamp();
bd += (tmp- ts )*((M-n)/M);
ts = tmp;
n++;

int blocking_overhead(thread T)
{
if T is not blocking from replay
return bT;
else
return \
(bT + get_timestamp()-ts-T);
}

Figure 2. (Left): the time measurement in which the cores are underutilized due to the delayed events by the replay scheduling. (Right): the
measurement of blocking overhead for a thread T.

measure online in the execution on multi-core systems. To
avoid the online measurement, we assume they can be
viewed as constants for a given system and can be estimated
offline using measurement programs.
The measurement program for cnd consists of multiple
threads which invoke their own locks. Thus, threads are independent of each other and there is no overhead from delayed events. Hence, nd*cd=0 and bd=0. To avoid any idle
CPU core caused by global locking, extra threads running
busy loops and with no synchronization and IO event are
added. Hence, the blocking overhead from the global locking
becomes zero (ng*bg=0) since all cores are utilized fully. The
execution time without replay, Cm1, is measured and we can
assume that Cm1 = Cestimate. Then, with Equation (1) and (5),
cnd can be calculated using the following equation:

time instant. For instance, if a function in a thread T is invoked at ts and finishes at te, then the execution time of the
function is measured as te-ts. If Cestimate-T(t) is the estimated
execution time of a thread T up to time t, then the real execution time of the function can be estimated as,
Cestimate-T(te)- Cestimate-T(ts)
We can start the measurements after the initialization of a
replay execution, thus Cinit=0. All counter values (nnd, nd, and
ng in Equation (5)) are maintained for each thread. Note that
all per-event measurements (cnd, cd, and bg in Equation (5))
are measured for concurrent executions on multiple cores.
Since each thread can only be run on a single core, the perevent measurements for each thread, denoted as cnd′, cd′, and
bg′, can be approximated as the product of M and cnd, cd, and
bg, respectively, where M is the number of cores in the system. The blocking delay (bd) is replaced with accumulated
blocking time for each thread and it can be calculated as
shown in Figure 2. Then, the replay overhead in a thread T at
a given instant t can be represented,
Oreplay-T(t)=nnd-T(t)* cnd′ + nd-T(t)* cd′ + ng-T(t)* bg′+ bT(t) (5)′
Let the thread local overhead from the profiler in a thread
T up to a given instant t be Oprofile-T(t). Then, the estimated
execution time for a thread T at time t can be expressed as,
Cestimate-T(t) = t – ( Oreplay-T(t) + Oprofile-T(t) )
IV.

(1)′

EVALUATION

The remaining constant bg is calculated using a similar
measurement program for measuring cd but without the extra
threads with busy loops.

In this section, we show the effectiveness of the analysis
approach in replay execution through several benchmark
experiments. First, we show the overheads of P-Replayer.
Second, we present the evaluation results of the execution
time estimation algorithm. Lastly, the accuracy of dynamic
analyses using P-Replayer is presented. All experiments
were performed on an Intel Core Duo processor running Ubuntu 12.04 with kernel version 3.2.0.
The two programs shown in section I are used to illustrate the effect of the minimized probe effect in record phase.
All other experiments were performed with 11 benchmarks
for desktop computing to reveal the efficiency and accuracy
of the dynamic analysis methods performed in the replay
phase. The benchmarks are from PARSEC-2.1 [1] and from
real-world multithreaded applications: FFmpeg [6], a multimedia encoder/decoder; pbzip2 [11], a data compressor; and
hmmsearch [7], a search/alignment tool in bioinformatics.

3) Execution Time Estimation at Thread Level
In thread profiling, the execution times of threads’ activities can be measured. As presented in section II, the measurement overhead from the thread profiler consists of 1)
thread local overhead and 2) execution time differences due
to event ordering. When the profiler runs in a replayed execution, the latter overhead is contained in the replay overhead for delayed events (nd*cd + bd). Therefore, the total
overhead from the profiling on a replay execution is simply
the sum of the replay overhead and the thread local overhead
of the profiler. To estimate the real execution time without
the overheads, we need per-thread measurement at a given

A. Overhead of Record/Replay Operations
Table 3 shows the overheads of the record and replay operations in P-Replayer. “Number of events/sec” column
shows the number of recorded events per second in the execution of each benchmark program, and from the column we
can have a general idea of how big the overheads will be.
The overheads of record and replay operations are 1.46%
and 2.78% on geometric mean, respectively. The results suggest that P-Replayer will be suitable for dynamic program
analysis in replay execution. One exceptional case is fluidanimate which incurs noticeable record/replay overheads due to
the large number of events in the execution.

Cm1 = Creplay – (Cinit + nnd*cnd)
A similar program is used for measuring cd, where a lock
is shared among all threads. Hence, nd*cd≠0. Using the extra
threads with busy loops, the program keeps ng*bg=0 and
bd=0. Assuming that the execution time without replay, Cm2,
is equal to Cestimate, cd can be calculated using the following
equation:
Cm2 = Creplay – (Cinit + nnd*cnd + nd*cd )

Table 3. Record/Replay overhead

Record
Replay
(sec)
(sec)
6.054
6.055
5.073
5.161
3.620
4.887
9.843
9.813
2.288
2.323
6.674
6.677
8.208
8.711
4.071
4.092
3.052
3.157
5.396
5.381
26.624 26.699

PIN Cache

FastTrack

0.0%
0.0%
0.0%

16.8%
36.1%
45.5%

0.3%
1.2%
1.9%

Queue
Length

Native
execution

Record

PIN Cache

FastTrack

Table 5. Revisit of Table 2 with recording operation

5
10

1.3/7.5
0.5/8

1.3/8.1
0/7.1

8.3/191.9
2.5/146.8

5.5/56.3
2.4/37.9

Tables 4 and 5 are the revisits of Tables 1 and 2 with the
additional measures collected in record phase. For the QT
application (shown in Tables 1 and 4), the QT libraries are
not instrumented in all the tools and no event recording is
done in the execution of the library code. The results in both
tables suggest that multithreaded program executions can be
recorded by P-Replayer with a minimal probe effect.
B. Execution Time Estimation
Based on the overhead analyses in Section III, the replay
overhead for each benchmark can be measured and calculated with Equation (5) and the real execution time can be estimated using Equation (1). In Table 6, the estimated execution times of the benchmark programs are listed in column 4
where column 5 gives the estimation error. On average, the
estimation error is 1.24%.
The replay overhead is classified into the 5 categories in
Equation (5). In Figure 3, the relative overhead in the 5 categories is illustrated for the four benchmarks that have more
than 5% of replay overhead, i.e., fluidanimate, x264, dedup,
and streamcluster. In the chart, the items are correspondent
to the five categories of Equation (5). The applications,
dedup and streamcluster, show relatively more percentages
of blocking overhead caused by delayed events (around
50%) than the other two applications. This implies that they

facesim
ferret
fluidanimate
raytrace
x264
canneal
dedup
streamcluster
ffmpeg
pbzip2
hmmsearch
Average

6.050
5.027
2.054
9.823
2.196
6.643
7.750
3.781
3.053
5.297
26.550

6.055
5.161
4.887
9.813
2.323
6.677
8.711
4.092
3.157
5.381
26.699

6.041
5.122
2.144
9.812
2.247
6.677
7.838
3.859
3.052
5.378
26.595

Error

Record

0.0%
0.0%
0.0%

Benchmark
Program

Estimation (sec)

Native
execution

150
300
450

Table 6. Execution time estimation without replay
Replay
(sec)

inputs/sec

Table 4. Revisit of Table 1 with recording operation

Number of
Record
Replay Overevents/sec
Overhead
head
2,200.5
0.07%
0.08%
2,066.2
0.92%
2.67%
2,163,267.3
76.24%
137.93%
9.8
0.20%
-0.10%
17,487.2
4.19%
5.78%
1.5
0.47%
0.51%
75,623.9
5.91%
12.40%
38,417.1
7.67%
8.23%
3,560.4
-0.03%
3.41%
622.2
1.87%
1.59%
3,644.7
0.28%
0.56%
1.46%
2.78%
9.78%
17.31%

Base time
(sec)

Base time
(sec)
6.050
5.027
2.054
9.823
2.196
6.643
7.750
3.781
3.053
5.297
26.550

-0.2%
1.9%
4.4%
-0.1%
2.3%
0.5%
1.1%
2.1%
0.0%
1.5%
0.2%
1.24%

100%
90%
80%
70%
% of overhead

PARSEC

Benchmark
Program
facesim
ferret
fluidanimate
raytrace
x264
canneal
dedup
streamcluster
ffmpeg
pbzip2
hmmsearch
Geometric mean
Average

60%
50%

40%
30%
20%
10%
0%
fluidanimate

x264

dedup

streamcluster

Benchmark
Initialization

No-delay

Delayed

Global -lock

Serialization

Figure 3. The decomposition of the replay overhead is shown for
benchmarks that have more than 5% replay overhead

experience more per event disturbance caused by the replay
scheduling than the other two applications.
C. Accuracy of Analysis in Replay Execution
In this section, we present experimental results to show
the accuracy of dynamic analysis in replay execution. Once
we have a recorded execution which is as same as the original execution, the analysis result in replay execution will be
as close as the analysis without any instrumentation. Since it
will be very hard to know what the analysis results will be
without any instrumentation, we assume that the 11 bench-

facesim
ferret
fluidanimate
raytrace
x264
canneal
dedup
streamcluster
ffmpeg
pbzip2
hmmsearch
Average

Number of
races detected

Benchmark
Program

Slowdown

Without
replay

With
replay

140
86
85
27
74
12
151
244
120
68
83
99

139
82
116
27
77
12
148
251
120
69
86
103

8907
2
1
13
1300
0
0
1053
1
0
1

Benchmark
Program

Slowdown
Without
replay

facesim
ferret
fluidanimate
raytrace
x264
canneal
dedup
streamcluster
ffmpeg
pbzip2
hmmsearch
Average

37
50
59
59
77
25
41
58
66
51
28
50

With
replay
37
50
127
59
79
24
43
52
66
50
29
56

# of different
function
entries

Table 8. Flat profiling comparison from Callgrind

Table 7. Data race detection with FastTrack

0
0
7
0
0
0
0
4
0
1
0

Table 9 Cache simulation results from PIN Cache

Benchmark
Program
facesim
ferret
fluidanimate
raytrace
x264
canneal
dedup
streamcluster
ffmpeg
pbzip2
hmmsearch
Average

Slowdown
Without
With
replay
replay
472
462
336
341
398
731
395
387
411
424
92
91
286
301
395
454
478
478
377
381
498
475
376
411

Cache miss rate - Without replay
L1L1L2Instruction
Data
Unified
0.00%
5.92%
12.02%
0.02%
4.73%
16.89%
0.00%
17.13%
1.15%
0.00%
2.77%
2.23%
1.34%
4.07%
8.10%
0.00%
4.06%
43.46%
0.00%
4.06%
7.25%
0.00%
6.55%
49.14%
0.01%
4.52%
11.56%
0.00%
4.03%
13.54%
0.00%
0.78%
7.14%

mark programs show no (or negligible) probe effect from the
instrumentation and compare the analysis results with the
ones collected from replay execution.
Table 7 compares analysis results of the FastTrack race
detector in normal program execution and in replay execution. The two approaches locate the same data races. For
facesim, there was a data race that may result in different
execution paths. P-Replayer stops the replay execution after
encountering the event diverging from the recorded one.
Then, the race is detected with the FastTrack detector and
fixed with correct synchronization. The data in the table
shows the detection result after fixing the race for facesim.
Table 8 compares the flat profiling results from Callgrind
[3]. The flat profiling lists the functions in decreasing order
that have fetched most instructions. For each benchmark
program, we compare the top 10 function entries from normal program execution and replay execution, and the number
of different function entries is shown in the last column of
Table 8. There are three cases showing different function
entries. For streamcluster, the function entries are different
from the 6th entries due to the functions invoked for replay
execution. However, from the 3rd function in the list, the
functions have used less than 1% of total instructions. Similarly, the 10th function entries are different in pbzip2 which

Cache miss rate - With replay
L1L1L2Instruction
Data
Unified
0.00%
5.92%
12.07%
0.02%
4.74%
16.14%
0.00%
18.72%
1.58%
0.00%
2.77%
2.23%
1.32%
4.07%
9.62%
0.00%
4.08%
43.12%
0.01%
3.93%
7.66%
0.00%
6.42%
48.95%
0.01%
4.61%
11.26%
0.00%
4.03%
14.40%
0.00%
0.78%
7.10%

fetches a negligible percentage of instructionss. For fluidanimate, after removing functions used in the replay execution,
the same function entries are shown in the same order.
Table 9 shows the comparisons of PIN Cache simulator
running in normal program execution and in replay execution. As can be seen from the comparisons, the differences
are negligible. There are only two measures of cache miss
rates with a more than 10% discrepancy between normal
program execution and replay execution.
V. RELATED WORKS
Event-based replay has been a preferable choice for program analysis and debugging. Instant Replay [15] is one of
the earliest works that record and replay a partial order of
shared object accesses. RecPlay [26] has proposed a record/replay mechanism that has low record overhead and a
data race detection algorithm. However, the replay overhead
is too high (around 2x) for uses with dynamic analyses. Replay Debugger [16] uses a similar approach as RecPlay for
record/replay operations, but it focuses on debugging techniques integrated with GDB.
As an effort to avoid probe effect, several hardware-base
dynamic analysis tools have been proposed. To detect data
races, CORD [23] keeps timestamps for shared data that are

presented in on-chip caches. With simplified but realistic
timestamp schemes, the overhead can be negligible. DAProf
[21] uses separate caches for profiling of loop executions. In
the approach, short backward branches are identified and
profiling results are stored in the cache. Both approaches are
non-intrusive with acceptable accuracies of analysis results.
However, the requirement of extra hardware mechanisms
may make the approaches impractical.
Moreno et.al [20] has proposed a non-intrusive debugging approach for deployed embedded systems. In the approach, the power consumption of a system is traced and
matched to sections of code blocks. Thus, a faulty behavior
in the execution of the code blocks can be identified only
with an inexpensive sound card for measuring power consumption and a standard PC for the power trace analysis.

[10] J. Gait, A probe effect in concurrent programs. Software

VI. CONCLUSIONS
In this paper, we have presented a dynamic analysis
method for embedded software. In the method, the execution
of a program is recorded with minimal overhead to avoid
probe effect, and then the program analysis is performed in
replay execution where event ordering is deterministic and is
not affected by instrumentation. For this end, we have described a prototype P-Replayer and demonstrated the use of
replay execution for dynamic program analyses on the
benchmark programs. In addition, P-Replayer provides execution time estimation which can be integrated for thread
profiling tools.

[17]

ACKNOWLEDGMENT

[20]

Practice and Experience, 16(3): 225-233, 1986.
[11] J. Gilchrist. Parallel BZIP2, http://compression.ca/pbzip2/.
[12] Intel PIN tool. http://software.intel.com/en-us/articles/pin-a-

dynamic-binary-instrumentation-tool.
[13] Intel Vtune Amplifier XE 2013. http://software.intel.com/en-

us/intel-vtune-amplifier-xe.
[14] L. Lamport. Time, clocks, and the ordering of events in a

[15]

[16]

[18]

[19]

This work was supported in part by the NSF I/UCRC
Center for Embedded Systems, and from NSF grant
#0856090.
REFERENCES
[1]
[2]

[3]
[4]

[5]
[6]
[7]

[8]

[9]

C. Bienia. Benchmarking Modern Multiprocessors. Ph.D.
Thesis. Princeton University, 2011.
M. D. Bond, K. E. Coons, and K. S. McKinley. PACER:
proportional detection of data races. In Proceedings of the
ACM SIGPLAN conference on Programming language design
and implementation (PLDI), pages 255-268, 2010.
Callgrind, Valgrind-3.8.1. http://valgrind.org/.
J-D. Choi and S. L. Min. Race Frontier: reproducing data
races in parallel-program debugging. In Proceedings of the
third ACM SIGPLAN symposium on Principles and practice
of parallel programming (PPoPP), pages 145-154, 1991.
CSE 438 – Embedded Systems Programming, ASU,
http://rts.lab.asu.edu/web_438/CSE438_Main_page.htm.
FFmpeg. http://www.ffmpeg.org/.
R. D. Finn, J. Clements, and S. R. Eddy. HMMER web
server: interactive sequence similarity searching. Nucleic
Acids Research Web Server Issue 39:W29-W37, 2011.
C. Flanagan and S. N. Freund. FastTrack: efficient and
precise dynamic race detection. In Proceedings of the ACM
SIGPLAN conference on Programming language design and
implementation (PLDI), pages 121-133, 2009.
N. Froyd, J. Mellor-Crummey, and R. Fowler. Low-overhead
call path profiling of unmodified, optimized code. In
Proceedings of the 19th annual international conference on
Supercomputing (ICS), pages 81-90, 2005.

[21]

[22]

[23]

[24]
[25]
[26]

[27]

[28]

distributed system. Communications of the ACM, 21(7):558565, 1978.
T. J. LeBlanc and J. M. Mellor-Crummey. Debugging
Parallel Programs with Instant Replay. IEEE Transactions on
Computers, 36(4): 471-482, 1987.
Y-H. Lee, Y. W. Song, R. Girme, S. Zaveri, and Y. Chen.
Replay Debugging for Multi-threaded Embedded Software. In
Proceedings of IEEE/IFIP 8th Int’l. Conf. on Embedded and
Ubiquitous Computing (EUC), pages 15-22, 2010.
C. Luk, R. Cohn, R. Muth, H. Patil, A. Klauser, G, Lowney,
S. Wallace, V. J. Reddi, and K. Hazelwood. Pin: building
customized program analysis tools with dynamic
instrumentation. In Proceedings of the ACM SIGPLAN
conference on Programming language design and
implementation (PLDI), pages 190-200, 2005.
A. D. Malony and D. A. Reed. Models for performance
perturbation analysis. In Proceedings of the 1991 ACM/ONR
workshop on Parallel and distributed debugging (PADD),
pages 15-25, 1991.
D. Marino, M. Musuvathi, and S. Narayanasamy. LiteRace:
effective sampling for lightweight data-race detection. In
Proceedings of the ACM SIGPLAN conference on
Programming language design and implementation (PLDI),
pages 134-143, 2009.
C. Moreno, S. Fischmeister and M. Anwar Hasan. Nonintrusive program tracing and debugging of deployed
embedded systems through side-channel analysis. In
Proceedings of the 14th ACM SIGPLAN/SIGBED conference
on Languages, compilers and tools for embedded systems
(LCTES), pages 77-88, 2013.
A. Nair, K. Shankar, and R. Lysecky. Efficient hardwarebased nonintrusive dynamic application profiling. ACM
Transactions on Embedded Computing Systems (TECS),
10(3): Article No. 32, 2011.
N. Nethercote and J. Seward. Valgrind: a framework for
heavyweight dynamic binary instrumentation. In Proceedings
of the ACM SIGPLAN conference on Programming language
design and implementation (PLDI), pages 89-100, 2007.
M. Prvulovic. CORD: cost-effective (and nearly overheadfree) order-recording and data race detection. In Proceedings
of the 12th International Symposium on High-Performance
Computer Architecture (HPCA), pages 232-243, 2006.
Qt Project, http://qt-project.org/.
RealView Profiler. http://infocenter.arm.com/help/index.jsp
?topic=/com.arm.doc.dui0412a/html/chapter_1.html.
M. Ronsse, M. Christiaens, and K. D. Bosschere. Debugging
shared memory parallel programs using record/replay. Future
Generation Computer Systems, 19(5):679-687, 2003.
F. Wolf, A. D. Malony, S. Shende, and A. Morris. TraceBased Parallel Performance Overhead Compensation. High
Performance Computing and Communications, LNCS 3726,
pages 617-628, 2005.
X. Zhuang, M. J. Serrano, H. W. Cain, and J. Choi. Accurate,
efficient, and adaptive calling context profiling. In
Proceedings of the ACM SIGPLAN conference on
Programming language design and implementation (PLDI),
pages 263-271, 2006.

Wireless Netw
DOI 10.1007/s11276-014-0878-8

Routing algorithm of minimizing maximum link congestion
on grid networks
Jun Xu • Jianfeng Yang • Chengcheng Guo
Yann-Hang Lee • Duo Lu

•

 Springer Science+Business Media New York 2014

Abstract As regular topology networks, grid networks
are widely adopted in network deployment. Link congestion and routing path length are two critical factors that
affect the delay and throughput of a network. In this paper,
we study the routing problem in grid networks concerning
these two factors. The main objective of our routing
algorithms is to minimize the maximum link congestion.
The shortest path minimum maximum (SPMM) link congestion and non-shortest path minimum maximum
(NSPMM) link congestion routing problems are studied.
The two problems are first formulated as integer optimization problems. Then, corresponding routing algorithms
(SPMM and NSPMM routing algorithms) are proposed.
For SPMM routing algorithm, the path length is optimal,
while for NSPMM routing algorithm, the path is limited in
a submesh. Thus, the path length can be bounded. At last,
we compare the proposed routing algorithms under different scenarios with other popular routing algorithms
(RowColumn, ZigZag, Greedy, Random routing algorithms). The performances are evaluated through different
J. Xu  J. Yang (&)  C. Guo
School of Electronic Information, Wuhan University, Wuhan,
Hubei, China
e-mail: yjf@whu.edu.cn
J. Xu
e-mail: eisxujun@whu.edu.cn
C. Guo
e-mail: netccg@whu.edu.cn
Y.-H. Lee  D. Lu
Computer Science Department, Arizona State University,
Tempe, AZ, USA
e-mail: yhlee@asu.edu
D. Lu
e-mail: duolv@asu.edu

metrics including link congestion, path length, path congestion, path congestion to path length ratio, delay and
throughput.
Keywords Grid networks  Link congestion  Routing
algorithm  Scheduling

1 Introduction
Network congestion occurs when some nodes or links carry
too much traffic. It derogates network performances in
several aspects, such as increasing the end-to-end delay,
packet loss, and decreasing throughput. Reducing maximum link/node congestion and balancing network traffic
are fundamental problems in network management. For
example, one of the main objectives of Traffic Engineering
(TE) [1] is load balancing. The load balancing problem has
been widely studied and some researchers surveyed the
load balancing problem on Internet [2], IP/MPLS [3],
WiMAX [4], Wireless Sensor Networks [5, 6], P2P networks [7] and MANET [8, 9]. Related schemes on network
congestion include rate control [10], queue management
[11], topology control [12] and joint congestion control and
medium access control [13]. Besides, designing a high
performance routing algorithm is also an efficient method,
such as multipath routing [14, 15]. However, the multipath
routing method induces packet reordering problem [16].
In this paper, we design routing algorithms aiming to
minimize the network congestion. The networks we study
are the 2-dimensional grid topology networks. As studied
in paper [17], in wireless mesh networks, grid topology
networks are much more suitable for large scale mesh
deployment compared to random topology networks. When
formulating and analyzing wireless communication

123

Wireless Netw

problems, the 2-dimensional grid network provides a
framework. In other communication networks, grid networks can be the testing topologies for routing and
scheduling algorithms [18].
In the study [19], Leighton, Maggs, and Rao proved that
there exist packet scheduling algorithms completing the
transmissions of all the packets within duration O(c ? d),
where c is the maximum link congestion of the network
and d is the largest path length. Motivated by this result,
many researchers tried to find effective routing algorithms
aiming to minimize the maximum link congestion by
adopting routing paths as short as possible.
Finding routing algorithms to minimize the maximum link
congestion has been studied in game theory area as the bottleneck routing problem [18, 20–22]. The bottleneck node/
link of a routing path is the most congested node/link on the
path. In the routing games studied in [18, 20, 22], the objective
of each player is to find a path with the minimum bottleneck
link congestion. In [18], it allows the path to take some bends
which implies that the routing paths adopted are not always the
shortest paths. The price of anarchy (PoA) is proportional to
the number of bends. In [22], the PoA is O(l ? log(n)) where
l is the length of the longest path in the player’s strategy sets,
and n is the size of the network. In [20], the splittable and unsplittable bottleneck games are studied. In [21], each user aims
to minimize the sum of the maximum link congestion and
service cost of the chosen path. In [23], the bottleneck congestion games are discussed, the aim is to minimize the
maximum resource congestion of the network. In the routing
game studied in [24], any player i tries to minimize its ci ? di
value which represents the sum of the congestion and path
length. Although the studies mentioned above tried to minimize the maximum link congestion of a network, all of them
considered only the maximum link congestion of its routing
path, while ignored the path congestion (which is the summation of all the link congestions along the path). However,
this may result in a higher maximum link congestion of the
network. An example is shown in Fig. 1. The numbers on the
links are the current link congestions. Let flow fi make a

decision before flow fj. If each flow only considers the bottleneck link congestion of the chosen path, then for flow fi, the
bottleneck link congestion of all the routing paths are 2. It
randomly chooses the path pi = {si, v3, v2, di}. Now the
congestions on links (si, v3), (v3, v2), and (v2, di) are increased
to 3. The maximum link congestion of the network is 3. Now,
consider flow fj, no matter which path it chooses, it will
increase the maximum link congestion of the network from 3
to 4. However, if the path congestion is considered after the
bottleneck link congestion, then flow fi should choose the path
pi = {si, v1, v2, di} which has the minimum path congestion.
Now, for flow fj, it will choose path pj = {sj, si, v3, dj} which
has the minimum bottleneck path congestion. In this situation,
the maximum link congestion of the network is only 3, which
is smaller than 4.
Motivated by the above observation, we consider not
only the maximum link congestion but also the path congestion when selecting path for a flow in a grid network to
minimize the maximum link congestion of the network.
In this paper, we propose two routing algorithms which
are denoted by SPMM and NSPMM routing algorithms,
corresponding to the two routing problems.
The evaluations of the proposed routing algorithms are
executed under three different traffic patterns: Partial Permutation Traffic Pattern, General Traffic Pattern and the
Specific Traffic Pattern. For the Partial Permutation Traffic
Pattern, no two flows in F share the same source or destination node, that is, for any two flows fi and fj, we have
that si 6¼ sj , si 6¼ dj , di 6¼ sj and di 6¼ dj . For the General
Traffic Pattern, the source and destination nodes for any
flow can be randomly selected. And more than one flow
can share the same source or destination node. The construction of the Specific Traffic Pattern will be introduced
in Sect. 5.
Our contributions include:
•

•
X
v1

1

v3

2

v4

1

Y

Fig. 1 An example of grid networks

123

dj
1

1
3

di

2

2

2
sj

2

2

1
si

v2

v5

Submesh Gi
of flow fi

•

The mathematical formulations of the SPMM and
NSPMM problems. In both of the problems, some
constraints such as the flow conservation constraints
should be met.
The SPMM and NSPMM routing algorithms. The
SPMM routing algorithm tries to find the shortest path.
However by NSPMM routing algorithm, the paths are
not always the shortest ones. The aim of the two routing
algorithms is the same.
Performance evaluation. The SPMM and NSPMM
routing algorithms are compared with other popular
routing algorithms designed for grid networks, namely
the RowColumn, ZigZag, Random and Greedy routing
algorithms. The performance metrics we evaluate
include link congestion, path length, path congestion,
path congestion to path length ratio, delay and
throughput.

Wireless Netw

The paper is organized as follows. In Sect. 2, we
introduce some related works of the routing algorithms on
grid networks. In Sect. 3, we present the network model
and formalize the SPMM and NSPMM routing problems.
The routing algorithms are described in Sect. 4. In Sect. 5,
we evaluate the performances of the SPMM and NSPMM
routing algorithms by comparing them with some existing
routing algorithms designed for grid networks. At last, in
Sect. 6, we conclude this paper.

2 Related work
Routing algorithms have been studied widely in different
kind of networks, such as the disruption tolerant networks
[25], delay tolerant networks [26], cognitive radio networks
[27], wireless vehicular sensor networks [28], data centers
[29] and multicast routing protocol [30]. Routing is an
essential management mechanism for supporting Qos [31].
For grid networks, some routing algorithms with different objectives have been studied [18, 32–36].
The authors in [18] have studied the routing games in
grid networks, each player aims to minimize the maximum
link congestion of its routing path. The ultimate objective
is to minimize the maximum link congestion of the network which is the same as that in this paper.
In [32], to minimize the makespan of a set of packets,
the authors proposed the RowColumn routing algorithm
combining the longest path first scheduling algorithm for
the partial permutation traffic pattern. The optimal makespan can be achieved when all the packets are starting to
transmit simultaneously. However, in applications like
industry control, each flow generates packets with a specified period, thus it’s hard to guarantee that the packets are
started to transmit simultaneously unless the flows are
simple periodic [37]. Adopting the RowColumn routing
algorithm, the maximum link congestion can be as high as
max{n1, n2} for the Partial Permutation Traffic Pattern. For
the General Traffic Pattern, the maximum link congestion
can be as high as nmax, where nmax represents the maximum
times of the nodes in a row or column being source or
destination nodes among all the rows and columns.
In [33] and [35], the objective is to maximize the reliability of packet transmissions with the assumption that all
the links in a grid network have the same packet delivery
probability. As shown in [33], the ZigZag shortest path
routing algorithm is optimal in 2-dimensional grid networks. However, it is not optimal in the torus [34]. In [35],
the MSP (Maximum-Shortest-Path) routing algorithm has
been proposed to achieve the same goal. By MSP, a packet
is always forwarded to the neighbor with the maximum
number of shortest paths to the destination. Unlike the

ZigZag routing algorithm, MSP is optimal not only in 2-D,
3-D meshes, but also in N 9 N torus with N being an even
number greater than 4.
A local-information-based routing algorithm denoted by
ZigZag has been presented in [36]. By this routing algorithm, any given path between two nodes can be transformed into a shortest path. This strategy is useful in
scenarios where nodes can move around.
Another type of routing algorithms is Random routing
strategy [33, 38]. By the random routing algorithm discussed in [33], a packet is forwarded randomly to a
neighbor which is nearer to the destination node. In [38], a
packet is forwarded randomly to a neighbor which is not
necessarily nearer to the destination node. With the purpose
of transmitting all the packets to their destinations as
quickly as possible, the author in [39] has studied the
random algorithms for packet routing on grid networks.
These algorithms consider packet routing as well as packet
scheduling. In paper [40], the authors have tried to control
C (the maximum link congestion) and D (path length,
which is called dilation in the paper) simultaneously. The
network congestion (maximum link congestion) achieved
by the algorithm in this paper is O(dC*log(n)) and the
stretch (ratio of path length to the shortest path length) is
O(d2), where C* is the optimal congestion and d is the
dimension of the grid (mesh as used in the paper). Randomization is used in the algorithm.
Compared to the above studies, our work aims to minimize the maximum link congestion of the network. One of
our routing algorithms achieves the optimal path length, in
other words, the stretch (the ratio of the path length to the
shortest path length [40] ) of the selected path of a flow is 1
and it also keeps the maximum link congestion low. The
other routing algorithm has a higher stretch. However, in
some instances, it can keep the maximum link congestion
lower than the routing algorithms with the optimal stretch.

3 Network model and problem formulation
3.1 Network model
A grid network is represented by an undirected graph
G = (V, E), where V is the set of nodes and E is the set of
edges. For any node v 2 V, its coordinate is represented by
(vx, vy). By convention, the grid networks studied are
2-dimensional topology networks.
For a n1 9 n2 grid network, it means that the numbers of
nodes in any row and any column are n1 and n2, respectively. The flows in the network are represented by
F = {f1,…, fn}. For any flow fi 2 F, it is represented by a
pair of source and destination nodes {si,di} where si 6¼ di .

123

Wireless Netw

For any grid network, the origin point [with coordinates
(0, 0)] is in the left-up corner. The X-axis is the horizontal
line while the Y-axis is the vertical line. The X-coordinate
of a node is the horizontal distance (hops) from the node to
the origin point. Similarly, the Y-coordinate of a node is
the vertical distance (hops) to the origin point. Thus, for a
flow fi whose source node is si with coordinate ðsxi ; syi Þ and
destination node is di with coordinateðdix ; diy Þ, the horizontal distance between the two nodes is hi;x ¼ jdix  sxi j
and the vertical distance is hi;y ¼ jdiy  syi j. In Fig. 1, we
show an example of grid networks. The coordinates of
node v1 is (0,0). Thus, pick a node, say v3, its coordinates
are (1,1). The horizontal distance from v3 to v1 is 1, so is
the vertical distance.
Considering that by sacrificing path length, a routing
algorithm may achieve a lower maximum link congestion
compared to routing algorithms adopting shorter paths,
thus we study two routing problems, which are shortest
path minimum maximum link congestion (SPMM) and
non-shortest path minimum maximum link congestion
(NSPMM) routing problems.
In the SPMM routing problem, only the shortest paths
will be adopted. However, in the NSPMM problem, the
routing path of any flow fi is restricted within its submesh
Gi. The submesh Gi is defined in the following.
Definition 1 (The submesh Gi) For the flow fi with source
node si and destination node di, the submesh Gi includes all
the nodes and links in the rectangle with si and di as two
opposite diagonal nodes.
In Fig. 1, the submesh of fi is the one in the red (dashed)
rectangle, it includes nodes Vi = {v1, v2, v3, si, di, dj} and
links Ei = {(v1, v2), (v2, di), (v1, si), (v2, v3), (di, dj), (si, v3),
(v3, dj)}. Path p ¼ fsi ; v1 ; v2 ; v3 ; dj ; di g is a path restricted
within the submesh Gi, while path p0 = {si, sj, v4, v3, v2, di}
is a path beyond the submesh Gi.
The formal definitions of the two routing problems are
given as follows.
Definition 2 (SPMM Routing Problem) Given a grid
network G = (V, E) and a set of flows F = {f1,…, fn}. For
each flow fi 2 F, find a shortest path pi for it such that the
maximum link congestion of the network is minimized.
Definition 3 (NSPMM Routing Problem) Given a grid
network G = (V, E) and a set of flows F = {f1,…, fn}. For
each flow fi 2 F, in the submesh Gi, find a path pi for it
such that the maximum link congestion of the network is
minimized.
Now, we give the following definitions which will be
used frequently when describing the routing algorithms.

123

Definition 4 (Shortest Path) For any flow fi, a shortest
path pi between its source node si and destination node di
includes exactly jsxi  dix j þ jsyi  diy j links.
Definition 5 (Link Congestion) The congestion l(e) on
any bidirectional link e is the total number of paths
P
including link e, that is, lðeÞ ¼ fi 2F ri;e where ri;e is 1
when pi includes link e, otherwise, it is 0.
Definition 6 (Bottleneck Path Congestion) The bottleneck path congestion b(p) of path p is defined as the
maximum link congestion among all the links on p, that is,
bðpÞ ¼ maxe2p lðeÞ.
Definition 7 (Path Congestion) The path congestion of
path p is the summation of congestions of all the links
P
included by p, that is, wðpÞ ¼ e2p lðeÞ.

3.2 Problem formulation
3.2.1 The SPMM routing problem
The minimizing maximum link congestion routing problem can be formulated as an integer optimization programming. In the problem formulation, the undirected
graph can be viewed as a directed graph by transforming
each edge into two directed edges, let E0 represent these
edges.
Note that, for ðu; vÞ 2 E0 and ðv; uÞ 2 E0 , they are the
same edge in E, thus the congestion of a link in E is the
summation of congestions on two links in E0 .

Now let Cþ
v and Cv be the incoming and outgoing links
adjacent to node v 2 V, respectively. Let e0 be any directed
link in E0 . The problem then can be formulated as follows
and the variables we need to obtain are
ri;e0 ði ¼ 1;    ; n; e0 2 E0 Þ.
X
X
Min: Max:
ri;ðu;vÞ þ
ri;ðv;uÞ
ð1Þ
ðu;vÞ2E0

Subject to:
X
e0 2Cþ
v

X

ri;e0 

X
e0 2C
v

ri;e0

ðv;uÞ2E0

8
if
< 1
¼ 1
if
:
0 otherwise

v ¼ si
v ¼ di ; fi 2 F
ð2Þ

ri;e0 ¼ hxi þ hyi ; fi 2 F

ð3Þ

e0 2pi

ri;e0 2 f0; 1g;

i ¼ 1; . . .; n; e0 2 E0

ð4Þ

Constraints (2) ensure flow conservation for each flow
on each node. Constraints (3) ensure that each flow can

Wireless Netw

only adopt the shortest path. Constraints (4) ensure that
each variable is a 0–1 integer. The above integer optimization programming problem is a variation of the multicommodity flow problem. For the original multi-commodity flow problem, price-directed decomposition,
resource-directed decomposition and partition methods are
introduced in [40]. We refer the interested readers to [41]
for a much more detailed discussion.

Compared to the SPMM routing problem, instead of
choosing a shortest path for each flow, a path restricted
within the submesh is considered in the NSPMM routing
problem. It is ensured by constraints (7) where each node
v included by the path pi is in the submesh Gi.

3.2.2 The NSPMM routing problem

4.1 Routing algorithm: SPMM

The motivation to consider the NSPMM routing problem is
that, with the sacrificing of the path length, we may achieve
smaller maximum link congestion. An example is shown in
Fig. 2. In Fig. 2(a), the path of flow f3 is not a shortest path
and the network congestion is 1 after all the three flows
choosing their paths. While in Fig. 2(b), the path is a
shortest path, the network congestion is 2. The tradeoff for
a shortest path is the increasing of the network congestion.
The NSPMM routing problem is formalized as follows.
X
X
Min: Max:
ri;ðu;vÞ þ
ri;ðv;uÞ
ð5Þ

In this section, we design a shortest path routing algorithm
(SPMM) for grid networks. The ultimate goal is to minimize the maximum link congestion of the network.
Let Pi denote the set of shortest paths of flow fi. The
SPMM routing algorithm will select a path pi 2 Pi for it.
When selecting the path, the following two conditions
should be satisfied:

ðu;vÞ2E0

Subject to:
X

ri;e0 

X

ri;e0

e0 2C
v

e0 2Cþ
v

ðv;uÞ2E0

8
if
< 1
¼ 1
if
:
0 otherwise

v¼s
v ¼ di ; fi 2 F

v 2 Gi ; for v 2 pi

ð7Þ

i ¼ 1; . . .; n; e0 2 E0

ð8Þ

d1

0

1

d2

0

d3

jux  sxi j þ juy  syi j\jvx  sxi j þ jvy  syi j; u 2 Piv and u 2 Gi :

0

s1

1

0

(b)
0

1

d2

0

Fig. 2 NSPMM routing versus SPMM routing

d3

0

s2

0

0

0

0

0

0
s3

d1

The SPMM routing algorithm is a dynamic programming algorithm. Before discussing it in details, we need to
introduce the concept of effective parent nodes and effective child nodes.

0

s2

1

0

0

s1
0

0

0

s3

0

The bottleneck path congestion of pi is the minimum
among all the routing paths in Pi, that is, bðpi Þ ¼
minfbðpÞjp 2 Pi g.
Let P0i represent the set of paths in Pi whose bottleneck
path congestions are equal to b(pi), that is
P0i ¼ fpjbðpÞ ¼ bðpi Þ; p 2 Pi g. The path congestion of
pi is less than or equal to the minimum path congestion
among all the paths in P0i , that is, wðpi Þ 
minfwðpÞjp 2 P0i g.

Definition 8 (Effective Parent Nodes) When considering
the routing for flow fi, the set of effective parent nodes Piv
of node v are the nodes which locate in the submesh Gi and
are nearer to the source node si. The following conditions
are satisfied.

(a)
0

•

•

ð6Þ

ri;e0 2 f0; 1g;

4 Routing algorithms

Definition 9 (Effective Child Nodes) When considering
the routing for flow fi, the effective child nodes Cvi of node
v are the nodes which locate in the submesh Gi are nearer to
its destination node di. The following conditions are
satisfied:
jux  dix j þ juy  diy j\jvx  dix j þ jvy  diy j; u 2 Cvi and u
2 Gi :

0

The pseudocode of the SPMM routing algorithm is
shown in Algorithm 1. For different flows, the number of

123

Wireless Netw

shortest paths may be different. The number of shortest
!
hi;x þ hi;y
paths of flow fi is
. For any flow, the more the
hi;x
choices are, the more likely it can avoid the congested
links. It implies that the sequence of flows selecting paths
is important to minimize the maximum link congestion. In
algorithm 1, I(u) represents the id of node u. Each node in
the network has a specified id. We can label the nodes row
by row with increasing ids.
In line 1, we first sort the flows with the increasing
number of shortest paths.
For each flow fi 2 F, we need to do the process from
line 3 to line 28. Since a shortest path of flow fi is
within its submesh Gi, in line 3, we first get the submesh. The set S records the current visited nodes. The
set T includes the nodes to be visited in the next round.
It records the effective child nodes of the nodes in set S.
The set Y records the unvisited nodes in Vi. Let bðpsi ;v Þ
represent the bottleneck congestion of the selected path
from node si to node v, wðpsi ;v Þ represent the path congestion of the selected path from node si to node v. In
fact, path psi ;v can be viewed as a subpath of the final
selected path pi.
From line 4 to line 7, we first initialize all the sets, and
the values of bðpsi ;si Þ and wðpsi ;si Þ. Considering that in the
submesh Gi, the destination node di is the farthest node
from node si, so it’s the last node to be visited. Thus, if set
Y is not empty (which means node di hasn’t been visited),
we will keep doing the process from line 9 to line 19. In
this process, for each node in set T, from line 10 to line 14,
a precedent node pre(v) is selected for it from its effective
parent nodes. First, in line 10, we get the effective parent
nodes for node v. In line 11, we set the bottleneck path
congestion for the subpath from source node si to node v,
P0v;i records the parent nodes from which node v can obtain
a path to si with the minimum bottleneck path congestion.
In line 12, the algorithm updates the path congestion as the
minimum path congestion of the subpath to si and selecting
from P0v;i the nodes from which node v can obtain a path
whose path congestion is the minimum, P00v;i records these
parent nodes. If there’re more than one nodes in set P00v;i ,
select the one with the smaller node id as the precedent of
node v, as shown in line 13 and line 14. From line 16 to line
18, the algorithm updates the corresponding sets. From line
20 to line 25, we can simply construct the path from the
source node si to the destination node di by tracking the
precedents from node di until we reach the source node.
From line 26 to line 28, we increase the congestion of the
links in path pi by one.

123

4.2 Routing algorithm: NSPMM
In this section, we present the NSPMM routing algorithm, the
basic idea is the same as that of the SPMM routing algorithm.
The biggest difference between the two routing algorithms is
that the path for a flow found by NSPMM routing algorithm is
not always the shortest one. In order to control the path length,
the path found by NSPMM routing algorithm is restricted
within the submesh of each flow. The main process of the
NSPMM routing algorithm is described as follows.
•

We first sort the flows according to the areas of the
submeshes. The area Ai of the submesh of flow fi is
calculated by Ai ¼ ðdix  sxi Þ  ðdiy  syi Þ.

Wireless Netw

•

•

Then we find the paths with the minimum bottleneck
path congestion. For flow fi, the paths are a subset of P^i ,
which are represented by P^0i .
Finally we find the path in P^0i with the minimum total
path congestion, which is the one we need.

Unlike the SPMM routing algorithm, it’s hard to find the
efficient parent nodes and efficient child nodes in the
NSPMM routing problem. Thus, it’s not practical to design
a dynamic programming routing algorithm. However, by
introducing the concept of critical congestion for each flow,
we can design an efficient NSPMM routing algorithm. The
definition of the critical congestion is given as follows.

Now, the only problem we need to solve is how to get
the critical congestion for each flow. The authors in [43]
proposed the method solving the problem of finding the
maximum minimum bandwidth path. By slightly modifying the algorithm, we can use it to find the minimum
maximum link congestion of a submesh. This link congestion will be the critical congestion of the corresponding
flow.

Definition 10 (Critical Congestion) For flow fi 2 F, at the
time for it to find a routing path, for each link e 2 E, the link
congestion is l(e). Let li be a link congestion value.
Removing all the links with link congestions greater than li ,
there exists at least one path within the submesh Gi connecting node si and di. However, removing all the links with
link congestion equal to or greater than li , we can’t find any
path within the submesh Gi connecting node si and di.
The pseudocode of the NSPMM routing algorithm is
shown in Algorithm 2.
First, in line 1, all flows are sorted. For each flow fi 2 F,
we need to execute the process from line 3 to line 11. For
flow fi, the algorithm first get its submesh Gi (line 3), then
find its critical congestion in Gi (line 4). From line 5 to line
9, we remove the links in Gi with link congestion greater
than the critical congestion. Now, in the residual network,
all the links have link congestion equal to or less than the
critical congestion. In line 10, using Dijkstra’s algorithm
[42], we can easily get the path with the minimum total
path congestion. Note that, when running the Dijkstra’s
algorithm, we always pick the node with the smaller node
id when there are more than one nodes that have the
minimum path congestions in the paths from itself to the
source node. In line 11, the congestions of the links in path
pi are increased by one.

In order to make this paper self-contained and consider
that our objective is different from [43], we give the
pseudocode of how to get the critical congestion for a flow,
as shown in Algorithm 3. For the detailed description of the
algorithm, we refer the readers to paper [43]. As proved in
the paper, the time complexity of algorithm CriticalCongestion(Gi) is O(|Ei |).
4.3 Algorithm performance
Theorem 1 For a n1  n2 2-dimensional grid network
with n flows, the time complexity of Algorithm 1 is
O(nn1n2).
Proof Sorting flows will take O(nlog(n)) steps. In Algorithm 1, for any flow fi, getting the efficient parent nodes of
any node v can be accomplished with time complexity O(1)
(using the coordinates of nodes). From line 8 to line 19, for
flow fi, each node in the submesh is visited exactly once,
thus the time complexity is O(|Vi|). The constructing of the
routing path (line 20 to line 25) takes O(hi) steps, where hi

Table 1 initial configuration
Network
size

Number
of nodes

Number
of links

Original
number of
flows

Initial link
congestion

10 9 10
20 9 20

100
400

180
760

50
200

0
0

123

Wireless Netw
0

1

2

3

4

5

0

s9

s10

s11

s12

s13

s14

1

s15

s5

s3

s4

s8

s16

2

s17

s1

s6

s7

s2

s18

X

3

d18

d2

d8

d5

d1

d17

4

d16

d7

d4

d3

d6

d15

5

d14

d13

d12

d11

d10

d9

Theorem 2 For a n1 9 n2 2-dimensional grid network,
the time complexity of Algorithm 2 is O(nlog(n) ? nn1n2log(n1n2)) where n is the total number of flows.

Y
Fig. 3 An example of flow construction with network size 6 9 6

(a)

Proof First, sorting the n flows could be completed with
time complexity O(nlog(n)). As proved in [43] (please refer
to Theorem 1 in [43] ), the time complexity of finding the
critical congestion (Algorithm 3) for flow fi is O(|Ei|).
Subtracting the links in Ei with congestions greater than li
takes |Ei| steps. The time complexity of the Dijkstra’s
algorithm of finding the path with minimum path congestion takes O(|Ei| ? |Vi|log(|Vi|)) when implemented by

(b)
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

25

20

15

Average Maximum Link Congestion

Maximum Maximum Link Congestion

is the shortest path length for flow fi. From line 26 to line
28, we update the congestions of links in path pi, the time
complexity is O(|Ei|). The time complexity for flow fi to get
its routing path is O(|Vi| ? hi ?|Ei|). For a n1 9 n2 grid, for
n1 n2
2
any flow fi, we have that, jVi j  n1 n
2 ; hi 
2 ; and
jEi j  ðn1  1Þn2 þ n1 ðn2  1Þ.Thus, we have that jVi jþ
hi þ jEi j  Oðn1 n2 Þ. In total, there are n flows, thus the time
complexity of the SPMM routing algorithm is O(nn1n2).h

10

5

0

10 * 10

20 * 20

25
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

20

15

10

5

0

10 * 10

20 * 20

grid size

grid size

123

(a)
8000
NSPMM
SPMM

7000
6000
5000
4000
3000
2000
1000
0

1.4

1.6

1.8

2

grid size: 10*10

2.2

Distribution of the Average Link Congestion

Fig. 5 Distribution of the
average link congestion.
a Average link congestion for
the 10 9 10 grid. b Average
link congestion for the 20 9 20
grid

Distribution of the Average Link Congestion

Fig. 4 Maximum link congestion: partial permutation flows. a Maximum maximum link congestion. b Average maximum link congestion

(b)
8000
NSPMM
SPMM

7000
6000
5000
4000
3000
2000
1000
0

3

3.2

3.4

3.6

3.8

grid size: 20*20

4

4.2

Wireless Netw

60
50
40

(b)
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

Average Maximum Path Length

Maximum Maximum Path Length

(a) 70

30
20
10
0

10 * 10

20 * 20

70
60
50
40

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

30
20
10
0

10 * 10

grid size

14
12
10

(d)

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

Average Average Path Length

Maximum Average Path Length

(c) 16

8
6
4
2
0

10 * 10

20 * 20

grid size
14
12
10
8
6
4
2
0

20 * 20

grid size

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

10 * 10

20 * 20

grid size

Fig. 6 Path length: partial permutation flows. a Maximum maximum path length. b Average maximum path length. c Maximum average path
length. d Average average path length

priority queue [44]. Thus, for flow fi, the time complexity
of finding a path is,

5.1 Scenarios and metrics

OðjEi jÞþjEi jþOðjEi jþjVi jlogðjVi jÞÞ;

We compare the SPMM and NSPMM routing algorithms with other four popular routing algorithms in
grid networks, which are Row Column, ZigZag, Random and Greedy routing algorithms. Although we have
introduced these four routing algorithms in Sect. 2, we
still briefly describe them for a better understanding
here.

which is also O(|Ei| ? |Vi|log(|Vi|)). For a n1 9 n2 2dimensional grid, we have that jEi j  n1  n2 and
jVi j  n1  n2 . There are in total n flows which implies that
the time complexity of the routing algorithm is
O(nlog(n) ? nn1n2log(n1n2)).
h

•
5 Performance evaluation
We evaluate the routing algorithms in grid networks in this
section. All the algorithms are implemented with JAVA
language in Eclipse. The programs are running on a laptop
equipped with an Intle(R) Core(TM) i5-2410 M CPU
whose frequency is 2.30 GHz. And the size of the RAM is
2.0 GB. The operation system is Windows 7. We test the
algorithms in networks with size 10 9 10 and 20 9 20, the
initial configurations of the networks are shown in Table 1.

•

•

In the RowColumn routing algorithm, each packet will
be first transmitted along the row to the node in the
same column of the destination node, and then along
the column to the destination node.
In the ZigZag routing algorithm, packets will be first
transmitted along the row or column to the diagonal
node of the destination node, and then transmitted to
the destination node using a ZigZag shape path.
In the Random routing algorithm, each node will select
randomly a next-hop node from the legible neighbors
which is nearer to the destination nodes.

123

Wireless Netw

20

15

(b)
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

Average Maximum Ratio

Maximum Maximum Ratio

(a) 25

10

5

0

10 * 10

25

20

15

10

5

0

20 * 20

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

10 * 10

grid size

(d) 25

15

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

Average Average Ratio

Maximum Average Ratio

(c) 25
20

10

5

0

20 * 20

grid size

10 * 10

20 * 20

grid size

20

15

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

10

5

0

10 * 10

20 * 20

grid size

Fig. 7 Ratio: partial permutation flows. a Maximum maximum ratio. b Average maximum ratio. c Maximum average ratio. d Average average
ratio

•

In the Greedy routing algorithm, each node will send
the packet to the next-hop node which has the smallest
congestion.

Besides the maximum link congestion and average link
congestion, we are also interested in the following
metrics.
•

•
•

Path Length For a single path p, the path length hðpÞ ¼
jfeje 2 pgj which is the number of links included by the
path.
Path Congestion It has been introduced in Sect. 3.
Path Congestion to Path Length Ratio For a single path
p, the path congestion to path length ratio is expressed
as rðpÞ ¼ wðpÞ
hðpÞ .

•
•

Network delay The average delay of all the flows is also
important.
Network throughput We estimate the network throughput by calculate the number of packets received within
a time duration.

123

For each setting, we run the simulation 20,000 times. Note
that, in the expression ‘‘Maximum Maximum’’, the first
‘‘Maximum’’ means the maximum value among the 20,000
runs, and the second ‘‘Maximum’’ means the maximum value
among flows generated in a single instance. Other expressions,
such as ‘‘Maximum Average’’, ‘‘Average Maximum’’ and
‘‘Average Maximum’’ follow the similar explanations.
All the above metrics are evaluated under three traffic
patterns. The Partial Permutation Traffic Pattern and
General Traffic Pattern have been introduced in the previous section. Here, we introduce the Specific Traffic
Pattern (W) which we believe will cause very high maximum link congestion of a network. The following construction of the traffic pattern Wn1 ;n2 is only suitable for
grids with size n1 9 n2 where n1 C 4 and n2 C 4. The
concept of constructing the traffic patterns W can be
extended to other situations easily.
The coordinates of the nodes in a grid network have
been introduced in Sect. 3. The construction of the specific

Wireless Netw

(b)
350
300
250
200

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

150
100
50
0

10 * 10

20 * 20

Average Maximum Path Congestion

Maximum Maximum Path Congestion

(a)

350
300
250
200

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

150
100
50
0

10 * 10

grid size

(c)

(d)
100

80

60

Average Average Path Congestion

Maximum Average Path Congestion

20 * 20

grid size

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

40

20

0

10 * 10

20 * 20

grid size

100

80

60

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

40

20

0

10 * 10

20 * 20

grid size

Fig. 8 Path congestion: partial permutation flows. a Maximum maximum path congestion. b Average maximum path congestion. c Maximum
average path congestion. d Average average path congestion

traffic pattern composes two steps. In step1, we need to
construct the four special flows. And in step2, for the
remaining nodes, we just construct a symmetric traffic
pattern.
1.

Step1: construct the special flows.
 
 
Now let x ¼ n21 and y ¼ n22 . As shown in Fig. 3,
this is the node d5 with coordinates (3, 3).

•

•

•

•

For node ðx ; y Þ, the coordinate of the corresponding
source node is ðx  2; y  2Þ(which is node s5 in
Fig. 3);
For node ðx ; y  1Þ(which is node s7 in Fig. 3), the
coordinate of the corresponding destination node is
ðx  2; y þ 1Þ(which is node d7 in Fig. 3);
For node ðx  1; y Þ(which is node d8 in Fig. 3), the
coordinate of the corresponding source node is
ðx þ 1; y  2Þ(which is node s8 in Fig. 3);
For node ðx  1; y  1Þ(which is node s6 in Fig. 3),
the coordinate of the corresponding source node is
ðx þ 1; y þ 1Þ(which is node d6 in Fig. 3).

2.

Step2: construct the symmetric traffic pattern.

For the other nodes, the source and destination nodes are
paired according to the following rules.
•

•

•

When n1 and n2 are both even. If a node with
coordinate ðsxi ; syi Þðsyi  y Þ is set to be a source/destination node, then the destination/source node is set to
be the one with coordinate ðn1  1  sxi ; n2  1  syi Þ.
When one of the values of n1 and n2 is odd. Without
loss of generality, let n1 be odd. If a node with
coordinate ðsxi ; syi Þðsxi  x Þ is set to be a source/destination node, then the destination/source node is set to
be the one with coordinateðn1  2  sxi ; n2  1  syi Þ.
After that, the remaining nodes can be arbitrarily
paired.
When both the values of n1 and n2 are odd. If a node
with coordinate ðsxi ; syi Þ (sxi  x and syi  n2  1) is set
to be a source/destination node, then the destination/
source node is set to be the one with coordinate
ðn1  2  sxi ; n2  2  syi Þ. After that, the remaining

123

Wireless Netw

(b)
Average Maximum Link Congestion

(a)
Maximum Maximum Link Congestion

Fig. 9 Maximum link
congestion: general flows.
a Maximum maximum link
congestion: 10 9 10. b Average
maximum link congestion:
10 9 10 c Maximum maximum
link congestion: 20 9 20.
d Average maximum link
congestion: 20 9 20

150
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

100

50

0

50

100

150

200

250

300

350

400

Number of Flows with Network Size 10*10
150
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

100

50

0
200

400

600

800 1000 1200 1400 1600

Number of Flows with Network Size 20*20

nodes are the ones on the bottom row and right-most
column. For a source node among this has the
coordinate ðsxi ; syi Þ, then the destination node coordinate
is ðsyi ; sxi Þ.
Note that, for a grid n1 9 n2 with n1 \ 4 or n2 \ 4, the
construction of a special traffic pattern is a little bit different from the above process. In Fig. 3, the Specific
Traffic Pattern of a grid network with size 6 9 6 is given.
As shown in Fig. 3, the 4 9 4 center grid is shown in the
dashed square. And for all grids, the construction of the
most central 4 9 4 grid is the same.
5.2 Performance evaluation under the partial
permutation traffic pattern
Figure 4 shows the maximum maximum link congestion
and average maximum link congestion of six routing
algorithms with network sizes 10 9 10 and 20 9 20. The
SPMM and NSPMM routing algorithms show the identical
results. The maximum and average maximum link congestions of these two routing algorithms are much smaller
than those of the other four routing algorithms. For the

123

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

100

50

0
50

100

150

200

250

300

350

400

Number of Flows with Network Size 10*10

(d)
Average Maximum Link Congestion

Maximum Maximum Link Congestion

(c)

150

150
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

100

50

0
200

400

600

800 1000 1200 1400 1600

Number of Flows with Network Size 20*20

SPMM and NSPMM routing algorithms, the improvement
ratios of the maximum maximum link congestions are
around 50, 65, 45, and 62 % compared to RowColumn,
ZigZag, Greedy, Random routing algorithms, respectively.
The average link congestions of all the shortest path
routing algorithms under a certain traffic pattern should be
the same. Thus, we only need to compare the average link
congestions of the non-shortest path routing algorithm with
the shortest path routing algorithms. In Fig. 5, it shows the
distribution of the average link congestions in grids with
size 10 9 10 and 20 9 20. The frequencies of SPMM
routing algorithm are higher than those of NSPMM routing
algorithm for small link congestions. However, for large
link congestions, the frequencies of SPMM routing algorithm are smaller than those of NSPMM routing algorithm.
It’s because of the longer path length generated by
NSPMM. The long path lengths may not lead to a larger
maximum link congestion (which can be seen from Fig. 4),
however they do increase the average link congestion.
Considering that the SPMM, RowColumn, ZigZag,
Random and Greedy routing algorithms are all shortest
path routing algorithms, the maximum/average path length
of each flow under these five routing algorithms are the

(a) 40

(b) 17.5
Average Maximum Path Length

Fig. 10 Maximum path length:
general flows. a Maximum
maximum path length: 10 9 10.
b Average maximum path
length: 10 9 10 c Maximum
maximum path length: 20 9 20.
d Average maximum path
length: 20 9 20

Maximum Maximum Path Length

Wireless Netw

35
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

30

25

20

15
50

100

150

200

250

300

350

400

17
16.5
16
15.5
15
14.5
50

Number of Flows with Network Size 10*10

150

200

250

300

350

400

(d) 38

90
80
70

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

60
50
40
30
200

100

Number of Flows with Network Size 10*10

Average Maximum Path Length

Maximum Maximum Path Length

(c)

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

400

600

800 1000 1200 1400 1600

Number of Flows with Network Size 20*20

same, which can be seen in Fig. 6. The differences of path
lengths between the non-shortest path routing algorithm
and the shortest path routing algorithms can be seen clearly
in Fig. 6. Under both the 10 9 10 and 20 9 20 grids, the
maximum maximum path lengths, average maximum path
lengths, maximum average path lengths and average
average path lengths of the NSPMM routing algorithm are
larger than those of the five shortest path routing
algorithms.
In Fig. 7, we show the path congestion to path length
ratios. It’s obvious that the maximum maximum, average
maximum, maximum average, and average average ratios
of the NSPMM and SPMM routing algorithms are smaller
than all the other four routing algorithms. The ratio reflects
the average link congestion for a path. The smaller the ratio
of a path is, the lighter the average link congestion the path
has. Still, the differences of the ratios between NSPMM
and SPMM routing algorithms are very small. However,
they do achieve different performances for some other
metrics. This can be seen in Fig. 8(a), the maximum
maximum path congestions of the NSPMM routing algorithm under grid networks with sizes 10 9 10 and 20 9 20
are very high. For SPMM routing algorithm, the values in

37
36
35
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

34
33
32
200

400

600

800 1000 1200 1400 1600

Number of Flows with Network Size 20*20

all the four subfigures are the smallest. Recall the maximum maximum path lengths in Fig. 6(a). The main reason
is that the long path lengths lead to the higher path congestions. In a network, let the minimum path congestion be
lm and the maximum path congestion be lM. By increasing a
path length by 1, the increasing congestion is at least lm ?1,
and in some cases, the path congestion may be increased by
lM ?1. Thus, the path length and network link congestion
play important parts in order to achieve smaller path
congestion.
5.3 Performance evaluation under the general traffic
pattern
For the scenarios with general traffic patterns, we run the
simulation for 20,000 times with varying the number of
flows. For the 10 9 10 grid, we vary the number of flows
from 50 to 400 with an increment of 50. For the 20 9 20
grid, we vary the number of flows from 200 to 1,600 with
an increment of 200.
In Fig. 9, we show the maximum maximum link congestions, average maximum link congestions for the grids
with sizes 10 9 10 and 20 9 20. The values increase with

123

Wireless Netw

(a) 150

(b)
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

100

Average Maximum Ratio

Maximum Maximum Ratio

Fig. 11 Maximum ratio:
general flows. a Maximum
maximum ratio: 10 9 10.
b Average maximum ratio:
10 9 10 c Maximum maximum
ratio: 20 9 20. d Average
maximum ratio: 20 9 20

50

0
50

100

150

200

250

300

350

Number of Flows with Network Size 10*10

Average Maximum Ratio

Maximum Maximum Ratio

100

50

0
200

400

600

800 1000 1200 1400 1600

Number of Flows with Network Size 20*20

the increasing of the number of flows. The NSPMM and
SPMM routing algorithms achieve the smallest maximum
maximum and average maximum link congestions. Like
the performances under the partial permutation traffic
patterns, the NSPMM and SPMM routing algorithms
achieve the identical maximum and average maximum link
congestions. This phenomenon motivates us to observe the
path lengths under these two routing algorithms. As shown
in Fig. 10, in all the four sub-figures, the path lengths of the
NSPMM routing algorithm are larger than those of the
other five shortest path routing algorithms. And the path
lengths of all the shortest path routing algorithms are the
same.
For the path congestion to path length ratios shown in
Fig. 11, the best performance routing algorithms are the
NSPMM and SPMM routing algorithms. Also, differences
between the two routing algorithms are not obvious under
this metric. However, in Fig. 12, the differences are
obvious. The maximum path congestions of the NSPMM
routing algorithm become very high. However, the
SPMM routing algorithm is still the best under this
metric.

123

50

100

150

200

250

300

350

400

Number of Flows with Network Size 10*10

(d)
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

100

0
50

400

(c) 150

150

150
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

100

50

0
200

400

600

800 1000 1200 1400 1600

Number of Flows with Network Size 20*20

5.4 Performance evaluation under the specific traffic
pattern
Now, given the evaluation of the performances of the
routing algorithms under the partial permutation and general traffic patterns, we’d like to predict that the SPMM and
NSPMM routing algorithms will still achieve the best
performances of the maximum link congestions and the
path congestion to path length ratios under the Specific
Traffic Pattern. The path congestions of the NSPMM
routing algorithm should be bad.
The prediction of the maximum link congestions and
path congestion to path length ratios are correct, as shown
in Figs. 13 and 14. However, for this traffic pattern, the
path congestions of the NSPMM are not the worst, as
shown in Fig. 15(a) and (b).
Besides the previous observations, there is something
interesting about the maximum link congestion which is
the main motivation for us to study this traffic pattern. For
the 10 9 10 and 20 9 20 grids, the maximum link congestions under this traffic pattern is 6 and 11, respectively.
When constructing 20,000 instances of partial permutation

Wireless Netw

(b)

Maximum Maximum Path Congestion

(a)
600
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

500
400
300
200
100
0
50

100

150

200

250

300

350

400

Average Maximum Path Congestion

Fig. 12 Maximum path
congestion: general flows.
a Maximum maximum path
congestion: 10 9 10. b Average
maximum path congestion:
10 9 10 c Maximum maximum
path congestion: 20 9 20.
d Average maximum path
congestion: 20 9 20

Number of Flows with Network Size 10*10

(d)

2500

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

2000

1500

1000

500

0
200

400

600

800 1000 1200 1400 1600

Number of Flows with Network Size 20*20

Maximum Link Congestion

30
25
20

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

15
10
5
0
10 * 10

400
300
200
100
0
50

100

150

200

250

300

350

400

2500
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

2000

1500

1000

500

0
200

400

600

800 1000 1200 1400 1600

Number of Flows with Network Size 20*20

smaller than the corresponding values under the Specific
Traffic Pattern.
Although we can’t prove it, we strongly believe that the
specific traffic pattern is a kind of worst-case traffic pattern
for our routing algorithms, which are NSPMM and SPMM.
And for a n1 9 n2 grid network, the maximum link con
 

gestion under this traffic pattern is maxf n12þ1 ; n22þ1 g. If

 

the guessing is true, maxf n12þ1 ; n22þ1 g will be the upper
bound of the maximum link congestion under the NSPMM
and SPMM routing algorithms for the Partial Permutation
Traffic Patterns.

40
35

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

500

Number of Flows with Network Size 10*10
Average Maximum Path Congestion

Maximum Maximum Path Congestion

(c)

600

20 * 20

5.5 Delay and throughput

grid size
Fig. 13 Link congestion: specific traffic pattern

traffic patterns randomly, the maximum link congestions
[refer to Fig. 4(a)] among the 20,000 instances is 5 and 7
for the 10 9 10 and 20 9 20 grids, respectively. Both are

Besides the above congestion related performance metrics,
delay and throughput are also importance metrics to estimate the network performance. However, the delay and
throughput are related to the path length, link congestion of
the flow path, and also the scheduling algorithm adopted
when transmitting packets.
Here, we choose the simple FCFS (First Come First
Serve) scheduling algorithm and run 20,000 instances for

123

Wireless Netw

(a) 40

(b)
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

30
25
20

30

15

25
20
15

10

10

5

5

0

10 * 10

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

35

Average Ratio

Maximum Ratio

35

40

0

20 * 20

10 * 10

grid size

20 * 20

grid size

Fig. 14 Ratio: specific traffic pattern. a Maximum ratio. b Average ratio

400

300

(b) 500
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

Average Path Congestion

Maximum Path Congestion

(a) 500

200

100

0

10 * 10

20 * 20

400

300

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

200

100

0

10 * 10

grid size

20 * 20

grid size

Fig. 15 Path congestion: specific traffic pattern. a Maximum path congestion. b Average path congestion

each setting. In this section we only show the results of the
general traffic pattern considering that the partial permutation and the specific traffic pattern are special cases of the
general traffic pattern.
We show the average delay of the network among all the
flows by varying the number of flows in Fig. 16. Also, we
choose the network size to be 10 9 10 and 20 9 20. From
the figures, we can see that for the average average delay,
the NSPMM and SPMM routing algorithms achieve the
shortest delays. And for the average maximum delay, the
SPMM routing algorithm achieves the lowest delay, and
the NSPMM routing algorithm achieves the second lowest
delays.
The throughputs of different routing algorithms are
shown in Fig. 17. For the 10 9 10 grid, we choose the
delay bound to be 10 slots and 20 slots. For the 20 9 20
grid, the delay bounds are chosen to be 20 slots and 40
slots. When increasing the number of packets, the SPMM

123

and NSPMM routing algorithms achieve the most number
of packets successfully received within the specific delay
bound compared to other routing algorithms, except in
Fig. 17(c) when the number of flows is 1,600. The reason
for this is that the network is in a congested state.
5.6 Discussion
5.6.1 Maximum link congestion
Under the Partial Permutation Traffic Pattern, a guessing of
the upper bound of the maximum link congestion achieved
by the NSPMM and SPMM routing algorithms is

 

maxf n12þ1 ; n22þ1 g in a n1 9 n2 grid network. If the
specific traffic pattern is a kind of worst-case traffic pattern,
the upper bound will be true.
Through the above numerical results, we know that the
NSPMM routing algorithm sacrifices path length to achieve

(a) 18
Average Average delay

Fig. 16 Delay performance.
a Average average delay:
10 9 10. b Average maximum
delay: 10 9 10. c Average
average delay: 20 9 20.
d Average maximum delay:
20 9 20

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

16
14
12
10
8
6

50

100

150

200

250

300

350

(b)

45

Average Maximum delay

Wireless Netw

40

Number of Flows with Network Size 10*10

25
20

100

150

200

250

300

350

400

(d) 90
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

30

25

20

15

10
200

30

Number of Flows with Network Size 10*10

Average Maximum delay

Average Average delay

(c) 35

35

15
50

400

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

400

600

800 1000 1200 1400 1600

Number of Flows with Network Size 20*20

smaller maximum link congestion. However, this disadvantage can be avoided by the SPMM routing algorithm
since it achieves the smallest maximum link congestion
and uses only the shortest paths.

NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

80
70
60
50
40
30
200

400

600

800 1000 1200 1400 1600

Number of Flows with Network Size 20*20

notice from the flow with priority i - 1, it then can start
the path seeking process.

6 Conclusions
5.6.2 Algorithm implementation
The implementation of the SPMM routing algorithm is
much easier than the NSPMM routing algorithm, it can be
implemented distributed. The only information that should
be informed to the source of each flow is when it should
start to seek the routing path. This can be achieved if all the
flows first broadcast a packet telling others the number of
its shortest paths. According to this information, each flow
knows its priority which determines when it can seek the
routing path. When a flow finishes the path seeking process, it then broadcasts a packet to inform other flows. If a
flow with priority i receives the routing path completion

In this paper, we have proposed the NSPMM and SPMM
routing algorithms aiming to minimize the maximum link
congestion in grid networks. By evaluating these two
routing algorithms under the Partial Permutation Traffic
Pattern, General Traffic Pattern and the Specific Traffic
Pattern, we know that these two routing algorithms can
achieve smaller link congestions compared to RowColumn,
ZigZag, Random and Greedy routing algorithms. However,
the NSPMM routing algorithm has sacrificed the path
length and path congestion. And the delay and throughput
performance of the SPMM and NSPMM routing algorithms are better than the other four routing algorithms.

123

Wireless Netw

(b)
140
120
100
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

80
60
40
20
50

100

150

200

250

300

350

400

Average Average Number of Packets

(a)
Average Average Number of Packets

Fig. 17 Throughput
performance. a Average average
number of packets: 10 9 10, 10
slots. b Average average
number of packets: 10 9 10, 20
slots. c Average number of
packets: 20 9 20, 20 slots.
d Average average number of
packets: 20 9 20, 40 slots

Number of Flows with Network Size 10*10

250
200
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

150
100
50
0
50

100

150

200

250

300

350

400

(d)
500
450
400
350
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

300
250
200
150
200

400

600

800 1000 1200 1400 1600

Number of Flows with Network Size 20*20

Overall, the SPMM routing algorithm is the most efficient
routing algorithm.
Acknowledgments This work was supported by the National Hightech R&D Program of China (863 Program) (Grant No.
2012AA010904), and the Scientific Research Fund of Sichuan
Province, China (Grant Nos. 2013GZ0016, 13ZA0296). And our
thanks to the China Scholarship Council (CSC) for the support for the
Joint Ph.D. Program.

References
1. Awduche, D., Chiu, A., Elwalid, A., Widjaja, I., & Xiao, X.
(2002). Overview and principles of internet traffic engineering.
Tech. rep., RFC 3272.
2. Siripongwutikorn, P., Banerjee, S. & Tipper, D. (2002). Traffic
engineering in the internet: A survey of load balanced routing.
White paper.
3. Ravindra Kumar Singh, N. S. C., & Saxena, K. (2012). Load
balancing in ip/mpls networks: A survey. Computer Science and
Communications, 4(2), 151–156.
4. Askarian, C., & Beigy, H. (2012). A survey for load balancing in
mobile wimax networks. Advanced Computing: An International
Journal, 3(2), 119–137.
5. Wajgi, D., & Thakur, N. V. (2012). Load balancing algorithms in
wireless sensor network: A survey. International Journal of
Computer Networks and Wireless Communications (IJCNWC), 2,
456–460.

123

300

Number of Flows with Network Size 10*10
Average Average Number of Packets

Average Average Number of Packets

(c)

350

1400
NSPMM
SPMM
RowColumn
ZigZag
Greedy
Random

1200
1000
800
600
400
200
0
200

400

600

800 1000 1200 1400 1600

Number of Flows with Network Size 20*20

6. Yao, Y., Cao, Q., & Vasilakos, A. V. (2013). EDAL: An energyefficient, delay-aware, and lifetime-balancing data collection
protocol for wireless sensor networks. In: MASS (pp. 182–190).
7. Shen, Zhijie, et al. (2011). Peer-to-peer media streaming: Insights and
new developments. Proceedings of the IEEE, 99(12), 2089–2109.
8. Suri, P. K., & Kaur, S. (2012). A survey of load balancing
algorithms in manet. Engineering Science and Technology: An
International Journal, 2(3), 495–504.
9. Maheshwari, D., & Nedunchezhian, R. (2012). Load balancing in
mobile ad hoc networks: A survey. International Journal of
Computer Applications, 59(16), 44–49.
10. Kelly, F. P., Maulloo, A. K., & Tan, D. K. (1998). Rate control
for communication networks: Shadow prices, proportional fairness and stability. Journal of the Operational Research Society,
49(3), 237–252.
11. Firoiu, V. & Borden, M. (2000). A study of active queue management for congestion control. In INFOCOM 2000. Nineteenth
Annual Joint Conference of the IEEE Computer and Communications Societies. Proceedings. IEEE (Vol. 3, pp. 1435–1444).
12. Li, Mo, et al. (2013). A survey on topology control in wireless
sensor networks: Taxonomy, comparative study, and open issues.
Proceedings of the IEEE, 101(12), 2538–2557.
13. Chen, L., Low, S. H. & Doyle, J. C. (2005). Joint congestion
control and media access control design for ad hoc wireless
networks. In INFOCOM 2005. 24th Annual Joint Conference of
the IEEE Computer and Communications Societies. Proceedings
IEEE (Vol. 3, pp. 2212–2222).
14. Kewei Sha, J. G., & Greve, J. G. (2013). Multipath routing
techniques in wireless sensor networks: A survey. Wireless Personal Communications, 70(2), 807–829.

Wireless Netw
15. Radi, M., Dezfouli, B., Bakar, K. A., & Lee, M. (2012). Multipath routing in wireless sensor networks: Survey and research
challenges. Sensors, 12(1), 650–685.
16. Piratla, N. M. & Jayasumana, A. P. (2006). Reordering of packets due
to multipath forwarding-an analysis. In ICC’06 (Vol. 2, pp. 829–834).
17. Robinson, J. & Knightly, E. W. (2007). A performance study of
deployment factors in wireless mesh networks. In INFOCOM
2007. 26th IEEE International Conference on Computer Communications. IEEE (pp. 2054–2062).
18. Busch, C., Kannan, R., & Samman, A. (2012). Bottleneck routing
games on grids. In Game Theory for Networks (Vol. 75 LNICST,
pp. 294–307).
19. Leighton, F. T., Maggs, B. M., & Rao, S. B. (1994). Packet
routing and job-shop scheduling in O (congestion ? dilation)
steps. Combinatorica, 14(2), 167–186.
20. Banner, R., & Orda, A. (2007). Bottleneck routing games in
communication networks. IEEE Journal on Selected Areas in
Communications, 25(6), 1173–1179.
21. Busch, C., Kannan, R., & Vasilakos, A. V. (2008) Quality of
routing congestion games in wireless sensor networks. In Proceedings of the 4th Annual International Conference on Wireless
Internet. no. p. 71.
22. Busch, C., & Magdon-Ismail, M. (2009). Atomic routing games
on maximum congestion. Theoretical Computer Science,
410(36), 3337–3347.
23. Rajgopal Kannan,et al. ‘‘Optimal Price of Anarchy of Polynomial
and Super-Polynomial Bottleneck Congestion Games.’’ GAMENETS. pp. 308-320, 2011.
24. Busch, Costas, et al. (2012). Approximating congestion ? dilation in networks via ‘quality of routing’ games. IEEE Transactions on Computers, 61(9), 1270–1283.
25. Spyropoulos, T., et al. (2010). Routing for disruption tolerant networks: Taxonomy and design. Wireless Networks, 16(8), 2349–2370.
26. Zeng, Y., et al. (2013). Directional routing and scheduling for
green vehicular delay tolerant networks. Wireless Networks,
19(2), 161–173.
27. Youssef, M., et al. (2014). Routing metrics of cognitive radio
networks: A survey. IEEE Communications Surveys and Tutorials, 16(1), 92–109.
28. Liu, Y., et al. (2010). Multi-layer clustering routing algorithm for
wireless vehicular sensor networks. IET Communications, 4(7),
810–816.
29. Chen, Kai, et al. (2011). Survey on routing in data centers:
Insights and future directions. IEEE Network, 25(4), 6–10.
30. Li, P., Guo, S., Yu, S., & Vasilakos, A. V. (2012). CodePipe: An
opportunistic feeding and routing protocol for reliable multicast
with pipelined network coding. In INFOCOM (pp. 100–108).
31. Demestichas, Panagiotis, et al. (2004). Service configuration and
traffic distribution in composite radio environments. IEEE Transactions on Systems, Man, and Cybernetics, Part C, 34(1), 69–81.
32. Britta, P., Martin, S., & Andreas, W. (2010). Packet routing on
the grid. In Theoretical Informatics—9th Latin American Symposium, Proceedings (Vol. 6034 LNCS, pp. 120–130).
33. Badr, H. G., & Podar, S. (1989). An optimal shortest-path routing
policy for network computers with regular mesh-connected topologies. IEEE Transactions on Computers, 38(10), 1362–1371.
34. Weller, T., & Hajek, B. (1994). Comments on’’an optimal
shortest-path routing policy for network computers with regular
mesh-connected topologies. IEEE Transactions on Computers,
43(7), 862–863.
35. Wu, J. (1999). Maximum-shortest-path (msp): An optimal routing
policy for mesh-connected multicomputers. IEEE Transactions
on Reliability, 48(3), 247–255.

36. Takatsu, S., Ooshita, F., Kakugawa, H., & Masuzawa, T. (2013).
Zigzag: Local-information-based self-optimizing routing in virtual grid networks. In International Conference on Distributed
Computing Systems, 33rd IEEE (pp. 357–368).
37. Liu, J. W. (2000). Real-time systems (pp.115–189). Upper Saddle
River, NJ: Prentice Hall PTR.
38. Dhanapala, D. C., Jayasumana, A. P., & Han, Q. (2009). Performance of random routing on grid-based sensor networks.
CCNC, 2009, 1–5.
39. Rajasekaran, S. (1991). Randomized algorithms for packet routing on the mesh. Technical Reports (CIS). Paper 328. http://
repository.upenn.edu/cis_reports/328.
40. Busch, C., Magdon-lsmail, M., & Xi, J. (2008). Optimal oblivious
path selection on the mesh. IEEE Transactions on Computers,
57(5), 660–671.
41. Ahuja, R. K., Magnanti, T. L., & Orlin, J. B. (1993). Network
flows: Theory, algorithms, and applications (pp. 649–684). Upper
Saddle River, NJ: Prentice Hall PTR.
42. Cormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2001).
Introduction to algorithms (Vol. 2). Cambridge: MIT press.
43. Kaibel, V., & Peinhardt, M. (2006). On the bottleneck shortest
path problem. Technical Reports.
44. Fredman, M. L., & Tarjan, R. E. (1987). Fibonacci heaps and
their uses in improved network optimization algorithms. Journal
of the ACM (JACM), 34(3), 596–615.

industry control
communications.

networks,

Jun Xu received her Bachelor’s
degree in the School of Electronic Information, Wuhan
University, China, in 2009. She
is currently a Ph.D. candidate in
the School of Electronic Information, Wuhan University,
China. And she is also a visiting
scholar in the School of Computing, Informatics, and Decision Systems Engineering,
Arizona State University, USA,
from Sep. 2012 to Mar. 2014.
Her research interests are in the
area of wireless mesh networks,
and real-time and reliability

Jianfeng Yang received his
Bachelor’s degree in Electronic
Information in 1998, Master’s
degree in Signal and Information System in 2003 and Ph.D.
degree in Communication and
Information System in 2009, all
from the school of Electronic
Information, Wuhan University,
China. He is Currently an
Associate Professor of Wuhan
University, his research interests
are in the areas of Embedded
System, Wireless Mesh Networks, Parallel Computing and
real-time and Reliability Communications. Also, he severed as the
committee member of China Computer Federation Technical Committee of Embedded System from 2010.

123

Wireless Netw
Chengcheng Guo received his
Ph.D. degree in the School of
Electronic and Information,
Wuhan University, China. He
received his Bachelor and Master degree in the Computer
School of Wuhan University,
China. He is currently a professor and a Ph.D. supervisor in the
School of Electronic and Information, Wuhan University,
China. His research interests are
Internet and Communication
technology, wireless mesh networks, industry control networks, and real-time and reliability communications.
Yann-Hang Lee received his
Ph.D. degree in Computer,
Information, and Control Engineering from the University of
Michigan, Ann Arbor, MI, in
1984. He is currently a professor
in the School of Computing,
Informatics, and Decision Systems Engineering, Arizona State
University, USA. Since 2008,
he has served as the Program
Chair of the Computer Science
and Engineering Program in the
School. Dr. Lee’s research
interests are in the areas of real-

123

time computing, embedded systems, software engineering, distributed
systems, and performance evaluation.
Duo Lu received his Bachelor’s
degree in Shanghai Jiao Tong
University, China, in 2009. He
is currently a Ph.D. student in
the School of Computing,
Informatics, and Decision Systems Engineering, Arizona State
University, USA. His research
interests are in the area of realtime embedded system and realtime wireless network.

2009
2009 International
International Conferences
Conference on
on Embedded
Embedded Software
Software and
and Systems
Systems

Ontology-based Smart Home Solution and Service Composition
Jingjing Xu1, Yann-Hang Lee1, Wei-Tek Tsai1, Wu Li1, Young-Sung Son2, Jun-Hee Park2, and
Kyung-Duk Moon2
1

Computer Science and Engineering Dept.
Arizona State University
Tempe, AZ 85287

2

Electronics and Telecommunications Research
Institute
Daejeon, South Korea

Abstract

Ontology has received significant attention in recent
years with the emergence of semantic web [1][2][3][6].
It is an explicit specification of conceptualizations
which organizes the semantic information as the
knowledge base of the specific application domains
[4][5][6]. Furthermore, its use is extended to ServiceOriented Computing (SOC) to facilitate more
intelligent service discovery and composition [6][8].
The current ontology languages, such as RDF and
OWL [9][10], are often represented in XML and can
be processed by machines. They support the
specification of concepts, relationships, and associated
classification and reasoning mechanisms for data.
Ontology has been established for context-aware and
smart home applications [11]. For instance, in [12], a
Getting up scenario is described to show whether the
ontology-based model is valuable for description of
context information in a home domain. In [12], an
ontology-based model of the Tele-health Smart Home
to initialize the Bayesian networks to recognize patient
activity. Furthermore, ontology is used to model the
relations between devices and services in a home
environment and to manage software configurations
[14].
In our smart home solution, ontology is introduced
to deal with the three issues stated above base on the
semantic information. Note that it is possible that a
requested function cannot be fulfilled by a single
device but a set of devices working together. Hence, in
addition to the specification of static relationships
between concepts in ontology systems, we introduce
the application template [15] for the service
composition specification. This paper presents our
investigation of constructing an ontology-based
knowledge representation for the semantic content and
process model of smart home applications. In the
following section, we propose an ontology-based
architectural design of smart home. In Section 3, we
go through the details of handling different scenarios
that may occur in a smart home system. To illustrate
the automatic function plan generation base on
semantic information, a temperature control
application scenario is used as case study in Section 4.
Finally, the conclusion is made in Section 5.

Given the diversity of home environment,
appliances, and residents, the applications for smart
homes must be configurable and adaptive. Instead of
programming each household, we propose an
ontology-based framework to facilitate the automatic
composition of appropriate applications. The system
composes appropriate services depending upon the
available equipments in each individual household
automatically. Meanwhile, it dynamically adjusts the
environment parameters to match the customer needs
and to encompass the available resource. With its
supporting on customized function template editing,
customers are able to specify their usual behavior
templates as different mode.

1 Introduction
Smart homes have emerged as a focused application
area of distributed embedded systems where digital
appliances, sensor devices, PDAs, and handheld
computers are integrated to facilitate intelligent
services for households. Smart home is different than
the traditional home on its ability to perform functions
by integrating appropriate appliances automatically.
Besides the automation, it also considers the different
habits of the residences and adjusts the setting
accordingly. For instance, to older people, the home
temperature should not be too low. However, for a
party, the guests will get sweaty with such a temporary
level.
To realize the aforementioned intelligence in smart
homes, there are three issues need to be addressed:
• Dynamically compose service plans to fulfill the
requested functions base on available devices.
• Automatically supply and rank a list of functions
that can be supported by a set of available devices.
• Dynamically adjust the system according to the
environment and resident information.
This work was supported by the IT R&D program of
MKE/IITA, 2006-S-066-02, Development of High Reliable
Adaptive Middleware for u-Home.

978-0-7695-3678-1/09 $25.00 © 2009 IEEE
DOI 10.1109/ICESS.2009.60

295
297

profile. It also responses for the resource control and
monitoring. When exception happens, it is in charge of
switching and deploying another executable plan to
the system. Each part will be discussed in the
following sections.

2 Smart Home System Architecture
2.1 Ontology for Smart Homes
To alleviate the limitation of the keyword search,
ontology is introduced to support the semantic
discovery of devices in the SOC framework for smart
homes. Four domain ontology systems – devices,
function concepts, environment profile, and policy
ontology, represent the knowledge base in smart home
systems. Device ontology describes the concepts
related to devices. Function ontology, as the core of
the whole knowledge base, describes the concepts
related to functions and templates are attached for
each composable function. Policy/preference ontology
describes the basic system constrains and preference
rules about the use of devices and functions. Those
rules may include conditions refers to environment
ontology. Environment ontology describes the
classification of environment profile, which includes
natural environment profile and person profile.

Knowledge Base
Knowledge Base supports the semantic knowledge
specification and the reasoning.
Knowledge Base
Device
Ontology

Refer To

Refer
To

Function
Ontology

Refer
To

Policy/
preference
Ontology

Refer
To

Environment
Ontology

Refer To

D1
D2

Household Data Manager
Device
Registry

User Template
Registry

...
D2

Household
Profile

Ranked Local
Function Plan

Plan Deployment
Parameter
Setting

Resource
control

Composition &
Collaboration

Monitoring

Service Manager
V&V Checking

Availability
Checking

When the picked
Plan cannot pass
the collaboration

Policy
Checking

Service
Ranking

Composition &
Collaboration

Figure 2 The Smart Home Architecture
As shown in Figure 1, function ontology is the core
of the knowledge base. It is different from other
ontology systems on its support of function template
specification. There are two kinds of function concepts
in the function ontology: atomic and composed. Both
of them can be referred by other three ontology
systems. The composed function concept has at least
one function template associated to it. Besides
ontology specification, it supports an important
function: raw plan logic generation. When one
function request comes, the inference query will be
performed. All the atomic functions and composed
functions, which can fulfill the request wholly or
partially, are returned with ranking information.

Figure 1 Ontology Systems
The ontology systems are cross-referenced to each
other as shown in Figure 1. The knowledge base
covers the necessary semantic information needed by a
smart home system. Moreover, the application
template is introduced in function ontology
specification, by which the system is informed about
how to assemble sub-functions to an integrated service.
2.2 Smart Home System Architecture
The proposed architecture supports the construction
and execution of smart home applications based on the
knowledge description in an ontology system. It
includes four main components: Knowledge Base,
Household Data Manager, Service Manager, and Plan
Deployment as show in Figure 2.
Knowledge Base provides general knowledge
representation related to smart home functioning.
Household Data Manager is the data center of the
whole system. It will store information about a
specific house. Service Manager is responsible for all
the checking and constructing the executable plan
according to the information stored in Household
Database. Plan Deployment sets the final parameter
according to the resident profile and environment

Household Data Manager
Household Data Manager stores the detail
information of the devices, user templates, the resident
and environment profile, and ranked local function
plans (RLFP) in a specific house.
The manager hosts a device registry which
performs as a UDDI [16]. The household devices
registration and the device discovery operation are all
ontology based. Besides the basic device information,
the associating protocols are also saved to enable the
communication between devices. The other registry,
user template registry, allows user register their

298
296

system. To save unnecessary time on the function plan
updating, the system only generates specific function
plans when they are requested. Function plan is
generated base on device ontology, function ontology,
and current available devices information. The control
flow of initial function construction is shown in Figure
3. It involves three phases: knowledge query, plan
construction, and plan deployment.
System enters into knowledge query phase when a
new function request comes in. It includes two steps.
First, all related function concepts are extracted from
function ontology with weight by inference query. For
the composed function, the corresponding plan logic
will also be included in the query results. Then, all
related device concepts are extracted from device
ontology base on the query result from the first step.
The elements in the plan logic in this stage are still
concepts on the knowledge level.

preferred functions and environment parameter
combinations.
Except the device information, household profile
keeps the other local information. It includes three
main aspects: environment profile, resident profile,
and policy. Environment profile records both static
and dynamic information. The dynamic information
can be obtained periodically via the web services, such
as daily weather. Resident profile is more static. It
includes the basic information and the self preference
of the residents. Policy has two parts: the static policy
and preference constrains, which is generated by
system automatically based on the knowledge base
and the local profile information. The RLFP are saved
locally to speed up the recomposition process.
Service Manager
Service manager mainly supports three functions:
verification and validation (V&V) checking, plan
ranking, and the composition and collaboration. V&V
checking performs availability checking and policy
checking. Service ranking is performed base on the
preference rules. The highest ranked function plan will
be selected and the details of protocol matching will
make sure that the plan is actually executable.
Plan Deployment
Plan deployment handles the remaining issues after
an executable function plan is generated. It includes
parameter setting, resource control, monitoring, and
recomposition and collaboration. After the parameter
setting and resource control phases, the selected
function plan will become executable and can be
performed in the smart home. The plan execution
status is monitored. If the current plan is no longer
feasible for the system due to device or network
failures, a new plan will be applied to continue
fulfilling the function. Since the generated plan is
stored, the system does not need to go through the plan
generation path again.

Figure 3 Initial Function Construction
After collecting needed knowledge from knowledge
base, system goes into plan construction phase, where
the result function concepts are all replaced by real
devices and various checking is performed. For
example, availability checking will filter out
unfeasible function or function plan, and policy
checking will make sure new function plan will not
cause any system confliction. Meanwhile, all the rest
function plans will be ranked base on the weight
generated from the first phase and also the reliability
and other attributes. Among all function plan
candidates, system will pick the best one to deploy and
save the whole list into RLFP database. Then, system
enters into the last phase: plan deployment.
In plan deployment phase, the collaboration details,
such as communication protocol and signal format, are
determined and established to make sure the selected
plan is executable. If any problem is found, the system
will go through the collaboration part again until an
executable plan is confirmed. After parameter setting

3 Service Composition
When system is running, it will meet three kinds of
situations: a function request comes from user, a
running device becomes unavailable, and a new device
is added into the system or an unused device is
removed. For the first two situations, system has to
initial/adjust a running function plan, while for the
third situation, RLFP database has to be maintained.
The detailed methodologies are discussed in the
following subsections respectively.
3.1 Initial Function Construction
The available devices in a house are not fixed.
Devices will be removed/added from time to time.
These changes may affect the function support of a

299
297

and resource control, the system will deploy the
function plan finally and monitor it.

3.3 Device Registration and RLFP Maintenance
The system available devices pool changes from
time to time. Some existing ones may be removed
while new devices will be added. Device upgrading
can be treated as device removing and adding. Not all
of them will affect stored RLFP. For example, if the
removed device does not have dependency with any
requested function, no matter the function is running
or pending, system will simply remove its registration
information. More details will be discussed in the
following.
Removing device
When a device is removed from system for any
reason, system has to response accordingly to what
kind of device it is. With the dependency relations
between the function template and device ontology,
the system can trace from the device concept, under
which the removed device registered, back to the
affected function concepts, then the RLFP. If the
device is not related with currently requested function,
its registration will be simply removed. If the device is
currently running, system has to perform
recomposition as shown in section 3.2. If it is not used
currently, however in the candidate function plan, the
related function plan will be removed from RLFP.
Adding device

3.2 System Recomposition
When a running device fails, instead of going
through initial function construction process again,
system recomposition process will be invoked to
achieve constantly function supply. System
recomposition has the same plan deployment phase as
in initial function construction. Because of pre-stored
reusable function plans, the knowledge query and plan
construction phases are eliminated, which accelerates
system response time.
Figure 4 shows the work flow of the recomposition
process. Instead of generate function plan list again,
system will pick function from RLFP database to
replace the broken one. Because there might be more
than one device are broken, or the broken device is
applied to fulfill more than one functions at the same
time, system will first run dependency checking to get
affected function list. Then, it will keep getting the
next candidate function plan until it can enable the
collaboration among the devices in the plan. If there is
more than one function to be re-composited, system
will perform confliction checking among the plan set,
such as power and bandwidth limitation, or if the an
exclusive device is used in two function plans. Once a
function plan is selected, the deployment phase is
carried out same as in initial function construction
scenario.

When a new device is added into system, it has three
different cases. Its processing logic is shown in Figure
5. Same as in device removing, the dependency
checking will be performed first.
If the device concept is not related with any current
requested function, system will simply keep the
registration information for future use.
If the device concept is related with currently
requested function, no matter if the function has a
feasible plan already running or not. The system has to
update the RLFP database. System will perform
dependency checking and find out if there is any
function that is affected by this change. If there is any,
system will further check if the affected function is in
pending status.
If any new function plan can be composed because
the new device being added, the system will go
through the plan construction phase as in section 3.1.
The generate function plan will be stored in the RLFP
database when the function request is fulfilled by the
current system already. Otherwise, user will be
notified that the function is ready. If user chooses to
apply it, system will deploy the new generated
function plan as in plan deployment phase in initial
function construction.

Figure 4 System Recompostion

300
298

New Device
Dependency Checking

Ranked
Function
Plan
No

4 Case Study

Related with Current
Running Function?

In this section, a “temperature control” example is
used to show how the proposed smart home system
generates function plan automatically and performing
service recomposition.

Plan Logic

Related with
Pending Function?

Yes

Yes

Generate Function Plan

Perform “Initial
Function Construction”

attribute must satisfy the constraints in the ontology
system. Any propagated changes need to be checked
for the system consistency.

Function
Ontology

Availability Checking

Notify User: Function is
availalbe

Policy Checking
Apply?

Device
Ontology
Device
Manager

4.1 Device and Function Ontology
The example device ontology of a sample smart
home is illustrated in Figure 6. Devices are
categorized into two classes: small power and larger
power. The categorization is base on the technical
parameters of the devices.

Policy/
preference

Service Ranking

Yes
Plan Deployment

Store Ranked Plan
End

Figure 5 New Device Adding
3.4 Ontology System Maintenance
Normally, the changes over ontology specification
will not affect the running function plan. However,
ontology maintenance may need to be performed for
the future use. There are three basic changes over
ontology system: adding a new item, deleting an
existing item, and modifying an existing item.
Adding any new information requires significant
update and checking. An item added must satisfy the
constraints in the ontology system. If a new relation is
added onto existing classes, the original and derived
relations needs to be examined to make sure there is
no violation among them. If a new item is added under
a given concept, all the relations associated with the
concept must be verified with the new item. In
addition, the consistency checking needs to be
performed over updated relations.
Deleting an existing item from the ontology in
general is difficult and should be discouraged because
the ontology systems are cross-referenced. Deleting an
item will not only affect the current ontology system
but the changes may propagate to the other referenced
ontology systems. Also, if a device concept is
removed, any device that references to this item will
realize it has referenced to a null item and will never
be visible to the system.
There are two kinds of modifications. One is
moving existing item. This kind of modifying is
difficult because moving is essentially removing an
item or a relation from one place, while adding the
same item in another place. Thus, all the complexities
of both adding and deleting will be there. The other is
modifying the attributes of the item. The modified

Figure 6 Device Ontology
Figure 7 shows the function ontology of the smart
home system. Functions that a smart home can supply
are divided into five classes: access control, security,
appliance control, information supply, and sensor. The
composed function “Temperature Control” is the
subclass of “Appliance Control”. It is composed by
two functions: heating and cooling. The function
template shown in Figure 8 describes how the
temperature control is composed using heating and
cooling services.
4.2 Policy Ontology
Figure 9 shows a simple policy classification.
Under each class, there will be several policy
templates. For example, every house has some
constraints about power consumption. It can be a
specific maximal AC current or expressed as “if two
devices are running in parallel, at most two of them
can be large-power devices according to device
ontology definition”. An example formal expression is
as following.
Status(A, “running”) ∩
∩ Status(C, “running”) =>

¬ (LargePower(A)
LargePower(C))

301
299

∩

Status(B,

“running”)

LargePower(B)

∩

Figure 7 Function Ontology
The other example of policy specification related
with temperature control is expressed in the following,
which states that “when the temperature control
function is running, the window needs to be closed”,
functionStatus(temperatureControl,
Status(window, “closed”)

“running”)

=>

The function can be attached to the policy
enforcement mechanism. For instance, the function
WindowControl.closeAll is attached to the 2nd policy.
To be compliant with this policy, the corresponding
function, i.e., WindowControl.closeAll, will be
performed.

Figure 8 Function Template Sample

4.3 Environment Profile Ontology
Environment profile includes two kinds of profiles:
Natural environment profile and person profile.
Natural environment indicates the characters of the
weather in different area. Person profile indicates the
age range and some other issues related with people
and will affect the preference of smart home parameter
setting. The example profile ontology is shown in
Figure 10.
Figure 9 Policy Ontology

Figure 10 Environment Profile Ontology

302
300

4.4 Composed Function Plan
In the device ontology specification, the concepts
defined in function ontology are referred. When the
device is registered into system, since it is based on
the device ontology, the device is registered with its
function information implicitly. As shown in Figure 11,
five devices are registered into system with functions
related with “temperatureControl” directly or
indirectly. They will be used when the
“temperatureControl” function is composed.
The system composed four different function plans
for “temperatureControl” according to the template
shown in based on the available devices. As an
example, one of them is depicted in Figure 12.

Once the ontology is specified, it refers to several
parameter tables. For example, for four seasons
specified in Table 1, the best indoor temperatures are
set differently to provide comfort indoor atmosphere.
Table 1 Temperature parameter setting 1
LTemp
HTemp

Spring
16
28

Summer
17
25

Autumn
16
28

Winter
16
25

The temperature will be further adjusted according
to person profile as shown in Table 2.
Table 2 Temperature parameter setting 2
LTemp
HTemp

Baby
LTemp+1
HTemp-1

Child/Adult
LTemp
HTemp

Senior
LTemp+1
HTemp+1

Figure 11 References between function ontology, device ontology, and device register
fulfill the required “temperatureControl” function.
When the deployed function cannot be satisfied,
system proposes to invoke the most related function
“cooling”, base on the function ontology definition,
such that it may be able to partially fulfill the
requested function.

5 Conclusion
Given the diversity of home environment,
appliances, and residents, the applications for smart
homes must be configurable and adaptive. In this
paper, we proposed a system architectural design for
smart home ontology and service composition. The
proposed system consists of a smart home knowledgebase (ontology), a household database, a service
manager, and a plan deployment component. A generic
knowledge representation is used to facilitate the
composition of appropriate applications based on user
profile and available appliances.
There are further optimizations and functionalities
can be addressed in the future. In addition, automated

Figure 12 A sample plan generated automatically
The running log of the system is shown in Figure 13.
The system automatically adjusts the running plan to

303
301

ontology and the process composition are parts of the
smart home systems.

code generation for a selected execution platform will
be the immediate tasks. This will allow us to test any
integration issues that could be raised when the

Figure 13 System running log

Reference
[1] N. F. Noy, M. Sintek, S. Decker, M. Crubezy, R.
W. Fergerson, and M. A. Musen. “Creating
Semantic Web Contents with Protege-2000”,
IEEE Intelligent Systems 16(2):60-71, 2001.
[2] M. Paolucci, T. Kawamura, T. R. Payne, and K.
P. Sycara. “Semantic Matching of Web Services
Capabilities”, 1st International Semantic Web
Conference, 2002.
[3] R. Chinnici, J. J. Moreau, A. Ryman, and S.
Weerawarana. “Web Services Description
Language (WSDL) Version 2.0. June 27, 2007.
http://www.w3.org/TR/wsdl20/.
[4] D. Fensel. “Ontologies: Silver Bullet for
Knowledge
Management
and
Electronic
Commerce”, Springer-Verlag, Berlin, 2001.
[5] “What
is
an
Ontology,”
http://wwwksl.stanford.edu/kst/what-is-an-ontology.html.
[6] A. Bernaras, I. Laresgoiti, and J. Corera.
“Building and Reusing Ontologies for Electrical
Network Applications”, ECAI96, 12th European
conference on Artificial Intelligence Ed., John
Wiley & Sons Ltd., pp. 298-302
[7] M. P. Papazoglou and G. Georgakapoulos.
“Service-Oriented Computing”, CACM, October
2003, 46(10).
[8] W. T. Tsai, "Service-Oriented System
Engineering: A New Paradigm", IEEE
International Workshop on Service-Oriented
System Engineering (SOSE), October 2005, pp.
3 - 8.
[9] “OWL-S: Semantic Markup for Web Services”,
http://www.w3.org/Submission/OWL-S, Nov 22,
2004.
[10] “OWL Web Ontology Language Reference, Feb
10, 2004 ”, http://www.w3.org/TR/owl-ref/
[11] T. Gu, X. H. Wang, H. K. Pung, D. Q. Zhang.

[12]

[13]

[14]

[15]

[16]

304
302

"An Ontology-based Context Model in
Intelligent Environments", In Proceedings of
Communication Networks and Distributed
Systems Modeling and Simulation Conference,
San Diego, California, January 2004.
E. Kim and J. Choi. “An Ontology-Based Context
Model in a Smart Home”, Workshop on
Ubiquitous Web Systems and Intelligence
(UWSI 2006), pp. 11-20.
F. Latfi, B. Lefebvre1 and C. Descheneaux,
“Ontology-Based Management of the Telehealth
Smart Home, Dedicated to Elderly in Loss of
Cognitive Autonomy”, Third International
Workshop,
OWLED
2007,
OWL: Experiences and Directions.
E. Meshkova, J. Riihijarvi, P. Mahonen, and C.
Kavadias. “Modeling the home environment
using ontology with applications in software
configuration
management,”
International
Conference on Telecommunications, 2008. ICT
2008, pp. 1-6.
W. T. Tsai, B. Xiao, Q. Huang, Y. Chen, and R.
Paul, "SOA Collaboration Modeling, Analysis,
and Simulation in PSML-C", Proc. of the Second
IEEE International Symposium on ServiceOriented
Applications,
Integration
and
Collaboration (SOAIC’06), October 2006.
UDDI
Technical
Committee.
Universal
Description, Discovery and Integration (UDDI)
http://www.oasis-open.org/committees/uddi-spec.

Schedulable Online Testing Framework for
Real-Time Embedded Applications in VM
Okehee Goh and Yann-Hang Lee
Computer Science and Engineering Department
Arizona State University, Tempe AZ, USA
{ogoh, yhlee}@asu.edu

Abstract. The paper suggests a VM-based online testing approach in
which software testing is piggybacked at runtime on a system that operates to serve actual mission. Online testing in VM is facilitated with
a framework that uses persistence service to initialize the testing operation with a consistent system state. The testing operation then runs in
an isolated domain which can be scheduled independently of the operating version. Thus, testing operation cannot cause unbounded pause time
nor spoil the normal operation. We evaluate the feasibility of schedulable online testing with a prototype developed in MONO CLI (Common
Language Infrastructure) and the experiment on the prototype.
Key words: Online Testing, Virtual Machine, Real-Time Embedded
Applications

1

Introduction

Nowadays, the applications of real-time embedded systems have proliferated
from industrial controls to home automation, communication consumer gadgets,
medical devices, defense systems and so forth. The apparent trends of the systems include sophisticated features and a short production cycle. The trends
have sought solutions more in software rather than in hardware and also have
led to the application of virtual software execution environment (VM) as a runtime environment. VM, populated with JVM [10] and CLI1 [5], features high
portability from using intermediate code, high productivity and reusability of
object-oriented languages, and a safe runtime environment. The features are
beneficial to the development of real-time embedded systems as the production
cycle and cost can be reduced.
As software plays an increasingly significant role in embedded systems, the
demands of upgrading software are anticipated for bug fixing and for extended
functionality. In fact, most embedded systems, which have long lifetimes and require high availability, are generally passive on software upgrade because upgrading software requires to restart the systems, and the newly upgraded software
1

CLR (Common Language Runtime), which is a CLI’s implementation by Microsoft
and an integral part of Microsoft .NET framework, is more popularly known than
CLI.

2

Okehee Goh, Yann-Hang Lee

can introduce new types of bugs and faults. Research work on online upgrade
[12, 4], and reconfigurable systems [14], has been focusing on providing facilities
to accomplish the software upgrade at runtime. In the meanwhile, there is no
doubt as to the importance of software testing to verify the correctness, the
completeness, and security, especially for mission- or safety-critical software in
which even minor changes of the software require extensive testing [15].
Software testing is generally conducted in off-line environment with test
cases generated by predetermined inputs. The weakness of the predetermined
inputs, particularly derived from a model-driven formal system specification, is
that the testing results are restricted to the correctness and completeness of a
given model. Furthermore, an off-line testing environment for embedded software
means that the software is tested in a simulation mode and does not participate
in an actual mission. However, the testing of embedded software should be able
to deal with external interferences and unexpected behavior of the target application environment. All possible inputs may not be known ahead, and the
generation of complete test cases for all execution conditions is very problematical.
To overcome the aforementioned limitations, we suggest an online testing
environment for software upgrades as a supplementary approach of an off-line
testing. The testing of software upgrades is piggybacked at runtime on the systems that operate to serve an actual mission. Hence, the execution of the software
dedicated to the actual mission coexists with the execution of the software to be
tested. The apparent benefit of online testing is that testing undergoes not in a
limited runtime environment but in an actual target environment connected to
physical world. That is, the software testing is conducted by using actual inputs.
To simplify terminologies to be used hereafter, the software for an actual mission, and the software to be tested are called software under operation (SUO),
and software under test (SUT), respectively.
Most embedded applications run periodically with long lifetimes. At each
period, they conduct computation by taking external input data, and then the
computation results, represented as output data, is used to activate the target
hardware. Some embedded applications’ computation at each period is based
on the state that has been accumulated from computations of preceding periods as well as the newly sampled input data. For online testing of this type of
applications–stateful software, SUT must be able to start with the accumulated
computation state of SUO, and since then, gets applied with the same inputs
that SUO receives. Furthermore, if SUO is characterized by time constraints,
whose virtue includes timely correctness, the online testing piggybacked has to
be nonintrusive: the latency and pause time that SUO encounters due to online testing must be predictable and controllable. Certainly, any faulty behavior
caused by SUT must be isolated to prevent SUO operations from any impact.
In this paper, we aim at a framework of schedulable online testing (SOTF)
for real-time embedded software in VM. With the advent of an online testing
request, the framework provides facilities to enable a testing mode where both
SUO and SUT are executed concurrently. On the termination of testing, the

Schedulable Online Testing Framework for RTEA in VM

3

system returns back to an operation mode of executing SUO only. It achieves
fault isolation by executing SUT in a separate partition. By checkpointing the
accumulated computation state of SUO, SUT begins to execute from a consistent state. In addition, the framework logs the external input data which SUO
receives, and reconstructs the logs for the execution of SUT. Finally, SOTF
employs a preemptible mechanism for checkpointing and recovery of persisted
states and provides the flexibility to resume the testing anytime. Hence, the
timely correctness of SUO can be ensured.
In the following section, we give a discussion of related works. The target
application model of online testing is introduced in Section 3. Then, the approaches and designs of the proposed schedulable online testing framework on
CLI (Common Language Infrastructure)[5]’s open source platform, MONO [17],
are presented in Section 4. In Section 5, the experiment on the prototyped SOTF
is used to show the space overhead incurred by the testing framework. The overhead and the source of latency of online testing with SOTF are identified. Finally,
Section 6 draws a conclusion.

2

Related Works

One of well-known research areas with a key role of checkpointing and/or logging
is log-based rollback recovery [6]. Logging-based recovery protocol, especially on
.NET framework [2, 1], is tuned at component oriented distributed applications
The work was motivated with the problems that process-based recovery protocol cannot detect the failure of components, and checkpointing/recovery in a
process level is very heavy. The prototype employs .NET’s Object Serialization
and Context to support checkpointing/recovery and to enable interceptions of
messages (to aid logging) on persistent components, respectively. The rebinding
of recovered components is done through .NET Remoting’s registry so that other
stable components can access the recovered components.
Simplex architecture[13] is to support the evolvability of dependable realtime computing systems. The architecture adopts analytical redundant logic:
running a trusted version (a fault-proof component) and an upgrade version (a
not-yet-fault-proof component) in parallel as separate software. The architecture
has decision logic that monitors the behavior of an upgrade version. If a faulty
action is detected from the version, the control of the system is switched to the
trusted version. Resource isolation is emphasized to prevent a trusted version
from being corrupted due to the faulty behavior of an upgrade version. Lee
et al.[9] extended the Simplex architecture for online testing and upgrade of
industrial controller in the Linux OS environment by applying the technique of
Process Resurrection [8].
RAIC (Redundant Arrays of Independent Components) [11] is a technology
that uses groups of similar or identical components to provide software dependability and allows component hot-swapping; the architecture allows addition or
removal of components at runtime. When a component is swapped, the state
transfer from an old component to a replaced component is supported, if the

4

Okehee Goh, Yann-Hang Lee

component is stateful. If the components are faulty, as examined using built-in
testing code on the controller, the controller handles the faulty exceptions and
recovers the application state so that the fault is not exposed to the applications.
The primary difference of our work from the previous works is that our framework aims a testing facility in VM that allows preemption to reduce blocking
delays. Thus, flexible scheduling of testing can be carried out while ensuring the
timeliness of regular service.

3

Target Application Model

SW under
Test
InputData
SW under
Operation

ControlProcess

Logging/Reconstructing

InputData

Sensor

OutputData

Checkpoint/Recovery

ControlProcess

OutputData

Plant

Actuator

Fig. 1. Target application model of Online Testing
The target application model we envision for SOTF is a closed-loop control
system. The systems basically include sensors, control processes, and actuators, and the control tasks run concurrently and periodically to control a target
plant. A simplified view of the system can be described by three applicationlevel objects for the three system components: InputData, ControlProcess, and
OutputData.
Figure 1 is a simplified closed-loop control system applying the schedulable
online testing. In the figure, InputData indicates the data collected from sensors
and taken by a control process, and OutputData are the computational results
generated by the process and passed to actuators. We assume that a control
process demands upgrades to meet performance enhancement or new business
requirements. The upgrade version’s control process has to access the same InputData as the operation version; that is, the upgrade version maintains the
same frequency and format for the access to InputData as the operation version
does. It is a reasonable assumption because InputData, which is generated by
a sensor as a result of monitoring the target plant, does not get changed unless
the sensor or the target plant gets replaced or upgraded. The same assumption
is applied to OutputData with respect to the actuator.

Schedulable Online Testing Framework for RTEA in VM

5

Consider a stateful control process where the computation result of each
periodic operation depends on not only the InputData obtained at the current
period but also the accumulated state from the computations of the preceding
periods. An upgrade version’s control process can start online testing by being
initialized with the state of an operation version’s control process at the moment
that online testing is triggered, and by accessing the same InputData as the
operation version.
The concern of online testing with real-time applications is to ensure time
constraints of regular service while online testing is under way. An approach
to address this issue is to schedule the testing as a background task, which
runs when no real-time task is ready. One of the problems that arise under this
scheduling approach is the availability of InputData. When an upgrade version is
ready to run, InputData that primarily stimulates the operation version’s control
process, may not be available any more. Also, the initial state must be preserved
until the upgrade versions start to execute. The foremost important issue is that
the testing operation, the logging of Inputdata, and the preservation of the initial
state must be preemptible. Thus, the testing process would not block the regular
service of the target system.

4

Online Testing Service

To enable schedulable online testing, we present the approaches in VM-based
real-time embedded system. Although the approaches are applicable to both CLI
and JVM, we built a prototype of SOTF in MONO CLI and, in the subsequent
text, refer to specific technologies and standard class library of CLI.
4.1

Online Testing Framework

Application-layer
OTTestManager

Operation Partition
OTReconstructDrv

OTRecordDrv

Control
Process

Test Partition

Input/Output Data

Control
Process

Input/Output Data

VM-layer
Class Loader
JIT
Multi-Threading

Checkpoint/Recovery
Garbage Collection
Exception Handler

Logging/Reconstructing
Verification
Native code interface

Fig. 2. Online Test Framework

6

Okehee Goh, Yann-Hang Lee

SOTF consists of three tasks (in the application layer) to drive online testing,
and two subsystems (in the VM layer) to aid online testing: three tasks including
OTTestManager, OTRecordDriver (OTRecordDrv), and OTReconstructDriver
(OTReconstructDrv), and two subsystems including a preemptible persistence
system and a logging/reconstructing system.
Figure 2 illustrates the architecture of the framework of schedulable online
testing. The roles of the three tasks are as follows. OTTestManager is responsible
for triggering online testing when software to be tested is ready. As a response
of the commands from OTTestManager, OTRecordDrv interacts with SUO to
checkpoint a consistent state of SUO and log input data sampled from sensors.
Correspondingly, OTReconstructDrv interacts with SUT to direct the recovery
and reconstruction operations of the persisted state and the logged input data. In
the figure, SUO and SUT are represented with a composition of ControlProcess,
InputData, and OutputData, as a simplified model suggested in Section 3.
4.2

An Isolated Testing Environment

As SUT concurrently runs with SUO, the possible faulty behavior of SUT can
affect the operation of SUO. To contain the faulty behavior of SUT, it should
reside in a runtime environment separated from that of SUO. We employ Application Domain [3] in CLI as a facility to provide an isolated runtime environment.
The application domain is a lightweight address space designed as a model of
scoping the execution of program code and the ownership of resources. Sharing
objects between different domains is prohibited: that is, objects in one domain
cannot access objects in other domains. Creating multiple application domains
by starting assembly2 with a main entry, is supported at runtime. Additionally, CLI facilitates unloading an application domain at runtime. This allows a
dynamically created SUT domain when a testing operation is requested.
4.3

Preemptible Checkpointing and Recovery

If SUO’s accumulated state from the computations of preceding periods is preserved, then SUT can start with the known initial state. To transfer the state
from SUO to SUT across the domain boundary, we adopt the approach of checkpointing SUO’s state and recovering the state for SUT. The challenge of the
approach, especially for real-time applications, is that the pause time due to the
checkpointing/recovery operation may be unbounded. The unpredictable latency
from checkpointing can hinder the timeliness of SUO if SUO is blocked until the
checkpointing operation finishes entirely.
To make it possible to bound the pause time due to checkpointing and recovery, our prior work, schedulable persistence system (SP system) [7] is adopted
with which the persistence service runs concurrently with real-time tasks. The
minimal length of the pause time, i.e. the minimal non-preemptible region in the
2

Assembly is a minimal unit of reuse, versioning, and deployment in CLI.

Schedulable Online Testing Framework for RTEA in VM

7

persistence service, can be adjusted to meet the scheduling needs of real-time
application tasks.
When the persisted state is deserialized, there may be a question whether the
state objects can be useful directly by the SUT. If the state objects of the control
process in SUT is the same object of SUO, i.e. the names of persistent classes
and persistent fields in SUT is identical to these in SUO, then the persisted
objects generated from SUO can be used to initialize SUT. Otherwise, we can
apply a transformation script to reconstruct the state objects for SUT based on
the persisted SUO objects.
4.4

Logging and Reconstructing

After being initialized with the checkpointed state of SUO, SUT is ready to execute. It should receive the sampled input data similar to the one applied to SUO
since the checkpoint. As a solution, the access to InputData by SUO’s ControlProcess is logged and then the access to InputData by SUT’s ControlProcess
is sufficed with the logs. This logging/reconstructing requires to intercept the
method calls on InputData objects. That is, the method calls by SUO’s ControlProcess to read InputData is post-processed to log the sampled data, and
the method call by SUT’s ControlProcess to read InputData is pre-processed to
reconstruct the sampled data based on the logs. The post- and pre-processing on
InputData objects are done through the Context in CLI which provides an object
with an execution scope. Additional services can be augmented during incoming/outgoing method calls on context-bounded objects which are derived from
the System.ContextBoundObject. This feature has been employed in a loggingbased recovery protocol on .NET framework [2, 1] to enable the interceptions of
messages (to aid logging) on persistent components.

5

Experiments

The experiment of the SOTF prototype on MONO CLI is performed to understand the source of latency on a testing sequence and to examine the concerns of
scheduling online testing in an example system. It is conducted with C# benchmarking applications on a PC workstation with 1.5GHz Pentium IV processor
and 256MB memory. To have a high resolution timer and preemptive kernel,
TimeSys’ Linux/RK (real-time kernel v4.1.147) [16] is used. For time measurement, a standard class library System.DateTime.Now.Ticks is used, which gives
100ns resolution. The C# language supports five levels of thread priorities, Highest, AboveNormal, Normal, BelowNormal, and Lowest. The priorities, are implemented using TimeSys RK’s POSIX real-time FIFO scheduling policy.
5.1

Cost Analysis for Testing Sequence

SOTF is implemented by integrating a wide range of facilities to satisfy the
requirements of online testing such as an isolated testing environment, interceptions of method calls, checkpointing/recovery, and logging/reconstructing.

8

Okehee Goh, Yann-Hang Lee

Using the facilities leads to some overhead. Although the amount of overhead
or cost depends on the techniques employed, these types of overhead or cost
are inevitable. In this experiment, we analyze the cost incurred in every stage
constituting the testing sequence.
The benchmarking application, SUO, used in this experiment is a seismic
event monitor, which computes the rate of seismic events by using both seismometric data newly obtained and seismometric data accumulated from a preceding duration. The seismic event monitor (SUO) runs periodically every 3ms
for 1ms WCET (Worst Case Execution Time) with AboveNormal priority. The
seismometric data read from its InputData object is 20Bytes. The seismometric
data accumulated from prior computations, a persistent state of SUO’s ControlProcess, will be checkpointed to aid for online testing. The size of the persistent state, consisting of about 1000 composite objects including primitive types’
fields, is about 20000Bytes. Its upgrade version, SUT, embodies a slightly different computation approach but generates basically the same results with the
operation version SUO. When online testing starts, SUT runs every 1ms with
Lowest priority. To just observe the overhead of the operation in a testing sequence, we allow the checkpointing on SUO and the recovery of persisted data
on SUT to perform in a nonpreemptible mode. Additionally, the termination
condition of testing is set to 300 periods of SUO’s operation.

Stages

Time (ms)

(1) Receive a testing request

0

(2) Turn on checkpointing on SUO

10

(3) Start checkpointing/logging on SUO

12

(4) Complete checkpointing

25

(5) Start SUT

29

(6) Complete initialization for testing on SUT

686

(7) Start recovery on SUT

691

(8) Complete recovery, and start testing on SUT

699

(9) Complete logging on SUO

862

(10) Complete testing on SUT

1015

(11) Start unloading of SUT

1044

(12) Complete unloading of SUT

1109

Table 1. Time line of a testing sequence

Table 1 shows the cost incurred in every stage constituting the testing sequence. The result is chosen as one with the longest completion time (e.g. the
moment that the unloading of SUT completes since the advent of a testing request) from 20 runs. The time specified at each stage is the elapsed time since

Schedulable Online Testing Framework for RTEA in VM

9

OTTestManager received the testing request. We speculate the costs involved
to carry out three functions: (1) coordinating SOTF tasks (OTTestManager,
OTRecordDrv, and OTReconstructDrv) and transferring information between
two different application domains, (2) conducting checkpointing and recovery,
and (3) starting and unloading software at runtime.
The cost in function (1) attributes to the coordination of SOTF tasks in
different application domains. For instance, the OTTestManager task, receiving an event for testing, informs OTRecordDrv to prepare for testing. What is
carried out in this step is that one task fires an event to wake up a dormant
thread, and the data specification for testing is transferred from one application
domain (where OTTestManager runs) to the other application domain (where
OTRecordDrv runs). To enable the communication between tasks in different application domains, CLI’s AutoResetEvent class and AppDomain class are used.
The result, leading to about 10ms delay, which is quite expensive and mostly
comes from marshaling (to pass objects between the domains), indicates that
the efficient communication mechanism between different domains is desired.
Checkpointing and recovery operations take 13ms ((4)-(3)), and 8ms ((8)(7)), respectively. The size of final persisted data, including metadata for the serialization protocol, is 33125Bytes. Compared to the experiment results (186ms
and 69ms for serialization and deserialization respectively) by standard serialization library, the schedulable persistence system (SP system) in [7] substantially
outperforms the standard serialization class libraries.
In Table 1, we also notice the cost for starting and unloading testing software
in a new domain at runtime. The operations are implemented using AppDomain
class’s CreateDomain, ExecuteAssembly, and Unload methods. The delay reaches
to 657ms((6)-(5)) to start a new software, and 65ms((12)-(11)) to unload the
software, respectively. This noticeable delays comes from loading and compiling
not only user classes of the new assembly but also a large portion of system
classes referenced by the user classes.
5.2

Scheduling Online Testing and Space Overhead

In SOTF, it is imperative that the timeliness of applications (SUO and other
real-time tasks) has to be guaranteed while testing is in progress. The simplest
scheduling approach is to treat the testing as a background job so that the testing
operations would have a minimal interference to the applications’ timeliness. One
of the concerns with the background testing job is the nonpreemptible regions
caused by SOTF. The other concerns is the overhead of space that is reserved
to save the logs. Logs produced by SUO have to remain until they are consumed
by SUT. The issue of space overhead also attributes to the characteristic of
embedded software testing, which requires to be exposed to the physical world
for a long period. Thus, it can encounter all possible input data sets. Here, we
consider a schedule example and use it to examine the space overhead in a testing
process.
In this experiment, the SUO (a seismic event monitor) and its corresponding
SUT (an upgrade version of the seismic event monitor) are same with ones in the

10

Okehee Goh, Yann-Hang Lee
CU

T1

T2

T3

T4

0.5

0.5/5

1.6/8

1/10

1.5/15

0.6

0.5/5

1.6/8

2/10

1.5/15

0.75

1/5

2/8

2/10

1.5/15

Priority AboveNormal Normal Normal BelowNormal

Table 2. Task Sets (CU: CPU Utilization, time unit: ms)

previous experiment so that the size of persistent data for checkpointing/recovery, and the size of each log are same with the previous experiment. Additionally, service launched in an operation partition includes three more tasks besides
SUO; that is, an operation partition consists of four tasks. Table 2 specifies
three different task sets, and their scheduling parameters according to varying
CPU utilization (CU), 0.5, 0.6, and 0.75. The table also specifies WCET, period, and priority of each task, which runs periodically; for example, T1 in the
CPU utilization set 0.5 has 0.5ms WCET and 5ms period, and runs with the
AboveNormal priority. Among the tasks, T1 is SUO which has a correspondent
SUT. SUT, which is not specified as a task in the table because it does not
account for CPU utilization, runs as a background task (with the Lowest priority). Checkpointing and recovery are carried out in a preemptible mode with
the Highest priority–3ms period, and 2ms WCET. That is, when checkpointing
or recovery is initiated, it runs 2ms every 3ms until it completes the requested
service. In this experiment, we focus on understanding space overhead during
online testing so that we ignore the testing overhead including checkpointing
and recovery although it affects the schedulability of the task sets. To ease the
termination condition of testing, the testing duration is limited to 300 periods
of SUO operation.

12000

CU 0.50
CU 0.60
CU 0.75

Size of Log Data (Bytes)

10000

8000

6000

4000

2000

0

200

400

600

800
1000
1200
1400
1600
Time elapsed since logging starts (ms)

1800

2000

2200

Fig. 3. Space for logs according to varying load

Schedulable Online Testing Framework for RTEA in VM

11

Figure 3 shows the space size of log data for three task sets over time until testing on SUT completes since logging on SUO started. According to the
graph, the execution of SUT, conducting testing by actually consuming the logs,
starts at around 1122ms, 1257ms, 1342ms for CU 0.5, CU 0.6, and CU 0.75,
respectively. This start time is influenced from higher priority tasks’ loads and
also the overhead of online testing: as we see in the second experiment, SUT
can start once the completion of checkpointing by SUO, the launch of SUT
by OTTestManager, and the completion of recovery by SUT are accomplished,
which take approximately 700ms. Regarding the space for logs, if the duration of
SUO logging (producing logs) is not overlapped with the duration of SUT testing
(consuming logs), the space for logs including metadata is 16500Bytes. In fact,
the result shows that the maximum space size for logs reaches to 9407Bytes,
10727Bytes, 11057bytes for CU 0.5, CU 0.6, and CU 0.75, respectively. The experiment shows that the space for logs reaches to the maximum during the initial
stage of testing; once testing by SUT starts by consuming the logs, the space
required for logs becomes less than during the initial stage. It indicates that
testing can be conducted for long duration without a severe burden of space if
the system can guarantee the maximum space needed in the initial stage.
Besides the space issue, conducting checkpointing and recovery in a preemptible mode shows that it can keep the maximum pause time 2ms and then
their response times become 18ms, and 11ms on average, respectively, due to the
execution of interleaved mutators. It indicates that checkpointing and recovery
do not stop application tasks for 13ms and 8ms as its nonpreemptible mode of
the second experiment.
Conclusively, predicting the upper bound of memory space reserved for logs
has to consider the cost and overhead of online testing and the workload of
applications.

6

Conclusion

In this paper, we depict a VM-based schedulable online testing framework for
testing software upgrade of real-time embedded applications. The testing can undergo with actual input data in a target runtime environment. The framework
is built by integrating a wide range of mechanisms of VM, including an isolated partition for testing, preemptible checkpointing/recovery, and logging/reconstructing the sampled input data. Meanwhile, in order to prevent the testing
from causing adverse effects on the ongoing regular services of the target systems, the testing task runs in the background mode and read in sampled data
via a log buffer. The experiment with the prototype of the framework, developed
on MONO, demonstrates the feasibility of online testing in VM environment as
well as the required capacity of the log buffer.

References
1. Roger Barga, Shimin Chen, and David Lomet. Improving logging and recovery
performance in phoenix/app. ICDE, 00:486, 2004.

12

Okehee Goh, Yann-Hang Lee

2. Roger Barga, David Lomet, Stelios Paparizos, Haifeng Yu, and Sirish Chandrasekaran. Persistent applications via automatic recovery. IDEAS, 00:258–267,
2003.
3. Don Box and Chris Sells. Essential .NET Volume 1: The Common Language
Runtime. Addison Wesley, 1st edition, 2002.
4. M. Dmitriev. The first experience of class evolution support in PJama. In Proc.
of The 8th International Workshop on Persistent Object Systems (POS-8) and
The 3rd International Workshop on Persistence and Java (PJW3), pages 279–296.
Morgan Kaufmann Publishers, Inc., 1998.
5. ECMA. Ecma-335 common language infrastructure, 2002.
6. E. N. (Mootaz) Elnozahy, Lorenzo Alvisi, Yi-Min Wang, and David B. Johnson. A
survey of rollback-recovery protocols in message-passing systems. ACM Computing
Surveys, 34(3):375–408, September 2002.
7. Okehee Goh, Yann-Hang Lee, Ziad Kaakani, and Elliott Rachlin. Schedulable
persistence system for real-time embedded applications in VM. In EMSOFT, pages
101–108, 2006.
8. K. Lee and L. Sha. Process resurrection: A fast recovery mechanism for realtime embedded systems. In Real Time and Embedded Technology and Applications
Symposium, pages 292–301, 2005.
9. Kihwal Lee and Lui Sha. A dependable online testing and upgrade architecture
for real-time embedded systems. In RTCSA, pages 160–165, 2005.
10. T. Lindholm and F. Yellin. The Java Virtual Machine Specification. AddisonWesley, 2nd edition, 1999.
11. C. Liu and D.J. Richardson. RAIC: Architecting dependable systems through
redundancy and just-in-time testing. In ICSE 2002 Workshop on Architecting
Dependable Systems, 2002.
12. Scott Malabarba, Raju Pandey, Jeff Gragg, Earl Barr, and J. Fritz Barnes. Runtime
support for type-safe dynamic Java classes. Lecture Notes in Computer Science,
1850:337–361, 2000.
13. Lui Sha. Using simplicity to control complexity. IEEE Software, 18(4):20–28,
July/August 2001.
14. Craig A. N. Soules, Jonathan Appavoo, Kevin Hui, Robert W. Wisniewski,
Dilma Da Silva, Gregory R. Ganger, Orran Krieger, Michael Stumm, Marc A.
Auslander, Michal Ostrowski, Bryan S. Rosenburg, and Jimi Xenidis. System support for online reconfiguration. In USENIX Annual Technical Conference, General
Track, pages 141–154, 2003.
15. J.A. Stankovic. Misconceptions about real-time computing: a serious problem for
next generation systems. Computer Magazine, pages 10–19, October 1988.
16. TimeSys Corporation. Timesys linux/real-time user’s guide, version 2.0, 2004.
17. Ximian. MONO. http://www.go-mono.com.

Trust-Propagation Based Authentication Protocol
in Multihop Wireless Home Networks
Han Sang Kim, Jin Wook Lee*, Sandeep K. S. Gupta and Yann-Hang Lee
Department of Computer Science and Engineering
Arizona State University
Tempe, Arizona 85287
{hanskim, sandeep.gupta and yhlee}@asu.edu,
*Communication Lab.
Samsung Advanced Institute of Technology (SAIT)
Yong Shi, Republic of Korea
thetruth.lee@samsung.com

Abstract— In this paper, we propose an authentication and secure channel establishment protocol that is reliable and adaptable
for multihop wireless home networks. The main idea is that a
home server hands over its authenticating capability to some already authenticated devices to enable these devices to be able to
authenticate other devices which cannot reach the home server directly due to their physical location and radio power constraints.
Key to our design is a neighbor device authentication protocol,
based on pre-user-injected network password, with minimal reliance on public key cryptographic operation. In addition, our
protocol supports a secure channel establishment among heterogeneous devices after authentication. Through the evaluation, we
show that our protocol is resistant to various attacks.
Index Terms— Security, Authentication, Wireless Multi-hop
home Network.

I. I NTRODUCTION
N the future, homes will use multihop wireless networks
to connect all the devices to one another. Multihop wireless technology could solve a number of limitations that extend
beyond simply eliminating dead zones in home. Depending
upon the data rate (bandwidth) requirement different wireless
technologies can be employed. For example, emerging wireless standards such as Zigbee (IEEE 802.15.4) [1] are suitable
for many home automation and personal healtcare applications
which require low data rate. Zigbee is optimized for prolonged
battery-powered operation. Due to short range (about 15m in a
cluttered office/home environment) of a single Zigbee device,
it supports (and requires) multihop routing to increase coverage
of a Zigbee network. To satisfy the demands of the digital home
with high-speed data rates, Bluetooth scatternets [3], multihop
WiFi networks [2] and Mesh networks [4] can be used.
Wireless home networks require a robust authentication
mechanism due to the accessibility to the devices, the heterogeneity of communication protocols and the wireless environment. Our authentication protocol relies on a home server to
propagate authentication to the network. In a common home
networking environment, a server-based approach is more advantageous, as we can impose most of the cost/complexity on
the home server. In this paper, we suggest a way to efficiently

I

authenticate home devices using trust-propagation in multihop
wireless home networks. The fundamental idea of the proposed
authentication protocol is that a home server hands over authentication privilege to the already authenticated devices. Eventually all the devices in the network are authenticated and then become authenticators on behalf of the home server. Our protocol
achieves low communication cost and scalability by allowing
multihop peer-to-peer communication of devices.
The rest of the paper is organized as follows: Section II describes our system model, assumptions and design goals. In
section III, we propose our trust-propagation based authentication protocol. In section IV, we presents the security analysis
of our protocol. Finally, we conclude this work in section V.
II. T HE A PPROACH
A. System Model and Assumptions
Our system model consists of a home server and multiple
home devices, such as TV, home theater system, and PDA. Locations of devices are dynamic but we assume that the mobility of devices is limited within the confines of a home. The
first device activation is done by an user by inputting a memorizable password. For instance, when a user buys a home network device and enters a password agreed with the home server.
This assumption is reasonable because unexpected home devices should not be participating in the home network. We assume that home devices have limited radio transmission power
so a multihop network should be installed at home in near future. Since many of the devices in the network may be batterypowered, our design goal is to make the authentication protocol
as efficient as possible in terms of communication overhead, i.e.
minimize number of messages exchanges, as well as computation overhead, i.e. minimize number of (possibly expensive)
public-key cryptography operations.
B. Definitions
We here briefly introduce two key concepts of our protocol.
• Badge: A badge is similar to the concept of digital signatures in terms of the properties such as its unforgeability,

verifiability, unreuseability, and undeniability. It is used
for determining whether authority for authentication belongs to an individual node. This is in contrast to using
it for verifying whether a public key belongs to an individual. Specifically, in our proposed protocol, Badge is
used by a home server to delegate only its authenticating
capability to an already authenticated device. Upon possessing a badge, an authenticated device can authenticate
other devices on behalf of the home server. Note that the
server gives a device only limited authority, i.e. authentication authority.

Fig. 1. One hop Authentication Domain

the home server S or other already authenticated devices and
to try to be authenticated. Following are the four possible responses the new device could expect in response to its beacon
message:
• Server response: If the home server receives the beacon
message from a new device, the server responds to let the
device start the Authentication Process.
• Neighbor response with Badge: If one of the neighbor
devices who has a Badge, this already authenticated device could respond to the new device’s request message.
The new device starts the Authentication Process with the
neighbor device.
• Neighbor response without Badge: In case no neighbor
device has the Badge. The beacon message is periodically
broadcasted to allow the device to probe other devices.
• No response: There is no neighbor device in the radio
range of the new device. In this case, the new device is
isolated in the network. In order to get connected to the
home network, the new device should be moved into a location from where the device can reach some other devices
in the network.
Only the home server can generate a Badge and grant it to
a device, even though the device is authenticated by one of its
neighboring devices. For ease of understanding, we list the notations used in the paper in Table I.
TABLE I
G LOSSARY

Notation
IDX
npwd
KX
KXY
Fig. 2. Multi hop Authentication Domain with Trust Propagation
•

Authentication Domain: When a user wants to introduce
a new device D to be used in an existing home network,
the device needs to be authenticated by the home server S
of the network. According to our system model, it is not
guaranteed that the the new device is located in the radio
range of a home server. In case D is out of range of S, it
tries to be connected to other authenticated devices to securely reach S. We define Authentication Domain (AD) as
a group of authenticated devices. Every home network has
just one authentication domain. Each device in a AD is allowed to communicate with other devices in the AD using
a secure channel. Specifically, all nodes that have same
Badge are part of the authentication domain AD(Badge).
Figure II-B shows an example of Authentication Domain.

III. T RUST-P ROPAGATION BASED AUTHENTICATION
P ROTOCOL
Once a new device D is initiated to be added to the user’s
home network by being provided with the password, it broadcasts a beacon message (also called hello message) to discover

RX
TS
L
AT H

REP (X)
RQA(X)
ALG

Description
Identity of device X
Initial shared network password for device
checking
X’s master key which is only shared with a
home server
Session key for X and Y, shared key between X
and Y
A random nonce generated by X
Timestamp
Life time of a key or Badge
Uses of the Badge. The Badge could be used
for various uses according to this limited authority
Report message about authentication of device
X
Request message of access to device X
An algorithm indicated by a home server

Our proposed protocol consists of two main phases. The initial authentication phase is followed by the expanded authentication phase. As mentioned earlier, our authentication process
is initiated by the password agreement. At this time, we assume
all devices have the same password in the hardware at the start
of the protocol.
A. Initial Authentication Phase: Phase I
Neighbor devices of the home server get securely authenticated by the four protocol packet exchanges as shown in Fig-

ure 3.

After having received Msg 2, device A sends the server back
a message encrypted with its own private key containing the
random number from the server and its own random number.
When the server decrypts Msg 3 with device A’s public key and
retrieves the two random numbers, the server authenticates that
the remote device is actually the device A.
[Msg 4] S ⇒ A : {Badge(A), KA , L}A pub
Badge(A) = {IDA , AT H, L}S priv

Fig. 3. Authentication in one hop from the server

In most of the proposed authentication protocols so far, it has
been assumed that the shared secret key is manually distributed
and typed, and then authentication protocol and key distribution
are built on this assumption. In a home network, it is unreasonable to expect a network user (a family member) to type a long
shared secret key to every device. It is also unreasonable to
assume that every device is physically connected to the home
server. Therefore, we use a simple password-based scheme.
Before a device is used for the first time in a home network,
a user manually enters a network password (shared secret) in
the device as its only network authentication key. This network
password is a weak human-memorizable password because of
it’s limited length. It is only used for verifying if the device
is user’s network device. We discuss the proposed protocol by
describing the packet exchange one by one. We define nine
protocol packets represented by [Msg number].
After bootstrap, any device broadcasts hello message to announce its presence. If there is an authenticated device including the home server, the device informs unauthenticated neighbor devices. An unauthenticated device, device A, sends Msg 1
to the home server, S as below.
[Msg 1] A ⇒ S : IDA , A-pub, {IDA , RA , T S}npwd
Device A generates a large random number RA as a challenge, and then unicasts the random number and time stamp
T S encrypted with a network password, its identity and public
key. At manufacture time, each device is given a public/private
key pair certified by the manufacturer in tamper resistant memory. When the home server receives Msg 1, it decrypts the
message with the network password the user registered. The
home server S confirms the freshness of the message (to prevent replay-attack by an adversary) by examining the decrypted
time stamp.
[Msg 2] S ⇒ A : IDS , S-pub, RS , {IDS , RA , RS }S priv
After checking the time stamp, the home server sends device A back a message containing the random number (RA )
encrypted with the network password and its own random number (RS ) as a challenge (Refer to Msg 2 above). When device
A gets Msg 2, it uses the server’s public key S−pub to decrypt
the last argument of the message to obtain RA and RS . The
message must have come from the home server, since a malicious device is not able to determine RA and encrypt it with the
server’s private key. Furthermore, it must be fresh and not be a
replay since device A just broadcasted RA .
[Msg 3] A ⇒ S : IDA , {IDA , RS , RA }A priv

Through Msg 1 to 3, not only does the server authenticates device A but device A also authenticates the home server (mutual
authentication). After the device A becomes aware of a part
of the network, the home server gives device A Badge and a
master key which is only shared between the home server and
device A so that it will be used for the remainder secure connection with the server instead of using the asymmetric key. The
Badge is a certificate signed and issued by the server as was described in an earlier section. It gives device A a permission to
authenticate other devices in its neighborhood on behalf of the
server whenever those devices cannot directly perform authentication procedure with the server due to their physical distance
from the server. In other words, if a home server puts another
device D’s identity instead of A’s identity in a Badge, it means
that the server has allowed the device D to act as the server for
authentication purpose.
B. Expanded Authentication Phase: Phase II
A device physically located where it cannot directly reach a
home server tries to get authenticated by the aid of an already
authenticated neighboring device with authentication authority.
The procedure requires devices’ relaying messages to the home
server as shown in Figure 4.
During Hello message exchange, device B is informed from
device A that device A is already authenticated by a home
server. But device B still needs to verify the claim of device
A. First, device B sends Msg 1 to device A.
[Msg 1] B ⇒ A : IDB , B-pub, {IDB , RB , T S}npwd
Suppose that device A is previously assigned Badge and a
master key by the server in Phase I, and device B is two hops
away from the server. The neighbor device A, which is in the
radio range of a device B, sends back a response to the challenge of a device A with Msg 5.
[Msg 5]
A ⇒ B : IDA , S-pub, RA , {IDA , RB , RA , Badge(A)}B pub
This response message includes Badge assigned by the
server. This message notably claims that device A is authorized
by a home server and has a permission to authenticate other device with Badge. When device B gets Msg 5, device B can easily verify that not only Badge was issued by the server since it
is encrypted by the server’s private key, but also the server gave
device A authority to authenticate others since Badge includes
device A’s identity and the authority of authentication.
[Msg 3] B ⇒ A : IDB , {IDB , RA , RB }B priv
After verifying the Badge sent from device A, device B sends
a response to device A in Msg 3. Of course, device B discards

Fig. 4. Authentication in more than one hop from the server

Msg 5 if there is no Badge in it or Badge is not issued by the
server.
[Msg 6] A ⇒ S : IDA , {IDA , REP (B), B-pub}KA
Authentication of device B is performed by neighbor device A
through Msg 1 to 3. After the authentication of device B, device
A reports the fact to the server in Msg 6. REP (B) is the report
message. Msg 6 is encrypted with the master key. Note that
the purpose of the master key is to enable the use of symmetric
cryptography for encryption/decryption instead of asymmetric
cryptographic technique, which consume more processing resources. Hence, any communication message between a server
and a device is encrypted using the master key after the master
key has been assigned.
[Msg 4] S ⇒ B : {Badge(B), KB , L}B pub
After receiving Msg 6 from device A, the server generates a
master key and makes Badge for device B, and then sends device B Msg 4 encrypted with device B’s public key which was
informed by device A in Msg 6. As a consequence, the trust is
propagated two hops away from the server. In a similar manner,
trust can be propagated multiple hops away from the server.
C. Security Access Control for Device Communication
A primary concern in home networks is access control, the
specification of how home network devices are allowed to interact with one another. The devices in an Authentication Domain are all networked with a home server through each master
key. For security purpose, however, they should be still have
only limited access to other devices in the domain. In other
words, even though a device belongs to the domain after being
authenticated, the device access must be controlled by the home
server in regards to what modes of accesses it is allowed to perform on other devices. This access control is managed by the
home server based on an access control list which is an array of
entries with the following format:
• subject: an identifier of the device in Authenticated Domain
• authorization: an indicator of the rights being granted to
the subject
• security level: for security services, a secure level of device categorized by a server. It is based on the capability

and security service required by the device.
Once a device is part of the Authentication Domain, which
means Badge and a master key has been given to the device,
the device is able to access the home server through the master
key. Whenever a device wants to establish a secure connection
with another device it contacts the home server and follows the
protocol as shown in Figure 5. The device sends an access request (Refer to Msg 7) to the home server. Second, the server
checks the access control list and distributes a session key to
the involved devices separately, which will be used for end-toend communication between the two devices. The length of this
key and algorithm for the communication is decided according
to the security level of access control list in the server. The goal
of access control constrained by a security level is to provide
appropriate access/service for heterogeneous home devices.
[Msg 7] B ⇒ S : IDB , {IDB , RQA(C)}KB
Suppose that a device B and C are in an Authentication Domain, which implies that each of them has its own master key
and Badge. Whenever device B wants to establish secure channel with device C, it sends the server a request access message
(RQA(C)) encrypted by its master key. The server checks the
access right in the access control list. If device B has the access right of the device C, the server generates a session key,
KBC , and then sends it to the both parties separately using the
following device specific Msg 8:
[Msg 8] S ⇒ B : {ALG, KBC , L}KB
[Msg 8] S ⇒ C : {ALG, KBC , L}KC
This session key is used not only for confidential data transmission but also message origin authentication during secure
communication. That is, any message encrypted with the session key after the authentication is believed to originate from
the peer principal who holds the session key. When the server
distributes the session key, the server assigns the appropriate
length of the session key which depends on the capability of
a device, and also lets devices have a choice for an appropriate encryption algorithm, which will be used for encrypting/decrypting communication message between devices. In
home networks, this session key length-agile and algorithmagile technique [5] [6] is useful for device communication session since various traditional computing and embedded Internet

Fig. 5. Establishment of a secure connection with another device

devices can be networked. Different applications of heterogeneous devices need different security requirement for communication sessions with one another since certain devices may
take a long time to encrypt/decrypt messages in each session.
After the setup, device B could send device C data messages
encrypted with the assigned session key.
[Msg 9] B ⇒ C : {M essage}KBC
IV. E VALUATION
A. Security Analysis
Man-in-the-middle attack: The adversary intercepts the messages between the parties (a device and the server) and replaces
them with its own messages. It plays the role of the device in
the messages it sends to the server and at the same time plays
the role of the server in the messages that it sends to the device.
Prevention: When a device which is more than one hop away
from the server broadcasts a random number and a timestamp
encrypted with a network password, its identity and its public key, an adversary puts himself between the parties on the
communication line where it is one hop from the server. The
parties would end up knowing the adversary’s public key as
a server’s or the same home network device’s. However, assuming that the network password has not been guessed or
compromised, the adversary has no way to construct the encrypted message with a network password, so it is forced to
resend {IDA , RA , T S}npwd with it’s own identity and public
key. When the server receives this message from the adversary,
the server discards it since the decrypted identity with the network password does not match the identity which the adversary
sends. Note that in case, the adversary tries to use the device’s
identity, this would be detected if the device continues to passively hear other on-going transmissions while the authentication process is not completed. If a device overhears a transmission with its own identity being used, it can alert the user in
device-dependent manner that an intruder has been detected.
Badge reuse attack: The Badge is transmitted in the following two cases. One is when a server grants a device a Badge
after authentication. The other is when an authorized device,
which has a Badge, sends it to the device in unauthenticated
domain for authenticating on behalf of a server. The adversary intercepts the message which includes a Badge and reuses

it, impersonating an authorized device, when a device in unauthenticated domain requests for an authentication.
Prevention: The transmission of a Badge is always encrypted
with receiver’s public key. This makes it hard for an adversary to get a Badge itself without possessing receiver’s private
key. However, we can think that the adversary intercepts the
message encrypted with receiver’s public key which includes
a Badge. The adversary then reuses it appropriately when authentication is requested by a device in unauthenticated domain.
In this case, the receiver figures out that the device’s identity
does not match with the identity in the Badge when it opens the
Badge.
V. C ONCLUSIONS
In this paper, we have proposed a reliable and adaptable
authentication and secure channel establishment protocol for
multi-hop wireless home environment. Central to our design
effort is the authentication by a neighboring device, on behalf
of the server, in a multi-hop wireless home networks, while supporting mutual authentication and minimizing reliance on public key cryptographic operation. In addition to this, our protocol supports efficient and flexible channel establishment among
heterogeneous devices after authentication. Through security
analysis, we show our protocol is secure because it is resistant
to various attacks. Further, it is efficient and adaptable for multihop wireless home network because it minimizes overheads
such as communication and computation costs.
ACKNOWLEDGEMENT
This work is supported in part by a grant from the Center for
Embedded Systems (CES).
R EFERENCES
[1] Zigbee Specification, version 1.0, http://www.zigbee.org.
[2] S. Lee, S. Banerjee, and B. Bhattacharjee, The Case for a Multihop Wireless Local Area Network, Proc. IEEE INFOCOM, vol. 2, pp: 941-951, Mar.
2004.
[3] Bluetooth Specification, http://www.bluetooth.com.
[4] Multi-hop Mesh Network, http://www.intel.com/technology/comms/cn02032.htm.
[5] P. Krishnamurthy, J Kabara, and T. Anusas-amornkul, Security in Wireless Residential Network, IEEE Transactions on Consumer Electronics,
Feb 2002.
[6] T.D. Taman et al., Algorithm-agile encryption in ATM networks, IEEE
Computer, pp. 57-64, Sep 1998.

2009 International
International Conferences
Conference on
2009
on Embedded
Embedded Software
Software and
and Systems
Systems

A Genetic Algorithm Based Approach for Event Synchronization Analysis in
Real-time Embedded Systems
Yan Chen1, Yann-Hang Lee2*
1

Xiaofeng Xu3, W.Eric Wong4, Donghui Guo1
3

The School of Information Science and Technology,
Xiamen University, China.
2
Computer Science and Engineering Department
Arizona State University, U.S.A.

Concurrent tasks in real-time embedded systems
usually communicate through message queues or shared
variables. Enforced by RTOS, the operations on message
queues are atomic. Similarly the accesses to shared
variables should be guarded by semaphores. However,
multiple accesses to message queues or shared variables
may be interleaved in arbitrary orders. This leads to the
so called message races [3] and semaphore races [4].
Figure 1. shows an example of a semaphore race where
tasks 1 and 2 can take the semaphore in distinct orders in
different execution scenarios.
In the past two decades, many static and dynamic
approaches [3], [4], [5], [6] are proposed to detect race
conditions in concurrent programs. However, the
problem of detecting all feasible race conditions is NPhard in general cases [2]. Due to the scheduling and
timing characteristics in real-time embedded systems,
many race conditions detected by those approaches may
be infeasible in practice. For instance, in the example of
Figure 1. , if we assume the system uses priority-based
preemption scheduling algorithm, the priority of Task 1
is higher than Task 2 and they are released at the same
instant, then the execution sequence (1) would always
happen, while the execution sequence (2) would never
happen. So, in order to ensure the correctness of realtime embedded systems, it is necessary to know exactly
whether a race condition is feasible or not, i.e., whether a
specific case of synchronization order of some potential
events can happen in an execution. If so, it is necessary
to find a corresponding execution sequence which
satisfies the case for understanding, analysis and testing.
To find the possible execution sequences, we can
consider a parametric analysis about the timing instants
that tasks are released and external events occur. Such an
analysis is not only time consuming, but also have to
face the exponential increase of state spaces in the
execution model. In this paper, a new method, based on
a genetic algorithm, is presented to analyze event
synchronization in real-time embedded systems. There
are several contributions of this method:
1. Timed event automata (TEA) are presented to describe
the timing behavior of a target real-time system, and
each state in the TEA is a synchronization event.
2. A race condition graph (RCG) [17] is used to specify
the synchronization order of events which have races.
3. A genetic algorithm (GA) working with a simulationbased approach [14], [15] is used to verify whether the

Abstract
In real-time embedded systems, due to race conditions,
synchronization order between events may be different
from one execution to another. This behavior is
permissible as in concurrent systems, but should be
fully analyzed to ensure the correctness of the system.
In this paper, a new intelligent method is presented to
analyze event synchronization sequence in embedded
systems. Our goal is to identify the feasible sequence,
and to determine timing parameters that lead to these
sequences. Our approach adopts timed event automata
(TEA) to model the targeted embedded system and use a
race condition graph (RCG) to specify event
synchronization sequence (SYN-Spec). A genetic
algorithm working with simulation is used to analyze
the timing parameters in the target model and to verify
whether a defined SYN-Spec is satisfied or not. A case
study shows that the method proposed is able to find
potential execution sequences according to the event
synchronization orders.
Keywords – embedded system, event, synchronization
sequence, timed event model, race condition, genetic
algorithm.

1 Introduction
Real-time embedded systems are computing systems
that must react within precise time constraints to current
events in the application environment. A reaction that
occurs late could not only be useless but also
catastrophic. A real-time system must have some notion
of time. The time base can be absolute, corresponding to
a physical clock, or relative, based on specific events. A
synchronization primitive can be implemented to
establish and maintain ordered execution between
computational tasks [1]. However, because of race
conditions [2], which are caused due to the nondeterminism in the inter-tasks communication and
synchronization mechanisms, the order that the
synchronization operations take place may be different
from on execution to another. This can result in different
order of concurrent operations which may lead to
incorrect behavior and output. Even if the orders of
concurrent operations are permissible, they should be
analyzed thoroughly to ensure the correctness of the
systems [1].
978-0-7695-3678-1/09 $25.00 © 2009 IEEE
DOI 10.1109/ICESS.2009.48

The Department of Physics, Xiamen University, China.
4
The School of Engineering and Computer Science,
University of Texas at Dallas, U.S.A.
*
Corresponding Author: yhlee@asu.edu

201
199

events synchronization order specified (SYN-Spec) is
satisfied or not by the target TEA; if it is satisfied, a
corresponding execution sequence is given.

cannot possibly result in a data race because of schedule
restrictions enforced by event posts and waits. Fumihiko
[13] presented a new parallel computational model, the
LogGPS model, which was useful to analyze
synchronization costs of parallel programs that used
message passing. All these approaches do not consider
scheduling and timing, so they cannot be used for
checking the feasibility of a race in real-time system.
Johan and Anders [14], [15] presented a simulationbased approach to do the impact analysis of real-time
system. In their approach, a simulation model of a target
system, which is described by ART-ML, is extracted
from both static and dynamic analysis. Though it was
only used for timing analysis, the simulation-based
approach can be used to do much further analysis of realtime systems.
In our previous work, we have proposed a
technology based on model checking to verify race
conditions in real-time embedded systems [16], and a
race condition graph to analyze the concurrent program
behavior is presented in [17]. In this paper, we propose a
new method to analyze the event synchronization order
based on simulation-based technology according to race
conditions in the target systems.

Binary Semaphore s = available;
Task 1
……
semTake(s);
//Access the critical resource
semGive(s);
……

Task 2
……
semTake(s);
//Access the critical resource
semGive(s);
……

(a) A program with two tasks
T1 Semaphore T2

T1 Semaphore T2
t1

t1

g1

g1
t2
g2

t2
g2
(2)

(1)

(b) Two execution sequences with different event
synchronization orders
Figure 1.

3 Embedded System Modeling And Simulation

An example of semaphore race

The analysis approach for event synchronization
order is shown in Figure 2. . There are two inputs to the
analysis: a target system which is represented as TEA
and a specific synchronization order SYN-Spec which is
described by RCG. Then, an execution algorithm is used
to simulate the target TEA based on a priority-based
preemptive
scheduling
algorithm,
inter-task
synchronization and communication constructs and
virtual clock. At last, GA working with simulation is
used to analyze the timing parameters in the TEA to
verify whether the SYN-Spec is satisfied or not. If the
SYN-Spec is satisfied, a corresponding execution
sequence is obtained.

The rest of the paper is organized as follows. In
Section 2, some related works are given. Section 3
presents the real-time system modeling and simulation
with TEA. Section 4 describes how to use RCG to
specify the events synchronization order. In Section 5, a
GA is presented to working with simulation approach to
analyze the timing parameters. Then, a case study of
real-time dining philosopher program is shown up,
followed by a conclusion and the future work.

2 Related Works
A well-known approach which can be used to
analyze and verify the real-time embedded systems is
model checking technology. In this approach, a system is
represented by a kind of formal model based on finite
state machine, such as timed automata [7], timed I/O
automata [8], and linear hybrid automata [9] and so on.
Properties required are specified by a kind of temporal
logic language, such as LTL, CTL, and TCTL [10], or
even by another formal model. The algorithms for model
checking are typically based on an exhaustive state space
search of the model of the target system: for each state of
the model it is checked whether it behaves correctly, that
is, whether the state satisfies the desired specification. In
its most simple form, this technique is known as
reachability analysis. The disadvantage of model
checking technology is the state explosion problem. So,
its capacity is restricted by the huge program state
spaces.
Perry [11] described a system that automatically
detects races in a parallel program. In this approach, the
dynamic execution trace of the program was used to
build a task graph and logged points of event style
synchronization. David [12] developed a static analysis
method for determining which dependences in a program

Target System
(TEA)

SYN-Spec
(RCG)

Genetic Algorithm
Simulation
Execution Algorithm
Scheduling
Algorithm

Synchronization &
Communication
Constructs

Virtual Clock

No

Figure 2.

Yes

Architecture of the method

3.1 Syntax and Semantics of TEA
A real-time system is composed of multiple
concurrent tasks with a scheduling algorithm to control

202
200

the transition a is triggered, and then γ (a) is
executed.
The
syntax
of
γ
(a)
is:

CPU resource. These tasks synchronize and
communicate with each other by using message passing,
shared variables, etc. In this paper, timed event
automata (TEA) are used to describe the timing
behaviors of a real-time system. It includes
synchronization events (SYN-Event), such as sending
message (MS), receiving message (MR), taking
semaphore (ST), giving semaphore (SG) and task delay
(TD) and so on.
Definition 1: SYN-Event. Each SYN-Event e in a realtime embedded system is defined as a 5tuple e =< p, λ , o, x, i > , where p is the calling
task; λ maps e into one of the following types: MS, MR,
ST, SG, TD; o is the operating object of e, which may be
a message queue, a semaphore or even null; x is the
related data of e, which may be a string, an integer, a
parameter or even null; i is an unique identifier of e. The
relation of λ , o and x is shown in TABLE I. .
TABLE I.
Event type ( λ )

γ (a) ::= L := R | (γ 1 ∨ γ 2 )

MS

Message Queue

Data Sent

Message Queue

Data Received

SG

Semaphore

Null

ST

Semaphore

Null

TD

Null

Delay Time

L ::= x

and

3.2 Simulating the TEA
The simulator engine is based on four parts: scheduler,
synchronization & communication constructs, virtual
clock
and
execution
algorithm.
Let
S =< P, X , E , A,τ , μ , γ > be a TEA, ∀ p ∈ P has three
states: ready, run, wait. A priority-based preemptive
scheduling algorithm [20] is used to schedule the tasks
and control the states transitions of tasks. With the
synchronization & communication constructs which
include message-passing, semaphore and task delay,
∀ e ∈ E is processed according to the characteristic of
each type of SYN-Events during an execution.
For timing-accurate simulation, TEA uses timing
predicate τ to describe the amount of CPU time required
by a task to execute from one SYN-Event to the next, i.e.
the execution time of the code between these model
events. In this simulation approach, a virtual clock C is
used to calculate the execution time of TEA. When an
execution is started, C is increased steadily until the
execution is ended. Let ai(ei , ei+1) ∈ A be a transition, and
τ (a i ) = [te(ai)][min(ai), max(ai)] be a timing predicate of
ai, then the algorithm for time consume from ei to ei+1 is
shown in Figure 3. . In this algorithm, the function of
getGlobalClockValue() is used to read the value of
virtual clock C.

Related Variable (x)

MR

and

R ::= n | x | ( x ~ y ) | ( x ~ n) , where x and y are
variables, n ∈ R+; ~ ∈ { +, − , *, /}.

Synchronization Events

Operating Object (o)

,

In this paper, we focus on the synchronization and
communication constructs with FIFO queues of
asynchronous message passing, binary semaphore and
counting semaphore, but other synchronization events
can also be added according to the requirement of cases.
Definition 2: TEA. A real-time embedded system is
described
as
a
TEA
which
is
a
7tuple: S =< P, X , E , A, τ , μ , γ > with the following
restrictions. P is a set of tasks; ∀ p ∈ P is a periodic or
an aperiodic task, including task name, task ID, task
priority, and entry and exit nodes. X is a set of variables
which may be integer variables (int), timing parameters
(para), message queues (mq), binary semaphores (bs) or
counting semaphores (cs). E is a set of SYN-Events.
A ⊆ E × E is a set of transitions. ∀ a(es, et) ∈ A, such
that

Process timeConsume (Transition a)
t = te(ai);
cc = getGlobalClockValue();
pc = cc;
while (pc − cc) < t, do
pc = getGlobalClockValue();
end while
End process
Figure 3.

(1)
(2)
(3)
(4)
(5)
(6)
(8)
(9)

An algorithm for time consume

In a simulation, all tasks in TEA are concurrent tasks
and the execution algorithm of each task is shown in
Figure 4. . In this algorithm, statement (2) means a task
executes from a “start” node; statement (5) searches for
the next transition a according to es and the
corresponding local predicates; statement (6) calls
timeConsume() function to consume execution time from
current event to the next event; statement (7) is used to
do a set of assignments; statement (8) gets the next event
and statement (9) uses executeEvent() to execute the
event according to the event type. Note that an executing
task may be preempted at any time by the other ready
task with higher priority.

♦ τ (a) is a timing predicate describing the effective
execution time and timing constraint from a source
event es to a target event et of a, and τ (a) ::= [x] |
[x] [y, z]
| [x] [y, +∞ ], where
x ∈ R+ ∧ y ∈ R+ ∧ z ∈ R+ ∧ z ≥ y, [x] is the effective
execution time, and [y, z] is a timing constraint.
♦ μ (a) is a local predicate describing the triggering
condition of a, and μ (a)::= true | (x ~ n) | (x ~ y) |
¬μ | ( μ 1 ∧ μ 2), where x and y are variables;
n ∈ R+; ~ ∈ { ≤ , < , = , !=, > , ≥ }; ¬ and ∧ are
Boolean negation and disjunction respectively. If
μ (a)::= true, it is usually omitted.

Process taskExecution
es = “start”;
et = null;
while not et = “end”, do
a = getNextTransition(es);

♦ γ (a) is a set of assignments. If τ (a) ∧ μ (a) = true,

203
201

(1)
(2)
(3)
(4)
(5)

timeConsume(a);
executeAssignment( γ (a));
et = getNextEvent(a);
executeEvent(et);
es = et;
end while
End process
Figure 4.

Figure 5.

(6)
(7)
(8)
(9)
(10)
(11)
(12)

Assume the timing parameters x and y equal to 4 and
1, Figure 5. (c) is an execution of the TEA. First, T1 is
selected to run by the scheduler as it has a highest
priority in all ready tasks; e1 of T1 is a task delay event
which makes it to sleep for 4 time units, then the
scheduler picks up T2 to run. The event e1 of T2 is also a
task delay event which makes it to sleep for 1 time units,
and T2 goes on executing after waking up. Because the
execution time from e1 to e2 in T2 is 1 time unit, the total
time spent on the first transition of T2 is 2 time units.
During the transition from e2 to e3 in T2, T1 wakes up,
so T2 is preempted by T1 which has a higher priority.
When T1 is going to simulate e2, which is a taking
semaphore event, it is blocked because the semaphore
has been taken by T2, so context switches to T2 again.
After T2 gives the semaphore by e3, it is preempted by
T1 because the semaphore is ready now. Finally, T1
finishes firstly and T2 finishes later. In this simulation,
the time spent on the transition from e1 to e2 in T1 is to =
4 + 1 + 1 = 6, while the time spent on the transition from
e2 to e3 in T2 is to = 1 + 1 + 1 = 3, so the execution
satisfies the timing constraints of the TEA, i.e., the
execution is feasible. In Section 5, we will use a GA to
generate the values of timing parameters according to the
SYN-Spec.

An execution algorithm of each task in TEA

Let to(ai) be the total time spent on the transition ai,
we have to(ai) = te(ai) + td(ai) + tb(ai), such that:
♦ te(ai) is the effective execution time of ai. It is the
time spent executing run-time or system services on
its behalf, without being pended or delayed.
♦ td(ai) is the sleeping time of ai. It is caused by a task
delay event TD.
♦ tb(ai) is the time spent by ai blocked due to the
unavailability of a resource, such CPU, semaphore,
or message.
Therefore, a feasible execution of TEA should
satisfy the following inequality: ∀ ai ∈ A,
min(ai) ≤ to(ai) ≤ max(ai).
Figure 5. (a) and (b) show a simple example of TEA.
It includes two tasks T1 and T2, and the priority of T1 is
higher than T2. In these two tasks, there are timing
constraints between e1 and e2, x and y are two timing
parameters.
Priority(T1)=1;
Priority(T2)=2;
Binary semaphore: s1;
Timing Parameter: x, y;

T1

4 Specification Of Event Synchronization
Order

to=1
T2

T1

T2

An execution of TEA exercises a sequence of
synchronization events. Let S =< P, X , E , A,τ , μ , γ > be
TEA, an execution sequence generated by a simulation
is denoted as: Q = {<t, e> | t ∈ R+ is the happened time
of e, e ∈ E}. There are two characteristics of Q:
♦ Q satisfies happened-before relation, denoted as
HB
“ ⎯⎯→
⎯ ”, which is a partial order over the SYNEvents and shows the sequence of events that
potentially affect one another [19].
♦ Q has a synchronization order, which is a total order
over all of the SYN-Events of an execution.
With a postmortem approach [3], an execution
sequence is analyzed to detect race conditions. In this
section, RCG is used to specify the synchronization
order of SYN-Events which have races.

TD(4)
to=1

[1]

[1]

e1:
TD(x)

e1:
TD(y)

TD(1)

[1][1, 10]

td=4

to=2

[1][1, 10]

e2:
ST(s1)

e2:
ST(s1)

[2]

[2]

e3:
SG(s1)

e3:
SG(s1)

[1]

[1]

ST(s1)
te=1
te=1
te=1
ST(s1)

SG(s1)

4.1 Race Set
Let Q be an execution sequence. A race set of Q is
HB
given as a triple RS = < ER , ⎯⎯→
⎯ , ⎯RD
⎯→ >, where ER
HB
is a finite set of events and ER ⊆ E and ⎯⎯→
⎯ and

to=2
SG(s1)

(a)

Examples of a timed event model and an potential
execution sequence

⎯RD
⎯→ are relations defined over ER.

(b)
to=1

The race related events in the set of ER, are the events
that have direct relationship with race conditions in Q.
The race-dependence relation, denoted by ⎯RD
⎯→ ,
shows the relative order in which events execute and the
race dependence with each other. In this paper, we focus
on message races and semaphore races. Assume there
are 3 events a, b, c ∈ E, i.e., a, b and c are events in the

te=1

(c)

204
202

SYN-sequence Q:
♦ Assume a and b are sending message events, c is the
receive message event, and there is a message race
between events a, b with respect to c, i.e., c may
receive the message from a first in one execution or
even receive the message from b first in another
execution. If a comes before b in the Q, then
a ⎯RD
⎯→ b, otherwise, b ⎯RD
⎯→ a.
♦ Assume a and b are taking semaphore events, and
there is a semaphore race between events a and b,
i.e., event a may take the semaphore firstly in one
execution or b may take the semaphore firstly in
another execution. If a comes before b in the Q, then
a ⎯RD
⎯→ b, otherwise, b ⎯RD
⎯→ a.

ST(T3, 2, s3) ⎯RD
⎯→ ST(T4, 4, s3);
ST(T4, 2, s4) ⎯RD
⎯→ ST(T5, 4, s4);
ST(T1, 4, s5) ⎯RD
⎯→ ST(T5, 2, s5)};
Figure 7.

Let G=<V, L> be an RCG with respect to an
execution sequence Q, and G’=<V’, L’> be a SYN-Spec.
If every vertex v ∈ V’ also belongs to V and every edge
l ∈ L’ also belongs to L, i.e., G’ belongs to G, denoted by
G’ ⊆ G, then we say that Q satisfies the SYN-Spec G’,
denoted by Q |= G’.
Proposition. A real-time system S described by TEA
satisfies a SYN-Spec G described by RCG, if and only if
there is at least one execution sequence Q obtained by
simulating TEA and Q |= G.

4.2 Race Condition Graph

5 Genetic Algorithm Based Approach for
Parametric Analysis

HB
Let RS = < ER , ⎯⎯→
⎯ , ⎯RD
⎯→ > be a race set of an
execution sequence Q. An RCG is a graph G = <V, L>,
where V = ER is the set of vertices of the graph G and
L ⊆ V2 is the set of relations of V. There are two kinds
of L in an RCG:
1) If events a ∈ ER, b ∈ ER and a ⎯HB
⎯→ b, a solid
arrow “ ⎯
⎯→ ” is used to show their happenedbefore relation in the RS.
2) If events a ∈ ER, b ∈ ER and a ⎯RD
⎯→ b, a dashed
arrow “- - ->” is used to show their racedependence relation in the RS.

Because of the non-determinacy of real-time
embedded systems, there usually have some parameters
unknown which influence the timing of the execution. In
this paper, a GA based approach is used to search
heuristically these timing parameters in a TEA. The goal
of the analysis is to identify a set of timing parameters
that result in a specific SYN-Spec. If no such timing
parameters can be found, we can conclude that the SYNSpec is not a valid one.
5.1 Chromosome Encoding
Assume there are n timing parameters in the TEA. In
the GA approach, each timing parameter xi (i ≤ n) is
represented as a binary string with the length of Li ,
which is decided by the range and precision of xi . A
chromosome is composed of a set of binary strings,
which is shown in Figure 8. .

The SYN-Spec, i.e., the synchronization order of
SYN-Events, can be described by an RCG. For example,
Figure 6. shows a SYN-Spec of the dining philosopher
case (which will be introduced in Section 6).
T1

T2

T3

T4

T5

e2:
ST(s1)

e2:
ST(s2)

e2:
ST(s3)

e2:
ST(s4)

e2:
ST(s5)

e4:
ST(s5)

e4:
ST(s1)

e4:
ST(s2)

e4:
ST(s3)

e4:
ST(s4)

x2

x1
b11

Figure 6.

The corresponding race set of the example

b12

…

L1

Figure 8.

An example of SYN-Spec

b21

b22
L2

xn
…

…

bn1

bn2

…

Ln

A chromosome encoding

5.2 Fitness Function
The fitness function gives a quantitative measure of
the suitability of the generated timing parameters for the
purpose of satisfying the SYN-Spec. Let G’=<V’, L’>
be a SYN-Spec. According to the proposition mentioned
in Section 4.2, if the TEA satisfies G’, then there is a
new RCG G=<V, L> derived from a new execution
sequence obtained by simulating the TEA, and G’ ⊆ G.
Therefore, in this paper the fitness function is used to
measure the similarity of G’ and G, i.e., the similarity of
two RCGs.
In order to know the similarity of two RCGs, it needs
to calculate the difference between them. Figure 9.
shows an algorithm to calculate the difference between
a SYN-Spec G’ and an RCG G derived from a new
execution sequence obtained by simulating the target

In this SYN-Spec, there are 5 tasks (T1, T2, …, T5),
5 binary semaphores (s1, s2, …, s5), and 10 ST events.
Assume the notation λ (p, i, o) denotes the ith SYN-event
in the task p operating on the object o, the
corresponding race set of this example is shown in
Figure 7. .
RS = { ST(T1, 2, s1) ⎯HB
⎯→ ST(T1, 4, s5);
ST(T2, 2, s2) ⎯HB
⎯→ ST(T2, 4, s1);
ST(T3, 2, s3) ⎯HB
⎯→ ST(T3, 4, s2);
ST(T4, 2, s4) ⎯HB
⎯→ ST(T4, 4, s3);
ST(T5, 2, s5) ⎯HB
⎯→ ST(T5, 4, s4);
ST(T1, 2, s1) ⎯RD
⎯→ ST(T2, 4, s1);
ST(T2, 2, s2) ⎯RD
⎯→ ST(T3, 4, s2);

205
203

TEA. The statement (1) initializes a variable named
difference. For each edge l’=<v’, u'> from G’, the
statement (3) aims to find the same vertices v and u in G
according to the SYN-event information described by v’
and u'. If it fails to find v or u in G, then we say that the
difference between G and G’ is infinity. The statement
(4) and (5) show that if the synchronization relation
between v and u isn’t the same as v’ and u’, then the
difference is increased.
Process calculateDifference (G’, G)
// G’ = <V’, L’ > is a SYN-Spec of the TEA;
// G = <V, L > is an RCG derived from an new
// execution sequence of the TEA;
set the difference = 0 initially;
for each edge l’=<v’, u' > ∈ L’
get the vertices v = v’ and u = u’ from V;
if the synchronization relation between v and u
isn’t the same as l’, then
difference = difference + 1;
end if
loop
return difference;
End process
Figure 9.

else
calculate the difference between G’ and G’;
end if
else
continue;
end if
loop
Step 3: Genetic operations.
Select one or two chromosome(s) from the population with
a probability based on fitness to participate in genetic
operations: selection, crossover, and mutation. Renew
individual chromosome(s) with specified probabilities based
on the fitness.

(1)
(2)
(3)
(4)

Step 4: Termination.
If the number of generations exceeds the upper limit
specified, then return fail; else go to step 2.

(5)
(6)
(7)
(8)

Specially, if a chromosome cause deadlock during a
simulation, the algorithm will mutate the chromosome
and repeat the simulation again and again until the
system runs smoothly.

Figure 10. Analysis process of the GA

6 A Case Study

An algorithm of calculating the difference
between two RCGs.

As a case study, a real-time dining philosopher
program, which is implemented on the VxWorks RTOS,
is used for the case study. There are 5 dining
philosophers with the same priorities: T1, T2, T3, T4
and T5. 5 chopsticks are represented by 5 binary
semaphores s1, s2, s3, s4, and s5. To eat their dinner, T1
takes s1 and s5, T2 takes s2 and s1, T3 takes s3 and s2,
T4 takes s4 and s3, T5 takes s5 and s4. Thus, there have
5 semaphore races in the program. In addition, to
represent a possible thinking time before picking up a
chopstick, there is a task delay event for random time
before each semaphore taking operation. So, there are
10 timing parameters in the system: x1, x2,…,x10. In
the study, we assume that for each timing parameter x1,
there has 1s ≤ x1 ≤ 8s; the timing constraint between
each task delay event and semaphore taking event is [1,
+∞].
Figure 11. shows the T1 described by TEA, the
effective execution time of each transition is calculated
from traces of original executions on the VxWorks OS
and a target processor board. The time unit is second.
Other tasks are described by TEA similarly. The static
data of the case is shown in TABLE II. .

Therefore, in the GA, the fitness of a chromosome
(i.e., a set of timing parameters) is dominated by the
difference value calculated using the algorithm shown
in Figure 9. : if the difference value between the SYNSpec G’ and a new RCG G becomes smaller, then G’
becomes more similar with G, i.e., the corresponding
chromosome has higher contribution and suitability; if
the difference value equals to 0, then G’ ⊆ G, i.e., the
SYN-Spec is satisfied by the target TEA.
5.3 Termination Condition
The genetic algorithm is terminated at the following
conditions: (1) it has successfully found a group of
timing parameters with which an execution sequence
generated by simulating the target TEA satisfies with the
SYN-Spec; (2) the number of generations exceeds the
upper limit and the genetic algorithm is terminated, at
the situation we consider that the target model does not
satisfy the SYN-Spec.
5.4 Analysis Process
Let S =< P, X , E , A,τ , μ , γ > be TEA and M is a set
of timing parameters. The process of analysis is
described in Figure 10. .
Step 1: Initialization.
Generate a set of initial chromosomes randomly.
Step 2: Calculate fitness.
for each chromosome, do
decode it and get a set of values X;
substitute X for M in S;
simulate S and get a new execution sequence Q;
if all timing constraints is satisfied in Q, then
generate a new RCG G from Q’;
if G’ ⊆ G, then
output the Q and X;
return success;

Figure 11. T1 described by TEA

206
204

TABLE II.

In this case study, the crossover rate of GA based
approach is 0.7, the mutation rate of GA is 0.001, the
revision rate is 0.5 and the number of initial population
of chromosomes is 6. After simulating the target TEA,
the result is shown in the TABLE III. . The result shows
that SYN-Spec 1 and 2 are satisfied by the TEA but SYNSpec 3 and 4 are not. SYN-Spec 3 is infeasible because a
global circle is included in SYN-Spec 3 [17], while the
SYN-Spec 4 causes a deadlock and the event e4 (take the
2nd chopstick) can never succeed.

Static Data of the case study

Items
Processes
Events
Transitions
Variables
Parameters

Number
5
35
40
15
10

Figure 12. shows 4 SYN-Specs described by RCGs,
which mean the different orders of taking chopsticks of
5 dining philosophers. For example, in SYN-Spec 1, T1
takes s1 before T2, T2 takes s2 before T3, T3 takes s3
before T4, while T5 takes s4 before T4 and T1 takes s5
before T5. The purpose of the case study is to verify
whether these event synchronization orders are possible
or not.
T1

T2

T3

T4

T5

e2:
ST(s1)

e2:
ST(s2)

e2:
ST(s3)

e2:
ST(s4)

e 2:
ST(s5)

e4:
ST(s5)

e4:
ST(s1)

e4:
ST(s2)

e4:
ST(s3)

TABLE III.

e 4:
ST(s4)

Result of the case study

SYNSpec No.

Result

Generations
of GA

1

Yes

61

2

Yes

578

3
4

No
No

Inf
Inf

Values of Timing
Parameters
x1=1, x2=1, x3=5,
x4=8, x5=2, x6=5,
x7=7, x8=1, x9=3,
x10=3
x1=1, x2=1, x3=7,
x4=7, x5=8, x6=1,
x7=6, x8=1, x9=3,
x10=2
None
None

7 Conclusion

(1) SYN-Spec 1
T1

T2

T3

T4

T5

e2:
ST(s1)

e2:
ST(s2)

e2:
ST(s3)

e2:
ST(s4)

e 2:
ST(s5)

e4:
ST(s5)

e4:
ST(s1)

e4:
ST(s2)

e4:
ST(s3)

e 4:
ST(s4)

T1

T2

T3

T4

T5

e2:
ST(s1)

e2:
ST(s2)

e2:
ST(s3)

e2:
ST(s4)

e 2:
ST(s5)

e4:
ST(s5)

e4:
ST(s1)

e4:
ST(s2)

e4:
ST(s3)

e 4:
ST(s4)

In this paper, a simulation-based analysis with GA
based search is presented to analyze event
synchronization in real-time embedded systems. The
TEA presented is composed of SYN-Events but not
system states. Instead of using reachability analysis, the
target TEA is simulated to generate execution sequences.
Via analyzing execution sequences with a postmortem
method, it is able to check whether a SYN-Spec
described by RCG is satisfied or not. In the mean time, a
GA based approach is used to analyze timing parameters
in the target TEA. Experiments show that the method
proposed can be used to find execution sequences
according to the event synchronization order defined by
user. As a consequence, the execution sequence defined
by SYN-Spec can be indentified. Also, test sequences
can be generated to verify the system operations.

(2) SYN-Spec 2

References
[1] N. Suri, M.M. Hugue, C.J. Walter. Synchronization
issues in real-time systems. Proceedings of the IEEE.
Vol.82(1), pp: 41-54. Jan 1994.
[2] R.H.B. Netzer and B.P. Miller. What are Race
Conditions? Some Issues and Formalizations, ACM
Letters on Programming Languages and Systems, Vol.
1(1), pp. 74-88. 1992.
[3] K.C. Tai, Race Analysis of Traces of Asynchronous
Message-Passing Programs, Proc. of ICDCS’97.
Baltimore, Maryland, USA. pp.261-268, May 1997.
[4] P.N. Klein, H.I. Lu and R.H.B. Netzer. Detecting Race
Conditions in Parallel Programs that Use Semaphores,
Algorithmica, Springer-Verlag New York Inc. Vol.35,
pp. 321-345. 2003.
[5] S. Savage, M. Burrows and G. Nelson. Eraser: A
Dynamic Data Race Detector for Multithreaded

(3) SYN-Spec 3
T1

T2

T3

T4

T5

e2:
ST(s1)

e2:
ST(s2)

e2:
ST(s3)

e2:
ST(s4)

e 2:
ST(s5)

e4:
ST(s5)

e4:
ST(s1)

e4:
ST(s2)

e4:
ST(s3)

e 4:
ST(s4)

(4) SYN-Spec 4
Figure 12. Specifications of events synchronization

207
205

[6]

[7]
[8]

[9]

[10]

[11]

[12]

programs, ACM Transactions on Computer Systems,
Vol. 15(4), pp. 391-411, 1997.
E. Pozniansky and A. Schuster. Efficient On-the-Fly Data
Race Detection in Multithreaded C++ Programs,
PPoPP’03, San Diego, California, USA. pp. 179-190. Jun
2003.
R. Alur, C. Courcoubetis, D.L. Dill. Model Checking for
Real-Time Systems, IEEE LICS, 1990.
DK Kaynar, N Lynch, R Segala, F Vaandrager. Timed
I/O automata: A mathematical framework for modeling
and analyzing real-time systems. In: Kaynar DK, Lynch
N, Segala R, Vaandrager F, eds. Proc. of the 24th IEEE
Int’l Real-Time Systems Symp. Washington: IEEE
Computer Society, pp. 166-177, 2003.
F. Wang. Symbolic Parametric Safety Analysis of Linear
Hybrid Systems with BDD-like Data-Structures. IEEE
Transactions on Software Engineering, IEEE Computer
Society. Vol.31(1), pp. 38-51. Jan 2005.
F. Wang, G.-D. Huang, F. Yu. TCTL Inevitability
Analysis of Dense-Time Systems: From Theory to
Engineering. IEEE Transactions on Software Engineering,
IEEE Computer Society, Vol. 32(7), July 2006.
P.A. Emrath, S. Chosh, D.A. Padua. Event
synchronization analysis for debugging parallel
programs. In Proceedings of the 1989 ACM/IEEE
conference on Supercomputing. Reno, Nevada, United
States. pp: 580-588. 1989.
D. Callahan, K. Kennedy and J. Subhlok, Analysis of
event synchronization in a parallel programming tool, in
Proceedings of the second ACM SIGPLAN symposium
on Principles & practice of parallel programming, Seattle,
Washington, United States, pp: 21-30. 1990.

[13] F. Ino, N. Fujimoto and K. Hagihara, LogGPS: a parallel
computational model for synchronization analysis, in
Proceedings of the second ACM SIGPLAN symposium
on Principles & practice of parallel programming,
Snowbird, Utah, United States, pp: 133-142. 2001.
[14] J. Andersson, J. Huselius, C. Norström and Anders Wall,
Extracting Simulation Models from Complex Embedded
Real-Time Systems. In Proc. of the International
Conference on Software Engineering Advances. Tahiti,
French Polynesia. pp.7-17. Oct. 29. 2006.
[15] J. Andersson, A. Wall, and C. Norström. A framework
for analysis of timing and resource utilization targeting
complex embedded systems. In ARTES - A Network for
Real-Time research and graduate Education in Sweden,
Editor: Hans Hansson, pp. 297–329. Uppsala University,
2006.
[16] Y.H. Lee, G. Gannod, K.S. Chatha, and W.E.Wong,
Timing and Race Condition Verification of Real-time
Systems, Progress Report, 2003.
[17] Y. Chen, Y.H. Lee, W.E. Wong and D.H. Guo, A Race
Condition Graph for Concurrent Program Behavior. Proc.
of ISKE’08, Xiamen, Fujian, China. pp: 662-667. Nov.
2008.
[18] Y. Lei and R.H. Carver, Reachability Testing of
Concurrent Programs, IEEE Transactions on Software
Engineering, Vol.32(6), pp. 382-403, 2006.
[19] L. Lamport. Time, Clocks, and the Ordering of Events in
a Distributed System. Communications of the ACM,
Vol.21(7), pp. 558-565, 1978.
[20] C.L. Liu, J.W. Layland. Scheduling Algorithms for
Multiprogramming in a Hard Real-time Environment(J).
Journal of ACM, Vol.20(1), pp. 46-61. 1973.

208
206

