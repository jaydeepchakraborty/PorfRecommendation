Preprocessing Time Series Data for
Classiﬁcation with Application to CRM
Yiming Yang1 , Qiang Yang2 , Wei Lu1 , Jialin Pan1 , Rong Pan2 , Chenhui Lu1 ,
Lei Li1 , and Zhenxing Qin3
1

2

Software Institute, Zhongshan University,
Guangzhou, Guangdong Province, China
yangym@gsta.com, sinno@sohu.com, luwei00@hotmail.com,
lncsri07@cs.zsu.edu.cn
Department of Computer Science, Hong Kong University of Science and
Technology,
Clearwater Bay, Kowloon Hong Kong, China
qyang@cs.ust.hk, panrong@cs.ust.hk
3
Faculty of Information Technology, University of Technology,
Sydney PO Box 123, Broadway NSW 2007 Australia

Abstract. We develop an innovative data preprocessing algorithm for
classifying customers using unbalanced time series data. This problem is
directly motivated by an application whose aim is to uncover the customers’ churning behavior in the telecommunication industry. We model
this problem as a sequential classiﬁcation problem, and present an effective solution for solving the challenging problem, where the elements
in the sequences are of a multi-dimensional nature, the sequences are
uneven in length and classes of the data are highly unbalanced. Our solution is to integrate model based clustering and develop an innovative
data preprocessing algorithm for the time series data. In this paper, we
provide the theory and algorithms for the task, and empirically demonstrate that the method is eﬀective in determining the customer class for
CRM applications in the telecommunications industry.
Keywords: Classiﬁcation of time series data for Telecommunications
Applications.

1

Introduction

With massive industry deregulation across the world, telecom customers are
facing a large number of choices in services. As a result, an increasing number
of customers are switching service providers from one institution to another.
This phenomenon is called customer “churning” or “attrition”, which is a major problem for these companies to keep proﬁtable. It is highly important for
these companies and institutions to identify the likely customers who churn and
formulate plans to combat the problem.
In machine learning and data mining research, the customer churning problems have been addressed in the framework of cost-sensitive classiﬁcation and
S. Zhang and R. Jarvis (Eds.): AI 2005, LNAI 3809, pp. 133–142, 2005.
c Springer-Verlag Berlin Heidelberg 2005


134

Y. Yang et al.

ranking [10,5,6,12]. For example, this was the central theme of the KDDCUP-98
competition [1]. However, the direct marketing problem in telecommunication
industry pose some new challenges. Similar to the traditional problem formulation, the data in the telecommunications industry are also highly unbalanced
with only 5% of customers who actually churn. However, unlike the KDDCUP98 problem, the telecom data is sequential in nature, where the elements of
the sequences are multi-dimensional in nature and the sequences themselves are
uneven. With sequential data, many traditional methods for ranking and classiﬁcation cannot be directly applied.
In this paper, we formulate the above problem as one of classiﬁcation of
time series data. This problem is diﬃcult due to several issues. First, these data
are unbalanced, where the positive data that correspond to churning customers
represent less than 5% of the population total. Second, each data record is a
time series consisting of monthly bills. From the bills we can also extract a
large number of features. Furthermore, these time series are of diﬀerent lengths,
some are quite long, while others are short. Third, it is not clear when the
customers have churned, given the bill. It may happen that some customers may
have already switched their main business to a competitor, while retaining some
remaining minor activity with the company in question. These problems make
it diﬃcult to design a classiﬁcation algorithm for identifying escaping customers
using the traditional methods. We model the problem as a two class sequential
classiﬁcation problem. We focus on how to transform the uneven data set into
one that can be applied to directly by any classiﬁcation algorithms. Thus, our
contribution is in the area of data preparation.
In particular, we consider those customers who churned as positive class, and
those who do not churn as negative data. We design an innovative method that
combines clustering and classiﬁcation under the framework of maximum likelihood estimation (MLE). Our general approach includes two major steps: ﬁrst we
compute clusters using model-based clustering algorithms. Then, we reﬁne the
clusters by injecting the class information, allowing the clusters to reﬁne themselves to suite the need for classiﬁcation. We then apply the computed models
to future data. Through extensive experiments, we have found our algorithm to
be very eﬀective on a large scale data set.
Our main contribution in this work consists of the following. First, we transform the variable-length time series data to ﬁxed-dimension vectors using an
innovative model-based clustering algorithm. Then, we then test a number of
classiﬁcation algorithms on the transformed data to test their eﬀectiveness in
the transformation method. Finally, we report the result from a real and large
sized CRM dataset.

2
2.1

Problem Statement and Related Work
Problem Statement

We ﬁrst deﬁne a time-series classiﬁcation problem. For this problem, we are
given a set S of sequences, where each sequence Si consists of elements pi , i =

Preprocessing Time Series Data for Classiﬁcation with Application to CRM

135

1, 2, . . . , n, and each element pi consists of a (large) collection of attribute-value
pairs (aj = vj ), j = 1, 2, . . . , k. In our CRM problem, each pi is a monthly bill.
Each sequence is also associated with a class label Cl , l = 1, 2, . . . , m; for our
discussion, we consider a two class problem (+, −), where the positive class +
means the customer is known to have churned. Our objective then is to obtain
classify test sequences Stest such that some optimization criterion is satisﬁed.
For example, one such criterion is the accuracy measure. However, as we will
show later in the paper, a more useful measure is the area under the ROC curve
(AUC) [9].
In our CRM example, a sequence can be a consecutive series of monthly bills
from a customer Si = p1 p2 ...pn , where p s are the telecom services that a user
used in a certain month. Examples of the services include long distance service,
local service, call forwarding service, and so on. Each service element in pi is
a vector of real values e1 , e2 , . . . , ek , where each ei is the dollar amount paid
by the customer in the corresponding month. If we use discrete integer numbers
as an identiﬁer to denote the individual services, the top-level sequences from a
service log is exempliﬁed in Table 1. A much longer (and uneven) sequence can
be obtained using the individual phone calls made by a customer, where each
phone call is represented by its duration, services used, cost, time of day and
many other factors.
Table 1. Sequences from a service log with class labels
Index
1
2
3
4
5
6

Service Sequence
Class
17 2 17 16 16 16
+
16 27 27 16 19 18
20 20 23 23 24 17 17 16 16 16 +
16 16 17 17 17 16 16
+
16 1 2 6 26
+
15 16 27 21 27 19 16
-



In this work, the measure that we optimize is area under the ROC curve.
n0
(ri −i)
The AUC can be calculated by the formula [7] AU C = i=1
, where n0 and
n0 n1
n1 are the number of positive and negative examples respectively, and ri is the
position of the ith positive example.
2.2

Related Work

In data mining and knowledge discovery, learning about sequences has captured
the special attention of many researchers and practitioners. Many techniques and
applications have been explored in the past, including modelling using Finite
State Automata [3,8], Markov Model-based classiﬁcation and clustering for Web
mining [4,11]), association-rule based methods for market-basket analysis ([2]).
Of these past work, user-behavior mining has been a central focus of much

136

Y. Yang et al.

research and applications. Because it is diﬃcult to provide labelled information
manually for massive databases and Web-based systems, many researchers have
turned to applying unsupervised learning on sequence data, using methods such
as sequence clustering.
In clustering, Web researchers such as Cadez et al. ([4]) grouped user behaviors by ﬁrst-order Markov models. These models are trained by an EM algorithm. These clustering algorithms reveal insight on users’ Web-browsing behavior through visualization.
Classiﬁcation of unbalanced data has been a major task in KDD. For example, a famous benchmark dataset is the the KDDCUP-98 dataset [1] which
was collected from the result of the 1997 Paralyzed Veterans of America fundraising mailing campaign. This data set is unbalanced in the sense that about
5% of records are responders. The majority of the competitive methods ranks
customers by their estimated probability to respond. Such works include [10].
Recognizing the inverse correlation between the probability to respond and the
amount of donation, a trend is to apply cost-sensitive learning, in which false positive and negative costs are considered together with the probability estimates.
[5] proposed the MetaCost framework for adapting accuracy-based classiﬁcation
to cost-sensitive learning by incorporating a cost matrix C(i; j) for misclassifying
true class j into class i. [13] examined the more general case where the beneﬁt
depends not only on the classes involved but also on the individual customers.
[9] studied AUC as a general metric for measuring the performance of a costsensitive classiﬁcation problem, and showed that AUC has a better discriminate
ability than accuracy in general.
Despite the similarity to our work, all the above methods are designed for
data that are not sequential in nature. Furthermore, they are aimed at maximizing only the classiﬁcation accuracy or related measures (such as maximal proﬁt),
rather than producing a good ranking of the customers. In this work, we focus
instead on optimizing AUC for ranking sequences, which is a novel problem and
a much harder one as compared to the special cases when the sequence lengths
are one. The most important feature of our work is that we apply a novel data
preprocessing method that transforms the data into equal-length vectors which
include the temporal features in the data. The transformation allows any standard classiﬁcation methods to be used, include hidden-Markov models, 1-class
and multi-class SVM, as well as Bayesian methods. In this paper, we present
an additional contribution, that is, standard maximum likelihood method can in
fact outperform more sophisticated methods such as SVM and boosting methods.
This is surprising to us, but is yet another evidence that “simple is beautiful.”

3
3.1

Time-Series Data Transformation for Classiﬁcation
Overview

The telecom time-series classiﬁcation problem has many general features common to all sequence classiﬁcation problems. Because the data come from a large

Preprocessing Time Series Data for Classiﬁcation with Application to CRM

137

number of diﬀerent customers, some data sequences are long, consisting over 20
months of billing data, while others are short, consisting of perhaps only three
or four months of billing data. Furthermore, each data item in a time series is
a multi-dimensional vector, instead of just a single number as in some stockmarket analysis data sets. These problems pose particular diﬃculties for many
time series analysis methods, because most standard classiﬁcation methods such
as decision trees, maximum likelihood and SVM methods require that the input
data be consist of equal length vectors of attribute-values pairs.
Our general approach is to follow a two-step process. In the ﬁrst step, we
transform the data into a equal-length vectors. A key issue is how to maintain
as much key temporal information as we can. To this end, we will apply a model
based clustering method to help us transform the data. In the second step, we
apply and compare diﬀerent standard classiﬁcation methods to achieve high
levels of the AUC metric.
3.2

Data Transformation Through Model Based Clustering

Our algorithm is shown in Table 2. As shown in the table, we partition the
positive-class and negative-class training data into two sets: DB+ and DB− . We
then apply a model-based clustering algorithm to build separate sets of clusters
within each partition. In particular, we build p+ number of clusters from the
positive data set DB+ and p− number of clusters from DB− . This is done in
Step 2. The details on how to build the clusters are given in the next subsection.
Then, in Step 4, the algorithm loops through all training examples. In each
iteration, every datum is measured against all clusters in terms of the maximum
likelihood of the sequence belonging to that clusters. This measurement generates
a number of probability measures, one for each cluster. The result is a new vector
vectori , which has a dimension of p+ + p− . This transformed data is then saved
in the set of all vectors V ectors, which is returned as the newly transformed
data set.
Below, we consider each step of the algorithm in detail.
Table 2. Model based clustering for transforming the input data

Input:
Output:
Steps
1
2
3
4
5
6
7
8

Time-Series Transformation Algorithm ClusterTrans(T rainDB, p+ , p− )
Training database T rainDB, numbers of clusters p+ and p− ;
transformed training data V ectors;
Algorithm
Partition T rainDB into a positive subset DB+ and a negative subset DB− ;
Clusters+ = M odelBasedClustering(DB+ , p+ );
Clusters− = M odelBasedClustering(DB− , p− );
M odel = (Clusters+ , Clusters− ); Vectors={};
For each input datum seqi ∈ T rainDB, do
vectori = maxlikelihood(seqi , M odel);
V ectors = V ectors ∪ vectori ;
end For
Return V ectors;

138

3.3

Y. Yang et al.

Model-Based Clustering

Step 2 of the ClusterTrans algorithm builds clusters from the positive-class or
negative class input data. However, each customer record is a complex structure,
which may contain both categorical or continuous data. As an example, consider
two sessions from users u1 and u2 in Table 3. The attribute ‘age’ is a numerical feature, while ‘gender’, ‘service’, ‘weekday’, ‘month’ are categorical features.
Finally, ‘duration’ is a range feature. Our ﬁrst task in applying the model based
clustering algorithm is to convert these data into discrete states. We ﬁrst discretize all numerical attributes to discrete attributes. This can be done in various
ways using some standard supervised or unsupervised discretization algorithms.
In this work, we take all customer bill data and performed clustering analysis
in this space. This returns N states: Si , i = 1, 2, . . . , n, where n is a parameter
which we can ﬁne tune later 1 .
Table 3. Example of Telecom Record, two sessions: u1 and u2 only. (The ’time’ is
the seconds from 1970-01-01 00:00:00. We removed private content to protect privacy.
Column ’Duration’ is measured in seconds.)
user service age gender time duration weekday
u1
S2
36
1
...6589 109
5
u1
S2
36
1
...6631
19
5
u1
S2
36
1
...6658
80
5
u2
S1
29
1
...5806
37
1
u2
S5
29
1
...9720
20
3
u2
S3
29
1
...9903
56
4
u2
S5
29
1
...1848
85
5

From the obtained state information, each customer data sequence is then
transformed into a state-transition sequence s1 , s2 , . . .. We apply a EM-based
clustering algorithm to general p+ or p− clusters from each data DB+ or DB− ,
respectively. Suppose that there are n states in Markov models and k Markov
chains inside each cluster. Suppose the prior state distribution is P r(si ) = ai ,
and the state-transition matrix is:
⎛ ⎞
⎛
⎞
a1
t11 t12 ...t1n
⎜ a2 ⎟
⎜
⎟
→
−
⎟ , Tp = ⎜ t21 t22 ...t2n ⎟
vp=⎜
(1)
⎝ ... ⎠
⎝ ...
⎠
an
tn1 tn2 ...tnn
where tij is the transition probability in each cluster of transferring from a state
si to sj . We have:
t=k
I(ct = i)
ai = t=1
(2)
k
1

In our experiment, we applied SOM-based clustering algorithm to obtain the states.
Other clustering algorithms can also be used.

Preprocessing Time Series Data for Classiﬁcation with Application to CRM

and

139

t=k s=it −1
tij =

t=1

I(cts = i, ct(s+1) = j)
s=1
t=k s=it −1
I(cts = i)
t=1
s=1

(3)

where I(x) = 1 if x is true and 0 otherwise. With these input data, we can then
train a mixture Markov model with K clusters of the form:
p(v|θ) =

K


p(ck |θ)pk (v|ck , θ)

(4)

k=1

where p(ck |θ) is the marginal probability of the k th cluster and

K


p(ck |θ) =

k=1

1, pk (v|ck , θ) is the statistical model describing the behavior for users in the
k th cluster, and θ denotes the parameters of the model. Here, v = v1 v2 ...vL
is an arbitrarily long sequence of feature vectors. We assume that each model
component is a ﬁrst-order Markov model, described by
L

pk (v|ck , θ) = p(v1 |θkI ) Π p(vi |vi−1 , θkT )
i=2

(5)

where θkI denotes the parameters of the probability distribution over the initial
feature vectors among users in cluster k, and θkT denotes the parameters of the
probability distributions over transitions from one feature vector to the next in a
cluster k. This model captures the order of users’ requests, including their initial
requests and the dependency between two consecutive requests.

4

Classifying and Ranking Sequences Using the Mixture
Model

After the preceding algorithm, we now have a collection of K = p− + p+ clusters; in this paper, we set p+ to be one. For any given time series seq, we can
then compute the probability distribution over the K clusters, and represent
this distribution as a vector V = p1 , p2 , . . . , pK . We can use this method to
transform all training and testing data into K-dimensional vectors. Each pi can
be calculated using Equation ??. This probability measure for the new data
sequence v is then used to rank among all test sequences. This will generate a
ranked list. In the experiment, we will compute the AUC measure on the list for
comparison, using the ground truth values.
During the testing phase, we can now apply any standard classiﬁcation and
ranking algorithm to each input data sequence. We apply the following methods.
Maximum Likelihood Method and SVM method This method, which is essentially
a naive Bayesian method, is the simplest among all methods we consider. In
particular, we ﬁrst convert a test data sequence seqi into a vector form: s1 →
s2 → . . . → sk , sk ∈ States{1, ...n}. This generates a vector Vi of size K, where K
is the number of clusters. We can then calculate the probability of this sequence

140

Y. Yang et al.

in the positive and negative Markov models, respectively as prob p and prob n .
If prob p ≥ prob n, then predict that the customer is escaping with class (Y = 1);
otherwise predict the customer as staying with (Y = 0). For each decision, we
can obtain the value of false positive F P and false negative F N . Furthermore,
Using prob p and prob n, we calculate a ranking score for the customer:
Score =

prob p
prob p + prob n

(6)

We then sort all test data set based on the value of Score, and calculate the
ﬁnal AUC value of generated list. With the SVM method, we use the distance
to the boundary to measure the ranking.

5

Experimental Setup and Analysis

Our experiments were designed to test the eﬀectiveness of the data transformation algorithm in terms of its ability to distinguish positive (churning) and
negative customers, measured on the AUC measure. We obtained a large amount
of data from the telecom company to form our training and testing sets. Below,
we discuss our experimental set up, the testing results of the clustering module
and the ranking module.
We randomly chose 20,000 customers from a telecom company’s database.
This database contains 96% of non-attritors (negative class) and 4% of known
attritors (positive class). Each customer is associated with a sequence of service
records and bills. The sequences are variable in lengths, consisting monthly service records and bills for the customer that range anywhere from 4 to 24 (i.e.,
two years). We split the data into 10 folds in a cross-validation test.
We test the algorithm on AUC-based ranking measure. In our ﬁrst test, we
compared our proposed Maximum Likelihood (MLE) method with the performance of a number of other classiﬁcation algorithms, including SVM, Boosting
(with Naive Bayesian as the base classiﬁer), and K-nearest neighbor algorithm
(where K is set to be 10). Our goal is to establish that MLE with performs
the best among all algorithms, due to our data preprocessing algorithm ClusterTrans. In this paper, we only have space to describe our results of comparing
with 2-class SVM.
Our empirical test varied the number of states as a result of clustering in
the vector space, where each multidimensional vector is a representation of a
customer bill at a time point. When the number of states is large, the transitions
from a state to another is more evenly distributed, and the preprocessing through
Cluster-Trans will have more diﬃculty. The result is shown in Figure 1. As can
be seen from the ﬁgure, the AUC ﬁgures for all classiﬁcation algorithms follow
a downward trend. The MLE algorithm is a clear winner, for all state numbers.
This is because our MLE-based algorithm when coupled with the ClusterTrans data transformation can take advantage of the knowledge about AUC
measures much better. This result is further conﬁrmed when we varied the number of clusters. We can also see that the MLE in fact performs very robustly

Preprocessing Time Series Data for Classiﬁcation with Application to CRM

141

0.7
0.68
0.66

AUC

0.64
0.62

2-SVM
MLE

0.6
0.58
0.56
0.54
0.52
0.5
9

12

14

16

18

20

22

26

30

Number of states

Fig. 1. Comparing two diﬀerent algorithms with changing number of states

when the number of clusters change from small to large. Thus, one advantage of
our preprocessing algorithm is that the resultant data set can be used to train
a robust classiﬁer for customer ranking. This eﬀect is demonstrated in a test
where we increased the percentage of positive data. This is in contrast to when
the data consists of the raw customer data, such as for KDDCUP 1998, when
the performance of classiﬁers are highly dependent on the composition of the
training data [12].

6

Conclusions

Sequence ranking is critical for many CRM applications such as customer churning behavior classiﬁcation. A major problem for this research is how to resolve
the many sequences which are ambiguous in that they confuse between positive
and negative instances during classiﬁcation. In this paper, we have presented a
novel method to solving this problem. Our solution is to employ a model based
clustering algorithm and repeated ﬁlter out negative instances that do not contribute to the optimization of the AUC measure.
In the future, we plan to expand the preprocessing component to including
the classiﬁcation algorithms as well, to consider what the best combination is
for a given classiﬁcation algorithm. We also plan to test our algorithms for other
multi-dimensional time series data.

Acknowledgement
We thank Ping Huang, Ian Li, George Li and Hui Wang for valuable comments.
Qiang Yang and Rong Pan are supported by a Hong Kong RGC Central Allocation grant CA03/04.EG01(RGC HKBU 2/03C), and all other authors are also
supported by Zhong Shan University.

142

Y. Yang et al.

References
1. http://kdd.ics.uci.edu/databases/kddcup98/kddcup98.html. 1998.
2. Rakesh Agrawal and Ramakrishnan Srikant. Mining sequential patterns. In Philip
S. Yu and Arbee S. P. Chen, editors, Eleventh International Conference on Data
Engineering, pages 3–14, Taipei, Taiwan, 1995. IEEE Computer Society Press.
3. Jose Borges and Mark Levene. Data mining of user navigation patterns. In The
Workshop on Web Usage Analysis and User Proﬁling (WEBKDD’99), pages 31–36,
San Diego, CA, August 1999.
4. Igor Cadez, David Heckerman, Christopher Meek, Padhraic Smyth, and Steven
White. Visualization of navigation patterns on a web site using model-based clustering. In Knowledge Discovery and Data Mining, pages 280–284, March 2000.
5. P. Domingos. Metacost: A general method for making classiﬁers cost sensitive.
In Proceedings of the Fifth International Conference on Knowledge Discovery and
Data Mining, pages 155–164. AAAI Press, 1999.
6. C. Elkan. The foundations of cost-sensitive learning. In Proceedings of the 17th
International Joint Conference on Artiﬁcial Intelligence, pages 973–978, 2001.
7. D. J. Hand and R. J. Till. A simple generalisation of the area under the ROC curve
for multiple class classiﬁcation problems. Machine Learning, 45:171–186, 2001.
8. Mark Levene and George Loizou. A probabilistic approach to navigation in hypertext. Information Sciences, 114(1–4):165–186, 1999.
9. C. X. Ling, J. Huang, and H. Zhang. AUC: a statistically consistent and more
discriminating measure than accuracy. In Proceedings of 18th International Conference on Artiﬁcial Intelligence (IJCAI–2003), pages 329–341, 2003.
10. C.X. Ling and C. Li. Data mining for direct marketing - speciﬁc problems and solutions. In Proceedings of Fourth International Conference on Knowledge Discovery
and Data Mining (KDD–98), pages 73–79, 1998.
11. Padhraic Smyth. Clustering sequences with hidden markov models. In Michael C.
Mozer, Michael I. Jordan, and Thomas Petsche, editors, Advances in Neural Information Processing Systems, volume 9, page 648. The MIT Press, 1997.
12. Ke Wang, Senqiang Zhou, Qiang Yang, and J.M.S. Yeung. Mining customer value:
from association rules to direct marketing.Journal of Data Mining and Knowledge
Discovery, 2005.
13. B. Zadrozny and C. Elkan. Learning and making decisions when costs and probabilities are both unknown. In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining (SIGKDD01), pages
204–213, San Francisco, CA, USA, 2001.

Robustness and Reliability Consideration in Product Design Optimization under
Uncertainty
Xiaotian Zhuang1, Rong Pan1, Lizhi Wang2
1

Department of Industrial Engineering, CIDSE, Arizona State University, Tempe, AZ, USA
2
Department of Systems Engineering, Beihang University, Beijing, China
(xiaotian.zhuang@asu.edu)

Robustness and reliability are two important issues to be
considered in product design optimization under uncertainty.
Robustness is used to counter manufacturing variation and
improve product quality, and reliability is used to maintain
structure design feasibility by probabilistic constraints. This
paper considers robustness and reliability simultaneously by
a
multi-objective
reliability-based
robust
design
optimization (RBRDO) problem. A Pareto frontier
optimization strategy is employed to indicate the trade off
tendency between two objectives. To enhance the efficiency
of RBRDO, a constraint moving approach is proposed to
convert probabilistic constraints into deterministic
constraints. An I-beam case study illustrating RBRDO
framework and applicability is proposed in the paper, the
result is a Pareto frontier to trade off robustness and
performance value.
Keywords - Robustness, reliability, uncertainty

I. INTRODUCTION
Product design optimization is concerned with
efficient and effective methods leading to new products.
Uncertainty always exists during the process of design
and production. Robustness and reliability are two issues
considered by engineers at the product design and
development stage under uncertainty. To address these
issues, robust design (RD) and reliability-based design
optimization (RBDO)
are
typically employed,
respectively.
Robust design, first proposed by Taguchi, is from
viewpoint of quality engineers. It is a method to improve
the product quality by minimizing the product
performance variation without eliminating the sources of
variation. Taguchi provides a three-stage robust design:
system design, parameter design and tolerance design, in
which parameter design is the most important and used to
derive optimal design parameters to satisfy the quality
requirement. The process of finding the right design
parameters is referred to as robust design optimization.
The difference between robust design optimization and
ordinary design optimization lies in the consideration of
noise factors and performance variation. In robust design
optimization, both the robustness of design objective and
the robustness of design constraints can be considered. It
is widely recognized the robustness of a design objective
can be achieved by optimizing the performance mean and
minimizing performance variation simultaneously [1].
Although there are different methods to express and
achieve the product robustness, it is even more important

978-1-4577-0739-1/11/$26.00 ©2011 IEEE

to maintain the design feasibility under uncertainties.
Reliability-based design optimization, which is from
viewpoint of mechanical engineers in structure design,
focuses on maintaining design feasibility at an expected
probabilistic level (reliability). Uncertainties are
considered as random design variables and random
parameters in RBDO. Probabilistic constraints are
employed to guarantee a system’s reliability.
RD and RBDO consider two important aspects of
product design optimization under uncertainty. However,
neither RD nor RBDO, if used individually, will ensure
the quality and reliability simultaneously in product
design. RD optimization attempts to minimize the
variance of a given quality characteristic but ignores the
probabilistic nature of the random variable. On the other
hand, RBDO concentrates on the rare events failure
probability assessment but does not attempt to minimize
the variability. Thus it is necessary to integrate the merits
of both approaches in order to provide a better and
reliable product design.
A multi-objective reliability-based robust design
optimization (RBRDO) is developed to integrate
robustness and reliability in product design optimization
under uncertainty.
An RBRDO framework was
previously discussed in [2], in which a weighted sum
single objective to integrate mean, variance and reliability
goals were proposed as the objective function in RBRDO.
In this paper, instead of using a weighted sum single
objective optimization, we employ a Pareto frontier multiobjective to optimize performance measure and minimize
performance variation simultaneously.
The paper is organized as follows: Section 2 reviews
basic concept and formulation of RD. Section 3
introduces the basic concept and approaches of RBDO.
Section 4 proposes the a Pareto frontier multi-objective
RBRDO framework. Section 5 provides a case study on a
classic I-beam example. Section 6 is the conclusion.
II. ROBUST DESIGN
In general parameter design, a conventional robust
design optimization can be formulated as:
(1)
Minimize : H  x,[ 
where H  x, [  is the quality loss function, x and [ are
control parameters and noise factors, respectively. Signalto-noise (SNR) is one important measure of quality loss
proposed by Taguchi:
(2)
SNR : 10log10  MSD

1325

Proceedings of the 2011 IEEE IEEM

2
1 k
ª y  x, [i   yt º¼ , and it denotes the
¦
i 1¬ i
k
mean square deviation. yi  x, [i  is the quality value of a

where MSD

single sample and yt is the desired target value. Maximum
SNR is desired.
SNR is only one measure of quality loss in robustness
objective. The quality loss function can also be measured
by several methods, for examples, a robust index derived
from the acceptable performance variation proposed in
[3], a coefficient of variation in [4], percentile difference
method (PDM) provided in [5].
Uncertainties may exist in the robust parameter design
process, and they are classified into four categories in [6]:
1) The Changing environmental and operating
conditions. Examples are operating temperature, pressure,
humidity, etc. They can be expressed by noise factors [ in
H function. 2) Production tolerance and actuator
imprecision. The design parameters of a product can be
realized only to a certain degree of accuracy. This type of
uncertainty enters H function in terms of perturbations G
of the design variable X, i.e., H H  x  G ,[  . 3)
Uncertainties in the system output. This kind of
uncertainty includes measuring errors and modeling errors.
Modeling errors include all kinds of approximation errors
due to the use of empirical models instead of the true
models, i.e., H H  x  G , [  . 4) Feasibility uncertainties.
This type of uncertainty is the one considered in RBDO
constraints, and it concerns the fulfillment of constraints
the design variables must obey. This kind of uncertainty is
different from 1) - 3). It does not consider the uncertainty
effects of H but on the design space.
III. RELIABILITY-BASED DESIGN OPTIMIZATION
RBDO is used to maintain design feasibility in
structure design, and uncertainties in design variables and
probabilistic constraints are typically considered. A
generic formulation is given below.

Minimize : f d, μX , μP 

Subject to : P ª¬Gi d, x, p d 0º¼ t Ri , i 1,

n (3)

dL d d d dU , μLX d μ X d μUX , μLP d μP d μUP

requirement, where Gi  0 denotes safe or successful
region, Gi ! 0 denotes failure region, and Gi 0 is
defined as limit state surface which is the boundary
between success and failure. The value Ri is the target
probability of the constraint function. Thus, this
probabilistic constraint guarantees the system’s reliability.
Reliability assessment or probabilistic constraints
evaluation is the key in RBDO. Methods are developed to
evaluate probabilistic constraints, and they can be
classified into five categories in [7]:
1) Simulation based design. Monte Carlo simulation
(MCS) is a basic method to evaluate probabilistic
feasibility. However, the computation cost is high
especially for high target reliability (approaching 1.0).
Then importance sampling is employed to improve the
sampling efficiency. A sampling method around the Most
Probable Point (MPP) is provided in [1]; The importance
sampling in reduced region is developed in [8];
Importance sampling is also employed to improve
sampling efficiency and estimation accuracy in [9].
2) Local expansion based methods such as Taylor
series method. It could not be efficient dealing with high
dimension input and nonlinear performance functions.
3) MPP based method, which is typically based on
first order reliability method (FORM). The direct
reliability analysis method is Reliability Index Approach
(RIA) [10] in which the first-order safety reliability index
and MPP are obtained using FORM by formulating an
optimization problem. Since the convergence efficiency is
low in traditional RIA, a modified RIA [11] revises the
reliability index definition and improves the efficiency.
Also, a new approach for RIA based on Minimum Error
Point (MEP) [12] is presented to minimize the error
produced by approximating performance functions.
Another indirect reliability analysis method is
Performance Measure Approach (PMA) [13], which is
more robust and effective than RIA.
4) Functional expansion based method. The
polynomial chaos expansion is in this category.
5) Numerical integration based methods. Dimension
reduction (DR) [14] is one common method of this
category, which deals with high dimension numerical
integration.
IV. PARETO FRONTIER MULTI-OBJTIVE RBRDO

where d is a vector of deterministic design variables, X is
a vector of random design variables, and P is a vector of
random design parameters. The objective function can be
viewed as a cost function of the system. Note that the
objective function above is the first-order Taylor
expansion approximation of the mean cost function
E ¬ª f d, x, p¼º . This approximation is generally

A. RBRDO Formulation
The reliability-based robust design optimization
(RBRDO)
tackles
reliability
and
robustness
simultaneously. The probabilistic constraints deal with
failure probability, while robustness minimizes the
product quality loss.
Performance mean and performance variation need to
be optimized in the RBRDO objective. Hence both

acceptable for linear and close-to-linear cost function.
However, we are more interested in the probabilistic
constraint function, which is the key difference of RBDO
from other engineering optimizations. The function
Gi d, x, p d 0 is the system’s performance or safety

1326

Proceedings of the 2011 IEEE IEEM

multi-objective optimization. However, finding a target
value for the mean or standard deviation can be difficult.
In this paper, A Pareto frontier multi-objective
optimization is performed by considering the following
objective functions: (a) a function describing the
performance of the structural response for a prescribed
reliability index of the structural system, and (b) a
function describing the robustness of the system related to
the variability of the structural response. The multiobjective optimization problem can be then established as:

Fig. 1. Reliability-based robust design optimization.

Minimize : H d, μ X , μP 

product reliability and robustness are improved
simultaneously by mean shift and variance reduction as
shown in Fig. 1. Several optimization strategies are
employed to address both objectives simultaneously.
A weighted sum single objective optimization is
proposed in [2], where both the mean and variation of cost
function are optimized. According to different cost
function types, the objective of RBRDO can be
formulated as:
1) Nominal-the-best type:
2

Minimize : VH d, μ X , μP 

Subject to : P ª¬Gi d, x, p  d 0º¼ t Ri , i 1,

where H d, μX , μP  denotes the performance mean by
using first-order Taylor expansion. The performance
variation VH d, μX , μP  is denoted by a performance

2

performance function H
weights to be determined
dimensionality problem of
normalized by the initial

2

·
§V
¸  w2 ¨ H
¸
¨ VH
¹
© 0

P ª¬H  x d H  x* º¼ D

Subject to : PH d, μ X , μP  d Ptarget

(11)

where D1 and D2 are two given confidence level, for

(5)

'HDD*2

2

1

(6)

Although weighted sum single objective optimization
is trivial to implement, it is difficult to choose weight with
high number of objectives.
Iterative constrained optimization (ICO) is another
optimization strategy proposed in [15], in which one of
the objectives is converted into a constraint by setting it to
a target value. The resulting robust design optimization
problem becomes:

Minimize : V H d, μ X , μP 

H D2  H D1

example, D1 0.05 and D2 0.95 . As indicated in Fig. 1,
the robust design objective reflects the need for shrinking
the dispersion of the performance distribution. For
example, the percentile difference is reduced from
*

§ PH ·
§ VH ·
Minimize : w1  sgn  PH  ¨ 0 ¸  w2 ¨ 0 ¸
© PH ¹
© VH ¹

H  x*  is called D  percentile . A percentile

'HDD12

3) Larger-the-better type:
2

(10)

difference can be derived as:

2

·
¸
¸
¹

percentile difference in this paper.
Given a confidence level D , the percentile can be
obtained based on the definition of cumulative
distribution function (CDF):
where H D

2) Smaller-the-better type:

§P
Minimize : w1  sgn  PH  ¨ H
¨ PH
© 0

HD2  HD1 to 'HDD12
*

*

H D2  H D1 by robust design

given confidence level. Therefore, percentile performance
difference is employed to measure performance variation
in RBRDO, and the robustness objective is denoted as:
(12)
VH d, μX , μP  'HDD12 H x*2  H x1*

 

 

(7)

where a target value is set to objective mean in advance.
Alternatively, another optimization problem can be
formulated with the set of standard deviation target value:

Minimize : PH d, μ X , μP 

Subject to : V H d, μ X , μP  d V target

(9)

dL d d d dU , μLX d μ X d μUX , μLP d μP d μUP

§ P h ·
§V ·
(4)
Minimize : w1 ¨ H t ¸  w2 ¨ H ¸
¨ PH  ht ¸
¨ VH ¸
0 ¹
© 0
© 0¹
where ht and ht0 are the target nominal value and the initial
target nominal value of the
respectively, and w1 and w2 are
by the designer. To reduce the
two objectives, each term is
value PH0 and V H0

n

(8)

All other probabilistic constraints will remain
unchanged. The efficiency for ICO method is better than

Fig. 2. Illustration of deterministic constraint moving approach.

1327

Proceedings of the 2011 IEEE IEEM

G(
G(d, μX ) G(d, PX1  V X1 Et1, PX2  V X2 E t2 , )

B. Reliability Assessment in RBRDO

Thus the probabilistic constraint P[G(d, x) d 0] t R is

Under the framework of a Pareto frontier multiobjective optimization in RBRDO, a constraint moving
approach method is employed in a single loop reliability
assessment [16] to convert probabilistic constraints to
deterministic constraints. Hence it is more efficient than
double loop and decoupled loop methods. To introduce
single loop reliability analysis approach, two important
concepts are reviewed.
Based on FORM, probabilistic constraint can be
converted to a standard CDF:

P(G d 0)
G  PG

VG

t,

³

0

f

PG
E
VG
E

³

f

ª 1 § G  P ·2 º § G  P ·
1
G
G
exp « ¨
¸ »d¨
¸
2S
«¬ 2 © V G ¹ »¼ © V G ¹

(15)

converted to deterministic constraint G(
G(d, μX ) d 0 .
C. Pareto Frontier Multi-Objective RBRDO Results

(13)

1
§ 1 ·
exp ¨  t 2 ¸ dt )  E 
2S
© 2 ¹

Pareto frontier multi-objective optimization is an
extension of weighted sum single objective optimization,
where the non-dominant solutions are generated by
different weight combination. Different from weighted
sum single objective optimization, in which weights are
preselected, no weight is specified by the user in Pareto
frontier multi-objective optimization. Thus a Pareto
solution frontier is derived, and it can indicate the trade
off tendency between multiple objectives.
V. I-BEAM CASE STUDY

where E PG VG is defined as reliability index, and it
indicates the distance from limit state surface to the mean
margin.
In the Standardized U-space, the mean margin is at the
origin. To satisfy the required reliability level, the point
on the limit state surface which has the minimized
distance to the origin is located, and the distance is just
reliability index E . Such a design point is named as most
probable point (MPP), which represents the worst case on
the limit state surface. If MPP is feasible, so does any
other point on the limit state surface.
Based on concept of reliability index and MPP, a
deterministic constraint moving approach in the single
loop reliability analysis is developed.
MPP denotes the worst case in probabilistic
constraints evaluation, hence we can derive:
(14)
P[G(d, x) d G(d, xMPP )] R
Thus evaluating P[G(d, x) d 0] t R is equivalent to
evaluating the constraint G(d, xMPP ) d 0 .

The objective cost function in this case study is the
performance function H of I-beam weight. The robust
design belongs to smaller-the-better type. Given the fixed
beam length and material, minimizing beam weight is
equivalent to minimizing beam cross section area. Two
geometric design variables X1 and X 2 are considered as
shown in Fig. 3, which are normally distributed with
V1 V2 0.225 . The beam is loaded by two random
design parameters P~N (600, 100) KN, and Q~N (50, 1)
KN. The maximum bending stress of the beam is
V 16N / cm2 , the reliability index E 3 (R=99.87%).
The RBDO objective is the expectation of
performance function H, and the robustness objective is to
minimize the variation of the performance. The percentile
difference is used to denote robustness objective as
VH H D2  H D1 , in which D2 95% and D1 5% .
Two objectives are simultaneously minimized subject
to one probabilistic constraint. P[G(x1, x2 , p, q) d 0] t R ,

However, G(d, xMPP ) d 0 is still a probabilistic
constraint, and any point on the limit state surface could
be considered as a potential MPP. Hence we shift all the
points on current the limit state surface G(d, x) 0 a E
distance in each gradient direction, and then a limit state
surface G(d, x) 0 is generated as shown in Fig. 2. Since
E indicates the distance from MPP to the mean margin,
we can guarantee that any point on the new limit state
surface has E distance to the original limit state surface.

G(x1, x2 , p, q) means the actual bending stress deducted
by the threshold, so G(x1, x2 , p, q) d 0 denotes feasible

To obtain the G(
G(d, x) , we move G(d, x) in gradient

The formulation of multi-objective optimization
problem is:

region, where

MY M Z

) V
ZY ZZ
0.3Qx2
 x1  2x2  x23  2x2 x13

G( x1 , x2 , p, q ) (
MY M Z

ZY ZZ


direction t with a step size E . Hence the point x on G(d, x)
is moved to x  σX Et :
When we obtain G(
G(d, x) , replace the random variable

0.3Px1
3
x2  x1  2x2   2x1 x2  4x22  3x12  6x1 x2 

Minimize : H

3P1P2  2P22

Minimize : VH

H D2  H D1

Subject to : P ª¬G  x1 , x2 , p, q  d 0º¼ t R

x with design variable μX , then the new limit state

10 d P1 d 80, 0.9 d P2 d 5

surface can be derived as G(d, μX ) 0 , where

1328

(18)

(17)

(18)

Proceedings of the 2011 IEEE IEEM

require preselected weights, and it can indicate the trade
off tendency between multiple objectives as well. A single
loop reliability analysis method with constraint moving
approach is employed in reliability assessment in RBRDO,
so that the algorithm efficiency is dramatically improved.

REFERENCES
[1] X. Du, and W.Chen, “Towards a better understanding of
modeling feasibility robustness in engineering design”
Journal of Mechanical Design, vol.122, pp. 385-394, Dec.
2000.
[2] O. Yadav, S. Bhamare and A. Rathore, “Reliability-based
robust design optimization: a multi-objective framework
using hybrid quality loss function”, Quality and Reliability
Engineering International, vol. 26, pp. 27-41, June, 2010.
[3] M. Li, S. Azarm, and V. Aute, “A multi-objective genetic
algorithm for robust design optimization”, Genetic and
Evolutionary Computation Conference, 2005.
[4] C. Antonio and L. Hoffbauer, “An approach for reliabilitybased robust design optimization of angle-ply composites”,
Composite Structures, vol. 90, pp. 53-59, 2009.
[5] Z. Mourelatos and J. Liang, “A methodology for trading-off
performance and robustness under uncertainty”,
IDETC/CIE, Long Beach, CA, Sept. 2005.
[6] H. Beyer and B. Sendhoff, “Robust optimization -- A
comprehensive survey”, Computer Methods in Applied
Mechanics and Engineering, vol. 196, no. 33-34, pp. 31903218, July, 2007.
[7] S.H. Lee, W. Chen, “A comparative study of uncertainty
propagation methods for black-box type functions”,
IDETC/CIE, vol.6, 2007
[8] F. Li. And T. Wu, “An importance sampling based
approach for reliability analysis”, The 3rd Annual IEEE
Conference on Automation Science and Engineering, 2007.
[9] C.M. Mok, N. Sitar, and A.D. Kiureghian, “Improving
accuracy of first-order reliability estimate by importance
sampling simulations”, ModelCARE, Prague, Czech
Republic, June, 2002.
[10] A.M. Hasofer and N.C. Lind, “Exact and invariant secondmoment code format”, Journalof Engineering Mechanics,
vol. 100, pp. 161-177, 1974.
[11] P.T. Lin, C.G. Hae, Y. Jaluria, “A modified reliability index
approach for reliability-based design optimization”,
IDETC/CIE,, San Diego, CA, Aug., 2009.
[12] D. Liu, W. Yue, P. Zhu and X. Du, “New Approach for
reliability-based design optimization minimum error point”,
Chinese Journal of Mechanical Engineering, vol.19,
no.4,2006.
[13] J. Tu and K.K.Choi, “A new study on reliability-based
design optimization”, IDETC/CIE, vol.121, no. 4, 1999.
[14] B.D.Youn, Z. Xi, L.J. Wells and P. Wang, “Enhanced
dimension reduction (EDR) method for sensitivity-free
uncertainty quantification”, Multidisciplinary Analysis and
Optimization Conference, Portsmouth, VA, 2006.
[15] A. Sopory, S. Mahadevan, Z.P. Mourelatos and J. Tu,
“Decoupled and single loop methods for reliability-based
optimization and robust design”, IDETC/CIE, Salt Lake
City, Utah, 2004.
[16] S. Shan and G.G. Wang, “Reliable design space and
complete single-loop reliability-based design optimization”,
Reliability Engineering & System Safety, vol. 93, pp.
1218-1230, 2008.

Fig. 3. I-Beam case study.

Fig. 4. Pareto frontier for I-beam case study.

where H  HD1 can be denoted by a function of P1 and
P2 .
The Pareto frontier for two objectives is shown in Fig.
4. It provides a set of efficient candidate solutions from
which the decision-maker can choose the solution based
on different preferences.
D2

VI. CONCLUSION
In this paper, both robustness and reliability are
considered in product design optimization under
uncertainty. Robust design is first explained, and different
measures of quality loss in robustness objective are
discussed. Also four categories of uncertainties are
covered in the robust parameter design process, in which
feasibility uncertainties are important and considered in
design space. To maintain the design feasibility of robust
design, RBDO is employed to consider probabilistic
constraints. Therefore an RBRDO multi-objective
optimization framework is proposed in the paper to
improve robustness and reliability simultaneously in
product design optimization.
Our research contribution is to propose a Pareto
frontier
multi-objective
optimization
framework.
Comparing with the weighted sum single objective
optimization and iterative constrained optimization (ICO),
the Pareto frontier multi-objective optimization does not

1329

Swoogle: A Search and Metadata Engine
for the Semantic Web ∗
Li Ding

Tim Finin
Anupam Joshi
Rong Pan
Pavan Reddivari
Vishal Doshi

R. Scott Cost
Joel Sachs

Yun Peng

Department of Computer Science and Electronic Engineering
University of Maryland Baltimore County, Baltimore MD 21250, USA
ABSTRACT

We introduce a prototype Semantic Web search engine
called Swoogle to facilitate the development of the Semantic
Web, especially the following three activities:

Swoogle is a crawler-based indexing and retrieval system
for the Semantic Web. It extracts metadata for each discovered document, and computes relations between documents.
Discovered documents are also indexed by an information
retrieval system which can use either character N-Gram or
URIrefs as keywords to find relevant documents and to compute the similarity among a set of documents. One of the
interesting properties we compute is ontology rank, a measure of the importance of a Semantic Web document.
Categories and Subject Descriptors:
H.3.3 [Information storage and Retrieval]: Information Search
and Retrieval - Retrieval models; Search process.
General Terms:
Algorithms, Design.
Keywords:
Semantic Web, Search, Metadata, Rank, Crawler.

1.

Finding appropriate ontologies. Failure to easily find an
appropriate ontology for a particular markup task typically leads to the creation of a new ontology (or to
the abandonment of the markup effort). Swoogle allows users to query for ontologies that contain specified terms anywhere in the document (including comments); for ontologies that contain specified terms as
Classes or Properties; or for ontologies that are about
a specified term (as determined by our IR engine). The
ontologies returned are ranked according to our Ontology Rank algorithm, which seeks to capture the extent
to which ontologies are used by the community. We believe that this use of Swoogle will both ease the burden
of marking up data, and contribute to the emergence
of canonical ontologies.
Finding instance data. In order to help users to integrate
Semantic Web data distributed on the Web, Swoogle
enables querying SWDs with constraints on what classes
and properties being used/defined by them.

INTRODUCTION

Currently, the Semantic Web, (i.e. online documents written in RDF or OWL), is essentially a web universe parallel to the web of HTML documents. Semantic Web documents (SWDs) are characterized by semantic annotation
and meaningful references to other SWDs. Since conventional search engines do not take advantage of these features,
a search engine customized for SWDs, especially for ontologies, is needed by human users as well as by software agents
and services. At this stage, human users are expected to
be semantic web researchers and developers who are interested in accessing, exploring and querying RDF and OWL
documents found on the web.

Characterizing the Semantic Web. By collecting metadata – especially inter-document relations – about the
Semantic Web, Swoogle reveals interesting structural
properties, allowing us to answer questions such as
“how is the Semantic Web connected?”, “how are ontologies referenced?”, and “how are ontologies modified externally?”.
Our development of Swoogle has highlighted a number of
interesting research issues such as “what is the best way to
index, digest and cache SWDs?”, and “is it possible to create
a meaningful rank measure that uses link semantics?”.
Swoogle is designed as a system that automatically discovers SWDs, indexes their metadata and answers queries
about it. This distinguishes it from other semantic web
repositories and query systems in the literature. Ontology
based annotation systems, such as SHOE [15], Ontobroker
[9], WebKB [16], QuizRDF [8] and CREAM [11], focus on
annotating online documents. However, their document indexes are based on the annotations rather than on the entire
document, and they use their own ontologies which may not
be suited for Semantic Web documents. It is notable that
CREAM [11] had indexed ‘proper reference’ and ‘relational

∗Partial research support provided by DARPA contract
F30602-00-0591 and NSF-ITR-IIS-0326460 and NSF-ITRIDM-0219649.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
CIKM’04, November 8–13, 2004, Washington, DC, USA.
Copyright 2004 ACM 1-58113-874-1/04/0011 ...$5.00.

652

sure. We consider a document to be a SWO when a significant proportion of the statements it makes define new terms
(e.g., new classes and properties) or extend the definitions
of terms defined in other SWDs by adding new properties or
constraints. A document is considered as a SWDB when it
does not define or extend a significant number of terms. A
SWDB can introduce individuals and make assertions about
them or make assertions about individuals defined in other
SWDs.
For example, SWD http://xmlns.com/foaf/0.1/index.rdf
is considered a SWO since its 466 statements (i.e. triples)
define 12 classes and 51 properties but introduce no individuals. The SWD http://umbc.edu/˜finin/foaf.rdf is considered to be a SWDB since it defines or extends no terms but
defines three individuals and makes statements about them.
Between these two extremes, some SWDs are intended
to be both an ontology that defines a set of terms to be
used by others, as well as a useful database of information about a set of individuals. Even a document that is
intended as an ontology might define individuals as part
of the ontology. Similarly, a document that is intended
as defining a set of individuals might introduce some new
terms in order to make it easier to describe the individuals, 5 (e.g. http://www.daml.ri.cmu.edu/ont/USCity.daml,
http://reliant.teknowledge.com/DAML/Government.owl).

metadata’. Ontology repositories, such as the DAML Ontology Library [1], SemWebCentral [4] and Schema Web [2],
do not automatically discover semantic web documents but
rather require people to submit URLs. They only collect
ontologies which constitute a small portion of the Semantic
Web. In addition, they simply store the entire RDF documents. Recently, some Semantic Web browsers have been
introduced. Ontaria [5] is a searchable and browsable directory of RDF documents under development by the W3C; it
stores the full RDF graphs of harvested SWDs, rather than
focusing on metadata. Semantic Web Search [3] indexes individuals of well-known classes (e.g. foaf:Person, rss:Item).
While the advantages of Swoogle’s crawler-based discovery
system are obvious, the decision to only store and reason
over metadata is less obviously a good one. We made this
choice since our key goal in building Swoogle is to design a
system that will scale up to handle millions and even tens
of millions of documents. Moreover, Swoogle enables rich
query constraints on semantic relations.
The Swoogle architecture consists of web crawlers that
discover SWDs; a metadata generator; a database that stores
metadata about the discovered SWDs; a semantic relationships extractor; an N-Gram based indexing and retrieval
engine; a simple user interface for querying the system; and
agent/web service APIs to provide useful services.
We describe an algorithm, Ontology Rank, inspired by the
Page Rank algorithm [14, 18, 19] which ranks online documents based on hyperlinks. Our algorithm takes advantage
of the fact that the graph formed by SWDs has a richer set
of relations than the graph of the World Wide Web. In other
words, the edges in this graph have explicit semantics. Some
are defined or derivable from the RDF and OWL languages
(e.g., imports, usesTerm, version, extends, etc.) and others
by common ontologies (e.g., FOAF’s knows 1 ).
Below, we describe the Swoogle architecture and the various metadata that we compute, and present some preliminary results summarizing the characteristics of the portion
of the semantic web that our system has crawled and analyzed.

2.

3.

SWOOGLE ARCHITECTURE

As shown in Figure 1, Swoogle’s architecture can be broken into four major components: SWD discovery, metadata
creation, data analysis, and interface. This architecture is
data centric and extensible; components work independently
and interact with one another through a database.

data
analysis
metadata
creation
SWD
discovery

SEMANTIC WEB DOCUMENTS

Semantic web languages based on RDF (e.g., RDFS 2 ,
DAML+OIL 3 , and OWL 4 ) allow one to make statements
that define general terms (classes and properties), extend
the definition of terms, create individuals, and to make assertions about terms and individuals already defined or created.
We define a Semantic Web Document (SWD) to be a document in a semantic web language that is online and accessible to web users and software agents. Similar to a document
in IR, a SWD is an atomic information exchange object in
the Semantic Web.
Current practice favors the use of two kinds of documents
which we will refer to as semantic web ontologies (SWOs)
and semantic web databases (SWDBs). These correspond
to what are called T-Boxes and A-Boxes in the description
logic literature [6, 13]. Since a document may consist of
both T-Boxes and A-Boxes, we adopt a threshold based mea-

IR analyzer

SWD analyzer

interface
Web Server

SWD Cache

SWD Metadata

Web Service
Agent Service

SWD Reader
Candidate
URLs

The
TheWeb
Web
Web Crawler

Figure 1: The architecture of Swoogle
The SWD discovery component discovers potential SWDs
throughout the Web and keeps up-to-date information about
SWDs.
The metadata creation component caches a snapshot of
a SWD and generates objective metadata about SWDs at
both the syntax level and the semantic level.
The data analysis component uses the cached SWDs and
the created metadata to derive analytical reports, such as
classification of SWOs and SWDBs, rank of SWDs, and the
IR index of SWDs.
The interface component focuses on providing data services to the Semantic Web community. We have implemented a Web interface at http://www.swoogle.org, and we

1

http://xmlns.com/foaf/0.1/
http://www.w3.org/TR/rdf-schema/
3
http://www.w3.org/TR/daml+oil-reference
4
http://www.w3.org/TR/owl-semantics/
2

5
Programming languages introduce the notion of a modules,
importing and exporting to make these intentions explicit

653

are working on making Swoogle a Web Service for software
agents.
We elaborate on each component in the following sections.

4.

We have initiated focused crawls from many Semantic Web
URLs know to us, and actively invite the SW community to
submit further URLs for focused crawling.
Since SWDs can be discovered by semantic links while
parsing SWDs, we developed the JENA2 6 based Swoogle
Crawler. It both analyzes the content of a SWD and discovers new SWDs. First, it verifies if a document is a SWD
or not, and it also revisits discovered URLs to check updates.
Secondly, several heuristics are used to discover new SWDs
through semantic relations: (1) a URIref is highly likely to
be the URL of an SWD; (2) owl:imports links to an external
ontology, which is a SWD; (3) the semantics of the FOAF
ontology show that the rdfs:seeAlso property of an instance
of foaf:Person often links to another FOAF document, which
often is a SWD.

FINDING SWDS

Finding URLs of SWDs is by itself an interesting engineering challenge. A straightforward approach is to search
through a conventional search engine. By May 25, 2004,
Google had indexed 4,285,199,774 web documents. It is not
possible for Swoogle to parse all documents on the web to
see if thery are SWDs. (Even if it were computationally feasible, most search engines, including Google, return at most
1,000 results per query). We developed a set of crawlers
employing a number of heuristics for finding SWDs.
First, we developed a Google crawler to search URLs using
the Google Web Service. We start with type extensions, such
as “.rdf”, “.owl”, “.daml”, and “.n3”. Although they are not
perfect SWD indicators, Table 1 shows that they have fair
precision.
extension
rdf
rss
owl
n3
daml
no extension

# discovered(100%)
184,992
8,359
4,669
4,326
3,699
154,591

5.

# SWD
111,350(60%)
7,712(92%)
3,138(67%)
1,523(35%)
2,256(61%)
7,258(5%)

Table 1: Extensions of SWD (Aug 30, 2004)
To overcome Google’s limit of returning only the first
1000 results for any query, we append some constraints (keywords) to construct more specific queries, and then combine
their results (see Table 2). Such query expansion techniques
have enabled us to collect as many as 20K candidate URLs
of SWDs at a time. Since Google changes its PageRanks
daily, we also expect to discover new SWDs by running the
same query weekly (in fact, Swoogle already has 200K urls
discovered by the Google Crawler).
query string
rdf
filetype:rdf rdf
filetype:rss rdf
filetype:daml rdf
filetype:n3 rdf
filetype:owl rdf
filetype:rdfs rdf

SWD METADATA

SWD metadata is collected to make SWD search more efficient and effective. It is derived from the content of SWDs as
well as the relations among SWDs. Swoogle identifies three
categories of metadata: (i) basic metadata, which considers the syntactic and semantic features of a SWD, (ii) relations, which consider the explicit semantics between individual SWDs, and (iii) analytical results such as SWO/SWDB
classification, and SWD ranking. The first two categories
are objective information about SWDs, and we will discuss
them in the rest of this section. The third category is subjective and will be discussed in section 6.
In order to simplify notations, we use qualified names
(QNames 7 ) in the following context. E.g. “rdf:” stands for
the RDF namespace, “daml:” stands for the DAML namespace, etc.

5.1

Basic metadata

The basic metadata about a SWD falls into three categories: language feature, RDF statistics and ontology annotation.
Language feature refers to the properties describing the
syntactic or semantic features of a SWD. Swoogle captures
the following features:
1. Encoding shows the syntactic encoding of a SWD. There
are three existing encodings, namely “RDF/XML”,
“N-TRIPLE” and “N3”.

number of pages
5,230,000
246,000
13,800
4,360
2,630
1,310
304

2. Language shows the Semantic Web language used by
a SWD. Swoogle considers four meta level languages,
namely “OWL”, “DAML”, “RDFS”, and “RDF”.
3. OWL Species shows the language species of a SWD
written in OWL. There are three possible species, namely
“OWL-LITE”, “OWL-DL”, and “OWL-FULL”.

Table 2: Google search results (May 25,2004)
We have also developed the Focused Crawler, which crawls
documents within a given website. In order to reduce search
complexity and improve precision, simple heuristics, like extension constraint (e.g. documents with “.jpg” or “.html”
extensions are seldom SWDs) and focus constraint (e.g. only
crawl URLs relative to the given base URL), are used to filter out those documents likely to be irrelevant. Swoogle
provides a web interface where registered users can submit
a URL of either a SWD or a web directory under which many
SWDs may be present, e.g. http://daml.umbc.edu/ontologies/.

654

RDF statistics refers to the properties summarizing node
distribution of the RDF graph of a SWD. We focus on how
SWDs define new classes, properties and individuals. In an
RDF graph, a node is recognized as a class iff it is not an
anonymous node and it is an instance of rdfs:Class; similarly,
a node is a property iff it is not an anonymous node and it
is an instance of rdf:Property; an individual is a node which
is an instance of any user defined class.
6

http://jena.sourceforge.net/
http://www.w3.org/TR/1999/REC-xml-names19990114/#NT-QName
7

Let foo be a SWD. By parsing foo into an RDF graph, we
may get RDF statistics about foo. Let C(f oo), P (f oo), I(f oo)
be the set of classes, properties and individuals defined in
the SWD foo respectively. The ontology-ratio R(f oo) is calculated by Equation (1). The value of ontology-ratio ranges
from 0 to 1, where “0” implies that foo is a pure SWDB and
“1” implies that foo is a pure SWO.
R(f oo) =

|C(f oo)| + |P (f oo)|
|C(f oo)| + |P (f oo)| + |I(f oo)|

(1)

Ontology annotation refers to the properties that describe
a SWD as an ontology. In practice, when a SWD has an
instance of OWL:Ontology, Swoogle records its properties
as the following:
1. label. i.e. rdfs:label
2. comment. i.e. rdfs:comment
3. versionInfo. i.e. owl:versionInfo and daml:versionInfo

5.2

Relations among SWDs

Looking at the entire semantic web, it is hard to capture
and analyze relations at the RDF node level. Therefore,
Swoogle focuses on SWD level relations which generalize
RDF node level relations. Swoogle captures the following
SWD level relations:
TM/IN captures term reference relations between two SWDs,
i.e. a SWD is using terms defined by some other
SWDs. By retrieving and processing the reference
SWD, the type of term (class, property or individual)
can be determined. The referenced SWDs are collected
by recording the namespaces of all valid URIrefs in the
given SWD.
IM shows that an ontology imports another ontology. The
URLs of referenced ontolgoies are collected by recording the objects in triples whose predicate is owl:imports
or daml:imports.
EX shows that an ontology extends another. Such a relation may be produced by many properties as shown in
Table 3. For example, if ontology A defines class AC
which has the “rdfs:subClassOf” relation with class
BC defined in ontology B, Swoogle will record the EX
relation from A to B.

where T1 , . . . , Tn are web documents that link to A; C(Ti )
is the total outlinks of Ti ; and d is a damping factor, which
is typically set to 0.85. The intuition of PageRank is to
measure the probability that a random surfer will visit a
page. Equation 2 captures the probability that a user will
arrive at a given page either by directly addressing it (via
P Rdirect (A)), or by following one of the links pointing to it
(via P Rlink (A)).
Unfortunately, this random surfing model is not appropriate for the Semantic Web. The semantics of links lead to
a non-uniform probability of following a particular outgoing link. Therefore, Swoogle uses a rational random surfing
model which accounts for the various types of links that can
exist between SWDs.
Given SWDs A and B, Swoogle classifies inter-SWD links
into four categories: (i) imports(A,B), A imports all content
of B; (ii) uses-term(A,B), A uses some of terms defined by
B without importing B; (iii) extends(A,B), A extends the
definitions of terms defined by B; and (iv) asserts(A,B), A
makes assertions about the individuals defined by B.
These relations should be treated differently. For instance,
when a surfer observes imports(A,B) while visiting A, it is
natural for it to follow this link because B is semantically
part of A. Similarly, the surfer may follow the extends(A,B)
relation because it can understand the defined term completely only when it browses both A and B. Therefore, we
assign different weights to the four categories of inter-SWD
relations.
Since we generalized RDF node level relations to SWD
level relations, we also count the number of references. The
more terms in B referenced by A, the more likely a surfer
will follow the link from A to B.
Based on the above considerations, given SWD a, Swoogle
computes its raw rank using Equation 3.
P
rawP R(x) ff(x,a)
rawP R(a) = (1 − d) + d
(x)
x∈L(a)
P
f (x, a) =
weight(l)
(3)
l∈links(x,a)
P
f (x) =
f (x, a)
a∈T (x)

where L(a) is the set of SWDs that link to a, and T (x) is
the set of SWDs that x links to.

PV shows that an ontology is a prior version of another.
CPV shows that an ontology is a prior version of and is
compatible with another.

Type
IM
EX

IPV shows that an ontology is a prior version of but is
incompatible with another.
The last five relations are types of inter-ontology relation.
They are extracted from each SWD by analyzing triples containing “indicators” listed in Table 3.

6.

RANKING SWDS
PV
CPV

PageRank, introduced by Google [18, 12], evaluates the
relative importance of web documents. Given a document
A, A’s PageRank is computed by Equation 2:
P R(A) = P Rdirect (A) + P Rlink (A)
P Rdirect (A) = (1
´
³ − d)
P Rlink (A) = d

PR(T1 )
PR(T )
+...+ C(T n)
C(T1 )
n

IPV
(2)

Classes and Properties
owl:imports, daml:imports
rdfs:subClassOf, rdfs:subPropertyOf,
owl:disjointWith, owl:equivalentClass,
owl:equivalentProperty, owl:complementOf,
owl:inverseOf, owl:intersectionOf, owl:unionOf
daml:sameClassAs, daml:samePropertyAs,
daml:inverseOf, daml:disjoinWith
daml:complementOf, daml:unionOf
daml:disjointUnionOf, daml:intersectionOf
owl:priorVersion
owl:DeprecatedProperty, owl:DeprecatedClass,
owl:backwardCompatibleWith
owl:incompatibleWith

Table 3: Indicators of inter-ontology relation

655

Then Swoogle computes the rank for SWDBs and SWOs
using Equations 4 and 5 respectively.
P RSW DB (a) = rawP R(a)
P RSW O (a) =

X

rawP R(x)

A

(4)

B

(5)

C

x∈T C(a)

where TC(a) is the transitive closure of SWOs imported by
a.
Our hypothetical Rational Random Surfer (RRS) retains
PageRank’s direct visit component; the rational surfer can
jump to SWDs directly with a certain probability d. However, in the link-following component, the link is chosen with
, where x is the current SWDB,
unequal probability – ff(x,a)
(x)
a is the SWD that x links to, f (x, a) is the sum of all link
weights from x to a, and f (x) is the sum of the weights of
all outlinks from x. The control flow of such a surfer is is
shown in Figure 2.

E

no

yes

yes

Explore all linked
SWOs

bored?
no

Follow a
random link

Figure 2: Rational Random Surfer
Figure 3 illustrate how the rank of a SWO is computed.
Let A, B, C, D, E and F be SWOs, and assume that the
probability for a RRS to visit any of these SWOs from
a SWDB is 0.0001. The probability that she visits B is
P RSW O (B) + (P RSW O (E) + P RSW O (D)) = 0.0003. The
probability that she visits F is P RSW O (F ) + P RSW O (C) =
0.0002.The probability that she visits A is 0.0006 since A
will be visited when any of {B, C, D, E, F} is visited.
Table 4 shows the top ranked SWDs discovered by Swoogle.

7.

F

Figure 3: Combine ranks of SWO

Jump to a
random page

SWO?

D

INDEXING AND RETRIEVAL OF SWDS

Central to a Semantic Web search engine is the problem of
indexing and searching SWDs. While there is significant semantic information encoded in marked-up documents, reasoning over large collections of documents can be expensive. Traditional information retrieval techniques have the
advantage of being faster, while taking a somewhat more
coarse view of the text. They can thus quickly retrieve a set
of SWDs that deal with a topic based on similarity of the
source text alone.

In addition to efficiency, there are a number of reasons
why one would want to apply IR techniques to this problem. For one thing, documents are not entirely markup.
We would like to be able to apply search to both the structured and unstructured components of a document. Related
to this point, it is conceivable that there will be some text
documents that contain embedded markup. In addition, we
may want to make our documents available to commonly
used search engines, such as Google. This implies that the
documents must be transformed into a form that a standard information retrieval engine can understand and manipulate. Information retrieval techniques also have some
value characteristics, including well researched methods for
ranking matches, computing similarity between documents,
and employing relevance feedback. These compliment and
extend the retrieval functions inherent in Swoogle.
There has been work [20, 17, 10] demonstrating that such
techniques can be made to work with both RDF dcuments
as well as text documents with embedded RDF markup,
and that they can be made to leverage some of the semantic
information encoded.
Traditional IR techniques look at a document as either
a collection of tokens, typically words or N-Grams. An NGram is an n-character segment of the text which spans
inter-word boundaries. The N-Gram approach is typically
employed by sliding a window of n-characters along the text,
and taking a sample at each one character step. The use of
N-Grams can result in a larger vocabulary, as single words
can contain multiple N-Grams. One advantage to this approach is that inter-word relationships are preserved, where
they are typically not in word based approaches. N-Grams
are also known to be somewhat resistant to certain kinds of
errors, such as mis-spellings.
The use of N-Gram is particularly important to this approach because of the treatment of URIrefs as terms. Given
a set of keywords defining a search, we may want to match
documents that have URIrefs containing those keywords.
For example, consider a search for ontologies for “time”.
The search keywords might be time temporal interval point
before after during day month year eventually calendar clock
durations end begin zone. Candidate matches might include
documents containing URIrefs such as:
http://foo.com/timeont.owl#timeInterval
http://foo.com/timeont.owl#CalendarClockInterval
http://purl.org/upper/temporal/t13.owl#timeThing

Clearly, exact matching based on words only would miss
these documents (based on the URIrefs given). However,
N-Grams would find a number of matches.

656

Rank
1
2
3
4
5
6
7
8
9
10
11
12

URL
http://www.w3.org/1999/02/22-rdf-syntax-ns
http://www.w3.org/2000/01/rdf-schema
http://www.daml.org/2001/03/daml+oil
http://www.w3.org/2002/07/owl
http://www.w3.org/2000/10/rdf-tests/rdfcore/testSchema
http://www.w3.org/2002/03owlt/testOntology
http://ontology.ihmc.us/Entity.owl
http://www.w3.org/2001/XMLSchema
http://www.daml.org/2000/12/daml+oil
http://www.daml.org/2000/10/daml-ont
http://ontology.ihmc.us/Group.owl
http://ontology.ihmc.us/Actor.owl

Value
2845.97
2814.21
311.65
192.18
59.82
22.50
21.28
17.45
10.44
8.88
7.67
6.83

Table 4: Top 12 ranked SWDs (May 25,2004)

It is also possible, however, to use a word based approach.
Bear in mind that ontologies define vocabularies. In OWL,
URIrefs of classes, properties and individuals play the role of
words in a natural language. We can take an SWD, reduce
it to triples, extract the URIrefs (with duplicates), discard
URIrefs for blank nodes, hash each URIref to a token, and
index the document. Whereas a conventional information
retrieval system treats a text document as a bag of words, we
could treat a SWD as a bag of URIrefs. This would support
retrieval using queries which are also sets of URIrefs as well
as other functions, such as document similarity metrics.
We are currently adapting the Sire, a custom indexing
and retrieval engine we built for the Carrot2 distributed
IR system [7], to augment Swoogle. Sire can be made to
use either n-grams or words, and employs a TF/IDF model
with a standard cosine similarity metric. Sire is being enhansed to process RDF docouments using either character
level n-grams computed over the RDF source or to process
the URIrefs in the document as indexible tokens.

8.

A general user can query with keywords, and the SWDs
matching those keywords will be returned in ranked order.
As shown in Table 4, Swoogle ranks SWOs higher than
SWDBs; thus, SWOs will be returned as query results before
SWDBs. The highest ranked SWDs typically are the base
ontologies that define the Semantic Web languages, which
are used by almost all SWDs and are always imported by
SWOs.
For advanced users, an “advanced” search interface is provided (Figure 5), which essentially allows them to fill in
the constraints to a general SQL query on the underlying
database.
The user can query using keywords, content based constraints (e.g. type of SWD, OWL syntax, number of defined classes/properties/individuals), language and encoding based constraints (N3 vs XML), and/or the Rank of the
document. Sample results are shown in Figure 6.
At present, the metadata are stored in a mySQL database,
and we have indexed about 135, 000 SWDs. We anticipate
that in future versions, we will need to migrate to a commercial database in order to scale to indexing millions of
SWDs.
In our database, 13.29% SWDs are classified as SWOs,
and others as SWDBs (with 0.8 as the ontology ratio threshold). We find that about half of all SWDs have rank of 0.15,
which means they are not referred to by any other SWDs.
Also, the mean of ranks is 0.8376, which implies that the
SWDs we have crawled are poorly connected.
We are completing are transition to Swoogle2, which featured three components: Swoogle Search, Ontology Dictionary and statistical measures of the collection of SWDs. We
invite readers to visit Swoogle at http://swoogle.umbc.edu.

CURRENT STATUS

Swoogle is an ongoing project undergoing constant development. This paper primarily describes the features of
version 1. Figure 4 shows the Swoogle start page.

9.

CONCLUSIONS AND FUTURE WORK

Powerful search and indexing systems are needed by Semantic Web developers and researchers to help them find
and analyze SWDs. Such systems can also be used to support tools (e.g. markup editors), as well as software agents
whose knowledge comes from the Semantic Web.
Current web search engines such as Google and AlltheWeb
do not work well with SWDs, as they are designed to work
with natural languages and expect documents to contain unstructured text composed of words. Failing to understand
the structure and semantics of SWDs, they do not take advantage of them.

Figure 4: Swoogle interface

657

Figure 6: Swoogle query result

Figure 5: Swoogle advanced query

We have described a prototype crawler-based indexing
and retrieval system for Semantic Web documents. It runs
multiple crawlers to discover SWDs through meta-search
and link-following; analyzes SWDs and produces metadata
about SWDs and the relations among SWDs; computes ranks
of SWDs using a “rational random surfing model”; and indexs SWDs using an information retrieval system by treating
URIrefs as terms. One of the interesting properties computed for each semantic web document is its ontology rank
– a measure of the document’s importance on the Semantic
Web.
The current version of our system (Swoogle2) has discovered and analyzed over 137, 000 semantic web documents
with 3,900,000 triples. It has been designed and partially
implemented to capture more metadata on classes and properties and to support millions of documents. We have also
built an ontology dictionary based on the ontologies discovered by Swoogle, which we continue to refine.

10.

REFERENCES

[1] http://www.daml.org/ontologies/, daml ontology
library, by daml.
[2] http://www.schemaweb.info/, schema web.
[3] http://www.semanticwebsearch.com/, semantic web
search, by intellidimension.

658

[4] http://www.semwebcentral.org/, semwebcentral, by
infoether and bbn.
[5] http://www.w3.org/2004/ontaria/, ontaria, by w3c.
[6] F. Baader, D. Calvanese, D. McGuinness, D. Nardi,
and P. Patel-Schneider. The Description Logic
Handbook: Theory, Implementation, and Applications.
Cambridge University Press, 2003.
[7] R. S. Cost, S. Kallurkar, H. Majithia, C. Nicholas, and
Y. Shi. Integrating distributed information sources
with carrot ii. In Proceedings of the 6th International
Workshop on Cooperative Information Agents VI,
pages 194–201. Springer-Verlag, 2002.
[8] J. Davies, R. Weeks, and U. Krohn. Quizrdf: search
technology for thesemantic web. In WWW2002
workshop on RDF and Semantic Web Applications,
11th International WWW Conference (WWW11),
2002.
[9] S. Decker, M. Erdmann, D. Fensel, and R. Studer.
Ontobroker: Ontology based access to distributed and
semi-structured information. In DS-8, pages 351–369,
1999.
[10] T. Finin, J. Mayfield, C. Fink, A. Joshi, and R. S.
Cost. Information retrieval and the semantic web,
January 2004.
[11] S. Handschuh and S. Staab. Cream: Creating
metadata for the semantic web. Comput. Networks,
42(5):579–598, 2003.

[12] T. Haveliwala. Efficient computation of pageRank.
Technical Report 1999-31, 1999.
[13] I. Horrocks. DAML+OIL: A description logic for the
semantic web. IEEE Data Engineering Bulletin,
25(1):4–9, 2002.
[14] M. Lifantsev. Rank computation methods for Web
documents. Technical Report TR-76, ECSL,
Department of Computer Science, SUNY at Stony
Brook, Stony Brook, NY, November 1999.
[15] S. Luke, L. Spector, D. Rager, and J. Hendler.
Ontology-based web agents. In Proceedings of the
First International Conference on Autonomous Agents
(Agents97), pages 59–66, 1997.
[16] P. Martin and P. Eklund. Embedding knowledge in
web documents. In Proceedings of the 8th
International World Wide Web Conference (WWW8),
pages 324–341, 1999.

[17] J. Mayfield and T. Finin. Information retrieval on the
semantic web: Integrating inference and retrieval. In
Proceedings of the SIGIR 2003 Semantic Web
Workshop, 2003.
[18] L. Page, S. Brin, R. Motwani, and T. Winograd. The
pagerank citation ranking: Bringing order to the web.
Technical report, Stanford Digital Library
Technologies Project, 1998.
[19] I. Rogers. The google pagerank algorithm and how it
works. http://www.iprcom.com/papers/pagerank/,
May 2002.
[20] U. Shah, T. Finin, and A. Joshi. Information retrieval
on the semantic web. In Proceedings of the 11th
international conference on Information and
knowledge management, pages 461–468, 2002.

659

1356

IEEE TRANSACTIONS ON RELIABILITY, VOL. 64, NO. 4, DECEMBER 2015

Planning Constant-Stress Accelerated Life
Tests for Acceleration Model Selection
Rong Pan, Member, IEEE, Tao Yang, and Kangwon Seo

Abstract—Accelerated life tests (ALTs) are widely used in industry to assist in product development. Acceleration models are
often obtained from physical principles or past experience. But in
some applications, particularly for investigating a new material
or a new product, their acceleration models cannot be precisely
speciﬁed. The uncertainty in model speciﬁcation may cause serious
problems in failure time prediction, and reduce the statistical efﬁciency of an optimal test plan. In this paper, the and
optimal
criteria are proposed for designing ALT plans that are good at selecting the best acceleration model among rival models. A generalized linear model (GLM) is developed for modeling ALT data with
censoring. This approach simpliﬁes the derivation of the information matrix of a test plan, and allows the experimenter to develop
optimal ALT plans under the GLM framework. The proposed optimal design approach is compared with other conventional approaches through examples. The high design efﬁciency and design
ﬂexibility of the proposed approach are demonstrated in the paper.
Index Terms—Accelerated life test, generalized linear model,
-optimal design, -optimal design, model discrimination.

ACRONYMS AND ABBREVIATIONS
ALT

Accelerated Life Test

FUS

Fraction of Use Space

GLM
PH

Generalized Linear Model

RH

Relative Humidity

Proportional Hazard

NOTATION
vector of stress levels
vector of regression coefﬁcients
observed time
indicator variable for censoring
hazard function
baseline hazard function
cumulative hazard function
baseline cumulative hazard function
ALT test plan
Manuscript received March 24, 2014; revised August 29, 2014 and December
12, 2014; accepted April 05, 2015. Date of publication April 21, 2015; date of
current version November 25, 2015. Associate Editor: L. Cui. (Corresponding
author: Rong Pan.)
The authors are with the School of Computing, Informatics, and Decision
Systems Engineering, Arizona State University, Tempe, AZ USA (e-mail: rong.
pan@asu.edu; tyang13@asu.edu; kseo7@asu.edu).
Color versions of one or more of the ﬁgures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identiﬁer 10.1109/TR.2015.2421514

experimental design matrix of a test plan
asymptotic information matrix of a test plan
weight matrix
linear predictor
tuning parameter
-efﬁciency
-efﬁciency
I. INTRODUCTION
A. Background and Motivation

T

HE accelerated life test (ALT) is a popular reliability
testing method for obtaining product failure time data
within an affordable period of testing. Although the stress variables of ALT, such as temperature, humidity, voltage, etc., are
set to some higher-than-normal stress levels to hasten failures,
there is always failure time censoring to be anticipated given
the constraint of testing time and the cost of the experiment.
There are two common failure time censoring schemes: right
censoring, and interval censoring. Right censoring may happen
when an ALT is ended at a pre-determined terminal time; so,
for those unfailing test units, only their survival times are observed. Interval censoring often happens when the continuous
monitoring of test units is not available, and only periodic
inspections of test units are employed throughout the test.
Optimal experimental designs are particularly important
to ALTs, because these tests are very expensive, and they
are often constrained by the availability of test chambers.
Statistically efﬁcient test plans enable engineers or analysts
to extract more useful information from a limited number of
tests. Early research on optimal ALT plans was conducted by
Nelson and Meeker and their collaborators (see, e.g., [1]–[5]).
Nelson [6], [7] summarized the literature of accelerated tests,
including the problem of designing test plans (e.g., selecting
stress levels and allocating test units), up to 2005. For more
recent literature on optimal ALT plans, see Pascual [8], Elsayed
and Zhang [9], Ng et al. [10], Seo et al. [11], Ka et al. [12],
Liu [13], among others. The typical approach to the optimal
ALT planning problem is to derive the Fisher information
matrix from the likelihood function of a speciﬁc failure time
distribution and censoring model, then optimize a function of
the information matrix. Depending on the assumed failure time
distribution and the censoring scheme deployed in the test, the
likelihood function and Fisher information matrix may not be
easily obtained. Monroe et al. [14], and Yang and Pan [15]
proposed to use an acceleration model with the proportional
hazard (PH) property for ALT planning. It is well-known
that, when the failure time variable is modeled by a Weibull

0018-9529 © 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

PAN et al.: PLANNING CONSTANT-STRESS ACCELERATED LIFE TESTS FOR ACCELERATION MODEL SELECTION

distribution (the exponential distribution is a special case), then
the PH regression model and the failure time regression model
are equivalent to each other (see [16]). However, the PH model
is more ﬂexible in characterizing the effects of stress variables
in general, as it is semi-parametric in nature. In addition, [17]
and [18] showed that, due to the proportional hazard property,
the problem of failure time censoring (both right censoring and
interval censoring) can be skillfully solved by reformulating
the problem to a generalized linear model (GLM). Therefore,
the information matrix derived from a GLM can be utilized for
designing optimal ALT plans.
Like any optimal experimental design for nonlinear models,
the optimal ALT plan depends on various model assumptions.
To plan the test, the planner has to pre-specify the failure time
distribution model, the life acceleration model, and even the
model parameter values. These assumptions are known as the
planning information and the planning values in the ALT literature [19]. Pascual [8] summarized three types of uncertainties involved in the ALT planning: the uncertainty of the failure
time distribution, the uncertainty of the lifetime-stress relationship, and the uncertainty of the model parameter value. The ALT
planning issues caused by the model parameter uncertainty have
been addressed in previous studies. For example, the sensitivity
analysis of ALT plans to the misspeciﬁcation of ALT model
parameters was discussed in Meeker et al. [20], and Monroe
et al. [21]. To prevent the mistake of applying a simple linear
acceleration function in place of a curved life-stress relationship, Meeker and Hahn [22] advocated the use of compromised
ALT plans, which simply add a middle stress level into a twostress-level test plan. Bayesian test plans were also proposed
to address the uncertainties in ALT models [23], [24]. In this
paper, we present a test plan that is good at discriminating different acceleration models. Choosing the right model is one of
the most important objectives of an ALT, particularly when we
test a new material or a new product. An example with the detrimental effect of the misspeciﬁcation of the acceleration model
is presented in Section III. Therefore, this paper concerns with
the acceleration model uncertainty, and how to design a statistically efﬁcient test plan that can assist in choosing the right
model through ALTs.
B. Previous Work
Designing experiments for the purpose of model discrimination has mostly been discussed in the linear model literature
(see, e.g., [25]–[28]). The comparison of different model discrimination criteria can be found in Jones et al. [29], Dette and
Titoff [30], and Agboto and Li and Nachtsheim [31]. Under the
Bayesian inference paradigm, DuMouchel and Jones [32] proposed a Bayesian -optimality for model checking, and their
method also handled the parameter dependency problem on assumed models. An interesting work from Waterhouse et al. [33]
considered a problem when there are two rival GLMs for a binomial response, and compared the designs based on four different
optimal criteria. Biedermann et al. [34] developed the optimal
design theory for additive partially nonlinear regression models,
and generalized their results to be robust to model parameter uncertainty.
In this paper, we propose a dual-objective optimal ALT plan
that is capable of discriminating two rival acceleration models
that are in a nested structure. For example, for an ALT with one

1357

stress variable, a quadratic acceleration model includes the intercept, ﬁrst-order effect, and second-order effect terms, but a
simpler model may have the intercept and the ﬁrst-order effect
term only. Thus, the simpler model, which is referred as the reduced model, is nested into the more complicated model, which
is called the full model. Similarly, for an ALT with two stress
variables, a full model may include an intercept, two main effects, and their interaction, while a reduced model may include
an intercept and main effects only. The
-optimal criterion
aims at checking the effects of extra terms in a full model. But,
as mentioned in [35], it ignores the estimation of other effects
that appear in both the full model and the reduced model, thus
reducing the overall model estimation performance. To amend
this drawback, we apply both the -optimal and
-optimal
criteria. We compare the proposed
and
dual-objective
test plan with the compromise plan and the Bayesian model
checking plan through examples, and demonstrate the advantages of the proposed method for acceleration model selection.
The rest of the paper is organized as follows. In the next section, we brieﬂy summarize the GLM approach to ALT planning with censored failure times. A discussion of the acceleration model uncertainty and its impact on failure time prediction
are given in Section III. In Section IV, the optimality criteria
for
-optimal designs, and for
and
dual-objective designs, are developed, and they are implemented on a real-world
example. In Section V, the performance of dual-objective test
plans are compared with other model-checking test plans. Finally, the paper is concluded in Section VI.
II. THE GLM APPROACH TO PLANNING ALTS
GLM is an extension of the linear model for modeling a response variable that follows a distribution from the exponential
families [36]. A GLM regression model consists of three parts:
the distribution model of the response variable, the linear model
of the predictor, and the link function that connects the predictor
and the statistical mean of the response (for details, one may
refer to [37] and [38]). Modeling censored failure time data via
GLMs was ﬁrst implemented in Aitkin and Clayton [39], and
Whitehead [17], where the authors found a Poisson regression
model for ﬁtting right censored survival data. Later on, Finkelstein [18] demonstrated a binomial regression model that can
ﬁt interval censored survival data. The GLM approach deﬁnes
an indicator variable of failure time censoring, and treats it as
the response variable, while the actual failure times or censoring
times are included in the link function. Such an approach uniﬁes
the data analysis methods of ALTs under many different censoring scenarios, and it can be easily extended to more complicated test plans, such as step-stress ALTs (see [40]). Furthermore, multiple stress factors can be included in the linear predictor, and the information matrix derived from the GLM is already available for statistical inference of the model parameter
or for test planning.
The GLM approach to censored failure time data analysis is
based on the Cox's Proportional Hazard (PH) model (see [41]),
which assumes that the hazard function at a stress level is
given by
(1)
Equation (1) shows that at any given time the product's hazard
function,
, is proportional to its baseline hazard function,

1358

IEEE TRANSACTIONS ON RELIABILITY, VOL. 64, NO. 4, DECEMBER 2015

. A basic feature of this model is that the baseline hazard
function is a function of time only. It can be shown that, to infer
coefﬁcients
does not need to be speciﬁed; therefore, the
PH model is a semi-parametric model. However, for the ALT
planning purpose, the hazard function must be fully parameterized. When
is a constant (i.e.,
), failure times
follow exponential distributions, and failure rates depend on the
setting of stress variables through a factor of
; when
(i.e., a power function of time), failure times
follow Weibull distributions with a shape parameter . Other
common failure time distributions, such as a piecewise exponential distribution or a Gompertz-Makeham distribution, can
be also speciﬁed by properly deﬁning the baseline hazard function.
In the following, we will brieﬂy describe the GLM formulation for the ALT data with right censoring. For more details,
one may refer to Collett [42]. Let the observation from an ALT
be a triplet
, where is the observation time (either
failure time or censoring time),
is an indicator variable of
failure or censoring (e.g.,
if a failure time is observed
and
otherwise), and
is the stress level applied on the
-th test unit. Then the likelihood function is given by

where
, and
are the failure density function,
and the reliability function, respectively. As
, and
, the loglikelihood function can be written as

(2)
.
Given the PH model of (1), we have
. One can see
Therefore,
that the second term on the right hand side of (2) is a function
of observed time and censoring indicator only. To maximize the
loglikelihood function with respect to stress factors, we would
only deal with the ﬁrst term, which is proportional to the loglikelihood function of statistically independent Poisson variables
with a statistical mean of
.
Assume an exponential failure time distribution such that
, and
. Then the
Poisson regression model can be written as follows for each
case.
• Distribution model:
• Linear predictor:
• Link function:
Here, the linear predictor of the GLM corresponds to the lifestress acceleration model used in ALT. Similar GLM formulations for the Weibull distribution and extreme value distribution
were discussed in Aitkin and Clayton [39]. When failure times
are interval censored, Finkelstein [18] showed that the number
of failures within each inspection interval can be modeled by
a binomial regression with a complementary log-log link function.

With GLMs, the asymptotic information matrix of a test plan,
, is given by
(3)
The design matrix,
, is generated from the test plan, and the
weight matrix, , is derived from the link function of GLM. A
-optimal test plan is constructed by maximizing the determinant of the information matrix, i.e.,
(4)
This result is the same as minimizing the volume of the conﬁdence region of regression coefﬁcient estimates.
III. UNCERTAINTY OF THE ACCELERATION MODEL
In most previous studies of ALTs, acceleration models were
assumed to be known. Some of these models can be derived
from physics or chemistry principles related to material degradation. However, when a new material system is developed,
the applicability of these existing models is often in doubt.
For example, Monroe and Pan [43] provided an industrial case
where the Cofﬁn-Mason acceleration model of lead-free solder
joint failure needs to be validated by experiments. Furthermore,
a product may experience multiple types of stresses during its
lifetime, and fail due to a more complicated mechanism that a
simple physics-based model cannot adequately explain. With
the advances in testing technology, it is nowadays possible
to apply multiple stresses on test units simultaneously, thus
to open the opportunity of studying the interaction effect of
these stresses on the product's lifetime. An optimal test plan
designed for a speciﬁc stress model may no longer be optimal
for multiple stress types. Therefore, there is a practical need
of designing ALT experiments for selecting the right acceleration model. Because the ALT data are often used to predict
the product's failure time under its normal use condition, the
prediction with an incorrect acceleration model could be far
off from the truth due to the model-based extrapolation. In
this section, we use an example to demonstrate the danger of
making a careless acceleration model assumption.
Yang and Pan [15] described an ALT of an electronic device, where two stress factors, temperature and humidity, were
applied on this device to accelerate its failure. The typical use
condition of this device is at 30 C, and 25% relative humidity
(RH). Its failure time is assumed to follow an exponential distribution. Suppose that the accelerated test stress conditions can be
set from 60 C to 110 C for temperature, and from 60% RH to
90%RH for humidity. The natural stresses of temperature and
humidity are deﬁned as
, where is the temperature in degrees Kelvin, and
, where is the
percentage of relative humidity. The PH model has the hazard
function to be an exponential function of the linear combination of these natural stress variables. We further deﬁne the design space of this experiment to be a unit square, and have the
use condition to be located at the ﬁrst quadrant. This deﬁnition
leads to a coding scheme of stress levels, given by

PAN et al.: PLANNING CONSTANT-STRESS ACCELERATED LIFE TESTS FOR ACCELERATION MODEL SELECTION

1359

and

where
, and
are the higher level, and the lower level
of a natural stress, respectively. Thus, after transformation, the
highest stress levels of both stress variables are coded as (0, 0),
and the lowest stress levels are coded as (1, 1).
We apply the Poisson regression formulation in Section II on
this example, and consider three different linear predictors. Engineers understand that both temperature and humidity are signiﬁcant factors that affect this device's lifetime; however, they
are uncertain about the interaction effect of these two stress factors or any higher-order effects. Therefore, the following three
acceleration models are speculated.

Fig. 1. FUS plots of the prediction variance with different models.

TABLE I
PREDICTING MEAN FAILURE TIME AT THE USE CONDITION

(5)
(6)
and
(7)
Model (7) is a full quadratic model, and Model (5) or (6) can be
viewed as a reduced model from this full model.
Suppose that the coded natural temperature stress at this device's use condition varies from 1.458 to 2.058, the coded natural humidity stress varies between 2.859 to 3.459, and the experimenter wants to ﬁnd a test plan that is good at predicting the
failure time at the device's use condition. An -optimal test plan
is ideal for this case, because the optimality, as deﬁned below,
minimizes the average prediction variance over the entire use
condition region.
(8)
is the use stress condition, is the region of the
where
use condition, and
is the area of the use condition region.
Note that Ye et al. [44] discussed a frailty failure time model for
handling varying use conditions. Their model essentially introduces a random effect into the life-stress acceleration function,
which will complicate the ALT planning; therefore, we choose
the simple -optimal criteria.
It is assumed that the regression coefﬁcients are as
(which corresponds to a failure rate of 1 per time unit at the
highest test stress level),
, and
; there are 100 test units; and
the test is terminated after 30 time units. Three different optimal
ALT plans can be derived from these acceleration models (for
brevity, these plans are not shown in the paper). One important
difference of these plans is that they choose different numbers
of testing conditions because of the size of their respective acceleration models. Assume that the ﬁrst model is in fact the true
acceleration model. Then what kind of effect will be brought
by a test plan that was derived from a wrong model? We use a
Fraction of Use Space (FUS) plot to illustrate the statistical efﬁciency of a test plan over the entire use condition region. The
curves in Fig. 1 plots the prediction variance versus the fraction
of the use stress region that has less or equal to this prediction
variance value. A good test plan will keep its FUS curve as low
as possible.

From Fig. 1, one can see that a wrong model will generate a
test plan that has an inferior prediction performance in the use
region. When there are more extra terms adding to the true acceleration model (i.e., a larger deviation from the true model),
it causes further deterioration in prediction power. This condition can be seen from the curves of the wrong models with full
quadratic effects, and with interaction effect only. More importantly, for the failure time prediction, extrapolation will exaggerate the difference between two models when the use condition is moving away from the test condition region. For example, to compare the acceleration models in (6) and (7), we
predict their mean time to failure (MTTF) at three different use
conditions. The results in Table I clearly show that, if a wrong
model is assumed, extrapolation will cause a severe problem in
failure time prediction.
The uncertainty in the acceleration model can cause a presumed optimal test plan no longer statistically efﬁcient, which,
in turn, affects the ability of the test plan to establish the correct
acceleration model. An incorrect assumption of an acceleration
model will certainly worsen the prediction of failure time at the
product's use condition due to extrapolation. Thus, it is important to choose the right acceleration model before any extensive
investigation of the failure distribution at the use condition. This
importance motivates us to ﬁnd a test plan that is capable of performing model discrimination among rival acceleration models.
IV. OPTIMAL TEST PLANS FOR ACCELERATION
MODEL SELECTION
A.

-Optimal Test Plans
Consider a linear predictor such as
(9)

Vectors , and
are, respectively, the vector of essential (required) predictors, and the vector of secondary (in-doubt) predictors; and , and
are their corresponding coefﬁcient vectors. The experimenter is interested in estimating the coefﬁ-

1360

IEEE TRANSACTIONS ON RELIABILITY, VOL. 64, NO. 4, DECEMBER 2015

cients of secondary predictors so as to determine whether or not
these terms are truly needed.
Accordingly, the design matrix of a test plan, , can be written
as
, where
is the design matrix for all required terms, and
is the design matrix for secondary
terms. To simplify our notation, we will drop in the equations
below as long as it does not cause confusion. Based on (3), the
information matrix of an ALT plan is given by
Fig. 2.

-optimal design: assessing

and

.

Fig. 3.

-optimal design: assessing

and

.

Fig. 4.

-optimal design: assessing

and

.

Denote
, and
, so they are the information
matrices associated with the required terms, secondary terms,
and their interactions. It is easy to show that
, as
is a diagonal matrix. So, the whole information matrix can
be partitioned as

Then, the covariance matrix for the maximum likelihood estimation of
is the bottom-right submatrix of the inverse of ,
which is given by

Minimizing the conﬁdence region of the estimation
is equivalent to maximizing the determinant of
of
, which is equivalent to
.
Note that both determinants are functions of the test plan, .
-optimal test plan (the subscript
Thus, is deﬁned to be the
stands for subset) if
(10)
Using the example of Section III, we consider the following
three scenarios. Each of them consists of two rival models: a full
model, and a reduced model. All scenarios have the same full
model, which is a full quadratic model with two stress variables.
Full Model:
(Scenario 1) Reduced Model:
(Scenario 2) Reduced Model:
(Scenario 3) Reduced Model:
In the ﬁrst scenario, the quadratic effects of stress variables
and
are in question; therefore, the regression coefﬁcients of
need to be assessed. In the second scenario, there is an uncertainty on the effect of the ﬁrst stress variable, so whether or not
alone can signiﬁcantly affect the product's life needs to be
tested, and the ﬁrst-order and second-order effects of this variable need to be assessed. Lastly, in the third scenario, the effects
of the second variable are put into check. For each of these three
scenarios, a -optimal test plan shall concentrate on the statistical efﬁciency in estimating the regression coefﬁcients that are
not included in the reduced model. Three
-optimal test plans
for these scenarios are given in Tables II through IV, and illustrated by Figs. 2 through 4. The area of a circle in these ﬁgures
is proportional to the number of test units allocated at their cor-

responding test condition. The legend in Fig. 2 demonstrates the
relative sizes of circles for 5, 10, 20 and 40 test units. To generate
these test plans, 100 test units are assumed, and the optimization algorithms described in [14] and [15] are used. Initially, an
optimal design matrix with 100 design points are found. These
design points are then clustered to a few distinct design points
based on their closeness to each other.
From Figs. 2 through 4, one can see that a
-optimal test
plan often includes many design points (test conditions). This
inclusion is because a
-optimal test plan has to simultaneously plan for two models. In the meantime, as these plans are
designed for assessing only a subset of parameters in the full
model, some design points are allocated with very few test units.
For example, Table II and Fig. 2 demonstrate that, to check the
quadratic effects
and , most test units should be allocated
close to the center of the design region and the centers of both
axes. Table III and Fig. 3 show that, to check the effect of ,
almost all test units are allocated on the
axis. Similar conclusions can also be drawn from Table IV and Fig. 4 for assessing the effect of . In general, a
-optimal test plan is

PAN et al.: PLANNING CONSTANT-STRESS ACCELERATED LIFE TESTS FOR ACCELERATION MODEL SELECTION

TABLE II
-OPTIMAL DESIGN: ASSESSING

1361

TABLE III
-OPTIMAL DESIGN: ASSESSING

TABLE IV
-OPTIMAL DESIGN: ASSESSING

good at checking the regression terms of interest under the two
rival models, but it allocates too few test units to the less important design points. This result is obviously not a good practice
for planning ALTs, because if all of the test units at a less important design point do not fail during the testing, there will be
no failure information obtained at this point, and consequently
some regression coefﬁcients in the link function cannot be estimated. This type of ill-conceived test plan was criticized in
Meeker and Escobar [45], and Meeker et al. [46]. Therefore, a
dual-objective design criteria will be proposed in the next section to alleviate this problem.
Furthermore, as pointed out by one reviewer, the robustness
of an ALT plan is a very important aspect of any good test plan.
In this paper, we address the issue of acceleration model uncertainty. In the case where there are several possible reduced
models, we need to derive a test plan that is robust enough to
all possible models. To achieve this goal, we propose the following two approaches. The ﬁrst approach is to pool the design points from each
-optimal design generated for a pair
consisting of a full model and a reduced model, and cluster
them to produce the ﬁnal test plan. This approach follows the
clustering recommendation from Dror and Steinberg [47], although in [47] their problem was the model parameter uncertainty problem of nonlinear models. In the previous example,
the design points of 100 test units were generated in each
optimal test plan, so there are totally 300 design points when
these plans are pooled together. Then, they are clustered to 9
design points, as shown in the left half of Table V. The second
approach is simply to provide a -optimal design for the full
model. As one can see from Figs. 2 through 4, a
design focuses on the estimation of the regression coefﬁcients that are
missing from the reduced model; when there are many possible
reduced models, one would want to check every coefﬁcient in
the full model, so a -optimal design suits this purpose. The
right half of Table V shows the -optimal design for this example. One can see that the test plans derived from the two approaches are similar to each other. They both have at least three
test units allocated at any design point, and a more evenly distributed number of test units across all design points, in comparison to the test plans in Tables II through IV. It reveals that a

TABLE V
ROBUST TEST PLANS WHEN CONSIDERING MULTIPLE
REDUCED ACCELERATION MODELS

-optimal criterion is very helpful for generating a robust test
plan when facing the reduced model uncertainty.
B.

and

Dual-Objective Test Plans

If an experimenter has prior knowledge of which stress factors are important, and how they may affect the product's life,
he or she may just want to check other secondary stress factors

1362

Fig. 5. Dual-objective test plan with

IEEE TRANSACTIONS ON RELIABILITY, VOL. 64, NO. 4, DECEMBER 2015

.

or effects that are in doubt. In such a case, the
-optimal test
plan is an efﬁcient plan. However, real-world applications are
often more complicated. First, ALT experiments are typically
very expensive, so testing just a few units at one test condition
is clearly uneconomical. Due to limited testing time, it is likely
that there is no failure observed at those test conditions that are
allocated very few test units, which will cause large variances
on regression coefﬁcient estimators (because of the ill-conditioned design matrix). Secondly, an experimenter of ALTs often
has multiple objectives in mind. One of the most common objectives is to estimate the effects of all required terms in the
reduced model, because these terms are known to be needed.
Therefore, we propose a dual-objective design with both and
-optimality criteria for ALT planning. The -optimality criterion is used to estimate the required parameters in the reduced
model, and the
-optimality criterion is used to check other
extra terms in the full model. To combine the two criteria together, the scaled determinants (by the number of model parameters) of information matrices for and
criteria are used. It
is convenient to use log scaled determinants, and combine them
by a tuning parameter, , with
. Thus, we deﬁne the
dual-objective optimal test plan to be

Fig. 6. Dual-objective test plan with

.

Fig. 7. Dual-objective test plan with

.

TABLE VI
DUAL-OBJECTIVE TEST PLAN WITH

(11)
The parameter in (11) is the number of required terms in the
reduced model, and is the number of secondary terms in the
full model.
We apply this dual-objective criterion on the ﬁrst scenario
from the last section with
. The dual-objective
test plans are presented in Tables VI through X, and plotted in
Figs. 5 through 7. Again, it is assumed that there are totally 100
test units, and they are allocated to 9 design points.
Parameter is the relative weight to the importance of and
objectives. When is large, the focus of dual-objective design shifts to -optimality criterion, and vice versa. So, -optimal and -optimal test plans are two special cases of dual-objective designs. In addition, it can be shown that, when
, the dual-objective test plan is the same as the -optimal
,
test plan for the full model. As shown in Fig. 5, when
more test units are allocated at the center of each axis, and the
center of the design region. This allocation is due to the larger
weight of
-optimality for checking the quadratic terms,

and . When
, as shown in Fig. 6, all design points receive a roughly equal number of test units, except for the center
point of the design region, which has more test units to test the
quadratic effects of stress variables; thus, this design is a more
balanced one. Similarly, Fig. 7 shows that, for
, more
test units are allocated at the corners of the design region, and
this result is caused by the emphasis of -optimality for estimating primary stress effects.

PAN et al.: PLANNING CONSTANT-STRESS ACCELERATED LIFE TESTS FOR ACCELERATION MODEL SELECTION

TABLE VII
DUAL-OBJECTIVE TEST PLAN WITH

1363

To better understand the effects of , an efﬁciency plot of
-efﬁciency and
-efﬁciency of dual-objective test plans for
-efﬁthis example is given in Fig. 8. The -efﬁciency, and
ciency of a given test plan, , are deﬁned, respectively, as
(12)
and
(13)

TABLE VIII
DUAL-OBJECTIVE TEST PLAN WITH

From Fig. 8, one can choose an appropriate value to balance
the efﬁciencies of these two criteria. In this example, to have the
-efﬁciency at least 0.6, and the
-efﬁciency at least 0.8, a
value between 0.5 and 0.8 will be appropriate. The dual-objective approach provides a trade-off between the test plan's performance on estimating primary stress effects and its performance on estimating secondary stress effects, thus an experimenter may choose a proper test plan based on his or her needs.
In the previous illustrative example, 9 distinct design points
(test conditions) are chosen. However, as ALT experiments are
very expensive, reliability engineers would prefer fewer design
points, which would require fewer testing chambers. Consequently, we can cluster the original 100 design points to 8, 7,
or 6 design points. Note that this number is not recommended
to be 5 or less, because it will be equal to or less than the number
of parameters in the acceleration model, and then the test plan
will become a saturated or supersaturated design, which causes
some model parameters to be unestimable. In the previous example with
, by reducing the number of design points
-efﬁciency is slightly reduced from
from 9 to 8, 7, and 6, the
1 to 0.99, 0.96, and 0.85, respectively; and the -efﬁciency is
reduced from 0.62 to 0.59, 0.54, and 0.5, respectively. These
test plans are given in Table IX.
V. COMPARISON WITH OTHER MODEL-CHECKING TEST PLANS

TABLE IX
DUAL-OBJECTIVE TEST PLANS WITH LESS DESIGN POINTS WHEN

A. Compromise Split Test Plans
Meeker and Escobar [19] proposed a compromise ALT planning strategy to avoid the pitfall of using optimal ALTs without
model checking. The compromise plan is derived from an optimal plan or an optimal degenerate plan if there are two stress
variables. Basically, a middle test level is inserted into the optimal test plan, and a certain percentage of total test units will
be tested at this middle level. Meeker and Hahn [22] recommended a 4:2:1 allocation scheme for the lower, middle, and
higher stress levels, respectively. The beneﬁts of compromise
test plans, as stated in [19], are that “they tend to be more robust
to misspeciﬁcation of unknown inputs and they allow one to estimate model parameters even if there are no failures at one level
of the accelerating variable.” Furthermore, when there are two
stress variables, it is suggested that a degenerate design point
should be split into two design points on the boundary of the
experimental design region so as to enhance the parameter estimation property of the test plan. An example of a 20% compromise split test plan for a two-stress ALT of insulation is given in
[19] (p. 555). Voltage and temperature are the two stress factors
in this example. It is assumed that failure times follow Weibull
distributions with a shape parameter value of 1.485, and all tests
are terminated after 1000 hours. Using the planning values from
[19], and the notations from this paper, the logarithm of the scale

1364

IEEE TRANSACTIONS ON RELIABILITY, VOL. 64, NO. 4, DECEMBER 2015

TABLE X
COMPARING THE COMPROMISE SPLIT TEST PLAN WITH TWO DUAL-OBJECTIVE MODEL DISCRIMINATION TEST PLANS

TABLE XI
BAYESIAN D-OPTIMAL TEST PLAN WITH

TABLE XII
BAYESIAN D-OPTIMAL TEST PLAN WITH

parameter of a Weibull distribution becomes the linear prediction, which can be written as

Note that and are the coded natural stress variables having
values between 0 and 1. We also consider a case where there
is a small amount of interaction effect between
and , by
assuming an additional term
.
and
are
The dual-objective test plans with
constructed, and they are compared with the compromise split
test plan from [19]. These plans, along with their and
efﬁciencies, and the prediction variance of mean failure time at the
use condition derived from these plans, are shown in Table X.
One can see that the two dual-objective test plans have better
-efﬁciency than the compromise split test
-efﬁciency and
plan. This result is expected because the dual-objective plan is

Fig. 8. Efﬁciency plot of scenario 1.

PAN et al.: PLANNING CONSTANT-STRESS ACCELERATED LIFE TESTS FOR ACCELERATION MODEL SELECTION

obtained by optimizing these design criteria. Moreover, it is noticed that the prediction variance of the mean failure time at use
conditions is also smaller with dual-objective plans. This outcome is in part due to the excellent model estimation property
of these plans. In addition, these plans use fewer design points
(test conditions) than the compromise split plan, and thus are
easier to be implemented in practice.

1365

TABLE XIII
BAYESIAN D-OPTIMAL TEST PLAN WITH

B. Bayesian Model-Checking Test Plans
DuMouchel and Jones [32] proposed a Bayesian approach to
a ﬂexible family of designs for parsimonious model checking.
The central problem of their study is similar to the acceleration model selection problem in this paper, but their study is
limited to linear models. Consider a linear model, where some
effects, such as the main effect or the interaction effect of the experimental factor, are known to be important, but other effects,
such as the quadratic effect, are less certain to be included in the
model. Let the parameter vector be divided into two groups:
for the required parameters, and for the secondary parameters. Assume the prior distribution of these parameters to be a
multivariate normal distribution such as

where
is the mean vector of the prior distribution of , because the effects of corresponding terms are in doubt; and
are scalars, representing the uncertainties in the required and
secondary parameters. Using a diffuse prior for , and letting
, the precision matrix, which is the inverse of the variance-covariance matrix, of the prior distribution will then go to
, and
terior information matrix for

. It can be shown that the posis given by

The posterior information matrix is a weighted sum of the prior
precision matrix and the information matrix of the test plan. In
general, represents how important is the prior uncertainty in
the secondary parameters to the posterior inference of model parameters, and
. When the uncertainty is
large, i.e.,
is large, then is approaching 1, so the posterior
information matrix becomes the same as the ordinary information matrix (i.e., no prior information will be utilized). Thus, in
this case, the optimal design based on the posterior information
is the same as the -optimal design for the full model. Conversely, when
is very small, which indicates that it is certain
a priori that those secondary terms are not needed in the model,
the optimal experimental design will become the -optimal design for the reduced model.
To adapt this Bayesian approach to the optimal design for
GLMs, we replace the information
by the asymptotic
information matrix of the GLM (i.e.,
), and
then optimize the determinant of the posterior information
matrix for different values of . Tables XI through XIII provide the Bayesian test plans for the ﬁrst scenario used in
Section IV (checking quadratic terms in a two-stress-factor
acceleration model) with
, respectively. These
Bayesian test plans are very sensitive to the tuning parameter
. When
, only four testing conditions are selected, as
for the reduced model fewer parameters need to be estimated.

When becomes larger, up to nine distinct testing conditions
are selected, and the plan becomes similar to the ones from
dual-objective designs. Comparatively, the test plans from the
dual-objective optimization are less sensitive to the tuning
parameter . This result is true because, unlike the Bayesian
plan where the secondary parameters are considered through
the weight of the full model, the dual-objective plans directly
target on the secondary parameters through the
criterion.
VI. CONCLUSION
When planning an ALT for a new material or a new product,
the uncertainty in the failure acceleration model cannot be ignored, and experimenters should consider test plans that are
good at checking the suspected effects of stress factors and their
interactions. The main contribution of this paper is that a
and
dual-objective approach is proposed for ﬁnding a balanced ALT test plan that can achieve high statistical efﬁciency in
the primary stress-effect estimation as well as in the secondary
stress-effect checking, thus helping experimenters to select the
correct acceleration model through ALT experiments. We develop a GLM formulation for modeling ALT data with censoring, and discuss the ALT planning as an application of the
optimal experimental designs for GLMs. In addition, our test
plans are compared with other test plans (compromise plan and
Bayesian model-checking plan) in the literature. It is shown that
the proposed and
dual-objective test planning has the advantage of obtaining higher and
efﬁciencies, and it provides viable alternative plans for experimenters.
REFERENCES
[1] W. Nelson and T. J. Kielpinski, “Theory for optimum censored accelerated life tests for normal and lognormal life distributions,” Technometrics, vol. 18, no. 1, pp. 105–114, 1976.
[2] W. Nelson and W. Q. Meeker, “Theory for optimum accelerated censored life tests for Weibull and extreme value distributions,” Technometrics, vol. 20, no. 2, pp. 171–177, 1978.
[3] W. Q. Meeker and W. Nelson, “Optimum accelerated life tests for the
Weibull and extreme value distributions,” IEEE Trans. Rel., vol. R-24,
no. 5, pp. 321–332, 1975.
[4] W. Q. Meeker, “A comparison of accelerated life test plans for Weibull
and lognomal distributions and type I censored data,” Technometrics,
vol. 26, pp. 157–172, 1984.

1366

[5] W. Q. Meeker and L. A. Escobar, “A review of recent research and
current issues in accelerated testing,” Int. Statist. Rev., vol. 61, pp.
147–168, 1993.
[6] W. Nelson, “A bibliography of accelerated test plans,” IEEE Trans.
Rel., vol. 54, no. 2, pp. 194–196, 2005.
[7] W. Nelson, “A bibliography of accelerated test plans part II—References,” IEEE Trans. Rel., vol. 54, no. 3, pp. 370–373, 2005.
[8] F. G. Pascual, “Accelerated life test plans robust to misspeciﬁcation of
the stress-life relationship,” Technometrics, vol. 48, no. 1, pp. 11–25,
2006.
[9] E. A. Elsayed and H. Zhang, “Design of PH-based accelerated life
testing plans under multiple-stress-type,” Rel. Eng. Syst. Safety, vol.
92, no. 3, pp. 286–292, 2007.
[10] H. K. T. Ng, N. Balakrishnan, and P. S. Chan, “Optimal sample size
allocation for tests with multiple levels of stress with extreme value
regression,” Naval Res. Logist., vol. 54, no. 3, pp. 237–249, 2007.
[11] J. H. Seo, M. Jung, and C. M. Kim, “Design of accelerated life test sampling plans with a nonconstant shape parameter,” Eur. J. Oper. Res.,
vol. 197, no. 2, pp. 659–666, 2008.
[12] C. Y. Ka, P. S. Chan, H. K. T. Ng, and N. Balakrishnan, “Optimal
sample size allocation for multi-level stress testing with Weibull regression under type-ii censoring,” Statistics, vol. 45, no. 3, pp. 257–279,
2011.
[13] X. Liu, “Planning of accelerated life tests with dependent failure modes
based on a gamma frailty model,” Technometrics, vol. 54, no. 4, pp.
398–409, 2012.
[14] E. M. Monroe, R. Pan, C. M. Anderson-Cook, D. C. Montgomery, and
C. M. Borror, “A generalized linear model approach to designing accelerated life test experiments,” Qual. Rel. Eng. Int., vol. 27, pp. 595–607,
2011.
[15] T. Yang and R. Pan, “A novel approach to optimal accelerated life test
planning with interval censoring,” IEEE Trans. Rel., vol. 62, no. 2, pp.
527–536, 2013.
[16] J. F. Lawless, Statistical Models and Methods for Lifetime Data, 2nd
ed. New York, NY, USA: Wiley, 2003.
[17] J. Whitehead, “Fitting Cox's regression model to survival data using
glim,” Appl. Statist., vol. 29, no. 3, pp. 268–275, 1980.
[18] D. M. Finkelstein, “A proportional hazards model for interval-censored
failure time data,” Appl. Statist., vol. 42, no. 2, pp. 845–854, 1986.
[19] W. Q. Meeker and L. A. Escobar, “Statistical methods for reliability
data,” in Wiley Series in Probability and Statistics, 1st ed. New York,
NY, USA: Wiley-Interscience, 1998.
[20] W. Q. Meeker, L. A. Escobar, and S. A. Zayac, “Use of sensitivity
analysis to assess the effect of model uncertainty in analyzing accelerated life test data,” in Case Studies in Reliability and Maintenance, W.
R. Blischke and D. N. P. Murthy, Eds. New York, NY, USA: Wiley,
2003.
[21] E. M. Monroe, R. Pan, C. M. Anderson-Cook, D. C. Montgomery, and
C. M. Borror, “Sensitivity analysis of optimal designs for accelerated
life testing,” J. Qual. Technol., vol. 42, pp. 121–135, Apr. 2010.
[22] W. Q. Meeker and G. J. Hahn, How to Plan an Accelerated Life Test:
Some Practical Guidelines. Milwaukee, WI, USA: Amer. Soc. Qual.
Control, 1985.
[23] K. Chaloner and I. Verdinelli, “Bayesian experimental design: A review,” Statist. Sci., vol. 10, pp. 273–304, 1995.
[24] A. Erkanli and R. Soyer, “Simulation-based designs for accelerated life
tests,” J. Statist. Plan. Infer., vol. 90, no. 2, pp. 335–348, 2000.
[25] A. C. Atkinson and V. V. Fedorov, “The design of experiments for
discriminating between two rival models,” Biometrika, vol. 62, no. 1,
pp. 57–70, 1975.
[26] A. C. Atkinson and V. V. Fedorov, “Optimal design: Experiments for
discriminating between several models,” Biometrika, vol. 62, no. 2, pp.
289–303, 1975.
[27] P. D. H. Hill, “A review of experimental design procedures for regression model discrimination,” Technometrics, vol. 20, no. 1, pp. 15–21,
1978.
[28] A. C. P. DeLeon and A. C. Atkinson, “Optimum experimental design
for discriminating between two rival models in the presence of prior
information,” Biometrika, vol. 78, no. 3, pp. 601–608, 1991.
[29] B. A. Jones, W. Li, C. J. Nachtsheim, and K. Q. Ye, “Model discrimination—Another perspective on model-robust designs,” J. Statist. Plan.
Infer., vol. 137, no. 5, pp. 1576–1583, 2007.
[30] H. Dette and S. Titoff, “Optimal discrimination designs,” Ann. Statist.,
vol. 37, no. 4, pp. 2056–2082, 2009.
[31] V. Agboto, W. Li, and C. Nachtsheim, “Screening designs for model
discrimination,” J. Statist. Plan. Infer., vol. 140, no. 3, pp. 766–780,
2010.

IEEE TRANSACTIONS ON RELIABILITY, VOL. 64, NO. 4, DECEMBER 2015

[32] W. DuMouchel and B. Jones, “A simple Bayesian modiﬁcation of
D-optimal designs to reduce dependence on an assumed model,”
Technometrics, vol. 36, no. 1, pp. 37–47, 1994.
[33] T. H. Waterhouse, D. C. Woods, J. A. Eccleston, and S. M. Lewis,
“Design selection criteria for discrimination between nested models
for binomial data,” S3RI Methodology Working Papers, 2006.
[34] S. Biedermann, H. Dette, and D. C. Woods, “Optimal design for
additive partially nonlinear models,” Biometrika, vol. 98, no. 2, pp.
449–458, 2011.
[35] A. Atkinson, A. Donev, and R. Tobias, “Optimum experimental designs, with SAS,” in Oxford Statistical Science Series. New York,
NY, USA: Oxford University Press, 2007.
[36] J. A. Nelder and R. W. M. Wedderburn, “Generalized linear models,”
J. Roy. Statist. Soc., Series A (General), vol. 135, no. 3, pp. 370–384,
1972.
[37] P. McCullagh and J. A. Nelder, Generalized Linear Models. Monographs on Statistics & Applied Probability, 2nd ed. Boca Raton, FL,
USA: Chapman and Hall/CRC, Aug. 1989.
[38] R. H. Myers, D. C. Montgomery, and G. G. Vining, “Generalized linear
models with applications in engineering and the sciences,” in Wiley
Series in Probability and Statistics, 2nd ed. New York, NY, USA:
Wiley, 2002.
[39] M. Aitkin and D. Clayton, “The ﬁtting of exponential, Weibull and
extreme value distributions to complex censored survival data using
glim,” Appl. Statist., vol. 29, no. 2, pp. 156–163, 1980.
[40] J. Lee and R. Pan, “Analyzing step-stress accelerated life testing
data using generalized linear models,” IIE Trans., vol. 42, no. 8, pp.
589–598, 2012.
[41] D. R. Cox, “Regression models and life tables (with discussion),” J.
Roy. Statist. Soc., Series B, vol. 34, pp. 187–220, 1972.
[42] D. Collett, Modelling Survival Data in Medical Research. Boca
Raton, FL, USA: Chapman & Hall/CRC, 1994.
[43] E. M. Monroe and R. Pan, “Experimental design considerations for acceleration life tests with nonlinear constraints and censoring,” J. Qual.
Technol., vol. 40, no. 4, pp. 355–367, 2008.
[44] Z.-S. Ye, Y. Hong, and Y. Xie, “How do heterogeneities in operating
environments affect ﬁeld failure predictions and test planning?,” Ann.
Appl. Statist., vol. 7, no. 4, pp. 2249–2271, 2013.
[45] W. Q. Meeker and L. A. Escobar, “Pitfalls of accelerated testing,” IEEE
Trans. Rel., vol. 47, no. 2, pp. 114–118, 1998.
[46] W. Q. Meeker, G. Sarakakis, and A. Gerokostopoulos, “More pitfalls
of accelerated tests,” J. Qual. Technol., vol. 45, no. 3, pp. 213–222,
2013.
[47] H. A. Dror and D. M. Steinberg, “Robust experimental design for multivariate generalized linear models,” Technometrics, vol. 48, no. 4, pp.
520–529, 2006.

Rong Pan (M’11) received his Ph.D. in industrial engineering from Penn State
University in 2002.
He is an Associate Professor in the School of Computing, Informatics, and
Decision Systems Engineering at Arizona State University. His research interests include failure time data analysis, design of experiments, multivariate statistical quality control, time series analysis, and control.
Prof. Pan is a senior member of ASQ and IIE, and a member of SRE.

Tao Yang received the B.Eng. degree in logistics and systems engineering from
Huazhong University of Science and Technology, the M.S. degree in industrial
engineering from Arizona State University, and the Ph.D. degree in industrial
engineering from Arizona State University in 2013.
His research interests include experimental design, quality, and reliability engineering.

Kanwon Seo received the the B.E. degree in industrial systems engineering
from Hongik University and the M.S. degree in industrial engineering from
Arizona State University. He is currently a Ph.D. student in the School of Computing, Informatics, and Decision Systems Engineering at Arizona State University.
His research interests include statistical design of experiments, reliability engineering, and statistical learning.

Leveraging Wikipedia Concept and Category Information
to Enhance Contextual Advertising
Zongda Wu

Guandong Xu

Rong Pan

Wenzhou University, China

Victoria University, Australia

Aalborg University, Denmark

zongda1983@163.com

guandong.xu@vu.edu.au

rpan@cs.aau.dk

Yanchun Zhang

Zhiwen Hu

Jianfeng Lu

Victoria University, Australia

Wenzhou University, China

Zhejiang Normal University

yanchun.zhang@vu.edu.au sunneyhu@gmail.com
ABSTRACT

important type of Web advertising, contextual advertising
aims to embed the most relevant ads into a page, so as to
increase the number of ad-clicks. Most contextual advertising approaches were based on the keyword matching, which
estimated the ad relevance based on the co-occurrence of the
same keywords between pages and ads [1, 2]. However, as
pointed out in [3, 4], the keyword matching may lead to the
problems such as homonymy and polysemy, low intersection
of keywords, and context mismatch, consequently, degrading
its effectiveness.
To solve these problems, in the area of text classification,
a new approach called Wikipedia matching was proposed in
[5, 6, 7], which uses Wikipedia, the largest knowledge base,
as a reference model to enrich the content representation of
text documents, so as to improve the accuracy of similarity
computation among documents. It was then applied into
contextual advertising in [4], and has been proved its effectiveness on solving the problems encountered in the keyword
matching. However, this approach may lead to the following
problems that decrease its effectiveness in practical contextual advertising. (1) Its limited coverage over semantic concepts. To enhance performance, it only chooses a small part
of articles from Wikipedia as a reference model. As a result,
for many pages not properly characterized by the reference
articles, it is impossible to find out the articles that share the
same topics with the pages, thus, leading to the returning of
irrelevant ads for the pages. (2) Its time-consuming performance. To solve the limited coverage over semantics, a very
straightforward way is to choose a sufficient number of reference articles from Wikipedia. However, this will result in a
seriously decreased performance, because the time spending
of fulltext matching between all the reference articles and
the ads (or pages) is very high.
In order to better balance effectiveness and efficiency, we
in this paper present a new contextual advertising approach
by combining Wikipedia knowledge with the keyword matching, which considers two aspects of similarity between ads
and pages, where the keyword-based similarity captures the
textual commonness, while the Wikipedia-based similarity
measures the relatedness from the semantic perspectives of
concepts and categories. Its process consists of the following three steps. First, we choose a sufficient number of articles from Wikipedia, to cover as many concepts as possible.
Next, we map each ad (as well as each page) into a keyword

As a prevalent type of Web advertising, contextual advertising refers to the placement of the most relevant ads into a
Web page, so as to increase the number of ad-clicks. However, some problems of homonymy and polysemy, low intersection of keywords etc., can lead to the selection of irrelevant
ads for a page. In this paper, we present a new contextual
advertising approach to overcome the problems, which uses
Wikipedia concept and category information to enrich the
content representation of an ad (or a page). First, we map
each ad and page into a keyword vector, a concept vector
and a category vector. Next, we select the relevant ads for
a given page based on a similarity metric that combines the
above three feature vectors together. Last, we evaluate our
approach by using real ads, pages, as well as a great number of concepts and categories of Wikipedia. Experimental
results show that our approach can improve the precision of
ads-selection effectively.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval

General Terms
Algorithms, Experimentation, Theory

1. INTRODUCTION
PwC1 predicts that Web advertising will become the 2nd
largest advertising medium in America after television within the next 4 years, and its spending will increase from 24.2
billion dollars in 2009 to 34.4 billion dollars in 2014. As an
1

lujianfeng@zjnu.cn

PricewaterhouseCoopers - http://www.pwc.com

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
CIKM’11, October 24–28, 2011, Glasgow, Scotland, UK.
Copyright 2011 ACM 978-1-4503-0717-8/11/10 ...$10.00.

2105

vector, a concept vector and a category vector. Last, combining the three feature vectors together, we make the top-N
ads selection. The experimental results show that our approach can improve the accuracy of ads selection effectively.
And, due to avoiding time-consuming fulltext matching between all the reference articles and the pages (or ads), our
approach also obtains a good running performance.

Webpage
page
AAWeb
Textual
ads

A Web page

2. RELATED WORK

Adsvector
vector
Ads
representation
Ads vector
representation
representation

4. Page-Ad text
matching

Page vector
representation

1. Construct
keyword vector

2. Construct
concept vector

3. Construct
category vector

2.1. Search
related concepts

2.2. Expand
related concepts

2.3. Generate
concept vector

A Web page +
Relevant ads

Wikipedia
Thesaurus

Figure 1: Contextual advertising framework

The keyword matching approach estimated the ad relevance by analyzing the co-occurrence of the same keywords
within ads and within pages. One of the recent results on
applying the keyword matching into contextual advertising
was presented in [2], whose main idea was to use a technique called “Impedance Coupling Strategy” to augment a
page with additional keywords from other similar pages, so
as to overcome the problem of low intersection of keywords
between pages and ads. Under the assumption that an ad
can be seen as a noisy translation of a page, it was proposed
in [8] to select the ads that provide the best translation for
a page. In [9], it was proposed to leverage sentiment detection to improve contextual advertising. In [10], the authors
proposed to use lexical graphs created from web corpora as
a means of computing improved content similarity metrics
between ads and pages. However, as pointed out in [4, 5,
6], the main drawback of the keyword matching approach is
that it may lead to the problems of homonymy and polysemy etc., resulting in degrading the relevance of selected ads
to their pages.
For solving the problems of homonymy and polysemy etc.,
the Wikipedia matching was proposed, whose main idea is
to leverage the Wikipedia as an intermediate reference model to enhance the semantic representation of text documents
and thus improve the precision of similarity measure among
documents. In [4], a solution to contextual advertising was
proposed by using the Wikipedia matching. In this solution,
a group of reference articles is first chosen from Wikipedia.
Next, through using the articles as the intermediate reference model on which the ads and the page is re-expressed
as feature vectors, the ads that exhibit more relevance to a
targeted page are determined, and a ranking function to select the most relevant ads to the page is constructed. In [5,
6, 7], a similar method was proposed aiming to textual document clustering. It was proposed in [11] to use Wikipedia
to understand a user’s query intent, without the need to collect large quantities of examples to train an intent classifier.
However, the main drawback of the traditional Wikipedia
matching is due to the problems that we have mentioned in
Section 1 (i.e., the problems of limited coverage of semantic
concepts and time-consuming performance) can dramatically degrade the relevance of selected ads with their pages.

3.1

Constructing Concept Vector

As shown in Figure 1, the process of generating a concept
vector includes the following three steps: (1) search related
concepts appearing in an ad (or a page); (2) add concepts
semantically related to the previous ones; and (3) generate a
concept vector based on the frequency values of the concepts
that are semantically related to the ad. We below introduce
these steps, respectively.
(1) Searching Related Concepts. This step aims to
determine a set of related concepts for an ad (or a page) and
to count their frequency values in the ad. First, we scan a
given ad to find all the Wikipedia titles that appear in the
ad. Such titles are called related titles, and the concepts
associated with them are called related concepts. We search
related titles using a similar method mentioned in [7]. Once
we determine a set of titles related to an ad, actually, we
determine a set of concepts related to the ad. Next, we
compute the frequency of each related concept to the ad. Let
count(t, a) be the frequency of a related title t appearing
in a page a, which is determined above. Let cots(t) be a
set of concepts associated with the title t, i.e., each of which
satisfying that at least one of its titles is identical to t. Then,
the frequency of occurrences of any concept c related to t in
a is computed as follows: if t is not a title of the concept c,
then count(c, t, a) = 0; and otherwise,



count(c, t, a) = 

simk (c, a)

∑
 · count(t, a)
simk (c′ , a)
c′ ∈cots(t)

where simk (c, a) denotes the similarity between a and c
computed using the traditional keyword matching.
In Wikipedia, a concept may have several titles [11], and
thus to count the actual frequency of occurrences of a concept in a given ad, we need to sum up the frequency value of
the concept related to each one of its titles appearing in the
ad. Let titles(a) be all the titles appearing in an ad a (i.e.,
the related titles of a). Then, the frequency of any concept
c appearing in a is computed as follows:
∑
count(c, a) =
count(c, t, a)

3. METHODOLOGY

t∈titles(a)

Figure 1 shows the framework of our approach to improving contextual advertising using the Wikipedia knowledge,
where each ad or page is mapped into a keyword vector, a
concept vector and a category vector, and then the three
vectors are combined together to measure the similarity between a page and an ad. This process is similar to that used
in [6] for clustering, but has a different way for constructing concept vector and category vector. We below introduce
how to construct such two types of feature vectors.

Nc

a
Now, we determine a set C(a) = {count(cj , a)}j=1
, consisting of frequency of each related concept in an ad a, where
Nac is the number of related concepts in a. Similarly, we dec
Np
termine a set C(p) = {count(cj , p)}j=1
for a page p.

(2) Expanding Related Concepts. There are fewer
keywords in an ad a than a generic document, due to its
limited size, so C(a) is of a smaller size, resulting in the

2106

decreased precision of similarity computation between pages
and ads. However, two Wikipedia concepts are hyperlinked
to each other, generally, denoting that the two concepts are
semantically related to each other [11]. So, this step aims
to expand and enrich C(a), based on the hyperlinks within
concepts. Actually, the expansion is a process of breadthfirst graph traversal that starts from the concepts associated
with C(a). First, for each concept c in C(a), we obtain all
the concepts hyperlinking to c, noted as cots(c). Second, we
calculate the frequency value of each concept e in cots(c):



e′ ∈cots(c)

where numk (e, c) denotes the number of hyperlinks between
the concepts e and c.
Third, if count(e, a) is greater a given parameter µ that
is assigned by users to control the depth of graph traversal,
then count(e, a) would be added into C(a). Such a process
is kept on until all the concepts (including the new added
ones) associated with C(a) are traversed. Similarly, we can
expand C(p) for a page p.

c∈cots(d)

∑

d′ ∈cats(d)

Item

Explanation

K
C
D
KC
KD
CD

solely based on keyword vectors
solely based on concept vector
solely based on category vector
based on keyword vector and concept vector
based on keyword vector and category vector
based on concept vector and category vector
based on keyword vector, concept vector and
category vector

categories. Furthermore, based on D(a), we can generate
a category vector for the ad a, which consists of the tf-idf
values of all the categories in D(a). Similarly, we also generate a category vector for a page p. Last, we can compute
the category-based semantic similarity simd (a, p) between
p and a, by using the two category vectors.

3.3

Similarity Computation

Now, each ad (or page) has been represented as three feature vectors: a keyword vector, a concept vector and a category vector. So, when measuring similarity between an ad
and a page, we combine the similarity values calculated using the three feature vectors. For a given page p and an
ad a, the similarity between p and a can be computed as
follows:

In this subsection, by combining concept vector and the
hierarchical relation between concepts and categories or within categories, we describe how to generate a category vector
for an ad (or a page), and leverage it further to enrich the
semantic representation for pages and ads.
Let cots(d) be all the concepts that belong to a category
d, and cats(d) all the immediate subcategories that belong
to d (i.e., there is a hyperlink from each category in cats(d)
to d). cots(d) and cats(d) are determined by the hierarchical categorization system in Wikipedia. Then, we define
the related frequency of any category d appearing in an ad
(or a page) a as follows:
count(c, a)
+
α1

50
27
10,244
260,000
12,000

Table 2: 7 contextual advertising strategies based
on different combinations of feature vectors

3.2 Category Vector Construction

∑

General pages in dataset
Ambiguous pages in dataset
Textual ads in dataset
Wikipedia articles (Wikipedia concepts)
Wikipedia categories

KCD

(3) Generating Concept Vector. After the above step,
C(a) would be expanded with more concepts that are all
semantically related to the ad a, i.e., obtaining a set of frequency values of concepts for a. Similarly to the traditional
keyword matching, based on C(a), we can generate a concept vector for a, which consists of the tf-idf values [12] of
all the concepts in C(a). We also generate a concept vector
for a page p. Last, we compute the concept-based semantic similarity simc (a, p) between p and a, by using the two
concept vectors.

count(d, a) =

Number

Table 1: Dataset characteristics

numk (e, c)

∑
 · count(c, a)
numk (e′ , c)


count(e, a) = 

Item

sim(a, p) = α · simk (a, p) + η · simc (a, p) + ζ · simd (a, p)
where the coefficients α, η and ζ indicate the importance of
concept vector and category vector in measuring the semantic similarity between the page and the ad, which also can
be used to balance the keyword matching and the Wikipedia
matching. And α + η + ζ = 1.

count(d′ , p)
α2

4.

EXPERIMENTS

We evaluated experimentally our approach using a dataset
that contains 50 generic pages, 27 ambiguous pages and 10,
224 ads, more detailed characteristics of which are shown
as Table 1. For each page, we collected human judgment
scores that describe the relevance of ads selected by each of
the candidate strategies (see Table 2). The human judgment

where α1 and α2 are two attenuation coefficients, used to
balance importance of frequency values of categories in different depths.
Nd

a
Now, we determine a set D(a) = {count(dj , a)}j=1
, consisting of frequency of each category related to an ad a,
where Nad is the number of related categories in a. The generation of D(a) for an ad a is also a process of breadth-first
graph traversal starting from the concepts associated with
C(a). However, the traversed graph consists of (1) nodes,
concepts and categories, and (2) edges, the hyperlinks within concepts and categories, as well as the hyperlinks within

Strategy

K

C

KC

KD

CD

KCD

Time (ms)

45

290 765 293

775

758

780

D

Table 3: Running performance for the 7 strategies

2107





710

and ads. We described how to map each ad (or page) into
a keyword vector, a concept vector and a category vector,
as well as how to combine the three feature vectors together
for making the top-N ads selection.
From the experimental results in Section 4, we have the
following conclusions. (1) Our approach obtains a satisfactory running performance (the time spending of ads-selection
for a page is less than 1000 ms). This is due to avoiding to
conduct the time-consuming fulltext matching operation between pages and all the referenced articles, which is used in
the previously published Wikipedia contextual advertising
approach. (2) Our approach obtains a better effectiveness,
i.e., it can well improve the accuracy of ads-selection (the
relevance score of embedded ads to their pages is generally
greater than 0.8). This is due to that we use the Wikipedia knowledge to enrich the semantic representation of pages
and ads, and use them to measure the semantic similarity between pages and ads; while, compared to the surface
text information contained in pages and ads, the semantic
information has a better stability, i.e., it can reflect out the
similarity between pages and ads more accurately.

016
015
014
012

8
9 
 89
7 2 

8


9


89


Figure 2: Average relevance for the ads selected by
the 7 candidate strategies for general pages





710

016
015
014

012

8
9 
 89 8


 

9


Funding: This work is supported by the National Science
Foundation of the Zhejiang Province of China, under grant
Y1100137, the Education Research Program of the Zhejiang
Province of China, under grant Y201016197, the Science and
Technology Program of the Wenzhou City of China, under
grant 2009G0339 and grant S20100055.

89


Figure 3: Average relevance for the ads selected by
the 7 strategies for general or ambiguous pages

6.

REFERENCES

[1] A. Lacerda, M. Cristo, M. G. Andre et al. Learning to
advertise. In SIGIR, 2006.
[2] B. Ribeiro-Neto, M. Cristo, P. B. Golgher et al.
Impedance coupling in content-targeted advertising.
In SIGIR, 2005.
[3] A. Anagnostopoulos, A. Broder, E. Gabrilovich et al.
Just-in-time contextual advertising. In CIKM, 2007.
[4] A. N. Pak, and C.-W. Chung. A Wikipedia matching
approach to contextual advertising. WWWJ, 13:
251-274, 2010.
[5] J. Hu, L. J. Fang, Y. Cao, H.-J. Zeng et al. Enhancing
text clustering by leveraging Wikipedia semantics. In
SIGIR, 2008.
[6] X. H. Hu, X. D. Zhang, C. M. Lu et al. Exploiting
Wikipedia as external knowledge for document
clustering. In SIGKDD, 2009.
[7] P. Wang, J. Hu, H.-J. Zeng et al. Using Wikipedia
knowledge to improve text classification. KAIS, 19:
265-281, 2009.
[8] V. Murdock, M. Ciaramita and V. Plachouras. A
noisy-channel approach to contextual advertising. In
SIGKDD workshops 2007.
[9] T.-K. Fan and C.-H. Chang, “Sentiment-oriented
contextual advertising”. KAIS, 23:321-344, 2010.
[10] S. Papadopoulos, F. Menemenis, Y. Kompatsiaris et
al. Lexical graphs for improved contextual ad
recommendation. In ECIR, 2009.
[11] J. Hu, G. Wang, F. Lochovsky et al. Understanding
user’s query intent with Wikipedia. In WWW, 2009.
[12] H. C. Wu, R. W. P. Luk, K. F. Wong et al.
Interpreting TF-IDF term weights as making
relevance decisions. ACM TOIS 26, 2008.

scores for the relevance of embedded ads to a page were
determined by using a similar method in [5, 6], and were
completed by at least two human assessors in a scale between
0.0 to 1.0.
The first group of experiments aimed to evaluate the execution time of selecting relevant ads for pages. Here, the
work of generating feature vectors for all the ads, has been
completed in advance (i.e., completed offline). In each adsselection for a page, we only concern the execution time consumed by (1) generating the feature vectors for the page, and
(2) computing the similarity between the page and each ad
to choose the most relevant ads. The results are presented in
Table 3. The second group of experiments aimed to evaluate
the relevance scores of embedded ads to their pages. In our
experiments, we invited evaluation assessors to mark score
for each ad based on the relevance of the ad to its page, and
then averaged the relevance scores given by the evaluation
assessors. The results are shown in Figure 2. In the third
group of experiments, we have chosen a special dataset that
consists of 27 ambiguous pages. In the pages, there are many
ambiguous keywords, such as Puma (company versus lion),
Rock (person versus music), Driver (software versus car),
Game (software versus sports), Window (OS versus glass)
and so on. The experimental results of running the seven
candidate strategies over the ambiguous pages are shown in
Figure 3.

5. CONCLUSIONS
In order to improve contextual advertising, we in this paper presented a new approach by incorporating the Wikipedia concept and category information into the traditional keyword matching to enrich the content representation of pages

2108

Prediction of Bike Rental using Model Reuse Strategy
Arun Bala Subramaniyan and Rong Pan
School of Computing, Informatics, Decision Systems Engineering,
Arizona State University, Tempe, USA.
{bsarun, rong.pan}@asu.edu

Abstract This paper describes the methodology used for ECMLPKDD
2015 Discovery Challenge on Model Reuse with Bike Rental Station Data
(MoReBikeS). The challenge was to predict the number of bikes in the new
stations three hours in advance. Initially, the data for the first 25 new
stations (station 201 to 225) was provided and various prediction methods
were utilized on these test stations and the results were updated every week.
Then the full test data for the remaining 50 stations (station 226 to 275) was
given and the prediction was made using the best method obtained from the
small test challenge. Several methods like Ordinary Least Squares, Poisson
Regression, and Zero Inflated Poisson Regression were tried. But reusing
the linear models learnt from the old stations (station 1 to 200) with lowest
mean absolute error proved to be the simple and effective solution.

1 Introduction
Majority of the knowledge intensive application areas have a high chance of
operating context variation. The reuse of the learnt knowledge might play a
critical importance in generalizing the notion of the operating context. In this
ECMLPKDD 2015 Discovery Challenge, the bike rental stations located in
Valencia are considered. The objective is to predict the number of bikes available
in each new stations (Station 201 to 275) three hours in advance. There are at least
two use cases given for such predictions [1]. First, a user plans to rent (or return)
a bike in 3 hour time and wants to choose a bike station which is not empty (or
full). Second, the company wants to avoid situations where a station is empty or
full and therefore needs to move bikes between stations. The data set consisted of
all the necessary details like location, time, weather and profile of bike availability
for model building and prediction.

2 Methodology
In order to make a successful prediction, the information about the current status
in the station, the weather condition and the time period at which the stations
would be empty or full were considered along with the profile of bike availability
in each station which was learnt from the historical information. This is because
the quality of the prediction can be the increased by collecting more historical
information. Considering all the above given information, various methods like
Ordinary Least Squares, Poisson Regression and Zero Inflated Poisson
Regression were tried.

Apart from the above information, the linear models developed for old stations
(station 1 to 200) based on the training dataset and their respective MAE values
were available. After trying out various methods for prediction, the idea of reusing
these models learnt from the old stations (station 1 to 200) to predict the number
of bikes in the new stations (station 201 to 275) provided the best solution based
on the MAE value. The selection of best models for the new stations and
prediction of results is discussed in this section.
2.1 Model Extraction and Prediction
There were 7 base models available and in addition to that, 6 trained models were
provided for each of the 200 old stations. As the deployment data for stations 201
to 275 was given, all the given linear models were utilized for predicting the
number of bikes in each of the stations 201 to 275. The model with less Mean
Absolute Error (MAE) was selected as the best model for a particular station. This
process continued for selecting the best model for all the new stations (201 to
275).
In some cases, the prediction results were negative or it exceeded the maximum
limit of the bikes that can be accommodated in a station. To overcome this
problem, a constraint was added in such a way that whenever the result is negative,
the predicted value is reset to zero and whenever the result exceeded the maximum
limit, the value is reset to the number of docks at that station. So, this helped in
reducing the MAE value further. Also, in some stations, the extraction algorithm
came up with two or more models with the same MAE values. In those cases, only
the first model was selected.
After the extraction algorithm selected the best models for each of the new
stations based on the given criteria, the number of bikes in each station for the
leaderboard data set were predicted using the extracted models. The same set of
constraints were applied to avoid negative values and over fitting during
prediction. The R software was used for model extraction and prediction. The
MAE values for the small test challenge using this strategy was 2.502 and the
MAE values for the full test challenge turned out to be 2.067.

3 Other Methods Tried For Prediction
Initially, before reusing the given linear models, new models were built with the
deployment data for the stations 201 to 275. The different approaches used for
building the models and their results are discussed in this section. The Minitab
and R software is used for this purpose. As the test statistics and graphs for all
stations cannot be included in this paper, a sample station data is chosen for
illustration and understanding. Similar procedures were adopted in building
models for all the other stations.
3.1 Ordinary Least Squares Method
After cleaning the given dataset, the first model was built using all the regressors
under consideration. A thorough analysis of this full model, including residual
analysis and multicollinearity check was done. Also, the scatter plot was used to
study the relationship between the regressor and response variable. From the
model summary, there was severe collinearity problem between the regressors.
Also, the test statistics showed that only few variables significantly contributed to
the model. The scatter plot of those variables is shown in Figure 1. The variable

‘y’ denotes the number of bikes. The variables x20, x21, x23 and x16 denotes
bikes 3 hours ago, full profile bikes, short profile bikes and temperature
respectively. The coefficient of determination value was not satisfactory and the
PRESS (prediction sum of squares) statistic was large, making the model doubtful
for prediction purposes.

Fig.1. Scatter plot for the initial model

The residual plot for the initial full model is shown in Figure 2. The normal
probability plot shows some deviations at the upper and lower tails. This might
be due to the reason of existence of many zeroes in the response variable. The
residual plot (deleted residuals versus the fitted values) shows a significant double
bow pattern, violating the assumption of constant variance. Also, there are some
outliers noticed from the residual versus observation plot.

Fig.2. Residual plot of initial full model

To explore about the outliers, the values of ordinary residuals, studentized
residuals, leverage (HI1), Cook’s distance, DFFIT were collected. Though some
outliers were observed, no influential points were noticed which was confirmed
from the cook’s distance. Since the reason for the unusual observations were not
explicit, these observations were not removed and included for modelling.
In order to identify the regressors that were contributing to the model, the
subset regression was done. The Mallows Cp and R-squared values were used in
determining the best set of regressors. Care was taken to choose less number of
regressors with low Cp and high R-squared value. Also, the stepwise regression,
forward selection, backward elimination techniques were used. The alpha values
for entering and removing the variables were set at 0.1 and 0.2 respectively.
Finally, the regressors that significantly contributed to the model were identified.

After selecting the best subset of regressors, the analysis was carried out once
again. The multicollinearity problem disappeared which was confirmed from the
Variance Inflation Factor (VIF) values (less than 5). The PRESS statistic showed
drastic improvement. Also, the significance of the regressors was examined.
The residual plot for subset regression is shown in Figure 3. Though the model
improved slightly, there is a problem with normality assumption. The residual plot
does not show any improvement as the double bow pattern still exists. This
strongly suggested a need for variance stabilizing transformation of the variables
along with the addition of polynomial and interaction terms for further
improvement.

Fig.3. Residual plot of subset regression

All the possible sets of transformations (from square root to inverse) were tried
on the response and regressor variables. Also, the models with polynomial terms
and interaction terms were built. Finally, the logarithmic transformation of the
regressor variables was tried and regressed against the response. This logarithmic
transformation was a good choice for the model since the data involved historical
information.
The ANOVA table provided all the necessary test statistics. The regressors that
contributed significantly to the model were identified. There was an evidence of
lack of fit for some models but it did not affect much. The PRESS statistic was
low but the R-squared value dropped further. There were no traces of
multicollinearity and the model seemed perfect.
The residual plot for final model is shown in Figure 4. The normal probability
plot still needs some improvement but the variance is much stabilized. There is
no pattern evident from the residual plot.

Fig.4. Residual plot of final model

3.2 Poisson Regression
In order to improve the model further and make it useful for prediction, the
Poisson Regression was tried. The reason for choosing Poisson regression was
that the response variable involved counting the number of bikes, which was
discrete. The log link was particularly attractive for Poisson regression as it
ensured that all of the predicted values of the response variable will be
nonnegative.
The initial full model was fitted with Poisson regression. This model seemed
to be good when compared to the final model built using the ordinary least square
method. There were some regressors which were not significant, noticed after
examining the test statistic and also their regression coefficients were negligible.
The Poisson regression along with the stepwise selection of regressors was
done in order to obtain the best subset of regressors. The final set of regressors
seemed to be almost the same as in case of ordinary least squares method. The
test statistic summary was used to understand the significance of regressors. The
R-squared value improved slightly for this initial model. The Akaike Information
Criteria (AIC) was also high, which denoted the expected entropy of the model
was maximum. The key insight provided by the AIC value is similar to R-squared
adjust and Mallows Cp. The multicollinearity problem was studied from the VIF
values. The standard residuals, studentized residuals, cook’s distance, leverage
values were examined
The Goodness of fit test provided the value of deviance with its significance.
The ratio of deviance to the degree of freedom value was near to unity. The
Pearson chi squared test value was also small with larger p-value indicating that
the fit was significant. Also, the partial deviance test indicated that all the selected
regressors were significant to the model.
The residual plot for the Poisson regression is shown in Figure 5. The upper
tail of the normal probability plot seems to be good but there is some problem
with the lower tail. Also, the assumption of constant variance is violated, observed
from the plot of deleted residuals versus fitted values. There is a nonlinear pattern
observed in this plot indicating a need for transformation and higher order terms.

Fig.5. Residual plot for initial model of Poisson Regression

Various transformations were tried out and the final combination of variables
was found. The natural log link function was used and the logarithmic
transformation of the regressors proved to be good. All the test statistics were
examined once again. Finally a better model when compared to all the previous
models was obtained.
The deviance table provided all the necessary test statistic. There were no
traces of lack of fit. The error values were low and no traces of multicollinearity

was observed from VIF values. The Confidence Interval limits were shrunken,
which was good. The R-squared value was good and the model seemed to be
perfect. The residual plot for the transformed model is shown in Figure 6.

Fig.6. Residual plot after transformation

The upper tail of the normal probability plot is good but there is still a problem
at the lower tail. But the assumption of constant variance is satisfied as observed
from the plot of deleted residuals versus fitted values. The values of the residuals
are distributed within a range of 4 (+2 to -2). There is no pattern observed from
the plot and the model has improved a lot when compared to the previous models.
Only thing that troubled much is the lower tail of the normal probability plot.
The presence of excess zeroes in the response than usual observations could have
resulted in larger residuals in the prediction. The existence of these excess zeroes
also caused trouble in fitting the model. So, in order to overcome this problem,
Zero Inflated Poisson Regression was tried.
3.3 Zero Inflated Poisson Regression
As there were excess of zeroes when examining the response data, there arose a
doubt that some of the zeroes might be inflated. So, in order to solve this problem,
the Zero Inflated Poisson Regression was tried. The glm2, ggplot and pscl
packages were used for zero inflated poisson regression in R software. Finally
two models were generated, one for the count model and other for the inflated
zeroes.
The best subset of regressors were selected and the model was built and
analyzed. The pearson residual was low. The R-squared value was similar to
poisson regression and also prediction sum of squares statistic was small relative
to the other methods. Apart from that, the log likelihood values were large enough
with good significance, indicating that one or more of the regressors in the subset
contributed significantly to the model. There was no evidence of lack of fit and
multicollinearity. The count model seemed to fit the data well. From the zero
inflated model, the various factors that contributed towards inflation of zeroes
were identified.
The normal probability plot (Q-Q plot) and the residual plot was studied. The
normal probability plot improved further when compared to the previous methods.
The residual plot did not have any problem apart from some outliers as shown in
Figure 7.

Fig.7. Deviance Residual plot of initial model

To improve the model further, transformation of the variables was done and
the results of the transformed model was studied. The results obtained after the
transformation and addition of interaction terms improved the model further. All
the test statistics similar to the poisson regression model were checked. The
normal probability plot and the residual plot is shown in Figures 8 and 9
respectively. The Zero Inflated Poisson model had only less number of terms and
found to fit the given data well. The validation of regression models is discussed
in the next section.

Fig.8. Normal probability plot of the transformed model

Fig.9. Deviance Residual plot of the transformed model

4 Validation of Regression Models
After the final model is built, it has to be validated to check whether the model is
adequate for prediction. Model validation is directed towards determining if the
model will function successfully in its intended operating environment.
Initially the new models were built based on the deployment data for the month
of October 2014. As the data for the next month was not available, data splitting
technique was used for validation. But the prediction capability of the model for
November 2014 was still doubtful by this method of validation. Also, the results
of the small test challenge were not satisfactory.
So, the validation approach was modified. As the training dataset for the
stations 1 to 10 were provided, the above mentioned model building approaches
were tried on the training dataset for October 2013 and MAE values were
calculated by predicting the bikes for November 2013. This method of validation
seemed to be a good approach and it revealed some interesting facts. The model
without transformation and addition of interaction terms had low MAE values
when compared to a model with many terms. The model with large number of
terms fitted the given data well but in case of prediction it was overfitting the data.
Also, the leaderboard results of the small test data supported this claim. The MAE
values for the prediction using models from OLS method, Poisson Regression and
Zero Inflated Poisson Regression were 2.724, 3.068 and 2.774 respectively. The
MAE value for the models with transformation and interaction terms was larger
than the baseline value of 3.288. So the models built by transforming the
regressors and adding interaction terms did not work well for predicting the bikes
in this challenge.
Even though these methods worked well, their MAE values were still larger
than the MAE values obtained from reusing the old models, which was 2.502. So,
reusing the models seemed to provide better results as they were obtained from
the training data sets of the stations. So, this method was selected to predict the
number of bikes in the full test data.

5 Results and Discussions
Finally, the idea of reusing the linear models built from the old stations was better
than building new models for the given deployment data. This was obvious
because, the old models were obtained from the training dataset with data
collected over two years, but the deployment data was just for a month.
Though R-squared values increased after transforming the regressors and
including interaction terms, the model was not suitable for prediction, which can
be confirmed from the MAE values of small test challenge. As the number of
terms increased, there was a risk of overfitting. The model with simpler terms
worked well for this challenge. Also, the models built using the training data
predicted the results better than the newly built models with limited data. This
indicated that the models should be robust in order to account for variations in the
data. Even though the models were built for some other stations, they seem to
predict well for new stations than the models built using only the deployment data
of new stations. Also, a good validation approach should be used for choosing the
best models.
One more approach that seemed to work was modelling of error values. This
was carried out in order to reduce further variation in the selected model. This was
done by collecting the error values from fitting each new station data by reusing
the models from old stations. These error values were treated as response variable
and regressed against the new station variables to build a model. Now, the model
selected from the old stations along with the model created from the error values
were combined to form a new model for the station. In addition to this, Lasso
Regression was tried but these methods were not included for predicting the full
test set in this challenge. Also, rounding the values affected the MAE values. In
most cases, the MAE values decreased after rounding the results. But, for some
cases, rounding the values did not have much effect.
Thus, it is understood that the reuse of the learnt knowledge can play a critical
importance in generalizing the notion of the operating context.

References
1. REFRAME (2012 – 2016), ECML/PKDD Discovery Challenge #1, 2015,
“MoReBikeS: Model Reuse with Bike rental Station data”. http://reframed2k.org/Challenge
2. Montgomery, D., Peck, E., & Vining, G. (2012). Introduction to linear
regression analysis (5th ed.). Hoboken, NJ: Wiley.
3. Cameron, A., & Trivedi, P. (1990). Regression-based tests for overdispersion
in the Poisson model. Journal of Econometrics, 46, 347-364.
4. Lambert, D. (1992). Zero-Inflated Poisson Regression, with an Application to
Defects in Manufacturing. Technometrics, 34(1), 1-14.
5. Allison, P., & Waterman, R. (2002). Fixed-Effects Negative Binomial
Regression Models. Sociological Methodology, 32, 247-265.
6. Hans, C. (2009). Bayesian lasso regression. Biometrika, 96(4), 835-845.
7. Zou, G. (2004). A Modified Poisson Regression Approach to Prospective
Studies with Binary Data. American Journal of Epidemiology, 159(7), 702-706.

Computers & Industrial Engineering 58 (2010) 717–728

Contents lists available at ScienceDirect

Computers & Industrial Engineering
journal homepage: www.elsevier.com/locate/caie

Pathological behaviors of ﬁsher conﬁdence bounds for Weibull distribution q
Huairui Guo a, Haitao Liao b,*, Rong Pan c, Alexander Aron a, Adamantios Mettas a
a

ReliaSoft Corporation, ReliaSoft Plaza, 1450 S. Eastside Loop, Tucson, AZ 85710, USA
Department of Nuclear Engineering/Department of Industrial and Information Engineering, University of Tennessee, Knoxville, TN 37996, USA
c
Department of Industrial Engineering, Arizona State University, Tempe, AZ 85287, USA
b

a r t i c l e

i n f o

Article history:
Received 24 March 2009
Received in revised form 30 January 2010
Accepted 1 February 2010
Available online 6 February 2010
Keywords:
Fisher information matrix
Conﬁdence bounds
Maximum likelihood estimates
Weibull distribution

a b s t r a c t
The Weibull distribution is widely used in reliability engineering. To estimate its parameters and associated
reliability indices, the maximum likelihood (ML) approach is often employed, and the associated Fisher
information matrix is used to obtain the conﬁdence bounds on the reliability indices that are of interest.
The estimates and the conﬁdence bounds usually behave similarly in terms of monotonic and asymptotic
properties. However, the conﬁdence bounds may behave differently under certain circumstances. As a
result, the Fisher matrix approach may not always be preferred in obtaining the desired conﬁdence bounds.
This paper provides some properties of Fisher conﬁdence bounds for the Weibull distribution. These properties can be used as guidelines when implementing the ML approach and Fisher information matrix to analyze failure time data and plan life tests.
Ó 2010 Elsevier Ltd. All rights reserved.

1. Introduction
The maximum likelihood (ML) method is a popular statistical
tool for estimating the parameters of a failure time distribution.
It is able to ﬁt various types of data and has many useful statistical
properties such as consistency, asymptotical efﬁciency and transformation-invariance. These properties make it the most widely
used statistical inference method to handle complete failure time
data and Type I, Type II or interval censored data in life data analysis (Meeker & Escobar, 1998).
In addition to obtaining those point estimates of interest, such
as speciﬁc functions associated with the failure time distribution,
conﬁdence bounds are often required in order to quantify the
uncertainty of these estimates. Many statistical approaches have
been developed to achieve such conﬁdence bounds (see Meeker
& Escobar, 1998, 1982). Speciﬁcally, two methods are often utilized
when the ML approach is employed. One method is based on the
likelihood ratio using the v2 distribution, and another uses the
Fisher information matrix and Taylor approximation. Wiel and
Meeker (1990) compare these two methods by conducting extensive simulations. The results show that the likelihood ratio method
may give more accurate bounds when the sample size is relatively
small. However, this method requires many iterations, and a constrained maximum likelihood estimate (MLE) needs to be obtained
in each iteration. It is not uncommon that the bounds may not be
found because the roots of the associated likelihood ratio function
q

This manuscript was processed by Area Editor E.A. Elsayed.
* Corresponding author. Tel.: +1 865 974 0984; fax: +1 865 974 0668.
E-mail address: hliao4@utk.edu (H. Liao).

0360-8352/$ - see front matter Ó 2010 Elsevier Ltd. All rights reserved.
doi:10.1016/j.cie.2010.02.001

do not exist. Cheng and Iles (1983) and Cheng and Iles (1988) propose a method to calculate conﬁdence bounds for the cumulative
distribution function (cdf) for the location-scale distribution family
based on the Fisher information matrix. By assuming that the MLEs
of the distribution parameters asymptotically follow the multivariate normal distribution, a joint range of these estimates is constructed based on the v2 distribution. From this range, the
bounds of the cdf can be calculated. Although the bounds have
been provided to analyze complete failure time data, they cannot
be applied directly to other data types or for other reliability metrics such as conditional reliability and failure rate.
An alternative way of calculating conﬁdence bounds is to use the
Fisher information matrix along with s-normal approximation.
Compared to the likelihood ratio and Cheng and Iles’ methods, this
method can be easily implemented and is not limited to certain data
types. This approach plays an important role in life data analysis and
accelerated life testing (ALT) data analysis. One of its most successful
applications is the optimum design of test plans studied by Nelson
and Kielpinski (1976), Nelson and Meeker (1978), Miller and Nelson
(1983), Meeker (1984), Bai, Kim, and Lee (1989), Khamis and Higgins
(1996), Pascual and Montepiedra (2003), Pascual and Montepiedra
(2005), Yang (2005), and Tang and Xu (2005). Due to its popularity
and ease to use, it has been included in many reliability analysis software packages.
The Fisher bounds are based on the large sample s-normal
approximation. However if this condition is not satisﬁed, the
resulting bounds would not be accurate. The accuracy of these
bounds on parameters and failure time percentiles for the Weibull
distribution has been studied by Wiel and Meeker (1990) via simulation. Analytical analysis of reliability bounds is provided by

718

H. Guo et al. / Computers & Industrial Engineering 58 (2010) 717–728

Nomenclature
ML
MLE
ALT
CL
pdf
cdf
f ðtÞ
FðtÞ
LðÞ
Kf
Ni
ti
KS
N0i

maximum likelihood
maximum likelihood estimation (or estimator)
accelerated life test
conﬁdence level
probability density function
cumulative distribution function
pdf of the Weibull distribution
cdf of the Weibull distribution
likelihood function
number of groups of failure time data
number of failures in the ith failure time group
failure time of the ith failure time group
number of groups of censoring time data
number of censoring times in the ith censoring time
group

Zhao, Pan, Aron, and Mettas (2006) for ALT involving time-varying
stresses.
In many applications of Fisher bounds, it is often found that the
bounds have abnormal behaviors. For instance, the reliability function is a decreasing function with time while its bounds may increase at the beginning and then start decreasing dramatically.
Such pathological behaviors have confused many engineers and
researchers in practice. In this paper, theoretical insights into such
abnormal phenomena are provided. In particular, Fisher conﬁdence
bounds on failure time percentiles, reliability function and failure
rate for the Weibull distribution are investigated. Conditions under
which the Fisher information bounds can be applied are discussed
in detail. Moreover, detailed study is performed to investigate the
effects of MLEs, their asymptotic variances and the assigned conﬁdence levels on the trend of these bounds. The understanding of
these effects is very important, especially for failure time data
analysis and planning reliability tests.

t 0i
KI
N00i
t 00iL
t 00iR
^
b; b
g; g^

RH^

time of the ith censoring time group
number of groups of interval failure time data
number of failures in the ith interval
the start time of the ith interval
the end time of the ith interval
shape parameter of the Weibull distribution and its MLE
scale parameter of the Weibull distribution and its MLE
variance–covariance matrix of MLEs of parameter vector

T
RðTÞ
kðTÞ

percentile of a failure time distribution
reliability function
failure rate of a failure time distribution

H

The MLE of b and g can be found by solving @ K=@b ¼ 0 and
@ K=@ g ¼ 0 simultaneously. This paper studies the cases where solutions for b and g and the Fisher information matrix discussed in Section 2.2 exist. For more discussions on the MLE, readers are referred
to Meeker and Escobar (1998), Nelson (1982) and Elsayed (1996).
2.2. The ﬁsher information matrix and inference
The Fisher information matrix associated with the Weibull distribution is given by:

2

h 2 i
h 2 i3
@ K
E0  @b@
E0  @@bK2
g 7
6
h 2 i 5:
F0 ¼ 4 h 2 i
@ K
@ K
E0  @g@b
E0  @ g2

The subscript 0 indicates that the quantity is evaluated at the true
values of the parameters. When failure data are available, one can
compute the ‘‘local” information matrix by:

2
F¼4

2. Preliminaries
2.1. Model and parameter estimation
The probability density function (pdf) f ðtÞ and cdf FðtÞ of the
Weibull distribution are:

f ðtÞ ¼

b

gb

t

b1 ðgt Þ

ð2:1Þ

;

t b

FðtÞ ¼ 1  eðgÞ ;

ð2:2Þ

where b is the shape parameter and g is the scale parameter. Let H
be the vector ½b; g. The general formulation for the likelihood function, including exact failure time, censoring time and interval censored time data, can be written as:

Lðb; gÞ ¼

Kf
Y

f ðt i ÞNi

i¼1

KS
Y


KI
 N 0 Y
  00 
 N00
1  F t0i i
F tiR  F t 00iL i :

i¼1

ð2:3Þ

i¼1

Taking the natural logarithm of Eq. (2.3) yields the log-likelihood
function:

ln Lðb; gÞ ¼ K ¼

Kf
X

Ni ln f ðt i Þ þ

i¼1

þ

KI
X
i¼1

N 00i

KS
X

3

2

5

@ K
 @b@
g

2

 @@ gK2

K
 @@g@b

:

ð2:6Þ

^ g¼g
^
b¼b;

"

RH^ ¼

^
^ g
^Þ
VarðbÞ
Cov ðb;
^
^Þ
^Þ
Cov ðb; g
Varðg

#

2
¼4

2

31

2

5

2

@ K
 @b@
g

2

 @@gK2

 @@bK2
K
 @@g@b

:

ð2:7Þ

^ g¼g
^
b¼b;

^ ¼ ½b;
^ g
^  has an asymptotically mulIt is well known that the H
tivariate normal distribution with mean H ¼ ½b; g and variance–
covariance matrix RH^ .
^ Þ can be
Let GðHÞ be a function of H. Then, the variance of GðH
approximated by:

^ ÞÞ ¼
VarðGðH


2

2
@GðHÞ
^ þ @GðHÞ Varðg
^Þ
VarðbÞ
@b
@g
@GðHÞ @GðHÞ
^ g
^ Þ:
Cov ðb;
þ2
@b
@g

ð2:8Þ

Using the large sample normal approximation, the approximate
one-sided lower or upper conﬁdence bound on GðHÞ at the conﬁdence level of 1  a is (Meeker & Escobar, 1998):


 
N 0i ln 1  F t0i

i¼1

  
 
ln F t 00iR  F t 00iL :

2

2

 @@bK2

Eq. (2.6) calculates the Fisher information using the estimated
parameters. By taking the inverse of F, the local estimate of the variance–covariance matrix can be obtained as:

b

e

ð2:5Þ

ð2:4Þ

^ Þ  Ka
GðHÞU;L ¼ GðH

rﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
  ﬃ
^ ;
Var G H

ð2:9Þ

719

H. Guo et al. / Computers & Industrial Engineering 58 (2010) 717–728

where K a is the 1  a percentile of the standard normal distribution.
Note that, when one-sided conﬁdence bound (lower or upper) is
utilized, either GðHÞL or GðHÞU will be presented. Similarly, the
two-sided conﬁdence bounds (both will be presented) at the conﬁdence level of 1  a can be expressed as:

^ Þ  K a=2
GðHÞU;L ¼ GðH

rﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
  ﬃ
^ :
Var G H

ð2:10Þ

From Eqs. (2.9) and (2.10), one can see that the only difference between them is the percentile values of the standard normal distribution. In fact, the one-sided lower (upper) conﬁdence bound at the
conﬁdence level of 1  a=2 takes the same value as the two-sided
lower (upper) bound at the conﬁdence level of 1  a. Note that, our
subsequent discussions in the following sections are based on Eq.
(2.9); in particular, the properties of the Fisher bounds on failure
time percentiles, reliability function and failure rate of the Weibull
distribution will be discussed next. Because of the relationship between one-sided and two-sided conﬁdence bounds, all the conclusions in the following sections apply for two-sided bounds.
3. Failure time percentiles, reliability, failure rate and their
inference
3.1. Failure time percentile and its inference
From Eq. (2.2), the log-percentile for a given reliability value can
be obtained as:

u¼

1
lnð ln RÞ þ ln g;
b

ð3:1Þ

where u ¼ ln T and T is the 1  R percentile. According to Section
2.1, the upper and lower bounds on u can be estimated by:

^ þ Ka
uU ¼ u

qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^ Þ;
Varðu

and

^  Ka
uL ¼ u

qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^ Þ;
Varðu

2

^
ln ð ln RÞ
^ þ VarðgÞ  2 lnð ln RÞ Cov ðb;
^ g
^Þ ¼
^ Þ:
Varðu
VarðbÞ
2
4
^
^
g
b
g^ b^2
The approximate Fisher bounds on percentile T are given by:

and T L ¼ euL :

ð3:2Þ

For non-repairable systems, T is a monotonically decreasing function of R. Therefore, the bounds on T also should be monotonically
decreasing. However, since Fisher bounds are based on the assumptions of large sample and s-normal approximation, the behaviors of T
and its bounds may be different if the assumptions are indeed not valid under certain circumstances. The following theorems provide the
mathematical conditions for these circumstances (see A for proofs).
Theorem 3.1. For the Weibull distribution, when R (reliability)
approaches 1, then:

lim T L ¼ 0;
R!1

8
>
< 0; if d < 1;
lim T U ¼ 1; if d > 1;
>
R!1
:
C 1 ; if d ¼ 1;

^ exp
where C 1 ¼ g

 ^2

lim T U ¼ 1;
R!0



^g
^Þ
b Cov ðb;
^
g^ VarðbÞ

and d ¼

Ka

ð3:3Þ

pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
VarðbÞ
.
^
b

When the value of reliability is close to 1, under the normal condition, the corresponding lifetime and its bounds should approach 0.
However, from Theorem 3.1, it can be seen that the upper bounds
may become a constant value or even can go to inﬁnity. This counterintuitive behavior is a special property of the Fisher bounds.

8
>
< 1; if d < 1;
lim T L ¼ 0; if d > 1;
>
R!0
:
C 1 ; if d ¼ 1;

^2 Cov ðb;
^ g
^Þ
b
^ exp
where C 1 ¼ g
^
g^ VarðbÞ

!
and d ¼

ð3:4Þ

Ka

qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
VarðbÞ
.
^
b

^ and b
^
From the above theorems, it can be seen that K a ; VarðbÞ
determine the behaviors of the bounds, while the scale parameter
g is related to the bound’s limiting values only. For the special case
^¼1
where the failure times are exponentially distributed, i.e., b
qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^ ¼ 0, we have K a VarðbÞ
^ =b
^ ¼ 0. According to Theorems
and VarðbÞ
3.1 and 3.2, one can see that limR!0 T L ¼ 1 and limR!1 T U ¼ 0.
From Theorems 3.1 and 3.2, we also know that when
qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^ =b
^ < 1, the behaviors of the percentiles and their bounds
K a VarðbÞ
are the same. Both are monotone functions of R. As a result, Fisher
qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^ =b
^ > 1,
bounds can be used in this case. However, when K a VarðbÞ
there is an inﬂection point at each curve of the two percentile
bounds; after that point, the bounds are no longer monotone. In
the following analysis, we will give the formulas of these inﬂection
points.
qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^ =b
^ > 1, then
Theorem 3.3. For the Weibull distribution, if K a VarðbÞ
T L ðRÞ reaches its maxima at the point R1 and T U ðRÞ reaches its minima
at the point R2 , where:

!!
^ g
^2 Cov ðb;
^Þ
b
½1
þ
R1 ¼ exp  exp
D ;
^
g^ VarðbÞ
!!
^ g
^2 Cov ðb;
^Þ
b
½1  D
R2 ¼ exp  exp
;
^
g^ VarðbÞ
where D ¼

^ is the ML estimate of u and Varðu
^ Þ is:
where u

T U ¼ euU

Theorem 3.2. For the Weibull distribution, when R approaches to 0,
then:

ð3:5Þ

rﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ	rﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
^
K 2a VarðbÞ
VarðbÞVarð
g^ Þ
1
 1.
^2
^g
^Þ
b
Cov 2 ðb;

From the above analysis, we can see that it is important to pay
attention when Fisher bounds can be used to plan either regular life
tests or ALT. Usually, in order to plan a test, baseline values of b and g
are given together with the precision (range of bounds) requirement
of a percentile. However, if the sample size is small, because of the
pathological behavior addressed in the above theorems, the precision will never be reached. If this occurs, either the conﬁdence level
ðK a Þ or the sample size should be adjusted. It also can be seen that b is
more important than g, which should be given more attention.
3.2. Reliability function and its inference
For the Weibull distribution, the reliability at time T is:
u

RðuÞ ¼ ee ;

ð3:6Þ

where u ¼ bðln T  ln gÞ. Therefore, R is a decreasing function of
time T for non-repairable systems. The Fisher bounds of R are:
u

RU ¼ ee L

u

and RL ¼ ee U ;

where uL and uU are the lower and upper bounds of u, which can be
^ is:
found from Section 3.1. Moreover, the variance of u

^Þ ¼
Varðu

^2
^
^2
2u
u
^ þ b Varðg
^ g
^Þ 
^ Þ:
VarðbÞ
Cov ðb;
2
2
^
^
^
g
g
b

Again, it can be seen that the Fisher bounds on R are not always
decreasing with time under certain conditions. The following theorems provide these conditions.

720

H. Guo et al. / Computers & Industrial Engineering 58 (2010) 717–728

Theorem 3.4. For the Weibull distribution, when T approaches 0,
then:

lim RU ¼ 1;
T!0

8
>
< 1; if d < 1;
lim RL ¼ 0; if d > 1;
>
T!0
:
C 2 ; if d ¼ 1;

ð3:7Þ

where uL and uU are the lower and upper bounds of u. The variance
of u is:

^Þ ¼
Varðu

Theorem 3.5. For the Weibull distribution, when T goes to inﬁnity,
then:

lim RL ¼ 0;

T!1

ð3:8Þ

pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ

^3 ^ 
^
K
VarðbÞ
v ðb;g^ Þ
where C 2 ¼ exp  exp b g^Co
.
and d ¼ a b^
^
VarðbÞ
From Theorems 3.4 and 3.5, it can be seen that the limiting values of the bounds jump between 0 and 1. This change depends on
the speciﬁed conﬁdence level 100ð1  a)% and the Fisher information obtained from data. Again, the scale parameter g does not affect the shape of the bounds. The non-monotone properties of the
bounds are caused by the ML method and the Fisher information
matrix. As we know, as more failures are observed, more information about the underlying distribution will be collected, and thus
qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^ =b
^ will ﬁ^ will decrease. The value of K a VarðbÞ
the value of VarðbÞ
nally be less than 1. In this case, no pathological behavior will appear for either the upper or lower bounds of the reliability
function. If the failure times are exponentially distributed, since
there is no uncertainty on b, then according to Theorems 3.4 and
3.5, we have limT!0 RL ¼ 1 and limT!1 RU ¼ 0.
Moreover, the following theorem gives the inﬂection points
where the reliability bound curve changes from decreasing to
increasing.
qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^ =b
^ > 1, then
Theorem 3.6. For the Weibull distribution, if K a VarðbÞ
RL ðTÞ reaches its maxima at the point T 1 , and RU ðTÞ reaches its
minima at the point T 2 , where:

!
^ g
^ v ðb;
^Þ
bCo
^ exp
½1  D ;
T1 ¼ g
^
g^ VarðbÞ
!
^
^
^Þ
bCov ðb; g
^ exp
½1 þ D :
T2 ¼ g
^
g^ VarðbÞ

ð3:9Þ

rﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ	rﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
^
K 2a VarðbÞ
bÞVarð
g^ Þ
where D ¼ Varð
1
 1.
^2
^g
^Þ
b
Cov 2 ðb;

For the Weibull distribution, the failure rate function at time T
is:

g g

¼ eu ;

ð3:10Þ

where u ¼ ln b þ ðb  1Þ ln T  b ln g. The behavior of kðTÞ depends
on the value of b: if b < 1, it is a decreasing function of time; if
b ¼ 1, it has constant values; if b > 1, it is increasing with time.
The approximate Fisher bounds of kðTÞ can be expressed as:

kU ¼ euU

and kL ¼ euL ;

^ þ
VarðbÞ

^2
b
^Þ
Varðg
g^ 2

Theorem 3.7. For the Weibull distribution with an increasing failure
^ > 1, then:
rate, i.e., b
 When T approaches 0:

8
>
< 0; if d1 < 1;
lim kU ¼ 1; if d1 > 1;
>
T!0
:
C 3 ; if d1 ¼ 1:

lim kL ¼ 0;
T!0

ð3:11Þ

ð3:12Þ

 When T goes to inﬁnity:

8
>
< 1; if d1 < 1;
lim kL ¼ 0; if d1 > 1;
>
T!1
:
C 3 ; if d1 ¼ 1;
^

where C 3 ¼ gb^ exp



^
1b
^
b

lim kU ¼ 1;

T!1

ð3:13Þ

pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ

^
^
^ v ðb;
^g
^Þ
K
VarðbÞ
bCo
and d1 ¼ a j1bj
þ ðb1Þ
.
^
^
g^ VarðbÞ

Theorem 3.8. For the Weibull distribution with an decreasing failure
^ < 1, then:
rate, i.e., b
 When T approaches 0:

8
>
< 1; if d1 < 1;
lim kL ¼ 0; if d1 > 1;
>
T!0
:
C 3 ; if d1 ¼ 1;

lim kU ¼ 1:

ð3:14Þ

8
>
< 0; if d1 < 1;
lim kU ¼ 1; if d1 > 1;
T!1
>
:
C 3 ; if d1 ¼ 1;

ð3:15Þ

T!0

 When T goes to inﬁnity:

lim kL ¼ 0;

3.3. Failure rate function and its inference


 b1
b T

!2

Again, one can see that the Fisher bounds on k do not always behave
as k. Both k and its bounds are constant only when b ¼ 1, i.e., when
the Weibull distribution becomes an exponential distribution,. The
limiting values of the failure rate bounds for the Weibull distribution are given in the following theorems.

T!1

From Theorem 3.6, the domain where the Fisher bounds can be
used for the reliability function can be found. For example, the lower bound RL ðTÞ reaches its maxima at time T 1 and it becomes a
monotone decreasing function after this point; as a result, it is
inappropriate to use RL ðTÞ before time T 1 .

kðTÞ ¼

1
^
 ln g
^
b

!
^
2b
1
^ g
^
^ Þ:
ln T þ  ln g Cov ðb;

^
g^
b

pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ

^3 ^ 
^
K a VarðbÞ
v ðb;g^ Þ
where C 2 ¼ exp  exp b g^Co
.
and
d
¼
^
^
VarðbÞ
b

8
>
< 0; if d < 1;
lim RU ¼ 1; if d > 1;
T!1
>
:
C 2 ; if d ¼ 1;

ln T þ

^

where C 3 ¼ gb^ exp



^
1b
^
b

pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ

^
^
^ v ðb;
^g
^Þ
K
VarðbÞ
bCo
þ ðb1Þ
.
and d1 ¼ a j1bj
^
^
g^ VarðbÞ

From Theorems 3.7 and 3.8, it can be seen that the performance
of the failure rate bounds is more complicated than the performance of the percentile and reliability function bounds. Again,
the shape parameter b is more important than the scale parameter
^ decide the shape of the
g. In fact, b^ together with K a and VarðbÞ,
bounds. For example, when d1 > 1 the upper bound of the failure
rate is an increasing function after the inﬂection point and goes
to inﬁnity. In particular, for the exponential distribution, i.e.,
^ is:
kðtÞ ¼ 1=g, the variance of u

^Þ ¼
Varðu


 2
1
^ Þ;
Varðg
g^

which is a constant value with time. Therefore, for a given K a , the
bounds on k are constant as well.
The inﬂection points where the bound starts changing its trend
are given in the following theorems.

721

H. Guo et al. / Computers & Industrial Engineering 58 (2010) 717–728

^ > 1 and K a
Theorem 3.9. For the Weibull distribution, if b
qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^ =ðb
^  1Þ > 1, then function kL ðTÞ has its maxima at point T 1
VarðbÞ
and function kU ðTÞ has its minima at point T 2 , where:

"
#
^ g
^ v ðb;
^Þ
1 bCo
^
ð1 þ D1 Þ ;
T 1 ¼ g exp  þ
^
^
g^ VarðbÞ
b
"
#
^ g
^ v ðb;
^Þ
1 bCo
^
ð1  D1 Þ ;
T 2 ¼ g exp  þ
^
^
b
g^ VarðbÞ

ð3:16Þ

rﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
where D1 ¼

^
^g
^ Þ1
VarðbÞVarð
g^ Þ=Cov 2 ðb;
.
2
^ b1Þ
^
K 2a VarðbÞ=ð
1

^ < 1 and
Theorem 3.10. For the Weibull distribution, if b
qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^ =ð1  bÞ
^ > 1, function kL ðTÞ has its maxima at point T 1
K a VarðbÞ
and function kU ðTÞ has its minima at point T 2 , where:

"
#
^ g
^ v ðb;
^Þ
1 bCo
^
ð1  D1 Þ ;
T 1 ¼ g exp  þ
^
^
b
g^ VarðbÞ
"
#
^ g
^ v ðb;
^Þ
1 bCo
^
ð1 þ D1 Þ ;
T 2 ¼ g exp  þ
^
^
b
g^ VarðbÞ

ð3:17Þ

rﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
where D1 ¼

^
^g
^ Þ1
VarðbÞVarð
g^ Þ=Cov 2 ðb;
.
2
^ b1Þ
^
K 2 VarðbÞ=ð
1
a

3.4. Summary of the properties of Fisher bounds
In summary, according to the theorems given in previous sections, the following conclusions can be drawn:
pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
K
VarðbÞ
< 1, the bounds on reliability and failure time per(1) If a b^
centile have the same monotone property as the reliability
and percentile functions. In this case, Fisher bounds can be
used.
pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
K
VarðbÞ
(2) If a b^
P 1, inﬂection points exist for the bounds. The
bounds on reliability and failure time percentile have the
same monotone property as the reliability and failure time
percentile functions only for a given time or reliability range.
The
ranges
are
affected
by
the
value
of
rﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ rﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
^
K 2a VarðbÞ
VarðbÞVarð
g^ Þ
 1=
 1. Fisher bounds are not appropri^2
^g
^Þ
b
Cov 2 ðb;
ate for these cases.
pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
K
VarðbÞ
(3) If a j1bj
< 1, the bounds on failure rate have the same
^
monotone property as the failure rate function (if b < 1, they
are decreasing with time; if b > 1, they are increasing with
time).
In this
case, Fisher bounds can be used.
pﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ﬃ
K

conﬁdence level. Five failure data points were obtained from a life
test: 1, 44, 50, 60 and 61 hours. The ﬁrst failure seems to be out of the
group. With only ﬁve failures, it is hard to say if the true distribution
is a mixed or single Weibull. Two-population mixed Weibull
distribution has ﬁve parameters. Using only ﬁve failures to ﬁt ﬁve
parameters, unstable results may be obtained. For this data set, no
MLE solution for a two-population mixed Weibull distribution was
found. Since this example is only used to illustrate the theory
proposed in this paper, a single population Weibull model is used
here. With only ﬁve failures to estimate two parameters, large
uncertainty of the estimated parameters is expected and the
abnormal behaviors of the bounds can be discovered easily.
The Weibull distribution is used for this data set. Using the ML
approach, the parameters for the Weibull distribution are esti^ ¼ 1:2351 and g
^ ¼ 45:0246. The plot with two-sided
mated as b
conﬁdence bounds on the reliability function and percentile (time)
is shown in Fig. 1. The conﬁdence level is 99%.
In Fig. 1, the dashed lines are the reliability bounds while the
solid lines are the bounds on time. The calculated variance and
^ and g
^ is:
covariance matrix for b

"

RH^ ¼

^
^ g
^Þ
VarðbÞ
Cov ðb;
^ g
^Þ
^Þ
Cov ðb;
Varðg

#


¼

0:2851

1:8993

1:8993 278:4318


:

For the two-sided conﬁdence bounds with the conﬁdence level
of 99%, K 0:005 ¼ 2:5758. From the theorems in Section 3, we have
pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
K
VarðbÞ
0:2851
d ¼ a b^
¼ 2:5758
¼ 1:1136. According to Theorems 3.1–
1:2351
3.3 and 3.4, time and reliability bounds are not monotone functions since d > 1 . From Eq. (3.9), we know that RL ðTÞ reaches its
maxima at point T 1 ¼ 9:78 and RU ðTÞ reaches its minima at point
T 2 ¼ 298:49. From Eq. (3.5), we know that T L ðRÞ reaches its maxima at point R1 ¼ 3:22e  5 and T U ðRÞ reaches its minima at point
R2 ¼ 0:859. These inﬂection points also can be identiﬁed easily
from Fig. 1.
To see how the bounds change and how the theory in this paper
can be used to predict the change, the conﬁdence level is increased
to 99.9%. From Eqs. (3.5) and (3.9), we know that with the increase
in the conﬁdence level, the domain in which Fisher bounds can be
applied becomes smaller. For the reliability bounds (see Fig. 2), the
time range is T = [23.624; 123.66], and for the time bounds, the
reliability range is R = [0.03; 0.6371].
In contrast, if the conﬁdence level is decreased to 90%, then
d ¼ 0:711. Since d < 1, we can calculate Fisher bounds for any time
and for any reliability value. Fig. 3 shows that the bounds are
monotone curves.

^
VarðbÞ

P 1, inﬂection points exist for the bounds. The
(4) If a j1bj
^
bounds on failure rate have the same monotone property
as the failure rate function only in a given time range. The
ranges
are
affected
by
the
value
of
rﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
^g
^ Þ1
VarðbÞVarð
g^ Þ=Cov 2 ðb;
D1 ¼
. For data satisfying this condition,
2
^ b1Þ
^
K 2 VarðbÞ=ð
1
a

Fisher bounds are not appropriate.
4. Numerical examples
In this section, two examples are provided. One is used to show the
properties of bounds on reliability function and percentiles. Another
is used to illustrate the properties of bounds on failure rate functions.
Example 1. Properties of percentile and reliability bounds
This example is used to illustrate how the conﬁdence bounds for
reliability and percentiles change their behaviors with the change in

Fig. 1. 99% two-sided conﬁdence bounds on reliability and time.

722

H. Guo et al. / Computers & Industrial Engineering 58 (2010) 717–728

Fig. 4. Failure rate with 90% two-sided conﬁdence bounds.

Fig. 2. 99.9% two-sided conﬁdence bounds on reliability and time.

For the failure times given in Table 1, the ML estimates of the
^ ¼ 0:9295 and g
^ ¼ 811:8514. The calcumodel parameters are: b
^ and g
^ is:
lated variance and covariance matrix for b

"

RH^ ¼

^
^ g
^Þ
VarðbÞ
Cov ðb;
^ g
^Þ
^Þ
Cov ðb;
Varðg

#


¼


0:0059 1:9970
:
1:9970 8304

For the two-sided conﬁdence bounds with a conﬁdence level of
90%, K 0:05 ¼ 1:6449. From the theorems in Section 3, we have
pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
K
VarðbÞ
0:0059
d1 ¼ a j1bj
¼ 1:6449
¼ 1:7922. Since d1 > 1, according to The^
0:0705

Fig. 3. 90% two-sided conﬁdence bounds on reliability and time.
Table 1
Failure times for failure rate bounds calculation.
1.46
13.75
137.20
229.02
309.39
373.32
475.20
637.80
767.58
948.05
1054.31
1284.83
1574.86
2000.85
3184.09

1.98
25.79
138.53
233.53
310.53
383.68
507.78
673.82
799.33
973.79
1055.44
1341.35
1680.36
2135.79
3304.92

2.13
28.51
147.17
254.37
329.05
390.13
554.68
688.02
841.37
994.36
1097.19
1354.07
1689.51
2252.29

3.17
46.56
147.84
254.55
335.20
439.71
557.95
688.54
849.86
999.90
1130.88
1380.44
1726.22
2441.73

3.88
54.62
166.80
255.74
344.72
441.26
559.02
731.64
858.26
999.98
1139.31
1387.16
1765.54
2571.87

10.13
107.75
183.04
263.92
347.81
446.66
581.00
744.69
876.76
1010.75
1151.90
1445.69
1820.57
2821.35

13.70
108.18
225.62
273.86
356.29
470.50
594.39
760.17
915.23
1039.14
1238.87
1491.42
1910.09
2868.16

Example 2. Properties of failure rate bounds
This example is used to illustrate the pathological conﬁdence
bounds for the failure rate of the Weibull distribution. 100 failures
are recorded in Table 1. Although the sample size is large, from the
theorems in Section 3, we know that the behavior of the bounds is
^ K a (determined by the conﬁdence
affected by the values of b;
^
level) and VarðbÞ (determined by the sample size and the value of
^ Therefore, pathological behavior still can appear under certain
bÞ.
conditions.

orem 3.8, the failure rate bounds are not monotone functions. Fig. 4
shows the 90% two-sided bounds together with the mean value of
the estimated failure rate. From Eq. (3.17) we know that kL ðTÞ
reaches its maxima at the point of T 1 ¼ 170:0803 and kU ðTÞ reaches
its minima at the point of T 2 ¼ 977:8258. These values also can be
seen from Fig. 4. Therefore, the use of Fisher information matrix for
failure rate bounds should be restricted to the range of
T 2 ½170:0803; 977:8258:
To see how the bound curves change, the conﬁdence level is reduced to 80%. For the two-sided conﬁdence bounds with the conﬁdence level of 80%, we have d1 ¼ 1:397. So the failure rate
bounds are not monotone functions since d1 > 1. However, we
see that the range where the Fisher information matrix can be applied is increased to T 2 ½107:401; 1548:4851. That is, decreasing

Fig. 5. Failure rate with 80% two-sided conﬁdence bounds.

723

H. Guo et al. / Computers & Industrial Engineering 58 (2010) 717–728

the conﬁdence level increases the feasible region for using Fisher
bounds. Fig. 5 shows the failure rate and its 80% two-sided bounds.

5. Simulation study on the relationship of sample size and
conﬁdence level
From the equations for d and d1 in Section 3, it can be seen that
for a given value of b, the sample size will determine the maximum
conﬁdence level for which the Fisher bounds will not be pathological. Monte Carlo simulations have been conducted to study their
relationships. Since g is the scale parameter, which will not affect
the simulation result for b, any value of g can be used in the simulation. For the simulation study, b of 0.5, 1, 1.5, 2 and 2.5 and sample size of 5, 10, 20, 50 and 100 were used. For each combination of
the b value and the sample size, 1000 data sets were generated. For
each data set, the ML estimate of b was calculated. Then the mean
^ were estimated and given
and the standard deviation of the 1000 b
in Tables 2 and 3.
From Tables 2 and 3, it can be seen that the values for the mean
and the variance are proportional to the b values used in the simulation. For example, the mean value for the simulation of b of 1 is twice
the mean value for the simulation of b of 0.5. This is also the case for
pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
VarðbÞ
the standard deviation. Therefore, the ratio of b^ is constant for
pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
K
VarðbÞ
all the b values for a given sample size. By setting a b^
¼ 1, the
corresponding K a can be solved for a given sample size. From K a ,
the corresponding maximum conﬁdence level, depending on
whether it is two-sided or one-sided bounds, can be obtained. The
maximum conﬁdence levels are given in Tables 4 and 5.
From Tables 4 and 5, it can be seen that Fisher bounds are relatively robust for reliability and time. If the sample size is 20, it will
be pretty safe to use Fisher bounds on reliability and failure time
percentile. The chance to get pathological bounds
isﬃ almost 0.
pﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
VarðbÞ

can be calcuSimilarly, using Tables 2 and 3, the ratio of j1bj
^
pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
K a VarðbÞ
lated. By setting j1bj
¼ 1, the corresponding K a can be solved
^

for a given sample size. From K a , the corresponding maximum conﬁdence level, can be obtained. They are given in Tables 6 and 7.
From Tables 6 and 7, it can be seen that Fisher bounds are not robust for failure rate. The maximum conﬁdence level is much smaller
than the corresponding values for reliability and time given in Tables
4 and 5. Even when the sample size is very large, for example 100, if
the true b value is close to 1, Fisher bounds for the failure rate still can
easily show the pathological pattern. In fact, this is predicted by the
theory in this paper. For the failure rate bounds in Tables 6 and 7, the
pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
K
VarðbÞ
¼ 1 by solving
maximum conﬁdence level is obtained from a j1bj
^
^ is more accurately estithe K a value. With the sample size increase, b
mated (closer to 1) which means the denominator is closer to 0. Since
the decreasing of the numerator (standard deviation) is slower than
pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
VarðbÞ
the decreasing of the denominator, the value of j1bj
^ increases. This

seems counterintuitive. However, it is well explained by the theory
in this paper.
The simulation result in the above tables matches what we
illustrated in Example 2 and predicted using the theorems in Section 3. In Example 2, the estimated b is 0.9295, which is close to 1.
Therefore, even for the conﬁdence level of 80% with a sample size
of 100, Fig. 5 shows that the bounds are not monotonic curves.

Table 3
The standard deviation values of b.
Standard deviation

b

0.5
1
1.5
2
2.5

Sample size
5

10

20

50

100

0.4032
0.8064
1.2096
1.6129
2.0135

0.1825
0.3662
0.5494
0.7325
0.9156

0.1034
0.2069
0.3103
0.4138
0.5172

0.0566
0.1131
0.1697
0.2262
0.2828

0.0403
0.0807
0.1210
0.1613
0.2017

Table 4
The maximum conﬁdence level (CL) for one-sided bounds on reliability and time.
Maximum CL (one-sided)

b

0.5
1
1.5
2
2.5

Sample size
5

10

20

50

100

0.9617
0.9611
0.9611
0.9617
0.9619

0.9993
0.9992
0.9993
0.9993
0.9993

1.0000
1.0000
1.0000
1.0000
1.0000

1.0000
1.0000
1.0000
1.0000
1.0000

1.0000
1.0000
1.0000
1.0000
1.0000

Table 5
The maximum conﬁdence level (CL) for two-sided bounds on reliability and time.
Maximum CL (two-sided)

b

0.5
1
1.5
2
2.5

Sample size
5

10

20

50

100

0.9234
0.9221
0.9221
0.9234
0.9238

0.9986
0.9985
0.9986
0.9986
0.9986

1.0000
1.0000
1.0000
1.0000
1.0000

1.0000
1.0000
1.0000
1.0000
1.0000

1.0000
1.0000
1.0000
1.0000
1.0000

Table 6
The maximum conﬁdence level (CL) for one-sided bounds on failure rate.
Maximum CL (one-sided)

b

0.5
1
1.5
2
2.5

Sample size
5

10

20

50

100

0.7610
0.6996
0.8255
0.8751
0.8991

0.9887
0.6687
0.9142
0.9658
0.9819

1.0000
0.6285
0.9761
0.9973
0.9995

1.0000
0.5844
0.9994
1.0000
1.0000

1.0000
0.5714
1.0000
1.0000
1.0000

makes K a decrease and a lower conﬁdence level is expected. This
Table 7
The maximum conﬁdence level (CL) for two-sided bounds on failure rate.

Table 2
The mean values of b.
Mean

b

Maximum CL (two-sided)

Sample size

0.5
1
1.5
2
2.5

5

10

20

50

100

0.7140
1.4219
2.1329
2.8558
3.5698

0.5836
1.1598
1.7509
2.3346
2.9182

0.5381
1.0678
1.6142
2.1522
2.6903

0.5166
1.0241
1.5497
2.0663
2.5828

0.5112
1.0145
1.5217
2.0448
2.5560

b

0.5
1
1.5
2
2.5

Sample size
5

10

20

50

100

0.5219
0.3992
0.6510
0.7501
0.7982

0.9775
0.3374
0.8284
0.9315
0.9638

1.0000
0.2569
0.9522
0.9946
0.9989

1.0000
0.1688
0.9988
1.0000
1.0000

1.0000
0.1428
1.0000
1.0000
1.0000

724

H. Guo et al. / Computers & Industrial Engineering 58 (2010) 717–728

Values given in Tables 4–7 can be used in practice to judge
whether Fisher bounds are appropriate for a given data set. In gen-

^ U;L
lim u
R!1

^ ¼ 1:
So limR!1 u

sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ!
2
^
ln ð ln RÞ
^ þ VarðgÞ  2 lnð ln RÞ Cov ðb;
^ g
^  Ka
^Þ
¼ lim u
VarðbÞ
2
4
^
^
R!1
g
b
g^ b^2
qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ1
0
2
^
^ Cov ðb;
^ g
^2 Varðg
K a VarðbÞ
^
^ Þ 2b
^Þ
^ 2 ln g
ln g
b
A:
^j@1 
¼ lim ju

1þ 2 
þ 2 2
^
^
^
^
^
^
^
^
^
R!1
u
u
u g VarðbÞ ug VarðbÞ
b

eral, when the sample size is above 20, Fisher bounds will not show
the pathological behavior for the reliability and time functions. So it
can safely be used. However, for the failure rate, users must pay
attention to both the b value and the sample size. When b is close
to 1, it is better not to use the Fisher bounds, no matter how large
the sample size is.

2^

ð3:18Þ

^2

^

0

^ j@1 
lim ju

^

T!0

^ j@1 
¼ lim ju

Acknowledgments
The authors would like to thank the editor and two referees for
their insightful comments that greatly improved the content of this
paper. This work was partially supported by the National Science
Foundation under Grants CMMI-0855812 and CMMI-0654417.

qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1

^ 

VarðbÞ
1 A
1þ n
^
2
b

Ka

qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ1
^
^
VarðbÞ
1 K a VarðbÞ
A:
 n
^
^
2
b
b

^ j@1 
¼ lim ju
T!0

For the lower limit of u:

0

^ j@1 
lim uL ¼ lim ju

Ka

T!0

ð3:19Þ

qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ1
^
^
VarðbÞ
1 K a VarðbÞ
A ¼ 1:
 n
^
^
2
b
b

^ U Þ ¼ e1 ¼
So limR!1 T L ¼ exp ðlimR!1 u

 0.
pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
^
bÞ
1 K a VarðbÞ
^ j 1 þ K a Varð
.
For the upper limit of u:limT!0 ju
þ
n
^
^
2
b
b
We have:
(1) If

Ka

pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ

^
VarðbÞ
^
b



pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
^
^ j 1 þ K a ^VarðbÞ þ 12 n K a ^VarðbÞ ¼ 1.
< 1, limT!0 ju
b
b

^ U Þ ¼ e1 ¼ 0.
So limR!1 T U ¼ exp ðlimR!1 u


pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
^
K a VarðbÞ
^ j 1 þ K a ^VarðbÞ þ 12 n
^ U ¼ limT!0 ju
> 1, limT!0 u
(2) If
^
b
b
pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
K a VarðbÞ
Þ ¼ 1.
^
b
^ U Þ ¼ e1 ¼ 1.
So limR!1 T U ¼ exp ðlimR!1 u
pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
K
VarðbÞ
¼ 1:
(3) If a b^

qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ1

 
^
VarðbÞ
1
A ¼ lim ju
^ j@1 þ
^j n
lim ju
^
^
T!0
T!0
2
b
b
!
2
2
^ Cov ðb;
^ g
^ Varðg
^ ln g
^
^ Þ 2b
^Þ
^ 2 ln g
u
b

þ 2 2

¼ lim 
^2
^
^ g
^
^g
^
^ VarðbÞ
^ VarðbÞ
T!0
2 u
u
u
u
0

^þ
¼ ln g

Ka

qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
VarðbÞ

1 Ka
þ n
2

^ g
^ Cov ðb;
^Þ
b
:
^
g^ VarðbÞ

^ exp
^U Þ ¼ g
So limR!1 T U ¼ exp ðlimR!1 u

ð3:20Þ
^

b

g^


^g
^Þ
Cov ðb;
.
^
VarðbÞ

h

Proof of Theorem 3.2. The Proof of Theorem 3.2 is similar to the
Proof of Theorem 3.1. Therefore, it is not repeated here. h

Appendix A
Proof of Theorem 3.1. Consider:

lnð ln RÞ
^¼
^;
þ ln g
u
^
b

0

T!0

pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 þ n  1 þ 12 n when

Ka

T!0

In this paper, reasons for the pathological Fisher bounds for the
Weibull distribution are mathematically explained. Through the
analysis, we ﬁnd that the conﬁdence bounds of a reliability metric
may not always have the same monotone properties as the metric.
The behavior of the bounds is determined by the combination of
the conﬁdence level, the estimated shape parameter and the variance of the estimated shape parameters. Among these three factors,
the latter two are affected by the sample size. Traditionally, it is believed that Fisher bounds can be safely used when sample size is
large enough. However, from Example 2 and the simulation results
given in this paper, we can see that this is not always true. Depending
on the combination of the model parameter, conﬁdence level and the
variance of the model parameter, pathological bounds still can appear even when the sample size is relatively large. The theory in this
paper provides the foundations for determining when the Fisher
bounds can be applied. Applying this theory, the change points of
the bounds and the limiting values of the bounds can be predicted
easily. Simulations were conducted to further study the conditions
when Fisher bounds can safely be applied. Tables for the relationship
between the sample size and the maximum conﬁdence level for the
conﬁdence bounds are provided. These tables can be used to check
whether Fisher bounds are appropriate for a given b value, a given
sample size and a certain conﬁdence level.
In this paper, although we use the Weibull distribution for discussion, similar discussion can be conducted on other distributions, such as the lognormal and Gumbel distributions. It can be
shown that similar results will also appear for these distributions.

¼ n, then

1
^ pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
VarðbÞ
1 þ nA
^
b

Ka

0
6. Conclusions

^g
^ Cov ðb;
^Þ
^
VarðbÞ

gÞ
2b
Let lnu^2g  2 lnu^ g þ u^2bg^ 2 Varð
^ u
^g
^
VarðbÞ
n  0.
Therefore: qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ

uU;L

qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^ Þ:
^  K a Varðu
¼u

Proof of Theorem 3.3. Consider:

^  Ka
uU;L ¼ u

qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^Þ
Varðu

725

H. Guo et al. / Computers & Industrial Engineering 58 (2010) 717–728

or:

uU;L ¼

lnð ln RÞ
^  Ka
þ ln g
^
b
sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2
^
ln ð ln RÞ
^ þ VarðgÞ  2 lnð ln RÞ Cov ðb;
^ g
^ Þ:

VarðbÞ
^4
g^ 2
b
g^ b^2

Taking the derivative with respect to R yields:

0

1


^g
^Þ
lnð ln RÞ
Cov ðb;
^
^
VarðbÞ  g^b^2
bK a
C
^4
@uU
1 B
b
B1 þ rﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ﬃC
¼
@
A
^
2
@R
^g
bR ln R
^Þ
ln ð ln RÞ
^ þ Varð2g^ Þ  2 lnð ln RÞCov ðb;
VarðbÞ
^4
g^
g^ b^2
b

rﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ	rﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
^
K 2a VarðbÞ
VarðbÞVarð
g^ Þ
1
 1.
^2
^g
^Þ
b
Cov 2 ðb;
h
^2
i
^g
^Þ
b Cov ðb;
For T U ; R P exp  exp g^ VarðbÞ
, and for T L ; R 6 exp ½ exp
^
^2

^
^
b Cov ðb;gÞ
, so Theorem 3.3 holds. h
^
g^ VarðbÞ
where D ¼

pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n
^
K
VarðbÞ
< 1,
Remark. From the above proof, we can see that when a b^
U
– 0. Instead, it can only be either positive or negthe derivative @u
@R
ative. As a result, the time bounds are monotonic function of R.
Proof of Theorem 3.4. Consider:

^
^ ¼ bðln
^ Þ;
u
T  ln g

Set it to 0, we have:

sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2
^
^ þ VarðgÞ  2 lnð ln RÞ Cov ðb;
^ g
^ ln ð ln RÞ VarðbÞ
^Þ
b
^4
g^ 2
b
g^ b^2
!
^ g
^ Þ lnð ln RÞ
Cov ðb;
^ :
¼ K a

VarðbÞ
^2
g^
b

^  Ka
uU;L ¼ u

qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^ Þ:
Varðu

^ ¼ 1.
So limT!0 u
^ U;L
lim u
T!0

ð3:21Þ

The right hand side of the above equation should always be positive,
so we have:

"

!#
^ g
^2 Cov ðb;
^Þ
b
; for T U ;
R P exp  exp
^
g^ VarðbÞ
"
!#
^ g
^2 Cov ðb;
^Þ
b
; for T L :
R 6 exp  exp
^
g^ VarðbÞ

sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ!
^2
^2
^
u
2u
^ þ b Varðg
^ g
^  Ka
^Þ 
^Þ
¼ lim u
VarðbÞ
Cov ðb;
2
^
^2
T!0
g
g^
b
q
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
0
sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ1
^
^2 Cov ðb;
^ g
^4 Varðg
K a VarðbÞ
^ Þ 2b
^Þ
b
@
A:
^
¼ lim juj 1 

1þ 2 2
^
^
^ g
^g
^
^ VarðbÞ
^ VarðbÞ
T!0
u
u
b
ð3:22Þ

^4 Varðg
^Þ
b
^
^2 g
^ 2 VarðbÞ
u

Let



^g
^2 Cov ðb;
^Þ
2b
^
^g
^ VarðbÞ
u

¼ n, then:

qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
^ pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
K a VarðbÞ
@
^ j 1 
1 þ nA
lim ju
^
T!0
b
0

0

Let lnð ln RÞ ¼ x, then:

"
#
"
#
^
^
^ g
^
^ g
^ ÞVarðbÞ
^Þ
VarðbÞ
K 2a Var 2 ðbÞ
K 2a Cov ðb;
Cov ðb;
2
x þ2
x


^2
^4
^2 g
g^
^
b
b
b
"
#
^ g
^2 Varðg
^ Þ K 2a Cov 2 ðb;
^Þ
b
þ
¼ 0:

g^ 2
g^ 2

^ j@1 
¼ lim ju

Ka

qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1

^ 

VarðbÞ
1 A
1þ n
^
2
b

Ka

qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ1
^
^
VarðbÞ
1 K a VarðbÞ
A:
 n
^
^
2
b
b

T!0

0
^ j@1 
¼ lim ju
T!0

ð3:23Þ

For the lower limit of u:
For the above quadratic function, the solution is:

x1;2

0

pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2
b  b  4ac
¼
2a
^

2

K a Var
bÞ
where a ¼ Varð
^2 
^4
b
b

^ j@1 
lim uL ¼ lim ju
T!0

2

^
ðbÞ

h 2 ^
i
^
^^
^2
^
g^ Þ
; b ¼ 2 K a Cov ðb^b;2gg^ÞVarðbÞ  Covg^ðb;gÞ ; c ¼ b Varð

g^ 2

^g
^Þ
K 2a Cov 2 ðb;
.
g^ 2

In order to have two different real-valued solutions (one is for
2
the upper bound and the other is for the lower bound), b  4ac
should be positive.

^ g
^
^ Þ K 2a VarðbÞ
Cov 2 ðb;
b  4ac ¼ 4
1
2
^
g^ 2
b
2

!

!
^
VarðbÞVarð
g^ Þ
 1 > 0:
^ g
^Þ
Cov 2 ðb;

^ and g
^ are not perfectly correlated,
Since b
Therefore,
2

^
K a VarðbÞ
 1 > 0:
2
^
b

^
VarðbÞVarð
g^ Þ
^g
^Þ
Cov 2 ðb;

 1 > 0.

^
K a VarðbÞ
> 1 or
2
^
b

^ U ÞÞ ¼ e1 ¼ 0.
So limT!0 RL ¼ exp ð exp ðlimT!0 u


pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
^
^
K
VarðbÞ
^ j 1 þ K a ^VarðbÞ þ 12 n K a ^VarðbÞ ¼ 1.
< 1, limT!0 ju
(2) If a b^
b
b
^ U ÞÞ ¼ e0 ¼ 1.
So limT!0 RL ¼ exp ð exp ðlimT!0 u
pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
K
VarðbÞ
¼ 1:
(3) If a b^

0
K na

qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
VarðbÞ
> 1:
^
b

Solving the above quadratic equation yields:

R1;2 ¼ exp  exp

!!
^ g
^2 Cov ðb;
^Þ
b
½1  D
:
^
g^ VarðbÞ

qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ1
^
^
VarðbÞ
1 K a VarðbÞ
A ¼ 1:
 n
^
^
2
b
b

^ L ÞÞ ¼ e0 ¼ 1.
So limT!0 RU ¼ exp ð exp ðlimT!0 u
^j
upper
limit
of
u:limT!0 uU ¼ limT!0 ju

 For pthe
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
^
K a VarðbÞ
1 K a VarðbÞ
, we have:
1 þ
þ 2n
^
^
b
b


pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
^
K a VarðbÞ
^ j 1 þ K a ^VarðbÞ þ
^ U ¼ limT!0 ju
> 1,
limT!0 u
(1) If
^
b
b
pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
1 K a VarðbÞ
n
Þ ¼ 1.
^
2
b

This requires:
2

T!0

Ka

^j@1 þ
lim ju
T!0

Ka

qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ1
^
^
VarðbÞ
VarðbÞ
K
a
1
A
þ n
^
^
2
b
b

!

 
^2 Cov ðb;
^ g
^4 Varðg
^ Þ 2b
^Þ
^ b
1
u

n ¼ lim 
^
^
^2 g
^ 2 VarðbÞ
^g
^ VarðbÞ
T!0
T!0
2
2 u
u
2
^ g
^ Cov ðb;
^Þ
b
¼
:
^
g^ VarðbÞ
^j
¼ lim ju

726

H. Guo et al. / Computers & Industrial Engineering 58 (2010) 717–728

^ U ÞÞ ¼ exp
So limT!0 RL ¼ exp ð exp ðlimT!0 u

 ^2

^g
^Þ
b Cov ðb;
^
g^ VarðbÞ


.

Proof of Theorem 3.7.

h

Proof of Theorem 3.5. The Proof of Theorem 3.5 is similar to the
Proof of Theorem 3.4. Therefore, it is not repeated here. h

Part (a) Consider:

^ þ ðb
^  1Þ ln T  b
^ ln g
^ ¼ ln b
^;
u

Proof of Theorem 3.6. Consider:

qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^ Þ or uU;L
Varðu
sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^2
^
^
^2
u
^ þ b VarðgÞ  2u Cov ðb;
^ g
^  Ka
^ Þ:
¼u
VarðbÞ
2
2
^
^
^
g
g
b

^  Ka
uU;L ¼ u

qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^ Þ:
Varðu

^ ¼ 1.
So limT!0 u
^ b
^ ln g
^
^
q
bþ
Let ln T ¼ ulnb1
¼ b1
^
^ . The limits of uU;L can be obtained as:

^  Ka
uU;L ¼ u

0

^ U;L
lim u
T!0

1
vﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
!2
!
u
u
^
^2
1
2
b
1
b
B^
t
^ þ Varðg
^ g
^Þ 
^ ÞC
^ VarðbÞ
^ Cov ðb;
¼ lim @u  K a
ln T þ  ln g
ln T þ  ln g
A
^
^
T!0
g^ 2
g^
b
b
sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ!
^1þb
^qÞ2
^2
^1þb
^q
pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðb
b
^ þ b Varðg
^ g
^j 1  K a
^ jð1  K a 1 þ nÞ;
^Þ  2
^ Þ ¼ lim ju
¼ lim ju
VarðbÞ
Cov ðb;
2
2
2
2
^  1Þ u2
^2 ðb
T!0
T!0
g^ u
g^ ðb^  1Þu
b

Taking the derivative with respect to T and set it to 0,
have:

@uU;L
@T

¼ 0, we

^Þ
Varðg
^

^

^^

q Cov ðb;gÞ
þ 2ðb1Þ
^ 2
^ . Then

For the lower limit of u:

0

^ j@1 
lim uL ¼ lim ju
T!0

Ka

Ka

qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
VarðbÞ
^
b

1 Ka
 n
2

qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ1
^
VarðbÞ
A:
^
b

ð3:26Þ

qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ1
^
^
VarðbÞ
1 K a VarðbÞ
A ¼ 1:
 n
^
^
2
b
b

^ U Þ ¼ e1 ¼ 1.
So limT!0 kU ¼ exp ðlimT!0 u


pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
^
^
K
VarðbÞ
^ j 1 þ K a ^VarðbÞ þ 12 n K a ^VarðbÞ ¼ 1.
< 1, limT!0 ju
(2) If a b^
b
b

"
!#


^
^ b
^2 ln g
^ þ b Cov ðb;
^ g
^Þ x
^ VarðbÞ
þ 2 K 2a VarðbÞ
g^

^ U Þ ¼ e1 ¼ 0.
So limT!0 kU ¼ exp ðlimT!0 u
pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
K
VarðbÞ
¼ 1:
(3) If a b^

"

#
^3 ln g
^4
^
2b
b
2
^
^
^
^ Þ VarðbÞ þ Varðg
^Þ þ
^Þ
þ ðb ln g
Cov ðb; g
g^ 2
g^

0

#2
¼ 0:

ð3:24Þ

Solving the above equation yields:

!
^ g
^ v ðb;
^Þ
bCo
^ exp
½1  D ;
¼g
^
g^ VarðbÞ

rﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ	rﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
h^ ^ i
^
^
K 2a VarðbÞ
VarðbÞVarð
g^ Þ
v ðb;g^ Þ
^ exp bCo
,
1
 1.For RU ; T P g
^
^2
^g
^Þ
g^ VarðbÞ
b
Cov 2 ðb;
h^ ^ i
v ðb;g^ Þ
^ exp bCo
, so Theorem 3.6 holds. h
and for RL ; T 6 g
^
g^ VarðbÞ
where D ¼

b1
^

^ L Þ ¼ e1 ¼ 0.
So limT!0 kL ¼ exp ðlimT!0 u
^j
For
the
upper
limit
of
u:limT!0 uU ¼ limT!0 ju


pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
^
K a VarðbÞ
1 K a VarðbÞ
, we have:
1 þ
þ 2n
^
^
b
b


pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
^
K a VarðbÞ
^ j 1 þ K a ^VarðbÞ þ
^ U ¼ limT!0 ju
(1) If pﬃﬃﬃﬃﬃﬃﬃﬃﬃ
> 1,
limT!0 u
^ﬃ
b
b
^
1 K a VarðbÞ
n
Þ ¼ 1.
^
2
b

i
^ b
^2  K 2 Var 2 ðbÞ
^ x2
VarðbÞ
a

T 1;2

^ 2

bu
bu
VarðbÞqﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
VarðbÞ
qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
0
1

^ pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^ 

K a VarðbÞ
K a VarðbÞ
1
^ j@1 
^ j@1 
1 þ nA ¼ lim ju
1þ n A
lim ju
^
^
T!0
T!0
2
b
b

T!0

Let ln T ¼ x, then:

 Ka

^

T!0

#
^ g
^ v ðb;
^Þ
bCo
^ exp
; for RL ;
T 6g
^
g^ VarðbÞ
"
#
^ g
^ v ðb;
^Þ
bCo
^ exp
; for RU :
T Pg
^
g^ VarðbÞ

^ ^
^
^ þ bCov ðb; gÞ
^ VarðbÞ
ln g
^
g

^

^ j@1 
¼ lim ju

"

"

2

0

The right hand side of the above equation should always be nonnegative, so:

2

^

0

sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^2
^
^2
^
u
^ þ b VarðgÞ  2u Cov ðb;
^ g
^
^Þ
VarðbÞ
b
2
2
^
^
^
g
g
b
!
^
b
^
^
^ ÞVarðbÞ  Cov ðb; g
^Þ :
¼ K a ðln T  ln g
g^

h

^

gÞ
g
where n ¼ ðln bln
 2 ln bln
þ
u
u2

ð3:25Þ

^ j@1 þ
lim ju
T!0

Ka

qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
VarðbÞ
^
b

1 Ka
þ n
2

qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ1
^
VarðbÞ
A
^
b

0
2

^
^
^  ln g
^
^ B ln b  ln g
1
ln b
u
^
¼ lim juj n ¼ lim  @
2
T!0
T!0
2
u2
u
2



^1
b
þ
^
bu

!2

1
^  1Þq Cov ðb;
^g
^ Þ 2ðb
^Þ
Varðg
A
þ
^
^
^ 2
VarðbÞ
VarðbÞ
bu

^ 1b
^
^  1Þb
^ Cov ðb;
^g
^Þ
b
ðb
:
¼ ln þ
þ
^
^
g^
g^
VarðbÞ
b

ð3:27Þ

727

H. Guo et al. / Computers & Industrial Engineering 58 (2010) 717–728

^U Þ
So limT!0 kU ¼ exp ðlimT!0 u

Solving the above equation yields:

!
!
^
^
^
^  1Þb
^ Cov ðb;
^ g
^
^  1Þb
^ Cov ðb;
^ g
^Þ
^Þ
b
b
1b
ðb
1b
ðb
þ
¼ exp
þ
:
¼ exp ln þ
^
^
^
g^
g^
g^
g^
b
b
VarðbÞ
VarðbÞ

Part (b) The proof is similar to the proof of part (a). Therefore, it
is not repeated here. h

"
#
^ g
^ v ðb;
^Þ
1 bCo
^ exp  
ð1  D1 Þ ;
T 1;2 ¼ g
^
^
b
g^ VarðbÞ
rﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^
^g
^ Þ1
VarðbÞVarð
g^ Þ=Cov 2 ðb;
.
2
^ b1Þ
^
K a2 VarðbÞ=ð
h^ 1 ^
i
^Þ
bCov ðb;g
1
^ exp
and
For
kU ; T P g
^
^ b
g^ VarðbÞ
h^
i
^g
^Þ
bCov ðb;
1
,
so
Theorem
3.9
holds.
h

^
^
b
g^ VarðbÞ
where D1 ¼

Proof of Theorem 3.8. The Proof of Theorem 3.8 is similar to the
Proof of Theorem 3.7. Therefore, it is not repeated here. h

for

^ exp
kL ; T 6 g

Proof of Theorem 3.9. Consider:

^  Ka
uU;L ¼ u

qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^ Þ;
Varðu

Proof of Theorem 3.10. The Proof of Theorem 3.10 is similar to the
Proof of Theorem 3.9. Therefore, it is not repeated here. h

^ þ ðb
^  1Þ ln T  b
^ ln g
^
uU;L ¼ ln b
vﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
!
!
u
2
u
^
^2
1
2b
1
b
t
^
^ g
^ VarðbÞ þ 2 Varðg
^ Cov ðb;
^Þ 
^ Þ:
 Ka
ln T þ  ln g
ln T þ  ln g
^
^
g^
g^
b
b

or:

References

Taking the derivative with respect to T and setting it to 0,

@uU;L
@T

¼ 0,

Bai, D. S., Kim, M. S., & Lee, S. H. (1989). Optimum simple step-stress accelerated life
tests with censoring. IEEE Transactions on Reliability, 32, 528–532.

vﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2
3ﬃ
u
!
!2
!
"
#
u
2
^
^
^
1
1
b
^  b Cov ðb;
^  1Þu
^ 2 T þ 2 1  ln g
^ g
^ þ b Varðg
^ g
^ Þ ln T þ 4
^Þ  2
^ Þ5
^ VarðbÞ
^ VarðbÞ
^
tVarðbÞln
ðb
Cov ðb;
 ln g
 ln g
^
^
^
g^
g^ 2
g^
b
b
b
¼ K a

!
!
^
1
b
^
^
^
^
^
 ln g VarðbÞ  Cov ðb; gÞ :
VarðbÞ ln T þ
^
g^
b

we have:
Since the right hand side of the above equation should always be
non-negative, we have:

^ exp
TPg

^ exp
T6g

"
#
^ g
^ v ðb;
^Þ 1
bCo
 ;
^
^
b
g^ VarðbÞ

"
^ g
^ v ðb;
^Þ
bCo

#
1
 ;
^
^
b
g^ VarðbÞ

for kL ;

for kU :

Let ln T ¼ x, then:
"

#
!2
^g
^ Cov ðb;
^Þ
1
1
b
^
^
 ln g
xþ
 ln g
^
^
^
g^ VarðbÞ
b
b
!
^
K 2a VarðbÞ
^ ^
^
^
^g
^ Cov ðb;
^2
2 Cov ðb; gÞ  VarðbÞVarðgÞ
^Þ 1
^
b
b
ðb1Þ
^
 2 ^

2
¼ 0:
 ln g þ
K a VarðbÞ
^ g
^
^
g^ VarðbÞ
^2
^
VarðbÞ
b
2  1 VarðbÞ
^
ðb1Þ

x2 þ 2

ð3:28Þ

Cheng, R. C. H., & Iles, T. C. (1983). Conﬁdence bands for cumulative distribution
functions of continuous random variables. Technometrics, 25, 77–86.
Cheng, R. C. H., & Iles, T. C. (1988). One-sided conﬁdence bands for cumulative
distribution functions. Technometrics, 30, 155–159.
Elsayed, A. (1996). Reliability engineering. Addison Wesley Longman, Inc.
Khamis, I. H., & Higgins, J. J. (1996). Optimum 3-step step-stress tests. IEEE
Transactions on Reliability, 45, 341–345.
Meeker, W. Q. (1984). A comparison of accelerated life test plans for Weibull and
lognormal distributions and type I censoring. Technometrics, 26, 157–171.
Meeker, W. Q., & Escobar, L. A. (1998). Statistical method for reliability data. John
Wiley & Sons, Inc.
Miller, R., & Nelson, W. (1983). Optimum simple step-stress plans for accelerated
life testing. IEEE Transactions on Reliability, 32, 59–65.
Nelson, W. (1982). Applied life data analysis. John Wiley & Sons, Inc.
Nelson, W., & Kielpinski, T. J. (1976). Theory for optimum censored accelerated life
tests for normal and lognormal life distributions. Technometrics, 18, 105–114.
Nelson, W., & Meeker, W. Q. (1978). Theory for optimum accelerated censored life
tests for Weibull and extreme value distributions. Technometrics, 20, 171–
177.
Pascual, F. G., & Montepiedra, G. (2003). Model-robust test plans with applications
in accelerated life testing. Technometrics, 45, 47–57.
Pascual, F. G., & Montepiedra, G. (2005). Lognormal and Weibull accelerated life test
plans under distribution misspeciﬁcation. IEEE Transactions on Reliability, 54,
43–52.
Tang, L. C., & Xu, K. (2005). A multiple objective framework for planning accelerated
life tests. IEEE Transactions on Reliability, 54, 58–63.

728

H. Guo et al. / Computers & Industrial Engineering 58 (2010) 717–728

Wiel, S. V., & Meeker, W. Q. (1990). Accuracy of approximate conﬁdence bounds
using censored Weibull regression data from accelerated life tests. IEEE
Transactions on Reliability, 39, 346–351.
Yang, G. (2005). Accelerated life tests at higher usage rates. IEEE Transactions on
Reliability, 54, 53–57.

Zhao, W., Pan, R., Aron, A., & Mettas, A. (2006). Some properties of conﬁdence
bounds on reliability estimation for parts under varying stresses. IEEE
Transactions on Reliability, 55, 7–17.

Correlated Tag Learning in Topic Model

∗

Shuangyin Li∗ , Rong Pan† , Yu Zhang∗ and Qiang Yang∗
Department of Computer Science and Engineering, Hong Kong University of Science and Technology, China
{shuangyinli, zhangyu, qyang}@cse.ust.hk
†
School of Data and Computer Science, Sun Yat-sen University, Guang Zhou, China. panr@sysu.edu.cn

Abstract
It is natural to expect that the documents
in a corpus will be correlated, and these correlations are reflected by not only the words
but also the observed tags in each document.
Most previous works model this type of corpus, which are called the semi-structured
corpus, without considering the correlations
among the tags. In this work, we develop
a Correlated Tag Learning (CTL) model for
semi-structured corpora based on the topic
model to enable the construction of the correlation graph among tags via a logistic normal
participation process. For the inference of
the CTL model, we devise a variational inference algorithm to approximate the posterior.
In experiments, we visualize the tag correlation graph generated by the CTL model on
the DBLP corpus and for the tasks of document retrieval and classification, the correlation graph among tags is helpful to improve the generalization performance compared with the state-of-the-art baselines.

1

INTRODUCTION

Documents are usually composed of a group of words
with different word frequencies, leading to the ‘bagof-words’ representation. Besides, it is natural to expect that documents in a corpus are highly correlated
with each other. This implicit relationship among documents may be embodied in the semantic meanings
of the words in each document, where we can use
topic models or other related methods to learn the
correlations. However, most of the documents contain not only unstructured contexts (e.g., the plain
text) but also metadata (e.g., tags). The metadata
usually consists of several tags, such as authors in an
article, keywords for a web page, and categories for

a product. To model this type of documents which
are called semi-structured documents, the metadata
information would play an important role in organizing, understanding, and summarizing them in many
applications.
Obviously, the tags in a corpus come from a compacted
space, taking higher-level semantic as one type of semantic abstraction than words. Thus, the tags should
be highly correlated with each other, which is consistent with documents’ correlations. That is, the correlation between two documents can be reflected via
their tags. Thus, modeling the correlations among
tags can benefit the learning of the relations among
documents and help obtain more meaningful representations for documents, which can be helpful for the
consequent tasks such as document classification and
retrieval. On the other hand, to model the documents,
only considering the word information is obviously not
enough if the tag information is available. Meanwhile,
ignoring the correlations among tags is deficient, because the correlations can help understand the documents in a better way. Hence, how to model the
correlations among tags together with the words is interesting and important for document modeling.
In fact, tags can be treated as high-level ‘topics’ in a
corpus. While differently, the observed tags would be
very complicated and high-dimensional, and belong to
a different semantic space, compared with latent topics discovered by topic models. Thus, there should be
a connection between the observed tags and the latent
topics, such as a distribution over topics for each tag.
In previous works such as the tag-weighted topic model
[15], the author topic model [18], and the labeled-LDA
[21], almost all of them define continuous distributions
for the observed tags over the latent topics. Each tag
is defined as a vector sampled from a certain probability distribution such as a Dirichlet distribution in
[15], where the vector for a tag indicates the distribution over all the latent topics. In this way, the
observed tags and the latent topics are combined to-

gether. However, under the Dirichlet distribution, the
tags are modeled to be independent, which ignores the
correlations among the tags.
On the other hand, we may model the correlations
among the tags only using the co-occurrence of the
tags. However, there are two main limitations in this
approach. Firstly, it ignores the importance of different tags in a specific document, where some tags
are more relevant to a document than others but in
another document the situation can be totally different. Secondly, as described above, the tags are a set
of semantic topic distributions, which are learned from
plain text, and so the correlations should be modeled
from the semantic level, while only considering the cooccurrences is not enough.
In this paper, we propose a novel CTL model based
on the topic model to learn the correlations among
the tags. In the CTL model, participation vectors of
the observed tags, which take advantage of both the
text information and the tags, for a semi-structured
corpus are used to learn the correlations. For inference, an effective inference method is devised to learn
the model parameters. The outputs of the CTL model
are the tags’ correlation matrix and the latent topics for documents, which are learned by utilizing the
learned tag correlations. After learning the CTL, we
can obtain a correlational graph which shows the relationships among the tags by ranking the correlational
values. In experiments, we trained the proposed model
on the DBLP corpus, where we treated authors as tags
and we can visualize the correlational graph among
the authors. Also, for one special author, there is
a ranking list to show the relevant authors not only
from the co-author information but also from whether
they have similar research interests. We also apply
the CTL model to the document retrieval and classification tasks on the Wikipedia corpus and the results
show that the CTL model outperforms the state-ofthe-art baselines.

2

RELATED WORKS

To date, many models are proposed for document
modeling via different approaches such as undirected
graphical models [24, 20, 13, 26, 25] or directed graphical models. As directed graphical models, topic models [11, 3, 1, 2, 4, 10] have been found to play an important role in analyzing unstructured texts. These
models have been applied to many text mining areas,
including information retrieval [28], document classification [6], and so on. However, most of these undirected and directed graphical models just consider the
unstructured text with the bag-of-word assumption.
More and more text mining tasks are emerging in

real-world applications to handle the semi-structured
corpora, such as document classification described in
[5, 16]. Based on the topic model, many methods have
been proposed to deal with the semi-structured corpora, such as the author topic model [18], labeled-LDA
[21], DMR [19], Tag-Weighted Topic Model (TWTM)
[15], Tag-Weighted Dirichlet Allocation (TWDA) [14],
partially LDA [22], TMBP [9], cFTM [8], statistical
topic models [23], and so on. Most of the models take
advantage of some given meta data (e.g., tags, labels,
or contextual information) in a document with different assumptions. For example, the author topic model
defines the distributions of the authors over the latent
topics and the authors are assumed to be independent under a Dirichlet prior. In the labeled-LDA and
partially LDA, the labels are defined as a set of distributions over the words from a vocabulary. For the
TWTM and TWDA, a weight vector is used to generate the topic distribution of a document with the given
tags. The DMR model is a Dirichlet-multinomial regression topic model which defines a log-linear prior
on the document-topic distributions. In [23], Timothy
et al. investigate a class of generative topic models for
multi-label documents that associate individual word
tokens with different labels, where the dependencyLDA is proposed to model the relations among the
labels and words. Some of the aforementioned models can obtain the topic distribution of the tags, which
can be used to measure the distance between the tags.
However, they fail to directly model the correlations
among tags.

3

THE CTL MODEL

In this section, we will mathematically define the Correlated Tag Learning (CTL) model, and discuss the
learning and inference methods.
We use the following terminologies and notations to
describe a corpus where each document is associated
with a set of tags, which we call the semi-structured
corpus.
Semi-Structured Corpus As a collection of
M documents, we define the corpus D =
{(w1 , t1 ), . . . , (wM , tM )}, where each 2-tuple (wd , td )
denotes a document with its tag vector. Let wd =
d
) denote the vector of N words associated
(w1d , . . . , wN
with document d. Let td = (td1 , . . . , tdL ) represent the
tag vector, each element of which is a binary indicator
for a tag, with L as the number of all the tags in the
corpus D.
Tag Matrix Here td is expanded to a ld × L tag
matrix T d , where ld is the number of tags in document d for the convenience of the inference. For each
i ∈ {1, . . . , ld }, Ti·d is a binary vector, where Tijd = 1 if

µ

Σ

η

td

d

εd

Λ

ϑd

θ
T ×K

z

π

ψ

W
N

K ×V
D

Figure 1: The graphical model of the CTL model,
where each node denotes a random variable, a shaded
node represents an observed variable, and edges indicate possible dependencies.
and only if the i-th tag in the document d is the j-th
tag in the tag set of the corpus D.
Topic Proportions Each document is associated
with a set of topic proportions ϑ. For a document
d, ϑd is a multinomial distribution over topics and it
reflects the probabilities of the words in document d
drawing from latent topics.
3.1

The Model

The proposed CTL model is a hierarchical Bayesian
model based on the topic model with assumptions that
each document in a corpus is modeled by an underlying set of latent topics and that each topic defines
a multinomial distribution over words. Besides, the
CTL model assumes that the topic distribution of each
document is determined by the given tags with a set
of participation values. With the participation values
in a participation vector, the CTL can model the topic
proportions of each document as the product between
the participation vector and the topic distributions of
each tags in the document.
In this paper, we use ϑd to denote the topic distribution of the document d, as shown in Figure 1. Let θ
represent a T × K matrix, where K is the number of
the latent topics and each row in θ describes the distribution of one tag belonging to the latent topics. Let ψ
represent a K × V distribution matrix, where each row
is a distribution vector of one topic over words and V
is the number of words in the dictionary of D.
3.1.1

Participation Vectors

εd , as shown in Figure 1, denotes the participation
vector of the given tags in the document d. In the
TWTM [15] and TWDA [14] models, it is called a
weight vector which follows a Dirichlet distribution.

As discussed above, under a Dirichlet distribution, the
components of the participation vector are nearly independent, leading to a strong and unrealistic assumption that the presence of one observed tag is not correlated to the presence of another one. In order to overcome this assumption, we use a flexible logistic normal
distribution to model the observed tags. As shown in
Figure 1, Σ is the covariance matrix of the logistic
normal distribution, µ is the expected value vector of
the random variables, and η d is a L-dimensional row
vector that follows the normal distribution with Σ as
the covariance and µ the mean. So the participation
vector is defined as follows:
εd = exp{T d × (η d )T },
where (η d )T is the transpose of η d , and εd is a ld × 1
column vector associatedPwith the document d. Note
that εd does not satisfy i εdi = 1, hence we call it a
participation vector instead of a weight vector.
With the participation vector, instead of a Dirichlet
distribution, we use a logistic normal distribution to
model the topic distribution of the document d:
ϑd = P

(εd )T × T d × θ
,
d T
d
i ((ε ) × T × θ)i

where (·)i denotes the i-th entry in a vector and ϑd ,
the multinomial
topic proportions of the document d,
P
satisfies i ϑdi = 1. With the multinomial topic proportions ϑ obtained by a participation vector and a set
of the observed tags in a document, we can generate
each word for the document in a similar way to the
topic model.
Thus, the CTL model assumes that a corpus with M
documents arises from the following generative process:
1. For each topic k ∈ {1, . . . , K}, draw ψk ∼ Dir(π),
where Dir(·) denotes a Dirichlet distribution and
π is a V -dimensional vector of hyperparameters.
2. For each tag t ∈ {1, . . . , L}, draw θt ∼ Dir(Λ),
where Λ is a K dimensional prior vector of θ.
3. For each document d:
(a) Draw η d ∼ N (µ, Σ) where N (·, ·) denotes a
multivariate normal distribution.
(b) Generate T d by td .
(c) Generate εd = exp{T d × (η d )T }.
(d) Generate ϑd =

d T
d
P (ε )d T×T ×θ
d ×θ) .
((ε
)
×T
i
i

(e) For each word wdn :

i. Draw zdn ∼Mult(ϑd ) where Mult(·) denotes a multinomial distribution.
ii. Draw wdn ∼Mult(ψzdn ).

distribution whose definition is:
H(q) = −

D
X

Eq [log q(η d )] −

The CTL model is different from the TWTM model
[15] where the weight vector in a document for the
given tags is drawn from a Dirichlet distribution. The
Dirichlet distribution is computationally convenient
but it has a nearly independent assumption among
the components of the weight vector. Differently, entries in the participation vector of the observed tags is
highly correlated as we described above.
The covariance matrix Σ induces the dependencies
between the components of the participation vector,
and allows a general pattern of variability between the
components. Using the covariance matrix of the logistic normal distribution, we can capture the correlated
relationships between the given tags associated with
each document.

3.2

Variational Inference

The logistic normal distribution used here brings not
only the capacity to model the correlations among
tags but also a challenge for the posterior inference
procedure since it is not a conjugate prior for the
multinomial distribution. We present a variational
expectation-maximization (EM) algorithm [12, 27] for
the inference. In the variational EM algorithm, the
E-step approximates the posterior by minimizing the
Kullback-Leibler (KL) divergence between the variational distribution and the true posterior distribution.
This method casts the inference problem as an optimization problem to approximate the posterior distribution of this latent model and some study in [2]
shows that minimizing the KL divergence is equivalent to maximizing the evidence lower bound (ELBO)
denoted by L(·).
For the CTL model, the ELBO can be derived by using
Jensen’s inequality:

L(·) =

D
X

Eq [log p(η d |µ, Σ)] +

d

d

+

D X
N
X

D X
N
X
d

n

+ H(q),

n

Eq [log p(wn |ψ, zn )] +

L
X

Eq [log p(θi |Λ)]

Eq [log q(θi )].

i

For the variational distribution q(·), we choose a fully
factorized distribution where all the variables are assumed to be independent:
q(η, z, θ|u, σ2 , γ, λ) =

L
Y

Dir(θi |λi )

i


D 
N
Y
Y
Mult(zn |γn ) ,
N (η d |u, σ2 )
n

d

where λ in the Dirichlet distribution, γ in the multinomial distribution, and (u, σ 2 ) in the Gaussian distribution are the variational parameters.
Before discussing the optimization procedure, we describe how to compute the ELBO in Eq. (1). In the
CTL model, the key inferential problem that we need
to solve is to compute the second term in Eq. (1),
which is the expected logorithm of a topic assignment
subject to a normalized multinomial parameter and
can be computed as



(εd )T × T d × θ
Eq [log p(zn |ϑd )] = Eq log p zn | P
d T
d
i ((ε ) × T × θ)i


 
K
X
(εd )T × T d × θ
P
γnk Eq log
=
,
d T
d
i ((ε ) × T × θ)i k
k

where γnk denotes the probability of the k-th topic
assigned to the n-th word. We see that computing the
CTL’s ELBO relies on the calculation of the expected
normalized topic distribution of a document, which
can be computed as
"

Eq log

P

(εd )T × T d × θ
d T
d
i ((ε ) × T × θ)i

! #
k

"
#
h
i
X d T
=Eq log((εd )T × T d × θ)k − Eq log(
((ε ) × T d × θ)i )
i



l
X



l
X

d

=Eq log
=Eq log

Eq [log p(zn |ϑd )]

L
X

Eq [log q(zn )]

n

d

d

−

D X
N
X

i

d

i





d

(i) 
εd
− Eq log
i θk



(i)
d
exp{η(i)
}θk 

l
X
i




εd
i



− Eq log

d

l
X
i



d
exp{η(i)
} ,

where θ(i) denotes the vector of the topic distributions
for the i-th tags in the document d corresponding to a
d
row in θ and η(i)
is the i-th entry of T d × (η d )T .

i

(1)

where q(·) denotes a variational distribution of the latent variables, Eq [·] denotes the expectation with respect to q, and H(q) is the entropy of the variational

By following the correlated topic model [1], the above
two expectations can be computed approximately with
Taylor expansions, respectively:
d

Eq [log

l
X
i

d

(i)

d
exp{η(i)
}θk ] ≈ log α +

l
1 X
(i)
d
Eq [exp{η(i)
}]Eq [θk ] − 1
α i

We use the conjugate gradient algorithm to solve this
problem, where the derivative is computed as

and
d

Eq [log

l
X

d

d
exp{η(i)
}] ≈ log β +

i

l
1X
d
Eq [exp{η(i)
}] − 1,
β i

′

L (u) =

n

where we introduce two new variational parameter α
d
and β. Note that Eq [exp{η(i)
}] is the mean of a log2
normal distribution and equals exp{u(i) +σ(i)
/2}. The
(i)

expectation of a Dirichlet random variable, Eq [θk ], is
PK
equal to [λk / j λj ](i) . Thus, for a document d, we
have
N
X

N X
K
X
γnk

α

k

γnk ∝ψk,vwn exp

n

k

#
"

ld
1X
λk
2
γnk log α +
exp{u(i) + σ(i)
/2} PK
α i
j λj
− log β −

1
β

ld
X
i

(i)

Thus, we can use the block coordinate-ascent variational inference to maximize Eq. (1) with respect to
variational parameters including σ 2 , u, γ, λ, α, and β.
We first maximize L(·) with respect to σ 2 for the document d with the objective function formulated as
L

k

−

N
β

ld
X

(


(2)

′

L(λ) =

K
X

L (σi2 ) =

(Λk − 1)(Ψ(λk ) − Ψ(

N
K
λk
1 X X γnk
2
exp{u(i) + σ(i)
/2} PK
2 n k α
j λj

#

1
1
N
2
exp{u(i) + σ(i)
/2} + 2 − Σ−1
,
−
2β
2σ(i)
2 ii

K
X

K
X

log Γ(λk ) +

λj )

j

(λk − 1)(Ψ(λk ) − Ψ(

K
X

λj )).

(7)

j

k

′

L (λk ) =

N
X
γnk (

PK

j λj − λk )
2
exp{u(i) + σ(i)
/2}
PK
α( j λj )2
′

′

+ (Λk − λk )(Ψ (λk ) − Ψ (

K
X

(8)

λj )).

j

For α and β, the optimal solutions can easily be found
as

α∝

PN PK
k

γnk



Pld
i

2
exp{u(i) + σ(i)
/2}

PN PK
n

d

β∝

l
X

k

γnk

2
exp{u(i) + σ(i)
/2}.



λ
PKk
λj
j



(i)



(9)

(10)

i

d

The objective function with respect to u is formulated
as

K
X

We use the gradient descent method to solve it, where
the derivative with respect to λk is:

(3)

where the subscript (i) ∈ (1, · · · , l ) indicates the i-th
tag in a specific document d.

λj )) − log Γ(

"
# 
 ld
K
N X
X
γnk X
λk
2
+
exp{u(i) + σ(i) /2} PK
α
j λj (i)
n
i
k

n

(i)

K
X
j

k

n

where tr(·) denotes the trace of a square matrix and
diag(·) converts a vector to a diagonal matrix. Obviously the problem with respect to σ has no analytic
solution and we solve it via the Newton’s method with
gradient computed as

(6)

For the variational parameter λ, the objective function
is formulated as

i

"

#
"
ld

1 X
λk
2
exp{u(i) + σ(i) /2} PK
α i
j λj (i)
)

k

2
exp{u(i) + σ(i)
/2},

(5)

(i)

where v wn denotes the index of wn in the dictionary.

+

(i)

λj

+ log α ,


2
exp{u(i) + σ(i)
/2} .

 X1
1
2
L(σ 2 ) = − tr diag(σ 2 )Σ−1 +
log σ(i)
2
2
i
"
#
 ld
K
N X
X
λk
γnk X
2
exp{u(i) + σ(i) /2} PK
+
α
j λj
i
n

j

#

We maximize Eq. (1) with respect to γnk to find the
maximizer as

n

≈

λk
PK

N
2
−
exp{u(i) + σ(i)
/2} − Σ−1 (u − µ).
β

Eq [log p(zn |ϑd )]

K
N X
X

exp{u(i) +

"

2
σ(i)
/2}

In the E-Step of the variational EM algorithm, we iteratively update the variational parameters including
σ 2 , u, γ, λ, α and β.

d

l
2
σ(i)
1
N X
L(u) = − (u − µ)T Σ−1 (u − µ) −
}
exp{u(i) +
2
β i
2
#
"
K
N X
ld
X

γnk X
λk
2
. (4)
+
exp{u(i) + σ(i) /2} PK
α
j λj (i)
n
i
k

3.3

Parameter Estimation

The parameters of the CTL model include Σ, µ, ψ and
Λ. In the M-step, given the semi-structured corpus, we
can estimate the parameters by maximizing a lowerbound of the log-likelihood based on the variational

E-step. The update rules for Σ, µ and ψ can easily be
obtained:
D
1 X
ud ,
D d

D 
1 X
Σ∝
Iσd2 + (ud − µ)(ud − µ)T ,
D

µ∝

Λ

µ

(11)

ϑd

t

Σ

θ
T ×K

(12)

ηd

z

εd

W

d

ψkj ∝

D X
N
X
d

γnk (wd )jn ,

t

L
X

D

log Γ(

+

K
X

Λj ) −

(Λi − 1)

i



K
X

Ψ(λli )

− Ψ(

K
X
j

λlj )



!

(14)
.

The derivative with respect to Λi is computed as
K
L 
K

 X
 X
X
′
l
l
λj ) . (15)
Ψ(λi ) − Ψ(
Λj ) − Ψ(λi ) +
L (Λi ) = L Ψ(
j

l

Figure 2: The two parts of the CTL model will be
degenerated if η d becomes equal to td , which means
that all the tags have the same effect on the document.

The second part shown in the right figure of Figure 2
means that all the tags have equal impacts on the topic
distribution ϑd . We can see that the second part is a
variant of the author topic model described in [18].
Thus, we can use the variational inference process to
compute the new ELBO bound as:

j

We can use the linear-time Newton-Raphson algorithm
to estimate Λ.

4

Lnew =

+

Actually, we can train the CTL model by only considering the co-occurrence information of tags in each
document. In this case, different tags have equal importance in a document d and hence η d is observed for
each document, where ηjd is set to 1 if and only if the
document d has the j-th tag. So the CTL model will
consist of two parts, as shown in Figure 2. The left
figure in Figure 2 is the first part containing Σ, µ, η d ,
ξ d and td , where η d , ξ d and td are the observed variables. We can use the traditional maximum likelihood
estimation to learn the correlation matrix Σ with the
D samples:
Σ=

Eq [log p(td |µ, Σ)] +

D
1 X d
(t − µ)(td − µ)T .
D d

−

D
X

n

Eq [log q(td )] −

L
X

Eq [log p(zn |ϑd )]

n

Eq [log p(wn |ψ, zn )] +

L
X

Eq [log p(θi |Λ)]

i

D X
N
X
d

d

−

D X
N
X
d

D X
N
X
d

The proposed CTL model can capture the correlations
among the tags in a semi-structured corpus, not just
only by considering the co-occurrences of the tags. The
CTL model presents the participation vector for each
document to estimate the correlations of the tags, and
the participation vector is learned by the text information with the basic assumption on latent topics. In
other words, the co-occurrence vector is binary, which
means that one tag is present or absent, while the participation vector is non-binary and the values in participation vector denote the importance of the tags.

D
1 X d
t ,
D d

D
X
d

DISCUSSION

µ=

K ×V
D

log Γ(Λi )

i

j

l

K
X

N

n

where (wd )jn is the count for the n-th word in the document d. For the Dirichlet parameter Λ, its objective
function is formulated as
L(Λ) =

ψ

(13)

Eq [log q(zn )]

n

Eq [log q(θi )],

i

PD
PD
where d Eq [log p(td |µ, Σ)] and d Eq [log q(td )] are
PD PN
d
2
fixed,
d
n Eq [log p(zn |ϑ )] does not involve σ
d
d
and u since η and ξ are known. In this case,
Lnew < L, which means the new lower bound Lnew
is lower than the former one when convergence. Thus,
treating the tags equally will not be a good choice.
Compared with the tag-weighted topic model [15], we
would obtain document embeddings with better quality when the tags in the corpus are highly correlated.
Thus, we will study the CTL model under this setting
in our experiments.

5

EXPERIMENTS

In this section, we will present the performance of the
proposed CTL model on document modeling, document classification, and document retrieval, respectively.

Experimental Settings

−1400000

ATM
−1600000

loglikelihood

CTL
LDA
TWDA
TWTM

−1800000

400

200

0

20

50

100

# of Topics

150

200

Figure 3: The 5-fold cross-validated held-out loglikelihood of different models on the Wikipedia corpus
with different number of topics.

5.2

Experiments on Document Modeling

To demonstrate the performance of the different models on document modeling, we computed the loglikelihood of the held-out data given a model estimated
from the remaining data by using five-fold cross validation, implying that 80% documents are for training and the remaining 20% for testing. We compared
1
2

http://www.informatik.uni-trier.de/~ ley/db/
http://www.wikipedia.org/

200

0
0

200

400

0

Authors[1~528]

(a) penalty factor p = 0.25

200

400

Authors[1~528]

(b) penalty factor p = 0.5

Figure 4: The scatter plots of the selected 528 authors
on the DBLP corpus, where a point is drawn if the
corresponding two authors are neighbors.
the CTL model with the Author Topic Model (ATM),
TWTM, TWDA, and LDA by varying the number of
latent topics. Since the LDA could not handle the tag
information directly, we treated the given tags as the
word features and added them into the document as
the input for the LDA model.
Figure 3 shows the average log-likelihood for each
model on the held-out set. The results demonstrate
that the CTL model has much better performance
than other baselines. One possible reason is that the
tags contained in Wikipedia corpus are highly correlated and with the help of the logistic normal distribution, the CTL model can obtain a more reasonable
and effective participation vector to form the topic distribution for each document.
5.3

−2000000

400

Authors[1~528]

We used two semi-structured corpora to evaluate the
CTL model. The first corpus is the Digital Bibliography and Library Project (DBLP),1 which is a collection of bibliographic information of technical papers
published in major computer science journals and conferences. We use the authors as the tags and removed
the authors that occur in fewer than 5 papers. We
use a subset of the DBLP that contains abstracts of
D = 40, 108 papers with 72, 748 words by removing
stop words and L = 6, 348 unique tags. The second
corpus is from Wikipedia.2 The Wikipedia corpus we
used contains 43, 217 articles. The size of the vocabulary is 22, 344 by removing stop words. We use the
category information, which is located at the bottom of
each article and provided by the MediaWiki software,
of articles as the tags, and in total there are 2, 900
tags. Moreover, each article belongs to different portals which can be viewed as the class label and all the
articles used in the experiments belong to 20 classes,
such as arts, sports, history, biography, education and
so on.

Authors[1~528]

5.1

Analysis on Tag Graph

The covariance of the logistic normal distribution for
the participation vector can be used to visualize the
relations among tags. Thus, we use the covariance
matrix to form a tag graph, where the nodes represent the tags appeared in the corpus and the edges
denote the relations between tags. To construct the
tag graph, we use the method introduced in [17] for
neighborhood selection based on the Lasso. As described in [17], the neighborhood selection with the
Lasso is used to estimate the conditional dependency
separately for each node in the graph. In the CTL
model, for a document d, η d follows a normal distribution with mean u and covariance Σ. Thus, {η d } are
treated as independent observations sampled from the
normal distribution N (µ, Σ), which are used to estimate the neighborhood based on [17].
We use the DBLP corpus for the experiment. For
the convenience of display, we select 528 authors to
illustrate the correlated connections among them by
drawing an edge if the corresponding two authors are
neighbors with different penalty factor p = 0.25 and

Wenyuan Dai

Robert A. Jacobs
Alice X. Zheng

Percy Liang

Zhi-Hua Zhou

ErHeng Zhong
Yong Yu

Steven E. Brenner
Andrew Y. Ng

Nathan Nan Liu

Zhihua Zhang
Sinno Jialin Pan

Xiaoyong Chai
Qiang Yang

Michael I. Jordan

Guang Dai

David M. Blei

Charles X. Ling
Vincent Wenchen Zheng

Lawrence K. Saul

Tommi Jaakkola

Zheng Chen

Jie Yin

Gert R. G. Lanckriet

Francis R. Bach

Derek Hao Hu
Sebastian Stein

Terry R. Payne

Weizhu Chen
Dou Shen

Enrico Gerding

Ramachandra Kota
Maria Polukarov

Hugo Larochelle
Jason Weston

Nicholas R. Jennings

Alex Rogers

Minghua He
Yoshua Bengio

Michael Wooldridge

Xudong Luo

Talal Rahwan

Aaron C. Courville
S. Shaheen Fatima
Pascal Vincent

Ioannis A. Vetsikas

Sarvapali D. Ramchurn

Figure 5: A subset of the author graph learned from 40,108 abstracts of the DBLP. The edges between authors
are computed by the neighborhood selection method [17] based on the Lasso.
Table 1: The ranking list of top correlated authors with eight authors on the DBLP corpus.
Michael I. Jordan
Yoshua Bengio
Qiang Yang

Nicholas R. Jennings

Micha Sharir

Jiawei Han

Jennifer Rexford

Franco Zambonelli

Alice X. Zheng(0.157837), Francis R. Bach(0.116478), Gert R. G. Lanckriet(0.107671), David M. Blei(0.103741), Steven E.
Brenner(0.092675), Zhihua Zhang(0.090492), Percy Liang(0.089226), Robert A. Jacobs(0.085318), Tommi Jaakkola
(0.082044), Guang Dai(0.058045), Lawrence K. Saul(0.057545), Martin J. Wainwright(0.045444), Nebojsa Jojic(0.043731),
David A. Patterson(0.041441), Tamar Flash(0.035152), Erik B. Sudderth(0.033563), Andrew Y. Ng(0.013234)
Pascal Vincent(0.410208), Hugo Larochelle(0.265349), Aaron C. Courville(0.132399), Jason Weston(0.049443)
Dou Shen(0.149414), Derek Hao Hu(0.144670), Sinno Jialin Pan(0.134137), Xiaoyong Chai(0.115358), Nathan Nan Liu
(0.0.113420), ErHeng Zhong(0.107736), Weizhu Chen(0.104334), Yong Yu(0.091473), Charles X. Ling(0.081368), Jie Yin
(0.077154), Wenyuan Dai(0.071907), Vincent Wenchen Zheng(0.062750), Zheng Chen(0.060292), Zhi-Hua Zhou(0.057265)
Alex Rogers(0.243331), Ramachandra Kota(0.142240), Maria Polukarov(0.137646), Talal Rahwan(0.134888), Ioannis A.
Vetsikas(0.126621), S. Shaheen Fatima(0.121799), Sebastian Stein(0.117208), Enrico Gerding(0.027282), Minghua
He(0.106050), Xudong Luo(0.086897), Sarvapali D. Ramchurn(0.071510), Terry R. Payne(0.068713),
Michael Wooldridge(0.065824)
Pankaj K. Agarwal(0.273844), Emo Welzl(0.170932), Natan Rubin(0.139646), Jnos Pach(0.110192), Haim Kaplan
(0.106137), Vladlen Koltun(0.104685), Boris Aronov(0.102123), Shakhar Smorodinsky(0.088530), Esther Ezra(0.083021),
Dan Halperin(0.074751, Rom Pinchasi(0.056282), Bernard Chazelle(0.044011), Jir Matousek(0.027263)
Xiaoxin Yin(0.196956), Deng Cai(0.103920), Guozhu Dong(0.101735), V. S. Lakshmanan(0.100016), Xin Jin(0.098451),
Charu C. Aggarwal(0.098256), Anthony K. H. Tung(0.092360), Jianyong Wang(0.087017), Hongjun Lu(0.072772),
ChengXiang Zhai(0.048570), Jiong Yang(0.042489), Philip S. Yu(0.036918), Ke Wang(0.032003)
David Walker(0.247776), Mung Chiang(0.162803), Eric Keller(0.152502), Renata Teixeira(0.141469),
Minlan Yu(0.123096), Nick Feamster(0.115787), Albert G. Greenberg(0.111072), Aman Shaikh(0.093805),
Matthew Caesar(0.049353), Michael J. Freedman(0.039058), Kang G. Shin(0.031615)
Marco Mamei(0.372887), Letizia Leonardi(0.213969), Giacomo Cabri(0.195711), Gabriella Castelli(0.188215),
Nicola Bicocchi(0.080601), Andrea Omicini(0.054696), Robert Tolksdorf(0.040949), Sara Montagna(0.020905),
Matthias Baumgarten(0.012859), Alberto Rosi(0.012646)

0.5, respectively. After using the spectral clustering
based on the conditional dependency obtained by the
method in [17], as shown in Figure 4, we clearly see
that the authors cluster together to different groups,
where the authors in a group may have similar research
interests.
We plot the author graph to show the correlations
among the 518 authors and in Figure 5, a part of the
graph is shown, where the nodes represent authors and
the edges denote the correlations between the authors.

In this graph, we can find some interesting insight.
For example, two authors ‘Zhi-Hua Zhou’ and ‘Guang
Dai’, who did not coauthor any paper, have a connection between them since they have similar research interests, which shows an advantage of the CTL model
over only using the coauthorship information that it
can find meaningful relations at the semantic level.
The CTL model can give a ranking list based on how
the authors are correlated. In Table 1, we pick several
authors from the 528 authors, and for each selected

author, we rank other authors according to their correlational values, which can be obtained by the process of neighborhood selection (see [17]). Based on the
results, we can easily see how close the two authors
are in the research and answer interesting questions
including that whether researcher A has more similar
interests to researcher B than to researcher C.
5.4

decide whether the retrieved document is relevant to
the query document. Figure 7 shows the F1 scores and
the precision-recall curves of the LDA, ATM, TWTM,
TWDA and CTL models. The experimental results
demonstrate the superiority of the embedding learned
by the CTL model for the document retrieval task.
0.8

ATM CTL LDA TWDA TWTM

Document Classification and Retrieval

0.6

We first test the classification performance by comparing the performance of the LDA, ATM, TWTM,
TWDA, and the proposed CTL model with the number of topics as 50 and 100, respectively. The LIBSVM
[7] with the Gaussian kernel and default parameters is
used as the classifier. In experiments, we use a subset
of the Wikipedia corpus which contains 14, 400 documents belonging to 20 classes. We reported in Figure
6 the precision of different methods on the Wikipedia
corpus by using the five-fold cross validation. According to Figure 6, the performance of the CTL model is
significantly better than that of other baseline methods. One possible reason is that the CTL model can
learn a better topic distribution for each document
than others.
0.80
ATM

CTL

LDA

TWDA

TWTM

ATM

CTL

LDA

TWDA

TWTM

0.75

Precision

Precision

0.70

0.70

F1

In this section, we conduct experiments on document
classification and retrieval tasks.

Precision

0.02

0.4

0.01

ATM

CTL

LDA

TWDA

TWTM

0.2

0.00

1

3

5

8

10 12

@

15

18

21

(a) F1-Score

0.0010.0020.0030.0040.0050.0060.0070.0080.009 0.01 0.1

Recall

0.5

1

(b) Precision-Recall curves

Figure 7: F1-score and precision-recall curves for document retrieval on the Wikipedia corpus for the LDA,
ATM, TWTM, TWDA, and CTL models with K =
100.

6

CONCLUSION

In this paper, we propose the CTL model, a statistical
model of semi-structured corpora, based on the topic
model to discover highly correlational relationships
among the tags observed in the semi-structured corpus. Besides, the experimental results demonstrated
that this method can model semi-structured corpora
better than the state-of-the-art models when the tags
are highly correlated.

0.65
0.65

0.60

LDA

ATM

TWTM

TWDA

(a) K = 50

CTL

0.60

LDA

ATM

TWTM

TWDA

CTL

(b) K = 100

Figure 6: Classification results on the Wikipedia corpus for the LDA, ATM, TWTM, TWDA and CTL
models with five-fold cross validation.
Moreover, we use the Wikipedia corpus to evaluate the
performance on the document retrieval task. In this
experiment, each document is represented by the vector of topic distribution generated by different models
with the topic number as K = 100. The Wikipedia
corpus used here is just the data set used in the above
classification experiment. We randomly sample 12, 400
documents for training and the rest for testing. For
each query, documents in the database were ranked
using the cosine distance as the similarity metric. For
evaluation, we check whether a retrieved document
has the same class label as the query document to

In our future study, we will apply the CTL model to
more text applications. Another possible direction is
to devise parallel algorithms for the CTL model to
further improve its efficiency.

ACKNOWLEDGEMENT
We thank the support of China National 973
project 2014CB340304 and Hong Kong CERG projects
16211214 and 16209715.

References
[1] David M. Blei and John D. Lafferty. Correlated
topic models. In NIPS, 2005.
[2] David M. Blei and Jon D. McAuliffe. Supervised
topic models. In NIPS, 2007.
[3] David M. Blei, Andrew Y. Ng, and Michael I.
Jordan. Latent Dirichlet allocation. Journal of
Machine Learning Research, 3:993–1022, 2003.

[4] Jordan L. Boyd-Graber and David M. Blei. Syntactic topic models. CoRR, abs/1002.4665, 2010.
[5] Andrej Bratko and Bogdan Filipic. Exploiting
structural information for semi-structured document categorization. Information Processing and
Management, 42(3):679 – 694, 2006.
[6] Deng Cai, Qiaozhu Mei, Jiawei Han, and Chengxiang Zhai. Modeling hidden topics on document
manifold. In CIKM, pages 911–920, 2008.
[7] Chih-Chung Chang and Chih-Jen Lin. Libsvm: A
library for support vector machines. ACM Transactions on Intelligent Systems and Technology,
2(3):27, 2011.
[8] Xu Chen, Mingyuan Zhou, and Lawrence Carin.
The contextual focused topic model. In KDD,
pages 96–104, 2012.

[18] Rosen-Zvi Michal, Chemudugunta Chaitanya,
Griffiths Thomas, Smyth Padhraic, and Steyvers
Mark. Learning author-topic models from text
corpora. ACM Transactions on Information Systems, 28(1):1–38, 2010.
[19] David M. Mimno and Andrew McCallum. Topic
models conditioned on arbitrary features with
Dirichlet-multinomial regression. In UAI, pages
411–418, 2008.
[20] Srivastava Nitish, Ruslan Salakhutdinov, and Geoffrey E. Hinton. Modeling documents with a
deep Boltzmann machine. In UAI, 2013.
[21] Daniel Ramage, David Hall, Ramesh Nallapati,
and Christopher D. Manning. Labeled LDA: A
supervised topic model for credit attribution in
multi-labeled corpora. In EMNLP, pages 248–
256, 2009.

[9] Hongbo Deng, Jiawei Han, Bo Zhao, Yintao Yu,
and Cindy Xide Lin. Probabilistic topic models with biased propagation on heterogeneous information networks. In KDD, pages 1271–1279,
2011.

[22] Daniel Ramage, Christopher D. Manning, and Susan Dumais. Partially labeled topic models for interpretable text mining. In SIGKDD, pages 457–
465, 2011.

[10] Matthew D. Hoffman, David M. Blei, and Francis R. Bach. Online learning for latent Dirichlet
allocation. In NIPS, pages 856–864, 2010.

[23] Timothy N Rubin, America Chambers, Padhraic
Smyth, and Mark Steyvers. Statistical topic models for multi-label document classification. Machine learning, 88(1-2):157–208, 2012.

[11] Thomas Hofmann. Probabilistic latent semantic
indexing. In SIGIR, pages 50–57, 1999.

[24] Ruslan Salakhutdinov and Geoffrey E. Hinton.
Replicated softmax: an undirected topic model.
In NIPS, pages 1607–1614, 2009.

[12] Michael I. Jordan, Zoubin Ghahramani, Tommi
Jaakkola, and Lawrence K. Saul. An introduction to variational methods for graphical models.
Machine Learning, 37(2):183–233, 1999.

[25] Nitish Srivastava and Ruslan Salakhutdinov.
Learning representations for multimodal data
with deep belief nets. In ICML Workshop, 2012.

[13] Hugo Larochelle and Stanislas Lauly. A neural
autoregressive topic model. In NIPS, pages 2717–
2725, 2012.

[26] Nitish Srivastava and Ruslan Salakhutdinov.
Multimodal learning with deep Boltzmann machines. In NIPS, pages 2222–2230, 2012.

[14] Shuangyin Li, Guan Huang, Ruiyang Tan, and
Rong Pan. Tag-weighted Dirichlet allocation. In
ICDM, pages 438–447, 2013.

[27] Martin J. Wainwright and Michael I. Jordan.
Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning, 1(1-2):1–305, 2008.

[15] Shuangyin Li, Jiefei Li, and Rong Pan. Tagweighted topic model for mining semi-structured
documents. In IJCAI, 2013.
[16] Pierre-Francois Marteau, Gildas Ménier, and Eugen Popovici. Weighted naive Bayes model for
semi-structured document categorization. CoRR,
abs/0901.0358, 2009.
[17] Nicolai Meinshausen and Peter Bühlmann. Highdimensional graphs and variable selection with
the lasso. The Annals of Statistics, pages 1436–
1462, 2006.

[28] Xing Wei and W. Bruce Croft. LDA-based document models for ad-hoc retrieval. In SIGIR, pages
178–185, 2006.

Adaptive Social Similarities for Recommender Systems
Le(Bill) Yu

Rong Pan∗

Zhangfeng Li

Dept. of Computer Science
Sun Yat-sen University
Guangzhou, P.R.C.

Dept. of Computer Science
Sun Yat-sen University
Guangzhou, P.R.C.

Dept. of Computer Science
Sun Yat-sen University
Guangzhou, P.R.C.

yule3@mail2.sysu.edu.cn

panr@mail.sysu.edu.cn

lizhangf@mail2.sysu.edu.cn

ABSTRACT

1. INTRODUCTION

Collaborative ﬁltering (CF) is an eﬀective recommendation
technique, which selects items for an individual user based
on similar users’ preferences. However, CF may not fully
reﬂect the procedure how people choose an item in real
life, for users are more likely to ask friends for opinions
instead of asking similar strangers. Recently, some recommendation methods based on social network have been
raised. These approaches incorporate social network into the
CF algorithms and users’ preferences can be inﬂuenced by
the favors of their friends. These social approaches require
the knowledge of similarities among friends. There are two
popular similarity functions: Vector Space Similarity (VSS)
and Pearson Correlation Coeﬃcient (PCC). However, both
friends similarity functions are based on the item-sets they
rated in common. In most cases, these functions are impractical, i.e. if two friends do not share the same items
in common, the similarity between them will be zeros. To
solve this problem, we propose an Adaptive Social Similarity
(ASS) function based on the matrix factorization technique.
We conduct our experiment on a large dataset: Epinions,
which is a widely-used dataset with social information. The
experiment results illustrate that our approach outperforms
the baseline models and achieves a better performance than
social-based method in [4].

We are coming into recommendation age: providing personalized recommendations has been an important problem
for more than ten years, e.g. Google News1 , Netﬂix DVDs2
and LastFM music3 . In the meantime, researches on recommender systems also attract a lot attention in the past
decade.
One of the most successful technologies for recommender
systems is Collaborative Filtering (CF)[9, 10, 6, 7], which
predicts items for certain users based on the items preferred
by the similar users. However, traditional CF techniques do
not consider social relationships between users. Therefore
the recommender systems may not reﬂect the real life recommendation, i.e. when we are planning for movies, we might
turn to our friends for recommendation instead of strangers
with similar tastes. Another problem of traditional recommender system is cold start users. In this paper, cold start
users are referred to those users who have no ratings or little
ratings before. Therefore, the system can hardly recommend
items for these cold start users, as the system knows little
characteristics about the users.
Recently, the advent of social networking information provides new topics for recommender systems. Some recommendation algorithms [2, 3, 1, 4] with social relationship
have emerged. These new approaches utilize the social network among users and make recommendations for a certain
user with the aid of the ratings of the user’s friends. The
experimental analysis shows these methods outperform the
recommender system without social information.
Another advantage of social network information is that
it helps to solve the cold start problem. Although old start
users have no or little ratings, some of them participate in
social network. In that case, we can recommend for these
social-active cold start users. For example, new users on
Amazon.com may have no purchase record before, so they
have no personal preference. But they might probably buy
items purchased by their friends.
The authors of [4] proposed Vector Space Similarity (VSS)
and Pearson Correlation Coeﬃcient (PCC) as similarity function among friends. These two similarity functions are calculated based on the item-sets friends rated in common. However, there are still two problems in [4]. One is the friendship
loss problem, i.e. if two friends do not rate the same items,
their similarity will be treated as 0 and therefore they will
lose their relationship. The other is cold start problem. To

Categories and Subject Descriptors
H.2.8.d [Information Technology and Systems]: Database
Applications—Data Mining

General Terms
Algorithms, Experimentation

Keywords
Collaborative Filtering, Recommender System, Social Network
∗

Corresponding author

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
RecSys’11, October 23–27, 2011, Chicago, Illinois, USA.
Copyright 2011 ACM 978-1-4503-0683-6/11/10 ...$10.00.

1

http://news.google.com
http://www.netﬂix.com
3
http://www.last.fm
2

257

network information to provide more data for recommendation. These methods explore the similar users to generate
recommendations.
In [4], the paper proposed social recommendation with
user similarity functions for recommendation. It employed
Vector Space Similarity (VSS) and Pearson Correlation Coeﬃcient (PCC) to deﬁne the similarity between users based
on the items they both rated. However, the similarity between users is impractical in most cases. For example, assuming each of the two friends rate a lot of movies, however,
these two users do not rate any movie in common. So the
similarity between these two friends will be considered as
0. In these cases, the social network among users will be
lost, for we cannot simply say these two friends have no relationship. So in Section 4, we put forward a new similarity
function to solve this problem and better simulate the real
world situation.

solve these two problems, we propose a social similarity function to keep the similarity between friends and also reduce
the cold start problem.
The contributions of the paper are as follows:
• We apply social network to improve the recommendation performance.
• We propose a more practical similarity function distinct from previous PCC and VSS techniques.
• The method proposed reduces the cold start problems.
The rest of this paper is organized as follows. Section 2
provides the overview of recommender system problem. Section 3 introduces several approaches for recommender systems. Section 4 details our method for social recommendation. The performance of our approach is presented in
Section 5. Finally, Section 6 concludes the paper.

2.

4. SOCIAL RECOMMENDATION METHOD

PROBLEM DEFINATION

In this section, we analyze the social recommendation
problem based on matrix factorization framework. Then
we propose a new social relationship similarity function.

Traditional recommender systems consist of three entities:
M users in the systems U = {u1 , ..., uM }, N items for recommendations V = {v1 , ..., vN } and the user-item ratings
matrix R, where Rij denotes the score ui votes for vj . The
problem discussed in this paper incorporates social network
information between users into the CF methods of the traditional recommender system to improve the recommendations. In social social recommender systems, prediction for
a user is not only judged by oneself, but also inﬂuenced by
one’s friends. The problem we investigates in this paper is
how to predict the missing values of the user-item matrix by
employing these social relation data.

3.

4.1 Matrix Factorization
Matrix factorization methods are one of the most popular CF techniques. These methods assume that user and
item characteristics can be expressed by some low dimensional latent and then make predictions based on the latent
features.
Considering a rating matrix matrix R = (Rij )M ×N , where
there are M users and N items. Matrix factorization method
can be approximate the rating matrix R by
R ≈ U T V,

RELATED WORK

In this section, we mainly introduce major approaches for
recommendation, traditional recommender systems and social recommender methods.

3.1 Traditional Recommender Systems
Traditional recommender systems refer to the systems only
use rating matrix for recommendation. Among all the algorithms, collaborative ﬁltering is eﬃcient in dealing with
large data sets. Certain user’s preference can be predicted
by information from other users who have similar interests.
CF techniques can be classiﬁed into two categories: memorybased and model-based methods. Memory-based methods
make rating prediction based on the rating behavior of other
similar items and users. The shortcoming of these methods
is that they require much denser data sets to estimate similar values. Therefore, these algorithms are not practical in
most real life system. Model-based methods use the rating data to train a model and then apply the trained model
to make predictions. Matrix factorization methods are one
of the model-based techniques. They express the user and
item features lie in low dimensional space and then make
predictions based on the latent features.

3.2 Social Recommender System

(1)

where U = (Uij )M ×D , V = (Vij )N×D , D < min(M, N ).
Matrix U and V can be regarded as the user latent matrix
and item latent matrix. We deﬁne I = (Iij )M ×N , if Rij = 0,
Iij = 1; else Iij = 0. Then we can rewrite the objective loss
funcion as

L(U, V ) =
Iij (Rij − Ui. Vj.T )2 .
(2)
i,j

To prevent overﬁtting problems, a regularization term is appended on the loss function,

Iij (Rij − Ui. Vj.T )2 +
L(U, V ) =
i,j

λ1 ||U ||2F + λ2 ||V ||2F ,

(3)

where ||.||F denotes the Frobenius norm.

4.2 Social Matrix Factorization
In the social age, people will ask friends for recommendation. Uses’ taste might be close to their friends’ tastes.
Therefore, we add a social regulizer to the loss function:

L(U, V ) =
Iij (Rij − Ui. Vj.T )2 +
i,j

λ1 ||U ||2F + λ2 ||V ||2F +
β 
Sim(i, f )||Ui − Uf ||2F .
2

Although CF methods have achieved great success in recommendation applications, there still exist some problems
which limit the performance. One of the biggest problems is
data sparsity. To solve this problem, [2, 3, 1, 4] utilize social

f ∈F (i)

258

(4)

5. EXPERIMENT

Here we impose a social regulizer term to Eq. (3) to constrain
user’s interest. β is the inﬂuencing impact of social network.
F (i) is the friends of user ui . Taste diﬀerence between two
friends can be described as Sim(i, f )||Ui − Uf ||2F : Ui means
the feature of user ui . If user ui s interest does not like ui s
friend uf , and their characters might be more diﬀerent and
||Ui − Uf ||2F will be larger. The friends’ overall opinions are
to combine all the friends’ interest. However, user ui might
not treat every friend equally, user ui might trust uf1 more
than uf2 . ui will much more follow the taste of uf1 . So
Sim(i, f ) depicts the similarity between ui and uj .
Our model is to minimize the loss function. By performing
gradient descent on Ui and Vj for user i and item j, we obtain
∂L2
∂Ui

=

N

j=1

β

Iij (Rij − Ui. Vj.T ) + λ1 Ui +


In this section, we report our experiments described in
Section 4 and compare the results with traditional recommendation and social recommendation methods.

5.1 Datasets
Our experiment is conducted on a social networking dataset:
Epinions [5]. Epinions is a well-known rating network for
research. Users rate the product or service on a rating
scale from 1 to 5 stars. Epinions member maintains a trust
list, and it shows a network of trust relationships between
users. It provides ”Twitter-like” social network. The Epinions dataset consists of 49,290 users who have rated a total
of 139,738 diﬀerent items. The total number of ratings is
664,824. As to the user social trust network, the total number of issued trust statements is 511,799.

(5)

5.2 Evaluation Measurement

Sim(i, f )(Ui − Uf ),

f ∈F (i)+

∂L2
∂Vj

=

M


Iij (Rij − Ui. Vj.T ) + λ2 Vj .

(6)

i=1

4.3 Impact of Similarity Functions
The similarity function Sim(i, f ) calculates the similarity
between user ui and uf . In [4], two similarity functions:
Vector Space Similarity (VSS) and Pearson Correlation Coeﬃcient (PCC) are applied for social relationship, as shown
in Eqs. (7) and (8).

Rij Rf j
Sim(i, f )

=



j∈C(i,f )



j∈C(i,f )

2
Rij





Sim(i, f )

=



j∈C(i,f )



j∈C(i,f )



j∈C(i,f )

(7)

Rf2 j


RM SE =

(Rij − Rij )(Rf j − Rf j )

(Rij − Rij )2




j∈C(i,f )

We use 5-fold cross validation to estimate the performance
of diﬀerent algorithms. In each fold, the validation datasets
are divided into train sets and test sets randomly. The training set contains 80% known positive examples and the other
20% elements of the matrix are treated as unknown. The
known positives in the training set are excluded in the test
process.
For all the experiments in this paper, the values of λ1 and
λ2 are set to 0.01. The experimental results using 5 and
10 dimensions to represent the latent features are shown in
Table 1.
The evaluation metric we use in the experiment is the
Root Mean Square Error (RMSE). The metrics RMSE is
deﬁned as:

(8),
(Rjf − Rjf )2

we deﬁne Item(i) means the items set user ui have rated,
and C(i, f ) = Item(i) ∩ Item(f ) means the items set user
ui and uj both rated.
However, both of the similarity functions are calculated
on the items both users rated in common. Suppose user ui
and uj are friends. And each of the two people rate a lot of
movies. However, they share no movies in common, that is
to say, two friends watch diﬀerent movies. Therefore, in the
traditional similarity functions PCC or VSS, their similarity
is 0. To solve this problem, we propose a new function ASS:
D
Uid Uf d
Sim(i, f ) = cos(Ui , Uf ) =  d=1 
, (9)
D
D
2
2
U
U
d=1 id
d=1 f d


i,j

(Rij − R̂ij )2
T

,

(10)

where Rˆij means predicted score for user ui on item ij and
T means the pairs number of (i, j) in the test set. Notice
smaller RMSE value means a better performance.

5.3 Comparisons
In this section, we compare the recommendation results
of the following algorithms:
• P M F : this method is proposed by Salakhutdinov and
Minh in [8]. It only uses rating matrix to recommend.
• SR1 : treat all friends equally, and similarity of friends
will be constant 1, which means Sim(i, f ) = 1.
• SRP CC : [4] models users’ rating based on social relationship. It uses similarity function Eq. (8).
• ASS: this method is proposed in Section 4. It also uses
social network, and it uses similarity function Eq. (9).

where Ui and Uj are the feature latent of these two friends.
Therefore, if user ui and uj have similar tastes, and their
characters Ui and Uj might be more similar, and cos(Ui , Uf )
might be close to 1, which shows their similarity Sim(i, f )
will become 1. Comparing to Eqs. (7)(8), this similarity
function will not lose the information among friends, i.e. if
two friends do not buy any products in common, similarity
calculated by Eqs. (7)(8) will be 0, while Eq. (9) can still
tell the similarity between them.

In the dataset of Epinions, ASS greatly outperforms the
traditional recommendation technique P M F by 5.67% for
D = 5 and 5.18% for D = 10. Our method also achieves
better performance of 2.82% for D = 5 and 2.44% for D = 10
than SRP CC method in [4].
From the results, the performance shows that the dimension parameter D = 5 is enough to depict the model.

259

Table 1: RMSE for Epinions with diﬀerent D
Approaches: PMF
SR1
SRP CC
ASS
D=5
1.1694 1.1487 1.1351
1.1031
D=10
1.1765 1.1579 1.1436
1.1156

1.22

1.2

5.4 Cold Start User Problem

1.18

Our algorithm also greatly reduces the cold start users
problem. In our experiment, cold start users are referring to
users who have less than 10 ratings before. So the baseline
method PMF might not solve the problem of cold start users.
In the SR1 , it achieves better performance than PMF, since
it utilize the social information. But all friends’ similarity
is treated the same. As for SRP CC , although it uses social
network to inﬂuence the friends’ preference, the similarity
between the users and friends will still be 0, for the similarity function is based on the movies two friend watched
together. In the ASS, since we use friends characteristics to
help recommend, social cold start users’ characteristic can
be inﬂuenced by their friends. The result in Table. 2 shows

1.16

1.14

1.12

1.1
−5

10

−3

10

−2

10

−1

10

0

10

Figure 1: Regulizer Parameters β.
problem. Experimental analysis shows that our method outperforms the baseline and social recommendation approach
in [4]. In the meantime, we also reduce the cold start user
problem. In the future, we plan to apply the new similarity
function to solve the friendship propagation problem. Our
future work will focus on the ranking performance of recommendations.

Table 2: Cold Start Problem on Epinions
Approaches: PMF
SR1
SRP CC ASS
D=5
1.1970 1.1833 1.1629
1.1489
D=10
1.1947 1.1811 1.1606
1.1454

Acknowledgments
We thank the anonymous reviewers for helpful comments.
This work was supported by National Natural Science Foundation of China (61003140, 61033010).

that by incorporating the social information, ASS recommendation on cold start users improves 4.02% on D = 5
than P M F . The social network of the cold start user might
relate those cold start users to their active friends. And
they might be close to their friends’ tastes and this greatly
solve the cold start problem. However, since cold start users
might be less active and have less friends, the improvement
of cold start user is not as great as full datasets (5.67%).

7. REFERENCES
[1] M. Jamali and M. Ester, A Matrix Factorization
Technique with Trust Propagation for
Recommendation in Social Networks, Proceedings of
the ACM conference on Recommender systems, 2010.
[2] H. Ma, H. Yang, M. R. Lyu, and I. King, social
recommendation using probabilistic matrix
factorization, CIKM 2008, pages 931-940. ACM, 2008.
[3] H. Ma, I. King, and M. R. Lyu, Learning to
recommend with social trust ensemble, In SIGIR 2009,
pages 203-210.
[4] H. Ma, D. Zhou, C. Liu, M. R. Lyu, I. King,
Recommender Systems with Social Regularization, In
Proceedings of ACM WSDM 2011.
[5] P. Massa and P. Avesani, Trust metrics in
recommender systems, Computing with Social Trust:
259-285, Springler.
[6] R. Pan, Y. Zhou, B. Cao, N. N. Liu, R. M. Lukose, M.
Scholz, and Q. Yang, One-class collaborative
Filtering,In IEEE International Conference on Data
Mining (ICDM 2008), pages 502-511.
[7] R. Pan and M. Scholz. Mind the gaps: Weighting the
unknown in large-scale one-class collaborative filtering.
In 15th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD), 2009.
[8] R. Salakhutdinov and A. Mnih, Probabilistic matrix
factorization, In NIPS 2008, volume 20.
[9] X. Su and T. M. Khoshgoftaar, A survey of
collaborative filtering techniques, Advances in Artiﬁcial
Intelligence, 2009.
[10] Y. Zhang, B. Cao, and D. Y. Yeung. Multi-domain
collaborative ﬁltering. In Proceedings of the 26th
Conference on Uncertainty in Artiﬁcial Intelligence
(UAI), Catalina Island, California, USA, 2010.

5.5 Regulizer Parameters λ and β
Parameter λ is the regularization parameter which is determined with cross-validation. In all of the algorithms, we
set λ = 0.01. The other parameter β controls how much
inﬂuence should social networking impose on the social network. In the extreme case, if β = 0, the social relationship
does not inﬂuence recommendation and it turns to traditional recommendation. In this part, we analyze how the
changes of β can aﬀect the recommendation prediction.
In this part, we only illustrate the inﬂuence of β. Figure. 1
shows the impacts of β in ASS model with D = 5. We
ﬁnd out that the value of β impacts the recommendation
results signiﬁcantly, which shows that utilization of social
network greatly improves the recommendation performance.
As we can learn from Fig. 1, as β increases, the RMSE values
decline at ﬁrst, and then increase. The result agree with the
real life problem. If we consider the interests of our friends,
we might get the items we want; however, if we only listen
to our friends without considering our own interest, then we
might get items we do not like.

6.

−4

10

CONCLUSIONS

In this paper, we present social-based method based on
matrix factorization. We apply social information to improve recommendation. The framework is based on matrix
factorization with social constraint regulizer. As for social
network, the similarity function plays a crucial part to compare the similarity between friends.
Our similarity function approach not only better calculates similarity between users, but also reduces the cold start

260

Expert Systems with Applications 37 (2010) 4955–4965

Contents lists available at ScienceDirect

Expert Systems with Applications
journal homepage: www.elsevier.com/locate/eswa

Mix-ratio sampling: Classifying multiclass imbalanced mouse brain images
using support vector machine
Min Hyeok Bae, Teresa Wu *, Rong Pan
Department of Industrial, Systems and Operations Engineering, Arizona State University, Tempe, Arizona 85287-5906, USA

a r t i c l e

i n f o

Keywords:
Sampling procedure
Imbalanced dataset
Multiclass classiﬁcation
Support vector machine
Data mining
Brain image segmentation

a b s t r a c t
Support Vector Machine (SVM) is a classiﬁer designed to achieve optimized classiﬁcation accuracy. It has
been applied to numerous applications associated with images. Yet challenges remain when applying
SVM on segmenting mouse brain images. This is due to the fact that each high-resolution mouse brain
image is a very large data set and it is a multiclass classiﬁcation problem with extremely imbalanced data
size for different classes. To address these issues, a mix-ratio sampling approach for SVM is proposed
which determines various over-sampling ratios for different minority classes. In addition, to improve
the imaging classiﬁcation accuracy, spatial information is incorporated into the classiﬁcation problem.
Five mouse Magnetic Resonance Microscopy (MRM) images are collected to test the accuracy of classifying 21 brain structures. The ﬁrst comparison experiment demonstrates the SVM with mix-ratio sampling
method relieves the imbalance problem for multiclass more effectively and efﬁciently than the SVM with
simple over-sampling method. In the second comparison experiment, another classiﬁer, Artiﬁcial Neural
Network (ANN) is used to compare against SVM based on the same mix-ratio sampled data and the
results indicate that SVM shows better classiﬁcation performance than ANN. Thirdly, the cross validation
is conducted to demonstrate SVM with mix-ration sampling can classify multiclass imbalanced data with
high accuracy.
Published by Elsevier Ltd.

1. Introduction
Progresses in the ﬁeld of medical imaging technologies such as
Magnetic Resonance Imaging (MRI), Positron Emission Tomography (PET), Computed Tomography (CT), etc. have enabled one to
non-invasively delineate anatomical structures of a subject (e.g.,
human brain, mouse brain). One major advantage of accurate and
precise delineation of neuroanatomical structures in the case of
human brain is that it gives strong foundation to help in the early
diagnosis of a variety of neurodegenerative disorders, as it is found
that neurodegenerative disorders are frequently related with
structural changes in the human brain (Fischl et al., 2002). For
example, the cause and progression of Alzheimer’s Disease (AD)
is associated with the abnormal accumulation of proteins forming
neuroﬁbrillary tangles and amyloid plaques in the brain, and this
results in volumetric changes in medial temporal lobe structures,
including the hippocampus and the parahippocampal gyrus (Busatto et al., 2003). To accurately capture the volumetric changes,
there is a need to segment brain structures precisely. Currently,
most segmentation practices on brain images are done by trained
anatomists or technician manually. Because of the large amount
* Corresponding author. Tel.: +1 480 965 4259; fax: +1 480 965 8692.
E-mail addresses: teresa.wu@asu.edu (T. Wu), rong.pan@asu.edu (R. Pan).
0957-4174/$ - see front matter Published by Elsevier Ltd.
doi:10.1016/j.eswa.2009.12.018

of data from high-resolution images, the execution time of the
manual segmentation could take several days, which makes it
impractical for high-throughput brain image analysis and delays
the research of brain disease and ﬁnding the cures. Therefore, there
have been increasing demands for automated segmentation methods which can delineate the neuroanatomical structures of human
brains more efﬁciently and reliably.
Several different automated segmentation methods have been
developed: atlas based segmentation, probabilistic information
based segmentation and machine learning-based segmentation.
The atlas based segmentation method is to non-linearly register
an image to a manually labeled atlas image (Hartmann, Parks, Martin, & Dawant, 1999; Heckemann, Hajnal, Aljabar, Rueckert, &
Hammers, 2006; Iosifescu et al., 1997). The nonrigid transformation maps the labels of each voxel in the atlas image to the image
being segmented. The probabilistic information based approach
uses probabilistic information extracted from training datasets of
MR images for the segmentation (Ali, Dale, Badea, & Johnson,
2005; Fischl et al., 2002). Probabilistic information of MR intensity,
prior probability of a label of a voxel at a location in a 3D images
and pairwise probability of a labeling given labels of neighboring
voxel are incorporated to predict a label of the voxel. The machine
learning based segmentation use various classiﬁers to classify each
voxel in a brain image into a number of classes using various MR

4956

M.H. Bae et al. / Expert Systems with Applications 37 (2010) 4955–4965

intensity information as input features. ANN was used to segment
brain images into the three tissues of white matter, gray matter
and cerebrospinal ﬂuid (Reddick, Glass, Cook, Elkin, & Deaton,
1997). Powell et al. (2008) demonstrate that machine learning
methods, such as Support Vector Machine (SVM) and ANN, outperform the template or probability-based methods in segmentation
of human brain MR images. They segmented MR images of human
brains into eight brain structures.
Even though the machine learning based segmentation method
has been successfully used for automated segmentation, it still has
some limitations to be applied to brain image segmentation problems which have high-resolution images as data sets and a large
number of neuroanatomical structures to be segmented, such as
mouse brain images. The multiclass classiﬁcation is a challenging
problem, especially when the data size is extremely large and sizes
of classes are imbalanced. In fact, most classiﬁers perform well for
classifying balanced data, but lose their classiﬁcation power when
dealing with imbalanced data (Li, 2007). The sizes of neuroanatomical structures of human or animal brains vary dramatically.
For example, each mouse brain MRM image used for this study
has over 4 million (128  128  256) voxels. The number of voxels
of Cerebral Cortex structure is over 500 K, while that the number of
voxels of Interpeduncular Nucleus structure is only 1600. In addition, 21 neuroanatomical structures to be segmented in this study
are considerably large number of classes comparing to other multiclass data mining applications. Another limitation for applying
the machine learning based segmentation method is class overlapping. The MR signals from each anatomical structure often overlap
with each other which make it even difﬁcult to get distinct intensity information to classify the neuroanatomical structures. As stated by Fischl et al. (2002), it appears that a global segmentation
method which can effectively segment the structures using only
the MR intensity information does not exist. These limitations have
made the machine learning based segmentation difﬁcult to be applied to automated brain image segmentation problems. Therefore,
it is necessary to develop an effective classiﬁcation approach which
is capable to handle multiclass, imbalanced data with overlapped
MR signals. In this research, we introduce a mix-ratio sampling
method to handle the imbalanced multiclass problem. This method
is a supervised sampling method, which ﬁrst identiﬁes minority
classes followed by determining different sampling ratios for the
minority classes. Secondly, the location information (spatial information) of each voxel in the MR brain image is added as features to
improve the classiﬁcation performance.
In this study, we introduce an automated segmentation method
for mouse brain using a data mining technique, SVM, which in general is linear classiﬁer aiming to locate the hyberplane that maximally separates the classes. Recently SVM has received a lot of
attention from the machine learning community due to the following reasons. First, SVM has the good generalization ability. Unlike
the artiﬁcial neural network (ANN) method, which is to minimize
the sum-of-square error between the output and the target label
so its performance depends on a large size of the training data
set, SVM attempts to maximize the margin between the classes,
so the generalization performance does not drop signiﬁcantly
when the training data are scarce (Abe, 2005). Secondly, by using
kernel functions, SVM can map the input space into a high-dimensional feature space. Therefore, SVM can classify objects which are
not linearly separable. In addition, since SVM is solved with quadratic programming, it can achieve a global optimal solution. As
summarized by Shin and Cho (2006), SVM has accomplished great
success in a variety of applications including handwritten character recognition, object detection and recognition, text classiﬁcation, fault detection, fraud detection, etc. In the ﬁeld of medical
image classiﬁcation, Nattkempera et al. (2005) use SVM in the
breast tumor classiﬁcation and ﬁnd that SVM performs better than

K Nearest Neighbor (KNN) and Decision Tree (DT) classiﬁer. Brain
tumor recognition using SVM is studied by Luts, Heerschap, Suykens, and Huffel (2007) and they conclude SVM provides better
classiﬁcation accuracy comparing to the Linear Discriminant Analysis (LDA). SVM also outperforms the Fisher Linear Discriminant
(FLD) classiﬁer in classiﬁcation of brain states from whole functional magnetic resonance imaging (fMRI) volumes (Mourao-Miranda, Bokde, Born, Hampel, & Stetter, 2005), and presents better
results than neural network Self-Organizing Maps (SOMs) in classifying normal and abnormal human brain MR images (Chaplot,
Patnaik, & Jagannathan, 2006).
Even though SVM had better classiﬁcation ability than any
other classiﬁers, it has some weak points (Abe, 2005). First, it is
hard to extend to multiclass problem as it was initially designed
to identify the decision function for binary class. Secondly, existing
concerns for SVM are the scalability and computational efforts required in training and testing. Since training of SVM is associated
with solving dual problem which has the same number of variables
with the number of training data, it is intractable to apply to large
scale problems. The third weak point is the choice of kernels and
their parameters. The generalization ability of SVM depends on
selection of kernels and their parameters. Since the selection is
done by monitoring generalization abilities of various different
SVM models, it is a hard task to choose good kernel and their
parameters for the large scale problem. Finally, SVM does not perform well for imbalanced data like most of classiﬁers. Therefore, it
is not straightforward to apply SVM to the brain image segmentation to be addressed in this paper since it is an imbalanced multiclass classiﬁcation problem with large scale datasets. In this
research, mix-ratio sampling is proposed to enables SVM to be
applicable to the imbalanced multiclass problem. We compare
the mix-ratio sampling method with the simple over-sampling
method to demonstrate the effectiveness of SVM on mix-ratio sampling in solving multiclass imbalanced problems. We then compare
SVM with another classiﬁer, ANN, using the same data set which is
sampled by the mix-ratio sampling method to see which classiﬁer
is appropriate to the imbalanced multiclass problem. Cross validation is further conducted to illustrate the general accuracy can be
achieved by the proposed method.
This paper is structured as following: Section 2 reviews SVM basics and several SVM classiﬁers for multiclass classiﬁcation. Section
3 describes the methods for handling the class imbalance problem
and class overlapping problem, including the mix-ratio sampling
approach. The mouse brain image dataset and the automated segmentation procedure that we implemented are described in Section 4 following by three sets of experiments. In Section 5, we
compare the two sampling methods, simple over-sampling and
mix-ratio sampling, to show the superiority of the proposed sampling method, report the performance comparison of SVM and
ANN and provide the 5-fold cross validation testing result of mouse
brain segmentation. Finally, conclusion and future work are provided in Section 6.

2. SVMs for classifying multiple imbalanced classes
2.1. Support vector machines basics
The basic idea of SVM is to construct an optimal hyperplane
which gives the maximum separation margin between two classes.
In a binary classiﬁcation problem, let us consider a training set
xi 2 Rn with its label set yi 2 fþ1; 1g, where yi ¼ þ1 for positive
class and yi ¼ 1 for negative class, for all the training data i = 1,
2, . . ., m, where m is the number of the data and n is the dimension
of the data. The hyperplane f(x), that separates the given data, is
deﬁned as:

4957

M.H. Bae et al. / Expert Systems with Applications 37 (2010) 4955–4965

f ðxÞ ¼

m
X

wi xi þ b ¼ wT x þ b;

ð1Þ

i¼1

where w is the n dimensional weight vector, b is a bias term. The w
and b describe the shape and position of hyperplane. The distance
between a hyperplane and the datum nearest to the hyperplane is
the margin which is calculated by 2=kwk. SVM training involves
ﬁnding the optimal hyperplane which has the maximum margin,
subject to the following constraint:

yi ðwT xi þ bÞ P 1 for i ¼ 1; . . . ; m:

s:t:

m
X
1 T
ni
w wþC
2
i¼1

ð3Þ

yi ðw  Uðxi Þ þ bÞ P 1  ni ; i ¼ 1; . . . ; m;
ni P 0;

i ¼ 1; . . . ; m:

where C is the penalty parameter,n ¼ fn1 ; . . . ; nm g is a slack variable.
The penalty parameter (C) controls the trade-off between the model
complexity and classiﬁcation error. A larger C usually leads to higher training accuracy, but may over-ﬁt the training data. The nonnegative slack variable ðni Þ allows Eq. (3) to always yield feasible
solutions even when the optimal hyperplane does not have the
maximum margin. Uðxi Þ is a non-linear transformation which projects the samples into a higher-dimensional dot-product space
called the feature space.
By introducing the non-negative Lagrangian multiplier ai and bi ,
we convert the constrained problem given by Eq. (3) into an
unconstrained problem as shown in Eq. (4):

Min Lðw; b; n; a; bÞ ¼

1 T
w wþC
2

m
X

Kðxi ; xj Þ ¼ expðckxTi  xj k2 Þ; c > 0;

ni 

m
X

i¼1

ai ðyi ðwT  Uðxi Þ

i¼1

þ bÞ  1 þ ni Þ 

where c in Eq. (8) is a parameter related to the span of an RBF kernel. The smaller the value is, the wider the kernel spans.
2.2. SVM for multiclass classiﬁcation

m
X

ð4Þ

bi ni ;

To extend the application of SVM for multiclass classiﬁcation, a
number of methods have been developed and they mainly fall in
three categories: One-Against-All (OAA), One-Against-One (OAO)
and All-At-Once (AAO). In OAA method, one SVM is trained with
the positive class representing one class and the negative class representing the others. Therefore, it builds n different SVM models
where n is the number of the classes. Given yi 2 f1; . . . ; ng, the
kth SVM solves the following problem:
k

Min Q P ðwk ; b ; nk Þ ¼
s:t:

m
X
1 k T k
nki
ðw Þ ðw Þ þ C
2
i¼1
k

ðwk ÞT  Uðxi Þ þ b P 1  nki ; if yi ¼ k;
k

k T

ðw Þ  Uðxi Þ þ b 6 1 þ
nki

where a ¼ fa1 ; . . . ; am g and b ¼ fb1 ; . . . ; bm g. For the optimal solution of Eq. (4), the Karush–Kuhn–Tucker (KKT) condition is satisﬁed
as follows:

@Q ðw; b; n; a; bÞ
@Qðw; b; n; a; bÞ
@Q ðw; b; n; a; bÞ
¼ 0;
¼ 0 and
¼ 0;
@w
@b
@n
ai ðyi ðwT  Uðxi Þ þ bÞ  1 þ ni Þ ¼ 0; bi ni ¼ 0; ai ¼ 0;
bi ¼ 0 and ni ¼ 0 for i ¼ 1; . . . ; m:
ð5Þ
By substituting the conditions of Eq. (5) into Eq. (4), we can convert
Eq. (4) to the following Lagrangian dual problem:

Max LðaÞ ¼

i¼1

s:t:

m
1X
ai 
ai aj yi yj Uðxi ÞT Uðxj Þ
2 i;j¼1

ð6Þ

Solving Eq. (9) yields a decision function for the kth SVM model:
k
ðwk ÞT  Uðxi Þ þ b . Totally, n decision functions will be derived. The
class of a sample, x, is predicted by the following equation:
k

f ðxÞ ¼ arg max ððwk ÞT  UðxÞ þ b Þ:

The optimal solution ai  for the dual problem determines the

parameters w and b of the following optimal hyperplane

¼ sign

m
X

yi ai Kðx; xi Þ þ b



m
X

!

T

i UðxÞ Uðxi Þ

yi a

þb



i¼1

!
;

ð10Þ

k¼1;...;n

In OAO method, a SVM is trained to classify the kth class and the
lth class. Therefore, it constructs nðn  1Þ=2 SVM models. To build
the SVM model for the kth class and the lth class, the following
problem is solved:
kl

Min Q P ðwkl ; b ; nkl Þ ¼
s:t:

m
X
1 kl T kl
nkl
ðw Þ ðw Þ þ C
i
2
i¼1
kl

ðwkl ÞT  Uðxi Þ þ b P 1  nkl
i ; if yi ¼ k;
kl

kl T

ðw Þ  Uðxi Þ þ b 6 1 þ

nkl
i ;

ð11Þ

if yi ¼ l;

nki P 0; i ¼ 1; . . . ; m and k; l ¼ 1; . . . ; n:
The idea of AAO is similar to that of OAA, but it determines n
decision functions at once where the kth decision function separates the kth class from the other classes. We deﬁne the decision
function for class k by:
k

j

ðwk ÞT  UðxÞ þ b P ðwj ÞT  UðxÞ þ b for k – j; k ¼ 1; . . . ; n:

Min Q ðw; b; nÞ ¼
s:t:

f ðxÞ ¼ signðw  Uðxi Þ þ b Þ ¼ sign

if yi – k;

n
m X
X
1X
ðwk ÞT ðwk Þ þ C
nki
2 k¼1
i¼1 k–y
i

i¼1



ð9Þ

ð12Þ

All the optimal decision functions are solved by the one problem given by:

0 6 ai 6 C; i ¼ 1; . . . ; m:
m
X
ai yi ¼ 0; i ¼ 1; . . . ; m:



nki ;

P 0; i ¼ 1; . . . ; m; and k ¼ 1; . . . ; n:

i¼1

m
X

ð8Þ

ð2Þ

The optimal hyperplane is obtained by solving the following optimization problem:

Min

that we can calculate the inner product, UðxÞT Uðxi Þ, easily. In this
research, Radial Basis Function (RBF) is used which is deﬁned as
follows:

ð7Þ

i¼1

where Kðxi ; xj Þ is a kernel function deﬁned as Kðxi ; xj Þ ¼ Uðxi ÞT Uðxj Þ.
The kernel function performs the non-linear mapping implicitly so

y

k

ðwyi ÞT  Uðxi Þ þ b i P ðwk ÞT  Uðxi Þ þ b þ 2  nki ;

ð13Þ

nki P 0; i ¼ 1; . . . ; m; k – yi ; k ¼ 1; . . . ; n:
Clearly OAA needs to build n classiﬁers and OAO will develop
nðn  1Þ=2 models while AAO requires only one model. Yet, it is relatively difﬁcult to separate the training data for OAA and AAO comparing to OAO (Abe, 2005). Hsu and Lin (2002) compared the
training and testing times and classiﬁcation accuracies of the three
multiclass SVM methods based on various datasets. They reported
that the training time of OAO method is faster than the OAA method and the AAO method. The experimental results on large scale

4958

M.H. Bae et al. / Expert Systems with Applications 37 (2010) 4955–4965

datasets showed that OAO is more efﬁcient for practical use than
OAA and AAO method. It is much more challenging to solve a multiclass problem which has imbalanced and overlapped class without an optimal SVM classiﬁer for multiclass problem.

3. Mix-ratio sampling for SVM
3.1. Class-balancing
To improve the performance of machine learning methods for
the imbalanced class classiﬁcation problem researchers have used
two different approaches: sampling approach and variant classiﬁers approach.
Sampling approach aims to sample training sets with different
ratios for the majority class and the minority class so as to boost
(suppress) the size of the training data from the minority (majority) class. There are different ways to draw samples from the original dataset: random sampling refers to randomly extract the
samples from the original dataset and informative sampling takes
the samples based on different rules speciﬁed for a speciﬁc problem. Other than sampling strategies, sampling approaches can also
be classiﬁed based on the number of samples taken from the
majority and minority classes. Three sampling approaches are
studied in the literature including under-sampling, over-sampling
and combination of over-sampling and under-sampling. Undersampling aims to reduce the size of a majority class. Over-sampling
is an approach used for minority classes by randomly duplicating
the existing data (Japkowicz, 2000) or creating synthetic data (Chawla, Bowyer, Hall, & Kegelmeyer, 2002). It is interesting to note
that Japkowicz (2000) reports that the over-sampling method in
general outperforms the under-sampling from the experimental
results. Still, research has also looked into the combination of under-sampling and over-sampling. Ling and Li (1998) obtain the
best lift index, which is a performance measure, when the sampling points of the majority class and minority class are the same.
Shin and Cho (2006) propose an informative sampling, neighborhood property based pattern selection algorithm (NPPS), to select
only data located around decision boundary because the data have
the most information on determining the decision boundary in
SVM. They can substantially reduce the number of training data
and the training time while maintaining the classiﬁcation
performance.
Other than sampling approach, researchers have studied how to
increase the sensitivity of the SVM classiﬁer to the minority classes. For example, Veropoulos, Campbell, and Cristianini (1999) employ different error costs in the objective function for majority
classes and minority classes. Wu and Chang (2005) propose a kernel-boundary-alignment algorithm (KBA) which adjusts the class
boundary by adaptively modifying the kernel matrix based on
the imbalanced training data distribution.
The approaches explained above have shown their efﬁciency
and effectiveness in dealing with the imbalanced dataset. However, these approaches are designed for binary classes where there
is only one majority class and one minority class. In multiclass
classiﬁcation problem, especially when the number of classes is
large, applying similar sampling procedures designed for the binary class problem is questionable. This is probably due to the fact
that it is difﬁcult to differentiate majority versus minority classes.
Assume there are n classes, the largest class and the smallest class
can be regarded as the majority class and the minority class,
respectively, then the rest n  2 classes can be either majority or
minority. Therefore, there are 2n2 possible cases of dividing the
classes into two groups. Simply applying the over-sampling or under-sampling techniques reviewed above with different sampling
ratios for the 2n2 possible cases is computational expensive and

intractable. Therefore, there is a need of intelligent sampling which
can provide an efﬁcient and effective sampling procedure in the
multiclass problem.
Akbani, Kwek, and Japkowicz (2004) point out that under-sampling of the majority class can hurt the classiﬁcation performance
because it discards valid data points which contain important
information. Therefore, our approach focuses on the over-sampling
procedure. While maintaining the number of the training data
from majority classes the same, our objective is to increase the
number of the training data from minority classes. Now the questions are which classes are the true minority classes, and what
sampling ratios are appropriate to each minority class, respectively. To determine the minority class and the appropriate sampling ratio, one baseline classiﬁcation on the original dataset is
conducted, followed by a number of classiﬁcations on the dataset
generated from different sampling ratios (e.g., 5 K, 10 K data size).
The classiﬁcation performances of each class with different sampling ratios are then compared with the baseline classiﬁcation.
The classes which have better performance from the sampled
training set than the original training set are identiﬁed as the
minority classes and the best sampling ratio among the experiments for the speciﬁc structure is recorded and used for the classiﬁcation in the following steps. On the other hand, the classes with
no performance improvement with different sampling ratios are
treated as majority classes. The original dataset for such classes
are then wholly used in SVM. Therefore, the selected sampling ratio of each minority class varies and it is termed mix-ratio
sampling.
As stated before, the best performance can be achieved when
the number of data from the majority class and the number of data
from the minority class are same (Ling & Li, 1998). Due to the large
size of the dataset and the large number of classes, we decide to
draw the same number of samples from different structures for
the speciﬁc sampling ratio. Thus, it is true that some classes will
be over sampled while others might be under sampled. However,
this will not lessen the classiﬁcation power as the ones under sampled will use the original dataset in the SVM model. The pseudocode of the mix-ratio sampling algorithm is presented in Fig. 1. It
initializes the training set (S) and the set of the sampling ratios
(N) (lines 2–4). A multiclass SVM is then implemented on the original dataset and its classiﬁcation performance for each class is
kept as the baseline classiﬁcation ðMB Þ (lines 5–8). Next, different
SVM models with different sampling ratios are implemented. The
training set (S) is updated by drawing the same number of samples
from each class according to the different ratios (N(i) = 1 k, 5 k,
10 k, 15 k and 20 k) (lines 10–11). Performance matrix M(i,j) is
used to record the classiﬁcation result for class j using SVM model
with sampling ratio i (lines 12–15). Finally, minority classes and
their over-sampling ratios are determined (lines 17–24). For each
class, the best performance (max(M(i,j)) among the different SVM
models i is compared with the baseline classiﬁcation ðMB ðjÞÞ). If
the best performance (max(M(i,j)) for a class is better than the
baseline classiﬁcation ðMB ðjÞÞ), then the class is determined as a
minority class and its sampling ratio (N(i)) is used as its over-sampling ratio. Otherwise, the class is determined as a majority class.
3.2. Class-overlapping
When applying SVM to MR image segmentation, another critical issue is the overlapping of MR signals. Consider a MR image
protocols, T2-weighted. Fig. 2 illustrates the histograms of the
T2-weighted MRM intensities for seven different neuroanatomical
structures in a mouse brain such as Cerebral cortex (CORT), Cerebellum (CBLM), Midbrain (MIDB), Thalamus (THAL), Olfactory bulb
(OLFB), Hippocampus (HC) and Cerebral peduncle (CPU). There are
serious overlaps among the T2-weighted intensities from the dif-

M.H. Bae et al. / Expert Systems with Applications 37 (2010) 4955–4965

4959

Fig. 1. Mix-ratio sampling algorithm.

Fig. 2. Intensity histograms for seven neuroanatomical structures.

ferent structures. Hence, it appears difﬁcult to effectively segment
the neuroanatomical structures of mouse brains using only the MR
intensity information.
To solve the class overlapping problem in MR image segmentation, the spatial information has been used by constructing a probabilistic atlas (Evans, Kamber, Collins, & MacDonald, 1994;
Thompson & Toga, 1996). The goal of constructing a probabilistic
atlas is to create neuroanatomical templates and investigate quan-

titative information on inter-subject variations in neuroanatomical
structures. The probabilistic atlas describes the statistical properties of neuroanatomical structures on each coordinate in MR
images. Fischl et al. (2002) explain how the spatial information
could help solve the class overlapping problem. They claim that
the number of possible neuroanatomical structures at a given atlas
coordinate is relatively small and the relative location of each
structure in the atlas is characteristic (e.g. the substantia nigra

4960

M.H. Bae et al. / Expert Systems with Applications 37 (2010) 4955–4965

(SNR) is anterior to the cerebral peduncle (CPED)). Therefore, information on global position in the atlas could help MR image
segmentation.
Nevertheless, the atlas based segmentation has some drawbacks. Because of the inherent variability of the brains in a population, some structures can be blurred out in the resulting average
brain (Thompson & Toga, 1996). The registration can be biased to
the individual anatomy of the brains in the population because
only a single average brain is used (Sharief, Badea, Dale, & Johnson,
2008). Since the approach depends on the statistical properties of
neuroanatomical structures on each coordinate, the classiﬁcation
performance heavily depends on the accuracy of registration process. The traditional SVM classiﬁer assumes that all the data are
independent of each other. The spatial information is not facilitated by the SVM framework. In our study, we will incorporate
the spatial information into the SVM classiﬁer. We add the x, y
and z coordinates of each data point in 3D MR images as features.
Even though the intensity features from some classes overlap each
other, the three location features from those classes are surely different; therefore, adding the location features can aid in differentiating overlapped classes. Because the spatial information is used as
features, we can easily implement it in any classiﬁer. As discussed
earlier, the probabilistic atlas based segmentations use the spatial
information of absolute position in MR images. Hence, registration
error can cause failure of the segmentation process. In this research, both the MR signal intensity and the voxel coordinates
are used as features, so it is robust to inaccurate registration.
4. Mix-ratio sampling based SVM for mouse brain segmentation
4.1. Dataset
The dataset used in this study was provided by the Center for In
Vivo Microscopy in Duke University Medical Center. This dataset
was also previously studied in Ali et al. (2005). Five formalin-ﬁxed
C57BL/6J male mice of approximately 9 weeks in age were used.
The MRM image acquisition consisted of isotropic 3D T2-weighted
scans. MRM is MRI at a microscopic level with ultra-high spatial
resolution. Typical spatial resolution is about 100 lm per voxel.
The matrix sizes of the images are 128  128  256. A 9-parameter
afﬁne registration was applied to each mouse brain. Manual labeling of 21 neuroanatomaical structures was done by two experts
using T2-weighted datasets of the ﬁve mouse brains. Table 1 presents the list of the 21 neuroanatomaical structures and abbreviations to be segmented in this work. These labelings were used as
true labeling for each voxel.

Table 1
21 segmented structures and abbreviations.
Structures

Abbreviation

Structures

Abbreviation

Cerebral cortex
Cerebral peduncle
Hippocampus
Caudate putamen
Globus pallidus
Internal capsule

CORT
CPED
HC
CPU
GP
ICAP

AC
CBLM
VEN
PON
SNR
INTP

Periacqueductal
gray
Inferior colliculus
Medulla oblongata
Thalamus
Midbrain

PAG

Anterior commissure
Cerebellum
Ventricular system
Pontine nuclei
Substantia nigra
Interpeduncular
nucleus
Olfactory bulb
Optic tract
Trigeminal tract
Corpus callosum

OPT
TRI
CC

INFC
MED
THAL
MIDB

OLFB

ference in spatial location of the two labelings can cause signiﬁcant
decreases in the numerator of VOP i ðLA ; LM Þ, this performance index
is more sensitive to the spatial difference of the two labelings than
the volumetric difference. The volume difference percentage for
class i is used for quantifying the size difference of the structures
delineated by the two segmentations, and it is deﬁned as:

VDPi ðLA ; LM Þ ¼

jVðLA Þ  VðLM Þj
 100%:
ðVðLA Þ þ VðLM ÞÞ=2

ð15Þ

This performance index is the smaller the better. When all the labelings from the two segmentations are identical, VDPi ðLA ; LM Þ is 0.
Usually in the imbalanced classiﬁcation problem, the accuracies
of the large classes are better than those of the small classes.
Therefore, the overall accuracy can be very high even though the
accuracies of the small classes are poor. For example, considering
an imbalanced multiclass classiﬁcation problem which has three
classes, the ratio of the number of data points in each class is
97:2:1. If a classiﬁer predicts all the test data as the majority class,
its accuracy is 97%. Such performance measure is questionable
since it obviously gives more considerations on larger structures.
To resolve this problem, the overall performance should reﬂect
equally on performances of each class regardless its size. This proposed measurements, average_VOP and average_VDP, consider different structures equally, and they are deﬁned as:

Av erage VOPðLA ; LM Þ ¼
Av erage VDPðLA ; LM Þ ¼

Pn

i¼1 VOP i ðLA ; LM Þ

;
n
i¼1 VDP i ðLA ; LM Þ
;
n

Pn

ð16Þ

where n is the number of the neuroanatomical structures.

4.2. Performance evaluation

4.3. SVM models with different sampling strategies

In order to validate the proposed automated segmentation procedure, the volume overlap percentage (VOP) and volume difference percentage (VDP) (Ali et al., 2005; Fischl et al., 2002) are
calculated to compare the automated delineation with the manual
delineation of each structure. Denote LA and LM as labeling of the
structure i by automated segmentation and manual segmentation,
respectively, and V(L) as a function which gives the volume of the
labeling. The volume overlap percentage for class i is deﬁned as:

The experiment is implemented in a sequence of ﬁve steps: (1)
Find the best parameters for SVM models, (2) train different SVM
models using the original training set and the sampled training
sets, (3) compare each class’s performance between the original
training set and the sampled training sets and determine the
minority classes and their sampling ratios, (4) train a SVM model
with the mix-ratio sampling, and (5) test and validate the
performance.

VOPi ðLA ; LM Þ ¼

VðLA \ LM Þ
 100%:
ðVðLA Þ þ VðLM ÞÞ=2

ð14Þ

So, it is the ratio of the volume where automated labeling agrees
with manual labeling and the average volume of automated labeling and manual labeling. This performance index is the larger the
better. When all the labelings from the automated and manual segmentation are coincided, VOP i ðLA ; LM Þ is 100%. Because a slight dif-

4.3.1. Designing SVM classiﬁer
A training set and a testing set are used for ﬁnding the best
parameters for the SVM models which will be used in this study.
The training set consists of the randomly chosen three mice and
the testing set consists of the rest two mice. Because of the limitation of the PC memory, we discarded the blank voxel from the data
set in this study to reduce the size of the data set. The RBF kernel is

4961

M.H. Bae et al. / Expert Systems with Applications 37 (2010) 4955–4965
Table 2
Summary of the 11 SVM models.
Model

Overall
sampling
ratio

Number of
under-sampled
classes

Number of
over-sampled
classes

R-1k and I-1k
R-5k and I-5k
R-10k and I-10k
R-15k and I-15k
R-20k and I-20k
O-SVM

4.55%
22.76%
45.52%
68.30%
91.04%
No sampling

19
12
9
9
8
0

2
9
12
12
13
0

chosen based on our preliminary experiment. The penalty parameter (C) and the RBF parameter ðcÞ should be determined for the
RBF kernel based SVM. The best parameters is determined through
a grid search over the range C = {10, 100, 500, 1000, 5000, 10,000}
and c ¼ f1; 10; 50; 100g. The combination of the C = 1000 and
c ¼ 10 is found to be the best parameter setting. In this study, LibSVM for Matlab (Chang & Lin, 2001) is used.

4.3.2. Investigating SVM performance with various sampling ratios
To ﬁnd better sampling procedure for this problem, we compare
a random sampling and an informative sampling. The informative
sampling used in our study is based on the rationale that the data
located around the boundary have the most information on determining the decision boundary in SVM. We pick only the data points
that have at least one different class’s neighbor among the k-nearest neighbors. The data points which have neighbors from different
class must be located around the boundary.
Table 2 summarizes different SVM models implemented including sampling ratios, the numbers of under-sampled and over-sam-

pled class and the numbers of training data for the different SVM
models. Taking R-1k and I-1k as an example, R-1k model represents the SVM model trained with the combination of random
over-sampling and random under-sampling, and its training set
has randomly sampled 1000 data points from each class. I-1k model indicates the SVM model trained with the training set which has
the sampled 1000 data points from each class with the informative
sampling procedure. The number of data used in the training set of
the R-1k or I-1k model corresponds to 4.55% of the original dataset.
Sampling 1000 data points from each class results in the two oversampled classes and the 19 under-sampled classes. Different sampling ratios will draw 22.76%, 45.52%, 68.30% and 91.04% for 5 k,
10 k, 15 k and 20 k respectively. Since the size of the dataset will
be larger than the original when the sampling ratio is greater than
20 k, considering the computation effort, 20 k is the maximum
sampling ratio used in this study. O-SVM model indicates the
SVM model trained with the original dataset. Using the best
parameters (C = 1000 and c ¼ 10) determined in Step 1, 11 different SVM models are studied (shown in Table 2).
Table 3 shows the testing result which is the average value of
the two testing mouse brain images. First, we can conclude that
the O-SVM model gives the best performance among all SVM models. As discussed previously, under-sampling of majority classes
can cause a negative effect on the performances of those classes.
This result conﬁrms that the negative effect of the under-sampling
on the overall performance is larger than the positive effect of the
over-sampling of minority classes. Secondly, we ﬁnd that random
sampling is more effective than informative sampling in this case,
which implies that information obtained from only the boundary
region is not enough to build the multi-hyperplanes in the imbalanced multiclass problem. This is probably due to the large number
of classes to be classiﬁed and a number of classes have very small

Table 3
The test performances of the eleven different SVM models.

Average_VOP
Average_VDP

R-1k

R-5k

R-10k

R-15k

R-20k

I-1k

I-5k

I-10k

I-15k

I-20k

O-SVM

64.12
29.57

66.23
21.59

66.37
21.03

66.85
20.57

66.66
20.17

54.56
33.64

56.36
30.54

58.12
28.36

57.52
32.56

57.88
29.80

68.55
19.21

Table 4
The test performances of each structure of the SVM models and mix-ratio.
Structure

O-SVM

R-5k

R-10k

R-15k

R-20k

Sampling ratio (%)

VOP

VDP

VOP

VDP

VOP

VDP

VOP

VDP

VOP

VDP

CORT.
CPED
HC
CPU
GP
ICAP
PAG
INFC
MED
THAL
MIDB
AC
CBLM
VEN
PON
SNR
INTP.
OLF.
OPT
TRI
CC

93.21
52.34
84.08
84.21
64.95
58.13
69.62
77.74
89.25
90.53
87.41
31.19
95.28
46.00
69.67
39.35
43.56
90.73
46.76
66.10
59.36

2.64
6.72
4.57
3.27
10.32
22.55
36.78
13.40
13.92
3.94
1.51
20.04
1.52
42.91
17.42
73.45
76.54
6.27
18.04
12.42
15.10

88.42
49.02
82.76
79.30
58.02
56.41
72.04
78.14
88.51
88.30
84.90
26.25
92.28
41.61
65.43
46.69
45.91
88.37
42.67
63.41
52.47

11.36
13.00
4.28
16.02
23.01
21.43
13.00
19.26
5.93
1.22
8.13
52.85
3.89
29.54
10.67
33.94
58.16
6.29
34.22
24.34
62.97

90.36
46.24
81.99
80.04
59.00
54.97
74.58
78.82
88.01
88.66
84.61
28.70
93.68
44.04
65.80
40.69
43.19
89.81
42.82
62.77
54.95

6.37
23.41
4.60
10.43
14.73
37.51
3.28
10.53
12.58
0.66
9.33
55.45
2.78
15.93
5.37
48.61
64.87
5.34
35.47
29.37
45.02

91.25
49.11
82.67
81.42
63.10
53.94
70.33
78.98
88.48
89.72
86.06
27.32
94.68
45.46
65.47
39.12
41.48
91.08
42.99
65.30
55.83

5.33
13.19
6.74
5.63
21.55
35.38
11.53
8.81
10.03
1.03
3.48
62.23
3.63
7.64
4.83
46.65
67.84
5.90
43.24
26.20
41.10

91.34
48.36
82.50
81.06
61.16
54.13
68.29
79.29
88.50
89.67
85.35
27.74
93.91
44.30
65.83
48.20
36.72
90.78
40.93
64.84
56.88

4.39
13.72
6.36
3.54
25.31
33.89
6.40
19.38
10.37
0.52
6.06
68.30
2.54
14.17
4.90
23.93
81.28
7.24
33.94
21.35
36.08

Average

68.55

19.21

66.23

21.59

66.37

21.03

66.85

20.57

66.66

20.17

Proposed approach
VOP

VDP

–
–
–
–
–
200
300
300
–
–
–
–
–
200
1300
1200
1300
–
–
–
–

93.17
49.23
84.09
84.21
64.96
57.90
74.68
78.25
89.37
90.31
87.17
32.42
95.06
49.85
65.91
44.54
45.28
90.77
46.60
65.96
59.28

2.16
14.46
3.59
3.38
10.32
23.04
6.83
2.78
12.89
1.46
2.57
11.44
3.00
26.21
4.52
42.19
47.20
6.17
17.75
14.22
15.11

–

69.00

12.92

4962

M.H. Bae et al. / Expert Systems with Applications 37 (2010) 4955–4965

sizes. Informative sampling tends to emphasize the larger classes
which have larger boundary and will therefore, discount the
importance of smaller classes.

sampled data on multiclass classiﬁcation problem, we further compared the two classiﬁers, SVM and ANN. ANN has already been
used for several brain segmentation studies (Magnotta et al.,
1999; Powell et al., 2008). In those studies, only small number of
structures (<9) were segmented. Therefore, we want to compare
the two classiﬁers when they are used for the large number of
structures and the variation of structure sizes is extremely large.
Finally, we present the test result of 5-fold cross validation. All
the test results presented in Section 4 are based on the ﬁxed training set and testing set; while by the cross validation implemented
in this section we can discuss the generalization ability of our proposed segmentation method.

4.3.3. Identifying minority classes and optimal sampling ratios
Once SVM models with different sampling ratios are implemented, the minority classes and their over-sampling ratios can
be determined by comparing each class’s performance between
the O-SVM model and the R-SVM or I-SVM models. If the performance of O-SVM model is better than that of R-SVM and I-SVM
models, the class is determined as a majority class; otherwise, it
is a minority class with the appropriate sampling ratio is set to
the model with the best performance.
Since Table 3 indicates that most R-SVM models (R-5k, R-10k,
R-15k and R-20k) in general outperform the I-SVM models in this
experiment. Therefore, the comparison experiments between the
four R-SVM models and O-SVM are conducted with the results
summarized in Table 4. The seven structures, ICAP, PAG, INFC,
VEN, PON, SNR and INTP, have better performance from the RSVM models. In the PAG structures, the R-10k model gives the best
performance. The R-10k model has 10,000 data points from the
PAG structure and the number of points of PAG in a mouse in the
training set is 3648. Hence, the R-10k model over-sampled from
the PAG structures with the ratio of about 274% (=10,000/3648),
rounded to 300%. Same calculation applies to the other six structures. The sampling ratios for SVM are highlighted in Table 4 and
the test performance of the mix-ratio sampling – based SVM model
is presented in the last column. Clearly, we obtain better class performances from the most of the seven structures by over-sampling
from the structures. That is, for the PAG structure, the class performance (VOP, VDP) is improved from (69.62, 36.78) of O-SVM to
(74.68, 6.83) of mix-ratio sampling – based SVM. For the INFC
structure, the improvement is made from (77.74, 13.40) to
(78.25, 2.78). In addition to the increase of the class performances
of the over-sampled structures, the class performances of the nonsampled classes, the majority classes, remained almost same. For
example, for the CORT structure, the class performance remained
almost same from (93.21, 2.64) to (93.17, 2.16). Thus, the overall
performance (Average_VOP, Average_VDP) is improved from
(68.55, 19.21) to (69.00, 12.92). This result shows that proposed
approach is effective to the imbalanced multiclass problem. In Table 4, the best performed model for each structure is highlighted,
which is then later used for training the SVM model. The result
in Table 4 is based on the experiments with the training set and
the testing set which is described in Section 4.3.1.

To show the better performance of the mix-ratio sampling over
the simple over-sampling we compared the test performance of
the mix-ratio sampling to those of the original data set and the
simple over-sampling as seen in Table 5. ‘No sampling’ in the second column in Table 5 means that the training set used for that
experiment is the original data set without any sampling. The simple over-sampling means that the seven minority classes are oversampled by a same sampling ratio. There are three different simple
over-sampled training sets which have the over-sampled minority
classes by the ratios (300%, 500% and 1000%). The third row of Table 5, ‘+/’, presents the margin of the test performance of the original training set and the different sampling training sets. Even
though there is little improvement in the VOP from the sampling
methods, the mix-ratio sampling shows the best VOP and VDP
defeating all the simple over-sampling. This means that there exist
different sampling ratios for the different classes which can improve the classiﬁcation performance of each class. There is a little
surprising result that the performance of the 500% simple oversampling is better than that of the 1000% simple over-sampling.
This means that increasing the number of data of the minority classes with a same ratio does not guarantee the aid in the classiﬁcation of the minority classes. The last row of Table 5 presents the
number of data of each training set. The number of data of the
mix-ratio sampling is similar to that of the 300% simple over-sampling and less than those of the 500% and 1000% simple over-sampling. That means the mix-ratio sampling gives better performance
with less training data than the simple over-sampling. The results
presented in Table 5 tell us the mix-ratio sampling is more effective and efﬁcient for the multiclass problem than the simple
over-sampling.

5. Results

5.2. Comparison with the artiﬁcial neural network (ANN) classiﬁer

In the previous section, we described how to determine minority classes that need to be over-sampled and their sampling ratios,
and developed a mix-ratio sampling SVM method. In this section,
we present some comparison experiments to show the proposed
method outperform existing methods. First, we compared the proposed sampling method, the mix-ratio sampling method, with the
simple over-sampling method to show the effectiveness and efﬁciency of the SVM on mix-ratio sampled data for imbalanced dataset. Second, to demonstrate the applicability of SVM on mix-ratio

We compared the multiclass classiﬁcation accuracy of SVM for
mouse brain images with an ANN by testing the two classiﬁers
to the same dataset which is sampled by the mix-ratio sampling
method. ANN has been successfully used for the brain image segmentation (Magnotta et al., 1999; Powell et al., 2008). We implemented OAA multiclass classiﬁcation which builds 21 ANN
models. To ﬁnd proper model parameters and architecture which
yield better classiﬁcation performance, several different ANN models were tried. The ANN models used in this research has feedfor-

5.1. Comparison with other sampling procedure

Table 5
The performance comparison of the mix-ratio sampling to the simple over-sampling.

Average VOP and VDP
+/
Number of data

No sampling

300% simple over-sampling

500% simple over-sampling

1000% simple over-sampling

Mix-ratio sampling

68.55
19.21
0.00%
0.00%
1,449,936

68.74
0.29%
1,576,642

68.77
0.33%
1,703,348

68.45
0.14%
2,020,113

69.00
0.66%
1,589,026

14.78
23.02%

13.70
28.65%

15.00
21.91%

12.92
32.73%

4963

M.H. Bae et al. / Expert Systems with Applications 37 (2010) 4955–4965
Table 6
The performance comparison of SVM and ANN (on mix-ratio sampled data set).
CORT.

CPED

HC

CPU

GP

ICAP

PAG

INFC

MED

THAL

SVM

93.17
2.16

49.23
14.46

84.09
3.59

84.21
3.38

64.96
10.32

57.90
23.04

74.68
6.83

78.25
2.78

89.37
12.89

90.31
1.46

87.17
2.57

ANN

88.95
3.81

23.74
53.12

74.22
18.76

79.88
6.33

64.55
4.89

60.36
11.25

77.83
5.09

81.45
13.79

84.86
27.00

90.72
4.65

84.97
4.05

AC

CBLM

VEN

PON

SNR

INTP.

OLFB.

OPT

TRI

SVM

32.42
11.44

95.06
3.00

49.85
26.21

65.91
4.52

44.54
42.19

45.28
47.20

90.77
6.17

46.60
17.75

65.96
14.22

59.28
15.11

69.00
12.92

ANN

32.17
15.56

93.59
1.75

31.33
9.36

59.04
53.20

65.06
52.30

69.25
51.69

88.36
17.97

52.45
20.59

61.09
42.35

16.03
113.53

65.71
25.29

CC

MIDB

Average

Table 7
Test result of the mix-ratio sampling with 5-fold cross-validation.

Volume
VOP
VDP

Volume
VOP
VDP

CORT.

CPED

HC

CPU

GP

ICAP

PAG

INFC

MED

THAL

37.34%
94.18
1.27

0.55%
65.09
9.15

6.02%
86.47
5.02

5.07%
86.78
4.16

0.47%
75.81
9.20

0.61%
70.62
5.94

0.71%
82.04
5.12

1.13%
84.98
4.47

4.72%
89.88
11.29

6.91%
92.83
3.36

AC

CBLM

VEN

PON

SNR

INTP.

OLFB.

OPT

TRI

CC

Average

0.37%
32.18
58.39

12.24%
96.41
1.40

1.89%
59.44
24.35

0.20%
74.21
20.23

0.33%
65.86
12.20

0.11%
55.82
74.51

6.03%
92.08
8.07

0.52%
62.64
17.01

1.11%
72.54
13.80

3.12%
62.09
21.82

100%
75.84
14.97

ward backpropagation networks with two hidden layers which
have ﬁve and three hidden nodes, respectively. Both hidden layers
used hyperbolic tangent sigmoid function as their transfer functions. The models were trained with a learning rate of 0.05,
momentum constant of 0.95. Table 6 presents the test performance
comparison between SVM and ANN. The upper rows show VOP of
each structure and the lower rows shows VDP. In the last column,
the average VOP and VDP of the two classiﬁers are presented.
Based on the average VOP and VDP, SVM outperform ANN by
5.01% and 48.91%, respectively.
5.3. Testing the mix-ratio sampling with cross-validation
Based on the minority classes and their sampling ratios determined by the mix-ratio sampling procedure, we tested the procedure with 5-fold cross-validation to show the generalization
ability of the procedure. Table 7 presents the test result of the
mix-ratio sampling based SVM using 5-fold cross-validation. All
the results in Table 7 is the average value of testing ﬁve mice with
ﬁve different training sets which have different four mice in each
training set. The upper rows in the table show the percentages of
each structure’s volume within a brain. The middle and bottom
rows show VOP and VDP of the 21 structures. The SVM method
based the mix-ratio sampling showed good segmentation performance for the larger structures, which take more than 5% of the
whole brain, such as CORT, HC, CPU, THAL, MIDB AND OLFB. The
VOP’s for all the larger structures were more than 85 and the VDP’s
for all the larger classes except OLFB were less than 5. That means
that the automated segmentation method works exceedingly well
for larger structures. Since the objective of the mix-ratio sampling
procedure is to cure the imbalance problem of multiclass dataset,
the test performance of the smaller classes is more important.
We consider the structures whose relative volumes are less than
1.5% as smaller structures. CPED, GP, ICAP, PAG, INFC, AC, VEN,
PON, SNR, INTP, OPT and TRI are considered as the smaller structures in this study. The proposed method works well for most of
the smaller structures; the segmentation performance for most of
the smaller structures except AC, VEN and INTP are (VOP > 65,

MIDB
8.40%
90.71
3.59

VDP < 20). The two smaller structures, AC and INTP, could not get
satisfactory results. The INTP structure is the smallest and the AC
structure is the third smallest structure. Because of their extremely
small volume in a brain, the segmentation method could not give
good performance for these structures. However, most of the smaller structures were well segmented by the proposed segmentation
method. Based on the test result of the cross-validation, the mixratio sampling based SVM can be an automated method for brain
image segmentation. Fig. 3 illustrates VOP (top) and VDP (bottom)
of the 21 mouse brain structures. The automated segmented and
manual segmented images of the slices at two speciﬁc coronal levels in a mouse brain are displayed in Figs. 4 and 5.
6. Conclusion and future work
In this study, we proposed a new sampling procedure, mix-ratio
sampling, which can handle the imbalanced problem of multiclass
classiﬁcation. In binary classiﬁcation problem, large number of
studies had proposed different sampling procedures to handle
the imbalance problem. However, applying the existing sampling
techniques to the multiclass class problems is computational
expensive and intractable when they have large number of classes.
The new sampling procedure proposed in this paper introduced a
new way to apply sampling procedures to imbalanced problem
which have large number of classes. The mix-ratio sampling procedure can determine minority classes which are hard to be differentiated by classiﬁers and need to be over-sampled. The proposed
procedure considers classes whose classiﬁcation performance can
be improved by over-sampling as minority classes. It also tells
the sampling ratios of the minority classes. The classiﬁcation performance of the proposed procedure was compared with the simple over-sampling procedure which is widely used for the
imbalanced problem. The comparison result showed that the
mix-ratio sampling procedure can alleviate the multiclass imbalanced problem more effectively and efﬁciently.
To show the applicability of the mix-ratio sampling procedure
to an imbalanced multiclass problem, we applied the procedure
to the mouse brain segmentation problem. The segmentation prob-

4964

M.H. Bae et al. / Expert Systems with Applications 37 (2010) 4955–4965

Fig. 3. The test performance of the mix-ratio sampling with 5-fold cross-validation.

Fig. 4. Images from a slice of 3D MRM. Left: automated segmentation and right: manual segmentation.

Fig. 5. Images from a slice of 3D MRM. Left: automated segmentation and right: manual segmentation.

lem is to classify each voxel in 3D MRM images of mouse brain into
21 neuroanatomical structures. We built the SVM model based on
the mix-ratio sampling. The comparison with ANN showed that

SVM yielded better classiﬁcation performance than ANN for this
dataset. The test result which is based on 5-fold cross-validation
showed that the enhanced SVM method based on the mix-ratio

M.H. Bae et al. / Expert Systems with Applications 37 (2010) 4955–4965

sampling to automate the segmentation of the MRM images of
mouse brains segmented well all the structures. Most of the small
structures were segmented well from the larger structures in spite
of the small sizes.
In an image segmentation problem, contextual information of a
voxel can aid in segmentation. The contextual information is obtained from the dependency between the voxel and its neighborhood. Markov Random Field (MRF) theory is a class of probability
theory for modeling the spatial or contextual dependencies of
physical phenomena. Therefore, MRF has been widely studied for
image segmentation. It has been used for brain image segmentation by modeling probabilistic distribution of the labeling of a voxel jointly with the consideration of the labels of a neighborhood of
the voxel (Ali et al., 2005; Fischl et al., 2002). If the MR intensity
information modeled by the SVM model and the contextual information modeled by MRF can be integrated into a segmentation
method, it can produce a more robust and more accurate segmentation of mouse brain MR images. That kind of approach will be
studied in our future research.
References
Abe, S. (2005). Support vector machines for pattern classiﬁcation (advances in pattern
recognition). Secaucus, NJ: Springer-Verlag, New York, Inc..
Akbani, R., Kwek, S., & Japkowicz, N. (2004). Applying support vector machines to
imbalanced datasets. Lecture Notes in Computer Science, 3201, 39–50.
Ali, A. A., Dale, A. M., Badea, A., & Johnson, G. A. (2005). Automated segmentation of
neuroanatomical structures in multispectral MR microscopy of the mouse
brain. NeuroImage, 27(2), 425–435.
Busatto, G. F., Garridob, G. E. J., Almeidac, O. P., Castrod, C. C., Camargoa, C. H. P.,
Cida, C. G., et al. (2003). A voxel-based morphometry study of temporal lobe
gray matter reductions in Alzheimer’s disease. Neurobiology of Aging, 24(2),
221–231.
Chang, C., & Lin, C. (2001). LIBSVM : A library for support vector machines. Software
available at <http://www.csie.ntu.edu.tw/~cjlin/libsvm>.
Chaplot, S., Patnaik, L. M., & Jagannathan, N. R. (2006). Classiﬁcation of magnetic
resonance brain images using wavelets as input to support vector machine and
neural network. Biomedical Signal Processing and Control, 1, 86–92.
Chawla, N. V., Bowyer, K. W., Hall, L. O., & Kegelmeyer, W. P. (2002). SMOTE:
Synthetic minority oversampling technique. Journal of Artiﬁcial Intelligence
Research, 16, 321–357.
Evans, A. C., Kamber, M., Collins, D. L., & MacDonald, D. (1994). An MRI-based
probabilistic atlas of neuroanatomy. Magnetic resonance scanning and epilepsy.
New York: Plenum Press. pp. 263–274.
Fischl, B., Salat, D. H., Busa, E., Albert, M., Dieterich, M., Haselgrove, C., et al. (2002).
Whole brain segmentation: Automated labeling of neuroanatomical structures
in the human brain. Neuron, 33, 341–355.
Hartmann, S. L., Parks, M. H., Martin, P. R., & Dawant, B. M. (1999). Automatic 3D
segmentation of internal structures of the head in MR images using a

4965

combination of similarity and free-form transformations: Part I, validation on
severely atrophied brains. IEEE Transactions on Medical Imaging, 18, 917–926.
Heckemann, R. A., Hajnal, J. V., Aljabar, P., Rueckert, D., & Hammers, A. (2006).
Automatic anatomical brain MRI segmentation combining label propagation
and decision fusion. NeuroImage, 33, 115–126.
Hsu, C. W., & Lin, C. J. (2002). A comparison of methods for multi-class support
vector machines. IEEE Transactions on Neural Networks, 13, 415–425.
Iosifescu, D. V., Shenton, M. E., Warﬁeld, S. K., Kikinis, R., Dengler, J., Jolesz, F. A., et al.
(1997). An automated registration algorithm for measuring MRI subcortical
brain structures. NeuroImage, 6, 1, 13–2.
Japkowicz, N. (2000). The class imbalance problem: Signiﬁcance and strategies. In
Proceedings of the 2000 international conference on artiﬁcial intelligence (ICAI’2000): Special track on inductive learning, Las Vegas, Nevada.
Li, C. (2007). Classifying imbalanced data using a bagging ensemble variation (BEV).
In Proceedings of the 45th annual southeast regional conference (pp. 203–208).
Ling, C., & Li, C. (1998). Data mining for direct marketing problems and solutions. In
Proceedings of the fourth international conference on knowledge discovery and data
mining (KDD-98). New York: AAAI Press.
Luts, J., Heerschap, A., Suykens, J. A. K., & Huffel, S. V. (2007). A combined MRI and
MRSI based multiclass system for brain tumour recognition using LS-SVMs with
class probabilities and feature selection. Artiﬁcial Intelligence in Medicine, 40,
87–102.
Magnotta, V. A., Heckel, D., Andreasen, N. C., Cizadlo, T., Corson, P. W., Ehrhardt, J. C.,
et al. (1999). Measurement of brain structures with artiﬁcial neural networks:
Two- and three-dimensional applications. Radiology, 211(3), 781–790.
Mourao-Miranda, J., Bokde, A. L. W., Born, C., Hampel, H., & Stetter, M. (2005).
Classifying brain states and determining the discriminating activation patterns:
Support vector machine on functional MRI data. NeuroImage, 28, 980–995.
Nattkempera, T. W., Arnricha, B., Lichtea, O., Timma, W., Degenhardb, A., Pointonc,
L., et al. (2005). Evaluation of radiological features for breast tumour
classiﬁcation in clinical screening with machine learning methods. Artiﬁcial
Intelligence in Medicine, 34, 129–139.
Powell, S., Magnotta, V. A., Johnson, H., Jammalamadaka, V. K., Pierson, R., &
Andreasen, N. C. (2008). Registration and machine learning-based automated
segmentation of subcortical and cerebellar brain structures. NeuroImage, 39,
238–247.
Reddick, W. E., Glass, J. O., Cook, E. N., Elkin, T. D., & Deaton, R. J. (1997). Automated
segmentation and classiﬁcation of multispectral magnetic resonance images of
brain using artiﬁcial neural networks. IEEE Transactions on Medical Imaging, 16,
6, 911–91.
Sharief, A. A., Badea, A., Dale, A. M., & Johnson, G. A. (2008). Automated
segmentation of the actively stained mouse brain using multi-spectral MR
microscopy. NeuroImage, 39, 136–145.
Shin, H., & Cho, S. (2006). Response modeling with support vector machines. Expert
Systems with Applications, 30, 746–760.
Thompson, P. M., & Toga, A. W. (1996). A surface-based technique for warping 3dimensional images of the brain. IEEE Transactions on Medical Imaging, 15, 1–16.
Veropoulos, K., Campbell, C., & Cristianini, N. (1999). Controlling the sensitivity of
support vector machines. In Proceedings of the international joint conference on AI
(pp. 55–60).
Wu, G., & Chang, E. Y. (2005). KBA: Kernel boundary alignment considering
imbalanced data distribution. IEEE Transactions on Knowledge and Data
Engineering, 17(6), 786–795.

COVER FEATURE

Search
on the
Semantic Web
To help human users and software agents find relevant knowledge on
the Semantic Web, the Swoogle search engine discovers, indexes, and
analyzes the ontologies and facts that are encoded in Semantic Web
documents.

Li Ding
Tim Finin
Anupam
Joshi
Yun Peng
Rong Pan
Pavan
Reddivari
University of
Maryland,
Baltimore
County

62

S

earch engines have assumed a central role
in the World Wide Web’s infrastructure as
its scale and impact have increased. In the
Web’s earliest days, people found pages of
interest by navigating (quickly dubbed
surfing) from pages whose locations they remembered or bookmarked. Rapid growth in the number of pages gave rise to Web directories like Yahoo
that manually organized Web pages into a hierarchy of topics.
As the growth continued, these directories were
augmented by search engines such as Lycos, HotBot,
and AltaVista, which automatically discovered new
and modified Web pages, added them to databases
and indexed them by their keywords and features.
Today, search engines such as Google and Yahoo
dominate the Web’s infrastructure and largely define
our Web experience.
Most knowledge on the Web is presented as
natural-language text with occasional pictures and
graphics. This is convenient for human users to read
and view but difficult for computers to understand.
It also limits the indexing capabilities of state-of-theart search engines, since they cannot infer meaning—
for example, does an occurrence of the word “raven”
refer to the bird or to Baltimore’s football team?
Thus, users share a significant burden in terms of
constructing search queries intelligently. Even with
increased use of XML-encoded information, computers still must use application-dependent semantics to process the tags and literal symbols.

Computer

SEMANTIC WEB SEARCH
The Semantic Web offers an approach in which
computers can use symbols with well-defined,
machine-interpretable semantics to share knowledge.1 Search on the Semantic Web differs from conventional Web search for several reasons.
First, Semantic Web knowledge content is intended
for publication by machines for machines—tools,
Web services, software agents, information systems,
and so forth. Although Semantic Web annotations
and markup can help users find human-readable documents, there will likely be an “agent layer” between
human users and Semantic Web search engines.
Second, knowledge encoded in Semantic Web
languages such as the Resource Description
Framework (RDF)2 differs from both the largely
unstructured free text found on most Web pages
and the highly structured information found in
databases. Such semistructured information
requires using a combination of techniques for
effective indexing and retrieval. RDF, RDF Schema
(RDFS),3 and the Web Ontology Language (OWL)4
introduce semantic features beyond those used in
ordinary XML, allowing users to define terms (for
example, classes and properties), express relationships among them, and assert constraints and
axioms that hold for well-formed data.
Third, even within a single document, Semantic
Web documents (SWDs) can be a mixture of concrete facts, class and property definitions, logic constraints, and metadata. Fully understanding the

Published by the IEEE Computer Society

0018-9162/05/$20.00 © 2005 IEEE

document can require substantial reasoning, so
developers must face the design issue of how much
reasoning search engines can do and when they
should do it. This reasoning produces additional
facts, constraints, and metadata that may also need
to be indexed, potentially along with the supporting justifications. Conventional search engines do
not try to understand document content because
the task is just too difficult and requires more
research on text understanding.
Finally, the graph structure of a collection of
online SWDs differs significantly from the structure that emerges from a collection of HTML documents. This difference influences both the
development of effective strategies for automatically discovering online Semantic Web documents
and the establishment of appropriate metrics for
ranking their importance.
Rather than using one uniform crawling technique to discover SWDs, Swoogle, a Semantic Web
search engine developed by the eBiquity group at
UMBC,5 employs a fourfold strategy:
• running metasearches on conventional Web
search engines, such as Google, to find candidates;
• using a focused Web crawler to traverse directories in which SWDs have been found;
• harvesting URLs when processing discovered
SWDs; and
• collecting URLs of SWDs and directories containing SWDs that users have submitted.
To help human users and software agents find
relevant knowledge on the Semantic Web, Swoogle
discovers, indexes, and analyzes the ontologies and
facts that are encoded in SWDs.

THE SEMANTIC WEB FRAMEWORK
The Semantic Web is a framework that allows
computers to publish, share, and reuse data and
knowledge on the Web and across application,
enterprise, and community boundaries. It is a collaborative effort led by the World Wide Web
Consortium based on the layered set of standards.
Figure 1 shows a simple Semantic Web document
encoded using the RDF/XML syntax.6
Line 1 declares that this is an XML document.
Lines 2-4 further define the content to be an RDF
document and provide abbreviations for three common “namespaces” for RDF, OWL, and Friend of
a Friend (FOAF), which define classes and properties for describing people, their common attributes,
and relations among them. The SWD’s vocabulary

1: <?xmlversion="1.0"encoding="utf-8"?>
2: <rdf:RDFxmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
3:
xmlns:owl="http://www.w3.org/2002/07/owl#"
4:
xmlns:foaf="http://xmlns.com/foaf/0.1/">
5:
<foaf:Person>
6:
<foaf:name>LiDing</foaf:name>
7:
<foaf:mbox rdf:resource="mailto:dingli1@umbc.edu"/>
8:
<owl:sameAsrdf:resource="http://www.csee.umbc.edu/~dingli1/foaf.rdf#dingli"/>
9:
</foaf:Person>
10: </rdf:RDF>

Figure 1. An example Semantic Web document written in RDF/XML. The document
is available at http://ebiquity.umbc.edu/get/a/resource/134.rdf.

foaf:name

foaf:person
rdf:type

Li Ding

foaf:mbox
owl:same As

mailto:dingli1@umbc.edu

http://www.csee.umbc.edu/~dingli1/foaf.rdf#dingli

Figure 2. The RDF graph of the foaf:Person instance.

consists of literals (“Li Ding” in line 6), URI-based
resources (mailto:dingli1@umbc.edu in line 7), and
anonymous resources (lines 5-9). Users assert statements using RDF triples such as the one in line 6,
which asserts of the anonymous foaf:Person subject introduced by Line 5 that it has a foaf:name
property with the value “LiDing.”
A higher level of granularity is class-instance,
which is supported by the object-oriented ontology
constructs in RDFS. Figure 2 is an RDF graph
stating that there is an instance of a foaf:Person
having foaf:name “Li Ding,” foaf:mbox mailto:
dingli1@umbc.edu, and this instance is owl:sameAs
another class-instance identified by http://www.csee.
umbc.edu/~dingli1/foaf.rdf#dingli.
The Semantic Web can be thought of as a collection of loosely federated databases that separates
physical Web storage (realized by online SWDs)
from the logical representation (conveyed by the
RDF graph model). In this view, the Semantic Web
represents a large, universal RDF graph whose
parts are physically serialized by SWDs distributed
across the Web. However, the formal semantics
associated with Semantic Web languages support
generating new facts from existing ones, while conventional databases only enumerate all facts.

SEARCH ENGINE TASKS
Search engines for both the conventional Web
and the Semantic Web involve the same set of highlevel tasks: discovering and revisiting online documents, processing users’ queries, and ordering
search results. Their details diverge, however, due
to differences in the distribution of SWDs and the
semantics of their content.
October 2005

63

Universal RDF graph
RDF document
Literal
Resource

Class-instance
Molecule

The Semantic Web
(about 10M documents)
Physically hosting knowledge
(about 100 triples per SWD average)
Triples modifying the same subject
Finest lossless set of triples

Triple
Atomic knowledge block

Figure 3. Semantic Web granularity. The granularity levels range from the
universal graph comprising all RDF data on the Web to individual triples and
their constituent resources and literals.

Discovering and revisiting documents
Conventional search engines either scan all possible IP addresses or employ crawlers to discover
new Web documents. A typical crawler starts from
a set of seed URLs, visits documents, and traverses
the Web by following the hyperlinks found in the
visited documents. The fact that the Web forms a
well-connected graph and that people can manually submit new URLs make this an effective
process.
A Semantic Web crawler must deal with several
problems. SWDs are needles in the Web’s haystack,
so an exhaustive crawl of the Web is not an efficient approach. Moreover, an SWD graph is not
yet as dense and well-connected as a conventional
Web page graph, so starting with a few seeds is
unlikely to yield many SWDs.
One approach is for a Semantic Web crawler to
use conventional search engines to discover a large
number of potential seed SWDs. These then need to
be validated by a semantic parser. Finally, many of
the URLs found in an SWD point to documents
that are not SWDs, so heuristics to limit and prune
candidate links are beneficial.
For the most part, the issue of how often to revisit
documents to monitor changes is the same for both
the conventional Web and the Semantic Web.
However, modifying an SWD can have far-reaching
effects if class or property definitions used by other
documents are changed. Depending on the nature
and amount of reasoning done when analyzing and
indexing documents, updating an SWD can trigger
significant work for a Semantic Web search engine.

Query processing
A search engine’s core task is processing queries
against the data it has indexed. This can be broken
down into three issues: What should be returned
as query results? Over what data should queries be
run? What constraints can be used in a query? As
Figure 3 shows, the Semantic Web can aggregate
data at several levels of granularity, ranging from
the universal graph of all RDF data on the Web to
64

Computer

a single RDF triple and the term URIs it comprises.
Unlike the Web, where users usually search knowledge at the document granularity, the Semantic
Web can be queried at various levels of granularity,
including the following:
• RDF database search. At the unified RDF
graph level, the Semantic Web is essentially a
simple database of triples, and search is done
by processing semistructured query languages
like RDQL and SPARQL.
• Semantic Web document search. We often
want to query the Semantic Web to find relevant SWDs. This helps users filter out huge
amounts of irrelevant Semantic Web knowledge and promotes the emergence of consensus
ontologies, which define common terms for
sharing and reusing knowledge. In comparison with Web search, the query results are also
document URLs, but the query constraints are
not simply keywords.
• RDF subgraph search. While RDF triples are
physically grouped by SWDs, they can be also
logically grouped by named graph,7 resource
description (that is, a collection of triples with
a common subject), or an RDF molecule.8
Search at this level is a refinement of the above
two searches.
• Semantic Web vocabulary search. At the URI
level, Semantic Web vocabulary terms (that
is, URIrefs) are analogous to words in natural language. Like dictionary lookup,
searching appropriate terms for a concept is
critical to query composition in all the above
searches.
The metadata for a Semantic Web document
should include metadata about itself (such as document URL and last-modified time) and its content
(such as terms being defined or populated and
ontology documents being imported).

Ranking
Google was the first search engine to order its
search results based in part on a Web page’s “popularity” as computed from the Web’s graph structure. This idea has turned out to be enormously
useful in practice and is equally applicable to
Semantic Web search engines.
However, Google’s PageRank9 algorithm for
ranking Web pages cannot be directly used in the
Semantic Web for several reasons. While all HTML
links are essentially the same, Semantic Web links
come in many varieties, each with semantics that

SWOOGLE SEMANTIC WEB DISCOVERY
Swoogle currently uses Google to find a large
number of initial “seed” documents that are likely
to be SWDs. Other seeds come from user submissions. Since SWDs typically use special file extensions such as .rdf or .owl, Swoogle queries for files
with such extensions. The extensions are dynamically selected (an extension is selected if more than
10 SWDs have used it and it has at least 50 percent
accuracy in classifying SWDs).
Since Google returns at most 1,000 results for
any query, Swoogle takes advantage of its feature
that restricts a search to results from a specified
domain or site. Site queries work because of the
locality hypothesis—a Web site hosting more than
two SWDs is likely to have more.
Swoogle uses the Jena2 parser (www.hpl.hp.com/
semweb/jena2.htm) to validate that the files Google
returns are SWDs. Once it has discovered an SWD,
Swoogle uses a simple focused crawler to explore
the Web environment around it to find more. After
filtering out the non-SWDs from the results,
Swoogle extracts a list of the sites on which the
SWDs were found and uses them as seeds for further crawls as well.

Discovery results
In 2002, Andreas Eberhard reported 1,479
SWDs with about 255,000 triples out of nearly 3 ×
106 Web pages.10 As of July 2005, Swoogle had
found more than 5 × 105 SWDs with more than 7
× 107 triples. Although this number is far less than
Google’s 8 × 109 Web pages, it represents a nontrivial collection of Semantic Web data.11
Figure 4 plots a power law distribution of lastmodified time of SWDs (swd curve), which demonstrates that the Semantic Web is either experiencing
a rapid growth rate or, at the very least, is being
actively maintained.
The apparent growth in the number of ontology
documents (onto curve) is somewhat biased by the
SWDs using the Inference Web namespaces (for
example, http://inferenceweb.stanford.edu/2004/
07/iw.owl), which are intended to be instance documents but, unfortunately, include many unnecessary class/property definitions. After removing such
documents, the distribution (onto* curve) ends

Number of SWDs last modified by month t

may affect a ranking algorithm. Moreover, conventional search engines only rank Web pages,
whereas a Semantic Web search engine must rank
SWDs as well as RDF graphs, triples, and terms.
Each admits different ranking models and algorithms.

1,000,000
100,000

swd
onto
onto*

10,000
1,000
100
10
1
Jan. Jan. Jan. Jan. Jan. Jan. Jan. Jan. Jan. Jan. Jan. Jan.
95 96 97 98 99 00 01 02 03 04 05 06
t (month, year)

Figure 4. The number of SWDs and ontologies last modified by month t.

with a much flatter tail. This in part indicates a
trend away from ontology development to populating and reusing ontologies.

SWOOGLE SEMANTIC WEB SEARCH
Swoogle concentrates on Semantic Web document and vocabulary searches, which emphasize
the Web aspects of the Semantic Web. It differs
from an RDF database or RDF subgraph search
in that it maintains compact metadata about documents and terms without recording all encountered triples. In addition to the conventional
metadata obtained without semantic parsing,
Swoogle indexes the semantic content of and relations among SWDs and terms.
To find SWDs, Swoogle supports constraints on
the following metadata:
• Document level metadata. For example, users
can search for SWDs using .rdf as the file
extension.
• Semantic content metadata. For example,
users can search for SWDs using RDF/XML
as the syntax language. Users can also search
for SWDs intended to be an ontology. This is
supported by an SWD’s ontology ratio—that
is, the fraction of its class-instances being recognized as classes and properties. Swoogle
considers an SWD to be an ontology document (SWO) if its ontology ratio exceeds an
empirical threshold (that is, 0.8);
• Relational metadata. For example, users can
search for SWDs that use the FOAF namespace
or that define instances of the foaf:Person class.
To find Semantic Web vocabulary terms,
Swoogle supports two types of queries:
• Search for terms. For example, the terms for
the concept “actor” can be found by checking
October 2005

65

Table 1. Comparison of OntoRank and PageRank in finding ontology documents.

Term
Name
Person
Title
Location
Description
Date
Type
Country
Address
Organization
Average

Ontologies found
by OntoRank (C1)

Ontologies found
by PageRank (C2)

9
10
13
12
11
14
13
9
11
9
11.1

6
7
12
6
10
10
11
4
8
5
7.9

Difference: percent
(C1-C2)/C2
50.00
42.86
8.33
100.00
10.00
40.00
18.18
125.00
37.50
80.00
40.51

if the local-names of their URIref have the
word “actor” but not “factor.”
• Search for resource description. When looking for a set of attributes that modifies
foaf:Person, users can search Swoogle’s ontology dictionary for properties having the class
foaf:Person as their domain. Such domain relations can be obtained by parsing relevant
ontology documents as well as reverse-engineering the class-instances of foaf:Person.
To our surprise, reverse engineering the foaf:Person
class-instances resulted in more than 500 properties, whereas parsing relevant ontology documents
returned only 167 findings.

SWOOGLE SEMANTIC WEB RANKING
Google’s success with its PageRank algorithm has
demonstrated the importance of ordering the results
that a query returns. Swoogle uses two custom ranking algorithms—OntoRank and TermRank—to
order a collection of SWDs or terms, respectively.
These algorithms are based on an abstract “surfing” model that captures how an agent might access
Semantic Web knowledge. Navigational paths on
the Semantic Web are defined by RDF triples as well
as by the resource-SWD and SWD-SWD relations.
However, revealing most of these connections
requires a centralized analysis.

Ranking SWDs using OntoRank
Since a Web document is the primary unit of data
access on the Web, Swoogle aggregates navigational
paths to the SWD level and recognizes three generalized interdocument links.12
• An extension relation holds between two
SWDs when one defines a term by using terms
defined in another.
• A use-term relation holds between two SWDs
when one uses a term that another defines.
66

Computer

• An import relation holds when one SWD
imports, directly or transitively, another SWD.
Google’s simple random surfer model is not
appropriate for these paths. For example, an agent
reasoning over the content found in an SWD should
access and process all of the ontologies it imports.
Swoogle’s OntoRank is based on the rational surfer
model, which emulates an agent’s navigation
behavior at the document level. Like the random
surfer model, an agent either jumps to a new random SWD with a constant probability or follows a
link in the current SWD to another SWD. However,
it is “rational” in that it follows a link nonuniformly and in accord with link semantics: When
encountering an SWD, the rational surfer will (transitively) import the “official” ontologies that define
the classes and properties the SWD references.
We used a data set containing 330,000 SWDs
(1.5 percent SWOs, 24 percent FOAF documents,
and 60 percent RSS documents) and 200,000 document-level relations to compare the effectiveness
of PageRank and OntoRank in finding ontologies.
Ten popular local-names (according to Swoogle’s
statistics) were selected as queries. For each query,
we sorted the results using PageRank and
OntoRank individually and then compared the
number of ontology documents among the top 20
highest ranked results by both PageRank and
OntoRank. Table 1 shows that OntoRank outperformed PageRank by an average of 40 percent.

Ranking terms
Swoogle uses TermRank to sort terms by their
popularity. This can be measured by the number of
SWDs using or populating a term. This approach,
however, ignores agents’ rational behavior in
accessing SWDs, so the SWDs’ OntoRank values
modulate the results. Table 2 lists TermRank’s
ordering of the 10 highest ranked classes having
“person” as the local-name. Not surprisingly, the
foaf:Person class is number one. The sixth term is
a common mistyping of the first one, so it has been
populated without being defined. The ninth term
appears in the list by virtue of the high OntoRank
score of the ontology that defines it.

APPLICATIONS
To explore what services a Semantic Web search
engine can provide, we have used Swoogle to support several applications and use cases. These projects include helping researchers find ontologies and
data, semantic search over documents representing
proofs, and finding and evaluating semantic asso-

Table 2. Top ten results when searching for classes with “person” in their local-name.

Rank
1
2
3
4
5
6
7
8
9
10

Resource URI
http://xmlns.com/foaf/0.1/Person
http://xmlns.com/wordnet/1.6/Person
http://www.aktors.org/ontology/portal#Person
ns1:Person1
ns2:Person2
http://xmlns.com/foaf/0.1/Person
http://www.amico.org/vocab#Person
http://www.ontoweb.org/ontology/1#Person
ns3:Person3
http://description.org/schema/Person

1

http://www.w3.org/2000/10/swap/pim/contact#

2

http://www.iwi-iuk.org/material/RDF/1.1/Schema/Class/mn#

3

http://ebiquity.umbc.edu/v2.1/ontology/person.owl#

ciations in large graph databases.
In the NSF-supported SPIRE project, a group of
biologists and ecologists is exploring how to use
the Semantic Web to publish, discover, and reuse
models, data, and services.13 Researchers need to
find appropriate ontologies and terms for annotating their data, and they also need resources for discovering data and services others have published.
With Swoogle’s ontology search interface, users
can search for existing ontology documents that
define terms in which user-supplied keywords are
the substring of their local-name. For example, to
find an ontology for describing temporal relations,
the search might use the keywords “before,”
“after,” and “interval.” Swoogle’s ontology dictionary provides definitions for a given property (or
class). It can assemble and merge definitions from
multiple sources, list terms sharing the same namespace or the same local-name, and list domain associations between classes and properties. Those
associations can either be “ontological” (for example, the foaf:knows property is defined as existing
between instances of foaf:person), or “empirical”
(for example, applying the dc:creator property to
an instance of foaf:Person). Judging the ranking or
popularity of terms and ontologies is also relevant.
Community consensus models as reflected in
ontologies tend to be ranked highly, thus searches
use them more often.
Researchers are using Swoogle in conjunction with
the Inference Web (IW),14 which explicitly represents
proofs using Proof Markup Language (PML),15 an
OWL ontology. One IW component, IWSearch
(http://iw4.stanford.edu/iwsearch/IWSearch), uses
Swoogle document search (searching SWDs using
IW namespaces) to discover newly published or
updated PML documents on the Web and itself is
powered by a specialized instance of Swoogle to
index and search instances found in a corpus of more
than 50,000 PML documents. Indexing the conclu-

No. of SWDs
populating term
74,589
2,658
267
257
277
217
90
32
0
10

No. of instances
1,260,759
785,133
3,517
935
398
5,607
90
522
0
10

No. of SWDs
defining term
17
80
6
1
1
0
1
2
1
0

sion part of a proof NodeSet instance can lead to the
discovery of additional NodeSets sharing the same
conclusion as the one from the given justification tree,
thus helping to expand the justification tree with
additional proofs.
SEMDIS, an NSF project jointly conducted with
researchers at the University of Georgia is also
using Swoogle. This project is automating the discovery, merging, and evaluation of complex semantic associations in RDF data drawn from a variety
of information sources. SEMDIS augments information collected from the Semantic Web with additional data extracted from text documents and
databases.16 The result, encoded as a large RDF
graph along with provenance assertions and trust
information, is processed to discover and evaluate
“interesting” semantic associations.17 SEMDIS conducts two kinds of Semantic Web searches:
• searching for a semantic association (connected subgraph) in the large-scale RDF graph,
and
• searching SWDs that (partially) support a
given semantic association.
The first kind of search finds paths between two
nodes in a graph, a common issue in RDF databases. The second is a provenance search to find a
set of SWDs that (partially) imply a hypothesized
semantic association. Researchers have prototyped
this type of search as an RDF molecule-based
approach at the RDF subgraph search level.8

s the Web has grown in size, search engines
have become a critical component of its infrastructure, and there is an increasing need for
search engines that can efficiently handle Semantic
Web content. While we cannot be sure what form
this content will take in the future, the current stan-

A

October 2005

67

dard is based on Semantic Web documents. We continue to use Swoogle to study the growth and characteristics of the Semantic Web and the use of RDF
and OWL. We are also developing new features and
capabilities and exploring how it can be used in
novel applications. Many open issues remain.
One set of open problems involves scale.
Techniques that work today with 5 × 106 documents may fail when the Semantic Web has 5 × 108
documents. Extending Swoogle to index and effectively query large amounts of instance data remains
a challenge. We estimate that the SWDs currently
on the Web contain more than 5 × 108 triples, a
number that neither current relational databases
nor custom triple stores can handle efficiently.
Some of these problems could potentially be
solved by moving away from the conventional database technology we are using and creating customdesigned index stores and distributed systems—
analogous to what Google has done for conventional
Web searches. It remains to be seen, however, if that
alone would suffice. We are also interested in developing a query system that can be used to find RDF
molecules in a reasonably efficient manner.8
We also need to explore how much and where a
Semantic Web search engine should reason over the
contents of documents and queries. In an earlier
system,18 we experimented with expanding documents using reasoning prior to indexing. A complementary approach is to expand queries containing RDF terms.19 This is related in part to the
problem of scale—the larger the collection becomes, the less efficient it is to reason over it.
Other issues involve trust and the use of local
knowledge that is not part of the Semantic Web.
Information encoded in RDF is now being embedded in other documents, such as PDF and XHTML
documents, JPEG images, and Excel spreadsheets.
When techniques for such embedding become standard, we expect the growth of Semantic Web content on the Web to accelerate dramatically. This will
add a new requirement for hybrid information
retrieval systems that can index documents based
on words as well as RDF content. More information about these issues, as well as Swoogle, can be
found in a companion technical report.20 ■

References
1. T. Berners-Lee, J. Hendler, and O. Lassila, “The
Semantic Web,” Scientific American, May 2001, pp.
35-43.
2. G. Klyne and J.J. Carroll, Resource Description
Framework (RDF): Concepts and Abstract Syntax,

68

Computer

Feb. 2004; http://www.w3.org/TR/2004/REC-rdfconcepts-20040210/.
3. D. Brickley and R.V. Guha, RDF Vocabulary
Description Language 1.0: RDF Schema, Feb. 2004;
http://www.w3.org/TR/2004/REC-rdf-schema20040210/.
4. M. Dean and G. Schreiber, OWL Web Ontology
Language Reference, Feb. 2004; http://www.w3.org/
TR/2004/REC-owl-ref-20040210/.
5. L. Ding et al., “Swoogle: A Search and Metadata
Engine for the Semantic Web,” Proc. 13th ACM
Conf. Information and Knowledge Management,
ACM Press, 2004, pp. 652-659.
6. D. Beckett, RDF/XML Syntax Specification
(Revised), Feb. 2004; http://www.w3.org/TR/2004/
REC-rdf-syntax-grammar-20040210/.
7. J.J. Carroll et al., Named Graphs, Provenance, and
Trust, tech. report HPL-2004-57, HP Lab, May 2004.
8. L. Ding et al., Tracking RDF Graph Provenance
Using RDF Molecules, tech. report TR-CS-05-06,
UMBC, Apr. 2005.
9. L. Page et al., The PageRank Citation Ranking:
Bringing Order to the Web, tech. report, Stanford
Digital Library Technologies Project, 1998.
10. A. Eberhart, Survey of RDF Data on the Web, tech.
report, Int’l Univ. in Germany, 2002.
11. Y. Guo, Z. Pan, and J. Heflin, “An Evaluation of
Knowledge Base Systems for Large OWL Datasets,”
Proc. Int’l Semantic Web Conf., Springer, 2004, pp.
274-288.
12. L. Ding et al., “Finding and Ranking Knowledge on
the Semantic Web,” Proc. Int’l Semantic Web Conf.
(ISWC), Springer, 2005, to appear.
13. T. Finin and J. Sachs, “Will the Semantic Web Change
Science?” Science Next Wave, Sept. 2004; http://
nextwave.sciencemag.org.
14. P. Pinheiro da Silva, D.L. McGuinness, and R.
McCool, “Knowledge Provenance Infrastructure,”
Data Eng. Bull., vol. 26, no. 4, 2003, pp. 26-32.
15. P. Pinheiro da Silva, D.L. McGuinness, and R. Fikes,
A Proof Markup Language for Semantic Web Services, tech. report KSL04-01, Stanford Univ., 2004.
16. B. Aleman-Meza et al., “Context-Aware Semantic
Association Ranking,” Proc. 1st Int’l Workshop
Semantic Web and Databases, 2003, pp. 33-50.
17. A. Sheth et al., “Semantic Association Identification
and Knowledge Discovery for National Security
Applications,” J. Database Management, vol. 16, no.
1, 2005, pp. 33-53.
18. T. Finin et al., “Information Retrieval and the Semantic Web,” Proc. 38th Int’l Conf. System Sciences,
IEEE CS Press, 2005, p. 113.
19. E.M. Voorhees, “Query Expansion Using LexicalSemantic Relations,” Proc. 17th Int’l Conf. Research

and Development in Information Retrieval (SIGIR
94), Springer-Verlag, 1994, pp. 61-69.
20. L. Ding et al., Swoogle Semantic Web Search, tech.
report TR-CS-05-09, UMBC, Sept. 2005.

land, College Park. He is a member of the American Association of Artificial Intelligence and the
International Neural Network Society. Contact
him at ypeng@umbc.edu.

Li Ding is a PhD student in computer science and
electrical engineering at the University of Maryland, Baltimore County. His research interests
include the Semantic Web, intelligent agents, and
data mining. He is a member of the American Association of Artificial Intelligence. Contact him at
ding.li@umbc.edu.

Pavan Reddivari is an MS student in computer science and electrical engineering at the University of
Maryland, Baltimore County. His research interests include the Semantic Web, pervasive computing, and intelligent agents. He is a member of the
American Association of Artificial Intelligence.
Contact him at pavan2@umbc.edu.

Tim Finin is a professor of computer science and
electrical engineering at the University of Maryland, Baltimore County. His research interests
include applications of artificial intelligence to
problems in information systems, software agents,
the Semantic Web, and mobile computing. He
received a PhD in computer science from the University of Illinois, Urbana-Champaign. He is a
member of the American Association of Artificial
Intelligence and the ACM. Contact him at finin@
umbc.edu.

Anupam Joshi is a professor of computer science
and electrical engineering at the University of
Maryland, Baltimore County. His research interests include mobile and pervasive computing, the
Semantic Web, and security. He received a PhD in
computer science from Purdue University. He is a
member of the IEEE, the IEEE Computer Society,
and the ACM. Contact him at joshi@umbc.edu.

Rong Pan is a PhD student in computer science and
electrical engineering at the University of Maryland, Baltimore County. His research interests
include probabilistic reasoning, the Semantic Web,
and information retrieval. He is a member of the
American Association of Artificial Intelligence.
Contact him at pan.rong@umbc.edu.

Yun Peng is an associate professor of computer science and electrical engineering at the University of
Maryland, Baltimore County. His research interests include the representation of and reasoning
with uncertainty in intelligent systems, machine
learning, and the Semantic Web. He received a PhD
in computer science from the University of Mary-

SCHOLARSHIP
MONEY FOR
STUDENT
LEADERS
Lance Stafford Larson Student Scholarship
best paper contest
✶
Upsilon Pi Epsilon/IEEE Computer Society Award
for Academic Excellence
Each carries a $500 cash award.

Application deadline: 31 October

Investing in Students
www.computer.org/students/
October 2005

69

An In-Band Easy-to-Deploy Mechanism for Network-to-Transport Signaling
Mayank Sharma
Stanford
msharma@stanford.edu

Dina Katabi
MIT
dk@mit.edu

Rong Pan
Stanford
rong@stanford.edu

Abstract— Network-to-transport signaling is desirable for ensuring efficient resource usage and timely notice of network
status. ICMP is the standard way for signaling, but unfortunately
it generates extra load and does not traverse firewalls. In this
paper, we develop M-ECN, an in-band network-to-transport
signaling mechanism, which does not generate any extra packets
and does not require dedicated header bits. The key idea is
to sneak messages into the stream of ECN bits, but without
interfering with ECN congestion signaling. Compared to other
alternatives, M-ECN is easy to deploy because routers read/write
to the IP header, and the mechanism requires no change to legacy
routers along the path which do not participate in the signaling.

I. I NTRODUCTION
In this paper, we are interested in network-to-transport
signaling, a restricted form of communication in which the
network reports to the end-points of a flow the occurrence of
a particular event. Signaling is desirable for ensuring efficient
resource usage and timely notice of network status. Some
example applications are: a router signaling to a sender that a
packet loss was due to wireless errors rather than congestion
[2], [4], [6], [10], [16], warning a source that it is suspected of
misbehaving [17], and informing sources of a drastic change
in link capacity [9].
Prior proposals for network-to-transport signaling require
either sending out-of-band ICMP messages or using dedicated
fields in packet headers. Both approaches have shortcomings:
Out-of-band messages generate extra load, do not cross firewalls, and create the potential for misuse (e.g., an attacker
may misinform a victim that the path capacity has been
drastically reduced). Using dedicated fields in the IP header
is problematic given that header real-estate has already been
allocated to different functionalities. The use of additional
application-dependent headers requires a change to router
architecture, interferes with tunnels, and violates end-to-end
semantics.
We propose multi-functional header fields which can simultaneously be used by more than one task. Specifically, we
develop the Multiplexed ECN (M-ECN) channel as a means
for allowing routers and end-systems to communicate low rate
information using ECN, but without interfering with ECN’s
signaling of congestion. The key observation we leverage
is this: The stream of ECN bits from a flow creates a
communication channel whose capacity can be up to one bit
per packet. But because the ECN mark is infrequently set, the
rate of information transfer over this channel is much smaller
than its capacity. Thus, there is some left-over capacity to
communicate other information. To use this spare capacity,
the router needs to spread a message across multiple packets
of a flow, while ensuring that ECN congestion marks are given

IEEE Communications Society
Globecom 2004

Balaji Prabhakar
Stanford
balaji@stanford.edu

priority over other information.
Our contribution is twofold. First, we introduce the MECN channel as a way of designing multi-functional header
fields. Prior work on using a field in the IP header for a
new task (which differs from the field’s actual purpose) did
not explicitly require the (peaceful) co-existence of different
functions on a single header field [23], [25]. Modeling the
M-ECN channel as a “Z-channel with erasures” allows us to
information-theoretically evaluate the capacity of the M-ECN
channel and compute the spare capacity available for other
information. Some other features of M-ECN are:
1. It is amenable to gradual deployment. Only routers and endpoints which participate in communication over the M-ECN
channel need to be modified. The channel is transparent to
legacy routers.
2. It can be simultaneously used by multiple routers along a
path to communicate with the end-points of a flow.
3. Different flows can use it for different applications.
4. It uses ECN-Nonce to prevent a receiver from conveying
the wrong M-ECN signal to the sender.
5. It is resilient to packet drops and reordering.
Second, as a concrete application of the M-ECN channel
abstraction, we develop and evaluate a novel protocol called
WiSE, for signaling wireless error drops to the source. Such
signaling is known to significantly improve TCP performance
over wireless links. We devise a simple scheme for signaling
packet corruption on the M-ECN channel via ECN mark
positions. We show through simulation that WiSE achieves a
performance gain comparable to previous end-to-end wireless
error recovery mechanisms, such as ELN [2] and ETEN [16],
but without generating extra messages or requiring dedicated
bits in packet header. Further, WiSE is fair to non-WiSE
sources and reacts properly to ECN congestion marks.
II. L IMITATIONS OF P RIOR S IGNALING M ECHANISMS
Current mechanisms for network-to-end-points signaling are
sometimes inadequate or suboptimal, as argued below.
(A) Out-of-Band ICMP Messages: ICMP is the standard way
for communicating information between network and users.
Unfortunately, as a signaling mechanism, ICMP has many
limitations. First, ICMP messages are filtered out by most
firewalls because they create a security hazard and a potential
for a denial-of-service attack [5]. Second, ICMP messages
create extra load and hence should not be used too often. Third,
ICMP does not deal with IPsec [15] or tunnels. Fourth, ICMP
may be inappropriate for certain applications. For example,
consider using ICMP messages to signal error drops over a
multi-hop wireless network. When fading occurs the quality

1278

0-7803-8794-5/04/$20.00 © 2004 IEEE

of the wireless channel degrades causing a large number of
packet losses in a short interval. The router will generate ICMP
messages to signal each corrupted packet to its sender. But, in
a multi-hop wireless network, the router will need to use the
same erroneous wireless channel to send the ICMP messages.
So the ICMP messages will compete with the data packets for
the low quality wireless channel causing even more errors and
drops, and consequently more ICMP messages.
(B) IP options: Information may be exchanged between
routers and end-systems in IP options. Currently the use of
IP options in packets removes them from the fast path, and
causes them to incur substantial delays. Thus, IP options are
not desirable for signaling repeated events (e.g., wireless error
drops). In general, the problem with IP options is gradual
deployment; even if one changes the signaling router to use
IPv6 or to implement IP options efficiently, legacy routers
along the path will either drop the packet (if it is IPv6) or
send it along the slow path.
(C) Dedicated Bits in the IP or TCP Header: Using
dedicated fields in the IP header is problematic given that
header real-estate has already been allocated to different
functionalities. Allowing routers to write to fields in the TCP
header requires the router to recompute the TCP checksum
for each packet, violates end-to-end semantics, interferes with
tunnels, and makes it hard to use the same service with other
transport protocols.
(D) Additional Dedicated Headers: Conceptually, it is possible to signal using an additional service-dependent header
that intervenes between the TCP and IP headers. In practice,
this causes deployment problems, might require changing
router architecture, and each new application will define a
new header. Currently routers strip the first few bytes of a
packet which usually contain the IP and TCP headers. Inserting
additional headers between IP and TCP may prevent access to
the TCP header, and confuse legacy routers which want to
read some TCP fields (e.g. port numbers) but access the new
header instead.
III. BACKGROUND & T ERMINOLOGY
(A) Explicit Congestion Notification (ECN): ECN allows
routers that use active queue management (AQM) to mark
packets as an indication of imminent congestion. This has the
potential of limiting packet drops due to a buffer overflow.
ECN uses the ECT and CE bits in the IP header for signaling
between routers and connection endpoints, and uses the ECNEcho, CWR, and NS bits in the TCP header for signaling
between sender and receiver. Figure 1 defines the meaning of
these bits as presented in RFC 3168 [22].
(B) Terminology: For simplicity, we will always refer to ECN
as being one bit. Thus, ECN = 1 means that the CE and ECT
flags are “11”, whereas ECN = 0 means that the flags are
either “01” or “10”.
IV. T HE M-ECN CHANNEL
M-ECN is an in-band easy-to-deploy mechanism for
network-to-transport signaling. We envision that some routers

IEEE Communications Society
Globecom 2004

ECT CE
Meaning
00
ECN incapable
10
no congestion, Nonce=0
01
no congestion, Nonce=1
11
congestion, Nonce cleared
(a) ECN encoding in the IP header (see §VI-A for nonce definition)
ECN-Echo: Set by the receiver in TCP ACK to signal to the sender
the reception of an ECN marked packet.
CWR: Set by the sender to acknowledge its receipt of and reaction
to the ECN-Echo flag.
NS: The receiver returns the cumulative sum of the nonce in the
NS bit in the TCP ACK.
(b) ECN bits in the TCP header
Fig. 1.

The bits used by ECN in both the IP and TCP headers.

in the network may be equipped to signal certain events.
For example, a subset of the wireless routers may be able
of signaling packet corruption; some other routers may be
equipped to signal quality of service, etc. Each one of these
services will have a unique well-known identifier. Using its
first packet, a flow (identified by its sender, receiver, and ports)
may subscribe to a particular M-ECN signal by specifying the
service identifier in an IP router-alert option [13]. (Subsequent
packets of the flow do not use IP options).
Routers signal to the end point of a flow using messages
constructed out of ECN marks without interfering with ECN
congestion signaling (see Figure 2). The key idea is to use
opportunistic signaling; i.e. to sneak messages into seldomused standardized fields, while giving priority to the original
functionality of the field. In §V-C, we discuss in detail how to
construct M-ECN messages, but at a high-level opportunistic
signaling works as follows:
1. It spreads an M-ECN message across multiple packets.
We call the sequence of ECN values (i.e. string of 1’s and
0’s) in a single M-ECN message a Codeword. Spreading a
codeword across multiple packets requires a router to maintain
per-subscriber state. Although this limits the number of flows
to which the router can signal at high speed, the space of
possible applications is still substantial. For example, both
wireless loss notification and warning a suspected misbehaver
are potential applications, since in the former the bottleneck
is the link bandwidth not the router, whereas in the latter the
router would need to maintain state to detect misbehaving
regardless of whether it uses the M-ECN channel [17].
2. It encodes information in patterns (i.e., strings) of 0’s & 1’s
that are unlikely to be generated by ECN congestion marking.
3. It gives priority to ECN congestion signaling over sneaked
messages. First, whenever an AQM router along the path
marks a packet because of congestion, it results in an illegal
M-ECN codeword which allows the end system to recognize
the congestion mark. Second, M-ECN codewords have a very
low density of 1’s, leaving most of the packets available for
congestion marking by AQM routers.
4. To construct M-ECN codewords, we use the observations:
1) ECN marks can be swapped between two packets that are
closely spaced in time; 2) a cluster of ECN marks is equivalent
to a single mark since TCP reacts to only one decrease signal
in a congestion window (cwnd). Similarly, an ECN mark

1279

0-7803-8794-5/04/$20.00 © 2004 IEEE

e

1

1

1

1

p

0

1−p

Fig. 2. M-ECN routers signal to the receiver using code-words constructed
out of ECN marks. If needed, the receiver can relay the message to the sender
using an end-to-end mechanism such as a TCP option.

inserted after a drop is harmless because TCP reacts only to
one congestion signal per cwnd.
We believe that the concept of opportunistic signaling may
be applicable to fields other than ECN (e.g. the DiffServ bits).
We have chosen to focus on ECN because it is standardized,
has been widely deployed in both routers and end systems,
and being in the IP header, is easily accessible to routers
for reading and writing. Further, there are a few proposals
for using ECN to signal alternative information such as
congestion price [11]. These proposals usually assume that
all routers along the path have been updated to understand
their alternative meaning of ECN. But if some routers perform
conventional ECN congestion marking and others use the alternative meaning, the resulting signal will contain unpredictable
errors. Hence, some of these proposals may be able to use MECN for gradual deployment to allow them to deliver a signal
even when the path traverses legacy ECN routers.
A. The Capacity of the M-ECN channel
Using methods from information theory, we model the MECN channel as a binary channel with erasures and compute
the left-over capacity after congestion marking. In the canonical formulation of the communication problem, there is a
transmitter communicating to a receiver over a noisy channel.
Here, the transmitter is an M-ECN router and the receiver
is a flow’s end-point. The effect of ECN congestion marking
on M-ECN messages is modeled as channel noise. This noise
flips a 0 to 1 with the congestion marking probability p, but
it faithfully transmits a 1 with probability 1, as illustrated in
Figure 3(a). The resulting channel is the Z-channel of information theory. The capacity of the Z-channel is known [12],
and asymmetric error correcting codes have been constructed
and shown to perform well [21].
While the Z-channel models the congestion marking process
at a regular AQM router, it does not model the effect of
packet drops. Information theoretically, packet drops lead to
the “erasure” of a 0 or a 1, which we model by the Z-channel
with erasures (or the ZwE-channel), shown in Figure 3(b).
In the ZwE-channel a 0 or a 1 is erased with probability ,
which is the drop probability along the path. When an erasure
(i.e. a packet drop) occurs, we receive an e (i.e. a gap in the
sequence number) instead of a 0 or a 1.
We shall now compute the capacity, C(p, ), of the ZwEchannel as a function of the congestion marking probability p
and the drop probability . This allows us to understand how

IEEE Communications Society
Globecom 2004

1−ε

1

p

ε

0

0

(a)

Fig. 3.

ε

1−p−ε

0

(b)

(a) The Z-channel

(b) The ZwE-channel

much spare capacity remains for transmitting extra information
over the M-ECN channel. By definition, (see [7]) the capacity
of a discrete memoryless channel is:
C = max I(X; Y ) = max H(Y ) − H(Y |X),
0≤q≤1

0≤q≤1

(1)

where I(X; Y ) is the Mutual Information between the input X
and the output Y , H(Y ) is the entropy of the output, H(Y |X)
is the conditional entropy of the output given the input and
the maximum is over all possible probability distributions on
the input. For the ZwE-channel, the inputs X are 0 or 1, and
the possible outputs Y are 0, 1 or e. We want to express
this capacity as a function of the density of 1’s in the code q
because we want to ensure that M-ECN messages have a low
density of ones so that there are enough unmarked packets
to allow a regular AQM router to signal congestion through
marking. Thus,
I(X; Y ) = H (, q(1 − ) + (1 − q)p, (1 − q)(1 − p − ))
−qH (, 1 − ) − (1 − q)H (, p, 1 − p − ) (2)
and the capacity C = C(p, ) can be computed as an explicit
function of p, , and q (details of the derivation are in [24]).
To give a feeling for the result, we plot in Figure 4 the
capacity for the particular case where p = 0.01 and  = 0.01.
The figure shows that there is reasonable capacity left even if
we constrain our codewords to have a very low density of 1’s.
For example, if the congestion marking probability p = 0.01,
the drop probability  = 0.01, and one allows the density of
ones in the codewords to be 0.02, then the capacity turns out
to be 0.11 bits/packet. Thus, the M-ECN channel provides
0.11 − p = 0.1 bits/packet to signal extra information. Said
differently, on an average, if we had a sequence of 100 packets
of which 1 got randomly dropped and 1 randomly marked,
and we were allowed to use only 2 ones (i.e., 2 marks) to
communicate a message, then we could transmit up to 10 bits
of information using this packet sequence.
To construct M-ECN messages, one needs to follow the
rules of opportunistic signaling in §IV. Next, we consider a
particular application called WiSE and in that context, devise
a simple scheme which constructs messages using the position
of ECN marks relative to wireless drops.
V. W I SE: W IRELESS S IGNALING VIA ECN
As a concrete application of M-ECN, we propose WiSE, a
protocol for signaling wireless losses to a TCP sender over
the M-ECN channel.

1280

0-7803-8794-5/04/$20.00 © 2004 IEEE

0.45

2. Whenever 802.11 discards a packet because of a wireless
error, the WiSE agent signals this information in a message
sent over the M-ECN channel (as described in §V-C). The
agent preserves ECN marks from upstream.
3. Receiver decodes the WiSE signal and interprets illegal
codewords as congestion. It informs the sender about error
losses using a TCP option. (Routers do not access this option).
4. Sender recovers from errors without reducing its rate.

Capacity of the ZwE-channel as a function of q

0.4

Capacity (bits/packet)

0.35
0.3
0.25
0.2
0.15
0.1
0.05

C. Legal & Illegal Codewords

0
0

0.02

0.04
0.06
0.08
q - The density of "1"s in the code

0.1

Fig. 4. Capacity of the ZwE-channel as a function of the density of 1’s for
congestion marking probability p = 0.01 and drop probability  = 0.01.

A. Problem & Previous Solutions
The wireless channel is well-known for exhibiting error
rates substantially higher than those observed in wired environments. Although current access technologies have mitigated
this problem to some extent for the last hop, short range,
indoor environment, they are still of concern in designing highrate channels for outdoor or mobile environments [8], [14].
TCP reacts adversely to the high error rate seen on wireless
links because it interprets any packet drop as a sign of
congestion. Thus, TCP reacts to wireless packet losses by
unnecessarily halving its congestion window.
Various strategies have been suggested in the literature to
combat this problem [1], [2], [3], [4], [6], [10], [16], [18],
and they can be classified into two main classes. The first
class shields the sender from the wireless link by using local
link-layer retransmissions. The second class, to which WiSE
belongs, exposes the reasons for the drops to the end-user thus
allowing the sender to recover appropriately.
WiSE Contributions: In addition to serving as a case study of
the M-ECN channel, WiSE provides a performance gain higher
than ETEN [16] and comparable to ELN [2], without using
additional message packets or changing the header structure. In
contrast to the current version of ELN, WiSE does not assume
route symmetry and works even if a flow traverses multiple
wireless hops. Finally, WiSE signaling is simple and can be
implemented at a router using 3 lines of C code [24].
B. Overview of the WiSE protocol
The WiSE protocol is implemented through two components: the WiSE-Agent and WiSE-TCP. Located at the router,
the WiSE-Agent is a data-link layer module that detects
wireless packet errors and signals them to WiSE-TCP over the
M-ECN channel. WiSE-TCP is an extension to standard TCP
which allows it to decode the “packet-drop type” information
sent by the WiSE-Agents along a flow’s path. We assume
802.11, where a corrupted packet is discarded at the sending
MAC after zero or more retrials. Thus, before dropping a
packet because of wireless errors, the router can read the ECN
value in the packet.
Below is an overview of how WiSE works (the implementation details are in [24]).
1. The first packet subscribes the flow to the WiSE service
using a router alert IP option [13].

IEEE Communications Society
Globecom 2004

Our objective is to make corruption drops look very different from congestion drops by associating them with a very
structured pattern of ECN bits that is unlikely to be created
randomly. Whenever a WiSE router (i.e. agent) drops a packet
because of corruption, it inserts a 1 in the stream of ECN bits
from the flow. Further, the WiSE router preserves the 1’s in
the ECN field of packets coming from upstream. Thus, if a
WiSE router drops a marked packet because of wireless errors,
then, in addition to inserting a new mark, the router remembers
the ECN mark on the dropped packet and moves it to a later
packet from the same flow. The same happens if the router
sets a WiSE mark on an already marked packet.
The resulting codewords used by WiSE start with the
occurrence of a drop and end with the occurrence of ECN=0,
and are characterized by having an equal number of drops and
ECN marks. For example, let X represents a drop then X10,
X1X10, and XX110 are all legal WiSE codewords. Further,
at any point through a legal codeword, the number of drops
equals or exceeds the number of ECN marks; e.g. X11X0
is not a legal codeword. Any sequence of drops/marks that
doesn’t satisfy the above properties is a sign of congestion.
D. Example Scenarios
In the examples, we will only show a flow’s ECN bits and
represent them as a stream. The ECN field of a dropped packet
will be marked with an X. Each packet, and thus the ECN bit
associated with it, will be referenced by its sequence number.
Example 1: Composability of WiSE Messages
This example (Figure 5) shows that WiSE encoding is accumulative in nature, and thus multiple routers in an ad-hoc network
can compose their signals. Packet 5 is corrupted at router
WiSE-1 leading to the ECN mark of packet 6 being set to 1.
But, packet 6 is corrupted at WiSE-2. So, WiSE-2 has to set
ECN=1 on the next two packets to account for the corruption
of packet 6 and the fact that packet 6 was already marked.
The receiver gets the valid WiSE word XX110, indicating that
packets 5 and 6 were dropped because of errors.
WiSE−1
9
0

8
0

7
0

Fig. 5.

6
1

5
X

WiSE−2
4
0

9
0

8
1

7
1

6
X

5
X

4
0

Composability of WiSE messages

Example 2: WiSE interaction with AQM routers
This example demonstrates that WiSE signaling does not
hamper the congestion signals sent by AQM routers. As in

1281

0-7803-8794-5/04/$20.00 © 2004 IEEE

Figure 6, the AQM router marks packet 6 with a 1 to signal
congestion. But packet 6 is discarded at the wireless link
because of errors. Then the WiSE router not only marks a
subsequent packet (packet 7) to signal a wireless drop, but also
reads the ”ECN” mark on the dropped packet and preserves
it by setting the ECN bit on packet 8 to 1. The receiver will
recognize X110 as an illegal WiSE codeword (since, marks =
drops) and convey the congestion signal to the sender.

8
0

Fig. 6.

7
0

6
1

WiSE
5
0

4
0

9
0

8
1

7
1

5
0

4
0

0.6
0.4
0.2

WISE
NewReno

0

Fig. 7.

WiSE interaction with an AQM router

E. WiSE Evaluation

(D) Comparison of WiSE, NewReno, ELN, and ETEN:
Figure 9 shows the average combined throughput of 5 longlived TCPs traversing a 10 Mb/s wireless link as a function
of the packet error rate. It shows that WiSE achieves a performance gain comparable to ELN and better than ETEN, without
generating extra messages or requiring dedicated header space.
ETEN [16] sends out-of-band ICMP messages to the sender to
report the packet corruption rate at the wireless link. ELN [2]

cwnd (pkts)

40

60
80
100
Simulation Time (sec)

120

140

160

WiSE boosts performance over wireless links
Wireless NewReno
Wired NewReno

20

30

40

160
120
80
40
0

50
60
Time (sec)

70

80

90

100

90

100

Wireless WiSE
Wired NewReno

10

20

30

40

50
60
Time (sec)

70

80

Fig. 8. A wired & wireless flows sharing a congested RED+ECN router;
(top) both flows are NewReno; (bottom) the wireless flow is WiSE.
5 TCP, Markov Error Model

(B) WiSE boosts performance over wireless links: In this
experiment, we have a 40 Mb/s wireless link traversed by
700 web sessions with Pareto distributed file sizes having an
average of 12 pkts/file; 10 long-lived FTPs and reverse traffic.
Average packet error rate is 0.01. Figure 7 shows the boost in
total throughput due to WiSE when the traffic mix is similar
to that expected in the Internet. Simulations over an ad-hoc
network with many wireless links showed similar results [24].
(C) Robustness & TCP-Friendliness: In this experiment,
the bottleneck is wired. Two flows share a congested 2
Mb/s RED+ECN router. One of these flows also traverses
a 10 Mb/s wireless link with error rate 0.01. Figure 8(top)
shows that when both flows are NewReno the wireless flow
suffers. In contrast, when the wireless flow uses WiSE, as in
Figure 8(bottom), its performance significantly improves. The
experiment reveals: 1) WiSE works correctly when there are
non-WiSE routers along the path; 2) The fact that WiSE is
fair to the non-WiSE flow shows that WiSE reacts properly to
ECN marks from the congested RED+ECN router.

20

160
120
80
40
0
10

Total TCP Throughput / Capacity

(A) Simulation Environment: Our simulations use ns-2 [20],
which we have extended to include a WiSE-Agent (derived
from RED queue) and a WiSE-TCP (a modified NewReno)
module. (We have also compared WiSE-SACK with standard
SACK, which lead to results similar in nature to those below.)
For wireless errors, we use a two-state Markov error model.
As per data cited in studies of the wireless channel [19],
we set the average burst size of errors in the Markov model
to 3 packets. Finally, in all simulations, wired routers use
RED queues (max= 2×min = 1/3 buffer), the buffer size is
a bandwidth-delay product, packets are 576 bytes, the round
trip time (RTT) is 50 ms, and the ECN option is on.

IEEE Communications Society
Globecom 2004

0.8

0
6
X

cwnd (pkts)

9
0

Total TCP Throughput / Capacity

AQM

Web Traffic + FTP + Reverse Traffic
1

1
0.8
0.6
0.4
0.2
0

WISE
NewReno
ELN
ETEN
0.001

Fig. 9.

0.01
Packet Error Rate

0.1

Comparison of WiSE, NewReno, ELN and ETEN

assumes route symmetry and uses a dedicated field in the TCP
header to report the corruption of a certain packet.
VI. S OME C HARACTERISTICS OF M-ECN AND W I SE
A. Security & Interaction with Nonce
To prevent a receiver from ignoring ECN congestion marks,
the sender sets a one bit random nonce in each packet. The
receiver returns the nonce sum to the sender in the Acks. But
when a router marks a packet it clears the nonce, and the
receiver would not know the correct nonce sum [22].
Communication on the M-ECN channel works properly
when nonce are used. Further, WiSE can use the nonce to
prevent a receiver from cheating by making a congestion drop
look as if it were a corruption drop. Since the WiSE router
can read the value of nonce before marking a packet, it can
help the receiver adjust its nonce sum, and prove to the sender
the correctness of a WiSE mark. In particular, the WiSE router
maintains a single bit called ∆-nonce in which it stores the

1282

0-7803-8794-5/04/$20.00 © 2004 IEEE

cumulative sum of the nonce in all packets that got corrupted
or marked during the encoding of a single WiSE codeword.
The ∆-nonce is added to the nonce value in the first unmarked
packet after the codeword. This causes the receiver’s nonce
sum to match that of the sender as long as there are no
congestion drops/marks. Said differently, the receiver cannot
claim that a congestion drop is a corruption drop because,
without the help of a WiSE agent, the receiver doesn’t know
the nonce value of the dropped packet.

to standard TCP and to halve its congestion window. More
work is needed to study potential M-ECN applications and the
applicability of opportunistic signaling to header fields other
than ECN.
ACKNOWLEDGMENT
Dina Katabi was supported by NSF under award No. ANI0335256. Balaji Prabhakar was supported in part by the NSF
grant ANI-9985446.
R EFERENCES

B. Robustness to Packet Reordering
Packet reordering does not flip 1’s to 0’s and thus does not
lead to the loss of congestion marks. Reordering is unlikely to
create legal M-ECN codewords because codewords are wellstructures patterns, which are hard to generate by random
reordering. Reordering may destroy an M-ECN codeword
causing the loss of the M-ECN signal. For example, in WiSE,
reordering could cause X10 → X01; thus TCP will mistake
the error drop to be a congestion drop, which is what would
have happened anyways if we didn’t use WiSE. Further, since
reordering occurs between outstanding packets, the reordered
WiSE marks don’t do any harm because TCP reacts to only
one congestion signal per congestion window.
C. Partial Deployment
The M-ECN channel is amenable to partial deployment
because it is completely transparent to routers and flows which
do not participate in the communication. For example, in §VE, we have shown that WiSE works well when the path
traverses both WiSE and non-WiSE routers. We have also
shown that WiSE functions correctly independently of whether
the bottleneck is a wired or wireless link. Finally, we have also
shown that WiSE interacts properly with non-WiSE TCPs.
VII. C ONCLUSION AND FUTURE WORK
We presented M-ECN, a novel mechanism for networkto-transport signaling, which generates no extra packets and
requires no specific bits in the packet header. M-ECN relies on
opportunistic signaling which sneaks messages in the stream
of ECN bits, while giving priority to ECN congestion signaling
over other information. As an example application of MECN, we have developed WiSE (for Wireless Signaling via
ECN) and shown that it boosts TCP performance over wireless
channels while reacting properly to ECN congestion marks.
Currently we are looking at using M-ECN to help flows
with a very large end-to-end pipe to quickly acquire spare
bandwidth and refine their decrease signal. We believe that
the number of gigabit flows over a path will be small and
thus, a per-gigabit-flow state might be acceptable. In contrast
to WiSE codewords, which rely on the position of ECN marks,
the codewords we devise for this application use fixed length
frames with a preamble [24]. M-ECN is attractive for this
application because it doesn’t require all routers along the path
to understand the new signaling. When legacy routers along
the path generate ECN congestion marks/drops, they create
an illegal M-ECN codeword. This causes the sender to revert

IEEE Communications Society
Globecom 2004

[1] A. Bakre and B. R. Badrinath. I-TCP: Indirect TCP for mobile hosts. In
Proc. 15th International Conference on Distributed Computing Systems
(ICDCS), May 1995.
[2] H. Balakrishnan and R. Katz. Explicit loss notification and wireless web
performance. In nms.lcs.mit.edu/papers/globecom98/, Nov. 1998.
[3] H. Balakrishnan, S. Seshan, and R. Katz. Improving reliable transport
and handoff performance in cellular wireless networks. In ACM Wireless
Networks, Dec. 1995.
[4] D. Barman and I. Matta. Effectiveness of loss labeling in improving
TCP performance in wired/wireless networks. Technical report, Boston
University, 2002.
[5] S. M. Bellovin. Security problems in the TCP/IP protocol suite. In
Computer Communications Review 2:19, pp. 32-48, Apr. 1989.
[6] S. Biaz and N. H. Vaidya. Distinguishing congestion losses from
wireless transmission losses: A negative result. Seventh International
Conference on Computer Communications and Networks (IC3N), 1998.
[7] T. M. Cover and J. A. Thomas. Elements of information theory. In
Wiley Series in Telecommunications, 1991.
[8] D. C. Cox. Wireless personal communications: what is it? In IEEE
Personal Communications, volume 2 Issue:2, pp 20-35, Apr. 1995.
[9] S. Dawkins, C. E. Williams, and A. E. Yegin. Problem statement
for Triggers for Transport(TRIGTRAN). In http://www.ietf.org/internetdrafts/draft-dawkins-trigtran-probstmt-00.txt, Oct. 2002.
[10] D. A. Eckhardt and P. Steenkiste. Improving wireless LAN performance
via adaptive local error control. Proceedings of IEEE ICNP ’98, 1998.
[11] Alternate proposals for the ECN field in IP, or for other ECN-like
semantics. http://www.icir.org/floyd/ecn.html.
[12] S. W. Golomb. The limiting behavior of the z-channel. In IEEE
Transactions on Information Theory, volume IT-26 No. 3, May 1980.
[13] D. Katz. IP Router Alert Option. RFC 2113, Feb. 1997.
[14] R. H. Katz. Adaptation and mobility in wireless information systems. In
IEEE Personal Communications, volume 1 Issue:1, pp 6-17, Apr. 1994.
[15] S. Kent and R. Atkinson. Security architecture for the internet protocol.
RFC 2401, Nov. 1998.
[16] R. Krishnan, M. Allman, C. Partridge, and J. P. G. Sterbenz. Explicit transport error notification for error-prone wireless and satellite
networks. In BBN Technical Report No. 8333, Mar. 2002.
[17] R. Mahajan, S. M. Bellovin, S. Floyd, J. Ioannidis, V. Paxson, and
S. Shenker. Controlling high bandwidth aggregates in the network. In
Computer Communications Review 32:3, pp. 62-73, July 2002.
[18] S. Mascolo, C. Casetti, M. Gerla, M. Y. Sanadidi, and R. Wang. TCP
Westwood: Bandwidth estimation for enhanced transport over wireless
links. In Mobile Computing and Networking, pages 287–297, 2001.
[19] G. T. Nguyen, B. Noble, R. H. Katz, and M. Satyanarayanan. A tracebased approach for modeling wireless channel behavior. In Proc. of the
1996 Winter Simulation Conference, Dec. 1996.
[20] The network simulator ns-2. http://www.isi.edu/nsnam/ns.
[21] P. Oprisan and B. Bose. ARQ in Optical Networks. In Proc. Pacific
Rim International Symposium on Dependable Computing, Dec. 2001.
[22] K. K. Ramakrishnan, S. Floyd, and D. Black. The addition of explicit
congestion notification (ECN) to IP. RFC 3168, Sept. 2001.
[23] S. Savage, D. Wetherall, A. Karlin, and T. Anderson. Practical network
support for IP traceback. In Proc. of ACM SIGCOMM, 2000.
[24] M. Sharma, D. Katabi, R. Pan, and B. Prabhakar. A General Multiplexed
ECN Channel and its use for Wireless Loss Notification. Technical
Report 896, MIT, 2003, http://nms.lcs.mit.edu/projects/mecn/.
[25] I. Stoica, S. Shenker, and H. Zhang. Core-stateless fair queuing: A
scalable architecture to approximate fair bandwidth allocations in high
speed networks. In Proc. of ACM SIGCOMM, Aug. 1998.

1283

0-7803-8794-5/04/$20.00 © 2004 IEEE

Belief Update in Bayesian Networks Using Uncertain Evidence*
Rong Pan, Yun Peng and Zhongli Ding
Department of Computer Science and Electrical Engineering
University of Maryland Baltimore County, Baltimore, MD 21250
{panrong1, ypeng, zding1}@csee.umbc.edu
Abstract
This paper reports our investigation on the problem of
belief update in Bayesian networks (BN) using uncertain
evidence. We focus on two types of uncertain evidences,
virtual evidence (represented as likelihood ratios) and
soft evidence (represented as probability distributions).
We review three existing belief update methods with uncertain evidences: virtual evidence method, Jeffrey’s rule,
and IPFP (iterative proportional fitting procedure), and
analyze the relations between these methods. This indepth understanding leads us to propose two algorithms
for belief update with multiple soft evidences. Both of
these algorithms can be seen as integrating the techniques
of virtual evidence method, IPFP and traditional BN evidential inference, and they have clear computational and
practical advantages over the methods proposed by others in the past.

1. Introduction
In this paper, we consider the problem of belief update
in Bayesian Networks (BN) with uncertain evidential
findings. There are three main methods for revising the
beliefs of a BN with uncertain evidence: virtual evidence
method [2], Jeffrey's Rule [1], and iterative proportional
fitting procedure (IPFP) [6]. This paper reports our analysis of these three belief update methods and their interrelationships. We will show that when dealing with a single
evidential finding, the belief update of both virtual evidence method and Jeffrey‘s rule can be viewed as IPFP
with a single constraint. Also, we present two methods we
developed for belief update with multiple soft evidences
and prove their correctness. Both of these methods integrate the virtual evidence method and IPFP, and they can
be easily implemented as a wrapper on any existing BN
inference engine.
We adopt the following notations in this paper. A BN
is denoted as N. X , Y, and Z are for sets of variables in a
BN, and x or xi are for a configurations of the states of X.
*

Capital letters A, B, C are for single variables. Capital
letters P, Q, R, are for probability distributions.

2. Soft Evidence and Virtual Evidence
Consider a Bayesian network N over a set of variables
X modeling a particular domain. N defines a joint distribution P (X ) . When giving Q (Y ) , an observation of a probability distribution on variables Y ⊆ X, Jeffrey's rule
claims that the distribution of all other variables under
this observation should be updated to
Q( X \ Y ) = ∑i P ( X \ Y | y i )Q( yi ) ,

where yi is a state configuration of all variables in Y. Jeffrey's rule assumes Q( X \ Y | Y ) = P( X \ Y | Y ) , i.e., invariance of the conditional probability of other variables,
given Y, under the observation. Thus
Q(Y )
Q( X ) = P ( X \ Y | Y )Q (Y ) = P ( X )
(2)
P (Y )
Here Q (Y ) is what we called soft evidence. Analogous to
conventional conditional probability, we can also write
Q (Y ) as P (Y | se) , where se denotes the soft evidence
behind the soft evidential finding of Q (Y ) . P (Y | se) is
interpreted as the posterior probability distribution of Y
given soft evidence se.
Unlike soft evidence, virtual evidence utilizes a likelihood ratio to represent the observer's strength of confidence toward the observed event. Likelihood ratio L(Y) is
defined as
L(Y ) = ( P (Ob( y1 ) | y1 ) : ... : P (Ob( ym ) | ym )) ,
where P (Ob( y i ) | y i ) is interpreted as the probability we
observe Y is in state yi if Y is indeed in state yi . The posterior probability of Y, given the evidence, is
P (Y | ve) = c ⋅ P (Y ) ⋅ L(Y )
(3)
= c ⋅ ( P ( y1 ) L ( y1 ), ..., P ( y n ) L( y n )),
where c = 1/ ∑i P ( y i ) L( y i ) is the normalization factor [3].
And since Y d-separates virtual evidence ve from all other
variables, beliefs on X \ Y are updated using Bayes’ rule.
Similar to equation (2), this d-separation leads to

This work was supported in part by DARPA contract F30602-97-1-0215 and NSF award IIS-0326460.

Proceedings of the 18th IEEE International
Conference on Tools with Artificial Intelligence (ICTAI'06)
0-7695-2728-0/06 $20.00 © 2006

(1)

P (Y | ve)
= c ⋅ P ( X ) ⋅ L(Y )
(4)
P(Y )
Virtual evidence can be incorporated into any BN inference engine using a dummy node. This is done by adding
a binary node veY for the given L(Y). This node does not
have any child, and has all variables in Y as its parents.
The CPT of veY should conform to the likelihood ratio. By
instantiating veY to True, the virtual evidence L(Y) is entered into the BN and the belief can then be update by any
BN inference algorithm.
P ( X | ve) = P ( X )

3. IPFP on Bayesian Network
Iterative proportional fitting procedure (IPFP) is a
mathematical procedure that modifies a joint distribution
to satisfy a set of probability constraints [6]. A probability
constraint R(Y) to distribution P(X) is a distribution on
Y ⊆ X . We say Q (X ) is an I1-projection of P (X ) on a
set of constraints R if the I-divergence between P and Q is
the smallest among all distributions that satisfy R.
I-divergence (also known as Kullback-Leibler distance
or cross-entropy) is a measurement of the distance between two joint distributions P and Q over X:
P( x )
.
I ( P || Q) = ∑ P ( x) log
(5)
P ( x )>0
Q( x )
I ( P || Q) ≥ 0 for all P and Q, the equality holds only if
P=Q.
For a given distribution Q0 ( X ) and a set of consistent1
constraints R = {R(Y1), …, R(Ym)}, IPFP converges to
Q * ( X ) which is an I1-projection of Q0 ( X ) on R (assuming there exists at least one distribution that satisfies R).
Q * ( X ) , which is unique for the given Q0 ( X ) and R, can
be computed by iteratively modifying the distributions
according to the following formula, each time using one
constraint in R:
Qk ( X ) = Qk −1 ( X ) ⋅

R (Yi )
,
Qk −1 (Yi )

(6)

where m is the number of constraints in R, and
i = ((k − 1) mod m) + 1 .
We can see that equations (2), (4) and (6) are in the
same form. We can regard the belief update with soft evidence by Jeffrey’s rule as an IPFP process of a single
constraint P(Y | se), and similarly regard belief update
with virtual evidence by likelihood ratio as an IPFP process of a single constraint P(Y | ve). As such, we say that
belief update by uncertain evidence amounts to change
the given distribution so that 1) it is consistent with the
evidence; and 2) it has the smallest I1-divergence to the

original distribution.
Moreover, IPFP provides a principal approach to belief
update with multiple uncertain evidential findings. By
treating these findings as constraints, the iterative process
of IPFP leads to a distribution that is consistent with ALL
uncertain evidences and is as close as possible to the
original distribution.
Note that, unlike virtual evidence method, both Jeffrey’s rule and IPFP cannot be directly applied to BNs
because their operations are defined on the full joint probability distribution, and they do not respect the structure
of BN [4].

4. Inference with Multiple Soft Evidential
Findings
Valtorta, Kim and Vomlel have devised a variation of
Junction-Tree (JT) algorithm for belief update with multiple soft evidences using IPFP [5]. In this algorithm, when
constructing the JT, a clique (the Big Clique) is specifically created to hold all soft evidence nodes. Let C denote
this big clique, Y = {Y1, ..., Yk} and {se1, ..., sek} denotes
soft evidence variables and the respective soft evidences,
and X denotes the set of all variables. This Big Clique
algorithm absorbs soft evidences in C by updating the
potential of C with the following IPFP formulae, iterating
over all evidences Q(Yj)s:
Q0 (C ) = P (C )
P (Y j | se j )
Qi (C ) = Qi −1 (C )
Qi −1 (Y j )
where j = 1+((i-1) mod k). The above procedure is iterated
until Qn(Yj) converges to P(Yj | sej) for all j. Finally, Q(C)
is distributed to all other cliques, again using traditional
JT algorithm.
This Big Clique algorithm becomes inefficient in both
time and space when the size of the big clique itself becomes large. Besides, it works only with Junction Tree,
and thus cannot be adopted by those using other inference
mechanisms2. Also, it requires incorporating IPFP operations into the JT procedure, causing re-coding of the existing inference algorithm. To address these shortcomings,
we propose two new algorithms for inference with multiple soft evidential findings. Both algorithms utilize IPFP,
although in quite different ways. The first algorithm combines the idea of IPFP and the encoding of soft evidence
by virtual evidence. The second algorithm is similar to the

2
1

A set of constraints R is said to be consistent if there exists a distribution Q(X) that satisfies all Ri in R. Obviously, two constraints are inconsistent if they give different distributions to the same variable. More
discuss of this matter is given in Section 7.

Proceedings of the 18th IEEE International
Conference on Tools with Artificial Intelligence (ICTAI'06)
0-7695-2728-0/06 $20.00 © 2006

Valtorta and his colleagues also developed another algorithm iteratively 1) updates the potential of the clique which contains variables of
one soft evidence by (6) and 2) propagates the updated potential to the
rest of the network. They mentioned the possibility of implementing this
method as a wrapper around Hugin shell or other JT engines, but no
suggestion of how this can be done was given [12].

Big Clique algorithm but it decouples the IPFP with Junction Tree.

4.1 Iteration on the Network
As pointed out by Pearl [3], soft evidence can be easily
translated into virtual evidence when it is on a single variable. Given a piece of soft evidence se on variable A, if
we want to find a likelihood ratio L(A) such that
P ( A) ⋅ L( A) = P ( A | se) ,
then we have
P(a1 | se)
P ( a n | se)
P ( A | se)
, ...,
).
L( A) =
=(
(7)
P ( A)
P(a1 )
P (an )
A problem arises when multiple soft evidences se1, se2,
…, sem are presented. Applying one virtual evidence vei
will have the same effect as applying the soft evidence sei ,
in particular, the posterior probability of Yi is made equal
to P(Yi | sei). This is no longer the case when all of these
virtual evidences are present. Now, the belief on Yi is not
only influenced by vei, but also by all other virtual evidences. As the result, the posterior probabilities of Yi ’s are
NOT equal to P(Yi | sei ). Therefore, what is needed is a
method that can convert a set of soft evidences to one or
more likelihood ratios which, when applied to the BN,
update the posterior probability of Yi to P(Yi | sei).
Algorithm 1 presented below accomplishes this purpose by combining the idea of IPFP and the virtual evidence method. Roughly speaking, this algorithm, like the
IPFP, is an iterative process and one soft evidence sei is
considered at each iteration. If the current probability of
Yi equals P(Yi | sei ), then it does nothing, otherwise, a new
virtual evidence is created based on the current probability of Yi and the evidence P(Yi | sei ). We will show that
when this algorithm converges, the probability of Yi is
equal to P(Yi | sei). To better describe the algorithm, we
adopt the following notations:
y P: the prior probability distribution.
y Pk: the probability distribution at kth iteration.
y vei,j: the jth virtual evidence created for the ith soft
evidence.
Algorithm 1. Consider a BN N with prior distribution
P(X), and a set of m soft evidential findings SE = (se1, se2,
…, sem) with P(Y1 | se1),…, P(Ym | sem). We use the following iteration method for belief update:
1. P0(X) = P(X); k = 1;
2. Repeat the following until convergence;
2.1 i = 1 + (k − 1) mod m ; j = 1 + ( k − 1) / m ;
2.2 construct virtual evidence vei,j with likelihood ratio
P( y i,1 | se)
P( y i, s | se)
,...,
)
L(Yi ) = (
Pk −1 ( yi ,1 )
Pk −1 ( yi , s )
where yi ,1 ,..., y i, s are state configurations of Yi ;

Proceedings of the 18th IEEE International
Conference on Tools with Artificial Intelligence (ICTAI'06)
0-7695-2728-0/06 $20.00 © 2006

2.3 Obtain Pk(X) by updating Pk-1(X) with vei,j using
standard BN inference;
2.4 k = k + 1;
■
The algorithm cycles through all soft evidences in SE.
At the kth iteration, the ith soft evidence sei is selected
(step 2.1) to update the current distribution Pk-1(X). This is
done by constructing a virtual evidence vei,j according to
equation (7). The second subscript here, j, is the number
of virtual evidences created for sei, it is incremented in
every m iterations. When converged, we can form a single
virtual evidence node vei for each soft evidence sei with
the likelihood ratio that is the product of likelihood ratios
of all vei,j, ve i = ∏ j ve i , j . The convergence and correctness of Algorithm 1 is established in Theorem 1.
Theorem 1. If the set of soft evidence SE = (se1, se2,
…, sem) is consistent, then Algorithm 1 converges with
joint distribution P* (X), and P* (Yi) = P(Yi | sei) for all sei
in SE.

4.2 Iteration on Local Distributions
Algorithm 1 may become expensive when the given
BN is large because it updates the beliefs of the entire BN
in each iteration (step 2.3). Following is another algorithm
that iterates virtual evidence on joint distribution of only
evidence variables:
Algorithm 2. Consider a Bayesian network N and a set
of m soft evidential findings SE = (se1, se2, …, sem) to N
with P(Y1 | se1),…, P(Ym | sem). Let Y =Y1 ∪ … ∪ Ym. We
use the following iteration method for belief update:
1. Use any BN inference method on N to obtain P(Y),
the joint distribution of all evidence variables.
2. Apply IPFP on P(Y), using P(Y1 |se1), P(Y2 | se2), …,
P(Ym | sem) as the probability constraints. Then we
have P(Y | se1, se2, …, sem).
3. Add to N a virtual evidence dummy node to represent
P(Y | se1, se2, …, sem) with likelihood ratio L(Y) calculated according to equation (7).
4. Apply L(Y) as a single piece of virtual evidence to
update beliefs in N.
■
Algorithm 2 also converges to the I1-projection of P(X)
on the set of soft evidences SE, even though the iterations
are carried out only on a subset of X.
Theorem 2. Let R1(Y1), R2(Y2), …, Rm(Ym) be probability constraints on distribution P(X). Let Y = ∪i Yi and Y ⊆
Z ⊆ X. Suppose from IPFP we get the I1-projection of
P(Y) on {R1, R2, …, Rm} as Q(Y) and the I1-projection of
P(Z) on {R1, R2, …, Rm} as Q’(Z). Let Q(X) and Q’(X) be
obtained by applying the Jeffrey’s rule on P(X) using
Q(Y) and Q’(Z). Then Q(X) = Q’(X).

4.3 Time and Space Performance

The iterations of Algorithm 1, Algorithm 2 and Big
Clique algorithm all lead to the same distribution. But at
each iteration, Big Clique algorithm updates beliefs of the
joint probabilities of the big clique C, Algorithm 2 updates the belief of evidence variables Y, and Algorithm 1
updates the belief of the whole BN, or say, of all variables
in X. Clearly, Y ⊆ C ⊆ X. However, the time complexity
for one iteration of Big Clique is exponential to |C|, and
Algorithm 2 exponential to |Y|, because both require
modifying a joint distribution (or potential) table. On the
other hand, the time complexity of Algorithm 1 equals to
the complexity of the BN inference algorithm it uses for
belief update. Both Big Clique and Algorithm 2 are space
inefficient. Big Clique needs additional space for the joint
potential of C, whose size is exponential to |C|. Algorithm
2 also needs additional space for the joint distribution of
Y, and the dummy node of virtual evidence in Step 4 leads
to a CPT with size exponential to |Y|. In contrast, Algorithm 1 only needs additional space for virtual evidence,
which is linear to |Y|.
Algorithm 2 is thus more suitable for problems with a
large BN but a few soft evidential findings and Algorithm
1 is more suitable for small to moderate-sized BNs. Also,
both Algorithm 1 and 2 have the advantage that users do
not have to stick to and modify the junction tree when
conducting inference with soft evidence. They can be
easily implemented as wrappers on any BN inference engines.

To empirically evaluate our algorithms and to get a
sense of how expensive these approaches may be, we
have conducted two experiments with artificially made
networks of different sizes. We implemented our algorithms as wrappers on a Junction-Tree-based BN inference algorithm. The reported memory consumption does
not include those that were used by the Junction Trees,
but the reported running time is the total running time.
The first experiment used a BN of 15 binary variables.
The results, as can be seen in Table 1 showed that both
the time and memory consumptions of Algorithm 1 increase slightly when the number of evidences increases.
However, those for Algorithm 2 increase rapidly, consistent with our analysis.
Table 1. Experiment 1
# Iterations
(Alg 1|Alg 2)
24
14
79
23
95
17

Exec. Time
(Alg 1|Alg 2)
0.57s
0.62s
0.63s
0.83s
0.71s
15.34s

Memory
(Alg 1|Alg 2)
590,736
468,532
726,896
696,960
926,896
2544,536

Experiment 2 involved BN of different sizes. In all
cases we entered the same 4 soft evidential findings involving a total of 6 variables. AS shown in Table 2, the
running time of Algorithm 2 increases slightly with the

Proceedings of the 18th IEEE International
Conference on Tools with Artificial Intelligence (ICTAI'06)
0-7695-2728-0/06 $20.00 © 2006

Table 2: Experiment 2.
Size
of N
30
60
120
240

# Iterations
(Alg 1|Alg 2)
43

14

Exec. Time
(Alg 1|Alg 2 (IPFP))
0.58s
0.67s (0.64s)
0.71s
0.69s (0.66s)
1.71s
0.72s (0.66s)
103.1s
3.13s( 0.72s)

Memory
(Alg 1|Alg 2)
721,848
691,042
723,944
691,424
726,904
691,416
726,800
696,842

6. Conclusions
In this paper, we analyzed three existing belief update
methods for Bayesian networks and established that belief
update with one piece of virtual evidence or soft evidence
is equivalent to an IPFP with a single constraint. Besides,
IPFP can be easily applied to BN with the help of virtual
evidence. We proposed two algorithms for belief update
with multiple soft evidences by integrating methods of
virtual evidence, IPFP and traditional BN inference with
hard evidence. Compared with previous soft evidential
update methods such as Big Clique, our algorithms have
practical advantage of being independent of any particular
BN inference engine.

7. References

5. Experiments and Evaluation

# of
findings
2
4
8

increase of the network size. Especially, the time for IPFP
(the time in parentheses) is stable when the network size
increases, which means that most increased time was
spent on constructing the joint probability distribution
from the BN (Step 1 of Algorithm 2). These experiments
results confirm our theoretical analysis for the proposed
algorithms.

[1] R. Jeffrey, The Logic of Decisions, 2nd Edition, University
of Chicago Press. 1983.
[2] J. Pearl, Probabilistic Reasoning in Intelligent Systems:
Networks of Plausible Inference. Morgan Kaufman, San Mateo,
CA. 1988.
[3] J. Pearl. “Jeffery’s Rule, Passage of Experience, and NeoBayesianism”. In H.E. et al. Kyburg, Jr., editor, Knowledge
Representation and Defeasible Reasoning, 245-265. Kluwer
Academic Publishers. 1990.
[4] Y. Peng and Z. Ding, “Modifying Bayesian Networks by
Probability Constraints”, in Proceedings of 21st Conference on
Uncertainty in Artificial Intelligence, Edinburgh, Scotland, July
26-29, 2005.
[5] M. Valtorta, Y. Kim, and J Vomlel, “Soft Evidential Update
for Probabilistic Multiagent Systems”, International Journal of
Approximate Reasoning, 29(1), 71-106, 2002.
[6] J. Vomlel, “Methods of Probabilistic Knowledge Integration”, PhD Thesis, Department of Cybernetics, Faculty of Electrical Engineering, Czech Technical University, December 1999.

Query Enrichment for Web-Query
Classification
DOU SHEN and RONG PAN
Hong Kong University of Science and Technology
JIAN-TAO SUN
Microsoft Research Asia
and
JEFFREY JUNFENG PAN, KANGHENG WU, JIE YIN, and QIANG YANG
Hong Kong University of Science and Technology

Web-search queries are typically short and ambiguous. To classify these queries into certain target
categories is a difficult but important problem. In this article, we present a new technique called
query enrichment, which takes a short query and maps it to intermediate objects. Based on the
collected intermediate objects, the query is then mapped to target categories. To build the necessary
mapping functions, we use an ensemble of search engines to produce an enrichment of the queries.
Our technique was applied to the ACM Knowledge Discovery and Data Mining competition (ACM
KDDCUP) in 2005, where we won the championship on all three evaluation metrics (precision, F1
measure, which combines precision and recall, and creativity, which is judged by the organizers)
among a total of 33 teams worldwide. In this article, we show that, despite the difficulty of an abundance of ambiguous queries and lack of training data, our query-enrichment technique can solve
the problem satisfactorily through a two-phase classification framework. We present a detailed
description of our algorithm and experimental evaluation. Our best result for F1 and precision is
42.4% and 44.4%, respectively, which is 9.6% and 24.3% higher than those from the runner-ups,
respectively.
Categories and Subject Descriptors: H.2.8 [Database Management]: Database Applications—
Data mining; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval;
I.5.2 [Pattern Recognition]: Design Methodology—Classifier design and evaluation
General Terms: Algorithms, Experimentation, Performance
Additional Key Words and Phrases: Query classification, query enrichment, synonym-based classifier, ensemble learning, KDDCUP2005

This work was supported by NEC China Lab under Grant NECLC05/06.EG01, and Hong Kong
RGC Grant HKUST 6187/04E.
Authors’ addresses: D. Shen, R. Pan, J. J. Pan, K. Wu, J. Yin, and Q. Yang, Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Clear Water
Bay, Kowloon, Hong Kong, China; email: {dshen,panrong,panjf,khwu,yinjie,qyang}@cse.ust.hk; J.T. Sun, Microsoft Research Asia, 49 Zhichun Road, Beijing, China; email: jtsun@microsoft.com.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is
granted without fee provided that copies are not made or distributed for profit or direct commercial
advantage and that copies show this notice on the first page or initial screen of a display along
with the full citation. Copyrights for components of this work owned by others than ACM must be
honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers,
to redistribute to lists, or to use any component of this work in other works requires prior specific
permission and/or a fee. Permissions may be requested from Publications Dept., ACM, Inc., 2 Penn
Plaza, Suite 701, New York, NY 10121-0701, USA, fax +1 (212) 869-0481, or permissions@acm.org.

C 2006 ACM 1046-8188/06/0700-0320 $5.00
ACM Transactions on Information Systems, Vol. 24, No. 3, July 2006, Pages 320–352.

Query Enrichment for Web-Query Classification

•

321

1. INTRODUCTION
Historically, search engine technologies and automatic text-classification techniques have progressed hand-in-hand. Ever since the early articles by the
pioneers [Jones 1971; Lewis and Gale 1994], people have recognized the
possibility of conducting both web search through classification, and vice
versa [Page et al. 1998; Beeferman and Berger 2000; Chekuri et al. 1997;
Chen and Dumais 2000; Kang and Kim 2003]. The 2005 ACM Knowledge
Discovery and Data Mining competition (KDDCUP2005, for short) made
this connection even stronger. This competition series (publicly available at
http://www.acm.org/sigs/sigkdd/kddcup/), which has a long history dating back
to 1997, is open to researchers and practitioners worldwide. Being one of the
most prominent data mining competitions, the datasets used therein are often
employed later as benchmarks. The task in KDDCUP2005 is to accurately automatically classify a subset of query-log data from one month in 2005 out of the
MSN search engine (http://search.msn.com), one of the major search engines,
into a set of given target categories. The log data contains 800,000 queries and
the target categories consist of 67 predetermined classes provided by the organizers. The dataset is available as a public domain benchmark for query classification (http://www.acm.org/sigs/sigkdd/kdd2005/kddcup.html). Several example queries are shown in Table I. An illustration of the hierarchical structure
for the target categories is shown in Figure 1 (see Appendix A for details).
The KDDCUP2005 task highlights the importance and difficulties of query
classification, which is a way to understand queries [Li et al. 2005; Shen et al.
2005; Kardkovács et al. 2005; Vogel et al. 2005; Shen et al. 2006]. Query classification can be used to support several important tasks in information retrieval
and Web search. In information retrieval, a potential area is to construct user
models so as to cater to both individual and group user preferences. The classification of user queries is a component of both constructing and utilizing user
models. In Web search, an important application is to organize the large number of Web pages in the search result after the user issues a query, according
to the potential categories of the results. Query classification can be used to
effectively organize these results. Furthermore, in Web search, many search
engine companies are interested in providing commercial services in response
to user queries, including targeted advertisement, product reviews, and valuedadded services such as banking and transportation, according to the categories.
In these applications of Web search, query classification is very important. Instead of classifying queries directly, some previous work focuses on classifying
search results as an alternative way to understand queries [Chen and Dumais
2000].
However, there are several major difficulties which hinder the progress of
query classification. First, many queries are short and query terms can be
noisy. As an example, in the KDDCUP2005 dataset the 800,000 queries vary
a lot. They can be as simple as a single number, such as “1939,” or as complicated as a piece of programming code involveing more than 50 words. Figure 2
shows statistical information about the number of words contained in each
query and their frequencies in the 800,000 queries. From this figure we can see
ACM Transactions on Information Systems, Vol. 24, No. 3, July 2006.

322

•

D. Shen et al.
Table I. Examples of Queries
1967 shelby mustang
actress hildegarde
Aldactone
alfred Hitchcock
amazon rainforest
section8rentalhouses.com
Sakpsabancnnhayat
auto price
a & r management” property management Maryland
netconfig.exe

Fig. 1. Illustration of the hierarchy structure of 67 target categories.

Fig. 2. Frequency of queries with different lengths.

that queries containing three words are the most frequent (22%). Furthermore,
79% of the queries have no more than four words.1 Each query is a combination
of words, names of persons or locations, URLs, special acronyms, program segments, and malicious codes. Some contain words which are very clean, while
others may contain typos or meaningless strings which are totally noisy. Some
words may just have their meanings as defined in a static dictionary, whereas
others may have some special meanings when used on the Internet.
1 This

observation differs slightly from those given in Silverstein et al. [1999] and Jansen [2000],
since there are no empty queries within the set of 800,000.

ACM Transactions on Information Systems, Vol. 24, No. 3, July 2006.

Query Enrichment for Web-Query Classification

•

323

A second difficulty of Web-query classification is that one user query often
has multiple meanings. For example, “Apple” can mean a kind of fruit or a computer company. In this case, query classification should provide both meanings,
ranked according to the likelihood of each. For “Apple,” the result can be given
in the form of “Computers\Hardware; Living\Food and Cooking.”
A third difficulty is that the meanings of queries may also evolve over time.
For example, the word “podcast” can now be interpreted as a kind of Web audio
blog-site, but such a word cannot be found in a traditional static dictionary.
The distribution of the meanings of this term is therefore a function of time
on the Web. In order to preserve the existing meanings of words as well as
finding out new ones, we cannot simply classify a query solely based on a static
and out-of-date training set [Beitzel et al. 2005]. Instead, we should obtain the
meanings of queries from the web in an online manner. Our approach is to
retrieve the most related documents for the query, and extract their semantic
features. The distribution of the meanings on the Web should influence the
ranking of the target categories, among other factors. Also, so as to obtain a
better and unbiased understanding of each query, we should not rely on a single
search engine, but combine multiple results from different search engines.
In summary, an important issue is how to automatically and effectively classify a large number of queries that are inherently short, noisy, and ambiguous
when there is a lack of clear definitions for this data (such as a dictionary)
and a lack of additional training data (such as a labeled query-log file). A major
traditional approach in handling short and ambiguous queries is through query
expansion using relevance feedback [Chang and Hsu 1998], whereby users provide information as to which retrieved result is relevant to the query. However,
this does not work for our problem because in many web search problems, the
results must be generated automatically and efficiently for the user’s. Another
major method is query expansion by using a dictionary or thesaurus [Voorhees
1994], whereby the user’s query words are enlarged to contain additional information. However, in a web search domain, many queries consist of newly
created words and their intended meanings are moving targets. In some sense,
generatings a sufficient thesaurus to handle query expansion requires that we
solve the query classification problem in the first place. According to our statistics, most queries are short; an illustration is shown for randomly selected
queries in Figure 2, where the occurrence frequency of queries is compared to
the many number of words in each query. In addition, queries can be noisy as a
result of misspellings. A distribution of queries meanings is shown in Figure 3,
where we plot the query count in percentage against the number of meanings
that each query corresponds to, using the 111 randomly chosen validation samples from the KDDCUP2005 dataset. These 111 queries are labeled by human
experts. As can be seen from Figure 3, many queries have more than three possible meanings. All of these characteristics indicate that we cannot solely rely
on a static thesaurus to classify them.
Recently, some interesting work has emerged on using query logs to expand the meanings of user queries [Wen et al. 2002; Beeferman and Berger
2000]. However, such a method requires that there exists a query log for us to
use, which contains a mapping from submitted queries to clicked Web pages.
ACM Transactions on Information Systems, Vol. 24, No. 3, July 2006.

324

•

D. Shen et al.

Fig. 3. Percentage of queries with different numbers of meanings (in terms of manually assigned
labels).

However, in general, such logs are not available for the timely training and
application of query classification models, as in the case of the KDDCUP2005
competition. Even when we can obtain such a log file, the mapping from queries
to clicked Web pages does not automatically provide the target categories because the pages themselves still need to be mapped to different categories. In
addition, such mapping may be biased by the choice of any single search engine
that collects the query logs, and thus the classification approach adopted may
not be as general and objective as possible. In order to obtain an objective view
of the categories that each query can be mapped to, views from multiple popular
search engines should be consulted. Finally, obtaining up-to-date click-through
data from multiple search engines raises serious privacy, legal, and business
issues, which are beyond the means of many practitioners, businesses, and
governments.
This article presents a new approach to classifying large quantities of search
queries automatically. Our approach is called query enrichment, which takes a
short query and classifies it into target categories by making use of a set of intermediate objects. In our application, these intermediate objects are Web pages
and category taxonomies such as that of the Open Directory Project (ODP).2
Query enrichment makes the following basic assumptions:
— Given that the Web is one of the largest data sources available, although
we do not know the true meanings of many user queries, their intended
meanings should be reflected by the Web as a whole. In addition, although a
particular search engine may not fully reflect the intended meanings of the
query for the user, when we combine many different search engines and Web
directories, the meanings of the query that are embedded in the Web as a
whole can be extracted. In other words, by searching the Web for answers,
users have expressed their “trust” that their answer is somewhere located on
its Web. By this assumption, our approach can be to first “enrich” a query by
covering its potential meanings through Web search, and then classify the
search results to the target categories.
— A set of objects exist that can “cover” the target categories. An example set
of intermediate objects is the ODP, which contains a collection of more than
170,000 different categories and can be taken as an extensive taxonomy.
2 http://dmoz.com

ACM Transactions on Information Systems, Vol. 24, No. 3, July 2006.

Query Enrichment for Web-Query Classification

•

325

— We use results from search engines to provide the intermediate objects. Even
though a particular search engine may be biased and therefore cannot guarantee high-quality answers to all the search queries from this huge collection,
each search engine might provide a different viewpoint in interpreting the
user query. The result of different viewpoints should be combined into a coherent whole for better coverage and higher answer robustness.
This last point means that we can submit the queries to multiple search
engines, thus the intended answers are among the returned results (i.e., resultant Web pages). In this way, the relative “weights” of multiple search engines
in generating collective answers to the queries can be learned from a validation
dataset, rather than being predefined.
One possibility is to exploit a metasearch engine. Metasearch engines [Howe
and Dreilinger 1997; Selberg and Etzioni 1995] submit queries to multiple different search engines and integrate the results into a single list to be returned
to the user. Our task is similar, except that in our case we are interested in
query classification rather than search. Therefore, we will follow a similar approach in which we submit the query to different search engines and then
classify the query based on the search results from each search engine. In a
final step, we integrate the classification results corresponding to each search
engine. In machine learning, this is known as an ensemble of classifiers. Similar
to metasearch engines, the ensemble of classifiers combines the results of different component classifiers using a collection of weights that are learned from
a validation dataset. However, as opposed to metasearch engines, the ensemble of classifiers is aimed at classifying a query using a collection of classifiers
where each one can be somewhat biased. By integrating the classification results, they compliment each other, and the results are typically more robust
than any single classifier. General introductions to ensembles of classifiers are
given in Hansen and Salamon [1990], Bauer and Kohavi [1999], Caruana et al.
[2004], and Fan et al. [1999].
In general, query enrichment consists of two major steps:
— First, we replace a query by a set of objects wherein the meanings of the
query are embedded;
— Second, we classify the query based on the set of objects into ranked target
categories.
In order to enrich an input query, in our application, we submit the query to
multiple Web search engines and obtain the search results as a ranked list. This
ranked list consists of two types of objects: the resultant Web pages and intermediate taxonomies such as the ODP. For example, a query “Apple” submitted
to Google (http://www.google.com) returns a list of resultant Web pages that can
be mapped to “Computers\Systems; Business\Food and Related Products” in
the ODP directory. The Web pages and the set of categories in intermediate
taxonomies combined serve as the basis to map to the target categories.
The rest of the article is as follows. In the next section, we give an overview
of our approach. In Sections 3 and 4, we explain the two phases of our solution,
respectively. Phase I corresponds to the training phase of machine learning
ACM Transactions on Information Systems, Vol. 24, No. 3, July 2006.

326

•

D. Shen et al.
Table II. Example Queries and Their Categories
Query
Aerosols

Cross pendant

Aberdeen police department

Categories
Information\Science & Technology
Living\Health & Fitness
Living\Family & Kids
Living\Gifts & Collectables
Living\Fashion & Apparel
Living\Religion & Belief
Shopping\Stores & Products
Shopping\Buying Guides & Researching
Information\Law & Politics
Information\Local & Regional

algorithms. Phase II corresponds to the testing phase. In Phase I, two kinds
of classifiers are developed as base classifiers. One is synonym-based and the
other is statistics-based. Phase II consists of two stages. In the first stage, the
queries are enriched such that for each query, its related Web pages together
with their category information (if they so have) are collected through the use
of search engines. In the second stage, the objects in the enriched result are
classified through the base classifiers trained in Phase I. Based on the classification results obtained by the base classifiers, we get the last classification
results through an ensemble of classifiers. In Section 5, we describe our experimental results on the KDDCUP2005 tasks. We show that through using our solutions we can achieve superior performance as compared to other competitive
methods, and similar performance to human labelers when we appropriately
integrate search engines and combine query classification results.
2. PROBLEM DEFINITION AND OVERALL APPROACH
The query classification problem is not as well-formed as others such as text
classification. The difficulties include short and ambiguous queries and a lack
of training data. In this section, we give a formal definition of the query classification problem, which is inspired by the tasks of KDDCUP2005 competition.
Query Classification. The aim of query classification is to classify a user
query Q i into a ranked list of n categories Li1 , Li2 , . . . , Lin , among a set of N
categories {L1 , L2 , . . . , L N }. Among the output, Li1 is ranked higher than Li2 ,
Li2 is higher than Li3 , and so on.
The queries are collected from real search engines submitted by Web users.
The meanings and intention of the queries are subjective.
The target categories consist of a tree with each node representing a category.
The semantic meanings of each category are defined by the labels along the path
from root to corresponding node.
In addition, the training data must be found online because in general, labeled training data for query classification is very difficult to obtain.
Table II shows several examples of queries and their categories chosen from
test data of the KDDCUP2005 competition. As we can see, each query, may have
more than one category, and each category is ordered according to the probabily
that the query belongs to it.
ACM Transactions on Information Systems, Vol. 24, No. 3, July 2006.

Query Enrichment for Web-Query Classification

•

327

To facilitate understanding the definition of query classification as well as
the formal discussion, we provide the following definitions.
Definition 1 (Target Categories).
we will classify user queries into.

Target categories are the categories that

For example, for the KDDCUP2005 task, there are 67 categories provided by
the organizers as the final targets of query classification (see Appendix A for
details).
Definition 2 (Intermediate Taxonomies). Associated with a search engine
or Web directory, there is often a taxonomy of categories. We wish to distinguish
between the target categories in Definition 1 and existing taxonomies on the
Web. Thus we call the latter “intermediate taxonomies” in this article.
For example, the ODP defines a taxonomy that consists of more than hundreds of thousands of categories organized in a tree structure.
Definition 3 (Query Enrichment Function). The query enrichment function u is a function that maps from a query Q to a set of intermediate objects
on the web: u: Q → intermediate objects. An example of an intermediate object
is a Web page. Another type of object is the category label in an intermediate
taxonomy such as the ODP directory.
Definition 4 (Query Classification Function). A query classification function f Q2C maps from a user query Q to one or more of the target categories:
f Q2C : Q → target categories.
Definition 5 (Text Classification Function). A text classification function
hT 2C maps from a body of text T to one or more of the target categories: hT 2C :
T → target categories.
Definition 6 (Intermediate Taxonomy to Target Category Mapping). The
intermediate taxonomy to target category mapping function l I T 2C is a function
that maps from a category in the intermediate taxonomy to one or more target
categories.
The query classification function f Q2C (Definition 4) can be constructed by one
of two strategies. The first corresponds to using text-based statistical classifiers.
For each query Q:
— We first map the query Q to web pages T ;
— We then apply a text classification function hT 2C so as to map T to the target
categories.
Alternatively, we can build a synonym-based classification function as follows:
— We first submit the query Q to search engines and obtain the category labels of
some intermediate taxonomy (these categories differ from target categories).
— We then map the intermediate categories thus obtained to the target
categories.
ACM Transactions on Information Systems, Vol. 24, No. 3, July 2006.

328

•

D. Shen et al.

Fig. 4. The architecture of our approach.

By combining the aforementioned two strategies, we can obtain the ensemblebased approach.
As in typical machine learning applications, we adopt two phases in our solution. In Phase I, which corresponds to the training phase of machine learning
algorithms, we collect data from the Web for training classifiers that can be used
to classify intermediate objects to target categories. In Phase II, corresponding
to the testing phase in machine learning research, we apply the classification
functions thus built to the target categories. The classifiers learned in Phase I
are applied to classify the queries. The overall architecture of our approach is
shown in Figure 4 and detailed architectures of the two phases are shown in
Figure 5.
For ease of understanding, we will take the KDDCUP2005 task as an example when describing our approach.
3. PHASE I: CLASSIFIER TRAINING
We now discuss Phase I of our approach in detail. In this phase, we train classifiers for mapping from intermediate objects to target categories. As noted, a
main problem here is the lack of training data, a difficulty which makes many
previous machine learning methods inapplicable. The objective in Phase I is to
collect the data from the Web that can be used to train classification functions.
3.1 Synonym-Based Mapping from Intermediate Taxonomies to Target Categories
We first discuss how to construct the mapping functions l I T 2C from intermediate
to target categories via synonym-based mapping functions. The taxonomies
from various search engines can differ both from each other, and from that of the
target categories. The mapping function l I T 2C can be built by synonym-based
ACM Transactions on Information Systems, Vol. 24, No. 3, July 2006.

Query Enrichment for Web-Query Classification

•

329

Fig. 5. The two phases: training and applying classifiers.

keyword matching. In this approach, we compare the terms used to describe
the target categories with those in the intermediate taxonomies.
Consider two categories, c1 and c2 , where c1 is a category label from the
target categories and c2 is a label from an intermediate taxonomy. If they
share some of the same keywords, we can map c2 to c1 directly. For example,
in the KDDCUP2005 task, the target categories have two levels, for example,
“Computers\Hardware.” The second level specifies a particular field within the
first level. For most target categories, we only consider keywords in the second
level because they cannot be confused with other categories. A typical example is “Computers\Internet and Intranet.” Even if we do not consider the first
level, “Computers,” there are no other categories which may be confused with
“Internet and Intranet.” Therefore, if a category from an intermediate taxonomy
contains either “Internet” or “Intranet,” we will map it to “Computers\Internet
and Intranet.” However, many categories are more difficult to deal with. For
these, we require that the keywords in the first and second levels match the
categories in the intermediate taxonomy simultaneously. Otherwise, we cannot distinguish between two categories that share the same keywords only in
the second level, such as “Computers\Hardware” and “Living\Tools and Hardware.” Although both have the keyword “Hardware” in the second level, they
ACM Transactions on Information Systems, Vol. 24, No. 3, July 2006.

330

•

D. Shen et al.

belong to the two different domains “Computers” and “Living,” as defined by
the first level.
In order to improve the coverage of the mapping function l I T 2C , we can in
advance extend the keywords in the category names to include both singular
and plural forms. For example, “Living\Book and Magazine” is extended to
“Living\Book and Magazine and Books and Magazines.”
After applying the preceding procedure, we may still miss a large number of
mappings. Many categories in the intermediate taxonomy do not occur in target
categories, although they share words with the same meanings. In response, we
expand the keywords in each label in the target categories through WordNet
[Miller et al. 1990] . For example, the keyword “Hardware” is extended to “Hardware and Devices and Equipments” and the keyword “Movies” is extended to
“Movies and Films.”
3.2 Text Data Collection and Statistical Classifier Training
We now discuss how to build the statistical classification function hT 2C to map
from a body of text T to a target category. As we will discuss in Section 4.2,
synonym-based classifiers have a certain drawback—they have low recall. In
order to address this problem, we consider statistical classifiers to help classify
queries. A statistical classifier classifies a query based on the semantic content
of a text, which can provide better recall as well as precision. Even when the
synonym-based classifier fails to give any meaningful mapping for a query, the
query can still be classified by a statistical classifier. Any kind of statistical text
classifiers, such as naive Bayes [McCallum and Nigam 1998], KNN [Yang 1999],
and support vector machine (SVM) [Joachims 1998, 1999], can be applied.
To construct a statistical classifier, the first step is to collect training data.
This step is nontrivial, since no training data is provided explicitly, as we have
stated. In order to collect the training data, we use the following methods:
— First, we collect Web pages from some online manually labeled Web page
directories, such as the ODP.
— By applying function l I T 2C , we can map a collected web page into the target
categories. Thus, the mapped Web pages can be used as the training document for target categories.
— The training data among the target categories is usually extremely unbalanced. In order to remove the impact of unbalanced distributions, as well as
speed-up the training step, we need to sample the training documents.
After training examples are collected for each target category, we can follow
the traditional procedure to train statistical classifiers, including some proper
preprocessing steps and parameter tuning.
To clarify the previous procedure, we illustrate it through our solution
to the KDDCUP2005 task. We use the SVM classifier because of its high
generalization performance when used for text classification tasks and its easy
implementation. About 1,540,000 Web pages are collected from ODP in total.
After applying the mapping function between the ODP and KDDCUP2005
categories, only 500,000 Web pages fall into KDDCUP2005 categories. To
ACM Transactions on Information Systems, Vol. 24, No. 3, July 2006.

Query Enrichment for Web-Query Classification

•

331

address the unbalanced distribution of Web pages among the 67 categories, we
randomly selected 15,000 Web pages from those categories containing more
than 15,000 Web pages and keep all the pages for the categories with less than
15,000. Document frequency (DF) and information gain (IG) methods are used
for feature selection [Yang and Pedersen 1997]. Then we use the SV M light software package (http://svmlight.joachims.org/) to train an SVM. A linear kernel
is used and the one-against-the-rest approach is applied for the multiclass case.
4. PHASE II: QUERY CLASSIFICATION
Phase I of our algorithm is designed to collect data for training the mapping
functions, which include synonym- and statistics-based classifiers. Phase II of
our algorithm is devoted to subsequently classifying a query to one or more
target categories based on the classifiers.
Phase II is conducted in two stages. The first is to enrich the queries by
searching those related pages that can specify the meanings of the queries
most accurately. Enrichment is necessary because the queries are rather short,
and their meanings ambiguous. We perform the enrichment process by finding
relevant text from related Web pages, as well as the category information of
these Web pages (if they so have) through Web search.
The second stage is to classify the queries based on the data collected in the
first stage and classifiers trained in Phase I. In this stage, we takes the two kinds
of classifiers with totally different mechanisms that were trained in Phase I
as the base classifiers and develop several ensemble classifiers with different
ensemble strategies to classify the queries. Experimental results show that
ensemble classifiers can improve the classification performance significantly.
4.1 Query Enrichment
Query enrichment is a key step because our goal is to classify short and ambiguous queries, without any additional descriptions about them. After this step,
two kinds of information for each query are collected. One is the list of Web
pages related to the target query. The other is the set of categories corresponding to the related pages. These two kinds of information will be leveraged by
the two kinds of classifiers trained in Phase I, respectively.
In our approach, we send each query to multiple search engines that can
provide options for both directory and Web search. Directory search in a search
engine refers to search algorithms that return the related pages of a query,
together with the page’s categories. Since these categories of Web pages are labeled by humans, it is appropriate to use them to classify the queries. However,
not all the pages indexed by the search algorithm contain category information;
in this case, Web search can return more related pages than directory search.
Based on the contents of the pages returned by Web search, we can classify the
queries using a text classification algorithm.
In summary, to enrich a query through search engines, we use the following
three steps:
(1) We first try to get the related pages through directory search;
(2) If we cannot get any results from Step 1, we try a Web search;
ACM Transactions on Information Systems, Vol. 24, No. 3, July 2006.

332

•

D. Shen et al.

(3) If we still cannot get any results, the queries must either be too noisy or
totally meaningless. Thus, we use some preprocessing approaches to clean
them up and then resubmit them to directory and Web search, in turn,
as done in Steps 1 and 2. If, after this step, there is still no result returned, the queries will not be processed any further, and no classification results are generated. Currently, two preprocessing approaches are
employed for the cleaning-up, which will be described in detail in the next
example.
In our solution to the KDDCUP2005 task, we use three search engines, including Google, Looksmart, and a search engine developed by ourselves based
on Lemur.3
Now, let us use Google as an example to illustrate the three steps in detail.
— Initially, all 800,000 queries are preprocessed by removing special characters such as “,#,%,” while keeping the letters and digits. Then these 800,000
queries are sent to the Google directory search. We are able to retrieve related
pages for about 500,000 (63%) queries in this way.
— We then send the remaining 300,000 queries to the Google Web search and
get results for an additional 200,000 queries.
— For the remaining 100,000 queries, we conduct further preprocessing. We
use the function of “Did you mean,” provided by Google, to trigger a search
for queries that are the most relevant to the originals. For example, given
the query “a cantamoeba,” neither Google directory search nor Web search
returned any results. However, by trying “Did you mean,” Google could return
results for the word “acanthamoeba,” which is related to health, disease, and
medicine. In this way, we can get the results for another set of 60,000 queries
from the Google directory search or Web search.
— However, after this step, there are still 40,000 queries left without any results. These are very noisy. They usually consist of connected words without
spaces, long, meaningless clobbers, or URL addresses containing parameters
or even malicious codes. We try to render these queries meaningful by extracting words from them through a maximum-length matching method based on
the WordNet dictionary. This method tries to extract as many meaningful
words as possible and to make each as long as possible. Taking the query
“wheelbearingproblemsdamage” as an example, Google cannot return any
results through either a directory or Web search or even, “Did you mean” a
function. Therefore, we can split the whole query into the four meaningful
words “wheel bearing problems damage.” After doing this, we can get reasonable results from the Google directory or Web search. In this way, we can
get the results for 30,000 of the remaining 40,000 noisy queries.
— The remaining 10,000 queries cannot receive any pages from Google as
answers. These queries are inherently noisy and meaningless, such as
“dddddfdfdfdfdf.” Therefore, our classifier will not return any answers for
these queries, although we can assign labels to them according to the prior
3 http://www.lemurproject.org/

ACM Transactions on Information Systems, Vol. 24, No. 3, July 2006.

Query Enrichment for Web-Query Classification

•

333

distribution of the target category. This potentially reduces recall (one measurement of classification performance) for the query classifier. But because
these outliers only correspond to a tiny portion of all queries (1.25%), they
do not have much effect on the final classification results.
We follow the same steps when using Looksmart. Among the 800,000 queries,
about 200,000 have directory search results. Another 400,000 have Web search
results, and the remaining 200,000 have none.
The third engine we use was developed by ourselves based on Lemur. We first
crawled more than one million Web pages with the category information from
ODP. Then we indexed this collection of pages with Lemur. Given a query, Lemur
can retrieve a number of pages that are most relevant, together with their
corresponding categories. Therefore, the function of this Lemur-based search
engine and the ODP data is similar to the directory search provided by Google
and Looksmart. Using this search engine, we can retrieve related pages for
most of the 800,000 queries (except 35,000).
In summary, after enriching queries through a search engine, we can obtain
two lists for each query. One is the returned Web pages list from a search engine,
and the other is a category list attached to the Web pages in the Web page list.
Note that some Web pages have no category information. These two lists will
be leveraged by different kinds of classifiers individually.
4.2 Query Classification Using Synonym-Based Mapping Functions
Using the synonym-based category mapping functions discussed in Section 3.1,
we can now build a query-to-target-category mapping function f Q2C . In particular, for each query Q, through the function u constructed in Section 4.1, we
can obtain a set of intermediate categories S of the related Web pages returned
by a given search engine. Then we apply the category-mapping function l I T 2C
to S, which returns an ordered list of target categories.
Using different search engines, we might obtain different intermediate categories. For each category of a given search engine’s taxonomy, we can construct
a mapping function l I T 2C between it and the target categories, as shown in
Section 3.1. After obtaining these mapping functions, we can perform query
classification based on the intermediate categories S returned by the search
engine. We then map intermediate to target categories using the constructed
mapping functions. We keep track of the number of times each target category
is mapped onto. We can then obtain the target categories, together with their
occurrence frequencies. By ranking categories in terms of occurrence frequencies, we get a ranked list of final target categories into which the query can
be classified. Based on the various intermediate category lists and their corresponding mapping functions, we obtain different classification results. The
classification functions thus obtained are known as synonym-based classifiers.
Based on the aforementioned approach, synonym-based classifiers tend to
produce results with high precision and low recall. They produce high precision because they are based on manually annotated Web pages and can utilize
the classification knowledge of human editors. Therefore, once a mapping function is constructed, the classification result is highly probable to be correct.
ACM Transactions on Information Systems, Vol. 24, No. 3, July 2006.

334

•

D. Shen et al.

Fig. 6. Illustration of the advantage of statistical classifiers.

For example, we have shown that categories in the intermediate taxonomy,
such as “. . . \Computers\. . . \ Hardware\. . .,” are mapped to the target category
“Computers\Hardware.” Therefore, once the Web pages associated with a given
query fall into the category “Computers\Hardware\Storage\Hard Drives,” we
can assign “Computers\Hardware” to the query with high confidence. Synonymbased classifiers produce low recall because it is hard to find all the mappings
from the intermediate taxonomy to the target categories by keyword mapping.
For example, about 80,000 out of 354,000 categories in Google’s taxonomy are
not mapped onto target categories. Therefore, we cannot map all the intermediate categories in the category list for a given query to the 67 target categories,
and may miss some correct categories for the query.
Another reason for the low recall problem is that a search engine may return
only a few or even no Web pages with categories. The synonym-based classifier
may fail because of the search results shortage problem. Therefore, we need to
construct other classifiers to help handle the low recall problem, which we will
describe in the next section.
4.3 Query Classification Using Statistical Classifiers
In this section we discuss the use of statistical classifiers to classify queries.
As shown in Section 4.1, after submitting a query Q to a search engine, we
get a list of Web pages. Then, we can extract a body of text from the Web
pages to capture the occurrence context of the issued query, which can help
determine its potential meanings. To accomplish this, we keep the top N results
in the returned list (the parameter N will be studied in Section 5) and use
the aggregate terms of the corresponding snippets, titles, and URL terms to
represent the query. The query’s bag of terms will be processed by stop-word
removing, stemming, and feature selection. The resultant term vector can be
used as input for the statistical classifiers and a ranked list of categories for
each query will be produced.
Statistical classifiers are expected to achieve a higher recall than that of
synonym-based classifiers. The reason is illustrated in Figure 6. For simplicity,
only two categories are considered. The circles and triangles represent samples
belonging to the two categories. The black symbols represent the training data
we collect for the two categories through the means shown in Section 3.2. The
white symbols represent the Web pages that we crawled from the ODP which
should have been mapped to the target categories, but were not because of
ACM Transactions on Information Systems, Vol. 24, No. 3, July 2006.

Query Enrichment for Web-Query Classification

•

335

the drawbacks of mapping function l I T 2C from the intermediate taxonomy to
the target categories. Given a query Q, after the query enrichment step, we
may happen to get only the white circles or triangles to represent the query.
Thus, it is clear that we cannot judge the labels of the query by synonym-based
classifiers, since there is no mapping relationship between the white circles and
triangles to the target categories. However, statistical classifiers can still obtain
the separating hyperplane based on the black circles and triangles, which can
be used to classify the white, and hence can further be used for classifying the
query Q.
4.4 Ensemble of Classifiers
From the previous two approaches, we can independently build various classifiers that can classify input queries into target categories. These approaches
are based on different mechanisms and can be complementary to each other.
Previous work has shown that the proper combination of different base classifiers can improve final classification performance [Hansen and Salamon 1990;
Kittler et al. 1998; Bauer and Kohavi 1999; Cann 2003; Fan et al. 1999]. In this
section, we consider how to combine them.
Dietterich [2000] and Alpaydin [2004] have categorized different ensemblebased methods. Of these, voting, bagging, and boosting are the major ones. In
voting, which defines a linear combination of existing classifiers, the main task
is to decide the weights used in the combination. In bagging, the base classifiers
are generated to differ by training them over slightly different training data
that is sampled from the given training set by bootstrap. In boosting, the base
classifiers are generated sequentially, where each is trained on the mistakes of
the previous. Both bagging and boosting require that there is sufficient labeled
data for generating base classifiers [Meyer and Brown 1998; Dietterich 2000].
Because in query classification, labeled data is very scarce (in our case, there are
only 111, which corresponds to 0.014% of all the data), voting becomes the most
natural choice. We thus focus on how to set the weights of different classifiers.
Figure 7 illustrates our ensemble-based method for query classification. As
we can see from the figure, the results of the classifiers are linearly combined
to generate the final category ranking. In our approach, we consider two ways
to assign the combination weights for different classifiers. The first is to make
use of the validation dataset involving a small number of queries labeled by
humans. Considering that the validation dataset is too small and easily overfitting, an alternative is to ignore this validation dataset and instead give each
base classifier equal weight. More details about the first strategy are shown to
follow.
As we can imagine, the different classifiers introduced in the preceding sections have differing performance. Some may work better than others on certain
categories. For example, a classifier may achieve high precision on one category,
while having high recall on another. This indicates that it is not proper to assign
a single weight to a classifier. Instead, we should differentiate the weight of a
classifier on different categories, according to its performance. To determine
the weights, we validate each base classifier on the validation samples. The
ACM Transactions on Information Systems, Vol. 24, No. 3, July 2006.

336

•

D. Shen et al.

Fig. 7. Illustration of the ensemble classifiers.

higher precision a classifier achieves on a given category, the higher the weight
assigned to it classifier on this category. As a result, each classifier may obtain
a weight value Wij on a target category j . Wij is defined by:
pi j
Wi j = 
,
pk j
k=1..n

where pi j is the precision of classifier i on categories j . The definition of precision will be given in Section 5.2.
Three additional ways to determine combination weights, similar to the previous, will also be discussed and tested in Section 5.
5. EXPERIMENTS AND DISCUSSIONS
To test our proposed approach, we conduct extensive experiments on the
KDDCUP2005 datasets. The experimental results validate the effectiveness
of our approach. In addition, we give an analysis of the consistency of the three
labelers on their judgments of classifier performance.
As introduced in the preceding section, we now have six classifiers in total
of three synonym-based, one statistical SVM and two ensemble. For simplicity,
we refer to the three synonym-based classifiers that rely on Google, Looksmart,
and Lemur as S1, S2, and S3, respectively. We denote the ensemble classifier
that relies on the validation dataset as EDP (since it assigns different weights
for each base classifier on (D)ifferent categories according to its (P)recision on
the category) and the one that does (Not) as EN.
5.1 Datasets
One of the KDDCUP2005 datasets contains 111 sample queries, together with
human labeled categories. These samples help exemplify the format of the
queries and provide the semantics for a tiny number of them. In fact, since
the category information of these queries is truthful, they can serve as the
ACM Transactions on Information Systems, Vol. 24, No. 3, July 2006.

Query Enrichment for Web-Query Classification

•

337

validation data for our proposed classifiers. Another dataset provided by the
organizers contains 800,000 queries in total, which are selected from MSN
search logs to test the submitted solutions. Since manually labeling all 800,000
queries is too expensive and time-consuming, the organizers at last randomly
selected 800 and invited three human labelers to label them. We denote the
three labelers (and sometimes the datasets labeled by them, if no confusion is
caused) as L1, L2, and L3, respectively. We refer to the former as the sample
dataset and the latter as the testing dataset in the following sections. Both of
these datasets can be used to evaluate the different classification approaches.
The sample dataset can in addition be used to determine the ensemble weights
in this article.
5.2 Evaluation Criteria
The evaluation criteria adopted by the KDDCUP2005 organizers are the standard measures for evaluating classification performance in information retrieval (IR), including precision, recall, and the F1-measure [Van 1979]. The
definitions of precision, recall and F1 in query classification context are given
as follows:

A:
# of queries correctly tagged as ci
i
B:
# of queries tagged as ci
i

C:
# of queries whose category is ci
i

Precision = BA
Recall = CA
× Recall
F 1 = 2×Precision
Presion + Recall
For the sample dataset, we report the precision, recall, and F1 evaluation
results for each classifier. For the testing dataset, since the three labelers are
asked to label the queries, the results reported are the average values [Li et al.
2005]. Take the calculation of F1 as an example:
Overall F1 =

3
1
(F1 against human labeler i)
3 i=1

For some of the ensemble classifiers, we need to know the performance of a
classifier on a certain category. It is easy to define such criteria according to
the preceding definition. For example, the precision of classifier i on category j
could be defined as:
Pij =

# of queries are correctly tagged as c j by classifier i
.
# of queries are tagged as c j by classifier i

Because there are 67 target categories, a random classification algorithm would
give a precision of 1/67 = 1.5% if one category is to be returned, and 5/67 =
7.5% if five are returned. Therefore, we are comparing against a baseline of
7.5% for the KDDCUP2005 task, whose requirement is to return, at most, five
results.
ACM Transactions on Information Systems, Vol. 24, No. 3, July 2006.

338

•

D. Shen et al.
Table III. Number of Queries with N Labels
N
1
2
3
4
5
Ave

L1
14
138
186
222
240
3.67

L2
118
365
225
71
21
2.39

L3
16
79
212
199
294
3.845

Fig. 8. The distribution of labels assigned by the three labelers.

5.3 Quality of the Testing Dataset
Because the testing dataset is provided by three human labelers, we wish to
evaluate its quality. In particular, in this section, we analyze the consistency
between the three labelers for the testing dataset.
Table III gives the distribution of the number of categories assigned by human labelers to each query, that is, how many queries are assigned N categories,
where N changes from one to five. The “Ave” row shows the average number
of categories for each query. From the table, it seems that the three labelers
disagree to quite an extent. However, from Figure 8, which shows the distribution of the 67 categories assigned by the three labelers to 800 testing queries,
we can see that the distributions for the three human labelers are very similar.
From Table III and Figure 8, we can conclude that the general distributions of
categories are very similar, though the labelers L1 and L3 tend to assign more
categories for each query than does L2.
Figure 9 shows the F1 values of the six classifiers on testing data labeled by
the three labelers. From the figure, we can see that the labelers have a high correlation with respect to the relative performance of the classifiers, especially L1
and L3. After ranking the six classifiers according to each labeler, we calculate
the Spearman rank-order correlation coefficient between each pair of labelers
[Hoel 1966]. Spearman correlation is a nonparametric approach to calculating
the relationship between two variables based on rank and no assumption about
the distribution of values is made. The results are shown in Table IV.
ACM Transactions on Information Systems, Vol. 24, No. 3, July 2006.

Query Enrichment for Web-Query Classification

•

339

Fig. 9. Performance of various classifiers evaluated by different labelers.

Table IV. Spearman Correlation
Between Each Pair of Labelers
L1.vs.L2
0.829

L1.vs.L3
0.943

L2.vs.L3
0.771

Table V. Performance of Each
Labeler Against Another

L1
L2
L3

(1) Precision
L1
L2
1.000
0.637
0.415
1.000
0.588
0.590

L3
0.561
0.367
1.000

L1
L2
L3

(2) Recall
1.000
0.415
0.637
1.000
0.561
0.367

0.588
0.590
1.000

L1
L2
L3

L1
1.000
0.502
0.574

(3) F1
L2
0.502
1.000
0.452

L3
0.574
0.452
1.000

In general terms, correlation coefficients of over 0.67 indicate strong relationships [Cann 2003]. So, we can conclude that the three labelers are highly
correlated when they determine the performance of classifiers.
Besides the aforementioned correlation analysis, we also investigate the performance of each labeler when taking the other two labelers as the ground truth.
The results are shown in Table V. The labelers in columns are tested against
labelers in rows (which are taken as the ground truth). The average F1 value
of the labelers is 0.509. Therefore, we can conclude that the query classification
problem is not easy and the performance of our proposed ensemble classifier
(with F1 equal to 0.444) is close to the average performance of the three human
labelers.
ACM Transactions on Information Systems, Vol. 24, No. 3, July 2006.

340

•

D. Shen et al.
Table VI. Summary of the Classifiers

Symbols of Classifiers
S1
S2
S3
EDP

EDF

EP

EF

EN

Meaning of the Classifier
Synonym-based classifier based on Google
Synonym-based classifier based on Looksmart
Synonym-based classifier based on Lemur
Ensemble classifier in which the weights for each base classifier
on Different categories are set in proportion to its Precision on the
category when tested on the validation dataset
Ensemble classifier in which the weights for each base classifier
on Different categories are set in proportion to its F1 on the
category when tested on the validation dataset
Ensemble classifier in which each base classifier is assigned a
single weight in proportion to its overall Precision when tested on
the validation dataset
Ensemble classifier in which each base classifier is assigned a
single weight in proportion to its overall F1 when tested on
the validation dataset
Ensemble classifier in which each base classifier is equally
weighted and does Not rely on the validation dataset

5.4 Experimental Results and Explanation
In the following, we first investigate the two main parameters affecting the performance of our proposed classifiers on the two datasets, and we then compare
the performance of these classifiers. Afterwards, we investigate several other
ensemble strategies, besides the two we introduced in Section 4.4. Finally, we
compare our approaches with those of other participants. Table VI shows a
summary of the compared classifiers, including the six we have introduced and
three additional ensemble classifiers we will introduce in Section 5.4.3.
5.4.1 Effect of Parameter Tuning. There are two main parameters that significantly impact the performance of our proposed classifiers. The first is the
number of result pages returned by the search engines, which we should use for
enriching each query. If we use too few pages, we may fail to cover its diverse
topics. However, if we use too many, we may introduce a great deal of noise.
Figure 10 shows the performance of different classifiers with respect to the increasing number of related pages used for classification. Here, related pages
are considered in the order in which they are returned by the search engines,
that is, in the order of degree of relevance with the query. The results shown
in the figure verify our conjecture. As the number of related pages increases,
the precision increases initially, and then tends to decrease. The reason is that
we need a certain amount of pages to get the meaning of the query. However,
if we include too many, noise may be introduced, which can reduce the precision. From Figure 10, we can see that the critical point for most classifiers is
40 pages. Before this, classifier performance increases as we use more pages,
while after this point, the performance begins to decrease. Therefore, in the
following experiments we keep only the top 40 pages for query classification.
Another parameter is the number of labels we should assign to each query.
The KDDCUP2005 competition rules allow us to assign, at most, five labels
for each query. However, in order to achieve higher precision, we should assign
ACM Transactions on Information Systems, Vol. 24, No. 3, July 2006.

Query Enrichment for Web-Query Classification

•

341

Fig. 10. Performances of different classifiers vary with the number of used related pages on the
two datasets.

ACM Transactions on Information Systems, Vol. 24, No. 3, July 2006.

342

•

D. Shen et al.

few, but accurate labels, whereas if we hope to achieve higher recall, we need
to assign more possible labels. Figure 11 shows the performance of different
classifiers by varying the number of classified categories. As we expected, as
the number of classified categories increases, the precision of all the classifiers decreases, while recall increases significantly. In contrast, the value of F1
increases initially, and then decreases. For most of the classifiers, maximum
values of F1 are achieved when four categories are generated for each query.
Although the F1 values are close to those obtained when five categories are
assigned, the precision values are much lower. We also adopted some heuristic
rules to decide the number of classified categories. As shown earlier, for each
query, our classifiers can return a ranked category list according to a certain
criterion, which is referred to as confidence. Among the top five categories, if
the confidences of two neighboring categories ci and ci+1 vary too much, we stop
at ci and discard the categories after ci . This heuristic rule can help us to some
extent, but not greatly.
5.4.2 Comparison Between Classifiers. From the previously described experimental results on both the sample and testing datasets, we can see that of
the four base classifiers, S1 works best while S2 works most poorly. The lack
of overlap among the search results from different search engines explains the
performance differences among S1, S2, and S3. The reason S2 does not work
well is that for many queries, we cannot obtain enough related pages through
the Looksmart search engine. For the SVM, we expect that it can solve the
low recall problem caused by the three synonym-based classifiers, as discussed
in Section 4. Figures 10 and 11 show that the SVM does obtain the highest
recall in most cases, as compared to synonym-based classifiers. We also notice
that the two ensemble classifiers can achieve better performance in terms of F1
than any other base classifier. For the peak F1 values of the three best classifiers on the testing dataset (EN, EDP, and S1), we can see that, compared with
S1, EN and EDP improve the F1 measure by 12.1% and 8.3%, respectively. In
fact, when we design these two ensemble classifiers, EDP is expected to achieve
higher precision because each component classifier is highly weighted on the
categories in which it achieves high precision, and EN is expected to achieve
higher F1 performance, since the recall is relatively high. According to our two
submitted results, the F1 value of EN (0.444) achieves a 4.2% relative improvement compared with that of EDP (0.426). However, the precision value of EDP
(0.424) improves by 2.3%, while that of EN is 0.414.
5.4.3 Study of Different Ensemble Strategies and the Effect of the Validation
Dataset. As shown earlier, we employed two strategies to decide the weights
for ensemble classifiers. In this section, we will study three more strategies.
Besides this, we also test the effect of validation data. The strategies we will
study are shown next. For clarity, the strategy EDP which was introduced in
Section 4.4 is repeated here.
— The weight for classifier i on category j is in proportion to Pij . Pij refers to
the precision of classifier i on category j . We denote this strategy as EDP.
ACM Transactions on Information Systems, Vol. 24, No. 3, July 2006.

Query Enrichment for Web-Query Classification

•

343

Fig. 11. Performances of different classifiers vary with the number of classified categories on the
two datasets.

ACM Transactions on Information Systems, Vol. 24, No. 3, July 2006.

344

•

D. Shen et al.
Table VII. Comparison Between Different
Ensemble Strategies

20%
40%
60%
80%
100%

EDP
0.375
0.381
0.386
0.388
0.390

(1) Precision
EDF
EP
0.364
0.389
0.372
0.388
0.376
0.387
0.378
0.387
0.380
0.387

20%
40%
60%
80%
100%

EDP
0.419
0.453
0.478
0.483
0.488

(2) Recall
EDF
EP
0.422
0.503
0.456
0.500
0.482
0.500
0.488
0.500
0.492
0.500

20%
40%
60%
80%
100%

EDP
0.393
0.410
0.424
0.425
0.429

(3) F1
EDF
0.388
0.406
0.419
0.422
0.425

EP
0.435
0.433
0.433
0.433
0.433

EF
0.385EF
0.387EF
0.386EF
0.386EF
0.386EF

EF
0.500
0.500
0.500
0.500
0.500

EF
0.431
0.432
0.432
0.432
0.432

— The weight for classifier i on category j is in proportion to F 1ij . F 1ij refers
to the F1 value of classifier i on category j , where F 1ij can be defined in a
similar way as Pij . We denote it as EDF.
— Each classifier is assigned a single weight in proportion to its overall precision. We denote this as EP.
— Each classifier is assigned a single weight in proportion to its overall F1. We
denote it as EF.
EF and EP differ from EDP and EDF in that we assign a unique weight for
a base classifier across all categories in EP and EF, while we assign each base
classifier different weights on the various categories in EDP and EDF.
In order for the four strategies to determine the weights, we rely on the
validation data (the 111 samples). The size of the validation data may affect the
performance of different strategies. To test its effect, we construct five subsets by
randomly picking 20%, 40%, 60%, 80%, and 100% samples. Then we construct
the ensemble classifiers based on these subsets, respectively. In order to remove
the variance, we conduct the experiments three times. The final reported values
have been averaged across these three runs.
From Table VII, we can see that with increasing increment of size of the validation dataset, the performance of EDP and EDF increases steadily. When the
validation dataset is small, the performance of a classifier on a given category
may not reflect the real performance of the classifier. Therefore, the weights
generated tend to be unreliable and cause poor performance of the ensemble
ACM Transactions on Information Systems, Vol. 24, No. 3, July 2006.

Query Enrichment for Web-Query Classification

•

345

Fig. 12. Performance of all ensemble classifiers constructed from the four base classifiers.

classifiers. However, with increasing size of the validation dataset, the performance of the base classifiers becomes more and more reliable, as do the weights
we obtain. Consequently, the performance of the ensemble classifiers improves.
Note that the performance of EP and EF does not change much with the change
of size of the validation dataset. By considering the performance of EN, which
assigns equal weights to each base classifier, we can obtain a reasonable explanation. As we can see, the performance among different base classifiers does not
vary too much on a given dataset in terms of precision and F1. Therefore, the
weights for each base classifier generated according to EP and EF are similar.
That is, EP and EF both perform similar to EN, regardless of which validation
dataset is used. Hence, the performance of EP and EF does not fluctuate much
with change of the validation dataset.
From Table VII, we also find that EDP (EP) always achieves better precision than EDF (EF) across different validation datasets, without sacrificing
the value of F1. We can conclude that in order to achieve higher precision from
ensemble classifiers, we should determine the weights of base classifiers according to their precision values. We can see that the precision achieved by
EDP increases steadily, whereas that achieved by EP is relatively stable, with
increase of the validation dataset. When 80% or more of the 111 samples are
used, EDP outperforms EP in terms of precision, although the advantage of
EDP is not obvious. Therefore, in order to reach higher precision, it is necessary to try EDP if we have a large validation dataset.
5.4.4 Effect of Base Classifiers. In the preceding section, we studied the
effects of different ensemble strategies, as well as that of the validation dataset.
In this section, we will study the effect of the number of base classifiers. For
simplicity, we assign each base classifier equal weight when constructing an
ensemble classifier, as with EN. Given four base classifiers, we can obtain 15
ensemble classifiers in total.
Figure 12 shows the performance of all the ensemble classifiers on the testing dataset. Each leaf in the tree represents an ensemble classifier. On the path
from a leaf to the root, “1” indicates that the corresponding base classifier is
included to construct the target ensemble classifier; “0” indicates the opposite
case. The three numbers in each leaf reflect the performance of the corresponding ensemble classifier in terms of precision, recall, and F-measure. From the
figure, we can conclude that the more base classifiers included, the better the
ACM Transactions on Information Systems, Vol. 24, No. 3, July 2006.

346

•

D. Shen et al.

Fig. 13. Averaged performance of different kinds of ensemble classifiers categorized according to
number of base classifiers.
Table VIII. Average Running-Time of Each Step for Testing a Query (seconds)
Retrieve Pages from Search Engines
Google
Looksmart
Lemur + ODP
0.98s
0.39s
0.11s

Process
Returned Pages
0.002s

Classify through Classifiers
Synonym-Based
SVM
0.0006s
0.0035s

performance. Figure 13 further clarifies the conclusion. In fact, by categorizing
the 15 ensemble classifiers according to number of base classifiers included, we
acquire four kinds. Figure 13 is obtained by averaging the different ensemble
classifiers within each type. From Figure 12, we also find that whenever the base
classifier SVM is included, there will be an obvious improvement in recall, while
precision does not change much, and thus F1 is improved. This observation once
again validates our idea that the two kinds of base classifiers complement each
other and the combination can improve the classification results.
5.4.5 Running-Time Analysis. In this section, we analyze the time complexity of our approaches. Since the training stage of our approaches can be
completed offline, we only consider the test stage. As shown before, to test a
query, we need four steps: (1) submit the query to search engines and fetch
the related pages; (2) process the returned pages to obtain the intermediate
category information for each page and textual representation of the query; (3)
apply the two kinds of base classifiers on the enriched representation of the
query; and (4) combine the results of the base classifiers. Table VIII shows the
running-time of each step for testing a query (averaged over the 800 queries in
the testing dataset) on a PC with 512M of memory and a 3.2Ghz CPU. We do not
show the time for Step 4, since its running-time is negligable as compared to
other steps. For example, when combining the results from any two classifiers,
the time for each query is about 1.8 × 10−5 seconds.
From Table VIII, we can observe that the bottleneck of the test stage is to
retrieve pages from search engines, which includes the time for finding and
crawling the related pages taken by the search engine. For the search engine
we developed based on Lemur and the crawled ODP pages (denoted by Lemur +
ODP), the time for page retrieval is far less than that of Google and Looksmart,
ACM Transactions on Information Systems, Vol. 24, No. 3, July 2006.

Query Enrichment for Web-Query Classification

•

347

since the pages are indexed locally and we do not have to crawl them. In fact,
the difference in time depends on the speed of the network. Therefore, when
we run our approach on the sever of a search engine, a query can be classified
in real time ( in the order of 10−1 ).
5.4.6 Comparison with Other Participants. To further validate our algorithm, we compare it with other KDDCUP2005 participants’ systems. First, we
briefly introduce two systems of the runner-ups in KDDCUP2005 competition
[Kardkovács et al. 2005; Vogel et al. 2005] . The runner-up for the F1 criterion
is the team from MEDai/AI Insight/Humboldt University. The runner-up for
the precision criterion is from Budapest University of Technology.
The MEDai/AI Insight/ Humboldt University team built a model containing
67 biclass classifiers, each of which corresponds to a target category. This model
can predict 67 probabilities for each query and categories with the highest
are chosen for each query. To build the model, they need to train 67 biclass
classifiers. The training dataset for each classifier consists of the 111 queries
given by the KDDCUP2005 organizers. For each of these queries, if it belongs
to category i, it is a positive training sample of the classifier of category i;
otherwise, it is negative. They send each query to the Google search engine and
get some relevant pages. They use these retrieved pages to represent the query.
With these training datasets, they train 67 biclass SVM classifiers. Given a test
query, they input it to the classifiers, and then they can get the probability of
each category. They choose the top categories as the final submitted result.
In the KDDCUP2005 competition, the Budapest University of Technology
team proposed an algorithm called the Ferrety, which is based on Internet
search engines and a document categorizer [Tikk et al. 2005]. As we observed,
they also find that both the query words and KDDCUP2005 categories lack
semantics, while no training data is available. Thus, they seek the meanings of
words by asking the Internet, which acts as an open dictionary and knowledge
base. They send each query to search engines and retrieve possible categories
defined by them. Since the obtained categories differ from target categories,
proper category mapping algorithms are needed. By observing that formal definitions of categories are available for many search engines, they perform mapping as follows: First, relevant word collections are expanded by WordNet synonyms of target category names; second, probable search engine categories are
attached to target ones by matching the similarities in the word collections and
category definitions. By calculating TF-IDF, more relevant words can be added
to the collections. This process is repeated until the word collections become
stable. Every search engine category is then mapped to a proper category defined by KDDCUP2005. Half of the 800,000 queries have results from search
engines. The remainder are sent to their own document categorizer, which is
a supervised algorithm for classifying text. They get another 320,000 answers
with their categorizer. The precision of their algorithm is 0.34088.
Figure 14 contains the evaluation results for the top ten submitted solutions,
as ranked by the organizers. For the F1 criterion, our value is 0.444 (submission ID: 22), that of the MEDai/AI Insight/Humboldt University team is 0.405
(submission ID: 8). Our F1 value is higher than that of the runner-up team by
ACM Transactions on Information Systems, Vol. 24, No. 3, July 2006.

348

•

D. Shen et al.

Fig. 14. Top 10 solutions in KDDCUP2005 in terms of precision and F1.

9.6% and higher than the mean of the other nine teams among the top ten by
32%. For the precision criterion, our value is 0.424 (submission ID: 37), while
that of the Budapest University of Technology team is 0.341 (submission ID:
21). Our precision value is higher than that of the runner-up team by 24.3% and
higher than the mean of the other nine top competitors by 40.3%. Besides this,
we also compared precision and F1 values of our solutions to the mean values
of all other participants. Our precision and F1 are 98.5% and 73.5% higher,
respectively, than the averaged precision and F1 of the other participants.
5.5 Some Failed Methods
In fact, we have tried several other approaches that did not work well. Here is
an example. The main idea is to build a bridge between a given query and the
67 KDDCUP2005 categories by counting the number of pages related to both.
We submitted a query to search engines and got its related pages. This set of
pages is denoted by Pq. Similarly, we can get the related pages of a category
by directly using the category name as a query. This set of pages is denoted
by Pc. In practice, each Pq includes 100 pages for each query, and each Pc
includes 10,000 pages. Then we can define the similarity between the target
ACM Transactions on Information Systems, Vol. 24, No. 3, July 2006.

Query Enrichment for Web-Query Classification

•

349

query and category as Pq ∩ Pc , where . is the size of a set. For each query,
we can return the most related categories according to the similarity. However, this method does not seem to work. One possible reason is that there is
correlation between some pairs of categories. For example, we found that the
overlap of the 10,000 retrieved pages of the categories “Computer\Hardware”
and “Living\Tools and Hardware” consists of about 1000 pages. Therefore,
we removed the overlap pages between each pair of categories. We repeated
the aforementioned approach. However, the final result is still not satisfactory.
One reason for the failure of this method is that we cannot judge the similarity between a query and a category simply by the size of the intersected set of
retrieved pages. The essence of the problem is how to automatically find the
exact collection of pages that can well represent the semantics of a query and
a category, and how to determine the similarity between them based on these
pages. This is a problem that requires further study.
We also tried another method, based on a dictionary, which also does not work
well. We submitted a query q into dictionary software, for example, WordNet,
to get the related words of a query. The result is denoted as Rq . The result
includes the synonyms or antonyms of the given query, which can be a verb,
noun, or adjective. Taking the query “car” as an example, the result contains:
car, auto, automobile, machine, motorcar, railcar, railway car, railroad car, cable
car, etc. Similarly, we can get the result of every category c in the same way,
which is denoted as Rc . A similarity between the target query and category
is defined as Rq ∩ Rc , as shown before. We can classify a query into the
top categories ranked according to similarity. When we tested this method on
the validation dataset, the F1 is only about 20%. The main reason for the bad
performance is that this method cannot obtain proper results for many queries
through WordNet. If additional dictionaries such as Wikipedia are leveraged,
the performance of this method may be improved.
6. CONCLUSION AND FUTURE WORK
In this article, we presented our approach to solving the query classification
problem with its application on the task provided by KDDCUP2005. Query
classification is an important as well as difficult problem in the field of information retrieval. Once the category information for a query is known, a search
engine can be more effective and return more representative Web pages to
users. However, since queries usually contain too few words, it is hard to determine their meanings. Another practical challenge, as shown in KDDCUP2005,
is that no training data is explicitly provided for the classification task.
To solve the query classification problem, we designed an approach based
on query enrichment that can map queries to some intermediate objects. With
the intermediate objects, we designed several ensemble classifiers based on two
different kinds of base classifiers, statistics-based and synonym-based. The experimental results on the two datasets provided by the KDDCUP2005 organizers validate the effectiveness of the base classifiers, as well as the ensemble
strategies. We have designed a demonstration system called Q 2 C@UST with a
dedicated Web site at http://q2c.cs.ust.hk/q2c/.
ACM Transactions on Information Systems, Vol. 24, No. 3, July 2006.

350

•

D. Shen et al.

The success of our approach in the KDDCUP2005 competition can be attributed to two factors: one is the way in which to enrich the queries and the
other is the way to combine the base classifiers. Therefore, in the future, we
will conduct more research following these two directions: (1) we will try to find
additional valuable information for the queries on which we can build base classifiers, including those from some search engines with special features (such as
that found at http://vivisimo.com/, which clusters the returned pages); and (2)
we will conduct further research to find more effective strategies to generate
ensemble classifiers.
APPENDIX A: Target Categories from the KDDCUP2005 Task Force
Computers\Hardware
Computers\Internet & Intranet
Computers\Mobile Computing
Computers\Multimedia
Computers\Networks & Telecommunication
Computers\Security
Computers\Software
Computers\Other
Entertainment\Celebrities
Entertainment\Games & Toys
Entertainment\Humor & Fun
Entertainment\Movies
Entertainment\Music
Entertainment\Pictures & Photos
Entertainment\Radio
Entertainment\TV
Entertainment\Other
Information\Arts & Humanities
Information\Companies & Industries
Information\Science & Technology
Information\Education
Information\Law & Politics
Information\Local & Regional
Information\References & Libraries
Information\Other
Living\Book & Magazine
Living\Car & Garage
Living\Career & Jobs
Living\Dating & Relationships
Living\Family & Kids
Living\Fashion & Apparel
Living\Finance & Investment
Living\Food & Cooking
Living\Furnishing & Houseware

Living\Gifts & Collectables
Living\Health & Fitness
Living\Landscaping & Gardening
Living\Pets & Animals
Living\Real Estate
Living\Religion & Belief
Living\Tools & Hardware
Living\Travel & Vacation
Living\Other
Online Community
Online Community\Chat & Instant Messaging
Online Community\Forums & Groups
Online Community\Homepages
Online Community\People Search
Online Community\Personal Services
Online Community\Other
Shopping\Auctions & Bids
Shopping\Stores & Products
Shopping\Buying Guides & Researching
Shopping\Lease & Rent
Shopping\Bargains & Discounts
Shopping\Other
Sports\American Football
Sports\Auto Racing
Sports\Baseball
Sports\Basketball
Sports\Hockey
Sports\News & Scores
Sports\Schedules & Tickets
Sports\Soccer
Sports\Tennis
Sports\Olympic Games
Sports\Outdoor Recreations
Sports\Other

ACKNOWLEDGMENTS

We thank Hong Kong University of Science and Technology, Department of
Computer Science and Engineering and Computer Systems Group for their
kind support.
ACM Transactions on Information Systems, Vol. 24, No. 3, July 2006.

Query Enrichment for Web-Query Classification

•

351

REFERENCES
ALPAYDIN, E. 2004. Introduction to Machine Learning. MIT Press, Cambridge, MA.
BAUER, E. AND KOHAVI, R. 1999. An empirical comparison of voting classification algorithms:
Bagging, boosting, and variants. J. Mach. Learn. 36, 1–2, 105–139.
BEEFERMAN, D. AND BERGER, A. 2000. Agglomerative clustering of a search engine query log. In
KDD ’00: Proceedings of the 6th ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining. ACM Press, New York, 407–416.
BEITZEL, S. M., JENSEN, E. C., FRIEDER, O., GROSSMAN, D., LEWIS, D. D., CHOWDHURY, A., AND KOLCZ,
A. 2005. Automatic web query classification using labeled and unlabeled training data. In
SIGIR ’05: Proceedings of the 28th Annual International ACM SIGIR Conference on Research
and Development in Information Retrieval. ACM Press, New York, 581–582.
CANN, A. J. 2003. Maths from Scratch for Biologists. John Wiley & Sons, New York, NY.
CARUANA, R., NICULESCU-MIZIL, A., CREW, G., AND KSIKES, A. 2004. Ensemble selection from libraries
of models. In ICML ’04: Proceedings of the 21st International Conference on Machine Learning.
ACM Press, New York, 18.
CHANG, C.-H. AND HSU, C.-C. 1998. Integrating query expansion and conceptual relevance feedback for personalized web information retrieval. In WWW7: Proceedings of the 7th International
Conference on World Wide Web 7. Elsevier Science Publishers B. V., Amsterdam, 621–623.
CHEKURI, C., GOLDWASSER, M., RAGHAVAN, P., AND UPFAL, E. 1997. Web search using automated
classification. 6th International World Wide Web Conference (WWW6). Poster presentation.
CHEN, H. AND DUMAIS, S. 2000. Bringing order to the web: Automatically categorizing search
results. In CHI ’00: Proceedings of the SIGCHI Conference on Human Factors in Computing
Systems. ACM Press, New York, 145–152.
DIETTERICH, T. G. 2000. Ensemble methods in machine learning. In Multiple Classifier Systems.
1–15.
FAN, W., STOLFO, S. J., AND ZHANG, J. 1999. The application of adaboost for distributed, scalable
and on-line learning. In KDD ’99: Proceedings of the 5th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining. ACM Press, New York, 362–366.
HANSEN, L. K. AND SALAMON, P. 1990. Neural network ensembles. IEEE Trans. Pattern Anal. Mach.
Intell. 12, 10, 993–1001.
HITEC. http://categorizer.tmit.bmr.hu.
HOEL, P. G. 1966. Elementary Statistics, 2nd ed. Wiley, New York, NY.
HOWE, A. E. AND DREILINGER, D. 1997. SAVVYSEARCH: A metasearch engine that learns which
search engines to query. AI Mag. 18, 2, 19–25.
JANSEN, B. J. 2000. The effect of query complexity on web searching results. Inf. Res. 6, 1.
JOACHIMS, T. 1998. Text categorization with suport vector machines: Learning with many relevant
features. In ECML’98 Proceedings of the 10th European Conference on Machine Learning, 137–
142.
JOACHIMS, T. 1999. Transductive inference for text classification using support vector machines.
In ICML ’99: Proceedings of the 16th International Conference on Machine Learning. Morgan
Kaufmann, San Francisco, 200–209.
JONES, K. S. 1971. Automatic Keyword Classifications for Information Retrieval. Butterworth,
London, UK.
KANG, I.-H. AND KIM, G. 2003. Query type classification for web document retrieval. In SIGIR ’03:
Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval. ACM Press, New York, 64–71.
KARDKOVÁCS, Z. T., TIKK, D., AND BÁNSÁGHI, Z. 2005. The ferrety algorithm for the kdd cup 2005
problem. SIGKDD Explor. Newsl. 7, 2, 111–116.
KITTLER, J., HATEF, M., DUIN, R. P. W., AND MATAS, J. 1998. On combining classifiers. IEEE Trans.
Pattern Anal. Mach. Intell. 20, 3, 226–239.
LEWIS, D. D. AND GALE, W. A. 1994. A sequential algorithm for training text classifiers. In SIGIR,
W. B. Croft and C. J. van Rijsbergen, Eds. Springer Verlag, Berlin, Germany, 3–12.
LI, Y., ZHENG, Z., AND DAI, H. K. 2005. Kdd cup-2005 report: Facing a great challenge. SIGKDD
Explor. Newsl. 7, 2, 91–99.
MCCALLUM, A. AND NIGAM, K. 1998. A comparison of event models for naive Bayes text classification. In Proceedings of the AAAI-98 Workshop on Learning for Text Categorization.
ACM Transactions on Information Systems, Vol. 24, No. 3, July 2006.

352

•

D. Shen et al.

MEYER, D. A. AND BROWN, T. A. 1998. Statistical mechanics of voting. Phys. Revei. Lett. 81, 8,
1718–1721.
MILLER, G., BECKWITH, R., FELLBAUM, C., GROSS, D., AND MILLER, K. 1990. Introduction to wordnet:
An on-line lexical database. Int. J. Lexicography 3, 4, 23–244.
PAGE, L., BRIN, S., MOTWANI, R., AND WINOGRAD, T. 1998. The pagerank citation ranking: Bringing
order to the web. Tech. rep., Stanford Digital Library Technologies Project.
SELBERG, E. AND ETZIONI, O. 1995. Multi-service search and comparison using the MetaCrawler.
In Proceedings of the 4th International World-Wide web Conference. Darmstadt, Germany.
SHEN, D., PAN, R., SUN, J.-T., PAN, J. J., WU, K., YIN, J., AND YANG, Q. 2005. Q2c@ust: Our winning
solution to query classification in kddcup 2005. SIGKDD Explor. Newsl. 7, 2, 100–110.
SHEN, D., SUN, J.-T., YANG, Q., AND CHEN, Z. 2006. Building bridges for web query classification.
In SIGIR ’06: Proceedings of the 29th Annual International ACM SIGIR Conference on Research
and Development in Information Retrieval.
SILVERSTEIN, C., MARAIS, H., HENZINGER, M., AND MORICZ, M. 1999. Analysis of a very large web
search engine query log. SIGIR Forum 33, 1, 6–12.
TIKK, D., BIRÓ, GY., AND YANG, J. D. 2005. Experiments with a hierarchial text categorization
method on WIPO patent collections. In Applied Research in Uncertainty Modelling and Analysis,
N. O. Attok-Okine and B. M. Ayyub, Eds. International Series in Intelligent Technologies, vol.
20, Springer-Verlag, 283–302.
VAN, R. C. 1979. Information Retrieval, 2nd ed. Butterworth, London, UK.
VOGEL, D., BICKEL, S., HAIDER, P., SCHIMPFKY, R., SIEMEN, P., BRIDGES, S., AND SCHEFFER, T. 2005.
Classifying search engine queries using the web as background knowledge. SIGKDD Explor.
Newsl. 7, 2, 117–122.
VOORHEES, E. M. 1994. Query expansion using lexical-semantic relations. In SIGIR ’94: Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in
Information Retrieval. Springer Verlag, Berlin, Germany. 61–69.
WEN, J.-R., NIE, J.-Y., AND ZHANG, H.-J. 2002. Query clustering using user logs. ACM Trans. Inf.
Syst. 20, 1, 59–81.
YANG, Y. 1999. An evaluation of statistical approaches to text categorization. Inf. Retr. 1, 1–2,
69–90.
YANG, Y. AND PEDERSEN, J. O. 1997. A comparative study on feature selection in text categorization.
In ICML ’97: Proceedings of the 14th International Conference on Machine Learning. Morgan
Kaufmann, San Francisco, CA, 412–420.
Received November 2005; revised April 2006; accepted June 2006

ACM Transactions on Information Systems, Vol. 24, No. 3, July 2006.

2012 Second International Conference on Cloud and Green Computing

Group Division for Recommendation
in Tag-based Systems
Rong Pan† , Guandong Xu*‡ , Peter Dolog† , and Yu Zong*§††
† Department

of Computer Science, Aalborg University, Denmark
for Applied Informatics, Victoria University, Australia
§ Department of Information and Engineering, West Anhui University, China
†† Department of Computer Science and Technology, University of Science and Technology of China, China
‡ Centre

proﬁles of users or documents for further web data management. As the tags are of syntactic nature, in a free style and do
not reﬂect sufﬁcient semantics, the problems of redundancy,
ambiguity and less semantics of tags are often incurred in
all kinds of social tagging systems[8,9]. Different from rating
data, social tagging data does not contain user’s explicit
preference information on resources, instead, reﬂecting the
personalized perceptions on resources by users. In particular,
such data involves three types of objects, i.e., user, resource
and tag. These differences bring in new challenges as well
as opportunities to deal with recommendation problems in
the context of social tagging systems. A primary concern of
recommender systems in tag-based recommender systems is to
present users with avenues for navigation that are most relevant
to their information needs. In the tag-based recommender
system, it has three types of entities which are considered
as: user, resource, tag. One user prefers some resources which
he is interested in and annotates them with some words. In
this case, one resource can be tagged by several tags or one
tag can be annotated on several resources. Therefore, tags
just serve as intermediaries between users and resources. So
far, we can see that one user may be interested in some
resources and annotate tags on them. Here, the tag which
has been annotated on resource describes user’s own opinion
and indicate his interests. Likewise, for the same resource,
different users may use different tags to annotate. If we want to
retrieve resources via these ambiguous tags, it is very common
that we can’t ﬁnd the desired results through just browsing
the returned resources. However, there are also some tags the
users have common view, i.e., these tags can also represent
resources properly. So, a user may annotate some tags on
various resources, we can illustrate these activities as a user
preferring resources based on their interests.
We can obtain more representative information about the
users’ preferences from the tags. We can cluster the tags to
obtain such topic groups. However, there are huge redundancy
information due to the freedom of chosen tags. We are interested in understanding whether this group information could
produce any practical beneﬁts. In this paper, we cluster the
tags based on the bipartite graph between tags and resources,
or between tags and users. Our objective is to research aims
on these tags to organize the groups and adjust their weights.

Abstract—The common usage of tags in these systems is to add
the tagging attribute as an additional feature to re-model users or
resources over the tag vector space, and in turn, making tag-based
recommendation or personalized recommendation. With the help
of tagging data, user annotation preference and document topical
tendency are substantially coded into the proﬁles of users or
documents. However, obtaining the proper relationship among
user, resource and tag is still a challenge in social annotation
based recommendation researches. In this paper, we utilize the
relationship from between tags and resources and between tags
and users to extract group information. With the help of such
relationship, we can obtain the Topic-Groups based on the
bipartite relationship between tags and resources; and InterestGroups based on the bipartite relationship between tags and
users. The preliminary experiments have been conducted on
MovieLens dataset to compare our proposed approach with
the traditional collaborative ﬁltering recommendation approach
approach in terms of precision, and the result demonstrates that
our approach could considerably improve the performance of
recommendations.
Index Terms—Recommender System, Social Tagging, TopicGroups, Interest-Groups.

I. I NTRODUCTION
Tagging, as a labeling of items with speciﬁc lexical information, plays a crucial role in such social collaborative tagging
systems. The common usage of tags in these systems is to
add the tagging attribute as an additional feature to re-model
users or resources over the tag vector space, and in turn,
making tag-based collaborative ﬁltering recommendation or
personalized recommendation. The user-contributed tags are
not only an effective way to facilitate personal organization
but also provide a possibility for users to search for needed
information. The success of Web 2.0 applications has faced
a new era for sharing and organizing documents in online
social communities. A large number of social tagging sites,
like Delicious, CiteUlike, Digg, or Flickr, have sprung up in
a short period.
Tag, as one kind of speciﬁc lexical information that is
user-generated metadata with uncontrolled vocabulary, plays
a crucial role in such social collaborative tagging systems.
With the help of tagging data, user annotation preference and
document topical tendency are substantially coded into the
* Corresponding Author.

978-0-7695-4864-7/12 $26.00 © 2012 IEEE
DOI 10.1109/CGC.2012.124

399

The main contributions made in this paper are:
• We extract the two groups as Topic Groups and Interest
Groups from the tags from the Folksonomy Model.
• The group formulation approach by clustering tags based
on such group information.
• We investigate how to involve the weight and make it
adaptive to manage the impact of Topic Groups and
Interest Groups on recommendations
The rest of the paper is organized as follows: Section 2
presents the related work in the ﬁeld of tag clustering. In
section 3, we describe the preliminaries for the data model.
Section 4 discusses the detailed process of extending the tag
group. The experiment design, evaluation measures and the
comparison of the results are in Section 5. We conclude the
paper in section 6.

clustering results to ﬁnd out better tag clusters. By proposing
an APProximate backbone-based Clustering algorithm for
Tags (APPECT), they ﬁx the approximate backbone as the
initial tag clustering result and then assign the rest of the tags
into the corresponding clusters based on the similarity.
In [8], [9], [10], [11] topic relevant partitions are created by
clustering resources rather than tags. By clustering resources,
the improvement of recommendations is made by distinguishing between alternative meanings of a query. In [12], an interesting approach is proposed to model the documents in social
tagging systems by document graphs. The relevance of tag
propagated along edges of the documents graph is determined
via a scoring scheme, with which the tag prediction was
carried out. In [13], an approach that monitors users activity
in a tagging system and dynamically quantiﬁes associations
among tags is presented. The associations are then used to
create tag clusters. In our work we focus on tag clustering
instead.
Zhou et al. propose a novel method to compute the similarity
between tag sets and use it as the distance measure to cluster
web documents into groups. Major steps in such method
include computing a tag similarity matrix with set-based vector
space model, smoothing the similarity matrix to obtain a set of
linearly independent vectors and compute the tag set similarity
based on these vectors. [14]. In this paper we propose a
different enhanced approach which utilizes tag clustering to
compute recommendations.
In the context of tag clustering, most of the researches are
directly using the traditional clustering algorithms such as Kmeans [15] or Hierarchical Agglomerative Clustering [16] on
tag data, which possess the inherent drawbacks, such as the
sensitivity of initial values and high computational cost etc.
[17] demonstrate how tag clusters serving as coherent topics
can aid in the social recommendation of search and navigation.
They present a personalization algorithm for recommendation
in folksonomies which relies on hierarchical tag clusters.
Their basic recommendation framework is independent of the
clustering method. They use a context-dependent variant of
hierarchical agglomerative clustering which takes into account
the user’s current navigation context in cluster selection. A
framework named Semantic Tag Clustering Search, which is
able to cope with the syntactic and semantic tag variations is
proposed in [18]. In our work we do not consider the semantics
of tags as it is an additional computation step. We show, that
even without the consideration of semantics the performance
of a recommender is reasonable.

II. R ELATED W ORK
Tags are used to index, annotate and retrieve resource as an
additional metadata of resource. Poor retrieval performance
remains a major problem of most social tagging systems
resulting from the severe difﬁculty of ambiguity, redundancy
and less semantic nature of tags. Clustering method is a useful
tool to address the aforementioned difﬁculties. We review the
related literatures from the perspectives of tag expansion and
tag clustering.
The authors in [1] formalize the notion of resource recommendation in social annotation systems. A linear-weighted
hybrid framework for making recommendations is proposed
and shown to be effective. Some of the hybrid recommender
systems [2] have been shown to be an effective method of
drawing out the best performance among several independent
component algorithms. Our work can be also considered
as a hybrid recommender system where we utilize tags to
understand both, users and documents.
Recently tagging has been widely used in recommender
systems for many applications [3], [4]. The common usage
of tags in these systems is to add the tagging attribute as an
additional feature to re-model users or resources over the tag
vector space, and in turn, making tag-based recommendation
or personalized recommendation. K. R. Bayyapu and P. Dolog
in [5] try to solve the problems of sparse data and low quality
of tags from related domains. They suggest using tag neighbors
for tag expression expansion. The tag neighbors are based on
the location within documents while in this paper the tag
neighbors are understood as similarity neighbors in vector
space model along users and documents.
Recommenders can assist users by suggesting resources,
tags or even other users. Authors in [6] have demonstrated that
an integrated approach which exploits all three dimensions of
the data (users, resources, tags) perform superior results in tag
recommendation. They extend the resource recommendation
approach and propose an approach for designing weighted
linear hybrid resource recommenders. In this paper, we differ
in calculating weights and in a way which we apply clustering.
The authors in [7] focus on the sensitivity of initialization
instead and make use of the approximate backbone of tag

III. P RELIMINARIES
We will review the preliminaries based on our previous work
in this section[19], [20], [21], [7].
A. Folksonomy
The folksonomy is a three-dimensional data model of social
tagging behaviors of users on various documents. It reveals
the mutual relationships between these three-fold entities, i.e.
user, document and tag. A folksonomy F according to [22]

400

is a tuple F = (U, T, D, A), where U is a set of users, T is
a set of tags, D is a set of web documents, and A ⊆ U ×
T × D is a set of annotations. The activity in folksonomy
is tijk ⊆ {(ui , dj , tk ) : ui ∈ U, dj ∈ D, tk ∈ T }, where U =
{U1 , U2 , · · · , UM } is the set of users, D = {D1 , D2 , · · · , DN }
is the set of documents, and T = {T1 , T2 , · · · , TK } is the set
of tags. tijk = 1 if there is an annotation (ui , dj , tk ); otherwise
tijk = 0.
[23], [24],with each (u, r, t) representing a user u annotates
tag t to resource r. Therefore a social tagging system can be
described as a set of four-tuples, where exists a set of users, U ;
a set of tags, T ; a set of resources, R; and a set of annotations,
AN . The constructed folksonomy data model is actually a
three-dimensional array (or called tripartite hyper-graph). In
real applications, we often decompose the tagging data into
two two-dimensional matrices, i.e., user vector and document
vector. User vector can be used to store the descriptive tags
of the user’s characteristics and preferences. The document
vector is represented by the tags generated by the group of
users tagging the documents. In the context of social tagging
systems, the user and document vectors thus are expected to
be represented by the representative tags. We will utilize them
based on our previous work [19].

matrix, by a number of matrix blocks to meet the similarity
value larger among the inside elements, while the similarity
value is small between the clusters [25], [26], [27]. We want
to utilize the clustering algorithm to discover the organizing
observations into groups. The members of the groups share
properties in common. From that we could predict behavior
and the properties based on group membership.
There are a lot of clustering algorithms such as k-means,
fuzzy c-means, single linkage and so on. In clustering analysis,
almost all approaches are based on the similarity between
subjects to partition the data points. Various clustering approaches have different advantages and drawbacks. Among
the traditional clustering algorithms, spectral clustering has the
superior capability of effectively group data by leveraging the
statistical property of similarity matrix of data.
In this paper, we will involve the Spectral Clustering Algorithm as our previous work in [19].
D. Weight for the Relationships of Tags
We conduct our work on the vector space model and
derived from the information retrieval principle. Each user u,
is modeled as a vector over the set of tags, where w(ti ), in
each dimension corresponds to the relationship of a tag ti with
this user u , u = {w(t1 ), w(t2 ), · · · w(t|T | )} . Likewise each
resource, r = {v(t1 ), v(t2 ), · · · v(t|T | )} , can be modeled as
a vector over the set of tags.If a user u, annotate a tag t, on
a resource r, the w will be “1” in this 3D matrix; otherwise
“0”.
We calculate the weight based on Tf-Idf for term frequencyinverse document frequency.Tf-idf stands for term frequencyinverse document frequency, such weight is a statistical measure used to evaluate how important a word is to a document
in a collection or corpus.

B. Similarity Measure for Tags
The similarity is a quantity that reﬂects the strength of
relationship between two objects. In the previous part, each
user and document can be represented by the pair of tags and
frequencies. In this manner, we perform the transformation of
above two matrices and utilize the cosine function to measure
the similarity between two tags. Its value ranges from 0 to 1,
the higher value of the similarity, the more similar the objects
are [19].
i ·U P
SM (Ui , Uj ) = |UUPiP|×|U
Pj |
The users’ relationship can be represented in the form of
bipartite graph model. Given a graph G = (U,E), where U is
a set of users as Ui = {U1 , U2 , · · · , Um }, and E is a set of
edges which entry SM (Ui , Uj ) reﬂects the similarity between
users, the similarity matrix SM (ui , uj ).
Similarly with the document proﬁles, we can deﬁne a graph
with N users G = (D,E), where D is a set of documents as
Di = {D1 , D2 , · · · , Dn }, and E is a set of edges which entry
SM (Di , Dj ) reﬂects the similarity between documents. The
similarity matrix SM (Di , Dj )is given by:
SM (Di , Dj ) =

IV. E XPANDING TAG G ROUP E XPRESSION
In this section, we propose an approach of generating tag
groups and re-calculate the weight of the tag relationships.
A. Tag Groups Generation Based on Spectral Clustering
In this section we involve the tag clustering algorithm
to ﬁnd out the tags within the same clusters with similar
functions or topics. The spectral clustering we introduced
before, aims to assign a set of observations into subsets so
that observations in the same cluster are similar in some sense
and it based on the graph partition which maps the original
co-occurrence observations onto a new spectrum space with a
reduced dimensionality.
We obtain a set of tag clusters from our previous work which
was based on the Spectral Clustering:
Especially in the context of social tagging systems, we
know each tag can be expressed a column vector of user and
document vector, i.e., Ti = T Ui and Ti = T Di . Then we
stack up T Ui and T Di and form a new tag vector over users
and documents, Ti = T Ui ∪ T Di . At last, we employ spectral
clustering on the integrated tag vectors to get tag clusters.
The pseudo codes of the spectral tag clustering are listed in
Algorithm 1.

DPi ·DPj
|DPi |×|DPj | .

C. Spectral Clustering
Upon the mutual tag similarity is determined, various
clustering algorithms could be applied to partition the tags.
Clustering algorithm aims to assign a set of observations into
subsets so that observations in the same cluster are similar in
some sense. It is specially designed for transactional data and
can efﬁciently partition historic user transactions into clusters.
The object of clustering is: similar objects have high similarity,
the similarity is low between objects of different clusters.
Clustering is the process to adjust the ranks of the similarity

401

two group information by a tunable factor λ ∈ [0, 1] , that

Algorithm 1: Spectral Tag Clustering
Input: The tag-user matrix and tag-document matrix, i.e.,
T U = {T Ui , i = 1, · · · , K},
T D = {T Di , i = 1, · · · , K}
Output: A set of C tag clusters
T C = {T Cc , c = 1, · · · , C} such that the cut of
C-partitioning of the bipartite graph is
minimized
1
2
3
4

5

U R = λU RT + (1 − λ) U RI
.
If λ = 0, which means such fusion is weighted on interest
group only while λ = 1, indicates on topic group only. The
value between 0 and 1 balance the weights between these two
groups.

Construct the integrated tag-user-document matrix by
stacking up the above two matrix, Ti = T Ui ∪ T Di ;
Calculate the diagonal matrices D of T ;
Form a new matrix RT = D−1/2 T D−1/2 ;
Perform SVD (Singular value decomposition) operation
on RT , and obtain k singular vectors Ls to create a new
projection matrix RV;
Execute a clustering algorithm on RV and return clusters
of tags in the integrated vectors:
T C = {T Cc , c = 1, · · · C}.

V. E XPERIMENTAL E VALUATIONS
We evaluate our approach through extensive experiments.
The experiments are conducted on the real world dataset:
“MovieLens” dataset and the “M-Eco system” datasets. Our
experiments focus on results conducted on the implemented
tag-based recommender system with tag neighbor expansion.
The goal of such experiments is to show how can we reduce
sparsity of data and improve the accuracy of recommendation
tag based recommenders.
A. Dataset and Experimental Setup
The “MovieLens” dataset is provided by GroupLens. We
utilize part of the “MovieLens” dataset, which contains tags
provided by users on movies. All users selected had rated
at least 50 movies with rank from 4. The “MovieLens” data
includes 377 users, 2572 documents and 5479 tags. Following
the common protocol in information retrieval domain which
chooses 20-30% data for the testing data, we use 75% of data
as the training data and the remaining 25% as the testing data
to evaluate our approach.

B. Tag Group Expansion
A group is deﬁned to be a virtual expression, in which tags
have similar co-occurrence. Such group information comes
from the bipartite graph between tags and resources (Topic
Group) and between tags or users (Interest Group).
Since the data in social annotation recommender system
is a triple model as < user, resource, tag >, we decompose
this to obtain two different pairs: < resource, tag > and
< user, tag > respectively, which can be represented by a
bipartite graph.By remapping from the original co-occurrence
between tags and users, we obtain the new co-occurrence
data between tag clusters and users. Here, we regard that the
clusters of tags as the user’s interest group information. By
this step, we obtain interest topics based on the relationship
between tags and users. In a similar way, we can also obtain
the topic groups from the resource-tag relationships.
We denote wab as the relevance weight between node a and
b, and wbc as that between node b and c. So the relevance
score between a and c is wab ∗ wbc . So that we can mining
the potential relevance weight among other tags.
By clustering the tags, each resource or user can be reexpressed by a vector over tag clusters. So each group can
be viewed as the representation of an interest area or a topic
area. The higher weight indicates the user is more interested
in such group or the resource is more closed to such group.
So we can obtain the potential relevance between user and
resource.We construct two matrixes U RT and U RI based on
these vectors.
for a given user, the relevance between him and resources
can be computed through different groups. Interest groups are
the interests exhibited by different user groups while topic
group can properly describe what the resources are. Here, topic
information plays an important role as adjusting user to obtain
the resources which they are really interested in. We fuse these

B. Precision Evaluation
We calculate the precision with the same process in our
previous work [21].
In the experiment, we examine the different algorithms’ precision of recommending documents to the individual user. We
calculate the precision in the following steps: The recommendation can be created by various approaches by ranking the
similarity values. Each one can generate the top-N documents
which will be recommended to the user. Then we can compare
them with the existing documents in the test data. If there k
documents appear out of the N recommended documents in
the test data, the number of test documents existing for each
user is Ki , the precision for the individual user is deﬁned as
t:
Ki
× 100%
t=
N
We assume that the existed documents are the preference of
each user. The high value of t the better recommendation we
got. There are 679 existing documents in the “MovieLens”
data for testing. We calculated the precision for each algorithms to evaluate the approach. So that we discussed different
strategies of our approach against existing approaches. The
“Pure Tag Approach” is to calculate the directly similarity
between the user and the document in the tag vector, and the
system will recommend the N documents to the user according
to the top-N similarity values. The “Collaborative Filtering

402

ACKNOWLEDGMENT
This research has been partially supported by the EU
FP7 ICT project M-Eco: Medical Ecosystem Personalized
Event-based Surveillance (No.247829),the Nature Science Research of Anhui (Grant No.1208085MF95), the Nature Science Foundation of Anhui Education Department(Grant No.
KJ2012A273 and KJ2012A274).
R EFERENCES

Fig. 1.

[1] J. Gemmell, T. Schimoler, B. Mobasher, and R. Burke, “Tag-based
resource recommendation in social annotation applications,” in Proceedings of the 19th international conference on User modeling, adaption,
and personalization, UMAP’11, (Berlin, Heidelberg), pp. 111–122,
Springer-Verlag, 2011.
[2] R. Burke, “Hybrid recommender systems: Survey and experiments,”
in User Modeling and User-Adapted Interaction, (Berlin, Heidelberg),
pp. 331–370, Springer-Verlag, 2002.
[3] F. Durao and P. Dolog, “Extending a hybrid tag-based recommender
system with personalization,” in SAC ’10: Proceedings of the 2010 ACM
Symposium on Applied Computing, (New York, NY, USA), pp. 1723–
1727, ACM, 2010.
[4] K. H. L. Tso-Sutter, L. B. Marinho, and L. Schmidt-Thieme, “Tag-aware
recommender systems by fusion of collaborative ﬁltering algorithms,”
in SAC ’08: Proceedings of the 2008 ACM symposium on Applied
computing, (New York, NY, USA), pp. 1995–1999, ACM, 2008.
[5] K. R. Bayyapu and P. Dolog, “Tag and Neighbour Based Recommender
System for Medical Events,” in Proceedings of MEDEX 2010: The First
International Workshop on Web Science and Information Exchange in
the Medical Web colocated with WWW 2010 conference, 2010.
[6] B. M. R. D. B. Jonathan Gemmell, Thomas Schimoler, “Resource recommendation in collaborative tagging applications,” in Proceedings of
the E-Commerce and Web Technologies: 11th International Conference,
(Berlin, Heidelberg), Springer-Verlag, 2010.
[7] Y. Zong, G. Xu, P. Jin, Y. Zhang, E. Chen, and R. Pan, “Appect: An
approximate backbone-based clustering algorithm for tags.,” in ADMA
(1) (J. Tang, I. King, L. Chen, and J. Wang, eds.), vol. 7120 of Lecture
Notes in Computer Science, pp. 175–189, Springer, 2011.
[8] N. R. Di Matteo, S. Peroni, F. Tamburini, and F. Vitali, “A parametric
architecture for tags clustering in folksonomic search engines,” in
Proceedings of the 2009 Ninth International Conference on Intelligent
Systems Design and Applications, ISDA ’09, (Washington, DC, USA),
pp. 279–282, IEEE Computer Society, 2009.
[9] Z. Guan, C. Wang, J. Bu, C. Chen, K. Yang, D. Cai, and X. He,
“Document recommendation in social tagging services,” in Proceedings
of the 19th international conference on World wide web, WWW ’10,
(New York, NY, USA), pp. 391–400, ACM, 2010.
[10] C. Hayes and P. Avesani, “Using tags and clustering to identify topicrelevant blogs,” in International Conference on Weblogs and Social
Media, March 2007.
[11] H. Chen and S. Dumais, “Bringing order to the web: automatically
categorizing search results,” in CHI ’00: Proceedings of the SIGCHI
conference on Human factors in computing systems, (New York, NY,
USA), pp. 145–152, ACM, 2000.
[12] A. Budura, S. Michel, P. Cudré-Mauroux, and K. Aberer,
“Neighborhood-based tag prediction,” The Semantic Web: Research and
Applications, pp. 608–622, 2009.
[13] V. E. R. Boratto L, Carta S., “A robust automated tag clustering
technique,” in Proceedings of the 10th International Proceedings on
E-Commerce and Web Technologies, pp. 324–335, 2009.
[14] L. Q. J. Z. Jingli Zhou, Xuejun Nie, “Web clustering based on tag set
similarity,” in Journal of Computers, pp. 59–66, 2011.
[15] M. G. Noll and C. Meinel, “Web search personalization via social
bookmarking and tagging,” in Proceedings of the 6th international
The semantic web and 2nd Asian conference on Asian semantic web
conference, ISWC’07/ASWC’07, (Berlin, Heidelberg), pp. 367–380,
Springer-Verlag, 2007.
[16] A. Shepitsen, J. Gemmell, B. Mobasher, and R. Burke, “Personalized
recommendation in social tagging systems using hierarchical clustering,”
in Proceedings of the 2008 ACM conference on Recommender systems,
pp. 259–266, ACM, 2008.

Precision for conditional recommended documents in “MovieLens”

Approach” is set as the benchmark of the experiment. The
“KNN-based Clustering TagNeighbor” is to utilize the KNN
algorithm combining with clustering to ﬁlter out the noisy tags
to improve the accuracy of tags to get the better performance
of recommendation. And our new approach named as “Tag
Group Explanation Approach”.
We average the whole precision for all of the users, and
compare the recommendations from top 1 to 50 documents to
the users in the experiments. We denote these four approaches
in the real world dataset are shown in Fig. 1. From the result
we can see that our approach obtains a satisfactory running
performance with the improvement of the precision. And our
approach obtains a better effectiveness to improve the accuracy
problems in the tag-based recommender system. In summary,
the evidence demonstrates the advantage of our approach in
recommendations.
VI. C ONCLUSION
Social annotations systems enable users to annotate resources with tags. Under social tagging systems, a typical
Web2.0 application, users label digital data sources by using
tags which are freely chosen textual descriptions. Tags are
used as one kind of speciﬁc lexical information that is usergenerated metadata with uncontrolled vocabulary, plays a
crucial role in such social collaborative tagging systems.
In this work, We aim on the major problem of most social
tagging systems resulting from the severe difﬁculty of ambiguity, redundancy and less semantic nature of tags, we utilize
the relationship from between tags and resources and between
tags and users to extract group information. With the help of
such relationship, we can obtain the Topic-Groups based on the
bipartite relationship between tags and resources; and InterestGroups based on the bipartite relationship between tags and
users. The preliminary experiments have been conducted on
MovieLens dataset to compare our proposed approach with the
traditional collaborative ﬁltering recommendation approach
approach in terms of precision, and the result demonstrates
that our approach could considerably improve the performance
of recommendations.

403

[17] A. Shepitsen, J. Gemmell, B. Mobasher, and R. Burke, “Personalized
recommendation in social tagging systems using hierarchical clustering,”
in Proceedings of the 2008 ACM conference on Recommender systems,
RecSys ’08, (New York, NY, USA), pp. 259–266, ACM, 2008.
[18] J.-W. v. Dam, D. Vandic, F. Hogenboom, and F. Frasincar, “Searching
and browsing tag spaces using the semantic tag clustering search
framework,” in Proceedings of the 2010 IEEE Fourth International
Conference on Semantic Computing, ICSC ’10, (Washington, DC, USA),
pp. 436–439, IEEE Computer Society, 2010.
[19] R. Pan, G. Xu, and P. Dolog, “User and Document Group Approach
of Clustering in Tagging Systems,” in Proceeding of The 18th Intl.
Workshop on Personalization and Recommendation on the Web and
Beyond, LWA 2010, 2010.
[20] G. Xu, Y. Zong, R. Pan, P. Dolog, and P. Jin, “On kernel information propagation for tag clustering in social annotation systems,” in
Proceedings of the 15th international conference on Knowledge-based
and intelligent information and engineering systems - Volume Part II,
KES’11, (Berlin, Heidelberg), pp. 505–514, Springer-Verlag, 2011.
[21] R. Pan, G. Xu, and P. Dolog, “Improving recommendations in tag-based
systems with spectral clustering of tag neighbors,” in Proceedings of
The 3rd FTRA International Conference on Computer Science and its
Applications (CSA-2011): Computer Science and Convergence, Lecture
Notes in Electrical Engineering, Volume 114, Part 1, CSA’11, (Berlin,
Heidelberg), pp. 355–364, Springer-Verlag, 2011.
[22] A. Hotho, R. Jäschke, C. Schmitz, and G. Stumme, “Folkrank : A ranking algorithm for folksonomies,” in LWA (K.-D. Althoff and M. Schaaf,
eds.), vol. 1/2006 of Hildesheimer Informatik-Berichte, pp. 111–114,
University of Hildesheim, Institute of Computer Science, 2006.
[23] Z. Guan, C. Wang, J. Bu, C. Chen, K. Yang, D. Cai, and X. He,
“Document recommendation in social tagging services,” in Proceedings
of the 19th international conference on World wide web, pp. 391–400,
ACM, 2010.
[24] Z. Guan, J. Bu, Q. Mei, C. Chen, and C. Wang, “Personalized tag recommendation using graph-based ranking on multi-type interrelated objects,”
in Proceedings of the 32nd international ACM SIGIR conference on
Research and development in information retrieval, pp. 540–547, ACM,
2009.
[25] F. Durao and P. Dolog, “Extending a hybrid tag-based recommender system with personalization,” in Proceedings of the 2010 ACM Symposium
on Applied Computing, pp. 1723–1727, ACM, 2010.
[26] M. G. Noll and C. Meinel, “Web search personalization via social
bookmarking and tagging,” in Proceedings of the 6th international
semantic web conference and 2nd Asian conference on Asian semantic
web, pp. 367–380, Springer-Verlag, 2007.
[27] A. Nanopoulos, H. H. Gabriel, and M. Spiliopoulou, “Spectral Clustering
in Social-Tagging Systems,” Web Information Systems EngineeringWISE 2009, pp. 87–100, 2009.

404

Discrete Mathematics 313 (2013) 1327–1337

Contents lists available at SciVerse ScienceDirect

Discrete Mathematics
journal homepage: www.elsevier.com/locate/disc

Determination of the sizes of optimal
(m, n, k, λ, k − 1)-OOSPCs for λ = k − 1, k✩
Rong Pan, Yanxun Chang
Institute of Mathematics, Beijing Jiaotong University, Beijing 100044, PR China

article

info

Article history:
Received 8 November 2012
Received in revised form 16 February 2013
Accepted 24 February 2013
Available online 27 March 2013
Keywords:
Optical orthogonal signature pattern code
Optimal
Optical orthogonal code

abstract
Optical orthogonal signature pattern codes (OOSPCs) were first introduced by Kitayama
in [K. Kitayama, Novel spatial spread spectrum based fiber optic CDMA networks for
image transmission, IEEE J. Sel. Areas Commun. 12 (4) (1994) 762–772]. They find
application in transmitting a two-dimensional image through multicore fiber in the CDMA
communication system. For given positive integers m, n, k, λa and λc , let Θ (m, n, k,
λa , λc ) denote the largest number of codewords among all (m, n, k, λa , λc )-OOSPCs. An
(m, n, k, λa , λc )-OOSPC with Θ (m, n, k, λa , λc ) codewords is said to be optimal. In this
paper, the number of codewords in an optimal (m, n, k, λ, k − 1)-OOSPC (i.e., the exact
value of Θ (m, n, k, λ, k − 1)) is determined for any positive integers m, n, k and λ = k − 1, k.
© 2013 Elsevier B.V. All rights reserved.

1. Introduction
Kitayama [2] proposed a novel code-division multiple-access (CDMA) communication system for image transmissions,
called spatial CDMA. The technology enables parallel transmission and simultaneous access of a two-dimensional (2-D)
image through multicore fiber in spatial CDMA. In this system, each pixel in a 2-D image is encoded into a signature
pattern, which is called an optical orthogonal signature pattern code (OOSPC). For more introductions of spatial CDMA and
the applications of OOSPC, the interested reader may refer to [1,2,4].
Definition 1.1. Let m, n, k, λa and λc be positive integers. An (m, n, k, λa , λc ) optical orthogonal signature pattern code
(briefly, (m, n, k, λa , λc )-OOSPC ) is a family C of m × n (0, 1)-matrices of Hamming weight k satisfying the following two
correlation properties.
(1) The auto-correlation property:
m−1 n−1



xi,j xi⊕s,j⊕
t ≤ λa

i=0 j=0

for any (xij ) ∈ C and every (s, t ) ∈ Zm × Zn \ {(0, 0)}.
(2) The cross-correlation property:
m−1 n−1



xi,j yi⊕s,j⊕
 t ≤ λc

i=0 j=0

for any distinct (xij ), (yij ) ∈ C and every (s, t ) ∈ Zm × Zn ,

✩ Supported by NSFC grant Nos. 61071221, 11271042 and the Fundamental Research Funds for the Central Universities 2011JBZ012.

E-mail address: yxchang@bjtu.edu.cn (Y. Chang).
0012-365X/$ – see front matter © 2013 Elsevier B.V. All rights reserved.
http://dx.doi.org/10.1016/j.disc.2013.02.019

1328

R. Pan, Y. Chang / Discrete Mathematics 313 (2013) 1327–1337

 are, respectively, reduced modulo m and n. The
where Zl denotes the group of integers modulo l. The additions ⊕ and ⊕
number of codewords of C is called the size of C . When λa = λc = λ, the notation (m, n, k, λa , λc )-OOSPC can be briefly
written as (m, n, k, λ)-OOSPC.
t ) : (x, y) ∈ X }.
Throughout this paper, let Ωk (m, n) be the set of all k-subsets of Zm × Zn and define X +(s, t ) = {(x ⊕ s, y⊕
The notation of OOSPCs can be reformulated by using the following set notation. By identifying codewords in C with
k-subsets of Zm × Zn representing the subscripts of the nonzero positions, then C can be viewed as a family F ⊆ Ωk (m, n)
satisfying:
(1′ ) the auto-correlation property:
|X ∩ (X + (s, t ))| ≤ λa
for any X ∈ F and every (s, t ) ∈ Zm × Zn \ {(0, 0)};
(2′ ) the cross-correlation property:

|X ∩ (Y + (s, t ))| ≤ λc
for any distinct X , Y ∈ F and every (s, t ) ∈ Zm × Zn .
For given positive integers m, n, k, λa and λc , let Θ (m, n, k, λa , λc ) denote the largest size among all (m, n, k, λa , λc )OOSPCs. An (m, n, k, λa , λc )-OOSPC with Θ (m, n, k, λa , λc ) codewords is said to be optimal (or maximum). When λa =
λc = λ, Θ (m, n, k, λa , λc ) can be briefly written as Θ (m, n, k, λ).
Note that an optimal (m, n, k, λa , λc )-OOSPC with Θ (m, n, k, λa , λc ) codewords always exists by the definition of optimal
OOSPC. Therefore, the calculation of Θ (m, n, k, λa , λc ) and constructions of optimal (m, n, k, λa , λc )-OOSPC are concerned
at all the time. When m and n are coprime, we know that an optimal (m, n, k, λa , λc )-OOSPC is equivalent to an optimal
(mn, k, λa , λc )-OOC [10]. In this case, a number of optimal OOSPCs are obtained by using rich results on optimal OOCs.
However, when m and n are not coprime, there is a gap between optimal OOSPCs and optimal OOCs. In this case, some
constructions of optimal OOSPCs are already known for very specific conditions. Yang [10] has presented three algebraic
constructions on optimal OOSPCs for the following parameters:
(1) (p, p,
(2) (

,

, 1)-OOSPC for any prime p ≡ 3(mod 4);

p−1 p−3
2
4
qn+1 −1
q −1
q−1

qn+1 −1

n

n−1

,
, qq−−11 , q q−1−1 , 1)-OOSPC for any prime power q and positive integer n;
(3) (p, p, k, 1)-OOSPC for any prime p ≡ 1(mod k(k − 1)).

In [6], Sawa and Kageyama have constructed an optimal (m, n, 3, 1)-OOSPC for any odd integers m, n such that either
m or n is not congruent to 5 modulo 6. They have also given a new upper bound of Θ (m, n, 3, 2, 1) and have presented
two algebraic constructions for optimal (m, n, 3, 2, 1)-OOSPC. In [5], Sawa has shown an equivalence relation between
optimal OOSPC and strictly invariant packing design and has constructed an optimal (2ε x, y, 4, 2)-OOSPC with ε ∈ {1, 2}
and x, y are positive integers, whose each factor being a prime less than 500 000 and congruent to 53 or 77 modulo 120 or
belonging to S = {5, 13, 17, 25, 29, 37, 41, 53, 61, 85, 89, 97, 101, 113, 137, 149, 157, 169, 173, 193, 197, 229, 233, 289,
293, 317}.
In the remainder of this paper, we shall not treat the construction of optimal OOSPCs, but instead, we focus our attention
on two classes of OOSPCs and obtain the formulas of Θ (m, n, k, k, k − 1) and Θ (m, n, k, k − 1).
2. The exact value of Θ (m, n, k , k , k − 1)
In this section, we will determine the exact value of Θ (m, n, k, k, k − 1).
The group Zm × Zn acts naturally on the set Ωk (m, n). The action ϕ of Zm × Zn on Ωk (m, n) is given by ϕ((s, t )) : X →
X + (s, t ), for all X ∈ Ωk (m, n) and (s, t ) ∈ Zm × Zn . For any X ∈ Ωk (m, n), we let

OX = {X + (s, t ) : (s, t ) ∈ Zm × Zn }
denote the orbit of X under Zm × Zn . If the cardinality of OX is mn, then the orbit is said to be full, otherwise short. Obviously,
Ωk (m, n) can be partitioned into some orbits under Zm × Zn . The subgroup
GX = {(s, t ) ∈ Zm × Zn : X + (s, t ) = X }
is called the stabilizer of X under Zm × Zn . If the stabilizer of X is trivial, i.e., GX = {(0, 0)}, it is easy to check that X is in
a full-orbit under Zm × Zn . With these objects, the following result is very useful in determining the value of Θ (m, n, k,
k, k − 1).
Lemma 2.1. Let F be the maximal subset of Ωk (m, n) which satisfies the following property. Any two distinct elements in F
are not in the same orbit. Then Θ (m, n, k, k, k − 1) = |F |.

R. Pan, Y. Chang / Discrete Mathematics 313 (2013) 1327–1337

1329


Proof. Suppose that F is the maximal subset of Ωk (m, n) with the property of the lemma. Namely, |X (Y + (s, t ))| < k
for any two distinct elements X , Y ∈ F . Clearly, F constitutes the family of codewords of an (m, n, k, k, k − 1)-OOSPC. Thus
|F | ≤ Θ (m, n, k, k, k − 1).
Conversely, let F0 be the family of codewords of an optimal (m, n, k, k, k − 1)-OOSPC. Clearly, F0 ⊆ Ωk (m, n). From the
cross-correlation property of an (m, n, k, k, k − 1)-OOSPC, we have |X ∩ (Y + (s, t ))| ≤ k − 1 for any two distinct elements
X , Y ∈ F0 and every (s, t ) ∈ Zm × Zn . It implies that OX ̸= OY , that is, any two distinct elements in F0 are not in the same
orbit. By the hypothesis of the lemma, Θ (m, n, k, k, k − 1) = |F0 | ≤ |F |.
Hence Θ (m, n, k, k, k − 1) = |F |. 
By Lemma 2.1, the problem to determine the exact value of Θ (m, n, k, k, k − 1) is equivalent to count the number of
orbits in Ωk (m, n) under Zm × Zn . For any (s, t ) ∈ Zm × Zn , let fix((s, t )) be the set of elements X ∈ Ωk (m, n) such that
X + (s, t ) = X . Then by the well-known Cauchy–Frobenius–Burnside Lemma [7, Lemma 1.25], we can obtain

Θ (m, n, k, k, k − 1) =

1



mn (s,t )∈Z ×Z
m
n

|fix((s, t ))|.

(2.1)

Thus we need to confine our attention to the value of |fix((s, t ))| for any (s, t ) ∈ Zm × Zn . For notational convenience we
˙ be the disjoint union.
shall let ∪
Lemma 2.2. Let (s, t ) ∈ Zm × Zn with order d. Then

|fix((s, t ))| =

 mn 

 d ,

d | (mn, k),




otherwise.

k
d

0,

Proof. Let A be the subgroup of Zm × Zn which is generated by (s, t ). Then |A| = d. For any X ∈ fix((s, t )), X + (s, t ) = X
and hence X + α = X for any α ∈ A. For any (x, y) ∈ X , we have

j) : (i, j) ∈ A} ⊆ X .
A + (x, y) = {(x ⊕ i, y⊕
It implies that X can be partitioned into some cosets of A in Zm × Zn . So, d|k. Since A is a subgroup of Zm × Zn with order d,
then d | mn. Hence, we have d | (mn, k).
Conversely, if X is a union of some cosets of A in Zm ×Zn , it is easy to check that X +(
s, t ) 
= X . It is shown that X ∈ fix((s, t ))
if and only if X is a union of k/d cosets of A in Zm × Zn . Hence, we have |fix((s, t ))| =

mn
d
k
d

if d | (mn, k) or 0 otherwise.

In what follows, let r ∗ (d) denote the number of elements in Zm × Zn with order d.
Theorem 2.3.

Θ (m, n, k, k, k − 1) =

1




mn d|(mn,k)

r (d)
∗

mn
d
k
d


,

where d | (mn, k) means that d takes over all positive factors of (mn, k).
Proof. Let o((s, t )) be the order of (s, t ) in Zm × Zn . From Eq. (2.1) and Lemma 2.2, we have

Θ (m, n, k, k, k − 1) =

=

=

=

1



mn (s,t )∈Z ×Z
m
n
1



mn d|(mn,k)
1



|fix((s, t ))|

(s,t )∈Zm ×Zn
o((s,t ))=d




mn d|(mn,k)
1

|fix((s, t ))|


(s,t )∈Zm ×Zn
o((s,t ))=d




mn d|(mn,k)

r (d)
∗

mn
d
k
d

mn
d
k
d




. 



1330

R. Pan, Y. Chang / Discrete Mathematics 313 (2013) 1327–1337

We cannot give a nice formula of r ∗ (d) for d in general, but the value of r ∗ (d) is easily calculated for some concrete cases.
As a special case, if d is prime, then we can get the following result.
Lemma 2.4. Let p be prime. Then
r ∗ (p) =


0,

if (mn, p) = 1,
if p | (m, n),
otherwise.

p2 − 1,
p − 1,

Proof. If (mn, p) = 1, by the Lagrange Theorem, r ∗ (p) = 0.
If (mn, p) = p, let o(s) be the order of s in the group Zm . Then we know that
o((s, t )) = [o(s), o(t )]
where [o(s), o(t )] is the least common multiple of o(s) and o(t ). If o((s, t )) = p, then o(s) = o(t ) = p, or o(s) = p and
o(t ) = 1, or o(s) = 1 and o(t ) = p. Note that the number of elements of Zl with order p is ϕ(p) if p | l or 0 otherwise, where
ϕ(p) is the Euler ϕ function.
If p | (m, n), then the element (s, t ) ∈ Zm × Zn with order p if and only if (s, t ) is of the form o(s) = o(t ) = p, or o(s) = p
and o(t ) = 1, or o(s) = 1 and o(t ) = p. Let r1 (p), r2 (p) and r3 (p) be, respectively, the number of elements of Zm × Zn with
order p of the three forms. Then we can have
r1 (p) = ϕ(p)2

and r2 (p) = r3 (p) = ϕ(p).

Thus
r ∗ (p) = r1 (p) + r2 (p) + r3 (p) = p2 − 1.
Finally, if only one of m and n can be divided by p, without loss of generality, we can assume that p | m. Then the element
(s, t ) ∈ Zm × Zn with order p if and only if (s, t ) is the form o(s) = p and o(t ) = 1. It is readily checked that
r ∗ (p) = ϕ(p) = p − 1.
The proof is complete.



Lemma 2.5. Let p be the prime. Then


0,



p(p − 1),
∗ 2
r (p ) =
p2 (p2 − 1),

 2
p (p − 1),

if (m, p2 ) ̸= p2 and (n, p2 ) ̸= p2 ,
if p2 | mn and (m, n, p2 ) = 1,
if p2 | (m, n),
otherwise.

Proof. Note that o((s, t )) = [o(s), o(t )] for any (s, t ) ∈ Zm × Zn . We can check all possible forms of (s, t ) ∈ Zm × Zn with
order p2 case by case.
Form 1: o(s) = o(t ) = p2 .
Form 2: o(s) = p2 and o(t ) = p.
Form 3: o(s) = p and o(t ) = p2 .
Form 4: o(s) = p2 and o(t ) = 1.
Form 5: o(s) = 1 and o(t ) = p2 .
If both m and n cannot be divided by p2 , by the Lagrange Theorem, r ∗ (p2 ) = 0.
If p2 | (m, n), the element (s, t ) ∈ Zm × Zn is with order p2 if and only if (s, t ) is of the above five forms. It is easy to check
that
r ∗ (p2 ) = ϕ(p2 )2 + 2ϕ(p2 )ϕ(p) + 2ϕ(p2 ) = p2 (p2 − 1).
If only one of m and n can be divided by p2 , without loss of generality, we can assume that p2 | m. If (n, p2 ) = p, then the
element (s, t ) ∈ Zm × Zn is with order p2 if and only if (s, t ) is of Form 2 or Form 4. It is readily checked that
r ∗ (p2 ) = ϕ(p2 )ϕ(p) + ϕ(p2 ) = p2 (p − 1).
If (n, p) = 1, then (s, t ) ∈ Zm × Zn is with order p2 if and only if (s, t ) is Form 4. Then we can have
r ∗ (p2 ) = ϕ(p2 ) = p(p − 1).
The proof is complete.



R. Pan, Y. Chang / Discrete Mathematics 313 (2013) 1327–1337

1331

Corollary 2.6. Let m, n be positive integers and let p be prime. Then

Θ (m, n, p, p, p − 1) =

 

mn
1


,


mn
p






1

mn

mn

p

if (mn, p) = 1,
mn



+ (p2 − 1)
,

mn
p
p







 1
mn
mn


+ (p − 1)
,

if p | (m, n),
otherwise.

p

Proof. By Theorem 2.3, we know that

Θ (m, n, p, p, p − 1) =

1




r (d)
∗

mn d|(mn,p)

mn
d
p
d



 
 1
mn
∗

r (1)
,


 mn
p


=
 

1
mn
mn


r ∗ (1)
+ r ∗ (p)
,

mn

p

p

if (mn, p) = 1,
if (mn, p) = p.

Note that there is a unique element of order 1 in any group, that is r ∗ (1) = 1. The conclusion then follows by Lemma 2.4.
As a simple example, if we take p = 3 in Corollary 2.6, we have the following result:

 2 2
m n − 3mn + 2


,


6


 2 2
m n − 3mn + 18
Θ (m, n, 3, 3, 2) =
,

6




2 2

 m n − 3mn + 6 ,
6

if (mn, 3) = 1,
if 3 | (m, n),
otherwise.

Applying Theorem 2.3 with k = p2 where p is a prime, we similarly have the following corollary.
Corollary 2.7. Let m, n be positive integers and let p be prime.

Θ (m, n, p2 , p2 , p2 − 1)
 1  mn 

,



mn p2



 

 mn 


1
mn


+ (p − 1) p
,


 mn
p2
p


 



 
 mn 

 1
mn
(p − 1)mn

p

+ (p − 1)
+
,



p2
p
p
 mn
 

=
 mn 

1
mn

2
2
p

+ (p − 1)
+ (p − 1)mn ,



mn
p2
p




 

 mn 


1
mn

2

+ (p − 1) p
,

 mn

p2
p





 

 mn 


1
mn

2

+ (p − 1) p + (p − 1)mn ,

2
mn

p

p

Proof. It follows by Theorem 2.3, Lemmas 2.4 and 2.5.



if (mn, p2 ) = 1,
if (mn, p2 ) = p,

if (mn, p2 ) = p2 and (m, n, p2 ) = 1,

if (m, n, p2 ) = p2 ,

if (m, p2 ) = p and (n, p2 ) = p,

otherwise.



1332

R. Pan, Y. Chang / Discrete Mathematics 313 (2013) 1327–1337

Taking p = 2 in Corollary 2.7, we have the following result:

 1  mn 
,



mn
4



 mn 


 1  mn 


+ 2
,


mn
2
4


 


 mn 



1  mn 
mn


+ 2 +
,



2
2
4
 mn


 mn 
Θ (m, n, 4, 4, 3) =
 mn 
1


+ 3 2 + 3mn ,



mn
2
4








mn 
 mn 

1



+3 2
,


mn
4
2




 
 mn 

 mn 


 1
2

+ mn ,
+3
mn

4

2

if (mn, 4) = 1,
if (mn, 4) = 2,
if (mn, 4) = 4 and (m, n, 4) = 1,
if (m, n, 4) = 4,
if (m, 4) = 2 and (n, 4) = 2,

otherwise.

3. The exact value of Θ (m, n, k , k − 1)
In this section, we determine the exact value of Θ (m, n, k, k − 1). First, we introduce the well-known Möbius function
and a related result as follows.
1,
µ(n) = 0,
(−1)r ,



if n = 1,
if n has a square factor,
if n is the product of r distinct prime numbers.

Lemma 3.1 ([8]).



µ(d) =

d|n

1,
0,



if n = 1,
if n > 1,

where d | n means that d takes over all positive factors of n.
The following result is similar to that of Lemma 2.1.
Lemma 3.2. Let F be the maximal subset of Ωk (m, n) which satisfies the following two properties.
(1) The stabilizer of X is trivial for each X ∈ F .
(2) Any two distinct elements in F are not in the same orbit.
Then Θ (m, n, k, k − 1) = |F |.
Proof. Let F be the maximal subset of Ωk (m, n) with the properties of the lemma. Since GX is trivial for any X ∈ F , namely,
X ̸= X + (s, t ) unless (s, t ) = (0, 0). It implies that |X ∩ (X + (s, 
t ))| < k for every (s, t ) ∈ Zm × Zn \ {(0, 0)}. And any two
distinct elements X , Y ∈ F are not in the same orbit, that is, |X (Y + (s, t ))| < k for every (s, t ) ∈ Zm × Zn . Clearly, F
constitutes the family of codewords of an (m, n, k, k − 1)-OOSPC. Thus |F | ≤ Θ (m, n, k, k − 1).
Conversely, let F0 be the family of codewords of an optimal (m, n, k, k − 1)-OOSPC. Clearly, F0 ⊆ Ωk (m, n). From the
auto-correlation property, |X ∩ (X + (s, t ))| < k for any X ∈ F0 and every (s, t ) ∈ Zm × Zn \ {(0, 0)}. It is readily checked
that X = X + (s, t ) if and only if (s, t ) = (0, 0), namely, GX is trivial. And from the cross-correlation property, we have
|X ∩ (Y + (s, t ))| ≤ k − 1 for any two distinct elements X , Y ∈ F0 and every (s, t ) ∈ Zm × Zn . It implies that OX ̸= OY , that is,
any two distinct elements in F0 are not in the same orbit. By the hypothesis of the lemma, Θ (m, n, k, k − 1) = |F0 | ≤ |F |.
Hence Θ (m, n, k, k − 1) = |F |. 
By Lemma 3.2, the problem to determine the exact value of Θ (m, n, k, k − 1) is equivalent to count the number of fullorbits in Ωk (m, n) under Zm × Zn . Let g (x) denote the number of k-subsets X ∈ Ωk (m, n) such that |GX | = x, that is,
g (x) = |{X ∈ Ωk (m, n) : |GX | = x}|.
In particular,
g (1) = |{X ∈ Ωk (m, n) : GX is trivial}|.

R. Pan, Y. Chang / Discrete Mathematics 313 (2013) 1327–1337

1333

Then, we can get

Θ (m, n, k, k − 1) =

1
mn

g (1).

(3.1)

Lemma 3.3. For any X ∈ Ωk (m, n), if |GX | = d, then d | (mn, k).

t ) : (s, t ) ∈ GX } ⊆ X . Then X can be partitioned into some
Proof. For any (x, y) ∈ X , we have (x, y) + GX = {(x ⊕ s, y⊕
cosets of GX in Zm × Zn . This gives d|k. Since GX is a subgroup of Zm × Zn with order d, we have d | mn. The conclusion
follows. 
Let H be a subgroup of Zm × Zn with order d. Define g (H ) to be the number of k-subsets X ∈ Ωk (m, n) such that GX = H,
that is,
g (H ) = |{X ∈ Ωk (m, n) : GX = H }|.
Notice that if d cannot divide (mn, k), then g (H ) = 0 by Lemma 3.3. Suppose d | (mn, k), for any positive integer x such that
d | x, define g (≥H , x) to be the number of k-subsets X ∈ Ωk (m, n) such that H ≤ GX and |GX | = x, where H ≤ GX means
that H is a subgroup of GX , that is,
g (≥H , x) = |{X ∈ Ωk (m, n) : H ≤ GX and |GX | = x}|.
Similarly, if x cannot divide (mn, k), we have g (≥H , x) = 0.
Lemma 3.4. Let m, n, and k be positive integers. For any subgroup H ≤ Zm × Zn with order d which divides (mn, k), we have




g (≥ H , x) =

d|ẋ|(mn,k)

mn
d
k
d


.

Proof. Let Ω (≥H ) denote the set of k-subsets X ∈ Ωk (m, n) such that H is a subgroup of GX . For any X ∈ Ω (≥H ), we have
X + (s, t ) = X for any (s, t ) ∈ H. Then X can be partitioned into some cosets of H in Zm × Zn . Conversely, if X is a union of
some cosets of H in Zm × Zn , it is obvious that X + (s, t ) = X for any (s, t ) ∈ H, which implies that X ∈ Ω (≥H ). This shows
that X ∈ Ω (≥H ) if and only if X can be written as a disjoint union of dk cosets of H in Zm × Zn . Hence, we have


|Ω (≥H )| =

mn
d
k
d


.

For any positive integer x such that d | x, let Ω (≥ H , x) be the set of k-subsets X ∈ Ω (≥H ) such that |GX | = x. Then
g (≥H , x) = |Ω (≥H , x)|. For any X ∈ Ω (≥H ), if |GX | = x, then d | x. And by Lemma 3.3 we have x | (mn, k). Then


˙

Ω (≥H ) =

Ω (≥H , x).

d|ẋ|(mn,k)

Therefore




g (≥H , x) = |Ω (≥ H )| =

d|ẋ|(mn,k)

mn
d
k
d


. 

Lemma 3.5. Let r (G, d) denote the number of subgroups of G with order d. For any positive integers d and x such that d | x and
x | (mn, k), we have
g ( x) =



g (≥H , x) −

H ≤ Z m ×Z n
|H |=d



(r (G, d) − 1)g (G).

G ≤ Z m ×Z n
|G|=x

Proof. Let G be a subgroup of Zm × Zn with order x and Ω (G) be the set of k-subsets X ∈ Ωk (m, n) such that GX = G. Then
g (G) = |Ω (G)|. Define Ω (x) to be the set of k-subsets X ∈ Ωk (m, n) such that |GX | = x. Then g (x) = |Ω (x)|. From the
definitions of Ω (G) and Ω (x), we have


˙ Ω (G).

Ω (x) =

G≤Zm ×Zn
|G|=x

Hence, |Ω (x)| =
g (x) =



G≤Zm ×Zn
|G|=x


G≤Zm ×Zn
|G|=x

g (G).

|Ω (G)|, that is,
(3.2)

1334

R. Pan, Y. Chang / Discrete Mathematics 313 (2013) 1327–1337

Now, let H be a subgroup of Zm × Zn with order d and Ω (≥H , x) be the set of k-subsets X ∈ Ωk (m, n) such that H ≤ GX
and |GX | = x. Then |Ω (≥H , x)| = g (≥H , x). Let G be a subgroup of Zm × Zn with order x such that H ≤ G. If X ∈ Ω (G)
then H ≤ G = GX and |GX | = |G| = x. Clearly, X ∈ Ω (≥H , x). Conversely, if X ∈ Ω (≥H , x), there is a unique subgroup
G ≤ Zm × Zn with order x which satisfies H ≤ G such that X ∈ Ω (G). Hence


˙

Ω (≥H , x) =

Ω (G).

H ≤G≤Zm ×Zn
|G|=x

Therefore



g (≥H , x) =

g (G).

(3.3)

H ≤G≤Zm ×Zn
|G|=x

From Eqs. (3.2) and (3.3), we have



g (≥H , x) =





g (G)

H ≤Zm ×Zn H ≤G≤Zm ×Zn
|G|=x
|H |=d

H ≤Zm ×Zn
|H |=d

=


 


G≤Zm ×Zn
|G|=x

=



H ≤G
|H |=d


1 g (G)



r (G, d)g (G)

G≤Zm ×Zn
|G|=x

=





g (G) +

G≤Zm ×Zn
|G|=x

(r (G, d) − 1)g (G)

G≤Zm ×Zn
|G|=x



= g (x) +

(r (G, d) − 1)g (G).

G ≤ Z m ×Z n
|G|=x

Hence
g (x) =





g (≥H , x) −

H ≤ Z m ×Z n
|H |=d

(r (G, d) − 1)g (G). 

G≤Zm ×Zn
|G|=x

Theorem 3.6. Let r (G, d) denote the number of subgroups of G with order d. Then

Θ (m, n, k, k − 1) =

1




mn d|(mn,k)

µ(d)r (Zm × Zn , d)

mn
d
k
d


−

1



mn x|(mn,k)

Proof. For given d which divides (mn, k), by Lemma 3.4 we have





g (≥H , x) =

d|ẋ|(mn,k) H ≤Zm ×Zn
|H |=d





g (≥H , x)

H ≤Zm ×Zn d|ẋ|(mn,k)
|H |=d


=


H ≤Zm ×Zn
|H |=d

mn
d
k
d




= r (Zm × Zn , d)

From Lemma 3.5, we have
g (x) =



g (≥H , x) −

H ≤ Z m ×Z n
|H |=d



(r (G, d) − 1)g (G).

G≤Zm ×Zn
|G|=x

Note that


d|(mn,k)

µ(d)


d|ẋ|(mn,k)

g (x) =

 
x|(mn,k) d|x

µ(d)g (x) = g (1).

mn
d
k
d


.

 
G≤Zm ×Zn
|G|=x

d|x

µ(d)(r (G, d) − 1)g (G).

R. Pan, Y. Chang / Discrete Mathematics 313 (2013) 1327–1337

1335

Hence, we obtain that
g (1) =



µ(d)

d|(mn,k)

=


d|(mn,k)



g ( x)

d|ẋ|(mn,k)




  

g (≥H , x) −
(r (G, d) − 1)g (G)
µ(d)

d|ẋ|(mn,k)

H ≤ Z m ×Z n
|H |=d


=



µ(d)r (Zm × Zn , d)

d|(mn,k)


=



µ(d)r (Zm × Zn , d)

d|(mn,k)

G≤Zm ×Zn
|G|=x

mn
d
k
d



mn
d
k
d



From Eq. (3.1) we have Θ (m, n, k, k − 1) =

−


d|(mn,k)

−





µ(d)

(r (G, d) − 1)g (G)

d|ẋ|(mn,k) G≤Zm ×Zn
|G|=x

 

x|(mn,k) G≤Zm ×Zn
|G|=x
1
g
mn



µ(d)(r (G, d) − 1)g (G).

d|x

(1). This completes the proof.



This formula is not as nice as the formula of Θ (m, n, k, k, k − 1), since we cannot give the generalized formulas of
r (Zm × Zn , d)

(3.4)

and



 

x|(mn,k) G≤Zm ×Zn
|G|=x

µ(d)(r (G, d) − 1)g (G).

(3.5)

d|x

But for some concrete cases, their values can be calculated. In particular, we focus our attention on the case of k being prime
power and give the generalized formulas in this case.
Lemma 3.7. Let m and n be positive integers. If p is the prime, then


r (Zm × Zn , p) =

0,
p + 1,
1,

if (mn, p) = 1,
if p | (m, n),
otherwise.

Proof. Notice that a group with order p is cyclic, and there are ϕ(p) elements with order p in the group. If we let r ∗ (p) be
the number of elements of Zm × Zn with order p, then
r ∗ (p) = ϕ(p)r (Zm × Zn , p).
Hence, from Lemma 2.4 we can obtain the result.




Lemma 3.8. Let m, n be positive integers and k = pa where p is the prime. Then the value of Formula (3.5) is −p
p | (m, n) and a ≥ 2, or 0 otherwise.

mn
p2
k
p2


if

Proof. Notice that
 for any group G, there is only one subgroup of G with order 1, that is, r (G, 1) = 1. If (mn, k) = 1, then
Formula (3.5) is
G≤Zm ×Zn (r (G, 1) − 1)g (G) = 0.
|G|=1

Now suppose that (mn, p) = p. We shall discuss Formula (3.5) in the following two cases.
Case (1) If a = 1 (i.e., k = p), since the group of order p is cyclic, there is a unique subgroup of order d for any positive
integer d which divides p. In this case, Formula (3.5) is


G≤Zm ×Zn
|G|=1

(r (G, 1) − 1)g (G) −

 
G≤Zm ×Zn
|G|=p

µ(d)(r (G, d) − 1)g (G) = 0.

d|p

Case (2) If a ≥ 2, we distinguish two subcases.
Subcase (2-1) (m, n, p) = 1, that is, m and n cannot be divided by p at the same time. Note that for any abelian p-group G, if
G has a unique subgroup with order p, then it is cyclic [3, Proposition 1.2.10]. In this case, we have r (Zm × Zn , p) = 1 from
Lemma 3.7. For any subgroup G ≤ Zm ×Zn with order pb (b ≥ 1), since p | |G|, there is at least one subgroup of G with order
p by the Sylow Theorem (refer to [3, Section 2.3]), namely, r (G, p) ≥ 1. On the other hand, r (G, p) ≤ r (Zm × Zn , p) = 1.
Then r (G, p) = 1. Thus G is cyclic. Similarly as Case (1), the value of Formula (3.5) is zero.

1336

R. Pan, Y. Chang / Discrete Mathematics 313 (2013) 1327–1337

Subcase (2-2) (m, n, p) = p, that is, p | m and p | n. In this case, Formula (3.5) is



 

x|(mn,k) G≤Zm ×Zn
|G|=x



=





µ(d)(r (G, d) − 1)g (G)

d|(mn,k) d|ẋ|(mn,k) G≤Zm ×Zn
|G|=x

d|x



(r (G, 1) − 1)g (G) −

1|ẋ|(mn,k) G≤Zm ×Zn
|G|=x





(r (G, p) − 1)g (G)

p|ẋ|(mn,k) G≤Zm ×Zn
|G|=x





p2 |ẋ|(mn,k)

G≤Zm ×Zn
G is non-cyclic
|G|=x

=−



µ(d)(r (G, d) − 1)g (G) =

(r (G, p) − 1)g (G).

(3.6)

If we set A = {G ≤ Zm × Zn : G is non-cyclic such that p2 | |G| and |G| | (mn, k)}, then Formula (3.6) is equal to

−


(r (G, p) − 1)g (G).

(3.7)

G∈A

Notice that a non-cyclic group with order p2 is isomorphic to Zp × Zp [3, Theorem 2.4.9]. From Lemma 3.7, we have
r (Zp × Zp , p) = p + 1. Since r (Zm × Zn , p) is also p + 1, then we can claim that there is only one non-cyclic subgroup of
Zm × Zn with order p2 . If not, suppose H , H ′ are distinct non-cyclic subgroups of Zm × Zn with order p2 . Then there is at
least one element α ∈ H and α ̸∈ H ′ with order p. It implies r (Zm × Zn , p) ≥ r (HH ′ , p) ≥ p + 2, a contradiction. Now we
let H be the unique non-cyclic subgroup of Zm × Zn with order p2 , and set

B = {G ≤ Zm × Zn : H ≤ G, p2 | |G|, |G| | (mn, k)}.
We shall claim that A = B . Clearly, B ⊆ A. On the other hand, for any G ∈ A, let H ′ be a non-cyclic subgroup of G with
order p2 . Then H ′ is also a subgroup of Zm × Zn . Thus H ′ = H by the uniqueness of H. Thus G ∈ B , namely, A ⊆ B . Hence,
A = B . Notice that for any G ∈ B ,
r (G, p) = r (H , p) = p + 1.
And by Lemma 3.4 and Eq. (3.3) we obtain




mn
p2
k
p2


=



g (≥H , x)

p2 |x|(mn,k)

=





g (G)

p2 |x|(mn,k) H ≤G≤Zm ×Zn
|G|=x

=



g (G).

G∈B

Then Formula (3.7) can be written as

−



(r (G, p) − 1)g (G) = −
(r (G, p) − 1)g (G)
G∈A

G∈B


= −p



g (G) = −p 

G∈B

The proof is complete.

mn
p2
k
p2


.



Now, we shall give a corollary for Theorem 3.6 as follows.
Corollary 3.9. Let m, n be positive integers and p be prime. Then

Θ (m, n, p, p − 1) =

 1  mn 

,


 mn p




 1
mn

mn
p


 


1
mn


mn

p

if (mn, p) = 1,
mn

− (p + 1)
p

mn
−
,
p



,

if p | (m, n),
otherwise.

R. Pan, Y. Chang / Discrete Mathematics 313 (2013) 1327–1337

1337

And for each a ≥ 2, it holds that

Θ (m, n, p , p − 1) =
a

a

 1  mn 

,


mn pa





 

 1
mn
a

if (mn, p) = 1,

− (p + 1)

mn
p



  



1
mn



−
a
mn

p

mn
p

p a −1

mn
p



pa−1






+p



mn
p2

pa−2

,

Proof. The conclusions follow by Theorem 3.6, Lemmas 3.7 and 3.8.

,

if p | (m, n),

otherwise.



4. Conclusion
In the present paper, formulas of Θ (m, n, k, k, k − 1) and Θ (m, n, k, k − 1) are given for any positive integers m, n and
k, that is, the number of codewords of an optimal (m, n, k, λ, k − 1)-OOSPC has been determined for λ = k − 1 and k.
When m and n are coprime, recall the equivalence between OOSPCs and OOCs, we can verify that the results in this paper
are consistent with the results which are obtained from optimal OOCs [9]. When m and n are not coprime, it is easily checked
that the theoretical results given by Corollaries 2.6, 2.7 and 3.9 tally with the results of computer searching for some small
parameters m, n, k which are listed below.

(m, n, k)
(2, 6, 3)
(2, 8, 3)
(3, 3, 3)
(3, 6, 3)
(2, 4, 4)
(2, 6, 4)
(3, 4, 4)
(4, 4, 4)
(2, 4, 5)
(3, 3, 5)

Θ (m, n, k, k, k − 1)

Θ (m, n, k, k − 1)

19
35
12
48
12
45
43
122
7
14

18
35
8
44
7
38
40
109
7
14

References
[1] A.A. Hassan, J.E. Hershey, N.A. Riza, Spatial optical CDMA, IEEE J. Sel. Areas Commun. 13 (3) (1995) 609–613.
[2] K. Kitayama, Novel spatial spread spectrum based fiber optic CDMA networks for image transmission, IEEE J. Sel. Areas Commun. 12 (4) (1994)
762–772.
[3] J.H. Kwak, M. Xu, Finite Group Theory for Combinatorists, 2010.
[4] W.C. Kwong, G.C. Yang, Image transmission in multicore-fiber code-division multiple-access networks, IEEE Commun. Lett. 2 (10) (1998) 285–287.
[5] M. Sawa, Optical orthogonal signature pattern codes with maximum collision parameter 2 and weight 4, IEEE Trans. Inform. Theory 56 (7) (2010)
3613–3620.
[6] M. Sawa, S. Kageyama, Optimal optical orthogonal signature pattern codes of weight 3, Biom. Lett. 46 (2) (2009) 89–102.
[7] D.R. Stinson, Combinatorial Designs: Constructions and Analysis, Springer-Verlag, New York, Inc., 2004.
[8] J.H. Van Lint, R.M. Willson, A Course in Combinatories, second ed., Cambridge University Press, 2001.
[9] Y. Yang, New enumeration results about the optical orthogonal codes, Inform. Process. Lett. 40 (1991) 85–87.
[10] G.C. Yang, W.C. Kwong, Two-dimensional spatial signature patterns, IEEE Trans. Commun. 44 (2) (1996) 184–191.

User and Document Group Approach of
Clustering in Tagging Systems
Rong Pan, Guandong Xu and Peter Dolog
IWIS — Intelligent Web and Information Systems
Department of Computer Science
Aalborg University
{rpan, xu, dolog}@cs.aau.dk
Abstract
In this paper, we propose a spectral clustering approach for users and documents group
modeling in order to capture the common
preference and relatedness of users and documents, and to reduce the time complexity
of similarity calculations. In experiments,
we investigate the selection of the optimal
amount of clusters. We also show a reduction of the time consuming in calculating the
similarity for the recommender systems by selecting a centroid first, and then compare the
inside item on behalf of each group.
keywords: User Profile, Document Profile,
Spectral Clustering, Group Profile, Modularity Metric

1

Introduction

The success of social tagging resulted in the proliferation of sites like Delicious, CiteUlike, Digg, or Flickr.
Such sites contain large amount of user tagged data
for information retrieval in social-tagging systems [6;
7; 12; 16; 17], or for the establishment of user profiles
and the discovery of topics, among other applications.
[5] uses the tags associated with specified objects
to build a single user profile. However, here comes a
problem: it is hard to express the entire user profile or
the document profile. The traditional user profile expresses the users’ preferences depending on collecting
users’ behaviors information, such as provides many
tedious options in their registrations. The disadvantage with such an approach is too much reliance on
users who is not able very often to express his entire
user profile and interests. The document profile shows
the background, categories, and keywords, it also depends on the description when it is added into the
system. However, with the increase of the number
and types of users, it’s hard to express the different
emphases for various users with the same document.
In social collaborative (tagging) systems, the common perception or judgment on documents are determined by a group of users rather than a single user. In
a similar way, by using a group of documents, rather
than one document, it might represent much more specific information during the information search.
Therefore, our assumption is that by utilizing the
community views of users and documents, we are able
to facilitate the organization of information resources
in search and navigation.

In social tagging systems, users express their judgment by annotations or tagging. The tag can endorse
their opinions on various web items, which is one of
the defining characteristics of Web 2.0 services, allowing them to collectively classify and index information
for later search and sharing. With social tagging, a
user can express his own perspective on web items,
e.g. resources like images, videos, scientific papers,
thus allowing other like-minded users to find and use
the similar information.
The tagging has been already utilized for organizing the resources. [3] develops a page rank algorithm
of resources based on preference tag vectors. [6; 8;
7] investigate social and behavioral aspects of a tagbased recommender system which suggests similar web
pages based on the similarity of users’ tags. However,
there is another problem emerging: not all of the social
tagging systems proposed so far maintain high quality
and quantity of tag data. It is particularly prominent
when a new user enters the system or a new document
is added into the system.
If the individual user profile or document profile can
be collected and grouped into several groups characterized by the significant tags, it is believed that common
tags annotated by the most objects inside the group
can reflect the characteristics of user preference or document functionality. Moreover, it will be of benefit for
solving the problem of low tag quality of individual
user or document. Even when a new user or a new
document is added into the system, the tags can be
extended to the user by referring to the majority tagging behavior of users on documents.
Regarding to the previous problems, even if the tag
is rich enough for the users and documents, the time
consuming is still very high when a user wants to get
the most appropriate document from a large document
database, since the system has to calculate the similarity between users or documents one by one.
We propose the method that calculates the similarity between the target tag vector and the centroids of
all clusters to determine the cluster with highest similarity, then calculate the similarity of the target tag
vector with the document profiles inside the cluster to
rank the whole documents. In such way the time consuming can be reduced. Since we have got the groups
of user profile or document profile, how to choose the
number of clusters is another problem. The traditional
way is to assign the initial clustering number manually. In this paper, we use the modularity metric [13]
to evaluate the optimal number for the clusters.
Based on the problems mentioned above, this paper

proposes an approach for group modeling by utilizing a
clustering algorithm. The group modeling aims at assigning the individual users or document profiles into
different groups, which correspond to various user preferences or content relatedness from the large amount
of data for tagging.
User Group Profile and Document Group Profile can
be generated from individual user profiles and document profiles; both of them are expressed by the tags.
Group profiling is not constructed based on stereotypes but based on the results of clustering algorithms
from transactional data. It can identify the objects inside the community with similar tags, and collect the
data for the similar objects. It can expand the tag set
for the individual object inside the community which is
helpful for the poor tag quality and quantity. Furthermore, for making tag-based recommendation, it will
significantly reduce the time consuming in calculating
the similarity between the user and document groups.
The main contributions in this paper are:
1. A group modeling method by utilizing the clustering algorithm.
2. The most appropriate number of clusters to generate the User Group Profile and the Document
Profile by using the modularity metric.
3. Reduction in time needed for computation for organizing the documents comparing to the other
methods.
The rest of the paper is organized as follows: Section
2 presents the related work in the field of clustering and
profiling. In section 3, we describe the preliminaries
for the data model. Section 4 discusses the details of
user profile and document profile with the introduced
mathematical models and how to get the group profile by utilizing the spectral clustering algorithm. The
experiment is designed in terms of datasets and evaluation measures in section 5, and experimental results
and comparisons are presented in this section as well.
We conclude the paper and discuss possible future research directions in section 6.

2

Related Work

The folksonomy in [3] has been defined as a data structure that evolves over time when people annotate resources with freely chosen words. It is user-contributed
data aggregated by collaborative tagging systems. In
such systems, users are allowed to choose terms freely
to describe their favorite web resources. A folksonomy is generally considered to consist of at least three
sets of elements, namely users, tags and resources. Although there can be different kinds of resources.
The prerequisite of personalization is to acquire
user profile that describes user’s interests, preferences and background knowledge about specified domains. Methods are used for modeling user profiles include logic-based representation and inference,
Bayesian models, feature-based filtering, Clique-based
filtering, and neural networks. However, such user profile is still for individual persons. Our approach is to
cluster the similar users in the same communities. [5]
proposes to create user profiles from the data available
in such folksonomy systems by letting user specify the
most relevant objects in the system. Instead of using

the objects directly to represent the user profiles, they
use the tags associated with the specified objects to
build the user profiles.
[20] presents analysis on the personal data in folksonomies, and investigates how accuracy rate user profiles can be generated from this data. They propose
an algorithm to generate user profiles which can accurately represent the multiple interests.
F. Durao and P. Dolog in [6; 8; 7] present a tagbased recommender system which suggests similar
Web pages based on the similarity of their tags from
a Web 2.0 tagging application. They also propose an
approach to extend the basic similarity calculus with
external factors such as tag popularity, tag representativeness and the affinity between user and tag.
K. R. Bayyapu and P. Dolog in [2] tries to solve the
problems of sparse data and low quality of tags from
related domains. They suggest using tag neighbors for
tag expression expansion. However the tag neighbors
are based on the content of documents. We propose
another approach to extend the tag set by the group
profiling.
[19] uses a framework of User-Profile Modeling based
on Transactional data for modeling user group profiles
based on the transactional data which can incorporate
external information, either by means of an internal
knowledge base or on dynamic data supplied by a specific information extraction system. Such user group
profiles consist of three types: basic information attributes, synthetic attributes and probability distribution attributes. User profiles are constructed by
clustering user transaction data and integrating cluster attributes with domain information extracted from
application systems and other external data sources.
And Teevan et al. apply group profiles to personalize
search by an algorithm to “groupiz” (versus “personalize”) in result ranking on group-relevant queries [18]
, Abel et al. [1] shows that the quality of search result
ranking in folksonomy systems can be significantly improved by introducing and exploiting the grouping of
resources and Mei and Church show that group profiles
facilitate Web search [11].
Clustering can divide the large amount of data into
several groups. Clustering algorithms, specially designed for transactional data, can efficiently partition
historic user transactions into clusters [9][15]. Each
cluster is a set of transactions representing the interests of a particular user group. It is the assignment of
a set of observations into subsets so that observations
in the same cluster are similar in some sense. We want
to use the clustering for unsupervised learning in the
group profiling.

3

Preliminaries for Folksonomy Data
Model

The user profile can be used to store the description
of the user’s characteristics. Such information can be
exploited in social tagging systems for taking the persons’ characteristics and preferences into account . For
example, the social tagging systems usually ask the
users to choose their own words as tags to describe
the favorite web resource. So the user profiles can justify the benefit and interest for various users.
The document profile is represented by the metadata generated by the community of users tagging the

documents. It is the process that refers to the construction of a profile for a specific via the extraction
from a set of tagging data.
When users want to annotate web resource for better organization and use the relevant information to
their needs later, they will tag such information with
free-text keywords. The tags, which are given by the
users, reflect the navigational preference and interest
of them. On the other hand, with the increase of documents number that the user visited and annotated,
each user has his own tag set which characterizes the
interest or preference. Likewise, each tagged document
also has its own tag set which expresses the content relatedness and subject of the document. In the context
of social tagging systems, the user profiles and document profiles thus are expected to be represented by
the representative tags. Therefore the process of user
and document modeling is to capture the significant
tags from a large volume of tagging data in a social
collaborative environment.
There are a number of studies on user and document
profiling (see for example [20; 19]). Amongst them, the
basic idea of such approaches is originated from the introduction of a specific mathematical modeling of folksonomy. The folksonomy is a three-dimensional data
model of social tagging behaviors of users. In social
tagging systems, both the user profiles and document
profiles are formulated starting from the folksonomy
model. In the following section, in order to well reveal
the mutual relationships between these three-fold entities, i.e. user, item and tag, we firstly briefly discuss
the data model used in the following group profiling
processes.
A folksonomy F according to [10] is a tuple F = (U,
T, D, A), where U is a set of users, T is a set of tags,
D is a set of Web documents, and A ⊆ U × T × D is a
set of annotations. The relationship is shown as Fig1.

t ∈ T.
Upon the folksonomy data model, we can derive the
user and document profile by utilizing the relationship
among the users, tags and documents in the tagging
procedures, which will be discussed in the following
section.

4

User Group Profiling and Document
Group Profiling by Clustering

As mentioned in the introduction section, the poor
quality and quantity of tag data would be a problem.
Meanwhile, the time complexity is also a big concern
when calculating the recommendation rank for the objects based on the large amounts of data. In the following parts, this paper will focus on solving such problems.

4.1

User Profile and Document Profile

In the social tagging systems, we can get the user profile and document profile by utilizing and analyzing
the relationships among the users, tags and documents
modeled in folksonomy.
First of all, we discuss the user profiling. For a given
user, if we want to study his interests, only the tags,
associated with documents, need to be concentrated
on. In the folksonomy data model, we can use a user
vector assgined with a unique id, for example, the user
Ui ∈ U, i= 1,..., M.

Figure 2: Matrix UT and Matrix DT in folksonomy

Figure 1: Relationship of users, tags, resources in folksonomy
We can construct the folksonomy data model from
the tagging data by such following steps: collecting
the data of users, tags and resources from the explicit information and implicit information. And then
represent them in the three-dimensional vector space.
Based on this we can define the documents’ data as
the X coordinate, the users’ data as the Y coordinate,
and tags’ data as the Z coordinate. The relationship
in folksonomy is Rtagging = U × T × D, Rtagging ∈ A,
where U = {U1 , U2 , ..., Um } is the set of users and T =
{T1 , T2 , ..., Tk } is the set of tags,D = {D1 , D2 , ..., Dn }
is the set of documents. Shown in Fig1, for each point
in the three-dimensional vector space, it can be defined
as user u ∈ U has tagged document d ∈ D with tag

As shown in Fig2, a two-dimensional matrix U Ti
is extracted from the relationship between the documents and tags for a particular user. In U Ti ,
each column is corresponding to the documents Dn ∈
D, n= 1,..., N that used by user Ui , and each row is
corresponding to the tags Tk ∈ T, k = 1, ..., K.


u11 , u12 , · · · , u1n
 u21 , u22 , · · · , u2n 
U Ti = 
, ukn ∈ {0, 1}
..
.. 
..
 ...
.
.
. 
uk1 , uk2 , · · · , ukn
Here ukn means that if there exists an association
between tag Tk and document Dn , annotated by user
Ui , the ukn sets to 1, otherwise it is 0.
By accumulating the row of matrix U Ti , the freN
P
quency of tag is defined as tik =
ukn which ren=1

veals the user’s
can obtain the
their frequency
in the form of

preference and interest. Then we
full set of the pairs of tags and
weights. So the profile of userUi
tag set can be defined as U Pi =

{(T1 , ti1 ), (T2 , ti2 ) · · · (Tk , tik )}, k = 1, · · · , K, where
N
P
ukn , Tk ∈ T, k = 1, · · · , K.
tik =
n=1

Similarly, given a document Dn , we can obtain another two-dimensional matrix DTi , where each column
denotes the user Ui and row is the related tags that
the userUi is used to annotate the documentDn . The
size of matrix DTi is M users by K tags.


v11 , v12 , · · · , v1m
 v21 , v22 , · · · , v2m 
DTi = 
, vkm ∈ {0, 1}
..
.. 
..
 ...
.
.
. 
vk1 , vk2 , · · · , vkm
The elementvkm in DTi is corresponding to the user
Um and tag Tk . The value of vkm is defined that, if
there exists an annotation between tag Tk and user
Um , that means Tk is associated with the Um , the vkm
sets to 1, otherwise it is 0.
By accumulating the row of matrix DTi , the freM
P
quency of tag is defined astik =
ukm Then we
m=1

can obtain the full set of the pairs of tags and their
frequency weights. So the profile of document Dn
in the form of tag set can be defined as DPi =
{(T1 , ti1 ), (T2 , ti2 ) · · · (Tm , tim )}, m = 1, · · · , M , where
M
P
tik =
ukm , Tk ∈ T, k = 1, · · · , K.
m=1

From the steps mentioned above, the user profiles
and document profiles are defined as a single user or
document respectively rather than a group of users or
documents. However, in social tagging systems, the
group profiles of users or documents are more likely to
reflect the common preference or relatedness of likeminded users or documents with similar functionality.
In the following section, we will discuss the group profiling approach by using clustering.

4.2

Similarity Matrixes for the Users
and Documents

The relationship among all of the users is to calculate
the similarity. The similarity is quantity that reflects
the strength of relationship between two objects. In
the last part, each user profile can be represented by
the pair of tags and frequencies. We utilized the cosine
distance between users. Its value ranges from 0 to 1,
the higher value of the similarity, the more similar the
objects are. The similarity matrix SM (Ui , Uj )is given
by,
i ·U P
SM (Ui , Uj ) = |UUPiP|×|U
Pj |
The users’ relationship can be represented in the
form of bipartite graph model.
Given a graph
G = (U,E), where U is a set of users as Ui =
{U1 , U2 , · · · , Um }, and E is a set of edges which entry SM (Ui , Uj ) reflects the similarity between users,
the similarity matrix SM (ui , uj ).
Similarly with the document profiles, we can define
a graph with N users G = (D,E), where D is a set
of documents as Di = {D1 , D2 , · · · , Dn }, and E is
a set of edges which entry SM (Di , Dj ) reflects the
similarity between documents. The similarity matrix
SM (Di , Dj )is given by:
SM (Di , Dj ) =

DPi ·DPj
|DPi |×|DPj | .

4.3

Group Profiling via Clustering
Algorithm

To accomplish the group profiling, one of the approaches is to group the user profile and document
profile into several groups based on the similarity so
that the objects in the same groups can share tag set.
The clusters of users or documents reveal the common
user preference or relatedness of documents. It can
benefit the user in the same group to share the similar
interests or documents.
Clustering algorithm aims to assign a set of observations into subsets so that observations in the same
cluster are similar in some sense. It is specially designed for transactional data and can efficiently partition historic user transactions into clusters [8; 14;
12]. Each cluster is a set of transactions representing
the interests of a particular user group. So it can find
the potential groups from the user profile and document profile.
The object of clustering is: similar objects have high
similarity, the similarity is low between objects of different clusters. Clustering is the process to adjust the
ranks of the similarity matrix, by a number of matrix blocks to meet the similarity value larger among
the inside elements, while the similarity value is small
between the clusters.
There are a lot of clustering algorithms such as kmeans, fuzzy c-means, single linkage and so on. In
clustering analysis, almost all approaches are based on
the similarity between subjects to partition the data
points. Various clustering approaches have different
advantages and drawbacks. Among the traditional
clustering algorithms, spectral clustering has the superior capability of effectively group data by leveraging
the statistical property of similarity matrix of data.
In this paper, we will introduce the Spectral Clustering Algorithm. Spectral clustering refers to a class
of techniques which rely on the eigenvalues of the adjacency similarity matrix; it can partition all of the
points into disjoint clusters, the points that have high
similarity will be classified under the same cluster.
One cluster’s points have low similarity with other
clusters’ points. The spectral clustering is based on
the graph partition. We have explained how to get
the similarity matrix from the graph in 4.2. It maps
the original inherent relationships onto a new spectral
space, on which the user or document profile is projected. After the projection, the whole user or document profiles are simultaneously partitioned into disjoint clusters with minimum cut optimization.
Compared to those algorithms, spectral clustering
has many fundamental advantages. Results obtained
by spectral clustering often outperform the traditional
approaches, it is easy to implement and can be solved
efficiently by standard linear algebra methods. Spectral clustering techniques make use of the spectrum of
the similarity matrix of the data to perform dimensionality reduction for clustering in fewer dimensions.
The original formula for the spectral clustering is:
L = I − D−1/2 SD−1/2
According to the spectral graph theory in [4], the
k singular vectors of the reformed matrix RMU ser =
D−1/2 SMU ser D−1/2 present a best approximation to
the projection of user-tag vectors on the new spectral
space.

And the RMDocument = D−1/2 SMDocument D−1/2
presents the document-tag vector on the new spectral
space.
The Du and Dd is the diagonal matrix of user similarity matrix and documents similarity matrix, which are defined as:
N
P
SM (Ui , Uj ) , i = 1, · · · , M
Du (i, i) =
Dd (i, i) =

j=1
N
P

SM (Di , Dj ) , i = 1, · · · , N

j=1

Let’s take the documents set for example.
In this case we assume that the first K singular
eigenvectors represent the best approximation of original profile space. Let Ls the m × k matrix of the k
singular vectors of RMDocument . As our aim is to conduct a clustering on the document profile attributes,
we create a new m×k matrix RV to reflect the projection of the row and column vectors on the new spectral
−1/2
space in lower dimension as: RV = [Dd Ls ].
The clustering results in the group profiling. The
full steps of group profiling via clustering algorithm is
summarized in the below Algorithm.
Input:
The N document profile collection DP
=
{DPi |i = 1, 2 · · · N }, DPj
=
{(T1 , t1 ), (T2 , t2 ) · · · (TK , tK )}.
Output:
A set of k clusters DGP
=
{DGPi |i = 1, 2 · · · k }such that the cut of k partitioning of the bipartite graph is minimized.
1. Construct
the
usage
similarity
matrix
SMDocument from the document profile, whose
element is determined by the distribution of tags
of all users;
2. Calculate the diagonal matrixes Dd ;
3. Form a new matrix
D−1/2 SMDocument D−1/2 ;

RMDocument

=

4. Perform SVD (Singular value decomposition) operation on RMDocument , and obtain k singular
vectors Ls create a new projection matrix RV ;
5. Execute a clustering algorithm on RV
and return clusters of documents:DGP =
{DGPi |i = 1, 2 · · · k }.
From the above steps, the N documents are divided
into t clusters, the document group profile for each
cluster is: centerlineDGPi = {U Pi1 , U Pi2 , · · · U Pit } =
{(T1 , wi1 ), (T2 , wi2 ) · · · (Ti , wit )}
Where Ts ∈ T, s = 1, · · · , t and (wi1 , wi2 , · · · wit ) is
the centroid of the document cluster DGPi .
Meanwhile, the selection of cluster number k is another concern in the context of clustering, which is
commonly encountered. The selection of k value has a
straight impact on the performance of clustering: the
bigger number of k results in the over-separation of
users and documents, while the smaller number of it
prevents the data from being sufficiently partitioned.
Thus it is necessary before performing the clustering
to select an appropriate value of k to achieve a better
clustering performance. In the experimental part, we
will investigate the study of k selection.
In similar way,
the user group profile
of k users can be generated in the same
way:
U GPi
=
{DPi1 , DPi2 , · · · DPij }
=
{(T1 , xi1 ), (T2 , xi2 ) · · · (TK , xiK )}

Where Ts ∈ T, s = 1, · · · , t and (xi1 , xi2 , · · · xit ) is
the centroid of the user cluster U GPi .

5

Experimental evaluations

In order to evaluate the proposed group profiling, we
performed experiments on the “MovieLens” dataset.
Our experiments focus on the cluster number selection;
demonstration of the group profiles; and the computational cost reduction.

5.1

Dataset and Modularity Metric

As for experiment dataset, we utilize the part of the
“MovieLens” data, which contains tags provided by
users on movies. It includes 521 users, 1399 documents
and 1956 tags. The differences of results are shown
when choosing different numbers of clusters in order to
get the optimal number of clusters. The data is based
on the average result of executing the same experiment
ten times over the same dataset.
The modularity metric is one of the standard quantitative measures for the evaluation of “goodness” of
the clusters. The modularity of a particular division
of a network is calculated based on the differences between the actual number of edges within a community in the division and the expected number of such
edges if they were placed randomly. Good divisions,
which have high values of the modularity, are those
dense connections between the nodes within modules
but sparse connections between different modules. It
will help to evaluate the quality of the cluster; i.e. the
similarity of each cluster.
After clustering, we can get several clusters. Consider a particular division of a network into k communities. We can define a k × k symmetric matrix
SM whose element smij is the fraction of all edges
in the network that link vertices in community p to
vertices in community q. The similarity of smCpq between the two clusters
C and Cq ,is defined as,[13]
P
Pp
cpq

smCpq =

cp ∈Cp cp ∈Cq

P

P

cpq

, p, q = 1, 2 · · · m

cp ∈C cq ∈C

where cpq is the element in the similarity matrix
SM. When p=q, the smCpq is the similarity between
the elements inside the clusters, while p 6= q, the
smCpq is the similarity between the cluster Cp and
the cluster Cq . P
So the condition of a high
Pquality cluster is arg max( smCpp ) and arg min( smcpq ), p 6=
p

p,q

q, p, q = 1, 2, · · · m.
Summing over all pairs of vertices in the same group,
the modularity, denoted Q, is given by:
m
m


P
P
[smcpp − (
smcpq )2 ] = T rSM − 
SM 2 
Q=
p=1

q=1

where the m is the amount of clusters. The trace of
m
P
this matrix T rSM =
smCpp gives the fraction of
p=1

edges in the network that connect vertices in the same
community Clearly a good division into communities
should have a high value of this trace. If we place all
vertices in a single community, the value of would get
the maximal value of 1 because there’s no information
about community structure at all.
This quantity measures the fraction of the edges in
the network that connects vertices of the same type
minus the expected value of the same quantity in a

network with the same community divisions. Utilizing
the value Q to evaluate the clusters [13] is a common method: the values approaching Q=1, which is
the maximum value, indicate the networks with strong
community structure. In practice, the values of such
networks typically range from 0 to 1. The higher value
of Q, the better quality for the cluster corresponding
to a predefined cluster number k. So examining the Q
value allows us get the optimal number of clusters.

5.2

Experimental Results

Optimal Cluster Number Selection
Here we compare the result of Q values by using Spectral Clustering, Single Linkage Clustering and Random
Clustering.
Of the entire 521 user profiles constructed, we employ various clustering algorithms to build up the
group user profiles. We generate the cluster from 2
to 260 and utilize the modularity method to evaluate
the results. We close the number until 260 because it is
half of the total amount. When the number of cluster
is higher than 260, the average number of members in
each cluster is lower than 2, which will not provide reasonable clustering information. The results are shown
in Fig3. that the value of Q for Spectral Clustering
Algorithm is consistently higher than the other two
algorithms. When the number is 25 the Q gets the
maximal as 0.381. With the growth of the number of
clusters, the value of Q is gradually decreasing to 0.19.
It is concluded that, for this dataset, 25 clusters is the
best choice.

Figure 3: Comparison of the three algorithms on 521
users
Of the entire 1399 document profile models constructed, we generate the cluster from 5 to 700 and
utilize the modularity method to evaluate the results.
Similarly as the user profile, we close the number until
700. As shown in Fig4, when the number is 20 the
Q gets to the maximum at 0.277. With the growth
of number of clusters, the value of Q is decreasing to
0.026. We then found that, for this dataset, 20 clusters
is the best choice.
Demonstration of the Group Profiles
It is shown that the 20 clusters is the optimal number
for the 1399 documents, we take 2 clusters of them for
analysis. One of the cluster contents 51 documents,
with 239 tags. The main tag in it is about the “classic”, “based on a book”, “black and white”, “National
Film Registry”, “breakthroughs”, “Disney” and so on.

Figure 4: Comparison of the three algorithms on 1399
documents
The movies in such cluster seem related to the movies
about life.
And another cluster has 79 documents with 397
tags, the dominant tags are “action”, “organized
crime”, “guns”, “hysterical”, “USA film registry”, “afternoon section”, “Oscar (Best Actor)”, “Oscar (Best
Cinematography)”, “Oscar (Best Director)”, “Oscar
(Best Picture)” and so on. Such movies tend to the
Oscar movies with some breathtaking content.
Comparison between the Time Consuming
When the user or document profiles are used in tagging
systems for further applications, similarity calculation
is a major operation involved. An advantage of group
profiling is the possibility of reducing the computational complexity. For example, cosine similarity is often executed to determine the ranking of candidates.
The traditional way is to calculate the similarity between its tags and each document’s tags. In such way
the time complexity is the O(n). It will cost much
time when the system has large dataset.
After clustering for all of the documents, each cluster will have its own centroid as the representation of
the group profile, which means the “center point” in
the cluster. Centroid can be generated by the average
frequency of tags inside the cluster. The similarity between the centroid and the items inside the same cluster should be the highest; the similarity between the
centroid and the items inside the other clusters should
be the lowest. So the centroid is the representative
of the cluster. If N documents have clustered into
m communities, the process of similarity calculation
is divided into two steps: firstly, calculate the similarity between the target tag vector and the centroids
of all clusters to determine the cluster with highest
similarity score; secondly, calculate the similarity of
the target tag vector with the document profiles inside the cluster to rank the whole documents. Since
the number of centroids, m, is equal to the number of
communities, which is highly lower than the number of
documents, N, the time consuming of calculating simiN
larity is dramatically reduced from O(N ) to O(m+ m
).
In our experiment, the time consuming computing
the similarity for all 1399 documents respectively is
152.83 seconds, however, it just needs 0.045 seconds
by our proposed approach to get the final ranking of
documents.

6

Conclusion and future work

In this paper, we discuss an algorithm for user group
profile and document group profile in social tagging
systems. Utilizing the clustering algorithm, group profiling can be processed by the user profile and document profiles. We implement experiments on real tagging dataset to validate the proposed approach, investigate the modularity method to compare the optimal
number of clusters, and demonstrate the content of
clusters. At last, we compare the time consuming for
the similarity calculation involved in real applications.
It is shown that the group profiling can be dealt with
the tasks outlined in the paper effectively.
For the future work, we intend to conduct research
on the optimization for the algorithm, and explore
the deployment of group profiling in tag-based recommender system. We will investigate clustering for tags
which we believe should help in tag recommendation
and representing user interests.

References
[1] F. Abel, N. Henze, D. Krause, and M. Kriesell. On
the effect of group structures on ranking strategies
in folksonomies. Weaving Services and People on
the World Wide Web, pages 275–300, 2009.
[2] K. R. Bayyapu and P. Dolog. Tag and Neighbour
Based Recommender System for Medical Events.
In Proceedings of MEDEX 2010: The First International Workshop on Web Science and Information Exchange in the Medical Web colocated with
WWW 2010 conference, 2010.
[3] J. Davies, D. Fensel, C. Bussler, and R. Studer.
The Semantic Web: Research and Applications.
In Proceedings of the First European Semantic
Web Symposium, 2004.
[4] I. S. Dhillon. Co-clustering documents and words
using bipartite spectral graph partitioning. In
Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and
data mining, pages 269–274. ACM, 2001.
[5] J. Diederich and T. Iofciu. Finding communities of practice from user profiles based on
folksonomies. In Proceedings of the 1st International Workshop on Building Technology Enhanced Learning solutions for Communities of
Practice. Citeseer, 2006.
[6] F. Durao and P. Dolog. A personalized tag-based
recommendation in social web systems. Adaptation and Personalization for Web 2.0, page 40,
2009.
[7] F. Durao and P. Dolog. Social and Behavioral
Aspects of a Tag-Based Recommender System.
In Ninth International Conference on Intelligent
Systems Design and Applications, 2009. ISDA’09,
pages 294–299, 2009.
[8] F. Durao and P. Dolog. Extending a hybrid
tag-based recommender system with personalization. In Proceedings of the 2010 ACM Symposium
on Applied Computing, pages 1723–1727. ACM,
2010.

[9] S. Guha, R. Rastogi, and K. Shim. Rock: A
robust clustering algorithm for categorical attributes* 1. Information Systems, 25(5):345–366,
2000.
[10] A. Hotho, R. Jäschke, C. Schmitz, and
G. Stumme. Folkrank : A ranking algorithm for
folksonomies. In K.-D. Althoff and M. Schaaf,
editors, LWA, volume 1/2006 of Hildesheimer
Informatik-Berichte, pages 111–114. University of
Hildesheim, Institute of Computer Science, 2006.
[11] Q. Mei and K. Church. Entropy of search logs:
how hard is search? with personalization? with
backoff?
In Proceedings of the international
conference on Web search and web data mining,
pages 45–54. ACM, 2008.
[12] A. Nanopoulos,
H. H. Gabriel,
and
M. Spiliopoulou. Spectral Clustering in SocialTagging Systems. Web Information Systems
Engineering-WISE 2009, pages 87–100, 2009.
[13] M. E. J. Newman and M. Girvan. Finding
and evaluating community structure in networks.
Physical review E, 69(2):26113, 2004.
[14] M. G. Noll and C. Meinel. Web search personalization via social bookmarking and tagging. In
Proceedings of the 6th international semantic web
conference and 2nd Asian conference on Asian
semantic web, pages 367–380. Springer-Verlag,
2007.
[15] S. Rendle, L. B. Marinho, A. Nanopoulos, and
L. Schmidt-Thieme. Learning optimal ranking
with tensor factorization for tag recommendation.
In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and
data mining, pages 727–736. ACM, 2009.
[16] J. Stoyanovich, S. Amer-Yahia, C. Marlow, and
C. Yu. Leveraging tagging to model user interests
in del. icio. us. AAAI SIP, 2008.
[17] P.
Symeonidis,
A.
Nanopoulos,
and
Y. Manolopoulos.
A unified framework for
providing recommendations in social tagging
systems based on ternary semantic analysis.
IEEE Transactions on Knowledge and Data
Engineering, 2009.
[18] J. Teevan, M. R. Morris, and S. Bush. Discovering and using groups to improve personalized
search. In Proceedings of the Second ACM International Conference on Web Search and Data
Mining, pages 15–24. ACM, 2009.
[19] Y. Yang and N. Marques. User group profile modeling based on user transactional data for personalized systems. Progress in Artificial Intelligence,
pages 337–347, 2005.
[20] C. M. A. Yeung, N. Gibbins, and N. Shadbolt.
A study of user profile generation from folksonomies. In Social Web and Knowledge Management, Social Web 2008 Workshop at WWW2008.
Citeseer, 2008.

J. Software Engineering & Applications, 2010, 3, 1080-1087
doi:10.4236/jsea.2010.311127 Published Online November 2010 (http://www.SciRP.org/journal/jsea)

Research and Analysis of Structural Hole and
Matching Coefficient
Penghua Cai, Hai Zhao, Hong Liu, Rong Pan, Zheng Liu, Hui Li
Department of Information Science and Engineering, Northeast University, Shenyang, China.
Email: caipenghua2008@yahoo.cn
Received August 28th, 2010; revised September 18th, accepted September 23rd, 2010.

ABSTRACT
Measure is a map from the reality or experimental world to the mathematical world, through which people can more
easily understand the properties of entities and the relationship between them. But the traditional software measurement methods have been unable to effectively measure this large-scale software. Therefore, trustworthy measurement
gives an accurate measurement to these emerging features, providing valuable perspectives and different research dimensions to understand software systems. The paper introduces the complex network theory to software measurement
methods and proposes a statistical measurement methodology. First we study the basic parameters of the complex network, and then introduce two new measurement parameters: structural holes, matching coefficient.
Keywords: Large-Scale Software, Trustworthy Measurement, Structural Holes, Matching Coefficient

1. Introduction
Now large software network is increasingly showing
“small world” and “scale-free”-characteristics of complex networks. The results of studying complex networks
provide strong support for people to explore characteristics of the overall structure of large-scale software network [1,2]. Using a network view research the software
network, this has been recognized by more and more
researchers. The traditional measurement methods focus
on the micro-level statistics and only do some aspects of
the software evaluation because of lacking parameters.
Therefore, the paper imports complex network theory
into the traditional measurement methods and introduces
some new metrics to measure the different characteristics
of the software from the different levels. This paper also
puts forward a measurement methodology, which make
the basic intrinsic property and the overall measures
properties of software as the core and use multiple measurement parameters (the basic parameters in complex
network, the newly introduced metric) to measure some
important characteristics and structural features, providing an important basis for measuring software quality.

structure of competition [3]. It is form social network
research. In brief, structural holes are the relationship
between the two non-duplicate persons. In Figure 1, we
use software network formed by four nodes A, B, C, D to
illustrate structural hole. In the left picture A has three
structural holes (BC, BD and CD); because the three
nodes B, C, D have no direct connection and only node
A is associated with these three classes. Compared with
other three nodes, node A has competitive advantage. It
is in the center, so most likely close to all the nodes in
the software network. The right picture is actually a
closed network, so there is no structural hole.
Figure 1 shows two extreme cases of structural hole in
the small-scale software network: the whole-hole structure network and no-hole structure network. In the actual
software, it has three types of structure as following:

2. Structural Hole
1) The theory of structural hole
The concept of structural holes is from the social

Copyright © 2010 SciRes.

Figure 1. Examples of structural holes.

JSEA

1081

Research and Analysis of Structural Hole and Matching Coefficient

Any node in the software network has direct contact
with other nodes. From the whole network view, it is
“no-hole” structure. This structure only exists in smallscale software network and such groups are actually
closed, so the importance of each node in the networks is
basically equal. There are many nodes needed to be updated. It is difficult to control them and update software.
In addition, the cost of maintaining this high redundancy
network is high.
Only the central node has direct link with every other
node in the network. The other nodes do not connect
with every node directly. From the whole view of the
network, the phenomenon of no direct contact or relationship breaking off is structural holes. There are no
direct connections among the rest nodes, which is
whole-hole structure.
2) The algorithm of structural holes
In the aspects of structural holes measurement, structural constraint algorithm and betweenness centrality
algorithm have been used. Structural constraint algorithm
uses closeness among nodes as measure targets, dependence among nodes as the evaluation criteria. It can determine the degree of software network structural holes.
At the same time if nodes across more structural holes,
they have less redundant connections, can access more
non-redundant information and are used more frequently.
Betweenness centrality algorithm largely determines the
centering level of the nodes. Therefore, the paper uses
structural constraint algorithm to compute structural
holes.
Definition 2.1 Network Constraint index: This index
describes direct or indirect closeness between a node and
other nodes. If the network Constraint index is higher,
the network is closer and the structural holes are fewer.
The concrete calculating steps is as follows [4]:
pij 

dij  d ji

(1)

 (dik  d ki )
k

pij is the ratio of the shortest path length between
node i and node j to the sum of the shortest path length
about all the neighboring nodes of node i. dij is the
shortest path length between node i and node j.
cij  ( pij 



k ,k i,k  j

pik pkj )

2

(2)

cij is the binding level between node i and node j.
When node j is the only adjacent node of node i, cij
gets maximal value 1.When node j is indirectly connected with node i through other nodes, cij gets minimum value pij 2 .Node k is the adjacent node of node i.
By formula (2) and formula (3) we can calculate network constraint index of node i.
Copyright © 2010 SciRes.

Ci   cij

(3)

j

Structural holes are used to describe a node in dependence on other nodes. Few structural holes show
strong dependence on other nodes. Network constraint
index is the quantization of structural holes. By calculating the network constraint index of structural holes, we
can understand the degree of structural holes in the software network.

3. Matching Coefficient
In 2002, Newman put forward another important statistical parameter used to mark the network, which is assortativity. Assortativity is represented by r. It is changing between –1 and 1 that means nodes are prior to establish side connection with similar nodes in the network
[5,6]. When r is greater than zero, nodes are prior to connect with similar nodes. Such network is called assortative mixing. When r is less than zero, nodes are prior to
connect with dissimilar nodes. Such network is called
disassortative mixing.
Definition 3.1 assortative coefficient: Incidence relation between nodes in the network can be described by
assortative coefficient [7,8]:
1
E 1  ji ki  [ E 1  ( ji  ki )]2
i
i 2
r
1 2
1
1
2
E  ( ji  ki )  [ E 1  ( ji  ki )]2
2
i
i 2

(4)

ji and ki are the degree of the i side’s two vertices.
E is the number of sides in the network.
If assortative coefficient is greater than 0, the network
is assortative mixing; if assortative coefficient is less
than 0, the network is disassortative mixing; if assortative coefficient is equal to 0, the network is randomized.
Assortative coefficient reflects the connectivity of network nodes. In the assortative mixing network, nodes of
a high degree tend to connect with nodes of a high degree. In the disassortative mixing network, nodes of a
high degree tend to connect with nodes of a low degree.
In Figure 2, it is a network composed by 10 nodes. In
Figure 2(a), r  0.372881 . Node 1’s degree is 5,
which is a high degree node and connect with nodes (degree is 2 or 1). Such network is disassortative mixing. In
Figure 2(b), r  1 . Degrees of all nodes are similar, that
is assortative mixing.

4. The Law and Analysis of Metrics in the
Network Software
4.1. Correlation Analysis of Degree and
Structural Holes
Degree is used to describe the connected complexity of a
JSEA

1082

Research and Analysis of Structural Hole and Matching Coefficient
Table 1. The statistical characteristics of 4 kinds of software
network.

(a)

(b)

Figure 2. Examples of assortative mixing and disassortative
mixing.

(a) Quartz

(c) Mozilla

Software

number of

isolated

number of

system

nodes

nodes

edges

average
degree

Quartz

255

63

231

1.81176

Abiword

1712

203

2484

2.84211

Mozilla

8354

1159

13581

3.32248

Eclipse

14730

1721

27560

3.74202

node and its neighboring nodes. The larger value of a
node degree, the more important it shows, but not for
chain network. Structural holes are used to show the importance of a node from another point.
First we analyze the network structure of four object-oriented networks (Quartz, Abiword, Mozilla and
Eclipse), as shown in Table1. As can be seen from Table
1, the scales of them vary widely. Compared with total
nodes, isolated nodes were few. So the four software

(b) Abiword

(d) Eclipse

Figure 3. Diagram of the distribution of network constraint index and degree.

Copyright © 2010 SciRes.

JSEA

Research and Analysis of Structural Hole and Matching Coefficient

networks have representativeness in all software samples.
The paper analyzes interdependency of degree and
structural holes about these four software network. As
the structural holes are quantified through network constraint index, so interdependency of degree and structural
holes is also interdependency of degree and network
constraint index. In Figure 3, horizontal ordinate is the
value of every node’s degree, vertical coordinates is the
value of network constraint index. In all software network, the greater the value of nodes degree, the smaller
network constraint index, the more structural holes, the
weaker dependency on the around nodes. A special case
is that a node’s degree is 0 and its network constraint
index is 1, then the node does not have structural holes. It
is isolated node. In the software network it will not be
called by other operations.
Since isolated nodes do not affect the software feature,
after removing isolated nodes we make curve fitting to
the relationship of degree and network constraint index.
In Figure 4, horizontal ordinate is the value of node’s

1083

degree, vertical coordinates is the value of network constraint index.
Relationship distribution curve of structural holes and
network constraint index is power curve, which shows an
important feature of software system modularization.
Fitting curve is the mathematical expression of this feature. For example, Software Network Quartz’s fitted
power function equation is as follows:
Y  1.003  X 0.918

(5)

X is the nodes’ degree value (abscissa). These four
software network’ parameter estimates are shown in Table 2. In software network, the greater the value of nodes
degree, the smaller network constraint index, the more
structural holes.
Whether a regression model is good or not, the most
commonly used index is the coefficient of determination
[9,10]. The index is based on the decomposition of the
dispersion quadratic sum. Coefficient of determination is
a comprehensive measure for regression model’s goodness of fit [11,12].

(a) Quartz

(b) Abiword

(c) Mozilla

(d) Eclipse

Figure 4. The fitness graph of relationship between network constraint index and degree.
Copyright © 2010 SciRes.

JSEA

Research and Analysis of Structural Hole and Matching Coefficient

1084

Table 2. Model summary and parameter estimation.
model summary

parameter estimate

Software
system

R

F

Sig.

constant

b1

Quartz

.955

3995.527

.000

1.003

–918

Abiword

.943

24989.658

.000

1.016

–892

Mozilla

.917

79097.785

.000

.996

–882

Eclipse

.960

313520.018

.000

1.004

–934

Formula of correlation coefficient:
r

N  XY   X  Y
N  X  ( X )2 N  Y 2  ( Y )2
2

(6)

Formula of determination coefficient:
R  r2

(7)

F test is mainly for variance analysis. Sig is result of F
test. If Sig is less-than 0.05, which declare that difference
is significant.
From Table 2 the coefficient of determination R =
0.958, Sig < 0.05. Therefore we can conclude goodness
of fit is very high and fitting power function can fully
reflect a power curve relationship between network constraint index and node degree. So fitting results is acceptable.
Through the four software networks we can see that
the structural holes obey specified rule. Enlarge sample,
and then test 200 software networks. The results are
shown in Figure 5. Abscissa is the software serial number. In Figure 5 vertical coordinates is the goodness of
fit; in Figure 5(b) vertical coordinates is the relation
fitting power function curve parameter estimates of network constraint index and degree.
In Figure 5(a), goodness of fit of the network constraint index and the degree is between 0.80 and 0.98.
This shows that relationship of the network constraint
index and the degree apparently obeys power function
distribution. Of course, 25 software networks’ goodness
of fit is between 0.50 and 0.80, which indicate the network constraint index and the degree are moderate correlation. In addition, 3 software networks’ goodness of
fit is less than 0.5, which indicate the network constraint
index and the degree are low correlation. In Figure 5(b),
power function relation of network constraint index and
degree changes little.
In software network, correlation of degree and structural holes contributes to analyze collaborative relationships between different types of software entities. It is
useful to discover software entities’ problems. Complex
class or module are tend to be composed by relatively
simple class or module. This is the software constructivity principle. On the other hand, correlation between the
network constraint index and the degree of structural
Copyright © 2010 SciRes.

(a)

(b)

Figure 5. Diagram of the distribution coefficient of etermination and parameter estimation.

holes is helpful to the analysis of system hierarchy and
modularity. Class or module with large degree are tend
to gather with class or module with small degree, that
shows a high cohesion

4.2. Law of Matching Coefficient
The paper makes a further analysis on the 200 samples
and calculates the matching coefficient for each software
network. The results are shown in Figure 6. In the 200
software networks, 80% of them are disassortative mixing; 20% of them are assortative mixing.
First of all, we analyze the disassortative mixing software network, because they occupy majority of the software samples. Software network currently in use most
are disassortative mixing. From Table 3 we can conclude
that disassortative mixing software networks have no
JSEA

Research and Analysis of Structural Hole and Matching Coefficient

Figure 6. Diagram of the distribution of the mixing coefficient.

concern with the total number of nodes. The average
degree, the average structural holes of the disassortative
mixing software network don’t have obvious law. Some
software networks are well known and have higher
evaluation. Their coefficient of determination of structural holes and degree are greater than 0.8.
Some assortative mixing software networks are shown
in Table 4. As can be seen from Table 4, assortative
mixing software network is different from disassortative
mixing software network; moreover network constraint
index and degree goodness of fit is relatively low. The
number of assortative mixing software network’ nodes
are generally small, and matching coefficient has nothing
to do with the average degree and structural holes.
By comparison, it is discovered that in the 200 software
networks, when the total number of nodes is more than
1,000, they are disassortative mixing software networks.
In these software networks, the nodes with lower degree
are considered as a relatively simple module in the software network. In disassortative mixing software network,
nodes with high degree tend to connect with nodes with
low degree. The nodes with lower degree are conducive
to the decomposition of software tasks, while the nodes
with higher degree are key points for software modules
completing the complex task.
Compared Table 3 with Table 4, it can be concluded
that matching coefficient was correlated with the number
of nodes. The relationship between matching coefficient
and software size is shown in Figure 7. The abscissa is
the total number of each software network’s nodes, the
vertical coordinates is the matching coefficient. In Figure 7(a), the number of each software network’s nodes is
less than 1000. The majority of software network’s
matching coefficients are below 0. In Figure 7(b), the
number of each software network’s nodes is greater than
1000. When the number of nodes is greater than 1000,
the software network is disassortative mixing.
Copyright © 2010 SciRes.

1085

(a) The number is less than 1000.

(b) The number is more than 1000.

Figure 7. Relationship between the mixing coefficient and
the number of nodes.

As the pressure of design and implementation, it is
necessary to keep each module simple and effective.
When a node has a high degree, it also has the features of
complexity and high multiplexing. In the assortative
mixing software network, if a node has a high degree or
connects with high degree nodes that will cause system
problems. System maintainability and modifiability fall
down. It is need to reconstruct for such modules

5. Conclusion
The paper uses structural holes and the matching coefficient to measure software. Software structural holes
measure the software network from the software dependent features. Correlation between network constraint
index and the degree obey power law distribution that
reflects an important software feature. As the study of
the software network is still in the exploration stage, so
the study of measurement methods of software network
JSEA

Research and Analysis of Structural Hole and Matching Coefficient

1086

Table 3. Aisassortative mixing.
Software name
kdegraphics-3.5.3
mysql_6.0.6
jEditR1.35
kdebase-3.5.3
kdevelop-3.4.0
qhacc-3.4
rpm-4.4.1
nss-3.9.2
freemind0.9.0
mysql_5.1.26
sim-0.9.4
kicad-20060626
kopete-0.12.1
qtiplot-0.8.2
kdeedu-3.5.4
mysql_5.0.67
mysql-5.0.56
ArgoUML-0.26.2
koffice-1.5.0
glib-2.16.5

matching coefficient
–0.121387
–0.12225
–0.125693
–0.126603
–0.128954
–0.130487
–0.131246
–0.133362
–0.13402
–0.136164
–0.138038
–0.139725
–0.140958
–0.141222
–0.141509
–0.142316
–0.142425
–0.143707
–0.143862
–0.144575

nodes
2014
3793
822
1677
1453
148
1260
910
713
3194
786
212
1512
166
1010
3133
3132
2031
4580
474

goodness of fit
0.931
0.836
0.868
0.879
0.917
0.932
0.815
0.849
0.877
0.843
0.93
0.854
0.891
0.958
0.891
0.921
0.92
0.828
0.915
0.816

average degree
3.32572
2.83048
1.74696
2.12165
1.97385
3.22973
2.05397
3.17363
2.61711
2.57044
2.44275
2.83019
2.65741
1.83133
2.04158
2.45388
2.45019
2.18316
2.57293
1.64979

average structural holes
0.600915
0.709367
0.815401
0.745993
0.77426
0.480221
0.781209
0.667572
0.714985
0.732296
0.72691
0.694856
0.665513
0.750994
0.765379
0.734618
0.735107
0.805827
0.695893
0.828419

Table 4. Assortative mixing.
Software name
gnuplot-4.0.0IDE
exim-4.62
courier-0.52.2
freeradius-1.1.0
maildrop-2.0.2
ups-3.38
coreutils-5.2.1
nedit-5.5
kdeartwork-3.5.4
evince-0.5.4
bash-3.2
electric-7.00
bibletime-1.6
freeradius-2.0.5
strongswan-2.6.4
evms-2.5.5
jabberd-2.0s11
glibc-2.3.6
dasher-4.0.4
cyrus-2.3.12

matching coefficient
0.516592
0.36244
0.345491
0.33909
0.280709
0.263252
0.223548
0.207817
0.199741
0.180686
0.163871
0.143483
0.0948479
0.064476
0.050583
0.0442493
0.0435754
0.0400406
0.0368399
0.110243

nodes
93
114
376
170
107
246
93
130
162
232
99
412
159
218
312
352
117
961
213
279

is also in the exploration stage. This paper studies the
single property, association of the property and holistic
measure of the software. However, arising deviation is
inevitable in the process. As the constraints of time and
energy, the samples of this paper’s research are still
small samples. It is need to enlarge samples. Next we
need to further examine the effectiveness of measurement methodology in the actual development project,
Copyright © 2010 SciRes.

goodness of fit
0.509
0.337
0.528
0.529
0.447
0.43
0.466
0.73
0.668
0.677
0.524
0.657
0.896
0.685
0.722
0.815
0.73
0.668
0.84
0.643

average degree
2.02151
1.21053
2.00532
1.38824
1.79439
1.72358
1.2043
1.63077
1.2963
1.2069
1.51515
3.35437
2.03774
1.6055
1.69872
1.79545
3.65812
1.26743
2.83568
1.94265

average structural holes
0.717183
0.826957
0.724346
0.855368
0.790998
0.829059
0.80448
0.79206
0.847517
0.860548
0.797375
0.635638
0.712981
0.823288
0.814492
0.790988
0.637675
0.853633
0.664322
0.791312

develop and integrate auxiliary means to guide the actual
software development.

REFERENCES
[1]

M. Gaertler and R. J. Mondragon, “Accurately Modeling
the Internet Topology,” Physics Review E, Vol. 66, No.
18, 2006, pp. 163-167.

JSEA

Research and Analysis of Structural Hole and Matching Coefficient

1087

[2]

R. S. Burt, “Structural Holes: The Social Structure of
Competition,” Harvard University Press, 1995, PP. 35-38.

[7]

S. Furey, “Why We Should Use Function Points,” IEEE
Software, Vol. 14, No. 2, 1997, pp. 28-30.

[3]

S. Abdelwahed, N. Kandasamy and A. Gokhale, “High
Confidence Sofware for Cyber-Physical Systems,” Proceedings of the 2007 Workshop on Automating Service
Quality, Atlanta, 2007, PP. 1-3.

[8]

[4]

Y. T. Ma, J. X. Chen and J. H. Wu, “Research on the
Phenomenon of Software Drift in Software Processes,”
Proceedings of 8th International Workshop on Principles
of Software Evolution, Lisbon, September 2005, pp.
195-198.

M. Arnold and P. Pedross, “Software Size Measurement
and Productivity Rating in a Large-Scale Software Development Department,” Proceedings of 20th International Conference on Software Engineering, Kyoto, 1998,
pp. 503-506.

[9]

M. Bauer, “Analysing Software Systems by Using Combinations of Metrics,” Proceedings of ECOOP’99 Workshops, Lisbon, 1999, pp. 170-171.

[5]

[6]

M. Alshayeb and W. Li, “An Empirical Validation of
Object-Oriented Metrics in Two Different Iterative Software Processes,” IEEE Transactions on Software Engineering, Vol. 29, No. 11, November 2003, pp. 10431049.
L. C. Briand, S. Morasca and V. R. Basili, “Property-Based Software Engineering Measurement,” IEEE
Transactions on Software Engineering, Vol. 22, No. 1,
January 1996, pp. 68-86.

Copyright © 2010 SciRes.

[10] S. R. Chidamber and C. F. Kemerer, “A Metrics Suite for
Object-Oriented Design,” IEEE Transactions on Software
Engineering, Vol. 20, No. 6, June 1994, pp. 476-493.
[11] F. B. e Abreu, “The MOOD Metrics Set,” Proceedings of
ECOOP’95 Workshop on Metrics, Aarhus, 1995, pp.
150-152.
[12] N. E. Fenton and M. Neil, “Software Metrics: Successes,
Failures and New Directions,” Journal of Systems and
Software, Vol. 47, No. 2-3, July 1999, pp. 149-157.

JSEA

720

IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 13, NO. 4, AUGUST 2011

Layered Internet Video Adaptation (LIVA):
Network-Assisted Bandwidth Sharing and Transient
Loss Protection for Video Streaming
Xiaoqing Zhu, Rong Pan, Mythili S. Prabhu, Nandita Dukkipati, Vijay Subramanian, and Flavio Bonomi

Abstract—As video traffic increases in the Internet and competes for limited bandwidth resources, it is important to design
bandwidth-sharing and loss-protection schemes that account
for video characteristics, beyond the traditional paradigm of
fair-rate allocation among data flows. Ideally, such a scheme
should handle both persistent and transient congestion as video
streaming applications demand low-latency transmissions and low
packet-loss ratios. This paper presents a novel scheme, layered
Internet video adaptation (LIVA), in which network nodes feed
back virtual congestion levels to video senders to assist both
media-aware bandwidth sharing and transient-loss protection.
The video senders respond to such feedback by adapting the rates
of encoded scalable bitstreams based on their respective video
rate-distortion (R-D) characteristics. The same feedback is employed to calculate the amount of forward error correction (FEC)
protection for combating transient losses. Simulation studies show
that LIVA can minimize the total distortion of all participating
video streams and hence maximize their overall quality. At steady
state, video streams experience no queueing delays or packet
losses. In the face of transient congestion, the network-assisted
adaptive FEC promptly protects video packets from losses. Our
Linux-based demonstration showcases how LIVA can be implemented in a simple manner in real systems. We also present a
solution for LIVA streams to coexist with TCP flows based on
explicit congestion notification signaling. Finally, our theoretical
analysis guarantees system stability for an arbitrary number of
streams with round-trip delays below a prescribed limit.
Index Terms—Explicit congestion notification (ECN), forward
error correction (FEC), media-aware bandwidth sharing, scalable
video coding (SVC).

I. INTRODUCTION
ECENT years have seen a rapid growth of video traffic
over the Internet. According to [1], Internet video is now
approximately one-third of all consumer Internet traffic and will

R

Manuscript received August 02, 2010; revised December 21, 2010; accepted
January 31, 2011. Date of publication February 17, 2011; date of current version July 20, 2011. Preliminary results of this work were presented at IEEE
INFOCOM Mini-Conference, March 2010. The associate editor coordinating
the review of this manuscript and approving it for publication was Prof. James
E. Fowler.
X. Zhu, R. Pan, M. S. Prabhu, V. Subramanian, and F. Bonomi are with
the Advanced Architecture and Research Group, Cisco Systems Inc., San
Jose, CA 95134 USA (e-mail: xiaoqzhu@cisco.com; ropan@cisco.com;
mysuryan@cisco.com; vijaynsu@cisco.com; flavio@cisco.com).
N. Dukkipati was with the Advanced Architecture and Research Group, Cisco
Systems Inc., San Jose, CA 95134 USA. She is now with Google Inc., Mountain
View, CA 94043 USA (e-mail: nanditad@stanfordalumni.org).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TMM.2011.2115996

account for over 60% by the year 2013. In addition to the sheer
volume increase in traffic, video streaming applications also impose new challenges to the best-effort Internet, in that they require persistently high bandwidth and timely packet delivery to
ensure continuous media playout. Furthermore, the predictively
compressed video streams are sensitive to packet losses, as error
propagation at the decoder can severely degrade received video
quality.
Most research that addresses these challenges, e.g., TFRC [2]
and RaDiO [3], has adopted end-to-end schemes in which the
video senders infer network condition from estimated packet
delay and loss statistics [4]. Such schemes can only react to
ongoing network congestion or packet losses and suffer from
long queuing delays and persistent packet losses even at steady
state. Moreover, they usually cannot adapt agilely enough to
abrupt changes in traffic or network conditions, such as sudden
arrival of new streams in a fully utilized network.
The benefit of active network participation for streaming
video has long been recognized. Most existing designs are
based on the bandwidth reservation model [5], requiring
per-flow quality of service (QoS) for all participating streams
[6]. Unfortunately, such an approach requires network nodes
to maintain per-flow states and incurs high implementation
complexity, especially at high link speeds. On the other hand,
simpler forms of explicit network feedback has been widely
explored, for instance by means of explicit congestion notification (ECN) [7]. Most earlier work along this direction is
designed for general Internet traffic, without catering to the
specific requirements of streaming video.
In this paper, we explore a novel framework and study how
simple network participation can improve the performance of
video streaming. The design of our scheme follows a few basic
criteria. First, an ideal bandwidth sharing scheme should result
in no standing queues at network nodes, so as to avoid queueing
delays and persistent packet losses at steady state. Second, when
multiple video streams compete for shared network bandwidth,
it is desirable that their allocated rates reflect their respective
rate-distortion (R-D) characteristics, while maintaining comparable rate share for background nonvideo traffic. Furthermore,
the loss protection mechanism should provide sufficient protection against transient losses, yet avoid overhead when losses are
unlikely. Finally, the system should be stable for arbitrary network topologies, scale well with an arbitrary number of streams,
and react quickly to changes.
We present the design, analysis, and performance evaluation
of such a scheme, named layered Internet video adaptation

1520-9210/$26.00 © 2011 IEEE

ZHU et al.: LIVA: NETWORK-ASSISTED BANDWIDTH SHARING AND TRANSIENT LOSS PROTECTION FOR VIDEO STREAMING

(LIVA). Our design focuses on combating congestive loss
induced over a typical wired Internet connection. In particular,
we assume that packet losses introduced by wireless transmission errors can be remedied efficiently by physical-layer and
MAC-layer techniques such as adaptive modulation, channel
coding and persistent retransmissions. Under LIVA, network
nodes feed back locally calculated virtual congestion levels
to video senders. Such information guides the video sender in
calculating both the optimal rate allocation and the percentage
of forward error correction (FEC) protection. The streams are
pre-encoded using the scalable video coding (SVC) extension
of the H.264/AVC standard, allowing on-the-fly rate adaptation
according to dynamically allocated rates and recommended
FEC percentage.1
Our simulation evaluation shows that LIVA’s simple network
feedback allows the video streams to adjust to network congestion in a proactive manner, without incurring large queuing
delays or packet losses. The feedback also facilitates fast
convergence of the video streaming rates. LIVA’s media-aware
bandwidth sharing algorithm leads to more balanced qualities
among video streams than conventional fair-rate allocation.
LIVA’s network-assisted FEC scheme effectively protects
video packets from transient losses without compromising the
steady-state video quality. Benefits of the LIVA scheme are
also showcased in a Linux-based system demonstration. Our
theoretical analysis guarantees that the LIVA system is stable
for arbitrary number of video streams with arbitrary round
trip times below a prescribed limit. Our proposed ECN-based
implementation of LIVA further ensures the coexistence of
video streams with background TCP traffic sharing the same
bottleneck queue.
The contributions of this paper are summarized here:
• a stable media-aware bandwidth sharing scheme that
achieves fast convergence, efficient bottleneck utilization,
and balanced video qualities among competing streams;
• a proactive adaptive FEC scheme that effectively protects
transient packet losses and efficiently avoids overhead at
steady state;
• a network-assisted framework that coherently incorporates
both schemes with the aid of the same explicit congestion
level feedback from the network;
• an ECN-based solution that supports coexistence of LIVA
streams with TCP flows, while maintaining media-aware
allocation among the video streams.
In what follows, Section II reviews prior research related
to LIVA. Section III explains the scheme in detail. Section IV
presents simulation study of the proposed scheme, and
Section V describes our Linux-based system demonstration of
LIVA. In Section VI, we present an ECN-based solution for
coexistence of LIVA and TCP streams.
II. RELATED WORK
The proposed LIVA system addresses two main challenges
for Internet video streaming: how to share the bottleneck bandwidth among multiple competing streams, and how to protect
1The basic LIVA scheme may also accommodate other forms of video rate
adaptation such as bitstream switching, transcoding, or packet pruning from a
nonscalable stream. We leave such explorations for future work.

721

the streams from packet losses. In the following, we review important contributions in each of these areas.
A. Multistream Bandwidth Sharing
For bandwidth sharing among multiple streams, TCPfriendly rate control (TFRC) [2] is a popular solution for
guiding the video source rate adaptation of each stream, both
for nonscalable and scalable streams [8], [9]. These works differ
from LIVA in two key aspects: first, they rely on end-to-end estimates of packet loss ratios or round-trip delays as indications
of congestion; second, they lead to the same TCP-friendly rate
irrespective of the streams’ R-D characteristics.
The importance of R-D optimized rate allocation has been
recognized for multiplexing multiple video streams over a
common bottleneck link [10], [11]. Unlike LIVA, most such
schemes are centralized in nature and require access to video
R-D information of all participating streams at a common
entity.
Kelly et al. have presented a mathematical framework to
achieve distributed rate allocation for streams with different
utility functions [12]. The end-to-end scheme in [13] have
combined Kelly’s framework with the video R-D information
to reduce quality fluctuation. The framework in [12] is also extended to rate allocation for wireless video streaming, whereby
cross-layer information exchange is leveraged to accommodate
heterogeneity in both video rate utilities and wireless link
speeds [14].
Various schemes, such as ECN [7], XCP [15], and MaxNet
[16], have explored the benefits of explicit network feedback.
Unlike LIVA, they are designed for generic data transfers,
without catering to the specific requirements of streaming
video.
To the best of our knowledge, no prior work has yet pieced
together both aspects, as in LIVA, in the practical design of a
bandwidth-sharing scheme that employs network feedback to
achieve R-D optimized rate allocation among competing video
streams over a shared network.
B. FEC Protection for Layered Video
A vast body of literature has explored how to protect video
streams against packet losses using forward error correction
(FEC). It can be broadly classified into the following two categories. Most work studies the application of FEC in settings
where there is no feedback between the senders and receivers,
such as in multicast and broadcast video [17], [18]. Senders
multicast all the source and channel coding layers to different
multicast groups; each receiver estimates the bandwidth and
packet loss probability of its channel and subscribes to the
optimal set of source and channel layers to minimize its expected reconstruction error. In LIVA, on the other hand, the
FEC scheme is designed for a system with feedback.
The second category of work describes how to adapt the FEC
protection level in a unicast setting based on end-to-end packet
loss rates measured at the sender [19]. The losses are usually
due to transmission errors, for instance, over wireless links.
The adaptive FEC scheme in LIVA differs from these works
by leveraging the virtual congestion level feedback from the
network, and by protecting video streams against packet losses

722

IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 13, NO. 4, AUGUST 2011

(2)
where
denotes the R-D function of each video stream.
The capacity of the bottleneck is denoted as ; the total rate over
the link comprises rates of all video streams ’s and rate of
all nonvideo streams traversing that link; the target utilization
is chosen to be slightly less than unity.
We adopt the parametric model from [21] for characterizing
the video R-D tradeoff
(3)
Fig. 1. Overview of the LIVA system. Each network node periodically calculates its VCL. The sender calculates the optimal rate based on its own video
R-D parameters, as well as the maximum VCL reported by the receiver. The
VCL information is also used to adjust the FEC protection percentage, which
determines the final SVC streaming rate.

due to transient congestion rather than losses from unreliable
transmissions.
Previous work has also investigated how to optimally allocate FEC protection among different layers of the video stream
bearing different R-D importance [17], [20]. These studies are
complementary to LIVA’s approach, and can be applied to enhance the current scheme.

The parameters , , and
are fitted from empirical R-D
points of a pre-encoded video stream . They are updated periodically to track changing contents in the video stream, typically once every group of pictures (GOP).
2) Distributed Solution: The optimization problem in (1)
and (2) can be readily solved in a distributed manner, following
Kelly’s framework [12]. The solution involves calculation of a
virtual congestion level at each network node and adaptation of
the video rate at each sender based on observed maximum congestion level along the path.
for
At the network node, the virtual congestion level
Link is updated as
(4)

III. LIVA SCHEME
Fig. 1 provides an overview of the LIVA system. Each video
packet carries a header field into which a network node can insert its virtual congestion level (VCL). This field is initialized
to zero at the sender and can be modified by any network node
if its virtual congestion level is greater than the one already in
the header. By the time a packet reaches the receiver, its header
carries the maximum value of VCL along the forward path. The
receiver then echoes this information back to sender in the video
acknowledgement (ACK) packet header. The virtual congestion
level information is used in the calculation of both R-D optimized rate allocation and adaptive FEC protection against transient congestion. The SVC stream rate adaptation module combines the above information to determine the maximum allowed
SVC rate point and pads the rest of the rate budget with FEC
protection. Both SVC video packets and FEC parity packets are
then sent out at the optimal rate. Next, we describe in greater
detail the three main functionalities of LIVA.
A. Media-Aware Bandwidth Sharing
1) Optimization Objective: Conventional bandwidth sharing
schemes typically aim at allocating equal rate among competing
flows, the underlying assumption being that they bear the same
rate utility function. In LIVA, we propose to share the bandwidth
among competing video streams in a media-aware fashion, by
adapting the rate of to each video stream according to its R-D
characteristics. We propose to minimize the total distortion of
all streams, while achieving a target utilization level at the bottleneck link. This can be formulated as an optimization problem
(1)

where denotes the update scaling factor. The initial value of
.
the virtual congestion level is chosen as
At the video sender, the optimal rate is calculated based on
the maximum congestion level along its path
(5)
The optimal rate depends on the video R-D function

as
(6)

As illustrated in Fig. 2, the optimal allocation balances between
the competing needs of increasing rate to reduce video distortion and decreasing rate to avoid network congestion. A higher
virtual congestion level leads to a lower allocated rate whereas
a lower virtual congestion level encourages a higher video rate.
In addition, it can be noted from (6) that the same value of
leads to different optimal rates for video streams with different
R-D parameters.
Note that the distributed nature of the solution (4)–(6) lends
itself well for distributed implementation: the intermediate
network nodes remain oblivious of the video R-D information
while each video sender only needs the end-to-end maximum
virtual congestion level. Optimality and stability of this simple
solution has also been established for the idealized fluid network model [12], [16].
3) Practical Design: Combining Optimality, Scalability and
Simplicity: To successfully integrate the above distributed solution in the real world, it is important to also maintain system stability for streams experiencing heterogeneous round trip times,

ZHU et al.: LIVA: NETWORK-ASSISTED BANDWIDTH SHARING AND TRANSIENT LOSS PROTECTION FOR VIDEO STREAMING

723

RTTs, to avoid big rate swings for streams with long RTTs. The
steps at video senders are summarized as follows.
Video Senders:
Upon receiving an ACK packet
1) Predict current congestion level

Fig. 2. Illustration of the optimal rate calculation.

(8)
2) Calculate the optimal target rate as
(9)
3) Gradually approach the target rate with step sizes tuned
by RTTs
Fig. 3. Illustration of the adaptive FEC algorithm.

(10)
to limit computational complexity at the network nodes, and to
ensure scheme scalability for arbitrary number of streams. We
now address these practical design issues.
At the network node, we update the congestion level once
every time interval , so as to limit the extra processing burden
on the network nodes. Besides, since no video-specific information is involved, such calculation can be performed without
keeping per-flow states. The procedures at the network nodes
are summarized as below.

In (8), denotes the rate update interval, is the estimated
round-trip time, and designates the reference time for congestion-level prediction. In (10), is the rate update scaling factor.
The following theorem formally establishes stability of the
above design.
be the maximum round-trip
Stability Theorem: Let
time in the system,
be the minimum distortion, and
be the corresponding maximum video rate in the system. Further
and
. Then, given
assume that

Network Nodes:
(11)
Every update interval
Calculate the virtual congestion level:
(7)

the overall system is stable for any number of streams with
round-trip times less than
.
Proof: See Appendix A.
B. Network-Assisted Adaptive FEC

Upon packet arrival
For each video packet, update the virtual congestion level
packet header field as
.
Ideally, the video senders should directly choose the optimal
rate accoring to (6). In reality, however, the VCL value received
at a sender lags by one round trip time behind the current observation at the bottleneck link. Video senders experiencing different RTTs also observe different delayed versions of the virtual congestion level.
We take two steps in LIVA to address the effect of heterogeneous RTTs. First, to compensate for the impact of delayed
observations, each video sender predicts the current bottleneck
and the
congestion level from both the past sample
. This predicted congestion inforfreshly received sample
is used for optimal rate calculation. Second, we
mation
make gradual rate updates to approach the target optimal rate
. The update step sizes are inversely proportional to the

While both FEC and retransmission can protect video
streams from network errors, the nature of the approaches is
rather different: the retransmission approach is reactive while
the adaptive FEC approach is proactive. Since retransmissions
only happen after losses have occurred, the advantage of this approach lies in its efficient use of bandwidth resources. However,
stringent video playout deadlines and long round-trip times
often do not allow sufficient time or opportunities for retransmission. In addition, congestion losses tend to clutter together,
and bursts of retransmissions tend to worsen the situation. The
adaptive FEC approach, on the other hand, can provide error
protection in a timely fashion, yet it often depends on inference
of network conditions in order to decide the amount of FEC
needed. Inaccurate estimation of network conditions can lead to
wasted bandwidth. However, if the network can directly signal
its congestion (or pending congestion) conditions as in LIVA,
this drawback can be greatly mitigated. So we choose the FEC
approach for LIVA for its proactive nature with minimized
drawbacks.

724

IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 13, NO. 4, AUGUST 2011

To protect video streams against transient network congestion, the simplest solution is to always add a fixed amount of
FEC, within the target rate as computed in the above section.
However, fixed FEC protection unnecessarily eats away from
the video rate budget during steady state, when congestion is
unlikely. Instead, we propose to leverage the virtual congestion
level feedback from the network, so as to adapt the FEC protection in a proactive and efficient manner.
Reed–Solomn (RS) erasure codes across
We apply
video packets within each frame to generate
parity
packets.2 The parameters and are adjusted on the fly for
each video frame based on past and current congestion levels.
lost packets within
This provides protection against any
.
the same frame, with an overhead percentage of
The additional delay introduced by such protection is on the
order of video frame intervals.
is calculated based on the virThe protection percentage
tual congestion feedback observed for each stream. An inserves as an early indication of imcrease in the value of
pending queue rise, hence increases linearly with increase in
the congestion level. In addition, the value of is capped above
. This can also be expressed as follows:
by
(12)
otherwise.
denotes the difference between two samples of the
In (12),
congestion level observed at the sender. We choose the time interval between the two observations to be 200 ms. This relatively large choice of observation interval avoids overreacting
to temporary local fluctuations in the observed virtual congesis invoked
tion-level information. Full FEC protection at
exceeds
. The value of
when the difference
can be chosen empirically by learning from RTT statistics. In
our current implementation, it corresponds to an oversubscription limit at the bottleneck queue by 50% of link capacity for
the entire duration of 200 ms.
The scheme also relies on a few heuristics to make the basic
idea practical and robust. We introduce a dead-zone of 5% to
reduce false alarms, i.e., no FEC packets are injected unless the
recommended amount is greater than 5%. If the recommended
FEC amount suddenly falls to zero, the scheme holds on to the
last positive value for at least three RTTs before following the
recommendation. The adaptive scheme also dictates full FEC
protection when a stream initially starts, before sufficient congestion level information is collected from the network.
C. SVC Rate Adaptation
The final step of the LIVA system is to determine the SVC
rate adaptation based on the optimal rate calculated from
media-aware bandwidth sharing and the amount of FEC needed
for transient loss protection. In this work, we consider video
streaming with pre-encoded contents. We first describe the

Fig. 4. Structure of a GOP in an SVC encoded stream with quality and temporal
scalability. In this example, the GOP length is four frames, corresponding to
three temporal layers.

basic GOP structure of an SVC video stream, and then explain how we choose the SVC rate point of each stream while
padding the rest of the rate budget with FEC.
In the scalable extension of H.264/AVC, each video frame is
encoded into multiple video packets corresponding to multiple
quality layers [22]. The video packets are classified as base layer
(BL) and enhancement layer (EL) packets. In addition, the video
frames are organized into multiple temporal layers, in that an
is bi-directionencoded video frame from temporal layer
ally predicted from adjacent reconstructed video frames from
temporal layer . Fig. 4 provides an example GOP structure in
an encoded stream with two quality layers and three temporal
layers. On-the-fly rate adaptation is achieved by omitting EL
packets starting from frames with highest temporal layers. Note
temporal layers can be streamed at
that a stream with
alternative rate points. The video R-D parameters are fitted from
a discrete set of available rates and qualities, and are stored as
meta data along with each stream.
Given a target optimal rate calculated from media-aware
bandwidth sharing and a recommended FEC percentage , the
is determined as:
SVC stream rate
(13)
where the set of rates
’s denote available rate points for the
stream. The rest of the optimal rate is padded with FEC packets,
. In practice, this rate can only be approxas
imated when transmitting each video frame comprising netFEC packets.
work packets, by adding
IV. SIMULATION EVALUATION
A. Setup
We first evaluate performance of the LIVA scheme with
ns-23 simulations involving various network topologies and
video streams. At the network node, the target utilization
; the virtual congestion level update interval is
is
10 ms; the update scaling factor is
0.1 MSE/Kbits.4
At the video sender, the reference time for virtual congestion
250 ms and the video rate update scaling
level prediction is
3[Online].

Available: http://www.isi.edu/nsnam/ns/

4We have performed additional simulations in which the VCL update interval
2We choose the RS code for FEC mainly due to its optimality and popularity
for erasure protection. The adaptive algorithm we have developed, on the other
hand, is general enough to accommodate other types of channel codes.

varies from 1 to 100 ms. Note that the final allocation results stay the same. It
is also observed in simulations that convergence behavior of the solution stays
the same for a wide range of value for the VCL update interval values.

ZHU et al.: LIVA: NETWORK-ASSISTED BANDWIDTH SHARING AND TRANSIENT LOSS PROTECTION FOR VIDEO STREAMING

Fig. 5. (a) R-D and (b) rate-PSNR tradeoff curves of standard-definition video
streams used in simulations. Empirical R-D points are fitted to the parametric
model in (3).

725

Fig. 6. Transient behavior of the media-aware bandwidth sharing scheme from
LIVA, when two video streams, Harbor and City, share a bottleneck link with
capacity 4 Mb/s.

factor is
. Unless otherwise stated, the default network
topology is a single link traversed by multiple video streams.
Fig. 5 shows the R-D tradeoff for the three 4CIF video sequences used in the simulations: Harbor, City, and Ice. They
have a spatial resolution of 704 576 pixels per frame and a
temporal resolution of 30 frames per second. Each stream is encoded using the reference codec for the scalable extension of
H.264/AVC [23] with two quality layers and a GOP length of
32 frames, corresponding to six temporal levels and seven available rate points. The video packets are further segmented into
network packets with a size of 1500 bytes for transmission. The
receiver sends an acknowledgement (ACK) packet upon receipt
of every network packet.
B. Effect of Explicit Network Feedback
We first illustrate the basic dynamics of the media-aware
bandwidth sharing scheme with a simple scenario, where two
different streams, Harbor and City, share a bottleneck link with
a capacity of 4 Mb/s. Fig. 6 shows traces of the total traffic rate
on the bottleneck link, of the corresponding congestion level,
and of the allocated video rates. Initially, when only Harbor
is active, the maximum rate of the SVC stream can be easily
accommodated by the link, consequently, the congestion level
remains at zero. When the City stream starts at time
20 s,
it introduces transient congestion over the network. The instantaneous traffic rate over the link exceeds the link capacity,
leading to a sharp increase in the congestion level, which in
turn drives the reduction in rate of both streams. It can be noted
that Harbor continues to stream at a higher rate than City, as a
consequence of its more demanding R-D characteristic. When
40 s, the congestion level
Harbor finishes streaming at time
drops quickly back to zero, thereby allowing the remaining City
sequence to stream at the maximum rate and quality.
As a reference, Fig. 7 compares the traces of the allocated
video rates, resulting video quality in PSNR, and the bottleneck queue sizes from the LIVA scheme against TFRC [2]. With
the help of VCL information provided by the network nodes,
the allocated rates from LIVA converges much faster than in
TFRC. The TFRC scheme further suffers from standing queues
and occasional packet losses at steady state, resulting in drastic
drops in received video quality and a higher delivery delay of
the video packets. The LIVA scheme, in contrast, manages to
avoid packet losses completely in this scenario. It also leads
to an empty queue at steady state, thereby reducing the packet

Fig. 7. Comparison of the LIVE scheme with TFRC. Two video streams share
a link with a capacity of 4 Mb/s. The first stream Harbor starts at time t 0 s
and ends at time t
80 s. The second stream City starts at time t
40 s and
ends at time t 120 s. Both streams experience the same RTT of 40 ms.

=

=

=

=

delivery delay. When both streams are active in the network,
the allocated video rates from LIVA reflect differences in the
video R-D characteristics of the two streams. The more complex Harbor receives higher rate than the less demanding City.
TFRC allocation, on the other hand, treats both streams equally,
leading to a larger quality gap between the two streams.
C. Effect of Media-Aware Bandwidth Sharing
Next, we examine the optimality of LIVA’s media-aware
bandwidth-sharing scheme. For the simple case of two streams
sharing a single link, we plot the average video quality of both
streams as achieved by LIVA, by fair-rate allocation, and by
all other admissible allocations. The latter refers to various rate
combinations of both streams that do not exceed the target link
utilization. Fig. 8 shows such comparisons for the sequence
pair Harbor versus City over a bottleneck link with capacity
of 4 Mb/s. The results are averaged over 40 s of simulation.
It can be noted that LIVA achieves the highest average video
quality of both streams among all admissible rate allocations.
It outperforms fair-rate allocation by 0.57 dB in PSNR of the
average video quality.
We now vary the link capacity from 3 to 6 Mb/s. It can be
observed from Fig. 9 that media-aware bandwidth sharing in
LIVA consistently allocates higher rate for Harbor and lower
rate for City than fair-rate allocation. Consequently, it reduces

726

Fig. 8. Comparison of fair-rate and media-aware allocation, with respect to all
admissible rate combinations. Two video streams, Harbor and City, compete
over a single link with capacity of 4 Mb/s. Both streams experience an RTT of
40 ms. The results are averaged over 20 s at steady state.

IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 13, NO. 4, AUGUST 2011

Fig. 10. Traces of actual packet loss ratio, its delayed observation at the sender
(as reported from the receiver via ACK packets), and the recommended FEC
protection percentage for each video stream during the transient period. In this
example, the first stream enters an empty link with capacity of 4 Mbps at time
t
0 s; the second streams starts at time t 20 s and lasts till time t 40 s.
Both streams experience an RTT of 40 ms.

=

=

=

Fig. 9. Comparison of fair-rate and media-aware allocation in terms of
(a) average stream rate and (b) corresponding video quality. The Harbor and
City streams compete over a single link, experiencing an RTT of 40 ms. The
link capacity varies from 3 to 6 Mb/s. The results are averaged over 40 s after
convergence.

the quality gap between the two streams from 4.2 to 1.8 dB
in PSNR when the link capacity is 3 Mb/s. Allocation results
from both schemes become identical for link capacities beyond
5.5 Mb/s, as the link can now accommodate both video streams
at their maximum rates and qualities.
D. Effect of Network-Assisted FEC
We first study the network-assisted FEC scheme in a simple
example. Stream 1 enters an empty link with a capacity of
0 s; Stream 2 starts at time
20 s. When
4 Mb/s at time
the new stream enters the network, transient congestion at the
queue inflicts packet losses on both streams. Fig. 10 shows
the packet losses observed at the link and at the video senders
(as reported by the receiver via ACKs). Note that, by the time
the sender of Stream 1 is notified about packet drops, the loss
episode at the network node is almost over. Because of the
sender’s delayed observation of packet losses, it is hard for
even the best of the end-to-end adaptive schemes to add timely
FEC protection. In contrast, LIVA’s network-assisted FEC can
shield the stream in a more proactive manner.
Effectiveness of the network-assisted FEC protection is
further demonstrated in a more demanding scenario, whereby
three additional video streams simultaneously arrive at a single
link previously occupied by a single stream. The link capacity
is 10 Mb/s; all four video streams experience an RTT of 240 ms
and have the same content of Harbor. Fig. 11 compares the

Fig. 11. Comparison of No-FEC, Network-Assisted FEC, and Max-FEC
schemes in terms of decoded video quality in PSNR during the transient period
(averaged over time t 20–22 s) and at steady states (averaged over time t
30–35 s). In this example, the first stream initially enters an empty link with
0 s. Three additional streams join the network
capacity 10 Mb/s at time t
simultaneously at time t 20 s. All four streams experience the same RTT of
240 ms and carry the same video content Harbor.

=

=

=

=

network-assisted FEC scheme against two other heuristics operating at extreme measures: the No-FEC scheme never injects
FEC packets; the Max-FEC scheme always streams the video at
base-layer quality only, padding the rest of the rate budget with
FEC packets. During the transient period, both the Max-FEC
scheme and the proposed network-assisted adaptive scheme
are successful in recovering most packet losses, therefore both
achieve a higher video quality than the No-FEC scheme. At
steady state, video quality from the network-assisted adaptive
scheme is only slightly lower than the No-FEC scheme. It
outperforms the Max-FEC scheme, as the latter compromises
video quality at steady state with constant FEC overhead. This
shows that LIVA’s adaptive FEC scheme can always provide
optimal protection regardless of network conditions, whereas
No-FEC and Max-FEC can only perform favorably either in
transient or steady state, but not for both cases.

ZHU et al.: LIVA: NETWORK-ASSISTED BANDWIDTH SHARING AND TRANSIENT LOSS PROTECTION FOR VIDEO STREAMING

727

Fig. 12. Transient behavior of the media-aware bandwidth sharing scheme
from LIVA, when two video streams, Harbor and City, share a bottleneck link
with capacity 4 Mb/s.
Fig. 14. Allocation traces for (a) streams with heterogenous RTTs and
(b) streams traversing multiple bottleneck links. In both examples, the video
streams carry the same content Harbor.

Fig. 13. Network topologies for simulation studies of (a) video streams experiencing heterogeneous RTTs and (b) multiple video streams over multiple
bottleneck links.

E. Varying Network and Traffic Conditions
We now revisit the simple scenario of two video streams
sharing a single bottlenect link, and introduce 1% random
packet loss in the simulation. As shown in Fig. 12, the traces
of total traffic rate, calculated virtual congestion level, and
allocated video rates are very similar to those in Fig. 6. This
confirms that although the design of LIVA focuses on combating congestion, the proposed scheme is still robust to the
presence of random packet losses.
We also test LIVA in more complicated network topologies,
as depicted in Fig. 13. Fig. 14(a) shows the traces of the allocated rates for two video streams, both Harbor, that are delivered over paths with RTT of 200 and 10 ms, respectively [see
Fig. 13(a)]. Response from the first stream with long RTT is
more sluggish than the second stream with a shorter RTT. Nevertheless, both streams converge to the same rate at steady state.
Fig. 14(b) shows traces of the virtual congestion levels and
the allocated rate to each of the four Harbor streams. They start
0, 0.5, 1.0, and 1.5 s, respectively, and traverse the
at times
two-link network in Fig. 13(b). The virtual congestion level is
nonzero for both links. The first link is bottleneck for Streams
1 and 2. The second link is bottleneck for Streams 1, 3, and 4,
which consequently receive the same allocated rates.
Finally, we study scalability of the proposed LIVA scheme, by
comparing its dynamics over low-speed and high-speed links.

In the low speed case, three video streams, Harbor, City, and
Ice, sequentially enter a link with a capacity of 4 Mb/s. In the
high-speed case, 30 video streams, containing ten streams of
Harbor, City, and Ice each, sequentially enter a link with a capacity of 40 Mb/s. Fig. 15 shows the traces of allocated rate
for each stream, virtual congestion level at the network node,
and the queue size for both cases. It can be noted that the traces
for the three streams sharing the 4-Mb/s link is almost identical
to the traces for the 30 streams sharing the 40-Mb/s link, except that the queue limit for the high-capacity link is scaled accordingly. Convergence time and allocated rates are unaffected
by the number of participating streams. For both cases, when
all streams are active, standard deviations of the allocated rates
for each stream range between 0.03–0.11 Mb/s, constituting
3.3%–7.9% of the streaming rates.
V. SYSTEM DEMONSTRATION
Here, we present a proof-of-concept system demonstration of
the LIVA scheme. As shown in Fig. 16, the system consists of
a sender, a receiver, and a relaying network node, all running
Linux 2.6.24.7.
The network node is implemented as an element in the
Click Modular Router.5 The LIVA element does not have any
builtin queues, but makes use of the queue element provided by
Click. The element periodically calculates the rate of incoming
traffic and modifies the extended RTP header of traversing
video packets. It maintains periodic updates of two variables
in its cache: the incoming traffic rate and the value of VCL.
Upon relaying each video packet, the network node stamps the
updated virtual congestion level into an RTP extension header
field, if its own VCL value is greater than the existing value
5[Online].

Available: http://read.cs.ucla.edu/click/

728

IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 13, NO. 4, AUGUST 2011

Fig. 15. Traces of allocated of each video stream, congestion level reported by the network node, and queue size at the bottleneck link, for three streams competing
over a link of 4 Mb/s (left) or 30 streams competing over a link of 40 M/ps (right). Among the 30 video streams, the first 10 streams have the content of Harbor,
the next 10 streams have City as content; the last 10 streams have the content of Ice.

Fig. 16. LIVA system demonstration, consisting of three Linux-based nodes
as sender, relay, and receiver. The decoded video of both competing streams
are displayed on the same screen. (a) Demonstration topology. (b) Linux nodes.
(c) Display of streaming results.

Fig. 17. Value of virtual congestion level is stored in the RTP header extension
field.

in the header field. Fig. 17 lists extended RTP header fields
utilized by the LIVA scheme.

The video sender and receiver are implemented as userspace
programs over UDP sockets. The receiver extracts the value of
maximum virtual congestion level along the path from the extended RTP header field of the received packet, and reports such
information in the corresponding field of an acknowledgment
packet. The sender then adjusts the outgoing rate of the video
stream accordingly, by dynamically tuning the interval of adjacent packet transmissions. For the purpose of rate adaptation,
each video sequence is pre-encoded into multiple quality versions using x264,6 a fast implementation of the H.264/AVC
standard [24]. At the boundary of each GOP, the video sender
switches the bitstream to the version with a rate corresponding
to the optimal allocation.
In our demonstration scenario, two different streams, City and
Mother & Daughter, share a bottleneck link with a capacity of
2.5 Mb/s. The target utilization is chosen at 95%. City starts
0 s and Mother & Daughter starts at time
streaming at time
50 s. Fig. 18 shows the recorded traces of allocated rates at
the senders, as well as traces of the virtual congestion level at the
network node. Initially when only the City sequence is present
in the network, its allocated rate stabilizes at 2.35 Mb/s and the
virtual congestion level fluctuates around a corresponding value.
Immediately after the Mother & Daughter stream enters the network, the virtual congestion level at the network node quickly
increases to a new equilibrium, leading to decreased allocated
rate for City and increased allocation for the new stream. After
the Mother & Daughter sequence has finished streaming at time
190 s, rate of City quickly returns to 2.35 Mb/s. During the
period when both streams are present, the standard deviation
of allocated rate for each stream are 0.146 Mb/s for City and
0.056 Mb/s for Mother & Daughter, respectively.
Fig. 19 shows the visual quality of both sequences, before
and after the allocation has converged. While the rate reduction
of 0.5 Mb/s from City introduces negligible reduction in its visual quality, it greatly improves the quality of the new Mother &
6[Online].

Available: http://developers.videolan.org/x264.html

ZHU et al.: LIVA: NETWORK-ASSISTED BANDWIDTH SHARING AND TRANSIENT LOSS PROTECTION FOR VIDEO STREAMING

729

Fig. 18. Recorded traces of (a) allocated video rates and (b) virtual congestion level at the bottleneck link. Two video streams, City and Mother & Daughter,
compete over a single link with capacity 2.5 Mb/s.

Fig. 20. Coexistence of LIVA video streams and TCP streams within the ECN
framework in the Linux-based system demonstration.

Fig. 19. Visual quality of the two competing video streams, before and after the
rate allocation has converged. (a) Before convergence. (b) After convergence.

Daughter stream. This is the main benefit of media-aware bandwidth allocation.
VI. COEXISTENCE WITH TCP FLOWS
So far, we have described and evaluated the LIVA scheme
from a clean-slate perspective, and have demonstrated how
video streams can benefit from simple network feedback. In
reality, however, the success of any newly proposed scheme
largely hinges on how well it can coexist with legacy systems.
We address this crucial question in this section by presenting a
solution for LIVA’s coexistence with TCP flows while retaining
most of its benefits.
We achieve this goal by leveraging the existing explicit congestion notification (ECN) mechanism [7]. A random marking
, based on the
probability is calculated as
current congestion level , the maximum congestion level
,
and the maximum marking probability
. With a probability
of , the network node then marks the Congestion-Experienced
(CE) bit within the IP header of traversing packets, as specified
by [7].
The parameters
and
are globally defined in
the same manner as , so that the receiver can recover the
congestion level based on estimated packet marking ratio ,
as
. The value of
is reported to

the video sender in an application-layer packet header field.
Subsequently, the sender calculates the optimal rate according
to (8)–(10), substituting
for . Since
is essentially
a low-pass filtered observation of , the scheme is expected
to converge more slowly, without affecting the final allocation
result. Note that this proposed solution is similar in spirit to
the random exponential marking (REM) scheme described in
[25]. The main difference in our scheme is the linear mapping
between congestion level and marking probability , instead
of the exponential mapping in the original paper.
No modification is required for the TCP flows, as long as the
TCP sender and receiver react to the ECN markings according
to the standard specifications [7]. The average TCP throughput
can be expressed as
(14)
with scaling factor , average packet size , and RTT [2]. The
average video rate from (6) can be rewritten as
(15)
In (15), the value of
is typically much smaller than
,
hence it can be omitted. Note the same square root dependence
on the marking probability in (14) and (15). This allows a TCP
flow to compete for bandwidth within the LIVA framework with
a video-like rate utility function.
Fig. 20 illustrates the effectiveness of the proposed ECNbased solution. In this Linux-based system demonstration, the

730

IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 13, NO. 4, AUGUST 2011

bottleneck link capacity is set to 2.5 Mb/s at the relaying network node. Instead of marking RTP header extensions as described in Section V, the relay now marks the CE bit in the IP
header of all traversing packets with probability . Initially, the
link accommodates a single City stream with full utilization. An
30 s and shares the bandFTP flow over TCP starts at time
width with the City stream. Shortly after the second video stream
Mother & Daughter arrives at the link, the rates of both the TCP
flow and the City stream are reduced. It can be noted that the
TCP flow receives a bandwidth share between the more complex City sequence and the less demanding Mother & Daughter
sequence. When all three streams are active, standard deviations
of the allocated rates are around 0.20 Mb/s, ranging between
10.4%–21.6% of the allocated rates for each stream. Throughout
the entire duration of this experiment, the ECN-based LIVA
scheme ensures full utilization of the bottleneck link, with a total
traffic rate of 2.5 Mb/s.
While preliminary study of this ECN-based solution shows
promising results, we note that more thorough analysis is needed
for understanding the impact of random marking on system stability. In networks where ECN is not universally deployed, the
proposed solution can resort to random early dropping schemes,
as in [26]. In this case, the random dropping probability corresponds to a congestion level calculated with full link utilization
. While the benefits from explicit network notification
can no longer be retained, LIVA can still achieve media-aware
bandwidth sharing among the different video streams.

Fig. 21. Laplace transform of the overall feedback loop in LIVA.

dynamics around their operating points based on small pertur, which leads to the following:
bations, e.g.,

if passes

(16)
(17)
(18)
(19)
(20)

where
,
, and
.
From the above, we can obtain transfer functions in Laplace
domain as shown in Fig. 21. Consequently, the closed-loop
transfer function of the system is
(21)

VII. CONCLUSION AND FUTURE WORK
In this paper, we described LIVA, a new architecture for
streaming video in the Internet. We showed via simulation
studies, Linux-based system demonstration, and theoretical
analysis that video streams benefit immensely from simple explicit network feedback. The media-aware bandwidth-sharing
scheme in LIVA allows the bottleneck link to be shared in an
efficient manner, minimizing total distortion of all competing
video streams. Rate allocation adapts to transient events with
fast convergence, while avoiding steady-state queuing delays
or persistent packet losses. With the network-assisted FEC
scheme, a video stream can proactively shield itself from
transient network congestion before its quality is impacted.
Going forward, we will explore how LIVA can benefit live
streaming applications, such as video conferencing, by estimating on the fly video R-D parameters. It is also possible to
extend the LIVA framework for video multicast, whereby each
network relay device has the capability to selectively forward
video packets.

(22)
where
for any given .
We choose the value of
and
. Equation (22) can be approximated by

and set

(23)
i.e.,

(24)

,
Note that the transfer function has the form of
where is a constant. It is proved in [27] that a system with a
feedback loop of this form can achieve scalable stability for all
values of given a proper value of . The Nyquist criterion
implies that the general loop achieves scalable stability for all
given that
, i.e.,
. This means our system
can achieve stability if the following is true:
(25)

APPENDIX A
STABILITY PROOF
Recall from (7)–(10) in Section III that a fluid model of the
and
system dynamics can be expressed as
. We further linearize our system

Typically,
bility is

, so a sufficient condition for sta-

(26)

ZHU et al.: LIVA: NETWORK-ASSISTED BANDWIDTH SHARING AND TRANSIENT LOSS PROTECTION FOR VIDEO STREAMING

Therefore, we can choose the value of
case scenario

according to the worst

(27)
so that the system is stable for any number of streams with RTTs
.
less than

REFERENCES
[1] “Cisco Visual Networking Index—Forecast and Methodology,
2008–2013,” Cisco White Paper. [Online]. Available: http://newsroom.cisco.com/dlls/index.html
[2] S. Floyd, M. Handley, J. Pahdye, and J. Widmer, “TCP Friendly Rate
Control (TFRC): Protocol Specification,” RFC 3448 (proposed standard).
[3] P. A. Chou and Z. Miao, “Rate-distortion optimized streaming of packetized media,” IEEE Trans. Multimedia, vol. 8, no. 2, pp. 390–404, Apr.
2006.
[4] D. Wu, Y. T. Hou, W. Zhu, Y.-Q. Zhang, and J. M. Peha, “Streaming
video over the Internet: Approaches and directions,” IEEE Trans. Circuits Syst. Video Technol., vol. 11, no. 3, pp. 282–300, Mar. 2001.
[5] M. Krunz, “Bandwidth allocation strategies for transporting variablebit-rate video traffic,” IEEE Commun. Mag., vol. 36, no. 1, pp. 40–46,
Jan. 1999.
[6] A. Demers, S. Keshav, and S. Shenker, “Analysis and simulaton of a
fair queueing algorihtm,” J. Internetworking Res. Experience, pp. 3–26,
Oct. 1990.
[7] K. K. Ramakrishnan, S. Floyd, and D. Black, “The Addition of Explicit
Congestion Notification (ECN) to IP,” RFC 3168 (proposed standard),
Sep. 2001.
[8] N. Wakamiya, M. Miyabayashi, M. Murata, and H. Miyahara,
“MPEG-4 video transfer with TCP-friendly rate control,” in Proc. 4th
IFIP/IEEE Int. Conf. Manag. Multimedia Netw. Services (MMNS’01),
2001, vol. 2216, pp. 29–42.
[9] P. Papadimitriou and V. Tsaoussidis, “SSVP: A congestion control
scheme for real-time video streaming,” Computer Netw., vol. 51, no.
15, pp. 4377–4395, Oct. 2007.
[10] L. Böröczky, A. Y. Ngai, and E. F. Westermann, “Statistical multiplexing using MPEG-2 video encoders,” IBM J. Res. Dev., vol. 43, no.
4, pp. 511–520, Jul. 1999.
[11] J. Chakareski and P. Frossard, “Rate-distortion optimized distributed
packet scheduling of multiple video streams over shared communication resources,” IEEE Trans. Multimedia, vol. 8, no. 2, pp. 207–218,
Apr. 2006.
[12] F. Kelly, A. Maulloo, and D. Tan, “Rate control in communication
networks: Shadow prices, proportional fairness and stability,” J. Oper.
Res. Soc., vol. 49, no. 3, pp. 237–252, Mar. 1998.
[13] M. Dai, D. Loguinov, and H. M. Radha, “Rate-distortion analysis
and quality control in scalable internet streaming,” IEEE Trans.
Multimedia, vol. 8, no. 6, pp. 1135–1146, Dec. 2006.
[14] X. Zhu and B. Girod, “Distributed media-aware rate allocation for wireless video streaming,” in Proc. Picture Coding Symp., Chicago, IL,
May 2009, pp. 1–4.
[15] D. Katabi, M. Handley, and C. Rohrs, “Congestion control for high
bandwidth-delay product networks,” in Proc. ACM SIGCOMM, Pittsburgh, PA, Aug. 2002, pp. 89–102.
[16] L. H. Andrew, K. Jacobsson, S. H. Low, M. Suchara, R. Witt, and B. P.
Wydrowski, “Maxnet: Theory and Implementation,” Netlab, Cal. Inst.
Technol., Pasadena, CA, 2006.
[17] W.-T. Tan and A. Zakhor, “Video multicast using layered FEC and
scalable compression,” IEEE Trans. Circuits Syst. Video Technol., vol.
11, no. 3, pp. 373–386, Mar. 2001.
[18] C. Hellge, T. Schierl, and T. Wiegand, “Receiver driven layered multicast with layer-aware forward error correction,” in Proc. IEEE Int.
Conf. Image Process., San Diego, CA, Oct. 2008, pp. 2304–2307.
[19] S. Kang and D. Loguinov, “Impact of FEC overhead on scalable video
streaming,” in Proc. Int. Workshop Netw. Operating Syst. Support for
Digital Audio and Video, Stevenson, WA, 2005, pp. 123–128.

731

[20] A. Argyriou, “Distributed resource allocation for network-supported
FGS video streaming,” in Proc. IEEE 16th Int. Packet Video Workshop
(PV’07), Lausanne, Switzerland, Nov. 2007, pp. 211–217.
[21] K. Stuhlmüller, N. Färber, M. Link, and B. Girod, “Analysis of video
transmission over lossy channels,” IEEE J. Sel. Areas Commun., vol.
18, no. 6, pp. 1012–1032, Jun. 2000.
[22] H. Schwarz, D. Marpe, and T. Wiegand, “Overview of the scalable
video coding extension of H.264/AVC,” IEEE Trans. Circuits Syst.
Video Technol., vol. 17, no. 9, pp. 1103–1120, Sep. 2007.
[23] ITU-T Recommendation H.264—ISO/IEC 14496-10(AVC), Advanced
Video Coding for Generic Audiovisual services, Amendment 3: Scalable Video Coding, ITU-T and ISO/IEC JTC 1, 2005.
[24] Advanced Video Coding for Generic Audiovisual services, ITU-T Recommendation H.264—ISO/IEC 14496-10 (AVC), ITU-T and ISO/IEC
JTC 1, 2003.
[25] D. Lapsley and S. Low, “Random exponential marking: An optimization approach to internet congestion control,” in Proc. IEEE 7th Int.
Conf. Networks (ICON ’99), Brisbane, Australia, Sep. 1999, pp. 67–74.
[26] S. Floyd and V. Jacobson, “Random early detection gateways for
congestion avoidance,” IEEE/ACM Trans. Netw., vol. 1, no. 4, pp.
397–413, Aug. 1993.
[27] F. Paganini, Z. Wang, J. Doyle, and S. Low, “Congestion control
for high performance, stability and fairness in general networks,”
IEEE/ACM Trans. Netw., vol. 13, no. 1, pp. 43–56, Feb. 2005.

Xiaoqing Zhu received the B.E. degree in electronics engineering from Tsinghua University,
Beijing, China, in 2001, and the M.S. and Ph.D.
degrees in electrical engineering from Stanford
University, Stanford, CA, in 2002 and 2009,
respectively.
She is currently with the Advanced Architecture
and Research Group, Cisco Systems Inc., San Jose,
CA. She held summer internships with IBM Almaden Research Center in 2003 and with Sharp Labs
of America in 2006. Her research interests include
wireless video networking, distributed resource allocation, and performance
enhancement for Internet video delivery.
Dr. Zhu was the recipient of a Stanford Graduate Fellowship from 2001 to
2005 and the Best Student Paper Award at ACM Multimedia 2007.

Rong Pan received the Ph.D. degree from Stanford
University, Stanford, CA, in 2002.
She joined Cisco Systems Inc., San Jose, CA, in
2004, where she is now with the Advanced Architecture and Research Group. She is interested in network
algorithms and system analysis. Her recent work has
been in the areas of distributed rate control (video
and data center networking), congestion control and
quality-of-service. She has 20 patents either awarded
or in the advanced processing stage. One of her distributed algorithms has been adopted by IEEE 802.1
for Data Center Bridging: 802.1Qau. Many of her other algorithms have played
key roles in multiple Cisco’s flagship products such as Nexus 7000 Data Center
Platform, the Catalyst 6500 high-end Enterprise Platform and Catalyst 3000
Edge Switch Platform.

Mythili S. Prabhu received the B.E. degree in
electronics and communication engineering from the
Viswesvaraya Technological University, Karnataka,
India, in 2006, and the M.S. degree in electrical engineering from the University of Southern California,
Los Angeles, in 2009.
She is currently with the Advanced Architecture
and Research Group, Cisco Systems Inc., San Jose,
CA.

732

IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 13, NO. 4, AUGUST 2011

Nandita Dukkipati received the B.S. degree from
Birla Institute of Technology and Science, India,
the M.S. degree from the Indian Institute of Science, Bangalore, India, and the Ph.D. degree from
Stanford University, Stanford, CA, all in electrical
engineering.
She is currently a Software Engineer with Google
Inc., where she is working on problems in Networking and Making the Web Faster.

Vijay Subramanian received the M.S. degree in
computer science from Arizona State University,
Phoenix, and the Ph.D. degree in computer systems
and engineering from Rensselaer Polytechnic Institute, Troy, NY.
He is currently an Engineer with the Advanced
Architecture and Research Group, Cisco Systems
Inc., San Jose, CA. His primary research interests
are in the field of wireless/mobile networking,
transport protocols, congestion control, and traffic
management.

Flavio Bonomi received the degree in electrical engineering from Pavia University, Pavia, Italy, and the
Ph.D. degree in electrical engineering from Cornell
University, Ithaca, NY.
He has worked at the boundary between networking research and development since 1985, when
he joined AT&T Bell Laboratories. He moved to
the Silicon Valley in 1995, and, after two startup
experiences, eventually joined Cisco Systems Inc.,
San Jose, CA, in 1999. After four years as a Senior
Architect in the development for Cisco Core Routers
(GSR 1200), for the past years he has been contributing to important innovations in the Data Center and Enterprise space. He is currently a Fellow at Cisco
and is leading the Advanced Architecture and Research Group.

The Scaling Hypothesis: Simplifying the Prediction of
Network Performance using Scaled-down Simulations
Konstantinos Psounis† , Rong Pan† , Balaji Prabhakar† , Damon Wischik‡
†

Stanford Univerity, ‡ Cambridge University
Email: kpsounis, rong, balaji@stanford.edu, D.J.Wischik@statslab.cam.ac.uk

ABSTRACT

mation to estimate the state of the network. The trouble is
that this requires careful traffic characterization and modelbuilding. The heterogeneity of the Internet makes this timeconsuming and difficult, since each scenario might potentially
require a different new model.
In this paper we explore a way to reduce the computational
requirements of simulations and the cost of experiments, and
hence simplify network measurement and performance prediction. We do this by combining simulations with sampling and
analysis. Our basic hypothesis, which we call SHRiNK1 , is
this: if we take a sample of the traffic, and feed it into a suitably scaled version of the system, we can extrapolate from the
performance of the scaled system to that of the original.
This has two benefits. First, by relying only on a sample of
the traffic, SHRiNK reduces the amount of data we need to
work with. Second, by using samples of actual traffic, it shortcuts the traffic characterization and model-building process
while ensuring the relevance of the results.
This approach also presents challenges. At first sight, it appears optimistic. Might not the behavior of a large network
with many users and higher link speeds be intrinsically different to that of a smaller network? Somewhat surprisingly
we find that, in several essential ways, one can mimic a large
network using a suitably scaled-down version. The key is to
find suitable ways to scale down the network and extrapolate
performance.
The outline of the paper is as follows: In Section 2 we study
the scaling behavior of an IP-network whose traffic consists of
long-lived TCP-like flows arriving in clusters. Networks with
such traffic have been used in the literature to test the behavior of control algorithms and queue management schemes.
Using simulations and theory we find that when such a network is suitably scaled, performance measures such as queueing delay and drop probability are left virtually unchanged. In
Section 3 we study IP networks at which flows arrive at random times (i.e. unclustered) and whose sizes are heavy-tailed.
Such networks are representative of the Internet. We find that
a different scaling to that in Section 2 leaves the distribution
of the number of active flows and of their normalized transfer
times unchanged. A simple theoretical argument reveals that
the method we suggest for “SHRiNKing” networks in which
flows arrive at random times will be widely applicable (i.e. for
a variety of topologies, flow transfer protocols, and queue management schemes). By contrast, we find that the theoretical
underpinning for SHRiNKing networks at which flows arrive
in clusters depends on the type of queue management scheme

As the Internet grows, so do the complexity and computational
requirements of network simulations. This leads either to unrealistic, or to prohibitely expensive simulation experiments.
We explore a way to side-step this problem, by combining
simulation with sampling and analysis. Our hypothesis is this:
if we take a sample of the traffic, and feed it into a suitably
scaled version of the system, we can extrapolate from the performance of the scaled system to that of the original.
We find that when we scale a network which is shared by
TCP-like flows, and which is controlled by a variety of active
queue management schemes, then performance measures such
as queueing delay and the distribution of flow transfer times
are left virtually unchanged. Hence, the computational requirements of network simulations and the cost of experiments
can decrease dramatically.

1. INTRODUCTION
Measuring the performance of the Internet and predicting its
behavior under novel protocols and architectures are important
research problems. These problems are made difficult by the
sheer size and heterogeneity of the Internet: it is very hard to
simulate large networks and to pinpoint aspects of algorithms
and protocols relevant to their behavior. This has prompted
work on traffic sampling [1, 2]. Sampling certainly reduces the
volume of data, although it can be hard to work backwards—to
infer the performance of the original system.
A direct way to measure and predict performance is with
exhaustive simulation: If we record the primitive inputs to the
system, such as session arrival times and flow types, we can
in principle compute the full state of the system. Further,
through simulation we can test the behavior of the network
under new protocols and architectures. But such large-scale
simulation requires massive computing power.
Reduced-order models can go some way in reducing the burden of simulation. In some cases [3, 13] one can reduce the dimensionality of the data, for example by working with traffic
matrices rather than full traces, while retaining enough infor-

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.

ACM SIGCOMM Computer Communications Review

1

SHRiNK: Small-scale Hi-fidelity Reproduction of Network Kinetics

35

Volume 33, Number 1: January 2003

1200
300
0

50

100 150 200
grp1

time

grp2

100Mbps
R1

# of TCP flows

2. SCALING BEHAVIOR OF IP NETWORKS
WITH LONG-LIVED FLOWS
In this section we explore how SHRiNK applies to IP networks used by long-lived TCP-like flows that arrive in clusters,
and controlled by queue management schemes like RED.
First, we explain in general terms how we sample traffic,
scale the network, and extrapolate performance.
Sampling is simple. We sample a proportion α of the flows,
independently and without replacement.
We scale the network as follows: link speeds and buffer sizes
are multiplied by α. The various AQM-specific parameters are
also scaled, as we will explain in the following section 2.1. The
network topology is unchanged during scaling. In the cases we
study, performance measures such as average queueing delay
are virtually the same in the scaled and the unscaled system.
Our main theoretical tool is the recent work on fluid models for TCP networks [8]. While [8] shows these models to be
reasonably accurate in most scenarios, the range of their applicability is not yet fully understood. However, in some cases
the SHRiNK hypothesis holds even when the fluid model is
not accurate, as shown in Section 2.1.2.

1200

0

50

100 150 200 time

100Mbps
R2

R3

grp3
600
0

50

100 150 200

time

Figure 1: Basic network topology and flow information

This network is scaled-down by factors α = 0.1 and 0.02,
and the parameters are modified as described above.
90

80

Queueing Delay (msec)

70

2.1 RED
The key features of RED are the following two equations,
which together specify the drop (or marking) probability. RED
maintains a moving average qa of the instantaneous queue size
q; and qa is updated whenever a packet arrives, according to
the rule

60

50

40

30
fluid model
scale = 1
scale = 0.1
scale = 0.02

20

10

0

qa := (1 − w)qa + wq,

0

20

40

60

80

100

120

140

160

180

200

Simulation Time (Sec)

where w is a parameter that determines the size of the averaging window. The average queue size determines the drop
probability p, according to the equation


if qa < minth
0

qa −minth
if
minth ≤ qa < maxth (1)
pRED (qa ) = pmax max
th −minth


1
if qa > maxth

Figure 2: Basic Setup: Average Queueing Delay at Q1
We plot the average queueing delay at Q1 in Figure 2. The
drop probability at Q1 is shown in Figure 3. Due to a lack of
space, we omit the plot of the average queueing delay and drop
probability for Q2 whose behavior is similar to those of Q1.
We see that the queueing delay and the drop probabilities are
almost identical at different scales. We draw attention to two
features which we shall comment upon later: (i) The transient
behaviors (e.g. the overshoots and undershoots) are quite wellmimicked at the smaller scales, and (ii) the variability increases
as the scale reduces.
Since the queueing dynamics and drop probabilities essentially remain the same, the dynamics of the TCP flows are also
unchanged. In other words, an individual flow which survives
the sampling process essentially cannot tell whether it is in the
scaled or the unscaled system.
We have also tested the case where the scale is α = 0.01. In
this case, only three flows in grp1 are present during the period
150s to 200s. Hence the sample is too meager to reproduce the
queueing dynamics. Certainly, 1% is not the limit of the scaling
hypothesis in general. Further study needs to be conducted to
find out theoretically where this limit is.

We scale the parameters pmax , minth , maxth and w as follows:
minth and maxth are multiplied by α; pmax is fixed at 10%; the
averaging parameter w is multiplied by α−1 . The reason of
choosing these parameters will become clear later in Section
2.1.1.
The Basic Setup
We consider two congested links in tandem, as shown in Figure
1. There are three routers, R1, R2 and R3; and three groups of
flows, grp1, grp2, and grp3. The link speeds are 100Mbps and
the buffers can hold 8000 packets. The RED parameters are
minth = 1000, maxth = 2500 and w = 0.000005. For the flows:
grp0 consists of 1200 TCP flows each having a propagation
delay of 150ms, grp1 consists of 1200 TCP flows each having
a propagation delay of 200ms, and grp2 consists of 600 TCP
flows each having a propagation delay of 250ms. Note that
75% of grp0 flows switch off at time 150s.

ACM SIGCOMM Computer Communications Review

# of TCP flows

# of TCP flows

used at the routers.
A word about the organization of the paper: Space limitations have necessitated a selective presentation of the material.
We have chosen to describe the method in detail. The theoretical complement and the validation using simulations are
abbreviated. More details can be found in a longer version of
the paper [10].

36

Volume 33, Number 1: January 2003

equations are:

0.08
scale = 1
scale = 0.1
scale = 0.02

0.07

W̄i (t)W̄i (t − τi )
1
dW̄i (t)
=
−
p̄(t − τi )
dt
Ri (q̄(t))
1.5Ri (q̄(t − τi ))

Drop Probability

0.06

dq̄(t) X
W̄i (t)
=
−C
dt
R
i (q̄(t − τi ))
i=1

(2)

N

0.05
0.04

(3)

dq̄a (t)
log(1 − w)
log(1 − w)
=
q̄a (t) −
q̄(t)
dt
δ
δ
p̄(t) = pRED (q̄a (t))

0.03
0.02

(4)
(5)

0.01

where τi = τi (t) solves τi (t) = Ri (q̄(t − τi (t))), δ is the average
packet inter-arrival time, and pRED is as in (1) 2 . Suppose we
have a solution to these equations

W̄i (·), q̄(·), q̄a (·), p̄(·) .

0
0

20

40

60

80
100
120
Simulation Time (Sec)

140

160

180

200

Figure 3: Basic Setup: Drop Probability at Q1

Now, suppose the network is scaled and denote by C0 , N 0 ,
etc., the parameters of the scaled system. When the network is
scaled, the fluid model equations change,
and so the solution

changes. Let W̄i0 (·), q̄0 (·), q̄a0 (·), p̄0 (·) be the solution of the
scaled system. It can be theoretically verified (but we do not
do this here due to lack of space) that


W̄i0 (·), q̄0 (·), q̄a0 (·), p̄0 (·) = W̄i (·), αq̄(·), αq̄a (·), p̄(·) ,

2.1.1

With Faster Links
Suppose we alter the basic setup, by increasing the link
speeds to 500Mbps, while keeping all other parameters the
same. Figure 4 (zoomed in to emphasize the point) illustrates
that, once again, scaling the network does not alter the queueing delay at Q1 (Q2 shows the same scaling behavior). Note
that high link speeds cause the queue to oscillate. There have
been various proposals for stabilizing RED [5, 9]. We are not
concerned with stabilizing RED here: we mention this case to
show that SHRiNK can work whether or not the queue oscillates.

which means the queueing delay q̄0 /C 0 = αq̄/αC is identical
to that in the unscaled system. The drop probability is also
the same in each case, i.e. p̄(t) = p̄0 (t). Thus, we will have
theoretical support for the observations in the previous section.
90
Scale = 1
Scale = 0.1

18

80

scale = 1
scale = 0.1
scale = 0.02

70

Queueing Delay (msec)

Queueing Delay (msec)

16

14

12

60

50

40

30

10
20

10

8

0

6
80

85

90

95
100
Simulation Time (Sec)

105

0

20

40

60

80
100
120
Simulation Time (Sec)

140

160

180

200

110

Figure 5: Fluid model predicts scaling behavior
Figure 4: With faster links: Average queueing delay
at Q1 (zoomed in)
Figure 5 presents the solution of the fluid model for the
queueing delay at Q1 under the scenario of Figure 1 for the
scale parameters α = 1 and 0.1. As can be seen, both the solutions are virtually identical, illustrating the scaling property
of the differential equations mentioned above.

Theory
We now show that these simulation results are supported by
the fluid model of TCP/RED [8].
Consider N flows sharing a link of capacity C. Let Wi (t)
and Ri (t) be the window size and round-trip time of flow i at
time t. Here Ri (t) = Ti + q(t)/C, where Ti is the propagation
delay for flow i and q(t) is the queue size at time t. Let p(t) be
the drop probability and qa (t) the average queue size at time
t.
The fluid model describes how these quantities evolve; or
rather, since these quantities are random, the fluid model describes how their expected values evolve. Let X̄ be the expected value of random variable X. Then the fluid model

ACM SIGCOMM Computer Communications Review

2.1.2 When the theory is not appropriate
Suppose we alter the basic setup, by decreasing the link
speeds to 50Mbps, while keeping all other parameters the same.
Once again, scaling the network does not alter the queueing
delay. Due to limitations of space we omit the corresponding
plot. For such a simulation scenario, especially in the time
frame 100sec-150sec, the fluid model is not a good fit as shown
2
We have the constant 1.5 in (2), not 2 as in [8]. This change
improves the accuracy of the fluid model; due to limited space
we omit the derivation.

37

Volume 33, Number 1: January 2003

in [12] and verified by us via simulations: actual window and
queue sizes are integer-valued whereas fluid solutions are realvalued; rounding errors are non-negligible when window sizes
are small as is the case here. The range of applicability of the
fluid model is not our primary concern in this paper: we mention this case to show that SHRiNK can work whether or not
the fluid model is appropriate.

study the scaling behavior of IP networks carrying heavy-tail
distributed, Poisson flows. Our finding is that with a somewhat
different scaling than in the previous section, the distributions
of a large number of performance measures, such as the number
of active flows and the delay of flows, remain the same.

3.1 Simulations
We perform simulations using ns-2 for the same topology as
in Figure 1. There are three routers, R1, R2 and R3, two links
in tandem, and three groups of flows, grp1, grp2, and grp3.
The link speeds are 10Mbps. We present simulations with both
RED and DropTail. The RED parameters are minth = 100,
maxth = 250 and w = 0.00005. When using DropTail, the
buffer can hold 200 packets.
Within each group flows arrive as a Poisson process with
some rate λ. We vary λ to study both uncongested and congested scenarios. (We use the ns-2 built-in routines to generate
sessions consisting of a single object each. This is what we call
a flow.) Each flow consists of a Pareto-distributed number
of packets with average size 12 packets and shape parameter
equal to 1.2. The packet size is set to 1000 bytes. The propagation delay of each flow of grp0, grp1, and grp2, is 50ms,
100ms, and 150ms respectively.

Queueing Delay (Router 2)
140
scale = 1
scale = 0.1

Queueing Delay (msec)

120
100
80
60
40
20
0
0

20

40

60

80
100
120
Simulation Time (Sec)

140

160

180

200

Figure 6: DropTail: Average queueing delay at Q2

Sampling and Scaling
The heavy-tailed nature of the traffic makes sampling a bit
more involved than before, because a small number of very
large flows has a large impact on congestion. To guarantee that
we sample the correct number of these flows, we separate flows
into large (elephants) and small (mice) and sample exactly a
proportion α of each.
Scaling the system is slightly different from Section 2. As
before, we multiply by α the link speeds. However, we do
not scale the buffer sizes or the RED thresholds. Further, we
multiply by α−1 the propagation delay of each flow4 . We will
elaborate on the intuition and theory behind these choices after
we present the simulation results.
Since we sample flows which arrive at random times and
have random sizes, quantities like the queueing delay cannot
be expected to scale as functions of time. However, simulations and theory show that we can exhaustively compare the
distributions of related quantities.
We run the experiments for scale factors α = 1 and 0.1, and
compare the distribution of the number of active flows as well
as the histogram of the normalized delays of the flows in the
original and the scaled system. (The normalized delays are
the flow transfer times multiplied by α.) We will also compare
more detailed performance measures such as the distribution
of active flows that are less than some size and belong to a particular group. Due to limitations of space we will not present
results when the links are uncongested, but only compare distributions for the more interesting case where drops occur.
(The performance of uncongested networks also scale.) The
flow arrival rate is set to be 60 flows/sec within each group.
The results don’t depend on whether the rates are larger or
smaller.

2.2 DropTail
While all the simulations above show the validity of SHRiNK,
the scaling behavior does not hold when we change the queue
management scheme to DropTail. Figure 6 shows the average
queueing delay at Q2. Clearly, the queueing delays for different scale do not match. This scheme drops all the packets that
arrive at a full buffer. As a result, it could cause a number of
consecutive packets to be lost. These bursty drops underlie the
reason the scaling hypothesis fails in this case [11]. Besides,
when packet drops are bursty and correlated, the assumption
that packet drops occur as a Poisson process (see [8]) is violated and the differential equations become invalid.

2.3 Summary
Besides the examples we have studied in this section, we have
also validated SHRiNK with heterogeneous end-systems (TCP,
general AIMD and MIMD protocols, UDP, HTTP), with a variety of active queue management policies such as the PI controller [6] and AVQ [7], with a range of system parameters, and
with a variety of network topologies (tandems, stars, meshes).
We have found that, in cases where TCP-like flows are longlived and drops are not bursty, basic performance measures
such as queueing delay are left unchanged, when we sample
the input traffic and scale the network parameters in proportion.

3. SCALING BEHAVIOR OF IP NETWORKS
WITH SHORT AND LONG FLOWS
It has been shown that the size distribution of flows on the
Internet is heavy-tailed [14]. Hence, Internet traffic consists of
a large fraction of short and and a small fraction of long flows.
It has been observed that sessions arrive as a Poisson process3 .
In this section we take these observations into account and

Simulation Results
We will start by comparing distributions when RED is used.

3
Further, for certain models of TCP bandwidth sharing, the
equilibrium distribution of the number of flows in progress is
as if flows arrive as a Poisson process, not just sessions [4].

ACM SIGCOMM Computer Communications Review

4
One should also multiply by α−1 the various protocol timeouts. In practice, since it is very rare for a timeout to expire,
leaving timeouts unscaled does not affect the results.

38

Volume 33, Number 1: January 2003

0.016

is used.
0.12
α=1
α = 0.1

0.012

0.1
0.01

Proportion of Flows

Probability of i Active Flows

0.014

0.008
0.006
0.004

0.08
α=1
α = 0.1
0.06

0.04

0.002
0
0

0.02
50

100

150

200

250

i

0
0

Figure 7: Distribution of number of active flows on
the first link.

500

1000
α Delay (msec)

1500

2000

Figure 9: Histogram of normalized delays of grp1 flows
when DropTail is used.
Figure 7 plots the distribution of the number of active flows
in the first link between routers R1 and R2. It is evident from
the plot that the two distributions match. A similar scaling
holds for the second link.

Figure 9 plots the histogram of the flow transfer times of the
flows of grp1 multiplied by α, when routers employ DropTail.
The distributions of the normalized delays match as before.
Although not shown here, the distribution of the number of
active flows also scales under DropTail.

0.08
0.07

0.035

α=1
α = 0.1

0.05

0.03
Probability of i Active Flows

Proportion of Flows

0.06

0.04
0.03
0.02
0.01
0
0

Figure 8:
flows.

500

α Delay (msec)

1000

α=1
α = 0.1

0.025

0.02

0.015

0.01

1500
0.005

Histogram of normalized delays of grp0

0
0

40

60
i

80

100

120

Figure 10: Distribution of number of active grp2 flows
with size less than 12 packets.

Figure 8 plots the histogram of the flow transfer times (delays) of the flows of grp0 multiplied by α. To generate the
histogram, we use delay chunks of 10
α ms each. There are 150
such delay chunks in the plot, corresponding to flows having a
10
20
delay of 0 to 10
α ms, α ms to α ms, and so on. The last delay
chunk is for flows that have a delay of at least 1500
ms. It is
α
evident from the plot that the distribution of the normalized
delays match. The results for the other two groups of flows
are also the same. The peaks in the delay plot are due to the
TCP slow-start mechanism. The left-most peak corresponds
to flows which send only one packet that face no congestion,
the portion of the curve between the first and second peaks
corresponds to flows which send only one packet but face congestion (but no drops), the next peak corresponds to flows
which send two packets and face no congestion, and so on.
We will now investigate if distributions scale when DropTail

ACM SIGCOMM Computer Communications Review

20

What about more detailed performance measures? As an example, we compare the distribution of active flows belonging
to grp2 that are less than 12 packets long. The AQM scheme
used at the routers is RED (DropTail behaves similarly). Figure 10 compares the two distributions from the original and
scaled system. Again, the plots match.

3.2 Theoretical support
The above results can be theoretically supported. First, consider a simplified model: suppose that flows arrive as a Poisson
process, and that the service time for each flow is independent
and drawn from some common distribution (perhaps heavytailed). This is known in queueing theory as an M/GI model.

39

Volume 33, Number 1: January 2003

bution of performance measures under any network topology,
active queue management mechanism, and transport protocol
remains unchanged. We have shown these results using simulations and theory.
Further work consists of validating SHRiNK in large experimental testbeds, obtaining a better understanding of the theory, and trying to extend the approach to web-server farms
and to wireless networks.

Suppose also that the service capacity of the link is shared between currently active flows according to the classic equation
for TCP throughput. (We will shortly extend the argument to
allow for a detailed packet-level model of the link.) This is the
sort of flow-level model used in [4].
Let J(t) be the number of jobs in this system at time t.
Now scale the process of arriving flows, by multiplying flow
interarrival times by 1/α. Also, multiply the service capacity
of the link by α. It is not hard to see that the scaled system
looks exactly like the original system, watched in slow motion.
˜ is the number of jobs in the scaled system
Specifically, if J(t)
˜ = J(αt).
at time t, then J(t)
Suppose that instead of stretching the flow arrival process
in time we had sampled it, retaining each flow independently
with probability α. It is a simple but far-reaching property
of the Poisson process that these two processes have exactly
the same distribution. In particular, if Jˆ(t) is the number of
jobs in the system which is scaled by sampling, then for each
ˆ and J(t)
˜ have
t (assuming the queues are in equilibrium), J(t)
the same distribution, which is the distribution of J(t). That
is, the marginal distribution of the number of jobs is the same
in the two systems.
This argument does not in fact depend on how the link
shares its bandwidth. It could be first-come-first-served, or
use priorities. More interestingly, let us instead model the behavior of the link as a discrete event system. Suppose that
flows arrive as a Poisson process as before, and that they arrive with a certain number of packets to send, independent and
with a common distribution. Consider a time-line showing the
evolution in time of the discrete event system. How could we
scale the parameters of the system, in order to stretch out the
time-line by a factor 1/α?
To be concrete, suppose that α = 0.1, and that a flow with
12 packets takes 1 second to transfer in the original system. We
would like to ensure that its transfer time in the scaled system
is 10 seconds. We can do this by making sure that each of
the 12 packets takes 10 times as long to transfer in the scaled
system. Now, the transmission time of a packet is the sum of
its queueing delay and propagation delay. We can multiply the
queueing delay by 10 by reducing the link speed by a factor of
10; we should also multiply the propagation delay by 10.
In general, we should multiply propagation times by 1/α,
and the service times by the same factor, which means multiplying the service rate by a factor α. As before we would
multiply flow interarrival times by 1/α, which has the same effect as sampling with probability α. Note that this model takes
account of retransmissions due to drops (assuming either there
are no timeouts, or that the timeout clock is also scaled). Note
also that the packet buffer at the link is not scaled. The conclusion holds just as before: the marginal distribution of the
number of jobs in the system is unchanged.

5. REFERENCES
[1] K. Claffy, G. Polyzos, and H.-W. Braun. Applications of
sampling methodologies to network traffic
characterization. In Proceedings of SIGCOMM, 1993.
[2] C. Estan and G. Varghese. New directions in traffic
measurement and accounting. In Proceedings of ACM
SIGCOMM Internet Measurement Workshop, 2001.
[3] Fluid models for large, heterogeneous networks.
http://www-net.cs.umass.edu/fluid/, accessed
January 2002.
[4] S. Ben Fredj, T. Bonalds, A. Prutiere, G. Gegnie, and
J. Roberts. Statistical bandwidth sharing: a study of
congestion at flow level. In Proceedings of SIGCOMM,
2001.
[5] C.V. Hollot, V. Misra, D. Towlsey, and W. Gong. A
control theoretic analysis of RED. In Proceedings of
INFOCOM, 2001.
[6] C.V. Hollot, V. Misra, D. Towlsey, and W. Gong. On
designing improved controllers for AQM routers
supporting TCP flow. In Proceedings of INFOCOM,
2001.
[7] S. Kunniyur and R. Srikant. Analysis and design of an
Adaptive Virtual Queue (AVQ) algorithm for active
queue management. In Proceedings of SIGCOMM, 2001.
[8] V. Misra, W. Gong, and D. Towsley. A fluid-based
analysis of a network of AQM routers supporting TCP
flows with an application to RED. In Proceedings of
SIGCOMM, 2000.
[9] T. Ott, T. Lakshman, and L. Wong. SRED: Stabilized
RED. In Proceedings of INFOCOM, 1999.
[10] R. Pan, B. Prabhakar, K. Psounis, and D. Wischik.
Shrink: A method for scalable performance prediction
and efficient network simulation.
http://www.stanford.edu/~kpsounis/scale1.html,
accessed October 2002.
[11] R. Pan, K. Psounis, B. Prabhakar, and M. Sharma. A
study of the applicability of the scaling-hypothesis. In
Proceedings of ASCC, 2002.
[12] S. Shakkottai and R. Srikant. How good are
deterministic fluid models of internet congestion control.
In Proceedings of Infocom 2002, to appear, 2002.
[13] J. Walrand. A transaction-level tool for predicting TCP
performance and for network engineering. In MASCOTS,
2000. http://walrandpc.eecs.berkeley.edu/Papers/
mascots1.pdf.
[14] W. Willinger, M.S. Taqqu, R. Sherman, and D.V.
Wilson. Self-similarity through high-variability:
Statistical analysis of ethernet lan traffic at the source
level. IEEE/ACM Transactions on Networking,
5(1):71–86, 1997.

4. CONCLUSION
In this paper we have presented a method, SHRiNK, to reduce the complexity of network simulations and performance
prediction. Our main finding is that when a sample of the network traffic is fed to a suitably scaled replica of the network,
performance measures of the original network are accurately
predicted by the smaller scale replica. In particular, (i) when
long-lived flows arrive in clusters, queueing delays and drop
probabilities in the two networks are the same as a function
of time in many interesting scenarios, and (ii) when flows arrive at random times and their size is heavy-tailed, the distri-

ACM SIGCOMM Computer Communications Review

40

Volume 33, Number 1: January 2003

The Undismissible Rats: How DoShort
Connections Affect Long-Lived TCP Flows
under Moderate Traffic Load?
Rong Pan
Stanford University
Stanford CA 94305, USA
rong~stanford.edu

Abstract. lt is well-known that the distribution of file sizes in the Internet has a long tail, and that the traffic mainly consists of small flows
- "the mice", while a large portion of the bytes are sent by large flows
- "the elephants". However, it is not yet well understood regarding how
the mice and the elephants interact with each other when they share a
common link. A simplified modeHing assumption is t hat the mice appear
as an uncontrolled and random background load while the elephants take
the remairring link bandwidth [7].
In this paper, we find that under moderate traffic load, short connections,
particularly large-sized mice whom we refer to as "rats", have a significant impact on the throughput of long-lived TCP flows . The occurrence
of rats, who are in the slow-start phase of the TCP protocol, impair the
ability of long-lived TCP flows to grab a link's available bandwidth. Our
analysis shows that the throughput of the long-lived TCP flows is a function of the rats' arrival rate; and since the file sizes in the Internet are
Pareto-distributed, this arrival rate is actually a function of the traffic's
shape parameter.

1

lntroduction

Studies have found that the file size distribution of Internet traffic, 90% of which
uses the TCP protocol, has a heavy-tail [2], [4], [9]. In simple terms, it means that
most TCP connections are "mice" (with short lifetimes, still in the slow start
phase), but a few large TCP flows referred to as "elephants" (with long lifetimes,
in congestion avoidance phase) generate most of the traffic. This behavior, known
as "the elephants and mice phenomenon" is considered to be one of t he few
invariants of Internet traffic.
The Pareto distribution is shown to be an accurate model to describe the
heavy-tailed distribution of file sizes [4]. The P areto distribution has a cumulative
distribution function as follows:

F(x ) = 1 -

( ~)a
X

for x :2: b,

N . Mitrou et a.l. (Eds .): NETWORKING 2004 , LNCS 3042, pp. 949-961 , 2004.

@ IFIP Interna.tiona.l Federation for Information Processing 2004

(1)

950

R. Pan

where a is the shape parameter which determines the tail behavior and b is the
scale parameter. The expected value of a Pareto-distributed random variable
equals to E(x) = b(a- 1)-1 when a > 1.
Separately, the performance of long-lived TCPs has also been well-studied
[5], [7], [10], [11] . Previous work has shown that the performance of long-lived
TCP flows depends on their round trip time (RTT) and packet loss probability.
Assuming that the packet loss probability, p, is not too high and the receiver's
window is not limited, the throughput Th(p) of a long-lived TCP flow is given
by
c
Th(p) ~ RTT fo

(2)

where c is a constant. This equation plays a fundamental role in determining a
long-lived TCP flow's throughput when the traffic's arrival rate .X is relatively
high. In this case, p is independent of any particular flow's behavior. Given p,
a flow with a Ionger RTT will get lesser throughput than a flow with a shorter
RTT. However, we note that pisnot always independent of Th, especially when
an elephant flow dominates a link's buffer space. For example, in the extreme
case that only one long-lived flow is present on a link, Th will be equal to the
link capacity C since the TCP protocol has the ability to fully utilize a link;
and this is true regardless the value of RTT as long as there is enough buffering
at the link. Here, Equation (2), while being true, only shows a secondary effect
through which the drop probability can be obtained.
What kind of a role does Equation (2) play when the traffic mix consists of
mice and elephants? Does the Pareto distribution play any role? It is not yet
understood how the mice and the elephants interact with each other when they
share a common link, i.e. how the bandwidth is distributed among the mice and
the elephants. This task becomes more intricate because of the following fact:
flows arrive and leave at random times; hence, the number of concurrent flows
are varying over time. A modelling assumption, suggested by Kelly [7], is that the
mice appear as an uncontrolled and random background load while the elephants
take the remaining link bandwidth. This assumption is quite simplified; Kelly
later indicates it might need more investigation [8].

. Rl

0.0

R2

rl

R3

r2

1.0

Fig. 1. Regions of Operations
This paper sturlies how the mice affect the performance of the elephants. We
distinguish the following three regions that a long-lived TCP flow might operate
in, shown in Figure 1. In Region R1, the link is lightly loaded. The boundary r1

The Undismissible Rats

951

could be around 10%. 1 Here, Kelly's assumption is valid. A long-lived TCP would
dominate the behavior of the buffering queue and take most of link 's bandwidth.
Conversely in Region R3, the link's utilization is high. The boundary r2 could
be around 70%. 2 In this region, since the aggregated arrival rate >. is high, no
single fiow will dominate. Hence, the drop probability only depends on the mixed
arrival rate of mice and elephants. Given p, a long-lived TCP flow's throughput
can be found using Equation (2). However, in Region R2, which is the typical
region of operation for a network link, it is not yet understood how the mice
and elephants share the link bandwidth. Our effort is focused on this normal,
less-understood operating region.
The main contribution of this paper is to show that in Region R2 where a
link is moderately loaded, the mice, especially those large-sized mice whom we
refer to as "rats", play a rather negative role in sharing bandwidth with other
TCP fiows. These rats are big enough to just exit the slow-start phase of the
TCP protocol, but not big enough to go through the saw-tooth like congestion
avoidance. As a result, they undermine the ability of long-lived TCP fiows to fill
up a link's available bandwidth, one of the major strengths of the TCP protocol.
Assuming that all fiows share a common FIFO queue, 3 our analysis shows that
the presence of rats causes extra delays and packet Iosses at the FIFO queue;
and therefore force a long-lived TCP fiow to back off and reduce its sending rate.
As a result, the throughput of a long-lived TCP fiow becomes a function of the
rats' arrival rate Arats as follows

(3)
Since we know that the file sizes are Pareto-distributed and recent studies have
shown that Internet traffic can be modeled as a stationary stochastic process [1],
we can rewrite Equation (3) as a function of the traffic's shape parameter a:
Th

1.5. ( b )- a:
:::::: RTT2 >. X
'

(4)

where >. is the traffic arrival rate and X is the minimum size of a rat. Equation
(3) and (4), derived later in this paper, explain the role played by the mice in
Region R2. Once again, they do not contradict the well-known Equation (2),
which is valid in all regions. However, they do point to the fact that, in region
R2, the drop probability becomes dependent on a fiow's own behavior, i.e. Th,
in this case.
We first study how mice and elephants interact with each other in Section 2,
in which we introduce the notion of "rats". Section 3 uses a first-order mathe1

2
3

We only use these boundaries as a generat guideline, the numbers chosen might not
be accurate.
Network operators start considering an upgrade to their equipment when a link's
average utilization is above 70%.
There are proposals to buffer the mice and the elephants separately with some extra
cost (3]. We do not study them in this paper.

R. Pan

952

matical model to analyze the role played by the rats. Then, we verify our findings
using real trace statistics. We conclude in Section 4.

2

Interaction of Mice and Elephants

To gain insight into how short and long lived TCP flows share a link's bandwidth,
we run a number of simulation experiments using the ns2 simulator [13].

2.1

Simulation Study

The simulation setup consists of one bottleneck link shared by multiple flows.
The link uses a buffer that can buffer up to 250ms worth of packets, which
means any flow with an RTT of less than 250ms has the ability to fully utilize
a link. Note this implies that the size of the buffer is different for different link
speeds. All packets are 1000 bytes long. Allconnections have a maximum receiver
window of 1000 packets so that a receiver window is not a limiting factor for
a flow's throughput. Also, a recent version of the TCP protocol, TCP /Sack, is
used throughout this paper.
The Basic Setup. In the basic setup, the bottleneck link bandwidth C is
15Mbps and the link uses a droptail buffer with a size of 450 packets. Web
sessions have a Pareta distribution with parameters a = 1.25 and b = 6.25.
Hence, the average file length, E(x) = b(a -1)- 1 , is 25 packets.

I

r--

·-·

t--- ·

.j

-·

- I-

·--+-

I

-

,_
... ltl l- ttJ~

f-

...

"

-l

Fig. 2. Trafik

... -ut

!
~

f

I

..........

00

~

o•

l

02

~

--

~!lill,i.CJpnory -

00

o
0

°t

0 tSl

1

-r I
0

1

0

=!.
~-

0~~--~--~--~--L-~

20000

40000

eo:,QO

FkJw SQ•

10000

(~ r ~

100000

, iZOOOO

, 40CIOCJ

PolltebtJ)

Fig. 3. Basic Setup: Long-lived TCP's
Throughput

In the first experiment, the traffic of 100,000 web Sessions consisting of mice
and elephants, arrives randomly with an average demand of 5.3Mbps as shown in
Figure 2. The mice have RTTs of either 30ms or 190ms (evenly distributed); and
the elephants have an RTT of 190ms. The reason for the mice to have different
RTTs is to reduce the phase effect under the droptail queue [6]. The rationale
for the long-lived TCP flows having the same RTT is the following: since the

The Undismissible Rats

953

link is moderately loaded, the degree of statistical multiplexing is not high. As
a result, only a few elephants occur randomly in this simulation setup as shown
in Figure 2.4 It is easier and more accurate to calculate the average throughput
of long-lived TCPs when they have the same RTT.
In this basic setup, the average traffic demand .X is 5.3Mbps. Hence the link
load, p =
is around 35%. According to statistical bandwidth sharing [1] ,
the throughput of long-lived TCP flows, which use all the capacitynot used by
other flows, should be approximately equal to the residual capacity C(1 - p)
at the bottlerreck link. However, Figure 3 shows that the throughputs actually
achieved by the long-lived TCP flows, whose sizes are over 30,000 packets, are
below the expected value. This figure illustrates that while the link has 65% of
its bandwidth available, the long-lived TCP flows fail to fully utilize it, yielding
an average throughput of 6.2Mbps, only 41% of the link capacity.
If we vary the arrivalrate of the traffic, the behavior of long-lived TCP flows
remain similar. We plot the average link utilization when at least one long-lived
TCP ftow is present in Figure 4. 5 One would expect the link to be 100% utilized
since with enough buffering a long-lived TCP flow has the ability to use up
a link's available bandwidth regardless of its RTT and the link's t raffic load
p. However, as shown in Figure 4, the long-lived TCP flows fail to fill up the
available bandwidth when the link is under a moderate load except when the
RTT is very short, such as 30ms in the plot. The worst link utilization (when
long-lived flows present) goes down quickly with increasing RTTs (from 90ms to
230ms).
Clearly, the link is not always fully-utilized in Region R2 when long-lived
T CP flows are present: in the worst case, the link's utilization is only around
60%. The plot also points out an interesting behavior: the link utilizat ion goes
down initially when p starts to enter Region R2 ; stays low in Region R2; and
increases again when the traffic demand is suchthat the link enters the congested
region, Region R3. This figure demonstrates the fact t hat a long-lived TCP flow,
under moderate traffic demands, could lose its strength t o fully utilize a link.

ff,

Link with a Higher Speed. Suppose we increase t he link speed to 20Mbps
and the buffer space to 600 packets (still 250ms worth of buffering), while keeping
the same traffic arrivals as in Section 2.1. Figure 5 depicts that, once again, the
long-lived TCP flows can fail to capture the available link bandwidth in Region
R2. The link utilization shows a similar, U-shaped behavior.
RED Queue instea d of DropTail Queue . Suppose we alter t he basic setup
again by changing the queue management scheme t o RED while keeping the
4
5

Although some of them might overlap a bit during their life-time, they are not on
and off at the exact same time.
Note the link utilizat ion plotted here is the average link utilization when at least one
long-lived TCP flow is present. lt is not the overall, average link utilization over the
entire simulation time. With p = 0.35, the average link utilization over the ent ire
simualtion is approximately 0.35 as weil.

954

R. Pan

RTT•toms ···• ···
!fTT ll,;Otr>.,-..ßTT • 23(1ms

~-a w.

• .• '----'---'----'-...___.___._..:.....:."---'.......:...-l

",'::-,~••::-----c.~,---7,-. -.""=.~,-=-.---'!':,,c..:..:,"".:::.....,.:--',

01

oz

03

O<~

o..s

•

oe

o7

oe

o.t

,

Fig. 5. With A Faster Link: Average
Link Utilization Under Long-lived TCP
Flows

Fig. 4. Varying Traflic Demands: Average Link Utilization Under Long-lived
TCP Flows
;;
~

~
~

0.95

ä:::

0.9

Z"'
:::>~

0.85

·~g

G'J:3
,.,.

__;;~
".__.

0.8

--'0

""'
'""
~~

0.75

<::!

0.7

<
"

0.65

j

0

~

0.6

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

Fig. 6. With A RED-Queue: Average Link Utilization Under Long-lived TCP Flows

other parameters the same. The minth and maxth are set to be 150 and 300
packets. Figure 6 shows that the dynamics observed in the previous simulations
does not change much when we use RED as the queue management scheme. This
indicates that the phenomenon we observed might not be related much to the
average queue length, rather it is related to the instantaneous queue length.
Mice with Different Round Trip Times. If the mice have shorter or Ionger
RTTs, would it affect the throughput of long-lived TCP flows? We now let the
mice have two different round trip times with equal probability, RTTm and
190ms, where RTTm is a variable. We keep the RTT of 190ms for those flows
whose sizes are above 30,000 packets. Table 1 shows that the average throughput of long-lived TCP flows depends little on the RTTs of mice. This behavior
is rather different from that of a TCP flow operating in Region R3, where a flow
can gain more bandwidth when competing against flows with shorter RTTs or
lose bandwidth when competing against flows with Ionger RTTs as implied in

The Undismissible Rats

955

Table 1. Average Throughput of Long-lived TCP flows as a Function of Mice's RTTs

II
II

RTTm
Throughput

I 30ms I 70ms I llOms I 150ms I 190ms I 230ms II
I 0.418 I 0.419 I 0.440 I 0.437 I 0.464 I 0.454 II

Equation (2). This serves as another indication that the TCP protocol experiences a different throughput-limiting factor.

Summary. We have also run other simulations such as traffic with a different
shape parameter or two-way traffic which are omitted here. All the simulation
studies demonstrate that, under a variety of seenarios in Region R2, a long-lived
TCP flow can fail to capture a link's available bandwidth even though it has the
ability to fully utilize a link when it is alone, i.e. when no mice flows are present.
Here, neither Kelly's assumption nor Equation (2) can help explain the behavior
of long-lived flows. The studies also show that the rates of mice flows, not the
round trip times of mice flows , play a role in limiting a long-lived TCP flow's
throughput.
2.2

A Key Observation

In the congestion-avoidance phase, a long-lived TCP flow regulates its window
size according to network conditions which are directly related to the traffic load
on a link. To examine the effect of the Internet traffic arrivals on the window
size, we run a simulation where the flows whose sizes are over 30000 packets in
the basic setup are combined into one single FTP flow since this "long" FTP
flow shows the evolution of the window size more visibly than those scattered
long-lived flows in the original setup.

Fig. 7. Window Size vs. Traffic Arrivals

Fig. 8. Window Size Comparison

The solid line in Figure 7 represents the window size of the FTP flow. The
dotted lines indicate the arrivals of mice whose sizes are above 750 packets. 6
6

Note that the heights of the dotted lines in Figure 7 have no meaning.

956

R. Pan

The plot illustrates that the time when the FTP flow cuts its window mostly
coincides with the time when there is an arrival of a mouse whose size is above
a few hundred packets. The arrivals of these big mice hinders the growth of
the FTP flow's window. Hence, the throughput of the long-lived TCP flow becomes constrained. As a result, the FTP flow fails to capture the available link
bandwidth.

2.3

The Notion of "Rats"

Since flows with a few hundred packets are significantly bigger than the common
concept of "mice" - who send only a few packets, we introduce a new notion of
"rats" to represent those flows whose sizes are big enough to disturb long-lived
TCP flows, but small enough to avoid the saw-tooth type of window adjustment
that a long-lived TCP flow would go through. The spike in Figure 8 shows an
example of a rat's window size in the above simulation. If we remove rats and
distribute their traffic load to flows under 600 packets (i.e. maintaining the same
traffic arrival rate >.), the window size of the FTP flow becomes much !arger, as
shown in Figure 8. This serves as another evidence of the influence of rats. Since
Internet traffic is Pareto-distributed, the rats are bound to occur. In other words,
an elephant, whose life time is long, is certain to suffer from the disturbances
caused by rats.

2.4

Discussion

One should be aware that the above observation does NOT indicate that longlived TCP flows or small mice would not cause losses. As a matter of fact, they
would. However, these drops occur less frequently as compared to the drops
caused by rats. Long-lived TCP flows occur scarcely under a moderate traffic
demand as shown in Figure 2. Although these flows may overlap over a small
period of time, the effect they have on each other is less than the effect brought
by rats which arrive much more frequently. These rats, in the slow start phase,
send many packets in one RTT time, and will overflow the buffer and cause
darnage to all other flows sharing the same link. Note that, here, the slow start
threshold, ssthresh, is not a limiting factor since its intial value is set to be
very high in most TCP implementations. For example, in Linux kernel 2.5.73,
ssthresh is initialized to be Ox7fffffff, which is practically infinity. Mice may
cause drops as well, but because their sizes are small they can not overflow the
buffer by themselves unless the buffer is already almost full when they arrive,
which is rare since the link is under a moderate traffic demand.

3

Modeling and Analysis of Rats

This section develops a first-order mathematical model for analyzing t he effect
of rats on long-lived TCP flows. The model shows that the throughput of a longlived TCP flow depends on the arrival rate of rats, Arats, and since the Internet

The Undismissible Rats

957

traffic is Pareto-distributed, Arats can be shown to be a function of the traffic's
shape parameter a.

3.1

Model

In our model of long-lived TCP's throughput, we assume a network setup as
follows: an infinite long TCP connection is sharing a link with rats which arrive
regularly. We also assume that the data buffer overfiows and packets are dropped
whenever a rat arrives. Note that we don't take into account the losses that are
caused by mice or other competing long-lived TCP fiows because these losses
are rare compared to the losess caused by rats as discussed in Section 2.4.
Although multiple packets might be dropped within a window, TCP /Sack
only cuts down its window by half instead of going into timeout. Hence one rat's
arrival is equivalent to a single drop event. After a drop event, the TCP sender
increases its congestion window by one packet per round trip time, until the next
rat arrives.

TCP throughput: a function of >-rats· Suppose the inter-rat arrival time
is X;a1ts> and the long-lived TCP connection has a rather constant round-trip
time of RTT seconds. Each time a rat arrives, the TCP sender has a congestion
window of W max packets as shown in Figure 9.

D

Acked Packet

~

Lost Packet

RTTi
No. of Rounds

Fig. 9. Gongestion Window Adjustment

Fig. 10. Evolution of Window Size

By cutting its window by half for each rat's arrival and increasing its window
by one per round-trip time subsequently, the TCP sender goes through 0.5Wmax
round-trip times for each drop event, i.e. each rat 's arrival. Hence, the following
equation holds:

Wmax

-2- *

RTT

>,-1

= rats =}

W

2

max = RTT Arats .

958

R. Pan

Since the average window size in a saw-tooth period equals 0.75Wmax, the
throughput of a long-lived TCP, Thtcp, can be expressed as follows:

Th

_ 0.75Wmax
1.5
tcp RTT = RTT2Arats .

(5)

TCP throughput: a function of the shape parameter a. As discussed
in Section 1, previous work has shown that sessions arrive in the Internet as a
Poisson process [1] and the file sizes in the Internet are Pareto-distributed [4].
The probability that a fiow is a rat equals

Prat=(sb )" - (sb
r~

de

)"~(Sb)",

(6)

r~

where Brat is the smallest size of a rat and Sele is the smallest size of an elephant. Since the Pareto-distributed file sizes imply that the probability is much
higher for a fiow being a rat than being an elephant, the above Equation can
approximated by ( s:at )".
Given that the traffic arrives as a Poisson process of rate >.., we can obtain
that the rats also arrive as a Poisson process with a rate of Arats ~ >..( -8 rat
b )".
Combining Equation (5) and Arats, we obtain that

Th

1.5 ( b )-<>
tcp ~ RTT2 >.. Brat
.

Certainly, a long-lived TCP fiow's throughput is also constrained by the
available link bandwidth, C(l- p). Hence we can find the throughput of a longlived TCP fiow in Region R2 as

1.5 ( b )-<> )
Th tcp ~ min ( C( 1- P) ' RTT2>..
Brat
.

(7)

Note that the above model does not t ake into account the throughput lasses due
to time out.
The size of rats. Generally, rats are defined in this paper to be any connections which can cause the long-lived TCP fiow to cut its congestion window size.
Despite this loose definition of rats, we do follow a guideline in calculating the
arrivalrate of rats. We know that a rat, like any TCP fiow, increases its congestion window geometrically in the slow start phase. For the ith round trip time,
RTTi, the congestion window size Wi is 2i. The total number of packets sent up
to RTTi equals

L 2m= 2i+l -

1 ~ 2i+l.

(8)

m=O

This equation means that, if a fiow has N packets to send and it incurs no drop
before the last round, then approximately N /2 are sent in the last round alone.

The Undismissible Rats

959

Given that N /2 is close to the buffer size B at an output link, the buffer is
bound to overflow and some packets are lost. The lost packets are then resent.
The evolution ofthe window size is illustrated in Figure 10. Although it normally
takes a burst size of less than B packets to overflow a buffer since other flows
take up buffer spaces as well, N ~ 2B serves as an upper-bound estimate of a
rat's minimum size. The theoretical analysis of a rat's minimum size is left for
future work.

3.2

Verification

We apply the analytical model to the simulations in Section 2. Table 2 shows
the comparison results. Recall that C is the bottlerreck link capacity and A is
sessions' arrival rate. The traffi.c load, p, equals the traffi.c arrival rate divided by
the link capacity, and U represents the link utilization when at least one longlived TCP flow present. The buffer size, B, is set to be the bandwidth-delay
productof the output link: 450 packets for a 15Mbps link and 600 packets for
a 20Mbps link. The table clearly demonstrates that, under various traffi.c and
network scenarios, our theoretical model is able to predict the performance of
long-lived TCP flows accurately.
The model can not only predict a long-lived TCP flow's performance but also
explain other phenomenon that we observed in Section 2: using a RED queue
or varying mice's RTT times does not change the TCP flows' performance.
Since a rat is in the slow-start phase of exponentially expanding its window
size, the sudden increase of its packet burst size would not change much the
average queue length of the buffer at the bottlerreck link. Therefore, packets are
dropped because the buffer overflows, not because the average queue length is
over a certain threshold so that RED needs to preemptively drop packets. As
a result, the RED queue behaves like the droptail queue. Separately, we know
from Equation (7) that the arrivals of mice only contribute to the traffi.c load, p,
and therefore their RTT values have little effect on the elephants' performance.
Another interesting fact that the model can help explain is how a flow's
throughput depends on its RTT. When a link is congested, the throughput of
a long-lived TCP flow is inversely proportional to its RTT as in Equation (2).
When a link is moderately loaded, however, the dependency on RTT is quite
different. When the RTT value is small, the long-lived TCP flow's throughput is
limited by the available link bandwidth, C(l- p), as t he model predicts, which
is consistent with what we have seen in Figure 4 when RTT = 30ms. When
the RTT value is big, the long tcp flow's throughput is inversely propotional to
RTT2 . The drop in a flow's throughput is more drastic as shown in Figure 4.

3.3

Real Traffic Statistics

In this section, we demonstrate that the phenomenon indeed still exists when
we use real traffi.c statistics. We obtained traces of traffi.c from backhone routers

960

R. Pan
Table 2. Theory vs. Simulation: a

II

C

I

15M
15M
15M
15M
15M
15M
15M
15M
15M
20M
20M
20M

>.

26 .7
26.7
26.7
26.7
26.7
33.3
33.3
50
50
26.7
33.3
50

I

p
0.356
0.356
0.356
0.356
0.356
0.444
0 .444
0.667
0.667
0.267
0.333
0.5

IB
450
450
450
450
450
450
450
450
450
600
600
600

= 1.25

ISratsiU(Sirnulation)ju(Model)l RTT
750
750
750
900
900
900
900
900
900
1200
1200
1200

II

30ms
90ms
150ms
190ms
230ms
190ms
230ms
190ms
230ms
190ms
190ms
190ms

1.0
0 .96
0 .83
0.72
0.61
0.74
0.65
0 .82
0.79
0.73
0.70
0.74

1.0
1.0
0.86
0.74
0.61
0.76
0.64
0.84
0.76
0.69
0.67
0.70

at two different locations: New York and Philadelphia. 7 It has been studied that
these traces have a heavy tail and the arrival process of sessions is Poisson. We
applied the sampled trace statistics to the access link of 15Mbps. The average
arrival rate to the access link is 4.48Mbps, which is equivalent to a traffi.c load
of0.3.

i'"

!.
<.l

i

...

0.8

""c

:::l

s

~

0.6

"'

~

Q.

0
0.4

~

1

0

0

."

"

Theory-

SimulatiOn

02

00

0~~~~~~~~~~~~~~--~

20000

40000

60000

80000

100000 120000 140000 160000 180000

Flow Size (Numbe• ol Packeis)

Fig. 11. Trace 1: Throughputs of Long-lived TCP fiows

Figure 11 shows the simulation result using the traffi.c statistics of one trace.
The circles in the figure represent the throughputs of long-lived TCP flows whose
sizes are bigger than 30,000 packets. It is clear that these long-lived TCP flows
fail to capture the available link bandwidth of 70%. The straight line in the plot
shows our model prediction. Since there lacks enough data to fully characterize
the shape parameter a for these traces, we measure the arrivalrate of rats Arats
directly from the sampled trace and apply Equation (5). The plot depicts that
the simulation result using real traffi.c statistics, with Arats = 0.062 , matches
well with the analytical result. Most importantly, the plot shows that the phe7

Due to the proprietary nature of these traces, we omit the details about them.

The Undismissible Rats

961

nornenon about the rats affecting elephants indeed exists for the real Internet
traffic: an elephant would encounter multiple rats in its life-time and hence suffer
from their presence.

4

Conclusion

In this paper, we have shown that short flows, especially "rats" , have an important effect on the throughput of long-lived TCP flows. When a link is under
moderate traffic demands, the occurrence of rats impairs the ability of the TCP
protocol to fully utilize a link's available bandwidth, which leads to the link
being under-utilized. We have also presented a theoretical model to explain the
effect of rats. Our analysis shows that the throughput of a long-lived TOP flow is
a function of the rats' arrivalrate Arats· Since the file lengths of Internet traffic
are Pareto-distributed and session arrivals are Poisson, Arats is in fact a function
of the traffic shape parameter.

References
1. Ben Fredj , S. , Bonald, T., Proutiere, A., Regnie, G. , and Roberts, J., "Statistical
Bandwidth Sharing: A Study of Gongestion at Flow Level", ACM SIGCOMM,
2001.
2. GAIDA, "Traflic Workload Overview" and "Graphs of Ames Internet Exchange
Traflic" , http:/ jwww.caida.org, 2003.
3. Ghen, X. and Heidemann, J . "Preferential Treatment for Short Flows to Reduce
Web Latency", Computer Networks, 41(6), April, 2003.
4. Grovella, M. and Bestavros, A. "Self-Similarity in Worlf Wide Web Traffic: Evidence and Possible Gauses", IEEE/ ACM Trans. on Networking, vol.5, no.6, Dec.
1997, pp.835-846.
5. Floyd, S. and Fall, K., "Promoting the Use of End-to-End Gongestion Gontrol in
theinternet ", IEEE/ACM Trans. on Networking, August, 1999.
6. Floyd, S. and Jacobson, V., "On Traffic Phase Effects in Packet-Switched Gateways", Internetworking: R esearch and Experience, V .3 N.3, September 1992, p.l15156.
7. Kelly, F. , "Mathematical modelling of the Internet", Proceedings of the Fourth
International Congress on Industrial and Applied Mathematics, July, 1999.
8. Kelly, F. , "Models for a self-managed Internet", Philosophical Transactions of the
Royal Society, A358(2000), pp.2335-2348.
9. Leland, W ., Taqqu, M., Willinger , W. and Wilson, D. , "On the self-similar nature
of Ethernet traffic", ACM SJGCOMM, 1993.
10. Misra, V., Gong, W. and Towsley, D., "A Fluid-based Analysis of a Network of
AQM Routers Supporting TGP Flows with an Application to RED", ACM SIGCOMM, 2000.
11. Padhye J. , Firoiu, V., Towsley, D . and Kurose, J ., "Modeling T GP T hroughput:
A Simple Modeland its Empirical Validation", ACM SIGCOMM, 1998.
12. Paxson, V . and Floyd, S., "Wide-area Traffic: The Failure of Poisson Modelling",
IEEE/ACM Trans. on Networking, 3(3) , J une 1995, pp226-244.
13. Network Simulator, Version 2. 1b8.

International Conference on Computing, Networking and Communications, Mobile Computing and Vehicle Communications
Symposium

Intelligent Interface Switching
among Heterogeneous Wireless Networks
for Vehicular Communications
Kevin C. Lee, Xiaoqing Zhu, Lillian Dai, Jiang Zhu, Sateesh Addepalli, Rong Pan and Flavio Bonomi
Advanced Architecture & Research, Cisco Systems Inc., San Jose, CA 95134, U.S.A.
{kevinl2, xiaoqzhu, lildai, jiangzhu, sateeshk, ropan, ﬂavio}@cisco.com

Abstract—As future vehicular communications will most likely involve
multiple wireless networks, intelligent interface switching is of essential importance to support various user preferences across different
performance metrics. In this paper, we present a general optimization
framework for solving the interface switching problem, and develop a
ﬂexible and efﬁcient solution based on stochastic dynamic programming
(SDP). Our framework is designed to accommodate different performance
metrics such as data transfer efﬁciency, monetary cost, and interface
switching overhead. Accordingly, the proposed SDP-based policy can
easily adapt its decision based on user-speciﬁed relative importance of
the various metrics. Simulation study conﬁrms the optimality of the SDPbased policy over a range of user preference choices, and shows that it
consistent outperforms several heuristic schemes.
Index Terms—vehicular communications, interface switching, heterogeneous networks, stochastic dynamic programming (SDP)

I. I NTRODUCTION
With the sharp increase of vehicles on roads in the recent years,
driving has not stopped from being more challenging and dangerous.
Roads are saturated, safety distance and reasonable speeds are hardly
respected, and drivers often lack enough attention. Without a clear signal
of improvement in the near future, leading car manufacturers decided
to jointly work with national government agencies to develop solutions
aimed at helping drivers on the roads by anticipating hazardous events
or avoiding bad trafﬁc areas. One of the outcomes has been a novel
type of wireless access called Wireless Access for Vehicular Environment
(WAVE) dedicated to vehicle-to-vehicle and vehicle-to-roadside communications. While the major objective has clearly been to improve the
overall safety of vehicular trafﬁc, promising trafﬁc management solutions
and on-board entertainment applications are also expected by the different
bodies (C2CCC, VII, CALM) and projects (VICS4, CarTALK 2000 [1],
NOW5, CarNet, FleetNet [2]) involved in this ﬁeld.
When equipped with WAVE communication devices, cars and roadside
units form a highly dynamic network called a Vehicular Ad Hoc Network
(VANET), a special kind of Mobile Ad Hoc Networks (MANETs).
In VANETs, vehicles are connected to the Internet through wireless
infrastructures via advanced wireless routers, usually named on-board
units (OBUs). These OBUs operate seamlessly across multiple wireless
interfaces (e.g., WiFi, 3G, and 4G) to different wireless networks (e.g.,
802.11a/b/p, 3G, and 802.16e or WiMax [3]), forming a WLAN/Cellular
type of vehicular network [4] for data communication between invehicle devices (e.g., laptops and smartphone) and the Internet. The
support for many interfaces on the OBUs is called multihoming. This
technology keeps the end-to-end connection alive as a multi-homing node
can leverage multiple access networks through its different interfaces.
It provides numerous technological beneﬁts, ranging from opportunistic
connection, throughput improvement, load balancing, path resiliency, to
fault tolerance.
While much research work have been done on maintaining the end-toend connection using multiple interfaces, this paper focuses on intelligent
interface switching based on OBUs’ requirements of performance and
cost. More speciﬁcally, given the different performance characteristics of
different wireless technologies, what is the desirable switching outcome
for OBUs to offer a user-tailored balance between reducing monetary

978-1-4673-0009-4/12/$26.00 ©2012 IEEE

cost and improving communication quality? For example, consider Alice
and Bob, who are both passengers on a city bus that is equipped with
an OBU with two external wireless interfaces: 3G and WiFi. While the
3G network is largely pervasive, the WiFi network is sparsely deployed.
However, 3G networks charge users a fee for data transfer whereas WiFi
networks are free. Alice and Bob are both connected to the Internet
through the OBU, but Alice is streaming a video for work purposes
whereas Bob is downloading a personal ﬁle. In this case, communication
quality may be more important to Alice than monetary cost, and vice
versa for Bob. Ideally, the OBU should allow Alice and Bob to set
personal preferences regarding various performance metrics, and then
automatically select the best wireless interface for each user based on
their preference speciﬁcation.
In general, the wireless interface selection algorithm can be cast within
a multi-objective optimization framework that incorporates any number
of user-deﬁned performance metrics. The system then attempts to strike
a balance between these terms. By adjusting the weighting factor on each
term, the user or OBU designer can inﬂuence the behavior of the OBU
to achieve the desired tradeoff. Moreover, the weighting factors can be
easily updated to reﬂect the user’s change of interest over time.
In this paper, we present a user-policy driven and multi-factor optimized method for OBU’s wireless interface selection between a 3G
link and a WiFi link. This formulation provides a ﬁrst practical and
elucidating example of intelligent, user-policy driven interface selection
in a vehicular environment. The performance metrics under consideration are monetary cost, data transfer efﬁciency, and interface switching
overhead. The proposed interface selection algorithm takes into account
of future wireless connection possibilities1 through a stochastic dynamic
programming (SDP) framework. The explicit focus on user preferences
and future AP encounter prediction distinguishes our approach from
other work on heterogeneous wireless network handovers or vertical
handoffs. Our formulation, while simple, is directly applicable to real
OBU deployments in the near future where the deployment of road-side
APs is sparse. The formulation can be easily extended to account for
any number of parameters, interfaces, users, performance metrics, and
deployment scenarios.
Simulation study of the proposed SDP-based policy conﬁrms its optimality. It is shown that the scheme can achieve the best tradeoff between
monetary, data, and overhead performance without the computational
demand of exhaustive search. Unlike heuristic schemes which are suitable
for only speciﬁc scenarios, the SDP approach is able to adapt its policy
according to user speciﬁed preferences, thereby consistently achieving
the best tradeoff among all performance metrics. The rest of the paper
is organized as follows. In Section II, we review related work on interface
switching among heterogeneous wireless networks and highlight the
difference of our approach. Section III provides an architecture overview
of our design of the vehicle OBU for hosting the interface switching
algorithm. In Sections IV and V, we explain in detail our system
1 WiFi access point (AP) locations and performance statistics may be known
a priori or can be obtained using crowd-sourcing where AP locations and
performance characteristics are reported to a cloud server by passing vehicles.
Over time, statistics on different APs, together with GPS location of vehicles,
trajectory, and street maps, can be used to predict future WiFi AP encounters.

766

model, as well as how we formulate the interface switching problem
within a stochastic dynamic programming framework. Section VI presents
simulation study of the proposed SDP-based policy. Finally, Section VII
concludes the paper and discusses the future work.

II. R ELATED W ORK
In [5], the authors describe a multi-homed mobile access router (MAR)
for on-the-move Internet access. MAR is capable of aggregating multiple
wireless access links for seamless handoff, throughput improvement and
fault tolerance. While MAR is capable of utilizing different wireless links,
there lacks a comprehensive, user-policy driven interface selection mechanism. Our interface selection algorithm ﬁlls that void. More broadly,
our optimization framework is independent of the underlying mobility
protocol and can be used for MAR or other systems using other mobility
protocols such as Mobile IP (MIP).
In [6], the authors investigate WiFi augmentation of 3G in mobile
environments for data ofﬂoading. Data is transmitted on WiFi instead
of 3G whenever WiFi and predicted future WiFi encounters can satisfy
the delay requirement of the application; otherwise, data is transmitted
on 3G only. This ﬁxed interface selection strategy does not account for
user preferences over different wireless link attributes. Our proposed interface selection algorithm addresses this deﬁciency and enables dynamic
strategy selection. Other work on vertical handoff, such as [7]–[13] and
references cited within, lacks user-speciﬁed preference input, or performs
one-time optimization that fails to account for future AP encounters, or
relies on ﬁxed interface switching strategies.
The closest to our work are [14] and [15]. In [14], the authors propose a
utility maximization framework for interface selection. User location and
trajectory information are used to predict the duration a user stays in the
coverage area of one AP. In contrast to our work, this does not take into
consideration of future AP encounters. In [15], the interface switching
problem is also cast into the Markov decision process formulation. A
value iteration algorithm is applied to compute a stationary policy for
total expected reward maximization. While the spirit of multi-criteria
cost optimization is similar to our work, a key difference lies in when
decisions are made. In their work, it is assumed that 3G and WiFi
periodically send information from collocated coverage areas. In this
way, the interface selection decision is calculated rather frequently at
periodic intervals. In the vehicular environment that we are concerned
with, WiFi APs are sparsely deployed, hence periodic decision-making
becomes impractical. We therefore consider the formation where the
selection is made whenever a WiFi AP is encountered, thereby greatly
reducing computational complexity.

III. A RCHITECTURAL OVERVIEW

Overview of future Vehicle-to-Infrastructure (V2I) architecture.
The presence of road side units (RSUs) and network controllers helps to
bridge the connection gap between clients inside vehicles and data center
services over the Internet.

Fig. 1.

System architecture of the connected vehicle environment. An
on-board unit pools together Internet connections from multiple available
access networks, and serves as a WiFi access point for conventional
mobile devices inside the car.

Fig. 2.

A. System States
As a vehicle drives along, it experiences persistent 3G coverage and
sporadic encounters with WiFi access. As illustrated in Fig. 4, we deﬁne
a stage as the time a vehicle spends between two consecutive WiFi
encounters. Each stage comprises of two sub-stages: the period with WiFi
coverage from a single access point, followed by the period with 3G-only
coverage. We use the subscript n to index a single stage, and characterize

Figures 1 and 2 provide an architectural overview of the connected vehicle environment. We envision the presence of network controllers to act
as proxies between the clients on the vehicles and the data center services
over the Internet. Road side units (RSUs) can be deployed to augment
existing wireless access networks, most likely in a heterogeneous fashion,
involving 3G, WiFi, and/or 4G (WiMax/LTE). Furthermore, the vehicles
will be equipped with OBUs, which are capable of pooling together
connections over multiple wireless network interfaces and dynamically
assigning application packets to one of the available interfaces. The
key components in the OBU are shown in Fig. 3.
•
•
•

The user preference selection module tracks user-speciﬁed preference among various performance metrics for each application ﬂow.
The mobility manager is in charge of providing seamless connection
across various underlying network interfaces.
The interface manager module takes into consideration communication quality over each interface in terms of available bandwidth
and expected duration over that link. It also calculates the optimal
interface switching policy based on the statistics over each access
network, and decides on the ﬂy which interface to use for each
application ﬂow whenever there is a change in the underlying set
of available network interfaces.

Key components of the on-board unit (OBU) for hosting the
interface selection algorithm.

Fig. 3.

767

Bng: 3G bandwidth

interface switching overhead: overhead in terms of disrupted connections or temporary packet drops as a result of switching from
one interface to another.
Our optimization framework accommodates all three metrics by deﬁnm , Rd , and Ro at each stage
ing their corresponding reward functions Rn
n
n
n, as follows. Note that it is straightforward to extend the framework to
accommodate other performance metrics as well.
•

Bnw: WiFi bandwidth

Rate

WiFi
3G
Tn+1w: 3G duration
Tnw: WiFi duration

stage n

Time
stage
n+1


Time

Persistent presence of 3G connection and sporadic presence of
WiFi connection.

Fig. 4.

each stage by the following statistics:
Tnw : duration of WiFi coverage.
Tng : duration of the 3G-only sub-stage.
w
Bn
: available bandwidth over WiFi when available.
g
Bn
: available bandwidth over 3G for the entire stage.

In this work, we assume that a priori knowledge in terms of the distributions for these random variables can either be learned by the vehicle
from previous observations, or is directly provided by the infrastructure
in the case of managed WiFi service.
Given continuous 3G coverage of the vehicle, it is highly likely that
the rate of 3G experienced at one stage is similar to that of the previous
one. In addition, according to [10], encounters of WiFi access points
tend to cluster together, i.e., short durations of 3G-only periods tend
to be followed by short durations of 3G-only periods, and vice versa.
This motivates us to model the evolution of Bng ’s and Tng ’s as ﬁrst-order
Markov processes. On the other hand, since WiFi coverage experienced
by the vehicle is disconnected over time, we choose to model Bnw ’s and
Tnw ’s as independent random variables, without any correlation over time.
As a shorthand, we denote the system state at the n-th stage as sn =
w , B g } ∈ S , with S representing the entire state space at
{Tnw , Tng , Bn
n
each stage. Note that since the random variables Tnw ’s and Bnw ’s are
independent over time, observations of their past will not help decision
making in the future. Therefore they are not included as system states.

m
Rn
(sn , an )

=

f m,g (Qgn ) + f m,w (Qw
n ),

(3)

d
Rn
(sn , an )
o
Rn (an , an−1 )

=

f d (Qgn + Qw
n ),

(4)

=

f o (Gn , Wn , Wn−1 ).

(5)

Qgn

Qw
n

In (3),
and
represent the amount of data transferred over
the 3G and WiFi networks respectively, as calculated in (1) and (2).
The monotonically decreasing functions f m,g (·) and f m,w (·) reﬂect
pricing schemes over each access network. Similarly, the monotonically
increasing function f d (·) indicates the utility of data transfer during
the nth stage. Finally, in (5), the monotonically decreasing function
f o (·) represents the severity of system overhead introduced by interface
switching.
For the rest of our work, we have chosen the following simple forms
for the reward functions:
m
Rn
(sn , an )

=

−κg Qgn − κw Qw
n,

(6)

d
Rn
(sn , an )
o
Rn (an , an−1 )

=

log(Qgn + Qw
n ),

(7)

=

−1{Gn = Wn } − 1{Gn = Wn−1 }.

(8)

κg

κw

Here, the coefﬁcients
> 0 and
> 0 correspond to the unit price
per data transfer over 3G and WiFi networks, respectively. For notational
o = 0 for for the ﬁrst stage n = 1.
convenience later on, we also set Rn

V. O PTIMAL I NTERFACE S WITCHING
A. Optimization Objective
Our system aims at maximizing the expected reward over a ﬁnite
horizon of N stages:
max

E

{an }N
1

N


Rn (sn , an , an−1 )

n=1
N


=E

IV. S YSTEM M ODEL

m
d
o
(λm Rn
+ λd Rn
+ λ o Rn
)

(9)

n=1

(6) − (8).

s.t.

A. Decision Variables
At the n-th stage, there are two decision variables involved: choice
of interface during the sub-stage with WiFi coverage Wn and choice of
interface during the 3G-only sub-stage Gn . As indicated below, Wn takes
on one out of three alternative values, whereas Gn is a binary variable.
⎧

⎪
⎨0 no transmission
0 no transmission
Wn = 1 use 3G
, Gn =
.
⎪
1 use 3G
⎩2 use WiFi
Again, we use the shorthand an = {Wn , Gn } ∈ A to denote the action
at the n-th stage, with A indicating the entire action space. By deﬁnitions
of Wn and Gn , it is straightforward to see that |A| = 3 × 2 = 6.
It can then be derived that the total amount of data transmitted over
the 3G and WiFi networks at the n-th stage are, respectively:
Qgn

=

g w
g g
1{Wn = 1}Bn
Tn + 1{Gn = 1}Bn
Tn ,

(1)

Qw
n

=

w w
1{Wn = 2}Bn
Tn .

(2)

In (9), the expectation is taken over all possible realizations of the random
variables Tng ’s, Tnw ’s, Bng ’s, and Bnw ’s. The parameters λm , λd , and λo
signify user-speciﬁed relative importance of each performance metric.
The goal of the optimization is to choose a set of optimal policies
∗
∗ N
{a∗n }N
1 = {Gn , Wn }1 to minimize the total expected cost.
Although it is straightforward to evaluate any given policy over a
ﬁnite horizon, and then to ﬁnd the optimal policy via exhaustive search,
the sheer computational complexity incurred in such an approach is
not suitable for practical implementation. We, therefore, present in this
section an alternative method based on stochastic dynamic programing
(SDP) for ﬁnding the optimal policy.

B. SDP-Based Policy
Given observations of past state sn−1 and action an−1 , we deﬁne
the maximum expected reward-to-go function at the n-th stage as
Vn (sn−1 , an−1 ):

In (1) and (2), 1{·} is the binary indicator function. It equals 1 if the
internal statement is true, and 0 vice versa.

Vn

= max E
{an }N
n

N


Rn 

n =n

= max E[Rn +

B. Reward Functions
In this work, we are concerned with the following aspects as performance metrics of the interface selection policy:
• monetary cost: payment incurred for data service over each access
network.
• data delivery efﬁciency: total amount of data transferred.

768

{an }N
n

= max E[Rn +
an

N


Rn  ]

n =n+1

max

{an }N
n+1

E

N


= max E[Rn + Vn+1 (sn , an )].
an

Rn  ]

n =n+1

(10)

Server

AP

3G
AP

BG CBR
traffic
traffi
fic

BG CBR
traffic
traf
f fic

BG CBR
traffic
fic

A
AP

FTP
TP
25mph

Fig. 5.

Qualnet simulation setup.

Parameter
Network simulator
MAC Protocol[1]
Transmission rate[1]
Transmission power[1]
Transmission range[1]
Transmission rate[2]
Max Transmission power[2]
Transmission range[2]
Propagation
Application trafﬁc
Vehicle speed
Simulation runs

Value
Qualnet 4.5
802.11b
2Mbps
8.8dBm
300m
2Mbps
50dBm
2000m
Two-Ray
FTP
20mph
10/policy

Fig. 6.
Tradeoff among rewards in money, data, and overhead, as
achieved by all interface switching policies.

TABLE I
S IMULATION PARAMETERS

Note that in (10), the expectation is taken over sn |sn−1 , Tnw , and Bnw .
The corresponding optimal policy at stage n is:
an ∗ = arg min E[Rn (sn , a, an−1 ) + Vn+1 (sn , a)].
a

(11)

For n = N , calculating the maximum terminal reward-to-go
VN (sN −1 , aN −1 ) is rather straightforward, as:
VN = max EsN |sN −1 ,T w ,B w RN (sN , aN , aN −1 ).
aN

N

N

(12)

We can then recursively derive values of the maximum reward-to-go
functions and corresponding optimal policies for stages n = N − 1, N −
2, · · · , 1, following (10). Note that for n = 1, the value of V1 corresponds
to the maximum reward achieved by the optimal policy.

C. Complexity Analysis
In the SDP policy, the number of state considered at each stage is
on the order of O(|S||A|), where |S| is size of the state space, and |A|
is size of the action space. More speciﬁcally, we have chosen to use
10 representative values for each of the random variables Bng , Bnw , Tng
and Tnw in our formulation, leading to a state space of |S| = 104 . As
mentioned earlier in IV-A, |A| = 6 in this work. The total computational
complexity in calculating the SDP policy for N stages is therefore
O(N |S||A|).
In comparison, the amount of computations associated with exhaustive
search is on the order of O(|A|N |S|N ), since one needs to consider all
possible |A|N policies and evaluate the expected reward of each policy
by accounting for all possible system states at each stage. It is obvious
that the computational burden of an exhaustive search approach quickly
grows prohibitive with as the optimization horizon grows.

VI. P ERFORMANCE E VALUATION
We evaluate our interface switching algorithm using Qualnet 4.5
network simulator [16]. Qualnet is modiﬁed to enable interface switching
between 3G and WiFi based on a given policy. As illustrated in Fig. 5, a
node is traveling at 25 miles-per-hour (mph) along a road of 2000 meters.
3G coverage is available for the entire length of the road. Three WiFi
access points (APs) are placed across the street such that WiFi coverage
is only available within the range of each AP. For a given policy, we
vary the background 3G and WiFi CBR trafﬁc and APs placement based

Fig. 7. Tradeoff in monetary and data transfer rewards achieved by the
SDP policy, with varying choices of λm , λd , and λo .

on Gaussian distributions. With CBR background trafﬁc, TCP application
trafﬁc are sent from the mobile node equipped with the switching policy.
A ﬁle transfer session is activated at the beginning of the simulation, and
keeps track of packet delivery over both interfaces. At the end of the
simulation, packet-level log ﬁle allows us to parse the amount of data
transfer over each interface, and calculate the associated monetary costs
over the 3G network2 . Table I summarizes the parameters used for the
evaluation.
While our optimization framework is general enough to accommodate
a wide range of statistical models, we have chosen a concrete example
for simulation evaluations. Based on [10], evolutions of both available
bandwidth and duration over 3G (Bng ’s and Tng ’s) are modeled as ﬁrstorder Gauss-Markov processes. Bandwidth and duration over WiFi (Bnw ’s
and Tnw ’s) are modeled as independent random variables following
Gaussian and exponential distributions, respectively.
Given three WiFi coverage areas in the simulation, it is easy to see
that there are all together (2 × 3)3 − 1 = 215 valid interface switching
policies, after discounting the option of no transmission throughout the
simulation. Figure 6 presents the tradeoff in monetary cost, data transfer
efﬁciency, and interface switching overhead rewards as achieved by all
these policies. It can be noted that they span a wide range in the 3D
tradeoff space. Note that policies lying on the paraeto surface in this
graph represent the optimal choices for certain relative weighting factors
between the three reward terms.
2 In our simulation evaluation, we have chosen κw = 0 to reﬂect the fact
that today’s WiFi access services tend to be free in public areas.

769

Figure 8 shows the comparison between the four schemes, when varying
one of the weighting parameters and keeping the other two constant.
It can be noted that the SDP-based policy consistently outperforms the
heuristic schemes for all combinations of weighting factors. This, again,
conﬁrms optimality of the proposed scheme.

VII. C ONCLUSIONS AND F UTURE W ORK
In this paper, we present an optimization framework for interface
switching among multiple heterogeneous networks in vehicular communications. Our problem formulation allows for ﬂexible tuning of
user-speciﬁed preferences among various performance metrics, such as
monetary cost, data transfer efﬁciency, and interface switching overhead.
As conﬁrmed by simulation studies, the proposed SDP-based policy can
strive the best tradeoff over a wide range of user preference choices, with
moderate computation complexity suitable for practical implementations.
For future work, we plan to investigate the scenario of more than two
available access networks, as well as extending the current optimization
framework for multiple competing application trafﬁc ﬂows.

(a)

R EFERENCES

(b)

(c)
Comparison of total rewards as achieved by the proposed SDPbased policy and a few heuristic schemes: Always Switching, 3G-only,
and WiFi-only. (a) Varying values of λd from 0 to 20, while λm = 10
and λo = 10. (b) Varying values of λm from 0 to 20, while λd = 10
and λo = 10. (c) Varying values of λo from 0 to 20, while λd = 10 and
λm = 10.
Fig. 8.

In Fig. 7, each subgraph shows the tradeoff between performance
and monetary rewards among all policies with the same overhead. In
addition, the red circles indicate policies calculated by the proposed SDP
algorithm with various choices of the relative weights λd , λm , and λo . As
expected, the SDP-based policies successfully attain the optimal tradeoff
between the two reward terms without the need of exhaustive search
within the entire policy space. It is also interesting to note that no SDPbased policies attains an overhead reward of −2 or −4 (corresponding
to twice and four times of interface switching), as these policies tend
to be outperformed by alternatives with overhead reward of −1 or −3,
respectively.
We also compare the proposed SDP-based policy against a few
heuristic schemes, as follows:
•
•
•

Always Switching: use 3G network as a default option, and switch
to WiFi whenever its presence is encountered.
WiFi-only: stay on WiFi regardless of observed WiFi presence
information.
3G-only: stay on 3G regardless of observed WiFi presence information.

[1] D. Reichardt, M. Miglietta, L. Moretti, P. Morsink, and W. Schulz,
“Cartalk 2000: Safe and comfortable driving based upon inter-vehiclecommunication,” in Intelligent Vehicle Symposium, 2002. IEEE, vol. 2,
june 2002, pp. 545 – 550 vol.2.
[2] H. Hartenstein, B. Bochow, A. Ebner, M. Lott, M. Radimirsch, and
D. Vollmer, “Position-aware ad hoc wireless networks for inter-vehicle
communications: the ﬂeetnet project,” in Proceedings of the 2nd ACM
international symposium on Mobile ad hoc networking & computing,
ser. MobiHoc ’01. New York, NY, USA: ACM, 2001, pp. 259–262.
[Online]. Available: http://doi.acm.org/10.1145/501449.501454
[3] C. Ribeiro, “Bringing wireless access to the automobile: A comparison
of wi-ﬁ, wimax, mbwa, and 3g.”
[4] K. C. Lee, U. Lee, and M. Gerla, Survey of Routing Protocols in
Vehicular Ad Hoc Networks. IGI Global, 2010.
[5] P. Rodriguez, R. Chakravorty, J. Chesterﬁeld, I. Pratt, and S. Banerjee,
“MAR: A commuter router infrastructure for the mobile Internet,” in
Proc. ACM Mobisys, June 2004.
[6] A. Balasubramanian, R. Mahajan, and A. Venkataramani, “Augmenting
mobile 3G with WiFi,” in Proc. ACM Mobisys, June 2010.
[7] T. Kim, S. wook Han, and Y. Han, “A QoS-aware vertical handoff
algorithm based on service history information,” IEEE Communications
Letters, vol. 14, no. 6, June 2010.
[8] H. Wang, R. Katz, and J. Giese, “Policy-enabled handoffs across
heterogeneous wireless networks,” in Proc. IEEE Workshop on Mobile
Computing Systems and Applications, 1999.
[9] J. McNair and F. Zhu, “Vertical handoffs in fourth-generation multinetwork environments,” IEEE Wireless Communications, vol. 11, no. 3,
Mar. 2004.
[10] A. Balasubramanian, R. Mahajan, A. Venkataramani, B. N. Levine, and
J. Zahorjan, “Interactive wiﬁ connectivity for moving vehicles,” in Proc.
ACM SIGCOMM, Aug. 2008.
[11] M. Kibria, A. Jamalipour, and V. Mirchandani, “A location aware threestep vertical handoff scheme for 4G/B3G networks,” in Proc. IEEE
GLOBECOM, Nov. 2005.
[12] C. Guo, Z. Guo, Q. Zhang, and W. Zhu, “A seamless and proactive
end-to-end mobility solution for roaming across heterogeneous wireless
networks,” IEEE Journal on Selected Areas in Communications, vol. 22,
no. 5, June 2004.
[13] O. Ormond, J. Murphy, and G. Muntean, “Utility-based intelligent
network selection in beyond 3G systems,” in Proc. IEEE International
Conference on Communications (ICC’06), June 2006.
[14] J. Zhang, H. Chan, and V. Leung, “A location-based vertical handoff
decision algorithm for heterogeneous mobile networks,” in Proc. IEEE
GLOBECOM, Nov. 2006.
[15] E. Stevens-Navarro, Y. Lin, and V. Wong, “An MDP-based vertical
handoff decision algorithm for heterogeneous wireless networks,” IEEE
Transactions on Vehicular Technology, vol. 57, no. 2, Feb. 2008.
[16] “Qualnet Network Simulator,” http://www.scalable-networks.com/.

Our performance metric is the overall reward function as calculated in
(9), over various choices of the relative weighting factors λd , λm , and λo .

770

Finding and Ranking Knowledge on the Semantic Web 
Li Ding, Rong Pan, Tim Finin, Anupam Joshi,
Yun Peng, and Pranam Kolari
Department of Computer Science and Electrical Engineering,
University of Maryland, Baltimore County, Baltimore MD 21250
{dingli1, panrong1, finin, joshi, ypeng, kolari1}@cs.umbc.edu

Abstract. Swoogle helps software agents and knowledge engineers find Semantic Web knowledge encoded in RDF and OWL documents on the Web. Navigating such a Semantic Web on the Web is difficult due to the paucity of explicit
hyperlinks beyond the namespaces in URIrefs and the few inter-document links
like rdfs:seeAlso and owl:imports. In order to solve this issue, this paper proposes a novel Semantic Web navigation model providing additional navigation
paths through Swoogle’s search services such as the Ontology Dictionary. Using
this model, we have developed algorithms for ranking the importance of Semantic Web objects at three levels of granularity: documents, terms and RDF graphs.
Experiments show that Swoogle outperforms conventional web search engine and
other ontology libraries in finding more ontologies, ranking their importance, and
thus promoting the use and emergence of consensus ontologies.

1 Introduction
As the scale and the impact of the World Wide Web has grown, search engines have
assumed a central role in the Web’s infrastructure. Similarly, the growth of the Semantic Web will also generate a need for specialized search engines that help agents1 find
knowledge encoded in Semantic Web languages such as RDF(S) and OWL. This paper discusses two important aspects of Semantic Web search engines: helping agents
navigate2 the Semantic Web and ranking search results.
The utility of Semantic Web technologies for sharing knowledge among agents has
been widely recognized in many domain applications. However, the Semantic Web itself (i.e., the unified RDF graph comprised of many decentralized online knowledge
sources) remains less studied. This paper focuses on the Semantic Web materialized
as a collection of Semantic Web Documents (SWDs)3 because web pages are well
known as the building blocks of the Web.


1

2

3

Partial support for this research was provided by DARPA contract F30602-00-0591 and by
NSF awards NSF-ITR-IIS-0326460 and NSF-ITR-IDM-0219649.
The term agents refers to programs, tools, and human knowledge engineers that might use
Semantic Web knoweledge.
The term navigation refers to a process of following a series of links (explicit or implicit) from
an initial starting point to a desired information resource.
A Semantic Web document is a web page that serializes an RDF graph using one of the recommended RDF syntax languages, i.e., RDF/XML, N-Triples or N3.

Y. Gil et al. (Eds.): ISWC 2005, LNCS 3729, pp. 156–170, 2005.
c Springer-Verlag Berlin Heidelberg 2005


Finding and Ranking Knowledge on the Semantic Web

157

One advantage of the Semantic Web is that people can collaboratively create ontologies and build common vocabulary without centralized control. One building block
of Semantic Web ontologies is a Semantic Web Term (SWT)4 , which plays the role
of a word in natural languages. SWTs bridge RDF statements with formal semantics
defined in RDF(S) and OWL, and are intended to be reused as universal symbols.
We call an SWD that defines a significant number of SWTs a Semantic Web Ontology(SWO) to distinguish it from documents that mostly populating and/or asserting
class instances5 . The Semantic Web depends on three “meta ontologies” (RDF, RDFS
and OWL) and, according to Swoogle [1], thousands of additional ones developed by
institutions (e.g., CYC, WordNet, DC6 , FOAF7 , and RSS) and individuals.
These ontologies often overlap by defining terms on similar or the same concepts.
For example, Swoogle finds over 300 distinct SWTs that appear to stand for the ‘person’ concept. This raises interesting issues in finding and comparing Semantic Web
ontologies for knowledge sharing. For example, how can an agent find the most popular domain ontology (currently FOAF is the best choice) to publish a personal profile?
Conventional web navigation and ranking models are not suitable for the Semantic
Web for two main reasons: (i) they do not differentiate SWDs from the overwhelming
number of other web pages; and (ii) they do not parse and use the internal structure
of SWD and the external semantic links among SWDs. Hence, even Google, one of
the best web search engines, can sometimes perform poorly in finding ontologies. For
example, the FOAF ontology (the most used one for describing a person) is not among
the first ten results when we search Google using the phrase “person ontology”8.
Although we are familiar with surfing on the Web, navigating the Semantic Web
is quite different. We have developed a Semantic Web navigation model based on how
knowledge is published and accessed. To publish content, information providers need
to obtain appropriate domain ontologies by reusing existing ones and/or creating new
ones, and then use them to create instances and make assertions. When accessing knowledge, consumers need to search for instance data and pursue corresponding ontologies
to fully understand the knowledge encoded. Meanwhile, the navigation model should
also acknowledge the context – the Web, which physically hosts RDF graphs in SWDs.
Most existing navigation tools (e.g., HyperDAML9 and Swoop10 ) employ the URL semantics of the URIref to a RDF resource; however, they cannot answer questions like
“find instances of a given class” or “list all URIs using the same local name person”
due to the limited number of explicit links.
The navigation model supports ranking the ‘data quality’ [2] of Semantic Web
knowledge in terms of common case importance. In particular, this paper focuses on
4

5

6
7
8

9
10

A Semantic Web term is an RDF resource that represents an instance of rdfs:Class (or
rdf:Property) and can be universally referenced by its URI reference (URIref).
Since virtually all documents will contain some definitions and instances, the classification
must either be a fuzzy one or depend on a heuristic threshold.
Dublin Core Element Set 1.1,http://purl.org/dc/elements/1.1/.
Friend Of A Friend ontology, http://xmlns.com/foaf/0.1/.
This example is not intended to undermine Google’s value; instead, we argue that the Semantic
Web is quite different from the Web and needs its own navigation and ranking models.
http://www.daml.org/2001/04/hyperdaml/
http://www.mindswap.org/2004/SWOOP/

158

L. Ding et al.

ranking ontologies at various levels of granularity to promote reusing ontologies. Ranking ontologies at the document level has been widely studied since most ontologies
are published through SWOs. Its common approaches include link-analysis [3, 1] and
semantic-content-analysis [4]. Document level ontology ranking, however, is not
enough. For example, foaf:Person and dc:creator together can describe the author of
a web page, and an ontology containing both of the concepts might not be as good as
the combination of FOAF and DC. Hence, a finer level of granularity (i.e., ranking at
SWT level) is needed especially to encode knowledge using popular terms from multiple ontologies11, but is seldom investigated in literature. Besides ranking individual
SWTs, agents may also rank inter-term relations (e.g., how frequently a property has
been used to modify the instances of a class). Such an ontology ranking approach is a
special case of ranking sub-graphs of an RDF graph [5, 6].
The remainder of this paper is structured as follows: Section 2 reviews the test-bed
(the Swoogle Semantic Web search engine) and related works on navigating and ranking Semantic Web knowledge. Section 3 introduces the novel Semantic Web navigation
model, which enriches navigation paths and captures surfing behaviors on the Semantic
Web on the Web. Sections 4 and 5 describe and evaluate mechanisms for ranking ontologies at different levels of granularity, namely document, term and sub-graph. Section 6
concludes that effective navigation support and ranking mechanisms are critical to both
the emergence of common ontologies and the growth of the Semantic Web on the Web.

2 Background and Related Work
2.1 Swoogle
The Swoogle [1] search engine discovers, indexes, and analyzes Semantic Web documents published on the Web and provides agents with various kinds of search services.
Its architecture, shown in Figure 1, is comprised of four components.

Analysis

SWD classifier

Ranking

Index
IR Indexer
SWD Indexer

…

Search Services
Semantic Web
metadata

Web
Server
html

document cache
Candidate
URLs

Discovery
SwoogleBot
Bounded Web Crawler
Google Crawler

Web
Service
rdf/xml

the Web
Semantic Web

human

machine

Legends
Information flow
Swoogle‘s
web interface

Fig. 1. Swoogle’s architecture involves four major components.

11

Importing part of ontologies is especially helpful when using large upper ontologies like CYC.

Finding and Ranking Knowledge on the Semantic Web

159

– The Discovery component collects candidate URLs to find and cache SWDs using four mechanisms: (i) submitted URLs of SWDs and sites; (ii) a web crawler
that explores promising sites; (iii) a customized meta-crawler that discovers likely
URLs using conventional search engines; and (iv) the SwoogleBot Semantic Web
crawler which validates and analyses SWDs to produce new candidates.
– The Indexing component analyzes the discovered SWDs and generates the bulk of
Swoogle’s metadata about the Semantic Web. The metadata not only characterizes
the features associated with individual SWDs and SWTs, but also tracks the relations among them, e.g., “how SWDs use/define/populate a given SWT” and “how
two SWTs are associated by instantiating ‘rdfs:domain’ relation”.
– The Analysis component analyzes the generated metadata and hosts the modular
ranking mechanisms.
– The Services module provides search services to agents, allowing them to access
the metadata and navigate the Semantic Web. It is highlighted by the “Swoogle
Search” service that searches SWDs using constraints on URLs, the SWTs being
used or defined, etc.; and the “Ontology Dictionary” service that searches ontologies at the term level and offers more navigational paths.
2.2 Related Work and Motivation
Random Surfing Model and PageRank. The random surfing model underlying the
PageRank [7] algorithm has been widely accepted as the navigation model for the Web.
In this model, the surfer begins by jumping to a random URL. After visiting a page,
he either (i) with probability d12 randomly chooses a link from the page to follow to a
new page; or (ii) with probability 1 − d jumps to another random URL. This model is
essentially a simple random walk modeled by a Markov chain. Based on this surfing
model, the basic PageRank algorithm computes the rank (indicating popularity rather
than relevance) for each web page by iteratively propagating the rank until convergence.
Variations of PageRank. The basic PageRank algorithm is limited by its assumptions
and relaxing them has resulted in several extensions. In Topic-Sensitive PageRank [8],
documents are accessed non-uniformly according to their topics. For Weighted PageRank extensions [9, 10, 11], links are followed non-uniformly according to their popularity. Several link-semantics-aware extensions [12, 13] recognize links with different
meanings and compute a PageRank weighted by the link semantics.
Navigating the Semantic Web. Navigating the Semantic Web is quite different from
navigating the conventional Web. It is currently supported by tools such as browsers
(e.g., HyperDAML and Swoop), ontology libraries (e.g., DAML ontology library13 and
SchemaWeb14 ), search engines (e.g., Ontaria15 and Swoogle), and crawlers (e.g., scutter16 and SwoogleBot). Most tools only capture navigational paths based on the seman12
13
14
15
16

d is usually a constant except in personalized ranking.
http://www.daml.org/ontologies/
http://www.schemaweb.info/
http://www.w3.org/2004/ontaria/
http://rdfweb.org/topic/Scutter

160

L. Ding et al.

tics of URIref. Swoogle, however, supports effective navigation by providing additional
navigational paths among SWDs and SWTs.
Ranking Semantic Web knowledge. Ranking knowledge can be considered as a problem of evaluating data quality [2, 14] which focuses on data product quality [15]. It has
been studied at various levels of granularity in Semantic Web and database literature.
– Ranking Semantic Web ontologies at the document level has been studied using
both content analysis [16, 4] and link-structure-based analysis [3, 1].
– Ranking knowledge at the instance or object level has been investigated by both
database and Semantic Web researchers, including ranking elements in XML documents [17]; ranking objects in databases [18] or the Web [19, 11]; and ranking
relevant class-instances in domain specific RDF database [20].
– Ranking knowledge at a sub-graph level has been studied using ontology-based
content analysis [5, 21, 6] in the context of ranking query results in the Semantic
Web, and using context-based trust computation [22, 23].
Ranking Semantic Web ontologies has remained at the document level even though
other granularity levels are applicable. For example, SWTs are a special kind of class
instances and should be ranked differently from normal instances. Doing so enables a
retrieval system to find a set of SWTs drawn from more than one ontologies to cover a
collection of target concepts.
Most link-analysis-based approaches have focused on either a particular domain
(e.g., bibliographic data) or a small set of SWOs. Swoogle is unique in its ambition to
discover and index a substantial fraction of the published SWDs on the Web (currently
over 7 × 105 SWDs of which about 1% are SWOs).

3 Semantic Web Navigation Model
In this paper, we consider the Semantic Web materialized on the Web. To navigate such
a Semantic Web, a user cannot simply rely on the URL semantics of URIref due to
three main reasons: (i) the namespace of a URIref at best points to an SWO, but there
are no reverse links pointing back; (ii) although rdfs:seeAlso has been widely used
to interconnect SWDs in FOAF based applications, it seldom works in other SWDs;
(iii) owl:imports does interlink ontologies, but such relations are rare since ontologies
are usually independently developed and distributed. In addition, many practical issues
should be addressed in web-scale Semantic Web data access, such as “how two reach
the SWDs which are not linked by any other SWDs” and “what if the namespace of a
URIref is not an SWD”. It is notable that the intended users of this navigation model
are both software agents, who usually search SWDs for external knowledge and then
retrieve SWOs to fully understand SWDs, and Semantic Web researchers, who mainly
search SWTs and SWOs for publishing their knowledge.
3.1 Overview
The navigation model is specialized for publishing and accessing Semantic Web knowledge as shown in Figure 2. Users can jump into the Semantic Web using conventional

Finding and Ranking Knowledge on the Semantic Web
sameNamespace, sameLocalname
extends
1 class-property bond

Term Search

RDF graph

Resource

& literal
2 uses
populates

SWT
5

4

3

isUsedBy
isPopulatedBy

Web

161

defines

officialOnto
isDefinedBy

rdfs:subClassOf

SWD

SWO

6

7

rdfs:seeAlso
rdfs:isDefinedBy

owl:imports
…

Document Search

The block arrows link search services to the Semantic Web. Paths 2 and 5 are straightforward since SWTs are referenced
by SWDs/SWOs. Paths 6, 7 and part of 4 are supported by most existing RDF browsers. Paths 1, 3 and the rest of 4 require
global view of the Semantic Web on the Web, and are currently only supported by Swoogle metadata.

Fig. 2. The Semantic Web navigation model

http://xmlns.com/foaf/0.1/index.rdf
http://xmlns.com/foaf/0.1/index.rdf

owl:Class
rdf:type
foaf:Person

http://www.w3.org/2002/07/owl

owl:InverseFunctionalProperty

rdfs:subClassOf

rdf:type

owl:Thing
rdf:type

rdf:type

http://www.cs.umbc.edu/~finin/foaf.rdf

foaf:Person

owl:imports

foaf:Agent

rdfs:domain

foaf:mbox

http://www.cs.umbc.edu/~dingli1/foaf.rdf

foaf:Person

foaf:mbox
mailto:finin@umbc.edu

rdfs:range

rdf:type
rdfs:seeAlso

http://www.cs.umbc.edu/~finin/foaf.rdf

A user can use Swoogle term search to find SWTs having local name ‘Person’. If she picks SWT foaf:Person, she can jump to
the corresponding SWO http://xmlns.com/foaf/0.1/index.rdf by following path 4 via isDefinedBy, jump to an SWT foaf:mbox
by following path 1 via sameNamespace, or jump to another SWD http://www.cs.umbc.edu/ dingli1/foaf.rdf by following
path 3 via isPopulatedBy. From the FOAF SWO, she can pursue OWL ontology by following path 7 via owl:imports,
jump to SWT rdfs:domain by following path 2 via populates, or jump to SWT foaf:Agent by following path 5 via defines.
For the SWD to the right, she can jump to another SWD http://www.cs.umbc.edu/ finin/foaf.rdf by following path 6 via
rdfs:seeAlso.

Fig. 3. A navigation use-case

Web search (e.g., Google and Yahoo) or Semantic Web search (e.g., Swoogle). Users
can also navigate the Semantic Web within or across the Web and RDF graph via seven
groups of navigational paths. An example is shown in Figure 3.

162

L. Ding et al.

In addition to conventional document search using document properties and/or bagof-word model, Swoogle lets users locate Semantic Web knowledge using navigational
paths, e.g., “a personal profile ontology can be located if it defines SWTs like ‘person’,
‘email’ and ‘homepage’ ”. We detail three groups of navigational paths as follows.
3.2 Paths Between SWTs
We concentrate on three of the many interesting navigational paths between SWTs
grouped by path 1 in Figure 2 as follows.
1. sameNamespace and sameLocalname. linking SWTs sharing the same namespace is needed because they are not necessarily defined in the document pointed
by the namespace. Linking SWTs sharing the same local name is needed to find
alternative SWTs because the local name part of an SWT usually conveys its
semantics.
2. extends. An SWT t1 extends another SWT t2 when either (i) there exists a triple
(t1, P , t2) where P (e.g., rdfs:subClassOf, owl:inverseOf and owl:complementOf)
connects two classes (or two properties), or (ii)there exists a triple (t1, P , LIST )
where P (e.g., owl:unionOf) connects a class t1 to a rdf:List LIST which has another class t2 as a non-nil member. For example, in Figure 3, foaf:Agent is extended
by foaf:Person because it is closer to the concept ‘person’ and its mobx property
can be inherit. The extends relation is a good indicator for the importance of term
because it implies that the term being extended is commonly accepted and welldefined but too general for instantiating the intended concept.
3. class-property bond. Although classes and their attributes have been tightly
bonded in frame-based systems, the connections between classes and properties are
loose in the Semantic Web. For example, Dublin Core defines widely used properties without specifying their domains and ranges. Swoogle links from a class to
its instance properties (i.e., class-property bond) using two sources: (i) rdfs:domain
assertions in SWOs and (ii) instantiation of such bond in class-instances.
3.3 Paths Between SWDs and SWTs
Swoogle maintains three types of navigational paths across SWDs and SWTs: (i) paths
2 and 5 in Figure 2 can be easily extracted from an SWD by analyzing the usage of
SWTs; (ii) paths 3 and 4 are mainly the reverse of paths 2 and 5. Generating such paths
requires the global view of the Semantic Web; and (iii) the officialOnto relation in path
4 links an SWT to an SWO. It is needed by software agents to locate ontologies defining
the encountered SWTs in the absence of explicit import instruction.
1. Swoogle recognizes six types of binary relations between an SWT T in an SWD
D as shown in Table 1. They can be further generalized to three groups namely,
defines, uses and populates. For example, in figure 3, http://xmlns.com/
foaf/0.1/index.rdf defines foaf:Person as class and populates rdfs:domain
as property. An SWD using or populating an SWT indicates that the publisher is
satisfied with the SWT’s definition.

Finding and Ranking Knowledge on the Semantic Web

163

Table 1. Six types of binary relations that can hold between an SWD D and an SWT T
Relation
define-class
define-property
use-class

Condition
D has a triple (T , rdf:type, MC) where MC is a sub-class of rdfs:Class.
D has a triple (T , rdf:type, MP) where MP is a sub-class of rdf:Property.
D has a triple ( , P , T ) where the range of P is a sub-class of rdfs:Class, or
D has a triple (T , P , ) where the domain of P is a sub-class of rdfs:Class.
use-property
D has a triple ( , P , T ) where the range of P is a sub-class of rdf:Property, or
D has a triple (T , P , ) where the domain of P is a sub-class of rdf:Property.
populate-class
D has a triple ( , rdf:type, T ).
populate-property When D has a triple ( , T , ).

Table 2. Heuristics for finding official ontologies, and their performance on 4508 namespaces
Type
Percent
The namespace of T ;
59%
the URL of an ontology which is redirected from T ’s namespace 0.4%
(e.g., http://purl.org/dc/elements/1.1/ is redirected to
http://dublincore.org/2003/03/24/dces);
the URL of an ontology which has T ’ namespace as its abso- 3.4%
lute path, and it is the only one that matches this criteria (e.g.,
http://xmlns.com/foaf/0.1/index.rdf is the official ontology
of http://xmlns.com/foaf/0.1/);
N/A, cannot decide
37.2%

2. Swoogle tracks the “official ontology” of an SWT T using heuristics listed in Table 2. The ‘percent’ column shows the percentage that the heuristics has been successfully applied. It is notable that heuristics 2 and 3 help find some important
official ontologies of DC and FOAF even though they have only improved the performance from 59% to 62.8%.
3.4 Paths Between SWDs
Swoogle also supports well-known navigational paths between SWDs.
1. Although not defined explicitly, the triples populating properties rdfs:isDefinedBy
and rdfs:seeAlso are widely used in linking to web pages or even SWDs. In practice,
many RDF crawlers use rdfs:seeAlso to discover SWDs.
2. Instances of owl:OntologyProperty is explicitly defined to associate two SWOs, and
owl:imports is frequently instantiated far more than the others. Therefore, Swoogle
indexes the usage of the imports17 relation.
3. Inspired by RDF test-case ontology18, we have developed a class wob: RDFDocument (which asserts that a resource is an SWD) to support explicit ‘hyperlinks’ in
17

18

An SWO D1 imports another D2 when there is a triple in D1 in form of ( D1, owl:imports,
D2), and so does daml:imports. This relation shows the dependency between ontologies and
is complemented by “officalOnto” relation.
http://www.w3.org/2000/10/rdf-tests/rdfcore/testSchema

164

L. Ding et al.

the Semantic Web. A consequent idea is RDF sitemap which let website publish
their SWDs through a special index file19 .

4 Ranking Semantic Web Documents
Since RDF graphs are usually accessed at the document level, we simplify the Semantic
Web navigation model by generalizing navigational paths into three types of document
level paths (see below) and then applying link analysis based ranking methods with
‘rational’ surfing behavior.
– An extension (EX) relation holds between two SWDs when one defines a term
using terms defined by another. EX generalizes the defines SWT-SWD relations, the
extends SWT-SWT relations, and the officialOnto SWT-SWD relation. For example,
an SWD d1 EX another SWD d2 when d1 defines a class t1, which is the subclass
of a classt2, and t2’s official ontology is d2.
– A use-term (TM) relation holds between two SWDs when one uses a term defined
by another. TM generalizes the uses and populates SWT-SWD relations, and the
officialOnto SWT-SWD relation. For example, an SWD d1 TM another SWD d2
when d1 uses a resource t as class, and t’s official ontology is d2.
– An import (IM) relation holds when one SWD imports, directly or transitively,
another SWD, and it corresponds to the imports SWD-SWD relation.
4.1 Rational Surfer Model and OntoRank
Swoogle’s OntoRank is based on the rational surfer model which emulates an agent’s
navigation behavior at the document level. Like the random surfer model, an agent
either follows a link in an SWD to another or jumps to a new random SWD with a
constant probability 1 − d. It is ‘rational’ because it emulates agents’ navigation on
the Semantic Web, i.e., agents follow links in a SWD with non-uniform probability
according to link semantics. When encountering an SWD α, agents will(transitively)
import the “official” ontologies that define the classes and properties referenced by α.
Let link(α, l, β) be the semantic link from an SWD α to another SWD β with tag l;
linkto(α) be a set of SWDs link directly to the SWD α; weight(l) be a user specified
navigation preference on semantic links with type l, i.e., TM and EX; OT C(α) be a
set of SWDs that (transitively) IM or EX α as ontology; f (x, y) and wP R(x) be two
intermediate functions.
OntoRank is computed in two steps: (i) iteratively compute the rank, wP R(α), of
each SWD α until it converges (equations 1 and 2); and (ii) transitively pass an SWD’s
rank to all ontologies it imported (equation 3).
wP R(α) = (1 − d) + d


x∈linkto(α)

19

http://swoogle.umbc.edu/site.php

wP R(x) × f (x, α)

f (x, y)
link(x, ,y)

(1)

Finding and Ranking Knowledge on the Semantic Web

165

Table 3. OntoRank finds more ontologies in each of the 10 queries
Query

C1:# SWOs C2:# SWOs Difference
by OntoRank by PageRank (C1-C2)/C2
name
9
6
50.00%
person
10
7
42.86%
title
13
12
8.33%
location
12
6
100.00%
description
11
10
10.00%
date
14
10
40.00%
type
13
11
18.18%
country
9
4
125.00%
address
11
8
37.50%
organization
9
5
80.00%
Average
11.1
7.9
40.51%

f (x, α) =


link(x,l,α)

OntoRank(α) = wP R(α) +

(2)

weight(l)


wP R(x)

(3)

x∈OT C(α)

4.2 Evaluation: OntoRank vs PageRank
OntoRank is evaluated on a real dataset DS-APRIL collected by Swoogle by April 2005.
DS-APRIL contains 330K SWDs (1.5% are SWOs, 24% are FOAF documents and 60%
are RSS documents) and interlink by 200K document level relations.
The first experiment compares the performance between PageRank and OntoRank
in boosting the rank of SWOs among SWDs, i.e., ranking SWOs higher than normal
SWDs. In this experiment, we first compute both ranks for SWDs in DS-APRIL20 ; and
then ten popular local-names (according to Swoogle’s statistics) were selected as the
keywords for Swoogle’s document search. The same search result for each query is
ordered by both PageRank and OntoRank respectively. We compared the number of
strict SWO (see definition 1) in the first 20 results in either order. Table 3 shows an
average 40% improvement of OntoRank over PageRank.
Definition 1. ontology ratio
The ontology ratio of an SWD refers to is the fraction of its class-instances being recognized as classes and properties. It is used to identify SWOs among SWDs. For example,
given an SWD defining a class “Color” and populating the class with three classinstances namely, ‘blue’, ‘green’ and ‘red’, its ontology ratio is 25% since only one
out of the four is defined as class. A document with a high ontology ratio indicates a
preference for adding term definition rather than populating existing terms. According
to Swoogle, an SWD is an ontology document if it has defined at least one term, and it
is called a strict SWO if its ontology ratio exceeds 0.8.
20

Note this PageRank is computed on the same dataset as OntoRank, which is a preprocessed
web of SWDs where no simply hyperlinks but only semantic links are considered.

166

L. Ding et al.
Table 4. Top 10 SWDs according to OntoRank and their PageRank
URL of Ontology
Ontology Ratio OntoRank PageRank
http://www.w3.org/2000/01/rdf-schema
94%
1
1
http://www.w3.org/2002/07/owl
86%
2
5
http://www.w3.org/1999/02/22-rdf-syntax-ns
81%
3
6
http://purl.org/dc/elements/1.1
100%
4
3
http://purl.org/rss/1.0/schema.rdf
100%
5
2
http://www.w3.org/2003/01/geo/wgs84 pos
100%
6
10
http://xmlns.com/foaf/0.1/index.rdf
84%
7
4
http://xmlns.com/wot/0.1/index.rdf
100%
8
29
http://www.w3.org/2003/06/sw-vocab-status/ns
75%
9
7
http://www.daml.org/2001/03/daml+oil
96%
10
11

The second experiment studies the best ranked SWDs using both ranking methods. In
table 4, RDFS schema clearly ranks first according to both OntoRank and PageRank.
OWL ranks higher than RDF because it is referred to by many popular ontologies. DC
and FOAF ontologies rank 4th and 5th by PageRank due to their many instance documents but rank lower by OntoRank due to their narrow domain and fewer references by
other ontologies. An interesting case is the web of trust (WOT) ontology which PageRank ranks only 29th since our data set only contains 280 FOAF documents referencing
it directly. OntoRank ranks it at 8 since it is referenced by the FOAF ontology, greatly
increasing its visibility. We are not expecting OntoRank to be completely different from
PageRank since it is a variation of PageRank. OntoRank is intended to expose more ontologies which are important to Semantic Web users in understanding term definition.

5 Ranking for Ontology Dictionary
Ranking ontologies at the term level is also important because SWTs defined in the
same SWO are instantiated in quite different frequency. For example, owl:versionInfo
is far less used than owl:Class. Users, therefore, may want to partition ontologies and
then import a part of an SWO [24, 25]. In addition, users often use SWTs from multiple
ontologies together, e.g., rdfs:seeAlso and dc:title have been frequently used modifying
the instances of foaf:Person.
These observations lead to the “Do It Yourself” strategy i.e., users can customize
ontologies by assembling relevant terms from popular ontologies without importing
them completely. To this end, Swoogle’s Ontology Dictionary helps users to find relevant terms ranked by their popularity, and supports a simple procedure CONSTRUCTONTO for publishing knowledge using class-instances.
CONSTRUCT-ONTO
1. find an appropriate class C
2. find popular properties whose domain is C
3. go back to step 1 if another class is needed

Finding and Ranking Knowledge on the Semantic Web

167

Table 5. Top ten classes with ’person’ as the local name ordered by Swoogle’s TermRank
TermRank
1
2
3
4
5
6
7
8
9
10
1
2
3

Resource URI
pop(swd) pop(i) def(swd)
http://xmlns.com/foaf/0.1/Person
74589 1260759
17
http://xmlns.com/wordnet/1.6/Person
2658 785133
80
http://www.aktors.org/ontology/portal#Person
267
3517
6
ns1:Person 1
257
935
1
ns2:Person 2
277
398
1
http://xmlns.com/foaf/0.1/person
217
5607
0
http://www.amico.org/vocab#Person
90
90
1
http://www.ontoweb.org/ontology/1#Person
32
522
2
ns3:Person 3
0
0
1
http://description.org/schema/Person
10
10
0

ns1 - http://www.w3.org/2000/10/swap/pim/contact#
ns2 - http://www.iwi-iuk.org/material/RDF/1.1/Schema/Class/mn#
ns3 - http://ebiquity.umbc.edu/v2.1/ontology/person.owl#

5.1 Ranking Semantic Web Terms
Swoogle uses TermRank to sort SWTs by their popularity, which can be simply measured by the number of SWDs using/populating an SWT. This naive approach, however,
ignores users’ rational behavior in accessing SWDs, i.e., users access SWDs with nonuniform probability. Therefore, TermRank is computed by totaling each SWD’s contribution (equation 4). For each SWD α, its contribution to each of its SWTs is computed
by splitting its OntoRank proportional to SWTs’ weight T W eight(α, t), which indicates the probability a user will access t when browsing α. T W egiht is the product of
cnt uses(α, t) - t’s popularity within α measured by the number of occurrence of t in
α and |{α|uses(α, t)}| – t’s importance in the Semantic Web measured by the number
of SWDs containing t (see equation 5).


OntoRank(α)×T
W eight(α,t)

uses(α,x) T W eight(α,x)

(4)

T W eight(α, t) = cnt uses(α, t) × |{α|uses(α, t)}|

(5)

T ermRank(t) =

uses(α,t)

Table 5 lists top ten classes in DS-APRIL having ‘person’ as the local name ordered
by TermRank. For each class, pop(swd) refers to the number of SWDs populating it;
pop(i) refers to the number of its instances; and def(swd) refers to the number of SWDs
defining it. Not surprisingly, foaf:Person is number one. The sixth term is a common
mis-typing of the first one, so it has been well populated without being defined. The
ninth term has apparently made the list by virtue of the high OntoRank score of the
SWO that defines it.
Table 6 lists top ten SWTs in Swoogle’s Ontology Dictionary. The type of an SWT
is either ‘p’ for property or ‘c’ for class. rdfs:comment is ranked higher than dc:title even
though the latter is better populated because the former is referenced by many important
SWDs. Properties are ranked higher than classes since they are less domain specific.

168

L. Ding et al.
Table 6. Top ten terms order by TermRank
TermRank
1
2
3
4
5
6
7
8
9
10

SWT
type pop(swd) pop(i)
rdf:type
p 334810 8174201
dc:description
p
60427 918644
rdfs:label
p
12795 197079
rdfs:comment
p
4626 137267
dc:title
p
60229 1452612
rdf:Property
c
4117 52445
dcterms:modified
p
11881 25321
rdfs:seeAlso
p
55985 1167786
dc:language
p 149878 225600
dc:type
p
9461 54676

5.2 Ranking Class-Property Bonds
A more specific issue directly related to step 2 in CONSTRUCT-ONTO is ranking classproperty bonds (see definition 2), which helps users choose the most popular properties for a class when they are publishing data with the desire of maximizing the data’s
visibility. For example, when publishing an instance of foaf:Person, we might always
supply a triple that populates the most common property foaf:mbox sha1sum.
Definition 2. A class-property bond (c-p bond) refers to an rdfs:domain relation between property and class. While c-p bonds can be specified in ontologies in various
ways, e.g., direct association (rdfs:domain) and class-inheritance; we are interested
in finding c-p bonds in class instances characterized by the two-triple graph pattern:
( x, rdf : type, class), ( x, property, ).
To rank c-p bonds, we cannot simply rely on the definition from ontologies because
that does not show how well a c-p bond has been adopted in practice. We evaluate
c-p bonds, therefore, by ranking the subgraph that instantiates c-p bonds, e.g., the
number instance of foaf:person modified by foaf:name. In DS-APRIL, the five highest
ranked properties (by the number of SWDs instantiated c-p bond) of foaf:Person are (i)
foaf:mbox sha1sum (67,136 SWDs), (ii) foaf:nick (62,266), (iii) foaf:weblog (54,341),
(iv) rdfs:seeAlso (47,228), and (v) foaf:name (46,590).

6 Conclusions and Future Work
Swoogle supports two primary use cases: helping human knowledge engineers find
ontologies and terms and serving agents and tools seeking knowledge and data. While
no formal evaluation has yet been done, we offer some observations that address how
well Swoogle meets its goals and informally compare it to the alternatives.
Swoogle’s web-based service has been available since Spring 2004 and has received
several million hits, supporting hundreds of regular users and thousands of casual ones.
Swoogle continuously discovers online SWDs and thus maintains a global view of the
public Semantic Web. The results reported here are based on a dataset (DS-APRIL)

Finding and Ranking Knowledge on the Semantic Web

169

of over 330,000 SWDs and 4,000 SWOs, about half the size of the current collection.
Swoogle has found many more SWDs, most of which are FOAF or RSS documents,
that are excluded from the database to make Swoogle’s dataset balanced and interesting. Swoogle’s ability to search content at various granularity levels and its ranking
mechanisms are novel and promote the emergence of consensus ontologies.
There are three alternatives to Swoogle that can be used to find knowledge on the
Semantic Web: conventional search engines, Semantic Web repositories, and specialized RDF data collections. Some conventional search engines index RDF documents
and can be used to find SWDs and SWTs. However, none understands the content being indexed, recognizes terms as links, or even correctly parses all RDF encodings.
Any ranking done by such systems ignores links between SWDs and their corresponding semantic relationships. Some useful SWD repositories are available (e.g., those at
www.schemaweb.info and rdfdata.org) but require manual submission and have limited scope. Several crawler-based systems exist that are specialized to particular kinds
of RDF (e.g., FOAF, RSS, DOAP, Creative Commons), but their scope and services
are restricted. Intellidimention has an experimental crawler based system 21 similar to
Swoogle but with abridged coverage.
A formal evaluation of Swoogle’s performance on finding and ranking SWDs and
SWTs would be based, in part, on measuring the precision and recall for a set of queries
against human judgments. This would allow us to compare Swoogle’s performance to
other systems, to evaluate different ranking algorithms and to evaluate the impact of
doing more or less inference. While we intend to carry out such an evaluation, it requires
careful design and significant labor to acquire the necessary human evaluations. User
studies through questionnaires or surveys on Swoogle ranking results are planned to
provide a subjective reference.
By enlarging the test dataset and compensating for biases due to the predominance
of FOAF and RSS documents, we expect to refine our evaluation of Swoogle’s navigation model and ranking algorithms. We are also improving the ranking algorithms
without generalizing the navigation model, motivated by the the success of XML objectlevel ranking [17, 11]. We are extending class-property bond ranking to a more general
issue – tracking the provenance of and ranking arbitrary RDF sub-graphs [26]. This
can be used to resolve, for example, a case where multiple RDF triples claim different
values for a person’s homepage (whose cardinality constraint is one).

References
1. Ding, L., Finin, T., Joshi, A., Pan, R., Cost, R.S., Peng, Y., Reddivari, P., Doshi, V.C., Sachs,
J.: Swoogle: A search and metadata engine for the semantic web. In: CIKM’04. (2004)
2. Wang, R., Storey, V., Firth, C.: A framework for analysis of data quality research. IEEE
Transactions on Knowledge and Data Engineering 7 (1995) 623–639
3. Patel, C., Supekar, K., Lee, Y., Park, E.K.: OntoKhoj: a semantic web portal for ontology
searching, ranking and classification. In: WIDM’03. (2003) 58–61
4. Alani, H., Brewster, C.: Ontology ranking based on the analysis of concept structures. In:
Proc. of the 3rd International Conference on Knowledge Capture (K-Cap). (2005)
21

http://www.semanticwebsearch.com/

170

L. Ding et al.

5. Stojanovic, N., Studer, R., Stojanovic, L.: An approach for the ranking of query results in
the semantic web. In: ISWC’03. (2003)
6. Anyanwu, K., Maduko, A., Sheth, A.: Semrank: Ranking complex relationship search results
on the semantic web. In: WWW’05. (2005) 117 –127
7. Page, L., Brin, S., Motwani, R., Winograd, T.: The pagerank citation ranking: Bringing order
to the web. Technical report, Stanford University (1998)
8. Haveliwala, T.H.: Topic-sensitive pagerank. In: WWW’02. (2002) 517–526
9. Jeh, G., Widom, J.: Scaling personalized web search. In: WWW ’03. (2003) 271–279
10. Xing, W., Ghorbani, A.A.: Weighted pagerank algorithm. In: Proc. of the 2nd Annual
Conference on Communication Networks and Services Research. (2004) 305–314
11. Nie, Z., Zhang, Y., Wen, J.R., Ma, W.Y.: Object-level ranking: Bringing order to web objects.
In: WWW’05. (2005) 567–574
12. Zhuge, H., Zheng, L.: Ranking semantic-linked network. In: WWW’03 Posters. (2003)
13. Baeza-Yates, R., Davis, E.: Web page ranking using link attributes. In: WWW’04 Posters.
(2004) 328–329
14. Wand, Y., Wang, R.Y.: Anchoring data quality dimensions in ontological foundations. Communications of the ACM 39 (1996) 86–95
15. Kanh, B.K., Strong, D.M., Wang, R.Y.: Information quality benchmarks: Product and service
performance. Communications of the ACM 45 (2002) 184–192
16. Supekar, K., Patel, C., Lee, Y.: Characterizing quality of knowledge on semantic web. In:
Proc. of 7th International Florida Artificial Intelligence Research Society Conf. (2002)
17. Guo, L., Shao, F., Botev, C., Shanmugasundaram, J.: XRANK: ranked keyword search over
XML documents. In: SIGMOD’03. (2003) 16–27
18. Balmin, A., Hristidis, V., Papakonstantinou, Y.: ObjectRank: Authority-based keyword
search in databases. In: VLDB’04. (2004) 564 – 575
19. Xi, W., Zhang, B., Chen, Z., Lu, Y., Yan, S., Ma, W.Y., Fox, E.A.: Link fusion: A unified link
analysis framework for multi-type interrelated data objects. In: WWW’04. (2004) 319–327
20. Rocha, C., Schwabe, D., Aragao, M.P.: A hybrid approach for searching in the semantic web.
In: WWW’04. (2004) 374–383
21. Aleman-Meza, B., Halaschek, C., Arpinar, I.B., Sheth, A.: Context-aware semantic association ranking. In: SWDB’03. (2003) 33–50
22. Bizer, C.: Semantic web trust and security resource guide. (http://www.wiwiss.fuberlin.de/suhl/bizer/SWTSGuide/ (last accessed 08-11-05))
23. Ding, L., Kolari, P., Finin, T., Joshi, A., Peng, Y., Yesha, Y.: On homeland security and the
semantic web: A provenance and trust aware inference framework. In: Proceedings of the
AAAI Spring Symposium on AI Technologies for Homeland Security. (2005)
24. Volz, R., Oberle, D., Maedche, A.: Towards a modularized semantic web. In: Proceedings
of the ECAI’02 Workshop on Ontologies and Semantic Interoperability. (2002)
25. Grau, B.C., Parsia, B., Sirin, E.: Working with multiple ontologies on the semantic web. In:
ISWC’04. (2004)
26. Ding, L., Finin, T., Peng, Y., da Silva, P.P., McGuinness, D.L.: Tracking rdf graph provenance
using rdf molecules. Technical Report TR-05-06, UMBC (2005)

CONGA: Distributed Congestion-Aware Load Balancing
for Datacenters
Mohammad Alizadeh, Tom Edsall, Sarang Dharmapurikar, Ramanan Vaidyanathan, Kevin Chu,
Andy Fingerhut, Vinh The Lam (Google), Francis Matus, Rong Pan, Navindra Yadav,
George Varghese (Microsoft)
Cisco Systems

ABSTRACT

to paths, hash collisions can cause significant imbalance if there are
a few large flows. More importantly, ECMP uses a purely local decision to split traffic among equal cost paths without knowledge of
potential downstream congestion on each path. Thus ECMP fares
poorly with asymmetry caused by link failures that occur frequently
and are disruptive in datacenters [17, 34]. For instance, the recent
study by Gill et al. [17] shows that failures can reduce delivered
traffic by up to 40% despite built-in redundancy.
Broadly speaking, the prior work on addressing ECMP’s shortcomings can be classified as either centralized scheduling (e.g.,
Hedera [2]), local switch mechanisms (e.g., Flare [27]), or hostbased transport protocols (e.g., MPTCP [41]). These approaches
all have important drawbacks. Centralized schemes are too slow
for the traffic volatility in datacenters [28, 8] and local congestionaware mechanisms are suboptimal and can perform even worse
than ECMP with asymmetry (§2.4). Host-based methods such as
MPTCP are challenging to deploy because network operators often
do not control the end-host stack (e.g., in a public cloud) and even
when they do, some high performance applications (such as low
latency storage systems [39, 7]) bypass the kernel and implement
their own transport. Further, host-based load balancing adds more
complexity to an already complex transport layer burdened by new
requirements such as low latency and burst tolerance [4] in datacenters. As our experiments with MPTCP show, this can make for
brittle performance (§5).
Thus from a philosophical standpoint it is worth asking: Can
load balancing be done in the network without adding to the complexity of the transport layer? Can such a network-based approach
compute globally optimal allocations, and yet be implementable in
a realizable and distributed fashion to allow rapid reaction in microseconds? Can such a mechanism be deployed today using standard encapsulation formats? We seek to answer these questions
in this paper with a new scheme called CONGA (for Congestion
Aware Balancing). CONGA has been implemented in custom ASICs
for a major new datacenter fabric product line. While we report on
lab experiments using working hardware together with simulations
and mathematical analysis, customer trials are scheduled in a few
months as of the time of this writing.
Figure 1 surveys the design space for load balancing and places
CONGA in context by following the thick red lines through the design tree. At the highest level, CONGA is a distributed scheme to
allow rapid round-trip timescale reaction to congestion to cope with
bursty datacenter traffic [28, 8]. CONGA is implemented within the
network to avoid the deployment issues of host-based methods and
additional complexity in the transport layer. To deal with asymmetry, unlike earlier proposals such as Flare [27] and LocalFlow [44]
that only use local information, CONGA uses global congestion
information, a design choice justified in detail in §2.4.

We present the design, implementation, and evaluation of CONGA,
a network-based distributed congestion-aware load balancing mechanism for datacenters. CONGA exploits recent trends including
the use of regular Clos topologies and overlays for network virtualization. It splits TCP flows into flowlets, estimates real-time
congestion on fabric paths, and allocates flowlets to paths based
on feedback from remote switches. This enables CONGA to efficiently balance load and seamlessly handle asymmetry, without requiring any TCP modifications. CONGA has been implemented in
custom ASICs as part of a new datacenter fabric. In testbed experiments, CONGA has 5× better flow completion times than ECMP
even with a single link failure and achieves 2–8× better throughput than MPTCP in Incast scenarios. Further, the Price of Anarchy for CONGA is provably small in Leaf-Spine topologies; hence
CONGA is nearly as effective as a centralized scheduler while being able to react to congestion in microseconds. Our main thesis
is that datacenter fabric load balancing is best done in the network,
and requires global schemes such as CONGA to handle asymmetry.
Categories and Subject Descriptors: C.2.1 [Computer-Communication
Networks]: Network Architecture and Design
Keywords: Datacenter fabric; Load balancing; Distributed

1.

INTRODUCTION

Datacenter networks being deployed by cloud providers as well
as enterprises must provide large bisection bandwidth to support
an ever increasing array of applications, ranging from financial services to big-data analytics. They also must provide agility, enabling
any application to be deployed at any server, in order to realize
operational efficiency and reduce costs. Seminal papers such as
VL2 [18] and Portland [1] showed how to achieve this with Clos
topologies, Equal Cost MultiPath (ECMP) load balancing, and the
decoupling of endpoint addresses from their location. These design principles are followed by next generation overlay technologies that accomplish the same goals using standard encapsulations
such as VXLAN [35] and NVGRE [45].
However, it is well known [2, 41, 9, 27, 44, 10] that ECMP can
balance load poorly. First, because ECMP randomly hashes flows
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGCOMM’14, August 17–22, 2014, Chicago, IL, USA.
Copyright 2014 ACM 978-1-4503-2836-4/14/08 ...$15.00.
http://dx.doi.org/10.1145/2619239.2626316 .

503

Centralized	
  

(e.g.,	
  Hedera,	
  B4,	
  SWAN)	
  
slow	
  to	
  react	
  for	
  DCs	
  

	
  	
  Host-­‐Based	
  

	
  	
  In-­‐Network	
  
Local	
  	
  
	
  (Stateless	
  or	
  Conges6on-­‐Aware)	
  	
  

analysis. We also prove that load balancing behavior and the
effectiveness of flowlets depends on the coefficient of variation of the flow size distribution.

Distributed	
  

Global,	
  Conges6on-­‐Aware	
  

(e.g.,	
  MPTCP)	
  
Hard	
  to	
  deploy,	
  
increases	
  Incast	
  

2.

(e.g.,	
  ECMP,	
  Flare,	
  LocalFlow)	
  
Poor	
  with	
  asymmetry,	
  
	
  General,	
  Link	
  State	
  
Leaf	
  to	
  Leaf	
  	
  
especially	
  with	
  TCP	
  traﬃc	
  
Complex,	
  hard	
  to	
  
Near-­‐op>mal	
  for	
  2-­‐>er,	
  
deploy	
  
deployable	
  with	
  an	
  overlay	
  

	
  Per	
  Flow	
  

Subop>mal	
  for	
  “heavy”	
  	
  
ﬂow	
  distribu>ons	
  (with	
  large	
  ﬂows)	
  

	
  Per	
  Flowlet	
  (CONGA)	
  

No	
  TCP	
  modiﬁca>ons,	
  
resilient	
  to	
  ﬂow	
  distribu>on	
  	
  

DESIGN DECISIONS

This section describes the insights that inform CONGA’s major
design decisions. We begin with the desired properties that have
guided our design. We then revisit the design decisions shown in
Figure 1 in more detail from the top down.

	
  Per	
  Packet	
  

2.1

Op>mal,	
  needs	
  
reordering-­‐resilient	
  TCP	
  

Desired Properties

CONGA is an in-network congestion-aware load balancing mechanism for datacenter fabrics. In designing CONGA, we targeted a
solution with a number of key properties:

Figure 1: Design space for load balancing.

1. Responsive: Datacenter traffic is very volatile and bursty [18,
28, 8] and switch buffers are shallow [4]. Thus, with CONGA,
we aim for rapid round-trip timescale (e.g., 10s of microseconds) reaction to congestion.

Next, the most general design would sense congestion on every
link and send generalized link state packets to compute congestionsensitive routes [16, 48, 51, 36]. However, this is an N-party protocol with complex control loops, likely to be fragile and hard to
deploy. Recall that the early ARPANET moved briefly to such
congestion-sensitive routing and then returned to static routing, citing instability as a reason [31]. CONGA instead uses a 2-party
“leaf-to-leaf” mechanism to convey path-wise congestion metrics
between pairs of top-of-the-rack switches (also termed leaf switches)
in a datacenter fabric. The leaf-to-leaf scheme is provably nearoptimal in typical 2-tier Clos topologies (henceforth called LeafSpine), simple to analyze, and easy to deploy. In fact, it is deployable in datacenters today with standard overlay encapsulations
(VXLAN [35] in our implementation) which are already being used
to enable workload agility [33].
With the availability of very high-density switching platforms
for the spine (or core) with 100s of 40Gbps ports, a 2-tier fabric can
scale upwards of 20,000 10Gbps ports.1 This design point covers
the needs of the overwhelming majority of enterprise datacenters,
which are the primary deployment environments for CONGA.
Finally, in the lowest branch of the design tree, CONGA is constructed to work with flowlets [27] to achieve a higher granularity
of control and resilience to the flow size distribution while not requiring any modifications to TCP. Of course, CONGA could also
be made to operate per packet by using a very small flowlet inactivity gap (see §3.4) to perform optimally with a future reorderingresilient TCP.
In summary, our major contributions are:
• We design (§3) and implement (§4) CONGA, a distributed
congestion-aware load balancing mechanism for datacenters.
CONGA is immediately deployable, robust to asymmetries
caused by link failures, reacts to congestion in microseconds,
and requires no end-host modifications.

2. Transport independent: As a network mechanism, CONGA
must be oblivious to the transport protocol at the end-host
(TCP, UDP, etc). Importantly, it should not require any modifications to TCP.
3. Robust to asymmetry: CONGA must handle asymmetry due
to link failures (which have been shown to be frequent and
disruptive in datacenters [17, 34]) or high bandwidth flows
that are not well balanced.
4. Incrementally deployable: It should be possible to apply
CONGA to only a subset of the traffic and only a subset of
the switches.
5. Optimized for Leaf-Spine topology: CONGA must work
optimally for 2-tier Leaf-Spine topologies (Figure 4) that cover
the needs of most enterprise datacenter deployments, though
it should also benefit larger topologies.

2.2

Why Distributed Load Balancing?

The distributed approach we advocate is in stark contrast to recently proposed centralized traffic engineering designs [2, 9, 23,
21]. This is because of two important features of datacenters. First,
datacenter traffic is very bursty and unpredictable [18, 28, 8]. CONGA
reacts to congestion at RTT timescales (∼100µs) making it more
adept at handling high volatility than a centralized scheduler. For
example, the Hedera scheduler in [2] runs every 5 seconds; but it
would need to run every 100ms to approach the performance of
a distributed solution such as MPTCP [41], which is itself outperformed by CONGA (§5). Second, datacenters use very regular topologies. For instance, in the common Leaf-Spine topology
(Figure 4), all paths are exactly two-hops. As our experiments (§5)
and analysis (§6) show, distributed decisions are close to optimal in
such regular topologies.
Of course, a centralized approach is appropriate for WANs where
traffic is stable and predictable and the topology is arbitrary. For
example, Google’s inter-datacenter traffic engineering algorithm
needs to run just 540 times per day [23].

• We extensively evaluate (§5) CONGA with a hardware testbed
and packet-level simulations. We show that even with a single
link failure, CONGA achieves more than 5× better flow completion time and 2× better job completion time respectively
for a realistic datacenter workload and a standard Hadoop Distributed File System benchmark. CONGA is at least as good
as MPTCP for load balancing while outperforming MPTCP
by 2–8× in Incast [47, 12] scenarios.
• We analyze (§6) CONGA and show that it is nearly optimal
in 2-tier Leaf-Spine topologies using “Price of Anarchy” [40]

2.3

Why In-Network Load Balancing?

Continuing our exploration of the design space (Figure 1), the
next question is where should datacenter fabric load balancing be
implemented — the transport layer at the end-hosts or the network?

1

For example, spine switches with 576 40Gbps ports can be paired
with typical 48-port leaf switches to enable non-blocking fabrics
with 27,648 10Gbps ports.

504

80

S0	
  

50

80

L0	
  

80
L1	
  

80

40

40

S0	
  

40

80

L0	
  

80
L1	
  

80

40

40

S0	
  

66.6

L0	
  

L1	
  

80

33.3

40

L0	
  

80

S0	
  

40

40

S1	
  

S1	
  

(a) Static (ECMP)

(b) Congestion-Aware:
Local Only

(c) Congestion-Aware:
Global (CONGA)

40
L1	
  

40

40
L1	
  

(a)  L0L2=0, L1L2=80

Figure 2: Congestion-aware load balancing needs non-local information with asymmetry. Here, L0 has 100Gbps of TCP traffic to L1, and the (S1, L1) link has half the capacity of the
other links. Such cases occur in practice with link-aggregation
(which is very common), for instance, if a link fails in a fabric
with two 40Gbps links connecting each leaf and spine switch.

S0	
  

40

40

40
L2	
  

40

S1	
  

40

40
40

L2	
  

40

S1	
  

L0	
  

40

S1	
  

(b) L0L2=40, L1L2=40

Figure 3: Optimal traffic split in asymmetric topologies depends on the traffic matrix. Here, the L1→L2 flow must adjust
its traffic through S0 based on the amount of L0→L2 traffic.
Per-­‐link	
  Conges*on	
  
Measurement	
  
Spine Tier
Conges*on	
  
Feedback	
  

The state-of-the-art multipath transport protocol, MPTCP [41],
splits each connection into multiple (e.g., 8) sub-flows and balances traffic across the sub-flows based on perceived congestion.
Our experiments (§5) show that while MPTCP is effective for load
balancing, its use of multiple sub-flows actually increases congestion at the edge of the network and degrades performance in Incast
scenarios (Figure 13). Essentially, MPTCP increases the burstiness
of traffic as more sub-flows contend at the fabric’s access links.
Note that this occurs despite MPTCP’s coupled congestion control
algorithm [50] which is designed to handle shared bottlenecks, because while the coupled congestion algorithm works if flows are
in steady-state, in realistic datacenter workloads many flows are
short-lived and transient [18].
The larger architectural point however is that datacenter fabric
load balancing is too specific to be implemented in the transport
stack. Datacenter fabrics are highly-engineered, homogenous systems [1, 34]. They are designed from the onset to behave like a
giant switch [18, 1, 25], much like the internal fabric within large
modular switches. Binding the fabric’s load balancing behavior to
the transport stack which already needs to balance multiple important requirements (e.g., high throughput, low latency, and burst tolerance [4]) is architecturally unappealing. Further, some datacenter
applications such as high performance storage systems bypass the
kernel altogether [39, 7] and hence cannot use MPTCP.

2.4

Overlay	
  
Network	
  

LB	
  Decision	
  
Source
Leaf

012 3

?

Destination
Leaf

Gap

Flowlet	
  Detec*on	
  

Figure 4: CONGA architecture. The source leaf switch detects flowlets and routes them via the least congested path to
the destination using congestion metrics obtained from leaf-toleaf feedback. Note that the topology could be asymmetric.

course, global congestion-aware load balancing (as in CONGA)
does not have this issue.
The reader may wonder if asymmetry can be handled by some
form of oblivious routing [32] such as weighted random load balancing with weights chosen according to the topology. For instance, in the above example we could give the lower path half
the weight of the upper path and achieve the same traffic split as
CONGA. While this works in this case, it fails in general because
the “right” traffic splits in asymmetric topologies depend also on
the traffic matrix, as shown by the example in Figure 3. Here, depending on how much L0→L2 traffic there is, the optimal split for
L1→L2 traffic changes. Hence, static weights cannot handle both
cases in Figure 3. Note that in this example, the two L1→L2 paths
are symmetric when considered in isolation. But because of an
asymmetry in another part of the network, the L0→L2 traffic creates a bandwidth asymmetry for the L1→L2 traffic that can only
be detected by considering non-local congestion. Note also that a
local congestion-aware mechanism could actually perform worse
than ECMP; for example, in the scenario in part (b).

Why Global Congestion Awareness?

Next, we consider local versus global schemes (Figure 1). Handling asymmetry essentially requires non-local knowledge about
downstream congestion at the switches. With asymmetry, a switch
cannot simply balance traffic based on the congestion of its local
links. In fact, this may lead to even worse performance than a static
scheme such as ECMP (which does not consider congestion at all)
because of poor interaction with TCP’s control loop.
As an illustration, consider the simple asymmetric scenario in
Figure 2. Leaf L0 has 100Gbps of TCP traffic demand to Leaf
L1. Static ECMP splits the flows equally, achieving a throughput
of 90Gbps because the flows on the lower path are bottlenecked
at the 40Gbps link (S1, L1). Local congestion-aware load balancing is actually worse with a throughput of 80Gbps. This is because as TCP slows down the flows on the lower path, the link
(L0, S1) appears less congested. Hence, paradoxically, the local
scheme shifts more traffic to the lower link until the throughput
on the upper link is also 40 Gbps. This example illustrates a fundamental limitation of any local scheme (such as Flare [27], LocalFlow [44], and packet-spraying [10]) that strictly enforces an
equal traffic split without regard for downstream congestion. Of

2.5

Why Leaf-to-Leaf Feedback?

At the heart of CONGA is a leaf-to-leaf feedback mechanism
that conveys real-time path congestion metrics to the leaf switches.
The leaves use these metrics to make congestion-aware load balancing decisions based on the global (fabric-wide) state of congestion (Figure 4). We now argue (following Figure 1) why leaf-to-leaf
congestion signaling is simple and natural for modern data centers.
Overlay network: CONGA operates in an overlay network consisting of “tunnels” between the fabric’s leaf switches. When an
endpoint (server or VM) sends a packet to the fabric, the source
leaf, or source tunnel endpoint (TEP), looks up the destination end-

505

Frac%on	
  of	
  Data	
  Bytes	
  

point’s address (either MAC or IP) to determine to which leaf (destination TEP) the packet needs to be sent.2 It then encapsulates
the packet — with outer source and destination addresses set to the
source and destination TEP addresses — and sends it to the spine.
The spine switches forward the packet to its destination leaf based
entirely on the outer header. Once the packet arrives at the destination leaf, it decapsulates the original packet and delivers it to the
intended recipient.
Overlays are deployed today to virtualize the physical network
and enable multi-tenancy and agility by decoupling endpoint identifiers from their location (see [38, 22, 33] for more details). However, the overlay also provides the ideal conduit for CONGA’s leafto-leaf congestion feedback mechanism. Specifically, CONGA leverages two key properties of the overlay: (i) The source leaf knows
the ultimate destination leaf for each packet, in contrast to standard
IP forwarding where the switches only know the next-hops. (ii)
The encapsulated packet has an overlay header (VXLAN [35] in
our implementation) which can be used to carry congestion metrics
between the leaf switches.
Congestion feedback: The high-level mechanism is as follows.
Each packet carries a congestion metric in the overlay header that
represents the extent of congestion the packet experiences as it
traverses through the fabric. The metric is updated hop-by-hop
and indicates the utilization of the most congested link along the
packet’s path. This information is stored at the destination leaf on
a per source leaf, per path basis and is opportunistically fed back
to the source leaf by piggybacking on packets in the reverse direction. There may be, in general, 100s of paths in a multi-tier
topology. Hence, to reduce state, the destination leaf aggregates
congestion metrics for one or more paths based on a generic identifier called the Load Balancing Tag that the source leaf inserts in
packets (see §3 for details).

2.6

Figure 5: Distribution of data bytes across transfer sizes for
different flowlet inactivity gaps.
flows through the network core. The cluster supports over 2000
diverse enterprise applications, including web, data-base, security,
and business intelligence services. The captures are obtained by
having a few leaf switches mirror traffic to an analyzer without disrupting production traffic. Overall, we analyze more than 150GB
of compressed packet trace data.
Flowlet size: Figure 5 shows the distribution of the data bytes versus flowlet size for three choices of flowlet inactivity gap: 250ms,
500µs, and 100µs. Since it is unlikely that we would see a gap
larger than 250ms in the same application-level flow, the line “Flow
(250ms)” essentially corresponds to how the bytes are spread across
flows. The plot shows that balancing flowlets gives significantly
more fine-grained control than balancing flows. Even with an inactivity gap of 500µs, which is quite large and poses little risk of
packet reordering in datacenters, we see nearly two orders of magnitude reduction in the size of transfers that cover most of the data:
50% of the bytes are in flows larger than ∼30MB, but this number
reduces to ∼500KB for “Flowlet (500µs)”.
Flowlet concurrency: Since the required flowlet inactivity gap is
very small in datacenters, we expect there to be a small number
of concurrent flowlets at any given time. Thus, the implementation cost for tracking flowlets should be low. To quantify this, we
measure the distribution of the number of distinct 5-tuples in our
packet trace over 1ms intervals. We find that the number of distinct 5-tuples (and thus flowlets) is small, with a median of 130 and
a maximum under 300. Normalizing these numbers to the average throughput in our trace (∼15Gbps), we estimate that even for
a very heavily loaded leaf switch with say 400Gbps of traffic, the
number of concurrent flowlets would be less than 8K. Maintaining
a table for tracking 64K flowlets is feasible at low cost (§3.4).

Why Flowlet Switching for Datacenters?

CONGA also employs flowlet switching, an idea first introduced
by Kandula et al. [27]. Flowlets are bursts of packets from a flow
that are separated by large enough gaps (see Figure 4). Specifically, if the idle interval between two bursts of packets is larger
than the maximum difference in latency among the paths, then the
second burst can be sent along a different path than the first without reordering packets. Thus flowlets provide a higher granularity
alternative to flows for load balancing (without causing reordering).

2.6.1

1	
  
Flow	
  (250ms)	
  
0.8	
  
Flowlet	
  (500μs)	
  
0.6	
  
Flowlet	
  (100μs)	
  
0.4	
  
0.2	
  
0	
  
1.E+01	
   1.E+03	
   1.E+05	
   1.E+07	
   1.E+09	
  
Size	
  (Bytes)	
  

Measurement analysis

3.

Flowlet switching has been shown to be an effective technique
for fine-grained load balancing across Internet paths [27], but how
does it perform in datacenters? On the one hand, the very high
bandwidth of internal datacenter flows would seem to suggest that
the gaps needed for flowlets may be rare, limiting the applicability
of flowlet switching. On the other hand, datacenter traffic is known
to be extremely bursty at short timescales (e.g., 10–100s of microseconds) for a variety of reasons such as NIC hardware offloads
designed to support high link rates [29]. Since very small flowlet
gaps suffice in datacenters to maintain packet order (because the
network latency is very low) such burstiness could provide sufficient flowlet opportunities.
We study the applicability of flowlet switching in datacenters using measurements from actual production datacenters. We instrument a production cluster with over 4500 virtualized and bare metal
hosts across ∼30 racks of servers to obtain packet traces of traffic

DESIGN

Figure 6 shows the system diagram of CONGA. The majority of
the functionality resides at the leaf switches. The source leaf makes
load balancing decisions based on per uplink congestion metrics,
derived by taking the maximum of the local congestion at the uplink
and the remote congestion for the path (or paths) to the destination
leaf that originate at the uplink. The remote metrics are stored in the
Congestion-To-Leaf Table on a per destination leaf, per uplink basis and convey the maximum congestion for all the links along the
path. The remote metrics are obtained via feedback from the destination leaf switch, which opportunistically piggybacks values in
its Congestion-From-Leaf Table back to the source leaf. CONGA
measures congestion using the Discounting Rate Estimator (DRE),
a simple module present at each fabric link.
Load balancing decisions are made on the first packet of each
flowlet. Subsequent packets use the same uplink as long as the
flowlet remains active (there is not a sufficiently long gap). The
source leaf uses the Flowlet Table to keep track of active flowlets
and their chosen uplinks.

2

How the mapping between end-point identifiers and their locations (leaf switches) is obtained and propagated is beyond the scope
of this paper.

506

A→B	
  
LBTag
=2	
  
CE=4	
  

Per-­‐link	
  DREs	
  
in	
  Spine	
  

1

0

2

3

Leaf	
  B	
  

3

2. The packet is routed through the fabric to the destination leaf.3
As it traverses each link, its CE field is updated if the link’s
congestion metric (given by the DRE) is larger than the current
value in the packet.

(Receiver)	
  
Per-­‐uplink	
  
DREs	
  

?

LB	
  Decision	
  
Conges7on-­‐To-­‐Leaf	
  
Flowlet	
  
Table	
  
Table	
  

Source	
  Leaf	
  

Dest	
  Leaf	
  

1. The source leaf sends the packet to the fabric with the LBTag
field set to the uplink port taken by the packet. It also sets the
CE field to zero.

Reverse	
  	
  
Path	
  Pkt	
  

Uplink	
  
01
k-­‐1	
  
B 25

Forward	
  	
  
Path	
  Pkt	
  

B→A	
  
FB_LBTag=1	
  
FB_Metric=5	
  

Leaf	
  A	
  

(Sender)	
  

To-Leaf Table at each leaf switch. We now describe the sequence
of events involved (refer to Figure 6 for an example).

LBTag	
  
01
k-­‐1	
  
B 25

3

3. The CE field of the packet received at the destination leaf gives
the maximum link congestion along the packet’s path. This
needs to be fed back to the source leaf. But since a packet
may not be immediately available in the reverse direction, the
destination leaf stores the metric in the Congestion-From-Leaf
Table (on a per source leaf, per LBTag basis) while it waits for
an opportunity to piggyback the feedback.

Conges7on-­‐From-­‐Leaf	
  
Table	
  

Figure 6: CONGA system diagram.

3.1

Packet format

4. When a packet is sent in the reverse direction, one metric from
the Congestion-From-Leaf Table is inserted in its FB_LBTag
and FB_Metric fields for the source leaf. The metric is chosen in round-robin fashion while, as an optimization, favoring
those metrics whose values have changed since the last time
they were fed back.

CONGA leverages the VXLAN [35] encapsulation format used
for the overlay to carry the following state:
• LBTag (4 bits): This field partially identifies the packet’s
path. It is set by the source leaf to the (switch-local) port number of the uplink the packet is sent on and is used by the destination leaf to aggregate congestion metrics before they are
fed back to the source. For example, in Figure 6, the LBTag
is 2 for both blue paths. Note that 4 bits is sufficient because
the maximum number of leaf uplinks in our implementation is
12 for a non-oversubscribed configuration with 48 × 10Gbps
server-facing ports and 12 × 40Gbps uplinks.

5. Finally, the source leaf parses the feedback in the reverse packet
and updates the Congestion-To-Leaf Table.
It is important to note that though we have described the forward
and reverse packets separately for simplicity, every packet simultaneously carries both a metric for its forward path and a feedback
metric. Also, while we could generate explicit feedback packets,
we decided to use piggybacking because we only need a very small
number of packets for feedback. In fact, all metrics between a pair
of leaf switches can be conveyed in at-most 12 packets (because
there are 12 distinct LBTag values), with the average case being
much smaller because the metrics only change at network roundtrip timescales, not packet-timescales (see the discussion in §3.6
regarding the DRE time constant).

• CE (3 bits): This field is used by switches along the packet’s
path to convey the extent of congestion.
• FB_LBTag (4 bits) and FB_Metric (3 bits): These two fields
are used by destination leaves to piggyback congestion information back to the source leaves. FB_LBTag indicates the
LBTag the feedback is for and FB_Metric provides its associated congestion metric.

3.2

Metric aging: A potential issue with not having explicit feedback
packets is that the metrics may become stale if sufficient traffic does
not exit for piggybacking. To handle this, a simple aging mechanism is added where a metric that has not been updated for a long
time (e.g., 10ms) will gradually decay to zero. Note that this also
guarantees that a path that appears to be congested will eventually
be probed again.

Discounting Rate Estimator (DRE)

The DRE is a simple module for measuring the load of a link.
The DRE maintains a register, X, which is incremented for each
packet sent over the link by the packet size in bytes, and is decremented periodically (every Tdre ) with a multiplicative factor α between 0 and 1: X ← X × (1 − α). It is easy to show that X is
proportional to the rate of traffic over the link; more precisely, if
the traffic rate is R, then X ≈ R · τ , where τ = Tdre /α. The
DRE algorithm is essentially a first-order low pass filter applied to
packet arrivals, with a (1 − e−1 ) rise time of τ . The congestion
metric for the link is obtained by quantizing X/Cτ to 3 bits (C is
the link speed).
The DRE algorithm is similar to the widely used Exponential
Weighted Moving Average (EWMA) mechanism. However, DRE
has two key advantages over EWMA: (i) it can be implemented
with just one register (whereas EWMA requires two); and (ii) the
DRE reacts more quickly to traffic bursts (because increments take
place immediately upon packet arrivals) while retaining memory of
past bursts. In the interest of space, we omit the details.

3.3

3.4

Flowlet Detection

Flowlets are detected and tracked in the leaf switches using the
Flowlet Table. Each entry of the table consists of a port number, a
valid bit, and an age bit. When a packet arrives, we lookup an entry
based on a hash of its 5-tuple. If the entry is valid (valid_bit ==
1), the flowlet is active and the packet is sent on the port indicated
in the entry. If the entry is not valid, the incoming packet starts
a new flowlet. In this case, we make a load balancing decision
(as described below) and cache the result in the table for use by
subsequent packets. We also set the valid bit.
Flowlet entries time out using the age bit. Each incoming packet
resets the age bit. A timer periodically (every Tf l seconds) checks
the age bit before setting it. If the age bit is set when the timer

Congestion Feedback

3
If there are multiple valid next hops to the destination (as in Figure 6), the spine switches pick one using standard ECMP hashing.

CONGA uses a feedback loop between the source and destination leaf switches to populate the remote metrics in the Congestion-

507

Spine 0

checks it, then there have not been any packets for that entry in the
last Tf l seconds and the entry times out (the valid bit is set to zero).
Note that a single age bit allows us to detect gaps between Tf l and
2Tf l . While not as accurate as using full timestamps, this requires
far fewer bits allowing us to maintain a very large number of entries
in the table (64K in our implementation).
Remark 1. Although with hashing, flows may collide in the Flowlet
Table, this is not a big concern. Collisions simply imply some load
balancing opportunities are lost which does not matter as long as
this does not occur too frequently.

4x40Gbps
Leaf 0
32x10Gbps

4x40Gbps

4x40Gbps
Leaf 0

Leaf 1
32x10Gbps

32x10Gbps

3x40Gbps
Leaf 1
32x10Gbps

(a) Baseline (no failure)
(b) With link failure
Figure 7: Topologies used in testbed experiments.
1	
  
Flow	
  Size	
  
0.8	
  
Bytes	
  
0.6	
  
0.4	
  
0.2	
  
0	
  
1.E+01	
   1.E+03	
   1.E+05	
   1.E+07	
   1.E+09	
  
Size	
  (Bytes)	
  

CDF	
  

1	
  
Flow	
  Size	
  
0.8	
  
Bytes	
  
0.6	
  
0.4	
  
0.2	
  
0	
  
1.E+01	
   1.E+03	
   1.E+05	
   1.E+07	
   1.E+09	
  
Size	
  (Bytes)	
  

Load Balancing Decision Logic

(a) Enterprise workload
(b) Data-mining workload
Figure 8: Empirical traffic distributions. The Bytes CDF shows
the distribution of traffic bytes across different flow sizes.

5.

Parameter Choices

EVALUATION

In this section, we evaluate CONGA’s performance with a real
hardware testbed as well as large-scale simulations. Our testbed experiments illustrate CONGA’s good performance for realistic empirical workloads, Incast micro-benchmarks, and actual applications. Our detailed packet-level simulations confirm that CONGA
scales to large topologies.

CONGA has three main parameters: (i) Q, the number of bits for
quantizing congestion metrics (§3.1); (ii) τ , the DRE time constant,
given by Tdre /α (§3.2); and (iii) Tf l , the flowlet inactivity timeout
(§3.4). We set these parameters experimentally. A control-theoretic
analysis of CONGA is beyond the scope of this paper, but our parameter choices strike a balance between the stability and responsiveness of CONGA’s control loop while taking into consideration
practical matters such as interaction with TCP and implementation
cost (header requirements, table size, etc).
The parameters Q and τ determine the “gain” of CONGA’s control loop and exhibit important tradeoffs. A large Q improves congestion metric accuracy, but if too large can make the leaf switches
over-react to minor differences in congestion causing oscillatory
behavior. Similarly, a small τ makes the DRE respond quicker, but
also makes it more susceptible to noise from transient traffic bursts.
Intuitively, τ should be set larger than the network RTT to filter the
sub-RTT traffic bursts of TCP. The flowlet timeout, Tf l , can be
set to the maximum possible leaf-to-leaf latency to guarantee no
packet reordering. This value can be rather large though because of
worst-case queueing delays (e.g., 13ms in our testbed), essentially
disabling flowlets. Reducing Tf l presents a compromise between
more packet reordering versus less congestion (fewer packet drops,
lower latency) due to better load balancing.
Overall, we have found CONGA’s performance to be fairly robust with: Q = 3 to 6, τ = 100µs to 500µs, and Tf l = 300µs
to 1ms. The default parameter values for our implementation are:
Q = 3, τ = 160µs, and Tf l = 500µs.

4.

Spine 1
Link
Failure

Load balancing decisions are made on the first packet of each
flowlet (other packets use the port cached in the Flowlet Table).
For a new flowlet, we pick the uplink port that minimizes the maximum of the local metric (from the local DREs) and the remote
metric (from the Congestion-To-Leaf Table). If multiple uplinks
are equally good, one is chosen at random with preference given
to the port cached in the (invalid) entry in the Flowlet Table; i.e., a
flow only moves if there is a strictly better uplink than the one its
last flowlet took.

3.6

Spine 0

CDF	
  

3.5

Spine 1

Schemes compared: We compare CONGA, CONGA-Flow, ECMP,
and MPTCP [41], the state-of-the-art multipath transport protocol. CONGA and CONGA-Flow differ only in their choice of
the flowlet inactivity timeout. CONGA uses the default value:
Tf l = 500µs. CONGA-Flow however uses Tf l = 13ms which is
greater than the maximum path latency in our testbed and ensures
no packet reordering. CONGA-Flow’s large timeout effectively
implies one load balancing decision per flow, similar to ECMP.
Of course, in contrast to ECMP, decisions in CONGA-Flow are
informed by the path congestion metrics. The rest of the parameters for both variants of CONGA are set as described in §3.6. For
MPTCP, we use MPTCP kernel v0.87 available on the web [37].
We configure MPTCP to use 8 sub-flows for each TCP connection,
as Raiciu et al. [41] recommend.

5.1

Testbed

Our testbed consists of 64 servers and four switches (two leaves
and two spines). As shown in Figure 7, the servers are organized
in two racks (32 servers each) and attach to the leaf switches with
10Gbps links. In the baseline topology (Figure 7(a)), the leaf switches
connect to each spine switch with two 40Gbps uplinks. Note that
there is a 2:1 oversubscription at the Leaf level, typical of today’s
datacenter deployments. We also consider the asymmetric topology
in Figure 7(b) where one of the links between Leaf 1 and Spine 1 is
down. The servers have 12-core Intel Xeon X5670 2.93GHz CPUs,
128GB of RAM, and 3 2TB 7200RPM HDDs.

IMPLEMENTATION

CONGA has been implemented in custom switching ASICs for
a major datacenter fabric product line. The implementation includes ASICs for both the leaf and spine nodes. The Leaf ASIC
implements flowlet detection, congestion metric tables, DREs, and
the leaf-to-leaf feedback mechanism. The Spine ASIC implements
the DRE and its associated congestion marking mechanism. The
ASICs provide state-of-the-art switching capacities. For instance,
the Leaf ASIC has a non-blocking switching capacity of 960Gbps
in 28nm technology. CONGA’s implementation requires roughly
2.4M gates and 2.8Mbits of memory in the Leaf ASIC, and consumes negligible die area (< 2%).

5.2

Empirical Workloads

We begin with experiments with realistic workloads based on
empirically observed traffic patterns in deployed datacenters. Specifically, we consider the two flow size distributions shown in Figure 8. The first distribution is derived from packet traces from our
own datacenters (§2.6) and represents a large enterprise workload.
The second distribution is from a large cluster running data mining

508

5	
  
4	
  
3	
  
2	
  
1	
  
0	
  

1.6	
  
1.4	
  
1.2	
  
1	
  
0.8	
  
0.6	
  
0.4	
  
0.2	
  
0	
  

10	
   20	
   30	
   40	
   50	
   60	
   70	
   80	
   90	
  
Load	
  (%)	
  

1.2	
  

FCT	
  (Norm.	
  to	
  ECMP)	
  

FCT	
  (Norm.	
  to	
  Op.mal)	
  

ECMP	
  
CONGA-­‐Flow	
  
CONGA	
  
MPTCP	
  

FCT	
  (Norm.	
  to	
  ECMP)	
  

6	
  

1	
  

0.8	
  
0.6	
  

ECMP	
  
CONGA-­‐Flow	
  
CONGA	
  
MPTCP	
  

0.4	
  
0.2	
  
0	
  

ECMP	
  
CONGA-­‐Flow	
  
CONGA	
  
MPTCP	
  
10	
   20	
   30	
   40	
   50	
   60	
   70	
   80	
   90	
  
Load	
  (%)	
  

10	
   20	
   30	
   40	
   50	
   60	
   70	
   80	
   90	
  
Load	
  (%)	
  

(a) Overall Average FCT
(b) Small Flows (<100KB)
(c) Large Flows (> 10MB)
Figure 9: FCT statistics for the enterprise workload with the baseline topology. Note that part (a) is normalized to the optimal FCT,
while parts (b) and (c) are normalized to the value achieved by ECMP. The results are the average of 5 runs.

8	
  
6	
  

ECMP	
  
CONGA-­‐Flow	
  
CONGA	
  
MPTCP	
  

4	
  
2	
  
0	
  
10	
   20	
   30	
   40	
   50	
   60	
   70	
   80	
   90	
  
Load	
  (%)	
  

1.4	
  
1.2	
  
1	
  
0.8	
  
0.6	
  
0.4	
  
0.2	
  
0	
  

1.2	
  

FCT	
  (Norm.	
  to	
  ECMP)	
  

10	
  

FCT	
  (Norm.	
  to	
  ECMP)	
  

FCT	
  (Norm.	
  to	
  Op.mal)	
  

12	
  

1	
  

0.8	
  
0.6	
  

ECMP	
  
CONGA-­‐Flow	
  
CONGA	
  
MPTCP	
  

0.4	
  
0.2	
  

10	
   20	
   30	
   40	
   50	
   60	
   70	
   80	
   90	
  
Load	
  (%)	
  

0	
  

ECMP	
  
CONGA-­‐Flow	
  
CONGA	
  
MPTCP	
  
10	
   20	
   30	
   40	
   50	
   60	
   70	
   80	
   90	
  
Load	
  (%)	
  

(a) Overall Average FCT
(b) Small Flows (< 100KB)
(c) Large Flows (> 10MB)
Figure 10: FCT statistics for the data-mining workload with the baseline topology.
jobs [18]. Note that both distributions are heavy-tailed: A small
fraction of the flows contribute most of the data. Particularly, the
data-mining distribution has a very heavy tail with 95% of all data
bytes belonging to ∼3.6% of flows that are larger than 35MB.
We develop a simple client-server program to generate the traffic.
The clients initiate 3 long-lived TCP connections to each server
and request flows according to a Poisson process from randomly
chosen servers. The request rate is chosen depending on the desired
offered load and the flow sizes are sampled from one of the above
distributions. All 64 nodes run both client and server processes.
Since we are interested in stressing the fabric’s load balancing, we
configure the clients under Leaf 0 to only use the servers under
Leaf 1 and vice-versa, thereby ensuring that all traffic traverses the
Spine. Similar to prior work [20, 5, 13], we use the flow completion
time (FCT) as our main performance metric.

5.2.1

erage FCT than ECMP. Similar to the enterprise workload, we find
a degradation in FCT for small flows with MPTCP compared to the
other schemes.
Analysis: The above results suggest a tradeoff with MPTCP between achieving good load balancing in the core of the fabric and
managing congestion at the edge. Essentially, while using 8 subflows per connection helps MPTCP achieve better load balancing,
it also increases congestion at the edge links because the multiple
sub-flows cause more burstiness. Further, as observed by Raiciu et
al. [41], the small sub-flow window sizes for short flows increases
the chance of timeouts. This hurts the performance of short flows
which are more sensitive to additional latency and packet drops.
CONGA on the other hand does not have this problem since it does
not change the congestion control behavior of TCP.
Another interesting observation is the distinction between the enterprise and data-mining workloads. In the enterprise workload,
ECMP actually does quite well leaving little room for improvement by the more sophisticated schemes. But for the data-mining
workload, ECMP is noticeably worse. This is because the enterprise workload is less “heavy”; i.e., it has fewer large flows. In
particular, ∼50% of all data bytes in the enterprise workload are
from flows that are smaller than 35MB (Figure 8(a)). But in the
data mining workload, flows smaller than 35MB contribute only
∼5% of all bytes (Figure 8(b)). Hence the data-mining workload is
more challenging to handle from a load balancing perspective. We
quantify the impact of the workload analytically in §6.2.

Baseline

Figures 9 and 10 show the results for the two workloads with
the baseline topology (Figure 7(a)). Part (a) of each figure shows
the overall average FCT for each scheme at traffic loads between
10–90%. The values here are normalized to the optimal FCT that
is achievable in an idle network. In parts (b) and (c), we break
down the FCT for each scheme for small (< 100KB) and large
(> 10MB) flows, normalized to the value achieved by ECMP. Each
data point is the average of 5 runs with the error bars showing the
range. (Note that the error bars are not visible at all data points.)
Enterprise workload: We find that the overall average FCT is similar for all schemes in the enterprise workload, except for MPTCP
which is up to ∼25% worse than the others. MPTCP’s higher overall average FCT is because of the small flows, for which it is up
to ∼50% worse than ECMP. CONGA and similarly CONGA-Flow
are slightly worse for small flows (∼12–19% at 50–80% load), but
improve FCT for large flows by as much as ∼20%.

5.2.2

Impact of link failure

We now repeat both workloads for the asymmetric topology when
a fabric link fails (Figure 7(b)). The overall average FCT for both
workloads is shown in Figure 11. Note that since in this case the
bisection bandwidth between Leaf 0 and Leaf 1 is 75% of the original capacity (3 × 40Gbps links instead of 4), we only consider
offered loads up to 70%. As expected, ECMP’s performance drastically deteriorates as the offered load increases beyond 50%. This
is because with ECMP, half of the traffic from Leaf 0 to Leaf 1

Data-mining workload: For the data-mining workload, ECMP is
noticeably worse than the other schemes at the higher load levels.
Both CONGA and MPTCP achieve up to ∼35% better overall av-

509

6	
  

20	
  

ECMP	
  
CONGA-­‐Flow	
  
CONGA	
  
MPTCP	
  

15	
  
10	
  

4	
  
2	
  
0	
  

0.8	
  
0.6	
  

ECMP	
  
CONGA-­‐Flow	
  
CONGA	
  
MPTCP	
  

0.4	
  
0.2	
  

5	
  

0	
  

0	
  

10	
   20	
   30	
   40	
   50	
   60	
   70	
  
Load	
  (%)	
  

(a) Enterprise workload: Avg FCT

1	
  

ECMP	
  
CONGA-­‐Flow	
  
CONGA	
  
MPTCP	
  

CDF	
  

8	
  

FCT	
  (Norm.	
  to	
  Op.mal)	
  

FCT	
  (Norm.	
  to	
  Op.mal)	
  

10	
  

10	
   20	
   30	
   40	
   50	
   60	
   70	
  
Load	
  (%)	
  

(b) Data-mining workload: Avg FCT

0	
  

2	
  
4	
  
6	
  
Queue	
  Length	
  (MBytes)	
  

8	
  

(c) Queue length at hotspot

CDF	
  

Figure 11: Impact of link failure. Parts (a) and (b) show that overall average FCT for both workloads. Part (c) shows the CDF of
queue length at the hotspot port [Spine1→Leaf1] for the data-mining workload at 60% load.
1	
  
0.8	
  
0.6	
  
0.4	
  
0.2	
  
0	
  

The throughput imbalance is defined as the maximum throughput
(among the 4 uplinks) minus the minimum divided by the average:
(M AX − M IN )/AV G. This is calculated from synchronous
samples of the throughput of the 4 uplinks over 10ms intervals,
measured using a special debugging module in the ASIC.
The results confirm that CONGA is at least as efficient as MPTCP
for load balancing (without any TCP modifications) and is significantly better than ECMP. CONGA’s throughput imbalance is even
lower than MPTCP for the enterprise workload. With CONGAFlow, the throughput imbalance is slightly better than MPTCP in
the enterprise workload, but worse in the data-mining workload.

ECMP	
  
CONGA-­‐Flow	
  
CONGA	
  
MPTCP	
  

CDF	
  

0	
  
50	
  
100	
  
150	
  
200	
  
Throughput	
  Imbalance	
  (MAX	
  –	
  MIN)/AVG	
  (%)	
  
1	
  
0.8	
  
0.6	
  
0.4	
  
0.2	
  
0	
  

(a) Enterprise workload
ECMP	
  
CONGA-­‐Flow	
  
CONGA	
  
MPTCP	
  

5.3

0	
  
50	
  
100	
  
150	
  
200	
  
Throughput	
  Imbalance	
  (MAX	
  –	
  MIN)/AVG	
  (%)	
  

Our results in the previous section suggest that MPTCP increases
congestion at the edge links because it opens 8 sub-flows per connection. We now dig deeper into this issue for Incast traffic patterns
that are common in datacenters [4].
We use a simple application that generates the standard Incast
traffic pattern considered in prior work [47, 12, 4]. A client process residing on one of the servers repeatedly makes requests for a
10MB file striped across N other servers. The servers each respond
with 10MB/N of data in a synchronized fashion causing Incast. We
measure the effective throughput at the client for different “fan-in”
values, N , ranging from 1 to 63. Note that this experiment does
not stress the fabric load balancing since the total traffic crossing
the fabric is limited by the client’s 10Gbps access link. The performance here is predominantly determined by the Incast behavior for
the TCP or MPTCP transports.
The results are shown in Figure 13. We consider two minimum retransmission timeout (minRT O) values: 200ms (the default value in Linux) and 1ms (recommended by Vasudevan et al. [47]
to cope with Incast); and two packet sizes (M T U ): 1500B (the
standard Ethernet frame size) and 9000B (jumbo frames). The plots
confirm that MPTCP significantly degrades performance in Incast
scenarios. For instance, with minRT O = 200ms, MPTCP’s throughput degrades to less than 30% for large fan-in with 1500B packets
and just 5% with 9000B packets. Reducing minRT O to 1ms mitigates the problem to some extent with standard packets, but even
reducing minRT O does not prevent significant throughput loss
with jumbo frames. CONGA +TCP achieves 2–8× better throughput than MPTCP in similar settings.

(b) Data-mining workload
Figure 12: Extent of imbalance between throughput of leaf uplinks for both workloads at 60% load.
is sent through Spine 1 (see Figure 7(b)). Therefore the single
[Spine1→Leaf1] link must carry twice as much traffic as each of
the [Spine0→Leaf1] links and becomes oversubscribed when the
offered load exceeds 50%. At this point, the network effectively
cannot handle the offered load and becomes unstable.
The other adaptive schemes are significantly better than ECMP
with the asymmetric topology because they shift traffic to the lesser
congested paths (through Spine 0) and can thus handle the offered
load. CONGA is particularly robust to the link failure, achieving
up to ∼30% lower overall average FCT than MPTCP in the enterprise workload and close to 2× lower in the data-mining workload at 70% load. CONGA-Flow is also better than MPTCP even
though it does not split flows. This is because CONGA proactively
detects high utilization paths before congestion occurs and adjusts
traffic accordingly, whereas MPTCP is reactive and only adjusts
when sub-flows on congested paths experience packet drops. This
is evident from comparing the queue occupancy at the hotspot port
[Spine1→Leaf1] for the different schemes. This is shown for example for the data-mining workload at 60% load in Figure 11(c).
We see that CONGA controls the queue occupancy at the hotspot
much more effectively than MPTCP, for instance achieving a 4×
smaller 90th percentile queue occupancy.

5.2.3

Incast

5.4

Load balancing efficiency

The FCT results of the previous section show end-to-end performance which is impacted by a variety of factors, including load
balancing. We now focus specifically on CONGA’s load balancing efficiency. Figure 12 shows the CDF of the throughput imbalance across the 4 uplinks at Leaf 0 in the baseline topology
(without link failure) for both workloads at the 60% load level.

HDFS Benchmark

Our final testbed experiment evaluates the end-to-end impact of
CONGA on a real application. We set up a 64 node Hadoop cluster (Cloudera distribution hadoop-0.20.2-cdh3u5) with 1 NameNode and 63 DataNodes and run the standard TestDFSIO benchmark included in the Apache Hadoop distribution [19]. The benchmark tests the overall IO performance of the cluster by spawning

510

MTU	
  1500	
  

Symmetric	
  Topology	
  

CONGA+TCP	
  (200ms)	
  
CONGA+TCP	
  (1ms)	
  
MPTCP	
  (200ms)	
  
MPTCP	
  (1ms)	
  

20	
  
0	
  

10	
  

20	
  
30	
  
MTU	
  
9000	
  40	
   50	
  
Fanout	
  (#	
  of	
  senders)	
  

0	
  

CONGA+TCP	
  (200ms)	
  
CONGA+TCP	
  (1ms)	
  
MPTCP	
  (200ms)	
  
MPTCP	
  (1ms)	
  

40	
  
20	
  
0	
  
0	
  

10	
  

20	
  
30	
  
40	
  
50	
  
Fanout	
  (#	
  of	
  senders)	
  

ECMP	
  

0	
  

60	
  

(b) MTU = 9000B
Figure 13: MPTCP significantly degrades performance in Incast scenarios, especially with large packet sizes (MTU).

20	
  

25	
  

30	
  

35	
  

40	
  

5	
  

CONGA	
  

10	
  

MPTCP	
  

15	
   20	
   25	
  
Trial	
  Number	
  

30	
  

35	
  

40	
  

FCT	
  (Normalized	
  to	
  ECMP)	
  

(b) Asymmetric Topology (with link failure)
Figure 14: HDFS IO benchmark.

a MapReduce job which writes a 1TB file into HDFS (with 3-way
replication) and measuring the total job completion time. We run
the benchmark 40 times for both of the topologies in Figure 7 (with
and without link failure). We found in our experiments that the
TestDFSIO benchmark is disk-bound in our setup and does not produce enough traffic on its own to stress the network. Therefore, in
the absence of servers with more or faster disks, we also generate
some background traffic using the empirical enterprise workload
described in §5.2.
Figure 14 shows the job completion times for all trials. We
find that for the baseline topology (without failures), ECMP and
CONGA have nearly identical performance. MPTCP has some outlier trials with much higher job completion times. For the asymmetric topology with link failure, the job completion times for ECMP
are nearly twice as large as without the link failure. But CONGA is
robust; comparing Figures 14 (a) and (b) shows that the link failure
has almost no impact on the job completion times with CONGA.
The performance with MPTCP is very volatile with the link failure. Though we cannot ascertain the exact root cause of this, we
believe it is because of MPTCP’s difficulties with Incast since the
TestDFSIO benchmark creates a large number of concurrent transfers between the servers.

5.5

15	
  

(a) Baseline Topology (w/o link failure)

1000	
  
800	
  
600	
  
400	
  
200	
  
0	
  

80	
  
60	
  

10	
  

MPTCP	
  

Asymmetric	
  
Trial	
  TNopology	
  
umber	
  

60	
  

(a) MTU = 1500B

100	
  

5	
  

CONGA	
  

1	
  
0.8	
  
0.6	
  
0.4	
  
0.2	
  

ECMP	
  
CONGA	
  

0	
  
30	
   40	
   50	
   60	
   70	
   80	
  
Load	
  (%)	
  

FCT	
  (Normalized	
  to	
  ECMP)	
  

40	
  

ECMP	
  

Comp.	
  Time	
  (sec)	
  

60	
  

0	
  

Throughput	
  (%)	
  

1000	
  
800	
  
600	
  
400	
  
200	
  
0	
  

80	
  

Comp.	
  Time	
  (sec)	
  

Throughput	
  (%)	
  

100	
  

1	
  
0.8	
  
0.6	
  
0.4	
  
0.2	
  

ECMP	
  
CONGA	
  

0	
  
30	
   40	
   50	
   60	
   70	
   80	
  
Load	
  (%)	
  

(a) 10Gbps access links
(b) 40Gbps access links
Figure 15: Overall average FCT for a simulated workload
based on web search [4] for two topologies with 40Gbps fabric links, 3:1 oversubscription, and: (a) 384 10Gbps servers,
(b) 96 40Gbps servers.
5:1. We find qualitatively similar results to our testbed experiments
at all scales. CONGA achieves ∼5–40% better FCT than ECMP
in symmetric topologies, with the benefit larger for heavy flow size
distributions, high load, and high oversubscription ratios.
Varying link speeds: CONGA’s improvement over ECMP is more
pronounced and occurs at lower load levels the closer the access
link speed (at the servers) is to the fabric link speed. For example, in topologies with 40Gbps links everywhere, CONGA achieves
30% better FCT than ECMP even at 30% traffic load, while for
topologies with 10Gbps edge and 40Gbps fabric links, the improvement at 30% load is typically 5–10% (Figure 15). This is because
in the latter case, each fabric link can support multiple (at least 4)
flows without congestion. Therefore, the impact of hash collisions
in ECMP is less pronounced in this case (see also [41]).
Varying asymmetry: Across tens of asymmetrical scenarios we
simulated (e.g., with different number of random failures, fabrics
with both 10Gbps and 40Gbps links, etc) CONGA achieves near
optimal traffic balance. For example, in the representative scenario
shown in Figure 16, the queues at links 10 and 12 in the spine
(which are adjacent to the failed link 11) are ∼10× larger with
ECMP than CONGA.

Large-Scale Simulations

Our experimental results were for a comparatively small topology (64 servers, 4 switches), 10Gbps access links, one link failure,
and 2:1 oversubscription. But in developing CONGA, we also did
extensive packet-level simulations, including exploring CONGA’s
parameter space (§3.6), different workloads, larger topologies, varying oversubscription ratios and link speeds, and varying degrees of
asymmetry. We briefly summarize our main findings.
Note: We used OMNET++ [46] and the Network Simulation Cradle [24] to port the actual Linux TCP source code (from kernel
2.6.26) to our simulator. Despite its challenges, we felt that capturing exact TCP behavior was crucial to evaluate CONGA, especially to accurately model flowlets. Since an OMNET++ model of
MPTCP is unavailable and we did experimental comparisons, we
did not simulate MPTCP.

6.

ANALYSIS

In this section we complement our experimental results with analysis of CONGA along two axes. First, we consider a game-theoretic
model known as the bottleneck routing game [6] in the context of
CONGA to quantify its Price of Anarchy [40]. Next, we consider
a stochastic model that gives insights into the impact of the traffic
distribution and flowlets on load balancing performance.

Varying fabric size and oversubscription: We have simulated
realistic datacenter workloads (including those presented in §5.2)
for fabrics with as many as 384 servers, 8 leaf switches, and 12
spine switches, and for oversubscription ratios ranging from 1:1 to

511

Spine Downlink Queues

Length	
  (pkts)	
  

300	
  
250	
  
200	
  
150	
  
100	
  
50	
  
0	
  

ECMP	
  
CONGA	
  

some failed links

(2)

(1)
(1)

1	
  

11	
  

21	
  

31	
  Link	
  #	
   41	
  

51	
  

61	
  

Leaf Uplink Queues

Length	
  (pkts)	
  

300	
  
250	
  
200	
  
150	
  
100	
  
50	
  
0	
  
11	
  

21	
  

31	
  Link	
  #	
   41	
  

Figure 17: Example with PoA of 2. All edges have capacity 1
and each pair of adjacent leaves send 1 unit of traffic to each
other. The shown Nash flow (with traffic along the solid edges)
has a network bottleneck of 1. However, if all users instead used
the dashed paths, the bottleneck would be 1/2.

CONGA	
  

51	
  

61	
  

(3)

71	
  
ECMP	
  

some failed links

1	
  

71	
  

the network bottleneck for a Nash flow and the minimum network
bottleneck achievable by any flow. We have the following theorem.

Figure 16: Multiple link failure scenario in a 288-port fabric
with 6 leaves and 4 spines. Each leaf and spine connect with
3 × 40Gbps links and 9 randomly chosen links fail. The plots
show the average queue length at all fabric ports for a web
search workload at 60% load. CONGA balances traffic significantly better than ECMP. Note that the improvement over
ECMP is much larger at the (remote) spine downlinks because
ECMP spreads load equally on the leaf uplinks.

6.1

(2)

(3)

Theorem 1. The PoA for the CONGA game is 2.
P ROOF. An upper bound of 2 can be derived for the PoA using
a similar argument to that of Corollary 2 (Section V-B) in Banner et
al. [6]. The proof leverages the fact that the paths in the Leaf-Spine
network have length 2. The corresponding lower bound of 2 for the
PoA follows from the example shown in Figure 17. Refer to the
longer version of this paper for details [3].

The Price of Anarchy for CONGA

Theorem 1 proves that the network bottleneck with CONGA is at
most twice the optimal. It is important to note that this is the worstcase and can only occur in very specific (and artificial) scenarios
such as the example in Figure 17. As our experimental evaluation
shows (§5), in practice the performance of CONGA is much closer
to optimal.

In CONGA, the leaf switches make independent load balancing
decisions to minimize the congestion for their own traffic. This uncoordinated or “selfish” behavior can potentially cause inefficiency
in comparison to having centralized coordination. This inefficiency
is known as the Price of Anarchy (PoA) [40] and has been the subject of numerous studies in the theoretical literature (see [43] for a
survey). In this section we analyze the PoA for CONGA using the
bottleneck routing game model introduced by Banner et al. [6].

Remark 2. If in addition to using paths with the smallest congestion metric, if the leaf switches also use only paths with the fewest
number of bottlenecks, then the PoA would be 1 [6], i.e., optimal.
For example, the flow in Figure 17 does not satisfy this condition
since traffic is routed along paths with two bottlenecks even though
alternate paths with a single bottleneck exist. Incorporating this criteria into CONGA would require additional bits to track the number of bottlenecks along the packet’s path. Since our evaluation
(§5) has not shown much potential gains, we have not added this
refinement to our implementation.

Model: We consider a two-tier Leaf-Spine network modeled as a
complete bipartite graph. The leaf and spine nodes are connected
with links of arbitrary capacities {ce }. The network is shared by
a set of users U . Each user u ∈ U has a traffic demand γu , to
be sent from source leaf, su , to destination leaf, tu . A user must
decide how to split its traffic along the paths P (su ,tu ) through the
different spine nodes. Denote by fpu the flow of user u on path
p. The set of all user flows, f = P
{fpu }, is termed the network
flow. A network flow is feasible if p∈P (su ,tu ) fpu = γu for all
P
u
u
P∈ U . Let fp = u∈U fp be the total flow on a path and fe =
p|e∈p fp be the total flow on a link. Also, let ρe (fe ) = fe /ce
be the utilization of link e. We define the network bottleneck for a
network flow to be the utilization of the most congested link; i.e.,
B(f ) , maxe∈E ρe (fe ). Similarly, we define the bottleneck for
user u to be the utilization of the most congested link it uses; i.e.,
bu (f ) , maxe∈E|feu >0 ρe (fe ).
We model CONGA as a non-cooperative game where each user
selfishly routes its traffic to minimize its own bottleneck; i.e., it only
sends traffic along the paths with the smallest bottleneck available
to it. The network flow, f , is said to be at Nash equilibrium if
no user can improve its bottleneck given the choices of the other
users. Specifically, f is a Nash flow if for each user u∗ ∈ U and
network flow g = {gpu } such that gpu = fpu for each u ∈ U \u∗ , we
have: bu∗ (f ) ≤ bu∗ (g). The game above always admits at least
one Nash flow (Theorem 1 in [6]). Note that CONGA converges
to a Nash flow because the algorithm rebalances traffic (between a
particular source/destination leaf) if there is a path with a smaller
bottleneck available.4 The PoA is defined as the worst-case ratio of

6.2

Impact of Workload on Load Balancing

Our evaluation showed that for some workloads ECMP actually
performs quite well and the benefits of more sophisticated schemes
(such as MPTCP and CONGA) are limited in symmetric topologies. This raises the question: How does the workload affect load
balancing performance? Importantly, when are flowlets helpful?
We now study these questions using a simple stochastic model.
Flows with an arbitrary size distribution, S, arrive according to
a Poisson process of rate λ and are spread across n links. For
each flow, a random link is chosen. Let N k (t) and Ak (t) respectively denote the number of flows and the total amount of traffic
PN k (t) k
sent on link k in time interval (0, t); i.e., Ak (t) =
si ,
i=1
k
th
where si is the size of i flow assigned to link k. Let Y (t) =
min1≤k≤n Ak (t) and Z(t) = max1≤k≤n Ak (t) and define the
traffic imbalance:
χ(t) ,

Z(t) − W (t)
λE(S)
t
n

.

This is basically the deviation between the traffic on the maximum
and minimum loaded links normalized by the expected traffic on
each link. The following theorem bounds the expected traffic imbalance at time t.

4
Of course, this is an idealization and ignores artifacts such as
flowlets, quantized congestion metrics, etc.

512

Theorem 2. Assume E(eθS ) < ∞ in a neighborhood of zero:
θ ∈ (−θ0 , θ0 ). Then for t sufficiently large:
E(χ(t)) ≤ √

1
1
+ o( ),
t
λe t

PoA is for the worst-case (adversarial) scenario. Hence, while the
PoA is better for the sum metric than the max, like prior work (e.g.,
TeXCP [26]), we used max because it emphasizes the bottleneck
link and is also easier to implement; the sum metric requires extra
bits in the packet header to prevent overflow when doing additions.

(1)

where:
λe =

λ

.
σS 2
8n log n 1 + ( E(S)
)

(2)

8.

Here σS is the standard deviation of S.
Note: Kandula et al. [27] prove a similar result for the deviation of
traffic on a single link from its expectation while Theorem 2 bounds
the traffic imbalance across all links.
The proof follows standard probabilistic arguments and is given
in the longer version of this paper [3]. Theorem 2 shows that
the√traffic imbalance with randomized load balancing vanishes like
1/ t for large t. Moreover, the leading term is determined by the
effective arrival rate, λe , that depends on two factors: (i) the perlink flow arrival rate, λ/n; and (ii) the coefficient of variation of
the flow size distribution, σS /E(S). Theorem 2 quantifies the intuition that workloads that consists mostly of small flows are easier to
handle than workloads with a few large flows. Indeed, it is for the
latter “heavy” workloads that we can anticipate flowlets to make a
difference. This is consistent with our experiments which show that
ECMP does well for the enterprise workload (Figure 9), but for the
heavier data mining workload CONGA is much better than ECMP
and is even notably better than CONGA-Flow (Figure 10).

7.

RELATED WORK

We briefly discuss related work that has informed and inspired
our design, especially work not previously discussed.
Traditional traffic engineering mechanisms for wide-area networks [15, 14, 42, 49] use centralized algorithms operating at coarse
timescales (hours) based on long-term estimates of traffic matrices. More recently, B4 [23] and SWAN [21] have shown near
optimal traffic engineering for inter-datacenter WANs. These systems leverage the relative predictability of WAN traffic (operating
over minutes) and are not designed for highly volatile datacenter
networks. CONGA is conceptually similar to TeXCP [26] which
dynamically load balances traffic between ingress-egress routers
(in a wide-area network) based on path-wise congestion metrics.
CONGA however is significantly simpler (while also being near
optimal) so that it can be implemented directly in switch hardware
and operate at microsecond time-scales, unlike TeXCP which is
designed to be implemented in router software.
Besides Hedera [2], work such as MicroTE [9] proposes centralized load balancing for datacenters and has the same issues with
handling traffic volatility. F10 [34] also uses a centralized scheduler, but uses a novel network topology to optimize failure recovery,
not load balancing. The Incast issues we point out for MPTCP [41]
can potentially be mitigated by additional mechanisms such as explicit congestion signals (e.g., XMP [11]), but complex transports
are challenging to validate and deploy in practice.
A number of papers target the coarse granularity of flow-based
balancing in ECMP. Besides Flare [27], LocalFlow [44] proposes
spatial flow splitting based on TCP sequence numbers and DRB [10]
proposes efficient per-packet round-robin load balancing. As explained in §2.4, such local schemes may interact poorly with TCP
in the presence of asymmetry and perform worse than ECMP. DeTail [52] proposes a per-packet adaptive routing scheme that handles asymmetry using layer-2 back-pressure, but requires end-host
modifications to deal with packet reordering.

DISCUSSION

We make a few remarks on aspects not covered thus far.
Incremental deployment: An interesting consequence of CONGA’s
resilience to asymmetry is that it does not need to be applied to
all traffic. Traffic that is not controlled by CONGA simply creates
bandwidth asymmetry to which (like topology asymmetry) CONGA
can adapt. This facilitates incremental deployment since some leaf
switches can use ECMP or any other scheme. Regardless, CONGA
reduces fabric congestion to the benefit of all traffic.
Larger topologies: While 2-tier Leaf-Spine topologies suffice for
most enterprise deployments, the largest mega datacenters require
networks with 3 or more tiers. CONGA may not achieve the optimal traffic balance in such cases since it only controls the load balancing decision at the leaf switches (recall that the spine switches
use ECMP). However, large datacenter networks are typically organized as multiple pods, each of which is a 2-tier Clos [1, 18]. Therefore, CONGA is beneficial even in these cases since it balances the
traffic within each pod optimally, which also reduces congestion
for inter-pod traffic. Moreover, even for inter-pod traffic, CONGA
makes better decisions than ECMP at the first-hop.
A possible approach to generalizing CONGA to larger topologies is to pick a sufficient number of “good” paths between each
pair of leaf switches and use leaf-to-leaf feedback to balance traffic across them. While we cannot cover all paths in general (as we
do in the 2-tier case), the theoretical literature suggests that simple
path selection policies such as periodically sampling a random path
and retaining the best paths may perform very well [30]. We leave
to future work a full exploration of CONGA in larger topologies.
Other path metrics: We used the max of the link congestion metrics as the path metric, but of course other choices such as the sum
of link metrics are also possible and can easily be incorporated in
CONGA. Indeed, in theory, the sum metric gives a Price of Anarchy (PoA) of 4/3 in arbitrary topologies [43]. Of course, the

9.

CONCLUSION

The main thesis of this paper is that datacenter load balancing is
best done in the network instead of the transport layer, and requires
global congestion-awareness to handle asymmetry. Through extensive evaluation with a real testbed, we demonstrate that CONGA
provides better performance than MPTCP [41], the state-of-theart multipath transport for datacenter load balancing, without important drawbacks such as complexity and rigidity at the transport
layer and poor Incast performance. Further, unlike local schemes
such as ECMP and Flare [27], CONGA seamlessly handles asymmetries in the topology or network traffic. CONGA’s resilience
to asymmetry also paid unexpected dividends when it came to incremental deployment. Even if some switches within the fabric
use ECMP (or any other mechanism), CONGA’s traffic can work
around bandwidth asymmetries and benefit all traffic.
CONGA leverages an existing datacenter overlay to implement a
leaf-to-leaf feedback loop and can be deployed without any modifications to the TCP stack. While leaf-to-leaf feedback is not always
the best strategy, it is near-optimal for 2-tier Leaf-Spine fabrics that
suffice for the vast majority of deployed datacenters. It is also sim-

513

ple enough to implement in hardware as proven by our implementation in custom silicon.
In summary, CONGA senses the distant drumbeats of remote
congestion and orchestrates flowlets to disperse evenly through the
fabric. We leave to future work the task of designing more intricate
rhythms for more general topologies.

[24] S. Jansen and A. McGregor. Performance, Validation and Testing
with the Network Simulation Cradle. In MASCOTS, 2006.
[25] V. Jeyakumar et al. EyeQ: Practical Network Performance Isolation
at the Edge. In NSDI, 2013.
[26] S. Kandula, D. Katabi, B. Davie, and A. Charny. Walking the
Tightrope: Responsive Yet Stable Traffic Engineering. In
SIGCOMM, 2005.
[27] S. Kandula, D. Katabi, S. Sinha, and A. Berger. Dynamic Load
Balancing Without Packet Reordering. SIGCOMM Comput.
Commun. Rev., 37(2):51–62, Mar. 2007.
[28] S. Kandula, S. Sengupta, A. Greenberg, P. Patel, and R. Chaiken. The
nature of data center traffic: measurements & analysis. In IMC, 2009.
[29] R. Kapoor et al. Bullet Trains: A Study of NIC Burst Behavior at
Microsecond Timescales. In CoNEXT, 2013.
[30] P. Key, L. Massoulié, and D. Towsley. Path Selection and Multipath
Congestion Control. Commun. ACM, 54(1):109–116, Jan. 2011.
[31] A. Khanna and J. Zinky. The Revised ARPANET Routing Metric. In
SIGCOMM, 1989.
[32] M. Kodialam, T. V. Lakshman, J. B. Orlin, and S. Sengupta.
Oblivious Routing of Highly Variable Traffic in Service Overlays and
IP Backbones. IEEE/ACM Trans. Netw., 17(2):459–472, Apr. 2009.
[33] T. Koponen et al. Network Virtualization in Multi-tenant Datacenters.
In NSDI, 2014.
[34] V. Liu, D. Halperin, A. Krishnamurthy, and T. Anderson. F10: A
Fault-tolerant Engineered Network. In NSDI, 2013.
[35] M. Mahalingam et al. VXLAN: A Framework for Overlaying
Virtualized Layer 2 Networks over Layer 3 Networks.
http://tools.ietf.org/html/
draft-mahalingam-dutt-dcops-vxlan-06, 2013.
[36] N. Michael, A. Tang, and D. Xu. Optimal link-state hop-by-hop
routing. In ICNP, 2013.
[37] MultiPath TCP - Linux Kernel implementation.
http://www.multipath-tcp.org/.
[38] T. Narten et al. Problem Statement: Overlays for Network
Virtualization. http://tools.ietf.org/html/
draft-ietf-nvo3-overlay-problem-statement-04,
2013.
[39] J. Ousterhout et al. The case for RAMCloud. Commun. ACM, 54,
July 2011.
[40] C. Papadimitriou. Algorithms, Games, and the Internet. In Proc. of
STOC, 2001.
[41] C. Raiciu et al. Improving datacenter performance and robustness
with multipath tcp. In SIGCOMM, 2011.
[42] M. Roughan, M. Thorup, and Y. Zhang. Traffic engineering with
estimated traffic matrices. In IMC, 2003.
[43] T. Roughgarden. Selfish Routing and the Price of Anarchy. The MIT
Press, 2005.
[44] S. Sen, D. Shue, S. Ihm, and M. J. Freedman. Scalable, Optimal Flow
Routing in Datacenters via Local Link Balancing. In CoNEXT, 2013.
[45] M. Sridharan et al. NVGRE: Network Virtualization using Generic
Routing Encapsulation. http://tools.ietf.org/html/
draft-sridharan-virtualization-nvgre-03, 2013.
[46] A. Varga et al. The OMNeT++ discrete event simulation system. In
ESM, 2001.
[47] V. Vasudevan et al. Safe and effective fine-grained TCP
retransmissions for datacenter communication. In SIGCOMM, 2009.
[48] S. Vutukury and J. J. Garcia-Luna-Aceves. A Simple Approximation
to Minimum-delay Routing. In SIGCOMM, 1999.
[49] H. Wang et al. COPE: Traffic Engineering in Dynamic Networks. In
SIGCOMM, 2006.
[50] D. Wischik, C. Raiciu, A. Greenhalgh, and M. Handley. Design,
Implementation and Evaluation of Congestion Control for Multipath
TCP. In NSDI, 2011.
[51] D. Xu, M. Chiang, and J. Rexford. Link-state Routing with
Hop-by-hop Forwarding Can Achieve Optimal Traffic Engineering.
IEEE/ACM Trans. Netw., 19(6):1717–1730, Dec. 2011.
[52] D. Zats, T. Das, P. Mohan, D. Borthakur, and R. H. Katz. DeTail:
Reducing the Flow Completion Time Tail in Datacenter Networks. In
SIGCOMM, 2012.

Acknowledgments: We are grateful to Georges Akis, Luca Cafiero,
Krishna Doddapaneni, Mike Herbert, Prem Jain, Soni Jiandani,
Mario Mazzola, Sameer Merchant, Satyam Sinha, Michael Smith,
and Pauline Shuen of Insieme Networks for their valuable feedback
during the development of CONGA. We would also like to thank
Vimalkumar Jeyakumar, Peter Newman, Michael Smith, our shepherd Teemu Koponen, and the anonymous SIGCOMM reviewers
whose comments helped us improve the paper.

10.

REFERENCES

[1] M. Al-Fares, A. Loukissas, and A. Vahdat. A scalable, commodity
data center network architecture. In SIGCOMM, 2008.
[2] M. Al-Fares, S. Radhakrishnan, B. Raghavan, N. Huang, and
A. Vahdat. Hedera: Dynamic Flow Scheduling for Data Center
Networks. In NSDI, 2010.
[3] M. Alizadeh et al. CONGA: Distributed Congestion-Aware Load
Balancing for Datacenters. http://simula.stanford.edu/
~alizade/papers/conga-techreport.pdf.
[4] M. Alizadeh et al. Data center TCP (DCTCP). In SIGCOMM, 2010.
[5] M. Alizadeh et al. pFabric: Minimal Near-optimal Datacenter
Transport. In SIGCOMM, 2013.
[6] R. Banner and A. Orda. Bottleneck Routing Games in
Communication Networks. Selected Areas in Communications, IEEE
Journal on, 25(6):1173–1179, 2007.
[7] M. Beck and M. Kagan. Performance Evaluation of the RDMA over
Ethernet (RoCE) Standard in Enterprise Data Centers Infrastructure.
In DC-CaVES, 2011.
[8] T. Benson, A. Akella, and D. A. Maltz. Network Traffic
Characteristics of Data Centers in the Wild. In SIGCOMM, 2010.
[9] T. Benson, A. Anand, A. Akella, and M. Zhang. MicroTE: Fine
Grained Traffic Engineering for Data Centers. In CoNEXT, 2011.
[10] J. Cao et al. Per-packet Load-balanced, Low-latency Routing for
Clos-based Data Center Networks. In CoNEXT, 2013.
[11] Y. Cao, M. Xu, X. Fu, and E. Dong. Explicit Multipath Congestion
Control for Data Center Networks. In CoNEXT, 2013.
[12] Y. Chen, R. Griffith, J. Liu, R. H. Katz, and A. D. Joseph.
Understanding TCP Incast Throughput Collapse in Datacenter
Networks. In WREN, 2009.
[13] N. Dukkipati and N. McKeown. Why Flow-completion Time is the
Right Metric for Congestion Control. SIGCOMM Comput. Commun.
Rev., 2006.
[14] A. Elwalid, C. Jin, S. Low, and I. Widjaja. MATE: MPLS adaptive
traffic engineering. In INFOCOM, 2001.
[15] B. Fortz and M. Thorup. Internet traffic engineering by optimizing
OSPF weights. In INFOCOM, 2000.
[16] R. Gallager. A Minimum Delay Routing Algorithm Using Distributed
Computation. Communications, IEEE Transactions on, 1977.
[17] P. Gill, N. Jain, and N. Nagappan. Understanding Network Failures
in Data Centers: Measurement, Analysis, and Implications. In
SIGCOMM, 2011.
[18] A. Greenberg et al. VL2: a scalable and flexible data center network.
In SIGCOMM, 2009.
[19] Apache Hadoop. http://hadoop.apache.org/.
[20] C.-Y. Hong, M. Caesar, and P. B. Godfrey. Finishing Flows Quickly
with Preemptive Scheduling. In SIGCOMM, 2012.
[21] C.-Y. Hong et al. Achieving High Utilization with Software-driven
WAN. In SIGCOMM, 2013.
[22] R. Jain and S. Paul. Network virtualization and software defined
networking for cloud computing: a survey. Communications
Magazine, IEEE, 51(11):24–31, 2013.
[23] S. Jain et al. B4: Experience with a Globally-deployed Software
Defined Wan. In SIGCOMM, 2013.

514

2013 IEEE 13th International Conference on Data Mining

Tag-Weighted Dirichlet Allocation
Shuangyin Li, Guan Huang, Ruiyang Tan and Rong Pan∗
School of Information Science and Technology
Sun Yat-sen University, Guangzhou, China
{lishyin@mail2., huangg6@mail2., tanry@mail2., panr@}sysu.edu.cn
Abstract—In the past two decades, there has been a huge
amount of document data with rich tag information during the
evolution of the Internet, which can be called semi-structured
data. These semi-structured data contain both unstructured
features (e.g., plain text) and metadata, such as tags in html
ﬁles or author and venue information in research articles. It’s
of great interest to model such kind of data. Most previous
works focused on modeling the unstructured data. Some other
methods have been proposed to model the unstructured data
with speciﬁc tags. To build a general model for semi-structured
documents remains an important problem in terms of both
model ﬁtness and efﬁciency. In this paper, we propose a novel
method to model the tagged documents by a so-called TagWeighted Dirichlet Allocation (TWDA). TWDA is a framework
that leverages both the tags and words in each document to
infer the topic components for the documents. This allows not
only to learn the document-topic and topic-word distributions,
but also to infer the tag-topic distributions for text mining
(e.g., classiﬁcation, clustering, and recommendations). Moreover,
TWDA can automatically infer the probabilistic weights of tags
for each document, that can be used to predict the tags in one
document. We present an efﬁcient variational inference method
with an EM algorithm for estimating the model parameters.
The experimental results show the effectiveness, efﬁciency and
robustness of our TWDA approach by comparing it with the
state-of-the-art methods on four corpora in document modeling,
tags prediction and text classiﬁcation.

I.

statistics and machine learning to discover the thematic contents of untagged documents. Topic models can discover the
latent structures in documents and establish links between
them, such as latent Dirichlet allocation (LDA) [5]. However,
as an unsupervised method, only the words in the documents
are modeled in LDA. Thus, LDA could only treat the tags
as word features rather than a new kind of information for
document modeling.
To model semi-structured data needs to consider the characteristics of different kinds of objects, including word, topic,
document, and tag, and also the relationship among them. In
this problem, topic is a kind of hidden objects, and the other
three are the observations. Relative to tag, word and document
are objective; tag can be either objective (e.g., author and
venue information of publications) and subjective (e.g., tags
in social bookmark marked by people). Similar to the topic
models, we should consider binary relationships between the
pairs of the objects, including topic-word and document-topic.
In addition, we may consider the binary relationships, like tagword, tag-topic, tag-document, and tag-tag. The tag-document
relationship implies that we should consider the weights of the
tags in each document. The tag-topic and tag-tag relationships
can be more complicated, thus are difﬁcult to model. Some
earlier works consider certain tags. For example, the authortopic model in [26] considers the authorship information of
the documents to be modeled. In this work, we don’t limit
the types and number of the tags in each document. In an
extreme case, where there is no tag in any document, the new
model may degenerate into LDA. On the other hand, since the
tags can be created by some people, they should be relevant
to topics of the documents; however, some of them may be
correlated, redundant, and even noisy. Therefore, the tag-topic
relationships should be general enough and we should also
model the weights of the tags in each document.

I NTRODUCTION

In the evolution of the Internet, there have been massive
collection of documents in many web applications. Such kinds
of documents with both text data and document metadata
(tags, which can be viewed as features of the corresponding document) are called the semi-structured data. How to
characterize the semi-structured document data becomes an
important issue addressed in many areas, such as information
retrieval, artiﬁcial intelligence and data mining, etc. The tags
can be more important than the text data in document mining.
There are many examples: in a collection of movie set, such as
Internet Movie Database (IMDB)1 , we may have an idea that
a movie has a higher chance to be a comedy when it has a tag
“Dick Martin”, without watch it; in a collection of scientiﬁc
articles, each document has a list tags(authors and keywords).

In the past few years, researchers have proposed approaches
to model documents with tags or labels [21], [24], [25].
Moreover, Labeled LDA [24] assumes there is no latent topics
and each topic is restricted to be associated with the given
labels. PLDA assumes that each topic is associated with only
one label [25]. Both Labeled LDA and PLDA have implicit
assumptions that the given labels should be strongly associated
with the topics to be modeled or the labels are independent to
each other.

Many solutions have been proposed to deal with the semistructured documents (e.g., SVD, LSI), and shown to be useful
in document mining [7], [20], [29], [27], e.g., text classiﬁcation
and structural information exploiting. For document modeling,
topic models have been used to be a powerful method of
analyzing and modeling of document corpora, using Bayesian

In this paper, we propose a tag-weighted Dirichlet allocation (TWDA) to represent the text data and the various
tags with weights to evaluate the importance of the tags,
which provides a novel method to model the semi-structured
documents.

*Corresponding author
1 http://www.imdb.com

1550-4786/13 $31.00 © 2013 IEEE
DOI 10.1109/ICDM.2013.11

It is based on latent Dirichlet allocation (LDA), and learns
438

PLDA [25], or modeling relationships among several variables,
such as Author-Topic Model [26]. Labeled LDA [24] get the
topic distribution for a document through picking out the several hyperparameter components that correspond to its labels,
and draw the topic components by the new hyperparameter
without inferring the topic distribution of labels. Labeled LDA
does not assume the existence of any latent topics [25]. PLDA
[25] provides another way of modeling the tagged text data,
which assumes the generation topics assignment is limited by
only one of the given tags for one word, and in the training
process, PLDA assumes that each topic takes part in exactly
one label, and may optionally share global label present on
every document. In Author Topic Model, it obtains the topic
distributions of authors, without giving the importance weights
among the given authors in each document. DMR [21] is a
Dirichlet-multinomial regression topic model that includes a
log-linear prior on the document-topic distributions, which is
an exponential function of the given features of the document.
However, DMR doesn’t output the tag weights either [26],
which is useful for tag ranking.

the weights among the Dirichlet prior and tags, not just
among the tags. Therefore, TWDA handle not only the semistructured documents, but also the unstructured documents. For
the unstructured documents, TWDA degenerates into LDA.
Moreover, in many web applications, not all the documents in
the corpora have the tags. There are lots of documents without
any tags or documents which have to remove all the tags after
data preprocessing for denoising. Only considering the tags
would not hold this case. However, TWDA can handle this
complex corpora effectively and easily.
Besides, TWDA also infers the topic distributions of tags.
The weights of observed tags in each document, which we
infer from the data set, give us an opportunity to provide a
method to rank the tags. The contribution of this paper is threefold:
1)

2)

3)

It provides a novel and accurate topic modeling
method to model the semi-structured data, leveraging
the weights among the Dirichlet prior and observed
tags in a document.
In TWDA, weights are associated with the observed
tags in a document providing a way to rank the tags.
In addition, this could be used to predict latent tags
in the document.
With TWDA, we can handle both the multi-tag
documents and non-tag documents simultaneously,
which is very useful to process some complicated
web applications.

TWTM [18] proposes a approach using the tags to document modeling, however, it just leverages the tags given in a
document by the weight values to model the topic distribution
of a document. In many real applications, when it comes to
a corpora in which some documents have no tags, TWTM
doesn’t work. So, in our paper, we consider both the Dirichlet
prior and the tags in the document by the weight values
among them, which provide a novel method to integrate tag
information into probabilistic topic models. The method can
automatically degenerate into LDA, treating the documents
without tags as unstructured data. Besides the topic discovery,
the method can be used to predict tags of documents by the
weight values with a better performance.

The rest of the paper is organized as follows. In Section II,
we ﬁrst analyze and discuss related works. In Section III, after
introducing the notations, we present a novel topic model, and
give the methods of learning and inference. In Section IV, we
present the experimental results on four domains to show the
performance of the proposed method in document modeling,
tag predicting, text classiﬁcation and the model robustness. We
end the paper in Section V.
II.

III.

TWDA M ODEL A ND A LGORITHMS

In this section, we will mathematically deﬁne the tagweighted Dirichlet allocation (TWDA), and discuss the learning and inference methods.

R ELATED W ORKS

Topic models provide an amalgam of ideas drawn from
mathematics, computer science, and cognitive science to help
users understand unstructured data. There are many topic
models proposed and shown to be powerful on document
analyzing, such as in [23], [14], [5], [4], [6], [10], which have
been applied to many areas, including document clustering
and classiﬁcation [8], and information retrieval [28]. They are
extended to many other topic models for different situation of
applications in analyzing text data [15], [17], [30]. However,
most of these models only consider the textual information and
can only treat the tag information as plain text as well.

A. Notation

TMBP [12] and cFTM [11] propose the methods to make
use of the contextual information of documents for topic
modeling. TMBP is a topic model with biased propagation
to leveraging contextual information, the authors and venue.
TMBP needs to predeﬁne the weights of the author and venue
information on word assignment, which limits the usefulness
in real applications. The method of cFTM has a very strong
assumption that each word is associated with only one tag,
either author or venue.

Similar to LDA [5], we formally deﬁne the following terms.
Consider a semi-structured corpus, a collection of M documents. We deﬁne the corpus D = {(w1 , t1 ), . . . , (wM , tM )},
where each 2-tuple (wd , td ) denotes a document, the bag-ofd
word representation wd = (w1d , . . . , wN
), td = (td1 , . . . , tdL )
is the document tag vector, each element of which being a
binary tag indicator, and L is the size of the tag set in the
corpus D. For the convenience of the inference in this paper,
td is expanded to a ld × (L + 1) matrix T d , where ld is one
more than the number of tags in document d (For example, if
the document d has ﬁve tags, ld is six). For each row number
i ∈ {1, . . . , ld } in T d , Ti·d is a binary vector, where Tijd = 1 if
and only if the i-th tag of the document d2 is the j-th tag of the
tag set in the corpus D. Note that, we set the last dimension
of the last row in T d to 1, and the other dimensions of the
last row equal to 0 for all documents. The detail of the above
setting will be shown later.

Several models have been proposed to take advantage of
tags or labels, such as Labeled LDA [24], DMR [21] and

2 Note that we can sort the tags of the document d by the index of the tag
set of the corpus D.

439

The generative process for TWDA is given in the following
procedure:

μ

1)
η

t

λ

α

ϑd

θ

2)

L

3)
π

εd

L×K

β

z

w

ψ
N

For each topic k ∈ {1, . . . , K}, draw ψk ∼ Dir(β) ,
where β is a V dimensional prior vector of ψ.
For each tag t ∈ {1, . . . , L}, draw θt ∼ Dir(α), where
α is a K dimensional prior vector of θ.
For each document d:
a) Draw λ ∼ Dir(μ).
b) Generate T d by td .
c) Draw εd ∼ Dir(T d × π).
d) Generate ϑd = (εd )T × T d × ( λθ ) .
e) For each word wdi :
i) Draw zdi ∼Mult(ϑd ) .
ii) Draw wdi ∼Mult(ψzdi ) .

In this process, Dir(·) designates a Dirichlet distribution,
Mult(·) is a multinomial distribution. Note that, L is the
number of tags appeared in the corpora and K is the number
of topics. π is a (L + 1) × 1 column vector and μ is a K × 1
column vector. Both of them are Dirichlet prior. λ is a 1 × K
row vector which is drawn from μ. (εd )T is the transpose of εd
and εd is drawn from a Dirichlet prior which obtained by the
matrix multiplication of T d × π. Clearly, the result of T d × π
will be a (ld × 1) vector whose dimension is depended on the
number of the observed tags in the document d. Note that, ld
is one more than the number of tags given in d as we described
above. (εd )T is the transpose of εd and εd is drawn from a
Dirichlet prior which obtained by the matrix multiplication of
T d × π. Clearly, the result of T d × π will be a (ld × 1) vector
whose dimension is depended on the number of the observed
tags in the document d.

K ×V
D

Fig. 1. Graphical model representation of TWDA, where θ is a distribution
matrix of all the tags, ψ is a distribution matrix of words. ϑd indicates the
topic proportions of each document

In this paper, we wish to ﬁnd a probabilistic model for
the corpus D that assigns high likelihood to the documents in
the corpus and other documents alike utilizing the given tag
information.
B. TWDA
TWDA is a probabilistic graphical model that describes a
process for generating a semi-structured document collection.
In topic models, such as LDA, we treat the words in a
document as generating from a set of latent topics which is a
set distributions over the vocabulary in the corpora. However,
there are many tags in the semi-structured document collection
and the tags have an important impact on the topic distributions
in documents. In this proposed model TWDA, we try to
consider the gain from tags information to the document topic
modeling.

In other words, we treat the λ as a topic distribution of one
latent tag, the Dirichlet prior μ. Each document is controlled
by a latent tag, that is the same idea both TWDA and Latent
Dirichlet Allocation[5]. The form of ( λθ ) is the augmented
matrix of θ and λ, which represents that we add the vector λ
to the matrix θ as the last row, so ( λθ ) becomes a (L + 1) × K
matrix. As we show above, T d is the matrix form of the given
tags in the document d, and the last row of T d is a binary
vector, of which only the last dimension equals to 1 and the
others equal 0. Here we deﬁne

Tag-weighted Dirichlet allocation (TWDA) is built on
latent Dirichlet allocation (LDA) [5], in which we treat the
topic proportions for a document as a draw from a Dirichlet
distribution. In tag-weighted Dirichlet allocation (TWDA), the
topic distribution of one document is not only decided by
a Dirichlet distribution parameter, but also by all the tags
appeared in the document. The TWDA mixes the topic proportions draw from the Dirichlet distribution hyperparameter
and given tags by importance or weight (tag-weighted).

θ
Θd = T d × ( ).
λ
Clearly, Θd is a ld × K matrix, whose last row is λ. Actually,
the purpose of Θd is to pick out the rows corresponded to the
tags appeared in d from tag-topic distribution matrix θ.
In the proposed model, the key idea of tag-weighted
Dirichlet allocation is to model the topic proportions of semistructured documents by document-special tags and text data.
Different from LDA, the topic proportion of one document
assumed in this paper is controlled not only by a Dirichlet
prior μ, but also by all the observed tags. The way to generate
the normalized topic distribution of the document d is that we
mix both Dirichlet allocation and tags information through a
weight vector εd . Thus, we obtain the topic distribution of d
by

The model parameters are as follows: a K × V matrix
ψ (each ψk is a vector of words probabilities), an L × K
matrix θ (each row presents the topic distribution of one tag),
a Dirichlet hyperparameter μ, a Dirichlet parameter π(π is the
Dirichlet parameter of weights among speciﬁed tags), and a
Bernoulli prior η for model completeness. Figure 1 shows that
how TWDA works in a probabilistic graphical model. In the
model, we use ϑd to denote the topic distribution of document
d and ξ d to denote the weight vector of the tags in d as shown
in Figure 1. This parameters are described in detail later.

θ
ϑd = (εd )T × T d × ( ).
λ

440

It is worth to note that the εd is draw by a Dirichlet prior
π, each row of θ is draw by a Dirichlet prior α, and λ is draw
by a Dirichlet prior μ, so εd and θ satisfy
ld d
K
K
k=1 θlk = 1,
k=1 λk = 1.
i=1 εi = 1,
d T

ξ

ρ

γd

εd

λ

z

N

Therefore, the linear multiplication
of (ε ) , T , θ and λ
K
maintains the condition of k=1 ϑdk = 1 without normalization of ϑd . With ϑd , the topic proportions of the document d,
the remaining part of the generative process is just familiar
with LDA [5].

Fig. 2. Graphical model representation of the variational distribution used to
approximate the posterior in TWDA

C. Compared with LDA and other Topic Models

1) Variational E-step: Given the document d, we can easily
get the posterior distribution of the latent variables in the
proposed model, as:

d

M

As shown above, in TWDA, we introduce a novel way to
model the semi-structured documents by leveraging the text
data and the structured information (observed tags) in the
documents. In LDA, the topic distribution is drawn from a
hyperparameter, without considering the given tags. However,
the tag information is useful for the generation of topic proportions. The proposed model in this paper can easily degenerate
into LDA when we ignore the tags in a document. In this case,
T d will be a binary row vector whose last dimension equals
to 1 and the others are 0, and ( λθ ) is simpliﬁed to λ. Thus,

p(εd , z, wd , T d |θ, η, ψ, π, μ)
.
p(wd , T d |θ, η, ψ, π, μ)
(1)
In Eq. (1), integrating over ε and summing out z, we easily
obtain the marginal distribution of d:



p(wd , T d |η, θ, ψ, π, μ) = p(td |η) p εd |(T d × π) ·
p(εd , z|wd , T d , θ, η, ψ, π, μ) =

p(λ|μ)
ϑd

N 
K

i=1 z d =1

θ
= (εd )T × T d × ( )
λ
= λ.

θ
p(zid |(εd )T × T d × ( )) ·
λ

i

p(wid |zid , ψ1:K )

dεd .

As with LDA[24], it is not efﬁciently computable. Thus, we
make use of mean-ﬁeld variational EM algorithm [1] to efﬁciently obtain an approximation of this posterior distribution
of the latent variables in TWDA. We maximize the evidence
lower bound(ELBO) L(·) [4] using Jensen’s inequality, and
for a document d we have the form:

The topic distribution of d is simpliﬁed to λ, and as we shown
above, λ is draw by a Dirichlet prior μ. It means that the topic
proportions for the document d as a draw from a Dirichlet
distribution which is the basic assumption of LDA [5].
It is important to distinguish TWDA from the Author-Topic
Model [26]. In the author-topic model, the words w is chose
only by one of the given tags’ distribution, while in TWDA,
for word w, all the observed tags in the document would make
the contributions.

L(ξ1:ld , γ1:K , ρ1:K ; η1:L , π1:L , μ1:K , θ1:L , ψ1:K )
= E[log p(T1:ld |η1:L )] + E[log p(εd |T d × π)]
N

θ
E[log p(zi |(εd )T × T d × ( ))]
+E[log p(λd |μ)] +
λ
i=1

TWDA has the capability to deal with not only the multitag corpus but also non-tag corpus. For non-tag corpora,
with the Dirichlet prior μ, TWDA may degenerate into LDA.
For multi-tag corpus, there are two main differences between
PLDA and TWDA. First, the generative process in PLDA
assumes that a word in a document d is generated only by
choosing one of the observed tags, while TWDA assumes the
topic distribution of d obtained by weighted average of all the
observed tags’ topic distributions and a Dirichlet prior λ, which
means that each word in d can be affected by some of the
tags, which is more reasonable since the tags can be relevant.
Second, a global topic distribution over tags (θ in Figure 1)
can be obtained in TWDA. The beneﬁt of this feature is that
we can analyze the characteristics of the tag set of the corpus.

+

N


E[log p(wi |zi , ψ1:K )] + H(q),

i=1

where ξ is a ld -dimensional Dirichlet parameter vector, ρ is a
1×K vector and γ is 1×K vector, all of which are variational
parameters of variational distribution shown in Figure 2, and
H(q) indicates the entropy of the variational distribution:
H(q) = −E[log q(εd )] − E[log q(λ)] − E[log q(z)].
Here the exception is taken with respect to a variational
distribution q(εd , q(λd ), z1:N ), and we choose the following
fully factorized distribution:

D. Learning and Inference

q(εd , λd , z1:N |ξ1:L , ρ1:K , γ1:K )
N

q(zi |γi ).
= q(εd |ξ)q(λd |ρ)

In this model, we treat π, μ, η, θ and ψ as unknown
constants to be estimated, and use a variational expectationmaximization (EM) procedure to carry out approximate maximum likelihood estimation.

i=1

441

where

However, the term of the expected log probability of a topic
assignment
θ
E[log p(zi |(εd )T × T d × ( ))]
λ
K

θ
γik E[log((εd )T × T d × ( ))k ]
=
λ



k=1

could be difﬁcult to compute, because of tag-weighted topic
assignment which is used in TWDA. Thus we use Jensen’s
inequality:
θ
E[log((εd )T × T d × ( ))k ]
λ
d
l
−1
(i)
εdi θk + εdld λk )]
= E[log(

=

≥ E[

	L+1
l



=

(i)

(i)

where the expression of θ , i ∈ {1, · · · , l − 1}, means the
i-th tag’s topic assignment vector, corresponding to the i-th
row of Θd .
ld
The ﬁrst expectation is E[εdi ] = ξi / j=1 ξj , and because
the variational distribution is fully factorized, so the second
expectation is
d

L[ρ]

=
+
+

j  =1

ρj  )) ld

j=1 ξj

].

+

γik ·

i=1 k=1
l

i=1

−

ld

i=1

j  =1

j=1
d

− log Γ(

(j)

Ck ξj /

l


K


· (Ψ(ρk ) − Ψ(

K


ρj )) ·

j=1
N


K


ξd
γni · ldl

j=1 ξj

ρj ) +

K


j  =1

)

log Γ(ρi ).

i=1

N


ξd
γni · ldl

j=1 ξj

.

j  =1

wi

ξj  )),

ρj )).

(5)

Optimization with respect to γ: Adding the Lagrange
multipliers to the terms which contain γik , taking the derivative
with respect to γik , and setting the derivative to zero yields,
we obtain the update equation of γik :
ld
(j)
ξ
γik ∝ ψk,vwi exp{ j=1 Ck ld j
},
(6)

ξj 

log Γ(ξi )
ld


K

j=1

(Ψ(ρi ) − Ψ(

n=1

i=1

(ξi − 1)(Ψ(ξi ) − Ψ(

=

ρi = μi +

d

ξi ) +

ρj ))

Taking the derivative with respect to ρi and setting it to zero,
we obtain a maximum at:

j =1
ld


K

j=1

j=1 ξj

j=1



ld


j=1

(ρi − 1)(Ψ(ρi ) − Ψ(

ξd
γik · ldl

− log Γ(

d

N 
K


K

ρj )) − log Γ(
ρj )

i=1

n=1

l
L+1
l



=
(
πl Tild − 1)(Ψ(ξi ) − Ψ(
ξj  ))
i=1 l =1

log Γ(ρi ) −

K


(μi − ρi +

Optimization with respect to ξ: We ﬁrst maximize L(·)
with respect to ξi for the document d. Maximize the terms
which contain ξ:



K


i=1

For a single document d, the variational parameters include
ξ d , ρd and γik . First, we maximize L(·) with respect to the
variational parameters to obtain an estimate of the posterior.

L[ξ]

(4)

j=1

K 
N


L[ρ]
ξl d

d

γid k ·

This simpliﬁes to:

k=1

+ (Ψ(ρk ) − Ψ(

K


k=1 i=1

K

K


(μi − 1)(Ψ(ρi ) − Ψ(

i=1


θ
γik ·
E[log p(zi |(εd )T × T d × ( ))] =
λ
i=1
i=1

ξj
(j)
[
log θk ld


j=1
j =1 ξj

K

i=1

E[εdld · log λk ] = E[εdld ] · E[log λk ],
ld
where E[εdld ] = ξld / j=1 ξj , and E[log λk ] = Ψ(ρk ) −
K
Ψ( j  =1 ρj  ). Thus, for the document d,

d
l
−1

N 
K


+

Optimization with respect to ρ: Next, we maximize L(·)
with respect to ρ. The terms that involve the variational
Dirichlet ρ are:

i=1

N

πl Tild − ξi

Here we use gradient descent method to ﬁnd the ξ to make
the maximization of L[ξ] .

log θk E[εdi ] + E[εdld · log λk ],

N


j=1




l=1
i =1 k=1
l d
(j) ld
(j)
Ck ( j=1 ξj ) − j=1 Ck ξj
).
(
l d
( j  =1 ξj  )2

εdi log θk + εdld · log λk ]

(i)

l=1

i=1

i=1
d
l
−1

log θk
K
Ψ(ρk ) − Ψ( j  =1 ρj  )

d

i=1
d
l
−1

(j)

j ∈ {1, · · · , ld − 1}
,
j = ld
(3)
and Ψ(·) denotes the digamma function, the ﬁrst derivative of
the log of the Gamma function. The derivative of Eq. (2) with
respect to ξi is
⎛ d ⎞


	L+1
l





= Ψ (ξi )
πl Tild − ξi − Ψ ⎝
ξj ⎠ ·
L (ξi )
(j)
Ck

ξj 

where v denotes the index of wi in the dictionary. In E-step,
we update the ξ, ρ and γ for each document with the initialized
model parameters.

(2)

442

2) M-step: The M-step needs to update ﬁve parameters: η,
the tagging prior probability, π, the Dirichlet prior of the tags’
weights, θ, the topic distribution over all tags in the corpus, ψ,
the probability of a word under a topic, and μ, a Dirichlet prior
of model. Because each document’s tag-set is observed, the
Bernoulli prior η is unused included for model completeness.
For a given corpus, the ηi is estimated by adding up the number
of i-th tag which appears in the corpus.

For the variational Dirichlet parameters μ, the involved
terms are:
L[μ]

d

L[π]

= log Γ(

d

(T × π)i ) −
d

i=1

+

l


+

d

i=1

j=1

D


L[πl ] =

d

Ψ(

d=1

l L+1



(7)

i=1 l =1
d

−

πl ·

D 
l


Ψ(

·

l


Tild

i=1

L+1

l =1

d=1 i=1

Tild )

d

πl · Tild ) · Tild +

D 
l


(Ψ(ξi )

d=1 i=1

d

−Ψ(

l


ξj )) · Tild .

(8)

j=1

The only term that involves θ is:
L[θ] =

ld

(j)
γik
log θk ξj /
ξj  ,
j=1
d=1 i=1 k=1
j  =1

D 
N 
K


d

l


(9)

where ξj , j ∈ {1, · · · , ld } in the document d needs to be
extended to tdl ·ξld , l ∈ {1, · · · , L+1} for convenient to simplify
L[θ] . With the Lagrangian of the Eq. (9), which incorporate the
constraint that the K-components of θl sum to one, we obtain
the estimation of θ over the whole corpus,
θlk ∝

D 
N


ξld tdl
d
γik
L+1 d d .
l=1 (ξl tl )
d=1 i=1

IV.

=

D 
N 
V
K 


+

λk (

k=1

v


γik (wd )ji log ψkj

ψkj − 1).

j=1

Take the derivative with respect to ψkj , and set it to zero, we
get:
ψkj ∝

D 
N


d
γik
(wd )ji .

(μi − 1)(Ψ(ρdi ) − Ψ(

K


ρdj ))).

(12)

j=1

E XPERIMENTAL A NALYSIS

In the experiments of this work, we used two semistructured corpora. The ﬁrst one consists of technical papers of
the Digital Bibliography and Library Project (DBLP) data set3 ,
which is a collection of bibliographic information on major
computer science journals and proceedings. In this paper, we
use a subset of DBLP that contains abstracts of D=27,435
papers, with W =70,062 words in the vocabulary and L=6,256
unique tags. The tags we used in DBLP include authors and
keywords. And the second document collection is the data
from Internet Movie Database (IMDB)4 . The data set includes
12,091 movie storylines, 52,274 words after removing stop
words, and 3,654 tags. These movies belong to 29 genres.
And the tags we used contain directors, stars, time, and movie
keywords.

d=1 i=1 k=1 j=1
K


log Γ(μi )

i=1

A. Experiment Settings
(10)

To maximize with respect to ψ, we isolate corresponding
terms and add Lagrange multipliers:
L[ψ]

j=1

K


Algorithm 1 The variational expectation-maximization (EM)
algorithm of TWDA
1: Input: a semi-structured corpora including totally V
unique words, L unique tags, and the expected number
K of topics
2: Output: Topic-word distributions ψ, Tag-topic distributions θ, π, μ, topic distribution ϑd and weight vector εd
of each training document.
3: initialize π and μ.
K
4: initialize θ and ψ with the constraint of
k=1 θlk equals
V
1 and i=1 ψki equals 1.
5: repeat
6:
for for each document d do
7:
update ξ d with Eqs. (2) and (4) using gradient descent
method,
8:
update ρ with Eq. (5),
9:
update γik with Eq. (6).
10:
end for
11:
update π with Eqs. (7) and (8) using gradient descent
method,
12:
update μ with Eq. (12) by Newton-Raphson algorithm,
13:
update θ by Eq. (10),
14:
update ψ by Eq. (11).
15: until convergence

d



μj ) −

We summarize the variational expectation-maximization
(EM) procedure of TWDA in Algorithm 1.

L+1
d
where (T d × π)i =
l=1 πl Til . We use gradient descent
method by taking derivative of Eq. (7) with respect to πl on
the corpus to ﬁnd the estimation of π. Taking derivatives with
respect to πl on the corpus, we obtain:


K


K


We can invoke the linear-time Newton-Raphson algorithm to
estimate μ as same as the Dirichlet parameter described in
LDA [5].

i=1

d

(log Γ(

i=1



log Γ (T d × π)i

l
l



 d
ξj )),
(T × π)i − 1 (Ψ(ξi ) − Ψ(

D

d=1

For the document d, the terms that involve the Dirichlet
prior π:
l


=

3 http://www.informatik.uni-trier.de/∼ley/db/

(11)

4 http://www.imdb.com

d=1 i=1

443

B. Results on Documents Modeling
In order to evaluate the generalization capability of the
model, we use the perplexity score that described in [5]. There
are two parts of the experiments.

8000

Perplexity

First, We trained three latent variable models including
LDA [5], CTM [3] and our TWDA, on the corpora of a set
of movie documents in IMDB, to compare the generalization
performance of the three models. In this part, LDA and CTM
trains text data without taking advantage of tag information.
We removed the stop words and conducted experiments using
5-fold cross-validation. Figure 3(a) demonstrates the perplexity
results on the IMDB data set. Clearly, TWDA excels both
CTM and LDA signiﬁcantly and consistently, and the results
show that TWDA works very well in semi-structured document
modeling.

7000

CTM

LDA

TWDA

6000

10 20

Second, in order to compare the performance of TWDA
with other topic models which take advantage of the tag
information, we trained TWDA, DMR5 , PLDA6 , Author Topic
Model (ATM) [26], CTM, and LDA on the set of movie
documents in IMDB and computed the perplexity on test data
set as described in DMR [21]. Since CTM and LDA could
not handle corpus with tags easily, in this experiment, we
treated the given tags as word features for them. Figure 3(b)
demonstrates the perplexity results of the ﬁve models on the
IMDB data. The experiment results shows that TWDA is better
than other models, and when T increases, DMR, CTM and
LDA are running into over-ﬁtting, while the trend of TWDA
keeps going down and the perplexity is signiﬁcantly lower than
those of the baselines.

50

100

150

# of Topics

200

(a) Perplexity results for TWDA, LDA and CTM

ATM

15000

CTM

DMR

LDA

TWDA

Perplexity

12500

10000

7500

As PLDA [25] assumes that one of tags may optionally
denote as a tag “latent” present on every document d, thus,
we trained PLDA and TWDA over 1021 and 2041 topics on
IMDB data set with 1020 tags, since in PLDA, each latent topic
takes part in exactly one tag in a collection. As shown in [25],
PLDA builds on Labeled LDA [24], and when it set one latent
topic and one topic for each tag, it is approximately equivalent
to Labeled LDA. For this case, we trained PLDA over 1021
topics. Figure 3(c) shows the perplexity results of TWDA and
PLDA. As the results of Figure 3 shown, TWDA works very
well compared with other topic models which make use of tag
information.

5000
10 20

50

100

# of Topics

150

200

(b) Perplexity results for TWDA, DMR, ATM, LDA and
CTM
4000
PLDA

TWDA

3000

Perplexity

C. Results on Tags prediction
In the preceding section we demonstrated the performance
of TWDA on the tags prediction by process the paper collection in DBLP. In addition to predicting the tags given a
document, we evaluate the ability of the proposed model,
compared with the Author Topic Model (ATM) [26] and DMR
[21], to predict the tags of the document conditioned on words
in the document. In this part, we treat the authors of each
paper as the tags, and the abstract as the word features, and
we predict the authors of one paper by modeling the paper
abstract document data using ATM, DMR, and TWDA. For
each model, we can evaluate the likelihood of the authors given
the word features of one document, and rank each possible

2000

1000

0

1021

# of Topics

2041

(c) Perplexity results for TWDA and PLDA
Fig. 3. Perplexity results of different models on IMDB corpora. LDA and
CTM only use the words when training in (a), and add the tags as the word
feature during the training process in (b).

5 We

used the Mallet code (http://mallet.cs.umass.edu/).
used the code in Stanford Topic Modeling Toolbox (http://wwwnlp.stanford.edu/software/tmt/tmt-0.4/).
6 We

444

0.8
1.00
ATM

DMR

TWDA

LDA+TFIDF

0.75

TFIDF

TWDA

TWDA+TFIDF

Recall

Precision

0.7

0.50

0.6

0.25

0.5

0.00
100

400

800 1000

1500

Rank

2000

2500

3000

3300

recall@1

(a) Results of Author retrieval recall for TWDA, DMR and ATM by setting
K=100

1.00

ATM

DMR

recall@3

recall@5

Fig. 5. Classiﬁcation results of different features on recall@1, recall@3 and
recall@5 with 5-fold cross-validation.

TWDA

For DMR and ATM, the method which deﬁne p(dtest |a) is
shown as [21]. Note that the likelihoods for a given author over
a document are not necessarily comparable among the topic
models, as described in [21], however, what we are interested
in is the ranking as same as [21].

0.75

Recall

We trained the three models on DBLP data set using 5fold cross-validation and shows the recall when the topic in
the corpora is set to be 100 and 200. Results are shown in
Figure 4(a) and Figure 4(b). TWDA ranks authors consistently
higher than ATM and DMR.

0.50

D. Results on Feature Construction for Classiﬁcation

0.25

The next experiment is to test the classiﬁcation performance utilizing feature sets generated by TWDA and other
baselines. For the base classiﬁer, we use LIBSVM [9] with
Gaussian kernel and the default parameters. For the purpose
of comparison, we trained four SVMs on tf-idf word features,
features induced by a 30-topic LDA model and tf-idf word
features, features generated by a TWDA model with the same
number of topics, and features induced by a 30-topic TWDA
model and tf-idf word features respectively.

0.00
100

400

800 1000

1500

Rank

2000

2500

3000

3300

(b) Results of Author retrieval recall TWDA, DMR and ATM by setting K=200
Fig. 4. Prediction results of TWDA, DMR and ATM for authors on DBLP
corpora. We set the number of topic in the corpora to be 100 in (a) and 200
in (b).

In these experiments, we conducted multi-class classiﬁcation experiments using the IMDB data set, which contains
29 genres. We calculated the evaluation metrics recall@1,
recall@3 and recall@5 with the provided class tags of movies’
genres, using 5-fold cross-validation. We report the movie
classiﬁcation performance of the different methods in Figure 5,
where we see that there is signiﬁcant improvement in classiﬁcation performance when using LDA and TWDA comparing
with only using tf-idf features, and TWDA outperforms both
LDA and tf-idf in terms of recall@1, recall@3 and recall@5.

author by the likelihood function of the author. First, for each
model, we can get the topic distribution over a test document
dtest given one author a. Then, we evaluate the p(dtest |a) for
dtest over each author a in the tags(authors) set by
p(dtest |a) =

N 

(
p(z|a)p(wi |z)).
i

In order to show the classiﬁcation performance better, we

z

445

12500

C LASSIFICATION RESULTS OF DIFFERENT FEATURES ON
F1- SCORE
F1-score

@1

@3

@5

TFIDF

0.5

0.41

0.39

LDA+TFIDF

0.5

0.42

0.39

TWDA

0.57

0.5

0.47

TWDA+TFIDF

0.58

0.5

0.47

10000

Perplexity

TABLE I.

TABLE II.
S OME EXAMPLES OF THE NORMALIZED WEIGHTS AMONG
THE ORIGINAL TAGS AND NOISE TAGS . T HE TAGS IN RED ARE NOISE TAGS ,
AND THE NUMBERS ARE THE WEIGHT VALUES .

7500

“Bug isolation via remote program sampling [19]”
Ben Liblit: 0.185
aAlice X. Zheng: 0.228

ATM

5000

Alex Aiken: 0.2257

DMR

TWDA

Michael I. Jordan: 0.349

K. G. Shin: 0.01
“Web question answering: is more always better? [13]”
Susan Dumais: 0.986

2500

Michele Banko: 0.0032

Eric Brill: 0.0038

Jimmy Lin: 0.0038

0.0

0.2

Andrew Ng: 0.0024

R. Katz: 0.00018

0.4

0.5

0.8

% of noise

1.0

(a) K=100

“Contextual search and name disambiguation in email using graphs [22]”
Einat Minkov: 0.425

16000

William W. Cohen: 0.342

Andrew Y. Ng: 0.128

J. Ma: 0.033

D. Ferguson: 0.07

“A Sparse Sampling Algorithm for Near-Optimal Planning
12000

in Large Markov Decision Processes [16]”
Michael Kearns: 0.296

Yishay Mansour:0.166

J. Blythe: 0.089

Perplexity

Andrew Y. Ng: 0.31

B. Adida: 0.027

P. J. Modi: 0.1

8000

“The nested Chinese restaurant process and bayesian nonparametric inference of
ATM

topic [2]”
David M. Blei:0.46

DMR

TWDA

Thomas L. Grifﬁths:0.186

Michael I. Jordan:0.225

B. Clifford:0.031

4000

R. Szeliski:0.048

X. Wang:0.05

also calculated the evaluation metrics F-Measure (F1-score) by
F1 = 2 ·

0.0

P recision · Recall
P recision + Recall

0.2

0.4

0.5

% of noise

0.8

1.0

(b) K=200
Fig. 6. The Results of adding noise to different models(ATM, DMR and
TWDA). (a) set K=100, and (b) set K=200. Steady trending means a good
performance on model robustness.

The results of F-Measure is reported in Table I. TWDA
provides substantially better performance on F-Measure.
E. Results on Model Robustness

set of the DBLP corpora and added into the paper as a noise
author.

We demonstrated the performance of the proposed model
on model robustness in the last part of experimental analysis.
In this part, we measured and compared the perplexity when
we added noise tags information to the test documents using
DBLP data set. Respectively, we randomly added 20%, 40%,
50%, 80% and 100% noise tags into a test document and then
calculated the perplexity. For example, if a paper document in
DBLP has ﬁve authors, adding 20% noise is that we randomly
selected one author (not appeared in the paper) from the author

And in some real applications, the noise tags appeared in
a document may have some relevance to the real tags. So in
this experiment, we selected the noise tags from the authortag set to meet the real applications to some extent. In this
experiment, the DBLP corpora contains more than 6,000 tags,
the noise tags we added into a test document would be very

446

sparse for the whole tag set in the corpora. So, we added the
different percentages noise tags into the test document to show
the trend of perplexity as the noise content increases. Figure 6
shows that both TWDA and ATM have a more steady trend
as the noise level increases, compared with DMR.

[9]
[10]

[11]

Table II shows some examples about the weights between
the original tags and noise tags. The red tags are the noise
added into the test data, and the values behind are the weights
among the tags we inference from the TWDA model. Note
that, we showed the weight values after normalized. As the
results shown, TWDA has a good performance on model
robustness, for the weight values of the noise tags are much
smaller than the other original tags. In some applications, we
can use the proposed model to rank the tags given in a document, which would be a good approach to tag recommendation
and annotation.
V.

[12]

[13]

[14]
[15]

C ONCLUSION

[16]

In this paper, we propose a topic model called tag-weighted
Dirichlet allocation, which builds on LDA and provides a
probabilistic approach for mining semi-structured documents.
This model shows a novel method to generate and analyze the
topic proportions of a document using normalized weights of
a Dirichlet prior and the structure information (observed tags)
in the document. And as shown in the experiments, it not
only provides signiﬁcant improvement in term of document
modeling compared to other topic models, but also gives a
way to predict the latent tags for tag-unknown documents.
Meanwhile, the TWDA can handle the multi-tag documents
and non-tag documents at the same time, since when it comes
to non-tag documents, TWDA degenerates into LDA easily.
In the future, more work should to be done on the effective
and efﬁcient solutions of large scale semi-structured documents
and the different practical areas (e.g., image classiﬁcation and
annotation, video retrieval).

[17]

[18]

[19]

[20]

[21]

[22]

ACKNOWLEDGMENT

[23]

We thank the anonymous reviewers for helpful comments.
This work was supported by National Science Foundation of
China (61003140 and 61033010).

[24]

[25]

R EFERENCES
Christopher M. Bishop and Nasser M. Nasrabadi. Pattern Recognition
and Machine Learning. J. Electronic Imaging, 16(4):049901, 2007.
[2] David M. Blei, Thomas L. Grifﬁths, and Michael I. Jordan. The nested
chinese restaurant process and bayesian nonparametric inference of
topic hierarchies. J. ACM, 57(2):7:1–7:30, February 2010.
[3] David M. Blei and John D. Lafferty. Correlated topic models. In NIPS,
2005.
[4] David M. Blei and Jon D. McAuliffe. Supervised topic models. In
NIPS, 2007.
[5] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent dirichlet
allocation. Journal of Machine Learning Research, 3:993–1022, 2003.
[6] Jordan L. Boyd-Graber and David M. Blei. Syntactic topic models.
CoRR, abs/1002.4665, 2010.
[7] Andrej Bratko and Bogdan Filipic. Exploiting structural information
for semi-structured document categorization. Information Processing
and Management, 42(3):679 – 694, 2006.
[8] Deng Cai, Qiaozhu Mei, Jiawei Han, and Chengxiang Zhai. Modeling
hidden topics on document manifold. In CIKM, pages 911–920, 2008.
[1]

[26]

[27]
[28]

[29]
[30]

447

Chih-Chung Chang and Chih-Jen Lin. Libsvm: A library for support
vector machines. ACM TIST, 2(3):27, 2011.
Jonathan Chang and David M. Blei. Relational topic models for document networks. Journal of Machine Learning Research - Proceedings
Track, 5:81–88, 2009.
Xu Chen, Mingyuan Zhou, and Lawrence Carin. The contextual focused
topic model. In KDD, pages 96–104, 2012.
Hongbo Deng, Jiawei Han, Bo Zhao, Yintao Yu, and Cindy Xide Lin.
Probabilistic topic models with biased propagation on heterogeneous
information networks. In KDD, pages 1271–1279, 2011.
Susan Dumais, Michele Banko, Eric Brill, Jimmy Lin, and Andrew
Ng. Web question answering: is more always better? In Proceedings
of the 25th annual international ACM SIGIR conference on Research
and development in information retrieval, SIGIR ’02, pages 291–298,
New York, NY, USA, 2002. ACM.
Thomas Hofmann. Probabilistic latent semantic indexing. In SIGIR,
pages 50–57, 1999.
Tomoharu Iwata, Takeshi Yamada, and Naonori Ueda. Modeling social
annotation data with content relevance using a topic model. In NIPS,
pages 835–843, 2009.
Michael Kearns, Yishay Mansour, and Andrew Y. Ng. A sparse
sampling algorithm for near-optimal planning in large markov decision
processes. Mach. Learn., 49(2-3):193–208, November 2002.
Simon Lacoste-Julien, Fei Sha, and Michael I. Jordan. Disclda:
Discriminative learning for dimensionality reduction and classiﬁcation.
In NIPS, pages 897–904, 2008.
Shuangyin Li, Jiefei Li, and Rong Pan. Tag-weighted topic model for
mining semi-structured documents. Proceedings of the Twenty-Third
International Joint Conference on Artiﬁcial Intelligence, 2013.
Ben Liblit, Alex Aiken, Alice X. Zheng, and Michael I. Jordan. Bug
isolation via remote program sampling. SIGPLAN Not., 38(5):141–154,
May 2003.
Pierre-Francois Marteau, Gildas Ménier, and Eugen Popovici. Weighted
naive bayes model for semi-structured document categorization. CoRR,
abs/0901.0358, 2009.
David M. Mimno and Andrew McCallum. Topic models conditioned on
arbitrary features with dirichlet-multinomial regression. In UAI, pages
411–418, 2008.
Einat Minkov, William W. Cohen, and Andrew Y. Ng. Contextual search
and name disambiguation in email using graphs. In Proceedings of
the 29th annual international ACM SIGIR conference on Research and
development in information retrieval, SIGIR ’06, pages 27–34, New
York, NY, USA, 2006. ACM.
James Petterson, Alexander J. Smola, Tibério S. Caetano, Wray L.
Buntine, and Shravan Narayanamurthy. Word features for latent dirichlet
allocation. In NIPS, pages 1921–1929, 2010.
Daniel Ramage, David Hall, Ramesh Nallapati, and Christopher D.
Manning. Labeled lda: A supervised topic model for credit attribution
in multi-labeled corpora. In EMNLP, pages 248–256, 2009.
Daniel Ramage, Christopher D. Manning, and Susan Dumais. Partially
labeled topic models for interpretable text mining. In Proceedings of the
17th ACM SIGKDD international conference on Knowledge discovery
and data mining, KDD ’11, pages 457–465, New York, NY, USA, 2011.
ACM.
Michal Rosen-Zvi, Chaitanya Chemudugunta, Thomas Grifﬁths,
Padhraic Smyth, and Mark Steyvers. Learning author-topic models from
text corpora. ACM Trans. Inf. Syst., 28(1):4:1–4:38, January 2010.
Markus Tresch, Neal Palmer, and Allen Luniewski. Type classiﬁcation
of semi-structured documents. In VLDB, pages 263–274, 1995.
Xing Wei and W. Bruce Croft. Lda-based document models for
ad-hoc retrieval. In Proceedings of the 29th annual international
ACM SIGIR conference on Research and development in information
retrieval, SIGIR ’06, pages 178–185, New York, NY, USA, 2006. ACM.
Jeonghee Yi and Neel Sundaresan. A classiﬁer for semi-structured
documents. In KDD, pages 340–344, 2000.
Jun Zhu, Amr Ahmed, and Eric P. Xing. Medlda: maximum margin
supervised topic models for regression and classiﬁcation. In ICML,
page 158, 2009.

NeuroImage 46 (2009) 717–725

Contents lists available at ScienceDirect

NeuroImage
j o u r n a l h o m e p a g e : w w w. e l s e v i e r. c o m / l o c a t e / y n i m g

Automated segmentation of mouse brain images using extended MRF
Min Hyeok Bae a, Rong Pan a,⁎, Teresa Wu a, Alexandra Badea b
a
b

Department of Industrial, Systems and Operations Engineering, Arizona State University, Tempe, AZ 85287–5906, USA
Center for In Vivo Microscopy, Box 3302, Duke University Medical Center, Durham, NC 27710, USA

a r t i c l e

i n f o

Article history:
Received 14 October 2008
Revised 26 December 2008
Accepted 7 February 2009
Available online 21 February 2009
Keywords:
Automated segmentation
Data mining
Magnetic resonance microscopy
Markov random ﬁeld
Mouse brain
Support vector machine

a b s t r a c t
We introduce an automated segmentation method, extended Markov random ﬁeld (eMRF), to classify 21
neuroanatomical structures of mouse brain based on three dimensional (3D) magnetic resonance images
(MRI). The image data are multispectral: T2-weighted, proton density-weighted, diffusion x, y and z
weighted. Earlier research (Ali, A.A., Dale, A.M., Badea, A., Johnson, G.A., 2005. Automated segmentation of
neuroanatomical structures in multispectral MR microscopy of the mouse brain. NeuroImage 27 (2), 425–
435) successfully explored the use of MRF for mouse brain segmentation. In this research, we study the use of
information generated from support vector machine (SVM) to represent the probabilistic information. Since
SVM in general has a stronger discriminative power than the Gaussian likelihood method and is able to
handle nonlinear classiﬁcation problems, integrating SVM into MRF improved the classiﬁcation accuracy. The
eMRF employs the posterior probability distribution obtained from SVM to generate a classiﬁcation based on
the MR intensity. Secondly, the eMRF introduces a new potential function based on location information.
Third, to maximize the classiﬁcation performance, the eMRF uses the contribution weights optimally
determined for each of the three potential functions: observation, location and contextual functions, which
are traditionally equally weighted. We use the voxel overlap percentage and volume difference percentage to
evaluate the accuracy of eMRF segmentation and compare the algorithm with three other segmentation
methods — mixed ratio sampling SVM (MRS-SVM), atlas-based segmentation and MRF. Validation using
classiﬁcation accuracy indices between automatically segmented and manually traced data shows that eMRF
outperforms other methods.
© 2009 Elsevier Inc. All rights reserved.

Introduction
MRI is often the imaging modality of choice for noninvasive
characterization of brain anatomy because of its excellent soft tissue
contrast. Its detailed resolution allows the investigation of normal
anatomical variability bounds, as well as the quantization of
volumetric changes accompanying neurological conditions. A prerequisite for such studies is an accurate segmentation of the brain;
therefore, many studies have focused on tissue classiﬁcation into
white matter, gray matter and cerebrospinal ﬂuid (CSF), as well as
anatomical structure segmentation. Several successful methods for
tissue segmentation include statistical classiﬁcation and geometrydriven segmentation (Ballester et al., 2000), statistical pattern
recognition methods based on a ﬁnite mixture model (Andersen et
al., 2002), expectation maximization algorithm (Kovacevic et al.,
2002), artiﬁcial neural network (Reddick et al., 1997), hidden Markov
random ﬁeld (Zhang et al., 2001), and region-based level-set and
fuzzy classiﬁcation (Suri, 2001). Compared to anatomical structure

⁎ Corresponding author. Fax: +1 480 965 8692.
E-mail address: rong.pan@asu.edu (R. Pan).
1053-8119/$ – see front matter © 2009 Elsevier Inc. All rights reserved.
doi:10.1016/j.neuroimage.2009.02.012

segmentation, tissue segmentation is a relatively easier task (Heckemann et al., 2006). This is because the MR signals are in general
distinctive enough to have the brain tissues classiﬁed into white
matter, gray matter and CSF, while the anatomical structures can be
composed of more than one tissue type, and have ambiguous
contours. Nevertheless, segmenting neuroanatomical structures has
recently attracted considerable attention since it can provide stronger
foundation to help in the early diagnosis of a variety of neurodegenerative disorders (Fischl et al., 2002).
Automated methods for segmenting the brain in anatomically
distinct regions can rely on either a single imaging protocol or multispectral data. For example, Duta and Sonka (1998) have segmented
T1-weighted MR images of the human brain into 10 neuroanatomical
structures using active shape models. Fischl et al. (2002) accomplished an automated labeling of each voxel in the MR human brain
images into 37 neuroanatomical structures using MRF. Heckemann
et al. (2006) segmented T1-weighted human MR images into 67
neuroanatomical structures using nonrigid registration with free-form
deformations, and combined label propagation and decision fusion for
classiﬁcation. Multispectral imaging was used as the basis of segmentation, for example, by Zavaljevski et al. (2000) and Amato et al.
(2003). Zavaljevski et al. (2000) used Gaussian MRF and maximum

718

M.H. Bae et al. / NeuroImage 46 (2009) 717–725

likelihood method on multi-parameter MR images (including T1, T2,
proton density, Gd+ T1 and perfusion imaging) to segment the
human brain into 15 neuroanatomical structures. Amato et al. (2003)
used independent component analysis (ICA) and nonparametric
discriminant analysis to segment the human brain into nine classes.
Developments in small animal imaging capabilities have led to
increased attention being given to the problem of mouse brain
segmentation. Mice provide excellent models for the anatomy,
physiology, and neurological conditions in humans, because with
whom they share more than 90% of gene structure. Unfortunately, the
methods for human brain anatomical structure segmentation cannot
directly be applied with the same success to the mouse brain. This is
due to the facts that (1) a normal adult mouse brain is approximately
3000 times smaller than an adult human brains, thus the spatial
resolution needs to be scaled accordingly from the 1-mm3 voxels
commonly used in the study for human brains to voxel volumes
b0.001 mm3; (2) the inherent MR contrast of the mouse brain is
relatively low compared to the human brain.
Research on mouse brain image segmentation is still limited thus
far and the major directions of research include the atlas-based
approach and the probabilistic information based approach. The atlasbased approach is to create a neuroanatomical atlas, also called reference atlas, using training brains and nonlinearly register the atlas to
test brains with the scope of labeling each voxel of the test brains
(Mazoyer et al., 2002; Kovacevic et al., 2005; Ma et al., 2005; Bock
et al., 2006). For example, Ma et al. (2005) segmented T2⁎-weighted
magnetic resonance microscopy (MRM) images of 10 adult male
formalin-ﬁxed, excised C57BL/6J mouse brains into 20 anatomical
structures. They chose one mouse brain image out of the 10 mouse
brains as a reference atlas. The rest nine testing images were aligned to
the reference atlas using a six-parameter rigid-body transformation
and then the reference atlas was mapped to the nine testing brain
images using a nonlinear registration algorithm. The registration step
yields a vector ﬁeld, called a deformation ﬁeld, which contains
information on the magnitude and direction required to deform a
point in the reference atlas to the appropriate point in the testing
brain images. Using the deformation ﬁeld, the labeling of the reference
atlas is transformed to the testing image to predict the labeling of the
testing images. Kovacevic et al. (2005) built a variational atlas using
the MR images of nine 129S1/SvImJ mouse brains. The MR images of
the genetically identical mouse brains were aligned and nonlinearly
registered to create the variational atlas comprised of an unbiased
average brain. The probabilistic information based approach uses
probabilistic information extracted from training datasets of MR
images for the segmentation. Ali et al. (2005) incorporated multispectral MR intensity information, location information and contextual information of neuroanatomical structures for segmentation
of MRM images of the C57BL/6J mouse brain into 21 neuroanatomical
structures using the MRF method. They modeled the location
information as a prior probability of occurrence of a neuroanatomical
structure at a location in the 3D MRM images and the contextual
information as a prior probability of pairing of two neuroanatomical
structures.
MRF has been widely used in many image segmentation problems
and image reconstruction problems such as denoising and deconvolution (Geman and Geman, 1984). It provides a mathematical formulation for modeling local spatial relationships between classes. In the
brain image segmentation problem, the probability of a label for a
voxel is expressed as a combination of two potential functions: one is
based on the MR intensity information and the other is based on
contextual information, such as the labels of voxels in a predeﬁned
neighborhood around the voxel under study. Bae et al. (2008)
developed an enhanced SVM model, called Mix-Ratio samplingbased SVM (MRS-SVM), using the multispectral MR intensity
information and voxel location information as input features. The
MRS-SVM provided a comparable classiﬁcation performance with the

probabilistic information based approach developed in Ali et al.
(2005). Furthermore, Bae's study also suggested that compared to
MRF, the MRS-SVM outperforms for larger structures, but underperforms for smaller structures.
Based on these studies which suggest that integrating a powerful
SVM classiﬁer into the probabilistic information based approach may
improve the overall accuracy of mouse brain segmentation, we
introduce a novel automated method for brain segmentation, called
extended MRF (eMRF). In this method (eMRF), we use the voxel
location information by adding a location potential function. Other
than using Gaussian probability distribution to construct a potential
function based on MR intensity, the SVM classiﬁer is used to model the
potential function of MR intensity. We assess the accuracy of the
automated brain segmentation method in a population of ﬁve adult
C57BL6 mice, imaged using multiple (ﬁve) MR protocols. All 21
neuroanatomical structures are segmented, and the accuracy is
evaluated using two metrics including the volume overlap percentage
(VOP) and the volume difference percentage (VDP). The use of these
two metrics allows us to compare the accuracy of our method with the
atlas-based segmentation, MRF and MRS-SVM methods.
Materials and methods
Subjects and magnetic resonance microscopy data
The MRM images used in this work were provided by the Center
for In Vivo Microscopy in Duke University Medical Center and they
were previously used in Ali et al. (2005) as well. Five formalin-ﬁxed
C57BL/6J male mice of approximately 9 weeks in age were used. The
MRM image acquisition consisted of isotropic 3D T2-weighted,
proton density-weighted, diffusion x, y and z weighted scans. Image
acquisition parameters for all acquisition protocols include the ﬁeld
of view of 12 × 12 × 24 mm and matrix size of 128 × 128 × 256. All
protocols used the same ﬂip angle of 135 degrees and 2 excitations
(NEX). Speciﬁc to the PD image, TE/TR was 5/400 ms and bandwidth
was 62.5 KHz; for the T2 weighted image, TE/TR was 30/400 ms and
bandwidth was 62.5 kHz bandwidth; and for the three diffusion
scans, TE/TR was 15.52/400 ms, and bandwidth was 16.25 MHz. The
Stejskal Tanner sequence was used for the acquisition of the
diffusion-weighted scans. Bipolar diffusion gradients of 70 G/cm
with pulse duration of 5 ms and inter-pulse interval of 8.56 ms were
applied along the three axes and the effective b value of 2600 s/mm2
was used. A 9-parameter afﬁne registration which accounts for
scaling, rotation and translation was applied to each mouse brain, to
bring it into a common space. Manual labeling of 21 neuroanatomical
structures was done by two experts using T2-weighted datasets of
the ﬁve mouse brains. These manual labelings were regarded as true
labeling for each voxel. Table 1 presents the list of the 21
neuroanatomical structures and abbreviations to be segmented in
this work.

Table 1
List of 21 segmented structures and their abbreviations.
Structures

Abbrev.

Structures

Abbrev.

Structures

Abbrev.

Cerebral cortex

CORT

INFC

Pontine nuclei

PON

Cerebral peduncle

CPED

MED

Substantia nigra

SNR

Hippocampus

HC

Inferior
colliculus
Medulla
oblongata
Thalamus

THAL

INTP

Caudate putamen
Globus pallidus

CPU
GP

MIND
AC

Internal capsule
Periacqueductal gray

ICAP
PAG

Midbrain
Anterior
commissure
Cerebellum
Ventricular
system

Interpeduncular
nucleus
Olfactory bulb
Optic tract

CBLM
VEN

Trigeminal tract
Corpus callosum

TRI
CC

OLFB
OPT

M.H. Bae et al. / NeuroImage 46 (2009) 717–725

Markov random ﬁeld theory
MRF theory is a class of probability theory for modeling the
spatial or contextual dependencies of physical phenomena. It has
been used for brain image segmentation by modeling probabilistic
distribution of the labeling of a voxel jointly with the consideration
of the labels of a neighborhood of the voxel (Fischl et al., 2002; Ali
et al., 2005). For simplicity, let us consider a 2D image represented
by an m × n matrix, let X be a vector of signal strength and Y be the
associated labeling vector, that is, X = (x11, x12,…, x1n,…, xm1,… xmn),
and Y = (y11, y12,…, y1n,…, ym1,… ymn). We will rewrite yab as yi where
i = 1,2,…,S, S = mn, Y is said to be a MRF on S with respect to a neighborhood N if and only if the following two conditions are satisﬁed:
P ðY ÞN 0


ð1Þ

P yi jys − fig





= P yi jyNi

ð2Þ

where S-{i} denotes the set difference, and Ni denotes the set of sites
neighboring site i. Eq. (1) is called the positivity property and Eq. (2) is
the Markovianity property, which states only neighboring labels (or
clique) have direct interaction with each other. If these conditions are
satisﬁed, the joint probability P(Y) of any random ﬁeld is uniquely
determined by its local conditional probabilities.
The equivalence between the MRF and Gibbs distribution (Hammersley–Clifford theorem) provides a mathematically efﬁcient way of
specifying the joint probability P(Y) of an MRF. The theorem states the
joint probability P(Y) can be speciﬁed by the clique potential function
Vc(Y) which can be deﬁned by any appropriate potential function
based on the speciﬁc system's behavior. For more information about
potential function, please refer to Li (2001). The probability P(Y) can
be equivalently speciﬁed by a Gibbs distribution as follows:
1
exp ½−U ðY Þ
ð3Þ
Z
P
where Z =
exp ½−U ðY Þ is a normalizing constant, ΩY is the set of

P ðY Þ =

YaXY

the all possible Y on S, and U(Y) is an energy function which is deﬁned
as a sum of clique potential Vc(Y) over all cliques c ∈ C:
U ðY Þ =

X

Vc ðY Þ

ð4Þ

caC

where a clique, c, is a set of points that are all neighbors of each other and
C is a set of cliques, or the neighborhood of the clique under study. The
value of Vc(Y) depends on a certain conﬁguration of labels on the clique c.
For the image segmentation problem, we try to maximize a label's
posterior probability for given speciﬁc features, that is P(Y|X). With
the assumption of feature independency the posterior probability can
be formulated using Bayesian theorem as:
P ðY jX Þ~P ðX jY ÞP ðY Þ = P ðY Þ

S
Y

P ðxi jyi Þ:

719

techniques, such as SVM, are integrated with MRF, the overall
segmentation performance will be improved due to their powerful
class discrimination abilities even for the cases with complex
relationship between features and labels.
Support vector machines theory
SVM (Vapnik, 1995) was initially designed for binary classiﬁcation
by constructing an optimal hyperplane which gives the maximum
separation margin between two classes. Considering a training set of
m samples (xi, yi), i = 1, 2, …, m where xi ∈ Rn and yi ∈ {+1, − 1}.
Samples with yi = + 1 belong to positive class while those with yi =
−1 belong to negative class. SVM training involves ﬁnding the
optimal hyperplane by solving the following optimization problem:
m
X
1 T
w w+C
ni
2
i=1
s:t: yi ðw · Φðxi Þ + bÞz 1 − ni ; ni z 0;

Min QP ðw; b; nÞ =

ð7Þ
i = 1; N m:

where w is the n dimensional vector, b is a bias term, ξ = {ξ1,…ξm} and
QP is the objective function of the prime problem. The non-negative
slack variable (ξi) allows Eq. (7) to always yield feasible solutions even in
a non-separable case. The penalty parameter (C) controls the trade-off
between maximizing the class separation margin and minimizing the
classiﬁcation error. A larger C usually leads to higher training accuracy,
but may over-ﬁt the training data and cause the classiﬁer un-robust. To
enhance the linear separability, the mapping function (Φ(xi)) projects
the samples into a higher-dimensional dot-product space called the
feature space. Fig. 1 shows the optimal hyperplane in solid line, which
can be obtained by solving Eq. (7). The squares represent the samples
from the positive class and the circles represent the samples from the
negative class. The samples which satisfy the equality are called support
vectors. In Fig. 1, the samples are represented as the ﬁlled squares and
the support vectors are the ﬁlled circles.
Eq. (7) presents a constrained optimization problem. By introducing the non-negative Lagrangian multiplier αi and βi, it can be
converted to an unconstrained problem as shown below:
Min QP ðw; b; n; α; βÞ
=

m
m
m
 


X
X
X
1 T
T
ni −
α i yi w · Φðxi Þ + b − 1 + ni −
βi ni
w w + C
2
i=1
i=1
i=1

ð8Þ

where α = α1,…,αm and β = β1,…,βm. Furthermore, by differentiating
with respect to w, b and ξi and introducing the Karush–Kuhn–Tucker

ð5Þ

i=1

Eq. (5) can be rewritten as:
"
#
X
X
1
logP ðxi jyi Þ +
Vc ðY Þ :
P ðY jX Þ = exp
Z
caC
iaS

ð6Þ

Typically in MRF, a multivariate Gaussian distribution is used for
P(X|Y) and the maximum likelihood estimation is performed to ﬁnd
the labeling based on MR signals. This model assumes that the relationship between the features and labels follows the Gaussian distribution. In some of image segmentation problems, this assumption
is too restrictive to model the complex dependencies between the
features and the labels (Lee et al., 2005). If some data mining

Fig. 1. SVM binary classiﬁcation problem (adapted from Vapnik, 1995). The solid line
between the two dashed lines is the optimal hyperplane. The squares represent the
samples from the positive class and the circles represent the samples from the negative
class. The samples represented as the ﬁlled squares and the ﬁlled circles are the
support vectors.

720

M.H. Bae et al. / NeuroImage 46 (2009) 717–725

(KKT) condition, Eq. (8) is converted to the following Lagrangian dual
problem:
m
 
1X
T
α i α j yi yj Φðxi Þ Φ xj
2
i=1
i;j = 1
m
X
α i yi = 0:
0 V α i V C i = 1; N ; m and

Max QD ðα Þ =
s:t:

m
X

P ðY jX; LÞ =

αi −

ð9Þ

i=1

The optimal solution αi ⁎ for the dual problem determines the
parameters w⁎ and b⁎ of the following optimal hyperplane, also
known as SVM decision function:


f ðxÞ = sign w4 · Φðxi Þ + b4
= sign
= sign

m
X
i=1
m
X

4

4

+b
!

ð10Þ

4

yi α i K ðx; xi Þ + b

i=1

where K(xi,xj)is a kernel function deﬁned as K(xi,xj) = Φ(xi)T Φ (xj).
The kernel function performs the nonlinear mapping implicitly. We
chose a Radial Basis Function (RBF) kernel, deﬁned as:


K xi ; xj





2
= exp −γjjxi −xj jj ;

γN0

ð11Þ

where γ in Eq. (11) is a parameter related to the span of an RBF kernel.
The smaller the value is, the wider the kernel spans.
To extend the application of SVM for multiclass classiﬁcation, a number of methods have been developed, which mainly fall in three categories: One-Against-All (OAA), One-Against-One (OAO) and All-At-Once
(AAO). In OAA method, one SVM is trained with the positive class
representing one class and the negative class representing the others.
Therefore, it builds n different SVM models where n is the number of the
classes. The idea of AAO is similar to that of OAA, but it determines n
decision functions at once in one model, where the kth decision function
separates the kth class from the other classes. In OAO method, a SVM is
trained to classify the kth class and the lth class. Therefore, it constructs
nðn − 1Þ = 2 SVM models. Hsu and Lin (2002) reported that the training
time of OAO method is less than that of OAA or AAO method. OAO is more
efﬁcient on large datasets than OAA and AAO method. Thus, OAO method
is used in this study. In OAO method, the following problem is to be solved:
m


X
1  kl T  kl 
kl
w
Min QP wkl ; bkl ; nkl =
w +C
ni
2
i=1
 T
s:t:
wkl · Φðxi Þ + bkl z 1 − nkl
if yi = k :
i ;
 T
kl
kl
kl
· Φðxi Þ + b V − 1 + ni ; if yi = l
w

nki z 0;

i = 1; N ; m

and

k; l = 1; N ; n

P ðX; L jY ÞP ðY Þ
:
P ðX; LÞ

ð13Þ

Assuming X and L independent from each other yields following
expression:
P ðX jY ÞP ðLjY ÞP ðY Þ
P ðX ÞP ðLÞ
P ðY jX ÞP ðX ÞP ðY jLÞP ðLÞP ðY Þ
~
P ðX ÞP ðLÞ
= P ðY jX ÞP ðY jLÞP ðY Þ:

P ðY jX; LÞ =

ð14Þ

If we make logarithmic transformation to Eq. (14), we obtain

!

4
T
yi α i ΦðxÞ Φðxi Þ

given a multivariate intensity vector X and a location vector L is
formulated as follows:

ð12Þ

eMRF
As discussed earlier in Eq. (6), traditional MRF mainly focuses on
MR intensity as well as the contextual relationship. Research on brain
segmentation has concluded that beside of MR intensity and
contextual relationship with neighboring structures, voxel location
within the brain also plays an important role (Fischl et al., 2002; Ali et
al., 2005, Bae et al., 2008). To incorporate the three different types of
information into an image segmentation model, Bayesian theorem is
used. Consider a 3D MR image represented by an m × n × o matrix,
with its associated multivariate intensity vector X = (x111, x112,…, x11o,
…, x1n1,… xm11,…, xmno), a location vector L = (l111, l112,…, l11o,…, l1n1,…
lm11,…, lmno), and class label conﬁguration Y = (y111, y112,…, y11o,…,
yl1n1,… ym11,…, ymno). We rewrite yabcas yi where i = 1,2,…,S, and
S = mno. The posterior probability of having a label conﬁguration Y

log P ðY jX; LÞ~ log P ðY jX Þ + log P ðY jLÞ + log P ðY Þ:

ð15Þ

Those terms on the right hand side of Eq. (15) can be regarded as the
contribution to labeling from MR signal intensities, voxel coordinates,
and prior belief of label, which incorporates the contextual information into the labeling decision. The function can be modiﬁed as
following:
P ðY jX; LÞ~
"
 exp w1

X

logAi ðyi ; xi Þ + w2

iaS

s:t:

X

log Bi ðyi ; li Þ + w3

iaS

w1 + w2 + w3 = 1

X

#


Vi yi ; yNi

iaS

ð16Þ

where w1, w2 and w3 are model parameters which indicate the degree
of contribution of each term to the posterior probability P(Y|X,L), and
Ni denotes the neighboring sites of site i.
The ﬁrst term, Ai(yi, xi), in Eq. (16) is the observation potential
function that models the MR intensity information. The eMRF model
employs SVM in order to take advantage of the excellent discriminative ability of SVM. Since SVM decision function gives as output the
distance from an instance to the optimal separation hyperplane, Platt
(2000) proposed a method for mapping the SVM outputs into
posterior probability by applying a sigmoid function whose parameters are estimated from the training process. The observation
potential function is deﬁned as follows, for voxel i:
Ai ðyi ; xi Þ = P ðyi = kjxi Þ =

1
1 + expðα fk ðxi Þ + βÞ

ð17Þ

where fk(xi) is the SVM decision function for class k, α and β are the
parameters determined from the training data. P(yi = k|xi) is the
likelihood that the label yi is labeled as class k given the MR intensity
xi. When applying SVM to MR image segmentation, the overlapping
of MR signals is a critical issue. Adding the three coordinates (x, y and
z coordinates of each data point in a 3D MR image) as features into
SVM classiﬁer helps in classiﬁcation by relieving the class overlapping problem of MR intensities between different structures (Bae
et al., 2008). In one of our preliminary experiments, we found that
using two MR protocols (T2-weighted and proton density-weighted)
along with coordinate features yield better segmentation performance than using all the ﬁve MR protocols1. Our preliminary result
also indicates that adding more MR contrasts does not aid the classiﬁer because it increases the overlapping among MR signals and
makes the dataset noisier. Thus, the intensity feature vector X forms a
ﬁve dimensional vector, consisting of the two MR intensities (T2-

1
Using the two indices VOP (the larger the better), VDP (the smaller the better)
introduced in the next section, the experiment on two MR protocols with coordinates
yields 72.42% VOP and 19.21% VDP in average. The experiment on all ﬁve MR protocols
with coordinates generates 59.38% VOP and 24.16% VDP.

M.H. Bae et al. / NeuroImage 46 (2009) 717–725

weighted and proton density (PD) weighted acquisition) and the
three coordinate features.
The second term Bi(yi, li) in Eq. (16), is the location potential
function. Fischl et al. (2002) point out that the number of possible
neuroanatomical structures at a given location in a brain atlas becomes small as registration becomes accurate. Therefore, the location
of a voxel in a 3D image after registration is critical for the classiﬁcation of voxels the neuroanatomical classes. The location potential
function is computed as follows:
Bi ðyi ; li Þ = P ðyi = kjli = r Þ =

of voxels labeled as k at location r
ð18Þ
of voxels at location r

where P(yi = k|li = r) is the probability that a voxel's label yi is
predicted as class k given that the location li of the voxel is r. The
denominator in Eq. (18) is equal to the number of mice used in the
training set. This location information is similar to the a priori
probability used in human brain segmentation study (Ashburner and
Friston, 1997) as implemented in SPM.
The third term Vi(yi,yNi) in Eq. (16) is the contextual potential
function which models the contextual information using MRF. Based
on the MRF theory, the prior probability of having a label at a given site
i is determined by the label conﬁguration of the neighborhood of the
site i. The contextual potential function for site i will have a higher
value as the number of neighbors that have the same label increases. It
is deﬁned as

X 
δ yi ; yj





1 if yi = yj
jaNi
where δ yi ; yj =
Vi yi ; yNi =
ð19Þ
0 if yi ≠ yj
nðNi Þ
where n(Ni) is the number of voxels in a neighborhood of site i. We
use a ﬁrst order neighborhood system as a clique, which consists of
the adjacent six voxels in the four cardinal directions in a plane and
the front and back directions through the plane.
In fact, other than observation and contextual potential function,
eMRF introduces location potential function to the problem. Noticing
that each potential function can independently classify a voxel, eMRF
model integrates them together and assigns weight optimally to each of
them based on their classiﬁcation performance from the training set.
The weight parameter indicates the contributions from the three
different potential functions to predict the labeling. The optimal weight
parameter is determined by grid search on several possible sets of w1, w2
and w3 using cross validation. The Y value that maximizes the posterior
probability P(Y|X,L) corresponds to the most likely label given the
information of X and L. This is known as the maximum a posterior
(MAP) solution which is well known in the machine vision literatures
(Li, 2001). Finding the optimal solution of the joint probability of a MRF
Y 4 = maxY P ðY jX; LÞ is very difﬁcult because of the complexity caused
by the interaction among multiple labels. A local search method called
iterated conditional modes (ICM) proposed by Besag (1986) is used in
this study to locate the optimal solutions. The ICM algorithm is to
maximize local conditional probabilities sequentially by using the
greedy search in the iterative local maximization. It is expressed as
4

yi = arg max P ðyi jxi ; li Þ:
yi aY

ð20Þ

Given the data xi and li, the algorithm sequentially updates y(t)
i into
+ 1)
y(t
by switching different labels to maximizing P(yi|xi, li) for every
i
site i in turn. The algorithm terminates when no more labels are
changed. In the ICM algorithm, how to set the initial estimator y
(0)
is very important. We use the MAP solution based on only the
location information as the initial estimator of the ICM algorithm, i.e.,
ð0Þ

yi

= arg max P ðyi jli Þ:
yi aY

ð21Þ

721

Performance measurements
The VOP and VDP are used to measure the performance of the
proposed automated segmentation procedure (Fischl et al., 2002; Ali
et al., 2005, Hsu and Lin, 2002). The VOP and VDP are calculated by
comparing the automated segmentation with the true voxel labeling
(from the manual segmentation). Denote LA and LM as labeling of the
structure i by automated segmentation and manual segmentation
respectively, and V(L) as the volume of the labeling. The volume
overlap percentage for class i is deﬁned as
VOPi ðLA ; LM Þ =

V ðLA \ LM Þ
× 100k:
ðV ðLA Þ + V ðLM ÞÞ = 2

This performance index is the larger the better. When all the
labels from the automated and manual segmentation coincide,
VOPi(LA, LM) is 100%. VOP is very sensitive to the spatial difference
of the two labelings, because a slight difference in spatial location of
the two labelings can cause signiﬁcant decreases in the numerator of
VOPi(LA, LM).
The VDP for class i is used for quantifying the volume difference
of the structures delineated by the two segmentations, and it is
deﬁned as
VDPi ðLA ; LM Þ =

jV ðLA Þ − V ðLM Þj
× 100k:
ðV ðLA Þ + V ðLM ÞÞ = 2

This performance index is smaller the better. When all the
labelings from the two segmentations are identical, VDPi(LA, LM) is 0.
Results
Segmentation and model validation
The eMRF algorithm was implemented and tested on a group of
90 μm isotropic resolution MR images of ﬁve C57/BL6 mice. To assure
the validity of all results from this experiment, a ﬁve-fold cross
validation was used in every steps of the experiment. Each mouse was
used as a test brain, while the remaining four mice were used as the
training set. Hence we tested the algorithm on ﬁve distinct training and
testing sets. To calculate the observation potential function in Eq. (17),
we built ﬁve SVM models for the ﬁve different training sets using the
MRS-SVM procedure (Bae et al., 2008). Building a SVM model for mouse
brain segmentation is challenging because the number of the classes is
large (N20) and the classes are highly imbalanced. The MRS-SVM
procedure enabled us to build SVM model for the brain segmentation
efﬁciently and effectively. Testing each SVM model yielded the posterior
probability P(Y|X) in Eq. (17) for each tested mouse. The location
potential function was calculated for the ﬁve mouse based on Eq. (18).
After the observation and location potential functions were calculated for
each mouse, the ICM algorithm was used to provide the contextual
potential function in Eq. (19) and the posterior probability P(Y|X,L) in
Eq. (16). To ﬁnd the best weight for each potential function we
conducted a grid search over the range of each wi = {0.1, 0.2,…, 0.9}
(i = 1, 2, 3), where w1 +w2 +w3 = 1. The best model parameters
determined were w1 = 0.1, w2 = 0.6 and w3 = 0.3 for observation, location and contextual functions, respectively.
We also implemented the atlas-based segmentation for the same
mouse brain images to compare its performance with the eMRF
method. One mouse brain image was chosen out of the ﬁve mouse
brains as a reference brain. First, the rest four testing images were
realigned to the reference brain using a 12-parameter afﬁne registration which adjusts the global position and size differences between
the individual brain images. Next, the reference brain was mapped to
the testing brain images using a nonlinear registration which deforms
each neuroanatomical structure into a common space. The registrations were done using Image Registration Toolkit (ITK) software and

722

M.H. Bae et al. / NeuroImage 46 (2009) 717–725

the registration parameters were optimized by the software. Each
image of the ﬁve mouse brain was used as the reference brain, while
the remaining four images were used as the testing brains. Therefore,
each brain image was segmented four times with different reference
brain each time.
Table 2 presents the segmentation performance based on two
metrics: VOP and VDP between the automated and manual labels.
These results are based on ﬁve mouse brains, and with the use of ﬁvefold cross validation. The results for the eMRF method are compared
with three other segmentation methods — the aforementioned atlasbased segmentation method, the MRS-SVM method (Bae et al., 2008)
and the MRF method (Ali et al., 2005). Overall eMRF outperforms all
the other three segmentation methods. Compared to MRS-SVM the
average VOP of eMRF is improved by 8.68%, 10.05% compared to the
atlas-based segmentation, and 2.79% compared to MRF. Corresponding to an increase in the average VOP, the average VOD of eMRF is
decreased. It is improved by 42.04% compared to MRS-SVM, 23.84%
compared to the atlas-based segmentation, and 12.71% compared to
MRF. Fig. 2 illustrates the VOP (top) and VDP (bottom) of the four
different segmentation methods.
The eMRF method outperforms the MRS-SVM method in most
structures. The improvement in segmentation performance as
assessed by VOP is greatest for white matter structures like the
anterior commissure (99.29%), and cerebral peduncle (15.55%) but
also for ventricles (35.91%). Exceptions where eMRF underperforms to
MRS-SVM are three large structures: CORT, CBLM and OLFB. The MRSSVM method performs better for large structures because the
classiﬁer tends to generate the separation hyperplane between a
large class and a small class such that it is biased towards the large
class. Therefore, a voxel that is located at the boundary between the
large class and the small class is more likely predicted as the large
class and this reduces the misclassiﬁcation of a large class voxel to a

small class voxel. However, the eMRF method balances the contributions from the MR intensity information, the location information and
the contextual information by assigning potential functions with
different weights. In our study, the weight assigned to observation
potential function (w1) is smaller than the other two. The averages of
the VOP and the VDP of the eMRF method are improved by 8.68%, and
42.04% respectively, compared to the MRS-SVM method, which indicates that the overall segmentation performance has been improved by
the use of eMRF.
The eMRF method provides better performances in most structures
than the atlas-based segmentation method. Based on the VOP, the
eMRF method outperforms the atlas-based segmentation method in
all the structures except the two structures, AC and INTP. The
performance differences in these two structures are only 0.45% in
AC and 5.49% in INTP. The eMRF method has a better VDP performance
than the atlas-based segmentation method in 17 structures. There are
four small structures (AC, VEN, INTP and CC) that the atlas-based
segmentation method has better VDP performance than eMRF. Since
each labeling of the voxels in the reference brain is mapped to the
testing brain in the atlas-based segmentation process, the number of
labeling for a structure in the reference brain is well maintained in the
number of labeling for the structure in the testing brain. Therefore, the
VDP values of the atlas-based segmentation method in all the
structures are quite consistent (11.14–15.23). However, this method
has the worst VOP performance among the four different segmentation methods, indicating that the accuracy of atlas-based segmentation is poor. The eMRF method can improve the averages of the VOP
and the VDP by 10.05% and 23.84%, respectively, which shows that
overall the eMRF method outperforms the atlas-based segmentation
method.
Based on the voxel overlap metric, the eMRF method shows
better performance in 13 out of 21 structures compared to the MRF

Table 2
Accuracy of the four segmentation methods (eMRF, MRF-SVM, atlas-based segmentation and MRF) for each of the 21 segmented brain structures is evaluated using the VOP (top row)
for each segmentation and VDP (second row) metrics.
CORT
eMRF
MRS-SVM
Improvement
Atlas-based segmentation
Improvement
MRF
Improvement

eMRF
MRS-SVM
Improvement
Atlas-based segmentation
Improvement
MRF
Improvement

VOP (%)
VDP (%)
VOP (%)
VDP (%)
VOP (%)
VDP (%)
VOP (%)
VDP (%)
VOP (%)
VDP (%)
VOP (%)
VDP (%)
VOP (%)
VDP (%)

VOP (%)
VDP (%)
VOP (%)
VDP (%)
VOP (%)
VDP (%)
VOP (%)
VDP (%)
VOP (%)
VDP (%)
VOP (%)
VDP (%)
VOP (%)
VDP (%)

91.10
3.34
93.46
1.63
− 2.53
104.24
84.19
12.24
8.20
72.74
90.77
4.35
0.36
23.24

CPED
73.15
8.32
63.31
11.50
15.55
− 27.60
64.31
11.76
13.75
29.20
67.69
12.17
8.06
31.62

HC
86.20
5.56
83.92
5.87
2.71
− 5.33
84.07
11.91
2.54
53.34
87.69
6.96
− 1.70
20.08

AC

CBLM

VEN

50.76
25.55
25.47
68.49
99.29
62.70
50.99
11.51
− 0.45
− 121.98
55.38
19.13
− 8.35
− 33.54

92.68
3.73
96.29
1.48
− 3.75
− 151.51
85.91
12.49
7.89
70.13
93.08
3.48
− 0.42
− 7.22

71.83
16.83
52.85
24.92
35.91
32.48
67.30
12.40
6.73
− 35.70
70.77
19.13
1.50
12.04

CPU

GP

ICAP

PAG

INFC

MED

THAL

MIDB

87.62
3.98
85.72
3.89
2.22
2.16
82.46
11.54
6.26
65.53
88.46
6.96
− 0.95
42.84

79.64
7.54
74.47
11.34
6.95
− 33.51
68.19
13.51
16.79
44.16
78.46
7.83
1.50
3.61

73.14
11.05
63.91
12.66
14.44
− 12.78
60.83
11.40
20.25
3.13
73.85
13.04
− 0.96
15.31

90.26
3.55
80.90
7.97
11.57
− 55.44
84.47
11.14
6.85
68.11
85.38
2.61
5.71
− 36.19

85.32
5.26
83.21
7.81
2.54
− 32.64
75.67
14.87
12.76
64.60
83.08
4.35
2.70
− 21.05

91.93
7.12
90.27
10.03
1.84
− 28.98
84.24
12.50
9.12
43.05
86.15
10.43
6.70
31.76

93.53
2.48
92.07
2.78
1.59
− 10.61
91.16
11.57
2.60
78.54
93.08
3.48
0.49
28.58

93.27
2.97
90.48
3.84
3.09
− 22.57
88.56
12.47
5.32
76.18
90.77
4.35
2.76
31.68

PON

SNR

INTP

OLFB

OPT

TRI

CC

Avg.

71.61
26.09
58.18
62.22
23.08
58.06
75.77
15.23
− 5.49
− 71.32
76.92
13.91
− 6.91
− 87.55

82.50
10.79
91.23
7.85
− 9.57
− 37.49
82.19
12.40
0.38
12.98
84.62
16.52
− 2.50
34.68

80.03
12.49
74.38
14.30
7.60
12.65
70.68
14.55
13.24
14.17
73.08
17.39
9.52
28.18

78.98
10.16
60.79
25.65
29.91
60.39
73.18
12.93
7.92
21.41
68.46
15.65
15.36
35.09

68.67
5.67
59.25
19.93
15.90
71.58
44.74
12.05
53.49
52.99
53.08
14.78
29.39
61.68

73.83
7.33
69.72
17.27
5.89
57.57
59.72
12.36
23.62
40.71
63.85
10.43
15.63
29.77

65.75
20.57
57.65
24.28
14.04
15.26
49.55
12.28
32.69
− 67.49
71.54
22.61
− 8.10
9.00

80.09
9.54
73.69
16.46
8.68
42.04
72.77
12.53
10.05
23.84
77.91
10.93
2.79
12.71

The percentage improvement in segmentation accuracy is computed for eMRF versus MRS-SVM, eMRF versus atlas-based segmentation, and eMRF versus MRF. The improvements in
VOP and VOD are calculated separately. A ‘+’ sign always means that eMRF method outperforms the other methods for the speciﬁc neuroanatomical structure and ‘−’ means that
eMRF underperforms.

M.H. Bae et al. / NeuroImage 46 (2009) 717–725

723

Fig. 2. Comparison of the relative performances of the eMRF, MRS-SVM, atlas-based segmentation and MRF methods based on the voxel overlap percent — VOP (top) and the volume
difference percent — VDP (bottom) indices.

Fig. 3. Coronal slices through the labeled brain at the level of anterior hippocampus and third ventricle (upper row), and pons and substantia nigra (lower row) show in a qualitative
manner the relative superiority of eMRF compared to MRS-SVM. Note that eMRF segmentation better preserved the shapes of striatum and corpus callosum (as seen in the manual
labels), compared to MRS-SVM; and also that eMRF was able to segment a small CSF ﬁlled region in the center of PAG, while MRS-SVM missed it.

724

M.H. Bae et al. / NeuroImage 46 (2009) 717–725

method. The eMRF method provides more than 5% performance
improvement in 7 structures (CPED, PAG, MED, PON, SNR, OPT and
TRI), the largest improvements are seen for optic tract (29.39%) and
trigeminal tract (15.63%). Based on the volume difference metric, the
eMRF method shows better performance in 16 structures out of 21
structures. VDP improvement ranged from 61.68% for optic tract and
to 3.61% for globus pallidus. In four small structures (PAG, INFC, AC
and INTP), which take less than 1% volume compared to the whole
brain volume, the MRF method shows better performance. This is
due to the fact that in the MRF method labeling a voxel depends on
the labelings of the neighborhood of the voxel, thus the identiﬁcation of small structures is enhanced. Nevertheless, the averages of
the VOP and the VDP of the eMRF method are improved by 2.79%,
and 12.71% respectively, compared to the MRF method. The eMRF
method can make a balance between the overﬁtting of the SVM
method to the larger classes and the overﬁtting of the MRF method
to the smaller classes so that the overall segmentation performance
is improved.
A visual comparison of the three segmentation methods, manual
labeling (the gold standard), eMRF and MRS-SVM, is shown for two
speciﬁc coronal levels (one at the level close to the hippocampal
commissure, and one at the level of the pons) in Fig. 3. On the left
column of Fig. 3 are displayed the manual segmentations (used as the
gold standard), on the middle column are the automated segmented
images by the eMRF method and the right column are those
produced by the MRS-SVM method. The eMRF method seems to
deviate less than the MRS-SVM method from the manual labeling,
and to respect better the topology of the structures. The testing time
of the MRS-SVM algorithm for a mouse brain dataset (472,100 voxels)
was 289.4 min with a 3.4-GHz PC. The testing experiment was run
using MATLAB, and executed the eMRF segmentation of one mouse
brain in 75 min.
Discussion and conclusion
In this paper, we developed an eMRF method, which ﬁrst adds a
potential function based on location information, then integrates the
SVM and MRF for a more accurate and robust segmentation of mouse
brain MR images by taking the advantages of the two methods. MRF
has been used in the human brain image segmentation task because it
utilizes the contextual information of a voxel, which describes the
correlation of the voxel and its neighbors for segmentation (Fischl
et al., 2002). A similar algorithm has been successfully implemented
for mouse brains (Ali et al., 2005). Other than the contextual
information and MR intensity information, Bae et al. (2008) recently
studied the use of SVM based on MR intensity information and
location information for mouse brain segmentation, and a novel
automated segmentation method, termed MRS-SVM, was proposed.
Experimental results indicate that the MRF method outperforms the
MRS-SVM method for smaller structures while the MRS-SVM method
outperforms the MRF method for larger structures. The complementary nature of the two methods directs the development of eMRF.
Speciﬁcally, other than using Gaussian probability distribution to
model the MR intensity signal, the eMRF method employs the
posterior probability distribution obtained from the SVM to generate
classiﬁcation based on the MR intensity information. Secondly, eMRF
introduces a new potential function based on the location information. Instead of considering the contributions from the three potential
functions – observation, location and contextual functions – equally,
eMRF further applies ICM to optimally determine the contribution
weights for each function.
To validate the proposed method, we conducted a comparison of
the four different algorithms: MRF (Ali et al., 2005), MRS-SVM (Bae
et al., 2008), atlas-based segmentation and eMRF for the automate
segmentation of mouse brain into 21 structures, using the same
dataset. Our test results show that the overall performance of the

eMRF method is better than all the other three segmentation
methods in terms of both the average VOP and average VDP, even
though the MRS-SVM method is slightly better on a small number of
large structures, and the atlas-based segmentation and MRF
methods are slightly better on a few small structures.
In the future, we will extend and adapt the eMRF method to human
brain segmentation and compare the results with the existing study in
literature (Powell et al., 2008). Further developments of this work
would also include obtaining higher resolution images and more
labels of neuroanatomical structures, which could provide more
information to be incorporated in atlases of normal mouse brains.
Moreover, we foresee that the automated segmentation method
described in the paper will accelerate the study of brain images of
large quantity, thus help developing small animal disease models for
many neurodegenerating disorders, such as Alzheimer disease and
Parkinson disease.
Acknowledgments
The authors would like to thank the Duke Center for In Vivo
Microscopy, an NCRR/NCI National Biomedical Technology Research
Resource (P41 RR005959/U24 CA092656), for providing the images.
They would also like to thank the Mouse Bioinformatics Research
Network (MBIRN) (U24 RR021760) for providing support for this
imaging study.
References
Ali, A.A., Dale, A.M., Badea, A., Johnson, G.A., 2005. Automated segmentation of
neuroanatomical structures in multispectral MR microscopy of the mouse brain.
NeuroImage 27 (2), 425–435.
Amato, U., Larobina, M., Antoniadis, A., Alfano, B., 2003. Segmentation of magnetic
resonance images through discriminant analysis. J. Neurosci. Methods 131, 65–74.
Andersen, A.H., Zhang, Z., Avison, M.J., Gash, D.M., 2002. Automated segmentation of
multispectral brain MR images. J. Neurosci. Methods 122, 13–23.
Ashburner, J., Friston, K., 1997. Multimodal image coregistration and partitioning — a
uniﬁed framework. NeuroImage 6, 209–217.
Bae, M.H., Wu, T., Pan, R., 2008. Mix-ratio sampling: classifying multiclass imbalanced
mouse brain images using support vector machine. Technical Report available at
http://swag.eas.asu.edu/vcie/.
Ballester, M.A., Zisserman, A., Brady, M., 2000. Segmentation and measurement of brain
structures in MRI including conﬁdence bounds. Medical Image Analysis 4, 189–200.
Besag, J., 1986. On the statistical analysis of dirty pictures. J. R. Stat. Soc., Ser. B 48 (3),
259–302.
Bock, N.A., Kovacevic, N., Lipina, T.V., Roder, J.C., Ackerman, S.L., Henkelman, R.M., 2006.
In vivo magnetic resonance imaging and semiautomated image analysis extend the
brain phenotype for cdf/cdf mice. J. Neuroscience 26 (17), 4455–4459.
Duta, N., Sonka, M., 1998. Segmentation and interpretation of MR brain images: an
improved active shape model. IEEE Trans. Med. Imag. 16, 1049–1062.
Fischl, B., Salat, D.H., Busa, E., Albert, M., Dieterich, M., Haselgrove, C., Van der Kouwe, A.,
Killiany, R., Kennedy, D., Klaveness, S., et al., 2002. Whole brain segmentation:
automated labeling of neuroanatomical structures in the human brain. Neuron 33,
341–355.
Geman, S., Geman, D., 1984. Stochastic relaxation, Gibbs distributions and the Bayesian
restoration of images. IEEE Trans. Pattern Anal. Mach. Intell. 6, 721–741.
Heckemann, R.A., Hajnal, J.V., Aljabar, P., Rueckert, D., Hammers, A., 2006. Automatic
anatomical brain MRI segmentation combining label propagation and decision
fusion. NeuroImage 33, 115–126.
Hsu, C.W., Lin, C.J., 2002. A comparison of methods for multi-class support vector
machines. IEEE Trans. Neural Netw. 13 (2), 415–425.
Kovacevic, N., Lobaugh, N.J., Bronskill, M.J., Levine, B., Feinstein, A., Black, S.E., 2002. A
robust method for extraction and automatic segmentation of brain images.
NeuroImage 17, 1087–1100.
Kovacevic, N., Henderson, J.T., Chan, E., Lifshitz, N., Bishop, J., Evans, A.C., Henkelman,
R.M., Chen, X.J., 2005. A three-dimensional MRI atlas of the mouse brain with
estimates of the average and variability. Cereb. Cortex 15 (5), 639–645.
Lee, C.H., Schmidt, M., Murtha, A., Bistritz, A., Sander, J., Greiner, R., 2005. Segmenting
brain tumors with conditional random ﬁelds and support vector machines. Lect.
Notes Comput. Sci. 3765, 469–478.
Li, S.Z., 2001. Markov Random Field Modeling in Image Analysis. Springer-Verlag, Tokyo.
Ma, Y., Hof, P.R., Grant, S.C., Blackband, S.J., Bennett, R., Slatest, L., Mcguigan, M.D.,
Benveniste, H., 2005. A three-dimensional digital atlas database of the adult
C57BL/6J mouse brain by magnetic resonance microscopy. Neuroscience 135 (4),
1203–1215.
Mazoyer, N., Landeau, B., Papathanassiou, D., Crivello, F., Etard, O., Delcroix, N.,
Mazoyer, B., Joliot, M., 2002. Automated anatomical labeling of activations in SPM
using a macroscopic anatomical parcellation of the MNI MRI single-subject brain.
NeuroImage 15, 273–289.

M.H. Bae et al. / NeuroImage 46 (2009) 717–725
Platt, J., 2000. Probabilistic Outputs for Support Vector Machines and Comparison to
Regularized Likelihood Methods. Advances in Large Margin Classiﬁers. MIT Press,
Cambridge, MA.
Powell, S., Magnotta, V.A., Johnson, H., Jammalamadaka, V.K., Pierson, R., Andreasen,
N.C., 2008. Registration and machine learning-based automated segmentation of
subcortical and cerebellar brain structures. NeuroImage 39, 238–247.
Reddick, W.E., Glass, J.O., Cook, E.N., Elkin, T.D., Deaton, R.J., 1997. Automated
segmentation and classiﬁcation of multispectral magnetic resonance images of
brain using artiﬁcial neural networks. IEEE Trans. Med. Imag. 16, 911–918.

725

Suri, J., 2001. Two-dimensional fast Magnetic Resonance brain segmentation. IEEE Eng.
Med. Biol. 84–95 July/August.
Vapnik, V.N., 1995. The Nature of Statistical Learning Theory. Springer Verlag.
Zavaljevski, A., Dhawan, A.P., Gaskil, M., Ball, W., Johnson, J.D., 2000. Multi-level
adaptive segmentation of multi-parameter MR brain images. Comput. Med.
Imaging and Graph. 24, 87–98.
Zhang, Y., Smith, S., Brady, M., 2001. Segmentation of brain MR images through a hidden
Markov random ﬁeld model and the expectation–maximization algorithm. IEEE
Trans. Med. Imag. 20, 45–57.

TAGA: Travel Market Framework in Agentcities
Youyong Zou, Tim Finin, Li Ding, Harry Chen, Rong Pan
Computer Science and Electrical Engineering
University of Maryland, Baltimore County

Travel Agent Game in Agentcities (TAGA) is a framework
that extends and enhances the Trading Agent Competition
(TAC) system to work in Agentcities, an open multiagent
systems environment of FIPA compliant systems. TAGA
makes several contributions: auction services are added to
enrich the Agentcities environment, the use of the semantic web languages RDF and DAML+OIL improve the
interoperability among agents, and the DAML-S ontology
is employed to support service registration, discovery and
invocation. The FIPA and Agentcities standards for agent
communication, infrastructure and services provide an
important foundation in building this distributed and open
market framework. TAGA is intended as a platform for
research in multi-agent systems, the semantic web and/or
automated trading in dynamic markets as well as a selfcontained application for teaching and experimentation
with these technologies. It is running as a continuous open
game at http://taga.umbc.edu/ and source code is available
for research and teaching purposes.

1 Introduction
The Trading Agent Competition (TAC) [7] is a test bed
for intelligent software agents that interact through
simultaneous auctions to obtain services for customers.
TAC trading agents operate within a travel market scenario, buying and selling goods to best serve their given
travel clients. TAC is designed to promote and encourage
research in markets involving autonomous trading agents
and has proven to be successful after three consecutive
year's competition.
Although TAC's framework, infrastructure and game
rules have evolved over the past three competitions, the
assumptions and approach of TAC limit its usefulness as
a realistic test bed for agent based automated commerce.
TAC has used centralized auctions as the sole mechanism
for service discovery, communication, coordination,
commitment, and control among the participating software agents. The agents communicate with the central
market servers through simple socket interfaces, exchanging pre-defined message. The abstractness and
simplicity of the TAC approach helped to launch it as a
research vehicle for studying bidding strategies, but are
now perceived as a limiting factor for exploring the wide
range of issues inherent in automated trading in open
environments.
Agentcities [4] is an international initiative designed to
explore the commercial and research potential of agentbased applications by constructing an open distributed
network of platforms hosting diverse agents and services.
The ultimate goal is to enable the dynamic, intelligent
and autonomous composition of services to achieve user
and business tasks, thereby creating compound services
to address changing needs.
INTELLIGENT SYSTEMS DEMONSTRATIONS

Inspired by TAC, we developed Travel Agent Game in
Agentcities (TAGA) built on the foundation of FIPA
technology and the Agentcities infrastructure. The agents
and services use standard FIPA supported languages,
protocols and services to create the travel market environment providing TAGA with a stable communication
environment in which messages in expressive semantic
languages can be exchanged. . The travel market is the
combination of auctions and varying markets including
service registries, service brokerage, wholesalers, peerto-peer transactions, bilateral negotiation, etc. This
provides a much richer test bed for experimenting with
agents and web services as well as a rich and interesting
scenario to test and challenge agent technology.

2. TAGA Game and Agents
We have designed TAGA as a general framework for
running agent-based simulations and games. Our first
use of TAGA has been to build a travel competition
along the lines of that used in the first three TACs. In
this competition, customers travel from City A to City B
and spend several days there. A travel package includes a
round-trip flight ticket, corresponding hotel accommodation and ticket to entertainment events. A travel agent (an
entrant to the game) competes with other travel agents in
making contracts with customers and purchasing the
limited travel services from the Travel Service Agents.
Customer selects the travel agent with best travel itinerary. The objective of the travel agent is to acquire more
customers, fulfill the customer's travel package, and
maximize the profit.
TAGA provides a flexible framework to run the travel
market game. Figure 1 show the structure of TAGA. The
collaboration and competition among six kinds of agents
that play different roles in this market, simulating the real
world travel market. We have found that basing our
implementation on FIPA compliant agents has made the
framework extremely flexible. We'll briefly describe the
different agents in our initial TAGA game.
The Auction Service Agent (ASA) operates all of the auctions
and markets in TAGA. Market types currently include English
and Dutch auctions as well as other dynamic markets similar to
Priceline and Ebay's fastbuy.
A Service Agent (SA) offers travel related service units such
as airline tickets, lodging and entertainment tickets. Each class
of travel related service has multiple providers with different
service quality level and with limited service units;
A Travel Agent (TA) is a business that helps customers acquire travel service units and organizes travel plan. The units
can be bought either directly from the service agents, or
through an auction server.
A Bulletin Board Agent (BBA) provides a mechanism
through which helps customer agents can find and engage one
or more travel agents.
1645

A Customer Agent (CA) represents an individual customer
who has particular travel constraints and preferences. Its goal is
to engage one or more TAs, negotiate with them over travel
packages, and select one to try to purchase.
The Market Oversight Agent monitors the simulation and
updates the financial model after each reported transaction and
finally announces the winning TA when the game is over.

We see two contributions in our work. First, TAGA
provides a rich framework for exploring agent-based
approaches to ecommerce like applications. Our current
framework allows users to create their own agent (perhaps based on our initial prototype) to represent a TA,
SA and to include it in a running game where it will
compete with other system provided and user defined
agents. We hope that this might be a useful teaching and
learning tool. Secondly, we hope that TAGA will be
seen as a flexible, interesting and rich environment for
simulating agent-based trading in dynamic markets.
Agents can be instantiated to represent customers, aggregators, wholesalers, and service provides all of which can
make decisions about price and purchase strategies based
on complex strategies and market conditions.

4. Conclusion and future work
Figure 1: TAGA Architecture
The basic cycle of the game is as follows. A customergenerating agent creates a new with particular travel
constraints and preferences chosen from a distribution.
The CA registers with the BBA, which facilitates contact
with a set of TAs, each of which must decide whether to
propose a travel package for the CA. Those that do,
contact the necessary ASAs and SAs and assemble an
itinerary to propose to the CA. Note that a TA is free to
implement a complex strategy using both aggregate
markets (ASAs) as well as direct negotiation with SAs.
The final proposal to a CA includes a set of travel units, a
total price and a penalty to be suffered by the TA if it is
unable to complete the transaction. The CA negotiates
with the TAs ultimately selecting one from which to
purchase an itinerary based on its constraints, preferences
and purchasing strategy (which might, for example,
depend on a TAs reputation). Once a TA has a commitment from a CA, it attempts to purchase the units in the
itinerary from the ASAs and SAs. There are two outcomes possible: the TA acquires the units and completes
the transaction with the CA resulting in a satisfied CA
and a profit or loss for the TA, or the TA is unable or
unwilling to purchase all of the units, resulting in an
aborted transaction and the invocation of the penalty
(which can involve both a monetary and a reputation
component).

3. Discussion
TAC relies on a few centralized market servers to handle
all interactions and coordination, including service
discovery, agent communication, coordination, and game
control. In contrast, TAGA framework uses a distributed
peer-to-peer approach based on standard agent languages,
protocols and infrastructure components (FIPA [8],
Agentcities), emerging standards for representing ontologies, knowledge and services (RDF, DAML+OIL,
DAML-S [1]) and web infrastructure (e.g., Sun's Java
Web Start). Several FIPA platform implementations are
currently used within TAGA, including Jade [2] and APP
[5], demonstrating agent interoperability.
1646

As a role-playing market game, TAGA can be used for
business research on marketing and auction strategy. In
the Agentcities community, TAGA serves as a test-bed
for FIPA agent communication in a distributed and open
environment. For our own research, has allowed us to
explore the integration of multi-agent systems technology
(FIPA) and the semantic web.
The Agentcities project is exploring the delivery and
use of agent-based services in an open, dynamic and
international setting. We are working to increase the
integration of TAGA and emerging Agentcities components and infrastructure and will include agents running
on handheld devices using LEAP [3]. We are also
working to enhance the ontologies which underlie TAGA
and to move them from RDF and DAML+OIL to the
W3C's Web Ontology Language OWL [5].
Acknowledgments
The research described in this paper is partly supported by DARPA
contract F30602-00-2-0591 and Fujitsu Laboratories of America.
References
[1] Ankolenkar et. al., DAML-S: Web Service Description for
the Semantic Web, Proc, lst Int. Semantic Web Conference,
Sardinia, June 2002.
[2] F. Bellifemine, A. Poggi, G. Rimassa, Developing multi
agent systems with a FIPA-compliant agent framework, in
Software - Practice And Experience, 2001 n31, pp. 103-128
[3] F. Bergenti and A. Poggi, LEAP: a FIPA Platform for
Handheld and Mobile Devices, ATAL, 2001.
[4] J. Dale et.al., Agentcities: Challenges and Deployment of
Next-Generation Service Environments. Proc. Pacific Rim
Intelligent Multi-Agent Systems, Tokyo, Japan, August 2002.
[5] J. Dale and L. Ccccaroni, Pizza and a Movie: A Case Study in
Advanced Web Services. In: Agentcities: Chalenges in Open
Agent Environments Workshop, Autonmous Agents and MultiAgent Systems Conference 2002, Bologna, Italy, July 2002.
[6] M. Dean, et. al.. OWL Web Ontology Language 1.0 Reference. W3C Working Draft.
[7] M. Wellman, et. al., The 2001 Trading Agent Competition.
In 14th Conference on Innovative Applications of Artificial
Intelligence, pp. 935-941, Edmonton, 2002.
[8] FIPA agent standards, http://www.fipa.ory/
INTELLIGENT SYSTEMS DEMONSTRATIONS

Rebellion on Sugarscape: Case Studies for Greed and
Grievance Theory of Civil Conflicts Using Agent-Based
Models
Rong Pan
Arizona State University, Tempe, Arizona, United States
rong.pan@asu.edu

Abstract. Public policy making has direct and indirect impacts on social
behaviors. However, using system dynamics model alone to assess these
impacts fails to consider the interactions among social elements, thus may
produce doubtful conclusions. In this study, we examine the political science
theory of greed and grievance in modeling civil conflicts. An agent-based
model is built based on an existing rebellion model in Netlogo. The
modifications and improvements in our model are elaborated. Several case
studies are used to demonstrate the use of our model for investigating emergent
phenomena and implications of governmental policies.
Keywords: agent-based model, civil conflicts, sensitivity analysis.

1 Introduction
In this research we are interested in modeling civil conflicts using the agent-based
computational approach, so as to study the behavior of the populace in a specific
national environment and to investigate the impact of governmental policies. Based
on the greed and grievance theory, a person rebels because 1) he has enormous
grievance towards the central government and 2) he sees the opportunities (e.g.,
economical benefits, political advantages, etc.) of becoming a rebel. Therefore, we
may say that the occurrence of civil conflict is the result of the interaction of populace
within its environment, including the reactions of individuals to their life conditions
and their interactions with neighbors, with the general national status (weak or strong
state) and the authoritarian power of government, etc. Agent-based models (ABMs)
are the natural choice of modeling such complex systems.
The NOEM model, developed by the Air Force Rome Laboratory, is a large-scale
computer simulation model built on system dynamics principles for simulating
nation-state operational environment. Through it we may study the causal relationship
between exogenous factors and nation-state’s social and economical status, as well as
the impact of various policy options. We postulate that the NOEM will provide a
valid nation-state environment for ABM simulation. For example, the indicators for
the economic conditions of a nation, such as GDP, GINI, can be imported from the
NOEM and they are used in ABMs to create a nation’s economy map. The agents in
ABMs will possess certain properties that link them to their living environment. They
J. Salerno et al. (Eds.): SBP 2011, LNCS 6589, pp. 333–340, 2011.
© Springer-Verlag Berlin Heidelberg 2011

334

R. Pan

also follow certain behavior rules and interact with the environment. The NOEM is a
system dynamics (SD) model, where macroscopic difference equations are used to
calculate the change of nation-state variables. The ABM technique takes a different
approach. It models at the microscopic level, modeling individual agent’s (it can be a
person) behavior, and how it interacts with its neighbors and its environment.

2 Theory of Greed and Grievance in Civil Conflicts
In the seminal paper by Collier and Hoeffler (2002), the outbreak of civil war was
explained from two possible views: severe grievance and atypical opportunity (greed)
for building a rebel organization. The grievance model is in line with the motivation
cause of conflict explained in the political science literature. Grievance is fueled by
the polarization of a society, which is caused by poor governance. The opportunity
model explains the outbreak of civil war from an economic feasibility point of view.
An interesting hypothesis proposed in Collier, Hoeffler and Rohner (2008) states that
where civil war is feasible it will occur without reference to motivation. This
“feasibility hypothesis” is substantiated by analyzing past civil war data and finding
that the variables, which are close proxies for feasibility, have powerful consequences
to the risk of a civil war.
Collier and Hoeffler’s theory has stirred a heated debate on the true causes of
conflicts among political scientists, sociologists and economists. Although most
empirical studies conducted at the World Bank support the greed model (see, Ganesan
and Vines, 2004, Bodea and Elbadawi, 2007), many researchers rebutted the notion
that the onset of conflict is purely determined by economic reasons or the motivation
of rebellion is driven by seeking financial opportunity only. Some argued that the
reductionist categories of greed and grievance not only obfuscate other social, ethnic
and religious factors, but also mislead the intervention policy of international
community (Kuran, 1989, Gurr and Moore, 1997, Gurr, 2000, Ballentine and
Nitzschke, 2003, Regan and Norton, 2003). This is specifically demonstrated by
studying conflicts in some African countries (Agbonifo, 2004, and Abdullah, 2006). It
seems that this debate will continue. In our study, we do not intend to validate this
theory, but simply apply it on an artificial society and to observe the interactions of
greed and grievance factors and the social consequence of governmental policy.

3 Agent-Based Models
An agent-based model (ABM) is a simulation environment where agents (e.g.,
simulated people) are defined to possess certain properties and behaviors, and they
can interact with each other and with their surrounding environment. This type of
model is different from other mathematical models, such as system dynamics, in
several ways. First, the ABM is built in a bottom-up fashion. It defines the
microscopic details of basic elements in a society and lets the society grow (or
evolve) through agent-to-agent and agent-to-environment interactions. Second, the
use of ABMs is typically to provide the macroscopic picture of how a society grows
and look for emergent phenomena. This is achieved by examining the statistics of

Rebellion on Sugarscape

335

collections of agents over the simulation time. The connections between micro-level
details and macro-level phenomena can be used to explain cause-and-effects for the
artificial society under study.
Naturally, ABMs have been applied in many social science studies. See Epstein
(1990) for supporting arguments of this computational approach to social science.
Epstein and Axtell (1996) gave a full-fledged description of how to build an artificial
society and examine its emergent phenomena on a sugarscape (an imagined land
which provides sugars for the agents living on it). For the civil conflict study, the
most famous model is the rebellion model, which implements the greed and grievance
theory on modeling agent’s behavior. It has been used to demonstrate certain conflict
phenomena found in the real world and the effects of changing governmental policy
(Epstein et al., 2001, Epstein, 2002, and Cederman and Girardin, 2007). More
recently, a group in Netherlands used ABMs to simulate the war game scenario of
foreign-force evasion on an island undergoing a civil war (van Lieburg et al. 2009,
Borgers et al. 2009).
In our study, we built an artificial land based on the sugarscape model and let
rebels grow on this land and interact with the environment; thus, we call it “rebellion
on sugarscape”. The sugarscape can be viewed as a resource map which is
geospatially bounded. The behavior of an agent depends on the agent’s living
environment (whether there are enough resources to support its living and whether
there are rebels/cops in its neighborhood) and the policy set imposed on this artificial
land. Our intention is to use this example to demonstrate the feasibility of the
marriage of civil conflict theory and a resource map, which has not been done in
the original rebellion model. In addition, we studied the behavior of rebellion and the
consequences of policy setting when the resource is pre-defined.
3.1 Rebellion on Sugarscape Model
Although the rebellion model has been successfully used to explain some phenomena
of civil conflicts, it lacks several aspects that we are interested in. First, the grievance
level of each person is randomly given. In modeling a nation, we would like to link
the grievance level to the person’s economic, social, and political status, or in other
words, its quality of life. Second, the number of cops is fixed. In reality, a government
needs to spend enormous wealth on maintaining its military, police and other civil
governing means. Third, in the current model, all agents can freely, randomly move.
In reality, there may be many restrictions on a person’s movement for economic and
political reasons.
To address these problems, we created a refined model, called “rebellion on
sugarscape”. The sugarscape is a layer of resource overlaid on the artificial land.
Sugars (or resources) constantly grow on this land and can be collected by each agent,
and by the government through taxation, to become personal wealth or the
government’s wealth. However, the distributions of sugars are uneven. There are
areas the sugar level is highest (4) and areas there is no sugar (0). We may interpret
the sugar-rich region as the resource-rich city area and the sugar-poor region as the
resource-depleted rural area, or the sugar-rich region as the region with oil fields and
the sugar-poor region as the region without oil. Initially, agents are randomly
scattered on the land. An agent’s grievance will be linked to the location it resides. If

336

R. Pan

it is in a sugar-rich region, its perceived hardship will be low, and vice versa. The
general population can be defined either mobile or non-mobile. If it is mobile, a
person will move to a place having at least a sugar level of 1 within its vision. This is
based on the assumption that people want to seek a better life for themselves. Every
person will collect wealth from the place it resides. The wealth it can collect is the
same as the sugar level at its location. Part of the wealth will be taxed, and the
government will use the tax revenue to either maintain the existing cops, or to create
new cops, or to assist the people who are in poverty. In our model, a cop needs at least
a value of 5-sugar level to survive. To create a new cop, the government needs to
spend 10 sugars. The poverty line is set at 1-sugar level. If a person has less than 1
sugar, it may be assisted by the government. The assisted amount is the total sugar
that the government allocated to the assistance divided by the total number of people
in poverty. Our model is depicted in Figure 1.
Host Nation
Government

No. of cops in
neighborhood

No. of active rebels
in neighborhood

Sugars
Tax rate

Hardship

Legitimacy

Jail term

Arrest probability
P=1-exp{ -k(C/A)}

Environment
Grievance
G=H(1- L)
Individual agent
cop

Agent’s net risk
N=RP
Active if
G-N>T
Arrest active agents
in neighborhood

Risk avoidance
R

(+)
(-)

Fig. 1. The rebellion on sugarscape model

4 Case Studies
In this section, we present several case studies of the rebellion on sugarscape model.
Our purposes are two-folded: 1) to validate the model against basic, intuitive
scenarios; 2) to look for “surprises” that will lead to a better understanding of the
model and of the consequences of policy setting.
4.1 Case Study #1: A Baseline Case
First we test a baseline case, where the number of agents (general population) is 1750
and the initial number of cops is 100. We prohibit the movement of agents (presenting
a non-mobile society), and set both the wealth distributed to the poor and the wealth
for creating new cops to be 0. Therefore, the government will use its wealth to
maintain the initial cop size throughout the simulation. The tax rate is set at 30%.

Rebellion on Sugarscape

337

This baseline case is a simple recast of the rebellion model using a layer that
defines the resources and links the people’s grievance to their wealth. Therefore, we
should expect a similar behavior of rebels as in the previous rebellion simulation,
except that the rebels will be most likely be in poor regions. We ran the simulation
over 200 cycles. We observed that rebellions always happen in the places with less
resource and the outburst of mass rebellion happens from time to time, and then it is
countered by the cops. These periodic rebellion outbursts are similar to what has been
described in the original rebellion model.
4.2 Case Study #2: Building Cops vs. Assisting the Poor
To suppress rebels, there are two basic strategies that the government can use –
increasing the number of cops and providing financial assistance to the poor. In the
following simulations, we tried both strategies and their combinations.
First, we set the Wealth distribution=0 and Wealth to create cops=0.5. This implies
that the government will send half of its wealth to maintain the existing cop force and
half of it to increase cops, and no assistance to the poor. As some cops will die due to
lack of sugars, the number of cops will eventually be stabilized at 158. There are
occasionally one or two rebels, but they will be quickly detained by the cops
surrounding them. Over time, the number of jailed individuals varies around 18 to 20.
Next, we set Wealth to create cops=1.0, so all of the wealth the government
collects will be spent on building new cops. Surprisingly, in the simulation, the cop
size drops to 90 and a constant mass of rebels is created in the poor regions. We
observe that a poor person is likely to choose cycling actions of being dormant, then
actively rebel, and then be jailed. It is because they receive no help from the
government and they cannot improve their lives by moving to rich regions, and in
addition, there are not enough cops to suppress rebellions. The drop of cop force is
surprising, given that the government wants to create more cops. But, in fact, most of
the existing cops will die due to lack of sugar supply from the government, the new
cops built by the government cannot keep up the pace of the dying cops. Increasing
the number of new cops is more expensive, and the new cops are located randomly
(less experience), less effective countering the rebels in the poor regions. Also, with
more people put into jail, less tax revenue will be collected by the government, thus
less funds for the cops. Moreover, from this simulation we observe that the periodic
rebellion outbursts have been replaced by constant rebellions, which indicates the
overall lack of a policing force.
However, when we set Wealth distribution=0.5 (half of the wealth will be
redistributed to the people under the poverty line) and Wealth to create cops=0, we
observe that there is still enough wealth to support 100 cops. In fact, there are very
few incidences of rebellion, and even if there is an active rebel, these will be quickly
detained by the cops. Over the course of the simulation, the number of jailed
individuals varies from 0 to 5. This is a peaceful society. The poor will be assisted by
the government, so their grievance levels cannot be too high. Also, the government
will maintain a sizable force of cops.

338

R. Pan

4.3 Other Cases
Table 1. Summary of the cases being studied and their emerging phenomena
Case

Parameter

Baseline case

As in Section 4.1

Increasing the
number of cops

Assisting the poor
Freedom to move

Less population

High tax

Less education

Less government
legitimacy

Emerging phenomenon

Periodic outburst of rebellion as in the
rebellion model
wealth-distribution=0, wealth- Number of cops increases, but still
to-create-cop=0.5
have rebels from time to time
wealth-distribution=0, wealth- Number of cops decreases, have a
to-create-cop=1
large number of rebels
wealth-distribution=0.5,
Very few rebels, the number of cops
wealth-to-create-cop=0
is maintained at the initial level
movement?=on, wealthPeople move to rich regions, cops
distribution=0, wealth-tomove to poor regions, rebellion
create-cop=0
happens at the boundaries of rich and
poor regions
movement?=on, wealthNo rebel
distribution=0.5, wealth-tocreate-cop=0.2
number of agents=250, initial The percentage of population who
number of cops=15,
rebels increases, the frequency of
movement?=on, wealthrebellion increases
distribution=0
number of agents=250, initial No rebel
number of cops=15,
movement?=on, wealthdistribution=0.5
number of agents=250, initial The cop number increases to 22, but
number of cops=15,
still have rebels in less wealthy
movement?=on, wealthregions
distribution=0, wealth-tocreate-cop=0.5
tax=0.5
Massive rebels in less wealthy regions
even when the government
redistributes some of its wealth to the
poor
vision=3 patches
Some people will be isolated in poor
regions, rebels happen at these poor
regions, government assistance will
help reduce rebels
government-legitimacy=0.35
More rebels than quiet population, cop
number is reduced, the government
collapses. The legitimacy level of 0.75
is the boundary value for maintaining
an effective government.

Rebellion on Sugarscape

339

5 Some Issues for Future Research
In this simple artificial society we simulated, an agent’s grievance is determined by its
perceived hardship and its perceived government legitimacy only. Although we are
able to link the agent’s perceived hardship to its wealth (sugar level), the model is far
away from a real world where people have various motivations for becoming rebels.
Regarding to the motivations of violent non-state actors (VNSA), Bartolomei et al.
(2004) postulated a framework of summarizing grievance from the perspective of
human needs. On the national level, we may identify a group of variables that are
important to the health of the nation, but to use ABMs we have to associate these
macroscopic measurements with microscopic perceptions of individual agents. Little
research has been done on this front so far, but this is a critical link between SD
model and ABM.
Finally the details of how to utilize the NOEM as a nation-state environment for
ABMs still need to be researched. The overall architecture that we envisioned is that
from the NOEM we can create multiple layers of maps, corresponding to the nationals
economic, social and political variables, and use them to define the properties and
behavior rules of agents living on the artificial land. However, to materialize this
concept, in-depth investigations of the NOEM’s capabilities and its potential use for
ABMs are required.

Acknowledgement
The author thanks the AFRL Summer Faculty Fellowship Program for its support of
this research, and thanks Dr. John Salerno for his advising during the author’s stay at
the Air Force Rome Laboratory in the summers of 2009 and 2010.

References
Abdullah, I.: Africans do not live by bread alone: against greed, not grievance. Africa Review
of Books, 12–13 (March 2006)
Agbonifo, J.: Beyond greed and grievance: negotiating political settlement and peace in africa.
Peace, Conflict and Development (4) (2004) ISSN: 1742-0601
Bartolomei, J., Casebeer, W., Thomas, T.: Modeling violent non-state actors: a summary of
concepts and methods. In: Institute for Information Technology Applications (IITA)
Research Publication 4, Information Series, United State Air Force Academy, Colorado
(2004)
Ballentine, K., Nitzschke, H.: Beyond greed and grievance: policy lessons from studies in the
political economy of armed conflict. IPA Policy Report. Program on Economic Agendas in
Civil Wars (EACW), International Peace Academy (2003)
Bodea, C., Elbadawi, I.A.: Riots, coups and civil war: revisiting the greed and grievance debate.
Policy Research Working Paper, 4397. The World Bank, Development Research Group,
Macroeconomics and Growth Team (2007)
Borgers, E., Kwint, M., Petiet, P., Spaans, M.: Training effect based reasoning; developing a
next generation of training simulators. In: Interservice/Industry Training, Simulation, and
Education Conference, I/ITSEC (2009)

340

R. Pan

Cederman, L.-E., Girardin, L.: A roadmap to realistic computational models of civil wars. In:
Takahashi, S., Sallach, D., Rouchier, J. (eds.) Advancing Social Simulation: The First World
Congress. Springer, Japan (2007)
Collier, P., Hoeffler, A.: Greed and grievance in civil war. World Bank (2000)
Collier, P., Hoeffler, A., Rohner, D.: Beyond greed and grievance: feasibility and civil war.
Technical Report, the World Bank (2008)
Epstein, J.M.: Agent-based computational models and generative social science. Complexity 4(5),
41–60 (1999)
Epstein, J.M.: Modeling civil violence: An agent-based computational approach. Proceedings
of National Academy of Science 99 (suppl. 3), 7243–7250 (2002)
Epstein, J.M., Axtell, R.L.: Growing Artificial Societies. The MIT Press, Cambridge (1996)
Kuran, T.: Sparks and prairie fires: a theory of unanticipated political revolution. Public
Choice 61, 41–74 (1989)
Ganesan, A., Vines, A.: Engine of war: resources, greed, and the predatory State. Technical
Report, the World Bank (2004)
Gurr, T.R., Moore, W.H.: Ethnopolitical rebellion: a cross-sectional analysis of the 1980s with
risk assessments for the 1990s. American Journal of Political Science 41(4), 1079–1103
(1997)
Gurr, T.R.: People Versus States: Minorities at Risk in the New Century. Unitied States
Institute of Peace Press, Washington, DC (2000)
van Lieburg, A., Petiet, P.J., Le Grand, N.P.: Improving wargames using complex system
practices. Technical Paper. TNO Defense and Security, The Hague, Netherlands (2007)
von Neumann, J.: Theory of Self-Reproducing Automata. In: Burks, A.W. (ed.), University of
Illinois Press, Urbana (1966)
Regan, P.M., Norton, D.A.: Greed, grievance, and mobilization: the onset of protest, rebellion,
and civil war. Technical Paper. Department of Political Science, Binghamton University
(2003)
Wilensky, U.: Netlogo rebellion model. In: Center for Connected Learning and ComputerBased Modelling, Northwestern University, Evanston (2004)

Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence

Tag-Weighted Topic Model for Mining Semi-Structured Documents
Shuangyin Li, Jiefei Li and Rong Pan∗
Department of Computer Science
Sun Yat-sen University, Guangzhou, China
{lishyin@mail2., lijiefei@mail2., panr@}sysu.edu.cn

Abstract

and celebrity content. Each movie has lots of tags, e.g., director, writers, stars, country, language and so on, and a storyline
as text data. Given a movie with a tag “Dick Martin”, we may
have an opinion that it has a higher probability to be a comedy, without read all its storyline or watch it.
In past decade, topic models have been used to be a powerful method of management of document corpus, such as latent Dirichlet allocation (LDA) [Blei et al., 2003], which assigns topics to corpora and generates topic distributions over
words given corpus. However, as an unsupervised method,
only the words in the documents are modeled in LDA. Thus,
LDA could only treat the tags as word features rather than
a new kind of information for document modeling. Researchers have proposed approaches to deal with tags or labels [Mimno and McCallum, 2008; Ramage et al., 2009;
2011]. However, there are still two main problems. Firstly,
in a document, the importance of the tags can be diﬀerent in
terms of document modeling. For instance, in the domain of
scientiﬁc paper modeling, there are three authors in a speciﬁc
paper, which can be three tags of the paper. In most cases,
the ﬁrst author would make more contribution than the third
author. In fact, diﬀerent authors have diﬀerent background.
Therefore the weights of the authors can aﬀect the formation
of the topic distribution of the document. Secondly, tags in
a corpus are diverse and evolving. For a news story in NYTimes, the tags set contains various types, such as reporters
and category, and both have an impact on topics assignment
with diﬀerent weights. It is clear that how to take advantage
of all the available tags is a challenge when sundry tags are
involved.
In this work, we focus on document collections where documents are tagged. We propose a tag-weighted topic model
(TWTM) to represent the text data and the various tags with
weights to evaluate the importance of the tags. Besides,
TWTM also infers the topic distributions of tags. The weights
of observed tags in document, which we infer from data set,
give us an opportunity to provide a method to rank the tags.
Compared to LDA, TWTM can conduct document modeling
with lower perplexity, and construct more discriminant features for classiﬁcation.
The proposed TWTM has three principal contributions.

In the last decade, latent Dirichlet allocation (LDA)
successfully discovers the statistical distribution of
the topics over a unstructured text corpus. Meanwhile, more and more document data come up
with rich human-provided tag information during
the evolution of the Internet, which called semistructured data. The semi-structured data contain
both unstructured data (e.g., plain text) and metadata, such as papers with authors and web pages
with tags. In general, diﬀerent tags in a document play diﬀerent roles with their own weights.
To model such semi-structured documents is nontrivial. In this paper, we propose a novel method to
model tagged documents by a topic model, called
Tag-Weighted Topic Model (TWTM). TWTM is a
framework that leverages the tags in each document
to infer the topic components for the documents.
This allows not only to learn document-topic distributions, but also to infer the tag-topic distributions for text mining (e.g., classiﬁcation, clustering, and recommendations). Moreover, TWTM automatically infers the probabilistic weights of tags
for each document. We present an eﬃcient variational inference method with an EM algorithm for
estimating the model parameters. The experimental
results show that our TWTM approach outperforms
the baseline algorithms over three corpora in document modeling and text classiﬁcation.

1

Introduction

Large collections of text with kinds of metadata appears in
many Web applications. The documents with both text data
and document metadata (tags, which can be viewed as features of the corresponding document) are called the SemiStructured Data. How to characterize the semi-structured
document data becomes an important issue addressed in many
area, such as information retrieval, artiﬁcial intelligence and
machine learning. The tags can be more important than the
text data in document mining. For example, IMDB, the
world’s most popular and authoritative source for movie, TV
∗

1. It is a novel topic modeling method to model the semistructured data, not only generating the topic distributions over words, but also infering topic distributions of

Corresponding author

2855

tags.

However, predeﬁning the weights of the author and venue information on word assignment limits the usefulness in many
real applications, and cFTM with the strict assumption of either author or venue be chosen for one word topic assignment
lack of scalability when there are various types of contextual
information.

2. TWTM automatically infers a topic distribution for each
individual document directly, with a function of tagweighted topic assignment.
3. Instead of constructing a new model for new tags in the
corpus, TWTM provides a framework to model a variety
of tags, which shows the extensibility of the model.

3

The rest of the paper is organized as follows. In Section 2,
we ﬁrst analyze and discuss related works. In Section 3, we
ﬁrst introduce basic notation and terminology, present a novel
topic model, and give the method of learning and inference.
In Section 4, we present the experimental results on three domains to show the performance of the proposed method in
document modeling and text classiﬁcation. We conclude and
discuss further research directions in Section 5.

2

TWTM Model and Algorithms

In this section, we will mathematically deﬁne the tagweighted topic model (TWTM), and discuss the learning and
inference methods. In this model, just like the latent Dirichlet allocation (LDA) [Blei et al., 2003], we treat the words of
document as arising form a set of latent topics.

3.1

TWTM

We formally deﬁne the following terms. Consider a semistructured corpus, a collection of M documents. We deﬁne
the corpus D = {(w1 , t1 ), . . . , (w M , t M )}, where each 2-tuple
(wd , td ) denotes a document, the bag-of-word representation
wd = (wd1 , . . . , wdN ), td = (t1d , . . . , tLd ) is the document tag vector, each element of which being a binary tag indicator, and L
is the size of the tag set in the corpus D. For the convenience
of the inference in this paper, td is expanded to a ld × L matrix T d , where ld is the number of tags in document d. For
each row number i ∈ {1, . . . , ld } in T d , T i·d is a binary vector,
where T idj = 1 if and only if the i-th tag of the document d1
is the j-th tag of the tag set in the corpus D. In this paper,
we wish to ﬁnd a probabilistic model for the corpus D that
assigns high likelihood to the documents in the corpus and
other documents alike utilizing the given tag information.
In the past topic models, each document d is typically characterized by a distribution over topics, θd , and each topic k
is represented by ψk , over words in a vocabulary. Take LDA
[Blei et al., 2003] for an example, the generative process of
topic distribution of document d is assumed as follows.

Related Works

There are many topic models proposed and shown to be useful on document analyzing, such as in [Petterson et al., 2010;
Hofmann, 1999; Blei et al., 2003; Blei and McAuliﬀe, 2007;
Boyd-Graber and Blei, 2010; Chang and Blei, 2009], which
have been applied to many areas, including document clustering and classiﬁcation [Cai et al., 2008], and information
retrieval [Wei and Croft, 2006]. They are extended to many
other topic models for diﬀerent situation of applications in
analyzing text data [Iwata et al., 2009; Lacoste-Julien et al.,
2008; Zhu et al., 2009]. However, most of these models
only consider the textual information while ignore humanprovided tag information.
Several models have been proposed to take advantage of
tags or labels, such as Labeled LDA [Ramage et al., 2009],
DMR [Mimno and McCallum, 2008] and PLDA [Ramage
et al., 2011], or modeling relationships among several variables, such as Author-Topic Model [Rosen-Zvi et al., 2004].
Labeled LDA [Ramage et al., 2009] get the topic distribution for a document through picking out the several hyperparameter components that correspond to its labels, and draw
the topic components by the new hyperparameter. Thus, it
is hard to infer the topic distribution of labels, that is very
useful for text classiﬁcation in some web applications. And
in Author Topic Model, it obtains the topic distributions of
authors, without gives the importance between the given authors in a document. DMR [Mimno and McCallum, 2008] is
a Dirichlet-multinomial regression topic model that includes
a log-linear prior on document-topic distributions that is a
function of given features of the document. However, there
is a lack of the information of the tag weights in DMR as
well as Author-Topic Model [Rosen-Zvi et al., 2004]. PLDA
[Ramage et al., 2011] provides another way of modeling the
labels’ text data, that assumes the generation of words of topics assignment is limited by only one of the given labels, and
estimates an appropriate size for each per-label topic set automatically using Dirichlet process.TMBP [Deng et al., 2011]
and cFTM [Chen et al., 2012] give methods to make use of
the contextual information of documents to model the topic
assignment. TMBP is a topic model with biased propagation
to leveraging contextual information, the authors and venue.

Choose θd ∼Dirichlet(α),
and choose zni ∼Multinomial(θd ),
where α is the hyperparameter. In LDA, the topic distribution θd is drawn from a hyperparameter, α without considering the tags’ information. However, the human-provided tag
information is useful to impact on the formation of θd .
In this paper, we use ϑd to denote the topic distribution of
document d, controlled by observed tags, as shown in Figure 1. And, let θ represent a L × K topic distribution matrix
over the tag set, where L is the size of the tag set in corpus
D, and K is the number of topics. Let ψ represent a K × V
distribution matrix over words in the dictionary, where V is
the number of words in the dictionary of D.
Similar to LDA, TWTM models the document d as a mixture of underlying topics and generates each word from one
topic. While the topic proportions ϑd of document d in
TWTM is a mixture of tags’ (given/speciﬁed) topic distributions, not restrict to be deﬁned only over a hyperparameter in
LDA. Meanwhile, the novel method we propose to model the
1
Note that we can sort the tags of the document d by the index of
the tag set of the corpus D.

2856

ψ

θ
L×K

which provides an eﬀective and directly method to infer the
weight of tags.

3.2

K ×V

εd
π

ϑd

η

t

z

w
N

L
D

Figure 1: Graphical model representation for TWTM, where
θ is distribution matrix of the whole tags, ψ is distribution
matrix of words. ϑd indicates the topic components for each
document.
topic proportions ϑd is diﬀerent from neither cFTM nor Labeled LDA, or other topic models, such as PLDA and DMR.
The generative process for TWTM is given in the following
procedure:
1. For each topic k ∈ {1, . . . , K}, draw ψk ∼ Dir(β) , where
β is a V dimensional prior vector of ψ.
2. For each tag t ∈ {1, . . . , L}, draw θt ∼ Dir(α), where α is
a K dimensional prior vector of θ.
3. For each document d:
(a)
(b)
(c)
(d)
(e)

Tag-Weighted Topic Assignment

As we expect that all the observed tags in document d make
contributions to infer the topic distribution ϑd of the document, and meanwhile, it is expected that diﬀerent tags works
corresponding to their weights. For example, a blog with
tags of an author, blog’s date, blog category and blog’s url.
Clearly, compared to other tags, the author plays the most
important role in constituting the topic components of this
blog.
The function of how to leverage the tag information or contextual for modeling topic distribution of document is deﬁned
as follows:
ϑ ←− f (T ag1 , · · · , T agl ),
where f (·) is the way of making use of the tag information.
Topic models with tag information or contextual in the past
are take advantage of diﬀerent f (·). In TWTM, we assume
that ϑd is made up by all the observed tags with their own
weight. Figure 1 shows that how TWTM works in probabilistic graphical model. As is shown in Figure 1, ϑd is controlled by two sides, the topic distributions over tags θ, and
the weights of the given tags of document d.
It is important to distinguish TWTM from the AuthorTopic Model [Rosen-Zvi et al., 2004]. In the author-topic
model, words w is chose by the distribution only from one of
the given tags, while in TWTM, for word w, all the observed
tags in the document would make the contributions.
The f (·) in the proposed model is assumed as this, for document d,
f (ϑd ) = (εd )T × T d × θ,
where the linearmultiplication of (εd )T , T d and θ maintains
the condition of kK ϑdk = 1 without normalization of ϑd , since
that εd and θ satisﬁes
ld d
K
i εi = 1, k θlk = 1.

For each l ∈ {1, . . . , L}, draw tld ∼ Bernoulli(ηl ).
Generate T d by td .
Draw εd ∼ Dir(T d × π).
Generate ϑd = (εd )T × (T d × θ) .
For each word wdi :
i. Draw zdi ∼Mult(ϑd ) .
ii. Draw wdi ∼Mult(ψzdi ) .

Firstly, we pick out the topic distributions of the given tags
in document d by T d × θ, for T d is a ld × L matrix and θ is a
L × K matrix. Here we deﬁne

In this process, Dir(·) designates a Dirichlet distribution,
Mult(·) is a multinomial distribution, and π is a L × 1 column
vector, a Dirichlet prior.
Note that εd indicates the weight vector of the observed
tags in constituting the topic proportions of document d. Furthermore, εd is drawn from a Dirichlet prior that is result of
the matrix multiplication of T d ×π. Clearly, the result of T d ×π
will be a (ld × 1) vector of which the dimension is depended
on the number of observed tags in document d.
In Step 3, for one document d, we ﬁrst generate the document’s tags tld using a Bernoulli coin toss with a prior probability ηl , as shown in (a). Then, after draw out the εd , we
generate the ϑd through εd , T d and θ, that will be discussed
in section 2.3. The remaining part of the generative process
is just familiar with the traditional LDA model [Blei et al.,
2003].
As shown above, TWTM assumes a novel way to
model the topic proportions of semi-structured document by
document-special tags and text data. The main key which
discussed in this paper is the tag’s weight topic assignment,

Θd = T d × θ,
where the Θd is a ld × K topic distribution matrix of given tags
in d as sub-components of θ.
Secondly, εd is the weight vector of observed tags in d,
and each dimension of εd represents the weight or importance
associated to the corresponding tag. So, ϑd is mixed by Θd
with corresponding weight values.
d
d
d
ϑd = ( li εdi Θdi1 , . . . , li εdi Θdij , . . . , li εdi ΘdiK ).
With ϑd , TWTM generates all the words of document d
with the assumption of bag-of-words, which is familiar with
the LDA. It is worth to note that the generative process in
PLDA [Ramage et al., 2011] is just diﬀerent from the TWTM
model, where the assumption of topic assignment for d in
PLDA is that the word generated by choosing only one of the
observed tags’ index, while TWTM assumes the topic distribution of d obtained by weighted average of all the observed
tags’ topic distributions.

2857

3.3

Model Learning and Inference

The way of tag-weighted topic assignment leads to diﬃculty in computing the expected log probability of a topic assignment:

In topic models, the key inferential problem that we need to
solve is that of computing the posterior distribution of the hidden variables given a document d. Given a document d, we
can easily get the posterior distribution of the latent variables
in the proposed model, as:
p(εd , z|wd , T d , θ, η, ψ, π) =

p(εd , z, wd , T d |θ, η, ψ, π)
.
p(wd , T d |θ, η, ψ, π)

E[log p(zi |(εd )T × T d × θ)]
K
= k γik E[log((εd )T × T d × θ)k ].
To preserve the lower bound on the log probability, we upper
bound the log normalizer using Jensen’s inequality again:

(1)

E[log((εd )T × T d × θ)k ]

In Eq. (1), integrating over ε and summing out z, we easily
obtain the marginal distribution of d:

 
p(wd , T d |η, θ, ψ, π) = p(td |η) p εd |(T d × π) ·
N K
d
d d
d T
d
d
i=1 zd p(zi |(ε ) × T × θ)p(wi |zi , ψ1:K ) dε .

=E[log
≥E[

However, this posterior distribution of the hidden variables ε
and z is intractable to compute in general. A variety of algorithms have been used to estimate the parameters of topic
models, such as Monte Carlo Markov chain(MCMC) sampling techniques [Andrieu et al., 2003; Griﬃths and Steyvers,
2004], variational methods [Attias, 1999] and others methods [Asuncion et al., 2009; Sato and Nakagawa, 2012]. For
sampling methods, actually, because of the nonconjugacy of
the function of tag-weighted topic assignment, we have to
appeal to a tailored solution of MCMC [Blei and Laﬀerty,
2005]. Such a technique will impede the requirement of convergence properties and speed, especially when the corpus
comprise millions of words. Thus, we make use of meanﬁeld variational EM algorithm [Bishop and Nasrabadi, 2007]
to eﬃciently obtain an approximation of this posterior distribution of the latent variables in the TWTM model. In meanﬁeld variational inference, we minimize the KL divergence
between the variational posterior probability and the true posterior probability through by maximizing the evidence lower
bound (ELBO) L(·) [Blei and McAuliﬀe, 2007]. For a single
document d, we obtain the L(·) using Jensen’s inequality:

ld


εdi log θk(i) ]

i

=

ld


log θk(i) E[εdi ],

i

where the expression of θ(i) , i ∈ {1, · · · , ld }, means the i-th
tag’s topic assignment vector, corresponding to the i-th row
of Θd . Note that the expectation of Dirichlet random variable
d
is E[εdi ] = ξ j / lj ξ j . Thus, for document d,
N
d T
d
i=1 E[log p(zi |(ε ) × T × θ)]
d
d
 
= iN kK γik · lj log θk( j) ξ j / lj ξ j .
Then, we maximize the lower bound L(ξ, γ; η, π, θ, ψ) with
respect to the variational parameters ξ and γ, using a variational expectation-maximization(EM) procedure.
In particular, by computing the derivatives of the L(·) and
setting them equal to zero, we obtain the following update
equations:
d
ξ
γik ∝ ψk,vwi exp{ lj=1 log θk( j) ld j },
(2)
j =1

L(ξ1:ld , γ1:K ; η1:L , π1:L , θ1:L , ψ1:K )
= E[log p(T 1:ld |η1:L )] + E[log p(εd |T d × π)] +
N
d T
d
i=1 E[log p(zi |(ε ) × T × θ)] +
N
i=1 E[log p(wi |zi , ψ1:K )] + H(q),

ξ j

where vwi denotes the index of wi in dictionary.
 
ψk j ∝ dD iN γikd (wd )ij .
 
ξ d td
θlk ∝ dD iN γikd L (ξl dl td ) .
l

where ξ is a ld -dimensional Dirichlet parameter vector and γ
is 1×K vector, which are variational parameters of variational
distribution, and H(q) indicates the entropy of the variational
distribution:

(3)
(4)

l l

The ξ j , j ∈ {1, · · · , ld }, in document d needs to be extended
to tld · ξld , l ∈ {1, · · · , L} for convenient to simplify L[θ] .
ξi can be implemented using gradient descent method, by
taking the derivative of the terms:
l d  L
ld
d


L[ξ] =
i ( l πl T il − 1)(Ψ(ξi ) − Ψ( j ξ j )) +
N K
l d
( j) ξ j
−
j log θk ld
i
k γik ·

H(q) = −E[log q(εd )] − E[log q(z)].
Here the exception is taken with respect to a variational distribution q(εd , z1:N ), and we choose the following fully factorized distribution,
N


εdi θk(i) ]

i

i

q(εd , z1:N |ξ1:L , γ1:K ) = q(εd |ξ)

ld


ld

ld



j

ξ j

log Γ( i ξi ) + i log Γ(ξi ) −
l d
l d

i (ξi − 1)(Ψ(ξi ) − Ψ( j ξ j )),

q(zi |γi ).

i=1

The dimension of parameter ξ is changed for diﬀerent documents.

(5)

where Ψ(·) denotes the digamma function, the ﬁrst derivative
of the log of the Gamma function.

2858

For document d, the terms that involve the Dirichlet π:
 d

 d

L[π] = log Γ li (T d × π)i − li log Γ (T d × π)i +

ld  d
l d 
(6)
i (T × π)i − 1 Ψ(ξi ) − Ψ( j ξ j ) ,

d 
where (T d × π)i = li lL πl T ild . We can also use gradient descent method by taking derivatives of Eq. (6) with respect to
πl on the whole corpus to ﬁnd the estimation of π. Because
each document’s tags-set is observed, the Bernoulli prior η is
unused included for model completeness. For a given corpus,
the ηi is estimated by adding up the number of i-th tag which
appears in all documents.
We show the variational expectation-maximization(EM)
procedure of TWTM as follows. At the beginning, TWTM
initializes
the parameter
K
V π ,θ and ψ, with the constraint of
i ψk equals 1. In the E-step, for each
k θl equals 1 and
document d, we update the variational parameter ξ and γ with
Eqs. (5) and (2) to maximize the L(ξ, γ; η, π, θ, ψ). In the Mstep, π ,θ and ψ are updated as in Eqs. (6), (4), and (3). We
repeatedly the update steps until the convergence condition
of likelihood is satisﬁed or maximum number of iterations is
reached.

4

and the perplexity is signiﬁcantly lower than those of the
baselines. Figure 3 demonstrates the perplexity results on the
IMDB data. Clearly, TWTM excels both CTM and LDA signiﬁcantly and consistently. Figures 2 and 3 show that TWTM
works very well in semi-structured document modeling.

2700

LDA

TWTM

Perplexity

2600

2500

2400

2300

10 20

50

100

150

200

# of Topics

Experimental Results

Figure 2: Perplexity results on the DBLP corpus for TWTM,
LDA and CTM

In the experiments of this work, we used three semistructured corpora. The ﬁrst one consists of technical papers of the Digital Bibliography and Library Project (DBLP)
dataset2 , which is a collection of bibliographic information
on major computer science journals and proceedings. In this
paper, we use a subset of DBLP that contains abstracts of
D=8,212 papers, with W=34,967 words in the vocabulary and
L=1,893 unique tags. The tags we used in DBLP include authors, time, and keywords. The second corpus is The New
York Times3 news stories in March 2011, which is made up
of D=4,307 stories, in which we treat 31 days of March 2011
as tags. And the last document collection is the data from Internet Movie Database (IMDB)4 . The dataset includes 12,091
movie storylines, 52,274 words after removing stop words,
and 3,654 tags which contain directors, stars, time, and movie
keywords. And these movies belong to 29 genres.

4.1

CTM

Perplexity

8000

7000

CTM

LDA

TWTM

6000

Document Modeling
10 20

In order to evaluate the generalization capability of the model,
we use the perplexity score that described in [Blei et al.,
2003]. We trained three latent variable models including LDA
[Blei et al., 2003], CTM [Blei and Laﬀerty, 2005] and our
TWTM, on two corpora, a paper collection in DBLP and a
set of movie documents in IMDB, to compare the generalization performance of the three models. In both datasets,
we removed the stop words and conducted experiments using
5-fold cross-validation. Figure 2 presents the results on the
DBLP dataset. The perplexity of TWTM is larger than or similar to LDA and CTM, when the number of topics T ≤ 100;
when T increases, both CTM and LDA are running into serious over-ﬁtting, while the trend of TWTM keeps going down

50

100

150

200

# of Topics

Figure 3: Perplexity results on the IMDB corpus for TWTM,
LDA and CTM

4.2

Examples of Tag-Topic Distributions

In the preceding section we demonstrated the performance of
TWTM on the learning of tags’ distribution by process the
New York Times’ full text. We treated the 31 days in March
2011 as the tags set of the news corpus, and set the number
of topics K = 100. Each topic is illustrated with the top 20
words most likely to be generated conditioned on the topic
from the matrix of ψ. We found one topic about the Japan
earthquake happened in March 11, 2011. The keyword representation of the topic is shown in Figure 4.

2

http://www.informatik.uni-trier.de/ ley/db/
http://www.nytimes.com
4
http://www.imdb.com
3

2859

japan
van
cern
weak

earthquake
spacecraft
meer
orbit

mercury
particles
axis
indonesian

messenger
association
indonesia
halid

der
hong
plate
opera

0.600
Recall@1

Recall@3

0.575

Figure 4: An illustration of the topic about the Japan earthquake from the 100-topic solution for the NYTimes collection, including the top 20 words, which TWTM considers are
most likely to be generated in the topic.

0.550

Topic Distribution of Japan Earthquake

0.525
0.08

0.06

0.500
TF−IDF

TWTM

0.04

Figure 6: Classiﬁcation results of TWTM, TF-IDF, and LDA
on recall@1 and recall@3 with 5-fold cross-validation.

0.02

ferent methods in Figure 6, where we see that there is significant improvement in classiﬁcation performance when using
LDA and TWTM comparing with tf-idf, and TWTM outperforms LDA in terms of both recall@1 and recall@3.

0.00
5

1011

15

20

25

30

Day of 2011 March

Figure 5: The components of the topic about the Japan earthquake distributed in 31 tags (days of March 2011).

5

Conclusions

With the tag-weighted topic model proposed in the paper, we
provide and analyze a probabilistic approach for mining semistructured documents. This model provides signiﬁcantly improved predictive capability in terms of perplexity compared
to the other topic models, such as LDA and CTM. Besides,
TWTM was shown to be able to obtain the topics distribution
of tags in the corpus, which is very useful for text classiﬁcation, clustering and other data mining applications. Meanwhile, TWTM proposes a novel framework of processing the
tagged text with a high extensibility, and uses a novel function
of tag-weighted topic assignment of documents. The primary
beneﬁt of the tag-weighted topic model is that it allows one
to incorporate diﬀerent types of tags in modeling documents,
and provides a general framework for multi-tags modeling at
not only the level of tags but also the level of documents. It
helps provide diﬀerent approaches in classiﬁcation, clustering, recommendation, and so on. In the future, we plan to
apply TWTM to diﬀerent practical areas, especially the problems of the large-scale document modeling.

Figure 5 shows that the topic we selected has the diﬀerent
value between the tags, the 31 days. The results shows that at
the label of “Mar. 11” the value reaches maximum and there
is another extreme value appeared at “Mar. 30”. The date of
Mar. 11 is when the earthquake happened with the highest
value, and at the date of Mar. 30, there are some other news
about Japan Nuclear leak , for the example of ﬁrst detected
the radioactive elements plutonium.
The capability of inferring the distributions of tags is obtained directly through the proposed model TWTM. And furthermore, there is no need to distinguish between the diﬀerent
types of labels so as to make use of all the tags observed in
the document.

4.3

LDA

Models

Classiﬁcation Performance Comparison

The next experiment is to test the classiﬁcation performance
utilizing feature sets generated by TWTW and other baselines. For the base classiﬁer, we use LIBSVM [Chang and
Lin, 2011] with Gaussian kernel and the default parameters.
For the purpose of comparison, we trained three SVMs on
tf-idf word features, features induced by a 100-topic LDA
model, and features generated by a TWTM model with the
same number of topics, respectively. Since the TWTM
method uses the tag information, we append the tag vector
td to both the tf-idf and LDA feature sets.
In these experiments, we conducted multi-class classiﬁcation experiments using the IMDB dataset, which contains 29
genres. We calculated the evaluation metrics recall@1 and recall@3 with the provided class tags of movies’ genres, using
5-fold cross-validation.
We report the movie classiﬁcation performance of the dif-

Acknowledgments
We thank the anonymous reviewers for helpful comments.
This work was supported by National Natural Science Foundation of China (61003140 and 61033010).

References
[Andrieu et al., 2003] Christophe Andrieu, Nando de Freitas, Arnaud Doucet, and Michael I. Jordan. An introduction to mcmc for machine learning. Machine Learning,
50(1-2):5–43, 2003.

2860

[Petterson et al., 2010] James Petterson, Alexander J.
Smola, Tibério S. Caetano, Wray L. Buntine, and Shravan
Narayanamurthy.
Word features for latent dirichlet
allocation. In NIPS, pages 1921–1929, 2010.
[Ramage et al., 2009] Daniel Ramage, David Hall, Ramesh
Nallapati, and Christopher D. Manning. Labeled lda:
A supervised topic model for credit attribution in multilabeled corpora. In EMNLP, pages 248–256, 2009.
[Ramage et al., 2011] Daniel Ramage, Christopher D. Manning, and Susan Dumais. Partially labeled topic models for
interpretable text mining. In Proceedings of the 17th ACM
SIGKDD international conference on Knowledge discovery and data mining, KDD ’11, pages 457–465, New York,
NY, USA, 2011. ACM.
[Rosen-Zvi et al., 2004] Michal Rosen-Zvi, Thomas L. Grifﬁths, Mark Steyvers, and Padhraic Smyth. The authortopic model for authors and documents. In UAI, pages
487–494, 2004.
[Sato and Nakagawa, 2012] Issei Sato and Hiroshi Nakagawa. Rethinking collapsed variational bayes inference
for lda. In ICML, 2012.
[Wei and Croft, 2006] Xing Wei and W. Bruce Croft. Ldabased document models for ad-hoc retrieval. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’06, pages 178–185, New York, NY, USA,
2006. ACM.
[Zhu et al., 2009] Jun Zhu, Amr Ahmed, and Eric P. Xing.
Medlda: maximum margin supervised topic models for regression and classiﬁcation. In ICML, page 158, 2009.

[Asuncion et al., 2009] Arthur U. Asuncion, Max Welling,
Padhraic Smyth, and Yee Whye Teh. On smoothing and
inference for topic models. In UAI, pages 27–34, 2009.
[Attias, 1999] Hagai Attias. A variational baysian framework for graphical models. In NIPS, pages 209–215, 1999.
[Bishop and Nasrabadi, 2007] Christopher M. Bishop and
Nasser M. Nasrabadi. Pattern Recognition and Machine
Learning. J. Electronic Imaging, 16(4):049901, 2007.
[Blei and Laﬀerty, 2005] David M. Blei and John D. Lafferty. Correlated topic models. In NIPS, 2005.
[Blei and McAuliﬀe, 2007] David M. Blei and Jon D.
McAuliﬀe. Supervised topic models. In NIPS, 2007.
[Blei et al., 2003] David M. Blei, Andrew Y. Ng, and
Michael I. Jordan. Latent dirichlet allocation. Journal of
Machine Learning Research, 3:993–1022, 2003.
[Boyd-Graber and Blei, 2010] Jordan L. Boyd-Graber and
David M. Blei.
Syntactic topic models.
CoRR,
abs/1002.4665, 2010.
[Cai et al., 2008] Deng Cai, Qiaozhu Mei, Jiawei Han, and
Chengxiang Zhai. Modeling hidden topics on document
manifold. In CIKM, pages 911–920, 2008.
[Chang and Blei, 2009] Jonathan Chang and David M. Blei.
Relational topic models for document networks. Journal
of Machine Learning Research - Proceedings Track, 5:81–
88, 2009.
[Chang and Lin, 2011] Chih-Chung Chang and Chih-Jen
Lin. Libsvm: A library for support vector machines. ACM
TIST, 2(3):27, 2011.
[Chen et al., 2012] Xu Chen, Mingyuan Zhou, and
Lawrence Carin. The contextual focused topic model. In
KDD, pages 96–104, 2012.
[Deng et al., 2011] Hongbo Deng, Jiawei Han, Bo Zhao,
Yintao Yu, and Cindy Xide Lin. Probabilistic topic models with biased propagation on heterogeneous information
networks. In KDD, pages 1271–1279, 2011.
[Griﬃths and Steyvers, 2004] Thomas L. Griﬃths and Mark
Steyvers. Finding scientiﬁc topics. In PNAS, pages 449–
455, 2004.
[Hofmann, 1999] Thomas Hofmann. Probabilistic latent semantic indexing. In SIGIR, pages 50–57, 1999.
[Iwata et al., 2009] Tomoharu Iwata, Takeshi Yamada, and
Naonori Ueda. Modeling social annotation data with content relevance using a topic model. In NIPS, pages 835–
843, 2009.
[Lacoste-Julien et al., 2008] Simon Lacoste-Julien, Fei Sha,
and Michael I. Jordan. Disclda: Discriminative learning
for dimensionality reduction and classiﬁcation. In NIPS,
pages 897–904, 2008.
[Mimno and McCallum, 2008] David M. Mimno and Andrew McCallum. Topic models conditioned on arbitrary
features with dirichlet-multinomial regression. In UAI,
pages 411–418, 2008.

2861

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,

VOL. 19,

NO. 1,

JANUARY 2007

43

Extracting Actionable Knowledge from
Decision Trees
Qiang Yang, Senior Member, IEEE, Jie Yin, Charles Ling, and Rong Pan
Abstract—Most data mining algorithms and tools stop at discovered customer models, producing distribution information on customer
profiles. Such techniques, when applied to industrial problems such as customer relationship management (CRM), are useful in
pointing out customers who are likely attritors and customers who are loyal, but they require human experts to postprocess the
discovered knowledge manually. Most of the postprocessing techniques have been limited to producing visualization results and
interestingness ranking, but they do not directly suggest actions that would lead to an increase in the objective function such as profit.
In this paper, we present novel algorithms that suggest actions to change customers from an undesired status (such as attritors) to a
desired one (such as loyal) while maximizing an objective function: the expected net profit. These algorithms can discover costeffective actions to transform customers from undesirable classes to desirable ones. The approach we take integrates data mining and
decision making tightly by formulating the decision making problems directly on top of the data mining results in a postprocessing step.
To improve the effectiveness of the approach, we also present an ensemble of decision trees which is shown to be more robust when
the training data changes. Empirical tests are conducted on both a realistic insurance application domain and UCI benchmark data.
Index Terms—Phrases decision making, data mining, machine learning.

Ç
1

INTRODUCTION

E

XTENSIVE

research in data mining has been done on
discovering distributional knowledge about the underlying data. Models such as Bayesian models, decision trees,
support vector machines, and association rules have been
applied to various industrial applications such as customer
relationship management (CRM) [3], [30], [32], [10]. Despite
such phenomenal success, most of these techniques stop
short of the final objective of data mining, which is to
maximize the profit while reducing the costs, relying on
such postprocessing techniques as visualization and interestingness ranking [23], [29]. While these techniques are
essential to move the data mining result to the eventual
applications, they nevertheless require a great deal of
human manual work by experts. Often, in industrial
practice, one needs to walk an extra mile to automatically
extract the real “nuggets” of knowledge, the actions, in
order to maximize the final objective functions.
In this paper, we present a novel postprocessing
technique to extract actionable knowledge from decision
trees. To illustrate our motivation, we consider customer
relationship management CRM [6], [16], [20], in particular,
where we take the telecommunications industry as an
example. This industry is experiencing more and more
competitions in recent years. The battle is over their most
valuable customers. With massive industry deregulation

. Q. Yang, J. Yin, and R. Pan are with the Department of Computer Science
and Engineering, Hong Kong University of Science and Technology, Clear
Water Bay, Kowloon, Hong Kong.
E-mail: {qyang, yinjie, panrong}@cse.ust.hk.
. C. Ling is with the Department of Computer Science, The University of
Western Ontario, London, Ontario N6A 5B7, Canada.
Email: cling@csd.wuo.ca.
Manuscript received 26 May 2005; revised 15 Mar. 2006; accepted 28 Aug.
2006; published online 20 Nov. 2006.
For information on obtaining reprints of this article, please send e-mail to:
tkde@computer.org, and reference IEEECS Log Number TKDE-0212-0505.
1041-4347/07/$20.00 ß 2007 IEEE

across the world, each customer is facing an ever-growing
number of choices in telecommunications and financial
services. As a result, an increasing number of customers are
switching from one service provider to another. This
phenomenon is called customer “churning” or “attrition,”
which is a major problem for these companies and makes it
hard for them to stay profitable. The data sets are often cost
sensitive and unbalanced. If we predict a valuable customer
who will be an attritor as loyal, the cost is usually higher
than the case when we classify a loyal customer as an
attritor. Similarly, in direct marketing, it costs more to
classify a willing customer as a reluctant one. Such
information is usually given by a cost matrix, where the
objective is to minimize the total cost. In addition, a CRM
data set is often unbalanced; the most valuable customers
who actually churn can be only a small fraction of the
customers who stay.
In the past, many researchers have tackled the direct
marketing problem as a classification problem [28], [27],
[14], [43], where the cost-sensitive and unbalanced nature of
the problem is taken into account. In management and
marketing sciences, stochastic models are used to describe
the response behavior of customers [9], [12], [25], [7]. In the
data mining area, a main approach is to rank the customers
by the estimated likelihood to respond to direct marketing
actions and compare the rankings using a lift chart or the
area under curve measure from the ROC curve [27], [31],
[22]. Domingos [14] proposed the MetaCost framework for
adapting accuracy-based classification to cost-sensitive
learning by incorporating a cost matrix and conditional
risk. Elkan and Zadrozny [17], [43] examined general cases
where a classification decision depends on both the
destination class and the customer in question. Various
ensemble-based methods are examined under the costsensitive learning framework; for example, Fan et al. [21]
Published by the IEEE Computer Society

44

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,

integrated boosting algorithms with cost considerations. An
association-rule-based approach is proposed in [40], where
a focus is placed on data preprocessing in which the costsensitive and data-imbalance knowledge about the data is
used in a data sampling phase. The resultant prediction
model produces higher net profits when compared against
the winners of the ACM KDDCUP 1998 competition [1].
Wang et al. [41], [39] took the actions and a data set as input
and generated utility-enhancing patterns which are then
pruned to produce the cost-effective action sets.
Recognizing the sequential nature of some CRM problems, Pednault et al. applied the framework of reinforcement learning to address the issue of sequential decision
making when interactions can occur among decision outcomes [34]. Reinforcement learning refers to a class of
problems and associated techniques in which the learner is
to learn how to make sequential decisions based on delayed
reinforcement so as to maximize cumulative rewards. In a
Markov Decision Process (MDP), the environment is
assumed to be in some state at any given point in time. In
the case of targeted marketing, such states would be
represented as feature vectors comprised of both the
categorical and numerical data fields on which decision
on what next actions to take is made.
Like most data mining algorithms today, a common
problem in current applications of data mining in intelligent
CRM is that people tend to focus on, and be satisfied with,
building up the models and interpreting them, but not to
use them to get profit explicitly. More specifically, most
data mining algorithms (predictive or supervised learning
algorithms) only aim at constructing customer profiles,
which predict the characteristics of customers of certain
classes. Examples of these classes are: What kind of
customers (described by their attributes such as age,
income, etc.) are likely attritors (who will go to competitors), and what kind are loyal customers? This knowledge is
useful but it does not directly benefit the enterprise. To
improve customer relationship, the enterprise must know
what actions to take to change customers from an undesired
status (such as attritors) to a desired one (such as loyal
customers). This can be done in the telecommunications
industry, for example, by reducing the monthly rates or
increasing the service level for a valuable customer.
Unlike distributional knowledge, to consider actionable
knowledge one must take into account resource constraints.
Actions, such as direct mailing and sales promotion, cost
money to the enterprise [42]. At the same time, enterprises
are increasingly constrained by cost cutting. There is thus a
strong limitation on the number of customer segments that
the company can take on, or in the number of actions the
company can exploit. To make a decision, one must take
into account the cost as well as the benefit of actions to the
enterprise. However, for each customer, there may be a
large number of possible actions or action sets that can be
applied to the customer. Which of the actions to take
depends not only on the particular customers’ situation, but
also on other customers who might benefit from the same
action.
In the past, several approaches have been designed to
extract knowledge from the CRM data. When the actions

VOL. 19,

NO. 1,

JANUARY 2007

and state information are given in the data, sequential
mining methods can be applied using MDP-like approaches
[34]. When the actions are initially unknown, however, few
approaches have been designed to invent new actions that
can minimize the total cost and bring about profitable
changes. In this paper, we present novel algorithms for
postprocessing decision trees to obtain actions that are
associated with attribute-value changes, in order to maximize the profit-based objective functions. This allows a
large number of candidate actions to be considered,
complicating the computation.
More specifically, in this paper, we consider two broad
cases. One case corresponds to the unlimited resource
situation, which is only an approximation to the real-world
situations, although it allows a clear introduction to the
problem. Another more realistic case is the limited-resource
situation, where the actions must be restricted to be below a
certain cost level. In both cases, our aim is to maximize the
expected net profit of all the customers. We show that
finding the optimal solution for the limited resource
problem and designing a greedy heuristic algorithm to
solve it efficiently is computationally hard. We compare the
performance of the exhaustive search algorithm with a
greedy heuristic algorithm, and show that the greedy
algorithm is efficient while achieving results with quality
very close to the optimal one. In order to improve the
robustness of the mined actions, we also describe an
ensemble-based decision tree algorithm [26], using a collection of decision trees rather than a single tree, to generate the
actions. We show that the resultant action sets are indeed
more robust with respect to training data changes.
An important contribution of the paper is that it
integrates data mining and decision making together, such
that the discovery of actions is guided by the result of data
mining algorithms (decision trees in our case). While
decision-making and optimization techniques are not new,
their application to data mining has resulted in a new
generation of learning algorithms, such as support vector
machines [38], [11], manifold regularization for subspace
learning [5]. The work of [34] that applies MDP to customer
databases allows new knowledge—probabilistic plans—to
be extracted from the data sets, allowing a new type of
knowledge to be discovered from the data sets. However,
they require the actions and state changes to be given as
part of the input. Our approach can be considered as a new
step in this direction, which is to discover action sets from
the attribute value changes in a nonsequential data set
through optimization.
The rest of the paper is organized as follows: We first
present our base algorithm for finding unrestricted actions
in Section 2. We then formulate two versions of action-set
extraction problems, and show that finding the optimal
solution for the problems is computationally difficult in the
limited resources case (Section 3). We show that our greedy
algorithms are efficient while achieving results very close to
the optimal ones obtained by the exhaustive search (which
is exponential in time complexity). We also present an
ensemble tree-based technique for making the technique
robust. Conclusions and future work are presented in
Section 4.

YANG ET AL.: EXTRACTING ACTIONABLE KNOWLEDGE FROM DECISION TREES

2

45

ACTION EXTRACTION IN DECISION TREES:
UNLIMITED-RESOURCE CASE

2.1 A First Step in Postprocessing Decision Trees
Decision-tree learning algorithms, such as ID3 or C4.5 [36],
are among the most powerful and popular predictive
methods for classification. In CRM applications, a decision
tree can be built from a set of examples (customers)
described by a rich set of attributes including customer
personal information (such as name, sex, and birthday, etc.),
financial information (such as yearly income), family
information (such as life style, number of children), and
so on. Because decision trees can be converted to rules for
explicit representation of the classification, one can easily
obtain characteristics of customers belonging to a certain
class (such as attritors). In this paper, we focus on the
output of decision tree algorithms as the input to our
postprocessing algorithms. Our algorithms rely on not only
a prediction, but also a probability estimation of the
classification, such as the probability of being loyal. Such
information is available from decision trees.
Our first step is to consider how to extract actions when
there is no restriction on the number of actions to produce.
Our first industrial case study of an application corresponds
to this case [10]. We call this the unlimited-resource case. Our
data set consists of descriptive attributes and a class
attribute. For simplicity, we consider a discrete-value
problem, in which the class values are discrete values.
Some of the values under the class attribute are more
desirable than others. For example, in the banking application, the loyal status of a customer “stay” is more desirable
than “not stay.” The overall process of the algorithm can be
briefly described in the following four steps:
Import customer data with data collection, data
cleaning, data preprocessing, and so on.
2. Build customer profiles using an improved decisiontree learning algorithm [36] from the training data.
In this case, a decision tree is built from the training
data to predict if a customer is in the desired status
or not. One improvement in the decision tree
building is to use the area under the curve (AUC)
of the ROC curve [22], [35] to evaluate probability
estimation (instead of the accuracy). Another improvement is to use Laplace Correction to avoid
extreme probability values.
3. Search for optimal actions for each customer (see
Section 2.2 for details). This is a key component of
the data mining system Proactive Solution [10].
4. Produce reports for domain experts to review the
actions and selectively deploy the actions.
In the next section, we will discuss the leaf-node
search algorithm used in Step 3 (search for optimal
actions) in detail.
1.

2.2

Leaf-Node Search in the Unlimited Resource
Case
We first consider the simpler case of unlimited resources
where the case serves to introduce our computational
problem in an intuitive manner. The leaf-node search
algorithm searches for optimal actions to transfer each leaf

Fig. 1. An example of customer profile.

node to another leaf node with a higher probability of being
in a more desirable class. After a customer profile is built,
the resulting decision tree can be used to classify, and more
importantly, provide the probability of customers in the
desired status such as being loyal or high-spending. When a
customer, who can be either a training example used to
build the decision tree or an unseen testing example, falls
into a particular leaf node with a certain probability of being
in the desired status, the algorithm tries to “move” the
customer into other leaves with higher probabilities of
being in the desired status. The probability gain can then be
converted into an expected gross profit.
However, moving a customer from one leaf to another
means some attribute values of the customer must be
changed. This change, in which an attribute A’s value is
transformed from v1 to v2 , corresponds to an action. These
actions incur costs. The cost of all changeable attributes are
defined in a cost matrix (see Section 2.3) by a domain
expert. The leaf-node search algorithm searches all
leaves in the tree so that for every leaf node, a best
destination leaf node is found to move the customer to. The
collection of moves are required to maximize the net profit,
which equals the gross profit minus the cost of the
corresponding actions.
Based on a domain-specific cost matrix (Section 2.3) for
actions, we define the net profit of an action to be as follows:
X
COSTi ;
ð1Þ
PNet ¼ PE  Pgain 
i

where PNet denotes the net profit, PE denotes the total profit
of the customer in the desired status, Pgain denotes the
probability gain, and COSTi denotes the cost of each action
involved. In Section 3.1, we extend this definition to a
formal definition of the computational problem.
The leaf-node search algorithm for searching the
best actions can thus be described as follows:
Algorithm leaf-node search
1. For each customer x, do
2. Let S be the source leaf node in which x falls into;
3. Let D be a destination leaf node for x the maximum
net profit PNet ;
4. Output ðS; D; PNet Þ;
To illustrate, consider an example shown in Fig. 1, which
represents an overly simplified, hypothetical decision tree
as the customer profile of loyal customers built from a bank.
The tree has five leaf nodes (A, B, C, D, and E), each with a

46

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,

probability of customers’ being loyal. The probability of
attritors is simply 1 minus this probability.
Consider a customer Jack who’s record states that the
Service = Low (service level is low), Sex = M (male), and
Rate = L (mortgage rate is low). The customer is classified
by the decision tree. It can be seen that Jack falls into the leaf
node B, which predicts that Jack will have only a 20 percent
chance of being loyal (or Jack will have an 80 percent chance
to churn in the future). The algorithm will now search
through all other leaves (A, C, D, and E) in the decision tree
to see if Jack can be “replaced” into a best leaf with the
highest net profit.
Consider leaf A. It does have a higher probability of
being loyal (90 percent), but the cost of action would
be very high (Jack should be changed to female), so
the net profit is a negative infinity.
2. Consider leaf C. It has a lower probability of being
loyal, so the net profit must be negative, and we can
safely skip it.
3. Consider leaf D. There is a probability gain of
60 percent (80 percent - 20 percent) if Jack falls into
D. The action needed is to change Service from L
(low) to H (high). Assume that the cost of such a
change is $200 (given by the bank). If the bank can
make a total profit of $1,000 from Jack when he is
100 percent loyal, then this probability gain
(60 percent) is converted into $600 (1; 000  0:6) of
the expected gross profit. Therefore, the net profit
would be $400 (600  200).
4. Consider leaf E. The probability gain is 30 percent
(50 percent - 20 percent), which transfers to $300 of
the expected gross profit. Assume that the cost of the
actions (change Service from L to H and change Rate
from L to H) is $250, then the net profit of moving
Jack from B to E is $50 (300  250).
Clearly, the node with the maximal net profit for Jack is
D, with suggested action of changing Service from L to H.
Notice that in the above example, the actions suggested
for a customer-status change imply only correlations rather
than causality between customer features and status.
Similarly to other data mining systems, the actions should
be reviewed by domain experts before deployment. This is
the Step 4 of the leaf-node search algorithm given at the
beginning of this section.
1.

2.3 Cost Matrix
In our discussion so far, we assume that attribute-value
changes will incur costs. These costs can only be determined by domain knowledge and domain experts. For each
attribute used in the decision tree, a cost matrix is used to
represent such costs. In many applications, the values of
many attributes, such as sex, address, number of children,
cannot be changed with any reasonable amount of money.
Those attributes are called “hard attributes.” For hard
attributes, users must assign a very large number to every
entry in the cost matrix.
If, on the other hand, some value changes are possible
with reasonable costs, then those attributes such as the
Service level, interest rate, and promotion packages are
called “soft attributes.” Note that the cost matrix needs not

VOL. 19,

NO. 1,

JANUARY 2007

to be symmetric. One can assign $200 as the cost of
changing service level from low to high, but infinity (a very
large number) as the cost from high to low, if the bank does
not want to “degrade” service levels of customers as an
action.
One might ask why hard attributes should be included in
the tree building process in the first place since they can
prevent customers from being moved to other leaves. This
is because that many hard attributes are important in
accurate probability estimation of the leaves. When the
probability estimation is inaccurate, the reliability of the
prediction would be low, or the error margin of the
prediction would be high.
For continuous attributes, such as interest rates that can
be varied within a certain range, the numerical ranges can
be discretized first using a number of techniques for feature
transformation. For example, the entropy-based discretization method can be used when the class values are known
[34]. Then, we can build a cost matrix for each attribute
using the discretized ranges as the index values.

3

POSTPROCESSING DECISION TREES:
THE LIMITED RESOURCE CASE

3.1 A Formal Definition of BSP
Our previous case considered each leaf node of the decision
tree to be a separate customer group. For each such
customer group, we were free to design actions to act on
it in order to increase the net profit. However, in practice, a
company may be limited in its resources. For example, a
mutual fund company may have a limited number k (say
three) of account managers, each manager can take care of
only one customer group. Thus, when such limitations
exist, it is a difficult problem to optimally merge all leave
nodes into k segments, such that each segment can be
assigned to an account manager. To each segment, the
responsible manager can several apply actions to increase
the overall profit.
This limited-resource problem can be formulated as a
precise computational problem. Consider a decision tree
DT with a number of source leaf nodes that correspond to
customer segments to be converted and a number of
candidate destination leaf nodes, which correspond to the
segments we wish customers to fall in. Formally, the
bounded segmentation problem (BSP) is defined as follows:
Given:
1.

2.
3.

4.

a decision tree DT built from the training examples,
with a collection S of m source leaf nodes and a
collection D of n destination leaf nodes (in Section 3.3, we extend from a single decision tree to
multiple decision trees),
a prespecified constant k (k  m), where m is the
total number of source leaf nodes,
a cost matrix CðAttri ; u; vÞ; i ¼ 1; 2; . . . , which specifies the cost of converting an attribute Attri ’s value
from u to v, where u and v are indices for Attri ’s
values,
a unit benefit vector BC ðLi Þ denoting the benefit
obtained from a single customer x when the x
belongs to the positive class in a leaf node

YANG ET AL.: EXTRACTING ACTIONABLE KNOWLEDGE FROM DECISION TREES

Li ; i ¼ 1; 2; . . . ; N, where N is the number of leaf
nodes in the tree DT , and
5. a set Ctest of test cases.
A solution is a set of k goals fGi ; i ¼ 1; 2; . . . ; kg,
where each goal consists of a set of source leaf nodes Sij
and one designation leaf node Di , denoted as:
ðfSij ; j ¼ 1; 2; . . . ; jGi jg ! Di Þ, where Sij and Di are leaf
nodes from the decision tree DT . The meaning of a goal
is to transform customers that belong to the source
nodes S to the destination node D via a number of
attribute-value changing actions. Our aim is to find a
solution with the maximal net profit (defined below).
Goals. Given a source leaf node S and a destination leaf
node D, we denote the objective of converting a customer x
from S to D as a goal, and denote it as S ! D. The concept
of a goal can be extended to a set of source nodes: To
transform a set of leaf nodes Si to a designation leaf node D,
the goal is fSi ; i ¼ 1; 2; . . .g ! D.
Actions. In order to change the classification of a
customer x from S to D, one may need to apply more than
one attribute-value changing action. An action A is defined
as a change to an attribute value for an attribute Attr.
Suppose that for a customer x, the attribute Attr has an
original value u. To change its value to v, an action is
needed. This action A is denoted as A ¼ fAttr; u ! vg.
Action Sets. A goal is achieved by a set of actions. To
achieve a goal of changing a customer x from a leaf node
S to a destination node D, a set of actions that contains
more than one action may be needed. Specifically,
consider the path between the root node and D in the
tree DT . Let fðAttri ¼ vi Þ; i ¼ 1; 2; . . . ; ND g be set of
attribute-values along this path. For x, let the corresponding attribute-values be fðAttri ¼ ui Þ; i ¼ 1; 2; . . . ; ND g.
Then, the actions of the form can be generated:
ASet ¼ fðAttri ; ui ! vi Þ; i ¼ 1; 2; . . . ; ND g, where we remove all null actions where ui is identical to vi (thus,
no change in value is needed for an Attri ). This action set
ASet can be used for achieving the goal S ! D.
Net Profits. The net profit of converting one customer x
from a leaf node S to a destination node D is defined as
follows: Consider a set of actions ASet for achieving the
goal S ! D. For each action Attri ; u ! v in ASet, there is a
cost as defined in the cost matrix: CðAttri ; u; vÞ. Let the sum
of the cost for all of ASet be Ctotal ; S ! DðxÞ.
Let the probability of x to belong to the positive class in S
be pðþjx; SÞ. Likewise, let the probability of a customer in D
be in the positive class be pðþjx; DÞ. Recall that from the
input, we have a benefit vector BC ðLÞ for the leaf nodes L.
Thus, we have BC ðSÞ as the benefit of belonging to node S
and BC ðDÞ as the benefit of belonging to node D. Then, the
unit net profit of converting one customer x from S to D is:
P unit ðx; S ! DÞ ¼ðBC ðDÞ  pðþjx; DÞ  BC ðSÞ  pðþjx; SÞÞ
 Ctotal ; S ! DðxÞ:
ð2Þ
Then, for a collection Ctest of all test cases that fall in node S,
the total net profit of applying an action set for achieving
the goal S ! D is:
P ðCtest ; S ! DÞ ¼ x2Ctest P unit ðx; S ! DÞ:

ð3Þ

47

Fig. 2. An example decision tree.

When the index of S is i, and the index of D is j, we denote
P ðCtest ; S ! DÞ as Pij for simplicity.
Thus, the BSP problem is to find the best k groups of
source leaf nodes fGroupi ; i ¼ 1; 2; . . . ; kg and their corresponding goals and associated action sets to maximize the
total net profit for a given test data set Ctest .
An Example. To illustrate, consider an example in Fig. 2.
Assume that for leaf nodes L1 to L4 , the probability values
of being in the desired class are 0.9, 0.2, 0.8, and 0.5,
respectively. Now consider the task of transforming L2 and
L4 to a higher probability node, such that the net profit of
making all transformations is maximized. To illustrate the
process, consider a test data set such that there is exactly
one member that falls in each leaf node of this decision tree.
In order to calculate the net profit, we assume all leaf
nodes to have an initial benefit of one unit. For simplicity,
we also assume that the cost of transferring a customer is
equal to the number of attribute value changes multiplied
by 0.1. Thus, to change from L2 to L1 , we need to modify the
value of the attribute Status, with a profit gain of
ð0:9  1  0:2  1Þ  0:1 ¼ 0:6.
To illustrate the limited resources problem, consider
again our decision tree in Fig. 2. Suppose that we wish to
find a single customer segment (k ¼ 1). A candidate group
H; Rate
is fL2 ; L4 g, with a selected action set fService
Cg which can transform the group to node L3 . Assume that
L2 and L4 only contain one example each. Transferring this
group to leaf node L3 , L2 changes the service level only and,
thus, has a profit gain of ð0:8  0:2Þ  1  0:1 ¼ 0:5 and L4
has a profit gain of ð0:8  0:5Þ  1  0:1 ¼ 0:2. Thus, the net
benefit for this group is 0:2 þ 0:5 ¼ 0:7.
The BSP problem has an equivalent matrix representation. From (2) and (3), we obtain a profit matrix M ¼
ðPij Þ; i ¼ 1; . . . ; m; j ¼ 1; . . . ; n formed by listing all source
leaf nodes Si as the row index and all the action sets ASetj ,
for achieving the goal (Si ! Dj ), as the column index (here,
we omit Si in the column headings). In this matrix M,
ðPij  0Þ, 1  i  m (where m is the number of source leaf
nodes), and 1  j  n (n is the number of destination leaf
nodes). Pij denotes the profit gain computed by applying
ASetj to Si for all test-case customers that falls in Si . If
Pij > 0, that is, applying ASetj to transfer Si to the
corresponding destination leaf node can bring about a net
profit, we say that the source leaf node Si can be covered by
the action set ASetj . From (2) and (3), the computation of
the profit matrix Mð:; :Þ can be done in Oðm  nÞ.
As an example of the profit matrix computation, a part
of the profit matrix corresponding to the source leaf node

48

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,

VOL. 19,

NO. 1,

JANUARY 2007

TABLE 1
An Example of the Profit Matrix Computation

TABLE 2
Illustrating the Greedy-BSP Algorithm

L2 is as shown in Table 1 where ASet1 ¼ fStatus ¼ Ag,
ASet2 ¼ fService ¼ H; Rate ¼ Cg, and ASet3 ¼ fService ¼
H; Rate ¼ Dg (here, for convenience, we ignore the source
value of the attributes, which is dependent on the actual
test cases).
Then, the BSP problem becomes one of picking the best
k columns of matrix M such that the sum of the maximum
net profit value for each source leaf node among the
k columns is maximized. When all Pij elements are of unit
cost, this is essentially a maximum coverage problem [19],
which aims at finding k sets such that the total weight of
elements covered is maximized, where the weight of each
element is the same for all the sets. A special case of the BSP
problem is equivalent to the maximum coverage problem
with unit costs. Thus, we know that the BSP problem is
NP-Complete. Our aim will then be to find approximation
solutions to the BSP problem.

3.2 Algorithms for BSP
Our first solution is an exhaustive search algorithm for
finding the k optimal action sets with maximal net profit.
Algorithm Optimal-BSP
1. for each ASetl 2 A, 1  l  n, choose any combination of
k action sets, do
1.1. Group the leaf nodes into k groups
2.1. Evaluate the net benefit of the action sets on the
groups
end for
2. return the k action sets with associated leaf node groups
that have the maximal net benefit using (2) and (3).
Since the optimal-BSP needs to examine every combination
of k action sets, the computational complexity is Oðnk Þ,
which is exponential in the value of k.
To avoid the exponential worst-case complexity, we have
also developed a greedy algorithm which can reduce the
computational cost and guarantee the quality of the solution

at the same time. Consider the following generalization of
the maximum coverage problem. Given a set with m leaf
nodes Ls ¼ fL1 ; L2 ; . . . ; Lm g, each associated with a different profit Pij ðPij  0Þ for each action set ASetj , 1  j  n.
Each ASetj can be denoted as a subset of Ls which only
contains the covered leaf nodes Li for Pij  0, 1  i  m.
The goal is to choose k action sets so as to maximize the net
profit of covered leaf nodes. We can solve this problem
using a greedy algorithm below, where C is the resulting
k action sets.
We consider the intuition of the Greedy-BSP algorithm
using an example profit matrix M as shown in Table 2,
where we assume a k ¼ 2 limit. In this table, each number is
a profit Pij value computed from the input parameters. The
greedy algorithm processes this matrix in a sequential
manner for k iterations. In each iteration, it considers
adding one additional column of the M matrix, until it has
considered all k columns. Initially, Greedy-BSP starts with
an empty result set C ¼ ;. The algorithm then compares all
the column sums that corresponds to converting all leaf
nodes S1 to S4 to each destination leaf node Di in turn. It
found that ASet2 ¼ ð! D2 Þ has the current maximum
profit of three units. Thus, the resultant action set C is
assigned to fASet2 g.
Next, Greedy-BSP considers how to expand the customer
groups by one. To do this, it considers which additional
column will increase the total net profit to a highest value, if
we can include one more column. As can be seen from
Table 3, if we in addition consider the first column, then the
node S1 can choose to be converted to D1 , instead of D2 as
in Table 2. In that case, the profit of C ¼ fASet1 ; ASet2 g can
be increased by two units, which is the maximum increase
among all other columns. Thus, we choose this subset to be
the current action set C.
Because we have now reached the resource limit k ¼ 2,
we will terminate with the action set C and two groups of
leaf nodes: G1 ¼ fS1 g, and G2 ¼ fS2 ; S3 ; S4 g. This example

YANG ET AL.: EXTRACTING ACTIONABLE KNOWLEDGE FROM DECISION TREES

49

TABLE 3
Illustrating the Greedy-BSP Algorithm (Continued)

returns a total net profit of 2 + 3 = 5 units, which is also the
optimal solution.
The algorithm Greedy-BSP is now described as follows:
Algorithm Greedy-BSP
1. C
;; Compute the matrix M ¼ ðPij Þ using (2) and (3);
2. for l = 1 to k
2.1. select ASetl 2 A that maximizes
X

maxfPij g; j ¼ 1; 2; . . . ; l

ð4Þ

Si 2coverðC[ASetl Þ

2.2. C
end for
3. return C

P rofitð[li¼1 ASeti Þ  P rofitð[l1
i¼1 ASeti Þ

C [ ASetl

This algorithm can be shown to perform close to the optimal
result in our subsequent analysis. In particular, we can
exploit the complexity analysis of the approximate maximum coverage algorithm given in [21] to reach this result.
In order to prove the approximation ratio of the solution
returned by Greedy-BSP to one by Optimal-BSP, we first
need to establish the following two lemmas.
If we let P rofitðGreedyÞ and P rofitðOP T Þ be the net
profit returned by the Greedy-BSP algorithm and the
Optimal-BSP algorithm, respectively, we have the following
property.
Lemma 1. For l ¼ 1; 2; . . . ; k, let ASet be an action set. We have
P rofitð[li¼1 ASeti Þ  P rofitð[l1
i¼1 ASeti Þ
ðP rofitðOP T Þ  P rofitð[l1
i¼1 ASeti Þ
:

k

can achieve maximal additional profit gain. On the other
hand, assume Greedy-BSP selects those ðk  mÞ optimal
action sets in the optimal solution and yet have not
chosen by itself, the profit gain of this batch procedure is
l1
ASeti Þ. According to
at least P rofitðOP T Þ  P rofitð[i¼1
the pigeonhole principle, there must exist one single action
set in the remaining ðk  mÞ optimal action sets, whose
P rofitðOP T ÞP rofitð[l1
i¼1 ASeti Þ
. Since this action
profit is at least
km
set is also a candidate for selecting the next ASetl , we
have

ð5Þ

Proof. Let the optimal solution returned by Optimal-BSP
consists of k optimal action sets. Suppose Greedy-BSP has
already selected ðl  1Þ action sets so far, m of which are
contained in the optimal solution. Now, we consider the
situation where Greedy-BSP selects the next ASetl action



P rofitðOP T Þ  P rofitð[l1
i¼1 ASeti Þ
km



P rofitðOP T Þ  P rofitð[l1
i¼1 ASeti Þ
:
k
t
u

Lemma 2. For l ¼ 1; 2; . . . ; k, we have
"

#
1 l
l
P rofitðOP T Þ:
P rofitð[i¼1 ASeti Þ  1  1 
k

ð6Þ

Proof Sketch. The proof can be done by induction using
Lemma 1, similar to the proof of Lemma 3.13 in [21]. t
u
Based on the above two established lemmas, we have the
following theorem:
Theorem 1. The Greedy-BSP is a ð1  1eÞ-approximation
algorithm.
"

 #
1 k
P rofitðOP T Þ
P rofitðGreedyÞ  1  1 
k
ð7Þ


1
> 1  P rofitðOP T Þ:
e

set. Because of the heuristic strategy used in the Step 2.1
of the Greedy-BSP algorithm, P rofitð[li¼1 ASeti Þ 
P rofitð[l1
i¼1 ASeti Þ represents the additional profit gain
achieved by ASetl . In addition, ASetl should be a set that

Proof. Theorem 1 follows directly from Lemma 2 by
letting l ¼ k. In addition, because limk!1 1  ð1  1kÞk ¼
1  1e and 1  ð1  k1Þk is decreasing, it follows that
u
t
1  ð1  k1Þk > 1  1e .

50

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,

3.3 Improving the Robustness Using Multiple Trees
The major advantage of the Greedy-BSP algorithm is that it
can significantly reduce the computational cost while also
guaranteeing the high quality of the solution at the same
time. In Greedy-BSP, the built decision tree always choose
the most informative attribute as the root node. However, as
pointed out in [26], many top-ranked attributes may exhibit
similar discriminating merits with little difference. It is
worthwhile to employ different top-ranked attributes as the
root nodes for building multiple decision trees. Therefore,
we have also proposed an algorithm referred to as GreedyBSP-Multiple which is based on integrating an ensemble of
decision trees in this paper. Ensemble-based methods have
been shown to improve the robustness of machine learning
systems greatly [45], [26], [44]. The basic idea is to construct
multiple decision trees using different top-ranked attributes
as their root nodes. For each set of test cases, the ensemble
decision trees return the median net profit and the
corresponding leaf nodes and action sets as the final
solution. We believe that this method will be more robust
when the training data set changes, because each change
can only alter a small number of trees. Thus, we expect that
when the training data are unstable, the ensemble-based
decision tree methods can perform much more stable as
compared to results from the single decision trees.
Algorithm Greedy-BSP-Multiple
1. Given a training data set described by p attributes
1.1. calculate gain ratios to rank all the attributes in
an descending order
1.2. for i ¼ 1 to p
use the ith attribute as root node to
construct the ith decision tree
end for
2. Take a set of testing examples as input
2.1. for i ¼ 1 to p
use the ith decision tree to calculate the net
profit by calling algorithm Greedy-BSP
end for
2.2. return k action sets corresponding to the median
net profit
The added advantage of Greedy-BSP-Multiple is that it is
more robust with respect to different sampled training data.
Since Greedy-BSP-Multiple relies on building multiple
decision trees to calculate the median net profit, different
sampling can only affect the construction of a small portion
of decision trees. Therefore, Greedy-BSP-Multiple can
produce net profit with less variance.

3.3.1 Difference from Previous Works
In business marketing, most of the marketing planning
activities have been done in a human-heavy process, which
is carried out by hand [4], [13]. Collecting customer data
and using the data for direct marketing operations has
increasingly become possible, thanks to the popularity of
data mining technology. One approach is known as
“database marketing,” which is defined as creating a bank
of information about individual customers from their
orders, enquiries, and other activity, using it to analyze
the customer behavior and developing intelligent strategies

VOL. 19,

NO. 1,

JANUARY 2007

[37], [33], [15]. An important computational aspect is to
segment a customer group into subgroups, often in terms a
binary decision variable such as customer’s willingness to
buy a product or not to buy. This segmentation corresponds
to traditional classification learning in machine learning,
such as decision trees [36], where the aim is to generate a
ranking function for the customers sorted on their likelihood to buy a product (or stay loyal to the company), so
that a “gain chart” or “lift chart” can be created for human
analysis. Analytical techniques such as linear and logistic
regression can be used to implement the ranking functions
[15]. The decision then is on which subset of the customers
to market a certain product to, in order to maximize the
total net profit [27]. However, even though the segmentations can be discovered using an machine learning
algorithm, the actions are still to be discovered by human
experts.
Machine learning and data mining research has contributed to the business practice by addressing some new
issues in marketing. One issue is that the typical marketing
data are cost sensitive, in that the false positive and false
negative costs are different. To solve this problem, [43], [14]
proposed the framework of cost-sensitive learning by
incorporating a cost matrix for balancing the total expected
misclassification costs. In addition to the cost issues, the
number of items belonging to one class may greatly
outnumber those in another class. As mentioned above,
machine learning methods have traditionally been applied
to produce a ranking of customers by the estimated
probability to respond to a marketing action, and selecting
some top portion of the ranked list [27], [31]. When the data
are biased, traditional accuracy-based measurements are no
longer adequate. In order to solve this problem, Wang et al.
[22] presented an association rule-based approach that
differentiates between the positive class and the negative
class members the most, and use these rules to for
segmentation.
Another direction in machine learning is to apply
sequential learning techniques using reinforcement learning
and Markov decision processes. For example, [43], [34], [2]
aimed to find a policy with which to direct an optimal action
based on a customer’s current status. Here, the objective is
to maximize the total benefits accrued over a period of time,
after a sequence of actions is taken, when deciding whether
to take an action or not.
However, all of the above research works are aimed at
either finding a segmentation of the customer database, or
deciding to take a predefined action for every customer
based on that customer’s current status. None of them
addressed the issue of discovering actions that might be
taken from a customer database. To the best of our
knowledge, ours is the first such work in machine learning
and business marketing that addressed this action-discovery issue.

3.4 Experimental Evaluation
In order to evaluate the performance of our proposed
algorithm, experiments were carried out on a real data set
from an insurance company and four data sets from the UCI
ML repository [8].

YANG ET AL.: EXTRACTING ACTIONABLE KNOWLEDGE FROM DECISION TREES

Fig. 3. Net profit versus number of action sets.

3.4.1 Experiments on Real Data
Experiments were first performed on a real data set
obtained from an insurance company in Canada. This data
set consists of more than 25,000 records for customers who
have the status of “stay” or “leave” the insurance company,
which are referred to as positive and negative examples,
respectively. Each example is described by more than
60 attributes, many of which are not hard attributes. About
20 attributes are soft attributes with reasonable costs for
value changes. We constructed a cost matrix for each
attribute contained in the data set according to their
semantics in the real domain.
In our experiment, we first selected 10 attributes based
on the Gain Ratio criterion. Since this data set has a highly
unbalanced data distribution, we randomly sampled
6,000 examples as the training data, with the ratio of
positive and negative as one to one, in order to prevent a
decision tree from predicting all the customers to be
negative. We also randomly sampled 300 examples from
the rest of the data to be used as the testing data. In this
setting, we built a decision tree with 153 leaf nodes. Eightyseven of them are considered negative leaf nodes because
their probability of being positive is less than 50 percent,
while the other 66 positive leaf nodes.
We applied Greedy-BSP and Optimal-BSP to calculate
the prespecified k action sets with maximal net profit. Fig. 3

51

Fig. 4. Runtime versus number of action sets.

shows the net profit obtained by the two algorithms with
respect to different numbers of action sets k. As shown in
the figure, the net profit increases for both Greedy-BSP and
Optimal-BSP with an increasing number of action sets k.
This is because if more customers are transformed to a
desired status, it is more possible to obtain higher profit. In
addition, an important property to note is that, for a specific
k, the net profit obtained by Greedy-BSP is very close to or
the same as that by Optimal-BSP, which can guarantee the
quality of solution provided by Greedy-BSP.
Table 4 compares the k action sets selected by both
Greedy-BSP and Optimal-BSP with respect to different
numbers of action sets k. There are a total of 66 action sets
provided by the decision tree built in our experiments. As
shown in the table, the action sets selected by Greedy-BSP
are very close to that by Optimal-BSP for the same number
of action sets k.
In our example, each action set contains a number of
actions; for example, the action set A3 consists of four
different actions (attribute changes), that is,

TABLE 4
Selected Action Sets versus Number of Action Sets

Fig. 5. Net profit versus number of action sets.

52

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,

VOL. 19,

NO. 1,

JANUARY 2007

TABLE 5
UCI Data Sets Used in the Experiments

Fig. 6. (a) German. (b) Australian. (c) Adult. (d) Endgame. Net profit comparisons of Optimal-BSP and Greedy-BSP on four UCI data sets.

fa4 : $0  $249 ! $500  $999; a7 : F ! E;
a8 : DIV ! W ID; a9 : 1980  1989 ! 1990  1994g:
As described above, there are a total of 87 negative leaf
nodes in our decision tree. By applying the action set A3 , a
group of customers falling into five negative leaf nodes
fL2 ; L3 ; L13 ; L56 ; L75 g can be transferred to third positive leaf
node, which corresponds to a customer segmentation.
Fig. 4 compares the runtime of the two algorithms with
respect to different numbers of action sets k. Note that the
runtime of y-axis uses a logscale. As expected, Greedy-BSP
is much more efficient and scalable than Optimal-BSP. For
Greedy-BSP, the runtime is 0.20 seconds irrespective of the
number of action sets k. In contrast, the runtime for
Optimal-BSP increases exponentially with the increasing

number of action sets k. This is because the optimal
algorithm needs to compare many more combinations for
larger values of k in order to obtain maximal net profits.
We performed another set of experiments to evaluate the
robustness of Greedy-BSP and Greedy-BSP-Multiple (where
10 decision trees are used in each ensemble). In this
experiment, we randomly selected 300 examples from the
whole data set as the testing data. From the rest of the data,
we randomly generated 10 training data sets of 6,000 examples, where the ratio of positive and negative equals 1:1. We
applied the two algorithms on these 10 groups of data.
Fig. 5 compares the performance of the two algorithms with
respect to different numbers of actions sets k in a box plot
(the figure shows a box and whisker plot for each set of

YANG ET AL.: EXTRACTING ACTIONABLE KNOWLEDGE FROM DECISION TREES

53

Fig. 7. Runtime comparisons of Optimal-BSP and Greedy-BSP on four UCI data sets. (a) German. (b) Australian. (c) Adult. (d) Endgame.

experiments, where a box has lines at the lower quartile,
median, and upper quartile values. The whiskers are lines
extending from each end of the box to show the extent of
the rest of the data. Outliers are data with values beyond the
ends of the whiskers.). We can see from the figure that
Greedy-BSP-Multiple can produce the net profit values with
much less variance than Greedy-BSP when 10 sets of
randomly sampled examples are used for training. Since
Greedy-BSP-Multiple relies on building multiple decision
trees to calculate the median net profit, different sampling
can only affect the construction of a small portion of
decision trees, while the rest are left unchanged. Therefore,
we can conclude that Greedy-BSP-Multiple is more robust
than Greedy-BSP.

3.4.2 Experiments on UCI Data
We also performed experiments on four UCI data sets to
evaluate the performance of the algorithms. The four data
sets used in our experiments are listed in Table 5. These
data sets were chosen because they have binary classes and
a sufficient number of examples. In our experiments, we
also used Gain Ratio [32] criterion to select eight significant
attributes for each data set. For Greedy-BSP and OptimalBSP, we calculated the net profit based on a decision tree
whose root node is the attribute with the maximal gain ratio
value. For Greedy-BSP-Multiple, the net profit is computed

using eight decision trees with different attributes as the root
node. For each data set, we randomly sampled the training
data set with the ratio of positive and negative as one to
one. Another independent data set generated randomly is
used for testing.
Fig. 6 shows the net profit obtained by Optimal-BSP and
Greedy-BSP on the four data sets. We can also observe that,
for each data set, the values of net profit computed by
Greedy-BSP are very close to or the same as those found by
Optimal-BSP with respect to the same number of action sets k.
Fig. 7 compares the efficiency of Optimal-BSP and
Greedy-BSP on the four data sets. Note that the values of
y-axis use a logscale in the figure. We can observe that the
runtime of Optimal-BSP increases exponentially as the
number of action sets k increases. This makes Optimal-BSP
intractable especially when the decision tree has a large
number of positive and negative leaf nodes. In contrast, the
runtime of Greedy-BSP remains approximately the same
regardless of different numbers of action sets k. Therefore,
Greedy-BSP is much more efficient and scalable than
Optimal-BSP.
We also performed experiments to evaluate the robustness of Greedy-BSP and Greedy-BSP-Multiple on the four
data sets. In this experiment, for each data set, we randomly
generated 10 training data sets, each of which has the same
positive and negative examples. The same testing data set

54

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,

VOL. 19,

NO. 1,

JANUARY 2007

Fig. 8. Robustness comparisons of Greedy-BSP and Greedy-BSP-Multiple on four UCI data sets. (a) German. (b) Australian. (c) Adult. (d) Endgame.

was used for evaluation. Fig. 8 shows the net profit obtained
by the two algorithms with respect to different sampled
training data. We can also observe that, when different
sampled examples are used for training, Greedy-BSPMultiple can produce the net profit with much less variance
than Greedy-BSP. Therefore, Greedy-BSP-Multiple is more
robust than Greedy-BSP.
We conclude from our experiments that, Greedy-BSP can
find k action sets with maximal net profit, which is very
close to those found by Optimal-BSP, at least for small
values of k for which Optimal-BSP terminates in a reasonable amount of time. At the same time, Greedy-BSP can
scale well with an increasing number of action sets k, which
is more efficient than Optimal-BSP. In addition, by building
multiple decision trees, Greedy-BSP-Multiple is more
robust than Greedy-BSP when different sampled examples
are used for training.

4

CONCLUSIONS

AND

FUTURE WORK

Most data mining algorithms and tools produce only the
segments and ranked lists of customers or products in their
outputs. In this paper, we present a novel technique to take
these results as input and produce a set of actions that can
be applied to transform customers from undesirable classes
to desirable ones. For decision trees, we have considered

two broad cases. The first case corresponds to unlimited
resources, and the second case corresponds to the limited
resource-constraint situations. In both cases, our aim is to
maximize the expected net profit of all the customers. We
have found a greedy heuristic algorithm to solve both
problems efficiently and presented an ensemble-based
decision-tree algorithm that use a collection of decision
trees, rather than a single tree, to generate the actions. We
show that the resultant action set is indeed more robust
with respect to training data changes.
The results discussed in this paper offer effective
solutions to intelligent CRM for enterprises. Our initial
applications case study (in unlimited resources case,
reported in [28], [42]) has shown strong promise in applying
this class of postprocessing techniques in practice.
In our future work, we will research other forms of
limited resources problem as a result of postprocessing data
mining models and evaluate the effectiveness of our
algorithms in the real-world deployment of the actionoriented data mining.

ACKNOWLEDGMENTS
The authors would like to thank Hong Kong RGC for
supporting this work under grant HKUST6187/04E. They
also thank the anonymous referees for their comments.

YANG ET AL.: EXTRACTING ACTIONABLE KNOWLEDGE FROM DECISION TREES

REFERENCES
[1]
[2]

[3]
[4]
[5]
[6]
[7]
[8]
[9]
[10]
[11]
[12]
[13]
[14]
[15]
[16]
[17]
[18]
[19]
[20]
[21]
[22]
[23]

[24]
[25]
[26]
[27]
[28]
[29]
[30]

The kdd-cup-98 result: http://www.kdnuggets.com/meetings/
kdd98/kdd-cup-98-results.html, 2005.
N. Abe, E. Pednault, H. Wang, B. Zadrozny, W. Fan, and C. Apte,
“Empirical Comparison of Various Reinforcement Learning
Strategies for Sequential Targeted Marketing,” Proc. Second IEEE
Int’l Conf. Data Mining (ICDM ’02), 2002.
R. Agrawal and R. Srikant, “Fast Algorithms for Mining
Association Rules,” Proc. 20th Int’l Conf. Very Large Data Bases
(VLDB ’94), pp. 487-499, Sept. 1994.
Bank Marketing Association, Building a Financial Services Plan:
Working Plans for Product and Segment Marketing, Financial
Sourcebooks, 1989.
M. Belkin, P. Niyogi, and V. Sindhwani, “On Manifold Regularization,” Proc. 10th Int’l Workshop Artificial Intelligence and Statistics,
pp. 17-24, Jan. 2005.
A. Berson, K. Thearling, and S.J. Smith, Building Data Mining
Applications for CRM. McGraw-Hill, 1999.
G. Bitran and S. Mondschein, “Mailing Decisions in the Catalog
Sales Industry,” Management Science, vol. 42, pp. 1364-1381, 1996.
C.L. Blake and C.J. Merz, “UCI Repository of Machine Learning,”
www.ics.uci.edu/~mlearn/mlrepository.html, 1998.
J.R. Bult and T. Wansbeek, “Optimal Selection for Direct Mail,”
Marketing Science, vol. 14, pp. 378-394, 1995.
M.-S. Chen, J. Han, and P.S. Yu, “Data Mining: An Overview from
a Database Perspective,” IEEE Trans. Knowledge And Data Eng.,
vol. 8, pp. 866-883, 1996.
N. Cristianini and J. Shawe-Taylor, An Introduction to Support
Vector Machines. Cambridge Univ. Press, 2000.
W. Desarbo and V. Ramaswamy, “Crisp: Customer Response
Based Iterative Segmentation Procedures for Response Modeling
in Direct Marketing,” J. Direct Marketing, vol. 8, pp. 7-20, 1994.
S. Dibb, L. Simkin, and J. Bradley, The Marketing Planning
Workbook. Routledge, 1996.
P. Domingos, “Metacost: A General Method for Making Classifiers Cost Sensitive,” Proc. ACM Conf. Knowledge Discovery and
Data Mining, pp. 155-164, 1999.
R.G. Drozdenko and P.D. Drake, Optimal Database Marketing. Sage
Publications, 2002.
J. Dyche, The CRM Handbook: A Business Guide to Customer
Relationship Management. Addison-Wesley, 2001.
C. Elkan, “The Foundations of Cost-Sensitive Learning,” Proc.
17th Int’l Joint Conf. Artificial Intelligence (IJCAI ’01), 2001.
W. Fan, S.J. Stolfo, J. Zhang, and P.K. Chan, “Adacost:
Misclassification Cost-Sensitive Boosting,” Proc. 16th Int’l Conf.
Machine Learning, pp. 97-105, 1999.
M.R. Garey and D.S. Johnson, Computers and Intractability: A Guide
to the Theory of NPCompleteness. WH Freeman, 1979.
B.J. Goldenberg, CRM Automation. Prentice Hall, 2002.
D.S. Hochbaum, “Approximation Algorithms for Np-Hard Problems,” chapter 3, p. 136. PWS Publishing Company, 1995.
J. Huang and C.X. Ling, “Using Auc and Accuracy in Evaluating
Learning Algorithms,” IEEE Trans. Knowledge and Data Eng.,
vol. 17, no. 3, pp. 299-310, 2005.
D.A. Keim and H.-P. Kriegel, “Visualization Techniques for
Mining Large Databases: A Comparison,” IEEE Trans. Knowledge
and Data Eng., special issue on data mining, vol. 8, no. 6, pp. 923938, Dec. 1996.
R. Kohavi and M. Sahami, “Error-Based and Entropy-Based
Discretization of Continuous Features,” Proc. Second Int’l Conf.
Knowledge Discovery and Data Mining, pp. 114-119, 1996.
N. Levin and J. Zahavi, “Segmentation Analysis with Managerial
Judgment,” J. Direct Marketing, vol. 10, pp. 28-37, 1996.
J. Li and H. Liu, “Ensembles of Cascading Trees,” Proc. IEEE Int’l
Conf. Data Mining (ICDM ’03), pp. 585-588, 2003.
C.X. Ling and C. Li, “Data Mining for Direct Marketing—Specific
Problems and Solutions,” Proc. Fourth Int’l Conf. Knowledge
Discovery and Data Mining (KDD ’98), pp. 73-79, 1998.
C.X. Ling, T. Chen, Q. Yang, and J. Cheng, “Mining Optimal
Actions for Intelligent CRM,” Proc. IEEE Int’l Conf. Data Mining
(ICDM), 2002.
B. Liu, W. Hsu, L.-F. Mun, and H.-Y. Lee, “Finding Interesting
Patterns Using User Expectations,” IEEE Trans. Knowledge and
Data Eng., vol. 11, no. 6, pp. 817-832, 1999.
H. Mannila, H. Toivonen, and A.I. Verkamo, “Efficient Algorithms for Discovering Association Rules,” Proc. Workshop Knowledge Discovery in Databases (KDD ’94), pp. 181-192, 1994.

55

[31] B. Masand and G.P. Shapiro, “A Comparison of Approaches for
Maximizing Business Payoff of Prediction Models,” Proc. Second
Int’l Conf. Knowledge Discovery and Data Mining (ACM KDD ’96),
pp. 195-201, 1996.
[32] T. Mitchell, “Machine Learning and Data Mining,” Comm. ACM,
vol. 42, no. 11, pp. 30-36, Nov. 1999.
[33] E.L. Nash, Database Marketing. McGraw-Hill Inc., 1993.
[34] E. Pednault, N. Abe, and B. Zadrozny, “Sequential Cost-Sensitive
Decision Making with Reinforcement Learning,” KDD ’02: Proc.
Eighth ACM SIGKDD Int’l Conf. Knowledge Discovery and Data
Mining, pp. 259-268, 2002.
[35] F. Provost, T. Fawcett, and R. Kohavi, “The Case against Accuracy
Estimation for Comparing Induction Algorithms,” Proc. 15th Int’l
Conf. Machine Learning, pp. 445-453, 1998.
[36] J.R. Quinlan, C4.5 Programs for Machine Learning. Morgan
Kaufmann, 1993.
[37] R. Shaw and M. Stone, Database Marketing. John Wiley and Sons,
1988.
[38] V. Vapnik, The Nature of Statistical Learning Theory. SpringerVerlag, 1995.
[39] K. Wang, Y. Jiang, and A. Tuzhilin, “Mining Actionable Patterns
by Role Models,” Proc. IEEE Int’l Conf. Data Eng., 2006.
[40] K. Wang, S. Zhou, Q. Yang, and J.M.S. Yeung, “Mining Customer
Value: From Association Rules to Direct Marketing,” Data Mining
and Knowledge Discovery, vol. 11, no. 1, pp. 57-79, 2005.
[41] A. Tuzhilin, Y. Jiang, K. Wang, and A. Fu, “Mining Patterns that
Respond to Actions,” Proc. IEEE Int’l Conf. Data Mining, pp. 669672, 2005.
[42] Q. Yang, J. Yin, C.X. Ling, and T. Chen, “Postprocessing Decision
Trees to Extract Actionable Knowledge,” Proc. IEEE Conf. Data
Mining (ICDM ’03), pp. 685-688, 2003.
[43] B. Zadrozny and C. Elkan, “Learning and Making Decisions When
Costs and Probabilities Are Both Unknown,” Proc. Seventh ACM
SIGKDD Int’l Conf. Knowledge Discovery and Data Mining (ACM
SIGKDD ’01), pp. 204-213, 2001.
[44] X. Zhang and C.E. Brodley, “Boosting Lazy Decision Trees,” Proc.
Int’l Conf. Machine Learning (ICML), pp. 178-185, 2003.
[45] Z.-H. Zhou, J. Wu, and W. Tang, “Ensembling Neural Networks:
Many Could Be Better Than All,” Artifical Intelligence, vol. 137,
nos. 1-2, pp. 239-263, 2002.
Qiang Yang received the PhD degree from
the University of Maryland, College Park. He
is a faculty member in the Hong Kong
University of Science and Technology’s Department of Computer Science and Engineering. His research interests are AI planning,
machine learning, case-based reasoning, and
data mining. He is a senior member of the
IEEE and an associate editor for the IEEE
Transactions on Knowledge and Data Engineering and IEEE Intelligent Systems.
Jie Yin received the BE degree in computer
science from the Xi’an Jiaotong University in
2001. Starting from the Fall of 2001, she has
been a PhD student in the Department of
Computer Science and Engineering at the Hong
Kong of Science and Technology. Her research
interests include artificial intelligence, data
mining, and pervasive computing.

56

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,

Charles Ling received the MSc and PhD
degrees from the Department of Computer
Science at the University of Pennsylvania in
1987 and 1989, respectively. Since then, he has
been a faculty member of computer science at
the University of Western Ontario, Canada. His
main research areas include machine learning
(theory, algorithms, and applications), cognitive
modeling, and AI in general. He has published
more than 100 research papers in journals (such
as Machine Learning, JMLR, JAIR, TKDE, and Cognition) and
international conferences (such as IJCAI, ICML, and ICDM). He has
been an associate editor for the IEEE Transactions on Knowledge and
Data Engineering, and guest editor for several journals. He is also the
director of Data Mining Lab, leading data mining development in CRM,
bioinformatics, and the Internet. He has managed several data mining
projects for major banks and insurance companies in Canada.

VOL. 19,

NO. 1,

JANUARY 2007

Rong Pan received the BSc and PhD degrees in
applied mathematics from Zhongshan University, China, in 1999 and 2004, respectively. He is
a postdoctoral fellow at the Hong Kong University of Science and Technology. His research
interest includes machine learning, data mining,
and case-based reasoning.

. For more information on this or any other computing topic,
please visit our Digital Library at www.computer.org/publications/dlib.

IEEE TRANSACTIONS ON RELIABILITY, VOL. 55, NO. 1, MARCH 2006

7

Some Properties of Confidence Bounds on Reliability
Estimation for Parts Under Varying Stresses
Wenbiao Zhao, Member, IEEE, Rong Pan, Alexander Aron, and Adamantios Mettas

Abstract—Reliability estimation is usually performed on a part
under a constant stress level. However, a part could experience several different stress levels, or profiled stress, during its lifetime.
One such example is when the part is subject to step-stress accelerated life testing. Studying the reliability estimation & its confidence bounds for a part under varying stresses will generalize
the existing estimation methods for accelerated life testing. In this
paper, we derive the reliability function of a part under varying
stresses based on a Weibull failure time distribution, and cumulative damage model. The reliability confidence bounds, based on
a -normal approximation, are given explicitly, and their limiting
properties are discussed. A step-stress accelerated life testing example is used to illustrate these interesting properties, which provides the insights of the limitation of the current test plan, and how
to design a better one.
Index Terms—Accelerated life test, confidence interval, cumulative damage model, maximum likelihood estimation.

ALT
SSALT
CDM
MLE
LR

Acronyms1
Accelerated Life Test
Step Stress Accelerated Life Test
Cumulative Damage Model
Maximum Likelihood Estimation
Likelihood Ratio
Notation
failure time
shape parameter of the Weibull distribution
scale parameter of the Weibull distribution
stress level as a function of
cumulative failure function
reliability function
parameters in the inverse power function
integrated standardized time used in the reliability
function

T

,

estimator of
asymptotic variance of the estimator
the upper percentile point of a standard -normal
distribution
upper bound of
Manuscript received May 4, 2004; revised April 15, 2005. Associate Editor:
J.-C. Lu.
W. Zhao is with American Express.
R. Pan is with the Department of Industrial Engineering, University of Texas
at El Paso, El Paso, TX 79968 USA.
A. Aron and A. Mettas are with ReliaSoft Corporation, Tucson, AZ 85710
USA.
Digital Object Identifier 10.1109/TR.2005.858091
1The

singular and plural of an acronym are always spelled the same.

lower bound of
likelihood ratio function on
I. INTRODUCTION

A

PART generally experiences various stresses, which either accelerate or decelerate the part’s failure, during its
lifetime. However, in conventional reliability analysis, the influence of varying stresses is seldom explicitly included into reliability estimation. Time-varying stress accelerated life testing
(ALT) is to study the failure time & failure mode of a part under
pre-planned changing stresses. Comparing to the constant stress
ALT, time-varying stress ALT is able to further reduce testing
time, as well as estimate product lifetime under a similar varying
stress operation condition. This is highly desirable given the
pressure to shorten a product’s time-to-market.
The pioneering work for varying stress ALT modeling has
been done by DeGroot & Goel [1], and Nelson [2]. DeGroot &
Goel [1] assumed that under higher stress, the mean product life
time would linearly shrink with a constant factor corresponding
to the stress level, and proposed a tampered random variable
model for simple step stress ALT (SSALT) in the framework of
Bayesian decision theory. Nelson [2], [3], and Miller & Nelson
[4] suggested that the life-stress model must take into account
the cumulative effect of the applied stresses when dealing with
data from accelerated tests with step stresses. The cumulative
damage is additive, and the remaining life of test units depends
only on the current stress & the current cumulative distribution function, regardless of the damage accumulation history.
Alternatively, Bhattacharyya & Soejoeti [5] developed a tampered failure rate model. Khamis & Higgins [6] proposed another model known as the KH model for step-stress ALT, which
is based on a time-transformation of the exponential model.
Zhao & Elsayed [7], and Pan & Ayala [8] presented a generalized accelerated life model for step stress testing, and a general
likelihood function formulation of step stress models.
Most of the above varying stress ALT models are based
on step-stress loading; however, the stresses experienced by a
product in service usually are not just simple step stresses. Some
products may undergo a repeated cyclic stress loading; some
may undergo a ramp, or cyclic-ramp-constant stress loading, or
a nonrepeating pattern stress loading. For example, insulation
under AC voltage sees a sinusoidal stress. The varying stress
ALT for such a product loads a specimen with the same stress
pattern at high stress levels to obtain the accurate reliability
estimation. Previous research has seldom explicitly modeled
the influence of varying stresses on product reliability. To our
knowledge, only Mettas & Vassiliou [9], and Ying & Sheng

0018-9529/$20.00 © 2006 IEEE

8

IEEE TRANSACTIONS ON RELIABILITY, VOL. 55, NO. 1, MARCH 2006

[10] extended the cumulative damage model to some cases of
general varying-stress loading.
is generally based on limThe estimation of reliability
ited failure & survival data; thus, one must consider the uncer. The issue of confidence bounds has started to
tainty of
draw many researchers’ attention, but most of the previous research focuses on field data. The property of maximum likelihood estimation of ALT parameters has been studied by McSorley et al. [11] using simulation. They pointed out that a
large sample size was needed in order to make maximum likelihood-type inferences on model parameters. Vander Wiel &
Meeker [12] investigated the accuracy of the likelihood-ratio
based confidence bounds, and asymptotic -normal-based confidence bounds using censored Weibull regression data from
constant ALT. Erto & Giorgio [13], and van Dorp & Mazzuchi
[14] used Bayesian approaches to assess the reliability function
from ALT. In this paper, we develop an analytical expression
of the reliability function, and its confidence bounds based on
the cumulative damage model (CDM) for a part under varying
stress loading. This generalizes the previous work on ALT &
SSALT modeling. We prove the nonmonotonicity of the confidence bounds on the reliability function by examining the limiting values of upper & lower bounds. We also demonstrate
the limitation of the maximum likelihood estimation (MLE)
method, and the importance of appropriately assigning a confidence level for parameter estimation when facing limited data.
II. CUMULATIVE DAMAGE MODEL
An accelerated life model consists of an underlying failure
time distribution function, and a stress-life relationship function. The percentile of the failure distribution will be modified
by the stress-life relationship when the test unit undergoes a
varying stress accelerated life test. The objective of reliability
data analysis becomes to estimate the parameters in the failure
time distribution, and the stress-life function. In this paper,
the Weibull distribution, and the inverse power function are
considered.
To derive the confidence bound of the reliability function
under a varying stress loading, we will briefly describe the cumulative damage model proposed by Nelson [2], which was iniof
tially used for analyzing SSALT data. The failure time
a product under a certain stress level follows a Weibull distribution, where two parameters, & , are used to characterize
the shape, and the scale of the distribution (when
, an
exponential distribution is obtained). It is assumed that is a
constant, and is a function on the stress level, i.e.,

in (1) as a standardized failure time, and
If we view
the parameter as the shrinking or expanding factor to the original failure time comparing with a natural reference stress (e.g.,
the normal in-use stress), the expression in the bracket in (3)
can be seen as the standardized cumulative exposure time to the
constant stress, , with respect to the natural reference point.
In a step-stress accelerated life test, the stress level is moved
to the next stage at a pre-specified time if the part in test has
not failed yet; so the standardized cumulative exposure time
, where
is the duration of testing under
is
stress . In a linear-stress accelerated life testing, the stress
, so the
amount is a linear function on time, i.e.,
.
standardized cumulative exposure time is
This can be used to model the ramp range when the stress level
is changed from one to another level (e.g., during the process
that the temperature of a furnace is adjusted to a higher level).
denote the integrated standardized time
In general, let
(4)
and the reliability function becomes
(5)
where
. is a decreasing function on
.
In the remaining part of this paper, we will first derive the
upper & lower confidence bounds of the reliability function analytically, then discuss the limiting properties of these bounds,
and finally, illustrate these properties by an example.
III. CONFIDENCE BOUNDS OF RELIABILITY FUNCTION
A. Confidence Bounds Based on Large Sample Normal
Approximation
The confidence bounds on the parameters, and a number of
other Quantities, such as the reliability, and the percentile, can
be obtained based on the asymptotic theory for maximum likelihood estimation (MLE). Denote as the maximum likelihood
estimator of parameter . By the invariance property of MLE, a
function of ML estimators is still a ML estimator of the function, so the MLE of & are
(6)
and

(1)

(7)

and
(2)

Based on the large sample -normal approximation (Meeker
& Escobar [15], Appendix), the upper, and lower limits of
with the confidence level of
are respectively

where is the stress level, and has an inverse power function
on . Parameters , , and are unknown. Combining (1) & (2),
the reliability function can be written as

(8)

(3)

and
(9)

ZHAO et al.: PROPERTIES OF CONFIDENCE BOUNDS ON RELIABILITY ESTIMATION FOR PARTS UNDER STRESS

where
is the upper percentile point of a standard -normal
is the asymptotic variance of the ML
distribution, and
estimator of . Here, is a function on parameter vector ,
where
. Therefore
(10)
and
i.e.,

is a variance-covariance matrix of the three parameters,

(11)

9

Equation (7) is the reliability estimation of a part subject to
varying stresses. The impact of stress is explicitly modeled into
the formula by continuously shrinking or expanding the part’s
in-use time comparing to a natural reference stress level. Accordingly, (19), and (20) are respectively the upper, and lower
confidence bounds of the reliability estimation of a part under
the natural reference stress level.
B. Confidence Bounds Based on the Likelihood Ratio
Another way to derive the confidence bounds of the reliability
function is to apply the likelihood ratio (LR) distribution approximation. We treat the likelihood ratio as a function on
(21)

After defining
(12)
it is easy to show that

where
is the likelihood function with three parameters , ,
and ; and , , and are their maximum likelihood estimators.
Parameter is as defined in (6), and it can be used to replace
. Because the log likelihood ratio statistic is approximately
distributed

(13)
(14)
and
(15)
Let
(16)
Then the upper, and lower limits of become equations (17)
and (18), shown at the bottom of the page.
Therefore, the corresponding upper, and lower confidence
bounds on the reliability function are
(19)
and
(20)

(22)
is approximately
the interval over which
confidence interval for . Then, the confithe
dence bounds on the reliability function can be obtained by
(19) & (20). There is no closed form solution available for this
type of confidence bound, and it can only be evaluated numerically; therefore, it is impossible to discuss LR bounds except
by simulation study, which has been done by others (e.g., McSorley, et al. [11]). It is known that LR bounds perform better
than ML-based bounds when the sample size is small. Nevertheless, the purpose of this paper is to explore the property of the
confidence bound derived through -normal approximation analytically, and to show the limitation of this type of confidence
bound.
IV. PROPERTIES OF CONFIDENCE BOUNDS
In this section, we explore some properties, such as monotonicity, and limiting values of upper & lower bounds, of the
reliability confidence bounds developed in Section III-A. The
reliability function is defined as a decreasing function on time,

(17)

and

(18)

10

IEEE TRANSACTIONS ON RELIABILITY, VOL. 55, NO. 1, MARCH 2006

which means the reliability of a product in use will always decrease along the time if the product is not repaired or renewed
during its service period. However, the confidence bound of a reliability estimation could appear nonmonotonic simply because
the failure data obtained from the life testing could be concentrated in a certain time period, and be sparse in other time periods. This becomes quite obvious for a part under varying stress
conditions, such as SSALT. In such a test, if a test unit survives
a pre-specified time range under a certain level of stress, then
it will be tested under a more severe stress condition to further
accelerate its failure possibility. We show the nonmonotonicity
property by examining the limiting values of upper & lower
confidence bounds. An illustrative example is given in the next
section.
Theorem 1: Suppose the initial stress level of the stress condition profile converges to a nonzero constant , i.e.,
and

(23)

then

(24)

where
is a constant between 0 & 1, as shown in the equation
at bottom of the page.
Proof: See Appendix.
Theorem 2: Suppose the final stress level converges to a
nonzero constant
and

(25)

then

Because a reliability function is always a monotone decreasing function from 1 to 0, it is reasonable to think that its
upper & lower bound functions are also monotone decreasing
functions from 1 to 0. However, one can see that under the con, the lower bound value returns
dition
to 0 when time approaches 0, which implies nonmonotonicity
of the bound function. The same property applies to the upper
bound when time approaches , as well. The typical interpretation of the MLE confidence interval is that by repeated
sampling & interval calculations, the true to-be-estimated value
. The
should be covered by these intervals in
results from Theorems 1 & 2 are apparently odd because, by
the definition of reliability, any reliability value cannot go
beyond 0 or 1, while a confidence interval such as (0, 1) means
that the true reliability
that there are still chances
value is beyond these bounds.
This dilemma is caused by 1) the precision of the estimator,
and 2) the ML estimation method itself. As one can see, when
(it could be seen as a signal/noise ratio) is relatively
), the ill-conditioning of both upper &
low (i.e., less than
lower bound functions appears. One remedy could be increasing
the life test sample size to better estimate , but it may not always be possible. Another remedy could be reducing the conis avoided. In other
fidence level such that
words, the confidence level of estimation could not exceed a certain value when life testing data are limited. This scenario will
be illustrated by an example in the next section. Due to the limited sample size & range of the failure time of these samples,
reliability estimation using the ML method may be only applicable to ascertain time frame.
, i.e., the failure disConsider a special case where
tribution function is exponential, then the limiting bounds
need not be eswill become well-conditioned. Because
timated,
, and
. According to Theorems 1 & 2
(27)

(26)
and
Proof: See Appendix.
When a constant stress is applied, Theorems 1 & 2 imply that
there exists similar but reversed values for lower bound
at
at time , of the estimated relitime 0, and upper bound
ability. For both limiting bounds, their values vary between 0
& 1. This change depends on the pre-specified confidence level
, and the shape parameter of the Weibull distribution only, while the parameters in the inverse power law have
no effect on limiting bound conditions. When a step-stress profile is applied, the initial & final stresses are normally at nonzero
stress levels; therefore, these theorems (with different values of
) are applicable to this case.

(28)
Therefore, when is known, the problem associated with the
limiting confidence bounds will be avoided.
Theorem 3: Let the initial stress level converge to 0
(29)
and the first derivative of the stress function on the initial time
is not zero
and

(30)

ZHAO et al.: PROPERTIES OF CONFIDENCE BOUNDS ON RELIABILITY ESTIMATION FOR PARTS UNDER STRESS

then

11

TABLE I
FAILURE DATA OF EXAMPLE 1

(31)

where

is a constant between 0 & 1

and

Proof: See Appendix.
Theorem 4: Let the final stress level converge to infinity
(32)
and the first derivative of the stress function at the infinity time
is not zero
(33)
then

(34)

Proof: See Appendix.
Suppose the normal in-use stress level is set to be , and
during life testing the stress level is linearly increased, i.e.,
; then, this stress model is called a progressive
stress model, as first investigated by Yin & Sheng [10]. Hypothetically, when time approaches infinity, the stress level also
approaches infinity. Similar to our previous discussion, Theorems 3 & 4 imply that the reversed values for the limiting upper
& lower bounds are applicable to the reliability of a part under
a progressively increasing stress profile. The nonmonotonicity
. But
of the bound function will emerge when
unlike the previous case, is a function on both , and .
Unlike the previous case, the exponential distribution
cannot avoid the dilemma such as
or
. Because
, according to Theorems 3 & 4

(35)

and

(36)

where

.

V. ILLUSTRATIVE EXAMPLE
The following data comes from an accelerated life test where
multiple stresses were applied simultaneously to a particular
automotive part to precipitate failures more quickly than they
would occur under normal in-use condition. The engineer responsible for the test was able to quantify the combination of
applied stresses in terms of a “percentage stress,” as compared to
the typical stress level. In this scenario, the typical stress (field &
use stress) was defined as 100%, and any combination of the test
stresses was quantified as a percentage over the typical stress.
For example, if the combination of stresses on test was determined to be two times higher than the typical condition, then
the stress on test was said to be at 200%. The test was set up
& run as a step-stress ALT, and the time on test was measured
in hours. The step-stress profile used was as follows: until 200
hours, the equivalent applied stress was 125%; from 200 to 300
hours, it was 175%; from 300 to 350 hours, it was 200%; and
from 350 to 375 hours, it was 250%. The test was terminated
after 375 hours, and any units that were still running after that
point were right-censored (suspended).
The test was conducted, and the times to failure & times to
suspension under the above stated step stress profile were given
in Table I. We are interested in the reliability function of the
part under its normal in-use condition (100% stress, a constant
profile), and the associated confidence interval with a certain
confidence level.
By assuming a Weibull failure time distribution, and
applying MLE, the ML estimates of those parameters are
,
,
, and
. At a 90% confidence level,
,
and
, which means that the nonmonotonicity will appear on both upper bound, and lower bound
functions. Fig. 1 illustrates this scenario. Here, the estimated
reliability function is for a part under normal in-use stress;
therefore, the stress profile is a constant line (a special case of
varying stress). At about 200 hours & less, the lower bound
value begins to decline; and at about 270 hours & more, the
upper bound value begins to increase.
To avoid reliability confidence bounds with very uneven sides
at a 90% confidence level, we need
,
i.e.,
. Because the variance of the ML estimator

12

IEEE TRANSACTIONS ON RELIABILITY, VOL. 55, NO. 1, MARCH 2006

Fig. 1. Reliability function under normal in-use stress, and its 90% confidence
bounds.

Fig. 2. Reliability function, and its 66.69% confidence bounds.

is approximately proportional to the inverse of the number of
value of 6.2934, and
failures, , based on the previous
number of failures of 8, we need
in order to have
. Under the current testing profile, the estimated integrated standardized time is
.
The estimated probability of a specimen survived at the test
terminal time is
, which means we need at least 33
specimens so as to expect 24 of them will fail by the end of the
test. Realize that this sample size calculation is only a rough
procedure. But from the practical point of view, it provides
engineers a minimum number of specimens required to avoid
the bizarre behavior of normal approximation-based reliability
confidence bounds at a 90% confidence level.
When the confidence level reduces to 66.69%, then
. The upper & lower bound will
when time approaches
converge to a certain value
infinity or zero respectively. When the confidence level reduces
to 60%, then the limiting upper & lower bounds will be well
behaved, i.e.,
and
. Figs. 2 & 3
demonstrate these two scenarios.
VI. CONCLUSION
A part could experience a variety of stresses during its
lifetime; however, the lifetime data analysis for the part under
general time-varying stresses has not been adequately researched before. A typical example is the varying-stress ALT,
which has become increasingly popular in today’s industry due
to the need to obtain product lifetime data quickly. However,
inadequate research limits further development, such as sample
size determination, and optimal testing plan. In this paper, we
explicitly model the stress damage to a product’s lifetime into
its reliability estimation when the stress profile is known. We
derive the reliability confidence bounds based on large sample

Fig. 3.

Reliability function, and its 60% confidence bounds.

-normal approximation, and prove the nonmonotonicity property of these bounds by examining their limiting values. A stepstress accelerated life testing example is used to illustrate these
interesting properties, which provides the insights of the limitation of the current test, and how to better design the test.
APPENDIX
Proof of Theorem 1
;
. The remaining parameters
in (17) do not depend upon time. Therefore
If

where

, then

, and

is shown in equation (37) at bottom of the next page.

ZHAO et al.: PROPERTIES OF CONFIDENCE BOUNDS ON RELIABILITY ESTIMATION FOR PARTS UNDER STRESS

As
constants, then
Then

and the remaining members of (37) are
.

There are three cases to be discussed:
1) If
, or

13

Then

, then

, and
and
2) If

, or

, then

, and

3) If
formula

This gives
, or

, applying

(38)
we have
and

.

Proof of Theorem 2
, and
;
. The remaining parameters
in (18) do not depend on time. Therefore
If

, then

where
where is the same as in (37).
Then

1) If

, or

, then

(39)

(37)

14

IEEE TRANSACTIONS ON RELIABILITY, VOL. 55, NO. 1, MARCH 2006

and

Proof of Theorem 3
, then

If
,

2) If

, or

, and

. Because
too.

, then

and

(40)
3) If
(38), we have

, or

, then using

Thus, (17) becomes

where

where
Then

is calculated from (39).
(41)
Because

and

and
using (40), we have

Hence

Then

1) If

and

.

ZHAO et al.: PROPERTIES OF CONFIDENCE BOUNDS ON RELIABILITY ESTIMATION FOR PARTS UNDER STRESS

or

15

3) If

or
then

then, using (38), we have

and

2) If

where

is shown in the equation at the bottom of the page.

This gives
of the page.
Then

shown in the equation at the bottom

Let

or

then

and
and

.

16

IEEE TRANSACTIONS ON RELIABILITY, VOL. 55, NO. 1, MARCH 2006

Proof of Theorem 4
, then

If

or
, and

,

. Because
too.
then

and

2) If
(42)
Thus, (18) becomes
or
where

is the same as in (41). Because

then

and

and

3) If
this gives

or

Hence,

then, using (38), we have the equation shown at the bottom
of the page.
Let
1) If

where

ZHAO et al.: PROPERTIES OF CONFIDENCE BOUNDS ON RELIABILITY ESTIMATION FOR PARTS UNDER STRESS

This gives the equation shown at top of the page.
Then,

and

.
REFERENCES

[1] M. H. DeGroot and P. K. Goel, “Bayesian estimation and optimal
designs in partially accelerated life testing,” Naval Research Logistics
Quarterly, vol. 26, pp. 223–235, 1979.
[2] W. Nelson, “Accelerated life testing-step-stress models and data analysis,” IEEE Transactions on Reliability, vol. R-29, no. 2, pp. 103–108,
1980.
[3]
, Accelerated Testing: Statistical Models, Test Plans and Data Analysis. New York: John Wiley and Sons, Inc., 1990.
[4] R. Miller and W. B. Nelson, “Optimum simple step-stress plans for accelerated life testing,” IEEE Transactions on Reliability, vol. R-32, pp.
59–65, 1983.
[5] G. K. Bhattacharyya and Z. Soejoeti, “A tampered failure rate model for
step-stress accelerated life test,” Communications in Statistics: Theory
Method, vol. 18, pp. 1627–1643, 1989.
[6] G. K. Bhattacharyya, Parametric Models and Inference Procedures for
Accelerated Life Tests, 1987.
[7] W. Zhao and E. Elsayed, A general accelerated life model for step-stress
testing, 2003.
[8] R. Pan and S. Ayala, “Two statistical models for step-stress accelerated
life test analysis,” in 47th Fall Technical Conference of ASQ and ASA,
2003.
[9] A. Mettas and P. Vassiliou, “Modeling and analysis of time-dependent
stress accelerated life data,” in Annual Reliability & Maintainability
Symposium, Seattle, 2002.
[10] X. Yin and B. Sheng, “Some aspects of accelerated life testing by
progressive stress,” IEEE Transactions on Reliability, vol. R-36(1), pp.
150–155, 1987.
[11] E. O. McSorley, J. C. Lu, and C. S. Li, “Performance of parameter estimates in step-stress accelerated life-tests with various sample sizes,”
IEEE Transactions on Reliability, vol. 51, pp. 271–277, 2002.
[12] S. A. vander Wiel and W. Q. Meeker, “Accuracy of approx confidence
bounds using censored Weibull regression data from accelerated life
tests,” IEEE Transactions on Reliability, vol. R-39(3), pp. 346–351,
1990.
[13] J. R. van Dorp and T. A. Mazzuchi, “A general Bayes Weibull inference
model for accelerated life testing,” in Reliability Engineering and System
Safety, 2005.

17

[14] P. Erto and M. Giorgio, “Assessing high reliability via Bayesian approach and accelerated tests,” Reliability Engineering and System Safety,
vol. 76, pp. 301–310, 2002.
[15] W. Q. Meeker and L. A. Escobar, Statistical Methods for Reliability
Data: John Wiley & Sons, Inc., 1998.

Dr. Wenbiao Zhao is a manager in the modeling group at American Express;
he received his Ph.D. in Industrial and Systems Engineering from Rutgers, the
State University of New Jersey in 2003; his dissertation focuses on Accelerated
Life Testing Modeling, Planning, and Optimization. He also received his M.S.
(1994), PhD (1997) in Automatic Control (Tsinghua University, Shanghai Jiao
Tong University) in China, and M.S. in Statistics (2002) & M.S. (2002) in Industrial and Systems Engineering from Rutgers University. He is a recipient of the
2005 W. J. Golomski Award for the outstanding paper. He has published many
papers in H-infinity Control, Structure Singular Value Theory, Neural Network,
and Reliability Engineering & Risk Management. He is a member of IIE, IEEE,
and INFORMS.

Dr. Rong Pan is an Assistant Professor of Industrial Engineering at the University of Texas at El Paso. He graduated with a Ph.D. in IE from Penn State University in 2002. He received his bachelor degree in engineering from Shanghai Jiao
Tong University in 1995, and M.S. degree in IE from Florida A&M University
in 1999. His current research interests include quality & reliability engineering,
multivariate quality control for short-run manufacturing processes, and supply
chain management for short life-cycle products. He is a member of INFORMS,
IIE, and ASQ.

Dr. Alexander Aron is the Research Scientist at ReliaSoft Corporation. He received his Ph.D. (1994) in Saratov Technical State University (Russia). He also
received his M.S. (1984) in Mechanics, and in Math in the Saratov State University (Russia). He fills a critical role in the advancement of ReliaSoft’s theoretical
research efforts & formulations in the subjects of Life Data Analysis, Accelerated Life Testing, and System Reliability and Maintainability.

Mr. Adamantios Mettas is the Senior Research Scientist at ReliaSoft Corporation. He holds a B.S. Degree in Mechanical Engineering, and an M.S. degree in
Reliability Engineering from the University of Arizona. He has played a key role
in the development of ReliaSoft’s software including Weibull++, ALTA, and
BlockSim; and has published numerous papers on various reliability methods.

Reliability Engineering and System Safety 138 (2015) 145–153

Contents lists available at ScienceDirect

Reliability Engineering and System Safety
journal homepage: www.elsevier.com/locate/ress

Enhancing product robustness in reliability-based design optimization
Xiaotian Zhuang a, Rong Pan b,n, Xiaoping Du c
a
b
c

Amazon, China
Arizona State University, United States
Missouri University of Science and Technology, United States

art ic l e i nf o

a b s t r a c t

Article history:
Received 4 June 2014
Received in revised form
23 January 2015
Accepted 30 January 2015
Available online 10 February 2015

Different types of uncertainties need to be addressed in a product design optimization process. In this
paper, the uncertainties in both product design variables and environmental noise variables are
considered. The reliability-based design optimization (RBDO) is integrated with robust product design
(RPD) to concurrently reduce the production cost and the long-term operation cost, including quality
loss, in the process of product design. This problem leads to a multi-objective optimization with
probabilistic constraints. In addition, the model uncertainties associated with a surrogate model that is
derived from numerical computation methods, such as ﬁnite element analysis, is addressed. A
hierarchical experimental design approach, augmented by a sequential sampling strategy, is proposed
to construct the response surface of product performance function for ﬁnding optimal design solutions.
The proposed method is demonstrated through an engineering example.
& 2015 Elsevier Ltd. All rights reserved.

Keywords:
Kriging metamodel
Robust design optimization
Sequential optimization and reliability
assessment
Sequential sampling

1. Introduction
Uncertainties that exist in a product's design-manufacturingusage life-cycle need to be addressed at the product's early design
stage. The traditional reliability-based design optimization (RBDO)
method deals with the uncertainties in product design variables by
treating them as random variables and by formulating a probabilistic constraint on product performance function so as to satisfy its
reliability (or safety) requirement. Although accounting for environmental noise and its effects on product quality has been a part
of the RBDO methodology, this issue was often ignored in previous
case studies. On the other hand, the theory of robust product
design (RPD) explores the interaction between noise variables and
design variables for the reduction of the total variance of product
performance. In this paper, these two perspectives of product
design optimization are integrated into a unifying design framework and, subsequently, an efﬁcient computational strategy for
generating good design candidates is proposed.
Our research contributions consist of the following components: ﬁrst, we deﬁne the robustness of a product from the quality
engineer's perspective; that is, it is the variance of product
performance due to environmental noise. A multi-objective optimization framework is proposed for combining the considerations
of product quality and product reliability (or safety) in the product

n

Corresponding author.
E-mail address: rong.pan@asu.edu (R. Pan).

http://dx.doi.org/10.1016/j.ress.2015.01.026
0951-8320/& 2015 Elsevier Ltd. All rights reserved.

design optimization process. Second, as product performance is
often evaluated by a computer model in modern design, we
assume that the product performance function is implicit and
propose a hierarchical experimentation method for constructing
the metamodel of product performance function at different noise
levels. This is a response surface approach to robust design;
however, it is applied on computer experiments, instead of
physical experiments. Third, a sequential sampling technique is
developed for searching the robustness-oriented most probable
design points. Our proposed algorithm is able to explicitly incorporate the modeling error of metamodel and the random effect
from noise variables into the product design optimization process
using a small number of experimental runs.
In the remainder of this section, the basic concepts of RBDO
and RPD are introduced and the different perspectives of these
two design philosophies are discussed. Through an example, we
demonstrate the beneﬁts of combining these two philosophies for
product design. In Section 2 a new unifying framework of design
optimization with the considerations of both product reliability
and product robustness is proposed. We investigate the problem
of using a surrogate model, or metamodel, to approximate product
performance function that can only be evaluated by computer
experiments. This problem is often met in complex engineering
designs, especially at a product's early design stage. To solve the
optimization problem, we derive a hierarchical experimentation
and sequential sampling strategy for updating the metamodel,
which are presented in Section 3. The solution of the illustrative

146

X. Zhuang et al. / Reliability Engineering and System Safety 138 (2015) 145–153

example is thoroughly discussed in Section 4. Finally, the paper is
concluded in Section 5.

1.1. Reliability-based design optimization
RBDO concerns with the effects of random design variables and
noise variables on product performance and product's production
cost. It requires the product performance function to satisfy a
performance criterion that is derived from the product's reliability
or safety concern. As both design variables and noise variables are
random, this performance requirement is formulated as a probabilistic constraint, which is referred as the reliability constraint.
The objective function in RBDO is the mean production cost, which
includes, e.g., material cost, manufacturing cost, labor cost, etc. A
generic model of RBDO is as follows:
Minimized;μx
Subject to

E½f ðd; x; pÞ
Pr½Gi ðd; x; pÞ Z 0 Z Ri ;

d rd r d ; μLx r μx r μU
x
L

U

i ¼ 1; 2; …; m
ð1Þ

In (1) d denotes a vector of deterministic design variables, x
denotes a vector of stochastic design variables, and p denotes a
vector of noise variables. A stochastic design variable is controllable in the sense that its mean value can be speciﬁed by the
designer even if its variance may not be reduced, due to, for
example, natural variations in materials and in manufacturing. A
noise variable is only observable, but not controllable. The function, f ðd; x; pÞ, is a production cost function. Due to the random
nature of design variables and noise variables, the objective
function of RBDO is the mean cost function E½f ðd; x; pÞ, which is
usually replaced by its ﬁrst-order Taylor expansion approximation,
f ðd; μx ; μp Þ. The function Gi ðd; x; pÞ in the probabilistic constraint in
Model (1) is one of the product's performance functions. To deﬁne
a reliable product, it requires Gi Z 0. Thus, Gi o 0 represents the
failure of product's ith function and Gi ¼ 0 deﬁnes a limit-state
surface, which is the boundary between success and failure. The
inequality constraint Pr½Gi Z0 Z Ri deﬁnes the product's reliability
level to be larger or equal to the targeted reliability level, Ri.
From a designer's perspective, the main task of RBDO is to keep
the designed product safe or reliable with minimum production
cost. Therefore, the right hand side of the constraint function in
Model (1) speciﬁes the required probability of an acceptable
product performance. The computational work involved in solving
an RBDO problem is dominated by the evaluation of this probabilistic constraint function. The methods developed for solving an
RBDO problem include double-loop methods [1], decoupled-loop
methods [2,3] and single-loop methods [4–6]. Oftentimes in
practice the effects of noise variables in RBDO on the robustness
of performance functions are ignored or are treated as pure
additive variability, thus having no impact on the problem solution. However, two issues arise when ignoring noise variables [7]:
the design feasibility could be in doubt because the effect of extra
variation from noise variables may lead to the shrinkage of feasible
region, and more importantly, the transmitted variation from
noise variables to the performance function can result in the
deterioration of product quality. From a quality engineer's perspective, reducing variance is the ultimate goal of product quality
control, as a large variance will lead to large potential cost, such as
warranty/repair cost, which is the long-term cost of qualityrelated deﬁciencies. In order to reduce the impact of noise
variables on both product quality and design feasibility, a robust
design process needs to be implemented.

1.2. Robust product design
Robustness is the state where a product or process' performance is less sensitive to the factors that may cause variation [8].
Robust product design is an approach for improving the quality of
a product by minimizing the effect of the cause of variation
without eliminating the source of variation [9]. Noise variables
formulated in RBDO are uncontrollable, but their effects on
product performance can be mitigated by adjusting controllable
design variables. The solution is made possible by exploiting the
interaction between design variables and noise variables. In Model
(1), however, the robustness of product performance is not directly
investigated. It is commonly seen in the literature that the noise
variable is either directly replaced by its mean value (0) or it is
simply treated as same as another random decision variable. Some
previous studies deﬁned the product robustness as the variability
of the production cost function due to these random variables
(see, e.g., [4,10,11]). This is different from our study where the
robustness is deﬁned as the sensitivity of product performance
function to noise variables.
To understand the effects of model uncertainty in robust
design, Apley [12] assigned normal distributions to noise variables
and proposed a Bayesian framework for quantifying the impact of
interpolation uncertainty on the robust design objective. Rangavajhala et al. [13] examined the challenge of equality constraints in
robust design optimization. Zaman et al. [14] analyzed the impact
of non-design epistemic variables on robustness-based design
optimization. Tang [15] developed a feasible robustness index
and integrated it into the RBDO formulation. Some authors
considered the worst case analysis and the moment matching
method for robust design [7–9]. For example, Xu [16] employed
the worst case analysis of maximum design parameter deviation
and proposed a robust design model based on maximum variation
estimation.
Taguchi's method is one of the most popular methods
employed in robust design. Based on different phrases of design
process, Taguchi provided a three-stage process [17]: system
design, parameter design, and tolerance design, while parameter
design is arguably the most important one. During a parameter
design, product design parameters are optimized for improving
product quality. Unlike the ordinary design optimization, Taguchi's
parameter robust design accounts for the product performance
variation due to noise factors. Suppose Gðx; pÞ is a performance
function, where x and p are controllable variables and noise
variables, respectively. A signal-to-noise ratio (SNR), as a measure
of quality loss, is deﬁned as follows:
SNR ¼  10 log ½MSD
ð2Þ
Pk
where MSD ¼ ð1=kÞ i ¼ 1 ½Gðxi ; pi Þ  GT 2 is the sum of mean
square deviations of the performance function from its targeted
value. The function Gðxi ; pi Þ denotes the product performance of a
single sample. In order to characterize this function, the techniques of design of experiments (DOE) are employed. By evaluating
different designs based on experimental results, the best combination of control factors is found. However, the orthogonal array
designs proposed by Taguchi are deﬁned in discrete space and
they cannot be easily extended to a wider design range. In
addition, it is not an efﬁcient method for a problem with a large
set of experimental factors [18].
An alternative approach is called robust design optimization
(RDO), which directly minimizes the variance of product performance function by exploiting interaction effects of design variables
and noise variables. A generic form of RDO model is given by
Minimize

Var½Gðd; x; pÞ

Subject to

E½Gðd; x; pÞ ZGT

X. Zhuang et al. / Reliability Engineering and System Safety 138 (2015) 145–153
L

U

d rdrd ;

μLX r μX r μUX

ð3Þ

Note that the variance function in (3) is used as a proxy measure of
product quality loss function. Some other measures of robustness
have been proposed in the literature. For example, a performance
percentile difference method was used in [19]; a robust index
derived from the acceptable performance variation was developed
in [20]; a coefﬁcient of variation measure was given in [21].
1.3. Impact of ignoring robustness in design process
An I-beam example is used in this section to demonstrate the
importance of considering the robustness of product performance
in the product design optimization process. The details of this
analysis will be presented in Section 4.
I-beams are commonly used in the construction industry as
major support trusses to ensure a structure to be physically sound.
To maintain structural safety, I-beams are designed to withstand a
great amount of weight. In Fig. 1, two variables x1 and x2 deﬁne the
cross-section of the I-beam. These variables are random variables
and they are assumed to follow normal distribution Nðμi ; σ 2i Þ, i ¼ 1,
2, where μi's are to be designed and σ 1 ¼ 2:025 cm, σ 2 ¼ 0:225 cm.
The variabilities of X1 and X2 are due to, for example, manufacturing variance. In addition, the vertical load, P, is a random variable,
but it is an uncontrollable noise variable as it represents the
uncertainty in the beam's working environment. We let it follow a
normal distribution with μP ¼ 600 kN and σ P ¼ 10 kN. In this
example the lateral load Q is assumed to be a constant, 50 kN.
Two objective functions are considered in the I-beam example.
As formulated in Section 4, the ﬁrst objective is to minimize an
upper percentile value of the beam's material cost, which is the
objective of RBDO. Given the ﬁxed beam length and constant
material density, minimizing the material cost is equivalent to
minimizing the area of the beam's cross-section; thus, the objective function is deﬁned as f ðμ1 ; μ2 Þ þ cV f ðμ1 ; μ2 Þ, where f ðμ1 ; μ2 Þ is
the mean cost function and V f ðμ1 ; μ2 Þ is the variance of cost
function. The second objective is to minimize the variance of the
performance function Gðx1 ; x2 Þ, which represents the quality loss
of the beam, and this is the objective of robust design. One
probabilistic constraint considered in this example is P½Gðx1 ;
x2 Þ Z 0 Z R, where Gðx1 ; x2 Þ is the beam's performance function
deﬁned by the allowable stress s¼0.016 kN/cm2 subtracting the
actual bending stress.
Two alternative designs are presented in Table 1. While the ﬁrst
design considers only the material cost objective, the second design
has both material cost and quality loss objectives addressed. Note that
the absolute values of material cost and quality loss cannot be
compared directly, as their cost coefﬁcients are unknown. However,
from the last two columns of Table 1 one may see the percentage of
change in these two types of cost. When the robustness objective is
not considered, the quality loss in Case 1 is almost ﬁve times larger

147

than that in Case 2. The solution in Case 2 proposes a better trade-off
between material cost and quality loss, since it achieves 79.26%
deduction in quality loss with only 2.56% increase in material cost. A
large quality loss will either increase repair/warranty cost or lead to
customer dissatisfaction and market share loss. It is, therefore, invaluable to integrate the robust design theory into the RBDO design
process. It is the purpose of this paper.
Some recent publications have proposed the reliability-based
robust design optimization (RBRDO) to tackle the problems of
reliability constraint and robustness simultaneously (see, e.g.,
[22,15]). Most of these publications attempted to reduce the
variance of cost function, f ðd; x; pÞ, in Model (1) [23,4,21,10,24].
Accordingly, the objective function can be either formulated as
multi-objective optimization or a single objective as a linear
combination of both mean and variance of cost function. Reducing
the variance of cost function is clearly important to product
designers, but to quality engineers the quality loss refers to the
potential cost due to the product-to-product variation in its
performance, which is a long-term cost that could be evaluated
over the product's whole life cycle. Yadav et al. [22] assigned a
targeted value to each product performance (quality characteristic) and formulated a quality loss function (QLF). They optimized
product designs by minimizing a weighted sum of QLFs. In this
paper, we will directly use the variance of performance function as
one of our objective functions. One of the issues with the QLF
approach is that in some design cases the appropriate target value
for the mean or variance of performance function may not be
found easily and inappropriate target values may result in infeasible solution. In addition, we consider an implicit performance
function; that is, the product performance is evaluated by a
computer model, which is often required in complex system
design. Therefore, we take a response surface approach to modeling the performance function and optimizing the robustness of
performance function. Tang et al. [15] studied the effect of product
performance variance on RBDO solutions and they formulated a
constraint function for the robustness requirement. In contrast,
our approach directly formulates the variance of performance
function as another objective function; thus, this formulation has
a straightforward interpretation of the dual objectives that are of
concerns by product designers and quality engineers.

2. A unifying design optimization framework
As mentioned in Section 1, when the design objective is solely
on minimizing production cost, it misses the opportunity of
Table 1
Two solutions for I-beam design.
Case

μ1

μ2

Material cost

Increase%

Quality loss

Decrease%

1
2

50.24
51.86

0.91
0.91

169.65
174.00

–
2.56

1.88
0.39

–
79.26

P

L
L/2

Q

X1

X2

X2

L/2

X1
Fig. 1. I-beam example.

Fig. 2. Noise variable impact on performance function.

148

X. Zhuang et al. / Reliability Engineering and System Safety 138 (2015) 145–153

reducing quality loss through minimizing the variance of product
performance. Two hypothetical product design solutions are
shown in Fig. 2, in which the product performance G versus a
noise variable p is plotted. Both design variables μ1x and μ2x can
achieve the expected minimum function value; however, the
performance of the second solution is less sensitive to the noise
variable p. This implies less long-term product quality-related cost.
The goal of robust design is to ﬁnd a set of decision variables, d and
μx , such that the mean product performance can satisfy the
designer's requirement and the variability induced by the noise
variables can be minimized. Integrating two major design aspects
– product reliability and product robustness – becomes possible
when the product designer acknowledges that the transmitted
variance from noise can be attenuated by the interaction of design
variables with noise. Therefore, a uniﬁed design optimization
formula is proposed as
Minimize

E½f ðd; x; pÞ
X
wi Var½Gi ðd; x; pÞ

Minimize

i

Subject to
L

Pr½Gi ðd; x; pÞ Z 0 Z Ri
U

d rd r d ;

μ

L
xr

i ¼ 1; 2; …; m

μx r μ

U
x

ð4Þ

In this formulation, the ﬁrst objective aims to minimize the
expected production cost function and the second objective aims
to minimize the weighted sum of transmitted product performance variation Var½Gi ðd; x; pÞ. Note thatpthe
ﬁrst objective funcﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
tion can be extended to be E½f ðd; x; pÞ þ c Var½f ðd; x; p, which is a
percentile point of the distribution function of production cost. In
some design literature, the variance of production cost was used
for assessing the robustness of cost function to design variables
(see, e.g., [4,21,23]); however, this type of robustness is different
from the product performance robustness as deﬁned in the second
objective in (4). The variance of product performance is a measure
of product quality, thus it represents the product's quality loss.
Without loss of generality, we assume the mean of a noise
variable being 0. Based on the ﬁrst-order Taylor expansion, the
ﬁrst objective function in (4) is approximated by f ðd; μX Þ, which is
a function of controllable decision variables d and μX . Assuming
that random variables x and p are independent, based on the delta
method [7,8], the second objective function in (4) can be written
as
!2
2 X
np
nx 
X
∂G
∂G
Var½Gðd; x; pÞ ¼
σx þ
σp
∂xj j
∂pj j
j¼1
j¼1
¼

nx
X
j¼1

½G0xj ðd; μx ; 0Þ2 σ 2xj þ

np
X
j¼1

½G0pj ðd; μx ; 0Þ2 σ 2pj

ð5Þ

where nx and np are the number of random variables and noise
variables, respectively.
Note that we do not assume any particular distributions for x or
p. The reliability of product performance is guaranteed by the
probabilistic constraints in (4), which are evaluated based on the
distribution function of each performance function Gi. In general,
when the interaction between controllable variables, x, and
uncontrollable noise variables, p, does exist in a given performance function, we can take full advantage of this interaction to
reduce the effect of random noise on the variance function by
adjusting controllable variables. With the multi-objective optimization (MOO) formulation in (4), the optimal solution is a Pareto
set or a Pareto frontier that consists of non-dominated objective
values. Finding the Pareto set is a computationally intensive
process; for efﬁciently obtaining a solution, we refer readers to
Lee et al. [25] and its relevant references. In this paper we choose
to use the weighted sum method to solve our MOO problem,
mainly because engineers can easily get insight into the tradeoff

between two objective functions by comparing their weights.
However, this method has its own drawback, as it cannot ﬁnd
the complete Pareto frontier if the frontier is not convex. Other
MOO methods such as the epsilon-constraint method [26] and
NSGA-II [27] can be applied to our problem too, but our initial
results show that they will be very computationally expensive due
to the implicit nature of product performance function. Developing other efﬁcient MOO algorithms for the integrated robust and
reliability-based design optimization problem is one of our future
research.
In many modern engineering tasks, analytical product performance functions may not be available but their numerical models
or surrogate models could be in use. For example, for a mechanical
structure it is common to use a ﬁnite element model (FEM) to
evaluate its stress–strain function. In such case, the performance
function G is an implicit or black-box function. Since the black-box
model is computational expensive, we replace it by a metamodel
^ which is constructed based on samples from
or surrogate model G,
computer experiments. In this paper a Kriging model [28] is
employed as the surrogate model to a FEM. The Kriging predictive
model considers the spatial correlation of design points in the
experimental space, so the response value of a certain design point
depends not only on the setting of design parameters but also on
the response values of its neighboring design points. Comparing
with polynomial models, fewer samples are needed to construct a
robust Kriging model as there are less parameters to be estimated
in a Kriging model. In addition, the Kriging model allows more
ﬂexibility than polynomial models, and it is well known that the
Kriging model provides a good ﬁt to the high nonlinear or twistyshaped response surface.

3. An efﬁcient computational strategy to design optimization
To build the Kriging metamodel for an implicit performance
function, an experimental design is needed for sampling product
performance values from the product's computer model. As the
product's robustness to noise is to be investigated, we propose a
hierarchical experimental design scheme to select initial design
points. Furthermore, in order to obtain an accurate optimal
solution, we take additional samples to update the metamodel
and to improve the design solution iteratively. In Zhuang and Pan
[29], a sequential sampling strategy is proposed for reducing the
metamodeling error and for searching the global optimal solution
simultaneously. In this paper, this sampling strategy is extended to
a more complicated scenario where a multi-objective design
optimization problem needs to be addressed. Our work is different
from [29] in that we propose to use a hierarchical experimental
design framework to formulate separate metamodels of the
performance function at the high noise level and at the low noise
level. MPPs are searched at both levels. The estimated performance model is the interpolation of these two models. Our main
goal is to achieve simultaneous reductions of production cost and
quality loss during the product design optimization process, while
[29] considered the traditional RBDO objective, production cost
reduction, only.
3.1. Hierarchical experimental design
A hierarchical experimental design is proposed in this paper to
build a Kriging model that consists of both random design
variables and noise variables. At the lower level of the hierarchy,
the levels of noise variables are ﬁxed and Latin hypercube
sampling (LHS) is used to construct the experimental design
[30,31]. A Kriging model will be built upon these LHS samples.
The LHS design is a type of space-ﬁlling experimental designs,

X. Zhuang et al. / Reliability Engineering and System Safety 138 (2015) 145–153

which are efﬁcient for building correlation-based models such as
the Kriging model. At the higher level of the hierarchy, the levels of
noise variables are varied according to a two-level factorial or
fractional factorial design with resolution three [32]. Since noise
variables are uncontrollable in the product's use environment, we
focus on exploring the effects of noise variables on product
performance variation, so it is better to set the ranges of noise
variables to their maximum possible values. A two-level fractional
factorial design is suggested when there are more than one noise
variable, in order to reduce the number of runs of the experiment.
In addition, if some prior knowledge of how each noise variable
may affect the product performance is available, these noise
variables can be categorized to the positive effect group and the
negative effect group, respectively, and then only two settings of
these variables are needed. At one run, set all noise variables in the
positive effect group to their highest possible levels and all noise
variables in the negative effect group to their lowest possible level,
and vice versa at the other run.
For the sake of simplicity, we assume that there is one noise
variable and denote its lower and higher levels to be  1 and þ1.
Then, two sets of LHS samples are generated and two Kriging
models are constructed at p ¼  1 and p ¼ þ 1, respectively. By

linear interpolation, the following combined Kriging metamodel is
used to replace the implicit performance function
1 p ^
1þp ^
^
Gðx;
pÞ ¼
ð6Þ
G  ðxÞ þ
G þ ðxÞ;
2
2
where G^  ðxÞ is the Kriging model at the lower noise level and
G^ þ ðxÞ is the Kriging model at the higher noise level. Suppose that
noise variable is distributed as N  ðμP ; σ 2P Þ, we may deﬁne the
lower and higher levels to be μP  2σ P and μP þ 2σ P , respectively. If
a non-probabilistic worst case analysis is in use, we can assign the
lower and higher levels to be μP  ΔP and μP þ ΔP , where ΔP is the
half range of noise levels.
3.2. Sequential sampling strategy in multi-objective design
optimization
Based on the combined Kriging metamodel, a multi-objective
design optimization problem becomes
qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Minimize f ðd; μX Þ þ c V f ðd; μx Þ
Minimize

V G^ ðμX Þ

Subject to

^
Pr½Gðd;
x; pÞ Z 0 Z R

L

U

d r d rd ;

Fig. 3. Algorithm ﬂowchart.

149

μLX r μX r μUX

ð7Þ

^
pÞ, as given by (5).
where V G^ ðμX Þ is the variance function of Gðx;
To balance the computational efﬁciency and the accuracy in
optimal solutions, we choose the sequential optimization and
reliability assessment (SORA) method [3] and extend it to solve
Eq. (20). By SORA, the nested-loop of optimization and reliability
assessment steps are decoupled into two iterative deterministic
optimization processes. The ﬁrst optimization process is to ﬁnd the
optimal design solution, μX , without considering the probabilistic
constraints. Based on this solution, the second optimization process evaluates probabilistic constraints. By the performance measure approach (PMA) [33,34], the second optimization is solved to
ﬁnd the most probable point (MPP) of inverse reliability [35]. Since
MPP is the worst case design point for a given reliability requirement, the design variables μX are feasible if MPP can satisfy the
targeted reliability. Otherwise, the ﬁrst optimization problem
needs to be adjusted accordingly and a new iteration begins.
As the product performance function is approximated by a
combined Kriging metamodel in Eq. (20), theoretically, the more
computer experiment samples are taken, the closer the Kriging
model will be to the true model. In reality, only limited number of
samples can be taken due to the experimental cost; therefore, we
choose a sequential sampling strategy to improve the Kriging
model building and the MPP prediction iteratively. The metamodel
G^ has prediction errors and these errors vary from area to area in
the experimental region, depending on the distribution of sampled
points. The areas with more sampled points will have smaller
prediction errors, while the areas with few sampled points will
have larger prediction errors. So, it is highly possible that the area
with few sampled points has the potential of containing the true
MPP that is better than the current solution. We employ the
expected improvement (EI) criterion, developed in [36,29], to
sequentially select additional samples.
Any multi-objective optimization algorithms could be used to
solve the proposed optimization model of Eq. (20). In this work,
we use the weighted sum approach as a demonstration. The two
objective functions in (20) can be combined to one using weighted
summation, then a Pareto frontier is generated by trying different
weight combinations. The proposed algorithm for solving (20) is
illustrated in Fig. 3 and it is explained below.
1. Assign m different weight combinations (w0, 1  w0 ) to the
production cost objective and the quality loss objective. Under

150

X. Zhuang et al. / Reliability Engineering and System Safety 138 (2015) 145–153

each w0 value, one optimization problem with probabilistic constraints is solved.
2. Following SORA, the stochastic optimization problem is
decomposed to two deterministic problems. The ﬁrst deterministic
optimization problem is as
qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Minimize w0 ½f ðd; μX Þ þ c V f ðd; μx Þ þ ð1  w0 ÞV G^ ðμX Þ
Subject to

k
G^ ðd; xk  1 ; pk  1 Þ Z 0;

1 r p r 1

k
G^  ðd; μX  s  ; pk  1 Þ Z 0

k
G^ þ ðd; μX  s þ ; pk  1 Þ Z0

ð9Þ

where s  and s þ are shifting vectors, as proposed in [3], for
moving the limit state surface of product performance function if
the current optimal solution cannot reach the targeted reliability.
From (5), the variance of product performance is derived as
!2
!2
∂G^
∂G^
V G^ ðμX Þ ¼
j μX ;μP σ 2X þ
j μX ;μP σ 2P
∂x
∂p



2
1p ^ 0
1þp ^ 0 2
1
1
¼
G þ
G þ j μX ;μP σ 2X þ G^ þ  G^  j μX ;μP σ 2P
2
2
2
2

2

2
1^0
1^0
1
1
¼ G  ðμX Þ þ G þ ðμX Þ σ 2X þ G^ þ ðμX Þ  G^  ðμX Þ σ 2P ;
2
2
2
2
ð10Þ
where G^  and G^ þ are built upon the Latin hypercube samples
when p is at the lower and the higher level, respectively.
3. Step 3 involves another optimization, which is used for
reliability assessment. To do it, the original X-space is transformed
to the standard normal variable U-space for the current solution of
d and μX [37], then the MPPs on G^  and G^ þ are found by solving
Minimize

G^  ðuÞ

Subject to

or G^ þ ðuÞ

J u J ¼ βtarget

ð11Þ

where β target ¼ Φ ðRÞ, and Φ  1 is the inverse function of standard normal distribution function. Two parallel optimization
problems are solved under models G^  and G^ þ in this step and
two MPPs, xMPP  and xMPP þ , are derived from the respective
optimization problems. However, G^  and G^ þ may not be accurate
due to the limited sample size, so the EI criterion is employed to
locate additional samples which make the largest expected
improvement around the current MPPs. Similar to the method in
[29], in order to achieve the global minimum, we employ a polar
coordinate system so as to transform the above two optimization
problems to two unconstrained optimization problems such as
1

Minimize

G^  ; þ ðθÞ

and
^
MaximizeðGmin
þ G þ Þ

ð8Þ

k
k
k
where G^ ¼ ðð1  pÞ=2ÞG^  ðxÞ þ ðð1 þ pÞ=2ÞG^ þ ðxÞ, which is the combined metamodel in the kth iteration, and xk  1 and pk  1 are the
MPPs obtained for the second optimization (reliability assessment)
k
k
k
in the ðk  1Þth iteration. If G^  Z0 and G^ þ Z 0, then G^ Z 0 is
guaranteed. Thus, we can reformulate the optimization problem as
follows:
qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Minimize w0 ½f ðd; μX Þ þ c V f ðd; μx Þ þ ð1  w0 ÞV G ðμX Þ

Subject to

These two maximization problems are
!
!
min
min
^
^
^  ÞΦ G   G  þ s  ϕ G   G 
MaximizeðGmin

G

s
s

ð12Þ

The optimal solutions θMPP  and θMPP þ are then transformed back
to be xMPP  and xMPP þ in the X-space. The current minima of G^ 
and G^ þ are obtained and they are added to the original sample
pool to update the Kriging metamodel, G^  and G^ þ , respectively.
4. The EI-based sequential sampling method is detailed in Step 4. In
order to ﬁnd additional sampling points and to decrease the prediction error in the neighborhood of current MPP  and MPP þ , two
maximization problems are solved to locate the samples which make
largest expected improvement on G  and G þ function estimation.

Gmin  G^ þ
Φ þ
sþ

!

!
^
Gmin
þ G þ
þsþ ϕ
;
sþ

ð13Þ

ð14Þ

and Gmin
denote the current minimal performance
where Gmin

þ
function values at the lower and higher noise level, respectively.
The solutions of these two optimization problems are the
sampling points that maximize EIs. They are added into the
original LHS sampling pool to update the metamodel, G^  and
G^ þ . Step 4 is repeated until the maximum EIs of G^  and G^ þ are
both less than a stopping criterion.
5. MPPs are found based on the updated metamodels G^  and
G^ þ . If both G^ MPP  Z 0 and G^ MPP þ Z 0, then the current solution of
d and μX are the desired optimal solution under the weight w0. If
any of G^ MPP  and G^ MPP  is less than zero, the respective shifting
vector s  ¼ μX  xMPP  or s þ ¼ μX  xMPP þ are derived to modify
the deterministic constraints on G^  and G^ þ in Step 2.
6. As mentioned in Step 1, m weight combinations of w0 and
1  w0 are used, and thus m optimal solutions will be generated.
These solutions are ﬁrst compared and added to the Pareto
solution set if they are shown to be non-dominated optimal
solutions, then solutions within the existing Pareto set are
removed if they become dominated. This Pareto solution set
contains the desirable solutions with tradeoffs between reliability
and robustness of product performance function.

4. The I-beam example
This section describes in detail the I-beam example that was
demonstrated in Section 1. Albeit simple in conﬁguration, I-beam
is a critical component in many complex engineering structures
such as airplane wings [38]. The analysis process described in this
section can also be applied on other structures such as the
cylindrical shell structure in [21] or the Ford side impact beam
structure in [39].
The material cost of an I-beam is determined by the area of
cross-section of the beam. From Fig. 1, we obtain the mean and the
variance of the material cost function as
f ðμ1 ; μ2 Þ ¼ 2μ1 μ2 þ μ2 ðμ1  2μ2 Þ ¼ 3μ1 μ2  2μ22

ð15Þ

and
V f ðμ1 ; μ2 Þ ¼



∂f
j μ σ1
∂x1 1

2


þ

∂f
j μ σ2
∂x2 2

2

¼ ð3μ2 Þ2 σ 21 þð3μ1  4μ2 Þ2 σ 22
ð16Þ

The vertical load, P, is an environmental stress variable, which
is a random variable in applications. We set the lower level of this
noise variable to be P¼ 570 kN and the higher level to be
P¼ 630 kN. The beam's bending stress–strain function is evaluated
by a ﬁnite element model for any given values of X1, X2 and P. The
threshold value of bending stress is given by s ¼0.016 kN/cm2. We
consider the bending stress function to be the beam's performance
function and this function is implicit, as it is evaluated by a
computer model. Therefore, in this example, a hierarchical experimental design is applied to obtain the initial LHS samples of X1 and
X2 at the lower and higher levels of the noise variable. Based on
the initial samples from the computer model, a combined metamodel G^ is constructed as the following:
630  p ^
p  570 ^
^
G  ðxÞ þ
G þ ðxÞ:
Gðx;
pÞ ¼
60
60

ð17Þ

X. Zhuang et al. / Reliability Engineering and System Safety 138 (2015) 145–153

No. Design
variables

1
2
3
4
5
6
7
8
9
10

Performance

No. Design
variables

x1

x2

G;

32.11
24.74
65.26
10.00
61.58
50.53
54.21
72.63
35.79
57.89

2.63
0.90
2.19
3.71
3.27
5.00
4.14
3.92
3.49
1.12

0.0046
0.0034 11
 0.0332  0.0384 12
0.0131
0.0128 13
 0.1643  0.1825 14
0.0137
0.0135 15
0.0135
0.0132 16
0.0135
0.0132 17
0.0146
0.0145 18
0.0088
0.0080 19
0.0091
0.0145 20

Gþ;

Performance

x1

x2

G;

Gþ;

80.00
28.42
39.47
21.05
76.32
68.95
13.68
43.16
46.84
17.37

3.06
1.76
4.35
4.57
1.33
4.78
1.55
1.98
2.84
2.41

0.0146
 0.0045
0.0111
 0.0062
0.0127
0.0147
 0.1002
0.0084
0.0113
 0.0338

0.0145
 0.0066
0.0106
 0.0084
0.0123
0.0146
 0.1121
0.0077
0.0109
 0.0388

The product quality loss function is represented by the variance
^ which is, according to (5)
function of G,
!2
!2
X ∂G^
∂G^
^ ¼
j μX ;μP σ P
V G^ ðμ1 ; μ2 Þ ¼ VarðGÞ
j μX ; μP σ i þ
∂xi
∂p
i


X 630  μ 0
μ  570 ^ 0 2
P ^
¼
Gi þ j μX σ 2i
Gi  þ P
60
60
i

2

X1 0
1 ^
1
1 0 2
þ
G þ  G^  j μX σ 2P ¼
G^i  þ G^i þ j μX σ 2i
60
60
2
2
i

2
1 ^
1
þ
ð18Þ
G þ ðμi Þ  G^  ðμi Þ σ 2P
60
60
where subscript i is for random variables X1 and X2.
The probabilistic constraint considered in this example is
P½GðX 1 ; X 2 Þ Z 0 Z R, where GðX 1 ; X 2 Þ is the difference of the threshold stress value and the actual bending stress value that is given by
a ﬁnite element model of the beam's bending stress-strain function. Finally, the formulation of design optimization becomes
Minimize :

f ðμ1 ; μ2 Þ þ cV f ðμ1 ; μ2 Þ

Minimize :

V G^ ðμ1 ; μ2 Þ

Subject to :

^ 1 ; X 2 Þ Z 0 Z99:87%
Pr½GðX

10 r μ1 r 80;

0:9 r μ2 r5

ð19Þ

In the ﬁrst objective function, assume c¼1, which is the same
as considering the 84th percentile point of the material cost
distribution. Following the procedure in Fig. 3, assign a series of
weights, (w0, 1  w0 ), to the two objectives and combine them to a
single objective function. A hierarchical design is employed and 40
samples are generated as shown in Table 2 to build the metamodel
^ Then, the deterministic optimization problem (9) can be solved
G.
by a stochastic optimization algorithm, such as genetic algorithm
(GA). In this example, we use the GA with 100 initial population
and 10 iterations to obtain the optimal decision variables μ1 and
μ2 . The reliability assessment is implemented for the implicit
performance functions, G  and G þ , at lower and higher noise
levels. We set the stopping criterion of the EI-based sequential
sampling strategy to be max EI o 0:05. Once both MPP  and
MPP þ satisfy the reliability requirement, the optimal solution
ðμ1 ; μ2 Þ is considered as a Pareto optimal solution candidate and
the algorithm enters the next iteration with a new set of weights.
To balance the numerical values of material cost and quality loss,
the quality loss objective is multiplied by 105 to keep two
objectives on a similar numerical scale in this example. A set of
non-dominated solutions are shown in Table 3.
The optimal solution of this example is a Pareto frontier,
instead of a single optimal point as for a traditional RBDO solution.
As indicated in Table 3, when the weight, w0, equals to 1, the

robustness objective is ignored and the traditional RBDO optimal
solution of (μ1, μ2) is obtained as ð50:24; 0:91Þ with a material cost
of 169.65 and a quality loss of 1.88. When the weight value is
altered, other non-dominated solutions can be obtained. As
expected, the material cost will increase from the traditional RBDO
solution, but the quality loss will decrease. For the purpose of
comparison, the percentages of increase or decrease from the
baseline value of the traditional RBDO solution are given in
Table 3. Considering the tradeoff between material cost and
quality loss, we determine that the optimal solution ð51:86; 0:91Þ
is a desirable solution because, in this case, a 2.56% increase in
material cost is offset by a 79.26% decrease in quality loss.
In order to show the effect of taking into account of product
performance robustness in the design optimization process, we
plot the response surfaces of the performance function given by
the Kriging model at the lower and higher noise level in Figs. 4 and
5, respectively. These plots correspond to the weight value of 0.43,
which produces the desired optimal solution.
The initial LHS samples and the augmented samples are
marked on the surfaces in these ﬁgures. A cross mark denotes an
initial sample and a circle or square mark denotes an additional
sample that was derived from the EI-based sequential sampling
method. The diamond mark indicates the optimal solution. One
can see that although the initial samples are scattered in the
design region, those additional samples are taken close to the
optimal solution so that in the neighborhood of optimal solution
the performance function can be estimated more precisely. The
optimal solution is selected in the area where the response surface
is relatively ﬂat, so the response is insensitive to the variation of
design variables X1 and X2. When these two response surfaces are
stacked on each other, as shown in Fig. 6, one can see that the
Table 3
Optimal solutions for I-beam design.
w0

μ1

μ2

Material
cost

Var.
cost

Mean
cost

Increase Quality
loss

Decrease

1.00
0.69
0.65
0.63
0.43
0.19
0.13
0.05
0.02
0.00

50.24
49.94
51.41
49.77
51.86
48.45
54.33
53.07
47.86
36.96

0.91
0.93
0.91
0.94
0.91
1.02
0.93
0.98
1.29
2.47

169.65
171.44
172.70
173.11
174.00
178.28
185.72
190.11
215.10
288.85

33.55
33.35
34.33
34.24
34.63
32.38
36.28
35.44
32.12
27.23

136.10
138.09
138.37
138.87
139.37
145.90
149.44
154.67
182.98
261.62

–
1.06%
1.80%
2.04%
2.56%
5.09%
9.47%
12.06%
26.79%
70.26%

–
72.34%
74.47%
76.06%
79.26%
79.79%
82.45%
87.77%
92.55%
95.21%

1.88
0.52
0.48
0.45
0.39
0.38
0.33
0.23
0.14
0.09

0.05
0
−0.05
G−

Table 2
Initial samples by hierarchical experimental design.

151

−0.1
−0.15
−0.2
6
4
X2

2
0

0

20

Fig. 4. G  and sequential samples.

60

40
X1

80

152

X. Zhuang et al. / Reliability Engineering and System Safety 138 (2015) 145–153

point on the new limit state surface, because the reliability index β
is equivalent to the target reliability. Then, the single-loop method
solves a deterministic optimization problem by replacing random
variables, x and p, by their means, μx and μp . Thus, our MOO
problem becomes
qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Minimize f ðd; μx Þ þ c V f ðd; μx Þ

0.05

G+

0
−0.05
−0.1
−0.15
−0.2
−0.25
6

V G^ ðμx Þ

Subject to

~
μx ; μp Þ Z0
Gðd;

L

4
2

X2

0

0

60

40

20

80

U

d r d rd ;

μLx r μx r μUx

ð20Þ

~
μx ; μp Þ is the deterministic constraint converted from
where Gðd;
the probabilistic constraint by a moving approach in the single
~
loop method and Gðd;
μx ; μp Þ ¼ Gðd; μx þ σ x βt; μp þ σ p βtÞ. Here, t is
the gradient moving direction used in the single loop method and
each component tj is derived by taking partial derivative of the
performance function with respect to the corresponding random
variable xj.
The I-beam example was solved again using the above formulation and the results are presented in Table 4. Comparing to
the solutions in Table 3 one can see that the robustness improvement by this method is not as good as our proposed method. The
material cost and the quality loss values of single-loop solutions
are generally larger than those of SORA solutions. This is expected
because the single-loop method takes a conservative approach to
satisfy the product reliability requirement.

X1

Fig. 5. G þ and sequential samples.

0.025
G+
0.02
0.015
G−,G+

Minimize

0.01
0.005
0
−0.005
80

G−
70

60
X1

50

40

5

4

2

3

1

0

X2

Fig. 6. G  and G þ comparison.

Table 4
Solutions by the single-loop approach.
w0

μ1

μ2

Material
cost

Var.
cost

Mean
cost

Increase Quality
loss

Decrease

1.00
0.71
0.68
0.64
0.60
0.45
0.41
0.33
0.24
0.12
0.07
0.02
0.00

51.24
50.95
51.01
51.87
52.06
51.88
51.11
52.15
53.98
52.73
50.23
48.83
47.68

1.02
1.04
1.05
1.04
1.05
1.07
1.10
1.11
1.12
1.21
1.38
1.87
2.16

200.69
204.26
206.70
207.26
210.16
214.16
218.43
224.36
233.13
250.28
264.78
330.93
361.96

45.97
47.44
48.22
47.58
48.38
49.91
52.19
53.17
54.26
61.79
60.64
63.99
62.32

154.72
156.82
158.48
159.68
161.78
164.25
166.24
171.19
178.87
188.49
204.14
266.94
299.64

–
1.77%
3.00%
3.27%
4.72%
6.71%
8.84%
11.80%
16.17%
24.71%
31.94%
64.90%
80.36%

–
39.60%
42.28%
42.95%
43.96%
45.97%
47.65%
50.33%
53.02%
57.72%
62.75%
66.78%
73.49%

2.98
1.80
1.72
1.70
1.67
1.61
1.56
1.48
1.40
1.26
1.11
0.99
0.79

distance of performance value from the lower noise level to the
higher noise level is short at the optimal solution, which indicates
that the product performance function is indeed robust to noise
disturbance at this design point.
Furthermore, our solutions are compared with the solutions
obtained by using a single-loop method for handling probabilistic
constraints. Unlike the SORA method, which adjusts the selection
of MPP iteratively based on the current evaluation of probabilistic
constraints, the single-loop method would simply shift all points
on the limit state surface a β distance to the gradient direction to
guarantee the satisfaction of reliability requirement for any design

5. Conclusion and future work
In this paper, both the reliability and robustness properties of
product performance function are considered in a product design
optimization process. The product performance is evaluated by a
computer model, which is further replaced by a Gaussian process
surrogate model. The product reliability is guaranteed by a
probabilistic constraint and the robustness is achieved by adding
a quality loss objective. In order to evaluate the impact of noise
variables under implicit performance functions, a hierarchical
experimental design of computer experiments is employed to
construct a combined Kriging metamodel. Using a sequential
sampling strategy, this metamodel is continually updated and
the design solution is constantly improved. Finally, a Pareto
solution frontier is produced to make a tradeoff of the shortterm cost associated with product production and the long-term
cost associated with product quality loss.
In this paper, we demonstrate our method using only one
product performance function, but in a realistic engineering
design there may be multiple performance functions. We plan to
extend our work to the case where there are multiple product
performance functions that measure multiple product quality
characteristics. It is also in our interest to investigate the potential
interaction between two performance functions and how it would
affect the optimal design solution. As mentioned before, there are
other MOO algorithms that can be adapted to this integrated
robustness and reliability-based design optimization problem and
they will be explored in the future.

Acknowledgments
This research is partially supported by the NSF Grant CMMI1301075.

X. Zhuang et al. / Reliability Engineering and System Safety 138 (2015) 145–153

References
[1] Chen X, Hasselman T. Reliability-based structural design optimization for
practical applications. In: 38th AIAA/ASME/ASCE/AHS/ASC structures, structural dynamics and materials conference and exhibit and AIAA/ASME/AHS
adaptive structural sorum. Reston, VA: AIAA; 1997.
[2] Royset J, Kiureghian AD, Polak E. Reliability-based optimal structural design by
the decoupling approach. Reliab Eng Syst Saf 2001;73:213–21.
[3] Du X, Chen W. Sequential optimization and reliability assessment for probabilistic design. In: International design engineering technical conferences.
New York, NY: ASME; 2002.
[4] Sopory A, Mahadevan S, Mourelatos Z, Tu J. Decoupled and single loop
methods for reliability-based optimization and robust design. In: Design
engineering technical conferences and computers and information in engineering conference. ASME; 2004.
[5] Liang J, Mourelatos Z, Nikolaidis E. A single-loop approach for system
reliability-based design optimization. J Mech Des 2007;129??(December
(12)):1215–24.
[6] Shan S, Wang G. Reliable design space and complete single-loop reliabilitybased design optimization. Reliab Eng Syst Saf 2008;93(8):1218–30.
[7] Parkinson A, Sorensen C, Pourhassan N. A general approach for robust optimal
design. Trans ASME 1993;115(March (1)):74–80.
[8] Parker G, Lee T, Lee K, Hwang K. Robust design: an overview. AIAA J
2006;44(1):181–91.
[9] Du X, Chen W. Towards a better understanding of modeling feasibility
robustness in engineering design. J Mech Des 2000;122(4):385–94.
[10] Youn B, Xi Z. Reliability-based robust design optimization using the eigenvector dimension reduction (EDR) method. Struct Multidiscip Optim
2009;37(5):475–92.
[11] Lee KH. A robust structural design method using the Kriging model to deﬁne
the probability of design success. J Mech Eng Sci 2010;224:379–88.
[12] Apley D, Liu J, Chen W. Understanding the effects of model uncertainty in
robust design with computer experiments. J Mech Des 2006;128(4
(July)):945–58.
[13] Rangavajhala S, Mullur A, Messac A. The challenge of equality constraints in
robust design optimization: examination and new approach. Struct Multidisicip Optim 2007;34:381–401.
[14] Zaman K, McDonald M, Mahadevan S, Green L. Robustness-based design
optimization
under
data
uncertainty.
Struct
Multidiscip
Optim
2011;44:183–97.
[15] Tang Y, Chen J, Wei J. A sequential algorithm for reliability-based robust design
optimization under epistemic uncertainty. J Mech Des 2012;134??(January
(1)) 014502-1–10.
[16] Xu H, Huang H, Wang Z, Zheng B, Meng D. Research on multi-objective robust
design. In: International conference on quality, reliability, risk, maintenance,
and safety engineering, ICQR2MSE; 2011. p. 885–90.
[17] Beyer H, Sendhoff B. Robust optimization—a comprehensive survey. Comput
Methods Appl Mech Eng 2007;196(July 1 (33–34)):3190–218.
[18] Vining GG, Myers RH. Combining Taguchi and response surface philosophies:
a dual response approach. J Qual Technol 1990;22:38–45.
[19] Mourelatos Z, Liang J. A methodology for trading-off performance and
robustness under uncertainty. In: International design engineering technical
conferences and computers and information in engineering conference. New
York, NY: ASME; 2005.

153

[20] Li M, Azarm S, Aute V. A multi-objective genetic algorithm for robust design
optimization. In: Genetic and evolutionary computation conference; 2005. p.
771–8.
[21] Antonio C, Hoffbauer L. An approach for reliability-based robust design
optimization of angle-ply composites. Compos Struct 2009;90(1):53–9.
[22] Yadav O, Bhamare S, Rathore A. Reliability-based robust design optimization: a
multi-objective framework using hybrid quality loss function. Qual Reliab Eng
Int 2010;26:27–41.
[23] Hacker K, Lewis K. Robust design through the use of a hybrid genetic
algorithm. In: Proceedings of DETC’02, AMSE 2002 design engineering
technical conferences and computers and information in engineering conference. New York, NY: ASME; 2002.
[24] Paiva R, Carvalho A, Crawford C. A robust and reliability based design
optimization framework for wing design. In: Second international conference
on engineering optimization; 2010.
[25] Lee L, Chew E, Teng S. Finding the pareto set for multi-objective simulation
models by minimization of expected opportunity cost. In: Proceedings of the
2007 winter simulation conference; 2007.
[26] Das I, Dennis JE. Normal-boundary intersection: a new method for generating
the Pareto surface in nonlinear multicriteria optimization problems. SIAM
J Optim 1998;8(3):631–57.
[27] Deb K, Pratap A, Agarwal S, Meyarivan T. A fast and elitist multiobjective
genetic algorithm: NSGA-II. IEEE Trans Evol Comput 2002;6(2):182–97.
[28] Sacks J, Welch W, Mitchell T, Wynn H. Design and analysis of computer
experiments. Stat Sci 1989;4(4):409–23.
[29] Zhuang X, Pan R. A sequential sampling strategy to improve reliability-based
optimization under implicit constraints. J Mech Des 2012;134(2) 021002-1–10.
[30] McKay M, Beckman R, Conover W. A comparison of three methods for
selecting values of input variables in the analysis of output from a computer
code. Technometrics 1979;21(2):239–45.
[31] Iman R, Helton J, Campbell J. An approach to sensitivity analysis of computer
models. Part 1. Introduction, input variable selection and preliminary variable
assessment. J Qual Technol 1981;13(3):174–83.
[32] Montgomery D. Design and analysis of experiments. Hoboken, NJ: John Wiley;
2009.
[33] Tu J, Choi K. A new study on reliability-based design optimization. In: IDETC/
CIE, vol. 121. New York, NY: ASME; 1999.
[34] Du X, Chen W. Sequential optimization and reliability assessment method for
efﬁcient probabilistic design. J Mech Des 2004;126(March):225–33.
[35] Du X, Sudjianto A, Chen W. An integrated framework for optimization
under uncertainty using inverse reliability strategy. Trans ASME
2004;126(4):562–70.
[36] Jones D, Schonlau M, Welch W. Efﬁcient global optimization of expensive
black-box functions. J Global Optim 1998;13(4):455–92.
[37] Hasofer A, Lind N. Exact and invariant second-moment code format. J Eng
Mech Div 1974;100(1):111–21.
[38] Steenackers G, Guillaume P, Vanlanduit S. Robust optimization of an airplane
component taking into account the uncertainty of the design parameters. Qual
Reliab Eng Int 2009;25:225–82.
[39] Gu L, Yang RJ, Tho H, Makowski M, Faruque L, Li Y. Optimization and
robustness for crashworthiness of side impact. Int J Veh Des
2001;26(4):348–60.

Fixing Multi-Client Oscillations In HTTP-based
Adaptive Streaming: A Control Theoretic Approach
Xiaoqing Zhu, Zhi Li, Rong Pan, Joshua Gahm, and Rao Ru
170

W

Ciseo Systems Ine.
Tasman Drive, San lose, CA 95134

{xiaoqzhu,zhi12,ropan,jgahm,hahu2}@cisco.com
Abstract-In recent years, the technology for video delivery
over the Internet is shifting towards a new paradigm: HTTP­
based adaptive streaming (HAS). An HAS dient receives video
contents on a segment by segment basis via standard HTTP
GET requests. It can dynamically change the rate and quality
of the video in the presence of time-varying bandwidth changes.
When multiple dients compete over a common bottleneck link,
however, they orten faH to converge to their respecitive fair share
of bandwidth. This leads to constant oscillations in the received
video quality.
In this paper, we uncover the cause of such oscillations based
on observations from large-scale test bed experiments. We then
propose a novel dient rate adaptation algorithm , which strives to
stabilize the playout butTer at a reference level via a proportional­

Fig. 1.

integral controller (PIC). Test bed evaluation results confirm

(HAS) system. Multiple quality versions of the same video content

Architecture overview of an HTTP-based adaptive streaming

the effectiveness of the proposed PIe scheme and its superior

are generated via transcoding or parallel live encoding. Each media

performance over Microsoft Smooth Streaming.

file is broken down into many physical or logical segments. The
dient can adaptively request different quality versions of the media

I.

segments based on its own estimate of available network bandwidth.

INTRODUCTION

Consumption of video content over the Internet grows at a
rapid pace. By the end of year 2011, video exceeded half of
global consumer Internet traffic, and video-on-demand traffic
will triple by 2016 [1]. A high percentage of these contents
are consumed in web browsers, via HTTP delivery over TCP.
YouTube, for instance, streams over 4 billion hours of video
each month [2] using a technique called progressive download.
Instead of streaming the video content packet by packet, or
downloading an entire video file before playing it out, the
client progressively downloads portions of the large video file
via standard HTTP GET requests. To guard against future
bandwidth fluctuations. It builds up a sufficiently large buffer
- typically tens of seconds - before starting the video
playout. With such an approach, video content providers can
avoid the expense of dedicated streaming servers by employing
standard HTTP web servers and caches. They can also build
upon the prevalence of existing content delivery networks
(CDNs) for large-scale distribution at relatively low costs.
Although progressive download works weil for delivering
short video clips over stable networks, its performance tends
to degrade in more dynamic environments, e.g., over wireless
links. Mismatch in network bandwidth and video source rate
can cause frequent client re-buffering, stuttered video playout,
and unpleasant viewing experience. As smart phones and

MMSP'13, Sept. 30 - Oet. 2, 2013, Pula (Sardinia), Italy.
978-1-4799-0125-8/13/$31. 00 @2013 IEEE.

tablets continue to proliferate, limitations of a plain progres­
sive download solution will aggravate over time.
In recent years, industry has shifted towards a more flexi­
ble design: HTTP-based adaptive streaming (HAS). Figure 1
depicts the overall architecture of an HAS system. Unlike
progressive download, HAS allows the client to dynamically
change video rate and quality on a segment by segment
basis. The client can switch to a low-quality version of the
video to avoid buffer underflow during temporary network
congestion, and switch back to high-quality video after the
connection speed recovers. The technology has been widely
deployed in commercial systems, including Netflix, Akamai,
Microsoft Smooth Streaming [3], Apple HTTP Live Stream­
ing (HLS) [4], and Adobe HTTP Dynamic Streaming (HDS).
In 2012, MPEG finalized a new standard to enable Dynamic
Adaptive Streaming over HTTP, named as MPEG-DASH [5].
The MPEG-DASH standard has intentionally left out of its
scope the definition of client behavior for content fetching, rate
adaptation heuristics, and video playout. In most conventional
clients, the rate adaptation algorithm is designed with the
underlying assumption that the network bottleneck resides at
the last hop of delivery, as typically experienced by mobile
devices. However, such a design turns out to be problematic
when multiple clients compete over a COlmnon bottleneck link.
As shown in [6], individual clients suffer from constant rate

230

MMSP2013

and quality oscillations, instead of converging to their fair
share of bandwidth. Moreover, the severity of fluctuation tends
to grow as the number of dients increases.
In this paper, we uncover the fundamental cause for the
multi-dient oscillation problem based on observations from
large-scale test bed experiments. We show that video dients
substantially over-estimate their fair share of bandwidth when­
ever the network is under-subscribed. The estimated bandwidth
drops drastically when the offered load over the network
shifts between 95% and 105%
a phenomenon dubbed as
the bandwidth cliff effect. Since individual dients measure
bandwidth based on segment download time, they undergo
a vicious cycle: over-estimating their bandwidth share, up­
shifting in requested video rate, observing prolonged segment
download time, and down-shifting video rate requests, so on
and so forth. Consequently, the dients never converges to a
stable share of bandwidth.
Based on such observations, we present a novel dient
adaptation algorithm to remedy the problem. Our design
hinges on one key idea: to derive requested video rate di­
rectly solely from dient playout buffer evolutions. Instead of
explicitly measuring bandwidth from segment download times,
our proposed scheme strives to stabilize the dient playout
buffer at a reference level via a proportional-integral controller
(PIC). By doing so, it implicitly matches the video rate with
the dient's available bandwidth in the long run. In addition,
the control-theoretic approach allows for rigorous guarantees
of system stability across multiple competing dients via
careful selection of algorithm parameters. Extensive test bed
evaluations have confirmed the effectiveness of PIC and its
superior performance over Microsoft Smooth Streaming.
The rest of the paper is organized as folIows. We first review
related work on HTTP-based adaptive streaming in the next
section. Section Irr provides an in-depth explanation for the
multi-dient oscillation problem based on observations from
test bed experiments. In Section IV, we present a buffer­
driven dient adaptation algorithm. The proposed PIC scheme
is evaluated with various test bed experiments in Section V.

Several novel rate adaptation algorithms have been proposed
for HAS dients [9][10] [11]. These schemes attempt to strike a
balance between competing performance goals: higher average
video quality, reduced rate oscillations over time, and minimal
dient re-buffering. The control-theoretic approach adopted by
this paper is similar to [11] and [12]. Our work stands apart
from [11] by conforming to the dient-driven architecture of
HAS, and differs from [12] by striving to stabilize the dient
buffer in terms of playout durations.

-

11. RELATED WORK

Various research efforts have focused on understanding
the behavior of commercially-deployed HTTP-based adaptive
streaming (HAS) clients. One such example is [7], which char­
acterizes and evaluates HAS dients from Microsoft Smooth
Streaming, Netflix, and Adobe OSMF in controlled test bed
experiments. The dient adaptation algorithms for all three
players follow a similar structure: they all switch between
hybrid modes of buffering and steady-state. The players react
to bandwidth variations with different levels of aggressiveness.
They either suffer from severe video quality oscillations or
fail to fully utilize the network. A similar study in [8]
the performance of Akamai Adaptive Video Service in the
presence of time-varying bandwidth and competing TCP flows.
The Akamai system has slow reaction time when bandwidth
increases; it experiences brief video playout interruptions
when bandwidth drops suddenly.

111.

T HE BANDWIDTH

CLIFF EFFECT

In this section, we provide an explanation for the multi­
dient oscillation problem based on our observations from
large-scale test bed experiments. Interested readers are referred
to [13] for a more detailed account of the study.
In our experiment, 36 Microsoft Smooth Streaming dients
share a 100 Mbps link, with random start times. Each dient
can choose from 10 different quality levels of the same
video content, with available rates ranging from 230 Kbps to
10 Mbps. As Fig. 2 shows, the Smooth dients collectively ex­
hibit an oscillatory behavior. Their aggregate requested video
rate periodically swings between 75 Mbps and 115 Mbps. In­
dividual dients constantly undergo substantial quality changes,
shifting back and forth across five different levels.
We now turn to a simple thought experiment for explaining
the key reason behind such oscillatory behavior. Consider N
dients each downloading a segment of size Lover a link with
capacity C, under idealized TCP behavior. All downloads start
at times L/N C apart. In this special case, each dient would
experience full bandwidth of C in downloading its segment,
and would over-estimate its available bandwidth by N folds.
In practice, various random factors result in partial overlapping
amongst the dients. Nevertheless, the over-estimation ratio
remains substantial. As Fig. 3 shows, the over-estimation ratio
falls sharply from 3 to unity, as the link subscription ratio
increases from 95% to 105%.
This phenomenon, henceforth referred to as the bandwidth
cliff effect, feeds a vicious cyde in the dient rate adaptation
process. As Fig. 4 illustrates, link under-subscription leads
to bandwidth over-estimation and up-shift in requested video
rates at all clients. This, in turn, triggers link over-subscription
and reduced bandwidth estimation. As dients down-shift their
requested video rates at about the same time, they lead to
under-subscriptions at the bottleneck link again, starting a new
round of the vicious cyde in rate oscillations.
IV.

BUFFER-DRIVEN QUALITY ADAP TATION SCHEME

We now present a novel dient adaptation algorithm to avoid
the vicious cyde caused by the bandwidth cliff effect. Instead
of explicitly estimating the available bandwidth from segment
download durations, we proposed to derive the requested video
rate from the dient's playout buffer evolution. Casting the rate
adaptation module as a proportional-integral controller (PIC),
our proposed scheme strives to stabilize the dient playout
buffer at a reference level, thereby implicitly matching the
requested video rate with network bandwidth in the long run.

231

MMSP2013

120 ,----,----,

"K
.Cl

CO

;::;:

100

Ql
CD

6
<D
'"
a:

s:

0"0

!E.-

60 L----L----�--�
400
450
500
550
350
200
250
300
600

12

130

10

120

8

110

6

100

(fJ
c:
0CI>

Q

"§:

4

O·
:::J

2

Requested Video Bitrate
500
550
600
650
Time (Sec)

0

Time(s)

450

6 ,-------,--,---,.---,

C.

:::J
A

-;R
�

Fig. 4. Illustration of the vicious cycle in video rate adaptation. Over­
estimation in network bandwidth, as measured by TCP throughput
(a) leads to up-shift in video requested rates (b). This, in turn,
triggers link over-subscription (c) and reduced bandwidth estimation
(d). In this experiment, 36 Microsoft Smooth Streaming clients share

O L-------�---L--�--�
250
350
400
200
300

a common bottleneck link of 100 Mbps.

Time(s)

Fig. 2.

Aggregate (top) and per-client (bottom) video request rate

for 36 Microsoft Smooth Streaming clients sharing a 100 Mbps link,
with random start time. Details of the test bed configurations are are
explained in Sec. V.

3.5
6 3
�2.5
CI>
0..
.0
�

e 2
1.5

�
I­
Cl...

Fig. 3.

95

10

100

1 05

110

Link Subscription (%)

115

The bandwidth cliff effect. In this experiment, 100 clients

share a link with capacity of 100 Mbps. Each client repeatedly
downloads a fixed-size segment once very 2 seconds. Increasing the
size of the segment leads to increased link subscription. The TCP
throughput, averaged over all segment downloads, is plotted against
the link subscription ratio.

A. Client buffer evolution
We denote the duration of each media segment as T, the
client's available bandwidth as C, and the playout buffer du­
ration at time t as L (t). The requested video rate is designated
as R. The playout bufter evolves as follows I:

L(t)

=

CT
- T, 0].
max[L(t - T) +
R(t T)
_

Calculation of requested video rate

Given current playout buffer level L (t), the client calculates
the requested video rate for the next segment as:

R(t)

,---�--�--�-�---,

�
Cl

o
I-

B.

=

R(t - T) + Ii(L(t) - La)T + TJ(L(t) - L(t - T)). (2)

Here, L a denotes reference buffer level. Typical choices of
desired buffering are between 10 - 30 seconds for video-on­
demand services using HTTP-based adaptive streaming.
The first term on the right hand side of (2) indicates the
amount inftuence the buffer level oft"set L (t) - L a exerts on
the rate choice. A playout buffer level exceeding the reference
indicates room for rate up-shifting and a buffer level below
the reference urges the client to down-shift the video rate.
The second term in reftects the impact of the rate of change
in the buffer level. Increasing buffer level encourages higher
requested video rate whereas decreasing buffer level leads to
reduced video rates.
Note that (2) follows the same form of as the proportional­
integral (PI) controller introduced in [14]. The scaling param­
eters Ii and TJ can be tuned to weight the relative inftuence of
these two terms, so as to strike a balance between responsive­
ness and stability. In the appendix, we establish the stability
criterion that guides the choice of these two parameters.
C.

Modulation of segment request intervals

(1)

For every T seconds, the amount of data arriving at the
buffer is CT, which will eventually be played out at the
requested video rate R(t - T) 2. Measured in terms of playout
time, the playout bufter drains by T seconds from t - T to t,
and is replenished by CT/ R(t - T) seconds.
lNote that L(t) is measured in terms of playout time, not data size.
2The video rate for the segment currently being downloaded is chosen
seconds ago, hence the delay term in the expression.

T

{

In practice, the client chooses a rate amongst the discrete
set of available choices {R1, . . . , RK}, such that:

R

=

Rl'
Rk,
RK,

R(t)::; R1
Rk::; R(t) < Rk+1
R(t) 2: RK.

(3)

In addition, the client delays the next segment request by:

T
A

=

max[O,

R(t) - R
T].
R(t)

(4)

Note that the value of f is in proportion to the gap between
target and requested rate R(t) - R, and corresponds to the
equivalent downloading time for a virtual segment of size

(R(t) - R)T.

232

MMSP2013

Vl c
« Q)
I=
U

6 �------�--,---,-

Vl c
« Q)
I=
U

-

0
Fig. 5.

video request rate
profile

----. BW

Network topology in test bed. The bottIeneck link at the

100
200
300
40
o�------�------�------��==��
�0��==�500
Time(s)

Cisco 7200 (ATS2) Router is configured with a one-way propagatIOn

�

delay of 10 ms in each direction, and a link ate of either 10 Mbps or

(a) Requested video rate

100 Mbps. Local links incur zero propagatIOn delay and effectlvely
unbounded bandwidth in OUf experiments.

Quality Index
0
1
2
3
4

Rates
459 Kbps
693 Kbps
937 Kbps
1.27 Mbps
1.75 Mbps

Quality Index
5
6
7
8
9

Rates
2.54 Mbps
3.76 Mbps
5.38 Mbps
7.86 Mbps
11.32 Mbps

- eHent

buffer
----. referenee

° � ------� 1 ------- � ------- � � ----�� = ����
200
300
400
00
500
0
Time(s)

TABLE I
LI ST OF AVA ILA BLE RATES FO R A DAPTI V E HTTP STREAMING IN
SIMULATION AND TESTBED EVALUATIONS.

(b) Client playout buffer level
Fig. 6.

A single PIC client over a time-varying bottleneck link that

changes between a high rate of 5 Mbps and a low rate of 2 MbPs.
.
It takes the client about 50 seconds to react to sudden changes In
bandwidth, before settling at the new rate.

V.

TEST BED EVALUATION

A. Setup

=

We now evaluate performance of the proposed proportional­
integral controller (PIC) client in a test bed environment, as
illustrated in Fig. 5. Multiple HAS clients are connected to
their respective HTTP streaming servers running on virtual
machines hosted by the Cisco UCS C250 Server. All con­
nections go through the same bottleneck link via a Cisco
7200 (ATS2) Router, with configurable delay and bandwidth
parameters. One-way propagation delay of the link is set at
lOms on both directions. The queue on ATS2 follows the
random early drop (RED) algorithm. When the link rate is
set at 10 Mbps, the RED queue has a limit of 90 packets, and
starts to randomly drop packets once the time-averaged queue
occupancy exceeds a minimum threshold of 30 packets. Its
maximum drop probability is 25%. When the link rate is set
at 100 Mbps, the RED queue limit is set at 900 packets, with
a minimum threshold of 300 packets and a maximum drop
probability of 100%.
The video content is represented in 10 different quality ver­
sions. The available rates range from 459 Kbps to 11.3 Mbps,
as listed in Table I. The duration of each video segment is
2 seconds. The reference playout buffer level in the PIC client
is set at 10 segments, i.e., 20 seconds of playout time. The
scaling parameters are chosen as ""
0.001 and 7]
0.15,
respectively.
=

B.

onds and recovers back to 5 Mbps at time t
300 seconds 3.
Figure 6 shows traces of requested video rates and client play­
out buffer duration from this experiment. The client starts out
by requesting the video at the lowest available rate and quality.
It gradually up-shifts its rate requests as the playout buffer
builds up. It reaches the highest quality level below available
bandwidth at 3.76 Mbps, and occasionally jumps up to the
quality level above available bandwidth at 5.33 Mbps. Starting
from time t
200 second, the abrupt drop in available
bandwidth quickly drains the playout buffer. In response, the
client rapidly down-shifts it requested video rate till it settles
at 1.75 Mbps, slightly below the available bandwidth. When
available bandwidth recovers at time t
300 seconds, the
playout buffer quickly builds up again, driving the requested
video rate back to the original level at 3.76 Mbps. In all three
transitions, the PIC client is able to switch to the new steady­
state rate within 50 seconds.

=

Single client with time-varying bandwidth

We first examine the behavior of the proposed PIC client
by itself, under time-varying bandwidth. Initially, the available
bandwidth is 5 Mbps; it drops to 2 Mbps at time t
200 sec=

=

=

C.

Multiple competing clients

Next, we consider multiple clients competing over the same
link with a constant bandwidth of 10 Mbps. Figure 7 shows
traces of requested video rate and playout buffer level for
two competing PIC clients. Again, both clients take about 50
seconds to stabilize around their fair share of bandwidth at
5 Mbps. At steady state, the playout buffer duration fluctuates
around the reference level of 20 seconds. The two clients also
share the bandwidth fairly over the long term.
3While our testbed hardware only supports a fixed link rate configuration at
10 Mbps, we injected background UDr trafftc at rates of 5 Mbps and 8 Mbps,
respectively, to achieve a equivalent time-varying avallable bandwldth for the
HAS c\ient.

233

MMSP2013

6 ,----,-----,--,
c.
.0

Ci)

�
*
CI:

�
I
�========�

I�����:::�: �

Ci) 6
Q)

:::l
er
Q)
CI:
0
Q)
'0

----, fair-share BW

O L-__-L____L-__-L____L-__-L__
100
400
500
200
300
600
o

700

10

800

Time(s)

4

:>

Q)
Cl
CIS

(a) per-client video request rate

>
«

2

- + - Smooth
....... PIC

,,

\,

,,
,

.
" -"
""
""

Q;

40 ,----,---,---.--,

0
1

2

"" ,
"

4
3
5
Number 01 Clients

7

6

(a) Average requested video rates

100

200

300

400

500

600

700

800

Time(s)

C)
."

0.1 rr========;-�--�----�--�-----:l
-+-Smooth
....... PIC
0,08

� 0,06

(b) playout buffer level

g

Fig. 7. Two PIC clients competing over a bottleneck link of 10 Mbps.

:0

<J)
.!:

.$ 0.04

We then compare the proposed PIC scheme against Mi­
crosoft Smooth Streaming by varying the number of clients
over the same link. Fig. 8 shows their performances at steady
state in terms of average per-client video request rate and
instability metric, as defined in [6]. The former corresponds
to overall utilization efficiency of the rate adaptation schemes;
the laUer denotes the ratio of video chunks experiencing a rate
shift. The PIC client consistently achieves higher average rates
and lower instability metric than Smooth Streaming.
Finally, we revisit the case of a high aggregation of 36
competing clients over a 100 Mbps link. Results from the
Microsoft Smooth Streaming clients were presented earlier in
Sec. III (see Fig. 2). Figure 9 shows the corresponding traces
of aggregate and per-client video rates from all 36 PIC clients.
In contrast to Smooth Streaming, the proposed PIC clients
collectively achieve a stable aggregate video request rate at
100 Mbps, thereby and fully utilizing the bottleneck link. At
steady state, the requested video rates of individual PIC clients
also stabilize between two adjacent levels around the clients'
fair share of bandwidth.
V I.

CONCLUSIONS AND FUTURE WORK

In this work, we uncover the key reason for rate and quality
oscillations among multiple competing clients in HTTP-based
adaptive streaming (HAS). Experiments from our test bed
show that estimating a client's fair share of bandwidth based
on segment download time leads to the bandwidth cliff effect.
Over-estimation of a client's fair share of bandwidth, in turn,
result in constant swings between client rate up-shifts and
down-shifts.
We then present a novel client rate adaptation algorithm to
remedy this problem. Instead of explicitly estimating band­
width based on segment downloading times, we propose to

4
3
5
Number 01 Clients

7

6

(b) Instability index, as defined in [6],
Fig. 8. Comparison between PIC and Microsoft Smooth Streaming in
terms of normalized standard deviation of requested video rates. The
number of competing clients increases from 2 to 14 over a common
bottleneck link of 10 Mbps. For each scheme, the results are averaged
over 200 seconds during steady state, across 5 independent runs of
the same experiment.

calculate the requested video rates based on client buffer evolu­
tion over time. Our proposed scheme employs a proportional­
integral controller (PIC) and strives to stabilize the playout
buffer at a reference level. By doing so, it implicitly matches
the video rate with a client's fair share of bandwidth in the
long run. Evaluation results from test bed experiments validate
the effectiveness of the proposed scheme. In comparison
with Microsoft Smooth Streaming, PIC clients tend to yield
higher network utilization and lower instability metric when
competing against each other.
By now, we have only considered the case where the same
type of clients (PIC or Smooth) share a common bottleneck.
In practice, it is more Iikely that heterogeneous HAS clients
coexist in the same network. Our future investigations will
extend to this more realistic scenario.
ApPENDIX: STABILITY ANALY SIS

In this section, we establish the stability criterion for the
system, and provide some guidelines for the choice of the
scaling parameters K, and 7] in (2). Following a fluid model of

234

MMSP2013

120 ,----,-----,--,
o.

�
100 b---���----�r-��--�
.0

o.

�

%l

er:

80

o.
o.

60 L---�-----L--�--L---�--�
500
200
250
300
350
400
450
550
600
Time(s)

o.

6

,

\
o
200

...

...

V \

250

,

I

A

o.

400

350

300

o.

...

Time(s)

Fig. 10.

Fig. 9.
Aggregate (top) and per-client (bottom) video request rate
for 36 PIe clients sharing a 100 Mbps link, with random start time.

x

the system dynamics in (1) and (2), we obtain:

C

-=-:------,- -

L(t)

R(t - T )

1,

=

C.

=

=

(6)

=

=

C

1

oL(t)

-

oR(t)

",oL + 1]oL.

C OR(t - T) ,

(7)
(8)

The cIose-loop transfer function in Laplace domain can be
derived as:
'" + 1] -ST
9( )
(9)

8

=

C82

8

e

.

Correspondingly, the magnitude and phase of
expressed as:

V",2 + 1]2 w2
Cw2

19(Jw)1

-7f

L9(Jw)

9(8)

can be

(10)

1]w

- WT + arctan(-)

(11)
'"
According to Bode stability criterion, the system is stable
only if 19(Jwc) I < 1 when L9(Jwc)
-7f at critical frequency
Wc [15]. It can be noted that Wc satisfies the following:
=

(12)
Although a cIosed-form solution for Wc does not exist, one
can solve for (12) numerically. Furthermore, since tan(4» > 4>
for 4> E (0, 7f / 2) , we need '" < 1]/T to ensure that the solution
exists for wc. It can then be checked numerically whether
19(jwc)1 < 1 or not.
As a concrete example, the stability region for ("',1]) is
marked in blue for
4 Mbps and T
2 s in Fig. 10. The
red line shows the upper bound of 1]
"'T.

C

=

=

4Mbps and

=

K, =

0.2, TJ

0.6, TJ

=

=

T =

2s. The

1.5; the black

2.0.

[1] Cisco White Paper, "Cisco visual networking index - forecast and
methodology, 2011-2016," http://newsroom.cisco.comldUs/index.html.
[2] "Youtube traffle statistics," http://www.youtube.com/t/press_statistics.
[3] A. Zambelli, "IIS Smooth Streaming Technical Overview,"
http://www.microsoft.com.
[4] Apple
Inc.,
"HTTP
Live
Streaming
Overview,"
http://developer.apple.com.
[5] MPEG, "ISO/lEC 23009-1:2012 Information technology - Dynamic
adaptive streaming over HTTP (DASH) - Part 1: Media presentation
description and segment formats," 2012.
[6] S. Akhshabi and A. C. Begen, "What happens when HTTP adaptive
streaming players compete for bandwidth," in Prac. ACM Workshop on

(5)

At equilibrium, R(t)
0 and L(t) 0, therefore L(t) Lo
and R(t)
Linearizing the system around its equilibrium,
L Lo + oL and R
+ oR, we get:
=

marks the unstable configuration of

=

'"

REFERENCES

",(L(t) - Lo) + 1]L(t).

R(t)

Stability region in blue for C

red + marks the stable configuration of

Network and Operating System Support Jor Digital Audio and Video
(NOSSDAV'12), Toronto, Ontario, Canada, Jun. 2012.
[7] S. Akhshabi, A. C. Begen, and C. Dovrolis, "An Experimental Evalua­
tion of Rate-adaptation Algorithms in Adaptive Streaming over HTTP,"
in Prac. ACM Multimedia Systems ConJerence (MMSys'11), San Jose,
CA, USA, Feb. 2011, pp. 157-168.
[8] L. D. Cicco and S. Mascolo, "An Experimental Investigation of the
Akamai Adaptive Video Streaming," in Prac. USAB'lO, 2010, pp. 447464.
[9] C. Liu, l. Bouazizi, and M. Gabbouj, "Rate Adaptation for Adaptiv
HTTP streaming," in Prac. ACM Multimedia Systems ConJerence (MM­
Sys'J1), San Jose, CA, USA, Feb. 2011, pp. 169-174.
[l0] D. Jamikov and T. Ozcelebi, "C\ient Intelligence for Adaptive Streaming
Solutions," EURASIP Journal on Signal Pracessing: Image Communi­
cation, Special lssue on Advances in lPT V Technologies, vol. 26, no. 7,
pp. 378-389, Aug. 2011.
[11] L. D. Cicco, S. Mascolo, and Y. Palmisano, "Feedback control for
adaptive live video streaming," in Proc. ACM Multimedia Systems
ConJerence (MMSys'11), San lose, CA, USA, Feb. 2011, pp. 145-156.
[12] C. Zhou, X. Zhang, L. Huo, and Z. Guo, "A control-theoretic approach
to rate adaptation for dynamic HTTP streaming," in Prac. SPIE Visual
Communications and Image Pracessing ( VCIP'12), San Diego, CA,
USA, Nov. 2012, pp. 1 - 6.
[13] Z. Li, X. Zhu, l. Gahm, R. Pan, H. Hu, A. C. Begen, and D. Oran,
"Probe and adapt: Rate adaptation for HTTP video streaming at scale,"
IEEE Journal on Selected Areas in Communications, 2013.
[l4] C. Hollot, Y. Misra, D. Towsley, and W.-B. Gong, "On designing
improved controllers for AQM routers supporting TCP ftows," in Proc.
IEEE International Conference on Computer Communications (lNFO­
COM'Ol), Anchorage, AK, USA, Apr. 2001.
[15] G. FrankIin, J. D. PoweU, and A. Emami-Naeini, Feedback Control oJ
Dynamic Systems.
NJ, USA: Prentice Hall, 2006.

=

235

MMSP2013

Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence

Bi-Weighting Domain Adaptation
for Cross-Language Text Classiﬁcation
Chang Wan, Rong Pan∗ and Jiefei Li
School of Information Science and Technology
Sun Yat-sen University, Guangzhou, China
{wanchang@mail2, panr@mail, lijiefei@mail2}.sysu.edu.cn
Abstract

ments and the test documents are in the same distribution, we
can use the traditional machine learning methods to solve it.
However, in the real world, this assumption cannot always be
satisﬁed. In reality, there are plenty of such resources in English since it has the largest population in the Internet world,
which is not true in many other languages. From the statistics
in ODP1 , it has sorted English web pages up to 1,429,760.
However the number of web pages in other languages are
much smaller (e.g. 13,293 classiﬁed Chinese web pages and
189,323 sorted Japanese web pages). It is well known that,
classiﬁcation requires a large number of labeled training data.
Generally, the more labeled training data we get, the better the
classiﬁcation accuracy is. Fortunately, there exist many Web
pages in English with class labels. Thus we should consider
how to make use of the information got from the Web pages
in English to classify the Web pages in other languages. This
problem is called cross-language text classiﬁcation, which we
address in this paper. In order to utilize Web pages in English to classify Web pages in other languages, we can use
a translation tool to translate the target data sets into English
language. In this way, a classiﬁer trained on English Web
pages can be applied. Unfortunately, this directly application
of this method may lead to some serious problems due to the
following reasons:

Text classiﬁcation is widely used in many realworld applications. To obtain satisﬁed classiﬁcation performance, most traditional data mining
methods require lots of labeled data, which can be
costly in terms of both time and human efforts. In
reality, there are plenty of such resources in English
since it has the largest population in the Internet
world, which is not true in many other languages.
In this paper, we present a novel transfer learning
approach to tackle the cross-language text classiﬁcation problems. We ﬁrst align the feature spaces
in both domains utilizing some on-line translation
service, which makes the two feature spaces under the same coordinate. Although the feature
sets in both domains are the same, the distributions of the instances in both domains are different,
which violates the i.i.d. assumption in most traditional machine learning methods. For this issue, we
propose an iterative feature and instance weighting (Bi-Weighting) method for domain adaptation.
We empirically evaluate the effectiveness and efﬁciency of our approach. The experimental results
show that our approach outperforms some baselines
including four transfer learning algorithms.

1

• First, due to the difference in language and culture, there
exists a word drift. This means that a word which frequently appears in English Web pages may hardly appear in other languages Web pages. In machine learning we call this different distribution in feature between
training data and test data. This problem needs to be
overcome in our study.

Introduction

Over the past few decades, a considerable number of studies have been made in data mining and machine learning.
However, many machine learning methods work well only
under the case where the training and test data are drawn
from the same feature space and the same distribution. A few
attempts have been made for leaning under different distributions. When the distribution changes, it could be costly in
terms of time and human efforts to re-collect the needed training data and rebuild the models. As a result, transfer learning
has been proposed to address this problem.
Recently, transfer learning was found to be useful in many
real-world applications. One important application is web
mining, where the goal is to classify a given web document into several predeﬁned categories. If the training docu∗

• Second, due to the errors introduced in the translation
process, there may be different kinds of errors in the
translated text. This noise problem produced in translation procedure should also be addressed effectively for
the purpose of improving the accuracy of classiﬁcation.
• Due to the word drift, some features functional for classiﬁcation on source domain may be useless for classifying data in target domain. Thus, we have to make the
distributions of source domain and target domain as sim1
According to the statistics reported from ODP on August 1,
2010

Corresponding author

1535

ilar as possible to let the features selected from source
domain also work for the target domain.
To solve the above problems, we develop a novel algorithm for cross-language text classiﬁcation. We introduce the
weights of features and training instances for domain adaptation. An objective function on these two type weights is built
to take the distance between source domain and target domain
and the inner distance in domain into consideration. We try
to optimize the objective function through Bi-Weighting. The
detail of our algorithm will be described later.
The rest of our paper is organized as follows. In section 2,
we discuss the related work. In section 3, we give the mathematical formulation of the problem we focus on in the paper. Section 4 describes our proposed algorithm in details,
including feature weighting and instance weighting. The experimental results are presented and analyzed in section 5. Finally, we conclude this paper with future works in section 6.

2
2.1

higher level features for transfer learning. Recently, manifold
learning has been an alternative method for transfer learning.
[Wang and Mahadevan, 2008] propose a Procrustes analysis based approach to manifold alignment without correspondences, which can be used to transfer the knowledge across
domains via the aligned manifolds.

3

In this section, we describe the problem we focus on in this
paper and introduce some notations and deﬁnitions used in
this paper.
A domain D includes two components: a feature space
X and a joint probability distribution P (x), where x ∈ X .
Given a speciﬁc domain D = {X , P (·)}, a task T associated with domain D consists of two parts: a label space
Y and an objective predictive function f (·) (represented by
T = {Y, f (·)}), which can be learned from a training sample
with data pairs {xi , yi }, where xi ∈ X and yi = f (xi ) ∈ Y.
Consider the case where there are one source domain
DS with its corresponding task TS and one target domain DT with its corresponding task TT . Here DS =
{xS1 , . . . , xSnS }, where xSi ∈ XS is an instance, XS is the
feature space in the source domain, and nS is the number
of instances in the source domain; the task in the source domain TS = {YS , fS (·)}, where YS is the label space in the
source domain and fS is the corresponding predictive function; DT = {xT1 , . . . , xTnT }, where xTi ∈ XT is an instance, XT is the feature space in the target domain, and nT
is the number of instances in the target domain; and the task
in the target domain TT = {YT , fT (·)}, where YT is the label space in the target domain and fT is the corresponding
predictive function. In addition, we set x = {x1 , · · · , xnf },
where xi stands for the value of the i-th feature Ai in instance
x and nf is the number of features.
In this paper, we address the cross-domain text classiﬁcation problems, which imply that DS = DT and YS = YT .
We aim at searching for a predictive function fP (·) approaching the target predictive function fT (·) ∈ TT as much as possible with aid of the knowledge in DS and TS .

RELATED WORK
Cross-Language Text Classiﬁcation

Many attempts have been made on addressing cross-language
classiﬁcation problems. [Bel et al., 2003] study EnglishSpanish cross-language classiﬁcation problem. [Rigutini et
al., 2005] proposes an EM-based learning method to address English-Italian cross-language classiﬁcation and acquires good empirical results. It applies feature selection before each iteration. [Ling et al., 2008] proposes an approach
based on information bottleneck theory [Tishby et al., 2000].
The method allows all the information to be put through a
“bottleneck”. Then, the approach maintains most of the common information and disregards the irrelevant information.

2.2

Problem Statement

Transfer Learning

[Pan and Yang, 2009] summarize the relationship between
traditional machine learning and transfer learning and give us
the categorization under three sub-settings, inductive transfer learning, transductive transfer learning and unsupervised
transfer learning. Instance transfer tries to re-weight the
source domain data or reduce the effect of the “bad” ones
encouraging the “good” source data to contribute more for
the classiﬁcation. Based on this deﬁnition, [Dai et al., 2007]
proposed a booting algorithm, TrAdaBoost, which is an extension of the AdaBoost algorithm. TrAdaBoost assumes that
the source and target domain data share the same set of features and labels, but the distributions of the data in the two domains are different. In addition, [Jiang and Zhai, 2007] propose a heuristic method to remove “misleading” training examples from the source data. Feature transfer focuses on ﬁnding “good” feature space to reduce the gap among domains
and minimize classiﬁcation or regression model error [Pan
et al., 2009; Chen et al., 2009]. Supervised feature learning is similar to methods used in multi-task learning. [Argyriou et al., 2006] propose a sparse feature learning method
for multi-task learning. In a follow-up work [Argyriou et al.,
2008] propose a spectral regularization framework on matrices for multi-task structure learning. [Raina et al., 2007] propose applying sparse coding [Lee et al., 2006],which is an
unsupervised feature construction method in order to learn

4

Our Approach

In this section, we present a novel domain adaptation approach to tackle the cross-domain classiﬁcation problems by
means of both feature and instance selection. The general
assumption in domain adaptation is that marginal densities,
P(xS ) and P(xT ), are very different. It is the key reason
leading to low classiﬁcation performance because a classiﬁer
having good performance in DS may lead to poor results on
DT . Here, we try to measure the distance of DS and DT and
make it as small as possible. On the other hand, a feature has
major difference of probability distribution among different
classes is more likely to help us classify the test data. Thus,
we also take the difference of a feature’s distribution among
class labels into consideration. The main idea here is to select featurs which have distinguished utility for classiﬁcation
from DS and make distributions of DS and DT as similar
as possible. In this way, we can assume that features useful
for classifying instances in DS could also be functional for

1536

where DSmi is the ith feature of instance xSm ∈ DS . Accordingly, DTri is the ith feature of instance xTr ∈ DT . Our
objective is to acquire WF and WI making J (WF , WI ) as
small as possible.
Now, we introduce how to iteratively update WF and WI .
In the ﬁrst step, we ﬁx WI and update WF to optimize
J (WF , WI ) by computing the derivate of J (WF , WI ) with
parameter WF :

classiﬁcation on DT . This can be achieved by feature and instance weighting. WF ∈ Rnf and WI ∈ RnS are the weights
of features and training instances respectively and used to get
the weighted DS and DT to realize the previous goals. For
∀i, j, WFi and WIj range from 0 to 1. From the above discussion, we can deﬁne the objective function as follows:
J (WF , WI ) = DB  − DI  + λWF 1F − WF 2
(1)
+λWI 1I − WI 2 ,
where DB , DI ∈ Rnf are column vectors. DB is used to
estimate the distance between DS and DT (the smaller, the
better). Besides, we let DI to estimate the difference of Ai
among different classes, which can be also considered as the
inner distance of a domain. 1F ∈ Rnf and 1I ∈ RnS are
column vectors with all ones. 1F − WF 2 and 1I − WI 2
are to control the change of two domains. λWF and λWI
are trade-off factors. Here, we use KL divergence[Kullback
and Leibler, 1951] to calculate the distance of features. Thus
Eq.(1) can be written as:

WFi ∗ DKL (p(Ai )q(Ai ))
J (WF , WI ) =


−
WF i ∗ (
DKL (pj (Ai )pk (Ai ))

u=il

∂J (WF , WI )
p(vu )
)
=
(p(vu ) log
∂WFi
q(vu )
u=i1

−(

∂J (WF , WI )
= 0,
∂WFi

WFi ∗ (

i

yj ,yk ∈Y

− 2 ∗ λWI ∗ (1 − WIi ),
(7)
where vu equals to DSmi . It is apparent that p(vu ), pj (vu ),
pk (vu ) are the ﬁrst-order function on WIm ’s reciprocal. Let
∂J (WF , WI )
=0
∂WIm

DKL (pj (WIm · DSmi )

yj ,yk ∈Y

+ λWF 1F − WF 2 + λWI 1I − WI 2


WFi ∗

i

−

u=il


(p(vu ) log

u=i1


i

WFi ∗ (



p(vu )
)
q(vu )

u=il


yj ,yk ∈Y u=i1

(pj (vu ) log

(8)

and we obtain the new value of WIm . We can use feature weighting and instance weighting iteratively until convergence or for the speciﬁc times. We summarize the above
process in Algorithm 1 .

pk (WIm · DSmi )))

=

(6)

we can get the new value of WFi . Next, we update WI when
WF is ﬁxed. We calculate the derivate of J (WF , WI ) with
parameter WI :
∂J (WF , WI ) 
p(vu )
)
=
(WFi ∗ ∂(p(vu ) log
∂WIm
q(vu )
i


pj (vu )
−
)
WFi ∗
∂(pj (vu ) log
pk (vu )
i

Note that KL-divergence is always non-negative due to the
Gibbls’ inequality[Cover and Thomas, 1991]. In addition,
p(Ai ), pj (Ai ) and pk (Ai ) are the distributions of Ai consisting of the weighted training instances. Each Ai has the
internals Vi = {vi1 , · · · , vil }. Eq.(2) can be rewritten as:

J (WF , WI ) =
WFi ∗ DKL (p(WIm · DSmi )q(DTri ))
−

pj (vu ) (5)
)
pk (vu )

Given WI and solving

(2)
where p(Ai ) and q(Ai ) are probability distributions of feature
Ai in DS and DT respectively. pj (Ai ) and pk (Ai ) reprsent
the distributions of Ai on class j and k in DS respectively.
DI only consider the inner distance of DS due to the absence
of class labels in DT . DKL (p(x)q(x)) is the KL-divergence
deﬁned as follows:
 ∞
p(x)
p(x) log
DKL (p(x)q(x)) =
q(x)
−∞
(3)

p(x)
,
p(x) log
=
q(x)
x



(pj (vu ) log

− 2 ∗ λWF ∗ (1 − WFi ).

+ λWF 1F − WF 2 + λWI 1I − WI 2 ,



u=il


yj ,yk ∈Y u=i1

yj ,yk ∈Y

i



pj (vu )
)
pk (vu )

+ λWF 1F − WF 2 + λWI 1I − WI 2 ,
(4)

1537

Algorithm 1 BI-Weighting (BIW) Domain Adaptation for
Cross-Domain Text Classiﬁcation
INPUT DS : source domain; DT : target domain; λWF :
smooth factor for feature weight; λWI : smooth factor
for source domain instance weight; CB : base classiﬁer
trainer (e.g. SVM).
OUTPUT the predictive function fP .
1: Initialize WF and WI ;
2: repeat
3:
Fixing WI , update WF in light of Eqs. (5 & 6);
4:
Fixing WF , update WI in light of Eqs. (7 & 8);
5: until convergence (or achieving iterNum times)
6: Revise XS to XSR with WF and WI ;
7: Build a classiﬁer fP with XSR and CB ;
8: RETURN fP

5

Experiments

two or third iteration. In the following experiments, we set
iterN um to be 3 to make sure that BIW is to converge.

In this section, we empirically evaluate the effectiveness and
efﬁciency of the algorithm proposed in Section 4.

5.1

GamesVsNews
RecreationVsScience

Experimental Setting
0.84

Our evaluation uses the Web pages crawled from the Open
Directory Project(ODP)2 during May 2010, including categories of Arts, Computers, Games, Health, Home, News,
Recreation, Reference, Science and Shopping. Each Web
page in ODP was classiﬁed by human experts. We preprocess the raw data as follows. First, all the Chinese Web pages
are translated into English by Google Translator3 . Then, we
transform all the letters to lowercase, and stem the words using the Porter’s stemmer [Porter, 1980]. Afterwards, stop
words are removed.
In order to evaluate our algorithm, we set up six crosslanguage classiﬁcation tasks. Five of them are binary classiﬁcation tasks, and the other one is for three-class classiﬁcation. We randomly resample 50000 instances from English
Web pages as the training set due to the computational issue.
In the following experiments, we choose two traditional
classiﬁers: Naive Bayes and Support Vector Machines
(SVM) [Chang and Lin, 2001], and four transfer learning approaches: Transductive SVM (TSVM) [Joachims, 1999], Information Bottleneck (IB) [Ling et al., 2008], Transfer Component Analysis (TCA) [Pan et al., 2009] and domain adaptation with Extracting Discriminative Concepts (EDC) [Chen
et al., 2009] for the purpose of comparisons. λWF and λWI
are set to 0.05.
With the help of the knowledge from the source domain,
transfer learning aims at predicting labels for instances in the
target domain with classiﬁcation performance as close as possible to the traditional classiﬁcation scenarios, training a classiﬁer with instances in the target domain and applying it to
the instances in the same domain. In particular, for the purpose of comparisons, we implement a “upper bound” algorithm by committing 5-fold cross-validations with NB, SVM
and TSVM individually over the data in the target domain
(the Chinese text), which are called NB-CN, SVM-CN and
TSVM-CN respectively. Note that, these classiﬁers are virtual “enemies” against the transfer learning algorithms. If
the gap between a speciﬁc transfer learning algorithm and the
virtual “enemies” is narrow, the transfer learning algorithm is
close to the “limit”; otherwise, there is still space to improve.
Precision, recall and F1 -measure are calculated in each experiment in this paper, which are widely used as evaluation
metrics in text classiﬁcation.

5.2

F1-measure

0.82

3

0.78
0.76
0.74
0.72

0

1

2

3

4

5

Number of Iterations

Figure 1: Impact of Iteration Times

5.3

Comparison with Instance Weighting Only

Since a few instance weighting methods have been applied on transfer learning, in this subsection, we compare
our algorithm BIW with the methods using only instance
weighting(Uni-Weighting) to see whether it is necessary to
integrate instance weighting and feature weighting. We will
compare BIW with feature weighting (or extracting) only
methods in the next subsection. Figure 2 shows the classiﬁcation performance of NB, Uni-NB and BIW-NB on two
cross-language classiﬁcation tasks. From the ﬁgure, it is clear
that Uni-NB outperforms NB but does worse than BIW-NB.
The results show the beneﬁt of combining feature weighting
and instance weighting.

5.4

Comparison with Baselines

In this subsection, we conduct experiments on all the six
datasets. Five of the tasks are binary class classiﬁcation while
the other one is a 3-class classiﬁcation. We use naive Bayes,
LibSVM and IB as base classiﬁers in our BIW algorithm,
which are named BIW-NB, BIW-SVM and BIW-IB respectively. Table 2 shows the experiments results of the comparisons with different baseline methods as well as the “upper
bound” methods. From these tables, we see that the BIW
algorithms are consistently better than their base classiﬁers.
Further more, the BIW algorithms in some tasks perform as
good as or even outperform the “upper bound” methods.
We compare the BIW algorithm with TCA [Pan et al.,
2009] and EDC [Chen et al., 2009] in smaller datasets ( randomly select around 1000 instances in each task with about
10000 features) for the computational issue including memory usage of EDC4 . Table 1 shows the result of the comparison. Apparently, BIW outperforms TCA and EDC by 5.2%
and 7.3% in the overall F1 -measure.

Impact of Iteration Times

Since our algorithm BIW is an iterative algorithm, an important factor of BIW is the number of iterations (iterN um) or
the convergence speed. We run a few tests to observe the
convergence speed. Figure 1 shows the impact of iteration
times on BIW in one of these tests. F1-measure at zero point
indicates the value of the algorithm without any feature and
instance weighting process. BIW usually converges at the
2

0.80

4
EDC needs O(m2 ) of memory, where m is the number of features.

http://www.dmoz.com/.
http://www.google.com/language tools.

1538

NB
Uni-NB
BIW-NB

Values

Value

0.90
0.88
0.86
0.84
0.82
0.80
0.78
0.76
0.74
0.72
0.70
0.68
0.66
0.64
0.62
0.60

Precision

Recall
Games vs News

F1-measure

0.90
0.88
0.86
0.84
0.82
0.80
0.78
0.76
0.74
0.72
0.70
0.68
0.66
0.64
0.62
0.60

NB
Uni-NB
BIW-NB

Precision

Recall
Recreation vs Science

F1-measure

Figure 2: Comparison with Instance Weighting Only
Table 1: Comparisons with other Transfer Learning methods 1:Games vs. News, 2:Health vs. Home, 3:News vs. Recreation,
4:News vs. Recreation, 5:Recreation vs. Science and 6:Recreation vs. Reference vs. Shopping.
Precision
Recall
F1-measure
Data set
TCA EDC BIW TCA EDC BIW TCA EDC BIW
1
0.833 0.808 0.835 0.759 0.813 0.823 0.794 0.810 0.829
2
0.803 0.703 0.848 0.607 0.712 0.686 0.691 0.706 0.758
3
0.77 0.783 0.889 0.846 0.802 0.849 0.806 0.792 0.868
4
0.815 0.775 0.856 0.720 0.694 0.701 0.765 0.732 0.771
5
0.845 0.830 0.953 0.876 0.840 0.831 0.860 0.835 0.888
6
0.533 0.561 0.614 0.666 0.524 0.633 0.592 0.542 0.623
Average 0.767 0.743 0.833 0.746 0.731 0.754 0.751 0.736 0.790

6

Conclusion and Future Works

regularization framework for multi-task structure learning.
In NIPS, pages 25–32, Cambridge, MA, 2008. MIT Press.

In this paper, we present a novel transfer learning approach to
tackle the cross-domain text classiﬁcation problems. We ﬁrst
align the feature spaces in both domains utilizing some online translation service, which makes the two feature spaces
under the same coordinate. Then we propose an alternated
method for domain adaptation. We empirically evaluate the
effectiveness and efﬁciency of our approach. The experimental results show that our approach outperforms some baselines
including supervised, semi-supervised and transfer learning
algorithms.
In the future, we plan to study other potentially better algorithms for the transductive transfer learning problems. We
will apply our approach to other domains. We also plan to
extend our method to the regression scenario.

[Bel et al., 2003] Núria Bel, Cornelis H. A. Koster, and
Marta Villegas. Cross-lingual text categorization. In Traugott Koch and Ingeborg Sølvberg, editors, ECDL, volume
2769 of Lecture Notes in Computer Science, pages 126–
139. Springer, 2003.
[Chang and Lin, 2001] Chih-Chung Chang and Chih-Jen
Lin. LIBSVM: a library for support vector machines,
2001. Software available at http://www.csie.ntu.
edu.tw/˜cjlin/libsvm.
[Chen et al., 2009] Bo Chen, Wai Lam, Ivor W. Tsang,
and Tak-Lam Wong. Extracting discriminative concepts
for domain adaptation in text mining. In John F. Elder IV, Françoise Fogelman-Soulié, Peter A. Flach, and
Mohammed Javeed Zaki, editors, KDD, pages 179–188.
ACM, 2009.

Acknowledgments
We thank the anonymous reviewers for helpful comments.
This work was supported by National Natural Science Foundation of China (61003140, 61033010) and the Fundamental
Research Funds for the Central Universities (09lgpy62).

[Cover and Thomas, 1991] Thomas M. Cover and Joy A.
Thomas. Elements of Information Theory. Wiley, New
York, 1991.

References

[Dai et al., 2007] Wenyuan Dai, Qiang Yang, Gui-Rong
Xue, and Yong Yu. Boosting for transfer learning. In
Ghahramani [2007], pages 193–200.

[Argyriou et al., 2006] Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil. Multi-task feature learning.
In NIPS, pages 41–48, 2006.
[Argyriou et al., 2008] Andreas Argyriou, Charles A. Micchelli, Massimiliano Pontil, and Yiming Ying. A spectral

[Ghahramani, 2007] Zoubin Ghahramani, editor. Machine
Learning, Proceedings of the Twenty-Fourth International
Conference (ICML 2007), Corvalis, Oregon, USA, June

1539

Table 2: The Precision, Recall and F1-measure on Six Datasets 1:Games vs. News, 2:Health vs. Home, 3:News vs. Recreation,
4:News vs. Recreation, 5:Recreation vs. Science and 6:Recreation vs. Reference vs. Shopping.
Precision
Data set
NB
BIW-NB SVM BIW-SVM TSVM
IB
BIW-IB NB-CN SVM-CN TSVM-CN
1
0.823
0.829
0.866
0.882
0.876 0.846
0.893
0.767
0.903
0.960
2
0.794
0.836
0.812
0.841
0.976 0.840
0.888
0.937
0.905
0.914
3
0.774
0.793
0.856
0.890
0.864 0.752
0.846
0.922
0.894
0.903
4
0.653
0.717
0.610
0.661
0.764 0.750
0.774
0.784
0.846
0.764
5
0.713
0.804
0.752
0.889
0.820 0.833
0.842
0.857
0.928
0.822
6
0.673
0.725
0.654
0.711
0.712
0.785
0.839
0.889
Average 0.738
0.784
0.758
0.812
0.789
0.838
0.851
0.894
Recall
Data set
NB
BIW-NB SVM BIW-SVM TSVM
IB
BIW-IB NB-CN SVM-CN TSVM-CN
1
0.815
0.820
0.871
0.888
0.788 0.862
0.902
0.946
0.956
0.912
2
0.803
0.848
0.835
0.859
0.719 0.842
0.900
0.808
0.904
0.811
3
0.799
0.819
0.875
0.923
0.669 0.813
0.827
0.793
0.989
0.816
4
0.662
0.730
0.583
0.685
0.569 0.722
0.779
0.773
0.852
0.877
5
0.727
0.824
0.768
0.901
0.749 0.773
0.865
0.854
0.916
0.932
6
0.659
0.714
0.662
0.726
0.736
0.802
0.83
0.891
Average 0.744
0.793
0.766
0.830
0.791
0.846
0.834
0.918
F1-measure
Data set
NB
BIW-NB SVM BIW-SVM TSVM
IB
BIW-IB NB-CN SVM-CN TSVM-CN
1
0.819
0.824
0.869
0.885
0.830 0.854
0.897
0.847
0.929
0.935
2
0.798
0.842
0.823
0.850
0.828 0.841
0.892
0.868
0.904
0.859
3
0.786
0.805
0.865
0.906
0.754 0.782
0.836
0.853
0.939
0.857
4
0.657
0.723
0.596
0.673
0.652 0.736
0.777
0.778
0.846
0.817
5
0.720
0.814
0.760
0.895
0.783 0.802
0.853
0.855
0.922
0.874
6
0.666
0.719
0.658
0.718
0.724
0.793
0.834
0.890
Average 0.741
0.788
0.772
0.821
0.790
0.841
0.839
0.905
20-24, 2007, volume 227 of ACM International Conference Proceeding Series. ACM, 2007.
[Jiang and Zhai, 2007] Jing Jiang and ChengXiang Zhai. Instance weighting for domain adaptation in nlp. In ACL.
The Association for Computer Linguistics, 2007.
[Joachims, 1999] Thorsten Joachims. Transductive inference
for text classiﬁcation using support vector machines. In
Ivan Bratko and Saso Dzeroski, editors, ICML, pages 200–
209. Morgan Kaufmann, 1999.
[Kullback and Leibler, 1951] S. Kullback and R. A. Leibler.
On information and sufﬁciency. The Annals of Mathematical Statistics, 22(1):79–86, 1951.
[Lee et al., 2006] Honglak Lee, Alexis Battle, Rajat Raina,
and Andrew Y. Ng. Efﬁcient sparse coding algorithms. In
Bernhard Schölkopf, John C. Platt, and Thomas Hoffman,
editors, NIPS, pages 801–808. MIT Press, 2006.
[Ling et al., 2008] Xiao Ling, Gui-Rong Xue, Wenyuan Dai,
Yun Jiang, Qiang Yang, and Yong Yu. Can chinese web
page be classiﬁed with english data source? In Proceedings of the Seventeenth World Wide Web Conference, 2008.
[Pan and Yang, 2009] Sinno Jialin Pan and Qiang Yang. A
survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering, 2009.
[Pan et al., 2009] Sinno Jialin Pan, Ivor W. Tsang, James T.
Kwok, and Qiang Yang. Domain adaptation via trans-

fer component analysis. In Craig Boutilier, editor, IJCAI,
pages 1187–1192, 2009.
[Porter, 1980] M. F. Porter. An Algorithm for Sufﬁx Stripping. Program, 14(3):130–137, 1980.
[Raina et al., 2007] Rajat Raina, Alexis Battle, Honglak Lee,
Benjamin Packer, and Andrew Y. Ng. Self-taught learning: transfer learning from unlabeled data. In Ghahramani
[2007], pages 759–766.
[Rigutini et al., 2005] Leonardo Rigutini, Marco Maggini,
and Bing Liu. An em based training algorithm for
cross-language text categorization. In Andrzej Skowron,
Rakesh Agrawal, Michael Luck, Takahira Yamaguchi,
Pierre Morizet-Mahoudeaux, Jiming Liu, and Ning Zhong,
editors, Web Intelligence, pages 529–535. IEEE Computer
Society, 2005.
[Tishby et al., 2000] Naftali Tishby, Fernando C. Pereira,
and William Bialek. The information bottleneck method.
CoRR, physics/0004057, 2000.
[Wang and Mahadevan, 2008] Chang Wang and Sridhar Mahadevan. Manifold alignment using procrustes analysis.
In William W. Cohen, Andrew McCallum, and Sam T.
Roweis, editors, ICML, volume 307 of ACM International
Conference Proceeding Series, pages 1120–1127. ACM,
2008.

1540

Feature Engineering and Tree Modeling for Author-Paper
Identification Challenge
Jiefei Li

Xiaocong Liang

Weijie Ding

Sun Yat-Sen University

Sun Yat-Sen University

Sun Yat-Sen University

lijiefei@mail2.sysu.edu.cn
yurilxc@gmail.com
dingweij@mail2.sysu.edu.cn
Weidong Yang
Rong Pan
Sun Yat-Sen University

Sun Yat-Sen University

ywdong@mail2.sysu.edu.cn panr@mail.sysu.edu.cn
ABSTRACT

1. INTRODUCTION

The ability to search literature and collect/aggregate metrics
around publications is a central tool for modern research.
Both academic and industry researchers across hundreds of
scientific disciplines, from astronomy to zoology, increasingly
rely on search to understand what has been published and
by whom. Microsoft Academic Search is an open platform,
which provides a variety of metrics and experiences for the
research community, in addition to literature search. As
the covering data came from many sources, the profile of
an author with an ambiguous name tends to contain noise,
resulting in papers that are incorrectly assigned to others.
KDD Cup 2013 Track 1 challenges participants to determine
which papers in an author profile were truly written by the
given author.
In this work, we present how to use tree-base models to accurately predict the paper author. We incorporate feature
engineering into the models with the advantages of them.
This paper introduces two kinds of tree-base models (GBDT [4], RGF [5]) and presents in detail the learning algorithm and how features can be generated for the task. The
experimental results show the effectiveness of the proposed
approach.

The KDD Cup 2013 contains two challenges proposed by
the Microsoft Academic Search, an open platform that provides a variety of metrics and experiences for the research
community, in addition to literature search. The data were
collected from many different resources. With more than
50 million publications and over 19 million authors across
a variety of domains, one of the main challenges is the ambiguity of the author names. Authors may publish under
several variations of their own name and different authors
might share a similar or even the same name. More details
about the data shows in [2].
Track 1 of the competition is focused on the problem of
author-paper identification, where some papers may be assigned to the wrong author because of the author-name ambiguity. Participants are asked to determine which papers
in an author profile were truly written by the given author.
Clearly, the identification problem could be viewed as
a classification problem. This data has several interesting
challenges that differentiate it from simple classification:

Categories and Subject Descriptors
H. 4 [Information Systems Applications]: Miscellaneous;
D. 2. 8 [Software Engineering]: Metrics—complexity measures, performance measures

General Terms
Theory

Keywords
Feature Engineering, Ensemble Tree Model

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
KDD Cup 2013 Workshop Chicago, Illinois USA
Copyright 2013 ACM978-1-4503-2495-3/13/08 ...$15.00.

• The data is collected from many different resources and
had not been cleaned yet. There are some authors and
papers with missing attributes, such as the published
year of paper, the affiliation of author.
• Many attributes about paper and author are shorttext, e.g.name, title, keywords, etc. It is known that
mining on short-text is a challenging problem.
• Only about 2.1% authors appears in both the training and validation data with totally 247204 authors.
It seems that we don’t have enough training data to
learn a model. We should excavate some important
predictive features in the whole data set and use some
suitable machine learning techniques to help us predict
on the test data.
To solve these challenges, we generate many predictive
features from the raw data of this competitions, such as the
year of paper, the count of papers which were published by
the author and so on. Finally we train two tree-base models
(GBDT [4], and RGF [5]) which will be introduced in the
Section 2 to ensemble these features. We find that the treebase model is effective tool to build our final model by using
the features which generated by us.
The remainder of the paper is organized as follows. We
introduce tree-base models used in our solution in Section 2.

In Section 3, we describe the our efforts in feature engineering. Section 4 present and analyze results of the experiments. Finally, we draw a conclusion in Section 5. Our implementation is available at https://github.com/LiJiefei/
kddcup2013_track1.

In this section, we will introduce two tree-based models
we used in our solution.

When applying the Gradient Boosting based on decision
trees we could get the Gradient Boosting Decision Tree model, which combines the elegance of decision tree without loss
of predicting accuracy. It had been proven of great practical
value when solving practical problems. Given the ‘training’
sample {yi , xi }N
1 , the goal is to find a function F (x) that
map x to y. A simple GBDT could be trained with the
following Algorithm 1.
There are many variances of GBDTs, we use the implementation in the python package scikit [7].

2.1 Tree-based Models

Algorithm 1 LAD TreeBoost

2.

MODELING

Many application problems in machine learning require
learning nonlinear functions from data. A popular method
to solve this problem is through decision tree learning (such
as CART [6] and C4.5 [8]), which has an important advantage for handling heterogenous data with case when different
features come from different sources.

2.1.1 The Basics
A tree-based model is usually constructed with decision
trees, which separate the feature space into different size of
cubes. When predicting, an instance will be given class label
or score according to the distribution of its corresponding
cube [10]. Such approach can avoid normalizing features
and handle nonlinear patterns gracefully.
In the author-paper identification problem, we are facing with many originated short-text attributes, from which
many numeric features will be extracted. The extracted
features are in many different ranges, e.g., tf-idf is a vector
restricted with components in range [0, 1] [1]. However, the
common words of papers will be integers varying from 0 to
over tens. When using models like Logistic Regression or
Factorization Machines it would be time consuming to scaling the features or searching the proper weights. However,
as mentioned above, tree-based models could free us from
such distress.
Despite the fact that a single tree can rarely reach a high
predicting accuracy, the simplicity of training a tree makes
it possible to combined trees together with approaches such
as boosting or ensemble to give a much better performance.

2.1.2 Gradient Boosting Decision Tree
The algorithm for Boosting Trees evolved from the application of boosting methods to decision trees. The general
idea is to compute a sequence of simple trees, where each
successive tree is built for the prediction residuals of the
preceding tree. It can be shown that such ‘additive weighted expansions’ of trees can eventually produce an excellent
fit of the predicted values to the observed values, even if
the specific nature of the relationships between the predictor variables and the dependent variable of interest is very
complex. Hence, the method of gradient boosting which fitting a weighted additive expansion of simple trees represents
a very general and powerful machine learning algorithm.
Gradient Boosting is one of the many boosting techniques
that integrate the strength of many different weak predictor
to produce a strong one. It builds the model in an addictive
way that the parameters of every new added predictor is optimized according to the gradient of the objective function.
When implementing with a square loss function, a naive implementation could be a series of predictors try to learned
the residuals of the previous ones [4].

1: F0 (x) = median{yi }N
1
2: for m=1 to M do
3:
ŷi = sign(yi − Fm−1 (xi ), i = 1, N )
4:
{Rj m}J1 = J-terminal node tree({ŷi , xi }N
1 )
5:
γjm = medianxi ∈Rjm {yi − Fm−1 (xi )}, j = 1, J
P
6:
Fm (x) = Fm−1 (x) + Jj=1 γjm 1(x ∈ Rjm )
7: end for

2.1.3 Regularized Greedy Forest
Despite its great power, GBDT still suffers from some
common issues of a machine learning model. It often gets
into a state of overfitting when training data is not enough or
some parameters are not selected properly, not to mention
the many parameters need to be selected manually or by
performing a grid-search, There are not only the gradient
boosting parameters, but also the decision tree itself, though
the parameter space is small enough. As GBDT treats the
decision tree model as a black box predictor, it will be helpful
if we could combine the two techniques together organically.
Regularized Greedy Forest is another tree-based model.
Its algorithm framework is similar with the GBDT algorithm. But it takes the structural information of a regression
tree into consideration rather than treats it as a black box
predictor. There are three differences between the RGF and
GBDT contributes to the improvement: an explicit regularization functionality preventing the model from overfitting,
fully-corrective greedy algorithm and the structured greedy
search directly over forest nodes based on the forest structure to accelerate the training progress [5].
In RGF algorithm framework 2, let F represent a forest,
Q(F ) is the objective function 1 . The following algorithm
essentially has two main components as follows.
• Fix the weights, and change the structure of the forest
(which changes basis functions) so that the loss Q(F )
is reduced the most (Line 2–4).
• Fix the structure of the forest, and change the weights
so that loss Q(F ) is minimized (Line 5)
The RGF [5] models take great performances in other
competitions, such as Bond Trade Price Challenge (www.
kaggle.com/c/benchmark-bond-trade-price-challenge),
Heritage Health Prize (www.heritagehealthprize.com/c/
hhp) and so on.

2.2 The MAP Metrics
In track 1, each author is given a paper list. We should
predict the confirmed probability for each paper in it and
1

for detail of the definition please read [5]

Algorithm 2 Regularized greedy forest framework
1: F ← {}
2: repeat
3:
ô ← argmino∈O(F )Q(o(F )) where O(F ) is a set of all
the structure-changing operations applicable to F
4:
if Q(ô(F )) ≥ Q(F ) then
5:
break
6:
end if
7:
F ← ô(F )
8:
if some criterion is met then
9:
optimize the leaf weights in F to minimize loss Q
(F)
10:
end if
11: until some exit criterion is met
12: Optimize the leaf weights in F to minimize loss Q (F)

sort it by confirmed probability in descending order for each
author. The MAP (Mean Average Precision) score is the
evaluation metrics in this competition.
ap@n =

n
X

P (k)/min(m, n)

i=1

M AP @n =

X

ap@n/|U |

u

where U is the user set with n users and ap is calculated by
the counting P (k), the precision at cut-off k in the paper
list, and the MAP is an average over all users.
The training data gives some confirmed and deleted papers for some authors. It is more intuitive that train the model
in a classification way rather then viewed it as a ranking
problem, although there are various ranking techniques to
optimize the MAP metrics. The tree-based models we used
can give the value of how likely the paper written by the
given author so that we can sort the paper list by this value.

3.

FEATURE ENGINEERING

Finding strong predictive features is critical to all machine
learning techniques. In this section, we introduce the features that are used in our final model. Since the underlying
dataset contains noise, the following generated features are
simply generated under the noise. We don’t remove any
noise from the dataset to generate our features.

3.1 Paper Features
The data in Track 1 provides some features about paper.
The following features about paper are included in the final
model:
• N umAuthorp : count of authors in the paper
• Y earp : the year of the paper, when the year is bigger
than 2013, we set it -1. Besides, when it is smaller
than 1600, we set it -2
• Cidp : the conference id of the paper
• Jidp : the journal id of the paper
• CountAf fp : count of different affiliations of the paper
• CountKwp : count of keywords of the paper

3.2 Author Features
Each author also has a set of basic features which are
extracted from PaperAuthor.csv and Author.csv. The following features are generated according to these two files.
• N umP apera : count of papers written by the author
• CountCona : count of papers which were published in
Conference by the author
• CountJoua : count of papers which were published in
Journal by the author

3.3 Author Paper Correlation Features
Each author has a set of papers in the dataset of Track 1.
We want to measure the correlation between the author and
paper. We can calculate the similarity between this paper
and other papers written by the same author. Beside, we
can find some interesting features which are very important,
such as the CountAppeara,p as following. If we just use
CountAppeara,p feature to rank, we can get the 0.956371
MAP in the public leaderboard.
• CountConOf P apera,p : count of papers which were
published in the paper’s conference by the author
• CountJouOf P apera,p : count of papers which were
published in the paper’s journal by the author
• CountAppeara,p: count of the pair of author and paper appeared in the file PaperAuthor.csv. It’s the most
important feature in our model. When the value of this
feature is lager than one, the author most likely confirm the paper. Because PaperAuthor table is collected
from some different sources. If there are more sources
indicate that the paper is written by the author, then
the author is more likely to confirm this paper.
• CountY earOf P apera,p: count of papers which were
published in the paper’s year by the author
• CountSameKwa,p : count of same keywords between
the paper and other papers written by the author

3.4 Coauthor Features
Besides, we can compute the similarity of coauthors to
help us do the classification. The following features are
about the coauthor features.
• SumP aperOf Coauthora,p : the sum count of papers
written by both the author and coauthors
• F lagSameLastN ameCoaa,p: whether there is any coauthor has the same last name with the author whose
name is indicated by the PaperAuthor.csv
• CountCoaN otInAutCsva, p: count of the coauthor
which does not appear in the Author.csv
• CountCommonSeta, p: firstly, we define the papers which were written by the author as a set S1a,p .
For the <author,paper>, we find all the other authors
which <other author, paper> appear in PaperAuthor.csv
more than once. Then we define the papers written by
any other author as a set S2a,p . The feature value is
the size of intersection between S1a,p and S2a,p . It
means that we calculate the size of intersection between some coauthors’ and the author’s published paper set.

• F lagSameW holeN ameCoaa,p: whether there is any
coauthor has the same whole name with the author
which the name is indicated by the PaperAuthor.csv.
We find that when some coauthor’s name is the same
with the author, the author is likely delete the paper.
• F lagSameLastN ameCoa2a,p: whether there is any
coauthor has the same last name with the author but
with different first name which the name is indicated
by the PaperAuthor.csv. When F lagSameLastN ame
Coa2a,p is equal to 1, it means that some coauthor
should not have the same first name with the author.
For example, when the author’s name is ‘Kaifu Lee’
and one coauthor also has the same name ‘Kaifu Lee’.
The F lagSameLastN ameCoaa,p is 1 while the F lag
SameLastN ameCoa2a,p is 0 because the first name
of coauthor is the same with the author. We find that
when the F lagSameLastN ameCoa2a,p is 1, the author more likely delete the paper.
• CountSameAf f ia,p : count of coauthor whose affiliation is the same with the affiliation of the author in
the Author.csv
• CountSameAf f i2a,p : count of coauthor whose affiliation is the same with the affiliation of the author in
the PaperAuthor.csv
• F lagSameN amea,p: whether the name of the author
in the PaperAuthor.csv is the same with the name in
the Author.csv
• F lagSameAf f ia,p : whether the name of the author
in the PaperAuthor.csv is the same with the name of
author in the Author.csv, when the pair <author, paper> appear more than once, we select the last given
affiliation as the affiliation of author
• F lagSameAf f i2a,p : whether the affiliation of author
in the PaperAuthor.csv is the same with the name of
author in the Author.csv, when the pair <author, paper> appear more than once, we select the affiliation
which appear more in the coauthors

3.5 Similarity Features
Trough the training data, we find that the name similarity between author and coauthors is very important. When
some coauthor’s name is similar with the author, the author
is most likely delete the paper. So we define two kinds of
similarity features about name as following. Firstly, we define a weighted bipartite graph about words of two names.
The nodes of graph are the words in the two names. The
weight of edge is the similarity between one word of one
name and one word of other name. The similarity has two
kinds, one is base on LCP (Longest Common Prefix) , the
other base on the Jaro-Winkler distance2 . Finally, we define
the name similarity as the value of maximum weighted bipartite matching about the weighted bipartite graph using
the Kuhn-Munkras algorithm3 .
We use Jaro-Winkler distance and LCP because they all
treat prefix of compared strings more important than other
parts of the strings. For example, ‘B. Lee’ and ‘Benjamin
2
http://en.wikipedia.org/wiki/Jaro%E2%80
%93Winkler_distance
3
http://en.wikipedia.org/wiki/Hungarian_algorithm

Lee’ are likely to be same names, even though ‘Benjamin’
differ from ‘B’ with the extra suffix ‘enjamin’. Both JaroWinkler distance and LCP will consider ‘B’ and ‘Benjamin’
are similar, because they do not count too much on suffixes.
But other common similarity functions, such as Levenshtein
distance4 , would severely affected by suffix ‘enjamin’ and do
not consider ‘B’ and ‘Benjamin’ are similar.
We use bipartite graph to deal with irregular name format. For example, ‘Benjamin Lee’ maybe written as ‘Lee
Benjamin M.’. We want the word ‘Benjamin’ and ‘Lee’ in
both names matched regardless of order and the extra word
‘M.’, so that we can judge these two names are similar. We
can split the first name into 2 words ‘Benjamin’ and ‘Lee’,
assuming their indices are 1 and 2. Similarly we can split
the second name into 3 words ‘Lee’, ‘Benjamin’ and ‘M.’, assuming their indices are 3, 4 and 5. Then we construct the
bipartite graph by adding edges between the two word sets,
setting the weight of the edges to be similarities calculated
by LCP or Jaro-Winkler distance. So the edge connecting
word 1 (‘Benjamin’ in first name) and word 4 (‘Benjamin’ in
second name) is weighted high. Similarly the edge connecting word 2 (‘Lee’ in first name) and word 3 (‘Lee’ in second
name) is also weighted high. But other edges, such as the
one connecting word 1 and word 5 (‘M.’ in second name)
is weighted low. After running maximum weighted bipartite matching we can correctly find the high-weighted edges
illustrated above and consider the two names are similar.
After using similarity functions focusing on prefix to build
bipartite graph, we can calculate the similarity of names
more precisely. Our model will consider ‘Benjamin Lee’ and
‘B. M. Lee’ are similar, but ‘Benjamin Lee’ and ‘M. Lee’ will
still not be considered similar.
In conclusion, assuming we want to compute the similarity
of name1 and name2. Firstly we remove the character ‘.’
and split the two names into two words array words1 and
words2.
Then the name similarity between name1 and name2 is
defined as following:
N ameSimi(name1, name2)
=

M W BP (graph(words1,words2))
,
min(size(words1),size(words2))

(1)

where MWBP means the maximum weighted bipartite matching on the graph builded by the words1 and words2.
• N ameSimiBaseLCPa,p : The weight of edge is the LCP of two words which the edge connect divided by
the minimum length of words. This feature contains 4
values, the mean, min, max and standard deviation
of similarity between coauthor’s name and author’s
name.
EdgeW eight(word1, word2)
=

LCP (word1,word2)
.
min(len(word1),len(word2))

(2)

• N ameSimiBaseJaroa,p :The weight of edge is the Jaro
similarity of two words which the edge connect divided by the minimum length of words. This feature
contain 4 values, the mean, min, max and standard
deviation of similarity between coauthor’s name and
author’s name.
4

http://en.wikipedia.org/wiki/Levenshtein_distance

=

EdgeW eight(word1, word2)

(3)

J aro(word1,word2)
.
min(len(word1),len(word2))

(4)

For example, when we calculate the similarity between
‘Benjamin Lee’ and ‘Lee Benjamin M.’. We use the LCP
similarity as the edge weight of the bipartite graph. Figure
1 shows the graph.

validation set, we merged the training data and the validation set as our whole training data. We randomly split the
whole training data into training set and validation set ten
times which always keep the train/test ratio 7 : 3 to be our
private validation data.
In the experiments, we first train the models on the training set, use the validation result to determine the parameters about our model (mainly the max depth and learning
rate in GBDT, the regularization and weight of instance in
RGF), and then apply the same parameters to the models
trained on the whole training set and get the predictions on
the test set. Finally we average some models to get our final
submission.
We conduct our experiment in an incremental way. During the competition, we add one feature into our model every
time. When the mean MAP score in our private validation
data increase, we will keep the feature.

4.2 Results and Discussions
Table 2 shows the results of different methods on the data
set. We get rather comparable results from both GBDT and
RGF, which implies that we may consider combining both
to get better results.
Figure 1: Example for bipartite graph
The maximum weighted bipartite matching on the this
graph is 2. The min length between these two names is 2.
So the similarity is 1.0.

3.6 Other Features
Here are some of other features that are included in the
final model.
• CountP aperLista : the count paper in the author’s
predict paper list.
• AvgP aperOf Coauthora,p: the average count of papers which were published by both the author and any
other coauthor of the paper
• RatioOf Joua,p : CountJouOf P apera,p / CountJoua
• RatioOf Cona,p : CountConOf P apera,p / CountCona
• RatioOf Af f ia,p : CountSameAf f i2a,p / CountAf fp
• RatioOf Kwa,p : CountSameKwa,p / CountKwp

3.7 Summary
Table 1 summary the above 39 features we used in our
final model.

4.

EXPERIMENTS

4.1 Experiment Setup
As some authors only have two papers in the predicted
paper list, we find that if we predict one author error, the
overall MAP sum will decrease by 0.000334 in the public
leaderboard because there are only 1497 authors in the validation set. When the test set is smaller, the MAP decrease
more. Thus, we should keep the ratio about the number
authors in training set and test set. We can find that the
ratio about the number of authors in training data, validation data and test data is 5 : 2 : 3. After we got the

4.2.1 Model Parameter
For the GBDT model, we use the parameters whose
max depth = 4, learning rate = 0.1, num iter = 100.
Other parameters are the default value of GBDT in sklearn.
For the RGF model, we use the parameters whose
max leaf f orest = 1000, reg L2 = 0.01, loss = Log,
algorithm = RGF and use the instance weight file. The
instance weight file indicates the weighting of each instance
when training the RGF model. For each author, the weight
for each paper is set
1/(number of papers which author confirm)
because we think that the author with less confirm papers
is more important.

4.2.2 Feature Selection
We do the feature selection base on these 39 features. We
just remove any 1-4 features, which don’t make any important impact on the final MAP score. Table 3 shows the MAP
scores, where we remove some features.

4.2.3 Postprocess
We find that author has higher confirm ratio when the
CountAppeara,p is lager than one. So we add a high score
to the paper whose CountAppeara,p is larger than one when
we generate the ranked paper list. So this kind of paper will
always be in front of other papers. Table 4 and Table 5 show
that the performance of using this postprocess algorithm.

4.2.4 Ensemble
To avoid overfitting, our final submission average some
models with different feature set and different parameter of
models (RGF and GBDT) and get 0.98075 on the private
leaderboard.
We finally generate 196 models, which use different model
parameters and different features set. For the GBDT model,
we generate 48 models, which use the max depth = 4,
learning rate = 0.1, 0.08, num iter = 100 for each remove feature set mentioned above and whether to use the

ID
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26-29
30-33
34
35
36
37
38
39

No.
1
2
3
4
5

Name
N umAuthorp
Y earp
Cidp
Jidp
CountAf fp
CountKwp
N umP apera
CountCona
CountJoua
CountConOf P apera,p
CountJouOf P apera,p
CountAppeara,p
CountY earOf P apera,p
CountSameKwa,p
SumP aperOf Coauthora,p
F lagSameLastN ameCoaa,p
CountCoaN otInAutCsva,p
CountCommonSeta,p
F lagSameW holeN ameCoaa,p
F lagSameLastN ameCoa2a,p
CountSameAf f ia,p
CountSameAf f i2a,p
F lagSameN amea,p
F lagSameAf f ia,p
F lagSameAf f i2a,p
N ameSimiBaseLCPa,p
N ameSimiBaseJaroa,p
CountP aperLista
AvgP aperOf Coauthora,p
RatioOf Joua,p
RatioOf Cona,p
RatioOf Af f ia,p
RatioOf Kwa,p

Table 1: Features
Description
count of authors in the paper
the year of paper
the conference id of paper
the journal id of paper
count of different affiliations of paper
count of keywords of paper
count of papers written by the author
count of papers which were published in Conference by the author
count of papers which were published in Journal by the author
count of papers which were published in the paper’s conference by the author
count of papers which were published in the paper’s journal by the author
count of the pair of author and paper appeared in the file PaperAuthor.csv
count of papers which were published in the paper’s year by the author
count of same keywords between the paper and other papers written by the author
the sum count of papers which were written by both author and coauthors
described in Section 3.4
count of the coauthor which does not appear in the author.csv
described in Section 3.4
described in Section 3.4
described in Section 3.4
described in Section 3.4
described in Section 3.4
described in Section 3.4
described in Section 3.4
described in Section 3.4
the mean, min, max and standard deviation of LCP similarity
the mean, min, max and standard deviation of Jaro similarity
the count paper in the author’s predict paper list
the average count of papers which were written by both author and coauthor papers
CountJouOf P apera,p / CountJoua
CountConOf P apera,p / CountCona
CountSameAf f i2a,p / CountAf fp
CountSameKwa,p / CountKwp

Table 2: MAP@10000 of different
GBDT
Model
validation
public
Paper Features + Author Features
0.866168
0.864056
1 + Author Paper Correlation Features 0.969840
0.971672
2 + Coauthor Features
0.979522
0.979051
3 + Similarity Features
0.981394
0.981628
4 + Other Features
0.982377
0.981967

postprocess algorithm. For the RGF model, we generate
144 models, which use the max leaf f orest = 1000, 2000,
reg L2 = 0.01, 0.02, 0.005, loss = Log, algorithm = RGF
for each remove feature set mentioned above and whether
use the postprocess algorithm.
First we should define the score of subset models
Subset M odels. For each author, we set the paper score is
X
Scoreauthor,paper =
Rankmodel,author,paper
model∈Subset M odels

(5)
Rankmodel,author,paper means the rank of paper in the author’s ranked list of the model. We use the paper score to
rank papers for each author. The score of Subset M odels is
the MAP score using the (5) to rank the papers.
We set the Subset M odels is some top 4 RGF models and

methods
private
0.86181
0.96812
0.97800
0.97983
0.98003

validation
0.863935
0.970083
0.979522
0.982490
0.983546

RGF
public
0.861282
0.971624
0.979051
0.981861
0.982229

private
0.85941
0.96895
0.97794
0.98026
0.98055

top 3 GBDT models in our private validation set. Finally,
we get the 0.9839 in our private validation set, which is small
lager than any single model but only get the 0.98075 in the
private leaderboard.

5. CONCLUSION
In this paper, we describe our solution to track 1 of the
KDD cup 2013 Author-Paper Identification task [2]. Feature engineering is the most important part of our model.
When we generate one feature, we will add this feature to
our feature set and train the model in our private validation
set. If the mean MAP in our private validation set increase,
we will keep this feature. After we generated the features,
we use the GBDT and RGF to build our classification model. Besides, we also use LambdaMart and other pairwise or

No.
1
2
3
4
5
6
7
8
9
10
11
12

No.
1
2
3
4
5
6
7
8
9
10
11
12

No.
1
2
3
4
5
6
7
8
9
10
11
12

Table 3: MAP@10000 of different features
GBDT
Features Removed
validation
public
private
CountJouOf P aper,
0.982448
0.982451
0.98029
CountConf Of P aper, N umP aper
RatioOf Con
0.982483
0.982728 0.98026
1+2
0.982448
0.982451
0.98025
N ameSimiBaseLCPa,p
0.982066
0.982213
0.98079
N ameSimiBaseJaroa,p
0.982518
0.982675
0.97982
1 + N ameSimiBaseJaroa,p
0.982546
0.982141
0.97958
2 + N ameSimiBaseJaroa,p
0.982166
0.981494
0.98044
3 + N ameSimiBaseJaroa,p
0.982615 0.982448
0.97962
1 + N ameSimiBaseLCPa,p
0.982542
0.981053
0.98034
2 + N ameSimiBaseLCPa,p
0.982228
0.981568
0.98074
3 + N ameSimiBaseLCPa,p
0.982302
0.981922
0.98023
None
0.982377
0.981967
0.98003

validation
0.983369

RGF
public
0.982760

private
0.98123

0.983306
0.983511
0.983247
0.983287
0.983178
0.983248
0.983165
0.983166
0.983212
0.983229
0.983546

0.982593
0.982726
0.982217
0.981750
0.981493
0.982450
0.982513
0.982347
0.982527
0.982463
0.982229

0.98099
0.98097
0.98098
0.98025
0.98031
0.98050
0.98033
0.98033
0.98122
0.98148
0.98055

Table 4: MAP@10000 of using Postprocess for GBDT
GBDT
Features Removed
validation
public
no-post
post
no-post
post
CountJouOf P aper,
0.982448
0.982569
0.982451
0.9829
CountConf Of P aper, N umP aper
RatioOf Con
0.982483
0.982592
0.982728 0.982941
1+2
0.982448
0.982754
0.982451
0.982727
N ameSimiBaseLCPa,p
0.982066
0.982304
0.982213
0.982739
N ameSimiBaseJaroa,p
0.982518
0.982579
0.982675
0.982873
1 + N ameSimiBaseJaroa,p
0.982546
0.982629
0.982141
0.982146
2 + N ameSimiBaseJaroa,p
0.982166
0.982265
0.981494
0.982046
3 + N ameSimiBaseJaroa,p
0.982615 0.982633
0.982448
0.982579
1 + N ameSimiBaseLCPa,p
0.982542
0.982701
0.981053
0.982197
2 + N ameSimiBaseLCPa,p
0.982228
0.982326
0.981568
0.981841
3 + N ameSimiBaseLCPa,p
0.982302
0.982498
0.981922
0.982328
None
0.982377
0.982536
0.981967
0.982224

Table 5: MAP@10000 of using Postprocess for RGF
RGF
Remove Feature
validation
public
no-post
post
no-post
post
CountJouOf P aper,
0.983369
0.983215 0.982760 0.983228
CountConf Of P aper, N umP aper
RatioOf Con
0.983306
0.983184
0.982593
0.982961
1+2
0.983511
0.983345
0.982726
0.983083
N ameSimiBaseLCPa,p
0.983247
0.983199
0.982217
0.982618
N ameSimiBaseJaroa,p
0.983287
0.983077
0.981750
0.981932
1 + N ameSimiBaseJaroa,p
0.983178
0.983011
0.981493
0.981689
2 + N ameSimiBaseJaroa,p
0.983248
0.983091
0.982450
0.982651
3 + N ameSimiBaseJaroa,p
0.983165
0.983024
0.982513
0.982718
1 + N ameSimiBaseLCPa,p
0.983166
0.983124
0.982347
0.982748
2 + N ameSimiBaseLCPa,p
0.983212
0.983156
0.982527
0.982953
3 + N ameSimiBaseLCPa,p
0.983229
0.983181
0.982463
0.982933
None
0.983546 0.98338 0.982229
0.982605

listwise ranking algorithms [3, 9] to build our model. But
they don’t increase our MAP base on our features. Finally,
we average some models to ensemble some models.
Our solution is ranked 5th on the private board, and 4th
on the public board. The top team have 0.9825 MAP on the
private board, showing that there are still lots of room for

private
no-post
post
0.98029
0.98069
0.98026
0.98025
0.98079
0.97982
0.97958
0.98044
0.97962
0.98034
0.98074
0.98023
0.98003

0.98049
0.98038
0.98097
0.98011
0.97967
0.98054
0.97969
0.98046
0.98080
0.98047
0.98045

private
no-post
post
0.98123
0.98099
0.98099
0.98097
0.98098
0.98025
0.98031
0.98050
0.98033
0.98033
0.98122
0.98148
0.98055

0.98059
0.98062
0.98046
0.97997
0.97995
0.98018
0.97999
0.98040
0.98093
0.98120
0.98022

our solution to improve.

6. REFERENCES
[1] Aizawa, A. An information-theoretic perspective of
tf–idf measures. Information Processing &
Management 39, 1 (2003), 45–65.

[2] Basu Roy, S., De Cock, M., Mandava, V.,
Dalessandro, B., Perlich, C., Cukierski, W.,
and Hamner, B. The microsoft academic search
dataset and kdd cup 2013. KDD Cup 2013 workshop
(2013).
[3] Burges, C. From ranknet to lambdarank to
lambdamart: An overview. Learning 11 (2010),
23–581.
[4] Friedman, J. H. Greedy function approximation: a
gradient boosting machine. Annals of Statistics
(2001), 1189–1232.
[5] Johnson, R., and Zhang, T. Learning nonlinear
functions using regularized greedy forest. Tech. rep.,
Technical report, 2012.
[6] Olshen, L. B. J. F. R., and Stone, C. J.
Classification and regression trees. Wadsworth
International Group (1984).
[7] Pedregosa, F., Varoquaux, G., Gramfort, A.,
Michel, V., Thirion, B., Grisel, O., Blondel,
M., Prettenhofer, P., Weiss, R., Dubourg, V.,
Vanderplas, J., Passos, A., Cournapeau, D.,
Brucher, M., Perrot, M., and Duchesnay, E.
Scikit-learn: machine learning in python. Journal of
Machine Learning Research 12 (2011), 2825–2830.
[8] Shalev-Shwartz, S., Srebro, N., and Zhang, T.
Trading accuracy for sparsity in optimization
problems with sparsity constraints. SIAM Journal on
Optimization 20, 6 (2010), 2807–2832.
[9] Xu, J., and Li, H. Adarank: a boosting algorithm for
information retrieval. In Proceedings of the 30th
annual international ACM SIGIR conference on
Research and development in information retrieval
(2007), ACM, pp. 391–398.
[10] Yuan, Y., and Shaw, M. J. Induction of fuzzy
decision trees. Fuzzy Sets and systems 69, 2 (1995),
125–139.

Reliability Engineering and System Safety 134 (2015) 1–9

Contents lists available at ScienceDirect

Reliability Engineering and System Safety
journal homepage: www.elsevier.com/locate/ress

Simulation-based Bayesian optimal ALT designs for
model discrimination
Ehab A. Nasir a, Rong Pan b,n
a
b

Intel Corporation, Chandler, AZ, United States
School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, United States

art ic l e i nf o

a b s t r a c t

Article history:
Received 15 March 2014
Received in revised form
5 September 2014
Accepted 2 October 2014
Available online 13 October 2014

Accelerated life test (ALT) planning in Bayesian framework is studied in this paper with a focus of
differentiating competing acceleration models, when there is uncertainty as to whether the relationship
between log mean life and the stress variable is linear or exhibits some curvature. The proposed criterion
is based on the Hellinger distance measure between predictive distributions. The optimal stress-factor
setup and unit allocation are determined at three stress levels subject to test-lab equipment and testduration constraints. Optimal designs are validated by their recovery rates, where the true, datagenerating, model is selected under the DIC (Deviance Information Criterion) model selection rule, and
by comparing their performance with other test plans. Results show that the proposed optimal design
method has the advantage of substantially increasing a test plan's ability to distinguish among
competing ALT models, thus providing better guidance as to which model is appropriate for the
follow-on testing phase in the experiment.
& 2014 Elsevier Ltd. All rights reserved.

Keywords:
Reliability test plans
Hellinger distance
Model selection
Deviance Information Criterion (DIC)
Non-parametric curve ﬁtting

1. Motivation for work
Most work of the optimal Accelerated Life Testing (ALT) designs in
the literature has focused on ﬁnding test plans that allow more precise
estimate of a reliability quantity, such as life percentile, at a lower
stress level (it is usually the use stress level); see, for example, Nelson
and Kielpinski [1] and Nelson and Meeker [2]. Nelson [3,4] summarized the ALT literature up to 2005 and a signiﬁcant portion of this
article is devoted to the optimal design of ALT planning. More recent
discussions of optimal ALT plans and/or robust ALT plans can be found
in, e.g., Xu [5], McGree and Eccleston [6], Monroe et al. [7], Yang and
Pan [8], Konstantinou et al. [9], and Haghighi [10]. In the previous
study, the associated conﬁdence intervals of an estimate reﬂect the
uncertainty arising from limited sample size and censoring at test, but
do not account for model form inadequacy. However, model errors can
be quickly ampliﬁed and potentially dominate other sources of errors
in reliability prediction through the model-based extrapolation that
characterizes ALTs. Implicit in the design criteria used in current ALTs
is the assumption that the form of the acceleration model is correct. In
many real-world problems this assumption could be unrealistic. A
more realistic goal of an initial stage of ALT experimentation is to ﬁnd
an optimal design that helps in selecting a model among rival or

n

Corresponding author. Tel.: þ 1 480 965 4259.
E-mail address: rong.pan@asu.edu (R. Pan).

http://dx.doi.org/10.1016/j.ress.2014.10.002
0951-8320/& 2014 Elsevier Ltd. All rights reserved.

competing model forms. The ALT designs that are good for model form
discrimination could be quite different from those that are more
appropriate for life percentile prediction under a speciﬁc model.
Extrapolation in both stress and time is a typical characteristic
of ALT inference. The most common accelerated failure time
regression models (based, for example, on Lognormal or Weibull
ﬁt to the failure time distribution at a given stress level) are only
adequate for modeling some simple chemical processes that lead
to failure (Meeker and Escobar [11]). However, for modern
electronic devices, more sophisticated models with basis in the
physics of failure mechanisms are required. These complicated
models are expected to have more parameters with possible
interactions among stress factors. Therefore, investigating ALT
designs with model selection capability is needed more than ever
before. Meeker et al. [12] in their discussion of ﬁgures of merit
when developing an ALT plan emphasizes the usefulness of a test
plan's robustness to the departure from the assumed model. For
example, when planning a single-factor experiment under a linear
model, it is useful to evaluate the test plan properties under a
quadratic model. Also, when planning a two-factor experiment
under the assumption of a linear model with no interaction, it is
useful to evaluate the test plan properties under a linear model
with an interaction term. We strongly believe that it is worthwhile
to consider these recommended practices ahead of time when the
test plan is being devised in the ﬁrst place by allowing a design
criterion that is capable of model form discrimination.

2

E.A. Nasir, R. Pan / Reliability Engineering and System Safety 134 (2015) 1–9

2. Previous work
A considerable work has been done in the development of
experimental designs for discrimination among linear regression
models; see, for example, Hunter and Reiner [13], Box and Hill
[14], Hill et al. [15], and Atkinson and Cox [16]. A comprehensive
review of early contributions is given by Hill [17]. More recently,
many authors focused on the development of T-optimum criterion
(non-Bayesian) for model discrimination (de Leon and Atkinson
[18] and Atkinson et al. [19]). Dette and Titoff [20] derived new
properties of T-optimal designs and showed that in nested linear
models, the number of support points in a T-optimal design is
usually too small to enable the estimate of all parameters in the
full model; Agboto et al. [21] reviewed T-optimality among other
new optimality criteria for constructing two-level optimal discrimination designs for screening experiments. These work resulted
in sequential experimentation procedures.
Bayesian criteria were also considered in model discrimination.
Meyer et al. [22] considered a Bayesian criterion that is based on
the Kullback–Leibler information to choose follow-up run after a
factorial design to de-alias rival models. Bingham and Chipman
[23] proposed a Bayesian criterion that is based on the Hellinger
distance between predictive densities for choosing optimal
designs for model selection with prior distributions speciﬁed for
model coefﬁcients and errors. For a comprehensive review on
Bayesian experimental design reader is referred to Chaloner and
Verdinelli [24].
There are three types of uncertainties involved in the ALT
planning—the uncertainty of failure time distribution, the uncertainty of lifetime-stress relationship and the uncertainty of model
parameter value (Pascual [25]). Bayesian methods have been
proposed for ALT planning to deal with the uncertainty of model
parameter (Zhang and Meeker [26]; Yuan et al. [27]), but, to our
knowledge, none has been explicitly targeting the model discrimination of life-stress functions. All of the previous attempts at
model discrimination have been in the context of traditional
experimental design for linear models, while the failure time
regression models used in ALTs are nonlinear. In particular, failure
time censoring is commonly expected in ALT experiments. Nelson
[28] (p. 350) has cautioned that the statistical theory for traditional experimental design is correct only for complete data, one
should not assume that properties of standard experimental
designs hold for censored and interval-censored data as they
usually do not hold. For example, aliasing of effects may depend
on the censoring structure. In addition, the variance of an estimate
of a model coefﬁcient depends on the amount of censoring at all
test conditions and on the true value of (possibly all) model
coefﬁcients. Thus, the censoring times at each test condition are
part of the experimental design and affect its statistical properties.
As such, our current work draws its importance from its attempt at
contributing to model discrimination literature for accelerated life
test planning when censoring is inevitable.

therefore the larger the distance (disagreement) in prediction the
better our ability to discriminate (distinguish) among these competing models. Therefore, we propose to use the relative prediction
performance of each model over the range of its parameters to
identify the optimal design. Fig. 1 shows how important it is for the
experimenter to arrive at the best representative model to reduce
prediction errors at use conditions (UCs). For example, if M 1 is the
true model but experimenter assumes M 2 , then under ALT extrapolation the error in prediction of a quantile of interest at use
conditions, Δ^τp ðUCÞ, is much worse than any predictions at tested
conditions.
To distinguish predictive distributions from rival models, the
Hellinger distance, as a measure of disagreement between predictive densities, is used in this work.
3.2. Distance (divergence) measure of probability distributions
There are a substantial number of distance measures applied in
many different ﬁelds such as physics, biology, psychology, information theory, etc. See Cha and Srihari [31] for a comprehensive survey
on distance/similarity measures between probability density functions. From the mathematical point of view, distance is deﬁned as a
quantitative measure of how far apart two objects are. In statistics
and probability theory, a statistical distance quantiﬁes the dissimilarity between two statistical objects, which can be two random
variables or two probability distributions. A measure Dðx; yÞ
between two points x; y is said to be a distance measure or simply
distance if
I.
Dðx; yÞ 4 0 when x a y and Dðx; yÞ ¼ 0 if and only if x ¼ y;
II.
Dðx; yÞ ¼ Dðy; xÞ;
III.
Dðx; yÞ þ Dðy; zÞ Z Dðx; zÞ:

Conditions (I) through (III) imply, respectively, that the distance
must be non-negative (positive deﬁnite), symmetric and subadditive (triangle inequality: the distance from point x to z directly
must be less than or equal to the distance in reaching point z
indirectly through point y).
The choice of a distance measure depends on the measurement
type or representation of quantities under study. In this study, the
Hellinger distance ðDH Þ (Deza and Deza [29]) is chosen to measure
the distance between the two probability distributions that
represent the distributions of τ^ p at lower and higher ALT stress

3. Proposed methodology
3.1. Rationale for model discrimination methodology
Suppose that the objective is to arrive at an ALT test plan that is
capable of discriminating among competing acceleration models.
Assume that there are two rival models and it is better that the
experimental data can help in choosing one of them. Intuitively, a
good design should be expected to generate far apart results based
on the two competing models, and then the experimenter can
select the model based on the actual observations from the
experiment. In ALT, the lifetime percentile is typically of interest;

^ 1 versus M
^ 2 at UCs: importance of identifying correct model.
Fig. 1. M

E.A. Nasir, R. Pan / Reliability Engineering and System Safety 134 (2015) 1–9

test conditions. Computing the distance between two probability
distributions can be regarded as the same as computing the Bayes
(or minimum misclassiﬁcation) probability of misclassiﬁcation
(Duda et al. [30] and Cha and Srihari [31]). For the discrete
probability distributions P ¼ ðp1 …pk Þ and Q ¼ ðq1 …qk Þ, the Hellinger
distance ðDH Þ is deﬁned as
sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
k
pﬃﬃﬃﬃ pﬃﬃﬃﬃ
1
DH ðP; Q Þ ¼ pﬃﬃﬃ ∑ ð pi  qi Þ2
ð1Þ
2 i¼1
This is directly related to the Euclidean norm of the difference
of the square root vectors
1 pﬃﬃﬃ pﬃﬃﬃﬃ
DH ðP; Q Þ ¼ pﬃﬃﬃ‖ P  Q ‖2
2

ð2Þ

For the continuous probability distributions, the squared
Hellinger distance is deﬁned as
Z

1 2
1  12
px  q2x dx
D2H ðP; Q Þ ¼
2
Z
pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
¼ 1
px qx dx
ð3Þ

Hellinger distance follows the triangle inequality and 0 r DH
ðP; Q Þ r 1: The maximum distance of 1 is attained when P assigns
probability zero to every set to which Q assigns a positive
probability, and vice versa.
3.3. Criterion for model discrimination
In Bayesian framework of experimental design, the problem of
n
optimal design can be thought of as ﬁnding a design, d , such that
it maximizes a utility function UðdÞ that quantiﬁes the objective of
the experiment (which is the model form distinguishability in
our case).
Suppose that under design d; the experimental outcome may
be generated by one of the following two models:

 Model 1, M 1 , with its parameter vector θ1 , its outcome denoted
by Y 1 ¼ ðy11 ; …; yN1 Þ

 Model 2, M2 , with its parameter vector θ2 , its outcome denoted
by Y 2 ¼ ðy12 ; …; yN2 Þ.
Consider, as an initial utility function to be maximized, the
difference in prediction of life percentile of interest τp at the low
stress τp ðS1 Þ of the ALT test setup across all pairs of competing
models. Ultimately, interest lies in the prediction of the 1st
percentile of life distribution at use condition, τ0:01 . Since the
lower stress level is the closest to the use stress level, a large
difference in prediction at the lower level will give rise to an even
larger difference in prediction at the use level (due to the
extrapolation error). Therefore, a design that may generate larger
difference in the failure time at the lower stress level among rival
models is preferable in discrimination sense. However, selection of
the lower stress level to optimize the local utility function may run
the risk of not enough fails obtained to sufﬁciently estimate life
distribution percentiles. Therefore, we consider the simultaneous
difference in prediction of life percentile of interest, τp , at the
lower stress τp ðS1 Þ and the higher stress τp ðS2 Þ test setup across all
pairs of competing models. This study considers constant-stress
ALT plans, where no interaction between stress variables is
assumed. It is also assumed that the disperse parameter of log
(life) distribution does not depend on stress.

3

For the two competing models, M 1 and M 2 , the pairwise local
utilities are as follows:


u2j1 ðd; M 1 ðθ1 ; Y 1 Þ; M 2 ðθ2 ; Y 1 ÞÞ ¼ DS1 τ^ p;ðM2jY1Þ ; τ^ p;ðM1jY1Þ


þ DS2 τ^ p;ðM2jY1Þ ; τ^ p;ðM1jY1Þ ¼ u2j1
ð4Þ


u1j2 ðd; M 1 ðθ1 ; Y 2 Þ; M 2 ðθ2 ; Y 2 ÞÞ ¼ DS1 τ^ p; ðM1jY2Þ ; τ^ p; ðM2jY2Þ


þ DS2 τ^ p; ðM1jY2Þ ; τ^ p; ðM2jY2Þ ¼ u1j2

ð5Þ

where DS1 ð U Þ and DS2 ð U Þ denote the Hellinger distance at the lower
stress and the higher stress, respectively. Eq. (4) represents the
difference in τp prediction of model (M 2 ) conditional on data from
model (M 1 ) relative to model (M 1 ) prediction of the same quantity,
while Eq. (5) represents the difference in τp prediction of model
(M 1 ) conditional on data from model (M 2 ) relative to model (M 2 )
prediction of the same quantity. That is the relative prediction
performance of each model over the range of its parameter vector.
At the time of designing an experiment, the experimental
outcome is yet to observe and the true model form and its
parameter vector are unknown. Therefore, the utility uijj ð UÞ of a
design is assessed by its expectation with respect to the sampling
distribution of the data pðy1 jθ1 ; dÞ, and pðy2 jθ2 ; dÞ, and the prior
distribution of the parameter vectors πðθ1 Þ and πðθ2 Þ. That is
calculating the pre-posterior expectation




E u2j1 ¼ ∬ u2j1 p y1 jθ1 ; d π ðθ1 jdÞdy1 dθ1
ð6Þ




E u1j2 ¼ ∬ u1j2 p y2 jθ2 ; d π ðθ2 jdÞdy2 dθ2

ð7Þ

Eq. (6) gives an expression of the expected pre-posterior
prediction difference in τp of M 2 conditional on data from model
M 1 relative to model M 1 prediction of the same quantity. The
reverse is true for Eq. (7).
Since it is not known which of the two models (M 1 ) or (M 2 ) is
the true model, a weighted sum of expected utilities Eðuijj Þ is
obtained as the desired global utility function U ðdÞ to be maximized. The weighing is achieved by priors assigned to the models,
π ðM 1 Þ and π ðM 2 Þ respectively.
U ðdÞ ¼

∑
π ðM i ÞEðuijj Þ
i; j ¼ 1; 2
i aj

¼ π ðM 1 ÞEðu2j1 Þ þπ ðM 2 ÞEðu1j2 Þ

ð8Þ

Eq. (8) can be interpreted as a measure of model distinguishability
between two models. The larger the value of U ðdÞ; the dissimilar the
two models are to each other. Extending (8) to account for situations
where more than two models are to be distinguished among is
straightforward.
As can be seen from Eqs. (6)–(8), arriving at an optimal design
n
d that maximizes (8) is a nontrivial task due to the high
dimensional integration and optimization required. There is no
closed form solution to (8). Numerical evaluation of the multiple
integral for a given choice of design will be needed, which in itself
is a formidable task given the fact that the integration is deﬁned
over the data space and parameter space. The obtained estimate of
U ðdÞ must then be maximized over the design variable, d, which is
in often cases a multidimensional vector. We use a Monte Carlo
n
simulation-based approach to ﬁnd the optimal design, d . As nonsequential ALT designs are always performed off-line, the relatively heavy computation requirement of this approach is not a
critical issue.
Fig. 2 presents a high level ﬂow of the proposed methodology.
For a candidate design, it is evaluated by the utility function (8)
according to the assumed model and the true model. The numbered steps of this process are explained below.

4

E.A. Nasir, R. Pan / Reliability Engineering and System Safety 134 (2015) 1–9

1

1
2

2

4a
3a

3a

4b

3bb

3b

5
Fig. 2. High level methodology ﬂow chart.

Step 1—Fail data Y 1 are generated from the acceleration model
M 1 (the assumed true model). For those failure times that
exceed the test censoring time, they are replaced by the
censoring time.
Step 2—Given model M 1 is assumed, the failed data are
combined through Bayes's theorem with the prior information
available on parameters π M1 ðθÞ to produce the posterior estimates of the parameter, P M1 ðθjY 1 Þ. Repeat it for model M 2 .
Step 3—Posterior distribution of the predicted life percentile of
interest τp is obtained using Gibbs sampler at both high and
low stress conditions, which are Steps 3a and 3b. Same steps
are repeated on same data set Y 1 using rival model M 2 .
Step 4—The Hellinger distances between the prediction distributions under the true model M 1 and the rival model M 2 are
obtained for both the lower and higher stress levels.
Step 5—The sum of the Hellinger distances is denoted as the
local utility.
If there are more than one rival model, this process will be
repeated for models M 2 through M m . Local utilities are then
weighted by model priors and summarized into a global utility.
This utility value gives the overall performance of model discrimination of a candidate design.
An R program is written to automate the process. It ﬁrst
generates random fail data according to the true acceleration
model. Then, it calls WinBUGS to perform Bayesian inference by
Markov chain Monte Carlo (MCMC). Using WinBUGS, a stream of

samples from the posterior distribution of the life percentile under
an assumed acceleration model will be generated and they are
feedback to the R program to compute the Hellinger distance
and the utility value. Eventually, multiple candidate designs will
be evaluated and a response surface model is used to ﬁt their
utility values. The best design can be found by maximizing the
ﬁtted model.

4. Model selection criterion
In this section, the tools that are used for validating the
obtained optimal designs are introduced. It can be shown that
these designs are indeed optimal under desired optimality criterion as they maximize the proportion of times (the recovery rate) in
which the true, data-generating, model is selected under an
appropriate model selection rule.
The Deviance Information Criterion (DIC) was introduced by
Spiegelhalter et al. [32] as an easily computed and rather universally applicable Bayesian criterion for posterior predictive
model comparison. It compromises between data ﬁt and model
complexity, like many other non-Bayesian criteria. It generalizes
Akaike's information criterion (AIC) that appears as a special case
under a vague prior (negligible prior information), and Bayesian
information criterion (BIC), also known as Schwarz criterion. DIC is
particularly useful in Bayesian model selection problems where
the posterior distributions of the models have been obtained by

E.A. Nasir, R. Pan / Reliability Engineering and System Safety 134 (2015) 1–9

MCMC simulation, because it can be directly computed using the
MCMC samples. Claeskens and Hjort (Ch. 3.5) [33] show that the
DIC is large-sample equivalent to the natural model-robust version
of the AIC.
Deﬁne the following

 Deviance is deﬁned as DevðθÞ ¼ 2 log ½pðyjθÞ þC, where y are




the data, θ are vector of model unknown parameters, pðyjθÞ is
the likelihood function and C is a constant term that cancels
out when comparing models.
Expectation is deﬁned as Dev ¼ Eθ ½DevðθÞ. This measures how
well a model ﬁts the data, the larger the value, the worse the ﬁt.
Effective number of model parameters is deﬁned as pD ¼
Dev  DevðθÞ, where θ is the expectation of θ: The larger the
pD ; the easier for the model to ﬁt the data.

Finally, DIC is deﬁned as a classical estimate of ﬁt plus twice the
effective number of parameters, i.e.
 
DIC ¼ Dev θ þ 2pD
¼ Dev þ pD

ð9Þ

When comparing models, models with smaller DIC are preferred to models with larger DIC. Models are penalized both by the
value of Dev, which favors a good ﬁt, but also (in common with
AIC and BIC) by the effective number of parameters pD . Since Dev
decreases as the number of parameters in a model increases, the
pD term compensates for this effect by favoring models with a
smaller number of parameters.

5. Methodology illustration
In this section, we use a real industrial example to demonstrate
the proposed methodology. The R and WinBUGS codes of this
example had been submitted to the publisher's website as the
supplementary material.
5.1. Description of design problem
Reliability engineer is interested in studying the intermetallic
growth of Au–Al interface in a semi-conductor assembly. It is
known that fail mechanism of interest is activated by temperature
stress so an accelerated life test is desired in order to estimate the
device lifetime. However, there is an uncertainty as to whether the
relationship between log (life) and the stress (possibly transformed) is linear or exhibit some curvature as indicated by an
early look-ahead data set. As a result, current interest lies in an
accelerated life test plan that is capable of discriminating between
linear and quadratic acceleration models in temperature stress.
There are also constraints imposed by available budget for testing
(test units), and stress-lab equipment availability and capability as
shown below.

 Bake stress chambers are available for 42 days (1008 h maximum test time).

 Two types of bake ovens are available with different temperature range capabilities.
– The lower stress bake oven can be set to run temperature
range from 60 1C to 115 1C.
– The higher stress bake oven can be set to run temperature
range from 100 1C to 250 1C.
The equipment's tolerance is estimated at þ/  5 C.

 Experimental budget allows for no more than 20 runs.

5

The engineer's objective is to determine a feasible test plan,
including the stress level settings and the test unit allocation at
each stress level, so as to discriminate between the two competing
acceleration models.
5.2. Competing acceleration models
Based on the past experience with similar fail mechanism, the
reliability engineer believes that Weibull distributions would
adequately describe Au–Al intermetallic growth life in a semiconductor package, which implies a smallest extreme value (SEV)
distribution for the log-life. That is, if T is assumed to have a
Weibull distribution, T  WEIBðα; βÞ, then log ðt Þ  SEVðμ; σ Þ;
where σ ¼ 1β is the scale parameter and μ ¼ log ðαÞ is the location
parameter. The Weibull CDF and PDF can be written as
"   #


log ðt Þ  μ
t β
¼ 1  exp 
F ðtjα; βÞ ¼ ΦSEV
ð10Þ
σ
α
"   #


 
1
log ðt Þ  μ
β t β1
t β
¼
exp 
;
f ðtjα; βÞ ¼ ϕSEV
σt
σ
α α
α

t 40
ð11Þ

In above parameterization, β 4 0 is the shape parameter and
α 4 0 is the scale parameter as well as the 0.632 quantile.
The Arrhenius life-temperature relationship was expected to
describe the acceleration behavior


Ea
;
ð12Þ
t ðTempÞ ¼ Aexp
K  Temp
where

 t ðTempÞ is the life characteristic related to temperature.
 A is constant, and ðEa Þ is the activation energy of the chemical
reaction in electron volts.

 Temp is temperature in Kelvin
(1C þ273.15).


 K is Boltzmann's constant 8:617385 E  5 eV=K .
However, due to the complexity of the material, engineer would
like to consider two possible life-stress relationships, namely, the
linear relationship M1 and the quadratic relationship M2.
The M1 model can be expressed in the linearized form by taking
the logarithmic of both sides of (12) as
μ1 ¼ β0 þ β1 x

ð13Þ

By standardizing the accelerating variable, the above model can
be expressed as
μ1 ¼ γ 0 þγ 1 ξ ;

ð14Þ
ðx  xlow Þ
ðxhigh  xlow Þ,

where the standardized variables are expressed a ξ ¼
ξ A ½0; 1.
New coefﬁcients are related to previous ones through γ 0 ¼ β0 þ
β1 xlow and γ 1 ¼ β1 ðxhigh  xlow Þ. Thus, we have μ1 low ¼ γ 0 and
μ1 high ¼ γ 0 þ γ 1 .
The M2 model is an extension of M1 by adding a quadratic term
as
μ2 ¼ β0 þ β1 x þ β2 x2

ð15Þ

By standardizing the accelerating variable, the above model
(15) can be expressed as
μ2 ¼ γ 0 þγ 1 ξ þ γ 2 ξ2 ;

ð16Þ


γ 1 ¼ β1 xhigh  xlow ,

and γ 2 ¼ β2 ðx2 high  x2 low Þ.
where γ 0 ¼ β0 þ β1 xlow þ β2 x2 low ,
Similarly, we have μ2 low ¼ γ 0 and μ2 high ¼ γ 0 þγ 1 þ γ 2 .

6

E.A. Nasir, R. Pan / Reliability Engineering and System Safety 134 (2015) 1–9

For both models, for Type-I censored data (time censoring), the
probability of obtaining a censored observation at time t c is given
by
h   i
β
Prðt 4 t c Þ ¼ exp  tαc ; t c 4 0
ð17Þ

5.3. Prior distributions elicitation
Engineer assumed an equal weight for both models to begin
with. That is, π ðM 1 Þ ¼ π ðM 2 Þ ¼ 0:5. For model M 1 , Eq. (14) shows
parameter vector θ as ðγ 0 ; γ 1 ; σÞT , and for model M 2 , Eq. (16)
shows parameter vector θ as ðγ 0 ; γ 1 ; γ 2 ; σÞT . One would need to
specify a prior distribution for each of the parameters or pM1 ðθÞ
and pM2 ðθÞ. We would initially use the parameters in their original
units (before transformation) to relate to the engineer's prior
knowledge. Standardization is applied once prior distributions in
original units have been effectively solicited from engineers.
Given historical learning and previous experience with similar
fail mechanism, the reliability engineer believes that appropriate
independent prior distributions on the parameters can be speciﬁed as follows: for the activation energy, a uniform distribution
that gives an equal likelihood for values that range from 1.0 to
1.05 eV would be appropriate to use. Note that in the case of the
quadratic model M 2 this parameter may no longer directly
correspond to the activation energy of the chemical reaction. Not
much was known about the intercept, and the quadratic coefﬁcient in M 2 so both were given a vague (diffuse) normal distribution with mean of 0.0 and low precision of 1:0E  6 ðσ ¼ 1000
or σ 2 ¼ 1E þ 6 Þ. A positive density support was assumed for the
Weibull shape parameter as gamma distribution with shape of
2 and scale of 1.
5.4. Construction of optimal design
The optimization algorithm is Monte Carlo simulation-based, in
n
which the optimal design d is arrived at by evaluating the design
criterion in (9) for each of the candidate designs, and selecting the
design that maximizes the design criterion (utility function of
interest). The optimization steps are summarized as follows:
1. For a given experimental run budget, N, and the number of
stress-factors to study, k, construct a Latin hypercube design
(LHD ðN; kÞ).
2. Over the design grid, for each candidate design d randomly
simulate fail data from the joint density ðθi ; yi Þd;Mi of each of
the rival models M i ði ¼ 1; 2Þ:


ð18Þ
θi ; yi d;M  pd;Mi ðθ; Y Þ ¼ pðθÞMi pd;Mi ðyjθÞ
i

That is, independently generate random fail data using the
competing acceleration models (using (14) for model M 1 and
(15) for model M 2 ). Consider all possible combinations of
sample sizes (unit allocation) at each stress factor-level
combinations. Computational time can be reduced if units
are allocated at increments 4 1 to each of the stress levels.
3. Simulated experiments (failure times) are compared against a
predetermined test duration t c to determine if a test unit
failure time is censored.
4. Calculate the relative prediction performance of each model
over the range of its parameters. This is done by using a Gibbs
sampler (WinBUGS) to compute posterior predictions of,
τp ðxS Þ, the 100 pth quantile of the lifetime distribution at
both the higher and lower stress conditions (S ¼ SHigh and
S ¼ SLow ). A typical reliability interest is when p ¼ 0:01, so in
the case of models M 1 and M 2 , the outcome of this step is the

posterior distribution of the predicted quantile values for each
model given the same data set; i.e., τ^ 0:01; ðM2jY1Þ , τ^ 0:01; ðM1jY1Þ ,
τ^ 0:01; ðM1jY2Þ and τ^ 0:01; ðM2jY2Þ at both the higher and lower stress
conditions.
5. Use the Hellinger distance measure, DH , to calculate pairwise
local utilities ðu2j1 Þ and ðu1j2 Þ as in (7) and (8), which are
reproduced below for convenience:
For model M 2 conditional on data from model M 1
u2j1 ¼ DHS

High





τ^ 0:01; ðM2jY1Þ ; τ^ 0:01; ðM1jY1Þ þ DHSLow τ^ 0:01; ðM2jY1Þ ; τ^ 0:01; ðM1jY1Þ

For model M 1 conditional on data from model M 2
u1j2 ¼ DHS

High





τ^ 0:01; ðM1jY2Þ ; τ^ 0:01; ðM2jY2Þ þ DHSLow τ^ 0:01; ðM1jY2Þ ; τ^ 0:01; ðM2jY2Þ

6. Since it is unknown which of the two models is the true data
generating model, we combine the Monte Carlo samples of
local utilities u2j1 and u1j2 to obtain the desired total observed
utility function uðdÞ ¼ u2j1 þ u1j2 to be maximized for an
optimal design.
7. Approximate the pre-posterior global utility U ðdÞ ¼ E½uðdÞ by
ﬁtting a smooth surface to the combined Monte Carlo sample
generated in Step 6 as a function of selected design.
n
8. The optimal design d is found by maximizing the ﬁtted
surface (the maximum pre-posterior Hellinger distance
between predictive densities).
A direct application of Monte Carlo simulation to ﬁnd the
optimal design will require very large scale simulations and it will
be computationally inefﬁcient due to the large number of iterations needed and the duplication of effort in neglecting valuable
information already generated at a nearby design points. Therefore, to reduce computational cost, in Step 7 and Step 8 the nonparametric surface ﬁtting approach, originally proposed by Müller
and Parmigiani [34] and Müller [35], is used for ﬁnding optimal
designs.

5.5. Results for discriminating linear versus quadratic ALT models
Table 1 lists the temperature stress ranges that were used in
the planning of the ALT experiment. The surface ﬁtting smoothing
approach for ﬁnding optimal design requires the simulation of
experiments ðdi ; θi ; yi Þ on a design grid. Full grid of the three
temperature ranges can be used in the simulation. However, we
instead use a modiﬁed Latin Hypercube design to replace the full
grid and reduce computational cost at no loss of coverage and to
allow available experimental budget. Table 2 shows the design grid
created using a modiﬁed Latin Hypercube design ðmLHDÞ for the
available budget of 20 experimental runs.
Following the simulation optimization algorithm steps, the
optimal design under criterion (8) for discriminating between
linear and quadratic acceleration models in single accelerating
variable (temperature) is summarized in Fig. 3.
Fig. 3 displays the pre-posterior expected value of the utility
	


function (Log U ðdÞ ) as a function of the stress magnitude and
Table 1
Temperature stress range used in experiment
Bake stress

T High
T Middle
T Low

Temperature range in oC (Oven tolerance 7 5 oC)
Lower

Upper

150
115
80

180
145
110

E.A. Nasir, R. Pan / Reliability Engineering and System Safety 134 (2015) 1–9

percent unit allocation to each of the three stress levels used in
planning of the experiment. The utility function is maximized
when

 The higher temperature level is set at 180 1C with an approxi


mated unit allocation of 12%:
The middle temperature level is set at 130 1C with an approximated unit allocation of 55%:
The lower temperature level is set at 100 1C with an approximated unit allocation of 33%:

5.6. Some remarks on the optimal model-discrimination test plan
The Bayesian model-discrimination test plan is compared to
some conventional test plans and the Meeker and Hahn's 4:2:1
compromise ALT plan. Although the primary objective of some of
these plans (model estimation accuracy) is quite different than
ours (model discrimination), pointing out similarities and dissimilarities between them is of an added value in our judgment.
Nelson [28] pointed out that “a good plan should be multi-purpose
Table 2
mLHD grid with 12 runs and 8 corner augmentations
Run
#

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

Low temperature oven High temperature oven setup
setup
Temperature 1C (Low)

Temperature 1C
(Mid)

Temperature 1C
(High)

85
80
90
100
95
100
110
95
110
90
105
80
80
110
110
110
80
80
80
110

140
125
115
120
145
130
135
130
120
125
135
140
115
145
115
115
115
145
145
145

155
150
170
160
165
180
160
165
175
150
175
170
150
180
150
180
180
150
180
150

Run
source

mLHD
mLHD
mLHD
mLHD
mLHD
mLHD
mLHD
mLHD
mLHD
mLHD
mLHD
mLHD
AUG-C1
AUG-C2
AUG-C3
AUG-C4
AUG-C5
AUG-C6
AUG-C7
AUG-C8

7

and robust and provide accurate estimates.” Assumptions used in
the stress setup and unit allocation for each plan are

 The same prior distributions are given to same parameters
across all models.

 All plans use three levels of stress (temperature) in the range of



150 1C  180 1C for high temperature, 115 1C  145 1C for
middle temperature, and 80 1C  110 1C for low temperature
stress. All plans share the same ﬁxed experimental budget
(sample size).
Stress setup and unit allocation are determined as follows
 Model-discrimination plan: unequally spaced test levels with
unequal allocation that puts more units at the middle of the
test range. Optimal design setup used: highest temperature
of ð180 1CÞ with 12% allocation, intermediate temperature of
ð130 1CÞ with 55% allocation, and lower temperature of
ð100 1CÞ, slightly above the intermediate value in the low
temperature range, with 33% allocation.
 Good compromise plan: equally spaced test levels with
unequal allocation that puts more units at the extremes of
the test range and fewer in the middle. We have used 50% at
lower level, 30% at higher level, and remaining 20% at the
middle level. For the equal spacing of stress levels 180 1C
was selected as highest possible, 110 1C as lowest, and
145 1C as the intermediate stress.
 Best traditional plan: equally spaced test levels with equal
allocation. Typically, the highest possible stress needs to be
selected, which is 180 1C. The lowest test stress is selected to
minimize standard error of ML estimate of log mean life at
design stress, which is 110 1C. The intermediate stress at an
equal space is then 145 1C. Equal allocation puts approximately 33.33% of units at each stress level.
 Meeker and Hahn's 4:2:1 compromise plan: Borrowing from
the best traditional plan, 180 1C, 145 1C and 110 1C are set
as the high, intermediate and low test stress levels, respectively. Allocation of samples follows 47 or ð57%Þ to low
stress, 27 or ð29%Þ to middle stress, and 17 or ð14%Þ to high
stress.

Some remarks on the obtained optimal model-discrimination
test plan are
1. The test plan allocates the larger proportion of units to the
intermediate stress level (  55%). This is favorable for test
robustness and for generating more failure observations. This
plan will be most sensitive for detecting nonlinearity of the
relationship (minimize variance of the estimate of the quadratic coefﬁcient).

Fig. 3. The pre-posterior expected Log (U) as a function of stress condition and unit allocation.

8

E.A. Nasir, R. Pan / Reliability Engineering and System Safety 134 (2015) 1–9

detecting curvatures in acceleration models and for generating
more failures.

6. Conclusions

Fig. 4. Plots of recovery rate versus sample size for different test plans.

2. The test plan allocates more test units to the lower stress level
(  33%) than to the higher stress level (  12%). This is favorable
for more accurate extrapolation with respect to stress, as
suggested by optimum plans.
3. The test plan sets the high temperature value to the highest
possible in its allowable range, this is known to be a good
practice when interest lies in minimizing the standard error of
the estimate of any percentile at the design stress (a very
common objective of ALTs).
4. The test plan does not set the lower temperature value to the
lowest possible in its allowable range as suggested by the
optimum test plan (effective if the design stress is close to the
test range), but rather it chooses an intermediate value. One
drawback to having to test at the lowest extreme of the test
range is the longer test time needed for units to fail.

5.7. Recovery rates of different test plans
Optimal model-discrimination designs are expected to maximize the proportion of times in which the true, data-generating,
model is selected under an appropriate model selection criterion.
We have chosen to use the DIC model selection rule as explained
in Section 4. Other methods of model selection such as BF (Bayes
Factor) or BIC (Bayesian Information Criterion) could have been
also used. The following deﬁnitions are used in creating the plan
comparison, as shown in Fig. 4.

 True model: The acceleration model from which fail data were



generated; that is, Eq. (14) for the linear model, and Eq. (16) for
the quadratic model.
Assumed model: The actual acceleration model ﬁtted to the
simulated data.
% recovery rate: The fraction of times the true model recovered
(correctly identiﬁed) under the DIC-based model selection.

Fig. 4 clearly illustrates the superior recovery rate of the
proposed model-discrimination plan. As the sample size increases,
this recovery rate converges to 90%, while the recovery rates of
other plans are still below 50%. As noted previously, the primary
objectives of these test plans could be different from model-form
discrimination; therefore, the apparent superiority of our test plan
w.r.t. to the recovery rate under DIC should come as no surprise. By
the comparison across different plans, it is demonstrated that the
proposed methodology is effective in recommending stress setup
and unit allocation for model discrimination. In general, modeldiscrimination plan tends to allocate a higher percentage of units
to the middle stress level, which is intuitively appealing for

This paper presents a simulation-based Bayesian approach to
the accelerated life test planning with the objective of differentiating competing acceleration models. It is different from the
previous research of Bayesian methods for ALT planning, which
is concerned with the model parameter uncertainty only. We
propose a design criterion that is based on the Hellinger distance
measure between the predictive distributions of a life percentile of
interest under different acceleration models. Therefore, when
facing the model form uncertainty, the experimental results from
this type of test plan can better assist the experimenter in
choosing the right model. Our approach was applied to a realworld application, where there was uncertainty as to whether the
relationship between log mean life and the stress variable is linear
or exhibits some curvature. Both the stress-factor setup and the
unit allocation at three stress levels were optimized and the
obtained optimal test plan was validated by its recovery rate of
correct model using simulated data. Comparing to other conventional test plans, such as the three stress-level good compromise
plan, the best traditional plan and the well-known 4:2:1 compromise ALT test plan, our test plan has the advantage of substantially
increasing the desirable ability of distinguishing among competing
model forms, thus provides better guidance as to which model is
appropriate for the follow-on product testing.
The main limitation of the proposed approach is the intensive
computation required for the point-wise evaluation of utility
function. This has been eased by the use of a modiﬁed Latin
Hypercube sampling scheme, followed by the application of curveﬁtting optimization approach. The simulation-based Bayesian
approach described in this paper could be extended to modeldiscrimination ALT planning problems with more than one accelerating variable and more complicated acceleration models.

Acknowledgment
This research is partially funded by U.S. NSF CMMI program,
Grant no. 0928746.
References
[1] Nelson W, Kielpinski TJ. Theory for optimum censored accelerated life tests for
normal and lognormal life distributions. Technometrics 1976;18(1):105–14.
[2] Nelson W, Meeker WQ. Theory for optimum accelerated censored life tests for
Weibull and extreme value distributions. Technometrics 1978;20(2):171–7.
[3] Nelson W. A bibliography of accelerated test plans. IEEE Trans Reliab 2005;54
(2):194–6.
[4] Nelson W. A bibliography of accelerated test plans, Part II – references. IEEE
Trans Reliab 2005;54(3):370–3.
[5] Xu X. Robust prediction and extrapolation designs for censored data. J Stat
Plan Inference 2009;139:486–502.
[6] McGree JM, Eccleston JA. Investigating design for survival models. Metrika
2010;72:295–311.
[7] Monroe EM, Pan R, Anderson-Cook CM, Montgomery DC, Borror CM. Sensitivity analysis of optimal designs for accelerated life testing. J Qual Technol
2010;42:121–35.
[8] Yang T, Pan R. A novel approach to optimal accelerated life test planning with
interval censoring. IEEE Trans Reliab 2013;62(2):527–36.
[9] Konstantinou M, Biedermann S, Kimber A. Optimal designs for two-parameter
nonlinear models with application to survival models. Stat Sin 2014;24:
415–28.
[10] Haghighi F. Optimal design of accelerated life tests for an extension of the
exponential distribution. Reliab Eng Syst Saf 2014;131:251–6.
[11] Meeker WQ, Escobar LA. A review of recent research and current issues in
accelerated testing. Int Stat Rev 1993;61(1):147–68.
[12] Meeker WQ, Sarakakis G, Gerokostopoulos A. More pitfalls of accelerated tests.
J Qual Technol 2013;45(3):213–22.

E.A. Nasir, R. Pan / Reliability Engineering and System Safety 134 (2015) 1–9

[13] Hunter WG, Reiner AM. Designs for discriminating between two rival models.
Technometrics 1965;7:307–23.
[14] Box GEP, Hill WJ. Discrimination among mechanistic models. Technometrics
1967;9:57–71.
[15] Hill WJ, Hunter WG, Wichern DW. A joint design criterion for the dual
problem of model discrimination and parameter estimation. Technometrics
1968;10:145–60.
[16] Atkinson AC, Cox DR. Planning experiments for discriminating between
models. J R Stat Soc Ser B 1974;36:321–48.
[17] Hill PDH. A review of experimental design procedures for regression model
discrimination. Technometrics 1978;20(1):15–21.
[18] Leon PD, Atkinson AC. Optimum experimental design for discriminating
between two rival models in the presence of prior information. Biometrika
1991;78:601–8.
[19] Atkinson AC, Donev AN, Tobias R. Optimum experimental designs with SAS.
Oxford, England: Oxford University Press; 2007.
[20] Dette H, Titoff S. Optimal discrimination designs. Ann Stat 2009;37
(4):2056–82.
[21] Agboto V, Li W, Nachtsheim C. Screening designs for model discrimination.
J Stat Plan Inference 2010;140(3):766–80.
[22] Meyer RD, Steinberg DS, Box GEP. Follow-up designs to resolve the confounding in multifactor experiments. Technometrics 1996;38:303–13.
[23] Bingham DR, Chipman HA. Optimal designs for model selection. (Technical
Report 388). Michigan, US and Ontario, CA: University of Michigan and
University of Waterloo; 2002.

9

[24] Chaloner K, Verdinelli I. Bayesian experimental design: a review. Stat Sci
1995;10:273–304.
[25] Pascual FG. Accelerated life test plans robust to misspeciﬁcation of the stresslife relationship. Technometrics 2006;48(1):11–25.
[26] Zhang Y, Meeker WQ. Bayesian methods for planning accelerated life tests.
Technometrics 2006;48(1):49–60.
[27] Yuan T, Liu X, Kuo W. Planning simple step-stress accelerated life tests using
Bayesian methods. IEEE Trans Reliab 2012;61(1):254–63.
[28] Nelson W. Accelerated testing. New York: John Wiley & Sons; 1990.
[29] Deza E, Deza MM. Dictionary of distances. Amsterdam, Netherlands: Elsevier
Science; 2006.
[30] Duda RO, Hart PE, Stork DG. Pattern classiﬁcation. 2nd ed. New York, United
States: Wiley; 2001.
[31] Cha S-H, Srihari SN. On measuring the distance between two histograms. J
Pattern Recognit Soc 2002;35:1355–70.
[32] Spiegelhalter DJ, Best NG, Carlin BP, van der Linde A. Bayesian measures of
model complexity and ﬁt. J R Stat Soc Ser B 2002;64:583–639.
[33] Claeskens G, Hjort NL. Model selection and model averaging. Cambridge,
United Kingdom: Cambridge University Press; 2008.
[34] Müller P, Parmigiani G. Optimal design via curve ﬁtting of Monte Carlo
experiments. J Am Stat Assoc 1995;90:1322–30.
[35] Müller P. Simulation-based optimal design. In: Bernardo JM, Berger JO, David
AP, Smith AFM, editors. Bayesian statistics 6. Oxford, U.K.: Oxford University
Press; 2000. p. 459–74.

Approximate Fairness through Differential Dropping
Rong Pan

Lee Breslau

Balaji Prabhakar

Scott Shenker

Stanford University
rong@stanford.edu

AT&T Research
breslau@research.att.com

Stanford University
balaji@stanford.edu

ICSI
shenker@icsi.berkeley.edu

ABSTRACT

sively implementable at extremely high speeds; thus, ﬁnding
more easily implementable variants of such algorithms may
be of signiﬁcant practical value and several such algorithms
have been proposed (e.g., [13, 24]).

Many researchers have argued that the Internet architecture would be more robust and more accommodating of
heterogeneity if routers allocated bandwidth fairly. However, most of the mechanisms proposed to accomplish this,
such as Fair Queueing [16, 6] and its many variants [2,
23, 15], involve complicated packet scheduling algorithms.
These algorithms, while increasingly common in router designs, may not be inexpensively implementable at extremely
high speeds; thus, ﬁnding more easily implementable variants of such algorithms may be of signiﬁcant practical value.
This paper proposes an algorithm called Approximate Fair
Dropping (AFD), which bases its dropping decisions on the
recent history of packet arrivals. AFD retains a simple forwarding path and requires an amount of additional state
that is small compared to current packet buﬀers. Simulation results, which we describe here, suggest that the design
provides a reasonable degree of fairness in a wide variety of
operating conditions. The performance of our approach is
aided by the fact that the vast majority of Internet ﬂows
are slow but the fast ﬂows send the bulk of the bits. This
allows a small sample of recent history to provide accurate
rate estimates of the fast ﬂows.

1.

This paper focuses on the design of a router mechanism
that combines approximately fair bandwidth allocations to
all users with relatively low implementation complexity. We
have three basic requirements of our design. First, it should
achieve reasonably fair bandwidth allocations, and by “fair”
we mean the max-min deﬁnition of fairness [6]. We make
no pretense at being able to match the packet-by-packet
fairness of Fair Queueing or other schemes that use intricate
packet scheduling algorithms to ensure that perfect fairness
is achieved on very short time scales. Instead, we aim for
approximate fairness over longer time scales, on the order of
several roundtrip times.
Second, we require that the design be easily amenable to
high speed implementations. To this end we limit ourselves to FIFO packet scheduling algorithms with probabilistic drop-on-arrival; as in RED [8] and FRED [13], when
a packet arrives either the packet is dropped or placed on a
FIFO queue. The dropping decisions must be simple with
O(1) complexity (that is, the complexity must not increase
with the number of ﬂows or packets). The amount of state
required to make these dropping decisions must be small;
the point of reference we choose to deﬁne “small” is the
amount of memory which is already devoted to the packet
buﬀers.

INTRODUCTION

Since the pioneering observations of Nagle [16], many researchers have argued that the Internet architecture would
be more robust (in the face of ill-behaved ﬂows) and more
accommodating of heterogeneity (by no longer requiring adherence to a single congestion control algorithm) if routers
allocated bandwidth fairly. This viewpoint is not universally shared,1 but even among adherents of this approach the
question of feasibility has been a major concern. This is because most of the proposed mechanisms, such as Fair Queueing [16, 6] and its many variants [2, 23, 15], involve complicated packet scheduling algorithms. These algorithms, while
increasingly common in router designs, may not be inexpen1

Third, the algorithm must employ some form of active queue
management (AQM). It need not mimic RED or any other
form of AQM precisely, but it should embody AQM’s fundamental principles of responding early (and gently) to congestion while absorbing small bursts of traﬃc.2

2
There is a fourth requirement that, for lack of space, we
do not discuss in this paper; this is the requirement that
the scheme be able to punish unresponsive ﬂows. That is,
the scheme should not just allocate bandwidth fairly, but
should be able to shut down (by dropping all their packets)
ﬂows that incur high drop rates for long periods of time.
The rationale for this is persuasively described in [10]; [24]
discusses the role of fair bandwidth allocation in implementing this. We are easily able to augment our design to meet
this goal and, as we see in Section 5, one of the designs
we present here already (but unintentionally!) achieves this
goal.

We discuss other viewpoints in Section 7.

ACM SIGCOMM Computer Communications Review

23

Volume 33, Number 2

stacles of CSFQ (and its variants) while providing similar
performance in the fairness domain. We are not aware of
any other algorithm that is both as deployable and as fair
as AFD.

To achieve these goals, we propose an algorithm called Approximate Fair Dropping (AFD). AFD uses a FIFO queue
with probabilistic drop-on-arrival. These probabilistic dropping decisions are based not only on the past measurements
of the queue size but also on the recent history of a ﬂow.3
AFD uses the history to estimate the ﬂow’s current sending
rate, and then drops with a probability designed to limit
that ﬂow’s throughput to the fair share. Thus, dropping
is not applied uniformly across ﬂows (as in RED) but is
applied diﬀerentially based on an estimate of the ﬂow’s current sending rate. The exact form of diﬀerential dropping is
chosen to approximate fair bandwidth allocations.

The second contribution of AFD lies in its technical novelty.
While several components of its design are present in existing algorithms, it is the ﬁrst algorithm to leverage the distribution of ﬂow rates in conjunction with per-packet sampling
in order to address the scalability requirements of per-ﬂow
state. By basing the design of AFD on this characteristic
of network traﬃc, we have achieved an important advance
in this class of algorithms. Furthermore, we demonstrate
techniques that reduce AFD’s complexity relative to other
algorithms while not sacriﬁcing performance.

Our design might initially appear to be seriously misguided.
The state required to keep enough history to accurately estimate each ﬂow’s rate is quite large, and grows linearly with
the number of ﬂows. This is clearly not a feasible approach.
However, note that our design only needs to estimate the
rates of ﬂows whose packets are likely to be dropped. Thus,
we need only keep enough state to estimate rates when those
rates are comparable to (or larger than) the fair share rate.
This is a crucial diﬀerence.

This paper has 7 sections. We describe a conceptual version
of our design in Section 2, and then use analysis to develop
guidelines for the various parameters in Section 3. We then
present the practical design in Section 4. We evaluate this
design using simulation in Section 5, and use trace data and
other measurements to estimate the state requirements for
the design in Section 6. We conclude in Section 7 with a
discussion of related work.

It is well known that the distribution of the sizes of ﬂows (the
total number of bytes transferred) has a long tail, and that
most ﬂows are small – the mice – but a large fraction of the
bytes are sent by large ﬂows – the elephants.4 As we show
using trace data in Section 6, and has been observed in [14],
the distribution of ﬂow rates has a similar property. While
perhaps not as long-tailed as the size distribution, initial
evidence suggests that the ﬂow rate distribution (measured
on the time scale of a second) is long-tailed. Most ﬂows are
slow (in bits/sec) – we will call them turtles – but most of
the bytes are sent by fast ﬂows – we will call them cheetahs.

2. CONCEPTUAL DESIGN
We ﬁrst present a high level conceptual design of AFD. Consider a link of speed C shared by n ﬂows, each sending at rate
ri . Assume, for convenience, that all packets are the same
size P . The total load on the link is R = i ri . The fair
share rf air is given by the solution to C = i min(ri , rf air ).
We can use diﬀerential dropping to accomplish our goal of
fair bandwidth allocations if we use dropping probabilities
r
)+ .5
for each ﬂow, di , given by the relation di = (1 − frair
i
The resulting throughput of each ﬂow is bounded by the fair
share: ri (1 − di ) = max(ri , rf air ).

In AFD, we only need state for the fast ﬂows and can ignore
the slow ﬂows because they won’t be dropped; the amount
of state required is roughly proportional to the number of
fast ﬂows, which is manageable. However, the challenge is
how to keep state only on the fast ﬂows when one doesn’t
know, in advance, which ﬂows are fast. A record of the very
recent packet arrivals contains mostly packets from the fast
ﬂows (because they send most of the bytes), while most of
the slow ﬂows won’t show up in the recent history. This is
the approach we take.

The key design question is: how can we estimate ri and
rf air . To estimate ri we keep a shadow buﬀer of b recent
packet headers. When a packet arrives, with probability 1s ,
where s is the sampling interval, we copy its header and
insert it into the shadow buﬀer; thus, we sample roughly
1 in s packets. When a packet is inserted into the shadow
buﬀer, we remove another packet at random to keep the total number of packets at b.6 Thus, at all times the shadow
buﬀer has a record of b recent packet arrivals. Note that the
shadow buﬀer contains copies of the packet headers, and
the insertion and deletion of packets from the shadow buﬀer
is parallel to the main forwarding of packets. The shadow
buﬀer is used to guide the dropping decisions in the following manner. When a packet (from, say, ﬂow i) arrives
(regardless of whether its header is copied into the packet
buﬀer) we compute the number of packets from that ﬂow in
the shadow buﬀer; we call this the number of matches, mi .
We then estimate the rate of that ﬂow to be riest = R mbi
(recall that R is the aggregate arrival rate).

While other algorithms with similar goals and building blocks
have been proposed, our work is unique in two important
ways. First, AFD ﬁlls a void in the space of performance and
deployability. In particular, simulation results show that it
has better fairness properties than other algorithms, such
as FRED, that have similar complexity and deployability.
At the same time, AFD does not have the deployment ob3
In this paper, we deﬁne a ﬂow as a stream of packets with
the same source and destination addresses. However, one
could use any deﬁnition of ﬂow discernible from an IP packet
header. The particular choice is orthogonal to our work.
Further, we do not address the impact of NAT boxes or
hosts with multiple IP addresses on ﬂow identiﬁcation; these
issues will aﬀect any algorithm that attempts to allocate
bandwidth on a per-ﬂow basis.
4
As we discuss later in Section 6, we use the term “longtail” to describe distributions that decay slower than exponentially.

ACM SIGCOMM Computer Communications Review

To avoid having to scan the shadow buﬀer each time a packet
5

We use the notation that z+ = max(0, z).
Random removal avoids synchronization problems. We
could also remove packets in a FIFO manner; it turns out
that this does not aﬀect performance greatly.
6

24

Volume 33, Number 2

arrives, we keep a table of the ﬂows and their current packet
counts; we call this the ﬂow table. A hash table is one natural data structure for the ﬂow table that has O(1) lookup
time. Each time a packet is inserted into the shadow buﬀer,
the appropriate counter in the ﬂow table is incremented,
each time a packet is removed the appropriate count is decremented, and every time a packet arrives the match count is
looked up in the ﬂow table. Insertions and deletions can
be somewhat slow (because we need only insert and delete
every s packets) but the lookup has to done at linespeed
(because it is done on every packet arrival).

800
Fair Share
Flow's Arrival Rate
Flow's Departure Rate

700

Throughput (kbps)

600
500
400
300
200
100
0
0

m

i

10

20

30

40

50

Flow Id

Estimating the rate of a single ﬂow requires only the recent
activity of that ﬂow and so is fairly straightforward. Estimating rf air is more complicated because the deﬁnition of
rf air depends on all the ri . We use an approach borrowed
from [24]. Upon each packet arrival we apply the dropping
r air
)+ , which can be rewritten as
probability di = (1 − rfest

Figure 1: Oﬀered Load and Throughput for 50 CBR
Flows under AFD

r

f air
)+ with mf air = b fRair . Note that if we
di = (1 − m
i
vary mf air the total throughput, i ri (1 − di ), varies from
r
R when mf air = ∞, to C when mf air = b fRair , to 0 when
mf air = 0. Thus, we can approximate rf air by varying
mf air so that the link is fully utilized but not overloaded.

sending below the fair share incur no drops, and ﬂows sending above the fair share have their excess packets dropped.
Fairness depends on the accuracy of the rate estimations,
and the rate estimations depend on the size of the shadow
buﬀer. Without suﬃcient history the rate estimations will
ﬂuctuate wildly and lead to quite unpredictable (and unfair) results. Rate estimation improves with larger b (the
size of the shadow buﬀer) and, to some extent, with larger
s (because s increases the time interval represented by the
contents of the shadow buﬀer). The question is how large
do b and s have to be in order to achieve reasonable fairness. This is crucial because we want the state required for
this algorithm to be small (at least compared to the packet
buﬀers). The viability of our scheme comes down to whether
fairness can be achieved without too much state, and we explore that issue in the next few sections.

In our approach, we vary mf air to ensure that the average
queue length stabilizes around the target value of qtarget . We
set qtarget to be 0.5(minth +maxth ), where minth and maxth
are the threshold values deﬁned in RED, to provide suﬃcient
packet buﬀering while retaining low queueing delays. We
borrow the AQM approach described in [12], which employs
a proportional-integral controller. Since this form of AQM is
very eﬀective at keeping the links fully utilized, the resulting
values of mf air Rb will be a good approximation to rf air .
We will defer a more complete comparison with other algorithms until Section 7, but we note that AFD builds upon
features present in earlier algorithms, such as CSFQ [24],
CHOKe [18] and FRED [13]. In addition, it is similar to
the concurrently-developed RED-PD [14].7 The result is an
overall design that attempts to approximate fairness with
reasonable overhead. FRED uses the number of packets
from each ﬂow in the packet buﬀer to guide its dropping
decisions; thus, our design can be seen as an extension of
FRED to use more history and a diﬀerent dropping function (which is the same as used in CSFQ).

In Section 3 we develop three guidelines for how to set b and
s. After describing a more practical (as opposed to the conceptual version just presented) version of AFD in Section
4, we evaluate the fairness that results from such parameter settings in Section 5. Finally, in Section 6 we estimate
the state requirements of AFD when using the parametersetting guidelines.

3. SETTING THE BASIC PARAMETERS
In this conceptual model, there are two basic parameters
that we need to set: the shadow buﬀer size b and sampling
interval s. Our goal is to achieve a reasonable degree of
fairness, and in this section we develop three guidelines on
what values of b and s achieve that goal. To do so, we
consider three scenarios and analyze them using very simple
models. These models are unrealistic but our hope is that
the resulting guidelines provide useful order-of-magnitude
estimates for the parameters of interest.

Does AFD achieve the goals we laid out in the Introduction? AFD’s probabilistic drop-on-arrival can be implemented with a very simple forwarding path. The dropping
decision has complexity O(1) and involves few computations. In addition, AFD incorporates active queue management. Fairness and feasibility (in terms of the state required) are the two remaining questions. If the rate estimates are accurate, we expect AFD to allocate bandwidth
fairly. While we delay the bulk of our simulation results, and
all of the simulation details, until Section 5, we now show initial evidence that this expected fairness is actually achieved.
Figure 1 shows the result of 50 CBR sources sending at ﬁve
diﬀerent rates (some above the fair share, some below) over
a single link. The bandwidth allocations are quite fair; ﬂows

3.1 Static Scenario
We ﬁrst consider the static case of n sources sharing a single
link of bandwidth C. All ﬂows are Poisson sources sending
at a constant rate ri and all packets are the same size. The
(R−C)+
overall drop rate D is given by D =
where, as above,
R
R = i ri . Let Pi (m) be the probability that when a packet
from ﬂow i arrives, there are m packets from ﬂow i in the

7
While the two schemes have some similarities, they are
targeted at somewhat diﬀerent goals. See Section 7.

ACM SIGCOMM Computer Communications Review

25

Volume 33, Number 2

b

shadow buﬀer. In our simple model,
Pi (m) =

ri b−m ri m
( )
)
R
R

(1 −

m

A choice of b that will always meet the error tolerance of E
(in this asymptotic limit) is:
b

For the purposes of this model, we assume that mf air is ﬁxed
r
to be the appropriate value: mf air = b fRair . We denote the
r
)+ . di (b) is the
ideal dropping rate by d˜i : d˜i = (1 − frair
i
average dropping rate of the i’th ﬂow for a given size of
shadow buﬀer:

 P (m)(1 − m
b

di (b) =

i

f air

m

m=0

r

Note that b fRair = mf air where, as deﬁned above, mf air
is the expected number of shadow buﬀer matches for a ﬂow
with ri = rf air . One can meet the error tolerance merely
by making the shadow buﬀer big enough to make sure that
E −2
) . For our purposes, we choose E ≈ .15.
mf air > ( 0.40
This might seem quite unambitious, but recall that we are
calculating an upper bound on the worst-case error (that is,
of ﬂows sending at a constant rate right at the fair share); the
errors for rates well above and well below the fair share are
signiﬁcantly smaller. Setting E = 0.15 results in mf air ≈
10. This is our ﬁrst guideline.

)+

The relative error in the throughput of the i’th ﬂow is
∆i (b) = |

d˜i − di (b)
ri (1 − di (b)) − ri (1 − d˜i )
|=|
|
˜
ri (1 − di )
1 − d˜i

Note that the sampling interval s drops out of all of these
quantities. This is not surprising given that a random sampling of a Poisson process remains a Poisson process.

Guideline 1: b

  b (1 − α)
b

m=0

  b (1 − α)

αm (1 −

bα
)+
m

αm (1 −

bα
)
m

b−m

m

b

=

m=bα

b−m

m

1
2πbα(1
− α)

πbα

0

∞

bα

1
< 
2πbα(1 − α)
 2(1 − α) 
∞

=




0

(x−bα)2

dx e

− 2bα(1−α)

∞

2

dy e

(1 −

y
− 2bα(1−α)

2

bα
)+
x

y
bα

 10

wi (t + 1) = wi (t) + 1 (mi ≤ mf air )
wi (t + 1) =

wi (t)
(mi > mf air )
2

bα
mi (t) =

Thus, we can bound the asymptotic expression for the relative error from above by the formula

1
s

 w (t − j)

ki −1

i

j=0

8

The error occurs because the time scale T has R in the
denominator; if we replace that by C then the dynamics are
completely independent in this simple model.

∆(b) ≤ .40(bα)−0.5

ACM SIGCOMM Computer Communications Review

or, equivalently, mf air

Consider ﬂows competing for bandwidth with diﬀering roundtrip
times τi . All packets are the same size P . Each ﬂow adjusts
its window size wi in a TCP-like fashion (with all the details of TCP omitted); the window (measured in terms of
packets) increases by one in time τi when there is no packet
loss during that RTT, and the window is halved if there is
a packet loss (and we neglect the case of multiple packet
losses, or of timeouts, in an RTT). The rates ri of the ﬂows
are roughly ri = wτiiP . The router has a shadow buﬀer of
size b and a sampling interval of s; the time interval T over
. We
which the shadow buﬀer measures usage is T = bsP
R
assume that the fair share rate rf air is constant and that
r
the fair share expected number of matches mf air = b fRair
is roughly 10 (as suggested by guideline 1). Consider a ﬂow
i and let mi denote the average number of its matches in
the shadow buﬀer. Because the fair share is ﬁxed, we can
approximate the dynamics of each ﬂow as independent.8 We
consider a series of time slots t = 1, 2, . . ., each of length τi .
For convenience, we assume that T = ki τi for some integer
ki . The average number of matches mi in the shadow buﬀer
ki −1
at some time t is roughly mi (t) = 1s j=0
wi (t − j). Thus,
the window dynamics can be written as:

 (1 − α)

dz ze−z ≈ .40

f air

Even if routers allocate bandwidth fairly on the time scale
over which they measure usage, ﬂows that respond slowly
when excess bandwidth is available are at a disadvantage
when competing with ﬂows that respond more rapidly. We
can illustrate this with a simple model involving ﬂows with
diﬀerent roundtrip times (RTTs).

For suﬃciently large b, we can approximate the binomial
form as a normal distribution with average αb and variance
bα(1 − α). Using an integral formulation, we ﬁnd:

∆(b) ≈

 10 r R

3.2 Dynamic Scenario

Our goal is to choose the size of the shadow buﬀer so that
we achieve some reasonable degree of fairness. We seek to
determine how large the shadow buﬀer must be so that the
maximal error, maxi ∆i (b), is below some target error tolerance E. Analysis and numerical calculations (which we do
not have space for here) indicate the maximal error occurs
when ri ≈ rf air . Consider a ﬂow with ri = rf air ; we can
approximate the relative error, call it ∆, in the limit of large
b for this ﬂow as follows (noting that the ideal dropping rate
for this ﬂow is zero, d˜i = 0, and so ∆(b) = di (b)). Letting
r
α = fRair , we have:

∆(b) =

rf air
E −2
>(
)
R
0.40

26

Volume 33, Number 2

We must also ensure that mf air sP is not so large that bursts
of that size dominate the packet buﬀer. If the packet buﬀers
are sized to be Cτ where C is the link speed and τ is some
nominal delay (on the order of 250msec) used to size the
packet buﬀers, and as above we assume the dropping rate
is low (so R ≈ C), then we require that mf air sP  Cτ or,
2
τ
.
equivalently, bs  PC
r

If we start oﬀ with wi (t = 0) = 1, the ﬁrst drop for ﬂow i
occurs when ﬂow i sends, in a TCP saw-tooth, mf air + 1
packets, at which point, the TCP window size wi (t) equals
smf air
+ k2i (we assume wi (t) is an integer). Then, the
ki
sm

f air
window is halved to wi (t) = 2k
+ k4i . If this halving
i
of the window brings the number of matches down below
mf air in the next time period then the increasing process
starts again. If not, the window is halved again (and again)
until mi (t) is below mf air . Let’s assume that the window
is halved only once; in this case wi (t) becomes a repeating
smf air
series of values starting at wi (t) = 2k
+ k4i and increasing
i

f air

The previous two guidelines presented lower bounds for b
and s. Our third guideline yields both upper and lower
bounds on the product bs.

sm

f air
by 1 until the value wi (t) =
+ k2i is reached. The
ki
average window over this periodic cycle of ﬂow i, w̄i , is:

w̄i =

Guideline 3:

sm

The average window size that yields the fair share is kfiair .
w̄i reaches that level when smf air = 32 ki2 . Thus, if we set
mf air = 10 we have, approximately,

 20s
3

For s = 1, ki ≈ 2.6, and for s = 10, ki ≈ 8.
This model suggests that the size of the shadow buﬀer should
be large enough so that it can average the rate of the longest
RTT ﬂow over a TCP saw-tooth. A time scale smaller than
that would forget periods during which larger RTTs send
less than the fair share; otherwise, it would overact during
periods when larger RTTs send more than the fair share.

Guideline 2: b s P
R

=

b
Cτ
P mf air

4. PRACTICAL DESIGNS
Our description of the conceptual design glossed over several
aspects of the algorithm. First, the algorithm for adjusting
mf air is borrowed from [12]. We sample the queue length
at a rate fs ; upon each sample we adjust the value of mf air
according to the following equation:
mf air (t) = mf air (t − 1) + α(q(t − 1) − qtarget )
−β(q(t) − qtarget )
where q(t) is the queue length at the t’th sample, q(t − 1)
is the queue length at the previous sample, and qtarget is
the target queue size. This procedure not only stabilizes the
average queue length around the target value and provides
active queue management, but also estimates mf air implicitly. We choose fs = 160Hz, which is the same as in [12],
and α , β are 1.7 and 1.8, respectively. Our algorithm does
not seem terribly sensitive to small changes in the parameter values, but we have not done a systematic study of these
parameters.9

 750msec

3.3 Burst Scenario
Consider a ﬂow that, after being quiescent, sends h packets
(of size P ) back to back. If h is smaller than mf air s then it
is likely that none of the incoming packets will be dropped,
and if h is signiﬁcantly larger than this amount then the
latter packets will probably be dropped. We want the quantity mf air s to be large enough to absorb the packet bursts
typical of TCP and other window-based congestion control
algorithms. If we take τ to be a typical RTT, and assume
that a ﬂow might send an entire window’s worth of packets
back-to-back (where the window size corresponds to the fair
share), then we would want mf air sP > rf air τ . For a link of
capacity C with R ≈ C, this becomes bs > Cτ
.
P

ACM SIGCOMM Computer Communications Review

C2 τ
P rf air

With these guidelines, we now see how the algorithm works
in practice. In the next section we describe two practical
versions of the algorithm, one that follows directly from our
conceptual model and one that requires less state.

Note that this model is vastly oversimpliﬁed – in particular,
the impact of multiple drops per round-trip times and possible time-outs are not modeled and it is assumed that dropping starts as soon as the number of matches is over mf air
– and it probably signiﬁcantly underestimates the hardship
imposed on ﬂows with long RTTs. We therefore choose to
be conservative and
√ increase (almost double) the ratio to be
closer to ki ≈ 5 s. Moreover, we choose to accommodate
roundtrips on the order of 150msec. Recalling that T = ki τ
and T = bsP
, we adopt as our guideline 2 the following
R
inequality:
√

< bs 

We treat these three guidelines not as hard-and-fast rules
but rather as general rules-of-thumb to guide how the parameters are set. There is no need for getting the parameters
b and s exactly right. If they are too small, the fairness will
be less than ideal; if they are too big, the network may be
more vulnerable to bursts. But in both cases, as we see in
Section 5, the degradation is gradual. We also note that
compliance with these guidelines can be easily checked by
operators. For the ﬁrst guideline, one need only record the
historical values of mf air (which, as we describe in Section
4, the algorithm sets automatically) to ensure that it is well
above 10. The second guideline is computable from the link
capacity. The third can be checked by using the values of
mf air .

3 smf air
ki
(
+ )
4
k
2

ki ≈

Cτ
P

Second, we will use a hash table for the ﬂow table. In so
doing, we need not store entire packet headers in the shadow
9
Our particular AQM design choice, borrowed from [12],
seemed particularly easy to adapt to the AFD algorithm.
However, we assume that there are countless ways to incorporate AQM in our diﬀerential dropping design.

27

Volume 33, Number 2

We now proceed to explore the performance of our two designs – AFD-SB and AFD-FT – through simulation.

buﬀer but merely enough bits to accurately identify the ﬂow
in the hash table. In practice we will store a k-bit hash of
the source-destination addresses (or whatever ﬂow signature
is chosen). Third, the design must be modiﬁed to accommodate variable size packets. This entails keeping byte counts
in the shadow buﬀer, and measuring matches in bytes rather
than packets in the hash table. When removing packets from
the shadow buﬀer (which are picked randomly), we seek to
keep the total number of bytes in the shadow buﬀer constant
(but tolerate some jitter). We call this design AFD-SB, with
the SB standing for shadow buﬀer.

5. SIMULATION
5.1 Simulation Preliminaries
We used the ns [28] simulator to evaluate our two designs
in a variety of scenarios. We compare our designs to two
other schemes: RED [8] and FRED [13].11 RED provides
a baseline comparison with an algorithm that makes no attempt to allocate bandwidth fairly.12 FRED provides a useful guidepost of how much fairer the allocations are when
the dropping decisions are informed by the current packet
buﬀer occupancy.13 AFD keeps a longer history than FRED,
and so the relative performance can be seen as an indication
of how important this additional state is. FRED is easier to implement than AFD and requires less state, and so
our simulation results should be taken as illustrating the
fairness vs. complexity/state tradeoﬀs provided by the two
algorithms.14

AFD-SB stores ﬂow information in two places, the shadow
buﬀer and the ﬂow table. A natural way to reduce the state
requirements of AFD would be to eliminate one of these. We
can’t eliminate the ﬂow table, since it is required to make
the packet lookups O(1), so we seek instead to eliminate
the shadow buﬀer. We argue in Section 6 that if a hash
table is used, the hash table and shadow buﬀer require similar amounts of state; eliminating the shadow buﬀer would
thereby reduce the state requirements by half. However, if
we used CAM memory for the ﬂow table the state reduction
would be far greater (since hash tables need to be sized an
order of magnitude larger than the number of ﬂows to avoid
collisions).

We envision AFD operating in an environment where ﬂows
use many forms of congestion control. In our simulations, we
used the following diﬀerent congestion control algorithms:

The question is whether we can perform the correct operations on the ﬂow table without the shadow buﬀer. Incrementing the ﬂow table upon packet insertions is straightforward. However, decrementing is hard. When we eliminated
packets from the shadow buﬀer, we picked one at random
(or in a FIFO order); all packets had equal chance to be
eliminated. Choosing a ﬂow at random with uniform probability in the ﬂow table is easy, but it isn’t clear how to pick a
packet at random with uniform probability without linearly
traversing the ﬂow entries.

TCP: We use TCP-sack as provided in the ns release. The
TCP ﬂows sometimes have diﬀerent RTTs, or diﬀerent
ﬁle sizes, as described below.
AIMD: We modify the increase parameter a and decrease
parameter b in TCP’s Additive-Increase/MultiplicativeDecrease algorithm. Standard TCP has (a, b) = (1, 0.5).
In addition, we use sources with the following parameters: (1, 0.9), (0.75, 0.31), (2.0, 0.5). We refer to these
as AIMD1, AIMD2 and AIMD3, respectively. The ﬁrst
and third are more aggressive than normal TCP, and
the second is slightly less aggressive than TCP.

To solve this problem, we adopt an approximation. Note
that one could choose a packet with uniform probability if
one chose ﬂows (from which to eliminate a packet) according
i
. However, choosing among all n ﬂows
to probabilities mm
j
j
in this manner requires a linear search. We propose picking
k ﬂows at random, where k  n and choosing among them
mi
where S is
with the same probability function:
j∈S mj
the set of ﬂows chosen at random. We use k = 5 in our
simulations.10

Binomial: Recently Bansal and Balakrishnan [1] introduced
a more general family of increase/decrease algorithms.
Increases are of the form w+aw−k and decreases are of
the form w − bwl for constants (a, b, k, l). We used two
versions: (1.5, 1.0, 2, 0) and (1.0, 0.5, 0, 1). The ﬁrst is
roughly comparable to TCP and the second is much
more aggressive than TCP. We refer to these as Binomial1 and Binomial2.





We call this AFD-FT (with the FT standing for ﬂow table).
We propose AFD-FT as an example of how one could introduce approximations into AFD in order to make it more
easily implementable (by requiring less state). However, we
expect that there are many other ways to implement the
ﬂow table; in particular, each router design will have its own
hardware constraints and to maximize implementability it
will be important to adapt AFD to each situation. We oﬀer
AFD-FT as an existence proof that AFD’s performance is
reasonably robust to approximations.

CBR: These are constant rate ﬂows that do not perform
congestion control.
11

We have not included algorithms such as Fair Queueing [6]
and CSFQ [24] in our performance evaluations. These algorithms are known to have good fairness properties and we do
not claim that AFD performs better. The advantage of AFD
over these algorithms is in complexity and deployability.
12
We use RED in gentle mode with the parameter settings
of minth = 25 and maxth = 125 for a 10Mbps link. The
thresholds are scaled proportionally for higher speed links.
13
We simulated FRED with minq = 4 and other parameters
as in the RED simulations.
14
Other algorithmic diﬀerences, such as the manner in which
the drop probabilities are computed, diﬀerentiate AFD from
RED.

10

To accommodate varying size packets, we decrement the
chosen ﬂow by the number of bytes in the arriving packet.
If the chosen ﬂow doesn’t have enough bytes, we remember
the deﬁcit and subtract more upon the next packet.

ACM SIGCOMM Computer Communications Review

28

Volume 33, Number 2

S(0)

100Mbps

The basic scenario we present consists of 7 groups of ﬂows,
each with ﬁve ﬂows, sharing the bottleneck link in Figure
2(a). The 7 groups are AIMD1, AIMD2, AIMD3, Binomial1, Binomial2, TCP, and TCP with a higher latency of
24.5msec on the 100Mbps access links (yielding a roundtrip
latency, without queueing, of 138msec for this ﬂow group,
as compared to 48msec for the other ﬂows). Figure 3(a)
shows the throughput received by each group of ﬂows (we
average within each group so that the data is more easily
presentable); the ﬂuctuations within groups are quite small,
as was seen in Figure 1 for the CBR simulation. The bar
chart shows the bandwidth allocations for the ﬂow groups in
the following order: AIMD1, AIMD2, AIMD3, Binomial1,
Binomial2, TCP, and TCP with increased latency.16

R(0)

100Mbps

R(1)

S(1)

Traffic Sources

R1

10Mbps

Traffic Sinks

R2

S(n-1)

R(n-1)

S(n)

R(n)

The throughput allocations for RED and FRED are substantially uneven, showing that this set of ﬂows includes several
rather aggressive congestion control algorithms. Therefore,
this presents a reasonable test of the fairness properties of
the various algorithms. AFD-SB and AFD-FT both have
reasonably fair allocations. Figure 3(b) shows the average
drop rate for each group of ﬂows; RED has uniform drop
rates, FRED has somewhat uneven drop rates, and both
AFD algorithms have extremely uneven drop rates. This
demonstrates that applying diﬀerentiated drop rates can
lead to fairness for diﬀerent ﬂows.

(a) Single Link Topology

flowgrp-1

flowgrp-3

flowgrp-3

flowgrp-1

100Mbps

10Mbps
R1

10Mbps

10Mbps

10Mbps

R2

R3

R4

R5

flowgrp-2

flowgrp-2

flowgrp-4

flowgrp-4

We stated at the outset that we did not intend to match
the packet-by-packet fairness of Fair Queueing, and we have
shown that AFD achieves reasonable fairness on long time
scales. We nonetheless examine the behavior of the algorithm on shorter time scales. Figure 4 shows the cumulative
distribution of the short term throughput of all 35 ﬂows
over 2, 5 and 10 second intervals normalized to the average throughput. When looking at the throughput over 10
second intervals we see that it is within 10% of the average
53.0% of the time and within 20% of the average 86.0% of
the time. Over 5 second intervals, throughput is within 10%
of the average 36.8% of the time and within 20% of the average 66.5% of the time. The corresponding percentages for
2 second intervals (which represents the amount of history
in the shadow buﬀer) are 18.4% and 36.4%.

source
sink

(b) Multiple Link Topology
Figure 2: Topologies

Most of our simulations are run on the topology depicted
in Figure 2(a), and a few are run on the topology in Figure
2(b). The congested links have transmission latencies of
20msec, and unless otherwise stated the uncongested links
have latencies of 2msec. The routers on the congested link(s)
have buﬀers that hold 600 packets.15 The simulations are
run for 10 minutes of simulation time, with the ﬁrst 100
seconds discarded for warmup. Unless otherwise stated, all
data packets are 1000 bytes, and AFD is run with b = 500
and s = 5.

Although long term behavior is fair, not surprisingly, short
term fairness suﬀers. This is undoubtedly due to the random
nature of dropping decisions. When packets are dropped
probabilistically and the drop probabilities are typically low,
it is diﬃcult to match the drop rates on short time scales.
Consequently, fairness suﬀers on intervals over which the
target and actual drop rates diverge. Nonetheless, the long
term behavior is fair. Further, and perhaps more importantly, because the short term unfairness is random, there
appears to be no way for a malicious ﬂow to exploit or manipulate this unfairness to its advantage.

5.2 Simulation Results
Mixed Traﬃc: In Section 2 we showed that AFD provides
equal bandwidth shares to CBR ﬂows sending at diﬀerent
rates. We now consider the more challenging case of a mixture of ﬂows using diﬀerent congestion control algorithms.

While the primary goal of AFD is fairness, it is also designed
to provide AQM. The part of AFD that provides AQM is
largely orthogonal to the fairness aspects of the algorithm.
We have not ﬁne tuned the AQM component and assume
that better AQM components could be incorporated into
AFD. Nonetheless, results (not shown here) indicate that

15

The actual physical buﬀer size is not critical as the algorithm attempts to maintain the queue length near a target
level, qtarget . We have repeated simulations with a buﬀer
size of 200 packets setting qtarget to 75 in both cases. The
throughput results are almost identical, indicating that large
variations in the queue size above qtarget are rare.

ACM SIGCOMM Computer Communications Review

16

Unless otherwise noted, we use the same ordering of ﬂow
groups in the remaining bar charts.

29

Volume 33, Number 2

500

0.2

Drop Probability

Throughput (kbps)

400
300
200

0.15

0.1

0.05

100

0

0
RED

FRED

AFD-SB

RED

AFD-FT

(a) Throughput

FRED

AFD-SB

AFD-FT

(b) Drop Rates
Figure 3: Mixed Traﬃc
fair throughput levels are preserved. The conservative assumptions used in deriving the guidelines provide a margin
for error.

1

0.9

Cumulative Fraction

0.8

0.7

Reduced Shadow Buﬀer: If we return to our canonical scenario and reduce the size of the shadow buﬀer, does
fairness suﬀer? Figure 7 shows the throughput levels for
AFD-SB (similar results hold for AFD-FT) for buﬀer sizes
of b = 250 and b = 125 (the latter violates Guideline 1 significantly) in addition to the canonical case of b = 500. In this
case, fairness degrades slightly as the shadow buﬀer size decreases, but AFD still outperforms the baseline algorithms.

0.6

0.5

0.4

0.3

0.2

10 sec
5 sec
2 sec

0.1

0
0

0.2

0.4

0.6

0.8

1

1.2

1.4

1.6

1.8

2

Normalized Throughput

While these cases establish that AFD-SB and AFD-FT can
provide fair bandwidth allocations when faced with a diverse
set of sources, there are cases when the AFD algorithms do
not perform as well. We now present three such cases.

Figure 4:
Cumulative distribution of per-ﬂow
throughput over 2, 5 and 10 second intervals

Long RTTs: First, consider the case where there are 4
ﬂow groups, each with 10 TCP ﬂows. The ﬂow groups
have diﬀerent RTTs, as determined by the latencies of the
connecting 100Mbps links. The latency of the central congested link remains the same (20msec), and the latencies
of the connecting links are (followed in parentheses by the
resulting total latency of the path, not counting queueing
delays): 2msec (48msec), 7msec (68msec), 12msec (88msec),
17msec (108msec). The congested link has queueing delays
of roughly 60msec which should be added to the total latency numbers to compute estimates of the RTTs. For each
algorithm, the resulting bandwidth allocations for each ﬂow
group are shown in the bar chart in Figure 8(a) ordered
from smallest to largest latency. The AFD designs provide
much better fairness than RED or FRED. The throughput
values diﬀer from the fair share by less than 4%. We chose
parameters that were designed (according to Guideline 2) to
accommodate RTTs of up to 150msec, and this scenario is
just a bit over that limit. If we increase the latency values
to 2msec (48msec), 22msec (111msec), 42msec (175msec)
and 62msec(238msec), the fairness is even further reduced,
as shown in Figure 8(b). Here the throughputs vary from
the fair share by as much as 20%. The largest RTT here
(roughly 300msec including queueing delay) is larger than
our b and s can accommodate (by guideline 2), and the re-

the link utilization, average queue size, and drop probability
under AFD are all comparable to those of RED.
We now check how well this scenario compares with the
guidelines. With b = 500 and 35 ﬂows, the average share
of the shadow buﬀer is roughly 14 packets which is in line
with the recommendation of 10 packets in Guideline 1. It
turns out that due to the ﬂuctuations induced by the window
based ﬂow control, in which packets tend to be bunched, the
average
of mf air is closer to 17. Guideline 2 sets bounds on
√
;
the
guideline calls for this quantity to be larger than
b sP
R
750msec and our scenario has 890msec. Lastly, Guideline 3
calls for 300 < bs  15, 000 if we choose τ = 250msec. Our
scenario has bs = 2, 000.
High Speed Link: To ensure that the fairness in this initial
simulation scales, we increased the speed of the congested
link, the size of the shadow buﬀer, and the number of ﬂows
by a factor of 10. Figure 5 shows the throughput levels that
result.
Increased Multiplexing: Next we tested what happens
if the number of ﬂows is increased by a factor of three (retaining the same mixture of ﬂows and the same b). Results
for AFD-SB are shown in Figure 6; results for AFD-FT are
similar. Even though this scenario violates Guideline 1 (because now the fair number of matches is less than 5), the

ACM SIGCOMM Computer Communications Review

30

Volume 33, Number 2

500

Throughput (kbps)

200
100

250
200
150
100
50

0

300
200
100
0
125

0

RED

FRED

AFD-SB

AFD-FT

Figure 5: High-Speed Link

105 flows

35 flows

Figure 6: Increased Multiplexing

1500

400

1200

Throughput (kbps)

500

400

200
100

300
200
100

500

Figure 7: Reduced Shadow Buﬀer

500

300

250
Shadow Buffer Size

Throughput (kbps)

Throughput (kbps)

300

Throughput (kbps)

300

400

Throughput (kbps)

400

350

900
600
300

0

0

RED

FRED

(a)

AFD-SB

AFD-FT

RED

FRED

(b)

AFD-SB

AFD-FT

Figure 8: Long RTTs (a) Max Latency = 200msec (b) Max Latency =
300msec
sulting decrease in fairness is evident.17

RED

FRED

AFD-SB

AFD-FT

Figure 9: Unresponsive Flow

the results. We believe this phenomenon is also responsible
for AFD-FT performing better than AFD-SB in the RTT
tests (see Figure 8).

Unresponsive Flow: Next, we return to the latency values and traﬃc mix used in Figure 3. To this traﬃc mix we
add a CBR ﬂow sending at 15% of the link capacity. Figure
9 shows that the throughput allocations in AFD-SB are not
perfectly fair, with the CBR receiving roughly 15% more
than the fair share. The CBR under AFD-FT receives very
little throughput at all, only about 15% of the fair share.
This was not our intention when designing AFD-FT. However, we are not unhappy with this result; as we observed
in Section 1, punishing unresponsive ﬂows that tolerate persistent high drop rates is an important design goal (see [10]
for the rationale). We have to (and can easily) add additional mechanism (which we do not discuss in this paper)
to AFD-SB to accomplish this goal but AFD-FT does this
by default. This occurs because the CBR ﬂow occupies 15%
of the shadow buﬀer (packets are selected to be inserted
into the shadow buﬀer whether or not they are destined
to be dropped), which is 5 times larger than the share of
any other ﬂow. The approximate packet deletion scheme in
AFD-FT is biased against smaller ﬂows. This in turn biases the dropping algorithm against larger ﬂows. When the
diﬀerences between ﬂows is small this bias is negligible, but
when the diﬀerence is a factor of ﬁve, the bias dominates

Multiple Congested Links: The third challenging scenario for AFD is when there are multiple congested links.
This scenario used the topology in Figure 2(b). There are
four diﬀerent source-destination pairs: three that traverse a
single congested link and one that traverses the long path
through 3 congested links. Each link has a latency of 5ms.
In the ﬁrst scenario we consider, each of these ﬂow groups
consists of 20 standard TCP ﬂows, so that every congested
link is traversed by the same number (i.e., 40) of ﬂows. Figure 10(a) shows that the ﬂows that traverse the long path
(the fourth group shown for each algorithm in the bar chart)
receive substantially less bandwidth than the others, roughly
83% of the fair share. While the throughputs are much
fairer than RED or FRED, these results are troubling. If
we double the number of ﬂows in the group that traverses
the link between R4 and R5, which reduces the fair share
on that link, then the results become much fairer. These results are shown in Figure 10(b), with the ﬁrst group of two
bars showing the throughput for the ﬂow groups traversing
each of the ﬁrst two congested links, respectively, and the
second group of two bars showing the throughput of the
group traversing R4-R5 and the throughput of the group
traversing the long path, respectively. Bandwidth allocation for pairs of ﬂow groups that share the same bottleneck
link is much fairer for the AFD algorithms relative to the
previous scenario. These results suggest, and simple anal-

√
The analysis leading to guideline 2 suggests that b s P
R
should be greater than 5 times
√ P the maximal RTT. In this
case, that would call for b s R ≈ 1.55sec whereas in this
√
= 890msec.
scenario b s P
R

17

ACM SIGCOMM Computer Communications Review

0

31

Volume 33, Number 2

RED

FRED

(a)

AFD-SB

Fairshare between R2-R3-R4 is 334Kbps, R4-R5 is 167Kbps

500
450
400
350
300
250
200
150
100
50
0

450
400
350
Throughput (kbps)

Throughput (kbps)

Throughput (kbps)

fairshare at each congested link = 250Kbps
500
450
400
350
300
250
200
150
100
50
0

RED

FRED

AFD-SB

AFD-FT

10000
1000
100
10

AFD-FT

1.
0
< s
10
< s
20
< s
30
< s
40
< s
50
< s
60
< s
70
< s
80
< s
9
< 0s
10
> 0s
10
0s

AFD-SB

(a)

Transfer Time (sec)
(b)

Figure 12: Web Traﬃc – (a) throughput of large ﬂows (b) transfer time
of small ﬂows

AFD-FT

400
350
300
250
200
150
100
0
0

100

200

300

Simulation Time (sec)

Figure 13: Changing Traﬃc

short-lived. However, the majority of the bytes belong to
long-lived and larger ﬂows.

ysis conﬁrms, that AFD does not perform well when there
are multiple congested links with exactly the same fair share.
Flows that are sending right at their fair share experience
probabilistic dropping at multiple hops, and therefore receive less than ﬂows that traverse only one congested link.
However, if ﬂows pass through multiple congested links that
have diﬀerent fair shares, then when sending at around the
fair share the multi-hop ﬂow only suﬀers drops at one link.
We conjecture (not based on any special insight but just
based on common sense) that traversing paths with multiple congested links with approximately the same fair shares
will be an unusual case.

Using this simulation setup, we hope to answer two questions: does the presence of many short ﬂows interfere with
the fairness properties of the longer-lived ﬂows, and does
AFD allow the shorter ﬂows to ﬁnish sooner? Figure 12(a)
shows the throughput received by the inﬁnite source ﬂow
groups under RED, AFD-SB and AFD-FT. AFD-SB and
AFD-FT still provide very good fairness in this scenario.
Figure 12(b) shows a histogram of the ﬁnishing times of
the short-lived ﬂows under AFD-SB and RED. Note that
this is on a logarithmic scale, so that while the diﬀerence in
heights between the bars representing the fastest ﬁnishing
times is small, it represents a diﬀerence of approximately
2000 ﬂows. In all the other histogram bins, RED has more
ﬂows. Thus, AFD-SB (and the same applies to AFD-FT)
aids short ﬂows by lowering their drop rates and allowing
them to ﬁnish sooner. With RED, the short ﬂows see the
same ambient drop rate as all other ﬂows.

We tested many other scenarios in simulation, including different traﬃc mixes and variable size packets, that we do not
present here. In all cases, the fairness properties of AFD
were maintained. Before concluding the description of our
simulations, we brieﬂy mention 3 additional experiments.
Many Small Flows: In the simulations described above,
all of our sources had an unlimited amount of data to send.
In the next scenario, keeping our basic mixture of 7 ﬂow
groups (with inﬁnite sources), we introduce approximately
22,000 short TCP ﬂows representing, for example, shorter
web transactions. We use a Pareto distribution of ﬂow
lengths. The average number of packets per ﬂow is 15 with
a shape parameter of 1.2. The resulting mix of traﬃc reﬂects the salient features of Internet traﬃc. Namely, many
ﬂows are coming and going, and most ﬂows are small and

ACM SIGCOMM Computer Communications Review

AFD-SB

50

<

RED

FRED

450

AFD-SB
RED

1

0

100

Figure 11: Two-Way Traﬃc

Throughput (kbps)

100000

500

Number of Flows

Throughput (kbps)

600

100

150

RED

Figure 10: Multiple Congested Links (a) same Fair Share (b) diﬀerent
Fair Share

200

200

0

(b)

300

250

50

AFD-FT

400

300

Two-Way Traﬃc: When two-way traﬃc is present, the
traﬃc can become burstier due to ACK-compression [27].
In this scenario, we have the 7 ﬂow groups from our basic
scenario sending in both directions. The congested link has
a latency of 5ms. Figure 11 shows the throughput for each
ﬂow group. AFD-SB and AFD-FT both provide much better fairness than RED and FRED. AFD-FT punishes one
particularly aggressive group of ﬂows (for reasons we dis-

32

Volume 33, Number 2

its packets are not dropped. When its average sending rate
is equal to the fair share, it experiences some packet drops
and its throughput is approximately 90% of the fair share
rate.18 The second point to note is that as the sending rate
of the bursty ﬂow increases (up to 16 times the fair share
rate), it does not receive more than the fair share. Finally,
as the sending rate of the bursty ﬂow increases, the fairness
among the other ﬂow groups is maintained. When the onoﬀ source sends at 16 times the fair share rate, all the other
ﬂow groups receive between 95% and 100% of the fair share.
Hence, these results indicate that AFD performs well in the
face of very bursty ﬂows, neither punishing nor rewarding
ﬂows for their burstiness while preserving the basic fairness
among all ﬂows.

400
350
Throughput (Kbps)

300
250
200
150
100
50
0
0.5

1
2
4
8
Ratio of Bursty Source Rate to Rfair

16

5.3 Discussion of Simulation Results
We now review the general conclusions that can be drawn
from our set of simulations. In most scenarios, as long as the
guidelines are followed AFD-SB and AFD-FT provide very
good approximations to fair bandwidth allocations. The
level of fairness is far superior to RED (which does not attempt to provide fairness) and FRED (which uses a very
restricted amount of state to guide the dropping decisions).
The performance of AFD-SB and AFD-FT seemed fairly robust to varying parameters and conditions. In those scenarios where b was not large enough, or the RTTs were too large,
the level of fairness degraded gracefully. Thus, AFD appears
to not need precise parameter tuning, and fails softly when
the parameters are far out of alignment.

Figure 14: Bursty Traﬃc

cussed in the Unresponsive Flow section above). The additional queueing delay incurred in the reverse direction is
enough to cause the performance of the TCP ﬂow group
with longer RTT (the 7th group) to suﬀer under AFD-SB
(their RTT with queueing delay is roughly 180msec, which is
larger than the 150msec our parameters were intended for).
Changing Traﬃc: We also tested the performance of AFD
under changing traﬃc conditions. In this scenario, three ﬂow
groups (AIMD1, Binomial2, TCP) begin sending data at the
start of the simulation. After 100 seconds, three additional
groups (of the same kinds) start transmitting, doubling the
oﬀered load. At time 200 seconds, the ﬁrst 3 groups stop
sending. Figure 13 shows that the AFD-SB algorithm is
able to adapt to the changing traﬃc, cutting the allocations
in half when the oﬀered load doubles, and increasing them
again when the load is reduced. In both cases, the allocations are adjusted in less than 500msec.

As we saw with the unresponsive CBR ﬂow, and in many
of our other simulations not shown here, AFD-FT responds
quite punitively when ﬂows are not responsive. We judge
this a good thing, but ultimately this is a policy question.
With AFD-SB we can turn this policy on or oﬀ depending
on the desires of the network operator;19 in AFD-FT this
policy is hardwired in. Moreover, the choice of k = 5 in our
version of AFD-FT (where k is the number of other ﬂows
that must be compared before a packet is eliminated from
the ﬂow table) will probably need to be made larger when
the number of persistent fast ﬂows is larger. AFD-FT dealt
quite well with the many small ﬂows, but if the number
of fast ﬂows is on the order of thousands the number of
comparisons will probably have to be on the order of 10 or
more. We have not yet investigated how this parameter will
scale.

Bursty Flows: We also examined the performance of AFD
in the presence of on-oﬀ sources. These sources exhibit maximally bursty behavior by sending a burst at the speed of
the access link (100Mbps) for a brief period and then going
idle. The burstiness of these on-oﬀ sources is varied by adjusting their burst times while holding the burst rate and
idle period constant. An on-oﬀ source can be characterized
×tburst
by the ratio of its average sending rate, rtburst
, to the
burst +tidle
fair share rate, rf air .

6. STATE REQUIREMENTS AND RATE DISTRIBUTIONS

The results of these experiments are shown in Figure 14.
Each group of bars in the ﬁgure represents one experiment.
In each experiment, there are 5 ﬂows in each of the original
7 groups of ﬂows (i.e., those used in the Mixed Traﬃc experiments above), and a single on-oﬀ source. The ratio of the
average sending rate of the on-oﬀ ﬂow to the fair share rate
is indicated on the X-axis (ranging from .5 to 16.) For each
experiment, the ﬁrst seven bars show the average throughput for the ﬂows in each of the 7 ﬂow groups. The eighth
bar shows the throughput of the on-oﬀ source.

These simulations show that when using the guidelines to set
the parameters, AFD provides reasonably fair bandwidth
allocations. However, can this algorithm be feasibly implemented? All the operations on the forwarding path are
O(1), so the main barrier to implementation would be if
AFD required an impractical amount of state. This feasibility requirement is economic, not technical. Clearly routers
18

Recall that deviations from the fair share are most likely
to occur for ﬂows sending at or near the fair share rate.
19
Again, we don’t present the algorithm which implements
this policy in AFD-SB, but it is straightforward and requires
only that the shadow buﬀer and ﬂow table keep separate
track of matches that were dropped and matches that were
not dropped.

There are three things to note in this ﬁgure. First, the
bursty ﬂows are not punished for their burstiness. When the
average rate of the on-oﬀ source is one-half the fair share,

ACM SIGCOMM Computer Communications Review

33

Volume 33, Number 2

16

could be built with vast amounts of additional storage; however, that would come at a steep price, and we are looking to
achieve fairness without greatly increasing the complexity or
the cost of future routers. We use, as our standard, the requirement that the additional state required by AFD should
be small compared to the memory already required by the
packet buﬀer. We compute the state requirements, with the
aid of two measurements, in Section 6.1. In Section 6.2 we
discuss how these state requirements depend crucially on
the rate distribution and, in Section 6.3, present some trace
data on current rate distributions.

14
12

Ratio

10
8
6
4
Trace 1
Trace 2
Trace 3

2
0
0.8

1

1.2

1.4
1.6
1.8
2
Shadow Buffer Length (sec.)

We compute the state requirements of our two designs –
AFD-SB and AFD-FT – separately. We start with AFDSB, whose state consists of two parts: the shadow buﬀer
and the hash table.
hbit = 10 ∗ 16 ∗ 12 ∗

Guideline 1 (Section 3.1) states that the shadow buﬀer should
hold roughly 10 rfRair packets. This estimate assumed that
all packets were the same size. To adjust this for variable
packet sizes, we ﬁrst deﬁne R̃ as the packet arrival rate in
terms of packets/sec, and r̃f air as the arrival rate of a ﬂow
sending at the fair share rate; also, let Pmax and Pave denote
the maximal and average packet sizes. The proper sizing of
the shadow buﬀer in terms of packets, when you have variable sized packets, is then b = 10 r̃fR̃air . The worst case,
the lowest value, for r̃f air is when a fair share ﬂow is sendr air
. The expression
ing maximal sized packets: r̃f air ≥ Pfmax
R
for R̃ is simply R̃ = Pave
. To represent each packet in the
shadow buﬀer we use, roughly 6 bytes.20 Assuming that
Pmax = 1500 bytes and Pave = 300 bytes,21 we then ﬁnd
that bbit , the shadow buﬀer size in bits, should be roughly:

R
300
rf air
1500

= 2400

2.6

1
4

R
300
rf air
1500

= 2400

R
rf air

Thus, to a ﬁrst approximation, these two data structures require similar amounts of memory. Many router vendors recommend having on the order of 250msec’s worth of memory
in the packet buﬀers. We compare the memory required by
AFD to the memory already recommended for the packet
buﬀer, which is roughly C4 where C is the speed of the link
(in bps). The ratio, call it ρSB , of AFD-SB’s state requirement to the size of the packet buﬀers is given by:

ρSB =

4800 rfRair
1
C
4

=

19.2kbits/sec
19.2kbits/sec
≈
(1 − D)rf air
rf air

where the last approximation is because C = R(1 − D) and
we suspect the aggregate dropping rate will typically be low.
Thus, the fraction of extra memory, ρSB , required by AFDSB depends on the typical size of the fair share rate on a
link.

R
rf air

We now calculate the state requirements of AFD-FT. If the
ﬂow table is implemented using a hash table, then we merely
use the state requirements above (only the hash table part)
and so the state requirements are roughly half that of AFDSB. If the ﬂow table is implemented using CAM then we no
longer need a table 12 times the number of ﬂows (as we did
with the hash table). The CAM-based design needs roughly
64 bits per ﬂow (48 bits for the hashed ﬂow-id and 16 bits
for a counter). Modifying the calculations above to reﬂect
these changes, we ﬁnd that the AFD-FT ratio is:

Similarly, we can estimate the storage required for a hash
table to keep track of the number of bytes sent by each ﬂow.
We describe our trace data in Section 6.3, but for now we use
the fact that, in the various traces we have seen and for the
shadow buﬀer sizes we are proposing, the number of ﬂows in
the hash table is typically less than a fourth the number of
packets in the shadow buﬀer (see Figure 15). Let’s assume
that we need 2 bytes per entry for counting (assuming we
count matches at the granularity of 40 byte units and ignore
roundoﬀ errors), and that we use a hash table 12 times larger
than the expected number of ﬂows to reduce the chance of
collisions in the hash table. The size of the hash table hbit ,
in bits, is then:

ρF T ≈

20

We need not store the full header in the shadow buﬀer,
merely the hash of the source-destination addresses. 6 bytes
is a suﬃciently large hash to comfortably accommodate
roughly 108 ﬂows with small chance of collision.
21
See [3] for measurements of average packet size. We’ve
used a fairly conservative estimate, as the average packet
size reported in [3] is over 400 bytes. The average packet
size over our three traces that we report on in Section 6.3 is
almost 500 bytes.

ACM SIGCOMM Computer Communications Review

2.4

Figure 15: Ratio of Packets to Flows in Shadow
Buﬀers

6.1 Calculation of State Requirements

bbit = 10 ∗ 48 ∗

2.2

3.2kbits/sec
rf air

which is one-sixth of the AFD-SB requirements.
Note that these estimates of the ratios ρ depend only on
the fair share of a link and not directly on its bandwidth.
For slow links, the fair share will be quite small (and thus ρ
could be 1 or larger), but the amount of memory required for
these slow links is insigniﬁcant; if the fair share is 19.2kbps,
so ρ = 1, on a T3 link, the required extra memory for AFD-

34

Volume 33, Number 2

amount of faster (and more expensive) SRAM. We can do
likewise with the shadow buﬀer state. However, the ﬂow
table will probably require SRAM or CAM. Thus, we cannot
conclude that AFD imposes a cost that is a fraction ρ of the
packet buﬀer cost. We leave the detailed implementation
issues for future work (by others); our goal here is to provide
a rough estimate of the state requirements.

SB is roughly 1.5Mbytes. Thus, we care mostly about these
ratios ρ on faster links where the absolute amount of memory
devoted to the packet buﬀers is quite large. We assume that
faster links will have larger fair shares, and these ratios ρ
estimating the relative memory requirements of AFD will
be smaller on such links.
We are not aware of many studies of the typical fair shares
on current Internet links.22 One can’t infer the fair share
merely by taking a packet trace; without fair bandwidth allocation one can’t tell which ﬂows are constrained by that link,
and which ﬂows are constrained elsewhere. One would have
to ﬁnd a link whose router has Fair Queueing (or some equivalent algorithm) on which to take traces (and even these
traces would be misleading because of the lack of fair bandwidth allocations elsewhere along in the network). Lacking
a solidly grounded method for measuring typical fair shares,
we turned to an available dataset that has some bearing on
the question.

The estimates above are very rough in nature. On any particular choice, such as the 250msecs for packet buﬀers or the
numbers of bits for counters, one could argue that we are
oﬀ by a factor of two. But we believe these calculations give
us an order-of-magnitude estimate of the state requirements
of our design, and suggest that AFD, in either of its incarnations, is likely able to achieve reasonable levels of fairness
without exorbitant amounts of extra state.
We now argue that the distribution of rates greatly aids the
AFD approach.

One way to obtain a very rough estimate of the fair share on
a path is to measure the end-to-end throughput obtained by
a TCP connection that traverses that path. This is a very
imperfect measure, since TCP throughput varies as a function of RTT and not all traﬃc is TCP-friendly. We obtained
a dataset of measured end-to-end throughputs of transfers
between NIMI measurement sites.23 47 sites participated
in this measurement by periodically transferring a 1MB ﬁle
with another randomly chosen NIMI site. The dataset contains roughly 35, 000 transfers. For lack of space we do not
show the distribution here, but in over 90% of the transfers the rate is greater than 250kbps. These measurements
are not focused on high-speed links. However, most of the
NIMI boxes are at universities and other sites that are wellconnected to the Internet, and none of them are behind very
slow modems, so the measurements are probably indicative
of moderate speed links. If we take 250kbps as a rough
1
1
estimate of the fair share then ρSB ≈ 13
and ρF T ≈ 78
.
That is, the extra memory required by AFD-SB is less than
a tenth of the memory already devoted to packet buﬀers.
We view these estimates as being a very conservative lower
bound, in that the fair shares on very fast links may be much
larger, and thus the ratios ρSB and ρF T much smaller. As
one moves up the network hierarchy towards the backbone
links, it is likely that a larger fraction of ﬂows are bottlenecked somewhere else in the network. That means that the
fair share available to ﬂows unconstrained elsewhere could
be quite large. If backbone links had rf air ≈ 2M bps then
1
1
ρSB ≈ 100
and ρF T ≈ 600
. However, the level of fair shares
remains an open question, and we hope in the future to ﬁnd
better ways to estimate the fair share on high-speed links.

6.2 Impact of the Rate Distribution
Our state calculations depended critically on the value of
r
rf air , and in particular on the ratio fRair . One intuitive
way of understanding this ratio is the following. Let’s call
a ﬂow a cheetah if it is sending above or near the fair share,
and let nc denote the number of such ﬂows. We call the other
ﬂows, the slow ones, turtles. Let γ = turtles rRi denote the
fraction of the oﬀered load that comes from these slow ﬂows.
The equation deﬁning the fair share i min(ri , rf air ) = C
becomes: Rγ + rf air nc = C. We can write this as
nc
R
=
rf air
(1 − D) − γ
where D is the overall drop rate. The quantity rfRair is
minimized when both the number of cheetahs is small and
the fraction of bandwidth consumed by the turtles is quite
small: in short, when most of the bandwidth is being sent by
a very few fast ﬂows. It is in this regime that AFD requires
very little state to perform well.
To make this more precise, we now look at how rf air varies
under diﬀerent rate distributions in a simple analytical model.
We consider a continuum of ﬂows whose rates are given
by a density function h(r). We will consider four continuum distributions that all have the same average rate (of
1 in arbitrary units) and normalize the distribution by n
(representing the number of ﬂows), so the total load offered by each distribution is n. For each distribution we
∞
∞
have n = 0 h(r)dr and 1 = n1 0 h(r)rdr. We assume
that the router allocates bandwidth fairly, and that ﬂows
respond by restraining their ﬂow to the fair share if their
assigned rate (by the distribution) is larger than the fair
share. As we vary the capacity C between 0 and n we
compute the fair share rf air (C) from the constraint: C =
∞
h(r) min(r, rf air (C))dr. The question is how rf air (C)
0
compares for the various distributions we consider.



This paper does not address detailed implementation issues.
However, since we’ve used the storage requirements of the
packet buﬀer as a yardstick for the state requirements of
AFD, we would be remiss if we did not point out that this
comparison is somewhat misleading. Packet buﬀers make
heavy use of slower (and cheaper) DRAM with a smaller



We consider four distributions, in order of increasing variance. For a point distribution, where all ﬂows are the same
C
rate, the fair share is merely rfpoint
air = n . For a distribution
where rates are uniformly distributed between 0 and 2, the

22

The available bandwidth, which is a somewhat diﬀerent concept, was studied in [19].
23
NIMI is the National Internet Measurement Infrastructure;
see [20] for a more detailed description. See [26] for details
of the measurement process.

ACM SIGCOMM Computer Communications Review



fair share is rfuni
air = 2(1 −

35



1−

C
).
n

For an exponential dis-

Volume 33, Number 2

1

Trace 1
Trace 2
Trace 3

Cumulative Fraction of Bytes

Complementary Fraction of Flows

1
0.1
0.01
0.001
0.0001
1e-05

0.6

0.4

0.2

1e-06
1e-07
100

1000

10000
100000
1e+06
1-Second Rate (bps)

1e+07

0
1e-06

1e+08

Figure 16: Complementary Distribution of 1-Second Rates
1

1e-05

0.0001
0.001
0.01
Fraction of 1-Second Rates

0.01
0.001
0.0001
1e-05

0.6

0.4

0.2

0
1e-07

1e-07
100

1

0.8

1e-06

10

0.1

1

Trace 1
Trace 2
Trace 3

0.1

Trace 1
Trace 2
Trace 3

Figure 17: Cumulative Distribution
of 1-Second Rates

Cumulative Fraction of Bytes

Complementary Fraction of Flows

0.8

1000 100001000001e+06 1e+07 1e+08 1e+09 1e+10
Flow Size (bytes)

Figure 18: Complementary Distribution of Flow Sizes

Trace 1
Trace 2
Trace 3
1e-06

1e-05

0.0001 0.001
Fraction of Flows

0.01

0.1

1

Figure 19: Cumulative Distribution
of Flow Sizes

C
tribution, h(r) = ne−r , the fair share is rfexp
air = − ln(1 − n ).
n −3
Finally, for a power-law distribution h(r) = 2 r for r ≥ 12
(and 0 otherwise; this restriction is to avoid the divergence
)−1 . The point of
at the origin), we have rfplair = 14 (1 − C
n
this exercise is that as the tail of the distributions got larger
(that is, the number of very fast ﬂows increases) the fair
share as a function of C became larger. While the fair share
diverges for both the exponential and the cubic power-law
distributions as C
approaches 1, the fair share is dramatin
cally larger in the power-law case.

6.3 Trace Data

Guideline 1 calls for the buﬀer size to be roughly b = 10 rfRair .
If the rate distribution is such that rf air is larger (for a
given R) then the shadow buﬀer size, and all the state requirements, become smaller. Note that if the fair share remains the same then the state requirements increase linearly
with the speed of the link (and the oﬀered load R). However, if we keep the same oﬀered load and let the fair share
increase to its natural level as we increase C, and study
b(C) = 10 r C (C) , then we ﬁnd two cases. For the point and
f air
uniform distributions, b(C) increases; this is what we expect,
more state is required on faster links. For the exponential
and power-law distributions, however, b(C) decreases; as the
speed of the link increases the state requirements of the algorithm go down! Note this is not a decrease in the ratio ρ,
this is a decrease in the absolute amount of state.

Guideline 2 calls for keeping state of about 1 second in order
to estimate the rates. For a variety of diﬀerent time intervals, Figure 15 shows the average ratio between the number
of packets in the shadow buﬀer and the number of distinct
ﬂows represented in the shadow buﬀer; this ratio was used
in Section 6.1 to estimate the state requirements for AFD.

best when the distribution of ﬂow rates has a long tail. We
now turn to empirical evidence to see if this is currently the
case.

We obtained traces of traﬃc from three separate locations:
a 100Mbps link at a national laboratory, a T3 peering link
between two providers, and a FDDI ring connecting a modem bank at a dial up provider to a backbone network. We
refer to these traces as Trace 1, Trace 2, and Trace 3, respectively. Trace 1 consists of approximately 22 million packets
collected during a two hour interval. Trace 2 consists of 34
million packets over a 45 minute span. Trace 3 collected
approximately 6 million packets in 70 minutes.

For each of the traces we estimated individual ﬂow rates
based on the packets seen in the last second; see [14] for
similar data and related discussions. Figure 16 shows the
complementary distribution of ﬂow rates for each of the
three traces. Trace 3 is not obviously inconsistent with a
slowly decaying exponential distribution, but the other two
clearly have long tails.24 This is even more pronounced on

We have no way of judging with any certainty what the fair
share rates will be in the future on high-speed links. However, the arguments above suggest that AFD will operate

ACM SIGCOMM Computer Communications Review

24

36

We use the term long tailed to refer to distributions that de-

Volume 33, Number 2

7. CONTEXT, RELATED WORK, AND DISCUSSION

1e+08

1e+07

This paper starts with the assumption that routers should
allocate bandwidth fairly. While this is a familiar and ofttold story, for context we once again brieﬂy review the rationale for fairness. We then discuss related work and conclude
with some comments on AFD’s underlying design principles.

Rate (bps)

1e+06

100000

10000

1000

100
10000

100000

1e+06
Size (bytes)

1e+07

Congestion control is one of the Internet’s most fundamental
architectural problems. In the current Internet, most routers
do not actively manage per-ﬂow bandwidth allocations, and
so the bandwidth received by a ﬂow depends on the congestion control algorithms used by other competing ﬂows. To
achieve relatively equitable bandwidth allocations, all ﬂows
must be TCP-compatible [4] (also known as TCP-friendly);
that is, they must use a congestion control algorithm that
results in bandwidth allocations similar to TCP’s. This approach requires uniformity and cooperation, both of which
might be problematic.

1e+08

Figure 20: Rates vs Size

a log-linear plot, which for space reasons we do not show
here. Figure 17 shows the cumulative distributions of the
1-second ﬂow rates. Note 10% or less of the ﬂows represent
roughly 60% of the bytes in the worst case, and roughly
90% of the bytes in the best case. This is in contrast to the
fastest 10% of the ﬂows carrying 33% of the bytes for an
exponential distribution. Thus, based on the very preliminary evidence presented here and in [14] we conjecture that
rate distributions are usually long-tailed. In the language
of Section 6.2, we expect that the number of cheetahs will
typically be small, but they will represent the bulk of the
bytes.

By restricting the world of congestion control algorithms to
those that are TCP-compatible, some applications may be
impaired. This would be the case if some applications required radically diﬀerent forms of congestion control that are
inherently unfriendly to TCP. However, recent advances in
Equation-Based Congestion Control (EBCC) [11] and other
TCP-compatible algorithms [22, 21] gives hope that a very
wide variety of application requirements could be fulﬁlled
within the sphere of TCP-compatibility. Thus, the uniformity requirement, while potentially a problem, may in fact
be tolerable.

Similar statements have long been made about the distribution of ﬂow sizes. Figures 18 and 19 show the analogous data for ﬂow sizes as measured by the total number of
bytes transferred. These distributions are signiﬁcantly more
skewed than the rate distributions. Figure 20 shows the total ﬂow size plotted against the ﬂow’s rate (as measured by
total bytes over completion time) for ﬂows in Trace 2 that
are larger than 10,000 bytes (the other traces have similar
results). No correlation is visually apparent, and when we
compute the correlation it is quite small; the correlations
between rate and size are .0075, .0031 and .00088 for the
three traces, and are .11, .22 and −.0045 when we correlate
the logarithms of the rate and size. This shows that the
distinction between cheetahs and turtles is diﬀerent from
the historical distinction between elephants and mice. The
long-tailed distribution of rates is clearly not directly driven
by the long-tailed distribution in sizes. Their underlying
mechanisms are presumably diﬀerent.

As for cooperation, any host can obtain more bandwidth
simply by using a more aggressive congestion control algorithm. Thus, the current approach relies on end-hosts (and
their users) voluntarily adopting the TCP-compatible guidelines. There has been some initial work to penalize ﬂows that
violate the rules [9], but so far the goal of reliably identifying
ill-behaved ﬂows has proved elusive.25
In response to these problems, there has been a long history
(dating back to Nagle [16]) of proposing that routers play
a more active role in allocating bandwidth. If the routers
ensure fair bandwidth allocations26 then end-hosts are no
longer required to adhere to any particular form of congestion control and the problems of uniformity and cooperation mentioned above no longer exist.27 The proposals
to accomplish this fair bandwidth allocation, such as Fair
Queueing [6] and related algorithms [2, 15, 23], all involve
complicated packet scheduling algorithms and require per-

We are not aware of work that analyzes the mechanisms
responsible for the distribution of ﬂow rates. We assume
that the rates on a link, at least for longer-lasting ﬂows, reﬂects the available bandwidth at that ﬂow’s bottleneck link.
However, it is not clear to us why the distribution of these
bottleneck rates should have a long tail. Much more work
remains to be done to characterize these rate distributions,
and explain their origin.

25

There has been more success with identifying highbandwidth ﬂows, and unresponsive ﬂows, but that still
leaves ﬂows a large leeway to cheat.
26
To give users an incentive to use some form of responsive
congestion control we add the requirement that ﬂows are
punished if they incur persistent high drop rates. See [10,
24] for discussions of this topic.
27
Some claim that such fair allocation mechanisms open
the door to denial-of-service attacks through ﬂow-spooﬁng;
while we do not believe such arguments are suﬃcient to nullify the desirability of fair bandwidth allocations, and that
this paper is not the place to delve into such arguments at
length, we did want to note the existence of objections to
the fair bandwidth allocation paradigm.

cay slower than exponentially. Power-law distributions are
examples of this, but so are Weibull and other distributions.
We do not intend to enter the rather lengthy and subtle
debates about whether or not a particular distribution is a
power law or Weibull or some other form. We merely observe that it obviously decays slower than an exponential.

ACM SIGCOMM Computer Communications Review

37

Volume 33, Number 2

FIFO packet scheduling will be signiﬁcantly easier to implement than certain non-FIFO scheduling algorithms; it uses
cheaper hardware that scales to faster speeds. Second, the
distribution of rates on high-speed links will be long-tailed.
We don’t expect this to necessarily be a power-law or any
other particular form, but we do expect that the majority
of ﬂows will be slow but the fast (high-rate) ﬂows will send
the bulk of the bytes. Third, we assume that memory –
fast, slow, and CAM – will continue to decrease in price
relative to the special logic needed to implement non-FIFO
packet scheduling. AFD’s main implementation burden is
additional memory; we assume that the marginal cost of
equipping routers with AFD will shrink (relative to packet
scheduling designs) as these commodity products become
cheaper over time. If these three assumptions hold, then we
believe AFD may be a cost-eﬀective scheme for providing
approximately fair bandwidth allocations.

ﬂow state. High-speed implementations of these algorithms
are just now becoming available. If such implementations
continue to scale to the highest speeds without causing undue costs, then the contents of this paper are probably moot.
However, it isn’t yet clear that these implementations will
scale inexpensively to increasingly higher speeds. If the cost
of adding this extra functionality is signiﬁcant, designs with
lower complexity but similar functionality may be preferable in commercial designs. This was our goal in designing
AFD, to provide fairness at a signiﬁcantly reduced level of
complexity.
Core-Stateless Fair Queueing (CSFQ) [24] and the subsequently proposed variations [5, 25] share the same goal.
While they achieve high degrees of fairness with scalable
mechanisms in the core routers, these schemes require a
change in the packet format (to accommodate another ﬁeld)
and the careful conﬁguration of routers into core, edge, and
peripheral regions. Thus, while CSFQ delivers a high degree
of fairness, it faces signiﬁcant deployment hurdles.

8. REFERENCES
[1] Bansal, D., and Balakrishnan, H., “Binomial
Congestion Control Algorithms ” Proceedings of
Infocom ’01.

There are several other proposals for low-complexity approximations of fairness, such as FRED [13], SRED [17], SFB [7],
CHOKe [18] and RED-PD [14]. These present a spectrum
of possible designs, diﬀering in the extent to which they
carefully manage the bandwidth allocation. The extremes
of the spectrum are complete fairness (Fair Queueing) on
one end and unmanaged allocation (RED) on the other; in
between, some algorithms carefully manage the bandwidth
of a few ﬂows (e.g. RED-PD) while others attempt to manage all ﬂows at the same time, but do a less careful job on
each one (e.g. AFD). Diﬀerent choices along this spectrum
embody diﬀerent expectations about the set of congestion
algorithms deployed. One possible view is that in the future almost all ﬂows will use a TCP-compatible congestion
control algorithm, and that there will only be a very few
malicious (or broken) ﬂows that are substantially more aggressive. In this scenario, routers only need to detect these
few outlying ﬂows and restrain their usage to an appropriate
level. RED-PD is an example of an algorithm well-suited to
this task. Another possible view is that ﬂows will use a very
wide variety of congestion control algorithms, not necessarily TCP-compatible, and that routers will need to allocate
bandwidth to ﬂows as the common case. AFD is designed for
this scenario. Which of these two approaches – identifying
a few outlying ﬂows and treating them as special or treating
allocation as the common case – depends on how the future
of congestion control unfolds.28 Our purpose in this paper
is not to argue that one vision is more likely than another,
merely to design an algorithm that would be suitable if the
second scenario comes to pass.

[2] Bennett, J. and Zhang, H., “Hierarchical Packet Fair
Queueing Algorithms”, SIGCOMM Symposium on
Communications Architectures and Protocols, pp.
143–156, Aug. 1996.
[3] http://www.caida.org/analysis/AIX/plen hist/
[4] Braden, B., Clark, D., Crowcroft, J., Davie, B.,
Deering, S.,Estrin, D., Floyd, S., Jacobson, V.,
Minshall, G., Partridge, C., Peterson,
L.,Ramakrishnan,K., Shenker,S., Wroclawski, J.,
Zhang, L., “Recommendations on queue management
and congestion avoidance in the internet”, IETF RFC
(Informational) 2309, April 1998.
[5] Cao, Z., Wang, Z. and Zegura, E., “Rainbow Fair
Queueing: Fair Bandwidth Sharing Without Per-Flow
State”, Proceedings of INFOCOM’00 March 2000.
[6] Demers, A., Keshav, S. and Shenker, S., “Analysis
and simulation of a fair queueing algorithm”, Journal
of Internetworking Research and Experience, pp 3-26,
Oct. 1990. Also in Proceedings of ACM
SIGCOMM’89, pp 3-12.
[7] Feng, W., Shin, K., Kandlur, D. and Saha, D.,
“Stochastic Fair Blue: A Queue Management
Algorithm for Enforcing Fairness”, Proceedings of
INFOCOM’2001 (to appear), April, 2001.
[8] Floyd, S. and Jacobson, V., “Random Early Detection
Gateways for Congestion Avoidance”, IEEE/ACM
Transaction on Networking, 1(4), pp 397-413, Aug.
1993.

The spectrum of approximate fairness designs also presents
us with diﬀerent tradeoﬀs between increased fairness and
reduced complexity. The desirability of one spot on the
spectrum versus another depends greatly on the nature of
Internet traﬃc and on router design constraints, both of
which change over time. We based AFD on the assumption that while traﬃc characteristics and hardware capabilities will evolve, there will be three enduring truths. First,

[9] Floyd, S., and Fall, K., “Router Mechanisms to
Support End-to-End Congestion Control”, LBL
Technical report, February 1997.
[10] Floyd, S., and Fall, K., “Promoting the Use of
End-to-End Congestion Control in the Internet”,
IEEE/ACM Transactions on Networking, August
1999.

28

That future will depend in part on the choices made for
router allocation mechanisms, so the dependencies are circular.

ACM SIGCOMM Computer Communications Review

38

Volume 33, Number 2

[11] Floyd, S., Handley, M., Padhye, J., and Widmer, J.,
“Equation-Based Congestion Control for Unicast
Applications”, Proceedings of ACM SIGCOMM’2000,
August 2000.

[26] Zhang, Y., Paxson, V., and Shenker, S., “The
Stationarity of Internet Path Properties: Routing,
Loss, and Throughput”, ACIRI Technical Report,
May 2000.

[12] Hollot, C.V., Misra, V., Towsley, D. and Gong, W.,
“On Designing Improved Controllers for AQM Routers
Supporting TCP Flows”, Proceedings of Infocom ’01.

[27] Zhang, L., Shenker, S. and Clark, D., “Observations
on the Dynamics of a congestion control Algorithm:
The Eﬀects of Two-Way Traﬃc”, Proceedings of ACM
SIGCOMM’91.

[13] Lin, D. and Morris, R., “Dynamics of random early
detection”, Proceedings of ACM SIGCOMM’97, pp
127-137, Oct. 1997.

[28] ns - Network Simulator (Version 2.1b6).

[14] Mahajan, R., Floyd, S. and Wetherall, D.,
“Controlling High-Bandwidth Flows at the Congested
Router”, 9th International Conference on Network
Protocols, November 2001.
[15] McKenney, P., “Stochastic Fairness Queueing”,
Proceedings of INFOCOM’90, pp 733-740.
[16] Nagle, J., “On packet switches with inﬁnite storage”,
Internet Engineering Task Force, RFC-970, December,
1985.
[17] Ott, T., Lakshman, T. and Wong, L., “SRED:
Stabilized RED”, Proceedings of INFOCOM’99, pp
1346-1355, March 1999.
[18] Pan, R., Prabhakar, B. and Psounis, K., “CHOKe - A
Stateless Active Queue Management Scheme For
Approximating Fair Bandwidth Allocation”,
Proceedings of INFOCOM’00 March 2000.
[19] Paxson, V., “End-to-End Internet Packet Dynamics”,
IEEE/ACM Transactions on Networking, vol. 7, no. 3,
pp. 277-292, 1999.
[20] Paxson, V., Mahdavi, J., Adams, A. and Mathis, M.,
“An Architecture for Large-Scale Internet
Measurement”, IEEE Communications Magazine, vol.
36, no. 8, pp. 48-54, August 1998.
[21] Rejaie, R., Handley, M. and Estrin, D., “An
End-to-end Rate-based Congestion Control
Mechanism for Realtime Streams in the Internet”,
Proceedings of INFOCOM’99, March, 1999.
[22] Rhee, I., Ozdemir, V. and Yi, Y., “TEAR: TCP
emulation at receivers – ﬂow control for multimedia
streaming”, Technical Report, Department of
Computer Science, NCSU, April, 2000.
[23] Shreedhar, M., and Varghese, G., “Eﬃcient Fair
Queueing using Deﬁcit Round Robin”, ACM
Computer Communication Review, vol. 25, no. 4, pp.
231–242, October, 1995.
[24] Stoica, I., Shenker, S. and Zhang, H., “Core-Stateless
Fair Queueing: Achieving Approximately Fair
Bandwidth Allocations in High Speed Networks”,
Proceedings of ACM SIGCOMM’98.
[25] Venkitaraman, N., Mysore, J., Srikant R.,and Barnes,
R. “Stateless Prioritized Fair Queueing”, Internet
Engineering Task Force July 2000.

ACM SIGCOMM Computer Communications Review

39

Volume 33, Number 2

Reliability Engineering and System Safety 152 (2016) 104–114

Contents lists available at ScienceDirect

Reliability Engineering and System Safety
journal homepage: www.elsevier.com/locate/ress

A computational Bayesian approach to dependency assessment
in system reliability
Petek Yontay, Rong Pan n
Arizona State University, School of Computing, Informatics, and Decision Systems Engineering, 699 S. Mill Avenue, Tempe, AZ 85281, USA

art ic l e i nf o

a b s t r a c t

Article history:
Received 24 July 2015
Received in revised form
1 February 2016
Accepted 5 March 2016
Available online 19 March 2016

Due to the increasing complexity of engineered products, it is of great importance to develop a tool to
assess reliability dependencies among components and systems under the uncertainty of system reliability structure. In this paper, a Bayesian network approach is proposed for evaluating the conditional
probability of failure within a complex system, using a multilevel system conﬁguration. Coupling with
Bayesian inference, the posterior distributions of these conditional probabilities can be estimated by
combining failure information and expert opinions at both system and component levels. Three data
scenarios are considered in this study, and they demonstrate that, with the quantiﬁcation of the stochastic relationship of reliability within a system, the dependency structure in system reliability can be
gradually revealed by the data collected at different system levels.
& 2016 Elsevier Ltd. All rights reserved.

Keywords:
Bayesian network
Dependency
Bayesian inference
Multi-level data
Incomplete information

1. Introduction
Due to increasing demands of product functionality, engineered
products have become more and more complex over time. Traditional reliability assessment methods for simple systems are often
inadequate in analyzing more complex systems. Conducting full
system tests is often too expensive to be implemented on such
systems. This situation calls for a method to develop reliability
models for complex systems and to integrate all available information for predicting system reliability.
There are situations that we do not have complete information
of how a complex system would fail in its operating environment.
We would like to learn more about the interaction between the
system and its components and how they work together. In this
paper, we use a Bayesian network (BN) to represent the probabilistic relationship between system and component reliability,
which is a natural extension of the deterministic relationship
typically modeled by block diagrams or fault trees when the failure structure is well understood.
The BN model has been proved to be a powerful tool that
provides important methodological advantages over traditional
techniques in reliability assessment. Traditional methods, such as
fault trees or reliability block diagrams, are still common representations in system reliability analysis; however, they are not
ﬂexible enough to capture the uncertainties in the dependencies
n

Corresponding author.
E-mail addresses: pyontay@asu.edu (P. Yontay), rong.pan@asu.edu (R. Pan).

http://dx.doi.org/10.1016/j.ress.2016.03.005
0951-8320/& 2016 Elsevier Ltd. All rights reserved.

among component, subsystem and system (see [1,19,2,17,34]). BNs
generalize fault trees by allowing components and subsystems to
be related by conditional probabilities, instead of deterministic
“AND” and “OR” relationships; thus, they provide analytical
advantages to the situation when we have less conﬁdence to the
reliability structure of a complex system, especially during the
early stage of a product’s design process. Another important
advantage of BN over the traditional approach is its ability of
combining information from multiple sources at multiple levels
for system reliability prediction when the BN model is coupled
with statistical Bayesian inference techniques. As a result, it is
worthwhile to explore the use of BN model and Bayesian inference
together for the dependency assessment of system reliability.
A BN model requires conditional probabilities to model the
dependencies among components, subsystems and system. These
conditional probabilities are capable of representing complex,
probabilistic failure relationships in a multilevel system conﬁguration. In a complex system, the failure relationship between
system and components could be signiﬁcantly more complicated
than a typical series or parallel system, especially when the speciﬁc failure cause and failure mechanism has yet to be understood;
for example, a newly developed system [35]. Therefore, investigating the conditional probability tables of a BN model can help
engineers to sort out those unknown inﬂuential factors.
The conditional probabilities in a BN model can be estimated by
combining information from different sources. There are objective
information sources, such as failure records of older generation
products, life tests of components and available ﬁeld data; on the
other hand, there are subjective sources too, such as expert

P. Yontay, R. Pan / Reliability Engineering and System Safety 152 (2016) 104–114

opinions. These data come with different types and different
structures, causing difﬁculties in the estimation of conditional
probabilities. Furthermore, as system designs evolve over time,
assigning ﬁxed values to these probabilities limits the ﬂexibility to
account for the evolution process of system development. Therefore, we choose Bayesian inference for parameter estimation in the
BN model. Bayesian inference is a statistical inference method that
enables model parameter estimation by deriving the posterior
distribution from a combination of prior distribution and likelihood function. It allows us to integrate both the prior information
of model parameters and the data coming from different sources
for model inference; therefore, we can obtain a more precise
estimation of BN model parameters.
The goal of this research is to develop the methodology of
estimating conditional probabilities of BN models using Bayesian
inference so that the reliability-relevant information from different sources at different reliability structure levels of a complex
system can be combined together. The next section presents a
literature review of BN models and Bayesian inference in reliability
assessment. Our BN framework for system reliability and its
inference method are discussed in Section 3. We start by discussing how to infer conditional probabilities using a conjugate
model for a simple 2-state Bayesian network and then extend it to
a multi-state model. We also brieﬂy discuss the case where we
have only system failure records. Finally, we develop a data analysis method for the scenario of having incomplete information of
components. We illustrate the proposed method with a case study
in Section 4 and conclude the paper in Section 5.

2. Background
2.1. Models for multilevel system reliability assessment
One of the primary goals in system design evaluation is to
predict the reliability of the full system. A system is comprised of
subsystems and components, or on functional wise, sub-functions
and elementary functions, which can be represented by nodes in
the system’s reliability topology. All nodes are potential sources of
failure. Consequently, reliability information may come from different levels of the system and it tends to be heterogeneous. With
data available at different system levels, the challenge becomes
how to combine them to learn about the reliability of the system.
The Bayesian method is very appealing for this challenging problem. Martz et al. [21] and Martz and Wailer [20] addressed the
problem of integrating multilevel binary data from various levels
of the system and from expert guesses about the reliability of
system components. These papers focused on series and parallel
systems, where component failure data were modeled using
binomial distributions, and beta distributions were used for the
prior information at components, subsystem and system levels.
Several follow-up papers considered other computational Bayesian approaches to model inference and system reliability prediction. For example, Johnson et al. [14] proposed a hierarchical
Bayes model approach to system reliability prediction. Their
approach utilized Markov chain Monte Carlo (MCMC) to infer
model parameters, thus avoided analytical approximation.
Hamada et al. [11] applied the same approach on the non-overlapping, continuous failure time data of basic and higher-level
failure events in a fault tree. Graves et al. [7] further extended this
line of research by considering multi-state fault trees. They used
Dirichlet distribution to deﬁne the prior information about the
probabilities of the states in the model. In addition, Graves et al.
[8] proposed a Bayesian approach to properly account for simultaneous multilevel data, i.e., use the simultaneous higher-level and
partial lower-level data to determine the event of component

105

failure. In another follow-up study, [26] considered lifetime data
throughout the system. They presented a Bayesian model that
accommodates multiple lifetime information sources and provided a method to model the time evolution of a system’s reliability. Wilson et al. [30] proposed a methodology that allowed for
the combination of different types of data at the component and
system levels, and took a Bayesian approach to the estimation of
reliability measure. Wilson et al. [29] showed how to combine
different types of reliability data with an example that had binomial data (modeled with a logistic regression) from the system
and one component, lifetime data from another component, and
degradation data from a third component. Guo [10] discussed a
uniﬁed Bayesian approach for simultaneously predicting system,
subsystem, and component reliabilities when there are pass/fail,
lifetime, degradation, or expert judgment data at any level of the
system, which extended the work in Wilson et al. [30]. However,
these studies were mostly based on fault trees and reliability block
diagrams and did not cover the BN representation of system
reliability.
In the system reliability literature, the idea of using BN model
as an alternative to fault tree or reliability block diagram for
representing system reliability structure has been discussed by
many authors (e.g., [1,19,2,17]; Wilson and Huzurbazar, 2007;
[18]). However, previous studies do not address the problem of
assessing reliability dependencies between system and its components. In this paper, we will assess these dependencies using a
computational Bayesian inference method; that is, given reliability
information from multiple sources and at multiple levels of a
system, we will provide the Bayesian estimation of the conditional
probability parameters required in the BN model. The posterior
distribution of these parameters can be used to quantify the
variability of the dependency of system reliability to its components. Furthermore, previous study has not addressed the effect of
simultaneous, incomplete data, drawn from different system
levels, on BN inference. Since we aim to measure the reliability
dependencies within a system, system and component data
should be collected simultaneously, as independent datasets will
not be able to capture these dependencies. However, getting
simultaneous data from all components/subsystems may not be
possible due to lack of sensors or other observation limitations.
Graves et al. [8] and Jackson [13] analyzed the effect of simultaneous data on system reliability prediction using a fault tree
model. The BN model is different from fault trees because it
represents the probabilistic relationship between system and
components. In this paper, we will discuss the Bayesian inference
method that allows us to analyze simultaneous data that are
drawn from the system and a subset of its components.
2.2. Computational methods in Bayesian inference
The posterior distribution resulting from a complex Bayesian
model often cannot be written in a closed form. This difﬁculty has
hindered the adoption of Bayesian reliability assessment for many
years. However, since the 1990s, advances in Bayesian computation through Markov chain Monte Carlo (MCMC) have facilitated
inference based on samples from the targeted posterior distribution [5]. MCMC is a simulation-based algorithm for performing
Bayesian inference when conjugation is impossible (thus analytical
result is impossible). It is particularly useful for high-dimensional
Bayesian inference. MCMC algorithms draw samples from the
targeted joint posterior distribution of model parameters. Gibbs
sampler, the most popular MCMC algorithm, relies on the fact that
samples drawn sequentially from complete conditional distributions will converge to the joint posterior distribution as long as
distribution parameters are constantly updated. So, after a certain
number of preliminary iterations, the samples drawn from

106

P. Yontay, R. Pan / Reliability Engineering and System Safety 152 (2016) 104–114

simulation chains can be viewed as if they are from the targeted
joint posterior distribution.
MCMC has also made the system reliability Bayesian model
solvable when addressing the multilevel system reliability problem. In the reliability literature, several authors had used the
MCMC technique for Bayesian inference (see [14,11,24,30,7]; Wilson and Huzurbazar, 2007; [8], [22], [10]). To implement MCMC,
we use WinBUGS, a statistical software for Bayesian inference [27].

3. Methodology

Fig. 1. A simple 2-component system BN example.

In this section, Bayesian inference methods for estimating
conditional probabilities in a Bayesian network are discussed. We
give a brief description of Bayesian network, and then present
three different data scenarios with decreasing amount of available
information along these scenarios. The ﬁrst scenario involves a
simple 2-state Bayesian network where the states of all nodes are
recorded. We demonstrate a conjugation model for inferring
conditional probabilities and also extend it to a multi-state BN. In
the second scenario, we discuss the case when we have only
system failure records. Lastly, we consider a scenario where only
the system and a subset of components are monitored by sensors,
thus the system’s reliability information is incomplete. We present
a Bayesian inference method for estimating reliability dependencies in such systems.
3.1. Bayesian parameter estimation in Bayesian network
Bayesian networks (BNs) are probabilistic graphical models
depicting conditional independence relationships and inducing a
factorization into the joint probability mass/density function over
the network variables [16]. The joint probability of all nodes in a
BN can be expressed as the product of conditional probabilities,
one for each node given the corresponding values of its parent
nodes. A Bayesian network consists of two main parts:

 Qualitative part: It consists of a directed acyclic graph (DAG)



where nodes represent random variables (either continuous or
discrete) and directed arcs representing causal relationships
between these nodes.
Quantitative part: It is the conditional probability table that
quantiﬁes the relationship between parent and child nodes.

In a BN, the nodes without any arrows directed into them are
called root nodes and they are characterized by their marginal
probability distributions. The nodes that have arrows directed into
them are called child nodes and the nodes that have arrows
directed from them are called parent nodes. Each child node has a
conditional probability table associated with it, given the values of
parent nodes.
Consider a BN over variables X 1 ; X 2 ; …; X n : By the chain rule of
probability, the joint probability PðX 1 ; X 2 ; …; X n Þ is
PðX 1 ; X 2 ; …; X n Þ ¼ ∏ni¼ 1 P ðX i j paðX i ÞÞ

ð1Þ

where paðX i Þ is the set of parents of node X i . Some nodes in a BN
may become conditionally uncorrelated if there is no direct link
between them. This is called conditional independence. These
conditional independences allow us to decrease the number of
terms in the chain rule, providing a simpler structure.
Fig. 1 shows a simple BN with 3 nodes and 2 arcs. Each node C i
is a random variable. If there is a directed arc from C i to C j , C i is
called a “parent” of C j . An arc characterizes the probabilistic
dependency of a node on its parent nodes. That is, depending on
the values a node's parents take on, the conditional probability
distribution of the node may be different. In this example, node C 0

Table 1
Marginal probability tables for the BN example.
C1 ¼ 0
1 p1
C2 ¼ 0
1 p2

C1 ¼ 1
p1
C2 ¼ 1
p2

Table 2
Conditional probability table (CPT) for the BN example.

C 1 ¼ 0; C 2 ¼ 0
C 1 ¼ 0; C 2 ¼ 1
C 1 ¼ 1; C 2 ¼ 0
C 1 ¼ 1; C 2 ¼ 1

C0 ¼ 0

C0 ¼ 1

1 p00
1 p01
1 p10
1 p11

p00
p01
p10
p11

has 2 parents, C 1 and C 2 . The marginal probabilities of these parent nodes are listed in Table 1. Assuming binary states for each
node (functional state is 0 and disfunctional state is 1), Table 2
shows the conditional probability table (CPT) for each combination
of the parents of C 0 . In this paper, a BN is employed to represent
the cause-and-effect failure relationship among elements of a
multilevel system, in which the ﬁnal child node represents the
system and other nodes represent either components or subsystems of the system.
The parameters, p1 and p2 , listed in Table 1 are the distribution
parameters of the marginal distributions (binomial) of the failure
count variables of these 2 components, while Table 2 gives the
parameters used in the conditional distribution of Eq. (1). A BN
model is fully deﬁned if all of these parameters are speciﬁed. The
joint distribution of all nodes, Eq. (2), has then become analytically
available. In a fault tree (or reliability block diagram) representation of system reliability, the conditional probabilities in Table 2
are already pre-speciﬁed for a given logic gate (or the block diagram conﬁguration). For example, for an “AND” gate, we have p00
¼ p01 ¼ p10 ¼ 0 and p11 ¼ 1, while for an “OR” gate, we have p00 ¼ 0
and p11 ¼ p10 ¼ p01 ¼ 1. However, in this paper, those relationships
are not pre-speciﬁed, as the dependency of system reliability to its
components is unknown and needs to be evaluated by the data
collected from the system and from other information sources
such as expert opinions.
In the following sections, we will discuss Bayesian inference on
model parameters in the context of a Bayesian network. Bayesian
framework requires us to specify a joint distribution over unknown
parameters and data instances. Suppose we want to estimate the
parameters of the BN in Fig. 1, then our model is parameterized by a


parameter vector p, where p ¼ p1 ; p2 ; p11 ; p10 ; p01 ; p00 . Given the
prior distribution of this parameter vector and the data collected
from all nodes, Bayesian inference will provide the posterior distribution of the parameter of interest and the posterior prediction of
system or component reliability. This process is called the

P. Yontay, R. Pan / Reliability Engineering and System Safety 152 (2016) 104–114

parameter learning of a Bayesian network. We start discussing
parameter learning with complete data in Section 3.2, and then
extend the case to incomplete data case in Sections 3.2 and 3.3.
3.2. Complete system log data
Although it is uncommon in practice, we start our discussion
with this naïve scenario – a complete history of the states of the
system and its components are available. This is possible if both
the system and its components are continuously monitored by
sensors and the log data from existing products can be obtained.
The log data consists of the records of the states of the system and
its components over a period of time and is commonly used in
condition monitoring of engineering systems. In this example,
each historical record is assumed to be as from a pass/fail test, and
it consists

 of
 fully observed instances of all network variables
C ¼ C ðiÞ ¼ C 0 ðiÞ; C 1 ðiÞ; C 2 ðiÞ , i ¼ 1; …; N, which describes a particular assignment (0 or 1) to nodes C 0 , C 1 and C 2 . The likelihood
function is then given by

Table 3
Marginal probability tables for the multi-state BN example.
C1 ¼ 0

C1 ¼ 1
p11

⋯
⋯

C1 ¼ l  1

p01
C2 ¼ 0
p02

C2 ¼ 1
p12

⋯
⋯

C2 ¼ m  1
1
pm
2

pl1 1

Table 4
Conditional probability table (CPT) for the multi-state BN example.
C0 ¼ 0

C0 ¼ 1

⋯

C0 ¼ k  1

C 1 ¼ 0; C 2 ¼ 0

p000

p100

⋯

C 1 ¼ 0; C 2 ¼ 1
⋮
C 1 ¼ i; C 2 ¼ j

p101
⋮
p1ij

⋯

pk00 1

p001
⋮
p0ij

⋱
⋯

⋮
C 1 ¼ l  1; C 2 ¼ m  1

⋮
p0l  1;m  1

⋮
p1l  1;m  1

⋱
⋯

P

N

L
L p1

LðC j pÞ ¼ ∏ P ðC 0 ðiÞ; C 1 ðiÞ; C 2 ðiÞj pÞ

107

¼ 1,

P

M
M p2

¼ 1 and

P

K
K pij

pk01 1
⋮
pkij  1
⋮
1
pkl 1;m
1

¼ 1 f or 8 i; j.

i¼1

¼ ∏ P ðC 1 ðiÞj pÞP ðC 2 ðiÞj pÞP ðC 0 ðiÞj C 1 ðiÞ; C 2 ðiÞ; pÞ
i

¼

!
∏ P ðC 1 ðiÞj pÞ
i

!
∏ P ðC 2 ðiÞj pÞ
i

!
∏ P ðC 0 ðiÞj C 1 ðiÞ; C 2 ðiÞ; pÞ
i

ð2Þ
According to the equation above, we have a separate factor for
each node. These factors are called local likelihood functions and
they depend on their corresponding node’s conditional or marginal probability table. We can maximize the likelihood function in
Eq. (2) and get maximum likelihood function estimates for the
parameters in probability tables. However, even this simple formula could become troublesome in practice when there are many
states for each component node. In such a case, the number of
combinations grows exponentially and there might be no observation for a particular combination, causing the data records to
have “zero” counts for the corresponding combination. Therefore,
it is better to combine the likelihood with expert opinions. This is
equivalent to assigning a prior distribution to model parameters.
In this approach, we encode our prior knowledge about p with
a probabilistic distribution. We now treat p as a random variable.
According to the Bayes’ formula, the posterior distribution over
parameters given the observed data is
Pr ðpC Þ ¼

PrðCjpÞPrðpÞ
PrðC Þ

ð3Þ

We can extend this discussion to multi-state models. In our BN
model so far, we have assumed that all nodes have two distinct
states: pass and fail. Now, consider the system and components
having multiple states; for example, k states for the system, C 0 ,


such as 0; …; k  1 , where state k  1 represents state of failure,
state 0 represents state of full functionality and the rest of the
states between represent degraded states; l states for component
C 1 and m states for the component C 2 (see Tables 3 and 4). The
likelihood function of the system can be derived from multinomial
distribution and the conjugate prior need to be speciﬁed by
Dirichlet distribution.
The likelihood function in this model has a similar form as in
the binomial case. However, since there are multiple states, there
will be a larger number of combinations of parent nodes. The
likelihood function has the following multinomial form:

!
 M C K0 ;C i1 ;C j
 L 	M½C L1   M 	M½C M2 
2
K
LðCpÞ ¼ ∏ p1
ð4Þ
∏ p2
∏ ∏ pij
L

8 i;j

M

K

If we assume a Dirichlet prior distribution, Dirichletðα0i ; …; αki  1 Þ,
for pi , then the posterior distribution of p is given by
P ðpj C Þ p P ðC j pÞP ðpÞ
 	αL þ M½C L1   1  M 	αM2 þ M½C M2   1
∏ p2
¼ ∏ pL1 1
L

M

∏ ∏



pKij

α K þ M
ij





C K0 ; C i1 ;C j2  1

!
ð5Þ

where, the term PrðpÞ is the prior distribution function of p, Pr
ðC j pÞ is the likelihood function and Pr ðC Þ can be viewed as a
normalizing constant. If a beta prior distribution, Betaðai ; bi Þ, is
assumed for each pi , it is easy to shown that the posterior distributions of the parameters of the Bayesian network are analytically available as

h i
h i
pi  Beta ai þ M C 1i ; bi þ M C 0i


h
i
h
i
pij  Dirichlet α0ij þ M C 00 ; C i1 ; C j2 ; …; αkij  1 þ M C k0  1 ; C i1 ; C j2


h
i
h
i
pij  Beta aij þ M C 10 ; C i1 ; C j2 ; bij þ M C 00 ; C i1 ; C j2


where M C x0 ; C y1 ; C z2 represents the counts where C 0 ðiÞ ¼ x, C 1 ðiÞ ¼ y
and C 2 ðiÞ ¼ z (x; y; z ¼ 0 or 1).
This result illustrates a conjugation property of the beta distribution when coupled with binomial likelihood (see Koller and
Friedman [16] for more details). Exact inference is possible in case of
binomial likelihood (pass/fail tests) and fully observed variables. In
literature, beta-binomial conjugation has been extensively used for
reliability prediction (see [21,20,14]; Wilson and Huzurbazar, 2007).

Bayesian conjugation is convenient for obtaining analytical
results; however, in most scenarios, the prior distribution may not
come from a conjugation family and the system is too complex to
model with conjugate pairs. For such cases, computational Bayesian methods such as MCMC need to be employed.
It is also of research interest to examine the effect of prior distribution assumption on posterior estimation. In general, specifying
a more informative prior reduces the variance of the posterior distribution, resulting in a more precise estimation. Therefore, eliciting
prior distributions in Bayesian inference is rather important for

8 i;j

K

Eq. (5) is comprised of posterior Dirichlet distributions for all
parameters in the BN; that is,

h i
h
i
pi  Dirichlet α0i þ M C 0i ; …; αki  1 þ M C ki  1

108

P. Yontay, R. Pan / Reliability Engineering and System Safety 152 (2016) 104–114

Fig. 2. Box plots of conditional probabilities with different prior distributions.

Table 5
System failure records.

Table 6
Data from
record.

System C 0

C1

C2

Failure event 1
Failure event 2
…

X
X
…

X

a

system

failure

Cause combinations Counts
C 1 ¼ 0; C 2 ¼ 0
C 1 ¼ 0; C 2 ¼ 1
C 1 ¼ 1; C 2 ¼ 0
C 1 ¼ 1; C 2 ¼ 1

…

representing prior knowledge more accurately and comprehensively. However, it is not usually a straightforward task to elicit prior
distributions for the parameters of the model and special techniques
must be used. One of the most commonly used techniques is expert
elicitation, which converts an expert’s opinions into a statistical
expression of these opinions ([6]). Experts are asked to give their
opinions about quantities for the distribution parameters such as the
mean, mode and median values. As a result, we can obtain an
appropriate prior for the parameters. A sensitivity analysis has been
carried out using the system in Fig. 1 to show the effect of using a
more informative prior. A dataset consisting of pass/fail data for all
components was simulated and used as observations for calculating
the likelihood. Beta(1,1) and Beta(10,10) were assigned as the priors
for model parameters, separately. The box plots of the posterior
samples of some model parameters are shown in Fig. 2. According to
the results, we get more precise results when Beta(10,10) is used as a
prior. Therefore, we would like to emphasize that special cares to
these prior distribution assignments are needed when Bayesian
inference is in use.
3.3. Summarized system failure data
A system failure record is often maintained within an organization in a summarized form, and it is commonly used for failure
diagnosis. In this case, once a system failure occurs, the components that are causing the failure are identiﬁed and this event is
recorded. For example, given a checklist such as Table 5, one can
see that a failure event occurred once due to C 1 and C 2 , and once
due to C 1 only, etc.
Notice that, with these records, we can directly estimate the
probability of component failure given a system failure, i.e.,
Pr ðC 1 ; C 2 j C 0 ¼ 1Þ, but not the probability of system failure given
the states of components. This is because, unlike the log data,
Table 5 records only system failure events. The joint probability of

4
9
12
75

component states and the marginal probability of system failure
are required in order to obtain the conditional probability of system failure, because
Pr ðC 0 j C 1 ; C 2 Þ ¼

PrðC 1 ; C 2 jC 0 ÞPrðC 0 Þ
PrðC 1 ; C 2 Þ

ð6Þ

In Eq. (6), Pr ðC 0 Þ represents the prior knowledge about system
failure and Pr ðC 0 j C 1 ; C 2 Þ represents the posterior failure distribution after observing the failure record data. Yontay et al. [32] discussed a method for deriving the prior probability, Pr ðC 0 Þ. If each
failure event is recorded with its time stamp, we can use failure
times to estimate the failure rate of the system. Assuming the time
to failure is exponentially distributed, after estimating the occurrence rate of the failure events, we can then convert failure times
to event probabilities based on exponential distribution. That is,
Pr ðC 0 ¼ 1Þ ¼ Pr ðT o t Þ ¼ 1  e  λF t

ð7Þ

where λF is the occurrence rate of system failure event and t is
the system lifetime. The next step is to calculate, Pr ðC 1 ; C 2 j C 0 Þ,
which is the likelihood for each combination of component states,
using Table 5.
As an example, consider the system in Fig. 1. In this scenario,
the system failure might be caused by C 1 or C 2 , or C 1 and C 2
together, or the system might fail even when both of the components are functioning (by an unknown failure cause). Given the
recorded failure times, we can obtain an initial estimate of the
prior distribution for system failure, which is deﬁned as Beta(1.28,
1.30). Field observations of the system, which are summarized as
the counts for each combination as shown in Table 6, can be
modeled by a multinomial distribution.
The simulation results from WinBUGS are shown in Table 7.
Since the system failure probability when at least one of the
components is working is very small, we can conclude that the

P. Yontay, R. Pan / Reliability Engineering and System Safety 152 (2016) 104–114

109

Table 7
Empirical mean, standard deviation and quantiles for posterior failure probabilities.

Pr ðC 0 ¼ 1C 1 ¼ 0; C 2 ¼ 0Þ
Pr ðC 0 ¼ 1C 1 ¼ 0; C 2 ¼ 1Þ
Pr ðC 0 ¼ 1C 1 ¼ 1; C 2 ¼ 0Þ
Pr ðC 0 ¼ 1C 1 ¼ 1; C 2 ¼ 1Þ

Mean

SD

2.5%

25%

50%

75%

97.5%

0.0516
0.1002
0.1294
0.7434

0.02172
0.02945
0.03291
0.04304

0.01779
0.05038
0.07218
0.6542

0.03574
0.07898
0.1059
0.715

0.04863
0.09769
0.1271
0.7451

0.06429
0.1186
0.1503
0.7735

0.1019
0.1646
0.2001
0.8227

system behaves like a parallel system. However, since there exists
an un-ignorable probability of system failure (its mean value is
0:0516 and 95% credible interval is ½0:01779; 0:1019) when both
components are functional, it indicates some unknown factor that
inﬂuences system reliability. As a result, we need to conduct further investigation of this unknown factor.
This approach can also been seen as an extension of the reliability parenting process presented in [25], in which the authors
utilized the failure information of old-generation products stored
in a failure database.
3.4. Incomplete and simultaneous system- and component-level
data
One big challenge in system reliability assessment is the lack of
the complete lower-level data as presented in previous sections. A
complex system does not necessarily have all components or
subsystems being monitored at the same time. There can be a
limited number of sensors deployed in the system to monitor the
states of some components or subsystems, but not all of them. In
addition, these sensor data are stored by sensor, not in the system
format such as the row entries in Table 5. Since system’s functionality conditions on the components and subsystems’ functionalities, analyzing data from different system levels collectively
yields signiﬁcant information about the reliability structure.
However, data collected by multiple sensors in the same system at
multiple system levels may contain duplicated system reliability
information, thus they require different data analysis technique.
The basic problem for analyzing this type of data is that we
cannot treat them as independent data although they come from
individual sensors. The dependencies between the states of systems and components under monitoring must be taken into consideration in data analysis. Only a few previous studies had
addressed this problem. Graves et al. [8] proposed a method that
incorporates overlapping data for traditional binary-state series/
parallel systems. Their methodology relies on disjoint cut-set
generation and considers each observation in isolation. Jackson
[13] extended this line of research by adding continuous failure
time data. However, their methodology can only apply to the
system failure that is represented by a fault tree. In this section, we
consider the data scenario with simultaneous, multi-level sensor
data from the same system and incorporate it into the BN model
analysis. A Bayesian inference method is developed for dealing
with simultaneous higher-level data and partial lower-level data.
Suppose that a system-level sensor monitors the system’s
health status. Some (not all) of its components/subsystems are also
monitored by their own sensors. Each sensor will store the information such as how many failures occurred in a time interval (e.g.,
a day). These failures at different levels are correlated, as they
come from the same system. For instance, considering a twocomponent series system, if the system is known to be functioning, this implies that both components must be functioning too.
But, if both components are not monitored and the system is not
functioning, it is impossible to know which component failed or
both of them failed. Only if we have one component monitored,
the other component’s state can be inferred by the observations at

both system and component levels. In general, tracking and consolidating the states of monitored system and components can be
done when a deterministic system reliability conﬁguration is
known. However, this process can be very tedious and the result
varies according to system conﬁguration. Using BN models, we are
able to provide a generic algorithm of sensor data consolidation
and code it into a computer program.
To develop the likelihood function of a BN model with simultaneous, multi-level sensor data, all possible instances of component and system states that imply the observed evidence by sensors need to be captured. To formulate the probability function for
each of these combinations, we start by constructing the state
vector of all nodes in a Bayesian network. The state variable of the
ith node is denoted by xi (0 for functional and 1 for dysfunctional).
The states of all nodes are given by the state vector, such as x ¼
fx1 ; x2 ; …; xn ; x0 g when the BN model has n component nodes and
one system node (x0 ).
Assume that all nodes are binary-state nodes, then there are
2n þ 1 possible combinations and hence 2n þ 1 possible state vectors.
For example, for a 2-component system, there are 22 þ 1 ¼ 8 possible state vectors, and they are x1 ¼ f0; 0; 0g, x2 ¼ f0; 0; 1g,
x3 ¼ f0; 1; 0g, x4 ¼ f0; 1; 1g, x5 ¼ f1; 0; 0g, x6 ¼ f1; 0; 1g, x7 ¼ f1; 1; 0g
and x8 ¼ f1; 1; 1g. The probability of each state vector’s occurrence
is deﬁned by the joint distribution of the BN (see Eq. (1)). As an
example, for the 2-component system in Fig. 1, we can deﬁne the
joint probability of each combination to be
Pr ðx1 Þ ¼ Prðx1 ¼ 0ÞPrðx2 ¼ 0ÞPrðx0 ¼ 0j x1 ¼ 0; x2 ¼ 0Þ
Pr ðx2 Þ ¼ Prðx1 ¼ 0ÞPrðx2 ¼ 0ÞPrðx0 ¼ 1j x1 ¼ 0; x2 ¼ 0Þ
Pr ðx3 Þ ¼ Prðx1 ¼ 0ÞPrðx2 ¼ 1ÞPrðx0 ¼ 0j x1 ¼ 0; x2 ¼ 1Þ
Pr ðx4 Þ ¼ Prðx1 ¼ 0ÞPrðx2 ¼ 1ÞPrðx0 ¼ 1j x1 ¼ 0; x2 ¼ 1Þ
Pr ðx5 Þ ¼ Prðx1 ¼ 1ÞPrðx2 ¼ 0ÞPrðx0 ¼ 0j x1 ¼ 1; x2 ¼ 0Þ
Pr ðx6 Þ ¼ Prðx1 ¼ 1ÞPrðx2 ¼ 0ÞPrðx0 ¼ 1j x1 ¼ 1; x2 ¼ 0Þ
Pr ðx7 Þ ¼ Prðx1 ¼ 1ÞPrðx2 ¼ 1ÞPrðx0 ¼ 0j x1 ¼ 1; x2 ¼ 1Þ
ð8Þ
Pr ðx8 Þ ¼ Prðx1 ¼ 1ÞPrðx2 ¼ 1ÞPrðx0 ¼ 1j x1 ¼ 1; x2 ¼ 1Þ
P8
with the constraint that i ¼ 1 Prðxi Þ ¼ 1.
After formulating these state vector probabilities, we need to
count how many times each state vector is observed in a speciﬁc
evidence set. Thus, we represent
the occurrence oof each state
n
vector by a count vector, y ¼ y1 ; y2 ; …; yj ; …; y2n þ 1 , where yj is
the number of occurrences of the jth state vector, xj .
Consider the 2-component system example in Fig. 1. We need to
keep track of the counts for each of the 8 state vectors. If we observe
the state vector x8 ¼ f1; 1; 1g twice in an evidence set, then y8 ¼ 2. If
we also observe x4 ¼ f0; 1; 1g once, then y4 ¼ 1. Combining them
together, the count vector is given by y ¼ f0; 0; 0; 1; 0; 0; 0; 2g.
The likelihood function of speciﬁc evidence set is derived from
a multinomial distribution. As a sensor signal only depends on the
state of the node under its monitoring, each observation set from
the system leads to exactly one state vector, then the count vector
clearly follows a multinomial distribution with its parameters
being the state vector probabilities deﬁned in Eq. (8). That is, the
random variables yj indicate the number of occurrence state vector
xj observed over N instances.

110

P. Yontay, R. Pan / Reliability Engineering and System Safety 152 (2016) 104–114

Therefore, the likelihood function of one speciﬁc evidence set is
given by
 
		y
N!
ðPr ðx1 ÞÞy1 ðPr ðx2 ÞÞy2 … Pr x2n þ 1 2n þ 1
y1 !y2 !…y2n þ 1 !
  		yj
2n þ 1 Pr x
j
¼ N! ∏
yj !
j¼1
( "
!
2n þ 1
n  	
	½1  ðxi Þj 
1
ðx Þ 
∏ pi i j 1  pi
¼ N! ∏
j ¼ 1 yj !
i¼1
)

ðx0 Þj 
½1  ðx0 Þj  
yj
1  pðx1 Þj …ðxn Þj
 pðx1 Þj …ðxn Þj

Pr ðyj pÞ ¼

ð9Þ

When there are only a partial set of components are monitored, it
is important to realize that there could be more than one count vector
that satisﬁes the evidence set from sensors. Thus, we need to keep
track of the count vector for each
scenario.
npossible
 Let the kth pos	  	
	 o
sible count vector to be yk ¼ y1 k ; y2 k ; …; yj ; …; y2n þ 1 k ,
k
 
where yj is the number of occurrences of the jth state vector, xj , in
k

the kth scenario that satisﬁes the given evidence. Then, the likelihood
of observing the evidence, E, should be the sum of the probability of all
possible count vectors that these evidences imply. That is,
X 
	
Pr yk j p
Pr ðEj pÞ ¼
8 yk

2
8
"
X62n þ 1 < 1
¼ N! 4 ∏  
: y !
8 yk j ¼ 1
j

n  	
	½1  ðxi Þj 
ðx Þ 
∏ pi i j 1  pi

!

i¼1

k

93

ðx0 Þj 
½1  ðx0 Þj  
ðyj Þk =
7
1  pðx1 Þj …ðxn Þj
 pðx1 Þj …ðxn Þj
5
;
Therefore,
2
8
"
X62n þ 1 < 1
LðEpÞ p
4∏  
: y !
8 yk j ¼ 1
j

n  	
	½1  ðxi Þj 
ðx Þ 
∏ pi i j 1  pi

ð10Þ

!

i¼1

k

93

ðx0 Þj 
½1  ðx0 Þj  
ðyj Þk =
7
1  pðx1 Þj …ðxn Þj
 pðx1 Þj …ðxn Þj
5
;

ð11Þ

To illustrate the computation, we use the BN model in Fig. 1 as
an example. In this 2-component system, suppose there is one
sensor placed on the component 1 node and another sensor on the
system node (see Fig. 3). Over the observation period, a series of
5 failure events were detected at the system level by sensor 1 and
one failure event was detected at the component level by sensor 2.
However, no direct information of component 2 is available, as it is
not monitored by sensor.
Since there are two components (i.e. n ¼ 2), the number of
possible state vectors is 2n þ 1 ¼ 23 ¼ 8. The state vectors are listed
in Table 8, along with their probabilities.

The ﬁve observed system failure events are certainly related to
the events at the component level. For each system event, it
invokes one or more of the 8 possible state vectors. In this
example, as we observe 5 failures at the system and 1 failure at
component 1, the state vectors must be four f0; x2 ; 1g’s and one
f1; x2 ; 1g. As there are two possible states for the unobservable
node x2 , the four events of f0; x2 ; 1g are distributed among two
possible state vectors and there are 5 distinct arrangements.
Similarly, there are 2 arrangements for the single event of f1; x2 ; 1g.
Thus, the total number of possible count vectors is 5  2 ¼ 10.
For example, among the 5 observed system events, one possible scenario is that x2 ¼ f0; 0; 1g occurred 4 times and x6 ¼ f1; 0; 1g
occurred 1 time. Correspondingly, we have y2 ¼ 4 and y6 ¼ 1, and
other yj ’s are zeros. This is the ﬁrst row in Table 9. Another possible scenario is that x4 ¼ f0; 1; 1g occurred 4 times and x8 ¼
f1; 1; 1g occurred 1 time. This is the last row in Table 9. After
enumerating all possible scenarios, their corresponding count
vectors are listed in Table 9.
We used the likelihood function given by Eq. (11) along with
uniform prior distributions of p to generate the posterior distributions of the parameters of the BN model. MCMC was performed to draw samples from the unnormalized joint posterior
distribution. We used the Bayesian software package, WinBUGS, to
carry out the computation. One advantage of using WinBUGS
software is that it can be also called from the statistical software R
(R Core [28]) through a package called R2WinBUGS, making it
more convenient for the analysis of simulation results.
The following results are based on discarding the ﬁrst 20,000
draws from the MCMC sampling chain and then keeping every
other sample (to reduce the autocorrelation of drawn samples) until
there were 100,000 draws from the joint posterior distribution.
Based on the results in Table 10, we can conclude that the system behaves like a series system, because the system has high
probability of failure when at least one of the components has
failed. It is also found that there is a notable probability of system
failure even when both components are functioning, so there might
be some unknown factor that affects the working mechanism of the
system. As a result, we are able to assess the dependencies between
the system’s health and the states of its components even when
only a partial set of components are monitored.

4. Case study
In this section, we demonstrate our methodology on a hypothetical mechatronic system: an active vehicle suspension (AVS),
previously presented in Zhong et al. [33]. In the previous study, the
system reliability conﬁguration was deterministic, represented by
a fault tree. In our study, we remodel one of its subsystems by a
BN, assuming that this subsystem is redesigned and its reliability
Table 8
State vectors of system in Fig. 3.
State vector # j

Fig. 3. Basic two component BN system with sensors on the system and
component 1.

State vector xj

Node states
ðx 1 Þj

ðx2 Þj

ðx0 Þj

1

0

0

0

f0; 0; 0g

2

0

0

1

f0; 0; 1g

3

0

1

0

f0; 1; 0g

4

0

1

1

f0; 1; 1g

5

1

0

0

f1; 0; 0g

6

1

0

1

f1; 0; 1g

7

1

1

0

f1; 1; 0g

8

1

1

1

f1; 1; 1g

 	
Probability Pr xj p



	
	
	
1  p2 1  p00
	
	
1 p1 1  p2 p00

	 
	
1 p1 p2 1 p01

	
1 p1 p2 p01

	
	
p1 1  p2 1 p10

	
p1 1  p2 p10

	
p1 p2 1  p11
p1 p2 p11


1 p1

P. Yontay, R. Pan / Reliability Engineering and System Safety 152 (2016) 104–114

structure is more complex than the old generation. We start by
introducing the AVS system.
The AVS system supports the vehicle body and reduces body
vibration from the road surface. The system consists of tires,
springs, dampers (shock absorbers) and linkages that connect a
vehicle to its wheels and allows relative motion between the two.
Suspension systems contribute to the vehicle's road handling and
braking for good active safety and keep vehicle occupants isolated
from road noise and bumps. The suspension also protects the
vehicle from damage and wear. Fully active suspension systems
use electronic monitoring of vehicle conditions, in order to impact
vehicle suspension and behavior in real time to directly control the
motion of the car.
Fig. 4 shows the fault tree of a simpliﬁed version of the system.
The system has a parallel structure. The parallel system is
Table 9
Possible state vector combinations of system in Fig. 6.
Count vector # k

1
2
3
4
5
6
7
8
9
10

Count vector, yk
(no. of jth state vectors)
 	
y1 k

 	
y2 k

 	
y3 k



0
0
0
0
0
0
0
0
0
0

4
4
3
3
2
2
1
1
0
0

0
0
0
0
0
0
0
0
0
0

0
0
1
1
2
2
3
3
4
4

y4

	


k

y5

	


k

0
0
0
0
0
0
0
0
0
0

y6

1
0
1
0
1
0
1
0
1
0

	
k

 	
y7 k

 	
y8 k

0
0
0
0
0
0
0
0
0
0

0
1
0
1
0
1
0
1
0
1

111

composed of two subsystems: a passive subsystem and an actuator
subsystem. The passive subsystem works in a series structure with
the spring and damper (shock absorber) components, where the
shock absorbers damp out the motions of a vehicle up and down
on its springs. The actuator subsystem also works in a series
structure with mechanical and electronic parts. Active suspensions
use actuators to raise and lower the chassis independently at each
wheel. The mechanical parts include components like pump, piston and servovalve; whereas the electronic parts include power,
sensors and the controller. The suspension reacts to signals from
the electronic controller (which means the suspension is externally controlled). Sensors continually monitor body movement
and vehicle ride level, constantly supplying the computer with
new data.
Next, we model the reliability structure of AVS system by a
Bayesian network as in Fig. 5. Suppose that the parallel structure of
the system reliability and the series structure of the actuator
reliability are unchanged, but, due to a redesign, the reliability
structure of the passive device reliability becomes uncertain.
Therefore, we are interested in exploring the relationship between
node X 2 and its parent nodes, X 4 and X 5 , through conditional
probabilities.
Sensors are necessary components for any mechatronic system.
A sensor can be run on some components of the system independently to check the state (functional) of that component. The
output is generally a signal that is converted to human-readable
display at the sensor location or transmitted electronically over a
network for reading or further processing. In this scenario we
continually monitor the system with sensors on nodes X 1 , X 4 and
X 7 . We observe a series of 10 events where 10 failures were
detected at the system level (by sensor 1), 2 failures were detected
by sensor 2 and no failures were detected by sensor 3 (see Fig. 6).

Table 10
Empirical mean, standard deviation and quantiles for p.

p1
p2
p00
p01
p10
p11

Mean

SD

2.5%

25%

50%

75%

97.5%

0.148967691
0.169756133
0.183846816
0.799470654
0.772466599
0.876228065

0.062107504
0.062686748
0.055111599
0.138742057
0.142763382
0.072134488

0.02870975
0.02449975
0.0452995
0.524
0.5171
0.7562

0.1008
0.1295
0.1531
0.6922
0.6534
0.8143

0.1535
0.1849
0.1987
0.8218
0.7832
0.8767

0.2015
0.2217
0.2279
0.9197
0.8973
0.9389

0.2452
0.2475
0.2481
0.9926
0.9899
0.9938

Fig. 4. A fault tree example of an active vehicle suspension.

112

P. Yontay, R. Pan / Reliability Engineering and System Safety 152 (2016) 104–114

Table 11
State vectors of system in Fig. 6.
State vector x1 x2 x3 x4 x5 x6 x7 Probability
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32

Fig. 5. The corresponding BN model.

0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
1
1
1
1

0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

0
0
0
0
1
1
1
1
1
1
1
1
1
1
1
1
0
0
0
0
1
1
1
1
1
1
1
1
1
1
1
1

0
0
1
1
0
0
0
0
0
0
1
1
1
1
1
1
0
0
1
1
0
0
0
0
0
0
1
1
1
1
1
1

0
1
0
1
0
0
0
1
1
1
0
0
0
1
1
1
0
1
0
1
0
0
0
1
1
1
0
0
0
1
1
1

0
0
0
0
0
1
1
0
1
1
0
1
1
0
1
1
0
0
0
0
0
1
1
0
1
1
0
1
1
0
1
1

0
0
0
0
1
0
1
1
0
1
1
0
1
1
0
1
0
0
0
0
1
0
1
1
0
1
1
0
1
1
0
1

(1-p4)(1-p5)(1-p6)(1-p7)(1-p00)
(1-p4)p5(1-p6)(1-p7)(1-p01)
p4(1-p5)(1-p6)(1-p7)(1-p10)
p4p5(1-p6)(1-p7)(1-p11)
(1-p4)(1-p5)(1-p6)p7(1-p00)
(1-p4)(1-p5)p6(1-p7)(1-p00)
(1-p4)(1-p5)p6p7(1-p00)
(1-p4)p5(1-p6)p7(1-p01)
(1-p4)p5p6(1-p7)(1-p01)
(1-p4)p5p6p7(1-p01)
p4(1-p5)(1-p6)p7(1-p10)
p4(1-p5)p6(1-p7)(1-p10)
p4(1-p5)p6p7(1-p10)
p4p5(1-p6)p7(1-p11)
p4p5p6(1-p7)(1-p11)
p4p5p6p7(1-p11)
(1-p4)(1-p5)(1-p6)(1-p7)p00
(1-p4)p5(1-p6)(1-p7)p01
p4(1-p5)(1-p6)(1-p7)p10
p4p5(1-p6)(1-p7)p11
(1-p4)(1-p5)(1-p6)p7p00
(1-p4)(1-p5)p6(1-p7)p00
(1-p4)(1-p5)p6p7p00
(1-p4)p5(1-p6)p7p01
(1-p4)p5p6(1-p7)p01
(1-p4)p5p6p7p01
p4(1-p5)(1-p6)p7p10
p4(1-p5)p6(1-p7)p10
p4(1-p5)p6p7p10
p4p5(1-p6)p7p11
p4p5p6(1-p7)p11
p4p5p6p7p11

Table 12
Empirical mean, standard deviation and quantiles for p.

p4
p5
p6
p7
p00
p01
p10
p11

Fig. 6. The AVS model with sensors.

Since there are 7 nodes in the BN model, the number of possible state vectors would be 27 ¼ 128 if we did not observe any
evidence. As some parts of system reliability structure are deterministic, we can eliminate a great amount of state vectors
according to the evidence coming from the sensor.
The ﬁrst step is to construct the state vectors as explained in
Section 3.4. The states of X 6 and X 7 uniquely deﬁne the state of X 3 ,
and the states of X 2 and X 3 uniquely deﬁne the state of X 1 .
Therefore, we only need to consider the stochastic nodes,
X 2 ; X 4 ; X 5 ; X 6 ; X 7 , in the model inference. Thus, we have 25 ¼ 32
state vectors. The joint probability is represented as
P ðX Þ ¼ P ðX 4 ÞP ðX 5 ÞP ðX 6 ÞP ðX 7 ÞP ðX 2 X 4 ; X 5 Þ

ð12Þ

Therefore, the parameters that we would like to estimate in this


system are the failure probabilities p ¼ p4 ; p5 ; p6 ; p7 ; p11 ; p10 ; p01 ; p00
where pij ¼ P ðX 2 ¼ 1j X 4 ¼ i; X 5 ¼ jÞ. Note that p3 ¼ 1  ð1  p6 Þð1 p7 Þ
(series system) and p1 ¼ p2 p3 (parallel system). The state vectors are
listed in Table 11, along with their probabilities.
For the 10 observed events (i.e. N ¼ 10), there are many possible state vector combinations, as each event will invoke one of
the 32 possible state vectors. We elicited these vectors by a

Mean

SD

2.5%

25%

50%

75%

97.5%

0.184684
0.517456
0.917133
0.083205
0.16214
0.885807
0.774643
0.881724

0.067958
0.076611
0.07627
0.07647
0.06627
0.099819
0.143001
0.072124

0.05203
0.3156
0.7163
0.002211
0.01738
0.6257
0.517
0.7572

0.1333
0.4831
0.8821
0.0256
0.1156
0.8343
0.656175
0.8203

0.1883
0.5398
0.9393
0.06082
0.1773
0.914
0.787
0.8849

0.2409
0.5747
0.9745
0.118
0.2183
0.9638
0.8993
0.9448

0.2936
0.5978
0.9977
0.2859
0.2471
0.9968
0.9908
0.9945

MATLAB program (MATLAB User’s [9]) and counted the occurrence
of each state vector for the given evidence. In this example, we
obtained 27 possible counts vectors, yk , that imply the evidence;
that is, there are 27 possible arrangements of state vectors that
match the evidence coming from the sensors.
The likelihood function is constructed by summing up individual likelihoods deﬁned by the multinomial distribution for each
count vector, yk , as was formulated in Eq. (11). The posterior distributions for these failure probabilities are obtained by using
uniform priors in WinBUGS.
The evidence set claims that, out of 10 system failures, sensor
2 only detected 2 failures, and sensor 3 did not detect any failures;
therefore, the probability of failure for node 4 and node 7 (p4 and
p7 ) should be very small. This is conﬁrmed by the MCMC output.
Since we do not have any information about node 5, p5 is around
0.5. The posterior failure probability of node 6 is very large
because it is needed to compensate the low failure probability of
node 7, for the series structure of their subsystem. More importantly, with the evidence set we are able to infer the reliability
structure of the passive device subsystem (including nodes 2,

P. Yontay, R. Pan / Reliability Engineering and System Safety 152 (2016) 104–114

113

Fig. 7. Prior (dashed lines) and posterior (solid lines) distributions of conditional probabilities.

4 and 5). The conditional probabilities listed in Table 12 shows that
this subsystem has a high probability of failure when at least one
of its components has failed. So, we can conclude that the reliability structure of the passive device subsystem is close to a series
system.
The plots of prior and posterior distributions of these conditional probabilities are shown in Fig. 7. From these plots we can
see that, after combining evidence from sensors, the uniform prior
evolves to a more narrowly distributed posterior. As a result, our
method proves to be an effective way to assess dependencies in
system reliability, even in the case of only a partial set of components being monitored.
4.1. Computational complexity
As one can see from this case study, the computation complexity of our algorithm is not trivial. The evaluation of the likelihood function presented in this paper relies on identifying
combinations of state vectors that are implied by the evidence. The
speed of evaluation is largely dependent on the generation of
possible state vectors for the system and then identifying all
combinations of those state vectors. Once the state vector combinations are developed, the likelihood function can easily be
calculated.
The generation of the set of combinations of state vectors is the
most computationally intensive part for developing the likelihood
function. We have developed an algorithm to rapidly identify
these combinations. The ﬁrst part of the algorithm, compiled in
MATLAB, constructs all combinations of the count vectors for a
given number of tests. The complexity of this part of the algorithm
is Oðmn Þ, where m is the number of tests and n is the number of
state vector combinations. Therefore, the number of count vectors
increases exponentially with number of state vectors. As a result,
the complexity of the algorithm is polynomial in the number of
tests, but exponential in the number of state vector combinations.
We admit that this might be problematic for very complex systems
with hundreds of components.
We, however, suggest an alternative solution for combining
state vectors. The matrix of count vectors is actually very sparse
due to the fact that we do not observe all of the combinations.
Therefore, it is not necessary to calculate all combinations. Note
that the number of combinations of state vectors is related to
number of ways distributing n identical objects among r groups

and this can be done in C ðn þ r  1; r  1Þ ways, where n is the
number of counts of a speciﬁc vector combination observed and r
is the number of possible combinations for unknown nodes. We
have devised a formula that will rapidly give us the number of
count vectors that satisfy the evidence and hence will provide
rapid analysis of the likelihood function for subsequent Bayesian
analysis.
For example, in the case study, according to the given evidence, we
can specify what vectors are possible to be observed, so we do not
need to combine all of the state vector combinations in our algorithm.
Following the evidence, we infer that the state vectors must be eight
f1; 1; 1; 0; x5 ; 1; 0g’s (n1 ¼ 8) and two f1; 1; 1; 1; x5 ; 1; 0g’s (n2 ¼ 2). As
there are two possible states for the unobservable node x5 ,
r 1 ¼ r 2 ¼ 2. Therefore, total number of count vectors satisfying the
evidence can be directly calculated as C ð9; 1Þ  C ð3; 1Þ ¼ 27. As a
result, we can generate count vectors without going through all the
possible combinations of state vectors.
Since probabilistic inference using BN is NP-hard [4], we suggest designing efﬁcient special-case algorithms, rather than using
general probabilistic inference algorithms, for a speciﬁc problem.
Stochastic simulation algorithms such as MCMC are very efﬁcient,
and they can be tuned to improve run times, especially in the
incomplete data case.

5. Conclusion
In this paper we generalize the system reliability structure of a
complex system to a Bayesian network model. We are interested
in exploring the relationship of system/subsystem reliability to its
components. This research is particularly meaningful to a new
system design where the system reliability conﬁguration is
uncertain. Using the Bayesian inference approach, we are able to
combine information from multiple sources and multiple levels of
the system to infer the conditional probabilities in BN.
Three data scenarios are discussed in this paper. In a naïve
scenario where the complete historical dataset of the states of the
system and its components are available, we develop the conjugate Bayesian method for estimating the parameters in a binary
state BN, and then extend it to a multi-state BN. When only failure
records are available, we propose a method for quantifying the
marginal distribution of system failure. Finally, we discuss the
scenario of incomplete lower-level system information.

114

P. Yontay, R. Pan / Reliability Engineering and System Safety 152 (2016) 104–114

Data drawn simultaneously from the same system are fundamentally different from independent datasets. The dependencies
between higher-level failure data and lower-level failure data are
characterized by the conditional probabilities in a BN model. In the
case of having incomplete lower-level data, the likelihood function
of evidence becomes a summation of several likelihoods that correspond to all possible state vectors of the system. For such complicated function, it is impossible to ﬁnd a closed form solution of
posterior probability; therefore, we employed the computational
Bayesian method, MCMC. The resulting method is successful at
quantifying system reliability structure with incomplete data.
In this paper, we studied simultaneous data analysis of binarystate systems. This research could be extended to Bayesian networks modeled by multi-state systems and continuous life metric
systems in the future. Our proposed Bayesian network model can
also be coupled with Hierarchical Bayesian (HB) inference to
enable model parameter estimation without explicitly specifying
its prior distribution. In this research, we developed a MATLAB
program to perform compilation of the set of combinations of state
vectors to be used in the MCMC simulation in WinBUGS. However,
a future research direction could be to develop more efﬁcient
algorithms that can handle multi-state systems and/or continuous
state systems.
Furthermore, in the Bayesian inference of multi-level systems,
one may encounter the problem of the prior distribution of system
reliability coming from two different channels. One is from the
direct estimation on the system, such as expert opinions on the
system reliability, and the other one is derived from component
priors, because system reliability is a function of component
reliability. Consequently, we need to combine the prior information from different channels. Guo [10] used the Bayesian melding
method originally proposed by Poole and Raftery [23]. In our
future research, we plan to incorporate Bayesian melding and
other prior speciﬁcation methods of system reliability into BN
models. Assessing the posterior distribution of conditional probabilities is critical to the understanding of both the functional and
physical structure of a system. More research is needed on the
techniques and tools for carrying out this activity. In our current
study, we used WinBUGS, a tool for applying MCMC simulation in
Bayesian inference. However, to reduce computational burden,
other computational Bayesian methods should be investigated in
future research.

References
[1] Bobbio Andrea, Portinale Luigi, Minichino Michele, Ciancamerla Ester.
Improving the analysis of dependable systems by mapping fault trees into
Bayesian networks. Reliab Eng Syst Saf 2001;71(3):249–60.
[2] Boudali Hichem, Dugan J Bechta. A continuous-time Bayesian network reliability modeling, and analysis framework. Reliab, IEEE Trans 2006;55(1):86–97.
[4] Cooper Gregory F. The computational complexity of probabilistic inference
using Bayesian belief networks. Artif Intell 1990;42(2):393–405.
[5] Gelman Andrew, Carlin John B, Stern Hal S, Rubin Donald B. Bayesian Data
Analysis, Vol. 2. London, UK: Taylor & Francis; 2014.

[6] Garthwaite Paul H, O'Hagan Anthony. Quantifying expert opinion in the UK
water industry: an experimental study. J R Stat Soc: Ser D ( Stat) 2000;49
(4):455–77.
[7] Graves Todd L, Hamada Michael S, Klamann R, Koehler A, Martz Harry F. A
fully bayesian approach for combining multi-level information in multi-state
fault tree quantiﬁcation. Reliab Eng Syst Saf 2007;92(10):1476–83.
[8] Graves Todd L, Hamada Michael S, Klamann RM, Koehler AC, Martz Harry F.
Using simultaneous higher-level and partial lower-level data in reliability
assessments. Reliab Eng Syst Saf 2008;93(8):1273–9.
[9] Guide MATLAB User’s. The Mathworks, Vol. 5. MA: Inc., Natick; 1998.
[10] Jiqiang Guo, Bayesian methods for system reliability and community detection, 2011.
[11] Hamada M, Martz Harry F, Shane Reese C, Graves T, Johnson Val, Wilson
Alyson G. A fully Bayesian approach for combining multilevel failure information in fault tree quantiﬁcation and optimal follow-on resource allocation.
Reliab Eng Syst Saf 2004;86(3):297–305.
[13] Christopher Stephen Jackson, Bayesian inference with overlapping data:
Methodology and application to system reliability estimation and sensor
placement optimization, 2011.
[14] Johnson Valen E, Graves Todd L, Hamada MICHAELS, Shane Reese C. A hierarchical model for estimating the reliability of complex systems. Bayesian Stat
2003;7:199–213.
[16] Koller Daphne, Friedman Nir. Probabilistic Graphical Models. Cambridge, MA,
USA: Principles and Techniques MIT press; 2009.
[17] Langseth Helge, Portinale Luigi. Bayesian networks in reliability. Reliab Eng
Syst Saf 2007;92(1):92–108.
[18] Li Mingyang, Liu Jian, Li Jing, Kim Byoung Uk. Bayesian modeling of multi-state
hierarchical systems with multi-level information aggregation. Reliab Eng Syst
Saf 2014;124:158–64.
[19] Mahadevan Sankaran, Zhang Ruoxue, Smith Natasha. Bayesian networks for
system reliability reassessment. Struct Saf 2001;23(3):231–51.
[20] Martz HF, Wailer RA. Bayesian reliability analysis of complex series/parallel
systems of binomial subsystems and components. Technometrics 1990;32
(4):407–16.
[21] Martz HF, Wailer RA, Fickas ET. Bayesian reliability analysis of series systems
of binomial subsystems and components. Technometrics 1988;30(2):143–54.
[22] Pan Rong, Steven E Rigdon. Bayes inference for general repairable systems. J
Qual Technol 2009;41(1).
[23] Poole David, Raftery Adrian E. Inference for deterministic simulation models:
the Bayesian melding approach. J Am Stat Assoc 2000;95(452):1244–55.
[24] Reese C Shane, Michael Hamada, David Robinson. Assessing system reliability
by combining multilevel data from different test modalities. Qual Technol
Quant Manag 2005;2:177–88.
[25] Mejia Sanchez Luis, Pan Rong. An enhanced parenting process: predicting
reliability in product's design phase. Qual Eng 2011;23(4):378–87.
[26] Shane Reese C, Wilson Alyson G, JIQIANG GUO, Hamada Michael S, Johnson
Valen E. A Bayesian model for integrating multiple sources of lifetime information in system-reliability assessments. J Qual Technol 2011;43(2):127–41.
[27] Spiegelhalter D, Thomas A, Best N, Lunn D. WinBUGS User Manual.MRC
Biostatistics Unit. Cambridge, UK.: Cambridge; 2005.
[28] Team R R Core. A Language and Environment for Statistical Computing.R
Foundation for Statistical Computing. Austria: Vienna; 2012. p. 2012.
[29] Wilson Alyson G, Christine M Anderson-Cook, Aparna V Huzurbazar. A case
study for quantifying system reliability and uncertainty. Reliab Eng Syst Saf
2011;96(9):1076–84.
[30] Wilson Alyson G, Graves Todd L, Hamada Michael S, Shane Reese C. Advances
in data combination, analysis and collection for system reliability assessment.
Stat Sci 2006:514–31.
[32] Yontay Petek, Sanchez Luis Mejia, Pan Rong. Bayesian network for reliability
prediction in functional design stage. IEEE 2015.
[33] Zhong Xiaopin, Ichchou Mohamed, Saidi Alexandre. Reliability assessment of
complex mechatronic systems using a modiﬁed nonparametric belief propagation algorithm. Reliab Eng Syst Saf 2010;95(11):1174–85.
[34] Wilson A, Huzurbazar A. Bayesian networks for multilevel system reliability.
Reliab. Eng Syst Saf 2007;92(10):1413–20.
[35] Mejia L, Pan R. Obtaining reliability insights of a new product in its conceptual
design stage, Research in Engineering Design, submitted.

2010 18th IEEE Symposium on High Performance Interconnects

AF-QCN: Approximate Fairness with Quantized Congestion Notification
for Multi-tenanted Data Centers
Abdul Kabbani∗ , Mohammad Alizadeh∗ , Masato Yasuda † , Rong Pan‡ , and Balaji Prabhakar∗
∗ Department of Electrical Engineering, Stanford University
† System IP Core Research Laboratories, NEC Coporation, Japan
‡ Cisco Systems, San Jose, California

{akabbani, alizade, balaji}@stanford.edu, m-yasuda@ct.jp.nec.com, ropan@cisco.com

max-min fairness satisfies all these requirements: different
tenants are assigned weights or priorities, and each tenant’s
share of the network bandwidth is determined by it’s weight or
priority. Any unused bandwidth is recursively divided among
the remaining tenants (again in weighted max-min fashion).
In this paper, we design and evaluate Approximately Fair
QCN (AF-QCN). AF-QCN is a simple lightweight enhancement to the Layer 2 end-to-end congestion management algorithm, QCN, recently adopted for the IEEE 802.1 Data Center
Bridging standard [12]. AF-QCN borrows ideas from the
Approximate Fair Dropping algorithm of Pan et. al. [1], and
extends the functionality of QCN, by adding an Approximate
Fairness (AF) controller to the QCN switch.1 As will be
demonstrated via simulations (Section IV-B), and a hardware
implementation (Section IV-C), this merger retains all the good
properties of QCN, while enabling (approximate) weighted
max-min fairness at the level of a few milliseconds (see
below).
It is worth mentioning an interesting scheme, called E2CM
[15], which was proposed at the IEEE 802.1Qau standards
discussion. E2CM achieves fair bandwidth partitioning simultaneously with congestion management via path probing and
destination-based per-flow fair share rate calculation. Because
it combines fairness and congestion management, E2CM must
be applied to all flows, regardless of whether they are large
enough to cause congestion. Moreover, weighted bandwidth
partitioning, if possible, seems difficult to achieve. By contrast,
AF-QCN is a switch-side function that runs on top of QCN to
provide weighted fairness to subset of flows or classes. Further,
the flows that are monitored for providing bandwidth slices can
be the few long-lived, elephant flows.
Congestion Management in Ethernet. The QCN (Quantized
Congestion Notification) algorithm [13], [16], has been developed for the IEEE 802.1Qau standard [12], to provide endto-end congestion management in switched Ethernet. In brief,
QCN enables a switch to control the packet sending rate of
an Ethernet source whose packets are traversing the switch.
In designing QCN, the major goals were:
(i) Stability: Queue occupancies should not oscillate widely, to
avoid underflows causing link underutilization, and overflows

Abstract—Data Center Networks represent the convergence of
computing and networking, of data and storage networks, and
of packet transport mechanisms in Layers 2 and 3. Congestion
control algorithms are a key component of data transport in this
type of network. Recently, a Layer 2 congestion management
algorithm, called QCN (Quantized Congestion Notification), has
been adopted for the IEEE 802.1 Data Center Bridging standard:
IEEE 802.1Qau. The QCN algorithm has been designed to be
stable, responsive, and simple to implement. However, it does not
provide weighted fairness, where the weights can be set by the
operator on a per-flow or per-class basis. Such a feature can be
very useful in multi-tenanted Cloud Computing and Data Center
environments.
This paper addresses this issue. Specifically, we develop an
algorithm, called AF-QCN (for Approximately Fair QCN), which
ensures a faster convergence to fairness than QCN, maintains this
fairness at fine-grained time scales, and provides programmable
weighted fair bandwidth shares to flows/flow-classes. It combines
the QCN algorithm developed by some of the authors of this
paper, and the AFD algorithm previously developed by Pan et. al.
AF-QCN requires no modifications to a QCN source (Reaction
Point) and introduces a very light-weight addition to a QCNcapable switch (Congestion Point). The results obtained through
simulations and an FPGA implementation on a 1Gbps platform
show that AF-QCN retains the good congestion management
performance of QCN while achieving rapid and programmable
(approximate) weighted fairness.

I. I NTRODUCTION
Multi-tenanted data centers host a large number of distinct
tenants on a shared infrastructure. These data centers need to
share computing, storage, and networking resources among
the tenants in a fair and programmable manner. In recent
years, advances in virtualization technologies have made great
strides towards solving the problems of sharing computing and
storage resources [8], [9]. However, progress in virtualizing the
network has been limited.
Network virtualization entails providing programmable
bandwidth guaranties to the different tenants, and ensuring
performance isolation among the tenants. The performance
of one tenant’s traffic must not suffer due to the traffic of
another; specifically, an aggressive, perhaps malicious, tenant
must not be allowed to deny other tenants their fair share
of the bandwidth resources. Also, ideally, isolation should
not be achieved at the cost of poor network utilization, as
can occur with static bandwidth reservations. It is, therefore,
desirable to perform bandwidth allocation only at the onset of
congestion when utilization is high. The notion of weighted

978-0-7695-4208-9/10 $26.00 © 2010 IEEE
DOI 10.1109/HOTI.2010.26

1 This is similar to the XCP protocol, which decouples efficiency control
(control of utilization or congestion) from fairness control [3]. In our case,
efficiency control is achieved by the QCN control loop, and fairness control
is provided by the AF algorithm (see Section III).

58

leading to packet drops.
(ii) Responsiveness: QCN should rapidly adapt source rates to
extreme link bandwidth variations.
(iii) Simplicity: The algorithm should be very simple, as it is
typically implemented in hardware.
Besides these primary goals, inter-flow fairness was also
considered. Specifically, QCN flows must not suffer from
systematic unfairness, which means that two flows starting
with different sending rates, should have the same average
rate2 . However, the fairness of QCN is on a coarse time-scale
(see Section IV-B and [17]), and, by design, it cannot provide
programmable weighted fairness.
Approximate Fairness. One canonical method for providing
fairness in a network has been to deploy schemes like Deficit
Round Robin (DRR) or Weighted Fair Queueing to obtain
packet-level fairness [4], [5]. However, such schemes require
intricate packet scheduling algorithms, per-flow queues, and
come at a high hardware implementation cost. In particular,
requiring separate queues imposes hard limits on how many
different classes can be supported, and is a major drawback of
these approaches in multi-tenanted data centers. For example,
major cloud providers today host tens of thousands of tenants
[6], [7], while most switches only support 8 or 16 class of
service queues.
In [1], Pan et. al. showed that by relaxing the packetby-packet fairness requirement, and aiming for approximate
fairness on longer time scales (on the order of several roundtrip times), a much simpler algorithm called Approximate Fair
Dropping (AFD) can be used. AFD is an active queue management scheme, which probabilistically drops packets based
on queue size measurements (congestion information) and the
estimated rate of individual flows (fairness information). The
packet drops are used by responsive flow sources, such as
TCP senders, as a congestion indication, prompting them to
adjust their rates accordingly. The simplicity of AFD, and it’s
“soft per-flow state” requirement (only the rates of the flows
needs to be monitored, as opposed to needing separate queues),
allow it to be much more flexible than fine-grained packet
scheduling algorithms, such as DRR. In fact, AFD can overlay
on top of DRR to provide a behavior consistent with a DRR
system that has a larger number of queues. The effectiveness of
AFD in providing approximate fairness, and its low complexity
design, make it amenable to high speed implementations, and
have spurred its wide deployment in several switch and router
platforms in industry [2].

Fig. 1. QCN (above) vs AF-QCN (below) – Basic example: Unlike QCN,
AF-QCN distinguishes between the 1Gbps and 9Gbps flows, and sends them
different values in the Congestion Notification Messages.

explicit messages to sources, asking them to cut their rates by
a factor proportional to the value (denoted Fb ) in the feedback
message. In AF-QCN, we take advantage of this explicit
feedback facility; rather than differentially dropping packets to
obtain fairness as in AFD, AF-QCN adjusts the value of the
feedback messages based on the fairness information provided
by the AF component (see below and Fig. 1 for an example).
Hence, on the one hand, the source rates are rapidly controlled
by explicit feedback, and, on the other hand, intentional and
undesirable packet drops at the switch are avoided.
Fig. 1 illustrates a basic example showing how AF-QCN
differs from QCN. Two flows with different starting rates
(1Gbps and 9Gbps) share the same bottleneck. Because QCN
only takes the present and past queue size samples into consideration, it sends the same feedback value to the two flows.
AF-QCN, however, distinguishes between the two flows based
on their (estimated) sending rates, and adjusts the feedback to
each flow accordingly. In this example, the amount of data that
the switch receives (within a sampling interval) from flow 2,
is 9 times that of flow 1. Therefore, AF-QCN sends a larger
feedback value to flow 2 (who is exceeding its 50% fair share),
and a smaller feedback value to flow 1 (who is below its fair
share). Of course, the same mechanism can be used to provide
programmable bandwidth allocation, as will be described in
detail in Section III.
Rest of the paper. The paper is structured as follows: in
Section II, we briefly review the QCN and AFD algorithms;
in Section III, we describe the AF-QCN algorithm in detail;
the simulation and hardware results are provided in Section
IV; and we conclude in Section V.

Preliminary Description of AF-QCN. AF-QCN inherits several features from the AFD design with two major differences:
(i) In AF-QCN, congestion management is already wellhandled by QCN, so the AF component is designed only to
enforce fairness, leaving attaining stability to QCN.
(ii) The rate control provided by QCN is based on switchto-source Congestion Notification Messages; switches send

II. BACKGROUND
A. The QCN Algorithm
We now briefly describe the QCN algorithm, focusing on
those aspects which are relevant to this paper (see [13] and
[12] for more details). The algorithm is composed of two parts:
(i) Switch or Congestion Point (CP) dynamics: the mechanism
by which a switch buffer attached to an oversubscribed link

2 QCN has this property because the sampling of packets at a QCN switch
(for congestion notification) is naturally biased to sampling high rate flows,
and reducing their rates more frequently (see Section II-A).

59

Fig. 2.

Congestion detection in QCN CP buffer

samples incoming packets and generates a feedback message
addressed to the source of the sampled packet. The feedback
message contains information about the extent of congestion
at the CP.
(ii) Rate limiter or Reaction Point (RP) dynamics: the mechanism by which a rate limiter (RL) associated with a source
decreases its sending rate based on feedback received from the
CP, and increases its rate voluntarily to recover lost bandwidth
and probe for extra available bandwidth.

Fig. 3.

As is evident in the above description, the RP performs rate
increases in cycles. The durations of the cycles are determined
based on the combination of a Byte Counter, which counts
cycles in terms of the number of bytes transmitted by the RP,
and a Timer, which counts cycles in terms of the amount of
time elapsed.4 The details of how the Byte Counter and Timer
cycles determine the RP rate increase cycles can be found in
[13].
Remark 1. QCN also has a Hyper-Active Increase phase to
quickly seize unused bandwidth, when it suddenly becomes
available [13].

Congestion Point
The CP buffer is shown in Fig. 2. The goal of the CP is
to maintain the buffer occupancy at a desired operating point,
Qeq . The CP samples incoming packets with an inter-sampling
period depending on the severity of congestion3 . With each
sample, a congestion measure Fb is computed as follows: Let
Q denote the instantaneous queue-size and Qold denote the
queue-size when the last packet was sampled. Let Qof f =
Q−Qeq and Qδ = Q−Qold . Then Fb is given by the formula:
Fb = Qof f + wQδ ,

QCN RP operation

B. The AFD Algorithm
AFD is an active queue management scheme that aims to
control bandwidth allocation among flows/classes5 that share
a common queuing system. Controlled bandwidth allocation is
achieved by probabilistically dropping the packets of classes
for which the sending rate ri is more than the fair share
rf air.6 As such, a key aspect of AFD is that unlike other
active queue management systems (e.g. RED) where the
decision to drop a packet is based solely on the queue depth,
in AFD, it also depends on the sending rate of the packet’s
class.
Let Di = (1 − rf air/ri )+ denote the probability with
which a packet from class i should be dropped. Thus, if
ri < rf air no drop will occur. If ri > rf air, the drop
probability increases as ri gets further away from rf air. As
a result, the throughput of each flow, ri (1 − Di ), is bounded
by its fair share: ri (1 − Di ) = min(ri , rf air). Hence, drops
do not occur evenly across flows but are applied differentially
to flows with different rates.
Two key algorithmic aspects of AFD lie in the manner in
which ri and rf air are estimated via measurements. There
are three elements in this procedure, as shown in Fig. 4.

(1)

where w is a positive constant (set to 2 by default).
The interpretation is that Fb captures a combination of
queue-size excess (Qof f ) and rate excess (Qδ ). Thus, when
Fb > 0, either the buffer or the link or both are oversubscribed.
A feedback message containing Fb , quantized to 6 bits, is sent
to the source of the sampled packet only when Fb > 0; nothing
is signaled when Fb ≤ 0.
Reaction Point
The basic RP behavior is shown in Fig. 3. The RP algorithm
has the following phases:
(i) Rate decrease: This occurs only when a feedback message
is received. The RP decreases its sending rate (denoted CR
for Current Rate) multiplicatively in proportion to the value
of Fb received; the larger the Fb , the more CR is decreased.
(ii) Fast Recovery (FR): Immediately following a rate decrease
episode, the RP enters the FR phase. In FR, the RP tries to
reclaim the bandwidth lost in the decrease episode. It does this
by successively increasing the Current Rate toward the Target
Rate (TR)—the rate just before the decrease episode.
(iii) Active Increase (AI): After 5 increase cycles in FR have
completed, the RP enters the AI phase where it probes for
extra bandwidth on the path. In this phase, the RP increases
its sending rate by adding a constant RAI (5Mbps by default)
to CR in each cycle.

Arrival Rate Estimation. Let Mi be the amount (measured
in bytes, packets, etc) of traffic from flow i during the interval
Ts .
4 In the baseline implementation, each Byte Counter cycle is 150KBytes,
and each Timer cycle is 15msec.
5 In the rest of the paper, the terminology of flow and class will be used
interchangeably; AFD (and AF-QCN) can be thought to provide fairness
between flows or classes depending on the setting.
6 For simplicity, in this section, we only consider the most basic version
were all classes have an equal fair share. We will consider the more general
case in the discussion of the AF-QCN scheme.

3 For example, the inter-sampling period is 150KB at low congestion, and
can be as small as 18.5KB at high congestion.

60

by the AF controller. As is the case with QCN, Fb is computed
each time a packet is sampled, and is only sent if positive.
Remark 2. It is crucial to note that Fb−QCN is not a function
of the flow: it only depends on congestion. However, Fb−AF
does depend on the flow and is different for different flows.
Calculating Fb−AF involves the same three elements which
exist in AFD and which we now detail.
Arrival Rate Estimation. This is nearly identical to AFD. We
estimate the amount of traffic received during a time interval
of Ts from each flow or flow class. The estimate for class i is
denoted by Mi , and is updated every Ts seconds as follows:
Mi ← (1 − β)Mi + βMi−new ,

Fig. 4.

where β ∈ (0, 1), and Mi−new is the actual amount of traffic
from class i during the last Ts interval.

Basic AFD Algorithm

Fair Share Calculation. In AFD, M f air was dynamically
estimated using (2). As mentioned in Section II-B, this was
appropriate because as an active queue management scheme,
AFD attempts to control the queue size around Qref , as well
as perform bandwidth allocation. But AF-QCN is built on top
of QCN, which already takes care of queue size management.
Therefore, AF-QCN takes a more direct approach.
For each class i, let Wi denote its associated weight for
bandwidth allocation. Then the fair share of class i is estimated
as:
Wi X
Mj .
(6)
M f airi = P
j Wj j

Fair Share Calculation. Similarly, let M f air be the fair share
amount of the traffic that the queue would have received from
each flow, if the sources sent at their fair share rate. This
is estimated dynamically, at the end of each measurement
interval, as follows:
M f air ← M f air − a1 (Q − Qref ) + a2 (Qold − Qref ), (2)
where Q is the instantaneous queue length measured at the
end of the current measurement interval; Qold is the queue
length measured at the end of the previous interval; Qref is
the reference queue length (set by the operator); a1 and a2 are
the averaging parameters (chosen as part of the design).
The main idea behind (2) is to try to dynamically find a
value for M f air which along with the probabilistic dropping
of packets by AFD, leads to the queue stabilizing around Qref .
This allows AFD to perform bandwidth allocation and queue
management simultaneously. For more details, see [1].

Fb−AF Calculation. Given Mi and M f airi , the AFD drop
probability would have been Di = (1 − M f airi /Mi )+ .
However, rather than dropping the packets of source i with
probability Di , AF-QCN incorporates Di into the feedback
message which will explicitly adjust the rate of source i.
Fb−AF is computed by simply encoding the same Di value
as a 6 bit number:

Drop Probability Calculation. The drop probability Di for
packets of flow i is chosen as:
Di = (1 − M f airi /Mi )+

Fb−AF = b64Di c

(3)

(7)

Since Di ∈ [0, 1), note that Fb−AF ∈ {0, 1, . . . , 63}.
Fb−QCN is also a 6 bit number, and so Fb computed by (4)
is a 6 bit number which will be fed back if positive.

III. A PPROXIMATE FAIR QCN (AF-QCN)
A. Algorithm Description
The main goal of the AF-QCN algorithm is to enable an
approximate bandwidth allocation on top of QCN. To this end,
AF-QCN adds to the basic QCN Congestion Point, a fairness
controller (henceforth called the AF controller), which is based
on AFD. Recall that the basic QCN CP ensures that the link
is well-utilized and the queue is stable around Qeq . The AF
controller ensures a weighted fairness among flows or flow
classes without affecting the system stability.
More specifically, AF-QCN computes the feedback message
value Fb as the weighted sum:
Fb = (1 − α)Fb−QCN + αFb−AF ,

(5)

Remark 3. A more robust estimation of the fair share can be
obtained by only considering those flows/classes whose traffic
exceeds a certain threshold. This ensures that short, transient
flows do not skew the fair share for long flows. Specifically,
(6) is replaced by:
Wi X
M f airi = X
Mj ,
(8)
Wj j∈A
j∈A

where A = {j : Mj > active thresh} is the set of active
flows. Of course, (8) is only used for flows i ∈ A. If a packet
is sampled from a flow which is not active, Fb−AF is set to
zero.

(4)

where Fb−QCN is the original QCN congestion measure given
by (1), and Fb−AF is a weighted fairness measure computed

61

Remark 4. The situation is slightly more complicated when
a maximum allowed rate, M axi , can be prescribed for each
class. In this case, the fair share values are set according to the
max-min allocation, given the Wi ’s and M axi ’s of the active
flows.
B. Setting the Parameters
We now describe how the parameters α, Ts , β, and
active thresh are chosen.
The weight α ∈ (0, 1) trades off stability for weighted
fairness. Since stability is paramount, a small value of α ensures good stability by weighting Fb−QCN more than Fb−AF .
An exact characterization of this trade off requires a controltheoretic study, which is left for future work. For now, based
on extensive simulations, we choose α ≤ 0.25 for good
stability.
The interval over which rates are estimated, Ts , must
be large enough to allow robust rate estimates not affected
by burstiness in packet arrivals (typically several round-trip
times). However, it shouldn’t be so large that convergence
to fairness is overly delayed. Based on these considerations,
assuming data center round-trip times are less than 500µsec,
a Ts of 1 to a few milliseconds is recommended.
Finally, the parameters β and active thresh should be
chosen so as to allow small transient (mice) flows to pass
without affecting the estimation of arrival rates and fair share
values for long (elephant) flows. Therefore, they should be
chosen with regard to the sizes we expect for the mice.
Following these considerations, the AF-QCN parameters
α = 0.125, Ts = 1msec, β = 0.125, and active thresh =
20KB are chosen for all experiments in this paper.

Fig. 5.

Dumbbell (top) and parking lot (bottom) topologies

transmit at the full line rate, and a set of connections which
are rate-limited and send through a QCN Reaction Point. A
new connection begins at line rate, and is associated with
a Reaction Point once it receives a Congestion Notification
Message. It is de-associated once the rate of the Reaction Point
reaches its maximum.
We simulate the AF-QCN design in various settings and
under different network dynamics using ns2 [19]. We are
interested in examining the performance of AF-QCN in the
case where all the connections are backlogged (static flows), as
well as in the case where we have a mix of static and dynamic
flows. We compare the performance of AF-QCN with that of
QCN using the two topologies in Fig. 5. All the links between
the sources, sinks and switches are 10Gbps, and each switch
hop involves a 50usec round-trip time delay (RTT), unless
otherwise stated. The default QCN configuration [12] is used
with 150KB switch buffers. Data packets are 1KB, and Wi
and M axi are set to 1 and 10Gbps respectively for all flows,
except when indicated otherwise.
While our main interest is to study the bandwidth partitioning capabilities of AF-QCN, we also consider other important
performance metrics such as queue size fluctuations and flow
completion times.

C. Complexity
We discuss the “unoptimized complexity”7 of the main
operations that AF-QCN needs in addition to those of QCN.
The operations are: (i) incrementing one Mi−new counter per
arrival, (ii) updating the Mi value for all flows, and (iii)
computing the M f airi value per sampling interval, every Ts
seconds. Hence, two registers are required per flow where
the counter in (i) is accessed online via a hash table or a
similar data structure. Because the computations at steps (ii)
and (iii) need to be done once every Ts seconds, these can be
done periodically and offline. Note that AF-QCN is meant to
operate at the level of priority classes or long-lived/elephant
connections. This essentially means that the state information
required to be stored is small and the hardware complexity
required to update the counters is fairly low.

B. Simulation Results
1) Static Flows:
Baseline experiment
We initiate 4 static flows traversing the single bottleneck in
the dumbbell topology. The switch service rate is cut down
from 10Gbps to 1Gbps at 2secs of simulation time and is
increased to 10Gbps at 4secs. Fig. 6 shows the rates of the
individual flows and the switch queue size plots (sampled
every 10msecs) for QCN and AF-QCN. Notice how AF-QCN
is successfully ensuring that all the flows are allocated their
equal fair share rates at 10Gbps and 1Gbps, while maintaining
the original queue size stability of QCN.

IV. E VALUATION
A. Preliminaries
We envision AF-QCN operating with QCN-compliant NICs.
Each NIC potentially has a set of connections which can

Long RTT
To ensure that stability and fairness are maintained as lags
increase, we repeat the previous experiment with a 400usec
RTT instead, which is considered to be very long for data
center networks. The rates of the individual flows and the
switch queue sizes plots are shown in Fig. 7 for QCN and
AF-QCN.

7 There are several obvious optimizations which can be performed to
significantly reduce the complexity of these operations. Indeed, some of these
have been performed in AFD and implemented on hardware platforms. Due
to a lack of space, and the obvious nature of the optimizations, we do not
describe them here.

62

Fig. 8.

CDF of flow rates with QCN and AF-QCN

Fig. 6. Rates of the individual flows (left) and switch queue size (right) with
QCN (top) and AF-QCN (bottom) at 50usec RTT

Fig. 9.

Individual rates of 4 static flows with different weights

initiate each of the 6 static flows, one flow every second. Also,
at 7sec, we set M ax1 to 1Gbps to cap the rate of Flow 1.
Fig.10 shows the rates of the individual flows with QCN and
AF-QCN.
Like TCP, QCN has a tendency to penalize multi-hop flows
which have more congested links on their path. This is due
to the relatively high frequency of Congestion Notification
Messages such flows receive from multiple Congestion Points
compared to those flows which pass fewer Congestion Points.
In this topology, when the queues stabilize, most of congestion
messages have small Fb values.8 However, the cumulative
effect of these messages adversely affect multi-hop flows and
force their rates to be below their fair share.
AF-QCN ensures that this phenomenon does not occur. It
compensates for the disparity in the frequency of Congestion
Notification Messages for the many-hop and few-hop flows,
by sending the few-hop flows larger Fb values when they
exceed their fair share. Thus, AF-QCN guarantees the flows
with fewer hops don’t exceed their fair share.
In fact, not only does AF-QCN solve the unfairness caused
to multi-hop flows, but the rate allocation it provides is the
max-min fair allocation at each stage of the experiment. We
have observed the same behavior in many other experiments
not shown in this paper. An analysis of the nature of bandwidth

Fig. 7. Rates of the individual flows (left) and switch queue size (right) with
QCN (top) and AF-QCN (bottom) at 400usec RTT

Increased multiplexing
To see the effect of increased multiplexing, we initiate
40 static flows simultaneously and keep them running for 6
seconds. The CDF of the individual flow rates normalized by
250Mbps (the fair share) is plotted in Fig. 8 for QCN and
AF-QCN. The rates are measured every 10msec. Notice that
with AF-QCN almost 99% of the flows fall within 25% of the
fair share. With QCN, however, more than 45% of the flows
are off by more than 25% of the fair share and around 10%
are off by more than 50%.
Different weights and rate cap
We initiate 4 static flows as in the baseline experiment.
However, this time the flows are given different weights:
W1 = 4 , W2 = 3 , W3 = 2 , and W4 = 1. We also set M ax1
to 1Gbps at 2sec to verify that AF-QCN can still maintain
fairness among the three other flows at the ratio 3:2:1. Fig. 9
shows that the ratios of the rates of the individual flows are
in accordance with the ratios of their weights before and after
setting the 1Gbps cap on Flow 1.
Parking lot
Using the parking lot topology in Fig. 5, we incrementally

8 In

63

fact, most of the Fb values during this experiment where equal to 1.

Fig. 11. Static and bursty flows throughput with QCN (top) and AF-QCN
(bottom) at 1Gbps (left) and 6Gbps (right) offered load

Fig. 10. Flows rates in the parking lot topology with QCN (top) and AFQCN(bottom)
Fig. 12. Sending rates for long-lived flows in the presence of dynamic flow
arrivals with QCN (left) and AF-QCN (right).

sharing achieved by AF-QCN in a general network topology
is left for future work.
2) Mix of Static and Dynamic Flows:
Bursty connection
We initiate 3 static flows at time 0 and an on-off source
(Bursty Flow) at time 0.5sec. The Bursty Flow is on for
10KB bursts, and its off duration is chosen so that its average
offered load is 1Gbps in one experiment and 6Gbps in another
experiment. We plot the rates of the individual flows in Fig.
11 for QCN and AF-QCN.
The bursty nature of the on-off source does not adversely
affect the fairness among the 3 static flows. In the case where
the Bursty Flow’s offered load is 1Gbps, the other static flows
are successfully able to utilize the remaining bandwidth. In the
other case, where the Bursty Flow’s offered load of 6Gbps is
more than its fair share of 2.5Gbps, its throughput is limited
to the 2.5Gbps fair share.

flows is 1Gbps. We run the experiment until 100K flows are
generated, and we investigate the effect of AF-QCN on the
flow completion time of the dynamic flows.

Flow completion time In this experiment, there are 8 QCN
Reaction Points in total that share the single bottleneck in the
dumbbell topology. The first 4 RPs each serve an infinitely
back-logged static flow. The second set of 4 RPs each serve
4 permanent connections9 (16 connections in total), over
which (short) dynamic flows with Poisson arrival processes are
initiated. The size of the dynamic flows is Pareto distributed
with a mean of 10KB and a shape parameter of 1.1. The flow
arrival rate is such that the total offered load from the dynamic

TABLE I
F LOW C OMPLETION T IME (FCT) WITH QCN AND AF-QCN

9 These

As shown in Table I, the average FCTs for dynamic flows
of different sizes is 30–50% smaller with AF-QCN when
compared to QCN. This is because most of these flows are
small, and are therefore not penalized by the AF controller.
Hence, when they are sampled, the AF controller sends small
Fb values, enabling them to finish faster. Note that this is
correct behavior from the congestion point of view as well:
small flows contribute very little to congestion. Furthermore,
as Fig. 12 demonstrates even in the presence of dynamic flows,
AF-QCN is able to maintain fairness between the long-lived
flows.

represent the permanent TCP connections common in data centers.

64

Flow Size
Bin (KB)

FCT with QCN
(µs)

FCT with AF-QCN
(µs)

[1,10[

2.346

1.586

[10,100[

2.610

1.732

[100,1000[

5.037

2.932

[1000, ∞[

33.14

17.14

AF component interacts with the QCN control loop, and the
tradeoffs in setting the different parameters. Understanding
the equilibrium fairness properties of AF-QCN in arbitrary
networks also warrants further investigation.
ACKNOWLEDGMENT
Mohammad Alizadeh is supported by a Caroline and Fabian
Pease Stanford Graduate Fellowship. The authors would like
to thank the anonymous reviewers, whose comments helped
improve the paper.
R EFERENCES
[1] R. Pan, L. Breslau, B. Prabhakar, and S. Shenker, “Approximate Fairness
Through Differential Dropping”, Computer Communication Review,
January 2003.
[2] R. Pan, B. Prabhakar, F. Bonomi, and B. Olsen, “Approximate fair bandwidth allocation: A method for simple and flexible traffic management,”
Allerton, September 2008.
[3] Dina Katabi, “Decoupling Congestion Control and Bandwidth Allocation Policy With Application to High Bandwidth-Delay Product
Networks,” Ph.D. Dissertation, Massechussetts Institute of Technology,
March 2003.
[4] Shreedhar, M., and Varghese, G., “Efficient Fair Queueing using Deficit
Round Robin”, ACM Computer Communication Review, vol. 25, no. 4,
pp. 231242, October, 1995.
[5] Bennett, J. and Zhang, H., “Hierarchical Packet Fair Queueing Algorithms”, SIGCOMM Symposium on Communications Architectures and
Protocols, pp. 143-156, August, 1996.
[6] http://aws.amazon.com/ec2/
[7] http://www.microsoft.com/azure/default.mspx
[8] M. Armbrust, A. Fox, R. Griffith, A. Joseph, R. Katz, A. Konwinski, G.
Lee, D. Patterson, A. Rabkin, I. Stoica, and Others. “Above the clouds:
A Berkeley view of cloud computing.” Technical Report UCB/EECS2009-28, Berkeley, 2009.
[9] R. Buyya, C. S. Yeo, and S. Venugopal. “Market-oriented cloud computing: Vision, hype, and reality for delivering IT services as computing
utilities.” In Proceedings of the 10th IEEE International Conference on
High Performance Computing and Communications, 2008.
[10] http://www.t11.org/ftp/t11/pub/fc/bb-5/09-056v5.pdf
[11] http://www.ieee802.org/1/pages/dcbridges.html
[12] http://www.ieee802.org/1/pages/802.1au.html
[13] M. Alizadeh, B. Atikoglu, A. Kabbani, A. Lakshmikantha, R. Pan, B.
Prabhakar, and M. Seaman, “Data center transport mechanisms: Congestion control theory and IEEE standardization,” Allerton, September
2008.
[14] http://www.ieee802.org/1/files/public/docs2008/au-prabhakar-qcn-losgatos-0108.pdf
[15] http://www.ieee802.org/1/files/public/docs2007/au-sim-IBM-ZRLE2CM-proposal-r1.09b.ppt
[16] http://www.ieee802.org/1/files/public/docs2008/au-rong-qcn-serial-haipseudo-code%20rev2.0.pdf
[17] http://www.ieee802.org/1/files/public/docs2008/au-pan-qcn-benchmarksims-0108.pdf
[18] http://www.ieee802.org/1/files/public/docs2009/au-kabbani-yasuda0509-HW implementation evaluation.pdf
[19] Network Simulator. ns2. http://www.isi.edu/nsnam/ns.

Fig. 13. AF hardware implementation - flow rates with equal weights (top)
and different weights (bottom).

C. Hardware Implementation
We implement the AF design on top of our existing 1Gbps
NetFPGA QCN switch [18] using the same AF-QCN parameters as mentioned previously and used in the simulations. The
implementation is straightforward given the simplicity of the
design. The correctness of this implementation is verified over
the 50usec RTT dumb-bell topology in Fig. 5.
We run two experiments with 4 static flows sharing a QCN
switch at a 950Mbps service rate. All the flows have the same
weights in one experiment and weights in the ratio 1:2:3:4 in
another experiment. The results are plotted in Fig. 13.
V. C ONCLUSION
In this paper, we proposed and evaluated AF-QCN, an
algorithm that adds a programmable bandwidth partitioning
component based on AFD to the QCN Congestion Point
mechanism. No changes are needed at a QCN Reaction Point.
AF-QCN achieves weighted fairness at the granularity of
a few milliseconds. This enables Data Center operators to
provide programmable differential bandwidth allocation for
flows or flow classes, a feature very useful in multi-tenanted
Cloud Computing and Data Center environments. The results
obtained via simulations and a hardware implementation show
that AF-QCN retains the good properties of QCN (stability,
responsiveness, and simplicity), while achieving rapid and
programmable bandwidth partitioning.
One area for further work is a control theoretic study of
AF-QCN for precisely characterizing the manner in which the

65

761

J. Software Engineering & Applications, 2010, 3, 761-766
doi:10.4236/jsea.2010.38088 Published Online August 2010 (http://www.SciRP.org/journal/jsea)

The Design and Implement of TCP/IP Protocol
Cluster on AVR Singlechip
Rong Pan, Hai Zhao, Jialiang Wang, Dan Liu, Penghua Cai
School of Information Science and Engineering, Northeastern University, Shenyang, China.
Email: panrong1012@126.com
Received June 3rd 2010; revised June 30th 2010; accepted July 6th 2010.

ABSTRACT
With the rapid development of the embedded technology, research and implement of the Internet of things will be a new
technology revolution, yet the implement of the Internet of things is on the base of the communication between the
things. For this reason, realizing the function of communication between singlechip is particularly important. Based on
the characteristics of the embedded microcontroller, we analyzed the traditional PC TCP/IP protocol, and appropriately tailored TCP/IP protocol cluster on the basis of the characteristics of embedded singlechip. At last, we realized
the reduced TCP/IP protocol cluster suitable for embedded singlechip, on AVR singlechip platform.
Keywords: The Internet of Things, Webit, Embedded System, TCP/IP Protocol, Ethernet

1. Introduction
The Internet of things means a kind of net that via information sense equipment such as FRID, infrared sensor,
GPS, laser scanner and so on, in arranged protocol, join
up between anything and the Internet to communicate
information and realize intelligent identification, tracing,
monitoring, and management. The concept of the Internet
of things is suggested in the year of 1999. It is the “Internet communicated with things”. It means two aspects:
the first one is, the core and foundation of the Internet of
things is still in the Internet, which based on, extending
and expanding the Internet; the second one is, its client
side extends and expands to anything, to make the information exchanging and communication [1-3].
With the rapid development of computer and network
technology, Internet has become an important means of
information transmission, more and more embedded
equipments are necessary to achieve the Internet’s network [4-5]. Relative to the PC, computing and storage
resources of embedded systems are relatively limited;
therefore to achieve all the TCP/IP protocol cluster in the
embedded singlechip is quite unrealistic. So that, in order
to save the system resources and ensure the reliability of
the system, under the condition of improving the performance of embedded system, it’s necessary to targeted
modular simplify TCP/IP protocol.

2. Adoptive Equipment and Testing Platform
Webit is an overall solution that makes the equipments
Copyright © 2010 SciRes.

intelligent and networking. It is the new network equipment system structure with the elements of Internet and
its basic idea is an independent, low-cost 3 W server
embedded in equipment, to make the equipment has independent network intelligence.
Webit is an embedded Internet product decided by
Liaoning Provincial Key Laboratory of Embedded
Technology by themselves. Webit 1.0 is successfully
pass technical appraisal and the trademark registration in
the year of 2000, and Webit 2.0 (Internet non-standard
electrical equipment access server) passed the appraising
meeting of scientific and technological achievements
held by science and technology commission of Liaoning
province in May 2001. Considering that webit is AVR 8
bit singlechip, its storage unit is very limited, therefore
it’s very important to design a kind of TCP/IP protocol
cluster suitable for the products.
The performance of Webit 2.0 as follows:

Without depending on PC system structure;

Using Atmel AVR RISC processors;

User-defined Web pages;

User-defined CGI programs used to control;

14 bit I/O interface (TTL level);

TTL level UART supported 115200 bps;

10 M Ethernet interface (RJ-45);

System programming (ISP);
Overview of Ethernet controller chip RTL8019AS in
Webit:
RTL8019AS is a highly integrated Ethernet controller,
it can simply answer plug and play NE2000 compatible
JSEA

762

The Design and Implement of TCP/IP Protocol Cluster on AVR Singlechip

adapter, which has two-fold and power decrease characteristics. Through the three-level control characteristic,
RTL8019AS is the best ideal choice for network equipment GREEN PC in all already known things. The twofold function can simulate send and receive the spread
between twisted-pair and all two-fold Ethernet switches.
This not only can make bandwidth stronger from 10
Mbps to 20 Mbps, but also avoid read muliaccess
agreement because of Ethernet channel fight character.
Microsoft's plug and play function can alleviate user
lower income and focus on the adapter resources, such as
the input and output, IRQ, memory address, etc. However, in special application without plug and play function of compatibility, RTL8019AS support s JUMPER
and JUMPERLESS options.
In order to provide complete plug and play solution,
RTL8019AS integrated 10BASET transceiver, and autoexamination function between AUI and BNC interface.
In addition, 8 IRQ BUS and 16 basic addresses BUS
provide comfortable environment for large resources
situation.
RTL8019AS supports 16 k, 32 k and 64 k bytes memory BROM and the flash memory interface. It provides
the page model function, which can only support 4 M
bytes BROM under 16 k bytes of memory system space.
BROM’s useless commands are used to release BROM
memory space. RTL8019AS designed the singlechip with
the 16 k bytes SRAM, so that not only provides more
friendly function, but also saves SRAM storage resources.

Figure 1. Simplified TCP/IP protocol architecture

dule, and make encapsulation for IP first address, the first
(such as the fields like address, type of agreement, etc.)
by corresponding function calls. Then transfer the datagram including IP first and TCP first to the lower layer
by function call, until the data is sent smoothly. The
TCP/IP protocol processing is shown in Figure 2.

4. The Design and Implement of Simplified
Embedded TCP Protocol
First, in Webit, we format and size of the MAC and IP
address, system address configuration, and size of the
buffer were defined already. We make the address format
of MAC, IP to become fixed value in system. The system
configuration is used for setting specific value of the IP
address, the port and MAC address. In this system, for
the limited data needed the singlechip to process, so we
don’t set the buffer larger than normal.

3. The design of Webit Reduced TCP/IP
Protocol Stack
In the AVR singlechip, due to the relatively limited resources, the TCP/IP protocol cluster of complete function
cannot be achieved. So according to the characteristics of
AVR singlechip, we cutting the original TCP/IP protocol
cluster obtains the reduced TCP/IP protocol cluster.
Meanwhile, based on the architecture of TCP/IP protocol stack, we adapted the design method of network
slice model. The architecture of TCP/IP protocol cluster
after simplify contains the ARP, IP, ICMP, UDP, TCP
protocol processing model, etc. [6-8]. Each layer of the
architecture of TCP/IP protocol stack is designed as a
module of independent function, handles their data. Different modules can be invoked by function to turn over
datum to upper or lower processing module [9]. Figure 1
shows the simplified TCP/IP protocol architecture.
Known by Figure 1, when the AVR singlechip receives data from network, data packet processing modules will base on certain condition to choose the ARP
module of link layer or the IP protocol module of network layer to process.
Likewise, when the data packets are processed by
UDP and TCP protocol modules of transport layer, it will
turn over processed packets to IP protocol processing moCopyright © 2010 SciRes.

Figure 2. The TCP/IP protocol processing
JSEA

The Design and Implement of TCP/IP Protocol Cluster on AVR Singlechip

.DSEG
.ORG
0x60
LocalMAC: .BYTE 6
LocalIP:
.BYTE 4
LocalPort:
.BYTE 2
RemoteMAC: .BYTE 6
RemoteIP:
.BYTE 4
RemotePort: .BYTE 2
Plugdelaytime: .BYTE 32
TCPCB:
.byte 30*2
RevBuffer:
.BYTE 260

4.1 The Implement of ARP Protocol
Because the embedded singlechip is normally in the service of the passive state. So while we design and implement the ARP protocol, we don’t implement the function
of address mapping table, neither realize the function of
querying any client mapped IP into the MAC address,
only need to achieve when other client to query the local
Mac address. Packet and get feedback of the relationship
between own IP and MAC address, and send.
When the embedded singlechip receives ARP packets
from Ethernet, we according to the type of operation
codes of the packets decide type of ARP packet, if the
ARP request packet, compare destination IP address field
of ARPP packet with the local settings of IP address. If
it’s equal, local MAC address packaging to responded
ARP reply packet, if not, don't do processing, discard it.
The processing flow of ARP packet is shown in Figure
3.

4.2 The Implement of IP Protocol
The IP protocol is the core of the TCP/IP protocol cluster.
All the ICMP, UDP and TCP data transmit as IP datagram

Figure 3. The ARP protocol processing
Copyright © 2010 SciRes.

763

format. In the IP protocol processing modules, While
implement the IP protocol module, we firstly received
the IP date packet from the Ethernet and decide whether
the destination IP address field values in the head of datagram equals local IP address, if not, discard; if consistent, check such field as the version number and checksum of the IP datagram, etc.
After examination, confirm the packet is right, and
then decide to choose ICMP protocol, UDP protocol or
TCP to submit to upper processing, according to the type
of IP data. In addition, another function of IP protocol
module we designed and implemented is to make the
message encapsulation delivered from upper into IP data,
then turn over IP data encapsulation to link layer to make
data frame encapsulation and sending. The processing
flow of IP protocol is shown in Figure 4.

4.3 The Implement of ICMP Protocol
ICMP protocol is a kind of information transfer control
protocol. We think about the embedded singlechip as a
server is responded the client commonly, as a passive
device, it does not need to initiatively send back the
message. So we only implement the receiving and handling the Echo Request between singlechip and other
devices in the ICMP protocol module, and also send the
Echo Reply. The implementation of ICMP protocol is as
follows: read type code of the first byte of ICMP data
packets, and check the ICMP packet types. If the type
code is 8, the type of packets will be modified to 0, fill
each field of packets to make the encapsulation of Echo
Reply packets needed to be sent back, finally calls SendIP function, make ICMP data packets into IP datagram
encapsulation to send. If the packets’ type code isn’t 8,
discard the packet. The processing flow of ICMP protocol is shown in Figure 5.

Figure 4. The IP protocol processing
JSEA

The Design and Implement of TCP/IP Protocol Cluster on AVR Singlechip

764

Figure 5. The ICMP protocol processing

4.4 The Implement of UDP Protocol
UDP protocol provides reliable, connectionless communication between applications, it transmits datum to the
IP layer and sends out, but does not guarantee they can
reach the destination.
When the UDP protocol modules receive packets, first
locate the port fields of UDP packets, save the remote
and purpose port of the UDP packets, then the compare
the objective port of the packet with the port of local
regulations, if not equal, discard it; if equal, call the corresponding function. Finally, set the source port, objective port, data length, checksum field in the header of
UDP packet, add datum to be sent, make encapsulation
and sending by IP layer. The processing flow of UDP
protocol is shown in Figure 6.

Figure 6. The UDP protocol processing

truction and closing the connection of the TCP protocol
are through the “three handshakes” and “four times specific wave”. Setting the mark field in the TCP packets to
different control bits is the specific approach. The processing flow of TCP protocol is shown in Figure 7.

5. Testing
In order to test whether the TCP/IP protocol realized can
achieve the desired objective, we carried on a series of
tests.
The Ping command is the most frequently used in net-

4.5 The Implementation of TCP Protocol
Due to the limited resources of singlechip, and to handle
TCP packet better, so while implementing the TCP protocol module, we reduced the common TCP/IP protocol,
and did not implement the sliding window protocol, flow
control and congestion control mechanisms. Meanwhile,
we set two TCP connection control block in the TCP
protocol modules, and adopt the response mode with
single window. When receiving TCP packets, first locate
mark field of TCP packets, if the TCP packet is required
to build a new connection, check whether still exist spare
TCP connection control block in the system. If present,
this spare control block will be used as the control block
for this connection, and establish connections. Conversely, if there is no spare TCP connection control block,
and do nothing.
When the mark field of the TCP packets is another
type, search whether exist TCP connection control block
corresponding to the TCP packets. If present, judge according to the mark field of the value of SYN, FIN, ACK
and so on, then choose corresponding function to process
packets. If don't exist TCP connection control block corresponding to the TCP packets, don’t do anything. ConsCopyright © 2010 SciRes.

Figure 7. The TCP protocol processing

JSEA

The Design and Implement of TCP/IP Protocol Cluster on AVR Singlechip

work. This command sends a network message and requests response via ICMP protocol. Therefore, through
the Ping command we can determine whether the current
network is connected correctly, and test whether the condition of network connections is available. So, for the
testing of ARP, IP, and ICMP protocol, we can finish the
testing through Ping command. The process of test is:
first, connect Webit and PC, do the network configuration through WebitNetIfConfig function, make configuration of suitable IP address (in this test set IP of Webit
as 192.168.180.94). Finally, input “ping 192.168.180.94”
on the PC. The running test of Figure 8 shows that the
network is connected, network equipment is available. It
states:
1) ARP module is normal, and can properly achieve
the address mapping;
2) The IP protocol modules work normally, and can
correctly analyze that this is an ICMP messages;
3) ICMP protocol modules work normally, and can
correctly return responses message.
The response condition of ICMP packet is listed in the
Table 1. The test is mainly by sending ping date packet
to singlechip to verify the success rate of date sending.
As the test date of the table shows, as the request packet
sending to the singlechip is smaller, so the request can be
effectively handled by server.
For the testing of TCP protocol, we can choose to
write a simple Telnet-Server program based on TCP/IP
protocol. Users can access on PC, according to system
cue, users can input some simple commands to obtain
relevant information. Testing method is: input “telnet
192.168.180.94” on PC. The result is: the system shows
that the connection is established successfully, and the
associated tip. Input something according to cue, and
then obtain appropriate information. Finally, the test
shows that TCP protocol modules in the TCP/IP protocol
cluster are correct, and the network layer protocol is correct.
In addition, we also test the established time of TCP

Table 2. The established time of TCP
The established
time of TCP

Request
packets

Reply
packets

Success rate

128 Byte

50

50

100%

Copyright © 2010 SciRes.

Standard
deviation

Maximum

2.314 /ms

0.53 /ms

2.437 /ms

6. Conclusions
This paper expounds the implement principle, method
and technology of the TCP/IP protocol on WEBIT platform. In implement of the TCP/IP protocol we bring in
the programming ideas of network layer and reduced
TCP/IP system structure which adapt to the characteristics of singlechip [10]. At the same time, the paper also
make certain explore and try in embedded singlechip and
network applications finally, the feasibility of this protocol is confirmed in simulation experiment system, so it’s
significant and full of reference value for embedded Internet system design and development.

REFERENCES
[1]

G. Y. Xu, Y. C. Shi and W. K. Xie, “Pervasive Computing,” Computer Journal, Vol. 26, No. 9, 2003, pp. 10421052.

[2]

D. A. Gregory and E. D. Mynatt, “Charting Past, Present
and Future Research on Ubiquitous Computing,” ACM
Transaction on Computer-Human Interaction, Vol. 7, No.
l, 2002, pp. 29-58.

Table 1. Response of ICMP packet
Size

Meanvalue

many times.
As the test results listed in the Table 2, the average
established time of TCP is about 2.314 ms, it means that
server can quickly respond according to external request.
In the process of test, while singlechip server accepting
the connection request, it has well reliability. Mean while,
it means after implementing the reduced TCP/IP protocol
on the AVR singlechip, it can well meet the requirement
of non-PC devices be connected to Internet.
For the testing of UDP protocol, we write a program
according to the realized UDP protocol that creates two
UDP Sockets, one is to achieve the function of sending
UDP data, and the other is used to achieve the function
of receiving UDP data, in order to test the correctness of
the UDP protocol. The procedure of testing is:
1) Initialize the equipment;
2) Create the UDP Sockets (S1 and S2) through WebitUdpCreateSocket;
3) S1 sends UDP data to S2 through the function of
WebitUdpSendTo, S2 receives data form S1 through the
function of WebitUdpReceiveFrom, and output the received data to the serial port;
4) Close the two created Sockets (S1 and S2) through
WebitUdpDestroySocket command.
Finally, the testing results show that S2 received data
form S1 successfully; this means the UDP protocol of
TCP/IP protocol cluster is correct.

Figure 8. The test results of ARP, ICMP and IP protocol

Response
of
ICMP

765

JSEA

The Design and Implement of TCP/IP Protocol Cluster on AVR Singlechip

766
[3]

H. Zhao and Y. Chen, “Pervasive Computing,” Northeastern University Press, Shenyang, 2005.

[4]

H. Zhao, “Embedded Internet,” Tsinghua University
Press, Beijing, 2002.

[5]

T. KindBerg and A. Fox, “System Software for Ubiquitous Computing,” IEEE Pervasive Computing, Vol. 1, No.
1, 2002, pp. 70-81.

[6]

[7]

V. Jonathan and P. Joseph, “Profiling and Reducing
Processing Overheads in TCP/IP,” IEEE/ACM Transactions on Networking (TON), Vol. 4, No. 6, 1996, pp.
817-828.
H. Jang, S.-H. Chung and D.-H. Yoo, “Design and Implementation of a Protocol Offload Engine for TCP/IP
and Remote Direct Memory Access Based on Hardware/Software Coprocessing,” Microprocessors & Mi-

Copyright © 2010 SciRes.

crosystems, Vol. 33, No. 5-6, 2009, pp. 333-342.
[8]

J. F. D. Rezende, M. M. D. A. E. Lima, N. L. S. D. Fonseca, “Mobility over Transport Control Protocol/Internet
Protocol (TCP/IP),” In: M. Llyas Ed., The Handbook of
Ad Hoc Wireless Networks, CRC Press, Boca Raton, 2003,
pp. 329-343.

[9]

A. Dunkels, “Full TCP/IP for 8-bit Architectures,” Proceedings of the 1st International Conference on Mobile
Systems, Applications and Services, San Francisco, May
2003, pp. 85-98.

[10] S. Ichiro, “Location-Aware Communication in Smart
Spaces,” Proceedings of 2007 International Conference
on Multimedia and Ubiquitous Engineering (MUE 2007),
Seoul, 26-28 April 2007, pp. 1027-1034.

JSEA

Q2C@UST: Our Winning Solution to Query Classification
in KDDCUP 2005
Dou Shen, Rong Pan, Jian-Tao Sun,
Jeffrey Junfeng Pan, Kangheng Wu, Jie Yin and Qiang Yang
Department of Computer Science, Hong Kong University of Science and Technology,
Clear Water Bay, Kowloon, Hong Kong, China
{dshen, panrong, jtsun, panjf, khwu,yinjie,qyang}@cs.ust.hk
http://webproject1.cs.ust.hk/q2c/

ABSTRACT
In this paper, we describe our ensemble-search based approach,
Q2C@UST (http://webproject1.cs.ust.hk/q2c/), for the query
classification task for the KDDCUP 2005. There are two aspects
to the key difficulties of this problem: one is that the meaning of
the queries and the semantics of the predefined categories are
hard to determine. The other is that there are no training data for
this classification problem. We apply a two-phase framework to
tackle the above difficulties. Phase I corresponds to the training
phase of machine learning research and phase II corresponds to
testing phase. In phase I, two kinds of classifiers are developed as
the base classifiers. One is synonym-based and the other is
statistics based. Phase II consists of two stages. In the first stage,
the queries are enriched such that for each query, its related Web
pages together with their category information are collected
through the use of search engines. In the second stage, the
enriched queries are classified through the base classifiers trained
in phase I. Based on the classification results obtained by the base
classifiers, two ensemble classifiers based on two different
strategies are proposed. The experimental results on the validation
dataset help confirm our conjectures on the performance of the
Q2C@UST system. In addition, the evaluation results given by
the KDDCUP 2005 organizer confirm the effectiveness of our
proposed approaches. The best F1 value of our two solutions is
9.6% higher than the best of all other participants’ solutions. The
average F1 value of our two submitted solutions is 94.4% higher
than the average F1 value from all other submitted solutions.

Keywords
Query classification, Synonym-based
learning, KDDCUP 2005.

Classifier,

ensemble

1. INTRODUCTION
Historically, search engine technologies and automatic text
classification techniques have progressed hand in hand. Ever
since the early papers by the pioneers [14][18][22], people have
recognized the possibility of conducting Web search through
classification, and vice versa [2][5][6][15]. The KDDCUP 2005
competition made this connection even stronger; the task being to
automatically classify 800,000 of the queries on a Web search
engine into a set of 67 predetermined categories provided by the
organizers. This task deviates from the traditional machine
learning and text classification formulation in the following
aspects:
x

There are no training data available. It is part of the
task to collect the training data. In fact, the manually

SIGKDD Explorations

annotated Web pages organized into topical directories
could serve as the training data. However, different
parts of the Web may provide training data of different
quality. In contrast, text classification in machine
learning and information retrieval has always included
the training data as part of the input. How to make the
best use of the Web, including its directory resources
and search tools, provides a fresh perspective.
x
The data are noisy and provide poor information. Most
queries are short. There are some mis-spelt queries.
Many words have multiple meanings and belong to
several categories. For example, “office” can mean the
working place as well as a kind of software. In contrast,
several queries may in fact mean the same thing, such
as “mainboard” and “planar board”.
x
The target categories suffer a shortage of semantic
meanings. The 67 categories are provided without
further explanation. Therefore, simple exact matches
between queries and category names would be certain
to fail.
Despite these difficulties, we are also encouraged by the
possibility of answering some exciting questions through
completing this challenging task. We have the following
questions:
x

How can machine learning, in particular classification,
help in designing better search engines?
x
Conversely, how can search engines help us design
better text classifiers?
x
How can the use of different training data impact on the
quality of classification?
x
How can similarity-based classification and statistics
based classification be best integrated together?
In the sections that follow, we explain our choice of solutions as
well as the results of testing these solutions on the validation and
test data set provided by the competition organizers. One
innovative aspect of our solution is the use of an ensemble of
search engines to compose the classifiers, and an ensemble of
classifiers to classify the massive input queries. The ensemble of
search engines and the final integration of the search result
showed that with moderate validation data, we can achieve a high
level of performance when we appropriately combine the query
categorization results.

2. OUR APPROACH
As in any machine learning application, we also adopt two phases
in our solution. In phase I, which corresponds to the training
phase of machine learning research, we collect data from the Web

Volume 7, Issue 2

Page 100

Figure 1. The architecture of our approach Q2C@UST

Figure 2. The two-stage framework in the “testing phase” of our approach Q2C@UST
for training two base classifiers that map a query to the 67
categories provided by the KDDCUP 2005 organizers (KDDCUP
categories).
In phase II, which corresponds to the “testing phase” in machine
learning research, we process each query through a two-stage
framework to handle the problem of query classifications. The
overall framework of our approach is summarized in Figure 1
and the detailed illustration of phase II is shown in Figure 2.
Phase II consists of two stages performed in a row. The first stage
is to enrich the queries by searching the relevant pages which can

SIGKDD Explorations

provide the meanings for the queries in order for them to be
classified. Enrichment is necessary because the queries are rather
short, and thus their meanings ambiguous. We perform the
enrichment process by finding the relevant text from related Web
pages or the category information of these pages through Web
search.
The second stage of phase II is to classify the queries based on the
data collected in the first stage and using the classifiers trained in
the first phase. At this stage, based on the classification results
through the two kinds of base classifiers, two ensemble classifiers

Volume 7, Issue 2

Page 101

with different ensemble strategies are employed to classify the
queries.

x

“…/internet/…” is mapped to “Computers/Internet and
Intranet”;

The experimental results show that the ensemble classifiers can
improve the classification performance significantly.

x

“…/Computers/…./Hardware/…”
is
mapped
to
“Computers/Hardware” while “…/Living/…/Hardware/…”
is mapped to “Living\Tools & Hardware”.

2.1 Phase I: Training base Classifiers
We now discuss phase I of our approach: training classifiers for
query classification. As noted above, a main problem of our task
is the lack of training data. This is a new problem in machine
learning: without training text documents with class labels, many
previous methods cannot be used.
We tackle this problem by developing two kinds of base
classifiers, which we later combine in an ensemble-search based
classifier (Section 2.3). The first kind of base classifier is the
synonym-based classifiers which uses the category information
associated with Web pages collected for each query. The second
kind of base classifier is statistical classifiers in which Support
Vector Machine (SVM) is employed. We obtain two ensemble
classifiers by combining these two kinds of classifiers according
to different ensemble strategies, which improves the classification
performance significantly more than the base classifiers.

In the implementation of the keywords matching method, we need
to consider three special cases.
x

On the second level of categories from the space of
KDDCUP, the words connected by “&” can individually
specify a concept. For example, in “Computers\Internet &
Intranet”, both “Internet” and “Intranet” can represent an
individual concept. However, the words connected by a
space can specify a concept only when they are joined
together. One example is “Sports\Olympic Games” in which
we need the two words together to define a concept.
Therefore when matching the keywords, we take each word
connected by “&” as a keyword and the combination of the
words connected by space as keywords.

x

We extend the keywords to include both the singular and
plural forms in advance. For example “Living\Book &
Magazine” is extended to “Living\Book & Magazine &
Books & Magazines”. Then we can conduct exact matching
without missing any possible mapping.

x

Seven out of the 67 KDDCUP categories just have a
keyword “Other” on the second level, such as
“Computers/Other”. It is impossible to determine their
semantics without taking other categories into consideration.
In our approach, given a KDDCUP category, say
“Computers/Other”, we firstly collect all the categories
containing the first level keyword, “Computers”, in the space
of a search engine. Then we remove the ones which can be
mapped to other KDDCUP categories with the same first
level as the target categories.

2.1.1 Synonym-based Classifiers
As discussed in Section 2.1, after the query enrichment stage, we
obtain a ranked category list for each query through a search
engine. Since the category hierarchies from different search
engines are distinct, the categories in different ranked lists vary a
lot. In addition, the hierarchies of the search engines differ greatly
from that given in the KDDCUP 2005 competition task. For
convenience, we call the former the search engine space for a
search engine, and the latter the KDDCUP space. For a particular
search engine, our objective is to build a mapping function
between the two spaces. Using this mapping function, we can
classify a query into the 67 KDDCUP categories.
The mapping function can be built by keyword matching. We
compare the keywords of categories in the KDDCUP space with
those in the space of a certain search engine. Consider two
categories, c1 and c2, where c1 is from the space of KDDCUP and
c2 is from the space of a certain search engine. If they share some
of the same keywords, we can map c2 to c1 directly. The
categories in the space of KDDCUP have two levels, such as,
“Computers\Hardware” and “Entertainment\Other”. The second
level specifies a particular field within the first level. For most of
the categories in the space of KDDCUP, we only consider the
keywords at the second level because they are not confused with
other categories. A typical example is “Computers/Internet &
Intranet”. Even when we do not consider the first level for all the
categories, there are no other categories which can be confused
with “Internet & Intranet”.
However, there are many categories that are more difficult to deal
with. For them we require that the keywords in the first level and
the second level should simultaneously match the categories in
the space of a certain search engine. Otherwise, we cannot
distinguish between two categories that share the same keywords
only in the second level, such as “Computers/Hardware” and
“Living/Tools & Hardware”. Although they both have the
keywords “Hardware” in the second level, they belong to two
different domains “Computers” and “Living” in the first level. We
give two examples to illustrate the above two cases:

SIGKDD Explorations

After applying the above direct mapping procedure, we may still
miss a large number of mappings; many categories in the space of
a search engine do not occur in the space of KDDCUP although
they share words with the same meanings. In response, we expand
the keywords in each label in the KDDCUP categories through
WordNet [25]. For example, the keyword “Hardware” is extended
to “Hardware & Devices & Equipments” and the keyword
“Movies” is extended to “Movies & Films”.
Using different search engines, we might have different category
spaces. For each space, we can construct a mapping function
between it and the space of KDDCUP categories. After obtaining
these mapping functions, we can perform query classifications
based on the category lists of each query collected in the data
collection stage. For each category list of a target query, we map
the categories to the space of KDDCUP according to the
corresponding mapping function. We accumulate the number of
times for each category in the space of KDDCUP being mapped
onto. Then we can obtain the categories for the target query in the
space of KDDCUP together with their occurrence count. By
ranking the categories in terms of the occurrence count, we get a
ranked list of KDDCUP categories into which the target query
can be classified. Based on different category lists and their
corresponding mapping functions, we gain different classification
results. Here, we refer this kind of classifier as a synonym-based
classifier.

Volume 7, Issue 2

Page 102

2.1.2 Statistical Classifiers
As discussed in the previous subsection, the synonym-based
classifiers have a low-recall drawback. In order to address this
problem, we proposed to use the statistical classifier to help
classify queries. The statistical classifier classifies a query based
on its semantic content. Thus even if the synonym-based classifier
fails, a query can also be classified by a statistical classifier. In
this paper, we use the Support Vector Machine (SVM) classifier
because of its high generalization performance for document
classification tasks and is easy to implement [12][13] .
When SVM is used for classification tasks (e.g., document
classification), the first step is to train a model using a set of
training examples. Then the model can be used to classify other
examples. Apparently, it is not straightforward to apply SVM for
a query classification task. There are at least two problems as we
have discussed. Firstly, there is no training data available, and we
only have the names of the 67 categories. Secondly, it is easy for
a human being to capture the semantic meanings of a query and to
identify its categories. However, for a statistical classifier, we
must put forward a method to represent a query, i.e., to construct
a query’s features.
The above problems are both dealt with in our approach. For the
first problem, we collect the training examples for the 67
categories by leveraging the mapping function constructed in the
synonym-classifier learning step and the manually labeled Web
page directory, such as ODP, etc. For each KDDCUP category,
we can find a set of categories from the ODP directory by the
mapping function. Thus Web pages contained in those collected
ODP categories are used as the training documents for the target
KDDCUP category. After the training examples are collected for
each KDDCUP directory, the SVM training algorithm can be used
to train the classifiers. For the second problem, we use the results
returned by a search engine to help represent a query. As
discussed in section 2.1, most queries contain only a few terms.
Therefore, it is common that a query may be relevant to several

SIGKDD Explorations

topics. By leveraging the search results returned by a search
engine, we try to capture the multiple topics relevant with the
issued query. We keep the top N results (the parameter N are
studied in section 3) and use the aggregate terms of the
corresponding snippets, titles, URLs terms and the category
names in the directory to represent the query. Finally, the query’s
bag of terms is processed by stop-word removal, stemming and
feature selection. The resultant term vector can be used as input
for the SVM classifiers and a ranked list of categories for each
query is produced.
In our approach, the ODP directory is used for the purpose of
collecting training documents. The SVMlight software package is
used for training (http://svmlight.joachims.org/). The linear kernel
is used and the one-against-rest approach is applied for the multiclass case. The DF and the information gain methods are used for
feature selection. For KDDCUP categories which contain too
many training examples, we use a sampling approach to decrease
the scale of SVM training.

200000
180000
160000
140000

Frequency

Based on the direct mapping approach, the synonym-based
classifiers tend to produce results with high precision but low
recall. They produce high precision because the synonym-based
classifiers are based on the manually annotated Web pages and
can utilize the classification knowledge of editors. Therefore,
once a mapping function is constructed, it is highly probable the
classification result is correct. For example, we have shown that
the categories such as “…/Computers/…./ Hardware/…” are
mapped to “Computers/Hardware”. Therefore, once a Web page
associated
with
a
query
falls
in
the
category
“Computers/Hardware/Storage/Hard_Drives”, we can assign
“Computers/Hardware” to the query with high confidence. They
produce low recall because it is hard to find all the mappings from
the search-engine categories to the KDDCUP categories by
keyword mapping. About 80,000 of the 354,000 categories in the
category space of Google are not mapped onto the space of
KDDCUP. Therefore, we cannot map all the categories in the
category list for a query to the 67 KDDCUP categories and may
miss some correct categories for the query. Another reason for the
low-recall problem is that a search engine may return only a few
or even no Web pages with categories. The synonym-based
classifier may fail because of the search results shortage problem.
Therefore, we need to construct other classifiers to help handle
the low-recall problem which is described in the next section.

120000
100000
80000
60000
40000
20000
0
1

2

3

4

5

6

7

Num be r of w ords pe r que ry

8

9

 10

Figure 3. Frequency of queries with different length

2.2 Phase II, Stage I: Query Enrichment
In our second phase, we have two stages. In stage one, we take
each query and enrich it so that there is a body of relevant text
that represents this query. This step is a key task because our goal
is to classify 800,000 queries into 67 categories without any
additional descriptions about these queries.
The 800,000 queries vary a lot. They might be as simple as a
single number such as “1939” or as complicated as a piece of
programming code which involves more than 50 words. shows
the statistical information about the number of words in each
query and the frequencies in the 800000 queries. From this figure
we can see that queries containing 3 words are most frequent
(22%). Furthermore, 79% of queries have no more than 4 words.
Each query is a combination of words, name of persons or
locations, URLs, special acronyms, program segments and
malicious codes. Some queries contain words which are very
clean while others may contain typos or meaningless strings
which is totally noisy. Some words may just have their meanings
as defined in a static dictionary while others may have some
special meanings when used on the Internet.
Moreover, the meaning of words may also evolve over time. For
example, “apple” is a kind of fruit as given in a dictionary.
However, it is also related to the name of a famous computer
corporation nowadays. Thus, we cannot classify a query solely

Volume 7, Issue 2

Page 103

relying on a static and out-of-date training set. Instead, we should
try to catch its meanings by asking the Internet, retrieving related
documents and extracting its semantic features. What is more, in
order to obtain a better and unbiased understanding of each query,
we should not rely on a single search engine but combine multiple
results from different search engines.
In our approach, we send each query into Google [10], Looksmart
[20] and a search engine developed by ourselves based on Lemur
[17]. We chose these three search engines because they can
provide both directory search and Web search. Directory search
refers to search algorithms that return the related pages of a query
together with the pages’ categories. Since these categories of Web
pages are labeled by people, it is appropriate to use them as the
ground truth to classify the queries. However, not all the pages
indexed by the search algorithm contain category information; in
this aspect, Web search, which is used to refer to search algorithm
as we typically use, can return more related pages that directory
search cannot. Based on the content of the pages returned by Web
search, we can classify the queries using a text classification
algorithm. To enrich a query through search engines, we use the
following three steps:
1.

In step one, we first try to get the related pages through
“Directory Search”;

2.

In step two, if we cannot get sufficient results from step
one, we try to apply “Web Search” to the query to
obtain the text;

3.

If the retrieved results from Step two are still not
enough, we check whether the queries are too noisy. If
so, we use some preprocessing approaches to clean
them up and repeat step 1 and step 2. Usually, after
preprocessing on the noisy queries, we can obtain some
meaningful and clean queries which make it possible to
get new retrieved results through step 1 and step 2.

We use Google as an example to illustrate the three steps as
shown in Figure 4.
x

At the root level, all 800,000 queries are preprocessed
by removing special characters such as “@,#,%” while
keeping letters and digits. These 800,000 queries are
first sent into the Google Directory Search. We can see
from Figure 4 that we are able to retrieve related pages
for 500,000 (63%) of all queries in this way.

x

We then send the remaining 300,000 queries into the
Google Web Search and obtain results for the additional
200,000 queries.

x

For the remaining 100,000 queries, we conduct further
preprocessing. We use the function of “Did you
mean…”, provided by Google to trigger the search for
the most relevant queries to the original ones. For
example, given the query “a cantamoeba”, neither
Google “Directory Search” nor “Web Search” returned
any results. However, by trying the function “Did you

Figure 4. Illustration of the three steps for query enrichment

SIGKDD Explorations

Volume 7, Issue 2

Page 104

Search” results. 400,000 have “Web Search” results and the
remaining 200,000 have no results.

mean…”, Google can return the results for the word
“acanthamoeba” which is related to health, disease and
medicine. In this way, we can get the results for
another set of 60,000 queries from Google’s
“Directory Search”or “Web Search”.
x

The third engine we use was developed by ourselves based on
search engine constructor Lemur [17]. We first crawled 2
million Web pages with the category information from Open
Directory Project (ODP) [21]. Then we indexed the collection of
these pages with Lemur. Given a query, Lemur can retrieve a
number of pages which are most relevant to the query together
with their corresponding categories. Therefore the function of
this search engine based on Lemur and the ODP data is similar
to the “Directory Search” provided by Google and Looksmart.
Using this search engine, we can retrieve related pages for all
but 35.000 of the 800,000 queries..

However, after this step, there are still 40,000 queries
left without any results. These queries are very noisy.
They are usually connected words without spaces,
long meaningless clobbers, or URL addresses
containing parameters or even malicious codes. We
try to convert theses queries into meaningful ones by
extracting words from them through a maximum
matching method based on the dictionary WordNet
[25]. This method tries to extract as many meaningful
words as possible and any extracted words should be
as long as possible.

In summary, after enriching the queries through a search engine,
we can obtain two lists for representing each query. One is the
relevant Web pages list. The other is a category list
corresponding to the Web pages in the Web page list.

Take the query “wheelbearingproblemsdamage” as an
example, Google can not return any results through
either “Directory Search”, “Web Search” or even “Did
you mean…”. Therefore, we can split the whole query
into four meaningful words “wheel bearing problems
damage”. After that, we can get reasonable results
from Google “Directory Search” or “Web Search”. In
this way, we can get the results for 30,000 out of the
remaining 40,000 noisy queries.

2.3 Phase II, Stage II: Query Classification
through Ensemble Classifiers
From Phase I, we have constructed various classifiers which can
classify the input queries into the KDDCUP categories. In this
section, we discuss how we combine the results of these
classifiers in the query-classification phase (testing phase) by an
ensemble classifier, so that we can complete the task of query
classification.

Based on the above procedures, there are only 10,000 queries
without any pages returned from Google. These queries are
inherently noisy and meaningless, such as “dddddfdfdfdfdf”.
We do not have to deal with these queries.

An ensemble is a set of models whose predictions are combined
by weighted averaging or voting. Dietterich states that "A
necessary and sufficient condition for an ensemble of classifiers
to be more accurate than any of its individual members is if the
classifiers are accurate and diverse." [7] Recent work shows that
models trained with different learning algorithms often make
uncorrelated errors. Therefore, an ensemble of good models
trained with different learning algorithms often outperforms the
best model trained by one of the learning algorithms [4].
Moreover, many empirical investigations have shown that
ensemble learning methods often lead to significant
improvements across a wide range of learning problems
[1][3][8][9][23].

Note that in the query processing tree shown in Figure 4, the
quality of the retrieved documents at different leave nodes may
be different; the closer a node is to the root, the higher the
quality of the result. Thus, the results of the 700,000 queries by
applying “Directory Search” and “Web Search” are of high
quality and reflect the true meanings of these queries. However,
the results of the other 100,000 may not be reliable because we
revise the queries to some extent.
We follow the same steps when using Looksmart [20]. Among
the 800,000 queries, about 200,000 queries have “Directory

w_
n6
7

...

Category 1

Category j

Sort categories by
voting score

Top k
Categories

...

classifier n

...

w w_n1
_n
j

...

j
_1 w_1n
w

Query

...

classifier 1

...

11
w_

Category 67

Figure 5. The ensemble paradigm

SIGKDD Explorations

Volume 7, Issue 2

Page 105

We use two approaches to training the ensemble classifiers. The
first is to make use of the validation data set which contains 111
pairs of query-to-category mappings provided by the KDDCUP
organizers, to adjust the combination weights. The second
approach is to ignore this validation data set in order to avoid
overfitting.

A:

Our primary method in combining different classifier functions is
to assign an ensemble weight value to the result returned by each
classifier function, and rank the final list of classified categories.
(See Figure 5) It is tricky to assign the ensemble weights.
Different classifiers introduced in the above sections have
different performance. Some may work better than others on
certain categories. For example, a classifier may achieve high
precision on one category while having high recall on the other
category. This indicates that it is not proper to assign a single
weight to a classifier. Instead, we should differentiate the weight
of a classifier on different classes according to its performance.
To determine the weights, we validate each component classifier
on the 111 samples given by the KDDCUP organizers. The
higher precision a classifier achieves on a category, the higher
weight assigned to the classifier on that category. As a result,
each classifier may obtain a weight value Wij on a KDDCUP
category j.

C:

However, the 111 samples may be too small to be a suitable
validation data set as it is easy to be overfitting on these samples.
Therefore, a conservative way to combine the classifiers is to
assign equal weights to them.
Both the weight-assignment strategies in ensemble classifier
construction are compared in the following experiments.

3. EXPERIMENT AND DISCUSSION
To test our proposed approach, we conduct some experiments on
the datasets provided by the KDDCUP 2005 organizer. The
experimental results validate the effectiveness of our approach. In
addition, we give some analysis of the consistency of the three
labelers on their judgment of the performance of the classifiers in
this part.

3.1 Dataset
The KCCCUP2005 organizer provides a dataset which contains
111 sample queries together with the manual categorization
information. These samples help exemplify the format of the
queries and provide the semantics of a tiny number of queries. In
fact, since the category information of these queries is truthful,
they can serve as the ground truth for the test data and validation
data for our proposed classifiers. Later, after the competition is
finished, to test the solutions submitted by the participants, the
organizer provides another dataset which contains 800 queries
with labels from three human labelers. We denote the three
labelers (and sometimes the dataset labeled by them if no
confusion is caused) as L1, L2 and L3, respectively. We refer to
the former as Sample Dataset and the latter as Testing Dataset in
the following sections.

3.2 Evaluation Criterion
The evaluation criteria adopted by the KDDCUP organizer is the
standard measures to evaluate the performance of classification in
Information Retrieval (IR), including precision, recall and F1measure [24]. The definitions of precision, recall and F1 in the
query classification context are given below:

SIGKDD Explorations

¦ # of queries are correctly tagged as c

i

i

B:

¦ # of queries are tagged as c

i

i

¦ # of queries whose category is c

i

i

Pr ecision

A
B

A
C
2 u Pr ecision u Re call
Pr esion  Re call

Re call

F1

For the Sample Dataset, we report the precision, recall and F1
evaluation results for each classifier. For the Testing Dataset,
three labelers are asked to label the queries, thus the results
reported on this dataset are the average values[19]. Take the
calculation of F1 as an example:
Overall F1

1
3

3

¦ (F1 against human labeler i)
i 1

3.3 Experimental Results and Explanation
As introduced in the previous section, we have a total of six
classifiers, three synonym-based classifiers, one statistical
classifier that is SVM and two ensemble classifiers. For simplicity,
we refer to the three synonym-based classifiers which rely on
Google, Looksmart, and Lemur as S1, S2 and S3 respectively. We
denote the ensemble classifier which relies on the validation
dataset as EV and the one that does not rely on validation data as
VN. In the following part, we first investigate the two main
parameters which affect the performance of our proposed
classifiers on the sample dataset and then we compare the
performance of those classifiers. Finally, we give some analysis
of the consistency of the three labelers on their judgment of the
classifiers’ performance.

3.3.1 Parameter Tuning
There are two main parameters which significantly impact on the
performance of our proposed classifiers. One is how many related
pages we should use for each query. If we use too few pages, we
may fail to cover all the diverse topics of a query. However if we
use too many pages, we may introduce much noise. Another
parameter is how many labels we should assign to each query.
The competition rules allow us to assign at most 5 labels for each
query. However, in order to achieve higher precision, we should
assign fewer but very accurate labels. If we hope to achieve
higher recall, we need to assign more possible labels.

Figure 6 shows the performance of different classifiers with
respect to an increasing number of related pages used for
classification. Here, the related pages are taken into consideration
in the order they are returned by the search engines, that is in the
order of the degree of relevance with the query. The results shown
in Figure 6 verify our conjecture. As the number of related pages
increases, the precision tends to decrease, although the precision
for some classifiers increase at first. The reason is that we need a
certain number of pages to get the meaning of the query. However,

Volume 7, Issue 2

Page 106

Figure 7 shows the performance of different classifiers by
varying the number of classified categories. As we expected,
when the number of classified categories increases, the precision
of all the classifiers decreases while the recall increases
significantly. In contrast, the value of F1 increases at the
beginning and then subsequently decreases. For most of the
classifiers, the maximum values of F1 are achieved when four
categories are generated for each query. Although the F1 values
are close to those obtained when 5 categories are assigned, the
precision values are much lower.
Based on the observation of the impact of two parameters, in
order to achieve higher precision without sacrificing F1 much, we
consider only the top 40 related pages of a query and return 4
categories for each query in most cases.

S1
SVM

S2
EN

0.55
0.50
0.45
0.40
10 20 30 40 50 60 70

S2
EN

(3) F1 of the Six Classifiers

Figure 6. Performance of different classifiers with the number
of used related pages on the 111-sample dataset

S2
EN

S3
EV

0.90

S3
EV

0.80

0.60

Pre

0.70

0.55
Pre

80 90 100

Top N Pages

S1
SVM
S1
SVM

S3
EV

0.60

F1

if we include too many pages, noise may be introduced which can
reduce the precision. From Figure 6, we can see that the critical
point for most classifiers is 40. Before that point, the performance
of the classifiers increases when we use more and more pages,
while after that point the performance begins to decrease.
Therefore in the following experiments we keep only the top 40
pages for subsequent query classification.

0.60
0.50

0.50

0.40
0.30

0.45

1
0.40

2

3

4

5

6

Number of classified categories
10 20 30 40 50 60 70 80 90 100
Top N Pages

(1) Precision of the Six Classifiers
S1
SVM

(1) Precision of the Six Classifiers

S1
SVM

S2
EN

S3
EV

0.70

S3
EV

0.60
0.50
Rec

0.65
0.60
Rec

S2
EN

0.40
0.30

0.55

0.20

0.50

0.10

0.45

1

0.40

2

3

4

5

6

Number of classified categories

10 20 30 40 50 60 70 80 90 100
Top N Pages

(2) Recall of the Six Classifiers

(2) Recall of the Six Classifiers

SIGKDD Explorations

Volume 7, Issue 2

Page 107

S1
SVM

S2
EN

labeler 1 and labeler 3. After ranking the six classifiers according
to each labeler, we calculate the spearman rank-order correlation
coefficient between each pair of the labelers [11]. The Spearman
correlation is a nonparametric approach to calculating the
relationship between two variables based on ranking the two
variables and no assumption about the distribution of the values is
made. The results are shown in Table 1.

S3
EV

0.60
0.55

F1

0.50
0.45
0.40
0.35

Labeler1

0.30
0.25
1

2

3

4

5

6

Labeler3

0.50

Number of classified categories

0.45
0.40

(3) F1 of the Six Classifiers

Figure 7. Performance of different classifiers with the number
of classified categories on the 111-sample dataset

0.35
0.30
S1

3.3.2 Comparison between Classifiers
From the above experimental results on the Sample Dataset, we
can see that among the four base classifiers, S1 works best while
S2 works worst. The reason why S2 does not work well is that for
many queries, we cannot obtain enough related pages through the
Looksmart search engine. For SVM, we expect that it can solve
the low-recall problem caused by the three synonym-based
classifiers, as discussed in section 2.1.1. Figure 6 and Figure 7
show that SVM obtains the highest recall in most cases. We also
notice that the two ensemble classifiers can achieve better
performance in terms of F1 than any other base classifier. For the
peak F1 values of the three best classifiers (EN, EV and S1), we
can see that, compared with S1, EN and EV improve the F1 by
4.53% and 8.67% respectively. In fact, when we design these two
ensemble classifiers, EV is expected to achieve higher precision
measure because each component classifier is highly weighted on
the classes where it achieves high precision and EN is expected to
achieve higher F1 performance since the recall is relatively high.
According to our two submitted results, the F1 value of EN (0.444)
achieves 4.2% improvement compared with the F1 value of EV
(0.426). While the precision value of EV (0.424) improves by
2.3% compared with that of EN (0.414).
The effectiveness of our proposed ensemble classifiers can also be
validated when compared with the other participants’ solutions on
the Testing Dataset. The F1 value of our solution generated by
EN is 9.6% higher than the best one among other participants’
results. The averaged F1 value of our submitted results is 94.4%
higher than the averaged F1 value of all the others.

3.3.3 Analysis of the Consistency between the Three
Labelers
In this section, we analyze and discuss the consistency between
the three labelers on their judgment of the performance of the
classifiers.
Figure 8 shows the F1 values of the six classifiers developed by
us on the testing data labeled by the three labelers. From Figure 8,
we can see that the three labelers have a high correlation with
respect to the relative performance of the classifiers, especially

SIGKDD Explorations

Labeler2

0.55

S2

S3

SVM

EN

EV

Figure 8. The distribution of the labels assigned by the three
labelers
L1.vs.L2

L1.vs.L3

L2.vs.L3

0.829

0.943

0.771

Table 1. Spearman correlation between each pair of labelers
In general terms, correlation coefficients over 0.67 indicate strong
relationships. So we can conclude that the three labelers are in
strong correlation when they determine the performance of
classifiers.

3.4 The Failed Methods
In fact, we have tried several other approaches which do not work
well. Here is an example. The main idea is to build a bridge
between a query and the 67 KDDCUP categories by counting the
number of pages relating to both of them. We submitted a query
into search engines and got its related pages. This set of pages is
denoted by Pq. Similarly, we can get the related pages of a
category, using the category name directly as a query. This set of
pages is denoted by Pc. In practice, each Pq includes 100 pages for
each query, and each Pc includes 10,000 pages. Then we can
define a similarity between the target query and category as | Pq ŀ
Pc |, where |.| is the size of a set. For each query, we can return the
most related categories by similarity. However, this method does
not seem to work. One possible reason is that there is a
correlation between some pairs of categories. For example, we
found that the overlap of the 10,000 retrieved pages of the
categories “computer\hardware” and “Living\Tools & Hardware”
are about 1000 pages. Therefore we removed the overlapping
pages between each pair of categories. We repeated the above
approach. However, the final result is not satisfactory. One reason
for the failure of this method is that the retrieved pages cannot
effectively represent the semantics of a category. The essence of
the problem is how to automatically find the exact collection of

Volume 7, Issue 2

Page 108

pages that can well represent the semantics of a query and a
category. This is a problem that needs further study.
We also tried another method based on a dictionary which does
not work well either. We submitted a query q into dictionary
software, e.g., WordNet, to get the related words of a query. The
result is denoted as Rq. The result includes the synonyms or
antonyms of the given query, which can be a verb, a noun, or an
adjective, among others. Take the query “car” as an example, the
result contains: car, auto, automobile, machine, motorcar, railcar,
railway car, railroad car and cable car, and so on. Similarly, we
can get the result of every category c through the same way which
is denoted as Rc. A similarity between the target query and
category is defined as | Rq ŀ Rc | as shown before. We can build a
matrix and get the most relevant categories based on similarity for
every query. When we test this method on the validation dataset,
the F1 is only about 20%. The main reason for the poor
performance is that this method cannot obtain proper results for
many queries through WordNet. If more dictionaries like
Wikipedia are leveraged, the performance of this kind of method
might be improved.

4. CONCLUSION AND FUTURE WORK
In this paper, we presented our approach to solving the query
classification problem provided by KDDCUP 2005. Query
classification is an important as well as a difficult problem in the
field of information retrieval. Once the category information for a
query is known, the search engine can be more effective and can
return more representative Web pages to the users. However,
since the queries usually contain too few words, it is hard to
determine their meanings. Another challenge in KDDCUP 2005 is
that, no training data are provided for the classification task. What
is more, the semantics of the predefined category structure in
KDDCUP 2005 are not given explicitly.
To solve the task of KDDCUP 2005, we rely on some extra
(external) information to overcome these difficulties. Therefore,
we proposed an ensemble search based method which consists
two phases with the second phase containing two stages. Phase I
corresponds to the training phase of machine learning research
and phase II corresponds to testing phase. In phase I, two kinds of
base classifiers are developed. One is synonym-based and the
other is statistics based. At the first stage of phase II, the queries
are enriched in that the related Web pages together with their
category information are collected for each query through search
engines. In the second stage, the queries are classified through
the classifiers trained in phase I. The synonym-based classifiers
perform classification using the category information of the
related Web pages for each query while the statistical classifier
uses the contents of the related pages for each query. The
experimental results on the two datasets provided by the
KDDCUP 2005 organizer validate the effectiveness of these
classifiers. By combining these two kinds of classifiers through
two different strategies, we obtained two ensemble classifiers.
One is expected to achieve higher precision with the help of the
validation dataset and the other is expected to achieve a higher F1
without using the validation dataset. Both our experimental results
on the datasets and the evaluation results given by the organizers
verified the effectiveness of our approach.
Our proposed approach is proved to be very effective for the
query classification problem. We have designed a demonstration
system called Q2C@UST, with a dedicated Web site at

SIGKDD Explorations

http://webproject1.cs.ust.hk/q2c/. Our success is due to two
factors: one is the method for enriching queries and the other is
the method of combining the base classifiers. In the future, we
will conduct more research work following the two directions: 1)
we would try to find more valuable extra information for the
queries based on which we can build the base classifiers; 2)we
will conduct some further research to find some more effective
strategies to generate ensemble classifiers.

5. ACKNOWLEDGMENTS
We thank Hong Kong RGC Grant HKUST # 03/04.EG01 HKBU
2/03C, HKUST 6187/04E and Hong Kong University of Science
and Technology, Department of Computer Science and Computer
Systems Group for their kind support. We also thank KDDCUP
2005 organizers for their suggestions on this paper.

6. REFERENCES
[1] E. Bauer, R. Kohavi. An empirical comparison of voting
classification algorithms: Bagging, boosting and variants.
Machine Learning, 36:1/2, 105-142. 1999.
[2] D. Beeferman and A. Berger. Agglomerative clustering of a
search engine query log. In Proceedings of the sixth ACM
SIGKDD International Conference on Knowledge Discovery
and Data Mining, pages 407-415, 2000.
[3] L. Breiman. Bagging predictors. Machine Learning, 24:2,
123-140. 1996.
[4] R. Caruana and A. Niculescu-Mizil. Ensemble selection from
libraries of models. In Proc. 21th International Conference
on Machine Learning (ICML'04), 2004.
[5] C. Chekuri, M. Goldwasser, P. Raghavan and E. Upfal. Web
Search Using Automated Classification. Poster at the Sixth
International World Wide Web Conference (WWW6), 1997.
[6] H. Chen, S. Dumais. Bringing order to the Web:
Automatically categorizing search results. In Proceedings of
the ACM SIGCHI Conference on Human Factors in
Computing Systems (CHI), pages 145-152, The Hague, The
Netherlands, April 2000.
[7] T. G. Dietterich. Ensemble methods in machine learning.
First International Workshop on Multiple Classifier Systems,
pages 1-15, 2000.
[8] W. Fan, S. Stolfo, J. Zhang. The application of AdaBoost for
distributed, scalable and on-line learning. In Proceedings of
the Fifth SIGKDD International Conference on Knowledge
Discovery and Data Mining, 362-366. 1999.
[9] Y. Freund, R. E. Schapire. Experiments with a new boosting
algorithm. In Proceedings of the Thirteenth International
Conference on Machine Learning, 148-156. 1996.
[10] Google, http://www.google.com
[11] P. G. Hoel, Elementary Statistics, Wiley, 1971.
[12] T. Joachims. Transductive inference for text classification
using support vector machines. In Proc. 16th International
Conference on Machine Learning (ICML), Bled, Slovenia,
June 1999.
[13] T. Joachims (1998): Text Categorization with Support
Vector Machines: Learning with Many Relevant Features.
European Conference on Machine Learning (ECML), Claire
Nédellec and Céline Rouveirol (ed.), 1998.

Volume 7, Issue 2

Page 109

[14] K. S. Jones. Automatic Keyword Classification for
Information Retrieval. Butterworths, London, 1971.
[15] I.H. Kang, G. Kim, Query type classification for web
document retrieval. In Proceedings of the 26rd annual

international ACM SIGIR Conference on Research
and Development in Information Retrieval. Toronto,
Canada, 2003, 64-71.
[16] J. Kittler, M. Hatef, R. P.W. Duin, and J. Matas. On
Combining Classifiers. IEEE Trans. Pattern Analysis and
Machine Intelligence, Vol. 20, No. 3, 1998, pp. 226-239.
[17] Lemur, http://www.lemurproject.org/
[18] D.D. Lewis, W. A. Gale. A sequential algorithm for training
text classifiers. In W. Bruce Croft and Cornelis J. van
Rijsbergen, editors, Proceedings of SIGIR-94, 17th ACM
International Conference on Research and Development in
Information Retrieval, pages 3-12, Dublin, IE, 1994.
Springer Verlag, Heidelberg, DE.
[19] Y.Li, Z.J.Zheng, K.Dai. KDD-CUP 2005. Presentation on
The Eleventh ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining. Chicago, USA.
August 21, 2005. http://kdd05.lac.uic.edu/kddcup.html.
[20] Looksmart, http://www.looksmart.com
[21] ODP: Open Directory Project, http://dmoz.com
[22] L. Page, S. Brin, R. Motwani, and T. Winograd. The
PageRank citation ranking: Bringing order to the web.
Technical report, Stanford Digital Library Technologies
Project, Stanford University, Stanford, CA, USA, 1998.
[23] J. R. Quinlan. Bagging, boosting and C4.5. In proceedings
of the Thirteenth National Conference on Artificial
Intelligence, 725-730. 1996.
[24] C.J. van Rijsbergen. Information Retrieval. Second Edition,
Butterworths, London, 1979, 173-176.
[25] Wordnet, http://wordnet.princeton.edu/

SIGKDD Explorations

About the authors:
Dou Shen is a Ph. D. student in the Department of Computer
Science at Hong Kong University of Science and Technology. His
research interest includes machine learning and data mining,
especially in the field of sequential data mining and text mining.
Jian-Tao Sun is a Ph. D. candidate at the Department of Computer
Science and Technology, Tsinghua University at Beijing, China.
He was a visiting scholar at Hong Kong University of Science and
Technology when he took part in the KDDCUP 2005 competition.
His research interests include text mining, Web usage mining and
kernel methods.
Rong Pan is a postdoctoral fellow at Hong Kong University of
Science and Technology. His research interest includes machine
learning, data mining and case-based reasoning.
Jeffrey Junfeng Pan is a Ph. D student in the Department of
Computer Science at Hong Kong University of Science and
Technology. His research interest includes machine learning
and data mining. Currently, he is working on location estimation
and activity recognition in wireless (sensor) networks and
pervasive environments.
Kangheng Wu is a Ph. D. student in the Department of Computer
Science at Hong Kong University of Science and Technology. His
research interest includes automated planning and machine
learning, especially in the field of learning action models from
observed plans.
Jie Yin is currently a Ph. D. student in the Department of
Computer Science at Hong Kong University of Science and
Technology. Her research interests include artificial intelligence,
pervasive computing and data mining. Currently she is working
on location estimation and human behavior recognition from
sensory data in pervasive environments.
Qiang Yang (http://www.cs.ust.hk/~qyang) is a faculty member in
the Department of Computer Science at Hong Kong University of
Science and Technology. His research interests are artificial
intelligence, Web search, data mining and pervasive computing.

Volume 7, Issue 2

Page 110

APPROXIMATE FAIR ALLOCATION
OF LINK BANDWIDTH
APPROXIMATE FAIR DROPPING (AFD), AN ACTIVE QUEUE MANAGEMENT
SCHEME, ALLOCATES LINK BANDWIDTH IN AN APPROXIMATELY FAIR
MANNER. AFD-NFT, AN ENHANCEMENT TO AFD, PERFORMS SIMILARLY AND IS
MUCH EASIER TO IMPLEMENT.

Rong Pan
Balaji Prabhakar
Stanford University
Lee Breslau
AT&T Labs–Research
Scott Shenker
International Computer
Science Institute

36

Internet designers assumed users
would be congestion sensitive—that is, they
would automatically cut down their sending
rates when the network became congested.
Thus, the Internet takes a passive role in providing quality of service (QoS) under heavy
user loads. As the Internet grows, however, the
variety of users increases, and this assumption
becomes invalid. Consequently, the Internet
can no longer guarantee high-quality service
to all users. If routers could more actively participate in bandwidth distribution, the Internet would be more robust and could
accommodate more diverse users.1
Two general categories of fair-bandwidthallocation mechanisms, each with its own
drawbacks, exist.2 The first category, which
includes fair queuing (FQ)3 and its many variants,4,5 uses complex packet-scheduling algorithms that are more difficult to implement
than first-in first-out (FIFO) queuing. Algorithms in the second category—active queue
management schemes with enhancements for
fairness, such as flow random early detection
(FRED),6 and stochastic fair blue (SFB)7—
are based on FIFO queuing. They are easy to
implement and are much fairer than the original random early detection (RED) design,8
but they don’t aim to provide max-min fairness among numerous flows.

Like RED, AFD is an active queue management scheme that uses a FIFO queue and
drops packets probabilistically as they arrive.
AFD, however, bases flow-dropping decisions
not only on queue size but also on its estimate
of the flow’s (say flow i) current sending rate
ri. To achieve max-min fairness, we define the
dropping function di as (1 − rfairri−1)+. As a
result, fair share ri(1 − di) = min(ri, rfair) bounds
each flow’s throughput. Hence, AFD does not
distribute drops evenly across flows but applies
them differentially to flows with different
rates. What sets AFD apart from other queue
management algorithms is its simple and systematic approach to estimating ri and rfair.
To estimate ri, AFD recognizes that, like
flow size distribution, flow rate distribution is
long-tailed—that is, fast flows send most
bytes, and most flows are slow. For example,
Figure 1 shows the cumulative distributions

Published by the IEEE Computer Society

0272-1732/03/$17.00  2003 IEEE

In other work, we propose approximate fair
dropping (AFD),9 a router mechanism that
achieves approximately max-min fair bandwidth allocations with relatively low complexity. In this article, we propose an AFD
implementation that can mimic the original
design’s performance while retaining much
less state.

Approximate fair dropping

mfair(t) = mfair(t − 1) + α(q(t − 1) − qtarget) −
β(q(t) − qtarget)
where q(t) is the queue length at the tth sample, q(t − 1) is the queue length at the previous sample, and qtarget is the target queue size.
Constants α and β are configurable parameters. We discuss in detail how we set these
parameters elsewhere.9
Using this method, we can infer mfair
dynamically with no additional state. Analysis indicates that AFD’s memory requirements
are a small fraction of those needed for the
packet buffers.9
We have evaluated AFD’s performance in
a variety of simulations. One simulation setup
consists of seven transmission control protocol (TCP) flow groups (five flows each) with
different congestion control mechanisms and
round-trip times (RTTs). The congested-link
bandwidth is 10 Mbps, thus Rfair equals 286
Kbps. Figure 2 compares AFD’s performance
to that of RED and FRED. Figure 2a shows
the average throughput received by each flow
group, and Figure 2b depicts the correspond-

1

Cumulative fraction of bytes

of the 1-second flow rates for three different
traces. In these data sets, 10 percent of the
flows represent between 60 and 90 percent of
the total bytes. Therefore, a sample of recent
traffic consists mainly of bytes from faster
flows and, typically, these flows send at or
above the fair share. Most slow flows won’t
show up in the sample and AFD can ignore
them because it won’t drop them. Thus, AFD
needs only keep state proportional to the
number of fast flows, which is much less than
per-flow state. To do this, AFD maintains a
shadow buffer of b arrival packet samples
(headers only).
Suppose flow i has mi packets in the shadow
buffer. AFD can approximate i ’s arrival rate
by mi = briR−1, where R is the aggregate arrival
rate. Clearly, we can rewrite the drop function
as di = 1 −(mfairmi−1), where mfair = brfairR−1.
AFD obtains mfair implicitly. Varying mfair
intentionally causes Σiri(1 − di) to change
accordingly, which makes the queue length
fluctuate. It will stabilize when Σiri(1 − di)
equals the outgoing link capacity, at which
point mfair = brfairR−1. To ensure the queue
length stabilizes near a target value, AFD
updates mfair, using the equation

Trace 1
Trace 2
Trace 3

0.8

0.6

0.4

0.2

0
1e–06

1e–05

0.0001

0.001

0.01

0.1

1

Fraction of 1-second flow rates

Figure 1. Complementary distribution of 1-second flow rates for three
traces. Most bytes in the sample are from fast flows, with 10 percent of the
flows representing between 60 and 90 percent of the total bytes.

ing drop probability of each flow group. These
results demonstrate that AFD provides a good
approximation to fair bandwidth allocation
by differentially dropping packets.

Implementing AFD
Although AFD theoretically requires only
one data structure—the shadow buffer—to
function, it is infeasible to recount mi on each
packet arrival. Hence, a direct implementation of the AFD algorithm, which we refer to
as the AFD-SB design, requires two data
structures:
• a shadow buffer that stores a recent sample of packet arrivals and
• a flow table that keeps the packet count
of each flow in the shadow buffer.
We can implement the flow table structure
using a hash table or a content-addressable
memory with O(1) lookup time. AFD updates
the shadow buffer probabilistically. When a
packet arrives with probability p (p−1 is the
update interval), AFD replaces a random packet in the shadow buffer with the arriving packet. Although we could remove packets using
FIFO, random replacement avoids synchronization problems. After a replacement, the

JANUARY–FEBRUARY 2003

37

HOT INTERCONNECTS 10

TCP flow group 0
TCP flow group 1
TCP flow group 2
TCP flow group 3

of size 12 holds packets from three flows.
These flows have two, six, and four packets in
the shadow buffer. When a flow 3 packet
arrives, AFD randomly chooses a flow 2 packet for the newly arrived packet to replace. Flow
2’s packet count in the flow table thus decreases by one while flow 3’s packet count increases by one. These operations maintain the data
structures (the shadow buffer and the flow
table) used to guide dropping decisions, which
are separate from the FIFO buffers in which
actual packets are queued.

TCP flow group 4
TCP flow group 5
TCP flow group 6

500

Throughput (Kbps)

400

300

200

Single data structure
100

0
RED

FRED

AFD

(a)

A randomized approximation of AFD that
keeps only one data structure, the flow table,
can reduce AFD-SB’s memory requirement.
The shadow buffer is only logically present in
the sense that

∑

N

i =1

0.20

Drop probability

0.15

0.10

0.05

0
RED

FRED

AFD

(b)

Figure 2. Performance of RED, FRED, and AFD for average throughput
received by seven simulated TCP flow groups (a) and the corresponding
drop probability for each flow group (b).

packet count for the flow to which the victim
packet belongs (say flow i) decreases by one,
mi = mi − 1. Conversely, the packet count for
the flow to which the incoming packet belongs
(say flow j) increases by one, mj = mj + 1.
Assume the shadow buffer contains b packets
from N flows, then Σi = 1N m = b.
Figure 3 shows a simple example of this
packet replacement process. A shadow buffer

38

IEEE MICRO

mi = b

still holds. AFD increments the flow table on
packet insertions as before. The challenge is
removing a packet from the logical shadow
buffer—that is, decreasing a flow’s packet count
by one—without linearly traversing the flow
entries. Ideally, a new algorithm would mimic
AFD-SB’s performance: It would remove a flow
i ’s packet with a probability pi = mib−1. Therefore, on average, new packets would replace all
flow i ’s packets, mi, after b updates.
The initial AFD-FT (flow table) design (to
be consistent with our other work,9 we refer to
this design as AFD-FT) works as follows: When
it is time to update the logical shadow buffer,
AFD-FT uniformly chooses a small set of flow
IDs, S. If s is the size of S, each flow has equal
probability sN −1 of being in the set. Given that
flow i is in S, it has a probability of mi(Σj∈Smj)−1
to have its count decreased by one. AFD-FT
tries to approximate pi = mib−1 under AFD-SB
with pi = sN −1 mi(Σj∈Smj)−1. AFD-FT can
approximate AFD-SB’s performance when
there are no large flows whose packet counts are
much larger than those of other flows. If such
flows exist, however, AFD-FT tends to limit
their throughput under the fair share. This gives
flows an equal chance of being present in S, even
though a flow (∈ S) with more packets has a
higher probability of being reduced. Therefore,
flow i with a higher packet count has a lower
than mib−1 chance of being reduced at each
update. Consequently, on average, its total

count deduction is less than mi after b updates,
leading to a higher drop probability.
Using the example in Figure 3, Figure 4
illustrates how AFD-FT behaves when s equals
one. By choosing one flow at random, flows
1, 2, and 3 each have a one-third chance of
being reduced by one. Under AFD-SB, however, the chances for these three flows are onesixth, one-half, and one-third. Thus, while
AFD-SB needs six updates on average to
reduce flow 1’s count by one, AFD-FT needs
only three updates. AFD-FT favors small flows
and is biased against fast flows. As our later
simulations show, this bias against larger flows
can lead to a significant throughput penalty.

Shadow buffer
b = 12

Flow table

Flow 1

2 packets

Flow 2

6 packets

Flow 3

4 packets

Flow 3 packet arrives; AFD selects
a flow 2 packet to be replaced

Shadow buffer
b = 12

New flow table design
To improve AFD-FT’s performance, we
propose a new AFD flow table design, which
we refer to as AFD-NFT (new flow table).
AFD-NFT achieves the performance of AFDSB with AFD-FT’s state requirement.
When it is time to decrease a flow’s packet
count by one (that is, remove a packet from
the logical shadow buffer), AFD-NFT draws
a small set S of flow IDs uniformly from the
flow entries, if such a set does not already exist.
A flow i(∈ S)’s packet count decreases by one
with a probability of mi(Σj∈S mj)−1. These
operations are exactly the same for both AFDFT and AFD-NFT. The next step, however,
represents the crucial difference between the
two: AFD-FT chooses new set S for each
update. AFD-NFT, on the other hand, uses
the same set S for the next u = a × (Σj∈S mj)
updates, where the constant a < 1. After u
updates, AFD-NFT chooses a new set and
repeats the same operations.
Figure 4 shows how AFD-NFT would
work if a = 0.5 and s = 1. Each flow has a onethird chance of being drawn. When AFDNFT selects flow 1, m1 decreases to one.
Because u = 1, the algorithm will draw a new
flow for the next table update. Suppose AFDNFT chooses flow 2 instead, with u = 3. Flow
2 will be the victim flow for the following two
table updates before the algorithm selects a
new flow. Similarly, if flow 3 is drawn, AFDNFT will use it for the next update.

Analysis
Recall that our goal for AFD-NFT is to
match the performance of AFD-SB, replac-

Flow table

Flow 1

2 packets

Flow 2

5 packets

Flow 3

5 packets

Figure 3. Packet-dropping decisions in AFD-SB. When a
packet arrives, AFD randomly selects a packet in the shadow buffer for replacement.

Flow 1

2 packets

Flow 2

6 packets

Flow 3

4 packets

Arrival of a flow 3 packet
If flow 2
If flow 1

If flow 3

Flow 1

1

Flow 1

2

Flow 1

2

Flow 2

6

Flow 2

5

Flow 2

6

5

Flow 3

5

Flow 3

4

Flow 3

AFD-FT stops here/AFD-NFT continues

New draw

Flow 2
will be used for the
next two updates

Flow 3
will be used for the
next one update

New draw

New draw

Figure 4. AFD-FT and AFD-NFT designs. When a new packet arrives at the
shadow buffer, the algorithms update the flow tables.

JANUARY–FEBRUARY 2003

39

HOT INTERCONNECTS 10

S(0)

Assuming s << N, the average total packet count in chosen set S is bsN−1. Thus, the
total packets to be replaced are absN−1. To
replace b packets, we need to draw

R(0)
100 Mbps

10
Mbps

Ns =

Router B

Traffic sources

R(1)

Router A

S(1)

Traffic sinks

S(n–1)

sets.
• Given a set S and flow i(∈ S), there are
on average
mi
NPS i =
×u =
mj

R(n–1)

S(n)

b
N
=
−1
as
absN

∑

R(n)

mi

∑

Throughput (Kbps)

Figure 5. Basic simulation topology used in our evaluations.

650
600
550
500
450
400
350
300
250
200
150
100
50
0

mj

a×

∑m

j

= ami

j∈S

flow i packets to be replaced.
• When we combine the above three arguments, after b updates, the average number of flow i packets replaced equals

Fair share
Flow arrival rate
AFD-FT
AFD-SB
AFD-NFT

NPi =

s
N
×
× ami = mi ,
N as

which matches AFD-SB’s behavior.

Simulation results

0

10

20

30

40

50

Flow ID

Figure 6. Offered load and throughput for 50 CBR flows using the three
AFD designs.

ing approximately mi of flow i ’s packets after
b updates. By the law of large numbers, we
can prove that AFD-NFT’s performance, on
average, is the same as AFD-SB’s. We outline
the proof as follows:
• Each flow has the same chance of being
chosen for set S, and the probability, ps ,
is sN −1.
• A flow’s average packet count equals

∑

i =N
i =1

N

40

j∈S

j∈S

IEEE MICRO

mi

=

b
.
N

We evaluate AFD-NFT’s performance in
several scenarios and compare it to AFD-SB
and AFD-FT. Figure 5 depicts our simulation
topology. Unless otherwise stated, the latencies at the access links are 2 ms, and the latency at the congested link is 20 ms. In all the
experiments, b = 1,000, a = 0.06, and s = 5.

Performance improvement
Figure 6 compares the performance of AFDSB, AFD-FT, and AFD-NFT for a simulation
run in which five constant bit rate (CBR) flow
groups of 10 flows each compete for the congested link bandwidth of 10 Mbps. The sending rates for the groups are 50 Kbps, 100 Kbps,
200 Kbps, 400 Kbps, and 600 Kbps. Results
show that AFD-NFT can mimic AFD-SB’s
performance by providing each flow its fair
share. AFD-FT penalizes the aggressive flows
by limiting their throughput to be under their
fair share.
Although the performance penalty is mild
in this scenario, AFD-FT can severely punish
aggressive flows. Table 1 lists the simulation’s
flow table access statistics for pS, the probabil-

Table 1. Flow table access results from both theoretical analysis and actual simulation data.

sN −1
0.1
0.1
0.1
0.1
0.1

ity of a flow being in a set; NPSi , the average
number of flow i’s packets replaced; NPi , the
average number of flow i ’s packets replaced
after b updates; and Ns , the average number of
sets drawn after b updates. Because the variance
between individual flows is very small, as Figure 6 shows, we average the statistical data within the 10 flows in each group to more easily
present them. Clearly, the data obtained from
the simulation closely agrees with predictions
from our analysis.
We next evaluate the performance of AFD
designs in the presence of an on-off source. In
this setup, an on-off source shares the congested link with 35 TCP flows, where Rfair equals
278 Kbps. The bursty source sends at the access
link speed (100 Mbps) for a very short period,
ton, and then is idle for time toff . Its average sending rate is 100 Mbps × ton(ton + toff )−1. We plot
only the bursty source throughput in Figure 7
because it shows the largest discrepancies
among the three AFD algorithms. The TCP
flows use the remaining link bandwidth; differences among those flows are small. In the
figure, the leftmost bars in each grouping represent the on-off source throughput when its
average sending rate is only half of Rfair, in which
case all three algorithms allocate the bandwidth
fairly—that is, they supply the flow its request
bandwidth. As the plot shows, however, as the
on-off flow becomes burstier and sends above
2Rfair, AFD-FT starts penalizing it. The burstier the flow, the more severe the penalty. Conversely, AFD-SB and AFD-NFT do not
penalize flows for burstiness.
Figure 8 (next page) represents a simulation
in which the traffic mix is one user datagram
protocol (UDP) source sharing the link with
seven groups (five flows per group) of TCP
flows with different congestion control meth-

NPSi (average
no. of packets)
Simulation
ami
data
0.222
0.207
0.444
0.411
0.888
0.808
1.778
1.646
2.667
2.453

NPi (average
no. of packets)
Simulation
mi
data
3.70
3.63
7.41
7.42
14.81
14.82
29.63
29.66
44.44
44.53

Ns
(no. of sets)
Simulation
N(as)−1
data

180

167

400
350
300
Throughput (Kbps)

Flow group
ID
0
1
2
3
4

pS
Simulation
data
0.098
0.100
0.100
0.100
0.100

0.5 × Rfair
1 × Rfair
2 × Rfair
4 × Rfair
8 × Rfair

250
200
150
100
50
0
AFD-FT

AFD-SB

AFD-NFT

Figure 7. Performance of the three AFD algorithms in the presence of a
bursty on-off source. As a flow becomes burstier, AFD-FT begins to penalize it, while AFD-SB and AFD-NFT do not.

ods. For generalized window control mechanisms, the window increase has a form of w +
c1w−k, and the decrease of a form w − c2wl,10
where k, l, and w are parameters defined in the
TCP algorithm. The seven groups in the simulation have different values of c1, c2, k, l, and
RTTs, as tabulated in Table 2. The normal TCP
flow has the form c1 = 1.0, c2 = 0.5, k = 1.0, and
l = 1.0). In Figure 8a, the rightmost bars in each
grouping represent the throughput of the more
aggressive UDP flow under the different algorithms. Results show once again that the AFDNFT design can mimic the performance of
AFD-SB while AFD-FT fails to do so.

Comparable performance
Removing the UDP flow from the simulation leaves only TCP flows with various con-

JANUARY–FEBRUARY 2003

41

HOT INTERCONNECTS 10

gestion parameters to compete against each
other. As Figure 8b shows, AFD-NFT performs as well as AFD-FT, and all three AFD
designs allocate bandwidth fairly.
AFD behaves reasonably well when flows
with different RTTs share a link.9 To ensure
TCP flow group 0
TCP flow group 1
TCP flow group 2
TCP flow group 3

TCP flow group 4
TCP flow group 5
TCP flow group 6
UDP flow

Memory requirement

Throughput (Kbps)

400

300

200

100

(a)

0

AFD-FT

AFD-SB

AFD-NFT

AFD-FT

AFD-SB

AFD-NFT

Throughput (Kbps)

400

300

200

100

(b)

0

Figure 8. Mixed TCP traffic with and without a UDP flow. Under AFD-FT, the
UDP traffic is not treated fairly (a). When TCP flows compete only against
each other, however, all three AFD algorithms allocate bandwidth fairly (b).

Table 2. Mixed TCP traffic configuration.
Flow group ID
0
1
2
3
4
5
6

42

that AFD-NFT’s performance is not worse,
we construct another experiment. In this simulation, we separate flows into four groups
with 10 flows in each group. The RTTs (propagation delay only) are 37.5 ms, 75 ms, 112.5
ms, and 150 ms. Figure 9 shows that AFDNFT’s performance is similar to that of AFDSB and AFD-FT. Although some discrepancies
among flows with different RTTs exist, they
are not significant.

c1
1.00
0.75
2.00
1.50
1.00
1.00
1.00

IEEE MICRO

c2
0.90
0.31
0.50
1.00
0.50
0.50
0.50

K
1.00
1.00
1.00
2.00
0.00
1.00
1.00

L
1.00
1.00
1.00
0.00
1.00
1.00
1.00

RTT (ms)
25
25
25
25
25
25
100

As we have shown, AFD-NFT provides reasonably fair bandwidth allocation, and all
operations on the forwarding path are O(1).
Thus, the main question regarding whether
AFD-NFT is practical lies in its memory
requirement. Because set S is small (usually
fewer than 10 flows), registers can easily store
the set’s flow IDs. The flow table, however,
requires some memory buffering.
The flow table size relates directly to number of flows N in the shadow buffer. In the
traces we have seen,9 N is typically less than
one-fourth of b, the number of packets in the
logical shadow buffer. We also find that to
achieve good performance, b should be roughly 10rfair−1R. Hence, N equals 2.5rfair−1R. It is
difficult to estimate rfair on a typical Internet
link. A conservative estimate assumes rfair
equals 56 Kbps, the slow telephone modem
speed. Then, for a 1-Gbps link, it is simple to
obtain that N is on the order of a few thousand. Therefore, we can easily implement the
flow table using a standard hash table or
CAM. The memory overhead is limited.

F

air bandwidth allocation designs differ in
the extent to which they carefully manage
bandwidth allocation. The extremes of the
spectrum are complete fairness (FQ) on one
end and unmanaged allocation (RED) on the
other. We believe that Internet flows will use a
wide variety of congestion-control algorithms,
which might not be TCP compatible. Routers
will thus frequently need to allocate bandwidth
to flows. We designed AFD for such a scenario.
AFD approximates FQ’s performance, but
drastically reduces its state requirement. This
and other data suggest that AFD-NFT will
provide a good approximation to fair bandwidth allocation in a wide range of scenarios,
typically providing bandwidth allocations

within ±15 percent of the fair share.

MICRO
37.5 ms
75 ms
112.5 ms
150 ms

References

Rong Pan is a research scientist at Stanford
University. Her research interests include congestion control, active queue management,
TCP performance, and efficient network simulation. Pan has a PhD in electrical engineering from Stanford University.
Balaji Prabhakar is an assistant professor of
electrical engineering and computer science at
Stanford University. His research interests
include network algorithms, wireless networks,

300
250
Throughput (Kbps)

1. S. Floyd and K. Fall, “Promoting the Use of
End-to-End Congestion Control in the Internet,” IEEE/ACM Trans. Networking, no. 4,
Aug. 1999, pp. 458-472.
2. B. Braden et al., “Recommendations on
Queue Management and Congestion Avoidance in the Internet,” Internet Eng. Task
Force RFC 2309 (informational), Apr. 1998;
www.ietf.org/rfc/rfc2309.txt.
3. A. Demers, S. Keshav, and S. Shenker,
“Analysis and Simulation of a Fair Queuing
Algorithm,” J. Internetworking Research
and Experience, Oct. 1990, pp. 3-26.
4. P. McKenny, “Stochastic Fairness Queuing,” Proc. Infocom 1990, IEEE Press, 1990,
pp. 733-740.
5. I. Stoica, S. Shenker, and H. Zhang, “CoreStateless Fair Queueing: Achieving Approximately Fair Bandwidth Allocations in
High-Speed Networks,” Proc. ACM SIGComm 1998, ACM Press, 1998, pp. 118-130.
6. D. Lin and R. Morris, “Dynamics of Random
Early Detection,” Proc. ACM SIGComm
1997, ACM Press, 1997, pp. 127-137.
7. W. Feng et al., “Stochastic Fair Blue: A
Queue Management Algorithm for Enforcing Fairness,” Proc. Infocom 2001, IEEE
Press, 2001, pp. 1520-1529.
8. S. Floyd and V. Jacobson, “Random Early
Detection Gateways for Congestion Avoidance,” IEEE/ACM Trans. Networking, vol. 1,
no. 4, Aug. 1993, pp. 397-413.
9. R. Pan et al., “Approximate Fairness through
Differential Dropping,” to appear in Computer Comm. Rev., vol. 33, no. 1, Jan. 2003.
10. D. Bansal and H. Balakrishnan, “Binomial Congestion Control Algorithms,” Proc. Infocom
2001, IEEE Press, 2001, pp. 631-640.

200
150
100
50
0

AFD-FT

AFD-SB

AFD-NFT

Figure 9. Four TCP flow groups with different RTTs (maximum is 150 ms).
All three algorithms perform similarly.

Web caching, network pricing, information
theory, and stochastic network theory. Prabhakar has a PhD from the University of California at Los Angeles.
Lee Breslau heads the Internetworking Research
Department AT&T Labs—Research in Menlo
Park, California. His research interests include
packet scheduling, real-time service, network
measurement, and routing. Breslau has a PhD
from the University of Southern California.
Scott Shenker is group leader at the International Computer Science Institute’s Center for
Internet Research. His research interests range
from computer performance modeling and
computer networks to game theory and economics. Shenker has a PhD from the University of Chicago.

Direct questions and comments about this
article to Rong Pan, Stanford University,
Packard Rm. 270, 350 Serra Mall, Stanford,
CA 94305-9510; rong@stanford.edu.
For further information on this or any other
computing topic, visit our Digital Library at
http://computer.org/publications/dlib.

JANUARY–FEBRUARY 2003

43

J Intell Inf Syst (2015) 45:95–112
DOI 10.1007/s10844-013-0262-7

KIPTC: a kernel information propagation
tag clustering algorithm
Guandong Xu · Yu Zong · Ping Jin ·
Rong Pan · Zongda Wu

Received: 22 June 2012 / Revised: 23 June 2013 / Accepted: 1 July 2013 /
Published online: 19 July 2013
© Springer Science+Business Media New York 2013

Abstract In the social annotation systems, users annotate digital data sources by
using tags which are freely chosen textual descriptions. Tags are used to index,
annotate and retrieve resource as an additional metadata of resource. Poor retrieval
performance remains a major challenge of most social annotation systems resulting
from several problems of ambiguity, redundancy and less semantic nature of tags.
Clustering is a useful tool to handle these problems in social annotation systems. In
this paper, we propose a novel tag clustering algorithm based on kernel information
propagation. This approach makes use of the kernel density estimation of the kNN
neighborhood directed graph as a start to reveal the prestige rank of tags in tagging
data. The random walk with restart algorithm is then employed to determine the
center points of tag clusters. The main strength of the proposed approach is the
capability of partitioning tags from the perspective of tag prestige rank rather than

G. Xu
Advanced Analytics Institute, University of Technology Sydney, Sydney, Australia
e-mail: guandong.xu@vu.edu.au
Y. Zong
Department of Computer Science and Technology,
University of Science and Technology of China, Hefei, Anhui, China
e-mail: Nick.zongy@gmail.com
P. Jin (B)
Department of Information and Engineering, West Anhui University, Luan, China
e-mail: Jinping@wxc.edu.cn
R. Pan
Department of Computer Science, Aalborg University, Aalborg, Denmark
e-mail: Rpan@cs.aau.dk
Z. Wu
Oujiang College, Wenzhou University, Wenzhou, China
e-mail: zongda1983@163.com

96

J Intell Inf Syst (2015) 45:95–112

the intuitive similarity calculation itself. Experimental studies on the six real world
data sets demonstrate the effectiveness and superiority of the proposed method
against other state-of-the-art clustering approaches in terms of various evaluation
metrics.
Keywords Social tagging systems · Tag clustering · Kernel information propagation

1 Introduction
In past years the emergence of Web 2.0 applications has created a new era for sharing
and organizing documents in online social communities. The shared documents could
range diversely from the social bookmarks Del.icio.us1 to scientific publications on
CiteUlike2 . One common characteristic from these kinds of documents possessing is
the phenomenon of Folksonomy—users could choose their own free style terms (i.e.
tags) to annotate various documents indicating their own perceptions or conceptual
judgments on these resources for the better indexing and annotation. In other words,
Tag, as a kind of specific lexical information that is user-generated metadata with
uncontrolled vocabulary, plays a crucial role in such social collaborative systems.
In addition to keywords or terms contained in the documents, tagging provides a
complementary feature for web documents modeling the user objective opinions
or comments, which could be used for information retrieval and recommendation.
Apart from the current use in social media, annotation techniques have also been
broadly applied in online service systems, e.g., embedding knowledge-management
methods into web systems to improve context-aware web services (Cuzzocrea and
Mastroianni 2003; Cuzzocrea 2006). Hence, the research on annotation characteristics, pattern and use has attracted a large amount of interest and possess the potential
to be extended to other related areas..
Recently tagging has been widely used in recommender systems for many applications (Durao and Dolog 2010; Tso-Sutter et al. 2008). The common usage of
tags in these systems is to add the tagging attribute as an additional feature to
re-model users or resources over the tag vector space, and in turn, making tagbased recommendation or personalized recommendation. However, as the tags are
of syntactic nature, in a free style and do not reflect sufficient semantics, the problems
of redundancy, ambiguity and less semantics are often incurred in all kinds of social
tagging recommender systems. For example, for a same resource, different users
have their own preference that means they would like to use their own words to
describe their feelings, such as “favorite, preference, like” or even the plural form of
“favorites”; and another obstacle is that not all users are willing to annotate the tags,
resulting in the problems of sparseness, redundancy and ambiguity of tags. In order
to deal with these difficulties, recently clustering method has been introduced into
social tagging recommender systems to find meaningful topic information conveyed
by tag aggregates. The aim of tag clustering is to reveal the coherence of tags from the

1 www.delicious.com
2 www.citeulike.org

J Intell Inf Syst (2015) 45:95–112

97

perspective of how resources are annotated and how users annotate in the tagging
behaviors. Undoubtedly, the tag cluster is able to deliver user tagging interest or
resource topic information in a more concise and semantic way, which handles above
problems to a certain extent, in turn, facilitating the tag-based recommender systems.
Thus this demand mainly motivates the research of tag clustering in social annotation
systems.
In the context of tag clustering, most of the research are directly using the
traditional clustering algorithms such as K-means (Noll and Meinel 2007) or Hierarchical Agglomerative Clustering (Shepitsen et al. 2008) on tagging data, which are
designed to deal with the spheric-shaped structure based on the mutual distances
between subjects. For data distributed in arbitrary shape such as the “S” shape or
oval clusters, it would be difficult for these approaches to accurately differentiate
these arbitrary clusters. In tag clustering, since each tag is co-occurred with other
semantically-relevant tags during user annotation activities and the latent topics of
tags are largely overlapped and mixed due to the free style of tagging data, we
can’t expect tags are scattered spherically and evenly in data space, resulting in the
difficulty of simply applying the above clustering algorithms . As an alternative,
density-based clustering methods are able to identify the core data subjects (i.e.,
cluster centers) with the locally highest probability density, in order to find clusters.
Hence, in this work we aim to particularly utilize the density measure to capture
the core central nodes in tagging space, in turn assigning the associative tags to
these core tags to form the corresponding clusters. Specially, various tags used in
the tagging data apparently have different significance in tag clusters due to the
semantic or domain topic tendency of tags. Some tags with the distinctive semantic
meaning, e.g. tag “image” should be distributed close to the central region of tag
cluster of “photograph”, while some other tags possessing a broad and diverse topic
relatedness to any other tag clusters, e.g. “blue”, are always scattered around the
outer boundary of aggregated tag clusters, i.e., in the region with lower probability
density. In other words, we need to treat these tags differently according to their
density estimates. This intuition inspires the idea proposed in this paper, relying on
the estimated density to differentiate the significance degree of each tag, in turning
completing the clustering.
On the other hand, from the perspective of optimization, simply utilizing the
estimated density alone can only capture the local optimal solution of tag influence
rather than the global optimal information in tagging space. As we discussed above,
social tagging data is always mutually interacted and interrelated, therefore their
significance scores can be transferred and propagated dynamically rather than
statically in an iterative way. Undoubtedly, capturing the global centrality of tags
helps accurately identify the “real” centers of tag clusters. Having these thoughts
in mind, we propose a multi-step solution to integrate the individual significance of
each tag in tagging space with significance propagation for clustering tags. The key
idea behind our approach is the propagated centrality (or prestige) of tag derived
from the tag neighborhood graph does reveal the significance of tags contributing to
tag clustering. In particular, we devise a new Kernel Information Propagation Tag
Clustering (KIPTC) algorithm to obtain the centrality (i.e. prestige) degree of each
tag via a random walk process.
In KIPTC, We first construct a kNN neighborhood directed graph of tags, which
is and estimate the kernel density of each tag in its neighborhood; we then use the

98

J Intell Inf Syst (2015) 45:95–112

accumulated kernel density of backlink nodes to define the local prestige of each tag;
thirdly, we execute the Random Walk with Restart (RWR) algorithm to repeatedly
propagate and update the prestige rank scores over the kNN directed graph until
they are converged (i.e. global prestige scores). Finally, by utilizing the calculated
prestige scores, we choose the core tag with the highest prestige score and conduct a
depth first graph search starting from this tag to include other cluster members until
reaching the marginal tags, and this operation is repeated until all tags are assigned
to corresponding clusters. In order to evaluate the effectiveness of the proposed
approach, we conduct experiments on real tagging data sets. The contributions of
our paper are as follows:
–
–

–

We address the tag clustering problem in social annotation systems via a kernel
density estimate approach.
We propose a new Kernel Information Propagation Tag Clustering algorithm,
in which we define Prestige Rank to capture the importance of tags in the
kNN neighborhood graph, and devise an iterative updating mechanism based
on random walk to identify the global prestige rank.
We conduct comparative experiments on six real world data sets to evaluate the
effectiveness of the proposed algorithm.

The remainder of this paper is organized as follows. We review the related work
in Section 2 and introduce the preliminaries in Section 3. The details of KIPTC
algorithm are discussed in Section 4. Experimental evaluation results are reported
in Section 5. Section 6 concludes this paper and outlines the future work.

2 Related work
In the past years, many studies have been carried out on employing clustering in
social tagging systems (Song et al. 2011; Milicevic et al. 2010). Dattolo et al. present
an approach for detecting groups of similar tags and relationships among them.
The authors find the different categories of related tags by applying a clustering
processes (Dattolo et al. 2011). Deutsch et al. propose clustering tags in a tag cloud
to help enhance user experience, where they present a study on the strengths and
weakness of using tag clouds in common Web-based contexts (Deutsch et al. 2011).
In Karydis et al. (2009), the authors focus on spectral clustering algorithms, working
on a similarity graph that connects every item to its k-Nearest Neighbors (kNN)and
mapping each item to a feature space defined by eigenvectors of the similarity graph.
The authors demonstrate how tag clusters serving as coherent topics can aid in the
social recommendation of search and navigation (Shepitsen et al. 2008). In Hayes and
Avesani (2007), topic relevant partitions are created by clustering resources rather
than tags. By clustering resources, it improves recommendations by distinguishing
from alternative meanings of query. While in Chen and Dumais (2000), clusters of
resources are shown to improve recommendation by categorizing the resources into
topic domains. A framework named Semantic Tag Clustering Search, which is able
to cope with the syntactic and semantic tag variations is proposed in van Dam et
al. (2010). In Vandic et al. (2011), the authors proposed a tag clustering method
to overcome the drawbacks that the existing cloud tagging system are unable to
cope with the syntactic and semantic tag variations during user search and browse

J Intell Inf Syst (2015) 45:95–112

99

activities. In order to improve the efficiency of information retrieval, Kathrin K, et al.
introduce tag co-occuences into the retriveal system by using single-link clustering
algorithm. P. Lehwark et al. use Emergent-Self-Organizing-Maps (ESOM) and
U-Map techniques to visualize and cluster tagged data and discover emergent
structures in collections of music (Lehwark et al. 2008). In Giannakidou et al. (2008),
a co-clustering approach is employed, that exploits joint groups of related tags and
social data resources, in which both social and semantic aspects of tags are considered
simultaneously. For more detailed analysis about tag clustering, please refer to the
lecture (Garcia-Plaza et al. 2012). State-of-the-art methods suffice for simple search,
but they often fail to handle more complicated or noisy high dimensional space due
to the various limitations. Kernel density estimate method is a widely used statistical
approach for non-parameter density estimation in high dimensions. For example,
Liu et al. (2007) has used it in capturing the local characteristics and density distribution in sparse high dimensional space. In this paper, our approach is originated
from the concept of kernel information and is extended to reveal the centrality
of tag in tag aggregates, which is principally different from the above clustering
approach.

3 Preliminaries
3.1 Social tagging system model
In this paper, our work is to deal with the tagging data. A typical social tagging system
has three types of objects, users, tags and resources which are interrelated with one
another. Social tagging data can be viewed as a set of triples, when users want to
annotate web documents for better organization and use the relevant information to
retrieve their needed information later, they often comment such information with
free-text terms. The tags reflect the navigational preference and interest of users.
Likewise, each tagged document also has its own tag expression which expresses the
content relatedness and subject of the document (Pan et al. 2011).
The folksonomy is a three-dimensional data model of social tagging behaviors
of users on various documents. It reveals the mutual relationships between these
three-fold entities, i.e. user, document and tag. A folksonomy F according to Hotho
et al. (2006) is a tuple F = (U, T, R, A), where U is a set of users, T is a set
of tags, R is a set of web resources, and A ⊆ U × T × R is a set of annotations.
The activity in folksonomy is tijk ⊆ {(ui , r j, tk ) : ui ∈ U, r j ∈ R, tk ∈ T}, where U =
{U 1 , U 2 , · · · , U M } is the set of users, R = {R1 , R2 , · · · , R N } is the set of documents,
and T = {T1 , T2 , · · · , T K } is the set of tags. tijk = 1 if there is an annotation (ui , r j, tk );
otherwise tijk = 0.
More detailedly (Guan et al. 2010, 2009), each (u, r, t) represents a user u annotating tag t on resource r. Therefore a social tagging system can be described as a set of
four-tuples, where we denote the data in the social tagging system as D and define it
as D =< U, R, T, A N >. The annotations are represented as a set of triples contains
a user, tag and resource defined as: A N ⊆ u, r, t : u ∈ U, r ∈ R, t ∈ T. Therefore a
social tagging system can be viewed as a tripartite hyper-graph (Mika 2005) with
users, tags and resources represented as nodes and the annotations represented as
hyper-edges connecting users, resources and tags. Figure 1 gives a snapshot of social

100

J Intell Inf Syst (2015) 45:95–112

Fig. 1 An example of
hyper-graph of tagging system

t1

u1

t2

u2

t3
t4

u3

t5

r1
r2
r3
r4

tagging data, from which we can find that u1 has assigned t1 to r1 , t3 to r2 and r4 , as
well as t5 to r3 and r4 . Resource r1 has been annotated by tag t1 , t2 and t4 and so forth.
Since the relations among users, resources and tags are modeled in a triadic
structure, tag could be expressed as either a weight vector over a user space or a
resource space after projecting the tripartite graph into two two-way matrices. The
former reflects the tag characteristics over user common annotations, whereas the
latter indicates the tag affiliation with various resources. In this paper, we compute
tag similarity based on these two types of vector expressions, which will be discussed
in the following section.
3.2 A working example of tag clustering
In the above model, the tag space is usually in a very high dimension due to the free
style of tag texts, which results in the problem of redundancy and ambiguity of tags,
in turn; bringing in the difficulty in tag computing such as the similarity calculation of
tag vectors. Therefore, clustering is often employed to capture the topical aggregates
of tags, i.e. one kind of statistical semantics of tags. In real applications, we usually
decompose the tripartite graph of social tagging data to form a resource-tag matrix
by accumulating the frequency of each tag in the resource vector along users. In this
expression, each tag is described by a set of resources to which this tag has been
assigned, i.e., ti = (wi1 , · · · , wim ), where wik denotes the the occurrence frequency on
resource rk dimension of tag ti . Thus, the similarity between any two tags is defined
as:


Definition 1 Given two tags ti = (wi1 , · · · , wim ) and t j = w j1 , · · · , w jm , the similarity is defined as the Cosine function of angle between two vectors of ti and t j:


Sim ti , t j =

ti · t j
 
ti  · t j

(1)

Upon the mutual tag similarity is determined, various clustering algorithms could
be applied to partition the tags. As we discussed above, tag vector can be expressed

101

J Intell Inf Syst (2015) 45:95–112

tag(resource,resource,resource,resource,...)

resource3

tag2
tag3

resource4

tag4

resource5

tag5

1 0 0 1 1

0.41

1 1 0 0 0

0.41

1

0

0.50

0.58

0

0 1 1 0 0
0 0 0 1 0
0 1 0 1 0

tag1

tags
1

tags

resource2

resources

tag1
tags

resource1

0

0.58

0.41

0

0.50

1

0

0.50

0

1

0.71

0.50

0.41 0.50 0.50 0.71

1

0.58

Cluster 0.41
0.41

tag2

0.250

tag4
0.71

0.50

Cluster

0.50

Topic tag3

tag5

Fig. 2 An example of tag clustering

over either user or resource space, we can compute the tag similarity over user
and resource space separately, and then combine them together equally to form an
overall similarity matrix. For the simplicity reason, Fig. 2 gives a toy working example
by using the tag vectors over resource space alone to partition five tags into two
groups via different clustering strategies (in red or black dashed circles).

4 Kernel information propagation for tag clustering algorithm
If we want to know some other people whom we did not know before in the real
world, how should we realize it? The recommendation from our friends is one of the
commonly used methods. In Web world, users are always using tags to appraise a
resource and other users can accept the resource according to the annotated tags.
This behavior of Web could be regarded as the copy of real world, that is, the social
network. Similarly, the tags could be regarded as the recommendation information.
If we assume that the most similar K tags are the K friends of one tag, we can use the
behavior of social network systems in the real world to simulate the tag’s significant
propagation. More specially, the significance of the tag is reflected by other nodes
surrounding it, i.e., its centrality of the cluster. To determine the centrality, one
common approach is to use the kernel density estimation, which is a nonparametric
density estimation approach from statistics. The general idea behind kernel density
estimation is simple. We treat an observed object as an indicator of high-probability
density in the surrounding region. The probability density at a point depends on
the distances from this point to the observed objects. The estimated kernel density
captures the centrality of nodes within a cluster.
However, the significance information estimated from the above steps only indicates an optimal centrality in a local region. Moreover, tags are mutually influenced
and semantically overlapped, hence the overall significance degrees of tags could be
propagated over the tag graph. This motivates the key idea of our proposed algorithm
to combine kernel density estimation and propagation. In this section, we first use
the kNN neighborhood method to find out the K nearest neighbors of one tag, upon
which a kNN directed graph is constructed. Local information is defined by using
the kernel density estimator method. In order to propagate the local information, we
define operators to transit the local information to all the tags and the prestige degree
of each tag is generated. After the tag with the largest prestige degree is obtained

102

J Intell Inf Syst (2015) 45:95–112

(a)

(b)

Fig. 3 kNN neighborhood graph example

(i.e., cluster center), we assign the associated tags to generate corresponding cluster.
So far most tag clustering approaches are mainly dependent on the co-occurrence
matrix of tagging data, e.g. the occurrence matrix of resource-tag, to partition tags
into various groups. Different from these approaches, our method is to reveal the
global prestige degree of each tag via a graph-based partition manner originated from
the calculation of kernel density distribution. In the follow section, we will discuss
the details of our proposed tag clustering algorithm based on kernel information
propagation.
4.1 kNN directed graph and kernel density
According to Definition 1, a similarity matrix S could be constructed from the tagging
data. From S, we can find kNN neighbors of each tag and then a kNN directed graph
G could be created, i.e. G =< V, E >, where V is the tag set and E is the directed
edge set between tags, < p, q >∈ E denotes that tag q is a kNN neighbor of tag p.
Figure 3a shows an example of a part of graph G with two-fold relationships. In
one fold, the central black point P has five kNN neighboring nodes of heavy black
circle with arches pointing from P to them, reflecting the neighborhood relationships
of P. In the other fold, P is the kNN-neighbor of each light circle nodes with
arches directing from these nodes to P. In this manner a kNN directed graph G is
constructed and its adjacency matrix A of G is defined as:
Definition 2 Given a kNN directed graph G, its adjacency matrix is defined as A,
where A( p, q) = 1, if the directed arch < p, q > exists, and A( p, q) = 0, otherwise.
Figure 3b depicts an example of kNN neighborhood graph corresponding to the
working example shown in Fig. 2.
The construction of kNN directed graph G is based on the similarity between
tags. From the clustering aspect, tags with larger mutual similarity values will be
assigned into the same cluster. The distribution of tags determines the aggregation
information about the main figure of clusters which are embedded in the data set.
In order to capture this aggregation information from the kNN directed graph, we
introduce a kernel density estimate function into our proposed method. The kernel
density estimate method (Liu et al. 2007) has been mainly used in capturing the local

103

J Intell Inf Syst (2015) 45:95–112

characteristics and density distribution in high dimensional space. In the context
of kNN neighborhood graph, particularly the kernel density function indicates the
probability density distribution of similarity function. Theoretically, a kernel can
be regarded as a function modeling the influence of a sample point within its
neighborhood, which is a non-negative real-valued integrable function. In our work,
we use a Gaussian kernel to estimate density based on the given set of objects to be
clustered. Particularly, the estimated kNN kernel density information of each node
represents the local centrality degree of the node in a possible cluster and the number
of arches pointed to the node reflects the “respectful” degree of the node contributed
by its neighbors. Bearing this assumption in mind, in this paper, we intend to adopt
the concept of kernel density of each node, and then define the measure of Local
Prestige (LP) to reflect the local centrality information of each node in the kNN
neighborhood graph.
Definition 3 Given a node p ∈ G and a backlink node set B( p), of which p is in the
kNN neighborhood, the Local Prestige (Centrality) of p is defined as the sum of the
kNN kernel density of B( p):

LP( p) =
f (q)
(2)
q∈B( p)

where f (q) denotes the estimated kNN kernel density of node q.
According to Definition 3, we can see that LP( p) is actually the aggregated kernel
density of supporting nodes which choose node p as their kNN neighbors. Intuitively,
the higher the value of LP( p), the larger centrality the node p possesses within the
cluster and the more likely the node p becomes the centroid of the cluster. Figure 4a
and b illustrate the initial kernel density values and its local prestige degrees of five
nodes in the working example.
4.2 Kernel information propagation for tag clustering algorithm
LP captures the local centrality information of each node in the kNN neighborhood
graph G. Apparently, the prestige score of one node is determined by the number

(a) Kernel Density

(b) Local Prestige

Fig. 4 The kernel information and local prestige of tags in kNN neighborhood graph

104

J Intell Inf Syst (2015) 45:95–112

of nodes pointing to this node and their local prestige scores. Meanwhile, known
from the directed graph theory, the prestige of a node is divided evenly and is
propagated to other nodes that are pointed by it. Hence the node p gets a boost
of its prestige from the nodes that point to p, i.e. the iterative prestige propagation
along the directed arches. Inspired by this thought, we envision a new tag clustering
algorithm based on Kernel Information Propagation (KIPTC). In particular, we first
adopt the Random Walk with Restart (RWR) algorithm (Sun et al. 2005) to deal with
the prestige propagation for global Prestige Rank, and then make use of the global
Prestige Rank to form tag clusters. The whole process consists of two main stages. In
the stage of global prestige propagation, via the random walk along the constructed
kNN neighborhood graph, the global Prestige Rank is calculated as follows:
Definition 4 Given PRi indicating the Prestige Rank of all nodes at i-th iteration
and M denoting the transition matrix of the kNN neighborhood graph, the updated
Prestige Rank at (i + 1)-th step, PRi+1 , is given by:
PRi+1 = α · M · PRi + (1 − α)

(3)

where α is the damping factor which controls the convergence of the algorithm, and
the element of M is determined by A(q, p)/N(q) and N(q) is the total number
of outgoing arches of the node q. The execution of Prestige Rank propagation is
repeated until it converges to a stable status. Below Table 1 gives the Prestige Rank
scores of five nodes of the working example, calculated at α = 0.85. From the table,
we can see the node #5 has the highest prestige score amongst all nodes, indicating
the highest appropriateness of being a centroid within one cluster.
After the global prestige scores are calculated, we then utilize them to determine
the centroids of clusters and includes other cluster members which are closely next
to the centroid. As the whole kNN neighborhood graph is composed by a number of
connected subgraphs, we then conduct the graph traversal operations to identify the
cluster member nodes. The process to generating the clusters is described as: (1) we
select a tag v, v ∈ V, with the highest PR as the starting center of a cluster; (2) we
use the Depth First Search (DFS) method to find the corresponding cluster members
until all nodes within the subgraph containing v are completely searched. After that,
the algorithm turns to locate a new starting center of another cluster. Steps (1) and
(2) are iteratively executed until all the nodes in V are assigned to its corresponding
cluster. Looking back to the working example, by selecting node #5 as the first cluster
center, we include node #1 and #4 as its cluster members via DFS. Then we turn to
choose node #2 as the center of the second cluster and add node #1 as its member,
eventually resulting in two clusters of C1 = 1, 4, 5 and C2 = 2, 3. Below Algorithm 1
gives the pseudo codes of KIPTC algorithm.
In Algorithm 1, three different parts are included: Preparing, Passing and Generating. In Preparing part, which includes step 1–3, a kNN directed graph G is
constructed based on similarity matrix S and the Local Prestige Rank LP of each tag

Table 1 The calculated prestige rank of the working example
Node#

1

2

3

4

5

PR

0.454

0.114

0.108

0.450

0.604

J Intell Inf Syst (2015) 45:95–112

105

Algorithm 1 Kernel information propagation for tag clustering
Input: The tag set V and the neighborhood parameter K
Output: The cluster result C
1 Generate the tag similarity matrix S based on (1), and calculate the kernel
density of each tag;
2 Construct a kNN directed graph based on Definition 2;
3 Calculate the Local Prestige LP of each tag using (2);
4 Iteratively update Prestige Rank via RWR by using (3) until the global PR
scores are obtained;
5
6
7
8
9
10
11

Sort the PR scores in a descending order;
For each unvisited tag v, v ∈ V
Select the tag v with the highest PR score;
Form Cv = DF S(G, v), where Cv denotes a cluster with v as the starting center;
C ← C ∪ Cv ;
Repeat step 6 until all tags are assigned;
Return C.

is calculated. For step 1, the time complexity of the similarity matrix construction is
O(n2 ), and the computational cost of each tag’s kernel density calculation is O(n); In
Step 2, we will construct a kNN directed graph, the time cost for this step is O(n ∗ K);
it needs a complexity of O(n) to generate the Local Prestige for each tag by using (2)
(step 3). For Passing part, the Local Prestige of each tag is passed on G by using RWR
(step 4) and the PR order is calculated in step 5. The time cost of RWR is O(n2 ) and
the calculation of PR order needs at least O(n ∗ logn); For the Generating part(step
6–10), we use a DFS method to find out the clustering result for each unvisited tag,
so the time cost of this part is O(n2 ) at most.
In conclusion, the total time complexity of KIPTC is 3 ∗ O(n2 ) + 2 ∗ O(n) + O(n ∗
K) + O(n ∗ logn).

5 Experimental evaluations
To evaluate our approach, we conduct extensive experiments on six real world
data sets as follow: MedWorm,3 MovieLens,4 DMOZ,5 BibSonomy, MOTOBOY
and Noise Tube.6 We perform the experiments using an Intel Core 2 Duo CPU
(2.4GHz) workstation with a 4G memory, running windows XP. All the algorithms
are implemented in Matlab 7.0.

3 http://www.medworm.com/
4 http://www.movielens.org/
5 http://www.michael-noll.com/dmoz100k06/
6 http://www.tagora-project.eu/data/

106

J Intell Inf Syst (2015) 45:95–112

Table 2 Statistics of experimental data sets
Property

MedWorm MovieLens Dmoz

No. of users
949
No. of resources
261,501
No. of tags
13,507
Avg. tags per user
132
Avg. tags per resource
5

4,009
7,601
16,529
11
9

BibSonomy MOTOBOY Noise tube

5,016 2,600
13,771 18,000
25,311 6,000
123
142
11
15

15
8,000
712
48
9

10
8,000
400
40
7

5.1 Experimental data sets
The first data set is extracted from the crawled MedWorm article repository during
April 2010. After stemming out the entity attributes from the data, four data files,
namely user, resource, tags and quads, are obtained as the source data sets. Here we
only use the fourth data, which presents the social annotations where for each row, it
denotes a user u annotates resource r by using tag t. The second data set is MovieLens
which is provided by GroupLens.7 This data set consists of three files—movies.dat,
rating.dat and tags.dat. The tags.dat has the same data format as the quads data in
MedWorm data set, and we utilize it to conduct the experiments. The dmoz100k06
is a large research data set about document metadata based on a random sample
of 100,000 web documents from the Open Directory combined with data retrieved
from Delicious.com/ Yahoo!, Google, and ICRA. For our study, we use the tagging
data available at del.icio.us, one of the most popular social bookmarking services.
The data from del.icio.us was collected over a period of three weeks in December
2006. This dataset contains 13,771 documents with 25,311 tags by 5,016 users. This
dataset only demonstrated for each document which tags are annotated by a given
number of users, but did not indicate by which users. To complement the user tagging
information, we randomly choose the given number users from total user set and
treat them as tagging persons to simulate the annotations. To provide the Consortium
with raw data for modeling and analyzing interactions in online social communities,
European Commission offers a benchmark data set from the collaborative tagging
system BibSonomy under the TAGora project. The data set consists of over 2.6
thousand users, 181 thousand bookmarks, 219 thousand publications, and over 816
thousand tag assignments. In this paper, we randomly select 18,000 bookmarks and
6,000 tags for experiments. The fifth data set is from the canal*MOTOBOY project,
which involves a small-scale community using tags to represent and communicate
their daily life experiences and has been made available to the TAGora consortium.
It contains over 8,000 resources, 712 tags and 15 users. The Noise Tube data set was
gathered in April 2009 and it currently contains 10 users, 8,000 resource and 400 tags.
These six data sets are pre-processed to filter out some noisy and extremely sparse
data subjects to increase the data quality. The statistical results of these six data sets
are listed in Table 2.

7 http://www.grouplens.org/

107

J Intell Inf Syst (2015) 45:95–112

5.2 Evaluation metrics
Evaluation of clustering results can be considered as a cluster validation. There
have been several metrics for measuring the similarity between two clusterings. Such
a measure can be used to compare how well different data clustering algorithms
perform on a set of data. These measures are usually tied to the type of criterion
being considered in assessing the quality of a clustering method.
When a clustering result is evaluated based on the data that was clustered itself,
this is called internal evaluation. These methods usually assign the best score to
the algorithm that produces clusters with high similarity within a cluster and low
similarity between clusters.
Since in our experiments the six real world data sets used don’t have class labels
and external benchmarks, so we prefer to internal valuations as our standard metrics.
The first metric is call Davies-Bouldin index, which is defined as follow:
Definition 5 Given a tag cluster set C = {C1 , . . . , C|C| }, the Davies-Bouldin index
can be calculated by the following formula.


σ k + σ k
1 |C|
k = 1, ..., |C|
max
(4)
DB =
k=1 k
=k
|C|
d(ck , ck )
where |C| is the number of clusters, cx is the centroid of cluster x, σx is the average
distance of all elements in cluster x to the centroid cx , and d(ck , ck ) is the distance
between centroids ck and ck .
Since algorithms that produce clusters with low intra-cluster distances (high intracluster similarity) and high inter-cluster distances (low inter-cluster similarity) will
have a low Davies-Bouldin index, the clustering algorithm that produces a collection
of clusters with the smallest Davies-Bouldin index is considered as the best algorithm
based on this criterion (Davies and Bouldin 1979).
The second used metric, Dunn index (Dunn 1974) aims to identify dense and
well-separated clusters. It is defined as the ratio between the minimal inter-cluster
distance to maximal intra-cluster distance as follows:
Definition 6 Given a tag cluster set C = {C1 , . . . , C|C| }, the Dunn index can be
calculated by the following formula.
⎫⎫
⎧
⎧
⎨
⎨ d(k, k ) ⎬⎬
D = min
(5)
min
1≤k≤|C| ⎩1≤k ≤|C| ⎩ max d (l) ⎭⎭
1≤l≤|C|



where d(k, k ) represents the distance between clusters k and k , and d (l) measures
the intra-cluster distance of cluster l.
Since internal criterion seeks clusters with high intra-cluster similarity and low intercluster similarity, algorithms that produce clusters with high Dunn index are more
desirable.
The Silhouette metric validates the clustering performance based on the pairwise
difference of between and within-cluster distances (Rousseeuw 1987). In particular,
we define Silhouette metric based on Similarity and Dissimilarity.

108

J Intell Inf Syst (2015) 45:95–112

Definition 7 Given a tag cluster set C = {C1 , . . . , C|C| }, the Similarity and Dissimilarity are defined as:
Similarity(C) =

1 |C| 2 · Sim(ti , t j)
, ti , t j ∈ Ck
k=1 |Ck | · (|Ck | − 1)
|C|

Dissimilarity(C) =
where Dissim(k) =
tags.

|C|
k=1

Dissim(k)
1 |C|
k=1 |Ck | · (|T| − |Ck |)
|C|

(6)

(7)

Sim(ti , t j), ti ∈ Ck , t j ∈
/ Ck , and |T| is the total number of

Definition 8 Given the Similarity and Dissimilarity of cluster C, its Silhouette measure is defined as:
Silhouette (C) = 1 −

Dissimilarity (C)
Similarity (C)

(8)

Obviously, the higher the silhouette value, the better the clustering quality is.
5.3 Experiments and discussions
Since the prestige score of each tag has a close relationship with the kernel density
which depends on the kNN neighborhood graph, we first intend to study the impact
of K parameter selection on prestige score. In order to present this relationship, we
manually choose thirty tags form two clusters of MedWorm data set and calculate
the prestige scores with varying K. The prestige scores of tags are shown in Fig. 5
with K being 4, 8, and 12, respectively.
From Fig. 5, we can see that the prestige score distributions of three K settings
behave similarly except the difference in absolute magnitudes. These change trends
indicate that the setting of K has little influence on tag prestige distribution patterns,

Fig. 5 The changes of tag prestige scores with varying K

109

J Intell Inf Syst (2015) 45:95–112
Table 3 Comparison of Silhouette on six data sets with varying K
K

MedWorm

MovieLens

Dmoz

BibSonomy

MOTOBOY

Noise tube

K=4
K=8
K = 12

0.9752
0.9757
0.9749

0.4809
0.4908
0.4746

0.7497
0.7364
0.7475

0.9125
0.9098
0.9115

0.8324
0.8295
0.8304

0.8564
0.8596
0.8499

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

1
K-means
DMK-means
Single-link
pKwikCluster
KIPTC

MovieLens

Dmoz
Data Sets

BibSonomy

Silhouette value

Silhouette value

in turn, affecting the forming of clusters. In the following part we will further conduct
experiments to evaluate the tag cluster’s quality with varying K.
Table 3 gives the comparisons in terms of Silhouette on Medworm, MovieLens,
Dmoz, BibSonomy, MOTOBOY and Noise Tube data sets, respectively. From
Table 3, we can first find that the cluster results on these six data sets are very
close under different K settings. This observation validates our previous experiment
of the relationship between tag’s prestige and various K settings. Interestingly, the
clustering results derived from Medworm look better than that of other five data
sets, which might be due to the fact that tags used in Medworm data set is focused
on a more specialized medical domain, while the domain topics of MovieLens,
Delicious, BibSonomy, MOTOBOY and Noise Tube datasets span more diversely.
From Table 3, we also can find that the clustering result of MovieLens data set is
the worst one. In our opinion, the reason for this phenomenon is that the sparsity of
MovieLens data set is the main cause of it.
In order to evaluate the effectiveness of the proposed method, we also implement the traditional clustering algorithm, namely K-means, Single-link, DMK-means
(ASB and HK 2012) and an efficient graph clustering algorithm, pKwikCluster
(Kollios et al. 2011) on these six real world data sets as well. For K-means, Singlelink and DMK-means algorithms, we directly run them on the similarity matrices of
those six data sets to generate the clustering results. Because the pKwikCluster is a
graph clustering algorithm, we run it on the kNN neighborhood directed graph G of
each data set to capture the clustering results. We use the described three evaluation
metrics to evaluate the efficient of five comparison clustering algorithm.
The experimental results are shown in Figs. 6, 7, 8. For each figure, there are
two parts: the left one shows the experimental results of five clustering algorithms
on MovieLens, Dmoz and BibSonomy real world data sets, on the other hand, the
right one gives the experimental results of five clustering algorithms on MedWorm,
MOTOBOY and Noise Tube data sets.
According to the Definition 5, 6 and 8, the lower the value of DB, the higher
the quality of clustering results is, while on the contrary, the bigger the value of D

0.8

K-means

0.6

DMK-means
Single-link

0.4

pKwikCluster

0.2

KIPTC

0

MedWorm

MOTOBOY
Data Sets

Noise Tube

Fig. 6 The Silhouette value comparison of five clustering algorithms on six data sets

J Intell Inf Syst (2015) 45:95–112
1

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

K-means
DMK-means
Single-link
pKwikCluster

D value

D value

110

KIPTC

0.8

K-means

0.6

DMK-means
Single-link

0.4

pKwikCluster
KIPTC

0.2
0

MovieLens

Dmoz
Data Sets

BibSonomy

MedWorm

MOTOBOY
Data Sets

Noise Tube

Fig. 7 The D value comparison of five clustering algorithms on six data sets

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

K-means
DMK-means
Single-link
pKwikCluster
KIPTC
MovieLens

Dmoz
BibSonomy
Data Sets

DB value

DB value

and Silhouette, the better the quality of clustering results is. From the comparison
results of Figures, we can find that the quality of clustering results obtained by
KIPTC is consistently better than those of K-means, DMK-means, Single-link and
pKwikCluster in terms of Silhouette, DB and D measure. This finding concludes
that KIPTC algorithm has shown the superiority of finding better clustering results
than K-means, DMK-means, Single-link and pKwikCluster. The reason for this is
probably due to that our proposed algorithm is able to, not only capture the local
prestige information of tags in the kNN neighborhood directed graph G, but also
obtain the global centrality of tags via prestige information propagation. By making
use of the global prestige rank, the KIPTC algorithm can effectively locate the more
appropriate cluster centers and form the meaningful tag clusters accordingly. By
contrast, K-means, DMK-means and Single-link clustering algorithms mainly rely
on the similarity matrix and suffers from the sensitivity problem of initializations.
Because pKwikCluster is a graph based clustering algorithm and it has ability to
capture the local information of the kNN neighborhood directed graph G, so the
quality of clustering results of pKwikCluster is better than those of K-means, DMKmeans and Single-link. However, the quality of clustering results of pKwikCluster is
still worse than that of KIPTC. In our opinion, the reason for the phenomenon is
that our proposed algorithm can capture not only the local information but also the
global information at the same time.
From Figs. 6–8, we can also see that the clustering results of K-means, DMKmeans and Single-link are in a similar poor range. The reason for this is probably
because the both traditional similarity-based clustering algorithms have the same
inevitable drawbacks in dealing with high-dimensional and massive data sets. The
clustering results of the MovieLen data set (the most sparse data set) by five
compared clustering algorithms are relative lower than those of other five data sets,
especially when using K-means, DMK-means and Single-link. This fact indicates that

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

K-means
DMK-means
Single-link
pKwikCluster
KIPTC
MedWorm

MOTOBOY Noise Tube
Data Sets

Fig. 8 The DB value comparison of five clustering algorithms on six data sets

J Intell Inf Syst (2015) 45:95–112

111

these three clustering algorithms are apparently effected by the sparsity of data sets.
By contrast, our proposed method outperforms K-means, DMK-means, Single-link
and pKwikCluster algorithms on all sparse or less sparse data sets.

6 Conclusion
Tag clustering is one of the most useful methods to find out interest tag clusters
embedded in tagging datasets and it has potential in improving the performance of
tag-based recommender systems. In this paper, we propose a novel tag clustering
based on kernel information propagation via random walk on graph.
We first use the kNN neighborhood directed graph and Kernel density estimate
method to find out the local prestige information of each tag, and then, employ the
random walk with restart algorithm to iteratively propagate the prestige rank until
convergence. At last, we use the prestige scores of tags to locate the appropriate
cluster center and conduct graph traversal search to include cluster members.
Experimental results conducted on six real world datasets have demonstrated the
effectiveness and advantage of the proposed method in comparison to other four
traditional clustering approaches. The future work can be conducted along the
directions of comparisons to more state-of-the-art clustering algorithms.
Acknowledgements The work described in this paper was supported by grants from Natural
Science Foundation of China (Grant No. 60775037, 61202171), the Key Program of National Natural
Science Foundation of China (Grant No. 60933013), the Nature Science Research of Anhui (Grant
No. 1208085MF95), the Nature Science Foundation of Anhui Education Department(Grant No.
KJ2012A273 and KJ2012A274), the China Postdoctoral Science Foundation funded project (No.
2012M521251). and the EU FP7 ICT project M-Eco: Medical Ecosystem Personalized Eventbased Surveillance (No. 247829). The authors would like to thank the reviewers for their valuable
comments.

References
Chen, H., & Dumais, S. (2000). Bringing order to the web: automatically categorizing search results.
In Proceedings of the SIGCHI conference on human factors in computing systems. ACM (pp.
145–152).
Cuzzocrea, A. (2006). Combining multidimensional user models and knowledge representation and
management techniques for making web services knowledge-aware. Web Intelligence and Agent
Systems, 4(3), 289–312.
Cuzzocrea, A., & Mastroianni, C. (2003). A reference architecture for knowledge management-based
web systems. In Proceedings of the fourth international conference on web information systems
engineering, 2003. WISE 2003. IEEE (pp. 347–351).
Dattolo, A., Eynard, D., Mazzola, L. (2011). An integrated approach to discover tag semantics.
In Proceedings of the 2011 ACM symposium on applied computing (pp. 814–820).
Davies, D.L., & Bouldin, D.W. (1979). A cluster separation measure. IEEE Transactions on Pattern
Analysis and Machine Intelligence PAMI, 1(2), 224–227.
Deutsch, S., Schrammel, J., Tscheligi, M. (2011). Comparing different layouts of tag clouds: findings
on visual perception. Human Aspects of Visualization, 2011(6431), 23–37.
Dunn, J.C. (1974). Well separated clusters and optimal fuzzy-partitions. Journal of Cybernetics,
4, 95–104.
Durao, F., & Dolog, P. (2010). Extending a hybrid tag-based recommender system with personalization. In SAC ’10: Proceedings of the 2010 ACM symposium on applied computing. New York,
NY, USA: ACM (pp. 1723–1727). doi:10.1145/1774088.1774457.

112

J Intell Inf Syst (2015) 45:95–112

Garcia-Plaza, A.P., Zubiaga, A., Fresno, V., Martinez, R. (2012). Reorganizing clouds: a study on tag
clustering and evaluation. Expert Systems with Application, 39, 9483–9493
Giannakidou, E., Koutsonikola, V., Vakali, A., Kompatsiaris, Y. (2008). Co-clustering tags and social
data sources. In The ninth international conference on web-age information management. IEEE
(pp. 317–324).
Guan, Z., Bu, J., Mei, Q., Chen, C., Wang, C. (2009). Personalized tag recommendation using graphbased ranking on multi-type interrelated objects. In Proceedings of the 32nd international ACM
SIGIR conference on research and development in information retrieval. ACM (pp. 540–547).
Guan, Z., Wang, C., Bu, J., Chen, C., Yang, K., Cai, D., He, X. (2010). Document recommendation
in social tagging services. In Proceedings of the 19th international conference on World wide web.
ACM (pp. 391–400).
Hayes, C., & Avesani, P. (2007). Using tags and clustering to identify topic-relevant blogs.
In International conference on weblogs and social media.
Hotho, A., Jäschke, R., Schmitz, C., Stumme, G. (2006). Folkrank: a ranking algorithm for folksonomies. In LWA (pp. 111–114).
Karydis, I., Nanopoulos, A., Gabriel, H.H., Spiliopoulou, M. (2009). Tag-aware spectral clustering
of music items. In Proceedings of the 10th international society for music information retrieval
conference, ISMIR 2009 (pp. 159–164).
Kollios, G., Potamias, M., Terzi, E. (2011). Clustering large probabilistic graphs. IEEE Transactions
on Knowledge and Data Engineering, 99, 1–13.
Lehwark, P., Risi, S., Ultsch, A. (2008). Visualization and clustering of tagged music data. In Data
analysis, machine learning and applications (XI) (pp. 673–680). Heidelberg; Springer.
Liu, H., Lafferty, J., Wasserman, L. (2007). Sparse nonparametric density estimation in high dimensions using the rodeo. In Proceedings of the eleventh international conference on artif icial
intelligence and statistics. San Juan, Puerto Rico.
Mika, P. (2005). Ontologies are us: a unified model of social networks and semantics. The Semantic
Web–ISWC, 5(1), 522–536.
Milicevic, A.K., Nanopoulos, A., Ivanovic, M. (2010). Social tagging in recommender systems:
a survey of the state-of-the-art and possible extensions. Artif icial Intelligence Review, 33(3),
187–209.
Noll, M.G., & Meinel, C. (2007). Web search personalization via social bookmarking and tagging. In
Proceedings of the 6th international the semantic web and 2nd Asian conference on Asian semantic
web conference, ISWC’07/ASWC’07. Springer-Verlag, Berlin, Heidelberg (pp. 367–380).
Pan, R., Xu, G., Dolog, P. (2011). Improving recommendations in tag-based systems with spectral clustering of tag neighbors. In Proceedings of The 3rd FTRA international conference on
computer science and its applications (CSA-2011): computer science and convergence, CSA’11.
Lecture notes in electrical engineering, vol. 114, Part 1 (pp. 355–364).
Rousseeuw, P. (1987). Silhouettes: a graphical aid to the interpretation and validation of cluster
analysis. Journal of Computational and Applied Mathematics, 20(1), 53–65.
Shafeeq, A., & Hareesha, K.S. (2012). Dynamic clustering of data with modified k-means algorithm.
In Proceedings of the 2012 conference on information and computer networks (pp. 221–225).
Shepitsen, A., Gemmell, J., Mobasher, B., Burke, R. (2008). Personalized recommendation in social
tagging systems using hierarchical clustering. In Proceedings of the 2008 ACM conference on
recommender systems. ACM (pp. 259–266)
Song, Y., Zhang, L., Giles, C.L. (2011). Automatic tag recommendation algorithms for social recommender systems. ACM Transactions on the Web (TWEB), 5(1), 4.
Sun, J., Qu, H., Chakrabarti, D., Faloutsos, C. (2005). Neighborhood formation and anomaly detection in bipartite graphs. In ICDM (pp. 418–425).
Tso-Sutter, K.H.L., Marinho, L.B., Schmidt-Thieme, L. (2008). Tag-aware recommender systems
by fusion of collaborative filtering algorithms. In SAC ’08: Proceedings of the 2008 ACM
symposium on applied computing. New York, NY, USA: ACM (pp. 1995–1999). doi:10.1145/
1363686.1364171.
van Dam, J., Vandic, D., Hogenboom, F., Frasincar, F. (2010). Searching and browsing tag spaces
using the semantic tag clustering search framework. In IEEE fourth international conference on
semantic computing, (ICSC) 2010. IEEE (pp. 436–439).
Vandic, D., van Dam, J.W., Hogenboom, F., Frasincar, F. (2011). A semantic clustering-based
approach for searching and browsing tag spaces. In Proceedings of the 2011 ACM symposium
on applied computing (pp. 1693–1699).

2008 Eighth IEEE International Conference on Data Mining

One-Class Collaborative Filtering
Rong Pan1

Yunhong Zhou2
Bin Cao3
Nathan N. Liu3
Rajan Lukose1
Martin Scholz1 Qiang Yang3
1.
HP Labs, 1501 Page Mill Rd, Palo Alto, CA, 94304, US
{rong.pan,rajan.lukose,scholz}@hp.com
2
Rocket Fuel Inc. Redwood Shores, CA 94065 yzhou@rocketfuelinc.com
3.
Hong Kong University of Science & Technology, Clear Water Bay, Kowloon, Hong Kong
{caobin, nliu, qyang}@cse.ust.hk

zon.com1 , DVDs at Netflix2 , News by Google 3 etc.
The central technique used in these systems is collaborative filtering (CF) which aims at predicting the preference of items for a particular user based on the items
previously rated by all users. The rating expressed in
different scores (such as a 1-5 scale in Netflix) can be explicitly given by users in many of these systems. However, in many more situations, it also can be implicitly
expressed by users’ behaviors such as click or not-click
and bookmark or not-bookmark. These forms of implicit ratings are more common and easier to obtain.
Although the advantages are clear, a drawback of
implicit rating, especially in situations of data sparsity, is that it is hard to identify representative negative
examples. All of the negative examples and missing
positive examples are mixed together and cannot be
distinguished. We refer to collaborative filtering with
only positive examples given as One-Class Collaborative Filtering (OCCF). OCCF occurs in different scenarios with two examples as follows.

Abstract
Many applications of collaborative filtering (CF),
such as news item recommendation and bookmark recommendation, are most naturally thought of as oneclass collaborative filtering (OCCF) problems. In these
problems, the training data usually consist simply of binary data reflecting a user’s action or inaction, such as
page visitation in the case of news item recommendation or webpage bookmarking in the bookmarking scenario. Usually this kind of data are extremely sparse
(a small fraction are positive examples), therefore ambiguity arises in the interpretation of the non-positive
examples. Negative examples and unlabeled positive examples are mixed together and we are typically unable
to distinguish them. For example, we cannot really attribute a user not bookmarking a page to a lack of interest or lack of awareness of the page. Previous research
addressing this one-class problem only considered it as
a classification task. In this paper, we consider the oneclass problem under the CF setting. We propose two
frameworks to tackle OCCF. One is based on weighted
low rank approximation; the other is based on negative
example sampling. The experimental results show that
our approaches significantly outperform the baselines.

1

• Social Bookmarks: Social bookmarks are very
popular in Web 2.0 services such as del.icio.us. In
such a system, each user bookmarks a set of webpages which can be regarded as positive examples
of the user’s interests. But two possible explanations can be made for the behavior that a user did
not bookmark a webpage. The first one is, the
page is of the users’ interest but she did not see
the page before; the second one is the user had
seen this page but it is not of her interest. We
cannot assume all the pages not in his bookmarks
are negative examples. Similar examples include
social annotation, etc.

Introduction

Personalized services are becoming increasingly indispensable on the Web, ranging from providing search
results to product recommendation. Examples of
such systems include recommending products at Ama-

1550-4786/08 $25.00 © 2008 IEEE
DOI 10.1109/ICDM.2008.16

1 http://www.amazon.com
2 http://www.netflix.com
3 http://news.google.com

502

• Clickthrough History: Clickthrough data are
widely used for personalized search and search result improvement. Usually a triple < u, q, p >
indicates a user u submitted a query q and clicked
a page p. It is common that pages that have not
been clicked on are not collected. Similar to the
bookmark example, we cannot judge whether the
page is not clicked because of the irrelevance of its
content or redundancy, for example.

study various weighting and sampling approaches using
several real world datasets. Our proposed solutions
significantly outperform the two extremes (AMAN and
AMAU) in OCCF problems, with at least 8% improvement over the best baseline approaches in our experiments. In addition, we show empirically that these two
proposed solution frameworks (weighting and sampling
based) for OCCF have almost identical performance.
The rest of the paper is organized as follows. In
the next section, we review previous works related to
the OCCF problems. In Section 3, we propose two approaches for OCCF problems. In Section 4, we empirically compare our methods to some baselines on two
real world data sets. Finally, we conclude the paper
and give some future works.

There are several intuitive strategies to attack this
problem. One approach is to label negative examples
to convert the data into a classical CF problem. But
this is very expensive or even intractable because the
users generating the preference data will not bear the
burden. In fact, users rarely supply the ratings needed
by traditional learning algorithms, specifically not negative examples [23]. Moreover, based on some user
studies [14], if a customer is asked to provide many
positive and negative examples before the system performs well, she would get a bad impression of it, and
may decline to use the system. Another common solution is to treat all the missing data as negative examples. Empirically, this solution works well (see Section
4.6). The drawback is that it biases the recommendation results because some of the missing data might
be positive. On the other hand, if we treat missing as
unknown, that is, ignore all the missing examples and
utilize the positive examples only and then feed it into
CF algorithms that only model non-missing data (as
in [24]), a trivial solution arising from this approach
is that all the predictions on missing values are positive examples. All missing as negative (AMAN) and
all missing as unknown (AMAU) are therefore two extreme strategies in OCCF.
In this paper, we consider how to balance the extent of treating missing values as negative examples.
We propose two possible approaches to OCCF. These
methods allow us to tune the tradeoff in the interpretation of so-called negative examples and actually result in better performing CF algorithms overall. The
first approach is based on weighted low rank approximation [24]. The second is based on negative example sampling. They both utilize the information contained in unknown data and correct the bias of treating them as negative examples. While the weightingbased approach solves the problem deterministically,
the sampling-based method approximates the exact solution with much lower computational costs for large
scale sparse datasets.
Our contributions are summarized as follows. First
we propose two possible frameworks for the one-class
collaborative filtering problem and provide and characterize their implementations; second, we empirically

2
2.1

Related Works
Collaborative Filtering

In the past, many researchers have explored collaborative filtering (CF) from different aspects ranging from
improving the performance of algorithms to incorporating more resources from heterogeneous data sources [1].
However, previous research on collaborative filtering
still assumes that we have positive (high rating) as well
as negative (low rating) examples. In the non-binary
case, items are rated using scoring schemes. Most previous work focuses on this problem setting. In all the
CF problems, there are a lot of examples whose rating is missing. In [2] and [19], the authors discuss the
issue of modeling the distribution of missing values in
collaborative filtering problems. Both of them cannot
handle the case where negative examples are absent.
In the binary case, each example is either positive or
negative. Das et al. [8] studied news recommendation,
while a click on a news story is a positive example, and
a non-click indicates a negative example. The authors
compare some practical methods on this large scale binary CF problem. KDD Cup 2007 hosted a “Who rated
What” recommendation task while the training data
are the same as the Netflix prize dataset (with rating).
The winner team [15] proposed a hybrid method combining both SVD and popularity using binary training
data.

2.2

One-class Classification

Algorithms for learning from positive-only data have
been proposed for binary classification problems. Some
research addresses problems where only examples of
the positive class are available [22] (refer to one-class

503

3.1

classification) where others also utilize unlabeled examples [16]. For one-class SVMs [22], the model is describing the single class and is learned only from positive
examples. This approach is similar to density estimation [4]. When unlabeled data are available, a strategy to solve one-class classification problems is to use
EM-like algorithms to iteratively predict the negative
examples and learn the classifier [28, 17, 26]. In [9],
Denis show that function classes learnable under the
statistical query model are also learnable from positive
and unlabeled examples if each positive example is left
unlabeled with a constant probability.
The difference between our research and previous
studies on learning from one-class data is that they aim
at learning one single concept with positive examples.
In this paper, we are exploring collaboratively learning
many concepts in a social network.

2.3

Suppose we have m users and n items and the previous viewing information stored in a matrix R. The
element of R takes value 1, which represents a positive
example, or ‘?’, which indicates an unknown (missing)
positive or negative example. Our task is to identify potential positive examples from the missing data based
on R. We refer to it as One-Class Collaborative Filtering (OCCF). Note that, in this paper, we assume
that we have no additional information about users
and items besides R. In this paper, we use bold capital letters to denote a matrix. Given a matrix A, Aij
represents its element, Ai. indicates the i-th row of A,
A.j symbolizes the j-th column of A, and AT stands
for the transpose of A.

3.2

Class Imbalance Problem

wALS for OCCF

Our first approach to tackle the one-class collaborative filtering problem is based on a weighted low-rank
approximation [11, 24] technique. In [24], weighted
low-rank approximations (wLRA) is applied to a CF
problem with a naive weighting scheme assigning “1” to
observed examples and “0” to missing (unobserved) values, which corresponds to the AMAU. Another naive
method for OCCF is to treat all missing values as negative examples. However, because there are positive
examples in missing values, this treatment can make
mistakes. We address this issue by using low weights
on the error terms. Next, we propose the weighted alternating least squares (wALS) for OCCF. We further
discuss various weighting schemes different from naive
schemes AMAU ([24]) and AMAN.
Given a matrix R = (Rij )m×n ∈ {0, 1}m×n with m
users and n items and a corresponding non-negative
weight matrix W = (Wij )m×n ∈ Rm×n
, weighted low+
rank approximation aims at approximating R with a
low rank matrix X = (Xij )m×n minimizing the objective of a weighted Frobenius loss function as follows.
X
2
Wij (Rij − Xij ) .
(1)
L (X) =

Our work is also related to the class imbalance problem which typically occurs in classification tasks with
more instances of some classes than others. The oneclass problem can be regarded as one extreme case of
a class imbalance problem. Two strategies are used
for solving the class imbalance problem. One is at the
data level. The idea is to use sampling to re-balance
the data [3] [18]. Another one is at the algorithmic
level where cost-sensitive learning is used [10] [27]. A
comparison of the two strategies can be found in [20].

3

Problem Definition

Weighting & Sampling based Approaches

As discussed above, AMAN and AMAU (no missing
as negative) are two general strategies for collaborative
filtering, which can be considered to be two extremes.
We will argue that there can be some methods in between that can outperform the two strategies in OCCF
problems; examples include “all missing as weak negative” or “some missing as negative”. In this section, we
introduce two different approaches to address the issue
of one-class collaborative filtering. They both balance
between the strategies of missing as negative and missing as unknown. The first method uses weighted low
rank approximations [24]. The idea is to give different weights to the error terms of positive examples and
negative examples in the objective function; the second one is to sample some missing values as negative
examples based on some sampling strategies. We first
formulate the problem and introduce the main notation
in this paper. In the next two subsections, we discuss
the two solutions in details.

ij

In the above objective function L (X) (Eq. (1)),
2
(Rij − Xij ) is the common square error term often
seen in low-rank approximations, and Wij reflects the
contribution of minimizing the term to the overall objective L (X). In OCCF, we set Rij = 1 for positive
examples; for missing values, we posit that most of
them are negative examples. We replace all the missing values with zeros. As we have high confidence on
the observed positive examples where Rij = 1, we set

504

Algorithm 1 Weighted Alternating Least Squares
(wALS)
Require: data matrix R, weight matrix W , rank d
Ensure: Matrices U and V with ranks of d
Initialize V
repeat
Update U i. , ∀i with Eq. (6)
Update V j. , ∀j with Eq. (7)
until convergence.
return U and V

the corresponding weights Wij to 1. In contrast, different from the simple treatment of missing as negative,
we lower the weights on “negative” examples. Generally
we set Wij ∈ [0, 1] where Rij = 0. Before discussing
the weighting schemes for“negative” examples, we show
how to solve the optimization problem argminX L (X)
effectively and efficiently.
Consider the decomposition X = U V T where U ∈
m×d
R
and V ∈ Rn×d . Note that usually the number
of features d ≪ r where r ≈ min (m, n) is the rank
of the matrix R. Then we can re-write the objective
function (Eq. (1)) 3 as Eq. (2)
2

X
Wij Rij − U i. V Tj. .
(2)
L (U , V ) =

gi. ∈ Rn×n is a diagonal matrix with the elewhere W
ments of W i. on the diagonal, and I is a d × d identity
matrix.
,V )
= 0, we have
Fixing V and solving ∂L(U
∂U i.

ij

To prevent overfitting, one can append a regularization
term to the objective function L (Eq. (2)):
2

X
Wij Rij − U i. V Tj.
L (U , V ) =

gi. V
U i. = Ri. W

or
L (U , V )

=

X
ij

Wij



Rij − U i. V Tj.

2

+ λ kU i. k2F + kV j. k2F

(3)



gi. V + λ
V TW

X

j Wij

 −1
,
I

∀1 ≤ i ≤ m. (6)


gi. V + λ P Wij I is
Notice that the matrix V T W
j
strictly positive definite, thus invertible. It is not difgi. V
ficult to prove that without regularization, V T W
can be a degenerate matrix which is not invertible.
Similarly, given a fixed U , we can solve V as follows.
X

 −1
g.j U + λ
g.j U U T W
V j. = RT.j W
,
i Wij I

ij


+λ kU k2F + kV k2F ,



. (4)

In Eq. (3) and Eq. (4), k.kF denotes the Frobenius
norm and λ is the regularization parameter which, in
practical problems, is determined with cross-validation.
Note that Eq. (4) subsumes the special case of regularized low-rank approximation in [21, 30]. Zhou et
al. [30] show that the alternating least squares (ALS)
[11] approach is efficient for solving these low rank approximation problems. In this paper, we extend this
approach to weighted ALS (wALS). Now we focus on
minimizing the objective function L (Eq. (4)) to illustrate how wALS works.
Taking partial derivatives of L with respect to each
entry of U and V , we obtain


1 ∂L (U , V ) X
T
=
j Wij U i. V j. − Rij Vjk
2 ∂Uik
X

+λ
(5)
j Wij Uik , ∀1 ≤ i ≤ m, 1 ≤ k ≤ d.

∀1 ≤ j ≤ n, (7)

g.j ∈ Rm×m is a diagonal matrix with the
where W
elements of W .j on the diagonal.
Based on Eq. (6) and Eq. (7), we propose the following iterative algorithm for wLRA with regularization
(based on Eq. (4)). We first initialize the matrix V
with Gaussian random numbers with zero mean and
small standard deviation (we use 0.01 in our experiments). Next, we update the matrix U as per Eq. (6)
and then update the matrix V based on Eq. (7). We
repeat these iterative update procedures until convergence. We summarize the above process in Algorithm
1 which we refer to as Weighted Alternating Least
Squares (wALS). Note that for the objective function
Eq. (3) with a uniform
term, we only

P regularization
P
need to change both
i Wij )
j Wij in Eq. (6) and (
in Eq. (7) to 1.

Then we have
1 ∂L (U , V )
2 ∂U i.


1 ∂L (U , V )
∂L (U , V )
=
,...,
2
∂Ui1
∂Uid
X

 
gi. V + λ
g
= U i. V T W
j Wij I − Ri. W i. V ,

3.2.1

Weighting Schemes: Uniform, User Oriented, and Item Oriented

As we discussed above, the matrix W is crucial to the
performance of OCCF. W = 1 is equivalent to the case
of AMAN with the bias discussed above. The basic idea

505

Table 1. Weighting Schemes

Uniform
User-Oriented
Item-Oriented

Pos Examples
Wij = 1
Wij = 1
Wij = 1

Negative Example Sampling

“Neg” Examples
Wij P
=δ
Wij ∝ j Rij
P
Wij ∝ m − i Rij

R

of correcting the bias is to let Wij involve the credibility
of the training data (R) that we use to build a collaborative filtering model.d For positive examples, they
have relative high likeliness to be true. We let Wij = 1
for each pair of (i, j) that Rij = 1. For missing data, it
is very likely that most of them are negative examples.
For instance, in social bookmarking, a user has very
few web pages and tags; for news recommendation, a
user does not read most of the news. That is why previous studies make the AMAN assumption although it
biases the recommendations. However, we notice that
the confidence of missing values being negative is not
as high as of non-missing values being positive. Therefore, essentially, we should give lower weights to the
“negative” examples. The first weighting scheme assumes that a missing data being a negative example
has an equal chance over all users or all items, that is,
it uniformly assign a weight δ ∈ [0, 1] for “negative” examples. The second weighting scheme posits that if a
user has more positive examples, it is more likely that
she does not like the other items, that is, the missing
data for this user is negative with higher probability.
The third weighting scheme assumes that if an item
has fewer positive examples, the missing data for this
item is negative with higher probability. We summarize
these three schemes in Table 1. A parameter for the
three schemes is the ratio of the sum of the positive
example weights to the sum of the negative example
weights. We will discuss the impact of the parameter
in Section 4.6. In the future, we plan to explore more
weighting schemes and learn the weight matrix W iteratively.

3.3

Matrix Reconstruction by ALS

e (1)
R

b (1)
R

e (2)
R

b (2)
R

...

...

e (l)
R

b (l)
R

b
R

Figure 1. A diagram that illustrates an ensemble based on negative exampling sampling
for OCCF

based on negative example sampling for OCCF as
shown in Figure 1. In phase I, we sample negative
examples from missing values. Based on an assumed
e (i)
probability distribution, we generate a new matrix R
including all positive examples in R. In phase II, for
e (i) , we re-construct the rating matrix R
b (i) by a
each R
special version of wALS which we discuss in Section 3.2.
b (i) with equal weights genFinally, we combine all the R
b which approximates R. We refer to
erating a matrix R
this method as sampling ALS Ensemble (sALS-ENS).
3.3.1

Sampling Scheme

Since there are too many negative examples (compared
to positive ones), it is costly and not necessary to learn
the model on all entries of R. The idea of sampling
can help us to solve the OCCF problem. We use a
fast (O (q)) random sampling algorithm [25] to generate
b from the original training data R
new training data R
by negative example sampling given a sampling probab and negative sample size q. As OCCF
bility matrix P
is a class-imbalanced problem, where positive examples
are very sparse, we transfer all positive examples to the
new training set. We then sample negative examples
b and the negative sample
from missing data based on P
size q.
b is an important input. In this
In this algorithm, P
paper, we provide three solutions which correspond to
the following sampling schemes:

Sampling-based ALS for OCCF

As we state above, for one-class CF, a naive strategy
is to assume all missing values to be negative. This
implicit assumption of most of the missing values being
negative is roughly held in most cases. However, the
main drawback here is that the computational costs
are very high when the size of the rating matrix R
is large. wALS has the same issue. We will analyze
its computational complexity in the next subsection.
Another critical issue with the naive strategy is the
imbalanced-class problem discussed in Section 2.3.
In this subsection, we present a stochastic method

1. Uniform Random Sampling: Pbij ∝ 1. All the missing data share the same probability of being sampled as negative examples.
P
2. User-Oriented Sampling: Pbij ∝
i I[Rij = 1],
that is, if a user has viewed more items, those items
that she has not viewed could be negative with
higher probability.
P
3. Item-Oriented Sampling: Pb(i, j) ∝ 1/ j I[Rij =
506

4

1], which means that if an item is viewed by less
users, those users that have not viewed the item
will not view it either.
3.3.2

4.1

Validation Datasets

We use two test datasets to compare our proposed
algorithms with possible baselines. The first dataset,
the Yahoo news dataset, is a news click through record
stream4 . Each record is a user-news pair which consists of the user id and the URL of the Yahoo news
article. After preprocessing to make sure that the
same news always gets the same article id, we have
3158 unique users and 1536 identical news stories. The
second dataset is from a social bookmarking site. It
is crawled from http://del.icio.us. The data contains
246, 436 posts with 3000 users and 2000 tags.

Bagging

After generating a new training matrix by the above
b to approxalgorithm, we can use a low-rank matrix R
e
e
b can
imate R using wALS. Because R is stochastic, R
also be biased and unstable. A practical solution to the
problem is to construct an ensemble. In particular, we
use the bagging technique [6] (Algorithm 2).
Algorithm 2 Bagging Algorithm for OCCF
b ∈ Rm×n , samRequire: matrix R ∈ Rm×n , matrix P
ple size q, number of single predictor ℓ
b
Ensure: Reconstructed matrix R
for i = 1 : ℓ do
fi by negative
Generate a new training matrix R
example sampling (Section 3.3.1 )
ci from R
fi by ALS [30]
Reconstruct R
end for
ℓ
ci
b = 1 PR
R
ℓ i=1
b
return R

3.4

Experiments

4.2

Experiment setting and Evaluation
Measurement

As a most frequently used methodology in machine
learning and data mining, we use cross-validation to
estimate the performance of different algorithms. The
validation datasets are randomly divided into training
and test sets with a 80/20 splitting ratio. The training set contains 80% known positive examples and the
other elements of the matrix are treated as unknown.
The test set includes the other 20% known positive and
all unknown examples. Note that the known positives
in the training set are excluded in the test process.
The intuition of good performance of a method is that
the method has high probabilities to rank known positives over unknown examples most of which are usually
negative examples. We evaluate the performance on
the test set using MAP and half-life utility which will
be discussed below. We repeat the above procedure
20 times and report both mean and standard deviation of the experimental results. The parameters of
our approaches and the baselines are determined by
cross-validation.
MAP (Mean Average Precision) is widely used in
information retrieval for evaluating the ranked documents over a set of queries. We use it in this paper
to assess the overall performance based on precisions
at different recall levels on a test set. It computes the
mean of average precision (AP) over all users in the test
set, where AP is the average of precisions computed at
all positions with a preferred item:
PN
prec (i) × pref (i)
,
(8)
APu = i=1
# of preferred items

Computational Complexity Analysis

Next, we analyze the running time of wALS and
sALS-ENS. Recall that U is a m × d matrix, V is a
n×d matrix, and d ≤ min{m, n}. For wALS,
 each step
of updating U (or M ) takes time O d2 nm (based on
Eqs. 6 and 7). The total running time of wALS is

O d2 nt mn

assuming that it takes nt rounds to stop.
For sALS-ENS similarly, assuming that ALS takes
on-average nt rounds to stop, and the number of NES
predictors is ℓ, then its total running time is

O d2 ℓnt (nr (1 + α) + (m + n) d) .
In practice, nt , ℓ, α are small constants (nt ranges
from 20 to 30, ℓ ≤ 20, and α ≤ 5), and (n + m) d ≤
nr (1 + α). Therefore the running time of wALS
is O(mn), while the running time of sALS-ENS is
O(nr ). Thus sALS-ENS is more scalable to largescale sparse data compared to wALS. To be precise,
 the running time
 ratio of wALS to sALS-ENS
mn
is ℓ(nr (1+α)+(n+m)d)
. When the data is very sparse

nr
mn ≪ 1 , then ALS-ENS takes less time to finish than
wALS; otherwise, wALS is faster.

where i is the position in the rank list, N is the number
of retrieved items, prec (i) is the precision (fractions of
4 We

507

thank “NielsenOnline” for providing the clickstream data.

algorithm, namely the one-class SVM [22] into our pool
of baseline methods. The idea is to create a one-class
SVM classifier for every item, which takes a user’s ratings on the remaining set of items as input features
and predicts if the user’s rating on the target item is
positive or negative. The set of training instances for
a SVM classifier consists of the rating profiles of those
users who have rated the target item, which should
consists of positive examples only in the one-class collaborative filtering setting, which could be used to train
a one-class SVM classifier for each target item.

retrieved items that are preferred by the user) of a cutoff rank list from 1 to i, and pref (i)is a binary indicator
returning 1 if the i-th item is preferred or 0 otherwise.
Half-life Utility (HLU): Breese et al. [5] introduced
a half-life utility [13] (“cfaccuracy” [12]) to estimate
of how likely a user will view/choose an item from a
ranked list, which assumes that the user will view each
consecutive item in the list with an exponential decay
of possibility. A half-life utility over all users in a test
set is defined as in Eq. (9).
P
Ru
(9)
R = 100 P u max ,
R
u u
where Ru is the expected utility of the ranked list for
user u and Rumax is the maximally achievable utility if
all true positive items are at the top of the ranked list.
According to [5], Ru is defined as follows.

4.4

Impact of Number of Features

where δ (j) equals 1 if the item at position j is preferred by the user and 0 otherwise, and β is the half-life
parameter which is set to 5 in this paper, which is the
same as in [5].

Figure 2 shows the impact of the number of features
(parameter d) on SVD and wALS. We can see for SVD,
the performance will first increase and then drop as we
increase the number of features. But for wALS, the
performance is much more stable and keeps increasing. The performance of wALS usually converge at
around 50 features. In our following experiments, we
will use the optimal feature number for SVD (10 for
Yahoo news data and 16 for user-tag data) and wALS
(50).

4.3

4.5

Ru =

X
j

δ (j)
,
2(j−1)(β−1)

(10)

Baselines

We evaluate our approaches weighting/sampling
negative examples by comparing with two categories
of baselines treating all missing as negative (AMAN)
or treating all missing as unknown (AMAU).
4.3.1

In Section 3, we introduced two approaches to
OCCF based on sampling and weighting. For each approach, we proposed three types of schemes, that is uniform, user-oriented and item-oriented. In this section,
we will compare the three schemes for both sampling
and weighting.
Table 2 compares these schemes. Among the weighting schemes, the user-oriented weighting scheme is the
best and the item-oriented weighting scheme is the
worst. The uniform weighting lies in between. This
may be due to the imbalance between the number
of users and the number of items. For the current
datasets, the number of users is much larger than the
number of items.

AMAN

In AMAN settings, most traditional collaborative filtering algorithms can be directly applied. In this
paper, we use several well-known collaborative filtering algorithms combined with the AMAN strategy
as our baselines, which include the alternating least
squares with the missing as negative assumption (ALSAMAN), singular value decomposition (SVD) [29], and
a neighborhood-based approach including user-user
similarity[1] and item-item similarity algorithms[1].
4.3.2

Sampling and Weighting Approaches

4.6

AMAU

Following the AMAU strategy, it is difficult to adapt
traditional collaborative filtering algorithms to obtain
non-trivial solutions, as we discussed in Section 1. In
this case, ranking items by their overall popularity is a
simple but widely used recommendation method. Another possible approach is to convert the one-class collaborative filtering problem into a one-class classification problem. In this paper, we also include one such

Comparison with Baselines

Figure 3 shows the performance comparisons of different methods based on the missing as unknown strategy (Popularity and SVM), methods based on the missing as negative strategy (SVD and ALS-AMAN) and
our proposed methods(wALS,
sALS).
P The x-axes α is
P
defined as α = ( ij∈Rij =0 Wij )/( ij∈Rij =1 Wij ). For
comparison consideration, given α, the negative sampling parameter q in Algorithm 2 (sALS-ENS) is set

508

11

15

10

14

13
9

HLU

MAP

12
8
11

7
10
6

SVD

SVD

9

wALS−Uniform
5

wALS−Uniform

wALS−UserOriented

wALS−UserOriented

8

wALS−ItemOriented
4

0

10

20

30

40

wALS−ItemOriented
50

60

70

80

90

7

100

0

10

20

30

# of Features

40

50

60

70

80

90

100

70

80

90

100

# of Features

14

28

13

26

12

24

HLU

MAP

(a) Yahoo news data

11

10

22

20

SVD

SVD
wALS−Uniform

9

wALS−Uniform

18

wALS−UserOriented

wALS−UserOriented
wALS−ItemOriented

wALS−ItemOriented
8

0

10

20

30

40

50

60

70

80

90

16

100

0

10

20

30

# of Features

40

50

60

# of Features

(b) User-Tag data
Figure 2. Impact of number of features

Uniform
UserOriented
ItemOriented

Yahoo
sALS-ENS
MAP HLU
9.55
13.98
9.80 14.42
9.42
13.76

News
wALS
MAP HLU
9.56
14.05
9.82 14.39
9.41
13.83

User-Tag
sALS-ENS
wALS
MAP
HLU
MAP
HLU
13.33
25.81
13.35
25.89
13.44 25.90 13.48 25.94
13.03
25.32
13.07
25.43

Table 2. Comparisons of different weighting and sampling schemes

509

to α × nr , where nr indicates the number of total positive examples. The baseline popularity will not change
with the parameter α, therefore it is shown in a horizontal line in the figures. The same holds for the baselines SVD and ALS-AMAN. It can be seen from the
figures that our methods outperform all the methods
based on missing as negative and missing as unknown
strategies.
Parameter α controls the proportion of negative examples. As α → 0, the methods are approaching the
AMAU strategies and as α → 1, the methods are approaching the AMAU strategies. We can clearly see
that the best results lie in between. That is to say
both weighting and sampling methods outperform the
baselines. The weighting approach slightly outperform
the sampling approach. But as indicated in Table (3),
the sampling approach is much more efficient when α
is relative small.

α=1
α=2

wALS
904.00
904.00

sALS
32.58
37.45

other weighting and sampling schemes and to identify
the optimal scheme. We also plan to study the relationship between wALS and sALS-ENS theoretically.

References
[1] G. Adomavicius and A. Tuzhilin. Toward the next generation of recommender systems: a survey of the stateof-the-art and possible extensions. TKDE, 17(6):734–
749, 2005.
[2] Y. Azar, A. Fiat, A. R. Karlin, F. McSherry, and
J. Saia. Spectral analysis of data. In STOC, pages
619–626. ACM, 2001.
[3] G. E. A. P. A. Batista, R. C. Prati, and M. C. Monard.
A study of the behavior of several methods for balancing machine learning training data. SIGKDD Explorations, 6(1):20–29, 2004.
[4] S. Ben-David and M. Lindenbaum. Learning distributions by their density levels: a paradigm for learning
without a teacher. J. Comput. Syst. Sci., 55:171–182,
1997.
[5] J. S. Breese, D. Heckerman, and C. M. Kadie. Empirical analysis of predictive algorithms for collaborative
filtering. In UAI, pages 43–52. Morgan Kaufmann,
1998.
[6] L. Breiman. Bagging predictors. Machine Learning,
24(2):123–140, 1996.
[7] N. V. Chawla, N. Japkowicz, and A. Kotcz. Editorial:
special issue on learning from imbalanced data sets.
SIGKDD Explorations, 6:1–6, 2004.
[8] A. Das, M. Datar, A. Garg, and S. Rajaram. Google
news personalization: scalable online collaborative filtering. In WWW, pages 271–280. ACM, 2007.
[9] F. Denis. PAC learning from positive statistical
queries. In ALT, LNCS 1501, pages 112–126, 1998.
[10] C. Drummond and R. Holte. C4.5, class imbalance,
and cost sensitivity: why under-sampling beats oversampling. In Proc. ICML’2003 Workshop on Learning
from Imbalanced Data Sets II, 2003.
[11] K. R. Gabriel and S. Zamir. Lower rank approximation
of matrices by least squares with any choice of weights.
Technometrics, 21(4):489–498, 1979.
[12] D. Heckerman, D. M. Chickering, C. Meek, R. Rounthwaite, and C. M. Kadie. Dependency networks for
inference, collaborative filtering, and data visualization. JMLR, 1:49–75, 2000.
[13] J. L. Herlocker, J. A. Konstan, L. G. Terveen, and
J. Riedl. Evaluating collaborative filtering recommender systems. TOIS, 22(1):5–53, 2004.
[14] D. Kelly and J. Teevan. Implicit feedback for inferring
user preference: a bibliography. SIGIR Forum, 37:18–
28, 2003.
[15] M. Kurucz, A. A. Benczur, T. Kiss, I. Nagy, A. Szabo,
and B. Torma. Who rated what: a combination of
SVD, correlation and frequent sequence mining. In
Proc. KDD Cup and Workshop, 2007.

sALS-ENS
651.67
749.13

Table 3. Runtime (seconds) of wALS, sALS
ans sALS-ENS with 20 sALS combinations
on the Yahoo News data.
We can also see that compared to the AMAU strategies the missing as negative strategy is more effective.
This is because although the label information of unlabeled examples are unknown, we still have the prior
knowledge that most of them are negative examples.
Disregarding such information does not lead to competitive recommendations. This is somewhat different with the conclusion in class imbalance classification
problems where discarding examples of the dominating
class often produces better results [7].

5

Conclusion and Future Work

Motivated by the fact that negative examples are
often absent in many recommender systems, we formulate the problem of one-class collaborative filtering
(OCCF). We show that some simple solutions such as
the “all missing as negative” and “all missing as unknown” strategies both bias the predictions. We correct the bias in two different ways, negative example
weighting and negative example sampling. Experimental results show that our methods outperform state of
the art algorithms on real life data sets including social
bookmarking data from del.icio.us and a Yahoo news
dataset.
For future work, we plan to address the problem how
to determine the parameter α. We also plan to test

510

10

15

9.5
14
9
13

8.5

12

HLU

MAP

8
7.5
7

11

USER_USER

USER_USER
10

ITEM_ITEM

6.5

ITEM_ITEM

SVD
6

SVD

ALS−AMAN
sALS

5.5

sALS

sALS−ENS
5
4.5

ALS−AMAN

9

sALS−ENS

8

wALS−UserOriented
1

2

4

6

8

10

12

14

wALS−UserOriented
7

16

1

2

4

6

8

α

MAP

10

12

14

16

α

POP
3.4528

OCSVM
POP
2.2545
HLU 5.3461
(a) Yahoo news data

OCSVM
2.5958

14
26
13
24
12

22
20

10

HLU

MAP

11

9

USER_USER
ITEM_ITEM

8

18
16

USER_USER
ITEM_ITEM

14

SVD

SVD

ALS−AMAN

7

ALS−AMAN

12

sALS

sALS
10

sALS−ENS

6

wALS−UserOriented
5

1

2

4

6

8

10

12

14

sALS−ENS
wALS−UserOriented

8
16

1

2

4

α

MAP

6

8

10

12

14

16

α

POP
7.315

OCSVM
POP
3.2500
HLU 15.7703
(b) User-Tag data

OCSVM
3.6260

Figure 3. Impact of the ratio of negative examples to positive examples (α).
[16] W. S. Lee and B. Liu. Learning with positive and
unlabeled examples using weighted logistic regression.
In ICML, pages 448–455. AAAI Press, 2003.
[17] B. Liu, Y. Dai, X. Li, W. S. Lee, and P. S. Yu. Building text classifiers using positive and unlabeled examples. In ICDM, pages 179–188. IEEE Computer Society, 2003.
[18] X.-Y. Liu, J. Wu, and Z.-H. Zhou. Exploratory undersampling for class-imbalance learning. In ICDM, pages
965–969. IEEE Computer Society, 2006.
[19] B. Marlin, R. Zemel, S. Roweis, and M. Slaney. Collaborative filtering and the missing at random assumption. In UAI, 2007.
[20] B. Raskutti and A. Kowalczyk. Extreme re-balancing
for svms: a case study. SIGKDD Explorations, 6:60–
69, 2004.
[21] R. Salakhutdinov, A. Mnih, and G. E. Hinton. Restricted boltzmann machines for collaborative filtering. In ICML, pages 791–798. ACM, 2007.
[22] B. Schölkopf, J. C. Platt, J. Shawe-Taylor, A. J.
Smola, and R. C. Williamson. Estimating the support
of a high-dimensional distribution. Neural Computation, 13(7):1443–1471, 2001.

[23] I. Schwab, A. Kobsa, and I. Koychev. Learning user interests through positive examples using content analysis and collaborative filtering, 2001. Internal Memo.
[24] N. Srebro and T. Jaakkola. Weighted low-rank approximations. In ICML, pages 720–727. AAAI Press,
2003.
[25] J. S. Vitter. Faster methods for random sampling.
Commun. ACM, 27(7):703–718, 1984.
[26] G. Ward, T. Hastie, S. Barry, J. Elith, and J. Leathwick. Presence-only data and the EM algorithm. Biometrics, 2008.
[27] G. M. Weiss. Mining with rarity: a unifying framework. SIGKDD Explorations, 6:7–19, 2004.
[28] H. Yu, J. Han, and K. C.-C. Chang. PEBL: positive
example based learning for web page classification using SVM. In KDD, pages 239–248. ACM, 2002.
[29] S. Zhang, W. Wang, J. Ford, F. Makedon, and
J. Pearlman. Using singular value decomposition approximation for collaborative filtering. In CEC, pages
257–264. IEEE Computer Society, 2005.
[30] Y. Zhou, D. Wilkinson, R. Schreiber, and R. Pan.
Large-scale parallel collaborative filtering for the netflix prize. In AAIM, LNCS 5034, pages 337–348, 2008.

511

Mind the Gaps: Weighting the Unknown in
Large-Scale One-Class Collaborative Filtering
Rong Pan

Martin Scholz

HP Labs,
1501 Page Mill Rd
Palo Alto, CA 94304, USA

HP Labs,
1501 Page Mill Rd
Palo Alto, CA 94304, USA

rong.pan@hp.com

scholz@hp.com

ABSTRACT

practice over the last decade. Prominent examples include
Amazon’s product recommender and the Google News website. Those systems leverage a large pool of training observations and rely to a large part on implicit feedback, i.e.,
which user performed which action.
Our work addresses a particularly hard case that emerges
from several real-world prediction tasks: We try to learn
from implicit feedback only, and under the restriction that
each observation is a positive example of e.g., a user’s interest. Examples include (i) tracking which items a user bought
in the past, and trying to predict the current interests, or
(ii) using the (social) bookmarks of a user to predict which
other yet unknown sites she also might like. In both these
cases, the observed events are positive examples of what a
user likes, so all observations belong to a single class.
There are two different strategies for handling such oneclass problems. The first one is to assume that all unobserved events (e.g., sites that were not bookmarked) are
negative examples. This is obviously simplistic, but allows
one to restate the problem in terms of traditional binary
classification. We refer to this strategy as all missing as
negative (AMAN). The second strategy is to explicitly state
that the missing values are unknown, and to turn to probabilistic density estimation techniques that are capable of
working with positives only. We refer to this strategy as
all missing as unknown (AMAU). We recently proposed the
use of instance weighting techniques after substituting zeros for unknown values [13]. This allows us to balance the
extreme cases of AMAN and AMAU: A weight of one after
substitution is identical with the AMAN strategy, whereas
a weight of zero results in an AMAU strategy. Choosing
fractional weights reflects degrees of confidence that missing
values are in fact negatives.
This paper entails an empirical study that covers the major families of algorithms: an AMAU-based probabilistic
model, an AMAN technique based on a low-rank approximation, and another AMAN technique that is based on a
maximum margin approach. Our previous empirical study
was limited by the scalability of algorithms [13]. Since collaborative filtering systems benefit from large user bases, we
mostly focus on scalability aspects and the performance on
large-scale benchmarks in this paper. Our goal is to gain a
better understanding which methods are truly valuable in
practice, and how much we gain by introducing weights. As
a major contribution, we propose two scalable algorithms
that allow to weight missing values in sparse matrices. The
first one is based on low-rank approximations, the second
one extends maximum-margin matrix factorization.

One-Class Collaborative Filtering (OCCF) is a task that
naturally emerges in recommender system settings. Typical
characteristics include: Only positive examples can be observed, classes are highly imbalanced, and the vast majority
of data points are missing. The idea of introducing weights
for missing parts of a matrix has recently been shown to help
in OCCF. While existing weighting approaches mitigate the
first two problems above, a sparsity preserving solution that
would allow to efficiently utilize data sets with e.g., hundred
thousands of users and items has not yet been reported. In
this paper, we study three different collaborative filtering
frameworks: Low-rank matrix approximation, probabilistic
latent semantic analysis, and maximum-margin matrix factorization. We propose two novel algorithms for large-scale
OCCF that allow to weight the unknowns. Our experimental results demonstrate their effectiveness and efficiency on
different problems, including the Netflix Prize data.

Categories and Subject Descriptors
G.1.2 [Approximation]: Least squares approximation; H.2.8
[Database Applications]: Data Mining

General Terms
Algorithms

Keywords
Collaborative Filtering, Large-Scale, One-Class

1.

INTRODUCTION

Collaborative filtering is at the core of modern personalization machinery. The longer-term vision is to be able
to automatically stream the right kind of news to our desks
and to recommend the movies and music we have not yet noticed we desired, without ever having to bother with search
queries. Recommender systems have found their way into

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
KDD’09, June 28–July 1, 2009, Paris, France.
Copyright 2009 ACM 978-1-60558-495-9/09/06 ...$5.00.

667

2.
2.1

LARGE-SCALE OCCF STRATEGIES

Require: data matrix R ∈ Rn×m , rank d
Ensure: Matrices X ∈ Rn×d and Y ∈ Rm×d
Initialize Y randomly
repeat
Update X r , ∀r ∈ {1, . . . , n}
Update Y c , ∀c ∈ {1, . . . , m}
until convergence.
return X and Y

Formal Background

The common goal of techniques discussed in this paper is
to decompose a given n×m matrix R, for example describing
which news articles were read by which users. We investigate the predictive performance of three different families or
tools. The first one (pLSI) is based on a probabilistic model,
the second one (ALS) on low-rank approximations, and the
third one follows the paradigm of maximum-margin classification. Surprisingly, despite those deep semantic differences,
all three methods yield a very similar output: All models
consist of two matrices X = (X)n×d and Y = (Y )m×d that
are chosen so that XY T approximates the original matrix
of observations R. We will discuss the different techniques
in more depth in the following sections.
Regarding notation, we will use bold upper-case letters for
matrices and vectors throughout this work. A matrix with
a single index e.g., Rr denotes the r-th row vector. Individual components are referred to when using two indices, e.g.,
Rr,c denotes the element in row r and column c. For column
vectors we use the notation R·,c . For any matrix R, ||R||2F
denotes the Frobenius norm. The vector 1 denotes the column vector that has 1 as the value of each component. Its
dimensionality can be concluded from the context. Finally,
I refers to the identity matrix.
Let R = (R)n×m be a user-item matrix. As discussed in
the problem statement, R contains only positives, since only
positives are observable. We might represent those as entries
with a value of 1. Unobserved values are not necessarily
negative though, so we try to avoid respresenting them using
any fixed value at this point. Let instead M := {1, . . . , m}
be the set of all column indices of R, that is, the set of
all items, and Mr denote the set of indices of all observed
(hence positive) items in row Rr .

2.2

Algorithm 1: Alternating Least Squares (ALS)
Given the AMAU nature of pLSI, introducing weights into
this framework is only reasonable for non-missing values.
As we will discuss later, we are exclusively looking into the
problem of weighting the missing values in this paper, so we
skip the option of weighting pLSI.

2.3
2.3.1

Least Squares Matrix Decomposition
Regular Alternating Least Squares

The techniques discussed throughout Section 2.3 have their
roots in singular value decomposition (SVD). For a given
matrix R ∈ Rn×m , an SVD is a decomposition of the matrix R = XSY T , where X ∈ Rn×r and Y ∈ Rm×r are orthonormal matrices, r is the rank of R, and S is a diagonal
matrix with the singular values of R on its main diagonal.
When projecting X, S, and Y onto the d ≤ r columns for
which S has the highest singular values, the product of the
three resulting matrices gives the best approximation R̃ of
R of rank d with respect to ||R − R̃||2F .
Unfortunately, the SVD framework loses its convexity if
it encounters missing values. The alternating least squares
algorithm (ALS) provides an efficient search strategy and
handles missing values gracefully, but only guarantees to
find local optima in such cases. Due to its success in the
Netflix Prize (where most of the values are missing) ALS
gained a lot of attention recently. It fits the objective of this
competition well: to optimize for the Frobenius norm.
The variations of ALS discussed in this work all have the
skeleton depicted in Algorithm 1 in common, and only differ
in terms of the loss function and corresponding updates of
X and Y . The algorithm starts with a random matrix Y ,
and then alternates steps of optimizing X for fixed Y and
of optimizing Y for fixed X. Since both these steps are
perfectly symmetric, we will for notational simplicity just
discuss the case of optimizing X throughout this section.
Some additional notation helps describing those updates.
ALS exploits sparsity and handles missing values by projecting matrices and vectors so that missing values in R have
no effect. Let πr denote a function that projects exactly
those components of a column vector into a lower dimensional space for which the indices are not missing in the
vector RTr , that is, it projects the components with index
in Mr into a |Mr |-dimensional subspace. Analogously, for
a matrix Y , let πr (Y ) denote the matrix that results from
projecting each column vector Y ·,c to πr (Y ·,c ). For illustration, if no values are missing in Rr , then

Probabilistic Latent Semantic Indexing

The first model we want to include in our study is called
probabilistic latent semantic indexing (pLSI) [9]. It was used
in the context of the Google News recommender engine [7],
an application that is very close to OCCF.
PLSI is based on a generative model: Each user has a
distribution over interests, and each interest “generates” the
action of picking a particular item with a fixed probability.
Interests are modeled as latent topics z that connect users
and items in a graphical model. The model specifies conditional probabilities P (z|u) for each user u to like / pick each
of the latent topics z, and for each topic z to generate a
particular item i, P (i|z). The assumption is that the choice
of an item is independent of the user given the latent topic.
The probability that user u picks item i simplifies to
X
P (u, i) =
P (i|z)P (z|u)
z

under this assumption. Since pLSI follows a probabilistic
framework, the goal is to find a maximum likelihood model
that best explains the observations. There is a well-known
EM algorithm for this task [9].
The pLSI model and algorithm provide a good example of
an AMAU strategy, because the model operates exclusively
on the positives. The resulting estimates of P (u, i) can be
used to rank items for each user according to the estimated
likelihoods of preference for each item.

πr (Y )T πr (RTr ) = Y T RTr ,
otherwise the multiplication after projection (left hand side)
simply ignores all products containing a missing value.
We use ALS with Tikhonov-regularization (parameter λ)
in this work, as regularization consistently gives better re-

668

sults. We refer to it as “regular ALS”. Its loss function is
`
´
L (X, Y ) = ||R − XY T ||2F + λ ||X||2F + ||Y ||2F .

Table 1: Weighting Schemes
Pos Examples
“Neg” Examples
Uniform
Wi,j = 1
Wi,jP
=δ
User-Oriented
Wi,j = 1
Wi,j ∝ j Ri,j
P
Item-Oriented
Wi,j = 1
Wi,j ∝ m − i Ri,j

When updating X for given R, Y , and λ, each row X r
of X can be updated separately, and there is a closed-form
solution for minimizing the loss:
“
”−1
X r := πr (Y )T πr (Y ) + λI
πr (Y )T πr (RTr )
(1)
Besides having this neat closed form solution, ALS has a few
other pleasant properties worth mentioning, see also [21]:
First, all ALS variants we evaluated had a stable behavior when it came to parameter tuning. We found it to be
substantially less sensitive to parameter settings and more
stable in its performance than e.g., pLSI. Second, the ALS
variant that includes regularization hardly ever overfits, so
by increasing the number of latent features d (the rank of
R̃) ALS tends to monotonically improve its predictive performance, just with diminishing returns for every extra CPU
cycle (additional features). Finally, it is easy to parallelize.

2.3.2

In turn, missing data points are likely to be negative examples. In social bookmarking, for example, each user has
very few web pages and tags; for news recommendation, each
user reads only a small fraction of the available articles. We
observe that the confidence in missing values being negative
is not as high as in non-missing values being positive, which
suggests to assign lower weights to allegedly negative examples. This is addressed by our first weighting scheme. It
uniformly assigns a confidence weight δ ∈ [0, 1] to each “negative” example. The rationale behind the second weighting
scheme is that the available data on some users is poor, so
we should lower our confidence in the data for these users
and not trust missing parts to be negative. Poor information means that we have only a few observations (positive
examples) for a user. The third weighting scheme resembles the popularity-based ranking of items. Here we expect
to have more positives in the missing parts if the items are
more popular. Table 1 summarizes these three schemes. We
plan to investigate more complex weighting schemes in our
future work.
There are two reasons why we do not consider weighting
the non-missing values in this paper. First, the success of
weighting known values inherently depends on the quality
and encoding of background knowledge into weights (e.g.,
[20]); in this case, we do not even trust our very few known
data points after all. Collecting and incorporating background knowledge is an orthogonal problem, that is out of
the scope of this paper. Second, weighting just the nonmissing values for all three matrix decomposition techniques,
PLSA, ALS, and MMMF, does not increase asymptotic runtime complexity. It is straightforward to extend all subsequently proposed techniques in this fashion.

Prior work on ALS for OCCF

For one-class problems, the ALS framework seems a bit
unmotivated at first. ALS provides a solution to a regressionstyle matrix completion problem, but in OCCF we only have
one kind of observed value, and all the remaining values are
missing. However, as a first step we can use the AMAN
substitution and rephrase OCCF as a binary classificationstyle problem, which is a special case of regression with the
response being either 0 or 1. A regularized low-rank approximation will fail to reproduce the matrix exactly, so for each
user we can rank items with originally unknown values with
respect to the predicted values in R̃.
The results in [13] indicate that this approach, which we
refer to as ALS after AMAN substitution, benefits from introducing weights for the originally unknown values of R;
different weighting schemes based on whether a value is
present or missing, and – optionally – based on the individual user and item under consideration improved the predictive power of collaborative filtering models.
A straightforward adaptation of the loss function L above
supports this kind of weighting [16]: If W is the weight
matrix, then we define weighted loss with respect to W as
“
X
LW (X, Y ) :=
Wi,j (Ri,j − Xi YjT )2

2.3.3

i,j

+

λ(||X i ||2F + ||Y j ||2F )

Runtime analysis

Scalability is decisive for practical OCCF applications.
We first look into the complexity of regular ALS. Let in the
subsequent analysis |R| denote the number of non-missing
values in R. Regular ALS can basically “handle” missing values by ignoring them. When updating the r-th row of X according to Eq. (1), ALS only requires the projection πr (Y ),
or equivalently, only the columns of Y T for which the column in the r-th row of R is not missing. In total, the whole
recomputation of X (for all rows together) hence requires to
read |R| rows from Y . In the next step, πr (Y )T πr (Y ) + λI
can trivially be computed in time O(d2 · |Mr |) for each individual row r. This term dominates the previous read operation and the subsequent multiplications with πr (Y )T and
πr (RTr ). In practice, it will usually also dominate the n matrix inversions, although for very sparse matrices or large
d one should keep in mind that the costs per matrix inversion are cubic when using naive implementations, and still
no better than O(d2.376 ) even for the fastest known algorithm [6]. The total costs for updating X (or analogously
Y ) without the matrix inversions are in O(|R| · d2 ), since

´

The problem of updating X to minimize the loss LW still
has a closed-form solution after AMAN substitution [13]:
!−1
X
T g
g
X r := Rr W r Y Y W r Y + λ(
Wr,i )I
,
(2)
i∈M

gr is a n × n diagonal matrix with the weights of
where W
row r on the main diagonal. In our experiments, we will
only consider the three weighting schemes proposed in [13].
The basic idea in all three cases is to let W reflect the
credibility of the training data (R) that we use to build a
collaborative filtering model. Positive examples are likely
indeed positive. Given that the information of this kind is
rare in typical one-class applications, we trust the data in
this case, and set Wij = 1 whenever Rij = 1.

669

the sum of |Mr | over all rows is |R|. Putting together all
costs and assuming a naive matrix inversion we still end up
with an upper bound of only
`
´
O |R| · d2 + (n + m) · d3
(3)

Require: matrices R ∈ Rn×m , Y ∈ Rm×d , U ∈ Rn×|D| ,
V ∈ Rm×|D| , substitute Wi,j = 1 if Ri,j = 1
Ensure: X with minimal loss LW (X, Y ) for fixed Y
/* init */
V P := V T 1
for all D̃ ∈ {1, . . . , |D|} do
b (D̃) ∈ Rd×d with:
create new matrix A
P
(D̃)
b
Aj,k := i∈M Yi,j · Yi,k · Vi,D̃

per iteration when updating both X and Y with regular
ALS. For a fixed number of latent variables d, computational
costs scale linearly with the number of non-missing values.
Regular ALS lacks the good generalization performance
of its gap-weighting counter-part though [13]. The weighting algorithm proposed in [20] is somewhere in between. It
supports weights only for the small set of non-missing values, which does not increase the runtime complexity above.
In order to support weights for missing values we want to
apply methods as described in [13] (see Eq. (2)). Unfortunately, explicitly substituting all missing values with zeros
and weighting the full matrix increases the runtime complexity from O(|R|) to Ω(n · m). Collaborative filtering relies on
a large number of users and items and is performed on sparse
matrices, so this last strategy is impractical.
One work-around that avoids this strategy is an ensemble
technique that runs ALS multiple times [13]. Each iteration
uses only a subsample of negative examples, while ignoring
the (still) missing values just like regular ALS does. This
makes the approach feasible in practice, but (i) it decreases
the amount of negative examples considered during training,
which reduces the expected predictiveness of results, and (ii)
it still increases the runtime considerably compared to the
case of ALS without substituting any examples, since ALS
is run multiple times on expanded data sets.

2.3.4

/* row-wise recomputation of X */
for rows r ∈ {1, . . . , n} do
read non-missing value indices Mr for row r
read rows of Y in Mr
(projection πr (Y ))
b := πr (Y )T πr (Y )
B
qr := πr (Y )T 1
read rows of V in Mr
(projection πr (V ))
ĉ := U r (V P − πr (V )T 1)
for all D̃ ∈ {1, . . . , |D|} do
e (D) ∈ Rd×d with:
create new
P matrix B
(D)
e
Bj,k := i∈Mr Yi,j Yi,k Vi,D
read row-specific weight vector U r
P
b (D)
A0 := D Ur,D · A
P
0
e (D)
b−
B := B
D Ur,D · B
0
C := λ(ĉ + |Mr |) · I
ˆ
˜T
X r := (A0 + B0 + C0 )−1 qr
return X
Algorithm 2: GALS update of X given R and Y when
weighting (only) missing values with W = U V T .

A novel gap-weighting algorithm that scales

We next describe an algorithm that is capable of solving the above-mentioned ALS optimization problem with
weighted missing values very efficiently. It supports all weighting schemes shown in Table 1 at the same asymptotic computational costs as the regular ALS algorithm, see Eq. (3).
In fact, the increase in runtimes compared to regular ALS is
quite small in practice, even in absolute terms. The same algorithm applies to a much larger family of weighting schemes
that can all be incorporated in time linear in the number of
non-missing values.
Before going into detail, it is worth mentioning that the
key property of ALS that enables this kind of optimization
is its closed-form solution for computing optimal updates.
Both pLSI and MMMF lack this property.
As our only constraint, we will assume that the weight matrix for the missing values can be expressed (or well approximated) by a low rank approximation: W = U V T . Positives
automatically receive a weight of 1, regardless of the corresponding value in U V T . All the weighting schemes proposed in [13] are covered as special cases of rank 1. Higher
ranks allow for much more complex weighting schemes that
can e.g., be motivated by domain knowledge or by iteratively
adding latent features as part of a weight learning scheme.
We leave the investigation of such schemes for future work.
To avoid confusion with the latent variables of X and Y ,
referred to by using the summation variable d, we will use
D as the variable for the latent features of U and V , and in
slight abuse of notation also denote the rank of W as |D|.
The novel gap-ALS algorithm (GALS) adapts Algorithm 1,
but uses a much more efficient update rule than Eq. (2). We
just show the case of updating X in Algorithm 2. Updat-

ing Y works analogously. Please refer to the appendix for
a proof that Algorithm 2 optimizes X with respect to LW
when substituting AMAN and setting Wi,j = 1 for positives.
The runtime complexity of Algorithm 2 benefits substanb(D̃) just once during
tially from precomputing the matrices A
j,k
the init-stage. The costs for this step, and for the whole initialization, are in Θ(|D| · d2 · m). The costs for computing
A0 , B 0 , C 0 and qr during each row-wise computation are
low, because the computations are based on projections of
matrices to Mr . These costs are dominated by computing
e (·) , which takes time Θ(|D| · d2 · |Mr |)
the |D| matrices B
for each individual row r, or Θ(|D| · d2 · |R|) for all rows
together. Finally we invert a d × d matrix for each row,
as in regular ALS. The initialization phase is dominated by
the row-wise updates. In the case of a rank 1 weighting,
the asymptotic costs of GALS are identical to those of regular ALS, see Eq. (3). When using more complex weight
matrices W , GALS scales linearly with the rank of W .
We want to conclude the discussion of runtime complexities by pointing out that GALS tends to converge quickly
in practice due to the effective closed form updates. Even
for large datasets, 20 to 30 iterations of updates are usually
sufficient. In contrast, pLSI required hundreds of iterations
in our experiments until convergence.

2.4

Maximum-Margin Matrix Factorization

In this section, we apply the idea of weighting unknowns
into the context of Maximum Margin Matrix Factorization
(MMMF) [17]. Given a matrix R = (Rij )m×n ∈ {±1}m×n
with m users and n items and a corresponding non-negative

670

weight matrix W = (Wi,j )m×n ∈ Rm×n
, weighted low+
rank approximation aims at approximating R with a low
e = (R
ei,j )m×n minimizing the objective of the
rank matrix R
e as follows.
weighted hinge loss and the trace norm of R
‚ ‚
“ ” X
“
”
e‚
e =
ei,j + λ ‚
L R
Wi,j h Ri,j , R
‚R
‚ ,
i,j

c ∈ Rm×n , sample
Require: matrix R ∈ Rm×n , matrix W
size q, number of single predictor `
e
Ensure: Reconstructed matrix R
for i = 1 : ` do
Generate a new training matrix R(i) by NES ([18])
e (i) from R(i) by MMMF-CG
Reconstruct R
e = 1 P̀ R
e (i)
R
` i=1
e
return R
Algorithm 3: sMMMF-ENS Algorithm for OCCF

(4)

Σ

where h(·) is the Smooth Hinge loss function in [17], λ is a
parameter trading off the weighted hinge loss and the margin and k · kΣ is the trace norm of a matrix. In the above
objective function (Eq. (4)), Wi,j reflects the contribution
e
of minimizing the term to the overall objective L(R).
Following the discussion in Subec. 2.3.2, we substitute
negative labels (−1) for missing values and adopt the weighting schemes depicted in Table 1: Positive examples receive
a fixed weight of Wi,j = 1, while the weights of negatives
vary based on our confidence that they are in fact negative.
e we
In order to solve the optimization problem min L(R),
T
e
consider the decomposition R = XY where X ∈ Rm×d
and Y ∈ Rn×d . Note that usually the number of features
d  r where r ≈ min (m, n) is the rank of the matrix R.
We can re-write the objective function (Eq. (4)) as Eq. (5)

In the post-processing step, we compute an unweighted
e (i) that were computed throughout the
average of all the R
e which apdifferent iterations. This yields the final matrix R
proximates R. Bagging is promising in this setting, because
it allows to exploit the stochastic, unstable nature induced
by the underlying boostrap sampling approach. We refer to
Algorithm 3 as sampling MMMF Ensemble (sMMMF-ENS).

3.

EXPERIMENTS

In this section, we empirically evaluate the effectiveness
and efficiency of the algorithms proposed in Section 2. We
first study the predictive performance of all algorithms discussed in this paper on the same (small) datasets that were
used in [13]. Then we study both predictive performance
and algorithmic efficiency on three large-scale datasets. We
use Mean Average Precision (MAP), Half-Life Utility (HLU)
[13] and the Area Under the ROC curve (AUC) as our metrics for the predictiveness of models.
MAP (Mean Average Precision) assesses the overall performance based on precisions at different recall levels on a
test set. It computes the mean of average precision (AP)
over all users in the test set, where AP is the average of
precisions computed at all positions with a preferred item:
PN
i=1 prec (i) × pref (i)
,
(7)
APu =
# of preferred items

L (X, Y ) =
(5)
«
„ “
” 1 `
X
2
2 ´
T
.
+
λ
kX
k
+
kY
k
h
R
,
X
Y
W
i F
j F
i,j
i
i,j
i,j
j
2
Taking partial derivatives of L with respect to each entry of
U and V , we obtain
”
“
X
∂L (X, Y )
T
=
Yj,k
j Wi,j h Ri,j , X i Y j
∂Xi,k
“X
”
+λ
(6)
j Wi,j Xi,k , ∀1 ≤ i ≤ m, 1 ≤ k ≤ d.
As suggested in [15], we apply the Conjugate Gradients
(MMMF-CG) algorithm to solve the optimization problem
min L(X, Y ). Note that the running time for wMMMF with
CG is Θ(m × n) however, if we substitute negatives for all
missing values. Considering the large number of missing values, this is intractable for large-scale problems, and it may
still cause issues related to class-imbalance, otherwise. Unlike for wALS, wMMMF updates do not have a closed-form
solution. It is not clear how to speed up wMMMF in a similar manner as discussed in Subsec. 2.3.4. We hence adapt
the previously discussed idea of subsampling (cf. Sec. 2.3.3).
Algorithm 3 summarizes this method. It combines the
idea of subsampling unknowns as negatives with the bagging technique [5]. The algorithm runs iteratively, and each
iteration i can be broken down into two phases.
In phase I, the algorithm samples a limited number of q
negative examples from missing values, proportionately to
our weight matrix W . It then generates a new matrix R(i)
that includes the sample together with all positives from R
(since positives are rare). For sparse matrices, the number
of negative examples is close to m × n, so care must be
taken when selecting a negative example sampling (NES)
technique to avoid complexities of Ω(m × n). We suggest
using a fast (O (q)) random sampling scheme [18] in these
cases to generate R(i) from R.
In phase II, the algorithm re-constructs the rating matrix
e (i) from R(i) by applying a conjugate gradients algorithm
R
(MMMF-CG) [15].

where i is the position in the rank list, N is the number of retrieved items, prec (i) is the precision (fractions of retrieved
items that are preferred by the user) of a cut-off rank list
from 1 to i, and pref (i) is a binary indicator returning 1 if
the i-th item is preferred or 0, otherwise.
Half-Life Utility (HLU) [4] estimates how likely a user will
view/choose an item from a ranked list, which assumes that
the user will view each consecutive item in the list with an
exponential decay of possibility. A half-life utility over all
users in a test set is defined as in Eq. (8).
P
Ru
R = 100 P u max ,
(8)
u Ru
where Ru is the expected utility of the ranked list for user
u and Rumax is the maximally achievable utility if all true
positive items are at the top of the ranked list. Ru is defined
as follows.
X
δ (j)
,
(9)
Ru =
(j−1)(β−1)
2
j
where δ (j) equals 1 if the item at position j is preferred by
the user and 0 otherwise, and β is the half-life parameter
which is set to 5 in this paper.

671

Table 2: Comparisons of MAP, HLU and AUC on Large-Scale Datasets
newsSmall
newsLarge
Netflix
MAP HLU AUC MAP HLU AUC
MAP
HLU
AUC
POP
0.62
1.09
0.736
0.50
0.90
0.842
8.40
15.52
0.936
SVD
2.57
5.13
0.88
1.97
4.16
0.851
13.28
25.69
0.86
GALS-Uniform 3.28
5.89
0.90
2.37
4.53
0.854
16.82 28.94 0.957
GALS-User
3.28
6.04
0.907
2.38
4.47
0.852 16.87 28.81
0.958
GALS-Item
3.30 6.05 0.908
2.39 4.54 0.849
16.79
28.85
0.958
sMMMF-ENS
2.67
5.24 0.913 1.39
2.85 0.884 11.73
19.04 0.961
pLSI
0.65
0.55
0.89
0.11
0.06
0.78
4.42
7.33
0.862

3.1

Predictive Performances on Small Data

data set contains 59202 unique users and 138, 575 different
news stories. We call this dataset newsLarge in our experiments. We also generated a smaller dataset from newsLarge,
such that each user or each item has at least ten records.
We call this dataset newsSmall. It contains 23, 495 users
and 36, 339 articles. The third dataset is generated from
the Netflix Prize data. It contains about 100 million ratings
with 480, 189 users and 17, 770 movies. The ratings in the
original data ranges from 1 to 5, but we just tried to predict
whether a user rated a movie at all. That is, we treat all
rated user-movie pairs as positive examples, and all pairs
without given ratings as missing values.
We randomly divided each of the three datasets into a
training, a validation and a test set with a 60/20/20 splitting ratio. The training set contains 60% known positive
examples. The other elements of the matrix are treated as
unknown. The validation set includes 20% known positives,
and all the unknown examples for parameter tuning. The
test set includes the remaining 20% known positives and all
unknown examples.
Note that the known positives in the training set are excluded from the test process. We evaluate the performance
on the test set using AUC, MAP, and half-life utility (HLU).
The parameters of all algorithms were determined by the
performance on validation set.

The following experiments compare the predictive performances of GALS, wMMMF, and some baseline algorithms.
For the purpose of comparisons, we use the same datasets as
in [13]. This includes data from the news domain (Yahoo!
News usage) and from the domain of social bookmarking
(delicious.com). For validation purposes, each dataset contains 20 pairs of training and test sets, each with an 80/20
splitting ratio. The Yahoo! data contains 84117 user-article
pairs with 3158 unique users and 1536 different articles. The
delicious.com data contains 246, 436 posts by 3000 users, and
it covers 2000 different tags.
Figure 1 shows results in terms of MAP, HLU and AUC,
averaged over the 20 folds. SVD (with all missing as negative substitution) and Popularity are two baselines also used
in [13]. As expected, wMMMF performs best with respect to
both MAP and HLU and is competitive for AUC, which verifies that the maximum-margin paradigm can also achieve
outstanding performance on classification problems in collaborative filtering setting. GALS yields competitive results
with all three weighting schemes. Surprisingly, pLSI works
poorly for OCCF. One possible reason is that it implicitly
makes the “all missing as unknown” assumption.
30
25

3.2.2

wMMMF

20

wALS-Uniform
15

wALS-User

10

wALS-Item
POP

5

SVD

0
MAP

HLU

AUC

MAP

Yahoo! News

HLU

AUC

pLSI

User-Tag

Figure 1: GALS vs. mMMMF

3.2
3.2.1

Predictive Performances

We conducted experiments on our three large datasets
to demonstrate the utility of the proposed algorithms for
OCCF. The algorithms involved in these experiments include Popularity, SVD (AMAU), pLSI, sMMMF-ENS (Algorithm 3)2 , and GALS with the three weighting schemes
discussed in Section 2.3.2. We fixed the number of features
to 50 throughout these experiments. Table 2 lists the results.
Although sMMMF-ENS achieves highest AUC values, it is
not as good as wMMMF (shown in Figure 1) on MAP and
HLU. In contrast, GALS with different weighting schemes
outperforms other algorithms on all three datasets, which
shows great merit in solving large-scale OCCF problems.
We will also illustrate the advantages of GALS in terms of
time complexity.

Large-Scale OCCF Experiments
Experimental setup

3.2.3

We used three large test datasets to compare our proposed
algorithms to state-of-the-art baseline algorithms. The first
one is a clickstream dataset of News articles1 . Each record is
a user-article pair that consists of a user ID and the URL of
a NYTimes, Washington Post, or Yahoo news article. The

Tests of Computational Costs

We implemented the GALS algorithm in Matlab and Java
to confirm that it gives identical results as the less scalable
counter-part and to study how it scales up as a function
of different variables. The results we report here were gen2

wMMMF is not included because it is intractable for largescale datasets, although it performs well in the above experiments.

1

We thank “NielsenOnline” for providing the clickstream
data.

672

4

CPU−Time (sec)

14

x 10

12
10
8

1000
Update M (new)

Update M

Update U (new)

Update U

800

Update U (old)

newsSmall
newsLarge
Netflix

Update M
Update U

3000

30000

Update M (old)

6
4

600

2000

20000

400

1000

10000

2
0
0

40000

4000

1

2

3

4

(a) Number of Users

5

200
20

40

60

80

(b) Ratio of Non−Missing

5

x 10

100

0
2

4

6

8

(c) Rank of Weight Matrix

10

0

20

40

60

80

(d) Number of Features

100

Figure 3: (a) Changing CPU time for a fixed number of items and a varying number of users for the explicit
full substitution (old) and our new method; (b) Changing CPU time for a fixed matrix if we only vary the
sparsity; (c) Changing CPU time as a function of an increasing rank of the weight matrix; (d) Changing CPU
time as a function of an increasing number of features.
GALS: MAP vs. Iterations

GALS: HLU vs. Iterations

2.45

4.

4.6

Many researchers have explored different aspects of collaborative filtering (CF) in the past, ranging from improving the performance of algorithms to incorporating more resources from heterogeneous data sources [1]. However, collaborative filtering research still assumes that there are explicit positive (high rating) and negative (low rating) examples and the goal is to distinguish between those two, which
is different from the one-class case studied in this paper.
Missing values (e.g., ratings) are a common issue in CF.
In [2] and [12], the authors discuss the issue of modeling
the distribution of missing values. Both approaches cannot
handle cases in which no negative examples are given.
In the binary case, each example is either positive or negative. Das et al. [7] describe a news recommendation problem
in which clicks on news stories are interpreted as positive examples, and “non-clicks” as negative examples. The authors
compare some practical methods on this large scale binary
CF problem, which is close to the one-class case. The KDD
Cup 2007 task was a “Who rated What” prediction problem
based on the Netflix Prize dataset. We included a similar
prediction problem in our experimental study. The winning
team [10] proposed a hybrid method that combined SVD
and popularity using binary training data.
Our work also has some commonalities with the class imbalance literature. Class-imbalance is typically studied for
classification tasks. The two strategies that are commonly
used to solve the problem are sampling to re-balance the
data [3, 11] and cost-sensitive learning [8, 19]. A comparison of the two strategies can be found in [14].

2.4
4.4

2.35
2.3

4.2

2.25
2.2
5
0.12

10

15

20

25

30

pLSI: MAP vs. Iterations

4
5
0.02

0.1

10
15
20
25
pLSI: HLU vs.Iterations

30

0.015

0.08
0.01

0.06
0.04
20

40

60

80

100

0.005
20

40

60

80

RELATED WORK

100

Figure 2: Convergence: GALS vs. pLSI

erated using the Matlab code on the binary (who ranked
what?) Netflix data set. The sparsity of the data is about
1%, there are about 20, 000 movies and 500, 000 users.
Figure 2 shows the convergence of GALS and pLSI. We
can see that GALS converges after roughly 15 to 20 iterations, much earlier than pLSI.
Figure 3 (a) compares the time consumed by the updates
for the old method that performs a full substitution of missing values and weights the whole matrix, and the new GALS
update method. “Updating U” refers to an update of the user
matrix (corresponds to X), while “updating M” corresponds
to the item matrix (Y ). We only evaluated the case of rank
1 weight matrices here. We failed to run the old method for
more than 400, 000 users. It is obvious that the new method
brings incredible savings in terms of CPU costs.
In Figure 3 (b), we fixed the matrix but removed positives
at random. This allows us to increase the sparsity from 1%
positives (right) to 0.1% left. Since we used the full matrix,
we could only run our new method on this set. The plot
confirms the linearity of our method in the number of nonmissing values.
Figure 3 (c) confirms that increasing the rank of the weight
matrix affects the computational costs only linearly.
Finally, Figure 3 (d) confirms that the number of features
has an impact of square order on CPU time as analyzed in
Section 2.3.

5.

CONCLUSIONS AND FUTURE WORK

We studied the question which matrix decomposition techniques work well on large-scale one-class collaborative filtering techniques. This question can be decomposed into
two major parts: Which techniques have a good predictive performance, and how well do the techniques scale up.
Our baseline was a popular probabilistic method, which we
compared to low-rank approximation and maximum-margin
methods. We recently demonstrated that weighting schemes
are very valuable in this area, so we were also concerned with
the question of how to incorporate the ability for weighting
the unknown parts of a matrix.
The results without the scalability aspect were close to
our expectation: The maximum-margin method performed
very well, closely followed by ALS. The only surprise was the

673

poor performance of pLSI. Weighting the gaps clearly helped
to further improve the results for both MMMF and ALS.
To the best of our knowledge, we ran the first experiments
with a weighted MMMF variant. As another theoretical
contribution, we proved that there is an exact, yet scalable
updating mechanism for ALS for large-scale data sets that
allows to weight all the gaps, as long as the corresponding
weight matrix has a good or exact low-rank approximation.
In contrast, at a large scale, MMMF can only make use
of weights via a subsampling / ensemble technique, because
the hinge loss is too complex to allow for adaptations similar
to those we incorporated into ALS. We empirically showed
that the subsampling work-around does not work as well as
using the full set of weighted negatives, so MMMF lost the
edge and the novel GALS algorithm excelled as the method
of choice in our large-scale experiments.
In our future research, we plan to experiment with more
complex weighting schemes and to learn such schemes iteratively.

6.

[13] R. Pan, Y. Zhou, B. Cao, N. N. Liu, R. Lukose,
M. Scholz, and Q. Yang. One-class collaborative
filtering. In IEEE International Conference on Data
Mining (ICDM), 2008.
[14] B. Raskutti and A. Kowalczyk. Extreme re-balancing
for SVMs: a case study. SIGKDD Explorations,
6:60–69, 2004.
[15] J. D. M. Rennie and N. Srebro. Fast maximum margin
matrix factorization for collaborative prediction. In
L. D. Raedt and S. Wrobel, editors, ICML, volume
119 of ACM International Conference Proceeding
Series, pages 713–719. ACM, 2005.
[16] N. Srebro and T. Jaakkola. Weighted low-rank
approximations. In International Conference on
Machine Learning (ICML), 2003.
[17] N. Srebro, J. D. M. Rennie, and T. Jaakkola.
Maximum-margin matrix factorization. In NIPS, 2004.
[18] J. S. Vitter. Faster methods for random sampling.
Commun. ACM, 27(7):703–718, 1984.
[19] G. M. Weiss. Mining with rarity: a unifying
framework. SIGKDD Explorations, 6:7–19, 2004.
[20] C. V. Yehuda Koren, Yifan Hu. Collaborative filtering
for implicit feedback datasets. In IEEE International
Conference on Data Mining (ICDM), 2008.
[21] Y. Zhou, D. Wilkinson, R. Schreiber, and R. Pan.
Large-scale parallel collaborative filtering for the
Netflix Prize. In Proc. of Algorithmic Aspect of
Information and Management (AAIM’08), 2008.

REFERENCES

[1] G. Adomavicius and A. Tuzhilin. Toward the next
generation of recommender systems: a survey of the
state-of-the-art and possible extensions. TKDE,
17(6):734–749, 2005.
[2] Y. Azar, A. Fiat, A. R. Karlin, F. McSherry, and
J. Saia. Spectral analysis of data. In STOC, pages
619–626. ACM, 2001.
[3] G. E. A. P. A. Batista, R. C. Prati, and M. C.
Monard. A study of the behavior of several methods
for balancing machine learning training data.
SIGKDD Explorations, 6(1):20–29, 2004.
[4] J. S. Breese, D. Heckerman, and C. M. Kadie.
Empirical analysis of predictive algorithms for
collaborative filtering. In UAI, pages 43–52. Morgan
Kaufmann, 1998.
[5] L. Breiman. Bagging predictors. Machine Learning,
24(2):123–140, 1996.
[6] D. Coppersmith and S. Winograd. Matrix
multiplication via arithmetic progressions. Journal of
Symbolic Computation, 9:251–280, 1990.
[7] A. Das, M. Datar, A. Garg, and S. Rajaram. Google
news personalization: scalable online collaborative
filtering. In WWW, pages 271–280. ACM, 2007.
[8] C. Drummond and R. Holte. C4.5, class imbalance,
and cost sensitivity: why under-sampling beats
over-sampling. In Proc. ICML’2003 Workshop on
Learning from Imbalanced Data Sets II, 2003.
[9] T. Hofmann. Latent semantic models for collaborative
filtering. ACM Transactions on Information Systems,
22(1):89–115, 2004.
[10] M. Kurucz, A. A. Benczur, T. Kiss, I. Nagy, A. Szabo,
and B. Torma. Who rated what: a combination of
SVD, correlation and frequent sequence mining. In
Proc. KDD Cup and Workshop, 2007.
[11] X.-Y. Liu, J. Wu, and Z.-H. Zhou. Exploratory
under-sampling for class-imbalance learning. In ICDM,
pages 965–969. IEEE Computer Society, 2006.
[12] B. Marlin, R. Zemel, S. Roweis, and M. Slaney.
Collaborative filtering and the missing at random
assumption. In UAI, 2007.

APPENDIX
Correctness of GALS (Algorithm 2).
We start from the objective function and break it down
into parts. For notational simplicity and brevity we omit
some arguments if clear from the context, and define L∗ , A
through C, A0 through C0 , and qr inline.
LW (X, Y )

=

X

LW (X r , Y )

r

LW (X r , Y )

=

X

Wr,i (Xr YiT − Rr,i )2

i∈M

|
+

X

{z

=:L∗ (X r ,Y )

}

Wr,i λ(||X r ||2F + ||Y i ||2F )

i∈M

|

{z

=:C

}

We now simplify the row-wise unregularized loss L∗ (X r , Y ),
exploiting that Rr,i = 1 and Wr,i = 1 whenever i ∈ Mr
(non-missing) and Rr,i = 0, otherwise. Then we decompose
it into a (mostly) global part A and a row-dependent B:

=

L∗ (X r , Y )
X
Wr,i · (X r Y Ti − Rr,i )2
i∈M

=

X
i∈M\Mr

674

U r V Ti (X r Y Ti )2 +

X
i∈Mr

(X r Y Ti − 1)2

X

=

U r V Ti (X r Y Ti )2

The matrix form to be plugged into the equation system is:
X
b −
e (D) , where
B0 := B
Ur,D · B

i∈M

|

{z

X “

+

}

=:A

(X r Y Ti − 1)2 − U r ViT (X r Y Ti )2

D

”

i∈Mr

{z

|

}

=:B

∂A
∂B
∂C
+
+
∂Xr,c
∂Xr,c
∂Xr,c

i∈M

=

2
2

X

Xr,d

d

X
D

:=

πr (Y )T 1

i∈Mr

i∈M\Mr

1 ∂C
2 ∂Xr,c
!
=

Xr,c λ (

X

U rV

T
i )

−(

i∈M

X

U rV

T
i )

+ |Mr |

i∈Mr

!#

"
=

(Yi,c · Ur,D · Vi,D · Xr,d · Yi,d )
X

qr

A trivial multiplication of U and V leads to non-linear costs,
so we instead decompose the derivative

d

Ur,D

:=

d

Xr,c λ |Mr | +

X

Ur,D

D

D i∈M

d

=

D

e (D)
B
c,d

πr (Y )T πr (Y )
X
Yi,c Yi,d Vi,D

The regularization part C can be rewritten as
1
! 0
X 2
X
X
2
T
C =
Xr,d + Yr,d λ @
U rV i +
1A

for all components Xr,c of vector X r to set them to 0. This
we do in the form of a linear equation system, where each
component (value of c ∈ {1, . . . , d}) corresponds to another
line. The equation system will be of the form (A0 + B0r +
C0 )X Tr = qr , where A0 X Tr through C0 X Tr correspond to
the partial derivatives of A through C. We start with A0 .
!
X
X
X
∂A
= 2
Yi,c
(Ur,D · Vi,D )(
Xr,d · Yi,d )
∂Xr,c
i∈M
D
d
!
X
XX
= 2
Yi,c
Ur,D · Vi,D · Xr,d · Yi,d
XX X

:=

i∈Mr

After this decomposition, we are interested in the derivatives
∂L(X r , Y )/∂Xr,c =

b
B

(

X

Vi,D ) − (

i∈M

X

Vi,D )

i∈Mr

and compute the corresponding matrix C0 as

(Yi,c · Yi,d · Vi,D )

i∈M

Based on the inner sum, we define |D| (the rank of the
b (1) , . . . , A
b (|D|) :
weight matrix W ) two-dimensional matrices A
X
b(D̃) :=
A
Yi,c · Yi,d · Vi,D̃
c,d

C0

:=

λ(ĉ + |Mr |) · I, where

ĉ

:=

U r (V P − πr (V )T πr (1))

VP

:=

V T1

Now we can plug in in the partial results used by Algorithm 2
and confirm that (A0 + B0 + C0 )−1 qr in fact solves the linear
equation system we get when setting all partial derivatives
to 0:

i∈M

We can now define A0 by weighting the |D| matrices according to the row-specific weight vector U r :
X
b (D) .
A0 :=
Ur,D · A

X Tr = (A0 + B0 + C0 )−1 qr
⇔ (A0 + B0 + C0 )X Tr = qr
∂B
∂C
∂A
+
+
)=0
⇔ ∀c : (
∂Xr,c
∂Xr,c
∂Xr,c
⇔ ∀c : ∂ [L∗ (X r , Y ) + C] /∂Xr,c = 0
⇔ ∀c : ∂ [LW (X r , Y )] /∂Xr,c = 0

D

B0r ,

We already dropped the factor of 2 that is shared by
C0 , and qr and cancels out later. The partial derivative of
B is:
#
"
”
X “
∂B
T
2
T
T 2
=
∂
(X r Y i − 1) − U r V i (X r Y i )
/∂Xr,c
∂Xr,c
Due to the regularization term, the matrix inversion is ali∈Mr
”
X “
ways well-defined. It can easily be seen that the overall
=
2Yi,c X r Y Ti − 2Yi,c − 2Yi,c U r V Ti X r Y Ti
optimization problem is still convex, so Algorithm 2 comi∈Mr

putes the global optimum for each X r , and hence for X.
q.e.d.

After removing factor of 2,
!
X

Xr,d

X

Yi,c Yi,d − Yi,c Yi,d

i∈Mr

d

X

Ur,D Vi,D

−

D

=

d

1

B
C
B
C
B X
C
X
X
B
C
Xr,d B(
Yi,c Yi,d ) −(
Ur,D
Yi,c Yi,d Vi,D )C
C
B
D
i∈Mr
B i∈Mr
C
{z
}
|
{z
}A
@|
b
=
bB

−

Yi,c

i∈Mr

0

X

X

X

(D)

e
=
bB
c,d

Yi,c

i∈Mr

|

{z

=q
b r

}

675

Reliability Engineering and System Safety 112 (2013) 38–47

Contents lists available at SciVerse ScienceDirect

Reliability Engineering and System Safety
journal homepage: www.elsevier.com/locate/ress

A Bayesian reliability evaluation method with integrated accelerated
degradation testing and ﬁeld information
Lizhi Wang a, Rong Pan b,n, Xiaoyang Li a, Tongmin Jiang a
a
b

School of Reliability and System Engineering, Beihang University, Beijing, PR China
Ira A. Fulton School of Engineering, Arizona State University, Tempe, AZ, USA

a r t i c l e i n f o

a b s t r a c t

Article history:
Received 30 March 2012
Received in revised form
17 September 2012
Accepted 25 September 2012
Available online 29 November 2012

Accelerated degradation testing (ADT) is a common approach in reliability prediction, especially for
products with high reliability. However, oftentimes the laboratory condition of ADT is different from
the ﬁeld condition; thus, to predict ﬁeld failure, one need to calibrate the prediction made by using ADT
data. In this paper a Bayesian evaluation method is proposed to integrate the ADT data from laboratory
with the failure data from ﬁeld. Calibration factors are introduced to calibrate the difference between
the lab and the ﬁeld conditions so as to predict a product’s actual ﬁeld reliability more accurately. The
information fusion and statistical inference procedure are carried out through a Bayesian approach and
Markov chain Monte Carlo methods. The proposed method is demonstrated by two examples and the
sensitivity analysis to prior distribution assumption.
& 2012 Elsevier Ltd. All rights reserved.

Keywords:
Degradation analysis
Information fusion model
Bayesian inference
Sensitivity analysis

1. Introduction
A product’s lifetime or degradation data is the major data
source for inferring its failure time distribution model. However,
it is very time-consuming to obtain enough failure data from
products operated under the ﬁeld-use condition. To address this
problem, accelerated life tests are widely used to obtain product
failure information in a short time frame. But for highly reliable
products, even accelerated life testing (ALT) may be inadequate
due to a large amount of censoring. Therefore, accelerated
degradation testing (ADT), in which product degradation data
are collected and analyzed for reliability extrapolation, becomes a
common approach to the reliability prediction for those highly
reliable products.
However, the extrapolated reliability measurement using ADT
only represents the reliability under the laboratory testing condition,
which could be quite different from the product’s ﬁeld condition. For
example, in ADT the testing tool cannot reproduce all the stresses
and stress variations that a product will experience in the ﬁeld.
Therefore, one should pay close attention to the model-based
extrapolation that has to be carried out for inferring the ﬁeld
reliability. The ﬁeld failure data are usually sparse, but the laboratory testing data could be abundant, thus it is meaningful to
establish the correlation between the laboratory and ﬁeld data and

n

Corresponding author. Tel.: þ1 480 965 4259.
E-mail address: rong.pan@asu.edu (R. Pan).

0951-8320/$ - see front matter & 2012 Elsevier Ltd. All rights reserved.
http://dx.doi.org/10.1016/j.ress.2012.09.015

to predict the ﬁeld reliability using the information from both
sources.
This paper aims to develop a reliability prediction model with
data from multiple data sources. A brief literature review on ADT
and ADT data analysis is given in the next section. Section 3
describes the degradation model, lifetime distribution and acceleration model considered in this paper. Section 4 provides the
Bayesian inference approach to model parameter estimation. A
synthetic example is used to demonstrate the proposed method
in Section 5, along with the sensitivity analysis of this method.
Section 6 describes a real product test and data analysis. Finally,
the paper is concluded in Section 7.

2. Literature review
Accelerated degradation testing is an effective approach to the
product reliability prediction, especially for a product with high
reliability and long lifetime, where even accelerated life testing is
difﬁcult to obtain sufﬁcient failure data within the limited time and
budget. Same as ALT, ADT analysis is a method based on the
assumption of identical failure mechanism; that is, the life characteristic of the product under the use stress level can be extrapolated from the degradation data obtained from the accelerated
stress level using a life-stress acceleration model. In this kind of
test, the stress is applied in the same way as in ALT, but it is not
necessary to observe the occurrence of failure event; instead, the
degradation measurement of some life characteristic of interest is
monitored.

L. Wang et al. / Reliability Engineering and System Safety 112 (2013) 38–47

In literature, Nelson [1] described a degradation model and the
statistical method for analyzing age-degradation data on product
performance. Meeker and Hamada [2] presented a conceptual
degradation-based reliability model to describe the role of, and
need for, integration of reliability data sources. Meeker and
Escobar [3] applied the Arrhenius acceleration model on the
ADT analysis. Bae et al. [4] investigated the link between a
practitioner’s selected degradation model and the resulting lifetime model. Lu [5] assumed that the degradation process follows
a Wiener process, so the ﬁrst passage time distribution to a
barrier is distributed as an inverse Gaussian distribution. Pettit
and Young [6] extended the analysis in Lu by using a fully
Bayesian approach to model estimation and prediction. Barker
and Newby [7] addressed the problem of determining inspection
and maintenance strategy for a system whose state is described
by a multivariate Wiener process. Carey and Koenig [8], Tseng and
Yu [9], Tseng et al. [10] and Onar and Padgett [11,12] presented
their relevant methods of ADT based on ﬁlm resistance, MOS,
logic chip, LED, carbolic ﬁber, respectively.
Unlike the ﬁeld-use condition, the stresses applied on the test
units in a laboratory testing condition can be precisely controlled,
so the extrapolated result from an ADT may not accurately
represent the real situation in the ﬁeld, where most stress
variables are in fact random. To solve this problem, Liao and
Elsayed [13] extended an ADT model obtained from constantstress ADT experiments to predict ﬁeld reliability by considering
the ﬁeld stress variations. Meeker et al. [14] developed a use rate
model to relate ALT data and ﬁeld data. Monroe and Pan [15]
made an effort of using global climate data to adjust the acceleration factor for any speciﬁc market place of electronic products.
However, it is difﬁcult to predict all stresses and stress variations
that the product will experience under its ﬁeld condition. Integrating reliability information from various sources is crucial to
the solution and Bayesian data analysis provides a feasible means
for information fusion. In a series of paper by Hamada and his
colleagues [16–18] the methods of using hierarchical Bayes model
to integrate reliability information were investigated. Touw [19]
studied the Bayesian estimation of mixed Weibull distribution
and others. Pan [20] introduced a calibration factor to lifetime
model to make a more accurate prediction of ﬁeld failure time by
considering the data from both ALT and ﬁeld. This calibration
factor statistically calibrates the ALT model’s parameter and
prediction result without explicitly considering speciﬁc stress or
inﬂuencing factors. In this paper we formulate calibration factors
for the ADT model and use Bayesian methods to integrate ADT
and ﬁeld data for reliability prediction.

3. Models
3.1. Degradation model
We assume that the degradation follows a Wiener process, and
then the ﬁrst passage time of this process to a threshold follows
the inverse Gaussian distribution. Chhikara and Folks [21] discussed the use of this distribution as a lifetime distribution
model. Whitmore and Schenkelberg [22] and Lu [5] had considered it for degradation data analysis. The degradation model of
Wiener process is
YðtÞ ¼ sBðtÞ þ dðsÞt þy0 ,

ð1Þ

where Y(t) is the performance degradation process of product, B(t)
is the standard Brownian motion with the mean value of 0 and the
variance of t, denoted as B(t) N(0, t), s is the diffusion coefﬁcient,
which is a constant and does not change with stress or time, y0 is a
known initial value of product performance, and d(s) is the drift

39

coefﬁcient, which represents the degradation rate of product and is
a function of stress. The acceleration model of degradation rate is
given by
dðsÞ ¼ exp ½b0 þ bjðsÞ,

ð2Þ

where j(s) is a function of stress factor s.
Known from the property of the Wiener process, the degradation increment Dy during the unit time Dt is subject to a normal
distribution with the mean of d(s)Dt and the variance of s2Dt, i.e.,

Dy  N ðdðsÞDt, s2 DtÞ

ð3Þ

Let l be the failure threshold and the product fails when
y(t)  lo0, then the ﬁrst passage time t to the threshold l has an
inverse Gaussian distribution. The probability density function of
an inverse Gaussian distribution is
rﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ




2
l
l 
exp  2 xm
,
ð4Þ
f x; m, l ¼
3
2px
2m x
and the cumulative distribution function is
rﬃﬃﬃ
rﬃﬃﬃ
!
!
 
l x
2l
l x
F ðxÞ ¼ PðX r xÞ ¼ F
1
þ exp
F
þ1
:
x m
m
x m
ð5Þ
For a Weiner process, its ﬁrst passage time t follows an inverse
Gaussian distribution with m ¼((l  y0)/(d(s))) and l ¼(l  y0/s)2,
thus its reliability function is given by

 	

	




2dðsÞ ly0
ly0 dðsÞt
ly0 þ dðsÞt
pﬃﬃ
p
ﬃﬃ
Rðt Þ ¼ F
F

 exp
:
ð6Þ
s2
s t
s t
From the reliability function above, we know that the key
parameters for reliability prediction are d(s) and s2. After these
parameters are obtained, the reliability prediction can be made.
3.2. Calibration factor
The purpose of ADT is to predict the lifetime and the reliability
of a product deployed in the ﬁeld. However, the extrapolated
reliability value from ADT data only represents the reliability in the
laboratory environment, which may be quite different from the
ﬁeld environment. For example, some stresses and variability of
stresses, experienced by the product in its ﬁeld, cannot be
simulated in a testing laboratory and it is difﬁcult to replicate
the environmental noise in a lab too.
As d(s) and s2 are the key parameters in the degradation model,
we introduce two calibration factors, k1 and k2, to update the
estimates of these two parameters so as to solve the problem
above. Calibration factor k1 is introduced to take into the consideration of additional environmental stresses and their effects on
the degradation path, d(s). For instance, there are multiple active
stress variables, s1, s2y sm, under the ﬁeld condition, but only the
stress s1 can be simulated in ADT, while other stresses are ignored.
In this paper, assuming these stress variables are independent, the
ﬁeld acceleration model is
df ðSÞ ¼ exp½b0 þ b1 j1 ðs1 Þ þ b2 j2 ðs2 Þ þ UUU, þ bm jm ðsm Þ
¼ exp½b0 þ b1 j1 ðs1 ÞUexp½b2 j2 ðs2 Þ þ UUU, þ bm jm ðsm Þ:

ð7Þ

But the ADT acceleration model is
dADT ðs1 Þ ¼ exp½b0 þ b1 j1 ðs1 Þ:
Let the effect of other stresses be represented by the calibration factor k1, then k1 is deﬁned as
k1 ¼ exp½b2 j2 ðs2 Þ þUUU, þ bm jm ðsm Þ,

ð8Þ

and we have
df ðSÞ ¼ k1 UdADT ðs1 Þ:

ð9Þ

40

L. Wang et al. / Reliability Engineering and System Safety 112 (2013) 38–47

The exponential acceleration function used in (7) (or the loglinear relationship between stress and accelerated parameter) is
very common in reliability analysis, but it is possible to use other
type of acceleration model depending on speciﬁc failure mechanism and the deﬁnition of calibration factor will be adjusted
accordingly.
Although the parameter s2 is not directly related to stress
variables, it governs the variability of lifetime distribution. Considering that in the ﬁeld environment the deployed products can
be different from those tested in laboratory, due to, for examples,
manufacturing variations, changed assembling conditions, etc., we
introduce another factor to calibrate this parameter. The degradation increments in ADT, DyADT, and under the ﬁeld condition, Dyf,
are, respectively,

DyADT ¼ dADT ðsÞUDt þ eADT ,

ð10Þ

and

Dyf ¼ df ðSÞUDt þ ef :

ð11Þ


2
 N 0, sADT , the calibration factor, k2, is additive to s2,

As eADT
i.e.,


ef  N 0, s2ADT þk2

ð12Þ

or

s2f ¼ s2ADT þ k2 :

ð13Þ

3.3. Extension to gamma process
Although we assume that the degradation process follows a
Wiener process above, our calibration method of ADT inference can
be easily extended to other stochastic processes as long as the
distribution function of degradation increment and the degradation model are well-deﬁned. Gamma process is a stochastic process
that is always positive and strictly increasing [23]. It can be used
to model a degradation in which the system deterioration is
measured and the deterioration cannot be reversed. A Gamma
process {Y(t),tZ0} has independent, non-negative increments Dy¼
Y(tþ t)Y(t) that follow a gamma distribution as [24,25]

Dy  Gðb, aðt þ tÞaðtÞÞ,

ð14Þ

where b (b 40) is a constant scale parameter and a(t) is a timevarying shape parameter with a(0)¼0. Let T be the ﬁrst passage
time to the failure threshold l and y0 be the initial value of the
process, then the reliability function is given by


P ðT 4 t Þ ¼ P ðY ðt Þ olÞ ¼ P Y ðt Þy0 o ly0


Z ly0
1
y
yaðtÞ1 exp 
dy:
ð15Þ
¼
b
0
GðaðtÞÞbaðtÞ
The shape parameter a(t) is assumed to be a function of stress
variable in Park and Padgett [23] and Pan and Balakrishnan [24]
as

aðtÞ ¼ dðsÞUt,

ð16Þ

where d(s) is the drift coefﬁcient, representing the degradation
rate as deﬁned by the acceleration model in Eq. (2). So, the
degradation increment Dy during the unit time Dt is subject to a
gamma distribution with the shape parameter of d(s)Dt and the
scale parameter of b, i.e.,

Dy  Gðb,dðsÞDtÞ:

ð17Þ

When we take into consideration of additional environmental
stresses and their effects on the degradation path, we can introduce a calibration factor k1 into the ﬁeld drift coefﬁcient, df(s), as in
Eq. (9). The parameter b of gamma distribution is a scale parameter
and when b is small the distribution is more concentrated. We can

introduce another calibration factor k2 to the effect of the ﬁeld
environmental noise on this parameter, then

bf ¼ k2 UbADT

ð18Þ

Now we have DyADT  G(bADT,d(s)Dt) and Dyf  G(k2bADT,k1d(s)Dt)
and the lab testing data and ﬁeld data can be combined together to
infer the model parameters of interest.

4. Bayesian inference
To understand the relationship between different environmental conditions and to estimate the reliability accurately, it is
important to evaluate the parameters, such as b0, b1 and s2, and
the calibration factors k1 and k2. It is also a difﬁcult task when the
model is complex, with a high dimensional parameter space.
Bayesian approach is a feasible method to integrate all available
information to infer unknown parameters. In Bayesian approach
parameters are treated as random variables and their probabilistic models are obtained by posterior distributions. Markov Chain
Monte Carlo (MCMC) is a simulation approximation method that
is widely used in Bayesian inference. By this method sampling of
an unknown variable is implemented through establishing an
appropriate Markov chain, as the stationary distribution of the
Markov chain converges to the variable’s posterior distribution.
A special case of MCMC is the Gibbs sampler, which involves
simulating from the conditional posterior distribution of each
parameter given the data and all of the other parameters. It is
effective in handling high dimensional problems and has been
applied on various reliability engineering applications.
4.1. Degradation data from both ADT and ﬁeld
When it is possible to collect degradation data, yﬁeld, from ﬁeld,
we may combine them with the ADT data, yADT, and model them
by

DyA  NðdA ðsÞUDtA , s2A UDtA Þ

ð19Þ

and




Dyf  N df ðs UDtf , s2f UDtf Þ:

ð20Þ

Applying the two calibration factors, it becomes


Dyf  Nðk1 UdA ðsÞUDtf , s2A þ k2 UDt f Þ:
In general, the degradation data is a vector, including Dyi, si
and ci, where Dyi is degradation increment, si is the stress level, ci
is the condition indicator variable (if it is the data from ADT, ci ¼0;
if it is from ﬁeld, ci ¼1), so a combined degradation model is as


Dyi  N m0 , s20 ,
ð21Þ
where






m0 ¼ dA ðsi ÞU Dt A þci k1 UDtf DtA Þ,
and




s20 ¼ s2A UDt A þci k2 UDtf þ s2A UDtf s2A UDt A :
Then the posterior distribution of unknown parameters is

p Y9D


¼ p b0 , b1 , s2A ,k1 ,k2 9Dy,s,c
!1=2
ð22Þ

2 !
n
Y


Dyi m0
1
p
exp 
Up b0 , b1 ,k1 ,k2 , s2A
2
2
s0
2Us0
i¼1


where n is the number of degradation observations, pðb0 , b1 ,k1 ,k2 , s2A Þ
is the joint prior distribution of b0, b1, k1, k2 and s2A .

L. Wang et al. / Reliability Engineering and System Safety 112 (2013) 38–47

4.2. Degradation data from ADT and failure time data from ﬁeld
Sometimes continuous product monitoring cannot be implemented in ﬁeld, so ﬁeld degradation data are not available; instead,
we may have a few of ﬁeld failure time data. Based
 on the Weiner

process model, these ﬁeld failure time data, X ¼ x1 ,x2 ,. . .,xn1 , are
inverse Gaussian distributed, so the probability density function is
!1=2
( 
 )
2
ldf ðs xÞ2
l
hðxÞ ¼
exp 
:
ð23Þ
2px3 sf 2
2xsf 2
Thus,
!1=2

2

hðxÞ ¼

l


2px3 sA 2 þk2

(
exp 

2

ðlk1 UdA ðsÞxÞ


2x sA 2 þ k2

)
:

ð24Þ


From (3), the degradation increment data DY ¼ Dy1 , Dy2 ,. . .,
Dyn2 Þ from ADT has a normal distribution with the following
probability density function,
!
1
ðDydA ðsÞDtÞ2
f ðDyÞ ¼ pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ exp 
:
ð25Þ
2sA 2 Dt
2psA 2 Dt
So the total likelihood of both ADT and ﬁeld data is
LðdðsÞ, s2 ,k1 ,k2 Þ ¼

n1
Y

5. A synthetic example and sensitivity analysis
5.1. Prior distribution selection
As the nature of prior distribution deﬁned in Bayesian inference
is subjective, it is important to select and compare feasible prior
distributions. For b0, b1and s2 are parameters in a normal regression model, according to Ntzoufras [26], the simplest approach is to
assume that these parameters have independent priors with
p
   
Y


f bj f s2 ,
f b, s2 ¼
j¼0





bj  N mbj , e2j ,

s2  IGaða,bÞ,

j¼1

where n1 is the number of ﬁeld failure time observations and n2 is
the number of ADT degradation increment observations.
To facilitate the MCMC computation, a zeros-ones trick [26] of
deﬁning an arbitrary likelihood function in WINBUGS is applied.
Using this method, either the Bernoulli or the Poisson distribution
can be utilized to indirectly specify any arbitrary likelihood function.
Assume there are n independent observations and each of them is
from a Poisson distribution with the Poisson parameter being the
negative value of log-likelihood of a single failure time or degradation increment, then the likelihood function can be written as
0
n
n
n
Y
Y
Y


eðli Þ ðli Þ
L z0 9y ¼
¼
eli ¼
f P ð0; li Þ
0!
i¼1
i¼1
i¼1

As one can see from (22) and (28), they are very complex
model and would require the MCMC and Gibbs algorithms to
obtain the posterior distribution of the parameters of interest
with WinBUGS. The reliability inference can be obtained by (9),
(13) and (6) after the model parameter estimation is conducted.

and

n2


Y
hðxi Þ
f Dyj :

i¼1

41

ð26Þ

Hence, the model likelihood can be written as the product of
the densities of new pseudo-random variables (all observations
are zeros) that follow the Poisson distribution with mean equal to
the negative log-likelihood value. Let


lf i ¼ logh xi 9b0 , b1 , s2A ,k1 ,k2 ,

that is, bj follows the normal distribution and s2 follows the
inverse gamma distribution.
For the calibration factor k1 and k2, either normal or gamma
distribution can be a feasible solution. We test four sets of
candidate prior distributions with the equal mean values. These
prior distributions are shown in Table 1.
The deviance information criterion (DIC) is typically used as a
measure in model comparison, but DIC must not be used when
the posterior distributions are highly skewed or bimodal (Ntzoufras [26]). We also ﬁnd that it is not a suitable method in this
application. To select the best prior distributions of k1 and k2, we
choose a simulation study and analyze the accuracy of the
estimated parameter values with their assumed values used in
the model. First, the data generated from an assumed model are
used to estimate the parameters, b0, b1, k1, k2 and s2, by the
method proposed in this paper with different k1 and k2 prior
distributions. Then, we evaluate the accuracy of these estimates.
Finally, the best k1 and k2 prior distributions are selected.
The analysis would be processed under the situation of degradation data from both ADT and ﬁeld, and the situation of degradation
data from ADT and failure time data from ﬁeld, respectively.

and


lAj ¼ logf Dyj 9b0 , b1 , s2A :
Let ci be the testing condition indicator variable, then
li ¼ ci Ulf i þ ð1ci ÞUlAi :
The data include zi, si and ci, where zi represents the failure
time data ti or the degradation increment data Dyi. Then the
model likelihood is
þ n2
n1Y
þ n2 ðli Þ
0
n1Y
þ n2

 n1Y
e
ðli Þ
L z,s,c9b0 , b1 , s2A ,k1 ,k2 ¼
eli ¼
f P ð0; li Þ:
¼
0!
i¼1
i¼1
i¼1

The posterior distribution is

p Y9D


¼ p b0 , b1 , s2A ,k1 ,k2 9z,s,c




pL z,s,c9b0 , b1 , s2A ,k1 ,k2 Up b0 , b1 ,k1 ,k2 , s2A ,

ð27Þ



5.1.1. Degradation data from both ADT and ﬁeld
Suppose there is a temperature step-stress ADT applied to a
product, where there are 4 test units and 4 temperature levels
(60 1C, 80 1C, 100 1C, 120 1C). The time of each level is 1250, 750,
500 and 500 h, and the inspection time interval is 5 h, so there are
600 (250þ150þ100þ100¼ 600) observations for each test unit
and 2400 observations in total. There is 1 ﬁeld sample which is
tested at 25 1C over 5000 h and the inspection time interval is also
5 h, so there are 1000 observations.
Suppose that the degradation of this product obeys the Weiner
process. The initial performance value y0 is 100 and the failure
Table 1
Candidate prior distribution.

ð28Þ

where pðb0 , b1 ,k1 ,k2 , s2A Þ is the joint prior distribution of b0, b1, k1,
k2 and s2A .

k1
k2

1

2

3

4

Normal
Normal

Normal
Inverse-gamma

Gamma
Inverse-gamma

Gamma
Normal

42

L. Wang et al. / Reliability Engineering and System Safety 112 (2013) 38–47

threshold l is 50. As the stress factor is temperature, the Arrhenius
acceleration function is applicable, so j(s)¼1/T, where T is the
absolute temperature, and
dðTÞ ¼ exp½b0 þ b1 =T:
In the simulation, the true values of b0, b1, k1, k2 and s are
given in Table 2.
To compare the evaluation results without other interference,
all prior distributions are deﬁned as having their mean values as
shown in Table 3, and their variances are equal to 105.
A total of 50 sets of simulation data are generated, and for every
set of prior distributions, the posterior estimations of these parameters are computed. When the MCMC simulation starts, checking
the distribution convergence from the sampled values of the parameter posterior distribution is necessary. The Gelman–Rubin ratio
Table 2
True value of model parameters.

(Gelman and Rubin [27]) is taken to monitor the convergence, it is the
value of width of the central 80% interval of the pooled runs over the
average width of the 80% intervals within individual runs. We start
with two Markov chains with different initial values. When the
Markov chain simulation sample size increases, if the process converges, the Gelman–Rubin ratio converges to 1. We run 40,000
iterations for each Markov chain in the simulation. Fig. 1 shows the
Gelman–Rubin ratio of b0, b1, k1, k2 and s from one set of simulation
data. One can see that the Gelman–Rubin statistics of all parameters
become stable and converge to 1 after 15,000 iterations, so the
remained 25,000 iterations of each chain (50,000 in total) are selected
as the samples from each parameter’ posterior distribution. Fig. 2
provides the histograms of posterior distributions. The average
relative errors of these 5 parameters are shown in Table 4. As b0
and b1 are parameters in d(s), the accuracy of d(s) is more meaningful
than these parameters. The relative errors of d(s) are included in
Table 4, where s is set to 25 1C. The relative error is
relative error ¼ 9evaluation valuetrue value9=true value

Parameter

b0

b1

s

k1

k2

True value

9.2103

4800

0.025

1.5

6e-4

ð29Þ

From Table 4, one can see that the gamma distribution and
inverse gamma distribution for k1 and k2, respectively, is the best
Table 4
Relative error.

Table 3
Mean value of prior distributions.
Parameter

b0

b1

s

k1

k2

True value

9.2103

4800

0.025

1

0

1
2
3
4

b0

b1

d (25 1C)

k1

k2

s

0.126
0.141
0.138
0.137

0.0099
0.0100
0.0105
0.0106

0.044
0.059
0.040
0.041

0.284
0.282
0.219
0.221

0.129
0.061
0.061
0.130

0.037
0.032
0.032
0.038

Fig. 1. Monitoring MC convergence by using Gelman–Robin statistic (red line). (For interpretation of the references to color in this ﬁgure legend, the reader is referred to
the web version of this article.)

Fig. 2. Posterior distributions of b0, b1, k1, k2 and s.

L. Wang et al. / Reliability Engineering and System Safety 112 (2013) 38–47

In this case, there is not a single prior candidate that is the best
at every parameter estimation. We compute the reliability values
at the product’s third year in ﬁeld use for the 4 sets of prior
distribution using the 50 sets of simulation data and compare
their relative errors. The average relative errors are shown in
Table 6.
Here we ﬁnd that normal distribution as the prior distribution
for k1 and k2 is the best. Let


k1  N mk1 , s2k1


k2  N mk2 , s2k2 ,

candidate. Let the prior distribution of k1 and k2 be


k1  G ak1 ,bk1


k2  IGa ak2 ,bk2 ,
then (22) can be written as


p Y9D


¼ p b0 , b1 , s2A ,k1 ,k2 9Dy,s,c
!1=2

2 !
n
Y
Dyi m0
1
p
exp 
s20
2Us20
i¼1
b m  b m  bak1
0
1
k
b0
b1
a þ1
Uf
U  1  ðk1 Þ k1
Uf
c0
c1
G ak1

ak2



bk
1
ebk1 k1 U  2 
G ak2 k2

ak

2

þ1

a

b
1
ebk2 k2 U
GðaÞ s2A

!a þ 1
eb=s

43

2
A

5.1.2. Degradation data from ADT and failure time data from ﬁeld
We follow the previous example, but let the ﬁeld information
be 20 failure times. The true values of model parameters b0, b1, k1,
k2 and s used in the simulation are the same as in Table 2. The
average relative errors of the 5 parameters, along with the relative
errors of d(s), are shown in Table 5.

then (28) can be written as


p Y9D


¼ p b0 , b1 , s2A ,k1 ,k2 9z,s,m


pL z9b0 , b1 , s2A ,k1 ,k2
!
! 
 

b0 mb0
b1 mb1
k1 mk1
k2 mk2
Uf
Uf
Uf
Uf
c0
c1
sk1
sk2
a

U

b
1
GðaÞ s2A

!a þ 1
2

eb=sA

Table 5
Relative error.

1
2
3
4

b0

b1

d (25 1C)

k1

k2

s

0.119
0.1179
0.1129
0.1169

0.0094
0.0099
0.0091
0.0095

0.038
0.042
0.039
0.041

0.0409
0.0411
0.0384
0.0424

0.0036
0.00053
0.00052
0.00058

0.0084
0.0508
0.0514
0.0512

Table 6
Relative error of reliability (3 years).

Relative error

1

2

3

4

0.0263

0.0524

0.0495

0.0489

Fig. 4. Sensitivity plot of reliability for different values of b0 prior mean.

Fig. 3. Sensitivity plot of b0 posterior mean for different values of b0 prior mean. (For interpretation of the references to color in this ﬁgure, the reader is referred to the
web version of this article.)

44

L. Wang et al. / Reliability Engineering and System Safety 112 (2013) 38–47

Fig. 5. Sensitivity plots for different values of b1 prior mean.

Fig. 6. Sensitivity plots for different values of k1 prior mean.

Fig. 7. Sensitivity plots for different values of k2 prior mean.

5.2. Sensitivity analysis
In this section, sensitivity analysis is implemented with different values of the prior mean to study the robustness of the
Bayesian method. There are 5 parameter prior means, thus we
carry out an orthogonally designed experiment to study their
inﬂuence on posterior evaluation. Again, this analysis would be
processed under the situation of degradation data from both ADT
and ﬁeld, and the situation of degradation data from ADT and
failure time data from ﬁeld.
When the degradation data are from ADT and ﬁeld, followed
the example in Section 5.1.1, we vary the prior mean value of the
parameter mb0 and observe changes in its posterior mean, which is

shown in Fig. 3 (the blue line is posterior mean, the green line is
the posterior quantile value at 97.5%, the red line is the posterior
quantile value at 2.5%). One can see that the posterior mean is
quite robust with values from 9.1 to 9.4, when the prior values are
from 8.6 to 9.6. Then we study the sensitivity of the reliability
evaluation result (3 years) over different values of the prior
parameter mb0 , the sensitivity is shown in Fig. 4, one can see that
the reliability evaluation value is also robust with values from
0.92 to 0.93, when the prior values vary from 8.6 to 9.6.
Same to mb0 , the sensitivity of the posterior mean and reliability
over different values of b1, k1, k2 and s’s prior means are shown in
Figs. 5–8. We can see that the posterior mean of b1 is from 4770 to
4820, reliability evaluation value is from 0.93 to 0.94, when the prior

L. Wang et al. / Reliability Engineering and System Safety 112 (2013) 38–47

45

Fig. 8. Sensitivity plots for different values of s prior mean.

Table 7
L16(4^5) orthogonal array of sensitivity analysis for the example in Section 5.1.1.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
Ij
IIj
IIIj
IVj
Rj

b0

b1

k1

k2

s2

1

2

3

4

5

120%
120%
120%
120%
80%
80%
80%
80%
140%
140%
140%
140%
60%
60%
60%
60%
0.0914
0.146
0.1306
0.0824
0.0636

120%
80%
140%
60%
120%
80%
140%
60%
120%
80%
140%
60%
120%
80%
140%
60%
0.1215
0.0919
0.0944
0.1426
0.0507

120%
80%
140%
60%
80%
120%
60%
140%
140%
60%
120%
80%
60%
140%
80%
120%
0.0809
0.0365
0.2390
0.0940
0.2025

120%
80%
140%
60%
140%
60%
120%
80%
60%
140%
80%
120%
80%
120%
60%
140%
0.0838
0.1396
0.0947
0.1323
0.0558

120%
80%
140%
60%
60%
140%
80%
120%
80%
120%
60%
140%
140%
60%
120%
80%
0.14
0.1266
0.1045
0.0793
0.0607

Table 8
L16(4^5) orthogonal array of sensitivity analysis for the example in 5.1.2.

REOR (3 years)

0.0168
0.0089
0.0434
0.0223
0.0069
0.0263
0.0236
0.0892
0.0749
0.0252
0.0186
0.0119
0.0229
0.0315
0.0088
0.0192
T¼ 0.4504

values are from 2800 to 6800; the posterior mean of k1 is from 1.3 to
1.6, reliability evaluation value is from 0.93 to 0.94, when the prior
values are from 0.9 to 2.1; the posterior mean of k2 is from 5e-4 to 7e4, reliability evaluation value is from 0.915 to 0.925, when the prior
values are from 1e-4 to 1.2e-3; the posterior mean of s is from 0.25 to
0.26, reliability evaluation value is from 0.925 to 0.935, when the
prior values are from 0.012 to 0.035. From here, we conclude that the
reliability prediction is robust to a certain extent, but the degrees of
inﬂuence of the ﬁve parameters are different. To which parameter
that the reliability prediction is more sensitive is a question we
should discuss next.
To study the inﬂuence of the priors and the sensitive difference
among these ﬁve parameters, a contrast test is applied, where the
relative error of reliability (REOR) is computed with Eq. (29). The
mean values of b0, b1, k1, k2 and s are the factors, and we let the
120%, 80%, 140% and 60% of their true values be the 1st, 2nd, 3rd and
4th level of these factors. If we pair every factor to every level, there
will be 4^5 ¼1024 combinations. A L16(4^5) orthogonal array [28] is
selected so as to reduce the number of experiments to 16 and the
sensitivity analysis of different parameters can still be carried out by
the analysis of range method. Using the example in Section 5.1.1, the
results are shown in Table 7.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
I
II
III
IV
R

b0

b1

k1

k2

s2

Relative error of
reliability
(3 years)

120%
120%
120%
120%
80%
80%
80%
80%
140%
140%
140%
140%
60%
60%
60%
60%
0.0560
0.0481
0.0420
0.0554
0.0140

120%
80%
140%
60%
120%
80%
140%
60%
120%
80%
140%
60%
120%
80%
140%
60%
0.0578
0.0374
0.0586
0.0477
0.0212

120%
80%
140%
60%
80%
120%
60%
140%
140%
60%
120%
80%
60%
140%
80%
120%
0.0403
0.0607
0.0510
0.0495
0.0204

120%
80%
140%
60%
140%
60%
120%
80%
60%
140%
80%
120%
80%
120%
60%
140%
0.0380
0.0585
0.0468
0.0582
0.0205

120%
80%
140%
60%
60%
140%
80%
120%
80%
120%
60%
140%
140%
60%
120%
80%
0.0618
0.0169
0.0923
0.0305
0.0754

0.0131
0.0070
0.0258
0.0101
0.0106
0.0187
0.0024
0.0164
0.0061
0.0090
0.0071
0.0198
0.0280
0.0027
0.0233
0.0014
T¼ 0.2015

In Table 7, Ij, IIj, IIIj and IVj are the sum of 1st, 2nd, 3rd and 4th
level’s REOR values in parameter j(j¼1,y,5), and T is the sum of the
16 REOR values. Take I1 for example, which is the sum of 1st level’
REOR in the column of b0, so the corresponding REOR values are
0.0168, 0.0089, 0.0434 and 0.0223, and
I1 ¼ 0:0168 þ 0:0089 þ 0:0434þ 0:0223 ¼ 0:0914
Rj is the range of parameter j, i.e.,


Rj ¼ maxðIj ,IIj ,IIIj ,IVj Þmin Ij ,IIj ,IIIj ,IVj
The value of Rj indicates the inﬂuence of the changed levels to
the target (REOR) for this factor, so it stands for the evaluation of
the sensitivities of REOR to different parameters. From Table 6 one
can see that the ranking of sensitivities is as k1 4 b0 4 s 4k2 4 b1;
therefore, when one should exercise more cautions when to assign
the prior distributions for those parameters that the reliability
prediction is sensitive to.
The same type of analysis is performed on the example in
Section 5.1.2, when the degradation data from ADT and the failure
data from ﬁeld condition are available. The results are shown in
Table 8. In this case, the ranking of the sensitivities is as
s 4 b1 4k2 4k1 4 b0.

46

L. Wang et al. / Reliability Engineering and System Safety 112 (2013) 38–47

209.45
209.4

output optical power

209.35
209.3
209.25
209.2
209.15
209.1
209.05
209
208.95

Fig. 9. Structure of SLD.

0

20

40

60

80

100

120

140

160

180

200

Time/day
Fig. 12. Degradation data of SLD under its working environment.

Fig. 10. Stress proﬁle of step-stress ADT.

18
16

output optical power

14
12
10
8
6
4
2

0

500

1000

1500

2000

2500

3000

3500

4000

Time/hour
Fig. 11. Degradation data of SLD.

6. A real example
Super luminescent diode (SLD) is a semiconductor product
that has a wide range of applications in optical ﬁber sensors,
optical ﬁber communication systems, clinical systems, and so on.
This product enjoys long lifetime and high reliability, as well as
other design and manufacturing advantages. The structure of SLD
is shown in Fig. 9. As failure time of SLD is difﬁcult to obtain,
degradation of SLD can be monitored to predict its reliability. The
performance degradation measurement of SLD is its output
optical power, which is sensitive to temperature.
This type of SLD had been preliminary used for many years,
but there was no systematical study of the effect of temperature
on its lifetime. We conducted a step-stress ADT experiment with

4 progressive stress levels on 4 SLDs. The test plan was followed
as 60 1C of 2400 h, 80 1C of 800 h, 100 1C of 300 h, and ﬁnally
110 1C of 300 h. The stress proﬁle of ADT is shown in Fig. 10.
The degradation data were recorded every 10 min with power
tester automatically controlled with a computer. These ADT data are
plotted in Fig. 11. SLD’s typical work condition is at around 25 1C. But
in the ﬁeld, it is difﬁcult to measure its output optical power as
frequent as in the experiment. We have measurements of one SLD
every 6 h for 200 days and they are plotted in Fig. 12. One can see
that they have signiﬁcant larger variability than the ADT data.
A hypothesis testing (p¼0.05) was applied on the degradation
increments at different temperatures and the result showed that
they followed normal distributions, so we elected to use the
Wiener process to describe the degradation process. We also know
the initial value of the degradation increment’s mean and variance
from the hypothesis testing, it would be used to deﬁne the
parameters’ prior values. The failure threshold value of SLD is the
60% of its initial output optical power value. Using the Weiner
process model and the ADT data, we can predict that the 5 years’
reliability of SLD was 0.99. But, known from experience, the ﬁeld
reliability of SLD in 5 years is much lower than 0.99, it is roughly
between 0.87 and 0.97. The ﬁeld reliability estimated from the ﬁeld
degradation data is 0.93 and the 95% conﬁdence interval is [0.89,
0.97], which is closer to engineer’s knowledge. However, using only
one ﬁeld sample cannot provide us an accurate result, so we
combined the 200 days’ ﬁeld data with the ADT data via the
method in this paper, the prior of parameter bj of drift coefﬁcient
follows the normal distribution and diffusion coefﬁcient s2 follows
the inverse gamma distribution, and the parameter values of these
priors are deﬁned with the hypothesis testing results above and
experience. Gamma distribution and inverse gamma distribution
are selected as the prior distribution for k1 and k2, with their means
to be 1 and 0, respectively. Their prior parameters are deﬁned such
that the gamma and inverse gamma distributions have such mean
values and variance as 105 The calibration factor k1 was calculated
to be 1.3 and k2 was 0.001, and the 5-year reliability of SLD became
0.935, with the 95% conﬁdence interval as [0.93, 0.94]. This result is
more precise than that with ﬁeld data only and it is much closer to
ﬁeld experience than using ADT data only. In addition, with the
estimated degradation model, we can study the effect of temperature on the SLD degradation rate.

7. Conclusions
In this paper a Bayesian approach is proposed to integrate the
product’s reliability information from both the ADT and ﬁeld-use
conditions. The degradation model and calibration factors are
introduced to bridge the reliability evaluation from the accelerated

L. Wang et al. / Reliability Engineering and System Safety 112 (2013) 38–47

degradation testing in a testing laboratory to the product’s actual
performance in the ﬁeld. These calibration factors model the
uncertainty of stress ﬂuctuations and the complexity of failure
behaviors of the product in its ﬁeld-use environment, so our
method can solve the dilemma that the abundant ADT information
cannot represent a product’s reliability accurately when the modelbased extrapolation has to be conducted for reliability prediction.
We provide two examples and a discussion on prior distribution
selection. The sensitivity analysis gives the guideline for prior
distribution assignment.

References
[1] Nelson W. Analysis of performance-degradation data from accelerated tests.
IEEE Transactions on Reliability 1981;30(2):149–54.
[2] Meeker WQ, Hamada M. Statistical tools for the rapid development and
evaluation of high-reliability products. IEEE Transactions on Reliability
1995;44(2):391–400.
[3] Meeker WQ, Escobar LA. Statistical Methods for Reliability Data. 1st ed. New
York: John Wiley and Sons; 1998.
[4] Bae SJ, Kuo W, Kvam PH. Degradation models and implied lifetime distributions. Reliability Engineering and System Safety 2007;92:601–8.
[5] Lu J, Degradation processes and related reliability models, Ph.D. thesis, McGill
University; 1995.
[6] Pettit LI, Young KDS. Bayesian analysis for inverse Gaussian lifetime data with
measures of degradation. Journal of Statistical Computation and Simulation
1999;63(3):217–34.
[7] Barker CT, Newby MJ. Optimal non-periodic inspection for a multivariate
degradation model. Reliability Engineering and System Safety 2009;94:33–43.
[8] Carey MB, Koenig RH. Reliability assessment based on accelerated degradation a case study. IEEE Transactions on Reliability 1991;40(5):499–506.
[9] Tseng ST, Yu HF. A termination rule for degradation experiments. IEEE
Transactions on Reliability 1997;46(1):130–3.
[10] Tseng ST, Hamada M, Chiao CH. Using degradation data from a factorial
experiments to improve ﬂuorescent lamp reliability. Journal of Quality
Technology 1995;27(4):363–9.
[11] Onar A, Padgett WJ. Accelerated test models with the inverse Gaussian
distribution. Journal of Statistical Planning and Inference 2000;89:119–33.

47

[12] Onar A, Padgett WJ. Inverse Gaussian accelerated test models based on
cumulative damage. Journal of Statistical Computation and Simulation
2000;66(3):233–47.
[13] Liao H, Elsayed EA. Reliability inference for ﬁeld conditions from accelerated
degradation testing. Naval Research Logistics 2006;53(6):576–87.
[14] Meeker WO, Escobar LA, Hong Y. Using accelerated life tests results to predict
product ﬁeld reliability. Technometrics 2009;51(2):146–61.
[15] Monroe E, Pan R. Knowledge-based reliability assessment for time-varying
climates. Quality and Reliability Engineering International 2009;25:111–24.
[16] Hamada M, Martz HF, Reese CS, Graves T, Johnson V, Wilson AG. A fully
Bayesian approach for combining multilevel failure information in fault tree
quantiﬁcation and optimal follow-on resource allocation. Reliability Engineering and System Safety 2004;86:297–305.
[17] Graves TL, Hamada MS, Klamann R, Koehler A, Martz HF. A fully Bayesian
approach for combining multi-level information in multi-state fault tree
quantiﬁcation. Reliability Engineering and System Safety 2007;92:1476–83.
[18] Graves TL, Hamada MS, Klamann R, Koehler A, Martz HF. Using simultaneous
higher-level and partial lower-level data in reliability assessments. Reliability
Engineering and System Safety 2008;93:1273–9.
[19] Touw AE. Bayesian estimation of mixed Weibull distributions. Reliability
Engineering and System Safety 2009;94:463–73.
[20] Pan RA. Bayes approach to reliability prediction utilizing data from accelerated life tests and ﬁeld failure observations. Quality and Reliability Engineering International 2009;25(2):229–40.
[21] Chhikara RS, Folks JL. The inverse Gaussian distribution as a lifetime model.
Technometrics 1977;19(4):461–8.
[22] Whitmore GA, Schenkelberg F. Modelling accelerated degradation data using
Wiener diffusion with a time scale transformation. Lifetime Data Analysis
1997;3(1):27–45.
[23] Park C, Padgett WJ. Accelerated degradation models for failure based on
geometric brownian motion and gamma processes. Lifetime Data Analysis
2005;11:511–27.
[24] Pan Z, Balakrishnan N. Reliability modeling of degradation of products with
multiple performance characteristics based on gamma processes. Reliability
Engineering and System Safety 2011;96:949–57.
[25] Guida M, Postiglione F, Pulcini G. A time-discrete extend gamma process for
time-dependent degradation phenomena. Reliability Engineering and System
Safety 2012;105:73–9.
[26] Ntzoufras I. Bayesian Modeling using WinBUGS. 1st ed. New York: John
Wiley and Sons; 2009.
[27] Gelman A, Rubin DB. Inference from iterative simulation using multiple
sequences. Statistical Science 1992;7:457–72.
[28] The University of York [Internet]. Available from: /http://www.york.ac.uk/
depts/maths/tables/l16.htmS.

Similar Image Search with a Tiny Bag-of-Delegates
∗
Representation
Weiwen Tu

Rong Pan

Jingdong Wang

Dept. of Computer Science
Sun Yat-sen University,
Guangzhou, China

Dept. of Computer Science
Sun Yat-sen University,
Guangzhou, China

Microsoft Research Asia
Beijing, China

tuywen@gmail.com

panr@sysu.edu.cn

ABSTRACT

complementary role to text-based image search using associated texts for indexing. For instance, similar image search
can help find images that look like an existing image at hand,
but it is uneasy to use textual queries to describe the search
intention and hence text-based image search cannot return
satisfactory results.
There are two key issues in similar image search1 . One is
the visual similarity evaluation, which is related to feature
selection. It is still unclear what kinds of features (e.g., texture, shape, or color) and what kind of similarity functions
should be used. The other one is how to efficiently search
in the large database, which is related to nearest neighbor
search. Both are important to similar image search [5]. This
paper focuses on the latter issue.
The straightforward solution to nearest neighbor search is
to exhaustively compare the query with each image in the
database to find exact nearest neighbors. This is apparently
infeasible for a large database due to the high computational
cost. Therefore, most research efforts are turned to approximate nearest neighbor (ANN) search. There are two main
categories of ANN search methods. One is hashing. Representative methods include locality sensitive hashing [3],
spectral hashing [16], and complementary hashing [17]. The
other one is spatial partition trees, including kd-trees [1],
PCA-trees [10], hierarchial k-means [8] and so on.
To search over the large database, the ANN search techniques, hashing or spatial partition trees, usually organize
the images using inverted indices. Each inverted index consists of a set of inverted lists, each corresponding to a subset
of points contained in a subspace, e.g., a bucket in a hash
table or a leaf node in a tree, and representing the points in
the bucket or the leaf node. Each inverted index can also
be viewed as a vocabulary, with each word corresponding
to a list. Using multiple inverted indices yields a bag-ofdelegates representation, each delegate corresponding to a
word in one vocabulary. To save an inverted index we need
to save the indices of the lists and the indices of the images
in each list. The main storage cost comes from the indices
of all the images if the number of points in each list is not
too small (e.g., not smaller than 100 for 1M images). Experiments show that spatial partition trees usually perform
better in terms of accuracy and efficiency [7]. However, because of too many overlapped contributions to ANN search
among spatial partition trees, this scheme requires too many

Similar image search over a large image database has been
attracting a lot of attention recently. The widely-used solution is to use a set of codes, which we call bag-of-delegates,
to represent each image, and to build inverted indices to organize the image database. The search can be conducted
through the inverted indices, which is the same to the way
of using texts to index images for search and has been shown
to be efficient and effective.
In this paper, we propose a tiny bag-of-delegates representation that uses a small amount of delegates with a high
search performance guaranteed. The main advantage is that
less storage is required to save the inverted indices while
having a high search accuracy. We propose an adaptive forward selection scheme to sequentially learn more and more
inverted indices that are constructed based on subspace partition, e.g. using spatial partition trees. Experimental results demonstrate that our approach can require a smaller
number of delegates while achieving the same accuracy and
taking similar time.

Categories and Subject Descriptors
H.3.1 [Information Storage and Retrieval]: Content
Analysis and Indexing—Indexing methods

General Terms
Algorithms, Experimentation

Keywords
Bag-of-delegates, spatial partition trees, adaptive forward
selection, similar image search

1.

INTRODUCTION

Similar image search over a large image database has been
a hot yet challenging problem in image retrieval. It plays a
∗This work was performed at Microsoft Research Asia.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
MM’12, October 29–November 2, 2012, Nara, Japan.
Copyright 2012 ACM 978-1-4503-1089-5/12/10 ...$10.00.

Area Chair: Qi Tian

jingdw@microsoft.com

1
In this paper, we differentiate similar image search from
particular object retrieval [9] that intends to search the images containing the same object to the one appearing in the
query image.

885

trees to get a good performance, and consequently takes a
large amount of storage and time costs.
In this paper, we propose a tiny bag-of-delegates descriptor to describe an image, which uses a small amount of spatial partition trees but achieves a good search performance.
We present an adaptive forward selection approach, which
sequentially chooses more and more spatial partition trees
adaptively according to previously-identified trees. Experimental results demonstrate that the proposed approach is
very efficient and effective and requires a smaller amount of
delegates. Searching for top 100 similar images over 1M tiny
images at the accuracy 0.75 (Figure 2(a)), our approach only
requires about 50 inverted indices, 53 of the number of indices
required by traditional random PCA-tree-based method. A
demo [14] based on the technology in [13] and this paper will
be shown in ACMMM12.

2.

on average there exist about 20% overlaps among a pair of
spatial partition trees, while 12% in our approach. The overlap between two trees is computed based on the number of
the true neighbor pairs appearing in both trees.

2.2 Vocabulary Construction
In this paper, we investigate the problem of using a small
amount of trees but with the search performance guaranteed, which is less studied before to the best of our knowledge. Our idea is to construct the trees using the supervised information. In similar image search, the supervised information is the true nearest neighbors of each image. We denote the true neighbors of an image Ii by a
list, Li = [ni1 ni2 · · · nig ], where nij is the index of a
similar image. The nearest neighbor candidates discovered
from the bag-of-delegates representation can be written as
Ci = F1 (wi1 ) ∪ · · · ∪ FM (wiM ). The criterion checking if Ci
i ∩Li |
is satisfactory is to check if the recall ri = |C|L
is maxii|
mized. The whole criterion is P
to find trees to maximize the
N
average recall R̄ = E(ri ) = N1
i=1 ri .
The problem is formally described as follows. Given a set
of Z candidate inverted indices T = {T1 , · · · , TZ }, the goal
is to find M (M 6 Z) inverted indices such that the recall
is maximized. This is equivalent to selecting as few inverted
indices as possible to guarantee that the recall is not lower
than the target recall. For convenience, we will describe our
solution in terms of the former one.
We denote a set of M candidates as TM = {Ti1 , · · · , TiM },
and the recall from them as R̄(TM ). The objective function
is then written as

TINY BAG-OF-DELEGATES

Given a large image set I = {I1 , I2 , · · · , IN }, the
goal is to find a bag-of-delegates descriptor to represent each image, e.g., Ii is represented by M delegates,
{wi1 , · · · , wiM }. To this end, we need to build M vocabularies {V1 , · · · , Vm , · · · , VM }, where Vm = {vm1 , · · · , vmJ }
with J being the size of each vocabulary. From each vocabulary an element is selected as a delegate for each image.
To conduct similar image search, we organize the image
set using M inverted indices {T1 , · · · , Tm , · · · , TM }, each
corresponding to a vocabulary, respectively. An inverted
index Tm is composed of J lists {Tm1 , · · · , TmJ }, with each
corresponding to a word, e.g., Tmj corresponds to vmj . Each
list consists of images that are mapped to the word vmj .
Given an image Ii whose bag-of-delegates is defined as
{wi1 , · · · , wiM }, we find the lists {F1 (wi1 ), · · · , FM (wiM )},
where Fm (w) is a function mapping from the delegate w to
the list in the inverted index Tm , then regard all the images
in these lists as the candidate similar images, and finally
order these images according to their similarities computed
with their features.

∗
TM
= arg maxTM ⊂T R̄(TM ).

(1)

This optimization problem is a combinatorial problem and
NP-hard. Selecting the optimal inverted indices resembles
the ensemble learning problem in pattern recognition that
selects a subset of classifiers to yield the best classification performance. We propose to adopt the greedy search
schemes that are studied in ensemble learning, because they
seem to be particularly computationally advantageous and
robust against overfitting. There are two basic schemes:
forward selection and backward elimination. In forward selection, the inverted indices are progressively incorporated,
yielding larger and larger subsets, whereas in backward elimination one starts with the set of all indices and progressively
eliminates the least promising ones.
We use the forward selection scheme as it may loose the
requirement, enumerating all the candidate indices at the
beginning. We progressively generate the candidate inverted indices for each step. At the beginning, we randomly generate a set of candidate inverted indices T (1) =
{T1 , · · · , Tj , · · · , TM1 }. We evaluate each inverted index Tj
to compute the recall, and then identify the first inverted
index that corresponds to the largest recall. Denote the identified inverted index as T1∗ and the current solution is
T1∗ = {T1∗ }.
The later steps sequentially find the inverted indices one
by one. Considering the (t + 1) step, we have found t indices
Tt∗ = {T1∗ , · · · , Tt∗ }. We generate a set of new candidates
T̄ (t+1) = {TMt +1 , · · · , TMt+1 } and form the whole candidates T (t+1) = T t ∪ T̄ (t+1) . The objective of identifying the
(t + 1)-th index is as follows,

2.1 Background
The basic requirement to vocabulary construction is to
map similar images to the same word. A single vocabulary
often is not satisfactory. Thus, multiple vocabularies are
exploited so that a pair of similar images are most likely to
be mapped to the same word at least in one vocabulary.
One of the common solutions to multiple vocabulary construction is locality sensitive hashing (LSH) [3]. LSH builds
multiple hash tables and maps each image into a bucket in
each hash table. Here, a table is equivalent to a vocabulary
and a bucket is equivalent to a word. The main characteristics of LSH include that (1) a hash table with a code length l
uses only l hash functions, and (2) the tables are constructed independently on the data set. The two points lead to a
worse performance than spatial partition tree-based methods [1, 10], and even learning-based hashing methods [16,
17] are still not good due to the first characteristic.
It has been shown that in practice spatial partition trees
produce promising performance for approximate nearest
neighbor search [7] and hence for similar image search. Similar to LSH, multiple random spatial partition trees are usually constructed. A larger amount of trees yield a better performance, while requiring more storage to save the inverted
indices and accordingly more query time. We observed that

∗
T(t+1)
= arg maxT(t+1) ∈T (t+1) R̄(Tt∗ ∪ {T(t+1) }).

886

(2)

Algorithm 1 Adaptive forward selection

indices to reach the same accuracy, and the latter aims to
show that our approach requires to access fewer number of
images, equivalently less time cost, to get the same accuracy.
We justify the proposed approach by
showing the improvement for the indices
constructed
from
random
PCA-trees.
It
is
theoretically
shown in [12] that the
principal-directionFigure 1:
Performance
based way to hierarcomparison showing the
chically partition the
superiority of PCA-trees
points can guarantee
over RP-trees and VPthat the diameters of
trees.
the subsets are reduced
quickly. In our implementation, the principle direction is
computed by the Lanczos algorithm [6]. Compared with
other space partitioning ways, e.g., random projections
trees (RP-trees) [2], vantage-point trees (VP-trees), our
experiments [15] show that the principal-direction-based
way is more efficient and effective, as illustrated in Figure 1.
Results. The comparison of the accuracy performance is
shown in Figure 2. We show the performance over various
numbers of indices. The horizontal axis corresponds to the
number of the trees. The vertical axis corresponds to the
accuracy score. To make the comparison more concrete,
we show the performance for searching different numbers of
true nearest neighbors. It can be seen that our approach
is consistently superior for both different numbers of trees
and various numbers of true nearest neighbors. As shown in
Figure 2(a) with the buckets size (the number of points in a
leaf node) being 500, searching for 100 NNs, our approach
gets a 0.08 improvement when using 60 trees. Considering
the case that we want to get a 0.7 accuracy, our approach
only needs 40 trees, while random PCA-trees need 60 trees.
We also report the performance against different sizes of
buckets. Using the same number of trees, the size of the
bucket roughly determines how many candidates there are
for an image. From Figures 2(b) and 2(c) with the bucket
sizes being 300 and 100, it can be seen that our approach
consistently performs better.
We also show the comparisons in Figure 3 according to
accuracy vs. # accessed images, which can roughly reflect
the time cost when achieving the same accuracy. We can
see that our approach consistently performs better. From
Figure 3(a), searching for 100 NNs, our approach gets about
0.04 improvement when searching 16, 000 images.
Out-of-sample search. To demonstrate the performance
of query by an out-of-sample image, we sample an additional
10K images from the tiny images data set to form an out-ofsample data set, with the guarantee that there is no overlap
with the 1M reference images. We build the ground truth
nearest neighbors for the out-of-sample images by comparing
them with the 1M images in a brute-force manner. The
comparisons are shown in Figures 4 and 5. It can be seen
that our approach consistently performs better.

// L: the set of all pairs of neighboring points;
// Q: a subset of L, maintaining the pairs of images that do not
appear in the same leaf node yet;
// e: the cardinality of Q; R: the obtained trees;

0.8

1. Initialization:
Q ← L, t ← 0, e ← |Q|, R ← ∅;
2. repeat
3.
Candidate proposal:
Randomly generate spatial partition trees T̄ per Q;
4.
Candidate selection:
Choose the spatial partition tree T from T̄ that keeps the large
number of pairs in Q;
5.
Update:
Discard all the pairs of points lying in the same bucket in T
from Q;
t ← t + 1;
e ← |Q|;
R ← R ∪ {T };
6. until e 6 ǫ && t > τ ;
7. return R;

0.7
0.6

accuracy

0.5

0.3
0.2

RP−trees
VP−trees
PCA−trees

0.1
0

0

20

40

60

80

100

indices

To generate more helpful candidates T̄ (t+1) , we exploit
previously-identified indices to adaptively generate the candidate indices. To this end, we maintain a set Q = {(Ii , Ij )},
where Ii and Ij are truly neighboring points but not appear
in the same bucket in the previously-identified indices Tt∗ .
Q initially includes all the pairs of neighboring points. This
set becomes smaller when more indices are discovered.
The following describes how to use Q = {(Ii , Ij )} to build
a random spatial partition tree, thus yielding an inverted
index. In the branch of each internal node v in the spatial
partition tree, we randomly sample a set of points, compute
top r sparse principal directions from them, and then form r
candidate partition hyperplanes, each formed by a principal
direction and the medium of the projections of the associated
data points along this direction. We select the best partition
hyperplane from the r ones so that the maximum number
of pairs of points, which appear in Q and belong to the set
of points associated with the internal node, lie in the same
side. The algorithm is outlined in Algorithm 1.
Our algorithm is related to adaboost [4] for ensemble
learning and complementary hashing [17]. It is different from them in that the problem we intend to solve is to
combine tree-based indices. Moreover, our problem is more
challenging because there are no closed-form solutions.

3.

0.4

EXPERIMENTS

Data set. We conduct our experiments on the tiny images
data set [11]. The tiny images data set consists of around
80M images, each being a 32 × 32 color image. We sample
1M tiny images to form the reference data set. We use
a global GIST descriptor to represent each image, a 384D
vector describing the texture within localized grid cells.
Evaluation criteria. We adopt the average accuracy score
to evaluate the quality of search with inverted indices. Given an image Ii , we first collect the points that lie in the same
bucket to Ii at least in one of the M inverted indices, and
reorder them according to the distances computed between
them and Ii to find the top K images Ri , where K = |Li |
is the number of true nearest neighbors. The accuracy is
i ∩Li |
computed as ai = |R|L
. The average accuracy is comi|
P
N
puted as ā = N1
a
.
We
compare the performances in
i
i=1
terms of two aspects: accuracy vs. # inverted indices and
accuracy vs. # accessed point numbers. The former aims to
show that our approach requires fewer number of inverted

4. CONCLUSIONS
In this paper, we study the problem of similar image
search and propose a tiny bag-of-delegates representation
approach. This approach uses a small number of inverted

887

0.7

0.8

0.8

0.6

0.7

0.7

0.5

0.6

0.5

0.3
10

20

30

40

50

60

70

80

90

0.6

0.5

AFS g=100
PCA g=100
AFS g=300
PCA g=300
AFS g=500
PCA g=500

0.4

accuracy

0.9

accuracy

accuracy

0.9

0.3
10

100

0.3

AFS g=100
PCA g=100
AFS g=300
PCA g=300
AFS g=500
PCA g=500

0.4

20

30

40

50

60

70

80

90

0.4

AFS g=100
PCA g=100
AFS g=300
PCA g=300
AFS g=500
PCA g=500

0.2

0.1

100

10

20

30

40

50

60

70

80

90

100

0.8
0.75

0.7

0.7

0.65

0.65

0.6

0.6

0.55
0.5
AFS g=100
PCA g=100
AFS g=300
PCA g=300
AFS g=500
PCA g=500

0.45
0.4
0.35
0.3

(a)

4000

6000

8000

10000
images

12000

14000

0.7

0.6

0.5
accuracy

0.8
0.75

accuracy

accuracy

indices
indices
indices
(a)
(b)
(c)
Figure 2: Accuracy vs. #indices over different bucket sizes: (a) 500, (b) 300 and (c) 100. g is the number of
target NNs. AFS means our approach. PCA means random PCA-trees.

0.55
0.5

0.4
0.35

16000

(b)

0.3

AFS g=100
PCA g=100
AFS g=300
PCA g=300
AFS g=500
PCA g=500

0.45

0.3
3000

4000

5000

6000

7000 8000
images

9000

10000 11000 12000

0.4

AFS g=100
PCA g=100
AFS g=300
PCA g=300
AFS g=500
PCA g=500

0.2

(c)

0.1
500

1000

1500

2000

2500
images

3000

3500

4000

4500

0.9

0.9

0.8

0.8

0.7

0.7
accuracy

accuracy

Figure 3: Accuracy vs. #(accessed images) over different bucket sizes: (a) 500, (b) 300 and (c) 100.

0.6

0.5

0.4

0.3
10

0.6

0.5

AFS g=100
PCA g=100
AFS g=300
PCA g=300
AFS g=500
PCA g=500

[3] M. Datar, N. Immorlica, P. Indyk, and V. S. Mirrokni.
Locality-sensitive hashing scheme based on p-stable
distributions. In Symposium on Computational Geometry,
pages 253–262, 2004.
[4] Y. Freund and R. E. Schapire. A decision-theoretic
generalization of on-line learning and an application to
boosting. J. Comput. Syst. Sci., 55(1):119–139, 1997.
[5] B. Kulis, P. Jain, and K. Grauman. Fast similarity search for
learned metrics. IEEE Trans. Pattern Anal. Mach. Intell.,
31(12):2143–2157, 2009.
[6] C. Lanczos. An iteration method for the solution of the
eigenvalue problem of linear differential and integral operators.
Journal of Research of the National Bureau of Standards,
45(4):255–282, 1950.
[7] M. Muja and D. G. Lowe. Fast approximate nearest neighbors
with automatic algorithm configuration. In VISAPP (1), pages
331–340, 2009.
[8] D. Nistér and H. Stewénius. Scalable recognition with a
vocabulary tree. In CVPR (2), pages 2161–2168, 2006.
[9] J. Philbin, O. Chum, M. Isard, J. Sivic, and A. Zisserman.
Object retrieval with large vocabularies and fast spatial
matching. In CVPR, 2007.
[10] R. F. Sproull. Refinements to nearest-neighbor searching in
k-dimensional trees. Algorithmica, 6(4):579–589, 1991.
[11] A. Torralba, R. Fergus, and W. T. Freeman. 80 million tiny
images: A large data set for nonparametric object and scene
recognition. IEEE Trans. Pattern Anal. Mach. Intell.,
30(11):1958–1970, 2008.
[12] N. Verma, S. Kpotufe, and S. Dasgupta. Which spatial
partition trees are adaptive to intrinsic dimension? In UAI,
pages 565–574, 2009.
[13] J. Wang and S. Li. Query-driven iterated neighborhood graph
search for large scale indexing. In ACM Multimedia, 2012.
[14] J. Wang, J. Wang, X.-S. Hua, and S. Li. Scalable similar image
search by joint indices. In ACM Multimedia, 2012.
[15] J. Wang, J. Wang, G. Zeng, Z. Tu, R. Gan, and S. Li. Scalable
k-nn graph construction for visual descriptors. In CVPR, pages
1106–1113, 2012.
[16] Y. Weiss, A. Torralba, and R. Fergus. Spectral hashing. In
NIPS, pages 1753–1760, 2008.
[17] H. Xu, J. Wang, Z. Li, G. Zeng, S. Li, and N. Yu.
Complementary hashing for approximate nearest neighbor
search. In ICCV, pages 1631–1638, 2011.

AFS g=100
PCA g=100
AFS g=300
PCA g=300
AFS g=500
PCA g=500

0.4

0.3
10

20

30

40

50
60
indices

70

80

90

100

0.8

0.8

0.75

0.75

0.7

0.7

0.65

0.65

0.6

accuracy

accuracy

(a)
(b)
Figure 4: Accuracy vs #indices for out-of-sample
queries over bucket sizes: (a) 500 and (b) 300.

0.55
0.5
AFS g=100
PCA g=100
AFS g=300
PCA g=300
AFS g=500
PCA g=500

0.45
0.4
0.35
0.3

20

30

40

50
60
indices

70

80

90

100

0.6
0.55
0.5
AFS g=100
PCA g=100
AFS g=300
PCA g=300
AFS g=500
PCA g=500

0.45
0.4
0.35
0.3
3000

(a)
(b)
Figure 5: Accuracy vs #(accessed images) for outof-sample queries over bucket sizes: (a) 500 and (b)
300.
4000

6000

8000

10000
images

12000

14000

16000

4000

5000

6000

7000 8000
images

9000

10000 11000 12000

indices leading to a small amount storage cost, but guarantees high search quality and efficiency. We present an adaptive forward selection scheme to effectively build the indices.
Empirical results justify the powerfulness of our approach.

Acknowledgements
Weiwen Tu and Rong Pan were supported by National Natural Science
Foundation of China (61003140, 61033010).

5.

REFERENCES

[1] J. L. Bentley. Multidimensional binary search trees used for
associative searching. Commun. ACM, 18(9):509–517, 1975.
[2] S. Dasgupta and Y. Freund. Random projection trees and low
dimensional manifolds. In STOC, pages 537–546, 2008.

888

Constrained Collective Matrix Factorization
Yu-Jia Huang

Evan Wei Xiang

Department of Computer
Science
Sun Yat-sen University
Guangzhou, China

Department of Computer
Science and Engineering
The Hong Kong University of
Science and Technology
huangyj9@mail2.sysu.edu.cn Hong Kong, China

wxiang@cse.ust.hk

ABSTRACT
Transfer learning for collaborative filtering (TLCF) aims to
solve the sparsity problem by transferring rating knowledge
across multiple domains. Taking domain difference into account, one of the issues in cross-domain collaborative filtering is to selectively transfer knowledge from source/auxiliary
domains. In particular, this paper addresses the problem of
inconstant users (users with changeable preferences across
different domains) when transferring knowledge about users
from another auxiliary domain. We first formulate the problem of inconstant users caused by domain difference and
then propose a new model that performs constrained collective matrix factorization (CCMF). Our experiments on
simulated and real data show that CCMF has superior performance than other methods.

Categories and Subject Descriptors
H.3 [INFORMATION STORAGE AND RETRIEVAL]:
Information Search and Retrieval

Keywords
Collaborative Filtering, Transfer Learning, Collective Matrix Factorization, Inconstant Users.

1.

INTRODUCTION

Collaborative filtering [9] in recommender systems aims to
predict users’ ratings in the future on a set of items based
on a collection of similar users’ rating history. In real-life
recommender systems, the rating matrix may be extremely
sparse. As reported in [11], the density of the available ratings in commercial recommender systems is often less than
1%. To alleviate the sparsity problem in collaborative filtering, one common approach is to pool together the rating
∗Corresponding author. This work was supported by National Natural Science Foundation of China (61003140,
61033010).

Rong Pan

∗

Department of Computer
Science
Sun Yat-sen University
Guangzhou, China

panr@mail.sysu.edu.cn

data from multiple rating matrices in related domains for
knowledge transfer and sharing. Methods like [2, 3] assume
that both users and items in an auxiliary domain are related to the target domain, while in practice it is often much
easier to find an auxiliary data source with either related
users or related items but not both. In this paper, we address the problem of user-sided transfer learning, although
the method proposed in this paper can also be applied to
item-sided transfer learning. One well-known approach to
such one- side-related problem is Collective matrix factorization (CMF) [12]. CMF is proposed for jointly factorizing two matrices with the constraint of sharing one-side
(user or item) latent features. CMF improves the prediction in target domain by increasing the rating records for
each user/item. When the domain difference is small between source domain and target domain, e.g. transferring
knowledge from a movie recommender system to another,
this approach can easily improve prediction performance by
simply jointing data from two systems.
CMF assumes that all users’ features are constant. However, in real life, some users may change their features / preferences across different domains. For example, users having
a preference over light music may like horror movies instead
of ones about love stories because horror movies entertain
audience in much more ways (pictures, stories and so on)
than horror music (sounds). Even though the target domain and auxiliary domain are of same type (e.g. movie),
users may also behave a little differently when rating different item sets. We refer “inconstant users” to those with
changeable preferences across different domains. For these
users, it would not hold that the source domain and target
domain share a user latent feature matrix. To improve recommendation for inconstant users, we need a new method
that takes the change of user feature into account.
To model the change of user feature caused by domain
difference, we make two assumptions:
• Bias of latent features are user-dependent. The bias
between user features across different domains varies
from one user to another. Therefore, to recommend
items for both constant and inconstant users, our model
should allow user-dependent bias of latent features.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
RecSys’12, September 9–13, 2012, Dublin, Ireland.
Copyright 2012 ACM 978-1-4503-1270-7/12/09 ...$15.00.

• The change lies in history. What items a user has
rated implies what the user was looking for, thus we
can infer whether and how much a user has inconstant
hobbies through his/her rating history. More specifically, if the items rated by a user are similar to those

237

ge of user

caused by
ions:


where N (x µ, σ 2 )is the probability density function of the
tgt
Gaussian distribution with mean µ and variance σ 2 . Iij
src
and Iij are the indicator functions that are equal to 1 if
user rated item and equal to 0 otherwise.
The zero-mean spherical Gaussian priors are also placed
on user and item feature vectors:


 Yn
 src 2
src
p U src σU
=
N (Ui.src |0, σU
I),
(3)
i=1


 Ymsrc


2
p V src σVsrc
=
N Vj.tgt |0, σVsrc I ,
(4)
j=1



Y


m
tgt
2

(5)
=
N Vj.tgt 0, σVtgt I .
and p V tgt σVtgt

in source/auxiliary domain, we assume that the user
tends to have constant preferences and vice versa.
To model the assumptions above, we extend CMF to a
new matrix factorization model named Constrained CMF,
denoted as CCMF in this paper. Unlike CMF, CCMF does
not assume that the users share a same feature matrix in
source domain and target domain.
The remainder of this paper is organised as follows. In section 2, we first introduce the problem setting. In section 3,
we formulate the model. Then we experimentally validate
the effectiveness of the proposed models in section 4. Related work is introduced in section 5. Finally we conclude
this paper in section 6.

2.

j=1

To transfer knowledge about users from auxiliary domain,
[12] factorizes two matrices with the constraint of sharing
one-side (user or item) latent features. Instead of jointly
factorizing rating matrices in source and target domain with
a same user feature matrix (U src = U tgt ), our model factorizes Rsrc with U src and V src and factorizes Rtgt with U tgt
and V tgt . We transfer the user feature matrix U src learned
in source domain to help factorize the rating matrix in target
domain by conducting a constraint on U src andU tgt :

PROBLEM SETTING

In our problem setting, we are given one source domain
Dsrc , say music, and one target domain Dtgt (movie in our
experiments). u1 , u2 , . . . , un denote n users having rating
records in both Dsrc and Dtgt . In rating matrix Rsrc of
src
Dsrc , users make ratings on msrc items {v1src , v2src , . . . , vm
},
src
src
src
the rating made by ui on vj is denoted as Rij . Similarly,
in rating matrix Rtgt of Dtgt , users make ratings on mtgt
tgt
items {v1tgt , v2tgt , . . . , vm
}, and the rating made by ui on
tgt
tgt
tgt
vj is denoted as Rij . Both Rsrc and Rtgt are observed
and another n × mtgt rating matrix Rtest is for testing and
is unknown
training
the in
model.
All the
elements
in 𝑅 tgt , 𝑅whileand
𝑅 are
the same
scale(e.g.
1- from
test
Rsrc5).
, R
and R
are atuse
theofsame
(e.g.to 15 or 0Our goal
is to make
𝑅 scale
and
𝑅
help
src
tgt
1 [5]).
Our
goal
is
to
make
use
of
R
and
R
to
predict the missing values in 𝑅 . Figure 1 shows help
predict the missing values in Rtest . Figure 1 shows a toy
a toy example of our problem.
example of our problem.

or Ui.tgt

we extend
del named
this paper.
at the users
omain and

ganised as
he problem
odel. Then
ness of the
d work is
nclude this

one source
et domain
𝑢 ,…,   𝑢



 Ymtgt

p Y σY 2 =
N Yj. 0, σY 2 I .
j=1

3. Constrained CMF

ln p

recommender systems is to factorize the user-item

A well-known and effective approach to recommender sysrating matrix, and utilize the latent user feature
tems is to factorize the user-item rating matrix, and utilize
feature
matrix
make
prediction
the matrix
latent and
user item
feature
matrix
and toitem
feature
matrix to
forprediction
future ratings[4,
5, 6,ratings
7].
make
for future
[1, 4, 8, 13, 14] .
In order
to learn
the latent
characteristics
of theofusers
In order
to learn
the latent
characteristics
the and
items
in source
and target
domain,
employ
probabilistric
users
and items
in source
and we
target
domain,
we
matrix
factorization[10]
factorizefactorization[5]
the user-item matrix.
employ
probabilistricto matrix
to
Thefactorize
conditional
over theThe
observed
ratings in
the distributions
user-item matrix.
conditional
two domains are defined as:
distributions
ratings in two
 over the observed


 tgt tgt tgt 2
p Rtgt
, V as:
, σR
=
domains
areUdefined
n
tgt
h 
 =
T tgt 2 iIij
𝑝 𝑅Y Y
𝑈 mtgt, 𝑉 N ,   𝜎
tgt  tgt
Rij
Ui. Vj.tgt , σR
, (1)
j=1

i=1

and p R

𝑝 𝑅

U

,V

𝑈 .src 2𝑉.

src

, σR

=

,   𝜎

where C is a constant that does not depend on the parameters and l denotes the feature number.
Maximizing the log-posterior over latent features with hyperparameters (i.e., the observation noise variance and prior
variances) kept fixed is equivalent to minimizing the following sum-of-squared-errors objective functions with quadratic

, (1)

src
𝑈 msrc
, 𝑉h ,  𝜎 src =
T src 2 iIij
N Rij |Ui.src Vj.src , σR
, (2)

n Y
Y

  ∏

∏

i=1

j=1

𝑁 𝑅

𝑈.

𝑉

.

,   𝜎

(8)



Y, U src , V src , V tgt Rsrc , Rtgt , σR , σY , σVsrc , σVtgt , σVsrc

n msrc
2 
1 XX
src
src
src T
I
R
−
U
·
V
(9)
=−
ij
ij
i.
j.
2σR 2 i=1 j=1
PM tgt !
n mtgt
1 XX
tgt
src
k=1 Iik Yk
−
Iij
Rij − Ui. + P
·
M
tgt
2σR 2 i=1 j=1
k=1 Iik
mtgt
mtgt
 
X tgt
1 X
1
T 2
T
T
Vj.tgt
−
Y
·
Y
−
Vj. · Vj.tgt
j.
j.
2
tgt
2σY 2 j=1
2σV
i=1
!
mtgt
n


1 XX
tgt 2
2
2
−
Iij ln σR
+ nl ln σY + nl ln σV
+ C,
2 i=1 j=1

A well-known and CMF
effective approach to
CONSTRAINED


  ∏ ∏ src  𝑁
src 𝑅

(7)

Hence, through a Bayesian inference, we have the log of the
posterior distribution over the user and item feature and Y :

Figure
Figure 11:AAToy
ToyExample
Example

3.

(6)

Here Ui.src is the prior mean of Ui.tgt and the second term
is the user feature bias caused by domain difference. Yk.
captures the feature bias for a user if the user rated item
vktgt . Intuitively, the more similar vktgt is to the items in
source domain, the closer Yk. is to zero. The Gaussian prior
for Y is given as follows:

dependent.
res across
ne user to
d items for
users, our
ent bias of

at items a
e user was
hether and
nt hobbies
ry. More
a user are
ry domain,
s to have
sa.

Pmtgt tgt
k=1 Iik Yk.
P
mtgt tgt ,
k=1 Iik
Pmtgt tgt
k=1 Rik Yk.
= Ui.src + P
mtgt
tgt .
k=1 Rik

Ui.tgt = Ui.src +

,

(2)

where 𝑁(𝑥|𝜇,   𝜎 ) is the probability density
function of the Gaussian distribution with mean μ
and variance   𝜎 . 𝐼 , as well as 𝐼 , is the
indicator function that is equal to 1 if user 𝑢 rated
item 𝑣 and equal to 0 otherwise.

238

s.
n
e

g
t

)

)

∑

∑

e
d

9)

e

t

𝑅

−    𝑈 .

∙ 𝑉.

+

regularization terms:
∑
∑ n msrc
𝐼
𝑅
−
𝑈
+
∙

.
2 
∑
1 X X src  src
E=
Iij
Rij − Ui.src · Vj.src T
2 i=1 j=1
!
𝑉.
∙n mtgt+ ∑ ‖𝑈 . ‖ P+
M
tgt
1 X X tgt
tgt
k=1 Iik Yk
− Ui.src + P
Rij
·
Iij
+
M
tgt
2 i=1 j=1
k=1 Iik
∑
∑
𝑉  λ X
+
𝑉 msrc
+
n
λsrc. X 
T .2
U

Vj.src 
2
+
Vj.tgt
kUi.src k2F ro + V
F ro
2 i=1
2 j=1
∑
‖𝑌 . ‖
,
(10)
mtgt
mtgt
X

λtgt X 
2

V tgt 
2 + λY
+ V
(10)
j.
  
   kYj. kF ro ,
  
F
ro
2 j=1
2 j=1
where 𝜆 =   
, 𝜆 =   
, 𝜆 =   
，
  
  
2
tgt
2
2
2
where λY = σR
/σY2 , λsrc
= σR
/σVsrc 2 , λtgt
, and
V
V = σR /σV
andk ‖· k. 2F‖ro denotes
denotes
the
Frobenius
norm.
Themodel
the Frobenius
norm. The
graphical
is shown model
in Figureis2.shown in Figure 2.
graphical

4.

∑

d
n
r
e
e
s

)

𝐼

=∑

𝐼

∙

𝑈.

∑

.

.

𝑅

∙ 𝑈.

+

∑

+

∑
∑

+ 𝜆 ∙ 𝑉.

𝑉.

−

,

(14)

EMPIRICAL ANALYSIS

4.1

Experimental Setting

To check whether CCMF can fit with different settings,
we evaluate CCMF on two data sets.
One is a simulated dataset sampled from the Netflix dataset.
The Netflix data set used in the experiments is constructed
as follows. We first randomly extracted a 10, 000 × 16, 000
dense rating matrix R from the Netflix data, and take the
submatrices Rtgt = R1∼10000,1∼8000 as the target rating matrix, and Rsrc = R1∼10000,8001∼16000 as the user side source
data, so that Rtgt and Rsrc share only common users.
The other is a real dataset crawled from Douban1 , which is
launched in 2005 and is a Chinese SNS website allowing registered users to record ratings and reviews related to movies,
books, and music. We crawled 290, 633 rating records rated
by 5, 000 users on 3, 000 musical items and 10, 000 movies.
In the experiments, we used the musical rating matrix as
auxiliary training data. Then we sampled randomly from
the movie rating matrix to generate training data and testing data. The final datasets are summarized in Table 1. We
adopt two evaluation metrics: RMSE (Root Mean Square
Tableand
1: Description
DoubanError).
data and Netflix
Error)
MAE (Mean of
Absolute
data
Table 1: Description of Douban data and Netflix data
dataset
target(training)
Netflix target(testing)
auxiliary
target(training)
Douban target(testing)
auxiliary

Then a local minimum of the objective function can
be found by performing gradient descent in Y, 𝑈 ,
𝑉 Figure
  and 𝑉2: Graphical
.
model of CCMF

type
movie
movie
movie
movie
movie
music

sparsity
<=0.60%
0.50%
2.50%
<=0.60%
0.60%
1.50%

We compare out CCMF methods with two non- transfer

learning methods: the UserMean and PMF [10], as well as
Figure 2: Graphical model of CCMF
CMF
4.2 [12].
Evaluation Metrics
4. CCMF,
EMPIRICAL
ANALYSIS
To learn
we first calculate
the partial derivatives
We adopt two evaluation metrics: RMSE (Root
Experimental Results
the objective
function
respect
to each variable:
To oflearn
wewith
first
calculate
the partial 4.2
4.1CCMF,
Dataset
Description
Mean Square Error) and MAE (Mean Absolute

To check
whether CCMF
can fit src
with different
∂E
Tfunction
src
src objective
derivatives
with
respect to
− Ron
· Vdata
Ui.src · Vj.src
= of Ithe
ij two
j.
ij we evaluate
src settings,
CCMF
sets.
m
src
X

∂Ui.







j=1

each variable:
mOne
X

!
!
is a simulatedPdataset
M
tgt sampled from the
src
tgt T
tgt
k=1 Iik Yk
Netflix dataset.Ui.The+ Netflix
setVj.used− in
+
Rijthe ·
PM data
tgt
Iik
k=1 as
j=1
experiments
is
constructed
follows.
We
first
=

src
randomly
.
Vj.tgt + λextracted
(11)
U · Ui. , a 10,000 × 16,000 dense rating
!
! take the submatrix
and
PM data,
mtgt R from the Netflix
tgt
X tgt
I
Y
∂E
T
k
src
tgt
tgt
ik
k=1
∑
2
𝐼=matrices
∙ Iij 𝑈
𝑉V. j.target−+Rrating
𝑅 . =𝑉
as∙ the
·
U𝑅i.. ~ + −
, 𝑅
~
P
ij
M
tgt
∂Yi.src
j=1
matrix,
and 𝑅
= 𝑅 ~ k=1, Iik ~
as the user
!
source
data, so ∑that 𝑅 and 𝑅
only
Iip
xtgt
ij
2∑
𝐼side
∙
𝑈
𝑉 . share
− 𝑅(12)
+
. λY+· Yi.∑,
P
common
users.
M
I
tgt

tgt
Iij

k=1

∑

∙

ik

𝐼𝑠𝑟𝑐
𝑖𝑗 ∙

𝑇

𝑠𝑟𝑐
𝑈𝑠𝑟𝑐
− 𝑅𝑠𝑟𝑐
∙ 𝑈𝑠𝑟𝑐
𝑖. 𝑉𝑗.
𝑖𝑗
𝑖.

239

𝑖=1

+𝜆 ∙ 𝑉

,

|

̂ |

better than non-transfer method PMF.

The final datasets are summarized in Table 1.

∂𝐸
=2
∂𝑉 .

∈

where
𝑟 and
𝑟̂ are themethods
true and(CMF
predicted
ratings, that
• Matrix
factorization
and CCMF)
transfer knowledge
from
domains
perform much
respectively,
and |𝑇 | is
the auxiliary
number of
test ratings.

ratingminimum
matrix as
auxiliary
training
data.
Then
we
Then a local
of the
objective
function
can be
found
src rating
src
tgt to
sampled
randomly
from
the
movie
matrix
by performing
gradient
∙∑
+ 𝜆 descent
∙ 𝑌 . , in Y , U , V and V .(12)
generate training data and testing data.
𝑁

,,

MAE
=
,
(16)
• UserMean
is worse
than| all
| other methods.

1
 from Douban

X
The
is
a real
dataset
crawled
.
N other
∂E
src
src
src T
src
src
=
I
·
U
·
V
−
R
·
U
ij
i.
j.
ij
i.
src Douban,
launched in 2005, is a Chinese SNS
i=1
∂V
𝑉 . j. + website
𝜆 ∙ 𝑈 allowing
, registered users to record ratings
(11)
.
src
+λsrc
·
V
,
(13)
j.
andV reviews
related to P
movies, books,
! and music.
!
N
M
tgt
X
Iik
Ykpopular
Douban
∂E
src ranks the
src 107th
tgt T website
tgt
k=1most
=
Iij ·
Ui. + PM tgt
Vj.
− Rij ·
and 20th
in China
according
to Alexa
∂Vj.src worldwide
∑ Iik
k=1
i=1
2
!!
= ∑ Internet
𝐼 . We
∙ PMcrawled
𝑈tgt. 290,633
+
𝑉 . rated
−
rating records
∑ src items
Iik
Yk musical
.
src users
tgt and 10,000
k=1
by 5,000
on
3,000
Ui. + PM tgt
+ λV · Vj. .
(14)
Iik
movies. In thek=1
experiments,
we used the musical

𝑅

We denote the CCMF methods that utilize different conError):
straints given in Eqs. (6) and (7) as CCMF1 and CCMF2,
)
respectively. The best ∑results
parameters
,,
∈of (usinĝ different
RMSE
  
,
as described
in the=previous
section
are
reported
in(15)
Table 2.
| |
We can make the following observations:

(13)

4.3 Baselines and Parameter
• CCMF and CCMF2, which model the inconstant users
Settings
outperform CMF at all sparsity levels.

We compare out CCMF methods with two non• CCMF1
and CCMF2
close results,
but as the
transfer
learning
methods:achieves
the UserMean
and PMF
training
data
in
target
domain
becomes
very
sparse (<
[Salakhutdinov and Mnih, 2008], as well as CMF
0.3%), the performance of CCMF1 deteriorates most
[Singh and Gordon, 2008].
slowly.

UserMean uses the mean value of every user to
One challenge of the transfer learning for recommender
predict
the missing values. For PMF, different
systems is that it is difficult to recommend items to users
regularization
parameters
𝜆 target
= 𝜆 domain.
∈ {0.001,
who
have very few
ratings inofthe
In order
arethetried;
for CMF,
different
to0.01,
further0.04}
compare
above methods,
we first
group all
the
users based on
the number
regularization
parameters
of of
𝜆 observed
= 𝜆 = ratings
𝜆
∈in the
target
domain,
accuracies of
{0.001,
0.01, and
0.04}then
are evaluate
tried; forprediction
CCMF, different
different user groups. The experimental results are shown
regularization parameters of 𝜆 = 𝜆 = 𝜆 = 𝜆
1
http://www.douban.com
∈ {0.001, 0.01, 0.04} are tried. For PMF, CMF and
CCMF, latent dimensions 5 and 10 are tried. Best
result of each method are reported.

4.4 Results

We denote the CCMF methods that utilize
different constraints given in Eq. (6) and (7) as

auxiliary   domains   perform   much   better   than  

CCMF1   and   CCMF2   achieves   close   results,  
non-transfer  method  PMF.
but   as   the   training   data   in   target   domain  
auxiliary  
perform  
much  
better   than  
than  
 CCMF1  
CCMF1  
and  CCMF2  
CCMF2  
achieves  
close  
results,   the  

CCMF  
and   domains  
CCMF2,  
which  
model  
the  
becomes  
very   achieves  
sparse   close  
(<0.3%),  
auxiliary  
domains  
perform  
much  
better  

and  
results,  
non-transfer  method  PMF.
but   as  
as   the  
the   training  
training   data  
data  in  
in  target  
target  domain  
domain  
non-transfer  method  PMF.
but  
inconstant  
users   outperform   CMF   at   all  
performance  
of   CCMF1  
deteriorates  
most  
 CCMF  
and  
CCMF2,  
which  
model  
the  
becomes  
very   in
sparse  
(<0.3%),   the  
the  simulated and read data show that CCMF performs better
Table
Performance
Comparisons.
Numbers
boldface
CCMF  
and   2:
CCMF2,  
which   model  
the  
becomes  
sparse  
(<0.3%),  
sparsity  levels.
slowly.   very  
inconstant  
users  
outperform  
CMF   at  
at   all  
all  
performance  
of   CCMF1  
CCMF1   deteriorates  
deteriorates  most  
most  
inconstant  
users  
outperform  
CMF  
performance  
of  
(i.e.
0.890)
and
in
Italic
(i.e.
0
.895
)
are
the
best
and
Table 2: Performance
Comparisons. Numbers in boldface (i.e.
0.890) and in Italic (i.e. 0.895) are thethan
best CMF at various sparsity levels. The study in this pasparsity  levels.
slowly.  
sparsity  levels.
slowly.  
and second
best
results among
all methods,
respectively.
second
best
results
among
methods,
respectively.
Table 2:
Performance
Comparisons.
Numbers
inall
boldface
(i.e. 0.890)
and in Italic (i.e. 0.895) are the bestper clearly demonstrates (a) the necessity of taking domain
Table 2: Performance Comparisons. Numbers in boldface (i.e. 0.890) and in Italic (i.e. 0.895) are the best
and
results
respectively.
without
transfer
with transfer
andsecond
second best
best
results among
among all
all methods,
methods,
respectively.
dataset sparsity of R
dataset
dataset

Douban
Douban
Douban

Netflix

Netflix
Netflix

PMF
CCMF1
UserMean
CMF
without transfer
transfer
with
transfer CCMF2
without
with
transfer
sparsity
of
R
sparsity
of R
PMF
CCMF1
CCMF20.775
0.20%
0.876
0.796 CMF
0.770
UserMean
CMF 0.778CCMF1
PMF
CCMF2
UserMean
0.20%
0.876
0.796
0.778
0.770
0.775
0.40%
0.863
0.775
0.759
0.749 0.775
0.750
0.20%
0.876
0.796
0.778
0.770
0.40%
0.863
0.775
0.759
0.749
0.750
0.40%
0.863
0.775
0.759
0.749
0.60%
0.860
0.766
0.756
0.750 0.750
0.750
0.60%
0.860
0.766
0.756
0.750
0.750
0.60%
0.860
0.766
0.756
0.750
0.20%
1.020
0.969
0.931
0.914 0.750
0.914
0.20%
1.020
0.969
0.931
0.914
0.914
0.20%
1.020
0.969
0.931
0.914
0.914
0.40%
1.007
0.951
0.917
0.904
0.902
0.40%
1.007
0.951
0.917
0.904
0.902
0.40%
1.007
0.951
0.917
0.60%
1.000
0.939
0.905 0.904
0.895 0.902
0.890
0.60%
1.000
0.939
0.905
0.895
0.890
0.60%
1.000
0.939
0.905
0.895
0.890

difference and change of user features into account, and (b)
the items rated by a user implies to whether and how much
the user’s feature has changed.
In this paper, we assumed that the rating matrices in auxiliary and target domain are one-sided aligned. But in real
life, some users may not have rating records in both domains.
Hence, in order to model the domain difference more realistically, for future work, we will extend CCMF so that it can
fit partial-users- aligned setting. In the future, we will also
extend CCMF in heterogeneous settings, e.g. for transfering
like/dislike knowledge from books or music to target domain
that involves rating.

7.

REFERENCES

(a)
Distribution of Testing
Data
(b)
Comparison
on
Distribution
(b) RMSE
RMSE
Comparison
onDifferent
Different
(a) (a)
Distribution
of Testing
(b)
RMSE
Comparison
[1] Y. Koren. Factorization meets the neighborhood: a
Rating
Scales
(a) Distribution of Testing Data
(b)User
RMSE
Comparison
on Different
User
Rating
Scales
Data
on
Different
User
Rating
on
Different
Users
Figure 3 Performance Comparison
Comparison
on
Different
Users
User Rating Scales
multifaceted collaborative filtering model. In KDD,
Scales
Figure 3 Performance Comparison on Different Users
pages 426–434, 2008.
Figure
3: Performance
Comparison
onperforms
Different
One challenge
challenge
of the
the
transfer learning for
consistently
better
than
One
of
for
consistently
performs
better Users
than other
other methods
methods [2] B. Li, Q. Yang, and X. Xue. Can movies and books
recommender systems
systems is that it is difficult to
when
when recommending
recommendingitems
itemsfor
forall
alluser
usergroups.
groups.
Onerecommender
challenge
of
the
transfer
learning
for
consistently
performs
better
than
other methods collaborate? cross-domain collaborative filtering for
recommend
items
to users
users
very
in items
Figure
3. who
Users
less than
160
ratings
are
grouped
recommend
to
havewith
very few
few
5.
WORK
recommender
that itIn
difficult
to
when
recommending
items for all user groups.
5. RELATED
RELATED
WORK
ratings in
insystems
the target
targetisdomain.
domain.
In isorder
order
to
ratings
the
to further
further
sparsity reduction. In IJCAI, pages 2052–2057, 2009.
into
classes:
“1-10”,
“11-20”,
“21-40”,
“41-80”,
PMF
Probabilistic
matrix
factorization
recommend
to6 users
who
very all
PMF
Probabilistic
matrix“81-160”,
factorization (PMF)
(PMF)
compareitems
the above
above
methods,
we have
first group
the
compare
the
methods,
we
first
group
allfew
the
5.
RELATED
WORK
[Salakhutdinov
and
Mnih,
2008]
is
a
method
for
[3]
B.
Li, Q. Yang, and X. Xue. Transfer learning for
and
“161-640”,
denoting
how
many
ratings
users
have
rated
[Salakhutdinov
and
Mnih,
2008]
is
a
method
for
users
based
on
the
number
of
observed
ratings
in
the
ratingsusers
in based
the target
order ratings
to further
on the domain.
number ofIn
observed
in the
missing
matrix.
PMF
Probabilistic
matrix
(PMF) collaborative filtering via a rating-matrix generative
missing value
valueprediction
predictionin
inaasingle
singlefactorization
matrix.
target
domain,
and domain.
then
evaluate
compare
the domain,
above
methods,
we first
groupprediction
all the
target
and
then
evaluate
prediction
in target
[Salakhutdinov
and Mnih,
2008](CMF)
is a method for
accuracies
of
different
user ratings
groups.in the
The
accuracies
different
user
groups.
The
users based
on theof
number
of observed
CMF
matrix
factorization
CMF Collective
Collective
matrix
factorization
(CMF)[Singh
[Singh
model. In ICML, page 78, 2009.
Figure
summarizes
the
distributions
testing
data
experimental
results then
are 3a
shown
in Fig.
Fig. 3.
Users
missing
value of
prediction
a singleacmatrix.
are
shown
in
3.prediction
Users with
with
and
Gordon,
2008]
isis in
proposed
for
target experimental
domain, results
and
evaluate
and
Gordon,
2008]
proposed
for jointly
jointly
less than
thancording
160 ratings
ratingsto
are groups
grouped into
into
66the
classes:
in
training
data
(sparsity
0.4%).
For
less
160
are
grouped
classes:
factorizing
two
matrices
with
the
constraints
ofof [4] I. Murray and R. Salakhutdinov. Evaluating
accuracies
of
different
user
groups.
The
factorizing
two
matrices
with
the
constraints
CMF
Collective
matrix
factorization
(CMF)
[Singh
“1−10”,   “11−20”,  
“11−20”,   “21−40”,  
“21−40”,   “41−80”,
“41−80”, and
“81  
−  
sharing
one-side
(user
item)
latent
“1−10”,  
“81  
sharing
one-side 2008]
(user ororto
latent
features.
example,
there
are
aand
total
user-item
pairs
be
pre-features.
experimental
results
are
in Fig.
3. Users
with−  19820
160”, denoting
denoting
howshown
many
ratings
users
have
and
Gordon,
is item)
proposed
for jointly probabilities under high-dimensional latent variable
However,  
CMF  
didn’t  
160”,
how
many
ratings
users
have rated
rated
However,  
CMF  
didn’t  address  
address  the  
the  problem  
problem  that  
that  
less than
160 domain.
ratings
are
grouped
into
6dataset
classes:
in
target
dicted
in
the
testing
in
which
the
related
users
in
the
factorizing
two different  
matrices interests  
with the
constraints
of models. In NIPS, pages 1137–1144, 2008.
users’  
may  
have  
in  
different  
in target domain.
users’  
may  
have  
different  
interests  
in  
different  
“1−10”,   Fig.
“11−20”,  
“21−40”,  the“41−80”,
andof“81  
−  
sharing
one-side
(user
or In
item)
latent features.
domains.
3(a)
summarizes
distributions
testing
training
dataset
have
rating
numbers
from
1
to
10.
Figdomains.
[5] R. Pan, Y. Zhou, B. Cao, N. N. Liu, R. M. Lukose,
Fig. 3(a)how
summarizes
the distributions
of testing
160”, denoting
many
ratings
users
have
rated
data according
according to
to groups
groups in
in the
the training
However,  
CMF  System
didn’t   Transfer
address  (CST)
the   problem  
that  
data
training
data
CST
was
aa
ure
3b,
we
observe
that data
our CCMF
algorithm
consistently
M. Scholz, and Q. Yang. One-class collaborative
in target
domain.
CST Coordinate
Coordinate
System
Transfer
(CST) in  
wasdifferent  
(sparsity
0.4%). For
For example,
example, there
there are
users’  
may   have  
different  
interests  
(sparsity
0.4%).
are aa total
total
method
proposed
by
[Weike
Pan
etetal,
2010].
CST
method
proposed
by
[Weike
Pan
al,
2010].
CST
performs
better
than
other
methods
when
recommending
19820
user-item
pairs
to
be
predicted
in
the
testing
filtering. In ICDM, pages 502–511, 2008.
domains. the data sparsity problem in a target
Fig.19820
3(a) summarizes
distributions
user-item pairsthe
to be
predicted inofthetesting
testing
addresses
addresses the data sparsity problem in a target
dataset in which the
related
users groups.
in the training
forrelated
all
dataset initems
which
the
users training
in the training
domain by transferring knowledge about both users [6] W. Pan, N. N. Liu, E. W. Xiang, and Q. Yang.
data according
to groups
in user
the
data
domainCoordinate
by transferring
knowledge
about (CST)
both users
dataset have rating numbers from 1 to 10. In Fig.
CST
System
Transfer
was
a
dataset
have rating
numbers from
1 to
Fig.
and items from auxiliary data sources. In our
(sparsity
For example,
are10.
a Intotal
and items
from auxiliary
dataPan
sources.
In our CST Transfer learning to predict missing ratings via
3(b),0.4%).
we observe
that ourthere
CCMF
algorithm
method
proposed
by knowledge
[Weike
et al,
we observe
thatpredicted
our CCMF
problem
setting, only
about
one2010].
side
198203(b),
user-item
pairs to be
in thealgorithm
testing
problem setting,
only sparsity
knowledge
about one
addresses
the data
problem
in side
a target heterogeneous user feedbacks. In IJCAI, pages
5. the RELATED
dataset in which
related users in theWORK
training
domain by transferring knowledge about both users
dataset have rating numbers from 1 to 10. In Fig.
2318–2323, 2011.
PMF Probabilistic matrix factorization
a
and items from(PMF)
auxiliary[10]
dataissources.
In our
3(b), we observe that our CCMF algorithm
[7]
W.
Pan, E. W. Xiang, N. N. Liu, and Q. Yang.
problem
setting,
only
knowledge
about
one
side
method for missing value prediction in a single matrix.
Transfer
learning in collaborative filtering for sparsity
CMF Collective matrix factorization (CMF) [12] is proreduction. In AAAI, 2010.
posed for jointly factorizing two matrices with the constraints
[8] J. D. M. Rennie and N. Srebro. Fast maximum margin
of sharing one-side (user or item) latent features. However,
matrix factorization for collaborative prediction. In
CMF didn’t address the problem that users’ may have difICML, pages 713–719, 2005.
ferent interests in different domains.
[9] P. Resnick, N. Iacovou, M. Suchak, P. Bergstrom, and
CST Coordinate System Transfer (CST) was a method
J. Riedl. Grouplens: An open architecture for
proposed by [7] CST addresses the data sparsity problem
collaborative filtering of netnews. In CSCW, pages
in a target domain by transferring knowledge about both
175–186, 1994.
users and items from auxiliary data sources. In our problem
setting, only knowledge about one side (user) from auxiliary
[10] R. Salakhutdinov and A. Mnih. Bayesian probabilistic
domain is available. Hence, CST are not applicable to the
matrix factorization using markov chain monte carlo.
problem in this paper.
In ICML, pages 880–887, 2008.
TCF [6] proposed a framework of Transfer by Collective
[11] B. M. Sarwar, G. Karypis, J. A. Konstan, and
Factorization that use the binary preference data expressed
J. Riedl. Item-based collaborative filtering
in the form of like/dislike to help reduce the impact of data
recommendation algorithms. In WWW, pages
sparsity of more expressive numerical ratings.
285–295, 2001.
[12] A. P. Singh and G. J. Gordon. Relational learning via
collective matrix factorization. In KDD, pages
6. CONCLUSIONS AND FUTURE WORKS
650–658, 2008.
In this paper, we formulated the problem of inconstant
[13]
K.
Yu, S. Zhu, J. D. Lafferty, and Y. Gong. Fast
users caused by domain difference and presented a new model
nonparametric matrix factorization for large-scale
to address the problem in transfer learning for collaborative
collaborative filtering. In SIGIR, pages 211–218, 2009.
filtering. Our method iteratively factorizes the rating ma[14]
Y.
Zhou, D. M. Wilkinson, R. Schreiber, and R. Pan.
trices in source/auxiliary domain and target domain with
Large-scale parallel collaborative filtering for the
a constraint on the user feature matrices for target domain
netflix prize. In AAIM, pages 337–348, 2008.
and auxiliary domain. Experimental results on both and

240

Learning Tree Structure of Label Dependency
for Multi-label Learning
Bin Fu1 , Zhihai Wang1 , Rong Pan2 , Guandong Xu3 , and Peter Dolog2
1

2
3

School of Computer and Information Technology,
Beijing Jiaotong University, Beijing 100044, China
{09112072,zhhwang}@bjtu.edu.cn
Department of Computer Science, Aalborg University, Denmark
{rpan,dolog}@cs.aau.dk
School of Engineering & Science, Victoria University, Australia
Guandong.Xu@vu.edu.au

Abstract. There always exists some kind of label dependency in multilabel data. Learning and utilizing those dependencies could improve the
learning performance further. Therefore, an approach for multi-label
learning is proposed in this paper, which quantiﬁes the dependencies
of pairwise labels ﬁrstly, and then builds a tree structure of the labels
to describe them. Thus the approach could ﬁnd out potential strong label dependencies and produce more generalized dependent relationships.
The experimental results have validated that compared with other stateof-the-art algorithms, the method is not only a competitive alternative,
but also has shown better performance after ensemble learning especially.
Keywords: classiﬁcation; multi-label instance; multi-label learning; label dependency.

1

Introduction

Classiﬁcation is to predict possible labels on unlabeled instance given a set of
labeled training instances. Traditionally, it is assumed that each instance is associated with only one label. However, an instance often has multiple labels
simultaneously in practice [1,2]. For example, a report about religion could also
be viewed as a politics report. Classiﬁcation for this kind of instance is called
multi-label learning. Nowadays, multi-label learning is receiving more and more
concerns, and becoming an important topic.
Various methods have been developed for multi-label learning, and these
methods mainly fall into two categories [2]: (1) algorithm adaptation, which
extends traditional single-label models so that they can deal with multi-label
instances directly. Several adapted traditional models include Bayesian method,


This research has been partially supported by the Fundamental Research Funds for
the Central Universities, China (2011YJS223), National Natural Science Fund of
China (60673089, 60973011), and EU FP7 ICT project M-Eco: Medical Ecosystem
Personalized Event-based Surveillance (No. 247829).

P.-N. Tan et al. (Eds.): PAKDD 2012, Part I, LNAI 7301, pp. 159–170, 2012.
c Springer-Verlag Berlin Heidelberg 2012


160

B. Fu et al.

AdaBoost, decision tree, associative rules, k-NN, etc. [3,4,5,6,7]. (2) problem
transformation, which converts a multi-label problem into one or several singlelabel problems. Thus traditional single-label classiﬁers can be used directly without modiﬁcation. Recently, many methods have been proposed to learn label
dependency as a way of increasing learning performance [1,2,8,9,10,11,12]. However, most of them do not give an explicit description of label dependency. For
example, classiﬁer chain, a model proposed recently [9], links the labels into a
chain randomly and assumes that each label is dependent on all its preceding
labels in the chain. However, each label may be independent with its preceding
labels while dependent on its following labels since they are linked randomly.
Moreover, more complex dependency such as a tree or DAG-like hierarchical
structure of labels often exists in practice, thus more appropriate models are
needed to describe them.
Hence, we propose one kind of novel method for aforementioned issues. We
quantify the dependencies of pairwise labels ﬁrstly, building a complete undirected graph that takes the labels as the set of vertices and the dependent values as edges. A tree is then derived to depict the dependency explicitly, so the
unrelated labels can be removed for each label and the dependency model is
generalized into a tree model. Furthermore, we also use ensemble technique to
build multiple trees to capture the dependency more accurately. The experimental results would show our proposed method is competitive and could further
enhance learning performance on most of datasets.
The remainder of this paper is organized as follows: We review the related
works in section 2. A formal deﬁnition of multi-label learning is given in section 3.
In section 4, we describe and analyze our proposed methods in detail. Section 5 is
devoted to the experiment design and result analysis. The last section concludes
this paper and gives some potential issues with further research.

2

Related Work

Many methods have been proposed to cope with multi-label learning by exploiting label’s dependencies. According to the order of dependency be learned, these
methods mainly fall into following categories.
(1) No label dependency is learned. Basic BR (Binary Relevance) method
decomposed one multi-label problem into multiple independent binary classiﬁcation problems, one for each label [2]. Boutell et al. used BR for scene classiﬁcation [13]. Zhang et al. proposed ML-KNN, a lazy method based on BR [7].
Tsoumakas et al. proposed HOMER to deal with a large number of labels [14].
(2) Learning the dependencies of pairwise labels. Hullermeier et al. proposed
the RPC method that learned the pairwise preferences and then ranked the
labels [15]. Furnkranz et al. extended the RPC by introducing a virtual label [16].
Madjarov et al. proposed a two stage architecture to reduce the computational
complexity of the pair-wise methods [17].
(3) Learning the dependencies within multiple labels. Basic LP (Label Powerset) method treated the whole set of labels as a new single label and learned
dependencies within all them [2]. Tsoumakas et al. proposed the RAkELd

Learning Tree Structure of Label Dependency for Multi-label Learning

161

method that divided the label set into disjoint subsets of size k randomly [18].
Stacking-base method was proposed to aggregate binary predictions to form
meta-instances [19]. Read proposed the PS, which decomposed the instance’s labels until a threshold was met [8]. Read et al. proposed the CC (Classiﬁer Chain)
algorithm to link the labels into a chain randomly [9]. Dembcynski et al. proposed PCC, a probabilistic framework that solved the multi-label classiﬁcation
in terms of risk minimization [10].
A number of models have also been used to depict the labels dependencies
explicitly, which include multi-dimensional Bayesian network, conditional dependency networks, conditional random ﬁelds [11,20,21,22,23]. Dembczynski et al.
formally explained the diﬀerence between the conditional dependency and unconditional dependency [24]. Similar with these methods, our proposed method
uses the tree to learn the label dependency explicitly. the diﬀerence is that we
simply ignore the feature set in the process of constructing a tree, whereas others
[11] build the models conditioned on the feature set.

3

The Concept of Multi-label Learning

Let X be the instance space, and L = (l1 , l2 , . . . , lm ) be a set of labels. Given a
training instance set D = {(x1 , C1 ), (x2 , C2 ), . . . , (xn , Cn )}, where xk ∈ X is an
instance, and Ck ⊂ L is a subset of L denoting xk ’s true labels, the target of
multi-label learning is to build a classiﬁer: f : X → 2L , that is a mapping from
the instance space to a set of label subset, where 2L is the power set of L. Ck
can also be represented by a Boolean vector (bk1 , bk2 , . . . , bkm ), where bkj = 1
indicates label lj is xk ’s true label (lj ∈ Ck ), while bkj = 0 indicates the opposite.
Let x ∈ X be an unlabeled instance, y = (y1 , y2 , . . . , ym ) be its Boolean
vector of predicted labels, we can also make prediction by calculating the joint
conditional probabilityP (y|x). For each label lk , let P arent(lk ) denote the set
of labels that label lk is dependent on, P arent(yk ) is the corresponding Boolean
counterpart. Hence P (y|x) can be transformed as Eq.(1).
P (y|x) =

m


P (yk |parent(yk ), x)

(1)

k=1

where yk denotes the kth label. Hence we can get the label vector’s posterior
probability by calculating each label’s posterior probability respectively, so the
transformation of Eq.(1) is a kind of problem transformation. A key issue is how
to exactly ﬁnd the set of dependent labels for each label in order to calculate
the posterior probability more accurately.

4

Learning a Tree Structure of Labels

As mentioned above, to eliminate weak dependencies in CC model, and ﬁt the
real data more accurately, we propose a new algorithm named as LDTS (Learning
dependency from Tree Structure of Labels) in this section.

162

B. Fu et al.

LDTS ﬁrstly measures the dependency for each pairwise labels li and lj , notated as dependency(li , lj ), thus an undirected complete graph G(L, E) is constructed, where the label set L denotes the vertices, and E = {dependency(li , lj ) :
li ∈ L, lj ∈ L} denotes the edges. To determine the dependent labels for each
label, a maximum spanning tree is then derived using Prim algorithm, and each
label is assumed to be dependent on its ancestor labels. A dataset is then created
for each label and their dependent labels are added into the feature set, so we
could utilize these dependency since the classiﬁers is trained based on the new
feature set. The whole training process is outlined in Algorithm 1.
Algorithm 1. The process of training LDTS classiﬁer
Input:
The training dataset: D = {(x1 , C1 ), (x2 , C2 ), . . . , (xn , Cn )};
The algorithm for training base classiﬁer: B.
Output:
Classiﬁers: (f1 , f2 , . . . , fm ).
1: for each pair of labels (li , lj ), measure their dependency: dependency(li , lj )
2: create a undirected full graph G = (L, E)
3: use the P rim algorithm to derive a maximum spanning tree: T
4: for label l1 , l2 , ..., lm , get the set consists of its ancestor labels: P arent(li )
5: for i = 0 to m do
6:
let the Di = D
7:
for j = 0 to m do
8:
if lj ∈ P arent(li ) then
9:
delete this label from Di
10:
end if
11:
end for
12:
for the Di , set li as its only label, train the classiﬁer fi
13: end for
14: return the m classiﬁers: (f1 , f2 , . . . , fm )

At step 1, mutual information is used to compute the dependencies of pairwise
labels. Its deﬁnition is shown as follows [25].
H(X, Y ) =


x,y

P (x, y) log

P (x, y)
P (x)P (y)

(2)

where X and Y are two variables, x and y are their all possible values.
The labels is organized using a tree for two purposes. Firstly, the properties
of maximum spanning tree ensure that each label is more dependent on its
ancestor labels than other labels, since they have greater mutual information
value. Hence it could eliminate weak dependency further by assuming each label
is only dependent on its ancestor labels. When generating the graph and tree of
labels, we simply assume that label dependency is independent with the feature
set and only consider the mutual inﬂuence among the labels, this is one kind
of the unconditional dependency described in [24]. Secondly, various kinds of
dependencies including tree hierarchy and DAG of dependency may exist within

Learning Tree Structure of Label Dependency for Multi-label Learning

163

labels. Therefore, we expect the performance could be improved, especially on
the datasets in which the labels are organized into a tree indeed.
It is also should be noted that the main purpose of our method is to ﬁnd more
accurate label dependency, and it does not impose a hierarchy on labels since
labels have the same parent or in diﬀerent paths could exist simultaneously.
This is diﬀerent from the hierarchical classiﬁcation that impose a strict label
hierarchy. Although the randomness in not eliminated fully, we do reduce it to
only select the strong dependency randomly. One possible issue is that a full
graph of labels needs to be learned with the computational complexity O(n2 ),
the eﬃciency needs further improvement to cater for a large number of labels.
When classifying an unlabeled instance x, each label li can not be predicted
until its dependent labels are all predicted. Hence the labels should be predicted
from the root of the tree and then its children recursively until all the leaves
are reached. The detailed process is depicted in Algorithm 2. For each label, the
labels dependencies are considered since its prediction is based on the feature
set and the predictions of its dependent labels.
Algorithm 2. The process of classiﬁcation using LDTS classiﬁer
Input:
A unlabeled instance x that needs to be classiﬁed.
Output:
The prediction Y = (y1 , y2 , . . . , ym ).
1: set the vector to be empty, Y ← ()
2: set the root label as the current label need to be predicted: t
3: predict current label t for 
x using corresponding classiﬁer ft
4: add ft (x) into Y , Y ← Y ft (x)
5: use the result ft (x) to update the x, x = (x, ft (x))
6: ﬁnd all the children labels of t
7: repeat
8:
for each children labels ci of t do
9:
repeat the step 2-6
10:
end for
11: until all the labels are predicted
12: return the predicted vector Y

When generating the directed tree in LDTS, the root node is selected randomly. However, selecting a diﬀerent label will result in a diﬀerent tree and thus
generating diﬀerent dependent labels for each label. Another issue is the label
dependency could not be utilized fully, since the dependency of pairwise labels
li , lj calculated here is mutual and useful equally to each other. One possibility
is that a label may also depend on its children labels, but the directed tree does
not allow for this situation. To address such issues, the ensemble learning is used
to generate multiple LDTS classiﬁers iteratively. In each iteration, the classiﬁer
is trained on a sampling of the original dataset, and the root label is reselected
randomly. Hence each iteration will get a diﬀerent label tree and combining them
will reduce the inﬂuence of the root’s randomness and take full advantage of the
label dependency. We call this extended method ELDTS(Ensemble of LDTS).

164

B. Fu et al.

The detail process is depicted in Algorithm 3. Given an unlabeled instance x,
all predictions of these classiﬁers will be aggregated into a ﬁnal result by voting
simply.
Algorithm 3. The process of training ELDTS classiﬁer
Input:
The training dataset: D = {(x1 , C1 ), (x1 , C1 ), . . . , (xn , Cn )};
The algorithm for training base classiﬁer: B;
The number of iteration: n.
Output:
An ensemble of LDTS classiﬁers F = (f1 , f2 , . . . , fm ).
1: set the F to be empty: F = ()
2: for i = 0 to m do
3:
generate a new dataset Di by sampling on the original dataset with replacement
4:
select the root label ri randomly;
5:
train a LDTS classiﬁer ti using B, based on the dataset Di and root label ri
6:
add fi into F
7: end for
8: return the ensemble of classiﬁers: F

All above are the description and analysis of our proposed algorithms. Comparison with other state-of-the-art algorithms and further analysis will be given
in the following section.

5
5.1

Experiment Design and Analysis
The Description of Datasets

We take several datasets from multiple domains for the experiments, and table
1 depicts them in detail.
Table 1. Description of the datasets used in experiments
Dataset
emotions
enron
medical
scene
yeast

Domain
music
text
text
image
biology

Instances
593
1702
978
2407
2417

Attributes
72
1001
1449
294
103

Labels
6
53
45
6
14

LC
1.869
3.378
1.245
1.074
4.237

LD
0.311
0.064
0.028
0.179
0.303

DLS
27
753
94
15
198

Several statistics as follows have
been used to characterize these datasets.
n
(1) Label cardinality: LC = n1 i=1 |Ci |. It calculates the average number
of labels for each instance, where |Ci | is the number of true labels of the ith
instance.
n  
(2) Label density: LD = n1 i=1  Cmi . It is calculated by dividing the label
cardinality by m, the size of original label set.

Learning Tree Structure of Label Dependency for Multi-label Learning

165

(3) Distinct label sets: DLS(D) = |{C|∃(x, C) ∈ D}|. It counts the number of
distinct label sets that appear in the dataset.
Seen from table 1, these datasets cover many domains including text categorization, scene classiﬁcation, emotion analysis, biology etc.. It should be noted
that there are no label hierarchies in these datasets and we use them to examine
whether our methods could ﬁnd more strong label dependency and gain better performance. More detail description can be found on the oﬃcial website of
Mulan1 .
5.2

Evaluation Criteria

In order to evaluate the algorithm performance, the criteria should be speciﬁed.
Let D = {(x1 , C1 ), (x2 , C2 ), . . . , (xn , Cn )} be a dataset, where xi is the ith instance, and Ci ⊂ L is its true labels. Given a classiﬁer f and an instance xi ,
Yi denotes the predicted labels for xi , while rank(xi ) or ranki denotes the predicted rank of labels, and rank(xi , l) denotes the label l’s position in the rank.
All the criteria we used are as follows.
(1) Hamming loss: It is proposed by Schapire and Singer [4].
H-Loss(f, D) =


n
1  Yi Ci
n i=1
m

(3)

 
n 
1   Yi Ci 

n i=1  Yi Ci 

(4)


The operator
calculates the symmetric diﬀerence of two sets, which is the
number of misclassiﬁed labels for an instance.
(2) Accuracy: It calculates the ratio between the intersection and union of the
predicted set of labels and the true set of labels for the instances on average.
Accuracy(f, D) =

(3) One-error: It calculates how many times that top-ranked label is not a
true label of the instance.
1
δ(arg min rank(xi , l))
l∈L
n i=1
n

One-Error =

(5)

where δ(x) = 1 if l is a true label of the instance, otherwise δ(x) = 0.
(4) Ranking loss: It expresses the number of times when the irrelevant labels
are ranked before the true labels.


1
1
{(la , lb ) : rank(xi , la ) > rank(xi , lb ), (la , lb ) ∈ Ci × Ci }
 


n i=1 Ci |Ci |
(6)
n

R-Loss=

1

http://mulan.sourceforge.net/

166

B. Fu et al.

(5) Average precision: This measurement calculates the average fraction of
labels ranked above a particular label l ∈ Ci , which are all also in Ci .
AvePrec =

n
1  1  |{l ∈ Ci : rank(xi , l ) ≤ rank(xi , l)}|
 
n i=1 Ci 
rank(xi , l)

(7)

l∈Ci

These criteria evaluate the diﬀerent aspects of these methods. While Hamming
loss and accuracy do not consider the relation between diﬀerent predictions, the
other 3 criteria take such a relation into considerations, since they are based on
the ranking of the probabilities predicted for all labels. Because our methods are
intended to get a more accurate probability for each label by ﬁnding more strong
labels dependencies, thus for each label, it should be predicted more accurately
and the true labels should be given greater possibilities. Therefore, we expect
that our method could gain better performance under Hamming loss and ranking
loss, since Hamming loss examines the predictions of all labels independently and
ranking loss focuses on whether the true labels are given greater probabilities
than other labels. For other 3 criteria, our method may be eﬀective, but they
are not what our method optimize for.
5.3

Algorithms and Settings

The algorithms used for comparison are listed in table 2 with their abbreviations
respectively. To examine the eﬀect of label dependency, BR algorithm is used as
a baseline since it does not consider the label dependency, then we compare our
proposed LDTS and ELDTS with CC and ECC methods to see their eﬀectiveness
after eliminating weak dependencies. RAkELd and RAkEL are also used for
comparison as other ways of learning label dependency.
The experiments are divided into two parts, according to the two purposes
mentioned in section 4. One part is on the ﬁve aforementioned datasets without
label hierarchy to see whether our method can ﬁnd more strong dependency and
thus gain better performance, the other part is on the dataset rcv1v2, a dataset
in which there exists a tree hierarchy of labels, to see its performance when a
tree structure is learned. Since only one tree exists in rcv1v2, we do not use the
ensemble method on it.
Table 2. The algorithms used for comparison
Compared with LDTS
BR: Binary relevance method
CC: Classiﬁer chain
RAkELd : Random disjoint k label subsets

Compared with ELDTS
EBR: Ensemble of BR
ECC: Ensemble of CC
RAkEL: Random k label subsets

All algorithms are implemented on the Mulan framework [26], an open platform for multi-label learning. The parameter values are chosen as those used in
the paper [9]. For the RAkEL, we set the k = m
2 . For the RAkELd , we set the

Learning Tree Structure of Label Dependency for Multi-label Learning

167

k = 3. For the ensemble algorithms, the number of iterations is 10, and for each
iteration, 67% of the original dataset is sampled with replacement to form the
training dataset. SMO, a support vector machine classiﬁer implemented in Weka
[27], is used as the base classiﬁer. All algorithms are executed 5 times using 10fold cross validation on all datasets expect rcv1v2 with diﬀerent random seeds
1, 3, 5, 7, 11, respectively, and the ﬁnal results are the averaged values. For the
rcv1v2, only 100 attributes are kept, and 10-fold cross validation is used only
one time since it has a huge amount of instances and attributes.
5.4

Experimental Results and Analysis

Based on above setup, we get the ﬁnal results and the following tables display
them in detail. The bold result indicates the best one, and result with the black
dot “•” indicates our proposed algorithm is better than itself indicated algorithm.
Table 3. The hamming loss of each algorithm on the datasets
Dataset
emotions
enron
medical
scene
yeast

BM
0.1939
0.0601
0.0101•
0.1046
0.1990

CC
0.2159•
0.0606•
0.0098
0.1037
0.2115•

LDTS
0.2038
0.0602
0.0099
0.1056
0.2060

EBM
0.1947•
0.0540•
0.0098•
0.1020•
0.1991

ECC
0.2108•
0.0536•
0.0096•
0.0997•
0.2106•

ELDTS RAkEL
0.1932 0.2283•
0.0535 0.0541•
0.0095 0.0102•
0.0968 0.1047•
0.2034 0.2371•

Table 4. The accuracy of each algorithm on the datasets
Dataset
emotions
enron
medical
scene
yeast

BM
0.5199•
0.4058•
0.7580•
0.5999•
0.5003•

CC
0.5336•
0.4083•
0.7750
0.6949
0.4879•

LDTS
0.5727
0.4085
0.7670
0.6496
0.5073

EBM
0.5226•
0.4370•
0.7655•
0.6137•
0.5031•

ECC
0.5451•
0.4110•
0.7799
0.7020
0.4929•

ELDTS RAkEL
0.5808 0.5119•
0.4396 0.4063•
0.7759 0.7686•
0.6783 0.6281•
0.5249 0.4692•

As shown from table 3 to table 7, our proposed LDTS method performs better
on the majority of datasets evaluated by the criteria. It is superior to CC on
3 datasets under the Hamming loss, accuracy, one-error, and ranking loss, but
inferior under other metrics. LDTS algorithm does not improve all the time or
the improvement is not signiﬁcant. The possible reason is that although LDTS
algorithm could learn the dependency further, it still ignore lots of useful dependency since it only considers unidirectional dependency of pairwise labels,
especially when the labels are dependent mutually. We expect that ensemble
learning that combines diﬀerent trees can further utilize the label dependency ,
since the dependent direction between pairwise labels is changed in a diﬀerent
tree by choosing a diﬀerent root.
To validate above assumption, we also use ensemble learning on these algorithms and compare them each other. Also shown from table 3 to table 7, our

168

B. Fu et al.

proposed ELDTS has a substantial improvement after the employing ensemble
learning. Under all 5 criteria, ELDTS is superior to ECC on most datasets. These
results show that through learning multiple label trees by ensemble learning, the
inﬂuence of the root label’s randomness could be mitigated, and the label dependencies are learned more eﬀectively. Although ECC algorithm also changed
the order of labels, it could not make sure that only the strong dependency is
considered each time, since it’s totally random when determining the dependent
relationship within labels.
It can be observed that the proposed algorithms do not perform well on two
datasets medical and scene. Seen from the table 1, these two datasets have very
small label cardinality, which means there tend to be less label dependency in
them and overemphasis on label dependency may not be preferable. Thus the
algorithms we propose are more suitable for the datasets that there are indeed
strong label dependency in them.
The results gotten on rcv1v2, a dataset with tree structure of labels, are also
given in table 8. We can clearly see that our proposed LDTS method is superior
under Hamming loss and ranking loss, the criteria it optimize for. Therefore,
it has been proven that our method is more eﬀective when there is complex
dependency within labels.
Table 5. The one-error of each algorithm on the datasets
Dataset
emotions
enron
medical
scene
yeast

BM
0.2989
0.4912•
0.2029•
0.3389•
0.2557

CC
0.3700•
0.4938•
0.1847
0.2793
0.2559•

LDTS
0.3103
0.4897
0.1932
0.3132
0.2557

EBM
0.2534
0.3078•
0.1407•
0.2553•
0.2557•

ECC
0.3181•
0.3071•
0.1415•
0.2609•
0.2583•

ELDTS RAkEL
0.2563 0.3096•
0.3054 0.3210•
0.1397 0.1634•
0.2485 0.2777•
0.2551 0.2895•

Table 6. The rank loss of each algorithm on the datasets
Dataset
emotions
enron
medical
scene
yeast

BM
0.2778•
0.2915
0.0952•
0.1718•
0.3188•

CC
0.2876•
0.2929•
0.0926
0.1576
0.3351•

LDTS
0.2334
0.2925
0.0932
0.1664
0.3181

EBM
0.2169•
0.1648•
0.0537•
0.1162•
0.2739•

ECC
0.2317•
0.1640•
0.0516
0.1115•
0.2771•

ELDTS RAkEL
0.1743 0.1964•
0.1622 0.1784•
0.0518 0.0598•
0.0945 0.1110•
0.2273 0.2300•

Table 7. The average precision of each algorithm on the datasets
Dataset
emotions
enron
medical
scene
yeast

BM
0.7384•
0.4682•
0.7977•
0.7752•
0.6697•

CC
0.7159•
0.4702
0.8115
0.8048
0.6611•

LDTS
0.7634
0.4693
0.8066
0.7881
0.6728

EBM
0.7814•
0.6284•
0.8672•
0.8353•
0.7003•

ECC
0.7573•
0.6287•
0.8710
0.8351•
0.6975•

ELDTS RAkEL
0.8040 0.7734•
0.6314 0.6011•
0.8707 0.8524•
0.8479 0.8285•
0.7253 0.7048•

Learning Tree Structure of Label Dependency for Multi-label Learning

169

Table 8. The performance of algorithms on dataset rcv1v2
criterion
H-Loss
Accuracy
One-Error
R-Loss
AvePrec

6

BM
0.0235
0.1810•
0.7638•
0.3560•
0.2537•

CC
0.0294•
0.2529
0.7065
0.3508•
0.3085

LDTS
0.0235
0.2237
0.7120
0.3381
0.2940

RAkELd
0.0237•
0.1783•
0.9538•
0.4590•
0.1001•

Conclusion

In this paper, one kind of novel approaches are proposed to exploit the label dependency. Speciﬁcally, the dependency degree of pairwise labels is calculated ﬁrstly
and then a tree is build to represent the dependency structure of labels. The methods assume that the dependencies only exist between each label and its ancestor
labels, resulting in reducing the inﬂuence of weak dependency. At the same time,
they also generalize the label dependency into a tree model. Furthermore, we utilize
ensemble learning to learn and aggregate multiple label trees to reﬂect the labels
dependencies fully. The experimental results show that the algorithms we proposed
perform better, especially after boosted by the ensemble learning.
One potential problem is that using mutual information to measure the dependency will give equal values to both of the labels, which assumes that the
dependency for pairwise labels is mutual and equal for each other. However, the
label dependency could be directed possibly and this assumption is often violated in reality. Hence how to measure the directed label dependency should be
one of the next directions. Additionally, how to generalize the tree structure of
labels further to graph or forest structure is another issue in the future work.

References
1. Cheng, W., Hullermeier, E.: Combining Instance-Based Learning and Logistic Regression for Multilabel Classiﬁcation. Machine Learning 76(2-3), 211–225 (2009)
2. Tsoumakas, G., Katakis, I., Vlahavas, I.: Mining Multi-label Data. In: Oded, M.,
Lior, R. (eds.) Data Mining and Knowledge Discovery Handbook, pp. 667–685.
Springer, New York (2010)
3. McCallum, A.K.: Multi-label Text Classiﬁcation with a Mixture Model Trained by
EM. In: Proceedings of AAAI 1999 Workshop on Text Learning (1999)
4. Schapire, R.E., Singer, Y.: Boostexter: a Boosting-Based System for Text Categorization. Machine Learning 39(2-3), 135–168 (2000)
5. Clare, A.J., King, R.D.: Knowledge Discovery in Multi-label Phenotype Data. In:
Siebes, A., De Raedt, L. (eds.) PKDD 2001. LNCS (LNAI), vol. 2168, pp. 42–53.
Springer, Heidelberg (2001)
6. Thabtah, F.A., Cowling, P., Peng, Y.: MMAC: a New Multi-class, Multi-label
Associative Classiﬁcation Approach. In: Proceedings of the 4th International Conference on Data Mining, pp. 217–224 (2004)
7. Zhang, M., Zhou, Z.: ML-KNN: A Lazy Learning Approach to Multi-label Learning. Pattern Recognition 7(40), 2038–2048 (2007)
8. Read, J.: Multi-label Classiﬁcation using Ensembles of Pruned Sets. In: Proceedings of the IEEE International Conference on Data Mining, pp. 995–1000. IEEE
Computer Society, Washington, DC (2008)

170

B. Fu et al.

9. Read, J., Pfahringer, B., Holmes, G., Frank, E.: Classiﬁer Chains for Multi-label Classiﬁcation. In: Buntine, W., Grobelnik, M., Mladenić, D., Shawe-Taylor, J. (eds.) ECML
PKDD 2009, Part II. LNCS, vol. 5782, pp. 254–269. Springer, Heidelberg (2009)
10. Dembczynski, K., Cheng, W., Hullermeier, E.: Bayes Optimal Multilabel Classiﬁcation via Probabilistic Classiﬁer Chains. In: Proceedings of the 27th International
Conference on Machine Learning, pp. 279–286. Omnipress (2010)
11. Zhang, M., Zhang, K.: Multi-label Learning by Exploiting Label Dependency. In:
Proceedings of the 16th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, pp. 999–1000. ACM Press, Washington, DC (2010)
12. Zhang, Y., Zhou, Z.: Multi-label Dimensionality Reduction via Dependence Maximization. ACM Transactions on Knowledge Discovery from Data 4(3), 1–21 (2010)
13. Boutell, M.R., Luo, J., Shen, X.: Learning Multi-label Scene Classiﬁcation. Pattern
Recognition 37(9), 1757–1771 (2004)
14. Tsoumakas, G., Katakis, I., Vlahavas, I.: Eﬀective and Eﬃcient Multilabel Classiﬁcation in Domains with Large Number of Labels. In: Proceedings of ECML/PKDD
2008 Workshop on Mining Multidimensional Data, pp. 30–44 (2008)
15. Hullermeier, E., Furnkranz, J., Cheng, W.: Label Ranking by Learning Pairwise
Preferences. Artiﬁcial Intelligence 172(16-17), 1897–1916 (2008)
16. Furnkranz, J., Hullermeier, E., Mencia, E.L.: Multilabel Classiﬁcation via Calibrated Label Ranking. Machine Learning 2(73), 133–153 (2008)
17. Madjarov, G., Gjorgjevikj, D., Dzeroski, S.: Two Stage Architecture for Multi-label
learning. Pattern Recognition 45(3), 1019–1034 (2011)
18. Tsoumakas, G., Katakis, I., Vlahavas, I.: Random k-labelsets for Multi-label Classiﬁcation. IEEE Transactions On Knowledge and Data Engineering 23(7), 1079–1089
(2011)
19. Tsoumakas, G., Dimou, A., Spyromitros, E.: Correlation-Based Pruning of
Stacked Binary Relevance Models for Multi-Label Learning. In: Proceeding of
ECML/PKDD 2009 Workshop on Learning from Multi-Label Data, Bled, Slovenia,
pp. 101–116 (2009)
20. Gaag, L., Waal, P.: Multi-dimensional Bayesian Network Classiﬁers. In: Third European Workshop on Probabilistic Graphical Models, pp. 107–114 (2006)
21. Bielza, C., Li, G., Larranage, P.: Multi-dimensional Classiﬁcation with Bayesian
Networks. International Journal of Approximate Reasoning 52(6), 705–727 (2011)
22. Guo, Y., Gu, S.: Multi-label Classiﬁcation using Conditional Dependency Networks. In: Proceedings of the 22nd International Joint Conference on Artiﬁcial
Intelligence, pp. 1300–1305 (2011)
23. Ghamrawi, N., McCallum, A.K.: Collective Multi-label Classiﬁcation. In: Proceedings of the 2005 ACM Conference on Information and Knowledge Management,
pp. 195–200 (2005)
24. Dembczynski, K., Waegeman, W., Cheng, W.: On Label Dependence in Multi-label
Classiﬁcation. In: Proceedings of the 2nd International Workshop on Learning From
Multi-label Data, pp. 5–12 (2010)
25. Chow, C.K., Liu, C.N.: Approximating Discrete Probability Distributions with Dependency Trees. IEEE Transactions on Information Theory 14(3), 462–467 (1968)
26. Tsoumakas, G., Spyromitros-Xiouﬁs, E., Vilcek, J., Vlahavas, I.: Mulan: A Java
Library for Multi-Label Learning. Journal of Machine Learning Research 12, 2411–
2414 (2011)
27. Witten, I., Frank, E.: Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations. Morgan Kaufmann, San Francisco (2000)

1338

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

A Novel Feature Selection Methodology for
Automated Inspection Systems
Hugo C. Garcia, Jesus Rene Villalobos, Rong Pan,
and George C. Runger
Abstract—This paper proposes a new feature selection methodology. The
methodology is based on the stepwise variable selection procedure, but, instead of
using the traditional discriminant metrics such as Wilks’ Lambda, it uses an
estimation of the misclassification error as the figure of merit to evaluate the
introduction of new features. The expected misclassification error rate (MER) is
obtained by using the densities of a constructed function of random variables,
which is the stochastic representation of the conditional distribution of the
quadratic discriminant function estimate. The application of the proposed
methodology results in significant savings of computational time in the estimation
of classification error over the traditional simulation and cross-validation methods.
One of the main advantages of the proposed method is that it provides a direct
estimation of the expected misclassification error at the time of feature selection,
which provides an immediate assessment of the benefits of introducing an
additional feature into an inspection/classification algorithm.
Index Terms—Feature selection, misclassification error rate, quadratic
discriminant function.

Ç
1

INTRODUCTION

ONE of the key steps in the development of any inspection or
classification system is the feature selection process. This process
often considers all possible subsets of features. The result is
combinatorial in nature that makes it a long and tedious process,
especially when the selection is performed by humans and the
features being considered present complex cross-correlation
patterns. A process that is particularly tedious is testing the results
rendered by the different subsets of features used during the
algorithm development process. During this process, each time a
new subset is evaluated, the resulting inspection algorithm needs
to be implemented and tested to assess its performance in terms of
misclassification error rate (MER). This paper addresses this
problem by introducing a novel methodology that speeds up the
identification of the appropriate subset of features. We estimate the
misclassification error rate of the resulting inspection algorithm
even before it is implemented. The methodology uses the MER as
the guiding evaluation criterion of a stepwise feature selection that
has as its main goals the minimization of MER, while minimizing
the number of features. This is important because the larger the
feature set used, the more expensive the development of the
inspection system.
The proposed methodology is based on the Quadratic
Discriminant Function. This discriminant function has several
advantages with respect to other classifiers. For example, for
small sets of training samples, it is preferred over complicated
classifiers when class separation is not complex [1]. This classifier
also tends to decrease the design error while improving the
. H.C. Garcia is with L3, Electro-Optical Systems, 1215 S. 52nd Street,
Tempe, AZ 85281. E-mail: Hugo.Garcia@L-3com.com.
. J.R. Villalobos, R. Pan, and G.C. Runger are with the Department of
Industrial Engineering, Arizona State University, PO Box 875906, Tempe,
AZ 85287-8692. E-mail: {rene.villalobos, runger, rong.pan}@asu.edu.
Manuscript received 26 Mar. 2008; revised 29 Sept. 2008; accepted 30 Oct.
2008; published online 10 Nov. 2008.
Recommended for acceptance by S. Li.
For information on obtaining reprints of this article, please send e-mail to:
tpami@computer.org, and reference IEEECS Log Number
TPAMI-2008-03-0166.
Digital Object Identifier no. 10.1109/TPAMI.2008.276.
0162-8828/09/$25.00 ß 2009 IEEE

Published by the IEEE Computer Society

VOL. 31,

NO. 7, JULY 2009

understandability of the classification properties [2]. The methodology is based on previous research in the areas of applied and
theoretical statistics, in particular, the works by McFarland and
Richard [3] and Hua et al. [4].
One of the main motivations of the proposed feature selection
methodology is to develop tools needed for the emergence of selfreconfigurable automated inspection systems [5]. One of the main
conditions for the emergence of these systems is the automation of
decisions that are traditionally performed by human developers
such as feature generation, selection, and testing of the resulting
algorithms. The methodology presented in this paper contributes
to this goal by speeding up the feature selection process and by
providing an accurate method to estimate the MER associated with
a particular feature subset, even before the inspection algorithm is
fully developed.
This paper is organized as follows: Section 2 gives a summary of
the literature review. Section 3 presents the new methodology for
the selection of features including the new method to estimate the
MER. In Section 4, the experimental results are presented. Section 5
presents the conclusions of this research and the recommendations
for future research followed by the list of references.

2

PROBLEM BACKGROUND

One of the problems faced by the developer of an automated
inspection system is the issue of what features to use to inspect a
particular component or product. The optimal feature selection
process is a very important step of any development process
because it has an impact on the overall performance of the resulting
inspection algorithm. For instance, inspection features need to be
carefully chosen to avoid using redundant features that may cause
system instability and require additional costs and training effort.
In fact, reducing the number of features can be accomplished by
taking advantage of cross correlation or redundancies among the
features [6]. The problem of optimal feature selection is typically
defined as follows: “Given a set of p features, select a subset of
size m that leads to the smallest classification error” [7].
The topic of optimal feature selection has been addressed
extensively for applications other than automated inspection
systems, for instance, see [8], [9], [10], [11], and [12]. Furthermore,
a new area of application closely related to feature selection is
called sensor selection. Some papers that address this topic are [13],
[14], and [15]. The literature associated with feature selection for
classification purpose is vast. Some examples include [16], [17],
[18], and [19]. However, the existing feature selection methods
present serious shortcomings for their use in reconfigurable
environments. For instance, one shortcoming is the amount of
time required to determine the performance of the resulting
inspection algorithms. This procedure is usually very long, because
once the feature selection process is performed, it is necessary to
asses the performance of the resulting subset, usually using some
of the cross-validation techniques, to estimate the MER. For
example, the “leave-one-out” cross-validation method consists of
using n  1 observations or data points to compute the classification rule and then classifying the omitted observation. This
procedure is repeated for each observation to estimate the MER.
A second problem is that the evaluation criteria used during the
feature selection process do not always correspond to the metrics or
statistics used to assess the performance of the resulting inspection
algorithm. For instance, when metrics such as Wilks Lambda are
used to guide the feature selection process, the objective is to
optimize the value of the metric. However, the optimization of such
indirect metrics does not guarantee the selection of a subset of
features that renders the smallest MER in the assembly line [17].
Thus, it is important to use a metric that would give a direct measure
of the expected MER during the selection of the inspection features.

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

This paper addresses this gap by proposing a metric, and the
method to compute it, to make the use of stepwise feature selection
methods significantly more efficient and accurate in the development and adaptation of inspection and classification algorithms.
The proposed method is based on an indirect estimation of the
MER that is used to guide the feature selection process. That is, the
MER is estimated through a distribution function instead of relying
on direct observations. The advantage of this approach is the
indirect computation of the MER from the multivariate distributions of the components’ features. The proposed method is
applicable when a Quadratic Discriminant Function (QDF) is used
as the algorithm to discriminate between the classes of components, or in a broader context, between two populations. The
proposed method of calculating MER consists of using conditional
distribution functions of the QDF (to be presented in (3) and (4)),
but, instead of calculating MER using simulation as proposed in [3]
or by doing cross-validation, as is commonly done, an approximation of the distribution, based on the normal distribution, is used.
Once this MER is obtained, it is used to guide the search for an
optimal feature subset.
Because of the inherent complexity of feature selection
problems, heuristics methods such as stepwise selection of
variables have been developed. A typical procedure for this
heuristic method is as follows:
Start with an empty subset and a list of potential feature
candidates to be included into this subset.
2. Add a feature to the subset from the list of available
features that maximizes (minimizes) a given evaluation
criterion.
3. Calculate the marginal contribution to the evaluation
criterion for each remaining potential feature in the list.
This marginal contribution is calculated over the feature(s)
already in the subset.
4. Select the feature that in combination with the included
feature(s) maximizes (minimizes) the evaluation criterion.
5. Include the selected feature into the subset if it meets the
constraints.
6. Reexamine the marginal contribution for each feature in
the subset. If a feature does not meet the constraints, it is
removed from the subset.
7. Repeat steps 3-6 until no more features can be added into
or removed from the subset.
For additional details about the previous procedure, the
reader is referred to [20], [21], and [22]. The previous
description of the stepwise methodology relied on an evaluation
criterion to guide the inclusion and exclusion of features. One
issue is which evaluation criterion to use in the heuristic
method. There are numerous criteria to conduct the feature
selection process. These include: Wilks’ Lambda, Unexplained
Variance, Mahalanobis Distance, Smallest F Ratio, and Rao’s V.
The reader is referred to [23] and [24] for more computational
details of these evaluation criteria.
For example, Wilks’ Lambda method is one of the most
commonly used in practice [25]. This method is based on Wilks’
-criterion, which is a ratio expressing the total variability not
explained by the population differences [24]. This criterion is
1.

¼

jWj
;
jB þ Wj

ð1Þ

where jWj is the determinant of the estimated generalized
variance within the two populations, B represents the variation
between populations, and jB þ Wj is the determinant of the
estimated generalized total variance of the training data. The
Lambda value is between 0    1, and larger values indicate a

VOL. 31, NO. 7,

JULY 2009

1339

poor separation between populations, while smaller values denote
good separation between populations.
Although these methods are the most commonly used to
conduct the selection of the features, they exhibit an important
disadvantage. That is, they do not provide a good indication of the
expected misclassification results of the underlying feature/
variable subset. That is, the principal assumption is that optimizing
the evaluation criterion results in the minimization of MER.
However, this assumption is not always true. For instance,
heuristic methods directly using the MER as the evaluation
criterion (i.e., the classifier is the evaluation function) render
superior performance than the other criteria, but they also tend to
be the most computationally expensive because every new subset
that is explored requires the acquisition of actual misclassification
data [17]. Thus, using the MER as an evaluation criterion for a large
number of features is not practical.
Ideally, we would like to have a method that is accurate, as
given by applying the MER evaluation criteria, and computationally efficient. This is the objective of the feature selection procedure
proposed in this paper. The proposed method seeks to estimate the
MER based on the stochastic representation of the conditional
distribution of the quadratic classifier. These misclassification rates
are estimated through the use of a “plug-in” QDF originally
proposed in [3], later expanded in [4], and modified to fit our
needs in this paper and [26]. A brief explanation of these works is
presented in the next section.

2.1

Stochastic Representation of the “Plug-In”
Quadratic Discriminant Function

Equation (2) presents what is called the “Plug-in” QDF for the
observation x. It is the representation of the QDF when the mean
vectors 1 and 2 , and the covariance matrices 1 and 2 of the
two competing normal populations (1 and 2 ) involved in the
classification are unknown and replaced by their unbiased
1 , x
1 , S1 , and S2 [27] and [28]:
estimators, x

 0 1
 1
1 
1
02 S1
 1 S1  x
 x0 S1
1  S2 x þ x
2 x
C
B 2
^¼B
C:
Q
A
@ 1 jS1 j 1 

0 1
0 1
 2 S2 x
1  x
2
 S x
 x
 log
2
2 1 1
jS2 j
0

ð2Þ

^ is a random variable, in theory, the appropriate
Since Q
distribution could be derived to determine misclassification
probabilities for the observation x. However, since x is also a
random variable that follows a particular normal distribution, the
^ is too complex to be obtained directly.
exact distribution of Q
McFarland and Richard [3] addressed this problem by deriving
instead approximations to the conditional distributions of the
^ i.e., PfQ
^  kjx 2 1 g and PfQ
^ < kjx 2 2 g to get the
variable Q,
expected probability of misclassifying an observation to one of the
two competing populations of multivariate normal distributions.
Through the study of its characteristic function, these authors
^ can be expressed as a
showed that the conditional distribution of Q
series of independent univariate random variables, which they
^ They used this result to
called the stochastic representation of Q.
^ through the use of Monte Carlo
estimate the distribution of Q
simulations based on the individual components of the stochastic
representation. In order to present this stochastic representation, it
is necessary to introduce some terminology.
Let H be an orthogonal matrix that diagonalizes the matrix
12
1
1
1
2 1 2 2 such that 2 2 1 2 2 ¼ HH0 , where  ¼ diagð1 ;
2 ; . . . ; p Þ is a diagonal matrix. The entities 1 ; 2 ; . . . ; p on the
column
main diagonal are the eigenvalues of 1
2 1 . The auxiliary
1
1  2 Þ.
vector  ¼ ð1 ; 2 ; . . . ; p Þ0 is defined such that  ¼ H0 2 2 ð

1340

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

Let k be the threshold value classifying x to belong to 1 or 2 .
Thus, the misclassification probabilities are defined as P ð2j1Þ ¼
^  kjx 2 1 g and P ð1j2Þ ¼ 1  PfQ
^  kjx 2 2 g, where
P  fQ
P ð2j1Þ is the probability of misclassifying an observation x into
2 when x belongs to 1 , and otherwise, is denoted by P ð1j2Þ. Let
Q1 and Q2 be the stochastic representations of P ð2j1Þ and P ð1j2Þ
defined by
1
0
p


2

1X 
2 1=2
2
Z2j þ j  1 Z1j C
2j !3j Z1j þ 1  !3j
B
C
B 2 j¼1
B
!C
Q1 ¼ B
C;




p1
C
B 1

 X
np T2
n2  j
A
@þ

log 1p
þ
F
 log1

log
1
j
2
2
n
n2 T1

j
1
j¼1
ð3Þ
0
B
B
B
Q2 ¼ B
B
@

1
p



2 
1X
2
2 1=2
~3j
~3j Z1j þ 1  !
Z2j þ ~j
~1 Z1j  ~2j !
C
2 j¼1
C
!C
C;
 p 


p1
C

 X
1
n T2
n2  j
A
þ
log 1p
F
 log1

log
þ
1
j
2
2
n1  j
n2 T1
j¼1
ð4Þ
2ni p

where, for i ¼ 1; 2 and j ¼ 1; . . . ; p, Ti ¼
is a chi-square
distribution with ni  p degrees of freedom, Zij ¼ Nð0; 1Þ is a
standard normal random variable, and Fj is an F distribution with
ðn2  j; n1  jÞ degrees of freedom
1 ¼

ðn1  1Þðn1 þ 1Þ
;
ðn1 T1 Þ

2j ¼

ðn2  1Þðj þ n1
2 Þ
;
T2

ðn2  1Þðn2 þ 1Þ
;
ðn2 T2 Þ
1
ðn1  1Þð1
j þ n1 Þ
~2j ¼
:
T1
~1 ¼

3

VOL. 31,

NO. 7, JULY 2009

DESCRIPTION OF THE FEATURE
SELECTION METHODOLOGY

The proposed methodology, in this paper called the Approximation Method (AM), is based on the indirect estimation of the MER
as the evaluation criterion in the stepwise feature selection method.
That is, MER is approximated by multivariate normal distribution
functions of Q1 and Q2 instead of relying on Monte Carlo
simulation experiments. The outline of the proposed method is
as follows:
Based on sample information, calculate the means
and variances of the stochastic representations, Q1
and Q2 .
2. Once the estimates of the parameters of the stochastic
representations are available, the calculation of MER is
based on a normal approximation of the stochastic
distributions.
These steps are explained in the following two sections.
1.

3.1

Mean and Variance Estimations

Q1 is assumed to follow normal distributions whose mean and
variance are given by (5) and (6):
11
0
0
n1 n2 j þ ðn1 þ n2 j þ 1Þ
p B
CC
B 1 ðn  1Þ X
n2 ðn1 þ 1Þ
CC
B
2
B
CC
B
B
AC
B 2 ðn2  p  2Þ j¼1 @
ðn

p

2Þðn

1Þðn
þ
1Þ
2
1
1
2
C
B
þ

j
C
B
ðn

p

2Þðn

1Þn
1
2
1
C
B

1C;
0 
 n  p
 n  p
Q1 ¼ B
C
B
n1  1 p
1
2
C
B

CC
B log n2  1 þ
B
2
2
CC
B
B
C
B þ1B


 C C
p
p1  
C
B
X
2B
n2  j
n1  j
AA
@ X
@


log j þ
2
2
j¼1
j¼1

The remaining random variables and constants are defined as


n1 n2 j
ðn1 þ 1Þðj n2 þ 1Þ

 1
1 2
;
j ¼ j j þ
n2

!3j ¼

12



;

n1 n2
ðn2 þ 1Þðj þ n1 Þ

 1
j 2
~j ¼ j 1 þ
:
n1

~3j ¼
!

12
;

Once Q1 and Q2 are available, then the calculation of the
misclassification probabilities can be performed through the use of
Monte Carlo simulation based on each one of the independent
stochastic components. However, in order to estimate these
probabilities, a large number of time-consuming simulations runs
need to be performed.
The results obtained in [3] were extended in [4] to find an
analytic method to produce a misclassification error curve as a
function of the number of features in the classifier so that the
optimal number of features could be determined. Two shortcomings of the work in [4] are that it assumed that all of the
features had normal distributions with identical parameters and
that samples of the same sizes (n1 ¼ n2 ) were drawn from each
population to estimate these parameters. These two shortcomings
were addressed by [26]. This work considered the case of
populations that had different parameters and unequal sample
sizes to accommodate the inspection and classification situations
in which there is a difference in number of samples available
from the targeted populations. The results of this latter work are
the basis for the feature selection methodology presented in the
next section. In particular, the proposed method extends the work
in [4] to directly estimate the MER associated with a particular
feature subset.

ð5Þ
0

ðn2  1Þ2

6
X

1

k þ c C
B
C n1 > p þ 2;
B ðn2  p  2Þ2 ðn2  p  4Þ k¼1
2Q1 ¼ B




 C
C; n > p þ 4;
B
p
X
n2  j
A 2
@ 1
0 n1  j
þ 0
þ
4 j¼1
2
2

ð6Þ

0
where
ðxÞ is the Digamma function and
ðxÞ is the
derivative of ðxÞ. The parameters k and c are defined as
follows:
!

2
p
p X
p
X
X
1
n1
1 ¼
ðn2  p  1Þ
2j þ
i j ;
2 n1 þ 1
j¼1
i¼1 j¼1

1
p
X
2
ðn

p

1Þ
ðn
þ
n

þ
1Þ
1
2 j
C

2 B 2
C
B
j¼1
1
1
C;
B
2 ¼
C
B
p
p
X
X
2 n2 ðn1 þ 1Þ @
A
þ
ðn1 þ n2 i þ 1Þðn1 þ n2 j þ 1Þ
0

i¼1 j¼1

1
p
X
j ðn1 þ n2 j þ 1Þ C
B ðn2  p  1Þ
C
B
j¼1
n1
1
C;
B
3 ¼
p
p
C
n2 ðn1 þ 1Þ2 B
A
@ XX
þ
j ðn1 þ n2 i þ 1Þ
0

i¼1 j¼1

!
p
p X
p
X
X
n1
2
2
4 ¼
ðn2  p  1Þ
j j þ
i j ;
n1 þ 1
j¼1
i¼1 j¼1

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 31, NO. 7,

1
p
X
2j ðn1 þ n2 j þ 1Þ C
B ðn2  p  1Þ
C
B
j¼1
1
C;
B
5 ¼
p
p
C
n2 ðn1 þ 1Þ B
A
@ XX 2
þ
i ðn1 þ n2 j þ 1Þ

JULY 2009

1341

0

TABLE 1
Wilks Lambda Steps with Correlation 0

i¼1 j¼1

0

p
X

!2

ðn1  1Þðn2  p  2Þ
þ
ðn2  1Þðn1  p  2Þ

1

C
B
2j
C
B
C
B
j¼1
C
B 0
1
p
C
B
X
C
B

p

4Þ

ðn
6 ¼ B B
2
j
C C;
C
B B
C
i¼1
B B
CC
B B
!
CC

C
B B
pðn1  1Þ2 ðn2  p  2Þðn2  p  4Þ
n1 þ 1 2 C
@ @
AA
þ
2ðn2  1Þðn1  p  2Þðn1  p  4Þ
n1
1
0 0

1
p 
ðn2  1Þ X
1
2

þ

þ
C
B B
j
j
C
C
B B 2ðn2  p  2Þ j¼1
n2
C
C
B B
C
C
B B
C


C
B B 0
1


C
n

p
2
p
C
B B
C
log 2n1 þ
C
B B
C
2
C
C
B B B
C
C
C
B B B
C
C
C
B B B
C
 p C
C
B B B
2
C
C
C
B B B 

log
2n
C
2
C
C
B B B ðn2  p  2Þ
C
C
C
B B B
C
C
C
B B B
C
A
C
B B @
n  p
C
C
B B
1
C
C
B B
þ
C
C
B B
2
C
C
B B
C


C
B B
C
C
B B
pðn1  1Þ
n1 þ 1
C
C
B Bþ
C
C
B B 2ðn1  p  2Þ
n
C
1
C
B B
C
c¼B B 0
C:
1C
 n  p
C
B B
C
 p
2
1
C
B B
C

þ
log
2n
C
B B B
2
C
2
ðn1  p  2Þ C
C
B B B
CC
C
B B B
C
C
C
B @ @
A
A


C
B
 p
n

p
2
C
B
 log 2n1 þ
C
B
2
C
B
B 0
11C
0
C
B
n1 n2 j þ ðn1 þ n2 j þ 1Þ
2
B B
þ j C C C
BX
C
B B
p
n
ðn
þ
1Þ
2
1
CCC
B B ðn2  1Þ B
CCC
B
B B
CCC
B B 2ðn2  p  2Þ B
@ j¼1 ðn2  p  2Þðn1  1Þðn1 þ 1Þ A C C
B B
CC
B B

CC
B B
ðn1  p  2Þðn2  1Þn1
CC
B B
CC
B B 

p
CC
B @




AA
@
n1  1
n1  p
n2  p
log

þ
n2  1
2
2
The derivation and computational details of the previous
equations are presented in [26]. Although the parameters of Q2
can be estimated in a similar way to Q1 , a more efficient method
originally proposed by Hua et al. [4], consists of representing Q2 as
a function of Q1 through the following transformation:
Q2 ð
;  Þ  Q1 ð
1=2  ; 1 Þ:

ð7Þ

Therefore, to obtain the mean and variance for Q2 using this
transformation,
it is necessary to replace the values of j and j by
1
j 2 j and 1
j , respectively, in (5) and (6).
Once the mean and variance equations are developed for the
general case of unequal sample size, the next step in our
proposed methodology is to approximate Q1 and Q2 with normal
distributions.

3.2

Approximation of the MER

The approximation of MER is based on the assumption that
the distributions of the random variables, Q1 and Q2 ,
follow normal distributions as NðQ1 ; 2Q1 Þ and NðQ2 ; 2Q2 Þ.
Thus, the proposed equation to approximate MER is
!
!
k  Q1
k  Q2
~ 
~
MER ¼ 
ð8Þ
p1 þ 
p2 ;
Q1
Q 2

R 1 u2 =2
~
e
du is the upper tail area of the standard
where ðxÞ
¼ p1ﬃﬃﬃﬃ
2 x
normal distribution and k is defined as follows: k ¼
logðp2 Cð1j2Þ=p1 Cð2j1ÞÞ, where p1 and p2 are known as a priori
probabilities for 1 and 2 . Further, Cð2j1Þ denotes the cost of
misclassifying x into 2 and the other misclassification cost is
denoted by Cð1j2Þ.
The results provided by the approximation method were
validated by using simulation data sets. The reader is referred to
[26] for a detailed discussion of the validation results. The next
section provides the results of the feature selection examples
using MER as the evaluation criterion in the stepwise feature
selection method.

4

EXPERIMENTAL RESULTS

The performance of the proposed evaluation criterion is compared against some of the most common existing evaluation
criteria [23] and [24]. These evaluation criteria are: Wilk’s
Lambda, Unexplained Variance, Mahalanobis Distance, Smallest
F Ratio, and Rao’s V. The five methods selected the same feature
subset in all our experiments; thus, these methods are grouped in
single class and referred to as Conventional Methods (CM). The
first part of this section gives one example to validate using
simulation data, while the second part presents one example
using real inspection data.

4.1

Simulated Data

This experiment was performed using 1,000 simulated pseudorandom data points for two multivariate normal distributions
with 15 features (A to O) for two populations: 1 and 2 . The
mean vector for 1 was set to 1 ¼ 1;000 and mean vector for 2
to  2 ¼ 0. The diagonals of the covariance matrices for 1 and
2 were set to 1j ¼ 2j ¼ 400 þ 100j, for j ¼ 1 to 15. Five data
sets were generated using different levels of correlation between
the features. The levels of correlation used were 0.0, 0.25, 0.5,
0.75, and 0.99.
One example sequence of the feature selection process (with the
level of correlation equal to 0) is presented in Table 1. This table
also presents, in the third column, the values of the Wilks Lambda
associated with each subset. The last column is the value of MER
for each subset computed using the Approximation Method
(MER_AM).
From Table 1, it can be seen that one of the disadvantages of the
conventional methods is that the statistic used as the main
evaluation criteria does not convey meaningful absolute information about the resulting MER. For example, when the Wilks
Lambda statistic is used as the evaluation criterion, the values of

1342

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 31,

NO. 7, JULY 2009

TABLE 2
Summary of the Feature Selection Methods

this statistic do not provide information in terms of the expected
MER. Therefore, the users of the Conventional Methods do not
obtain information to interpret the values of the evaluation
criterion in terms of MER. This disadvantage is eliminated when
using the MER estimated by the proposed approximation method.
The summary of the feature selection experiments using the five
different levels of correlation and three methods to estimate MER is
presented in Table 2. The methods are the Conventional Methods,
the Approximation Method, and the Resubstitution Method. The
first column presents the level of correlation. In order to obtain the
different number of subsets, the MER for the 32,767 subsets was
calculated by using the resubstitution method. The number of
different subsets based on the MER is presented in the second
column for each correlation of the section Data. The MER of the
full subset (using the 15 features) and the position (rank) of this
subset with respect to the optimal subset based on the MER
obtained for each level of correlation are presented under the
section labeled “Full.” The summary of the feature selection
methods is presented in three sections in this table. These
sections are called: Conventional, Approximation, and Resubstitution. The selected subset of features is presented in the first
column (Subset) of each section. The second column (MER) of
each section presents the corresponding MER using the
resubstitution method. The next column (Size) shows the
number of features in the selected subset. The final column
(Rank) of each section gives the position of this subset with
respect to the optimal subset of features based on the MER.
Additionally, the last section of the table under the heading
“Optimal” shows the optimal subset with its corresponding MER
and a number of features. For the last two rows of the optimal
subset column, there are multiple subsets with the target MER
equal to zero; the size for these examples represents the number of
features in the largest subset.
The previous results support the hypothesis that the approximation method renders similar performance to those of the
conventional methods. The values obtained in the three factors
analyzed on the previous experiment are very similar. However, it
is necessary to explaIn that, in the case of the approximation
method, no threshold value was used for the inclusion or
elimination of features. This means that any reduction of the
MER was enough to include a feature in the final subset. For
instance, see Table 3, which shows the progression of the feature
selection process for the example with correlation equal to 0. For
instance, this table presents the feature that was added in each step
of the feature selection process. The first column gives the step
number of the stepwise process. The second column shows the
subset for each step of the process. Columns 3 and 4 present MER

estimated from the Approximation Method (denoted as MER_AM)
and the Resubstitution Method (denoted as MER_R), respectively.
The fifth column shows the value differential between consecutive
estimates of MER.
As an example, consider step 4 of Table 3. Feature D was
added/included to the best subset of step 3 {A, B, C}. This feature
was added/included because it provided the maximum reduction
to MER_AM. The marginal reduction of the MER_AM from feature
D was 0:0804  0:0646 ¼ 0:0158. As was explained before, the
threshold used to include a feature was 0. Therefore, any reduction
in the MER_AM of feature being considered is enough to be
included in the best subset. This threshold was used to understand
the experimental behavior of the MER_AM versus the sample size
of the best subset. However, this approach may not be useful from
a practical point of view. For example, the reduction in MER_AM
from step 11 to step 12 in Table 3 is 0.000461. This reduction was
enough to include feature L although the reduction of MER_AM
was minimal. In practice, this reduction may not justify including
this feature in the resulting subset.
Given that the MER_AM statistic used in the feature selection
process is very similar to MER obtained using the resubstitution
method (MER_R), it can be used to determine a stopping point in
the feature selection progression according to the needs of the
users. The similarity between the MER_AM and the MER_R can be
analyzed in Table 3. For instance, the information provided in this
table can be used to determine the optimal number of features

TABLE 3
Feature Selection Summary

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 31, NO. 7,

JULY 2009

1343

TABLE 4
Feature Selection Summary

Fig. 1. Histograms of the EAL features.

from the cost/benefit trade-off including features in the final
subset. For example, a user can determine that an error of 0.05 is
acceptable for the classification of the populations. Therefore, in
the first experiment, the feature selection process should be
stopped at step 6 with the features {A, B, C, D, O, E} instead of
step 12 with the features {A, B, C, D, O, E, N, F, G, M, H, L}
because, in step 6, the level of discrimination determined by the
user was achieved. Another example is provided by assuming that
the user cannot afford more than seven features due to the
inspection cost/time constraints; therefore, the process should stop
with the subset {A, B, C, D, O, E, N} with an MER AM ¼ 0:0438050
and the MER R ¼ 0:0435. It is important to note that this process of
setting a prespecified stopping MER could not be achieved by
using conventional methods alone.
Regarding the time the AM takes to estimate an MER, it is
significantly lower than the alternative of running simulations as
proposed by [3]. For instance, an experiment that consisted of
running 100,000 simulation points in a personal computer with an
Intel Core 2CPU T5600 at 1.83 GHz to estimate the MER took 190 sec,
while the same estimation based on the AM took 2 seconds.

4.2

Automated Visual Inspection Data

This example is applied to the inspection of Surface-Mounted
Devices (SMD) to determine if these electronic components are
present or absent on a Printed Circuit Board (PCB). The
experimental data used to validate the feature construction
methodology were collected in an experimental AVI system
available at the Electronics Assembly Laboratory (EAL) at Arizona
State University. For a description of the experimental setting, the
reader is referred to [29].
The experimental results are based on a single class of SMD.
There are 281 components on the PCB used in this experiment. The
AVI system inspected one PCB with all the SMD components
present and another PCB with each one of the components absent.
Therefore, there were 281 elements for the training phase for each
population. Six inspection features were utilized in this analysis.
These features are called Energy (E), Correlation (C), Diffusion (D),
Fenergy (F), Texture (T), and Blob (B). The details of these features
were discussed in [29]. In order to conduct more feature selection
experiments, an assumption was made that the full data set was
composed of only five features instead of six. This assumption
gives six extra combinations of feature sets that can be analyzed as
separate experiments. For example, the full set of features is
labeled {ECDFBT}, under the new assumption, and the six new

feature sets are: {ECDFB}, {ECDFT}, {ECDBT}, {ECFBT}, {EDFBT},
and {CDFBT}.
The features used were selected to highlight the application of
the selection methodology and not for their discrimination power.
Fig. 1 depicts, with histograms, the discrimination provided by the
features to highlight the degree of separation between the Present
(1 ) and Absent (2 ) populations of components.
In order to evaluate the performance of the new evaluation
criterion, its performance is compared again to the Conventional
Methods (CM).

4.3

Feature Selection Summary

The summary of the seven experiments with two methods to
estimate MER is presented in Table 4. The first column
presents the number of features. The total number of different
subsets using the original number of features in each experiment is presented in the second column (Sets) of the section
Data. The MER of the full subset and the position of this
subset with respect to the optimal subset for each combination
of features are presented under the section labeled “Full.”
The results of the feature selection process using the two
methods to estimate MER are presented in the sections of the table
under the headings “Conventional” and “Approximation,” respectively. The selected subset of features is presented in the first
column of each section. The second column of each section
presents the number of features in the selected subset. The next
column shows the change of the position of this subset with respect
to the optimal subset of features based on MER using the
resubstitution method.
The previous results support the hypothesis that the approximation method renders similar or better performance as compared to those provided by conventional methods.

5

CONCLUSIONS

The main contribution of this research was the design of a new
feature selection methodology based on the “plug-in” Quadratic
Discriminant Function. One of the key contributions of the paper is
to conduct the feature selection process through an estimate of the
misclassification error rate obtained from the densities of the
stochastic representations of the conditional quadratic discriminant function.
The application of the proposed methodology will result in
significant savings of computational time in the estimation of MER
over the traditional simulation and cross-validation methods. This
will significantly shorten the time needed to reconfigure a
preexisting inspection system, a key feature for the emergence of
self-reconfigurable inspection systems; systems that will be better
capable to deal with the rapid introduction and retirement of
products.
The proposed methodology for feature selection was developed
assuming that the density functions of the stochastic representations of the conditional probabilities of the “plug-in” QDF follow
normal distributions. While this assumption was numerically
supported in this paper, it is necessary to further validate it.

1344

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

Another interesting topic for future research is to consider the
strategy to include or exclude more than one feature at a time.
Also, addressing marginal feature inclusion from an economical
point of view is an interesting area of future research. For example,
it could be useful to include economical factors such as the cost
and time required for including/excluding preexisting features.
This is something that is also left for future research.

ACKNOWLEDGMENTS
The authors would like to acknowledge the support provided by
the US National Science Foundation through grant DMI-0300361
for the realization of this research. They would also like to
acknowledge the helpful suggestions of the anonymous reviewers
of this paper.

REFERENCES
[1]
[2]
[3]

[4]

[5]

[6]
[7]

[8]

[9]

[10]

[11]
[12]

[13]

[14]

[15]

[16]

[17]

[18]

[19]

[20]
[21]
[22]

[23]

R. Duda, P. Hart, and D. Stork, Pattern Classification, second ed. John Wiley
& Sons, 2003.
L. Devroye, L. Gyorfi, and G. Lugosi, A Probabilistic Theory of Pattern
Recognition. Springer, 1996.
R. McFarland and D. Richard, “Exact Misclassification Probabilities for
Plug-In Normal Quadratic Discriminant Function—The Heterogeneous
Case,” J. Multivariate Analysis, vol. 82, pp. 299-330, 2002.
J. Hua, Z. Xiong, and E. Dougherty, “Determination of the Optimal
Number of Features for Quadratic Discriminant Analysis via the Normal
Approximation to the Discriminant Distribution,” Pattern Recognition,
vol. 38, pp. 403-421, 2005.
H.C. Garcia and J.R Villalobos, “Development of a Methodological
Framework for the Self Reconfiguration of Automated Visual Inspection
Systems,” Proc. Fifth Int’l Conf. Industrial Informatics, July 2007.
S. Kachigan, Multivariate Statistical Analysis. Radius Press, 1982.
A. Jain, R. Duin, and J. Mao, “Statistical Pattern Recognition: A Review,”
IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 22, no. 1, pp. 4-37,
Jan. 2000.
J.A. Zongker, “Feature Selection: Evaluation, Application, and Small
Sample Performance,” IEEE Trans. Pattern Analysis and Machine Intelligence,
vol. 19, no. 2, pp. 153-158, Feb. 1997.
P. Mitra, C. Murthy, and S. Pal, “Unsupervised Feature Selection Using
Feature Similarity,” IEEE Trans. Pattern Analysis and Machine Intelligence,
vol. 24, no. 3, pp. 285-297, Mar. 2002.
Y. Saeys, I. Inza, and P. Larranaga, “A Review of Feature Selection
Techniques in Bioinformatics,” Bioinformatics, vol. 23, no. 19, pp. 2507-2517,
2007.
P. Liu, N. Wu, and J. Zhu, “A Unified Strategy of Feature Selection,” Proc.
Int’l Conf. Advanced Data Mining and Applications, pp. 457-464, 2006.
H. Wei and S. Billings, “Feature Subset Selection and Ranking for Data
Dimensionality Reduction,” IEEE Trans. Pattern Analysis and Machine
Intelligence, vol. 29, no. 1, pp. 162-166, Jan. 2007.
S. Sampatraj, K. Abhishek, and Y. Ding, “A Survey of Inspection Strategy
and Sensor Distribution Studies in Discrete-Part Manufacturing Processes,”
IIE Trans., vol. 38, no. 4, pp. 309-328, 2005.
I. Gunyon, S. Gunn, M. Nikravesh, and L. Zadeh, “Variable-Feature
Selection and Ensemble Learning: A Dual View,” Feature Extraction,
Foundations and Applications, I. Guyon, L. Zadeh, and M. Nikravesh, eds.,
Springer-Verlag, 2005.
S. Jiang, R. Kumar, and H. Garcia, “Optimal Sensor Selection for DiscreteEvent Systems with Partial Observation,” IEEE Trans. Automatic Control,
vol. 48, no. 3, pp. 369-381, Mar. 2003.
H.C. Garcia, J.R. Villalobos, and G. Runger, “Automated Feature Selection
for Visual Inspection Systems,” IEEE Trans. Automation Science and Eng.,
vol. 3, no. 4, pp. 394-406, Oct. 2006.
H. Liu and L. Yu, “Toward Integrating Feature Selection Algorithms for
Classification and Clustering,” IEEE Trans. Knowledge and Data Eng., vol. 17,
no. 4, pp. 491-502, Apr. 2005.
N. Louw and S. Steel, “Variable Selection in Kernel Fisher Discriminant
Analysis by Means of Recursive Feature Elimination,” Computational
Statistics and Data Analysis, vol. 51, no. 3, pp. 2043-2055, 2006.
M. Ashihara and S. Abe, “Feature Selection Based on Kernel Discriminant
Analysis,” Proc. Int’l Conf. Artificial Neural Networks, Part 2, pp. 282-291,
2006.
C. Le, Applied Categorical Data Analysis. Wiley, 1998.
R. Khattree and D. Naik, Multivariate Data Reduction and Discrimination with
SAS Software. Wiley-Interscience, 2000.
C. Park, J. Koo, and P. Kim, “Stepwise Feature Selection Using Generalized
Logistic loss,” Computational Statistics and Data Analysis, vol. 52, no. 7,
pp. 3709-3718, 2008.
A. Rencher, “The Contribution of Individual Variables to Hotelling T2 ,
Wilks and R2 ,” Biometrics, vol. 49, pp. 479-489, 1993.

[24]
[25]
[26]

[27]
[28]

[29]

VOL. 31,

NO. 7, JULY 2009

R. Jenrich, “Stepwise Discriminant Analysis,” Statistical Methods for Digital
Computers, K. Enslein, ed., John Wiley & Sons, 1997.
A. Rencher, Methods of Multivariate Analysis, second ed. John Wiley & Sons,
2002.
H.C. Garcia, “A Framework for the Self Reconfiguration of Automated
Visual Inspection Systems,” PhD dissertation, Dept. of Industrial Eng.,
Arizona State Univ., 2008.
T.W. Anderson, An Introduction to Multivariate Statistical Analysis, third ed.
Wiley, 1984.
A. Wald, “On a Statistical Problem Arising in the Classification of an
Individual into One of Two Groups,” The Annals of Math. Statistics, vol. 15,
no. 2, pp. 145-162, 1944.
J.R. Villalobos, M. Arellano, A. Medina, and F. Aguirre, “Vector
Classification of SMD Images,” J. Manufacturing Systems, vol. 22, no. 4,
pp. 265-282, 2004.

. For more information on this or any other computing topic, please visit our
Digital Library at www.computer.org/publications/dlib.

NADA: A Unified Congestion Control Scheme for
Low-Latency Interactive Video
Xiaoqing Zhu and Rong Pan
Cisco Systems Inc., 170 W Tasman Drive, San Jose, CA 95134
{xiaoqzhu,ropan}@cisco.com
Abstract—Low-latency, interactive media applications (e.g.,
video conferencing) present a unique set of challenges for
congestion control. Unlike TCP, the transport mechanism for
interactive media needs to adapt fast to abrupt changes in
available bandwidth, accommodate sluggish responses and output
rate fluctuations of a live video encoder, and avoid high queuing
delay over the network. An ideal scheme should also make
effective use of all types of congestion signals from the network,
including packet losses, queuing delay, and explicit congestion
notification (ECN) markings.
This paper presents a unified approach for congestion
control of interactive video: network-assisted dynamic adaptation (NADA). In NADA, the sender regulates its sending rate
based on a composite congestion signal calculated and reported
by the receiver, which combines both implicit (e.g., loss and
delay) and explicit (e.g., ECN marking) congestion indications
from the network. Via a consistent set of sender adaptation
rules, the scheme can reap the full benefits of proactive, explicit
congestion notifications supported by advanced queue management schemes. It remains equally responsive in the absence of
such notifications. Extensive simulation studies show that NADA
interact well with a wide variety of queue management schemes:
conventional drop-tail, random early detection (RED), recently
proposed CoDel (controlled delay) and PIE (Proportional Integral
controller Enhanced), as well as a token-bucket-based random
marking scheme based on Pre-Congestion Notification (PCN).
Furthermore, NADA reacts fast to changes over the network,
allows for weighted bandwidth sharing among multiple competing
video streams, and sustains a substantial share of bottleneck
bandwidth when coexisting with TCP.

I.

I NTRODUCTION

Recent years have seen a flourishing of video conferencing
services, such as Microsoft Skype 1 , Google Hangout 2 , and
Apple Facetime 3 . Their growing popularity attests to the rising
user demand for interactive rich media applications. Despite
decades of research efforts and advances in video coding and
networking technologies, however, it remains an elusive task
to support low-latency interactive video over the Internet with
satisfactory quality of experience.
The design of an effective congestion control scheme
for interactive media faces a set of unique challenges. First,
the scheme needs to achieve both high data rates and low
queuing delays at steady state. It needs to react fast to abrupt
changes in network bandwidth, while accommodating sluggish
responses and random fluctuations in the output rate of a
live video encoder. As an end-to-end solution, the congestion
control scheme should interact well with a wide variety of
1 http://www.skype.com
2 http://www.google.com/+/learnmore/hangouts/
3 http://www.apple.com/iphone/features/facetime.html

queue management schemes that may be present in a practical
network. Ideally, it should also be capable of integrating all
forms of congestion indications from the network — either
implicitly in the form of queuing delays and packet losses,
or explicitly via random early congestion notification (ECN)
markings [1] — into the calculation of a target sending rate.
In this paper, we present a scheme that fulfils the aforementioned requirements: Network-Assisted Dynamic Adaptation
(NADA). It responds to both implicit and explicit congestion
signals in a unified manner, and effortlessly achieves weighted
bandwidth sharing amongst multiple competing video streams.
A stream with a higher user-specified priority or more dynamic
video contents will naturally acquire a greater portion of the
bottleneck network bandwidth. Since the NADA sender reacts
to both network delay and loss, by design, it can withstand the
coexistence of TCP flows sharing the same bottleneck queue.
In NADA, the receiver aggregates per-packet drops, ECN
markings, and one-way-delay measurements into a composite
congestion signal. It periodically reports back a time-smoothed
version of the congestion signal to the sender in the forms of
an Real-Time Transport Control Protocol (RTCP) [2] message.
Upon receipt of such feedback, The NADA sender calculates
the reference rate as a function of the congestion signal
value, the dynamic rate range associated with the current
video content, and the user-specified weight of priority for the
stream. It accommodates delayed responses and output rate
fluctuations of a live video encoder via a local rate shaping
buffer. Size of the rate shaping buffer exerts an influence on
both the outgoing rate at the sender and the target rate of the
live video encoder.
Extensive simulations confirm that the proposed NADA
scheme works well with a wide variety of network queue
management schemes. Our test scenarios span across conventional drop-tail queues, random early detection (RED) [3], the
recently proposed CoDel (Controlled Delay) [4] and PIE (Proportional Integral controller Enhanced) [5], as well as a random
marking scheme based on a token bucket algorithm originally
designed for Pre-Congestion Notification (PCN) [6]. In all
tests, NADA streams react fast to time-varying network bandwidth. It is also shown that multiple competing NADA streams
can share a common bottleneck in a stable manner, without
introducing oscillations in the rates of individual streams.
In the following, we first review related work in Section II.
We then present an overview of the NADA scheme in Section III. Sections V and IV describe the proposed receiver
and sender behavior, respectively. Section VI establishes the
rate of a NADA stream at equilibrium and derives a stability
criterion for the end-to-end system. In Section VII, we evaluate
performance of NADA via extensive network simulations.

978-1-4799-2172-0/13/$31.00 ©2013 IEEE

II.

BACKGROUND AND R ELATED W ORK

Research on congestion control has attracted long-standing
interests over the past 25 years [7]. Early works aimed at
tailoring congestion control to real-time media traffic so as to
satisfy two criteria: TCP-friendliness and media-friendliness.
The former indicates that the outgoing rate of the video stream
is equal to that of a comparable TCP flow [3] [8]. The latter
requires that the media streaming rate remains smooth, thereby
avoiding the typical sawtooth patterns in rate variations of TCP
[9] [10] [11].
The recently publicized phenomenon of “bufferbloat” —
the prevalent use of excessive buffering over access networks,
as documented in [12] — has rekindled the interest in making congestion control work again for low-latency interactive
video. Delay-based congestion control schemes emerge as
natural candidate solutions for avoiding excessive self-inflicted
queuing [13]. Two recent proposals to the Internet Engineering
Task Force (IETF) Working Group on RTP media congestion
avoidance techniques, [14] and [15], both rely on one-way
delay measurements as the primary source of congestion
indication. However, it is well-known that delay-based congestion control schemes tend to lose throughput significantly
when competing against their loss-based counterparts [16].
Our proposal of NADA [17] resolves this coexistence issue
by integrating all forms of network congestion indications —
delay, loss, and marking — into one composite congestion
signal in the calculation of a target sending rate.
III.

•

Rs . The size of the rate shaping buffer Ls , together
with the reference rate Rn , determines the value of
encoder target rate Rv and the sending rate Rs .
•

Network node. The NADA system is designed to work
with different modes of operations at the network
node. The supported queuing disciplines range from
drop-tail to random early detection (RED) [3] and
random early marking based on a token bucket algorithm for Pre-Congestion Notification (PCN) [6]. It
also works with the more recently proposed schemes,
such as CoDel [4] and PIE [5], which are designed to
explicitly control queuing delays.

•

NADA receiving agent. The NADA receiver agent
derives one-way delay of each packet dn from timestamps in the real-time transport protocol (RTP) [2]
header. It records per-packet events of losses and
explicit congestion notification (ECN) markings extracted from the IP header [1]. Combining all forms
of congestion signals, the receiver calculates the value
of a composite congestion signal in the form of an
equivalent delay d˜n . It periodically reports the timesmoothed value of the composite congestion signal xn
back to the sender via RTP control protocol (RTCP)
messages.

S YSTEM OVERVIEW

In this section, we first introduce the key components in an
end-to-end system for congestion control of interactive video.
Figure 1 provides such an overview. The list of notations
are summarized in Table I. Our proposed NADA scheme
comprises the sending and receiving agents in the system.
They are designed to interact with various forms of live video
encoders and network queue management schemes, in a unified
manner.
•

Fig. 1. Overview of the NADA system. The live video encoder adapts its
sending rate based on aggregated congestion feedback from the receiver.

Live video encoder. It encodes the incoming raw video
frames into RTP packets. The target rate for the video
encoder rate control is denoted as Rv . Note that the
actual output rate from the encoder Ro naturally falls
within the dynamic range of [Rmin , Rmax ], which
depends on the video scene complexity and may
change over time. The value of Ro may also fluctuate
randomly around the input target rate Rv . Moreover,
the live video encoder can only react to changes in
target encoding rate over coarse time intervals, on the
order of seconds. We designate the typical encoder
reaction time as τv . In our design of NADA, the
operation of the live video encoder is treated as a black
box. We have designed the NADA sending agent to
interact with an arbitrary live video encoder.
NADA sending agent. It is responsible for calculating
a reference rate Rn based on the composite network
congestion signal as reported by the receiver. The
NADA sending agent further regulates the video outgoing rate at Rs . A rate shaping buffer is employed
to absorb the instantaneous difference between video
encoder output rate Rv and the regulated sending rate

In the following, we first describe how the NADA receiver aggregates various forms of congestion signals from
the network. We then explain how the NADA sender reacts to
periodic receiver reports of the composite congestion signals
by regulating both the target rate for the live video encoder
and the sending rate over the network.
IV.

NADA R ECEIVER B EHAVIOR

The role of the NADA receiver is fairly straightforward.
It is in charge of four steps: a) to monitor per-packet oneway delay measurements, packets losses, and random marking
statistics; b) to aggregate all forms of congestion indication in
the form of a composite signal; c) to calculate a time-smoothed
value of the congestion signal; and d) to send periodic reports
of the composite congestion signal back to the sender.

TABLE I.
Symbol
n
dn
1M
n
1L
n
˜
dn
xn
Rmin
Rmax
τv
Rv
Ro
Rn
Rs
Ls

L IST OF NOTATIONS .

Explanation
index of video packet, as subscript;
measured one-way delay for the n-th packet;
binary indicator of random marking for the n-th packet;
binary indicator of loss of the n-th packet;
composite congestion signal in the form of an equivalent delay;
time-smoothed value of the composite congestion signal;
minimum encoder output rate for the current video content;
maximum encoder output rate for the current video content;
typical reaction time of encoder rate control;
target rate for encoder rate control;
output rate from live video encoder;
reference rate based on the composite congestion signal;
regulated sending rate as output of the rate shaping buffer;
size of the rate shaping buffer

A. Monitoring and aggregating per-packet statistics
The receiver observes and estimates one-way delay dn for
the n-th packet, ECN marking event 1M
n , and packet loss
M
L
event 1L
.
Here,
1
and
1
are
binary
indicators:
the value
n
n
n
of 1 corresponds to a marked or lost packet and the value of
0 indicates no marking nor loss. The equivalent delay d˜n is
calculated as follows:
L
d˜n = dn + 1M
n dM + 1n dL .

(1)

Here, dM is a prescribed delay penalty term corresponding to
an observed ECN marking event (e.g., dM = 200 ms); dL is
a prescribed delay penalty term corresponding to an observed
packet loss event (e.g., dL = 1 second).
B. Calculating time-smoothed values
The receiver calculates a time-smoothed version of the
composite congestion signal value via exponential averaging:
xn = αd˜n + (1 − α)xn−1 .

(2)

The weighting parameter 0 < α < 1 adjusts the level of
smoothing. A larger value of α ensures faster responsiveness
of the NADA rate adaptation, at the expense of reduced system
stability.
C. Sending Periodic Feedback
Periodically, the receiver sends back the updated value of
xn ’s in RTCP messages, to aid the sender in its calculation of
target rate. The size of acknowledgement packets are typically
on the order of tens of bytes, and are significantly smaller than
average video packet sizes. Therefore, the bandwidth overhead
of the receiver acknowledgement stream is sufficiently low.
V.

NADA S ENDER B EHAVIOR

Figure 2 provides a more detailed view of the NADA
sender. Upon receipt of an RTCP report from the receiver,
the NADA sender updates its calculation of the reference rate
Rn as a function of the network congestion signal. It further
adjusts both the target rate for the live video encoder Rv and
the sending rate Rs over the network based on the updated
value of Rn , as well as the size of the rate shaping buffer.

Fig. 2.

Components of a NADA sender.

A. Rate shaping buffer
The rate shaping buffer is employed to absorb any instantaneous mismatch between encoder rate output Ro and regulated
sending rate Rs . The size of the buffer evolves from time t−τ
to time t as:
Ls (t) = max[0, Ls (t − τ ) + (Ro − Rs )τ ].

(3)

A large rate shaping buffer contributes to higher end-to-end
delay, which may harm the performance of real-time media
communications. Therefore, the sender has a strong incentive
to constrain the size of the shaping buffer. As will be discussed
later in Section V-C, the NADA sender can deplete the rate
shaping buffer by increasing the sending rate Rs , or limit its
growth by reducing the video encoder target rate Rv .
B. Calculation of Reference Rate
The sender calculates the reference rate Rn based on
the composite network congestion signal from receiver RTCP
reports. It first compensates the effect of delayed observation
via a linear predictor:
x(t) − x(t − δ)
τo
(4)
δ
In (4), δ denotes the interval between two consecutive received
video packets. The prediction parameter τo is a reference time
lag for the compensation step.
The reference rate is then calculated as a function of x̂:
xref
Rn = Rmin + w(Rmax − Rmin )
(5)
x̂
Here, Rmin and Rmax denote the content-dependent rate
range the encoder can generate. The weight of priority level
is designated as w. The reference congestion signal xref is
typically chosen as the expected one-way propagation delay
over t the path, so that the maximum rate of Rmax can be
achieved over an empty queue. The final reference rate Rn is
constrained within the dynamic range of [Rmin , Rmax ].
x̂ = x(t) +

Intuitively, a rising value of x̂ indicates increased network
congestion and leads to a lower value of Rn via (5). The
back-off in video sending rate shall, in turn, help to relieve
network congestion, thereby resulting in a new equilibrium
of the overall system. It can also be noted that a stream
with a higher priority w or a wider dynamic range of rate
(Rmax − Rmin ) will settle at a higher rate than another
stream under the same network condition and observing the
same network congestion level. As explained in Sec. VI, this
naturally leads to weighted bandwidth sharing amongst NADA
streams. Finally, the combination of w and xref determines
the sensitivity of the rate adaptation scheme in reaction to
fluctuations in the observed composite congestion signal x.
Note that the sender does not need any explicit knowledge
of the queue management scheme inside the network. Rather, it
reacts to the aggregation of all forms of congestion indications
via the composite congestion signal xn from the receiver in a
coherent manner.
C. Updating video encoder target and sending rate
The target rate for the live video encoder is updated based
on both the reference rate Rn and the rate shaping buffer size
Ls , as follows:

Fig. 3.

Illustration of the proposed slow-start mechanism.

VI.

T HEORETICAL A NALYSIS

In this section, we first characterize how multiple NADA
streams will share the rate of a bottleneck link at equilibrium.
We then derive a stability criterion for the NADA congestion
control scheme near its operating point, based on a fluid traffic
model of the system.
A. NADA Rate at Equilibrium

Rv = Rn − βv

Ls
.
τv

(6)

Similarly, the outgoing rate is regulated based on both the
reference rate Rn and the rate shaping buffer size Ls , such
that:
Ls
Rs = Rn + βs .
(7)
τv
In both (6) and (7), the first term indicates the rate calculated from network congestion feedback alone. The second
term indicates the influence of the rate shaping buffer. A large
rate shaping buffer nudges the encoder target rate slightly
below — and the sending rate slightly above — the reference
rate Rn . Intuitively, the amount of extra rate offset needed to
completely drain the rate shaping buffer within the same time
frame of encoder rate adaptation τv is given by Ls /τv . The
scaling parameters βv and βs can be tuned to balance between
the competing goals of maintaining a small rate shaping buffer
and deviating the system from the reference rate point.
D. Slow-start mechanism
Finally, special care needs to be taken during the startup phase of a video stream, since it may take several roundtrip-times before the sender can collect statistically robust
information on network congestion. We propose to regulate
the reference rate Rn to grow linearly, no more than: Rss at
time t:
Rss (t) = Rmin +

t − t0
(Rmax − Rmin ).
T

(8)

As illustrated in Fig. 3, the start time of the stream is t0 ,
and T represents the time horizon over which the slow-start
mechanism is effective.

At equilibrium, all streams sharing a common bottleneck
experience approximately the same one-way delay do 4 , packet
loss ratio pL , and random marking ratio dM . The composite
congestion signal can be expressed as:
xo = (1 − pL )do + pM dM + pL dL .

(9)

According to (5), the rate at steady state R for any stream
satisfies the following:
xref
R − Rmin
=w
.
(Rmax − Rmin )
xo

(10)

Figure 4 illustrates functional form of the relatioship between
R and xo . The left hand size of (10) can be considered
as the relative bandwidth of the stream. When streams bear
similar propagation delays and reference delay parameter xref
(typically chosen as the expected propagation delay over the
path), it becomes clear form (10) that the ratio between
relative bandwidth of different streams will be dictated by their
relative weights of importance. In other words, the excess rate
each NADA stream receives above its minimum requirement
R−Rmin is weighted by its dynamic rate range (Rmin −Rmin )
together with the user-specified priority level w.
B. Stability Criterion
We now establish the stability criterion for a single NADA
stream over a single bottleneck link with capacity C. For
simplicity, we carry out our derivations for the case of a droptail queue. Derivation of the system stability criterion over
other types of queues can be carried out following the same
methodology.
4 For low-latency interactive video applications, we assume that self-inflicted
queuing delay constitutes the majority portion of the one-way delay.

VII.

P ERFORMANCE E VALUATION

A. Simulation Setup

Fig. 4.

Illustration of the NADA sending rate at equilibrium.

Recalling from (2), (4), and (5), the update for the reference
rate R(t) = Rn in NADA can be expressed as differential
equations in the fluid traffic model:
q̇(t) = R(t) − C;
− log(1 − α)
q(t)
ẋ(t) =
(x(t) −
);
δ
C
x̂(t) = x(t) + ẋ(t)τo ;

xref
.
R(t) = Rmin + w(Rmax − Rmin )
x(t − τ̃ )

(11)
(12)
(13)
(14)

In (11), q(t) corresponds to the bottleneck queue size, and
q̇(t) indicates the first-order time derivative q(t). Following
the derivations in [18], the exponential smoothing filter in (2)
is expressed in the form of (12), where δ corresponds to the
inter-packet arrival interval. In (14), τ̃ denotes round-trip time
of the network.
δ
We introduce τk = − log(1−α)
as the time constant in the
smoothing filter (2). Linearizing the system around the equilibrium points, R = C and xo = w(Rmax − Rmin )xref /(Ro −
Rmin ), we have:
˙
δq(t)
= δR;
δq
1
˙
(δx − );
δx(t)
=
τk
C
˙
δ x̂ = δx + τo δx(t);
w(Rmax − Rmin )xref
δx(t − τ̃ ).
δR = −
x2o

(15)

We evaluate the proposed NADA scheme within
ns-2 [20], an event-driven packet level network simulator.
All simulations follow a simple dumbbell network topology,
as shown in Fig. 5. The one-way propagation delay of the
path is fixed at 10 milliseconds (ms). We mimic the behavior
of a live video encoder in the form of a traffic generator in
ns-2. The encoder can only update its output rate as dictated
by the NADA sender after a delay of τv = 1 s. Furthermore,
the output rate of the encoder randomly fluctuates by 10 %
around the target rate. Unless otherwise stated, the parameters
of the NADA scheme are fixed throughout all experiments.
The rate range is set at Rmax = 6 Mbps and Rmin =0.1 Mbps.
The reference delay is fixed at xref = 20 ms. The smoothing
parameter is set at α = 0.001 at the receiver; the prediction
parameter is set at τo = 0.1 s. We choose the scaling factors
as βs = 0.1 and βv = 0.1 for slightly shifting the sending and
video encoding target rates from the NADA reference rate.
In our experiments, we investigate the interaction of NADA
with a wide variety of queue management schemes, as follows:
•

Drop-tail: In a conventional drop-tail queue, packets
are dropped upon arrival if the queue size exceeds its
limit at that time. The queue limit is set at 100 packets
for a typical packet size of 1200 bytes. This corresponds to a queuing delay of 480 ms over a 2 Mbps
link when the queue is full.

•

RED: In a random-early-drop (RED) queue, packets
are dropped upon arrival randomly with probability
p. The drop probability p is calculated as a function
of the time-smoothed queue length, according to [3].
In our experiments, we set the minimum threshold at
5 packets and the maximum threshold at 95 packets.
The maximum dropping probability is set at 100%.

•

CoDel: As a newly proposed queue management
scheme to address the bufferbloat problem, CoDel is
designed for explicitly controlling the queuing delay
at a given target level [4]. Excess packets are dropped
upon departure based on per-packet measurements of
queuing delay from built-in timestamps. The target
queuing delay is set at 20 ms in our experiments.

•

PIE: As an alternative, lightweight queuing scheme
to address the bufferbloat problem, PIE aims at stabilizing the queuing delay around a target level by
adaptively tuning the random drop probability [5].
Unlike CoDel, packets are dropped upon arrival and
no explicit timestamps are required. The target delay
is also set at 20 ms in our experiments.

•

PCN: In this scheme, the early congestion notification
(ECN) bit in the IP header of packets are marked
randomly. The marking probability is calculated based
on a token-bucket algorithm originally designed for
the Pre-Congestion Notification (PCN) standard [6].
The target link utilization is set as 90%; the marking
probability is designed to grow linearly with the token
bucket size when it varies between 1/3 and 2/3 of the
full token bucket limit.

(16)
(17)
(18)

For very low rates of Rmin ≈ 0, xo ≈ wRmax xref /C.
Therefore, (18) can be expressed as δR = − xCo δx.
Consequently, we obtain the open-loop transfer function in
the Laplace domain:
1 + τo s
G(s) = −
e−sτ̃ .
(19)
(1 + τk s)xo s
Note that the critical frequency of the system can be solved
numerically, such that:
1 + jωc τo
| G(jωc ) |=|
|= 1.
(20)
(1 + jωc τk )ωc xo
Following Bode analysis [19], we need a positive phase margin
for the system to be stable:


G(jωc ) = − 90◦ + arctan(ωc τo ) − arctan(ωc τk ) − ωc τ̃
(21)
> − 180◦ .

Fig. 5. Network topology for simulation evaluation of NADA. The one-way
propagation delay is fixed at 10 ms.

Fig. 7. A single NADA stream over a time-varying link. The bottleneck
queue follows the random early detection (RED) algorithm [3].

Fig. 6. A single NADA stream over a time-varying link. The bottleneck
queue follows the conventional drop-tail discipline.

B. Single Stream over Time-Varying Link
We first consider the simple scenario of a single NADA
stream over a time-varying link. The bottleneck queue follows
the simple drop-tail rule. The queue limit is set at 100 packets,
or, equivalently, 960 Kbits when the typical packets size is
1200 bytes. The bottleneck bandwidth changes over time from
4 Mbps to 2 Mbps at time t = 20 seconds, and then back to
4 Mbps at time t = 80 seconds.
Figure 6 shows traces of video streaming rate, measured
one-way-delay of the stream, as well as the composite congestion signal reported by the receiver. As link rate decreases over
time, the network congestion signal – measured in terms of
one-way packet delivery delay for drop-tail queues – increases
accordingly. This leads the NADA agent to reduce its sending
rate, according to (5) - (7). Similarly, when link capacity
recovers, decreasing one-way delay leads to increasing sending
rate. It can be noted that the NADA stream reacts fast to such
abrupt changes in link capacity, resulting in only brief periods
of delay bursts.
C. Reaction to Losses and Markings
We then investigate how NADA reacts to other forms of
congestions signals, such as packet drops and random early
markings of the early congestion notification (ECN) field in
the IP header [1]. Figures 7 and 8 further show the traces of
sending rate, one-way delay, and composite congestion signals

Fig. 8. A single NADA stream over a time-varying link. The bottleneck
queue performs random early markings based on a token bucket algorithm
originally designed for PCN [6]. Target utilization of the link is set at 90%.

for two different queuing mechanisms at the bottleneck link:
random early drops based on RED [3], and ECN markings
based on a token bucket algorithm originally designed for PreCongestion Notification (PCN) [6].
It can be observed that the sending rate of NADA closely
follows the available link capacity over both RED and PCN,
and the traces of the composite congestion signal look similar
to the previous case with drop-tail queuing. On the other hand,
the contributing factors of the congestion signal are different
for different queuing schemes. During the periods of the low
link rate of 2 Mbps, a persistent packet drop ratio around
1% contributes to the higher composite congestion signal and
consequently lower rate of NADA. In the case of PCN, the
random marking ratio increases with decreasing link rate and
leads the sending rate to always stabilize at 90% of link
capacity. The slight link under-utilization, by design, results
in an empty queue at steady states.

Fig. 9. Packet loss ratio and one-way delay of a single NADA stream at
steady state, when interacting with different queuing schemes over a bottleneck
link of 2 Mbps.

Fig. 10. Traces of sending rate of multiple NADA streams, as they share a
common bottleneck link of 9 Mbps. The bottleneck queue follows the randomearly-detection (RED)] algorithm [3].

D. Tradeoff Between Loss and Delay
Figure 9 summarizes the steady-state performance of
NADA in terms of one-way delay and packet loss ratio over
a single bottleneck link of 2 Mbps. The different data points
correspond to the presence of different queuing schemes at
the bottleneck: conventional drop-tail queuing, random early
detection (RED) [3], CoDel [4], PIE [5], and random early
markings based on a token bucket algorithm originally designed for Pre-Congestion Notification (PCN) [6].
It can be noted that the presence of early drops in RED,
CoDel, and PIE lead to lower one-way delay of the video
stream. Unlike RED, which increases the packet loss ratio
linearly with average queueing delay, both CoDel and PIE aim
at controlling the queueing delay at a target level of 20 ms.
Consequently, they yield lower one-way delay of the video
stream, at the expense of slightly higher packet loss ratios. By
design, the early random markings in PCN leads to the lowest
delay and zero packet loss at steady state while streaming at
90 % of link capacity.

F. Competing with TCP Flows
Finally, we study the behavior of NADA streams in the
presence of competing TCP traffic. Figure 12 summarizes
the average rate obtained by the two NADA streams and the
competing TCP flow over various queuing mechanisms. The
bottleneck link has a capacity of 4 Mbps. Since NADA senders
react to the combined congestion signal of delay, loss, and
markings, they are able to sustain a substantial portion of
the bottleneck link capacity in the presence of a competing
TCP flow. The behavior of TCP is the most aggressive when
only conventional drop-tail queue is in use. All three randomdropping based queueing schemes, RED, CoDel, and PIE,
lead to similar bandwidth allocations between NADA and
TCP flows. With PCN, the high random marking ratio around
45% leaves very little room for the TCP flow to survive. The
two NADA streams, nonetheless, are able to maintain their
substantial share of bandwidth with a relative ratio of 2:1.
VIII.

E. Multiple NADA Streams
Next, we consider the scenario where multiple NADA
streams share the same bottleneck link. Figure 10 shows the
traces of sending rate of four NADA streams, as they gradually
enter a link with capacity 9 Mbps. The bottleneck queue
follows the random-early-detection (RED) scheme. Half of the
streams have a priority weight of w = 1, the other half have
a priority weight of w = 2. During the period when all four
streams are active, the sending rate stabilizes around 1.5 Mbps
and 3.0 Mbps, respectively, for the two classes of streams.
Figure 11 shows the per-stream sending rate, one-way
delay experienced by each stream, as well as the composite
congestion signal, when the link rate varies from 6 Mbps to
12 Mbps. All streams experience the same amount of one-way
delay, packet loss ratio, and consequently the same values of
composite congestion signal. The ratio between the rates of
the two classes of streams consistently stays at 1:2.

C ONCLUSIONS AND F UTURE W ORK

This paper describes a unified approach for congestion
control of real-time media: NADA (network-assisted dynamic
adaptation). Our design of NADA follows the key idea of
integrating all forms of congestion indications — delay, loss,
and marking — into a composite signal. Consequently, it
remains robust in the presence of loss-based congestion control
schemes. As confirmed by extensive simulation results, NADA
works well with a wide variety of queue management schemes,
including drop-tail, RED, CoDel, PIE, and PCN-based random
marking. Both theoretical analysis and simulation studies further confirm that NADA supports weighted bandwidth sharing.
The bandwidth consumed by each stream is naturally weighted
by its own rate dynamic range, as dictated by video content,
as well the user-specified level of priority.
Encouraged by these preliminary results, we plan to evaluate performance of NADA in a testbed setting. In particular, we
are interested in studying how NADA interacts with an on-line
encoder rate control process, over time-varying video contents
captured in a real-world conferencing setting. We will also

(a) One-way delay and Composite
Congestion Signal

(b) Per-Stream Packet Loss Ratio

(c) Per-Stream Sending Rate

Fig. 11. Four NADA streams competing over a single bottleneck link. Streams 1 and 3 have a priority weight of w = 1; Streams 2 and 4 have a priority
weight of w = 2. The link rate varies between 6 Mbps and 12 Mbps. The bottleneck queue follows the random-early-detection (RED) algorithm [3].

Fig. 12. Average rate of two NADA streams and a competing TCP flow,
when sharing a common bottleneck link of 4 Mbps. The two NADA streams
maintain a relative ratio of 1:2 in their sending rates, across all experiments.

explore mechanisms for automatically tuning the parameters in
the NADA according to network load at steady state, content
characteristics and loss-resiliency of the video stream.

R EFERENCES
[1] K. K. Ramakrishnan, S. Floyd, and D. Black, “The addition of explicit
congestion notification (ECN) to IP,” RFC 3168 (Proposed Standard),
Sep. 2001.
[2] H. Schulzrinne, S. Casner, R. Frederick, and V. Jacobson., “RTP: A
transport protocol for real-time applications,” RFC 3550 (Standard),
Jul. 2003.
[3] S. Floyd and V. Jacobson, “Random early detection gateways for
congestion avoidance,” IEEE/ACM Transactions on Networking, vol. 1,
no. 4, pp. 397–413, Aug. 1993.
[4] K. Nichols and V. Jacobson, “A modern AQM is just one piece of the
solution to bufferbloat,” ACM Queue, May 2012.
[5] R. Pan, P. Natarajan, C. Piglione, M. S. Prabhu, V. Subramanian,
F. Baker, and B. VerSteeg, “”pie: A lightweight control scheme to
address the bufferbloat problem”,” in Proc. IEEE International Conference on High Performance Switching and Routing (HPSR’13), Taipei,
Taiwan, Jul. 2013.

[6] M. Menth, F. Lehrieder, B. Briscoe, P. Eardley, T. Moncaster, J. Babiarz,
A. Charny, X. J. Zhang, T. Taylor, K.-H. Chan, D. Satoh, R. Geib,
and G. Karagiannis, “A survey of PCN-based admission control and
flow termination,” IEEE Communications Survey and Tutorials, vol. 12,
no. 3, pp. 357–375, Jul. 2010.
[7] V. Jacobson, “Congestion avoidence and control,” in ACM Conference
on Communications Architectures, Protocols and Applications (SIGCOMM’88), vol. 18, Stanford, CA, USA, Aug. 1988, pp. 157–173.
[8] S. Floyd, M. Handley, J. Pahdye, and J. Widmer, “TCP friendly
rate control (TFRC): Protocol specification,” RFC 5348 (Proposed
Standard), Sep. 2008.
[9] B. Wang, J. Kurose, P. Shenoy, and D. Towsley, “Multimedia streaming
via TCP: An analytic performance study,” in Proc. ACM Multimedia
(MM’04), New York, NY, USA, Oct. 2004.
[10] Z. Wang, S. Banerjee, and S. Jamin, “Media-friendliness of a slowlyresponsive congestion control protocol,” in Proc. 14th International
Workshop on Network and Operating Systems Support for Digital Audio
and Video (NOSSDAV’04), Cork, Ireland, 2004, pp. 82–87.
[11] J. Yan, K. Katrinis, M. May, and B. Plattner, “Media- and TCP-friendly
congestion control for scalable video streams,” IEEE Trans. Multimedia,
vol. 8, no. 2, pp. 196–206, Apr. 2006.
[12] J. Gettys, “Bufferbloat: Dark buffers in the Internet,” IEEE Internet
Computing, vol. 15, no. 3, pp. 95–96, May 2011.
[13] J. Wang, D. X. Wei, and S. Low, “Modelling and stability of FAST
TCP,” in Proc. 24th IEEE INFOCOM, Miami, FL, USA, Mar. 2005.
[14] H. Lundin, S. Holmer, and H. Alvestrand, “A Google congestion control
algorithm for real-time communication,” Internet-Draft (Informational),
Feb. 2013.
[15] P. O’Hanlon, “Congestion control algorithm for lower latency and lower
loss media transport,” Internet-Draft (Informational), Apr. 2013.
[16] Ł. Budzisz, R. Stanojevic, A. Schlote, F. Baker, and R. Shorten, “On
the fair coexistence of loss- and delay-based TCP,” IEEE/ACM Trans.
Networking, vol. 19, no. 6, pp. 1811–1824, Dec. 2011.
[17] X. Zhu and R. Pan, “NADA: A unified congestion control scheme for
real-time media,” Internet-Draft (Informational), Mar. 2013.
[18] C. Hollot, V. Misra, D. Towsley, and W.-B. Gong, “A control theoretic
analysis of RED,” in Proc. IEEE International Conference on Computer
Communications (INFOCOM’01), Anchorage, Alaska, USA, Apr. 2001.
[19] G. Franklin, J. D. Powell, and A. Emami-Naeini, Feedback Control of
Dynamic Systems. NJ, USA: Prentice Hall, 2006.
[20] “The network simulator NS-2,” http://www.isi.edu/nsnam/ns/.

International Conference on Computing, Networking and Communications Invited Position Paper Track

Cloud-Assisted Streaming for Low-Latency Applications
Xiaoqing Zhu, Jiang Zhu, Rong Pan, Mythili Suryanarayana Prabhu, and Flavio Bonomi
Advanced Architecture & Research, Cisco Systems Inc., San Jose, CA 95134, U.S.A.
{xiaoqzhu,jiangzhu,ropan,mysuryan,ﬂavio}@cisco.com
Invited Paper

Abstract—Media cloud services offer an unique opportunity for alleviating many of the technical challenges faced by mobile media streaming,
especially for applications with stringent latency requirements. In this
paper, we propose a novel cloud-assisted architecture for supporting lowlatency mobile media streaming applications such as onling gaming and
video conferencing. A media proxy at the cloud is envisioned to calculate
the optimal media adaptation decisions on behalf of the mobile sender,
based on past observations of packet delivery delays of each stream. The
proxy-based intelligent frame skipping problem is formulated within the
Markov Decisio Process (MDP) framework, which captures both the timevarying nature of video contents as well as bursty ﬂuctuations in wireless
channel conditions. The optimal frame skipping policy is calculated using
the stochastic dynamic programing (SDP) approach, and is shown to
consistently outperform greedy heuristic schemes. Our simulation studies
further characterize how system performance is inﬂuenced by various key
factors, such as application playout latency, network round-trip-time, and
wireless link throughput.
Index Terms—cloud computing, low latency media streaming, mobile
video conferencing, video adaptation

I. I NTRODUCTION
Recent years have seen a proliferation of smart mobile devices,
which, in turn, has fueld the rapid growth of mobile media trafﬁc.
Many of the applications supported by today’s mobile devices have
stringent latency requirements. Examples include media sharing of
live events, online gaming, mobile video conferencing, and mediarich virtual desktops. According to [1], the latency threshold for
ﬁrst-person-based avatar games such as racing and combating is
in the range of 50 - 100ms. Whereas for video conferencing, the
recommended one-way latency is below 150ms [2]. Packet delivery
latency, in addition to bandwidth, has is a key performance metric
for such applications.
On the other hand, mobile media streaming remains a daunting
challenge due to inherently time-varying wireless communication
channel, unpredictable user demand in the media cloud, and ﬂuctuating source rate of media contents generated on-the-ﬂy. Moreover,
the mobile sender typically has limited battery power and computational resources, hence may not afford to implement sophisticated
adaptation algorithms for matching the rate of streaming media to
available wireless network throughput.
Mobile media streaming could beneﬁt from some form of assistance from media cloud servers or proxies in many ways. For
instance, the relatively abundant and low-cost computational power
of cloud servers could be leveraged for carrying out analysis and
estimation of network conditions based on past packet measurements.
Accordingly, they can make intelligent media adaptation decisions
on behalf of the mobile devices. The cloud media proxy can fuse
network measurement reports from many mobile users in the same
coverage area, and derive robust statistical models for the wireless
communication channel. In addition, as the cloud media proxy is
typically situated half way along the path between sender and
receiver, it can prompt the sender to take more agile adaptation
actions in face of sudden changes, which is of particular importance
for low-latency streaming applications.

978-1-4673-0009-4/12/$26.00 ©2012 IEEE

In this work, we showcase the potential beneﬁts of cloud-assisted
media streaming in the application scenario of mobile video conferencing. For simplicity, we consider the option frame rate adaptation at
the sender. The proposed cloud media proxy takes into consideration
past observations such as measured round-trip-times and recent
packet delivery delays, and dictates the mobile sender whether to
encode or skip the next video content frame as captured by the
camera. The intelligent frame skipping problem is formulated within
the Markov decision process (MDP) framework. Our formulation
captures the impact of many contributing factors to end-to-end system
latency: time-varying nature of wireless communication channels,
trafﬁc shaping delay at the sender, and video content ﬂuctuation. It is
shown that the optimal frame skipping policy can be calculated using
the stochastic dynamic programing (SDP) approach. Alternatively, the
optimal solution can be closely approximated using a greedy heuristic
scheme that only takes into account the most recent observed packet
delivery delay. Simulation results show how system performance
is inﬂucence by various key factors, including application playout
deadlines, network round-trip-times, and wireless link throughputs.
The rest of the paper is organized as follows. The next section
reviews related work. Section III provides an overview of the
proposed cloud-assisted media streaming architecture. Section IV
explains how we model the various components in end-to-end media
streaming delay. Section V presents our MDP-based formulation of
the intelligent frame skipping problem, together with the SDP-based
optimal policy. In Section VI, we study fundamental performance
tradeoffs of the system via simulation results under various network
conditions. Section VII concludes the paper.
II. R ELATED W ORK
There exists a rich body of literature on low-latency video streaming in the conventional server-client architecture. For instance, the
work in [3] has applied linear quadratic optimal control theory to
the design of an optimal streaming rate controller, which achieves
low startup delay, continuous playback, and efﬁcient bandwidth
utilization. In [4], the rate-distortion optimized packet scheduling
problem is cast within the MDP framework, which accounts for
random packet losses and delay over the best-effort network, as well
as the interdependencies between media packets. The MDP approach
is also used for video encoder rate control [5] and joint packet pruning
and adaptive playout [6] over time-varying wireless channel. Our
work follows a similar mathematical framework, but differs from the
above by leveraging a cloud media proxy for calculating the media
adaptation decisions on behalf of the mobile senders.
Recent research has recognized the potential beneﬁts of leveraging
proxy servers at the cloud for augmenting the computational and
power constraints of the mobile devices [7] [8]. It is shown in [9]
that joint adaptation in rendering and encoded video qualities by the
cloud gaming proxy can effectively improve the user experience. We,
instead, consider generic low-latency media streaming applications in
this work, and study their fundamental performance tradeoffs.

949

sender, the proxy records from the receiver’s report one-way delay
over the rest of the network. The cloud media proxy can also track
network round-trip-time (RTT) for each stream. It then calculates
the optimal frame skipping strategy accordingly, and indicates the
updated frame skipping decision for the next available frame to the
mobile sender via a separate control message. Figure 2 summarizes
how information ﬂows between the the sender, the proxy, and the
receiver.
IV. S YSTEM M ODEL

System overview for cloud-assisted media streaming. The cloud
media servers act as proxy between the mobile sending and receiving
devices. It is in charge of collecting packet delivery statistics based on
sender and receiver reports, as well as calculating the optimal media
adapation decision on behalf of the mobile sender.

Fig. 1.

Consider a live media stream generated at the original frame rate of
vf and corresponding frame interval of τf = 1/vf . The time instant
at which the n-th frame is available at the sender (e.g., captured by
the builtin camera, or rendered by the game server) is denoted as
ts (n) = nτf . The total number of frames is N . In the following,
we explain how we model the uncertainties in various components
of the media streaming system.
A. Video Content Variation
Denoting the encoded frame size for the n-th frame as B(n), we
model its evolution as a ﬁrst-order Gauss-Markov process. Introducing X(n) = B(n) − μB , we have:
X(n)

=

ρX(n − 1) + Y (n)

X(n)

∼

2
N (0, σB
)

Y (n)

∼

2
).
N (0, (1 − ρ2 )σB

(1)

Here, the correlation coefﬁcient ρ reﬂects similarities across contents
in adjacent frames. The mean and standard deviation of the frame
sizes are captured by μB and σB , respectively.
B. Wireless Channel Fluctuation

Fig. 2.

Time diagram of cloud-assisted media stremaing protocol.

III. A RCHITECTURE OVERVIEW
As illustrated in Fig. 1, we envision the presence of a media
proxy within the cloud, which helps to relay the video packets
from the mobile sender to a (potentially mobile) receiver.1 The
cloud media servers act as proxy between the mobile sending and
receiving devices. The cloud media proxy is in charge of collecting
packet delivery statistics based on sender and receiver reports. It
also calculates the optimal media adapation decision on behalf of
the mobile sender.
In our design, both the sender and the receiver periodically report
to the cloud media proxy observed per-packet delivery delay. Such
information can either be embedded as part of meta data in the video
packet header, e.g., as part of the extension header in the realtime
transport protocol (RTP) [10], or can be conveyed separately in the
form of control messages.
Upon forwarding media packet from the sender to the receiver, the
cloud media proxy extracts from the sender’s report encoded frame
size, transmission delay and trafﬁc shaping delay of the forwarded
frame. Upon forwarding acknowledgement packet from receiver to

Random variations in the wireless link upload throughput B(n)
are modeled as a two-state hidden Markov process. The two states
correspond to good (G) or fading (F) channel conditions, and capture
the bursty nature of the wireless communication channel. During the
good channel state, the value of R(n) follows the normal distribution:
2
R(n) ∼ N (μG , σG
). During the fading state, R(n) ∼ (μF , σF2 ).
The expected duration of each state can be calculated from the
state transitional probabilities pGF and pF G , as:
pF G
τf ,
TG =
pGF + pF G
pGF
τf .
TF =
pGF + pF G
Here,
pGF := Pr{frame n is in good state | frame n − 1 is in fading state},
pF G := Pr{frame n is in good state | frame n − 1 is in fading state}.
C. End-to-end Frame Delivery Delay
We assume that in this work the bottleneck along the path is
the upload wireless link at the mobile sender. Consequently, the
end-to-end delivery delay d(n) for the n-th frame consists of three
components: queuing delay at the sending buffer dq (n), transmission
delay of that frame over the wireless channel dw (n), and one-way
delay over rest of the network dOW D . This can be expressed as:

1 Note

that the applications supported by such a system can involve bidirectionaly trafﬁc, as in the case of video conferencing. Without loss of
generality, we limit our discussions in this work to the media stream along
one of the directions.

950

dq (n)

=

B(n)
R(n)
max[0, dq (n − 1) + dw (n − 1) − τf ]

d(n)

=

dw (n) + dq (n) + dOW D .

dw (n)

=

(2)
(3)
(4)

B. Actions
Each frame is associated with two available actions:

0, skip frame n
, an ∈ A.
an =
1, transmit frame n
Hence the action space is binary A = {0, 1}.
To accommodate the fact that a skipped frame does not enter
the sending buffer and incur transmission delay, we modify the
expressions in (2) as:
dw (n) = an

Illustration of media delivery timelines at the sender and the
receiver. The frames encompassed by red encircles are voluntarily skipped
at the sender.

Fig. 3.

B(n)
.
R(n)

(9)

Note that the correponding end-to-end delay for frame n is not only
affected by past systems states sn , n < n, but also by past actions
an , n < n. Combining (9), (3), and the statistical models for B(n)’s
and R(n)’s in Section IV, one can one can derive the state transitional
probability Pa (s , s) =Pr{sn+1 = s |sn = s, an = a} accordingly.
C. Cost Function

In this work, the one-way-delay is assumed to be half of round-triptime across the network: dOW D = RT T /2. The value of RT T , in
turn, can be estimated at the cloud media proxy by periodicly probing
the sender and receivers using small control messages.
Correspondingly, the time at which the frame arrives at the receiver
is tr (n) = ts (n) + d(n). Given a playout deadline of To , the system
should strive to deliver all transmitted frames before their playout
deadline, such that tr (n) < ts (n) + To . Frames arriving after the
playout deadline are discarded at the receiver. Figure 3 illustrates the
evolution of the timeline both at the sender and at the receiver.
V. MDP- BASED P ROBLEM F ORMULATION
We now explain how the intelligent frame skipping problem can
be formulated within the MDP framework. The following subsections
describe the cloud-assisted media streaming system in terms of
system states, actions, cost functions, and optimal policies.

The ﬁnal video quality at the receiver is inﬂucenced by
following factors2 :
• frame skipping: percentage of voluntarily skipped frames at
sender, as dictated by the cloud media proxy.
• frame dropping: percentage of late frames being dropped at
receiver due to missed playout deadlines.
The impact of skipped and dropped frames are captured by
following cost function:
g(sn , an ) = α(1 − an ) + βan 1{d(n) > To },

Following the timeline of the mobile sender, the system state of
the n-th frame is deﬁned as:
sn = {B(n − k), R(n − k), dw (n − k), dq (n − k)}, sn ∈ S. (5)
In (5), k = RT T /τf  corresponds to the lag of observation
introduced by round trip time RT T in the system. The value of
the observations are discretized as B(n − k) ∈ B, R(n − k) ∈ R,
dw (n − k) ∈ D, and dq (n − k) ∈ D:
B = {b1 , b2 , · · · , bLB },

bl =

R = {r1 , r2 , · · · , rLR },

rl =

D = {d1 , d2 , · · · , dLD },

dl =

l
B
,
LB max
l
R
,
LR max
l
Dmax ,
LD

min E

N


the
the

(10)

[g(sn , an )].

(11)

n=1

The expectation is taken over all possible realizations of d(n), n =
1, · · · , N given the chosen actions (an ’s) for all frames.
D. SDP-based Policy
Given state transitional probability Pa (s , s) and cost function
g(sn , an ), it is possible to solve the optimization problem in (11)
using various standard algorithms [11]. In this work, we follow the
stochastic dynamic programming (SDP) approach, and recursively
minimize the expected cost-to-go function as:

l = 1, · · · , LB . (6)
Jn (s)

l = 1, · · · , LR . (7)
l = 1, · · · , LR . (8)

In (6) - (8), the maximum value of each variable is determined
by Bmax , Rmax , and Dmax , respectively. The number of units for
each variable is LB , LR , and LD , respecitvely. Correpondingly, the
granularities of the discretization are inﬂuenced by both the range
and number of units for each variable. The compound system state
space is represented as S = B × R × D × D. The value of each
state variable is discretized into multiple units: The overall size of
the state space is |S| = |B||R||D|2 .
As illustrated in Fig. 2, the cloud media proxy can collect the
value of B(n), dw (n), and dq (n) based on sender’s report. It can
then derive the value of R(n) according to (2).

the

where the coefﬁcients 0 < α < β denote the relative penalties
introduced by the percentage of skipped and dropped frames. The
binary indicator function 1(.) takes on the value of 1 if its argument
statement is true, and 0 if it is false.
The overall expected cost for all N frames is expressed as:
{an }N
1

A. System States

the

=
=

min

{an }N
n+1

E

N


[g(sn , an )]

n =n+1

min E[g(sn , an ) +
an

Pan (s , s)Jn+1 (s )].

(12)

s ∈S

Note that the optimal choice of frame skipping policy π(n) = an
at each step is determined not only by the outcome of skipping or
dropping the current frame, but also by the inﬂuence of that decision
on the queuing delays of future frames.
2 In modern systems, wireless transmission errors are mostly elleviated
by physical-layer and MAC-layer techniques such as channel coding and
persistent retransmission. We therefore assume that no frame losses in the
proposed system.

951

While the SDP-based policy can yield the optimal expected
performance, it is fairly complicated. For each media stream it
supports, the media cloud proxy needs to pre-compute the cost-togo function with O(M |S|) entries, where M is the optimization
horizon and |S| is the size of the state space. It also needs to precompute the state transitional probability matrix with 2|S|2 ) entries.
The online computational complexity is on the order of O(|S|)
for calculating the expected future cost-to-go function based on
state transitional probabilities. While such computational burdens can
easily overwhelm a mobile client, it is still acceptable in the media
cloud, with relatively aboundant computational resources at low cost.
VI. S IMULATION R ESULTS
A. Setup
In this section, we evaluate the proposed frame rate adaptation
schemes using numerical simulations. The original frame rate of
the video sequence is chosen at 30 frames-per-second (fps). The
correlation coefﬁcient in the Gauss-Markov model parameters for the
video sequences is chosen as ρ = 0.8. The average frame size is
μf = 3000 bytes, corresponding to an average video streaming rate
of 720 Kbps if no frames are skipped at the sender. The standard
deviation of the frame size is set as σf = 300 bytes. The application
playout deadline varies between 60ms to 600ms.
The wireless channel rate ﬂuctuates between a good and a fading
state. The average throughput of the good channel state varies
between 600 Kbps to 2 Mbps, whereas the average throughput of the
fading state is chosen to be μF = 0.3μG . We further ﬁx the standard
deviation of the channel rates to be 10% of the average throughputs,
i.e., σG = 0.1μG and σF = 0.1μF . Network round-trip-time varies
between 20ms and 100ms.

Fig. 4. Percentage of skipped and dropped frames as achieved by the
NFS, RFS, DFS, and SDP-based policies. The playout deadline varies
between 60ms to 600ms. The average wireless link throughput μG is
1 Mbps during good channel and μF = 0.3μG during fading state.
Network round-trip-time is at 60ms.

B. Competing Schemes
As a basis for comparison, we also consider the system performance without frame skipping, referred to as No Frame Skipping
(NFS), together with two heuristic schemes:
• In the Random Frame Skipping (RFS) scheme, the mobile
sender randomly chooses to skip a ﬁxed percentage η of the
content frames, without adapting to any observed packet delivery
statistics.
• In the Delay-based Frame Skipping (DFS) scheme, the cloud
media proxy dictates the sender to skip frame n + k if the
observed delay for the n-th frame d(n) exceeds a prescribed
portion of the playout deadline: d(n) > γTo . The scaling factor
γ < 1 can be tuned to adjust how cautious the sender is in
reacting to observed long delivery delays for past frames.
C. Varying Playout Deadline
Figure 4 compares the heuristic schemes and the SDP policy
in terms of the percentage of skipped and dropped frames, as a
function of varying playout deadline of the application. The resulting
normalized video quality are shown in Fig. 5. As the playout
deadline becomes more relaxed, the percentage of frames missing
their deadlines decreases for all schemes. The two adaptive schemes,
DFS and SDP, also tend to reduce the percentage of voluntarily
skipped frames along with increasing playout deadline. Consequently,
the normalized video quality at the receiver increases with more
relaxed playout deadlines. It is also worth noting that while overall
performance of DFS heuristic scheme closely tracks that of the SDP
policy, for more latency-sensitive applications with a playout deadline
of 60ms, the SDP scheme signiﬁcantly outperforms DFS, mainly by
avoiding unnecessary volutary frame drops.

Fig. 5. Normalized video quality as achieved by NFS, RFS, DFS, and the

SDP-based policy. The playout deadline varies between 60ms to 600ms.
The average wireless link throughput is μG = 1000 Kbps during good
rate and μF = 300 Kbps during fading state. Network round-trip-time is
at 60ms.

D. Varying Network RTTs
Next, we examine how network latency affects system performance. We vary the round-trip-time (RTT) between 20ms to 100ms,
while keeping the application playout deadline at 60ms. Figure 6
compares performance of NFS, RFS, DFS, SDP in terms of normalized video quality at the receiver. It is clear to see that the SDPbased scheme signiﬁcantly outferforms others when network roundtrip-time is moderate, below 60ms. As RTT increases, performance
of all four schemes degrades due to the increasing lag between past
observation at the media cloud proxy and its frame skipping decision
for future available frames at the mobile sender. At the very extreme,
when network one-way-delay approaches the playout deadline, it
is no longer feasible to support the streaming application. In our
experiments, this corresponds to the data point where round-trip-time
is 80ms, as a one-way-delay of 40ms accounts for 67% of end-to-end
packet delivery delay.

952

utilization. On the other hand, even when there is sufﬁcient overall
bandwidth over-provisioning, the time-varying nature of the video
contents and the link qualities still prevents the system from achieving
full received video quality, as frames may still occasionally need to
be skipped or miss their rather stringent latency deadlines.
VII. C ONCLUSIONS

Fig. 6. Normalized video quality as achieved by NFS, RFS, DFS, and
the SDP-based policy. Network round-trip-time varies between 20ms and
100ms. The average wireless link throughput μG is 1 Mbps during good
channel and μF = 0.3μG during fading state. The playout deadline is
chosen at 60ms.

In this paper, we propose a novel cloud-assisted architecture
for supporting mobile media streaming applications with stringent
latency constraints. The media proxy at the cloud is envisioned to take
on the burden of calculating the optimal media adaptation decisions
on behalf of the mobile sender, based on past observations of packet
delivery delays for each stream. The actual adaptation actions of
skipping a frame for encoding and transcoding are still performed
at the mobile sender.
The proxy-based intelligent frame skipping problem is formulated
within the Markov Decisio Process (MDP) framework, which captures the time-varying nature and uncertainty both in video encoded
frame sizes and in wireless channel conditions. It is shown that the
optimal frame skipping policy can be calculated using the stochastic
dynamic programing (SDP) approach. Alternatively, the optimal
solution can be closely approximated using a greedy heuristic scheme
that only takes into account the delivery delay information of the most
recently observed frame. Simulation results over varying application
playout deadlines, wireless link throughputs, and network round-triptimes conﬁrm the optimality of the SDP approach. It is also shown
that while relaxing the application playout deadline tends to gradually
improve the received video quality, quality degradation introduced by
increasing network round-trip-time or reducing network bandwidth
tends to be more drastic once the system resources approach a certain
limit.
R EFERENCES

Normalized video quality as achieved by NFS, RFS, DFS,
and the SDP-based policy. The average wireless uplink bandwidth μG
varies between 600 Kbps to 2 Mbps during good channel state, with
μF = 0.3μG during fading state. The playout deadline is chosen at
60ms. Network round-trip-time is at 60ms.
Fig. 7.

E. Varying Wireless Uplink Bandwidth
Finally, we study how bandwidth over-provisioning can help with
accommodating low-latency streaming. We ﬁx the playout deadline
at 60ms and network round-trip-time at 60ms. The average wireless
uplink bandwidth μG varies between 600 Kbps and 2 Mbps during
good channel state. Accordingly, the average throughput of the
wireless link during fading state is kept as μF = 0.3μG , varying
between 180 and 480 Kbps.
Figure 7 shows the normalized video quality achieved by NFS,
RFS, DFS, and the SDP-based policy as a function of wireless
uplink bandwidth during good channel state (μG ). When the available
bandwidth is lower than the full video source rate, both the SDPbased policy and DFS can gracefully downgrade the quality of the
received video within reasonable range, by directing the sender to
voluntarily dropping the video frames. It can also be noted that the
SDP-based scheme consistently outperforms DFS, especially around
the region where μG =1 Mbps, corresponding to 80% of bandwidth

[1] M. Claypool and K. Claypool, “Latency and player actions in online
games,” Communications of the ACM, vol. 49, no. 11, pp. 40–45, nov
2006.
[2] “IITU-T Recommendation G.114 - One-way Transmission Time,” ITU-T
(Standard), Feb. 2003.
[3] C. Huang, P. A. Chou, and A. Klemets, “Optimal coding rate control
for scalable streaming media,” in Proc. 14th International Packet Video
Workshop (PV’04), Irvine, CA, USA, December 2004.
[4] P. A. Chou and Z. Miao, “Rate-distortion optimized streaming of
packetized media,” IEEE Trans. Multimedia, vol. 8, no. 2, Apr. 2006.
[5] J. Cabrera and A. Ortega, “Stochastic rate control of video coders
for wireless channels,” IEEE Trans. Circuits and Systems for Video
Technology, vol. 12, no. 6, pp. 496–510, June 2002.
[6] Y. Li, A.Markopoulou, N.Bambos, and J.Apostolopoulos, “Joint powerplayout control for media streaming over wireless links,” IEEE Trans.
Multimedia, vol. 8, no. 4, pp. 830–843, Aug. 2006.
[7] B.-G. Chun and P. Maniatis, “Augmented smart phone applications
through clone cloud execution,” in Proc. HotOS XII, 2009.
[8] B. Zhao, B. C. Tak, and G. Cao, “Reducing the delay and power
consumption of web browsing on smartphones in 3g networks,” in Proc.
IEEE 31st International Conference on Distributed Computing Systems
(ICDCS’11),, Minneapolis, MN, USA, July 2011, pp. 413–422.
[9] S. Wang and S. Dey, “Rendering adaptation to address communication
and computation constraints in cloud mobile gaming,” in Proc. IEEE
Global Telecommunications Conference (GLOBECOM’10), 2010.
[10] H. Schulzrinne, S. Casner, R. Frederick, and V. Jacobson., “RTP: A
transport protocol for real-time applications,” RFC 3550 (Standard),
2003.
[11] M. Puterman, Markov Decision Processes: Discrete Stochastic Dynamic
Programming. John Wiley and Sons, 1994.

953

Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence

Personalized Tag Recommendation
through Nonlinear Tensor Factorization Using Gaussian Kernel
Xiaomin Fang and Rong Pan∗

Guoxiang Cao, Xiuqiang He and Wenyuan Dai

Department of Computer Science
Sun Yat-sen University
Guangzhou, China
{fangxmin@mail2, panr@}.sysu.edu.cn

Huawei Technologies Co., Ltd.
Bantian, Longgang District
Shenzhen, China
{benjamin.cao, hexiuqiang, dai.wenyuan}@huawei.com

Abstract

Some tag recommender systems rank tags by means
of tensor factorization techniques. Tensor factorization is
a high-order extension of matrix factorization. Tensor
factorization-based models decompose the user-item-tag
tensor into three matrices to learn the latent features of
the users, items and tags. Higher Order Singular Vector
Decomposition (HOSVD) (Symeonidis, Nanopoulos, and
Manolopoulos 2008), Multiverse (Karatzoglou et al. 2010)
and RTF (Rendle et al. 2009) are based on Tucker Decomposition (TD), which can improve the state-of-the-art recommenders in prediction quality, but require too much computation power. In contrast, pairwise interaction tensor factorization (PITF) (Rendle and Schmidt-Thieme 2010) is based
on another factorization approach, Canonical Decomposition (CD). It is linear both in the dataset size and the feature
dimensionality. In this paper, we present a novel approach
based on Canonical Decomposition and can be considered
as a nonlinear generalization of CD. We use Gaussian radial
basis function to capture the complex relations between the
users, items and tags. We will analyze why exploiting Gaussian can make better use of the latent features and its relation to the traditional linear tensor factorization techniques.
Our experimental results show that our proposed model outperforms other competitive methods and can achieve good
performance with only a small number of latent features.
The contributions of our work are summarized as follows.

Personalized tag recommendation systems recommend
a list of tags to a user when he is about to annotate
an item. It exploits the individual preference and the
characteristic of the items. Tensor factorization techniques have been applied to many applications, such
as tag recommendation. Models based on Tucker Decomposition can achieve good performance but require
a lot of computation power. On the other hand, models based on Canonical Decomposition can run in linear
time and are more feasible for online recommendation.
In this paper, we propose a novel method for personalized tag recommendation, which can be considered as a
nonlinear extension of Canonical Decomposition. Different from linear tensor factorization, we exploit Gaussian radial basis function to increase the model’s capacity. The experimental results show that our proposed
method outperforms the state-of-the-art methods for tag
recommendation on real datasets and perform well even
with a small number of features, which verifies that our
models can make better use of features.

Introduction
Nowadays, a great amount of information springs up in the
Internet and users struggle to find the items they are really
interested in. To tackle this problem, recommendation systems are designed to recommend appropriate items to the
users according to their habits. Collaborative tagging systems like Delicious, Last.fm and Movielens allow users to
upload resources and annotate them. The activity that users
annotate items, such as movies, songs and pictures with
some keywords, is called tagging. A tag can be viewed as
an implicit rating and be used to identify not only the features of the items, but also the users’ personality. Tag recommendation is a task to suggest tags to a user when he
is about to annotate an item. Given an item, different users
may use different tags to annotate it. Personalized tag recommender systems utilize users’ past tagging behaviors to
predict their future behaviors. For example, if two users annotated an item with the same tag before, it is likely that they
will annotate another item with the same tag in the future.

• We propose a novel nonlinear approach based on Canonical Decomposition, which employs Gaussian radial basis function. The time performance is comparable to PITF
and runs in linear time.
• We compare our nonlinear approach to the traditional linear methods and analyze the reasons why the Gaussianbased method can make better use of the latent features.
• The experimental results demonstrate that our method
outperforms other methods on real datasets and performs
well even with only a small number of latent features.

Related Work
Tag Recommendation Techniques
There is a lot of previous work on tag recommendation.
Work by Krestel, Fankhauser, and Nejdl proposed an approach based on Latent Dirichlet Allocation(LDA). Given a

∗

Corresponding author
c 2015, Association for the Advancement of Artificial
Copyright 
Intelligence (www.aaai.org). All rights reserved.

439

document, the top terms composing its latent topics are recommended to the users. By this way, the terms in the same
topic but showed in the other documents have the opportunity to be presented. Lipczak et al. employed probabilistic
latent topic model for tag recommendation as well. They
added a variable of tag to LDA, so as to link the tags to
the topics of the document. Liu et al. ranked the tags for
images by using random walks. However, the systems mentioned above are impersonalized. Different users may prefer
different tags for the same item. Work by Guan et al. ranked
tags by a graph-based ranking algorithm. It takes personalized tag recommendation as a ”query and ranking” problem.
The query consists of two parts, the user and the item. The
tags are related to the item and the user. Besides, Durao and
Dolog introduced a hybrid approach. It makes use of several factors, including tag similarity, tag popularity, tag representativeness and affinity between user and tag to provide
appropriate tags.

XVHU

XVHUX

"
LWHP

"

LWHP

   

LWHP

"

"
"

"
"

"

   

LWHP

WDJ

WDJ

WDJ

WDJ

WDJ

LWHP

Figure 1: Tensor construction and post-based ranking interpretation for personalized tag recommendation
man 1970) can be seen as higher-order extension of the matrix factorization, Singular Value Decomposition (SVD).
Tucker Decomposition (TD) decomposes a tensor into a
core tensor multiplied by a matrix along each mode. Take a
three-order tensor A ∈ RI×J×K for example and it can be
written as
Ai,j,k =

Tensor Factorization Techniques

R1 X
R2 X
R3
X

Cr1 ,r2 ,r3 · Xi,r1 · Yj,r2 · Zk,r3 ,

r1 =1 r2 =1 r3 =1

(1)
where X ∈ RI×R1 , Y ∈ RJ×R2 , Z ∈ RK×R3 and C ∈
RR1 ×R2 ×R3 is a core tensor. R1 , R2 and R3 are the number
of latent features.
Canonical Decomposition (CD) decomposes a tensor into
a sum of component rank-one tensors. Let A ∈ RI×J×K be
a tensor and it can be written as

Tucker Decomposition and Canonical Decomposition are
the high-order extension of Singular Value Decomposition
(SVD). Nickel, Tresp, and Kriegel applied tensor factorization to collective entity resolution. Some studies (Dunlavy,
Kolda, and Acar 2011; Ermiş, Acar, and Cemgil 2012;
Spiegel et al. 2012) proposed tensor-based models to solve
the problem of link prediction by incorporating time information. Moreover, tensor factorization is applied to
personalized recommendation, including personalized web
search (Sun et al. 2005), hyperlink recommendation (Gao
et al. 2012) and personalized tag recommendation (Rendle
and Schmidt-Thieme 2010; Symeonidis, Nanopoulos, and
Manolopoulos 2008; 2010; Rendle et al. 2009; Chi and Zhu
2010). With respect to personalized tag recommendation,
Higher Order Singular Vector Decomposition (HOSVD)
(Symeonidis, Nanopoulos, and Manolopoulos 2008) and
RTF (Rendle et al. 2009) are based on Tucker Decomposition (TD), while PITF (Rendle and Schmidt-Thieme 2010)
is based on Canonical Decomposition (CD). HOSVD can
not deal with missing values and it fills the unknown entries
with 0. RTF handles the missing values by adding pairwise
ranking constraint. Although TD-based methods outperform
other state-of-the-art tag recommendation approaches, they
require a lot of computation power. Therefore, they are not
feasible for online recommendation. Compared with TDbased methods, CD-based methods have a huge advantage
in running time, because they can be trained in linear time.
PITF, an extension of CD, splits the ternary relation of users,
items and tags into two relations, user-tag and item-tag and
its quality is comparable to RTF.

Ai,j,k =

R
X

Xi,r · Yj,r · Zk,r ,

(2)

r=1

where X ∈ RI×R , Y ∈ RJ×R , Z ∈ RK×R and R is the
number of latent features.
X, Y, Z in Eqs. (1) and (2) are in analogy with the factor
matrices in Singular Value Decomposition (SVD) and values
in core tensor C is in analogy with eigenvalues. Besides, if
we compare the running time of TD in Eq. (1) and CD in
Eq. (2), we can find that CD is linear in feature dimensionality but the time complexity of TD is much higher.

Personalized Tag Recommendation
Tagging is a process that a user associates an item with its
keywords, which are called tags. We use the similar notation
of (Rendle et al. 2009; Rendle and Schmidt-Thieme 2010)
for the formulation of personalized tag recommendation. Let
U be the set of users, I be the set of items, and T be the
set of tags. The process that user u annotate item i with tag
t is symbolized by a triple (u, i, t). Let S ∈ U × I × T
denote the set of tagging history. If a triple (u, i, t) ∈ S,
it means a particular user u has annotated a specific item i
with tag t. The relation between the users, items and tags
for personalized tag recommendation can represented by a
three-order tensor, which is depicted in Figure 1.
Rendle et al. introduced post-based ranking interpretation
scheme for personalized tag recommendation and we employ the schema in this paper. Let PS represent the set of all
of the user-item pairs (u, i) in S and it can be written as

Notations and Preliminaries
Tensor Factorization
A tensor is a multidimensional or multi-way array and the
order of a tensor is the number of the dimensions.
Tucker Decomposition (TD) (Tucker 1966) and Canonical Decomposition (CD) (Carroll and Chang 1970; Harsh-

PS = {(u, i)|∃(u, i, t) ∈ S}.

440

i obeys Gaussian distribution. The smaller the distance is,
the more relevant are item i and tag t. It can be written as

Post-based schema generates positive examples and negative
examples only from the observed posts (u, i) ∈ PS . Given
a post (u, i) ∈ PS , all of the triples (u, i, t) ∈ S are taken
as positive examples and the triples (u, i, t) ∈
/ S are taken
as negative examples. If (u, i) ∈
/ PS , we do not know that
user’s preference for that item. Thus, all the entries for the
unobserved posts are taken as missing values. We formulate
the relevance of (u, i, t) with f (u, i, t). Given a post (u, i),
if user u has annotated item i with tag tpos rather than tag
tneg , it implies that user u prefers tag tpos to tag tneg for that
post. In other words, f (u, i, tpos ) > f (u, i, tneg ), iff triple
(u, i, tpos ) ∈ S and triple (u, i, tneg ) ∈
/ S. Thus, in this
paper, we use notation tpos for “positive tags”, and notation
tneg for “negative tags”. As a result, the pairwise ranking
constraints DS from S is defined as
DS = {(u, i, t

pos

neg

,t

)|(u, i, t

pos

) ∈ S ∧ (u, i, t

neg

||Ii,k − Tt,k || ∼ N (0, σ 2 ),
where σ is the standard deviation of the Gaussian distribution. The situations of user-tag and user-item relations are in
analogy to that of the item-tag relation. However, the distributions of the latent topics of user-item, user-tag and itemtag relationships might be different. Thus, we define two sets
of the features for the users, items and tags, respectively, and
we have
2
||UIu,k − IU
i,k || ∼ N (0, σ ),
2
||UTu,k − TU
t,k || ∼ N (0, σ ),

and ||ITi,k − TIt,k || ∼ N (0, σ 2 ),

)∈
/ S}.

where UT , UI ∈ R|U |×R , IU , IT ∈ R|I|×R , TU , and TI ∈
R|T |×R .
With the assumptions made, given a particular triple
(u, i, t), we define its relevant score f (u, i, t) as
X
1
2
f (u, i, t) =
ζkU I · exp(− β||UIu,k − IU
i,k || )
2
k
X
1
2
+
ζkU T · exp(− β||UTu,k − TU
t,k || )
2
k
X
1
+
ζkIT · exp(− β||ITi,k − TIt,k ||2 ), (3)
2

The elements in DS are used as training examples for tag
recommendation. Figure 1 shows the tensor construction
and a toy example of post-based ranking interpretation for
personalized tag recommendation. User u has labeled item
4 with tag 1 and tag 3. Thus, we assume user u prefers tag
1 and tag 3 to tag 2 and tag 4. Besides, since user u has not
labeled item 1 and item 4, the values of all the entries in the
first row and the third row are missing.

Nonlinear Tensor Factorization using
Gaussian Kernal for Tag Recommendation

k

Nonlinear Tensor Factorization using Gaussian

2

where β = 1/σ . ζkU I is the weight or the importance of the
k-th topic for relation user-item. Similarly, ζkU T and ζkIT are
the importance of the k-th topic for relations user-tag and
item-tag, respectively.
In fact, the NLTF model can be represented by a neural
network with a specific activation function and a three-layer
networks structure. The framework of the neural network
is depicted in Figure 2. The neural network contains three
layers, input layer, hidden layer and output layer. The input layer of the neural network is composed of six blocks
and the hidden layer is composed of three blocks. The first
two block of the input layer and the first block of hidden
layer corresponds to relation user-item. The representation
of relations user-tag and item-tag are in analogy with relation user-item. For the input representation, one-of-n encoding schema is used. Given a triple (u, i, t), the u-th unit in
the first block is set to be 1 and all the other units in the first
block are set to be 0. The settings for the other input blocks
are similar. For the hidden layer, we use exp(− 21 β||p − q||2 )
as the activation function, where p and q are the weights of
the nonzero input units. Take relation user-item for example.
p and q correspond to UIu,k and IU
i,k in Eq. (3), respectively.
ζkU I in Eq. (3) corresponds to the weight of the k-th unit in
the first block of the hidden layer.
For a single training example (u, i, tpos , tneg ) ∈ DS , we
define the objective function for personalized tag recommendation with respect to that single example as follows.

In this paper, we introduce a nonlinear way for tensor decomposition, and we refer to it as Nonlinear Tensor Factorization (NLTF). The ternary relation between users, items
and tags is represented by a three-order tensor with |U |
users, |I| items and |T | tags. The tensor can be decomposed
into three components, a user matrix U ∈ R|U |×R , an item
matrix I ∈ R|I|×R and a tag matrix T ∈ R|T |×R , where R
is the number of features. The entries in the i-th row of matrix U, matrix I and matrix T indicate the latent features of
i-th user, i-th item and i-th tag, respectively. Each column of
matrices U, I and T can be considered as a latent topic.
Analogous to the previous work (Rendle and SchmidtThieme 2010), we model three pairwise relations user-item,
user-tag and item-tag rather than directly model ternary relation use-item-tag. As we have mentioned, an entry in a
feature vector stands for a latent topic. Without loss of generality, we take relation item-tag for example. We first make
an assumption that the latent topics are independent to each
other. Fox example, if the items are movies, the first latent
topic might be comedy and the second latent topic might be
action. If tag t is intimately related to item i with respect
to the first topic, the value of the first latent feature of tag t
should be close to the value of the first latent feature of item
i. Taking consideration of it, we assume if item i and tag t
are relevant with the k-th topic, then Tt,k obeys the Gaussian distribution N (Ii,k , σ 2 ) and vice versa, where Tt,k represents the k-th feature of tag t and Ii,k represents the k-th
feature of item i. In other words, the distance between the
value of the k-th feature of tag t and the k-th feature of item

J(u, i, tpos , tneg ) = sigmoid(f (u, i, tpos ) − f (u, i, tneg )),
(4)

441

RXWSXWXQLW

KLGGHQXQLWV





XVHU




LWHP

XVHU

WDJ

 LQSXWXQLWV


LWHP

WDJ

Figure 2: Neural network representation for NLTF
1
where sigmoid(x) = 1+exp(−x)
. It is clear that the sigmoid function, sigmoid(x), is a monotonically increasing
function. If a particular user u has annotated item i with tag
tpos , but not tag tneg , it indicates that he prefers tag tpos to
tag tneg . Thus, the difference of the relevant scores between
(u, i, tpos ) and (u, i, tneg ) should be large and we use sigmoid function to model it.
Given a training set DS , the overall objective function is
defined as
X
J(u, i, tpos , tneg ).
(5)
J(DS ) =

With the gradients, we can update the parameters of our
model. We first initialize all of the parameters and then learn
the parameters until convergence.

Comparison with Other Tensor Factorization
Models
HOSVD (Karatzoglou et al. 2010), RTF (Rendle et al. 2009)
and PITF (Rendle and Schmidt-Thieme 2010) are models
based on linear tensor factorization. In particular, HOSVD
and RTF are TD-based and PITF is CD-based. Our proposed
model NLTF can be seen as a nonlinear extension of the CDbased models. We use Gaussian radial basis function rather
than employing dot product used in PITF.
Let’s assume the number of features of users, items and
tags are the same for TD-based models, which is R. If we
exploit stochastic gradient descent (SGD) to learn the parameters for all models, given a training example, TD-based
models take O(R3 ) to update the parameters, while CDbased models take O(R), where R is the number of features.
For prediction, given a post (user, item), TD-based models
take O(|T | · R + R3 ) to rank the tags, while CD-based models take O(|T |·R). Therefore, CD-based models outperform
TD-based models in terms of the running time and are more
feasible for online tag recommendation.
On the other hand, our model that exploits Gaussian radial basis function to calculate the similarity is better than
models exploiting dot product. Models employing dot product can be viewed as two-class classifiers, while models
employing Gaussian radial basis function can be seen as
multi-class classifiers. For example, we want to provide a
set of tags to user u who has just watched movie i. Let’s
assume there is only one latent feature Ii,1 . For dot product, if the value of Ii,1 is positive, it may indicate the
movie is a comedy; otherwise, the movie is not. Thus, if
the value of Ii,1 is positive and tag t is related to comedies,
the value of Tt,1 should be positive so as to make the dot
product of Ii,1 and Tt,1 large, so that tag t can rank high
given post (u, i). For Gaussian-based models, a toy example is depicted in Figure 3. The horizontal axis represents
the value of Ii,1 .If Ii,1 ∈ (−3, 3), the movie is probably
an action; if Ii,1 ∈ (−9, −5), the movie is probably an romantic; if Ii,1 ∈ S
(5, 9), the movie is probably an drama; if
Ii,1 ∈ (−∞, 20) (20, ∞), it does not belong to any of the
categories in Figure 3. For instance, tag t is “comedy” and
movie i1 and movie i2 are annotated with tag t. As we assume, if an item and a tag are related, the distance between
the feature of that item and the feature of that tag obeys

(u,i,tpos ,tneg )∈DS

The parameters estimation for the tag recommendation task
can be formulated as the optimization problem of maximizing J(DS ).
Note that, given an example (u, i, tpos , tneg ), the first
component in f (u, i, t) is eliminated when calculate
f (u, i, tpos ) − f (u, i, tneg ) in Eq. (4). Thus, we drop it from
f (u, i, t). Besides, when maximizing the objective function,
the scale of ζkU I , ζkU T and ζkIT will get large, which leads to
overfitting. To address this issue, we simply set the values of
ζkU I , ζkU T and ζkIT to 1. Therefore, given a triple (u, i, t), the
relevant score f (u, i, t) is simplified as
X
1
2
f (u, i, t) =
exp(− β||UTu,k − TU
t,k || )
2
k
X
1
(6)
+
exp(− β||ITi,k − TIt,k ||2 ).
2
k

Parameters Learning
We use stochastic gradient descent, a widely used technique,
to optimize the objective function J(DS ). We randomly select a training example (u, i, tpos , tneg ) from DS and perform an update for that example. This process repeats until
convergence. The gradients for the nonlinear tensor factorization model are
∂fu,i,t
1
= −β · exp(− β||UTu,k − TIt,k ||2 )(UTu,k − TU
t,k ),
2
∂UTu,k
∂fu,i,t
1
= −β · exp(− β||ITi,k − TIt,k ||2 )(ITi,k − TIt,k ),
2
∂ITi,k
∂fu,i,t
1
= β · exp(− β||UTu,k − TIt,k ||2 )(UTu,k − TU
t,k ),
2
∂TU
t,k
and

1
∂fu,i,t
= β · exp(− β||ITi,k − TIt,k ||2 )(ITi,k − TIt,k ).
2
∂TIt,k

442

SUREDELOLW\

FRPHG\

URPDQFH

DFWLRQ

Table 2: Data statistics

VFLIL

GUDPD

Dataset
Delicioussmall
Last.fm
Movielens
Deliciouslarge

,L
-20

-15

-10

-5

0

5

10

15

20

Figure 3: A toy example of nonlinear tensor factorization
using Gaussian radial basis function

Users
1.8K

Items
35.9K

Tags
9.3K

Posts
66.7K

Triples
312.0K

1.6K
0.7K
1.5M

7.3K
2.6K
7.5M

2.3K
1.6K
2.6M

61.1K
17.8K
198.9M

164.5K
30.8K
675.0M

Besides, given item i and tag t, we define NIT (i, t) as

Gaussian distribution. That means the distance between the
feature of tag t and the feature of movie i1 is likely to be
small and the distance between the feature of tag t and the
feature of movie i2 is likely to be small as well. Thus, the
feature of i1 is likely to be close to the feature of i2 . In other
words, the features of the movies that belong to the same categories are likely to be close to each other and obey Gaussian
distribution. We can distinguish several categories by using
only one feature, because different categories fall into different part of the coordinate axis. In conclusion, Gaussianbased models can deal with more than two classes with only
one feature, but dot product can handle only two classes.
Thus, exploiting Gaussian function on radial basis distance
can make better use of the latent features than dot product.
The experimental results on real datasets in the next section
demonstrate this point.

NIT (i, t) = |{u|(u, i, t) ∈ S}|.
We compare our proposed model NLTF with several baselines, including PITF, SVD, Top-UT, Top-IT and TopUT+IT, where SVD only employs two-dimension relation
item-tag, Top-UT ranks tags based on NU T (u, t), Top-IT
ranks tags based on NIT (i, t) and Top-UT+IT ranks tags
based on NU T (u, t) + NIT (i, t). Here, Top-UT represents
the affinity between users and tags, Top-IT represents the tag
popularity and Top-UT+IT can be seen as a simple ensemble
of Top-UT and Top-IT. We do not compare NLTF with TDbased models like HOSVD and RTF, because the computation time is unacceptable in practice, especially for larger
dataset Delicious-large. Besides, the experimental results in
(Rendle and Schmidt-Thieme 2010) show that the performance of PITF is comparable to TD-based models. For each
dataset. we tuned the hyper-parameters of all models by grid
search to achieve the best results.

Experimental Results
Experimental Setup

Evaluation Methodology

We use four datasets to evaluate the performance of our proposed model, including Delicoius-small1 , Last.fm2 , Movielens3 and Delicious-large4 (Wetzker, Zimmermann, and
Bauckhage 2008). Three smaller datasets Delicious-small,
Last.fm and Movielens come from the 2-nd International
Workshop on Information Heterogeneity and Fusion in
Recommender Systems (HetRec 2011). The larger dataset
Delicious-large (Wetzker, Zimmermann, and Bauckhage
2008) contains the complete bookmarking activity for almost 2 million users from the launch of the social bookmarking website in 2003 to the end of March 2011. For each
dataset, we removed the infrequent users, items occurred in
less than 5 posts and tags occurred in less than 5 triplets.
The statistics of datasets after removal are described in Table
2. To avoid randomness of the results, we perform 10-fold
cross-validation for the three smaller datasets, Delicioussmall, Last.fm and Movielens. For larger dataset Deliciouslarge, we randomly sample about 40 thousand posts for test
and the remaining data is used for training.
To evaluate our method, we introduce two baseline functions NU T (u, t) and NIT (i, t). Given user u and tag t, we
define NU T (u, t) as

We treat personalized tag recommendation as a information
retrieval problem and employ Normalized Discounted Cumulative Gain (NDCG) as our metrics to evaluate the performance of the models. NDCG is widely used in information
retrieval and it makes the assumption that relevant tags are
more valuable if it appears earlier in the recommendation
list.
Discounted Cumulative Gain (DCG) evaluates the gain of
a tag based on its position in the ranked sequence returned
by the model. The DCG accumulated at a particular position
p is defined as
Xp
rel(k)
DCG@p = rel(1) +
,
(7)
k=1 log2 (k)
where rel(k) is 1 if the tag at position k is relevant or 0 otherwise. In order to normalize the metric, Ideal Discounted
Cumulative Gain (IDCG) is introduced, which is the DCG
of the ideal ranked sequence. Normalized Discounted Cumulative Gain (NDCG) is defined as
N DCG@p =

NU T (u, t) = |{i|(u, i, t) ∈ S}|.

DCG@p
.
IDCG@p

(8)

Performance on Smaller Datasets

1

Comparisons with Baselines Table 3 shows the performance of different methods on three smaller datasets
Delicious-small, Last.fm and Movielens. For NLTF and
PITF, the number of features R = 64, and for SVD, R =

http://www.delicious.com
http://www.lastfm.com
3
http://www.grouplens.org
4
http://www.zubiaga.org/resources/socialbm0311
2

443

-40

-30

-20

-10

Oscal and AFI

movie categories

0
movie content

10
adjec!ve

20

30

others

Figure 4: Tag distribution on dataset Movielens with the number of features R = 1
Table 1: Description of tags on dataset Movielens with the number of features R = 1
Oscar and AFI
afi 100
oscar (best directing)
oscar (best cinematography)
oscar (best supporting actor)
afi 100 (thrills)
oscar (best actor)

Movie Categories
scifi
fantasy
comedy
action
anime
classic

Movie Content
war
world war ii
surreal
serial killer
time travel
true story

256 because NLTF and PITF saturate at 64 features and SVD
saturates at 256 features. It can be seen from Table 3 that
NLTF outperforms other competitive methods on Delicioussmall and Last.fm, but PITF performs a little bit better than
our method on Movielens. Since the data in Delicious-small
and Last.fm is relatively sparser than the data in Movielens,
it should be the case that our method has some advantages
when the data is sparser. Moreover, our method beats TopUT+IT on all the datasets, which integrates tag popularity
and affinity between users and tags, but PITF works worse
than Top-UT-IT on Delicious-small.

Delicious-small
0.2518
0.2718
0.1089
0.2416
0.1110
0.2607

Last.fm
0.6715
0.6986
0.3633
0.3811
0.4683
0.4588

Others
erlends dvds
my movies
johnny depp
boring
based on a book
franchise

Table 4: NDCG@5 for different number of features on three
smaller datasets
feature
PITF
NLTF
PITF
NLTF
Movielens PITF
NLTF
Delicious
-small
Last.fm

4
0.068
0.075
0.369
0.445
0.321
0.479

8
0.100
0.126
0.502
0.562
0.468
0.564

16
0.157
0.188
0.601
0.652
0.576
0.606

32
0.231
0.247
0.654
0.692
0.629
0.624

64
0.252
0.272
0.672
0.699
0.647
0.631

serve whether the tag distribution is corresponding to the toy
example in Figure 3. In order to get better observation, the
unpopular tags are removed. The tag distribution on dataset
Movielens with the number of features R = 1 is depicted in
Figure 4 and the description of which is showed in Table 1.
As we can see, NLTF divides the tags into four groups automatically, including Oscar and AFI, movie categories, movie
content and adjective. Tags in the same group are close to
each other and some of their labels overlapped. Tags in others do not belong to any groups, thus they might do little
help to personalized tag recommendation.

Table 3: NDCG@5 of different methods on three smaller
datasets
PITF-64
NLTF-64
SVD-256
Top-UT
Top-IT
Top-UT+IT

Adjective
atmospheric
disturbing
deliberate
quirky
reflective
visceral

Movielens
0.6466
0.6309
0.1673
0.4082
0.2851
0.4909

Performance on Large-Scale Dataset

Impact of Number of Features The performances of the
NLTF and PITF depend on the number of features. Table 4
shows the impact of the number of features on the three
datasets Delicious-small, Last.fm and Movielens. As expected, the performances of NLTF and PLTF increase when
we increase the number of features. Their performances saturate at 64 features on all three datasets. Besides, the performance of NLTF is much better than PITF when the number
of feature is small, because models exploiting Gaussian like
NLTF can make better use of the latent features than models
exploiting dot product like PITF, just as we analyze in Section and the effect is more significant when only a smaller
number of features are used.
Besides, in order to verify the point that using Gaussian
can make better use of features than using dot product, we
carry out another experiment. We train our proposed model
NLTF by using only one feature on Movielens, so as to ob-

In addition, we compare our method NLTF with PITF on
a large-scale dataset Delicious-large, which contains more
than 0.6 billion triples and about 0.2 billion posts. There is
no previous work has conducted experiments for personalized tag recommendation on such a large-scale dataset before to our best knowledge. We conduct experiments on a
computer with a 6-core intel i7 CPU and the performance
after 30 iterations is showed in Table 5. From the figure, we
can see that our method greatly outperforms PITF even on
large scale dataset.
Table 5: Comparison of NLTF and PITF on larger dataset
NLTF
PITF

444

NDCG@1
0.3143
0.2845

NDCG@3
0.3053
0.2758

NDCG@5
0.3214
0.2907

NDCG@10
0.3579
0.3247

Conclusion and Future Work

Karatzoglou, A.; Amatriain, X.; Baltrunas, L.; and Oliver,
N. 2010. Multiverse recommendation: n-dimensional tensor factorization for context-aware collaborative filtering. In
Proceedings of the fourth ACM conference on Recommender
systems, 79–86. ACM.
Krestel, R.; Fankhauser, P.; and Nejdl, W. 2009. Latent
dirichlet allocation for tag recommendation. In Proceedings
of the third ACM conference on Recommender systems, 61–
68. ACM.
Lipczak, M.; Hu, Y.; Kollet, Y.; and Milios, E. 2009. Tag
sources for recommendation in collaborative tagging systems. ECML PKDD discovery challenge 157–172.
Liu, D.; Hua, X.-S.; Yang, L.; Wang, M.; and Zhang, H.-J.
2009. Tag ranking. In Proceedings of the 18th international
conference on World wide web, 351–360. ACM.
Nickel, M.; Tresp, V.; and Kriegel, H.-P. 2011. A threeway model for collective learning on multi-relational data.
In Proceedings of the 28th international conference on machine learning (ICML-11), 809–816.
Rendle, S., and Schmidt-Thieme, L. 2010. Pairwise interaction tensor factorization for personalized tag recommendation. In Proceedings of the third ACM international conference on Web search and data mining, 81–90. ACM.
Rendle, S.; Balby Marinho, L.; Nanopoulos, A.; and
Schmidt-Thieme, L. 2009. Learning optimal ranking with
tensor factorization for tag recommendation. In Proceedings of the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining, 727–736. ACM.
Spiegel, S.; Clausen, J.; Albayrak, S.; and Kunegis, J. 2012.
Link prediction on evolving data using tensor factorization.
In New Frontiers in Applied Data Mining. Springer. 100–
110.
Sun, J.-T.; Zeng, H.-J.; Liu, H.; Lu, Y.; and Chen, Z. 2005.
Cubesvd: a novel approach to personalized web search. In
Proceedings of the 14th international conference on World
Wide Web, 382–390. ACM.
Symeonidis, P.; Nanopoulos, A.; and Manolopoulos, Y.
2008. Tag recommendations based on tensor dimensionality
reduction. In Proceedings of the 2008 ACM conference on
Recommender systems, 43–50. ACM.
Symeonidis, P.; Nanopoulos, A.; and Manolopoulos, Y.
2010. A unified framework for providing recommendations
in social tagging systems based on ternary semantic analysis. Knowledge and Data Engineering, IEEE Transactions
on 22(2):179–192.
Tucker, L. R. 1966. Some mathematical notes on threemode factor analysis. Psychometrika 31:279–311.
Wetzker, R.; Zimmermann, C.; and Bauckhage, C. 2008.
Analyzing social bookmarking systems: A del.icio.us cookbook. In Mining Social Data (MSoDa) Workshop Proceedings, 26–30.

We introduce a nonlinear tensor factorization method based
on Canonical Decomposition in this paper. Our proposed
method exploits Gaussian radial basis function and can make
better use of features than dot product. The models exploiting Gaussian can be considered as a multi-class classifiers,
while the models exploiting dot product can be considered as
a two-class classifiers. Additionally, our experimental results
show that our method outperforms the baseline methods and
can achieve very good performance with only a small number of features.
For future work, we plan to leverage the profile of users,
the content of the items and tags to make better predictions
for tag recommendation.

Acknowledgements
We would like to thank the many referees of the previous
version of this paper for their extremely useful suggestions
and comments. This work was supported by Huawei Innovation Research Program (HIRP) and National Science Foundation of China (61033010).

References
Carroll, J., and Chang, J. 1970. Analysis of individual differences in multidimensional scaling via an n-way generalization of “eckart-young” decomposition. Psychometrika
35(3):283–319.
Chi, Y., and Zhu, S. 2010. Facetcube: a framework of incorporating prior knowledge into non-negative tensor factorization. In Proceedings of the 19th ACM international conference on Information and knowledge management, 569–578.
ACM.
Dunlavy, D. M.; Kolda, T. G.; and Acar, E. 2011. Temporal
link prediction using matrix and tensor factorizations. ACM
Transactions on Knowledge Discovery from Data (TKDD)
5(2):10.
Durao, F., and Dolog, P. 2010. Extending a hybrid tag-based
recommender system with personalization. In Proceedings
of the 2010 ACM Symposium on Applied Computing, 1723–
1727. ACM.
Ermiş, B.; Acar, E.; and Cemgil, A. T. 2012. Link prediction
via generalized coupled tensor factorisation. arXiv preprint
arXiv:1208.6231.
Gao, D.; Zhang, R.; Li, W.; and Hou, Y. 2012. Twitter
hyperlink recommendation with user-tweet-hyperlink threeway clustering. In Proceedings of the 21st ACM international conference on Information and knowledge management, 2535–2538. ACM.
Guan, Z.; Bu, J.; Mei, Q.; Chen, C.; and Wang, C. 2009.
Personalized tag recommendation using graph-based ranking on multi-type interrelated objects. In Proceedings of the
32nd international ACM SIGIR conference on Research and
development in information retrieval, 540–547. ACM.
Harshman, R. A. 1970. Foundations of the PARAFAC procedure: Models and conditions for an “explanatory” multimodal factor analysis. UCLA working papers in phonetics
16:1.

445

Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)

Recurrent Attentional Topic Model
Shuangyin Li ∗

Yu Zhang

Department of Computer Science and Engineering Department of Computer Science and Engineering
Hong Kong University of Science and Technology, China
Hong Kong University of
shuangyinli@cse.ust.hk
Science and Technology, China
zhangyu@cse.ust.hk

Rong Pan

Mingzhi Mao

School of Data and Computer Science School of Data and Computer Science
Sun Yat-sen University, China
Sun Yat-sen University, China
panr@sysu.edu.cn
mcsmmz@mail.sysu.edu.cn

iPIN
Shenzhen, China
yangyang@ipin.com

language modeling (Sutskever, Martens, and Hinton 2011;
Frinken et al. 2012). Moreover, some works consider the
syntactic structure of sentences over words to model the document (Boyd-Graber and Blei 2009).
Although topic models have been widely used for document modeling, the topic coherence between sentences,
which does exist in natural language, is ignored in existing
works. To see this, let us consider the following four sentences describing “Machine Learning” from the Wikipedia:
(1) Machine learning is closely related to and often overlaps with computational statistics, a discipline which also
focuses in prediction-making through the use of computers.
(2) It has strong ties to mathematical optimization, which delivers methods, theory and application domains to the ﬁeld.
(3) Machine learning is employed in a range of computing
tasks where designing and programming explicit algorithms
is infeasible. (4) Example applications include spam ﬁltering, optical character recognition, search engines and computer vision. Obviously, sentence (4) is about the applications of machine learning, whose topics are highly coherent with those of the three preceding sentences and so the
topics in a sentence could recurrently aﬀect the following
sentences. Moreover, the topics of sentence (4) are more
relevant to those of sentences (2) and (3) since they all discuss the applications of machine learning. Thus, besides
the topic relevance among sentences, it is intuitive that a
sentence is related to the preceding sentences with diﬀerent
weights, which are called attention signals just like the attentional mechanism in deep neural networks (Mnih et al. 2014;
Bahdanau, Cho, and Bengio 2014; Gregor et al. 2015).
To the best of our knowledge, there is no work to consider
sentence coherence and attention signals in Bayesian modeling. To ﬁll this gap, we develop a Recurrent Attentional
Topic Model (RATM). Based on a proposed Recurrent Attentional Bayesian Process (RABP), the RATM can model
sequences of sentences by considering the dependency between sentences as well as attention signals.
Speciﬁcally, the contributions of this work are follows.
Firstly, We propose a novel RABP to handle sequential

Abstract
In a document, the topic distribution of a sentence depends
on both the topics of preceding sentences and its own content, and it is usually aﬀected by the topics of the preceding
sentences with diﬀerent weights. It is natural that a document can be treated as a sequence of sentences. Most existing works for Bayesian document modeling do not take these
points into consideration. To ﬁll this gap, we propose a Recurrent Attentional Topic Model (RATM) for document embedding. The RATM not only takes advantage of the sequential orders among sentence but also use the attention mechanism to model the relations among successive sentences. In
RATM, we propose a Recurrent Attentional Bayesian Process
(RABP) to handle the sequences. Based on the RABP, RATM
fully utilizes the sequential information of the sentences in a
document. Experiments on two copora show that our model
outperforms state-of-the-art methods on document modeling
and classiﬁcation.

1

Yang Yang

Introduction

Probabilistic topic models provide a suite of algorithms to
obtain good representations when facing a collection of documents. The representation obtained by a topic model often corresponds to latent topics in an interpretable space,
which is an advantage over other models. Topic models have improved document classiﬁcation and information
retrieval (Wei and Croft 2006) on unstructured text, and
many extended models have been applied to many structured text data and non-text data in computer vision (Fei-Fei
and Perona 2005) and collaborative ﬁltering (Marlin 2003).
Topic models usually assume that words are interchangeable, which is helpful for eﬃcient inference on large corpora (Blei 2012). Actually, documents are sequences of
words, sentences, and paragraphs in a hierarchical manner and some works have modeled a document as a sequence of words, including the n-gram language modeling (Brown et al. 1992) and recurrent neural networks for
∗

Part of this work was performed at iPIN.
c 2017, Association for the Advancement of Artiﬁcial
Copyright 
Intelligence (www.aaai.org). All rights reserved.

3223

data and to allow a local recurrent information transmission
through a sequence. Secondly, We establish a previously
unexplored connection between recurrent Bayesian methods and dynamic attention signals in the principled RATM
model, where the attention signals are adaptively learned for
sequences of sentences, and we develop an eﬃcient variational inference algorithm. Lastly, a new topic model with
RABP is proposed, and the experiments show that RABP
can recover meaningful topics in the sequences of sentences.
Based on it, RATM has better performance in terms of perplexity and classiﬁcation accuracy.
The rest of the paper is organized as follows. In Section 2,
we discuss related works. In Section 3, we propose the
RABP mathematically. In Section 4, we present the RATM
and its inference method. In Section 5, we present experimental results on two copora for document modeling and
classiﬁcation. Also, we show some case studies of attention
signals among the sentences.

2



π

θt−i

θt

1≤i<C

w
N

Figure 1: The recurrent attentional Bayesian process with
the bag-of-words assumption. The shaded circles denote observed words and others are the hidden variables.  denotes
the attention signal, π is a Dirichlet parameter, and C is the
length of time windows.

Related Works

Many probabilistic topic models have been proposed, including (Hofmann 1999; Blei, Ng, and Jordan 2003; Blei
and Laﬀerty 2005; Blei and McAuliﬀe 2007; Boyd-Graber
and Blei 2009; Hoﬀman, Blei, and Bach 2010; Li et al.
2015). These models and their extensions have been applied
to many tasks such as information retrieval (Wei and Croft
2006; Li, Li, and Pan 2013), document classiﬁcation (Cai et
al. 2008; Li et al. 2016), and so on. Some models, such as
the Latent Dirichlet Allocation (LDA) (Blei, Ng, and Jordan
2003) and Correlated Topic Model (CTM) (Blei and Lafferty 2005), are used to model unstructured documents with
assumptions that words in a document arise from a mixture
of latent topics and that each topic is a distribution over the
vocabulary. Many topic models such as (Griﬃths et al. 2004;
Gruber, Weiss, and Rosen-Zvi 2007) take the order of words
and the syntactic of sentences into consideration. In (Grifﬁths et al. 2004), authors focus on short-range syntactic dependencies and long-range semantic dependencies between
words. The HTMM proposed in (Gruber, Weiss, and RosenZvi 2007) models the topics of words in a document as
a Markov chain. The syntactic topic model proposed in
(Boyd-Graber and Blei 2009) generates words via both thematically and syntactically constraints among them. Almost
all the existing topic models consider the sequentiality of
documents on the word level only.
Recurrent Neural Networks (RNN) (Sutskever, Martens,
and Hinton 2011) provide an eﬃcient way to handle the
sequentiality of documents on both the word (Mikolov et
al. 2010; 2013) and the sentence levels, and they have
been applied to various tasks, including machine translation
(Bahdanau, Cho, and Bengio 2014), summarization (Rush,
Chopra, and Weston 2015), dialog system (Serban et al.
2016), document modeling (Lin et al. 2015) and so on.
Attention signals are widely applied to language modeling for many text mining tasks (Bahdanau, Cho, and Bengio 2014; Rush, Chopra, and Weston 2015; Ling et al. 2015;
Serban et al. 2016) and speech tasks (Chorowski et al. 2015).
However, all the proposed models with the attention mechanism are under the neural network framework and few

Bayesian models focus on language modeling with attention
signals.

3

Recurrent Attentional Bayesian Process

A Recurrent Attentional Bayesian Process (RABP), denoted
by RABP(G0 , π), is parameterized by a base measure G0 and
a concentration parameter π. The generative process for the
RABP is deﬁned as follows:
1. Draw θ1 from G0 .
2. For t > 1
(a) Draw  = (1 , . . . , C )T from Dir(π), where Dir(π) denotes a Dirichlet distribution with parameter π;
(b) With probability i , draw θt from δθt−i for i = 1, . . . , C −
1, where δa denotes a discrete distribution whose probability mass function is equal to 1 at the point a;
(c) With probability C , draw θt from G0 .
In this generative process, G0 is the base distribution and C
is the length of the time window. Here i for i = 1, . . . , C is
deﬁned as the attention signal and it captures the 
importance
of a preceding neighbour θt−i to θt .  satisﬁes Ci=1 i = 1
as it follows a Dirichlet distribution with parameter π. The
graphical model of RABP is shown in the right of Figure 1.
Based on the generative process, θt can be represented as
θt |θt−C+1:t−1 , G0 , π ∼

C−1


i · θt−i + C · G0 ,

(1)

i=1

where K is the length of each θi and θ j−C+1: j−1 is a (C −1)×K
matrix containing C − 1 preceding parameters. The attention
signal i reﬂects the importance of a preceding parameter in
the sequence to current one and C indicates that θt can have
dependency with the C − 1 preceding ones.
The RABP is partly related to the Recurrent Chinese
Restaurant Process (RCRP) (Ahmed and Xing 2008) and

3224

Dirichlet-Hawkes Process (DHP) (Du et al. 2015). The
RCRP, an extension of the Chinese restaurant process, deﬁnes a distribution over Dirichlet distribution. The main
diﬀerence between the RABP and RCRP is that the RABP
considers several preceding time points with dynamic attentional weights, while in the RCRP the dependency of the parameter over the preceding ones is invariant to both positions
and the content information. The DHP focuses on modeling
the intensity of discrete events using a Hawkes process but
the RABP can model recurrent sequences in a discrete space
with attention signals. The RABP considers the local reﬂection for the current state and can be used as a prior to
model documents as sentence-level sequences, which will
be shown in the next section.

4

α

π

ϑd

j

θ s j−i

θs j

1≤i<C

z

Recurrent Attentional Topic Model

As a collection of M documents, a corpus is deﬁned as
D = {d1 , . . . , d M }, where di , i ∈ {1, . . . , M} denotes the i-th
document. A document di is a sequence of S i sentences denoted by di = (si1 , . . . , siS i ), where sij , j ∈ {1, . . . , S i } denotes
the j-th sentence in di . Let sij = (wij,1 , . . . , wij,N i ) denotes the

β

w
N
S

j

vector of N ij words associated with sentence sij .
It is clear that the topic distribution of one sentence is related to those of previous sentences, which is called cohesion or coherence in linguistics. This observation matches
the motivation of the RABP and then based on the proposed RABP, we can model a document as a sequence of
sentences, leading to the proposed RATM.
By considering a document as a sequence of sentences,
the RATM attempts to capture the joint inﬂuences of previous sentences to current one. Moreover, the topics of a
sentence are also aﬀected by those of the host document that
the sentence belongs to. Hence, the topic distribution of a
sentence is generated from those of both its preceding sentences and the host document.
Let θ s j denote the topic distribution of the j-th sentence
s j in document d, which follows the RABP with parameters
G0 and π. G0 is deﬁned as a K-dimensional Dirichlet distribution for θ and hence θ denotes the topic distribution of a
sentence over K latent topics. Based on these notations, the
generation process of RATM is deﬁned as

Figure 2: The graphical model of the RATM. ϑd is the topic
distribution of a document. θ s j−i denotes the topic distribution of a preceding sentence where 1 ≤ i < C and C is the
length of time windows used in the RABP.
used in the RABP without explicitly introducing in the generative process and note that attention signals are dynamic
in diﬀerent sentences. zn is the topic assignment for each
word n and it describes the topic distribution of a word.
θ s j ∼ RABP(δϑd , π) indicates that the topic distribution of
current sentence s j is generated by a RABP, which means
that sentence s j depends on the C − 1 preceding sentences
via a vector of adaptive attention signals,  j , as described in
RABP. Figure 2 shows the graphical model of the RATM.

Inference
The main problem in the inference for the RATM is to
estimate the posterior distribution of latent variables conditioned on observed data. Typical topic models can be
learned by Gibbs sampling methods due to the conjugate
property between the topic assignment and the prior over the
topic distribution. However, the RATM does not enjoy such
conjugate property due to the prior for the topic distribution
of a sentence (see Eq. (1)), making the posterior distribution
in the RATM intractable to compute. Thus, we resort to the
variational inference.
In the variational inference, the posterior distribution is
approximated by a group of variational distributions with
free variational parameters and the group of variational distributions are enforced to be close to the true posterior. For
each sentence s j with N j words in document d, we use the
following fully factorized variational distribution:

1. For each topic k ∈ {1, . . . , K}, draw βk ∼ Dir(π), where π
is a V-dimensional prior vector of β;
2. For each document di , i ∈ {1, . . . , M}:
(a) Draw ϑd ∼ Dir (α);
(b) For sentence s j , j ∈ {1, . . . , S i } in the document d:
i. Draw θ s j ∼ RABP(δϑd , π);
ii. For each word wn , n ∈ {1, . . . , N j } in sentence s j :
A. Draw zn ∼Mult(θ s j );
B. Draw wn ∼Mult(βzn ).
In this generative process, ϑd , a K-dimensional vector following a Dirichlet distribution, describes the topic distribution of a document and it is used as G0 in the RABP for the
topics of sentences. A topic is a distribution over a ﬁxed
vocabulary which is denoted by βk . Attention signals are

q s ( j , z|ξ, γ) = q( j |ξ)

Nj

n=1

3225

q(zn |γn ),

where ξ is a variational parameter of a Dirichlet distribution
for sentence s j and {γn } is a variational parameter of a multinomial distribution. Thus, the Jensen’s lower bound on the
log probability of sentence s j can be computed as
L s j (β, π; ξ, γ) = Eq [log p( j |π)] +

Nj


where v is the index of wn in the dictionary.

Document Embedding
As a kind of topic models, the RATM is to extract the topic
distribution of each document for document embedding. In
the above variational inference framework, we can deﬁne
G0 = δϑd and update the topic distribution for a whole document, ϑd , which is the embedding of one document. Note
that we treat C as the attention signal for ϑd as described in
the RABP. Based on the above variational inference framework, we introduce
a new variational variable for the doci
ument di , ρd , which follows a Dirichlet distribution. Thus,
we can use the Jensen’s inequality to lower-bound the logprobability of a document di as:

Eq [log p(zn | j , θ j−C+1: j−1 )]

n=1

+

Nj


Eq [log p(wn |zn , β)] − Eq [log q( j )] − Eq [log q(z)],

n=1

where the G0 is ignored for the ease of presentation. Note that, even though it is diﬃcult to compute
Eq [log p(zn |, θ j−C+1: j−1 )], we can obtain its lower-bound by
following the method described in (Li et al. 2013). Then we
need to maximize the lower-bound L s j (β, π; ξ, γ) to ﬁnd the
estimations of the variational parameters and model parameters, which are detailed in the following sections.

i

s

C−1


C−1


(πl − 1)(Ψ(ξl ) − Ψ(

l =1

l=1

−

C−1


C−1


ξl )) − log Γ(

C−1


(ξl − 1)(Ψ(ξl ) − Ψ(

l=1

l=1

ξl )) +

l =1

ξl ) +

Nj K



γnk

n=1 k=1

C−1


C−1


log Γ(ξl )

j−C+1: j−1

ξl
C−1


l =1 ξl

l=1

,

where Ψ(·) is the digamma function, which is the ﬁrst derivative of the logarithm of the Gamma function. We use the
gradient descent method to estimate ξ.

i

ρdk = αk +

C−1


log θlj−C+1: j−1 

l=1

ξl
,
l ξl

i

(2)

l =1

l=1

i

Nj
Si 
M 


Nj
Si 


γnk ,

j=1 n=1

where γnk are the probability assignments for all the words
in each sentence, and it is same as the equation of variational
topic distribution in LDA. Thus, it is interesting to note that
RATM degenerates into LDA when attentional signals are
ignored.

5

Experiments

The proposed model is evaluated on two corpora. The ﬁrst
corpus is a subset of the Wikipedia. We extract abstracts of
each page in the Wikipedia and remove abstracts with less
than 5 sentences to form the corpus, which contains 241,290
documents. We remove stop words and obtain a vocabulary
of 21,968 words. Each document belongs to one of 68 categories such as education, book, arts, and so on, and the average number of sentences in this corpus is about 7. The second corpus we used is news articles from New York Times
(NYTimes) from January 1st, 2016 to May 8th, 2016. After removing news which contain less than 5 sentences, we

l =1

We can invoke the linear-time Newton-Raphson algorithm
described in the LDA to estimate π.
For β, we set the derivative of the variational lower-bound
with respect to βkv to 0, leading to the following solution:
βkv =

i

ρdk = αk +

Si
M 
C−1
C−1
C−1
C−1





(log Γ( ξl ) −
log Γ(ξl ) +
(ξl − 1)(Ψ(ξl ) − Ψ( ξl ))).
l=1

(3)

Discussion When we let C = 1, Eq (1) will be
θt |θt−C+1:t−1 , G0 , π ∼ C · G0 where C = 1, which means that
the topic distribution of current θt follows the base distribution G0 . Thus, with G0 = δϑd and C = 1 in RATM, the
i
variational parameter ρdk will be

Parameter Estimation The model parameters include π
and β. Based on L s j (β, π; ξ, γ), the objective function for π
to be maximized over the whole corpus is formulated as
i=1 j=1

ξC
γnk · C .
l ξl
n=1

With ρdk , we can obtain ϑd as normalized {ρdk }, i.e., [ϑd ]k =
i 
i
ρdk / k ρdk , according to (Blei, Ng, and Jordan 2003).

where vwn denotes the index of word wn in the dictionary.
The traditional topic models based on the bag-of-words
assumption would stumble when the document is too short,
which has been discussed in (Tang et al. 2014). The proposed model is capable of handling short documents because it fully utilizes the topic information from the preceding sentences (see the summand in Eq. (2)) and adaptive attention signals to generate the topic distribution for current
sentence.

L[π] =

Nj
Si 

j=1

Variational Update for Word Assignment For each
word wn in sentence s j , a topic index zn is assigned to wn and
γnk is the variational parameter corresponding to the probability that the topic k is assigned to the word wn . The variational update for γnk can easily be obtained as
γnk ∝ βk,vwn exp

i

L s j (β, π; ξ, γ) + Eq [log p(ϑd |α)] − Eq [log q(ρd )],

j

l=1

log θl

Si


where α is initialized by LDA and then ﬁxed as LDA did.
We use an alternating optimization to solve the above obi
jective function. When ρd and ϑd are ﬁxed, the variational and model parameters for diﬀerent sentences in the
document are independent and we can follow the variational approach described in the previous sections to update {β, π, ξ, γ}. When {β, π, ξ, γ} are ﬁxed, we maximize
i
i
i
Ld (β, π, ϑd ; ξ, γ, ρd ) with respect to ϑd and ρd . By setting
di
the derivative with respect to ρ to 0, we can obtain an analytical solution as

Variational Update for Attention Signals Based on
L s j (β, π; ξ, γ), for the variational parameters ξ corresponding to the attention signals of sentence s j , the objective is to
maximize the following equation:
L[ξ]j =

i

Ld (β, π, ϑd ; ξ, γ, ρd ) =

γnk · wvn ,

i=1 j=1 n=1

3226

obtain 27,523 articles, each of which belong to one of 42
categories such as world, movies, sports, magazine and so
on. After removing stop words, we obtain a dictionary with
12,047 words and the average number of sentences in the
NYTimes corpus is about 40.

Models
LDA
CTM
HDP
RSM
RATM-N
RATM-D

Results
The baseline methods include the LDA, CTM, Hierarchical
Dirichlet processes (HDP) (Teh et al. 2012), and Replicated
Softmax Model (RSM) (Hinton and Salakhutdinov 2009).
For the proposed RATM model, we trained two variants under diﬀerent settings. A RATM model called the RATM-N is
trained without using G0 for the generation of each sentence
and hence it does not update the ϑd for each document as
i
well as the responding variational parameters ρd . Another
RATM model called the RATM-D just uses the inference described in Section 4. In the RATM-N and RATM-D, the ﬁrst
C − 1 sentences in a document do not have C − 1 preceding
sentences, which bring diﬃculties to the use of the attention
signals in the RABP. In this case, we just use the topic distribution of the host document, ϑd , as the topics for the unused
attention signals. For the Wikipedia corpus, C is set to 4,
and it is 6 in the NYTimes corpus.
To compare the performance of diﬀerent models, we use
the held-out perplexity as a measure, which is deﬁned for
the RATM as
 
perplexity(Dtest ) = exp(−

T=50
585.08
524.11
728.03
752.36
553.87
532.72

Wikipedia
T=100
493.73
435.67
728.03
750.08
402.47
392.05

# on Wikipedia / T=100
RATM-D

C=2
424.56

T=200
402.98
440.89
728.03
767.08
328.6
314.47
C=3
396.68

T=50
794.81
730.60
1582.67
1259.10
576.06
529.08
C=4
392.05

NYTimes
T=100
768.24
645.41
1582.67
1251.71
493.44
442.65

T=200
748.45
736.38
1582.67
1266.5
500.07
440.68

C=5
398.51

C=6
430.15

Table 1: The top table shows the perplexity of diﬀerent models on the two corpora with T = 50, 100, 200. The table
below shows the perplexity of diﬀerent C of RATM-D on
Wikipedia with T =100.
The attention signals indicate the importance of the preceding sentences to current one in a document. We train the
RATM-D model on the Wikipedia corpus and set C to be 4
and 5. We do not use the topic distribution of the host document as G0 since we just want to show the local relations
among sentences and hence we manually set all the probabilities to sample from G0 in step 2(c) of the RABP to be 0.
Thus, we can show the values of attention signals with 3 and
4 preceding sentences for each current sentence.
Table 3 shows the values of attention signals for some
documents in the Wikipedia corpus and the numbers in red
are the values of attention signals of the preceding sentences
for the last sentence which is in italic. From the results, we
can see that, in some cases, the values of attention signals
increase for the closer sentences, for example, in the ”Machine learning” case. While, in other cases, the values of
attention signals could be related to the similarities of topics
between current sentence and the preceding ones.
To examine the robustness of the RATM-D model based
on the attention signals, we randomly selected two sentences
and inserted them into the document “Artiﬁcial intelligence”
of the Wikipedia corpus as follows:

Si
i
j=1 log p(s j )
 M S i i ),
i=1
j=1 N j

M
i=1

 i i
where the test set has M documents and Sj=1
N j is the total
number of words in document di . The lower the perplexity
is, the better the performance is.
In each corpus, 80% documents are used for training and
the rest is for testing. That is, for the Wikipedia corpus, there
are 20,000 documents for training and 4,000 documents for
testing. For the NYTimes corpus, 22,000 documents are
used for training and 5,523 documents for testing.
To see the eﬀect of the number of latent topics, Table 1
shows the held-out perplexities of diﬀerent methods on the
Wikipedia and NYTimes corpora when the number of latent topics takes three values, i.e., 50, 100 and 200. The
results show that the RATM-N and RATM-D have much
better performance than baseline models. The performance
of the RATM-D is better than that of the RATM-N, which
demonstrates that the incorporation of the topic modeling of
the whole document can bring beneﬁts for performance improvement. Moreover, when the number of topics increases,
the performance of the RATM-N and RATM-D tends to
become better due to the increasing capacity of the models. Besides, we compare diﬀerent C on RATM-D based
on the perplexity. We set C = 2, 3, 4, 5, 6 with T = 50 on
Wikipedia. Note that, when C = 1, the RATM-D is equivalent to the LDA. As shown in Table 1, we ﬁnd that RATM-D
reaches the best result when C = 4, and then becomes worse
due to the overﬁtting when C is increasing.

(1) The central problems or goals of AI research include reasoning knowledge planning learning natural language processing communication perception and the ability to move
and manipulate objects. (2) Hype and glory is memoir from
William Goldman which details his experiences as judge at
the Cannes Film festival and miss America pageant. (3) The
book includes an interview with Clint Eastwood and proﬁle
on Robert Redford. (4) There are large number of tools used
in AI including versions of search and mathematical optimization logic methods based on probability and economics
and many others. (5) The AI ﬁeld is interdisciplinary in which
#
Sentence (4)
Sentence (5)
Sentence (6)

Analysis on Attention Signals

Values of attention signals
(1) 0.73 (2) 0.15 (3) 0.12
(2) 0.03 (3) 0.01 (4) 0.96
(3) 0.09 (4) 0.48 (5) 0.43

Table 2: The values of attention signals for each sentence
in a row. The numbers in red are the values of the attention
signals of noisy sentences.

In the section, we show the eﬀect of the attention signals
used in the RATM model.

3227

(0.591437) The central problems (or goals) of AI research include reasoning, knowledge, planning, learning, natural language processing (communication),
perception and the ability to move and manipulate objects. (0.219417) General intelligence is still among the ﬁeld’s long-term goals. (0.189146) Currently
popular approaches include statistical methods, computational intelligence and traditional symbolic AI. There are a large number of tools used in AI,
including versions of search and mathematical optimization, logic, methods based on probability and economics, and many others.
(0.045228) Machine learning is closely related to and often overlaps with computational statistics; a discipline which also focuses in prediction-making
through the use of computers. (0.280551) It has strong ties to mathematical optimization, which delivers methods, theory and application domains to the ﬁeld.
(0.674221) Machine learning is employed in a range of computing tasks where designing and programming explicit algorithms is infeasible.
Example applications include spam ﬁltering, optical character recognition (OCR), search engines and computer vision.
(0.203636) They require empathy and the rule of law and impose an obligation on persons to respect the human rights of others. (0.216774) They should not
be taken away except as a result of due process based on speciﬁc circumstances; for example, human rights may include freedom from unlawful imprisonment,
torture, and execution. (0.050603) The doctrine of human rights has been highly inﬂuential within international law, global and regional institutions.
(0.528987) Actions by states and non-governmental organizations form a basis of public policy worldwide. The idea of human rights suggests that if the
public discourse of peacetime global society can be said to have a common moral language, it is that of human rights.
(0.313444) Rigorous arguments ﬁrst appeared in Greek mathematics, most notably in Euclid’s Elements. (0.336590) Since the pioneering work of Giuseppe
Peano, David Hilbert, and others on axiomatic systems in the late 19th century, it has become customary to view mathematical research as establishing truth
by rigorous deduction from appropriately chosen axioms and deﬁnitions. (0.102) Mathematics developed at a relatively slow pace until the Renaissance,
when mathematical innovations interacting with new scientiﬁc discoveries led to a rapid increase in the rate of mathematical discovery that has continued to
the present day.(0.247966) Galileo Galilei said, ”The universe cannot be read until we have learned the language and become familiar with the characters in
which it is written.It is written in mathematical language, and the letters are triangles, circles and other geometrical ﬁgures, without which means it is
humanly impossible to comprehend a single word.

Artiﬁcial
intelligence

Machine
learning

Human
rights

Mathematics

Table 3: The values of attention signals for sentences in some documents of the Wikipedia corpus. The numbers in red are the
values of attention signals of the preceding sentences for the last sentence which is in italic.
number of sciences and professions converge including computer science mathematics psychology linguistics philosophy
and neuroscience as well as other specialized ﬁelds such as
artiﬁcial psychology. (6) The ﬁeld was founded on the claim
that central property of human intelligence the sapience of
Homo sapiens can be so precisely described that machine can
be made to simulate it.

0.6

0.5

0.4

0.5

RSM

LDA

CTM

Para2V RATM−D

(a) T = 100

0.4

RSM

LDA

CTM

Para2V RATM−D

(b) T = 200

Figure 4: Classiﬁcation results on the NYTimes corpus for
diﬀerent models with 5-fold cross-validation.

Precision

Precision

0.80

0.75

generated by RATM-D and baseline methods in two dimensions, 100 and 200, respectively. We use β generated by the
LDA model to initialize the topic distributions over words in
the proposed RATM-D. The baseline models we used here
include the LDA, CTM, RSM and Para2V (Le and Mikolov
2014). Here we do not include the RATM-N for comparison
since it cannot obtain embeddings for documents. The SVM
with the LIBSVM implementation (Chang and Lin 2011)
and the Gaussian kernel is used as the classiﬁer. We use
the held-out precision as the performance measure. From
the results shown in Figures 3 and 4, we see that the performance of the RATM-D model is signiﬁcantly better than
that of baseline methods. One reason could be that compared with the LDA, CTM, RSM and Para2V, the proposed
RATM-D uses not only the word counts in a document but
also the sequential information between sentences, leading
to more eﬀective embeddings for documents.

0.75

0.70

0.65

Precision

0.6

Sentences (2) and (3) are noises since they are unrelated to
others. We record the values of the attention signals for the
follow-up sentences (4), (5) and (6) in each row of Table 2.
We can see that the attention signals of noisy sentences are
much smaller than those of the normal sentences in the three
cases and so our attention signals are robust to noisy sentences.

0.80

0.7

Precision

0.7

0.70

RSM

LDA

CTM

Para2V RATM−D

(a) T = 100

0.65

RSM

LDA

CTM

Para2V RATM−D

(b) T = 200

Figure 3: Classiﬁcation results on the Wikipedia corpus for
diﬀerent models with 5-fold cross-validation.

Experiments on Document Classiﬁcation
In this section, we evaluate the performance of diﬀerent
models on the Wikipedia and NYTimes corpora for the document classiﬁcation task. We utilize the document features

3228

6

Conclusion

Griﬃths, T. L.; Steyvers, M.; Blei, D. M.; and Tenenbaum, J. B.
2004. Integrating topics and syntax. In NIPS.
Gruber, A.; Weiss, Y.; and Rosen-Zvi, M. 2007. Hidden topic
Markov models. In Proceedings of International Conference on
Artiﬁcial Intelligence and Statistics.
Hinton, G. E., and Salakhutdinov, R. R. 2009. Replicated softmax:
an undirected topic model. In NIPS.
Hoﬀman, M. D.; Blei, D. M.; and Bach, F. R. 2010. Online
learning for latent Dirichlet allocation. In NIPS.
Hofmann, T. 1999. Probabilistic latent semantic indexing. In
SIGIR.
Le, Q. V., and Mikolov, T. 2014. Distributed representations of
sentences and documents. arXiv preprint arXiv:1405.4053.
Li, S.; Huang, G.; Tan, R.; and Pan, R. 2013. Tag-weighted
Dirichlet allocation. In Proceedings of 13th IEEE International
Conference on Data Mining.
Li, S.; Li, J.; Huang, G.; Tan, R.; and Pan, R. 2015. Tagweighted topic model for large-scale semi-structured documents.
arXiv preprint arXiv:1507.08396.
Li, S.; Pan, R.; Zhang, Y.; and Yang, Q. 2016. Correlated tag
learning in topic model. In Proceedings of the 32nd Conference
on Uncertainty in Artiﬁcial Intelligence (UAI).
Li, S.; Li, J.; and Pan, R. 2013. Tag-weighted topic model for
mining semi-structured documents. In IJCAI.
Lin, R.; Liu, S.; Yang, M.; Li, M.; Zhou, M.; and Li, S. 2015.
Hierarchical recurrent neural network for document modeling. In
EMNLP.
Ling, W.; Chu-Cheng, L.; Tsvetkov, Y.; and Amir, S. 2015. Not
all contexts are created equal: Better word representations with
variable attention. In EMNLP.
Marlin, B. M. 2003. Modeling user rating proﬁles for collaborative ﬁltering. In NIPS.
Mikolov, T.; Karaﬁát, M.; Burget, L.; Cernockỳ, J.; and Khudanpur, S. 2010. Recurrent neural network based language model.
In Proceedings of the 11th Annual Conference of the International
Speech Communication Association.
Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and Dean, J.
2013. Distributed representations of words and phrases and their
compositionality. In NIPS.
Mnih, V.; Heess, N.; Graves, A.; et al. 2014. Recurrent models of
visual attention. In NIPS.
Rush, A. M.; Chopra, S.; and Weston, J. 2015. A neural attention model for abstractive sentence summarization. arXiv preprint
arXiv:1509.00685.
Serban, I. V.; Sordoni, A.; Bengio, Y.; Courville, A.; and Pineau,
J. 2016. Building end-to-end dialogue systems using generative
hierarchical neural network models. In AAAI.
Sutskever, I.; Martens, J.; and Hinton, G. E. 2011. Generating text
with recurrent neural networks. In ICML.
Tang, J.; Meng, Z.; Nguyen, X.; Mei, Q.; and Zhang, M. 2014.
Understanding the limiting factors of topic modeling via posterior
contraction analysis. In ICML.
Teh, Y. W.; Jordan, M. I.; Beal, M. J.; and Blei, D. M. 2012. Hierarchical Dirichlet processes. Journal of the American Statistical
Association.
Wei, X., and Croft, W. B. 2006. LDA-based document models for
ad-hoc retrieval. In SIGIR.

In this work, we propose the RATM to handle sequences
in a discrete space and apply it to document modeling by
viewing a document as a sequence of sentences. We evaluate
the approach on topic modeling based on two measures: the
held-out perplexity and classiﬁcation accuracy. Moreover,
we analyze the attention signals learned from our model for
sentences in two diﬀerent corpora. A future direction is to
devise parallel algorithms for the RATM to further improve
its eﬃciency.

Acknowledgments
We thank the support of iPIN and Shenzhen project
KC2016KCLT0009A. We also thank the great support of
Prof. Qiang Yang.

References
Ahmed, A., and Xing, E. P. 2008. Dynamic non-parametric mixture models and the recurrent Chinese restaurant process: with
applications to evolutionary clustering. In ICDM.
Bahdanau, D.; Cho, K.; and Bengio, Y. 2014. Neural machine
translation by jointly learning to align and translate. arXiv preprint
arXiv:1409.0473.
Blei, D. M., and Laﬀerty, J. D. 2005. Correlated topic models. In
NIPS.
Blei, D. M., and McAuliﬀe, J. D. 2007. Supervised topic models.
In NIPS.
Blei, D. M.; Ng, A. Y.; and Jordan, M. I. 2003. Latent Dirichlet
allocation. Journal of Machine Learning Research.
Blei, D. M. 2012. Probabilistic topic models. Communications of
the ACM.
Boyd-Graber, J. L., and Blei, D. M. 2009. Syntactic topic models.
In NIPS.
Brown, P. F.; Desouza, P. V.; Mercer, R. L.; Pietra, V. J. D.; and
Lai, J. C. 1992. Class-based n-gram models of natural language.
Computational Linguistics.
Cai, D.; Mei, Q.; Han, J.; and Zhai, C. 2008. Modeling hidden topics on document manifold. In Proceedings of International
Conference on Information and Knowledge Management.
Chang, C., and Lin, C. 2011. LIBSVM: A library for support
vector machines. ACM Transactions on Intelligent Systems and
Technology.
Chorowski, J. K.; Bahdanau, D.; Serdyuk, D.; Cho, K.; and Bengio, Y. 2015. Attention-based models for speech recognition. In
NIPS.
Du, N.; Farajtabar, M.; Ahmed, A.; Smola, A. J.; and Song, L.
2015. Dirichlet-Hawkes processes with applications to clustering
continuous-time document streams. In SIGKDD. ACM.
Fei-Fei, L., and Perona, P. 2005. A bayesian hierarchical model for
learning natural scene categories. In Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition.
Frinken, V.; Fischer, A.; Manmatha, R.; and Bunke, H. 2012. A
novel word spotting method based on recurrent neural networks.
IEEE Transactions on Pattern Analysis and Machine Intelligence
34(2).
Gregor, K.; Danihelka, I.; Graves, A.; and Wierstra, D. 2015.
DRAW: A recurrent neural network for image generation. arXiv
preprint arXiv:1502.04623.

3229

A Flow Table-Based Design to Approximate
Fairness
Rong Pan
Lee Breslau
Balaji Prabhakar
Scott Shenker
Stanford University AT&T Labs-Research
Stanford University
ICIR
rong@stanford.edu breslau@research.att.com balaji@stanford.edu shenker@icir.org
500
1

Throughput (kbps)

Cumulative Fraction of Bytes

400
0.8

0.6

0.4

200
100

0.2

0
1e-06

300

Trace 1
Trace 2
Trace 3
1e-05

0.0001
0.001
0.01
Fraction of 1-Second Rates

0.1

0

1

RED

Fig. 1. Complementary Distribution of 1-Second Rates

AFD

Fig. 2. Background: Mixed Traffic - throughput
0.2

Drop Probability

Abstract— The current Internet architecture relies on congestion avoidance mechanisms implemented in the transport layer protocols, like TCP,
to provide good service under heavy load. If routers distribute bandwidth
fairly, the Internet would be more robust and could accommodate more diversity of end hosts. Most of the mechanisms proposed to accomplish this
can be grouped into two general categories. The first category, which includes Fair Queueing (FQ [4]) and its many variants, uses packet scheduling algorithms that are more difficult to implement compared to FIFO
queueing. The algorithms in the second category, active queue management schemes with enhancements for fairness (e.g., FRED [8], SFB [5]), are
based on FIFO queueing. They are easy to implement and are much fairer
than the original RED [6] design, but they don’t provide max-min fairness
among a large population of flows. Recently, a router mechanism, AFD
[14] (Approximate Fair Dropping), has been proposed to achieve approximately max-min fair bandwidth allocations with relatively low complexity.
In this paper, we propose an implementation of AFD which can mimic the
performance of the original design with much less state.

FRED

0.15

0.1

0.05

0
RED

FRED

AFD

Fig. 3. Background: Mixed Traffic - drop probability

I. BACKGROUND
Approximate Fair Dropping (AFD) [14] is an active queue
management scheme which uses a FIFO queue and drops packets probabilistically upon arrival. However, the decision whether
to drop a packet (say from flow i) or not is based not only
on the queue size but also on an estimate of the flow’s current sending rate ri . To achieve max-min fairness, the droprfair
ping function is defined to be di = (1
ri )+ . As a result, the throughput of each flow is bounded by the fair share:
ri (1 di ) = min(ri ; rf air ). Hence, drops do not occur evenly
across flows but are applied differentially to flows with different rates. The key design aspects of AFD lie in the methods by
which ri and rf air are estimated.
To estimate ri , AFD uses the observation that, like the distribution of flow sizes, the distribution of flow rates is also longtailed, i.e. most bytes are sent by fast flows and a vast majority
of flows are slow. For example, Figure 1 shows the cumulative distributions of the 1-second flow rates for three different
traces; in these data sets, 10% of the flows represent 60% - 90%
of the total bytes transmitted. Therefore, a sample of the recent

traffic would largely consist of bytes from faster flows and, typically these are the flows that send at rates equal to or above
the fair share. Most slow flows won’t show up in the sample
and can be ignored anyway since they won’t be dropped. AFD
takes advantage of this property of traffic samples to estimate
flow rates. Therefore, it only needs to keep state proportional
to the number of fast flows, which is much less than per-flow
state. Specifically, AFD maintains a shadow buffer of b arrival
packet samples (header only). Suppose currently a flow i has
mi packets in the shadow buffer. Then its arrival rate can be approximated by ri = R mbi , where R is the aggregate arrival rate.
It is clear that the drop function can be rewritten as

di = (1

mf air
)
mi +

(1)

where mf air = b fair
R .
In AFD, mf air is obtained implicitly. Note that if mf air is
varied intentionally, i ri (1 di ) would change accordingly.
As a result, the queue length fluctuates. It will stabilize when
r

Proceedings of the 10TH Symposium on High Performance Interconnects Hot Interconnects (HotI’02)
0-7695-1650-5/02 $17.00 © 2002 IEEE

P

Pr

di ) equals the outgoing link capacity, at which point
i i (1
mf air = b rfair
. To enforce the queue length stabilizing around
R
a target value, AFD updates mf air periodically as follows,
mf air (t) = mf air (t 1) + (q(t 1) qtarget )
 (q(t) qtarget )

shadow buffer
b=12

flow table

(2)

where q (t) is the queue length at the t-th sample, q (t 1) is
the queue length at the previous sample, and qtarget is the target
queue size. Constants  and  are configurable parameters. The
detailed discussion regarding how to set these parameters can
be found in [14]. Using the above method, we can infer mf air
dynamically with no additional state.
The performance of AFD has been evaluated in a variety of
scenarios using simulation. One typical simulation result is
shown in Figures 2 and 3. The simulation set up consists of 7
TCP flow groups (5 flows each) with different congestion control mechanisms and RTTs.1 The congested link bandwidth is
10Mbps, therefore Rf air equals 286Kbps. The performance
of AFD is compared against RED and FRED. Figure 2 shows
the average throughput received by each flow group. The corresponding drop probability of each flow group is depicted in
Figure 3. The results demonstrate that AFD provides a good approximation to fair bandwidth allocation by differentially dropping packets.
In Section II, we discuss ways to implement AFD and introduce an improved mechanism, AFD-NFT. Analysis in Section
III shows that by the law of large numbers AFD-NFT behaves
like AFD on average. We evaluate the performance of these
schemes and present the results in Section IV. Finally we conclude in Section VI.
II. I MPLEMENTATION
Although theoretically AFD requires only one data structure,
the shadow buffer, to function, it is infeasible to recount mi
on each packet arrival. Hence, a direct implementation of the
AFD algorithm, which we refer to as the AFD-SB design, requires two data structures: a shadow buffer which stores a recent sample of packet arrivals and a flow table which keeps the
packet count of each flow that appeared in the shadow buffer.
The flow table structure can be implemented using a hash table or a CAM, which has O(1) lookup time. The update of the
shadow buffer occurs probabilistically. When a packet arrives,
with probability 1s (s is the update interval), a random packet
in the shadow buffer is chosen and replaced with the arriving
packet. Although we could remove packets in a FIFO way, random replacement avoids synchronization problems. After a replacement, the packet count for the flow which the victim packet
belongs to (say flow i) is reduced by one, mi = mi 1. Conversely, the packet count for the flow which the incoming packet
belongs to (say flow j ) is increased by one, mj = mj + 1.
Assume the shadow buffer size is b packets and there are N
flows which have packets present in the shadow buffer, then
N
i=1 mi = b.

P

1 The topology and the simulation set up is the same as the one to be discussed
in subsection IV-B.

when a

Flow 1

2

Flow 2

6

Flow 3

4

packet arrives, and a

packet is selected to be replaced

shadow buffer
b=12

flow table

Flow 1

2

Flow 2

5

Flow 3

5

Fig. 4. AFD-SB Design

Figure 4 shows a simple example of the above process. The
shadow buffer of size 12 hold packets from three different flows:
Flow 1, 2 and 3. These flows have 2, 6 and 4 packets present in
the shadow buffer respectively. When a Flow 3’s packet arrives,
a randomly chosen Flow 2’s packet is replaced by this newly
arrived packet. As a result, Flow 2’s packet count in the flow
table is decreased by one while Flow 3’s packet count is incremented by one. These operations maintain the data structures
(the shadow buffer and the flow table) used to guide dropping
decisions. Note that these data structures occupy memory which
is separate from the FIFO buffers in which actual packets are
queued. In the next two subsections, we discuss ways in which
we can simplify the implementation of AFD.
A. Reducing Memory
Aiming to reduce the memory requirement of AFD-SB, we
propose a randomized approximation of AFD, which keeps only
one data structure, the flow table. The shadow buffer is only logN
ically present in the sense that i=1 mi = b still holds. Incrementing the flow table upon packet insertions is straightforward
and is the same as before. The challenge is how to remove a
packet from the logical shadow buffer, i.e. to decrement a flow’s
packet count by one, without linearly traversing the flow entries. In the ideal case of mimicking AFD-SB’s performance,
we would like a flow i’s packet to be removed with a probability
pi = mi b 1 . Therefore on average, all mi of flow i’s packets
are replaced after b updates.
The initial AFD-FT design2 works as follows: When it is time

P

2 To be consistent with [14], we refer to this design as AFD-FT throughout this

Proceedings of the 10TH Symposium on High Performance Interconnects Hot Interconnects (HotI’02)
0-7695-1650-5/02 $17.00 © 2002 IEEE

Flow 1

2

Flow 2

6

Flow 3

4

S(0)

when a

100Mbps

S(1)

1/3

1/3

Flow 1

1

Flow 1

2

Flow 1

2

Flow 2

6

Flow 2

5

Flow 2

6

Flow 3

5

Flow 3

5

Flow 3

4

R(0)
R(1)

Traffic Sources

packet arrives

1/3

100Mbps

R1

10Mbps

Traffic Sinks

R2

S(n-1)

R(n-1)

S(n)

R(n)

Fig. 6. Basic Simulation Topology

new draw

will be used for
the next 2 updates

new draw

will be used for
the next 1 update

new draw

Fig. 5. AFD-FT and AFD-NFT Designs

Throughput (kbps)

AFD-FT stops here/AFD-NFT continues

CBR Traffic

650
600
550
500
450
400
350
300
250
200
150
100
50
0

Fair Share
Flow's Arrival Rate
Flow's Throughput (FT)
Flow's Throughput (SB)
Flow's Throughput ( NFT)

0

10

20

30

40

50

Flow Id

to update the logical shadow buffer, a small set of flow ids, S , are
chosen uniformly. Let s be the size of the set S . Then each flow
has an equal probability of sN 1 to be present in the set. Given
that a flow i is chosen, with a probability mi ( jS mj ) 1 , its
count is decremented by one. AFD-FT tries to approximate
pi = mi b 1 under AFD-SB with pi = sN 1 mi ( jS mj ) 1 .
AFD-FT can approximate AFD-SB’s performance when there
are no large flows whose packet counts are much bigger than
that of other flows. However if such flows do exist, then AFDFT tends to penalize them by limiting their throughput to be
below the fair share. The reason for this is that all flows have an
equal chance of being present in the set S , even though a flow
(  S) with higher packet count has a higher probability of being decremented. Therefore, a flow i with a higher packet count
has a lower chance than mi b 1 to be decremented for each update. As a result, on average, its total count deduction is less
than mi after b updates, leading to a higher drop probability.
Using the same example in Figure 4, Figure 5 illustrates how
AFD-FT would behave when s equals one. By choosing one
flow at random, Flow 1, 2 and 3 have an equal chance of 13 to
be decremented by one. Under AFD-SB, however, the chances
for these three flows are 16 , 12 , and 13 respectively. Thus, while it
needs 6 updates on average to reduce the Flow 1 count by one
in AFD-SB, it takes only 3 updates to do so in AFD-FT. Small
flows are favored, and there is a bias against fast flows. As our
later simulations show, this bias against larger flows can lead to
a very significant throughput penalty.

P

P

Fig. 7. Offered Load and Throughput for 50 CBR Flows under AFD designs

B. New Flow Table Design
To improve upon the performance of AFD-FT, we propose a
new AFD flow table design, which we refer to as AFD-NFT
(New Flow Table). AFD-NFT achieves the performance of
AFD-SB with the state requirement of AFD-FT, and it works
as follows: When it is time to decrement a flow’s packet count
by one (i.e. removing a packet from the logical shadow buffer),
draw a small set of S flow ids uniformly from the flow entries
if such a set does not exist. A flow i ( S)’s packet count is reduced by one with a probability of mi ( jS mj ) 1 . Notice that
the above operations are exactly the same for both AFD-FT and
AFD-NFT. The next step, however, represents the crucial difference between the two. Under AFD-FT, a new set S is chosen for
each update. Under AFD-NFT, on the other hand, once a set S is
chosen, it is used for the next m = a  ( jS mj ) updates. The
constant a is a parameter < 1. After m updates, a new set is chosen again and the same operations are repeated. Figure 5 shows
how AFD-NFT would work with a = 0:5 and s = 1. Each flow
has a chance of 13 of being drawn. When Flow 1 is selected, m1
is reduced to one. Since m = 1, a new flow will be drawn for
the next table update. Suppose Flow 2 is chosen instead, with
m = 3, Flow 2 will be the victim flow for the following two
table updates before a new flow is selected. Similarly, if Flow 3
is drawn, the flow will be used for the next update. In the next
two sections, we will demonstrate that AFD-NFT performs as
well as AFD-SB.

paper.

Proceedings of the 10TH Symposium on High Performance Interconnects Hot Interconnects (HotI’02)
0-7695-1650-5/02 $17.00 © 2002 IEEE

P

P

N P Si
ami statistics
0.222 0.207
0.444 0.411
0.888 0.808
1.778 1.646
2.667 2.453

N Pi
Ns
mi statistics N (as) 1 statistics
3.70 3.63
7.41 7.42
14.81 14.82
180
167
29.63 29.66
44.44 44.53

TABLE I
F LOW TABLE A CCESS S TATISTICS

Mixed Traffic with 1 UDP Source

500

Throughput (kbps)

Ps
FlowGrp ID sN 1 statistics
0
0.1
0.098
1
0.1
0.100
2
0.1
0.100
3
0.1
0.100
4
0.1
0.100

400
300
200
100
0

On-Off Bursty Sources

AFD-FT

400
0.5*Rfair
2*Rfair
8*Rfair

350

1*Rfair
4*Rfair

AFD-SB

AFD-NFT

Fig. 9. Mixed TCP Traffic with a UDP flow
Mixed Traffic

500

250
200
150
100
50
0
AFD-FT

AFD-SB

AFD-NFT

Throughput (kbps)

Throughput (Kbps)

300

400
300
200
100

Fig. 8. Bursty On-Off Source
0
AFD-FT

III. A NALYSIS

AFD-SB

AFD-NFT

Fig. 10. Mixed TCP Traffic

Recall that our goal in designing AFD-NFT is to match the
performance of AFD-SB so that, after b updates, a flow i has on
average mi of its packets replaced. By the law of large numbers,
we can prove that the performance of AFD-NFT is the same as
that of AFD-SB on average. An outline of the proof follows:

4) Combine the above three arguments, after b updates, the
average number of flow i packets replaced is equal to

1) We know from the above that each flow has the same
chance of being chosen in the set S , and the probability, Ps ,
is sN 1 .

which matches the desired behavior of matching AFD-SB.

i=N
i=1

N

mi

=

b
:
N

Therefore, assume s << N , the average total packet counts in
a chosen set S is sbN 1 . As a result, the total of packets that
are replaced equals sbaN 1 . So, to replace b packets, we need
to draw,

Ns =

b
sbaN

1

=

N
as

number of sets.

N
 as
 ami = mi :

We evaluate the performance of AFD-NFT in a variety of scenarios and compare it against AFD-SB and AFD-FT. Our simulation topology is depicted in Figure 6. Unless otherwise stated,
the latencies at the access links are 2ms and the latency at the
congested link is 20ms. In all the experiments, b is chosen to be
1000, a is set to 0.06 and s equals 5. We present five simulation
results in this section, which are separated into two subsections:
in subsection IV-A, we demonstrate that AFD-NFT can perform
as well as AFD-SB in the cases where AFD-FT behaves poorly;
second, we show that all three algorithms perform similarly in
other cases.
A. Performance Improvement

3) Given a set S and a flow i ( S), there are on average
NPSi =

s
N

IV. S IMULATION R ESULTS

2) The average packet count of a flow equals

P

NPi =

P

mi

jS

 m = Pm m a 
i

mj

jS

j

number of flow i packets to be replaced.

X
jS

mj = ami

CBR Traffic: Figure 7 shows a simulation run in which five
CBR flow groups (10 flows each) compete for the congested
link bandwidth of 10Mbps. The sending rates for each group are
50Kbps, 100Kbps, 200Kbps, 400Kbps and 600Kbps. The performance comparison among different AFD designs is presented
in Figure 7. The result shows that AFD-NFT can mimic AFDSB’s performance by providing each flow its fair share. AFD-FT
penalizes the aggressive flows by limiting their throughput to be

Proceedings of the 10TH Symposium on High Performance Interconnects Hot Interconnects (HotI’02)
0-7695-1650-5/02 $17.00 © 2002 IEEE

300

RTT Comparison (max: 150ms)

FlowGrp ID
0
1
2
3
4
5
6

Throughput (kbps)

250
200
150

a
1.0
0.75
2.0
1.5
1.0
1.0
1.0

b
0.9
0.31
0.5
1.0
0.5
0.5
0.5

k
1.0
1.0
1.0
2.0
0.0
1.0
1.0

l
1.0
1.0
1.0
0.0
1.0
1.0
1.0

RT T
25ms
25ms
25ms
25ms
25ms
25ms
100ms

100

TABLE II
M IXED TCP T RAFFIC C ONFIGURATION

50
0
AFD-FT

AFD-SB

AFD-NFT

Fig. 11. Four TCP Flow Groups with Different RTTs (max = 150ms)

under their fair share. Although the performance penalty is mild
in this scenario, we will show below that AFD-FT can severely
punish aggressive flows. The flow table access statistics in this
simulation are collected and tabulated in Table I. Since the variance among individual flows is very small, as seen in Figure 7,
the statistical data is averaged within the ten flows in each group
so that it can be more easily presented. It is clear that the data
obtained from the simulation is in very close agreement with
what the analysis predicts.
Bursty On-Off Sources: We next evaluate the performance
of AFD designs in the presence of an on-off source. In this
setup, an on-off source is sharing the congested link with 35
TCP flows, where Rf air equals 278Kbps. The bursty source
sends at the speed of the access link (100Mbps) for a very short
period, ton , and then goes idle for time tof f . Its average sending
rate is 100Mbps*ton(ton + tof f ) 1 . Only the throughput of the
bursty source is plotted in Figure 8 since it shows the biggest
discrepancies among different AFD algorithms. The TCP flows
utilize the rest of the link bandwidth and the differences among
those flows are small. Note that the left-most bars in the diagram
represent the throughput that the on-off source gets when its average sending rate is only half of Rf air , in which case all three
algorithms allocate the bandwidth fairly, i.e. provide the flow
its request bandwidth. However, as the plot shows, when the
on-off flow gets more bursty and sends above 2Rf air , AFD-FT
starts penalizing it. The more bursty a flow is, the more severe
the penalty. Conversely, AFD-SB and AFD-NFT allocate bandwidth fairly; flows are not penalized for their burstiness.
Mixed TCP Traffic with one UDP source: Figure 9 represents
a simulation case where the traffic mix is one UDP source sharing the link with 7 groups (5 flows per group) of TCP flows
with different congestion control methods. For generalized window control mechanisms, the window increase has a form of
w + aw k , and the decrease of a form w bwl . The 7 groups
in the simulation have different values of (a,b,k,l) and RTTs,
which is tabulated in Table II. Note that the normal TCP has the
form of (1.0, 0.5, 1.0, 1.0). The right-most bars represent the
throughput of the UDP flow under the different algorithms. The
result shows once again that the AFD-NFT design can mimic
the performance of AFD-SB while AFD-FT fails to do so.
B. Comparable Performance
Mixed TCP Traffic: We remove the UDP flow from the above
simulation. Only TCP flows with various congestion parame-

ters compete against each other. The results in Figure 10 show
that AFD-NFT performs equally well in the cases where AFDFT excels. All three AFD designs allocate bandwidth in a fair
manner.
Different RTTs: AFD behaves reasonably well, though not
ideally, in the cases where flows with different RTTs are sharing
a link [14]. To exhibit that AFD-NFT does not perform worse,
we perform this experiment. In this simulation, flows are separated into 4 groups, 10 flows in each group. The RTTs (propagation delay only) are 37.5ms, 75ms, 112.5ms and 150ms respectively. Figure 11 shows that AFD-NFT’s performance is similar
to that of AFD-SB and AFD-FT: although there are some discrepancies among flows with different RTTs, the differences are
not significant.
V. M EMORY R EQUIREMENT
In the previous session, we have shown that AFD-NFT provides reasonably fair bandwidth allocation. All the operations
on the forwarding path are O(1). So the main question regarding
whether AFD-NFT is practical or not lies in its memory requirement. Since the size of the set S is small(usually less than 10),
the flow ids in S can be easily stored using registers. It is the
flow table that requires some memory buffering.
The size of the flow table is directly related to the number of
flows, N , present in the shadow buffer. In the various traces
we have seen [14], N is typically less than one fourth of b, the
number of packets in the logical shadow buffer. We also find that
, in order to achieve a good performance, b should be roughly
R
R
10 r
. Hence, N equals 2:5 rfair
. It is hard to estimate the
fair
value of rf air on a typical Internet link. To make a conservative
estimate, we assume rf air equals 56Kbps, the slow telephone
modem speed. Then for a link capacity of 1Gbps, it is simple
to obtain that N is on the order of a few thousand. Therefore,
the flow table can be easily implemented using a standard hash
table or CAM. The memory overhead is very limited.
VI. C ONCLUSION
We have proposed a new flow table based AFD design, AFDNFT. AFD-NFT reduces drastically the state requirement of
AFD algorithm, and yet has virtually identical performance.
This and other data suggests that in a wide range of scenarios
AFD provides a good approximation to fair bandwidth allocation, typically providing bandwidth allocations within +/-15%
of the fair share.

Proceedings of the 10TH Symposium on High Performance Interconnects Hot Interconnects (HotI’02)
0-7695-1650-5/02 $17.00 © 2002 IEEE

R EFERENCES
[1]
[2]
[3]

[4]

[5]
[6]
[7]
[8]
[9]
[10]
[11]
[12]
[13]

[14]

[15]
[16]
[17]

Bansal, D., and Balakrishnan, H., “Binomial Congestion Control Algorithms ” Proceedings of Infocom ’2001, April 2001.
http://www.caida.org/analysis/AIX/plen hist/
Braden, B., Clark, D., Crowcroft, J., Davie, B., Deering, S.,Estrin,
D., Floyd, S., Jacobson, V., Minshall, G., Partridge, C., Peterson,
L.,Ramakrishnan,K., Shenker,S., Wroclawski, J., Zhang, L., “Recommendations on queue management and congestion avoidance in the internet”,
IETF RFC (Informational) 2309, April 1998.
Demers, A., Keshav, S. and Shenker, S., “Analysis and simulation of a
fair queueing algorithm”, Journal of Internetworking Research and Experience, pp 3-26, Oct. 1990. Also in Proceedings of ACM SIGCOMM’89,
pp 3-12.
Feng, W., Shin, K., Kandlur, D. and Saha, D., “Stochastic Fair Blue: A
Queue Management Algorithm for Enforcing Fairness”, Proceedings of
INFOCOM’2001, April, 2001.
Floyd, S. and Jacobson, V., “Random Early Detection Gateways for Congestion Avoidance”, IEEE/ACM Transaction on Networking, 1(4), pp 397413, Aug. 1993.
Floyd, S., and Fall, K., “Promoting the Use of End-to-End Congestion
Control in the Internet”, IEEE/ACM Transactions on Networking, August
1999.
Lin, D. and Morris, R., “Dynamics of random early detection”, Proceedings of ACM SIGCOMM’97, pp 127-137, Oct. 1997.
Mahajan, R. and Floyd, S. “Controlling High-Bandwidth Flows at the
Congested Router”, ACIRI, Berkeley, California, Nov. 2000.
McKenny, P., “Stochastic Fairness Queueing”, Proceedings of INFOCOM’90, pp 733-740.
Nagle, J., “On packet switches with infinite storage”, Internet Engineering
Task Force, RFC-970, December, 1985.
Ott, T., Lakshman, T. and Wong, L., “SRED: Stabilized RED”, Proceedings of INFOCOM’99, pp 1346-1355, March 1999.
Pan, R., Breslau, L., Prabhakar, B. and Shenker, S., “Approximate Fairness through Differential Dropping (summary)”, Student Poster Session at
Proceedings of SIGCOMM’01, also appeared at Computer Communication Review, January 2002.
Pan, R., Breslau, L., Prabhakar, B. and Shenker, S.,
“Approximate
Fairness
through
Differential
Dropping”,
http://www.research.att.com/˜breslau/papers/afd-techreport.ps.gz, under
submission.
Pan, R., Prabhakar, B. and Psounis, K., “CHOKe - A Stateless Active
Queue Management Scheme For Approximating Fair Bandwidth Allocation”, Proceedings of INFOCOM’00 March 2000.
Stoica, I., Shenker, S. and Zhang, H., “Core-Stateless Fair Queueing:
Achieving Approximately Fair Bandwidth Allocations in High Speed Networks”, Proceedings of ACM SIGCOMM’98.
ns - Network Simulator (Version 2.1b6).

Proceedings of the 10TH Symposium on High Performance Interconnects Hot Interconnects (HotI’02)
0-7695-1650-5/02 $17.00 © 2002 IEEE

Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)

Automatic Emphatic Information Extraction from Aligned
Acoustic Data and Its Application on Sentence Compression
Yanju Chen

Rong Pan∗

Department of Computer Science
Sun Yat-sen University
Guangzhou, China
chyanju@gmail.com

Department of Computer Science
Sun Yat-sen University
Guangzhou, China
panr@sysu.edu.cn
generated words constitute subsequences of the input sequences. Existing systems mostly rely heavily on syntactic information (McDonald 2006; Berg-Kirkpatrick, Gillick,
and Klein 2011), resulting in a vulnerability to error propagation. Thus, competitive systems without any syntactic information are proposed (Filippova et al. 2015), which beneﬁts from LSTM structures and efﬁcient heuristic search
in the scope of neural network sequence labeling, providing possible solutions to the weakness of the traditional
methods. More recent advances include using eye-tracking
recordings (Barrett, Agić, and Søgaard 2015) to improve
performance of LSTM-based sentence compression models
(Klerke, Goldberg, and Søgaard 2016).
In this paper we address the following question: can useful emphatic information be automatically extracted from
the prevailing acoustic data without any manual feature extraction and be used to help improve the performance of natural language processing tasks such as sentence compression? While sentence compression requires the models of a
good comprehension of the semantic context and the exact
intention of the input sentence, we believe the supervision
of additional emphatic data can be a boost to the later, and
the LSTM structures dealing with the former, which will be
supported by our evidence. Meanwhile, with the Speech-ToText alignment techniques, we present a faster approach to
automatically extract approximate emphatic patterns from
aligned acoustic data, thus lowering the cost of manual
feature extraction in emphatic words detection and prediction and providing weak supervision as an auxiliary task
to improve sentence compression performance using LSTM
structures.
The contributions of our work are summarized as follows:

Abstract
We introduce a novel method to extract and utilize the semantic information from acoustic data. By automatic Speech-ToText alignment techniques, we are able to detect word-based
acoustic durations that can prosodically emphasize speciﬁc
words in an utterance. We model and analyze the sentencebased emphatic patterns by predicting the emphatic levels using only the lexical features, and demonstrate the potential
ability of emphatic information produced by such an unsupervised method to improve the performance of NLP tasks, such
as sentence compression, by providing weak supervision on
multi-task learning based on LSTMs.

Introduction
Speciﬁc words can be prosodically emphasized in an utterance by a speaker in order to draw attentions on them,
which can be modeled by pitch accents of words (Bolinger
1958). Also referred as prosodic prominence, pitch accent
is found to emphasize several semantic information in an utterance such as uncertainty, contrast, turn-taking cues and so
on, whose changes in an utterance can be perceived by listeners and thus convey certain kinds of emphasis (Terken
1991). The detection of prosodic prominence shows improvements on different tasks, such as Text-to-Speech synthesis and spoken language summarization. With most of
the detections of prosodic prominence are done by using
acoustic features (acoustic durations and intensities, extremity of fundamental frequency minima and maxima), there
are also works investigating predictions of emphatic words
using only lexical features (Brenier, Cer, and Jurafsky 2005;
Brenier 2008), which shows promising results and potential
improvements on more NLP tasks but are partly restricted
by the cost of high-quality manual feature extraction.
As one of the standard NLP tasks, the target of sentence
compression is to generate shorter paraphrases of sentences,
which can be further used both to assist other tasks such
as automatic summarization (Berg-Kirkpatrick, Gillick, and
Klein 2011) and to provide assistive applications for poor
readers (Canning et al. 2000), as well as to generate readable news headlines (Filippova 2010). In sentence compression systems that deal with deletion-based compression,

• We propose a faster approach to automatically extract
the emphatic information from prevailing aligned acoustic data.
• We model and analyse the extracted emphatic patterns
and demonstrate how the disjoint emphatic data can be
added to improve the performances of sentence compression tasks using LSTM structures.
• We observe competitive improvements on our multi-task
sentence compression models with the disjoint emphatic
data, compared with the baselines.

∗

Corresponding Author
c 2017, Association for the Advancement of Artiﬁcial
Copyright 
Intelligence (www.aaai.org). All rights reserved.

3422

Related Works

a model without syntactic information.
Our model for experiment is similar to the two above, with
no additional heuristic search or syntactic information attached, which is a pure multi-task learning neural network.

There are some related works on sentence compression and
the detection and prediction of emphatic words respectively.
However, to our knowledge, there is no competitive research
that connects the two tasks together and improve the former with a weak supervision from the later. There are several works that prove the beneﬁts of deep multi-task learning
with low level tasks supervised at lower layers (Søgaard and
Goldberg 2016), and works that improve sentence compression by introducing gaze measures prediction as an auxiliary
task (Klerke, Goldberg, and Søgaard 2016), and therefore
we argue that emphatic patterns of sentences from acoustic
data can also bring improvements even when they are extracted automatically without manual annotation.

Emphatic Words Detection in Acoustic Data
Usually acoustic data is a good resource for emphatic words
detection. In this paper, we refer emphatic words to those
words that are a subset of pitch accents that have been shown
to be categorically interpreted as distinct from neural pitch
accents (Ladd and Morton 1997) and conveying an acute
degree of emphasis (Brenier, Cer, and Jurafsky 2005).

Semantic Information in Prosodic Prominence
Prosodic prominence is found to convey various semantic information (Pon-Barry and Shieber 2009; Wang et al.
2013; Cao et al. 2014; Ferguson, Durrett, and Klein 2015;
Levitan et al. 2016) through changes between words and
syllables. Such information including contrast, incredulity,
uncertainty, adverbial focus and so on, correlates with the
speaker’s intentions and can be perceived by listeners. Major
indicators to judge whether a word is pitch accented or not
include duration, intensity and extremity of fundamental frequency minima and maxima. The detection of the three indicators are usually done by annotators using several acoustic tools according to general standards before the annotated
data can be used to train an automatic prominence classiﬁer.

Emphatic Words Prediction from Text
While several classiﬁers for prominence detection and prediction have been proposed (Chen, Hasegawa-Johnson, and
Cohen 2004; Sun 2002; Brenier, Cer, and Jurafsky 2005;
Hovy et al. 2013), with some of them utilizing lexical features, most of the features used require labeling by experienced human annotators, and speciﬁcally for lexical features, the manual alignment of text and speech are required.
The labeling cost is thus high especially for some of the
advanced linguistic text features that only professional annotators can afford (Brenier 2008). As a result, the overall
accuracies of these classiﬁers are high and this provides better improvements in further applications like text-to-speech
synthesis.
Speciﬁcally for those classiﬁers that predict prominence
from only lexical features, the overall accuracies can be
around 79.7% (Brenier, Cer, and Jurafsky 2005) using maximum entropy classiﬁers. Since the weak supervision method
is considered a new possibility to solve classiﬁcation problems with different constraints in the access to the class information (Hernández-González, Inza, and Lozano 2016), a
balance between the accuracy and the quantity of the data
can then be reached to provide sufﬁcient information for
many NLP tasks (Hoffmann et al. 2011), which indicates
that an approximate approach that generates a larger but less
accurate corpus and consumes less human labors can be considered in the perspective of natural language processing.

Automatic Speech-To-Text Alignment
Also known as forced alignment, automatic speech-to-text
alignment has received some attention for different research
goals. Given an exact transcription of what is being spoken
in the acoustic data, the aligner is asked to identify the time
when each word in the transcription was spoken in the utterance.
Among the three indicators of prosodic prominence, word
duration is a relatively easier indicator to be detected using
an automatic speech-to-text alignment system, because an
aligner provides the exact time that a transcribed word occurs and thus we can estimate the word’s duration from the
aligner’s outputs, thus making it possible to collect emphatic
data in a faster and unsupervised way, as well as modeling
the emphatic patterns of a given aligned sentence. An automatic data collection procedure can then be designed to
establish a large scale corpus of emphatic data. We will examine later how the information extracted based on a single
indicator can help improve certain NLP tasks.

Robust Sentence Compression without Linguistic
Information
Recent advances of sentence compression involve the removal of syntactic information (no Part-of-Speech tags,
Name Entity tags or dependencies). A competitive system
(Filippova et al. 2015) is proposed using only word embeddings and gold-standard labels of previous words during training (generated labels during decoding). The model
outperforms the baseline (McDonald 2006), which employs
syntactic information. Stacked LSTMs together with beam
search on top layer generate competitive results. Another
model (Klerke, Goldberg, and Søgaard 2016) uses stacked
Bi-LSTMs with eye-tracking measures as auxiliary task is
also reported competitive results, but the model has CCGtags prediction as a second auxiliary task and is not strictly

Gathering Emphatic Patterns from Aligned
Acoustic Data
We collect acoustic data with corresponding transcriptions
from Lit2Go1 , a free online collection of stories and poems
in audiobook format with transcriptions. There are over 200
books read by distinct readers. Each book is segmented to
several passages, and we collect over 4,000 passages and
1

3423

http://etc.usf.edu/lit2go/

align them in cmusphinx2 to establish the emphatic corpus.
Table 1 shows more details about the data.

Modeling Emphatic Patterns in Aligned Text

Algorithm 1 Binary Emphatic Levels Generation
// BinEmpSeqs: binary emphatic levels of sentences
// EmpLvs: emphatic levels of one sentence
// BinEmpLvs: binary emphatic levels of one sentence
// elv−1 : previous word’s emphatic level
// bslot−1 : previous word’s binary emphatic level
BinEmpSeqs = []
for emphatic pattern of each sequence do
// 1. generate emphatic levels of a single sentence
P5 = 5th percentile in the emphatic pattern
P95 = 95th percentile in the emphatic pattern
EmpLvs = []
for each standard duration sd do
if sd > P95 then
cslot = 30
else if sd < P5 then
cslot = 1
else
5
)
cslot = round(2 + 28 ∗ Psd−P
95 −P5
end if
EmpLvs.add(cslot)
end for
// 2. binarize emphatic levels of a single sentence
BinEmpLvs = []
for each emphatic level elv in EmpLvs do
if elv − elv−1 > 4 && elv > 15 then
bslot = 1
else if elv−1 − elv < 2 && elv > 15
&& bslot−1 == 1 then
bslot = 1
else
bslot = 0
end if
BinEmpLvs.add(bslot)
end for
BinEmpSeqs.add(BinEmpLvs)
end for

We extend the view of emphatic words prediction from modeling the context of a single word to modeling the whole
sentence’s emphatic pattern. The Long Short Term Memory (Hochreiter and Schmidhuber 1997) structure can model
larger context as well as controlling the access of error signals, making the learning of sentence-level information and
long-term dependencies possible. We use only lexical features of the emphatic data to predict the emphatic patterns in
a sequence labeling manner.

Alg. 1 shows the details of how a sequence of binary emphatic levels is generated in every sentence. As a post processing, binary emphatic levels of those words on a stopword list or in the ﬁrst positions of sentences will be set to
0 before the data is used for the emphatic words prediction
model. Several labeling results are shown in Table 2 after the
post processing is done.

Generating Word-Based Emphatic Levels

Emphatic Words Prediction with LSTMs

We normalize and binarize the words’ standard durations
in a sentence to alleviate the effects of different reading
styles (e.g., speeds, tones) of readers. The standard durations
within a sentence will ﬁrst be normalized and mapped to a
list of integer emphatic levels, which denotes the relative
standard duration of each word in a sentence ranging from 1
to 30. According to the changes between the emphatic levels, they are then binarized to be 0 (not emphatic) or 1 (emphatic), which we denote as binary emphatic levels.

LSTM is used to model the binary emphatic patterns. Let X
be the input sequence and A be the output emphatic patterns,
as deﬁned below:

Table 1: Basic Information of Collected Acoustic Data
authors:
208
books:
205
genres:
22
passages:
4198
sentences:
286,083
words:
5,881,720
vocab. size: 48,204 mean sent. len.:
20

The General Process of Aligning Data
First, we do a pre-processing on the transcriptions, including a separation of the punctuations from the attached words
and so on. Second, we feed the aligner with acoustic data
and their corresponding transcriptions. Third, we calculate
the standard duration (S.Duration) of each word according
to Eq. (1). Finally, we extract each sentence with the corresponding S.Duration sequence from the aligned data as an
emphatic pattern.
S.Duration =

total time duration
.
num. of syllables

(1)

We deﬁne an emphatic pattern here as an ordered sequence of standard duration of a sentence. To give a faster
estimation of each word’s acoustic duration, we adopt the
assumption that each syllable has an equal length of time duration in Eq. (1), so that we can alleviate the effects of word
length on the word’s total time duration by considering the
number of syllables a word has.
Thus, a typical entry in the corpus of emphatic data is
a pair of two components: a lexical sentence and a corresponding emphatic pattern. There are over 280,000 pairs in
the corpus.

2

X = (x1 , ..., xN ), A = (a1 , ..., aN ).
We are to optimize the following problem:

log p(A|X; θ).
θ∗ = arg max
θ

(2)

X,A

For the basic LSTM model depicted in Figure. 1, p can be

http://cmusphinx.sourceforge.net/

3424

Type
V
L
V
L
V
L
V
L
1

Table 2: Example Binary Emphatic Levels Generated from Acoustic Data after Post Processing
Sample Labeling Results
Jim was laid up for four days and nights .
18 12 20 30 17 22 26 11 21 .
But it was too dark to see yet , so we made the canoe fast and set in her to wait for daylight .
12 9 11 17 20 11 30 17 23 11 17 8 16 25 14 19 18 13 22 17 5
22
I didn’t need anybody to tell me that that was an awful bad sign and would fetch me some bad luck , so I was
30 9
14
14 13 18 28 13 22 14 11 18 25 30 13 9
27 28 14 19 24 , 20 22 10
scared and most shook the clothes off of me .
25
7 12 24 9
16 20 7 30 .

”V” means visualized results, and ”L” means original emphatic levels.
Underlined bold-faced words are emphatic, otherwise non-emphatic.

2

Figure 1: Basic LSTM Unrolled Through Time
decomposed as follows:
p(A|X; θ) =

Figure 2: Bi-Directional LSTM Unrolled Through Time

N


p(At |X0 , X1 , ..., Xt ; θ).

(3)

word. Speciﬁcally, to enhance the robustness of the model,
the following operations are applied:

t=1

For the bi-directional LSTM model depicted in Fig. 2, p can
be decomposed as follows:
p(A|X; θ) =

N


pf (A|X; θ)pb (A|X; θ),

• Length of input sequences will be at most 50 words, otherwise cut short.
• The embeddings only include words that appear no less
than 2 times in the data.

(4)

t=1

where
pf (A|X; θ) =

N


pf (At |X0 , X1 , ..., Xt ; θ),

(5)

pb (At |XN , XN −1 , ..., Xt ; θ).

(6)

• We use <UKN> to represent words that does not appear
in the embeddings.
Training, validating and testing sets are split. Table 3 shows
the information of the experiment settings.

t=1

pb (A|X; θ) =

N


Results

t=1
∗

We measure and record the following two metrics:

Using the optimal θ , the prediction can then be estimated:
Â = arg max p(A|X; θ∗ ).

• Word-Based Accuracy (W.Acc): how many words are
correctly labeled

(7)

A

• Sentence-Based Accuracy (S.Acc): how many sentence
are fully correctly labeled

We use LSTM in the experiment of emphatic words prediction (see Fig. 1), with a shared softmax classiﬁer connected to each hidden state that predicts the binary emphatic
labels of the current time step. We also use bi-directional
LSTM (see Fig. 2) as a further observation. Adadelta (Zeiler
2012) is used to maximize the training objective.
We used pre-trained word embeddings from the skipgram model3 (Mikolov and Dean 2013) for every input
3

We record a word-based accuracy of 82.90% and a
sentence-based accuracy of 9.47% on the test set, within 10
training epochs. The basic LSTM sequence labeling model
shows a potential capability of capturing and predicting
the emphatic patterns. The Bi-LSTM model has recorded
85.24% word-based accuracy and 14.42% sentence-based
accuracy.

https://code.google.com/p/word2vec/

3425

Improvements on Sentence Compression Task

Experiments

To further verify the potential capability of the extracted semantic information over speciﬁc NLP tasks, we carry out a
series of experiments on deletion-based sentence compression tasks to evaluate the performance of several basic models when emphatic data is added as an auxiliary task.

We carry out comparative experiments. The performance
of a baseline model is compared with a multi-task version
that has emphatic data as an auxiliary task. Additionally we
carry out another set of experiments with the same multitask model but different extra data, as a further comparison.

Sentence Compression Using Multi-Task LSTMs
Table 3: Datasets Characteristics
Dataset
Train Valid
Test
Del.Rate1
GOOGLE
8,000 1,000 1,000
0.59
BROADCAST
880
78
412
0.33
Emphatic2
230k* 25k*
28k*
0.73

Similar to the models used by Klerke et al. (2016), we
adopt even simpler model structures in order to better investigate the actual effects of the emphatic data over the
whole task, without either the prediction of CCG tags or the
CASCADED-LSTM structure. We follow part of the task
settings (Filippova et al., 2015) which introduce no additional syntactic or linguistic information into the models but
only pre-trained embeddings processed with similar operations in the emphatic words prediction experiment.

*
1
2

These are approximate numbers.
Sentences with more than 50 words are cut short.
For sentence compression, the whole set is used as an auxiliary task; for emphatic words prediction, the whole data
set is then split as above.

Compression Data We use two different sentence compression datasets, details shown in Table 3:
• The publicly available subset of GOOGLE4
• BROADCAST (Clarke and Lapata 2006)
Baselines As shown in Figs. 1 and 2, we use LSTM and
bi-directional LSTM for the compression data, and additionally we evaluate the 3-layered stacked bi-directional LSTM
on the GOOGLE (Filippova et al. 2015) dataset. There are
no auxiliary tasks in the three baseline models.
Auxiliary Tasks We evaluate the emphatic data as an
auxiliary task in the multi-task learning models. Meanwhile, the ﬁrst pass duration of eye-tracking data (denoted as
Gaze.fp in tables) is also used as an auxiliary task in another
comparative experiment, pre-processed by Klerke (2016).
Evaluation Metrics For different deletion rates (after the
pre-processing) of datasets shown in Table 3, the F1-Scores
of both label 0 and 1 are of equal importance and thus we
evaluate both F1-Score of label 0 (F 10 ) and F1-Score of
label 1 (F 11 ). We also evaluate the word-based accuracy
(W.Acc) and the sentence-based accuracy (S.Acc).
Multi-Task Learning In each training epoch, the model
is ﬁrst fed with certain amount of random samples from data
of the auxiliary task, and then certain amount of samples
from the compression task. The process may be repeated
once in the same epoch (see Table 4 for more details).
Other Settings The dimensions of input and hidden layers and the embeddings (pre-trained and pre-processed, see
previous section) are 300, and at the output layer we predict sequences of two categories (compression task and emphatic task) or six categories (eye-tracking task). All models
are trained for 30 iterations and we adopt an early-stopping
strategy to select parameters with highest word-based accuracy (W.Acc) on validation set and perform and record evaluations on testing set. We run each experiment for 32 times.
The average of each evaluation metrics is recorded and we
also perform signiﬁcance tests between the sample metrics

Figure 3: Unrolled Stacked Bi-LSTM High-Level Overview
A bi-directional LSTM reads a sequence in both regular
and reversed orders. Our basic model structure is depicted
in Figs. 1 and 2. For some larger datasets, we use 3-layered
stacked bi-directional LSTM to capture deep semantic information from the sequences, as shown in Fig. 3. Two different
softmax classiﬁers are connected to each of the hidden states
(connected to the top hidden states for stacked bi-directional
LSTM) to perform multi-task learning. One for the classiﬁcation of task data and the other for the emphatic data (or
other extra data for comparison).
Additionally for sentence compression, we deﬁne Y as
the output compression labels (1 if a word is retained, 0 if
deleted) as follows:
Y = (y1 , ..., yN ).
Besides the auxiliary optimization problem deﬁned in
Eq. (2), our major optimization problem here is:

log p(Y |X; θ).
(8)
θ∗ = arg max
θ

X,Y

The sentence compression predictions can be estimated by:
Ŷ = arg max p(Y |X; θ∗ ).

(9)

Y

4
http://storage.googleapis.com/sentencecomp/compressiondata.json

The parameters θ are shared by the two tasks.

3426

Table 6: Performance on BROADCAST1 Dataset

Table 4: Feeding Order in An Epoch
Stacked
Experiment LSTM Bi-LSTM
Bi-LSTM
Baseline1
C:Full
C:Full
C:Full
A:2500
A:2000
A:2000
Emphatic
C:1500
C:2000
C:2000
A:2000
A:2000
&Gaze.fp2 A:2500
C:1500
C:2000
C:2000
Emphatic
A:2500
A:2000
C:Full
&Gaze.fp3 C:Full
*

*
1
2
3

Model
LSTM
Bi-LSTM

”Full” means all 8,000 training samples in GOOGLE dataset,
all 880 training samples in BROADCAST dataset, otherwise
randomly sampled; ”C” means samples from compression
task; ”A” means samples from auxiliary task.
Samples are fed in the order as shown in each cell from top to
down alternately.
The rules apply to all datasets.
The rules apply to GOOGLE dataset.
The rules apply to BROADCAST datasets.

LSTM
Bi-LSTM

Model

Our results are presented below. Across all datasets, the emphatic data leads to improvements over the baselines in all
evaluation metrics. As the structures become deeper and
more complex, when capturing more contexts, the improvements are much more signiﬁcant.
• GOOGLE Dataset

LSTM
Bi-LSTM

LSTM
Bi-LSTM
Stacked
Bi-LSTM

Baseline
Emphatic
Gaze.fp
Baseline
Emphatic
Gaze.fp
Baseline
Emphatic
Gaze.fp

W.Acc
79.15
79.40
79.27
79.79
80.14
79.71
79.94
80.30
79.95

S.Acc
10.93
10.87
10.90
11.30
11.95
11.81

Data
Baseline
Emphatic
Gaze.fp
Baseline
Emphatic
Gaze.fp

W.Acc
79.10
79.34
79.42
79.78
80.37
80.24

BROADCAST2
F 10
F 11
13.27 88.12
17.20 88.19
15.98 88.27
22.89 88.35
26.60 88.66
26.11 88.59

S.Acc
22.19
22.25
22.12
22.82
23.19
22.97

Data
Baseline
Emphatic
Gaze.fp
Baseline
Emphatic
Gaze.fp

W.Acc
66.85
67.06
67.19
67.58
68.35
68.23

BROADCAST3
F 10
F 11
36.22 77.55
37.93 77.52
40.38 77.34
38.94 77.86
38.39 78.66
38.01 78.59

S.Acc
9.60
9.70
8.56
11.48
11.65
11.57

LSTM models, while the LSTM models don’t show much
signiﬁcant improvements, as shown in Tables 6 , 7 and 8.
Speciﬁcally, we observe that the emphatic data always
provides a solid regularization over the whole sentences,
resulting in steady improvements on sentence-based accuracies.

Table 5: Performance on GOOGLE Dataset
Data

BROADCAST1
F 10
F 11
14.56 83.43
19.37 83.53
18.56 83.56
21.17 83.51
25.93 83.87
23.98 83.82

Table 8: Performance on BROADCAST3 Dataset

Results and Discussion

Model

Baseline
Emphatic
Gaze.fp
Baseline
Emphatic
Gaze.fp

W.Acc
72.28
72.70
72.69
72.76
73.56
73.34

Table 7: Performance on BROADCAST2 Dataset
Model

drawn from the baseline system and from the comparative
systems to give conﬁdence levels.

GOOGLE
F 10
F 11
82.73 73.70
82.86 74.19
82.84 73.81
83.31 74.40
83.73 74.50
83.40 73.97
83.40 74.63
83.74 74.99
83.48 74.50

Data

S.Acc
6.51
7.18
6.58
7.19
8.11
7.53
8.00
9.26
8.53

Conclusion and Future Work
We present, to our knowledge, a ﬁrst attempt at the automatic extraction of the semantic information from acoustic data to help improve sentence compression task. The results of our experiments indicate the potential ability of the
aligned acoustic data when modeled to capture emphatic information in the text and embedded as an auxiliary task during multi-task learning. The faster approach to extract emphatic patterns proves to work well and generate improvements in related experiments. The weak supervision of the
emphatic data provides positive regularization to many of
the training process.
There remain many improvements in our work. For example, the automatic extraction can be better tuned according
to different tasks, and a more sophisticated multi-task training manner can be thus applied to make better use of the
regularization effects of the emphatic data.

As shown in Table 5, all models with emphatic data as
an auxiliary task outperform their comparative models in
all evaluation metrics with signiﬁcant performances. We
observed that in all three LSTM models, as an important indicator, the sentence-based accuracy (S.Acc) has
more than 10% improvements over the baselines. As a
result, the emphatic data coordinates with the GOOGLE
dataset’s aggressive compressions, imposing a positive
regularization during multi-task training.
• BROADCAST Datasets
The BROADCAST datasets are manually annotated by
three different annotators. We observe signiﬁcant improvements on nearly every metric on bi-directional

3427

Acknowledgments

Hoffmann, R.; Zhang, C.; Ling, X.; Zettlemoyer, L.; and
Weld, D. S. 2011. Knowledge-based weak supervision for information extraction of overlapping relations.
In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies-Volume 1, 541–550. Association for Computational Linguistics.
Hovy, D.; Anumanchipalli, G. K.; Parlikar, A.; Vaughn, C.;
Lammert, A. C.; Hovy, E. H.; and Black, A. W. 2013. Analysis and modeling of ”focus” in context. In INTERSPEECH.
Klerke, S.; Goldberg, Y.; and Søgaard, A. 2016. Improving sentence compression by learning to predict gaze. arXiv
preprint arXiv:1604.03357.
Ladd, D. R., and Morton, R. 1997. The perception of intonational emphasis: continuous or categorical? Journal of
Phonetics 25(3):313–342.
Levitan, S. I.; An, G.; Ma, M.; Levitan, R.; Rosenberg, A.;
and Hirschberg, J. 2016. Combining acoustic-prosodic, lexical, and phonotactic features for automatic deception detection. Interspeech 2016 2006–2010.
McDonald, R. T. 2006. Discriminative sentence compression with soft syntactic evidence. In EACL.
Mikolov, T., and Dean, J. 2013. Distributed representations
of words and phrases and their compositionality. Advances
in neural information processing systems.
Pon-Barry, H., and Shieber, S. 2009. The importance of
sub-utterance prosody in predicting level of certainty. In
Proceedings of Human Language Technologies: The 2009
Annual Conference of the North American Chapter of the
Association for Computational Linguistics, Companion Volume: Short Papers, 105–108. Association for Computational
Linguistics.
Søgaard, A., and Goldberg, Y. 2016. Deep multi-task learning with low level tasks supervised at lower layers. In The
54th Annual Meeting of the Association for Computational
Linguistics, 231.
Sun, X. 2002. Pitch accent prediction using ensemble machine learning. In INTERSPEECH.
Terken, J. 1991. Fundamental frequency and perceived
prominence of accented syllables. The Journal of the Acoustical Society of America 89(4):1768–1776.
Wang, W. Y.; Biadsy, F.; Rosenberg, A.; and Hirschberg,
J. 2013. Automatic detection of speaker state: Lexical,
prosodic, and phonetic approaches to level-of-interest and
intoxication classiﬁcation. Computer Speech & Language
27(1):168–189.
Zeiler, M. D. 2012. Adadelta: an adaptive learning rate
method. arXiv preprint arXiv:1212.5701.

We would like to thank the many referees of the previous
version of this paper for their extremely useful suggestions
and comments. Thanks to our colleagues for helpful discussions, and to anonymous reviewers for their suggestions for
improving the paper.

References
Barrett, M.; Agić, Ž.; and Søgaard, A. 2015. The dundee
treebank. In The 14th International Workshop on Treebanks
and Linguistic Theories (TLT 14).
Berg-Kirkpatrick, T.; Gillick, D.; and Klein, D. 2011. Jointly
learning to extract and compress. In Proceedings of the
49th Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies-Volume 1, 481–
490. Association for Computational Linguistics.
Bolinger, D. L. 1958. A theory of pitch accent in english.
Word 14(2-3):109–149.
Brenier, J. M.; Cer, D. M.; and Jurafsky, D. 2005. The detection of emphatic words using acoustic and lexical features.
In INTERSPEECH, 3297–3300.
Brenier, J. M. 2008. The Automatic Prediction of Prosodic
Prominence from Text. ProQuest.
Canning, Y.; Tait, J.; Archibald, J.; and Crawley, R. 2000.
Cohesive generation of syntactically simpliﬁed newspaper
text. In International Workshop on Text, Speech and Dialogue, 145–150. Springer.
Cao, H.; Benus, S.; Gur, R. C.; Verma, R.; and Nenkova,
A. 2014. Prosodic cues for emotion: analysis with discrete
characterization of intonation. Speech prosody 2014.
Chen, K.; Hasegawa-Johnson, M.; and Cohen, A. 2004.
An automatic prosody labeling system using ann-based
syntactic-prosodic model and gmm-based acoustic-prosodic
model. In Acoustics, Speech, and Signal Processing, 2004.
Proceedings.(ICASSP’04). IEEE International Conference
on, volume 1, I–509. IEEE.
Clarke, J., and Lapata, M. 2006. Constraint-based sentence
compression an integer programming approach. In Proceedings of the COLING/ACL on Main conference poster sessions, 144–151. Association for Computational Linguistics.
Ferguson, J.; Durrett, G.; and Klein, D. 2015. Disﬂuency
detection with a semi-markov model and prosodic features.
In Proc. NAACL HLT.
Filippova, K.; Alfonseca, E.; Colmenares, C.; Kaiser, L.; and
Vinyals, O. 2015. Sentence compression by deletion with
lstms.
Filippova, K. 2010. Multi-sentence compression: ﬁnding
shortest paths in word graphs. In Proceedings of the 23rd International Conference on Computational Linguistics, 322–
330. Association for Computational Linguistics.
Hernández-González, J.; Inza, I.; and Lozano, J. A. 2016.
Weak supervision and other non-standard classiﬁcation
problems: a taxonomy. Pattern Recognition Letters 69:49–
55.
Hochreiter, S., and Schmidhuber, J. 1997. Long short-term
memory. Neural computation 9(8):1735–1780.

3428

Detecting Communities by Sentiment Analysis
of Controversial Topics
Kangwon Seo1 , Rong Pan1(B) , and Aleksey Panasyuk2
1

School of Computing, Informatics, and Decision Systems Engineering,
Arizona State University, Tempe, AZ, USA
{kangwon.seo,rong.pan}@asu.edu
2
Air Force Research Lab, Rome, NY, USA
aleksey.panasyuk@us.af.mil

Abstract. Controversial topics, particularly political topics, often provoke very diﬀerent emotions among diﬀerent communities. By detecting
and analyzing communities formed around these controversial topics we
can paint a picture of how polarized a country is and how these communities evolved over time. In this research, we made use of Internet data
from Twitter, one of the most popular online social media sites, to identify a controversial topic of interest and the emotions expressed towards
the topic. Communities were formed based on Twitter users’ sentiments
towards the topic. In addition, the network structure of these communities was utilized to reveal those Twitter users that played important
roles in their respective communities.
Keywords: Topic modeling · Sentiment analysis · Twitter · Social network analysis

1

Introduction

Using social media to detect and monitor emerging phenomena has attracted
more and more attention from social scientists, political scientists, military
strategists, etc. With the ever-growing Internet connectivity, including mobile
connectivity, it is possible to collect millions of expressions from social media
users on a current event in an almost uninterruptable fashion. Twitter, for
example, is an ideal source for collecting information of public opinions and
public sentiments. With less than 140 characters, each tweet is more likely to
directly express a Twitter user’s feeling towards a particular event. Some recent
research eﬀorts have been carried out on statistical modeling of Twitter messages
(e.g., [3–5,13]); however, there is less research of using sentiment analysis for
online community detection and monitoring.
In this research we demonstrated a practical approach to social network
analysis that aimed at identifying distinct online communities and prominent
nodes in these communities. We analyzed Twitter data during the period of
the Egyptian Revolution of 2011 to identify communities around a controversial
c Springer International Publishing Switzerland 2016

K.S. Xu et al. (Eds.): SBP-BRiMS 2016, LNCS 9708, pp. 206–215, 2016.
DOI: 10.1007/978-3-319-39931-7 20

Detecting Communities by Sentiment Analysis of Controversial Topics

207

topic of interest. On January 25 of 2011 there was a large scale of demonstration in Egypt whose aim was of overthrowing the former Egyptian President,
Hosni Mubarak. The Twitter messages posted during this time period showed
people’s expressions and feelings without any censorship. Our data analysis was
conducted with the following procedure: First, we extracted several popular topics in this Twitter dataset, and then, the most interesting topic was chosen by
historical or political knowledge. Second, sentiment analysis was applied on the
tweets that were closely related to the selected topic and communities were constructed using the sentiment score. Lastly, we examined the network structure
of these communities to deduce the characteristics of each community. These
characteristics can reﬂect the polarization on the topic within a society and may
shed light on some early signs of potential physical conﬂicts.
1.1

Related Work

Traditional research on community detection in a network was based on the
graph theory (e.g., [8,16]). [25] deﬁned ﬁnding community in a social network
as “to identify a set of nodes such that they interact with each other more frequently than with those nodes outside the group”. These types of research often
involved solving optimization problems to discover subgraphs or clusters that
represent regions with higher density of edges than other portions of the graph.
[18] provided a survey on the performance characteristics of several community detection methods for social media. Some other recent work on this aspect
included [6,20]. Meanwhile, some researchers considered not only network structure but also social media contents. The analysis of these contents may provide
more insights about the polarized opinions expressed on speciﬁc topics in the
online society. [19] analyzed both opinions and social interactions to ﬁnd similar people and see how this similarity relates to their social interactions. They
built graphs based on the available opinions and social data and then used the
Infomap community detection algorithm [23] to identify community structures.
[5] described methods for predicting the political alignment of Twitter users
based on the content and structure of their political communication in the runup to the 2010 U.S. midterm elections. They used manually annotated data
set and trained it with several classiﬁcation features such as TF-IDF and hashtags obtained by content analysis, and cluster structures obtained by network
analysis.
The social media data related to the Arab’s revolutions had been of great
interest, as it played an important role in shaping the events in the region and
had been studied by several researchers. [3], for example, analyzed content and
network of Twitter to detect users who switched their polarity during the turmoil
in Egypt in 2013. They used a supervised method to classify the tweets into two
groups, pro-military intervention and anti-military intervention, then examined
if a user polarity has been swapped during some time period. [4] examined reply
and retweet networks of tweets related to the revolution in Egypt and the civil
war in Libya in order to identify interactions between the English and Arabic
language groups. [13] analyzed information ﬂows across diﬀerent types of users

208

K. Seo et al.

(activists, bloggers, journalists, mainstream media outlets, and other engaged
participants). They described the symbiotic relationship between media outlets
and individuals and the distinct roles between diﬀerent user types.
Theories in the social-psychological science indicate that sentiments and emotions of an individual often directly inﬂuence the individual’s behavior [1]. Political scientists have postulated hypotheses on how the emotion of a political group
may escalate or deescalate conﬂicts with opponents and how a political actor may
manipulate the general populace sentiment towards an event or an institution
to their own advantage [17]. From this research it is seen that the sentiments
expressed in tweets could be an important factor in predicting possible trends
of a political event. In addition, by analyzing opposing sentiments along with
diﬀerent ways of expression of emotions towards a political topic, we can group
tweeter users to diﬀerent communities. The characteristics of these communities,
such as its size, emotion intensity, etc., will provide a quantitative measure of
the political environment in a country.
1.2

Overview of Dataset

The Twitter dataset used in this research comes from a collection of tweets
about Egypt during 19 days, from February 1 to 19, 2011, provided by Army
Research Lab (ARL). It consists of over 950,000 tweets. Each data entry has
14 ﬁelds, which includes identiﬁcation number, language code, text message,
user name of who posted the tweet(“from user”), user name of whom the tweet
replies to(“to user”), date and time the tweet is posted(“created at”) and some
other metadata of each tweet. We extracted only tweets in English, which consist of 619,635 tweets. The type of a Twitter message can be categorized by
Original tweet, Retweet and Reply. Retweet messages include a string pattern
“RT @username” in their texts and Reply messages start from “@username”
and they also have a user name of whom the tweet replies to in the “to user”
ﬁelds. Our dataset consists of 75 % of Original tweets, 20 % of Replies and 5 %
of Retweets, approximately. The total number of users who posted at least one
tweet is 144,648. Meanwhile, the total number of users who received at least one
reply from the other user is 36,149. We found that majority of users posted only
one or two tweets. The number of tweets for each day varies between 20,000
to 40,000, and peaked around February 2, corresponds to Mubarak’s refusal to
resign and on February 11, corresponds to Mubarak’s resignation.

2

Methodology

A ﬂowchart of our data analysis process is shown in Fig. 1. It has two phases: In
Phase I, a controversial topic of interest is identiﬁed and irrelevant tweets to this
topic are ﬁltered out; in Phase II, we perform sentiment analysis and identify
communities.

Detecting Communities by Sentiment Analysis of Controversial Topics

209

Fig. 1. Summary of data analysis

2.1

Phase-I: Topic Extraction

The ﬁrst step of making sense of online community formation is to understand
what people are talking about. This requires topic modeling, a special area of text
mining. With a compiled document of online text messages, called corpus, we
are interested in ﬁnding multiple sets of keywords (topics) in the corpus that are
closely related to each other and the association of each message to any topic.
Latent Dirichlet Allocation (LDA) is a ﬂexible probabilistic generative model
for this topic-modeling problem [2]. It treats a document (or a message) as a
random mixture of latent topics and these latent topics have word distributions.
The parameter estimation of this model is typically carried out by the variational
Expectation Maximization (EM) method. After extracting topics from all tweets,
we are able to ﬁlter out irrelevant tweets based on the strength of association
with the topic of interest and conduct further analysis on the remaining tweets.
Figure 2 shows the graphical model representation of LDA, where M is the
number of documents in a corpus and N is the number of words in a document.
The boxes represent replicates. The generative model of LDA consist of the
following steps: (1) The term (word) distribution β for each topic is determined;
(2) The topic proportion θ for a document is chosen from a Dirichlet distribution,
Dirichlet(α); (3) Choose a topic Z from M ultinomial(θ) for each word in a
document; (4) Choose a word W from a multinomial probability distribution
conditioned on the topic Z. The LDA result shows which words are most likely
to appear for a topic and the proportions of topics for each document.

210

K. Seo et al.

Fig. 2. Graphical model representation of LDA [2]

One challenge of applying LDA on twitter messages comes from the reality
that a single twitter message is too short to be used as a document [27]. In
order to overcome this diﬃculty, a few suggestions have been proposed that we
categorize as two distinct approaches. The ﬁrst approach is to use aggregated
Twitter messages without modifying the standard LDA model. For examples,
[10] obtained a higher quality of learned model by training a topic model on
aggregated messages; [15] described various tweet aggregating schemes such as
author-based pooling, temporal-pooling and hashtag-based pooling, and they
suggested the hashtag-based aggregating scheme as the best one evaluated by
the ability of topics to reconstruct clusters and topic coherence. The second
approach is to modify the standard LDA model so that it can be adapted to
short documents. [27] assumed a single tweet is associated with a single topic and
proposed the Twitter-LDA model. The labeled LDA proposed by [21] is another
variation of LDA for microblog data. In this research, we used the ﬁrst approach
to aggregate messages and analyzed the data using an LDA implementation
available in R package: “topicmodels” [9]. Speciﬁcally, we randomly selected
50 % of all tweets to constitute the training dataset, and then aggregated them
by the date that they were posted (daily-based pooling) so that aggregated
documents have relatively similar lengths compared to cases of author-based or
hashtag-based aggregating schemes.
2.2

Phase-II: Sentiment-Based Community Detection

A basic task in sentiment analysis is to quantify the polarity in a given text whether the expressed opinion in the text is positive, negative, or neutral [12]. A
commonly used method is to compare the text with known lexicons of positive or
negative words. We used Bing Liu’s sentiment lexicon [11] as a dictionary of positive and negative words, which includes around 2,000 positive words and 4,800
negative words. This lexicon is useful for social media text because it includes
mis-spelling, morphological variants, slang and social media mark-up [26].
With the selected topics from twitter corpus, we assess the twitter user’s
attitude to these topics based on the words used and the way of expression. Subsequently, a similarity measurement based on the sentiment score between two
tweets can be obtained, which represent the likeness of the minds of two Twitter
users in terms of their attitudes to a common topic of interest. We will partition

Detecting Communities by Sentiment Analysis of Controversial Topics

211

these users to diﬀerent communities based on similarity measures. It needs to
be mentioned that our proposed community detection task is diﬀerent from the
other graph based community detection problems since it is utilizing sentiment
analysis unlike other approaches introduced in Sect. 1.1. We also exploit tweetretweet and tweet-reply networks to identify the users who play a role of opinion
leader in the community.

3

Data Analysis and Results

It is reasonable to assume that the topics of reply-tweets are closely related to
those of original tweets they are replying to and hence reply-tweets should be
considered with their original tweets. In our dataset, about 20 % of tweets are
reply-tweets and unfortunately the original tweet of a reply-tweet is unknown
in the dataset we have been working with. Instead, they only provide the user
names who posted the original tweets through “to user” attribute(or @username
in text). For these reasons we assume that a reply-tweet has been written for relatively recent post of an original tweet, so we add at-most 5 recent tweets, if any
exist, posted by @username before the posting of a reply-tweet. In our dataset
approximately 30 % of reply-tweets have at least one original tweet posted by
@username before the reply was posted and these original messages are added to
the reply-tweet. Meanwhile, URL, @username, punctuations and English stopwords are removed from all tweet messages. We also removed words with less
than 3 letters and words that appeared less than 10 times in the whole dataset.
We chose to ﬁt 30 topics based on perplexity [2] from 10-fold cross validation.
Table 1 shows the most heavily weighted words from some selected topics. It is
observed that most topics are dominated by a few similar terms such as “egypt”,
“tahrir”, “jan25” and “cairo”; while some terms are relatively unique, such as
“cbs”, “logan”, “lara” in topic 15. We also found that the document of each
day involves only one or two topics, and hence we could determine the topical
subjects of each day by looking at the highly ranked words in corresponding
topics.
Table 1. Extracted topics(part) by LDA
1

5

egypt

egypt egypt

cairo

cairo

bahrain

tahrir jan25

revolution can

8
cairo
google

12

15

23

28

30

egypt egypt egypt

cairo

egypt

egypt

cairo

libya

egypt

cairo

cairo

tahrir logan jan25

jan25

tahrir

will

tahrir

revolution jan25
like

cbs
lara

19

tahrir

video protests jan25 cairo

news

people

will

square news

tahrir

like

news

mubarak
tahrir

square people
news

square

The next step is to ﬁlter Twitter messages which may not be related to any
topic extracted from LDA. For this we ﬁt the individual tweets to LDA model

212

K. Seo et al.

once again given the term distributions of β for each topics provided by the
previously ﬁtted model, and obtain the topic proportions θ for each tweet. We
calculate the following measure to determine relevance to the topics from LDA
of each tweet.
n
log p(wi )
(1)
relevance(T ) = i=1
n
where T = {w1 , w2 , . . . , wn } is a tweet with n words, p(wi ) is the likelihood
of the word wi . If relevance(T ) of a tweet is too small, then it probably be a
message that can be ignored. We removed tweets which have relevance smaller
than the 5 % quantile of the normal distribution ﬁtted to relevance values.
With the reduced dataset, topic modeling can be repeated. This time, a
smaller number of topics can be given to LDA in order to zoom in the topics which have more exposure to the Twittersphere. We repeated the whole
process of topic modeling on the remaining Twitter data with 20 topics. We
then removed more irrelevant tweets using relevance. The topics obtained in
the second iteration are similar to the topics in the ﬁrst iteration. Therefore,
this iterative modeling/ﬁltering process is stopped.
Now we can choose a controversial topic that we are interested in. During this
time period in Egypt, the most important historical event was the resignation of
Mubarak. Among the 20 topics, Topic 7 and Topic 15 were thought to be highly
related to the resignation of Mubarak since those two topics allocate relatively
high probability on the term “resignation”, comparing to other topics. Then,
we extracted the tweets that had the largest weights on Topic 7 or Topic 15. In
addition, based on the fact that the resignation of Mubarak was announced at
4 pm on Feb. 11, we removed all tweets posted before this time point, resulting
in 27,494 tweets of interest.
The sentiment scores of each tweet were calculated by a simple voting between
the number of positive words and of negative words in the tweet [14]. Treating
the sentiment scores of {−1, 0, 1} as neutral, we used the tweets with scores
higher than 1 or lower than −1 to construct two opposite communities. We also
investigated some categorized emotional items in each community using the R
package “sentiment”. It classiﬁes the emotion of a set of texts using a naive
Bayes classiﬁer trained on Strapparava and Valitutti’s emotions lexicon [24]. It
shows the magnitudes of 6 emotional items - anger, surprise, sadness, joy, fear
and disgust. Table 2 summarizes the results of sentiment and emotion analysis.
From the sentiment analysis result, it shows that the size of positive community
(for Mubarak’s resignation) is much larger than the size of negative community
(against Mubarak’s resignation). In addition, the average sentiment scores of positive and negative communities show that the positive community has stronger
sentiment intensity. From the emotion analysis, the positive community has a
much higher score for joy and the negative community has a relatively higher
score for anger than the opposite community.
The network structure of tweets provides useful information of how Twitter
users communicate with each other. Using the R package “igraph” we construct

Detecting Communities by Sentiment Analysis of Controversial Topics

213

Table 2. Summary of sentiment & emotion analysis
Sentiment
users tweets score
Pos 2,978 3,831
Neg

525

635

Emotion
anger disgust fear joy

sadness surprise

2.36 1.74

3.12

2.11 6.82 2.15

3.46

−1.98 2.27

3.32

2.47 2.40 2.64

3.04

a network from all pairs of users in both communities who were linked by tweetretweet or tweet-reply. We ignore independent pairs of users which do not have
a link with other part of the network to focus on a dense region. Figure 3 shows
the graph, in which the network is dominated by users of the positive community
and we observe some sub-networks of radial shape which have one user playing
the role of center. It shows that the user “ghonim” is in the center of the network
and has the highest degrees of connection with 35, indicates that “ghonim” is
likely to be the opinion leader in this network. In fact we found that it was
the username of Wael Ghonim, who had led anti-government protesters during
Egypt revolution. The username of the U.S. president, “BarackObama”, ranks
the second highest degrees with 16, and it is no surprise based on a fact that
he expressed welcome at Mubarak’s decision to step down. Lastly “samiyusuf”,
which has 12◦ , is the username of Sami Yusuf, who is a British Muslim singersongwriter and released a song prompted by Egypt revolution.

BarackObama

AymanM

kalnaga

TEDxCairo

AJEnglish
ghonim

BeeboHenein
xDianaBiebs

Mamoudinijad
Gsquare86
etharkamal
monasosh
PrplDinosore
monaeltahawy embee LilyKhalil
NevineZaki

Sandmonkey
justinbieber
Che_Twittara

samiyusuf

Fig. 3. Tweet-retweet & tweet-reply network visualization: Blue dots represent users
in the positive community, red dots are users in the negative community, and gray
dots are neutral. User names (vertex label ) of Twitter users who have more than 5◦ of
connection are presented. (Color ﬁgure online)

214

4

K. Seo et al.

Conclusion

Although Twitter users do not represent the whole range of people in a country,
it is still useful for understanding the popular topics of the day and how people
express their feelings online. This knowledge is valuable for predicting some
future events such as violent civil conﬂicts. In this research we proposed a twophased approach that integrated statistical topic modeling, sentiment analysis
and network structure analysis for social network. We analyzed the Twitter data
in the period of Egypt revolution in 2011. We are able to group Twitter users to
diﬀerent communities based on their sentiment expressions on a topic of interest.
It is found that for this historical event, the resignation of Mubarak, online
society is dominated by sentiments supporting the resignation. In addition, we
provide the network structures of these communities and identify the center of
information ﬂow.
Some limitations of our study could be addressed in future research. First,
in this study we used a simple unsupervised way to evaluate the sentiment of a
message by the pre-speciﬁed lexicon. In recent years, however, more advanced
supervised learning methods such as SVM or maximum entropy are actively
used in the task (e.g., the SemEval shared task on Sentiment Analysis in
Twitter [22]). Secondly, other languages (e.g., Arabic) could be included in the
analysis by translating them into English. Finally, more elaborate sentiment
analysis requires several natural language processing techniques; e.g., part of
speech. By utilizing these techniques, we expect to achieve more reliable results
of sentiment orientation.
Acknowledgments. The authors thank Sue E. Kase and Liz Bowman at the Army
Research Lab for their help in getting access to the Egypt data. The views expressed
in this paper do not represent the views of the U.S. government.

References
1. Ajzen, I.: Attitudes, Personality, and Behavior. Dorsey Press, Chicago (1988)
2. Blei, D.M., Ng, A.Y., Jordan, M.I.: Latent dirichlet allocation. J. Mach. Learn.
Res. 3, 993–1022 (2003)
3. Borge-Holthoefer, J., Magdy, W., Darwish, K., Weber, I.: Content and network
dynamics behind egyptian political polarization on twitter. In: CSCW 2015, pp.
700–711. ACM (2015)
4. Bruns, A., Highﬁeld, T., Burgess, J.: The arab spring and social media audiences
english and arabic twitter users and their networks. Am. Behav. Sci. 57(7), 871–898
(2013)
5. Conover, M.D., Gonalves, B., Ratkiewicz, J., Flammini, A., Menczer, F.: Predicting
the political alignment of twitter users. In: PASSAT/SocialCom 2011, pp. 192–199.
IEEE (2011)
6. Du, N., Wu, B., Pei, X., Wang, B., Xu, L.: Community detection in large-scale
social networks. In: WebKDD/SNA-KDD 2007, pp. 16–25. ACM (2007)
7. Feinerer, I., Meyer, D., Hornik, K.: Text mining infrastructure in R. J. Stat. Softw.
25(5), 1–54 (2008)

Detecting Communities by Sentiment Analysis of Controversial Topics

215

8. Fortunato, S.: Community detection in graphs. Phys. Rep. 485, 75 (2010)
9. Grün, B., Hornik, K.: topicmodels: An R package for ﬁtting topic models. J. Stat.
Softw. 40(13), 1–30 (2011)
10. Hong, L., Davison, B.D.: Empirical study of topic modeling in twitter. In: SOMA
2010, pp. 80–88. ACM (2010)
11. Hu, M., Liu, B.: Mining and summarizing customer reviews. In: KDD 2004, pp.
168–177. ACM (2004)
12. Liu, B.: Sentiment Analysis and Opinion Mining. Morgan & Claypool Publishers,
San Rafael (2012)
13. Lotan, G., Graeﬀ, E., Ananny, M., Gaﬀney, D., Pearce, I.: The arab spring—the
revolutions were tweeted: information ﬂows during the 2011 tunisian and egyptian
revolutions. Int. J. Commun. 5, 31 (2011)
14. Marwick, B.: Discovery of emergent issues and controversies in anthropology using
text mining, topic modeling, and social network analysis of microblog content. In:
Data Mining Applications with R, p. 514. Academic Press, New York (2014)
15. Mehrotra, R., Sanner, S., Buntine, W., Xie, L.: Improving LDA topic models for
microblogs via tweet pooling and automatic labeling. In: SIGIR 2013, pp. 889–892.
ACM (2013)
16. Newman, M.E.J., Girvan, M.: Finding and evaluating community structure in networks. Phys. Rev. E69(2), 026113 (2004)
17. O’Brien, S., Shellman, S.: Eﬀects of Emotions on Dissident and Government Behavior. Strategic Analysis Enterprise (SAE) Inc., Williamsburg, VA (2013)
18. Papadopoulos, S., Kompatsiaris, Y., Vakali, A., Spyridonos, P.: Community detection in social media. Data Min. Knowl. Disc. 24(3), 515–554 (2012)
19. Parau, P., Stef, A., Lemnaru, C., Dinsoreanu, M., Potolea, R.: Using community
detection for sentiment analysis. In: ICCP 2013, pp. 51–54. IEEE (2013)
20. Qi, G.J., Aggarwal, C.C., Huang, T.: Community detection with edge content in
social media networks. In: ICDE 2012, pp. 534–545. IEEE (2012)
21. Ramage, D., Dumais, S.T., Liebling, D.J.: Characterizing microblogs with topic
models. In: ICWSM 2010, p. 1 (2010)
22. Rosenthal, S., Nakov, P., Kiritchenko, S., Mohammad, S.M., Ritter, A., Stoyanov,
V.: SemEval-2015 Task 10: Sentiment analysis in twitter. In: SemEval 2015, pp.
451–463 (2015)
23. Rosvall, M., Bergstrom, C.T.: Maps of random walks on complex networks reveal
community structure. PNAS 105(4), 1118–1123 (2008)
24. Strapparava, C., Valitutti, A.: WordNet aﬀect: an aﬀective extension of wordnet.
In: LREC, vol. 4, pp. 1083–1086 (2004)
25. Tang, L., Liu, H.: Community detection and mining in social media. Synth. Lect.
Data Min. Knowl. Discovery 2(1), 1–137 (2010)
26. Xie, R., Li, C.: Lexicon construction: A topic model approach. In: ICSAI 2012, pp.
2299–2303. IEEE (2012)
27. Zhao, W.X., Jiang, J., Weng, J., He, J., Lim, E.-P., Yan, H., Li, X.: Comparing
twitter and traditional media using topic models. In: Clough, P., Foley, C., Gurrin,
C., Jones, G.J.F., Kraaij, W., Lee, H., Mudoch, V. (eds.) ECIR 2011. LNCS, vol.
6611, pp. 338–349. Springer, Heidelberg (2011)

On Kernel Information Propagation for Tag
Clustering in Social Annotation Systems
Guandong Xu1,3 , Yu Zong2,4, , Rong Pan3 , Peter Dolog3 , and Ping Jin2
1

2

School of Engineering & Science, Victoria University, Vic 8001, Australia
Department of Information and Engineering, West Anhui University, Liuan, China
3
Department of Computer Science, Aalborg University, Dk-9220, Denmark
4
Department of Computer Science and Technology, University of Science and
Technology of China, 230036, China

Abstract. In social annotation systems, users label digital resources by
using tags which are freely chosen textual descriptors. Tags are used to
index, annotate and retrieve resource as an additional metadata of resource. Poor retrieval performance remains a major challenge of most
social annotation systems resulting from the severe problems of ambiguity, redundancy and less semantic nature of tags. Clustering method is
a useful approach to handle these problems in the social annotation systems. In this paper, we propose a novel clustering algorithm named kernel
information propagation for tag clustering. This approach makes use of
the kernel density estimation of the KNN neighbor directed graph as a
start to reveal the prestige rank of tags in tagging data. The random
walk with restart algorithm is then employed to determine the center
points of tag clusters. The main strength of the proposed approach is
the capability of partitioning tags from the perspective of tag prestige
rank rather than the intuitive similarity calculation itself. Experimental
studies on three real world datasets demonstrate the eﬀectiveness and
superiority of the proposed method.

1

Introduction

In past years the emergence of Web 2.0 applications has created a new era
for sharing and organizing documents in online social communities. The shared
documents could range diversely from the social bookmarks Del.icio.us 1 to scientiﬁc publications on CiteUlike 2 . One of the common characteristic these kinds
of document possessing is the phenomenon of Folksonomy - users choose their
own free style terms (i.e. tags) to annotate various documents indicating their
own perceptions or conceptual judgments on these resources for better indexing
and annotation. In other words, Tag, as one kind of speciﬁc lexical information
that is user-generated metadata with uncontrolled vocabulary, plays a crucial
role in such social collaborative systems.

1
2

Corresponding Author.
www.delicious.com
www.citeulike.org

A. König et al. (Eds.): KES 2011, Part II, LNAI 6882, pp. 505–514, 2011.
c Springer-Verlag Berlin Heidelberg 2011


506

G. Xu et al.

Recently tagging has been widely used in recommender systems for many
applications [3,7,14]. The common usage of tags in these systems is to add the
tagging attribute as an additional feature to re-model users or resources over
the tag vector space, and in turn, making tag-based recommendation or personalized recommendation. However, as the tags are of syntactic nature, in a free
style and do not reﬂect suﬃcient semantics, the problems of redundancy, ambiguity and less semantics of tags are often incurred in all kinds of social tagging
recommender systems. In order to deal with these diﬃculties, recently clustering
method has been introduced into social tagging recommender systems to ﬁnd
meaningful topic information conveyed by tag aggregates. The aim of tag clustering is to reveal the coherence of tags from the perspective of how resources
are annotated and how users annotate in collaborative annotations.
In the context of tag clustering, most of the researches on tagging clustering
are directly using the traditional clustering algorithms such as K-means [11] or
Hierarchical Agglomerative Clustering [12] on tag data, which possess the inherent drawbacks, such as the sensitivity of initial values and high computational
cost etc. On the other hand, various tags used in the tagging data apparently
have diﬀerent importances in tag groups due to the semantic or domain topic
tendency of tags. Bearing this intrinsic phenomena in mind, we propose to make
use of the individual signiﬁcance of each tag in tagging data for tag clustering.
The basic idea behind our approach is that the propagated centrality (or prestige) degree of one tag derived from the tag neighborhood graph does reveal the
importance that is contributed to the tag cluster forming. In particular, we devise a new Kernel Information Propagation Tag Clustering (KIPTC) algorithm
to obtain the centrality (i.e. prestige) degree of each tag via a random walk over
the graph approach. In order to evaluate the eﬀectiveness of the proposed approach, we conduct experiments on real tagging datasets. The contributions of
our paper are as follows:
– We address the tag clustering problem in social annotation systems via a
graph-based optimization approach.
– We propose a new Kernel Information Propagation Tag Clustering algorithm, in which we deﬁne the Prestige Rank to capture the importance of
tags in the KNN neighbor directed graph, and devise an iterative updating
mechanism based on random walk to identify the global prestige rank.
– We conduct comparative experiments on three real world datasets to evaluate
the eﬀectiveness of the proposed algorithm.
The remainder of this paper is organized as follows. We review the related work
in Section 2 and introduce the preliminaries in Section 3. The details of KIPTC
algorithm are discussed in Section 4. Experimental evaluation results are reported in Section 5. Section 6 concludes this paper and outlines the future work.

2

Related Work

In past years, many studies have been carried out on tagging clustering. [12]
demonstrates how tag clusters serving as coherent topics can aid in the social

On Kernel Information Propagation for Tag Clustering

507

recommendation of search and navigation. In [6] topic relevant partitions are
created by clustering resources rather than tags. By clustering resources, it
improves recommendations by distinguishing between alternative meanings of
query. While in [1], clusters of resources are shown to improve recommendation
by categorizing the resources into topic domains. A framework named Semantic
Tag Clustering Search, which is able to cope with the syntactic and semantic
tag variations is proposed in [2]. P. Lehwark et al. use Emergent-Self-OrganizingMaps (ESOM) and U-Map techniques to visualize and cluster tagged data and
discover emergent structures in collections of music [8].
Kernel density estimate method is a widely used statistical approach for nonparameter density estimation in high dimensional space. For example, [9] has
used it in capturing the local characteristic and density distribution in sparse
high dimensional space. In this paper, our approach is originated from the concept of kernel information, and extends it to reveal the centrality of tag in tag aggregates, which is fundamentally diﬀerent from the above clustering approaches.

3
3.1

Preliminaries
Social Tagging System Model

In this paper, our work is to deal with the tagging data. A typical social tagging
system has three types of objects, users, tags and resources which are interrelated
with one another. Social tagging data can be viewed as a set of triples [5,4], with
each (u, r, t) representing a user u annotates tag t to resource r. Therefore a social
tagging system can be described as a four-tuple, where there exists a set of users,
U ; a set of tags, T ; a set of resources, R; and a set of annotations, AN . We denote
the tagging data in the social tagging system as D i.e., D =< U, R, T, AN >.
The annotations are represented as a set of triples containing user u, tag t and
resource r: AN ⊆ u, r, t : u ∈ U, r ∈ R, t ∈ T . Therefore a social tagging system
can be viewed as a tripartite hyper-graph [10] with users, tags and resources
represented as nodes and the annotations represented as hyper-edges connecting
users, resources and tags.
3.2

A Working Example of Tag Clustering

In the above model, the tag is usually in a very high dimension due to the free
style of tag texts, which results in the problem of redundancy and ambiguity; in
turn, bringing in the diﬃculty in tag computing such as the similarity calculation of tag vectors. Therefore, clustering is often employed to capture the topical
aggregates of tags, i.e. one kind of structural semantics of tags. In real applications, to fulﬁll this we usually decompose the tripartite graph of social tagging
data to form a resource-tag matrix by accumulating the frequency of each tag
in the resource vector along users. In this expression, each tag is described by
a set of resources, to which this tag has been assigned, i.e., ti = (wi1 , · · · , wim ),
where wik denotes the the occurrence frequency on resource rk dimension of tag
ti . Thus, the similarity between any two tags is deﬁned as follow:

508

G. Xu et al.

Definition 1. Given two tags ti = (wi1 , · · · , wim ) and tj = (wj1 , · · · , wjm ), the
similarity is defined as the Cosine function of angle between two vectors of ti
and tj :
ti · tj
Sim (ti , tj ) =
(1)
ti  · tj 
Upon the mutual tag similarity is determined, various clustering algorithms could
be applied to partition the tags. Fig. 1 gives a simple working example to show
how ﬁve tags are assigned into two groups by using diﬀerent clustering strategies
(in red or black dashed circles). We will use this example to demonstrate our
approach in the later section.
tag(resource,resource,resource,resource,...)

resource2

tag2

resource3

tag3

resource4

tag4

resource5

tag5

resources
1

0.41

1 1 0 0 0

0.41

1

0

0.50

0 1 1 0 0

tag1

tags

1 0 0 1 1
tags

tag1
tags

resource1

0 0 0 1 0
0 1 0 1 0
Tag-Resource Matrix

0.58

0

0
0.50
1
0

0.58

0.41

0

0.50

0

0.50

1

0.41 0.50 0.50 0.71

0.71
1

Tags Similarity Matrix

Cluster 0.41
tag2

0.58
0.41
0.250

tag4
0.71

0.50
0.50

Cluster

tag5
Topic tag3
Cluster Extraction

Fig. 1. A Working Example of Tag Clustering

4

Kernel Information Propagation for Tag Clustering
Algorithm

So far most tag clustering approaches are mainly dependent on the co-occurrence
matrix of tagging data, e.g. the occurrence matrix of resource-tag, to partition
tags into various groups. Diﬀerent from these approaches, our method is instead,
to reveal the global prestige degree of each tag via a graph-based partition manner originated from the calculation of kernel density distribution. In the follow
section, we discuss the details of our proposed tag clustering algorithm based on
kernel information propagation.
4.1

KNN Directed Graph and Kernel Density

According to Deﬁnition 1, a similarity matrix S could be constructed from the
tagging data, to indicate the aﬃnity of tags. From S, we can ﬁnd KNN neighbors
of each tag and then create a KNN directed graph G, i.e. G =< V, E >, where
V is the node set of tags and E is the directed edge set between each pair of
tags, < p, q >∈ E denotes that tag q is a KNN-neighbor of tag p.
Fig.2 shows an example of a part of graph G with two-fold relationships. In one
fold, the central black point P has ﬁve KNN neighboring nodes denoted by heavy

On Kernel Information Propagation for Tag Clustering

509

black circle with arches pointing from P to them, reﬂecting the neighborhood
relationships of P . In the other fold, P is the KNN neighbor of each light circle
nodes with arches directing from these nodes to P . In this manner, a KNN
directed graph G is constructed and its adjacency matrix A of G is deﬁned as:

Fig. 2. An Example of KNN Neighbor Directed Graph

Definition 2. Given a KNN directed graph G, its adjacency matrix is defined
as A, where A(p, q) = 1, if the directed arch < p, q > exists, and A(p, q) = 0,
otherwise.
The kernel density estimate method [9] has mainly been used in capturing the
local characteristics and density distribution in high dimensional space. In the
context of KNN neighbor directed graph, particularly the kernel density function
indicates the density distribution of similarity function. Moreover the estimated
KNN kernel density information of each node represents the local centrality
degree of the node in a possible cluster and the number of arches pointed to the
node reﬂects the “respectful” degree of the node contributed by its neighbors.
Bearing this idea in mind, in this paper, we intend to adopt the concept of
kernel density of each node, and then deﬁne the measure of Local Prestige (LP)
to reﬂect the local centrality information of each node in the KNN neighbor
directed graph.
Definition 3. Given a node p ∈ G and a backlink node set B(p), of which p is
in the KNN neighbors, the Local Prestige (Centrality) of p is defined as the sum
of the KNN kernel density of B(p):
LP (p) =


q∈B(p)

f (q)

(2)

where f (q) denotes the estimated KNN kernel density of node q.
According to Deﬁnition 3, we can see that LP (p) is actually the aggregated
kernel density of supporting nodes which choose node p as their KNN neighbors.
Intuitively, the higher value of LP (p) the more centrality the node p possesses
within the cluster and the more likely the node p becomes the cores of the cluster.
Fig.3(a) and (b) illustrate the initial kernel density values and their local prestige
degrees of ﬁve nodes in the working example.

510

G. Xu et al.

(a) Kernel Density

(b) Local Prestige

Fig. 3. The kernel Information and Local Prestige of Tags in KNN Neighbor Directed
Graph

4.2

Kernel Information Propagation for Tag Clustering Algorithm

LP captures the local centrality information of each node in the KNN neighbor
directed graph G. Apparently, the prestige score of one node is determined by the
number of nodes pointing to this node and their local prestige scores. Meanwhile,
known from the theory of directed graph, the prestige of a node is divided evenly
and is propagated to other nodes that are pointed by it. Hence the node p
gets a boost of its prestige from the nodes that point to p, i.e. the iterative
prestige propagation along the directed arches. Inspired by this thought, we
envision a new tag clustering algorithm based on Kernel Information Propagation
(KIPTC). In particular, we ﬁrst adopt the Random Walk with Restart (RWR)
algorithm [13] to deal with the prestige propagation for global Prestige Rank,
and then make use of the global Prestige Rank to form tag clusters. The whole
process consists of two main stages. In the stage of global prestige propagation,
via the random walk along the constructed KNN neighbor directed graph, the
global Prestige Rank is calculated as follows:
Definition 4. Given P Ri indicating the Prestige Rank scores of all nodes at ith
iteration and M denoting the transition matrix of the KNN neighbor directed
graph, the updated Prestige Rank at (i+1)th step, P Ri+1 , is given by:
P Ri+1 = α · M · P Ri + (1 − α)

(3)

where α is the damping factor which controls the convergence of the algorithm,
and the element of M is determined by A(q, p)/N (q) and N (q) is the total
number of outgoing arches of the node q.
The execution of Prestige Rank propagation is repeated until it converges to
a stable status. As for the above example shown in Fig.1, the Prestige Rank
scores of ﬁve nodes are 0.454, 0.114, 0.108, 0.450 and 0.604, respectively,
with α = 0.85. From these scores, we can further infer that the node #5 has the
highest prestige score amongst all nodes, indicating the highest appropriateness
of being a core within one cluster.
After the global prestige scores are calculated, we then utilize them to determine the cores of clusters and include other tag members, which are closely
next to the core tag. As the whole KNN neighbor directed graph constitutes a
number of connected subgraphs, we then conduct the graph traversal operation

On Kernel Information Propagation for Tag Clustering

511

Algorithm 1. Kernel Information Propagation for Tag Clustering

1
2
3
4
5
6
7
8
9
10
11
12
13

Input: The tag set V and the neighborhood parameter K
Output: The cluster result C
Generate the tag similarity matrix S based on Eq.(1), and calculate the kernel
density of each tag;
Construct a KNN directed graph based on Deﬁnition 2;
Calculate the Local Prestige LP of each tag using Eq.(2);
while Not all nodes are calculated do
update Prestige Rank (PR) via RWR by using Eq.(3);
Sort the PR scores in a descending order;
end
for each unvisited tag v, v ∈ V do
Select the tag v with the highest P R score;
Form Cv = DF S(G, v), where Cv denotes a cluster with v as the core;
C ← C ∪ Cv ;
end
Return C.

to identify the cluster member nodes. The process to generating the clusters is
divided as: (1) we select a tag v, v ∈ V , with the highest PR as the starting
core of a cluster; (2) we use the Depth First Search (DFS) method to ﬁnd the
corresponding cluster members until all nodes within the subgraph containing v
are completely searched. After that, the algorithm turns to locate a new starting
center of another cluster. Steps (1) and (2) are iteratively executed until all the
nodes in V are assigned to corresponding cluster. Looking back to the working
example, by selecting node #5 as the ﬁrst cluster core, we include node #1 and
#4 as its cluster members via DFS. Then we turn to choose node #2 as the core
of the second cluster and add node #1 as its member, eventually resulting in
two clusters of C1 = 1, 4, 5 and C2 = 2, 3. Below Algorithm 1 gives the pseudo
codes of KIPTC algorithm.

5

Experimental Evaluations

5.1

Datasets and Evaluation Metrics

To evaluate our approach, we conduct preliminary experiments on three real
world datasets: MedWorm3 , MovieLens4 and DMOZ5 . We perform the experiments using an Intel Core 2 Duo CPU (2.4GHz) workstation with 4G memory,
running windows XP. All the algorithms are implemented in Matlab 7.0.
The statistical results of these three datasets are listed in Table 1. These
three datasets are pre-processed to ﬁlter out some noisy and extremely sparse
data subjects to increase the data quality.
3
4
5

http://www.medworm.com/
http://www.movielens.org/
http://www.michael-noll.com/dmoz100k06/

512

G. Xu et al.
Table 1. Statistics of Experimental Datasets
Property
MedWorm MovieLens Dmoz
Number of users
949
4,009
5,016
Number of resources
261,501
7,601
13,771
Number of tags
13,507
16,529 25,311
Total entries
1,571,080
95,580 97,587
Average tags per user
132
11
123
Average tags per resource
5
9
11

In our study, we expect to assign the similar tags serving for the same topic
into the same tag cluster. Hence we assume that tag clusters with good quality
are composed by lots of similar tags, and the tags in diﬀerent tag clusters are
dissimilar to each other. In particular, we deﬁne Similarity, Dissimilarity and
Silhouette metric to validate our method.
Definition 5. Given a tag cluster set C = {C1 , · · · , C|C| }, the Similarity and
|C| 2·Sim(ti ,tj )
1
Dissimilarity are defined as: Similarity(C) = |C|
k=1 |Ck |·(|Ck |−1) , ti , tj ∈ Ck ;
Dissim(k)
1 |C|
and Dissimilarity(C) = |C| k=1 |Ck |·(|T |−|Ck |) ,
|C|
where Dissim(k) =
/ Ck , and |T | is the total
k=1 Sim(ti , tj ), ti ∈ Ck , tj ∈
number of tags.
Definition 6. Given the Similarity and Dissimilarity of cluster C, its Silhouette
measure is defined as: Silhouette (C) = 1 − Dissimilarity(C)
Similarity(C)
Obviously, the higher the silhouette the better the clustering quality is.
5.2

Experiments and Discussions

To evaluate the impact of K (i.e., neighborhood size) selection on clustering,
we conduct experiments to evaluate the quality of tag clusters with varying K.
Table 2 gives the comparisons in terms of Silhouette on Medworm, MovieLens
and Dmoz datasets, respectively. From Table 2, we can ﬁrst ﬁnd that the cluster
results on three datasets are very close under diﬀerent K settings. This observation validates that the selection of K does not impose a signiﬁcant impact
on clustering quality. Interestingly, the clustering results derived from Medworm
look better than those of Movielens and Dmoz, which might be due to the fact
that tags used in Medworm dataset is focused on a more specialized medical domain, while the domain topics of MovieLens and Delicious datasets span more
diversely. In order to evaluate the eﬀectiveness of the proposed method, we
also implement the traditional clustering algorithm, i.e., K-means on these three
real world datasets with the cluster number being set to be the same as that of
KIPTC obtained in DFS. The experimental results are shown in Table 3. According to the comparison results of Table 3, we can ﬁnd that the quality of clustering
results obtained by KIPTC is consistently better than that of K-means in terms
of Silhouette measure. This ﬁnding concludes that KIPTC algorithm has shown

On Kernel Information Propagation for Tag Clustering

513

Table 2. Clustering Comparisons of Silhouette with Varying K

K=4
K=8
K=12

Medworm Movielens Dmoz
0.9752
0.4809 0.7497
0.9757
0.4908 0.7364
0.9749
0.4746 0.7476

Table 3. Comparisons of Various Clustering on Silhouette

K-means
KIPTC

Medworm Movielens Dmoz
0.9261
0.1414 0.1912
0.975
0.5377 0.7522

the capability of ﬁnding better clustering results than K-means. The reason for
this is probably due to that our proposed algorithm is able to, not only capture
the local prestige information of tags in the KNN neighbor directed graph G, but
also obtain the global centrality of tags via a prestige information propagation.
By making use of the global prestige rank, the KIPTS algorithm can eﬀectively
locate the more appropriate cluster cores and form the meaningful tag clusters
accordingly. In contrast, the K-means clustering algorithm mainly relies on the
similarity matrix and demonstrates the poor clustering results, especially with
the diverse datasets, e.g., MovieLens and Dmoz.

6

Conclusion and Future Work

Tag clustering is a useful method to ﬁnd out tag clusters embedded in tagging
data and has a potential in improving the performance of tag-based recommender
systems. In this paper, we propose a novel tag clustering based on kernel information propagation via random walk on graph. We ﬁrst use the KNN neighbor
directed graph and Kernel density estimate method to ﬁnd out the local prestige
information of each tag, and then employ the random walk with restart algorithm to iteratively propagate the prestige rank until convergence. At last, we
use the prestige scores of tags to locate the appropriate cluster core and conduct the graph traversal search to include cluster members. Experimental results
conducted on three real world datasets have demonstrated the eﬀectiveness and
superiority of the proposed method in comparison to the traditional K-means
clustering approach. The future work can be carried out along the direction of
comparisons to more state-of-the-art clustering algorithms.
Acknowledgement. This work has been partially supported by EU FP7 ICT
project M-Eco: Medical Ecosystem Personalized Event-Based Surveillance (No.
247829); grants from Natural Science Foundation of China (No. 60775037), the
Key Program of National Natural Science Foundation of China (No. 60933013),
and Research Fund for the Doctoral Program of Higher Education of China
(20093402110017).

514

G. Xu et al.

References
1. Chen, H., Dumais, S.: Bringing order to the web: Automatically categorizing search
results. In: Proceedings of the SIGCHI Conference on Human Factors in Computing
Systems, pp. 145–152. ACM, New York (2000)
2. van Dam, J., Vandic, D., Hogenboom, F., Frasincar, F.: Searching and browsing
tag spaces using the semantic tag clustering search framework. In: IEEE Fourth
International Conference on Semantic Computing (ICSC), pp. 436–439. IEEE, Los
Alamitos (2010)
3. Durao, F., Dolog, P.: Extending a hybrid tag-based recommender system with personalization. In: SAC 2010: Proceedings of the 2010 ACM Symposium on Applied
Computing, pp. 1723–1727. ACM, New York (2010)
4. Guan, Z., Bu, J., Mei, Q., Chen, C., Wang, C.: Personalized tag recommendation
using graph-based ranking on multi-type interrelated objects. In: Proceedings of
the 32nd International ACM SIGIR Conference on Research and Development in
Information Retrieval, pp. 540–547. ACM, New York (2009)
5. Guan, Z., Wang, C., Bu, J., Chen, C., Yang, K., Cai, D., He, X.: Document recommendation in social tagging services. In: Proceedings of the 19th International
Conference on World Wide Web, pp. 391–400. ACM, New York (2010)
6. Hayes, C., Avesani, P.: Using tags and clustering to identify topic-relevant blogs.
In: International Conference on Weblogs and Social Media (March 2007)
7. Jäschke, R., Marinho, L., Hotho, A., Schmidt-Thieme, L., Stumme, G.: Tag recommendations in folksonomies. In: Kok, J.N., Koronacki, J., Lopez de Mantaras,
R., Matwin, S., Mladenič, D., Skowron, A. (eds.) PKDD 2007. LNCS (LNAI),
vol. 4702, pp. 506–514. Springer, Heidelberg (2007)
8. Lehwark, P., Risi, S., Ultsch, A.: Visualization and clustering of tagged music data.
Data Analysis, Machine Learning and Applications, 673–680 (2008)
9. Liu, H., Laﬀerty, J., Wasserman, L.: Sparse nonparametric density estimation in
high dimensions using the rodeo. In: Proceedings of the Eleventh International
Conference on Artiﬁcial Intelligence and Statistics, San Juan, Puerto Rico (2007)
10. Mika, P.: Ontologies are us: A uniﬁed model of social networks and semantics.
In: Gil, Y., Motta, E., Benjamins, V.R., Musen, M.A. (eds.) ISWC 2005. LNCS,
vol. 3729, pp. 522–536. Springer, Heidelberg (2005)
11. Noll, M.G., Meinel, C.: Web search personalization via social bookmarking and
tagging. In: Aberer, K., Choi, K.-S., Noy, N., Allemang, D., Lee, K.-I., Nixon,
L.J.B., Golbeck, J., Mika, P., Maynard, D., Mizoguchi, R., Schreiber, G., CudréMauroux, P. (eds.) ASWC 2007 and ISWC 2007. LNCS, vol. 4825, pp. 367–380.
Springer, Heidelberg (2007)
12. Shepitsen, A., Gemmell, J., Mobasher, B., Burke, R.: Personalized recommendation
in social tagging systems using hierarchical clustering. In: Proceedings of the 2008
ACM Conference on Recommender systems, pp. 259–266. ACM, New York (2008)
13. Sun, J., Qu, H., Chakrabarti, D., Faloutsos, C.: Neighborhood formation and
anomaly detection in bipartite graphs. In: ICDM, pp. 418–425 (2005)
14. Tso-Sutter, K.H.L., Marinho, L.B., Schmidt-Thieme, L.: Tag-aware recommender
systems by fusion of collaborative ﬁltering algorithms. In: SAC 2008: Proceedings
of the 2008 ACM Symposium on Applied Computing. pp. 1995–1999. ACM, New
York (2008)

IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS, VOL. 32, NO. 4, APRIL 2014

719

Probe and Adapt: Rate Adaptation for HTTP Video
Streaming At Scale
Zhi Li, Xiaoqing Zhu, Member, IEEE, Joshua Gahm, Rong Pan, Hao Hu, Student Member, IEEE,
Ali C. Begen, Senior Member, IEEE, and David Oran

Index Terms—CDN, DASH, HAS, HTTP Adaptive Streaming,
TCP, Video.

I. I NTRODUCTION
VER the past few years, we have witnessed a major
technology convergence for Internet video streaming towards a new paradigm named HTTP-based adaptive streaming
(HAS). Since its inception in 2007 by Move Networks [1],
HAS has been quickly adopted by major vendors and service
providers. Today, HAS is employed for over-the-top video
delivery by many major media content providers. A recent
report by Cisco [7] predicts that video will constitute more
than 90% of the total Internet traffic by 2014. Therefore, HAS
may become a predominant form of Internet traffic in just a
few years.

O

Manuscript received May 1, 2013; revised August 29, 2013.
The authors are with Cisco Systems, San Jose, CA USA (email:
zhil2@cisco.com,
xiaoqzhu@cisco.com,
jgahm@cisco.com,
ropan@cisco.com, hahu2@cisco.com, abegen@cisco.com, oran@cisco.com,
leeoz@alumni.stanford.edu).
Digital Object Identifier 10.1109/JSAC.2014.140405.

Fetched Bitrate Aggregated over 36 Streams
Fetched (Mbps)

120
100
80
60
200

300

400

500

600

700
800
Time (Sec)

900

1000

1100

1200

Fetched Bitrate of Individual Streams (Zoom In)
6
Fetched (Mbps)

Abstract—Today, the technology for video streaming over
the Internet is converging towards a paradigm named HTTPbased adaptive streaming (HAS), which brings two new features.
First, by using HTTP/TCP, it leverages network-friendly TCP
to achieve both firewall/NAT traversal and bandwidth sharing.
Second, by pre-encoding and storing the video in a number of
discrete rate levels, it introduces video bitrate adaptivity in a
scalable way so that the video encoding is excluded from the
closed-loop adaptation. A conventional wisdom in HAS design
is that since the TCP throughput observed by a client would
indicate the available network bandwidth, it could be used as a
reliable reference for video bitrate selection.
We argue that this is no longer true when HAS becomes a
substantial fraction of the total network traffic. We show that
when multiple HAS clients compete at a network bottleneck, the
discrete nature of the video bitrates results in difficulty for a
client to correctly perceive its fair-share bandwidth. Through
analysis and test bed experiments, we demonstrate that this
fundamental limitation leads to video bitrate oscillation and
other undesirable behaviors that negatively impact the video
viewing experience. We therefore argue that it is necessary
to design at the application layer using a “probe and adapt”
principle for video bitrate adaptation (where “probe” refers to
trial increment of the data rate, instead of sending auxiliary
piggybacking traffic), which is akin, but also orthogonal to the
transport-layer TCP congestion control. We present PANDA –
a client-side rate adaptation algorithm for HAS – as a practical
embodiment of this principle. Our test bed results show that
compared to conventional algorithms, PANDA is able to reduce
the instability of video bitrate selection by over 75% without
increasing the risk of buffer underrun.

4
2
0
400

420

440

460

480

500
520
Time (Sec)

540

560

580

600

Fig. 1. Oscillation of video bitrate when 36 Microsoft Smooth clients compete
at a 100-Mbps link. For more detailed experimental setup, refer to §VII-B.

In contrast to conventional RTP/UDP-based video streaming, HAS uses HTTP/TCP – the protocol stack traditionally
used for Web traffic. In HAS, a video stream is chopped
into short segments of a few seconds each. Each segment is
pre-encoded and stored at a server in a number of versions,
each with a distinct video bitrate, resolution and/or quality.
After obtaining a manifest file with necessary information, a
client downloads the segments sequentially using plain HTTP
GETs, estimates the network conditions, and selects the video
bitrate of the next segment on-the-fly. A conventional wisdom
assumes that since the bandwidth sharing of HAS is dictated
by TCP, the problem of video bitrate selection can be resolved
straightforwardly. A simple rule of thumb is to approximately
match the video bitrate to the observed TCP throughput.
A. Emerging Issues
A major trend in HAS use cases is its large-scale deployment in managed networks by service providers, which
typically results in aggregating multiple HAS streams in the
aggregation/core network. For example, an important scenario
is that within a single household or a neighborhood, several
HAS flows belonging to one DOCSIS1 bonding group compete for bandwidth. Similarly, in the unmanaged wide-area
Internet, as HAS is growing to become a substantial fraction
of the total traffic, it will become more and more common to
have multiple HAS streams compete for available bandwidth
at a bottleneck link.
1 Data

over cable service interface specification.

c 2014 IEEE
0733-8716/14/$31.00 

720

IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS, VOL. 32, NO. 4, APRIL 2014

Fetched Bitrate Aggregated over 8 Streams

Fetched Bitrate Aggregated over 6 Streams
14
Fetched (Mbps)

Fetched (Mbps)

20
15
10
5
300

350

400

450
Time (Sec)

500

550

12
10
8
400

600

Fetched Bitrate of Individual Streams

550
Time (Sec)

600

650

700

650

700

4
Fetched (Mbps)

Fetched (Mbps)

500

Fetched Bitrate of Individual Streams

6
4
2
0
300

450

350

400

450
Time (Sec)

500

550

600

Fig. 2. Video bitrate fluctuates when 8 Apple HLS clients compete at a
24-Mbps link. Accompanied with the fluctuation are frequent video playout
stalls, which are not shown in the plot. For more detailed experimental setup,
refer to §VII-B.

While a simple rate adaptation algorithm might work fairly
well for the case where a single HAS stream operates alone
or shares bandwidth with non-HAS traffic, recent studies
[14], [3] have reported undesirable behaviors when multiple
HAS streams compete for bandwidth at a bottleneck link. For
example, while studies have suggested that significant video
quality variation over time is undesirable for a viewer’s quality
of experience [20], in [14] the authors reported unstable video
bitrate selection and unfair bandwidth sharing among three
Microsoft Smooth Streaming clients (or Smooth clients for
short) sharing a 3-Mbps link. In our own test bed experiments
(see Fig. 1), we observed significant and regular video bitrate
oscillation when multiple Smooth clients share a bottleneck
link. We also found that oscillation behavior persists under
a wide range of parameter settings, including the number of
players, link bandwidth, start time of clients, heterogeneous
RTTs, random early detection (RED) queueing parameters, the
use of weight fair queueing (WFQ), the presence of moderate
web-like cross traffic, etc. When extending our tests to other
types of clients, we found somewhat different behaviors –
neither the Apple HLS clients nor the Adobe HDS clients
show oscillation on the aggregate fetched rate as regular as
the Smooth clients, yet individual clients exhibit video bitrate
fluctuations and frequent video playout stalls (see Fig. 2 and
Fig. 3, respectively).
Our study shows that these rate oscillation and instability behaviors are not incidental – they are symptoms of a
much more fundamental limitation of the conventional HAS
rate adaptation algorithms, in which the TCP downloading
throughput observed by a client is directly taken as its fair
share of the network bandwidth. This fundamental problem
would also impact a HAS client’s ability to avoid buffer
underrun when the bandwidth suddenly drops, which results
in video playout stalls. In brief, the problem derives from the
discrete nature of HAS video bitrates and the resulting mismatch between the fetched bitrate and the network bandwidth.
This leads to link undersubscription and on-off downloading
patterns. The off-intervals then become a source of ambiguity
for a client to correctly perceive its fair share of the network

3
2
1
0
400

450

500

550
Time (Sec)

600

Fig. 3. Video bitrate fluctuates when 6 Adobe HDS clients compete at a
10-Mbps link. Accompanied with the fluctuation is frequent video playout
stalls, which are not shown in the plot. For more detailed experimental setup,
refer to §VII-B.

(a) PANDA

(b) Conventional Bimodal
Fig. 4. Illustration of PANDA’s fine-granular request intervals vs. a conventional algorithm’s bimodal request intervals.

bandwidth, thus preventing the client from making accurate
rate adaptation decisions2 .
B. Overview of Solution
To overcome this fundamental limitation, we envision a solution based on a “probe and adapt” principle. In this approach,
the TCP downloading throughput is taken as an input only
when it is an accurate indicator of the fair-share bandwidth.
This usually happens when the network is oversubscribed (or
congested) and the off-intervals are absent. In the presence
of off-intervals, the algorithm constantly probes3 the network
bandwidth by incrementing its sending rate, and prepares to
back off once it experiences congestion. This new mechanism
shares the same spirit with TCP’s congestion control, but it
operates independently at the application layer and at a persegment rather than a per-RTT time scale. We present PANDA
(Probe AND Adapt) – a client-side rate adaptation algorithm
– as a specific implementation of this principle.
2 In [3], Akhshabi et al. have reached similar conclusions. But they identify
the off-intervals instead of the TCP throughput-based measurement as the
root cause. Their sequel work [4] attempts to tackle the problem from a very
different angle using traffic shaping. In §VIII, we provide a justification for
our design choice.
3 By probing, we refer to small trial increment of data rate, instead of
sending auxiliary piggybacking traffic.

LI et al.: PROBE AND ADAPT: RATE ADAPTATION FOR HTTP VIDEO STREAMING AT SCALE

TABLE I
N OTATIONS USED IN THIS PAPER

Notation
w
κ
α
β

Δ


τ
B
Bmin ; Bmax
T
T̂
T̃
x
x̂
ŷ
x̃
R
r
S(·)
Q(·)

Explanation
Probing additive increase bitrate
Probing convergence rate
Smoothing convergence rate
Client buffer convergence rate
Quantization margin
Multiplicative safety margin
Video segment duration (in video time)
Client buffer duration (in video time)
Minimum/maximum client buffer duration
Actual inter-request time
Target inter-request time
Segment download duration
Actual average data rate
Target average data rate (or bandwidth share)
Smoothed version of x̂
TCP throughput measured, x̃ := r·τ
T̃
Set of video bitrates R := {R1 , ..., RL }
Video bitrate available from R
Rate smoothing function
Video bitrate quantization function

Probing constitutes fine-tuning the requested network data
rate, with continuous variation over a range. By nature, the
available video bitrates in HAS can only be discrete. A main
challenge in our design is to create a continuous decision space
out of the discrete video bitrate. To this end, we propose to
fine-tune the intervals between consecutive segment download
requests such that the average data rate sent over the network
is a continuous variable (see Fig. 4 for an illustrative comparison with the conventional scheme). Consequently, instead of
directly tuning the video bitrate, we probe the bandwidth based
on the average data rate, which in turn determines the selected
video bitrate and the fine-granularity inter-request time.
There are various benefits associated with the probe-andadapt approach. First, it avoids the pitfall of inaccurate
bandwidth estimation. Having a robust bandwidth measurement to begin with gives the subsequent operations improved
discriminative power (for example, strong smoothing of the
bandwidth measurement is no longer required, leading to
better responsiveness). Second, with constant probing via
incrementing the rate, the network bandwidth can be more
efficiently utilized. Third, it ensures that the bandwidth sharing
converges towards fair share (i.e., the same or adjacent video
bitrate) among competing clients. Lastly, an innate feature of
the probe-and-adapt approach is asymmetry of rate shifting –
PANDA is equipped with conservative bitrate level upshift but
more responsive downshift. Responsive downshift facilitates
fast recovery from sudden bandwidth drops, and thus can
effectively mitigate the danger of playout stalls caused by
buffer underrun.
C. Paper Organization
In the rest of the paper, we first give a brief overview of
the related work (§II). After formalizing the problem (§III),
we first introduce a method to characterize the conventional
rate adaptation algorithms (§IV), based on which we analyze
the root cause of its problems (§V). We then introduce our
probe-and-adapt approach (§VI) to directly address the root
cause, and present the PANDA rate adaptation algorithm as a

721

concrete implementation of this idea. We provide comprehensive performance evaluations (§VII). We conclude the paper
with final remarks and discussion of future work (§VIII).

II. R ELATED W ORK
AIMD Principle: The design of the probing mechanism in
PANDA shares similarity with Jacobson’s additive-increasemultiplicative-decrease (AIMD) principle for TCP congestion
control [12]. Kelly’s framework on network rate control [15]
provides a theoretical justification for the AIMD principle, and
proves its stability in the general network setup.
HAS Measurement Studies: Various research efforts have
focused on understanding the behavior of several commercially deployed HAS systems. One such example is [2],
where the authors characterize and evaluate HTTP streaming
players such as Microsoft Smooth Streaming, Netflix, and
Adobe OSMF via experiments in controlled settings. The
first measurement study to consider HAS streaming in the
multi-client scenarios is [3]. The authors identify the root
cause of the player’s rate oscillation/fluctuation problem as the
existence of on-off patterns in HAS traffic. In [11], the authors
measure behavior of commercial video streaming services, i.e.,
Hulu, Netflix, and Vudu, when competing with other longlived TCP flows. The results reveal that inaccurate estimations
can trigger a feedback loop leading to undesirably low-quality
video. In [8], the authors conduct a measurement study on
Akamai’s HAS service. The study shows that an asymmetric
rate shift mechanism has been implemented, where the downshift resembles a multiplicative decrease function.
Existing HAS Designs: To improve the performance of
adaptive HTTP streaming, several rate adaptation algorithms
[17], [21], [22], [19], [18] have been proposed, which, in
general, fit into the four-step model discussed in Section III-B.
In [13], a sophisticated Markov Decision Process (MDP) is
employed to compute a set of optimal client strategies in order
to maximize viewing quality. The MDP requires the knowledge of network conditions and video content statistics, which
may not be readily available. Control-theoretical approaches,
including use of a PID controller, are also considered by
several works [6], [21], [22]. A PID controller with appropriate
parameter choice can improve streaming performance. Serverside mechanisms are also advocated by some works [10], [4].
Two designs have been considered to address the multi-client
issues: in [4], a rate-shaping approach aiming to eliminate the
off-intervals, and in [14], a client rate adaptation algorithm
design implementing a combination of randomization, stateful
rate selection and harmonic mean based averaging.

III. P ROBLEM M ODEL
In this section, we formalize the problem by first describing
a representative HAS server-client interaction process. We
then outline a four-step model for an HAS rate adaptation
algorithm. This will allow us to compare the proposed PANDA
algorithm with its conventional counterpart. Table I lists the
main notations used in this paper.

722

IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS, VOL. 32, NO. 4, APRIL 2014

Algorithm 1 Conventional
At the beginning of each downloading step n:
1) Estimate the bandwidth share x̂[n] by equating it to the
measured TCP throughput:
x̂[n] = x̃[n − 1].
2) Smooth out x̂[n] to produce filtered version ŷ[n] by

Fig. 5. The HAS segment downloading process.

ŷ[n] = S({x̂[m] : m ≤ n}).
A. Process of HAS Server-Client Interaction

(1)

where T [n] is the actual inter-request time. That is, if the
download duration T̃ [n] is shorter than the target delay T̂ [n],
the client waits time T̂ [n] − T̃ [n] (i.e., the off-interval) before
starting the next downloading step (Scenario A); otherwise,
the client starts the next download step immediately after the
current download is completed (Scenario B).
Typically, a rate adaptation algorithm also measures its TCP
throughput x̃ during the segment downloading, via:
x̃[n] :=

r[n] · τ
.
T̃ [n]

(2)

The downloaded segments are stored in the client buffer.
After playout starts, the buffer is consumed by the video
player at a natural rate of one video second per real second on
average. Let B[n] be the buffer duration (measured in video
time) at the end of step n. Then the buffer dynamics can be
characterized by:
B[n] = max (0, B[n − 1] + τ − T [n]) .

(5)

3) Quantize ŷ[n] to the discrete video bitrate r[n] ∈ R by

Consider that a video stream is chopped into segments of τ
seconds each. Each segment has been pre-encoded at L video
bitrates, all stored at a server. Denote by R := {R1 , ..., RL }
the set of available video bitrates, with 0 < R < Rm for
 < m.
For each client, the streaming process is divided into
sequential segment downloading steps n = 1, 2, .... The
process we consider here generalizes the process used by
conventional HAS clients by further incorporating variable
durations between consecutive segment requests. Refer to Fig.
5. At the beginning of each download step n, a rate adaptation
algorithm:
• Selects the video bitrate of the next segment to be
downloaded, r[n] ∈ R;
• Specifies how much time to give for the current download, until the next download request (i.e., the interrequest time), T̂ [n].
The client then initiates an HTTP GET request to the server
for the segment of sequence number n and video bitrate
r[n], and the downloading starts immediately. Let T̃ [n] be
the download duration – the time required to complete the
download. Assuming that no pipelining of downloading is
involved, the next download step starts after time
T [n] = max(T̂ [n], T̃ [n]),

(4)

(3)

B. Four-Step Model
We present a four-step model for an HAS rate adaptation
algorithm, generic enough to encompass both the conventional

r[n] = Q (ŷ[n]; ...) .

(6)

4) Schedule the next download request depending on the
buffer fullness:

0, B[n − 1] < Bmax ,
T̂ [n] =
(7)
τ, otherwise.

algorithms (e.g., [17], [21], [22], [19], [18]) and the proposed
PANDA algorithm. In this model, a rate adaptation algorithm
proceeds in the following four steps.
• Estimating. The algorithm starts by estimating the network bandwidth x̂[n] that can legitimately be used.
• Smoothing. x̂[n] is then noise-filtered to yield the
smoothed version ŷ[n], with the aim of removing outliers.
• Quantizing. The continuous ŷ[n] is then mapped to the
discrete video bitrate r[n] ∈ R, possibly with the help of
side information such as client buffer size, etc.
• Scheduling. The algorithm selects the target interval until
the next download request, T̂ [n].
IV. C ONVENTIONAL A PPROACH
Using the four-step model above, in this section we introduce a scheme to characterize a conventional rate adaptation
algorithm, which will serve as a benchmark.
To the best of our knowledge, almost all of today’s commercial HAS players4 implement the measuring and scheduling
parts of the rate adaptation algorithm in a similar way, though
they may differ in their implementation of the smoothing
and quantizing parts of the algorithm. Our claim is based
on a number of experimental studies of commercial HAS
players [2], [11], [14]. The scheme described in Algorithm
1 characterizes their essential ideas.
First, the algorithm equates the currently available bandwidth share x̂[n] to the past TCP throughput x̃[n−1] (as in (2))
observed during the on-interval T̃ [n − 1]. As the bandwidth is
inferred reactively based on the previous downloads, we refer
to this as reactive bandwidth estimation.
The algorithm then obtains a filtered version ŷ[n] using a
smoothing function S(·) that takes as input the measurement
history {x̂[m] : m ≤ n}, as described in (5). Various filtering
methods are possible, such as sliding-window moving average,
exponential weighted moving average (EWMA) or harmonic
mean [14].
4 In this paper, the terms “HAS player” and “HAS client” are used
interchangeably.

LI et al.: PROBE AND ADAPT: RATE ADAPTATION FOR HTTP VIDEO STREAMING AT SCALE

2
Rate

Rate

2
1
0
0

723

1

2

3

4

5

6

7

1
0
0

8

1

(a) Perfectly Subscribed, Round-Robin
Rate

Rate

5

6

7

8

1

2

3

4

5

6

7

8

5

6

7

8

1
0
0

1

(b) Perfectly Subscribed, Partially Overlapped

2

3

4

(e) Undersubscribed
2
Rate

2
Rate

4

2

1

1
0
0

3

(d) Oversubscribed

2

0
0

2

1

2

3

4

5

6

7

8

(c) Perfectly Subscribed, Fully Overlapped

1
0
0

1

2

3

4

5

6

7

8

(f) Single-Client

Fig. 6. Illustration of various bandwidth sharing scenarios. In (a), (b) and (c), the link is perfectly subscribed. In (d), the bandwidth sharing starts with
round-robin mode but then link becomes oversubscribed. In (e), the bandwidth sharing starts with fully overlapped mode when the link is oversubscribed.
Starting from the second round, the link becomes undersubscribed. In (f), a single client is downloading, and the downloading on-off pattern exactly matches
that of the blue segments in (a).

The next step maps the continuous ŷ[n] to a discrete video
bitrate r[n] ∈ R using a quantization function Q(·). In general,
Q(·) can also incorporate side information, including the past
fetched bitrates {r[m] : m < n} and the buffer history
{B[m] : m < n}.
Lastly, the algorithm determines the target inter-request time
T̂ [n]. In (7), T̂ [n] is a mechanical function of the buffer
duration B[n − 1]. If B[n − 1] is less than a pre-defined
maximum buffer Bmax , T̂ [n] is set to 0, and by (1), the next
segment downloading starts right after the current download
is finished; otherwise, the inter-request time is set to the video
segment duration τ , to stop the buffer from further growing.
This creates two distinct modes of segment downloading – the
buffer growing mode and the steady-state mode, as shown in
Fig. 4(b). We refer to this as the bimodal download scheduling.
V. A NALYSIS OF THE C ONVENTIONAL A PPROACH
In this section, we take a deep dive into the conventional
rate adaptation algorithms and study their limitations.
A. Bandwidth Cliff Effect
As we have seen in the previous section, conventional
rate adaptation algorithms use reactive bandwidth estimation
(4) that equates the estimated bandwidth share to the TCP
throughput observed during the on-intervals. In the presence
of competing HAS clients, however, the TCP throughput does
not always faithfully represent the fair-share bandwidth. In this
section, we extend the analysis first presented in [3].
First, we illustrate with simple examples. Fig. 6 (a) - (e)
show the various scenarios of how a link can be shared by two
HAS clients in steady-state mode. We consider three different
scenarios: perfect link subscription, link oversubscription and
link undersubscription. We assume ideal TCP behavior, i.e.,
perfectly equal sharing of the available bandwidth when the
transfers overlap.
Perfect Subscription: In perfect link subscription, the total
amount of traffic requested by the two clients perfectly fills the
link. (a), (b) and (c) illustrate three different modes of bandwidth sharing, depending on the starting time of downloads

relative to each other. Essentially, under perfect subscription,
there are unlimited number of bandwidth sharing modes.
Oversubscription: In (d), the two clients start with roundrobin mode and perfect subscription. Starting from the second
round of downloading, the bandwidth is reduced and the link
becomes oversubscribed, i.e., each client requests segments
larger than its current fair-share portion of the bandwidth.
This will result in unfinished downloads at the end of each
downloading round. Then, the unfinished segment will start
overlapping with segments of the next round. This repeats
and the downloading will become more and more overlapped,
until all the clients enter the fully overlapped mode.
Undersubscription: In (e), initially the bandwidth sharing
is in fully overlapped mode, and the link is oversubscribed.
Starting from the second round, the bandwidth increases
and the link becomes undersubscribed. Then the clients start
filling up each other’s off-intervals, until a link utilization gap
emerges. The bandwidth sharing will eventually converge to
a mode which is determined by the download start times.
In any case, the measured TCP throughput (as in (2))
faithfully represents the fair-share bandwidth only when the
bandwidth sharing is in the fully overlapped mode; in all
other cases the TCP throughput overestimates the fair-share
bandwidth. Thus, most of the time, the bandwidth estimate is
accurate when the link is oversubscribed. Bandwidth overestimation occurs when the link is undersubscribed or perfectly
subscribed. In general, when the number of competing clients
is n, the bandwidth overestimation ranges from one to n times
the fair-share bandwidth.
The preceding simple examples assume idealized TCP behavior which abstracts away the complexity of TCP congestion
control dynamics. Our assumption is justifiable based on
the fact that TCP and HAS operate at very different time
scales – TCP dynamics occur in milli-seconds whereas HAS
bandwidth sharing converges in seconds. It is easy to verify
that similar behavior above occurs with real TCP connections.
To see this, we conducted a simple test bed experiment as
follows. We implemented a “thin client” to mimic an HAS
client in the steady-state mode. Each thin client repeatedly

IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS, VOL. 32, NO. 4, APRIL 2014

TCP Throughput (Mbps)

724

3.5
3
2.5
2
1.5
1
90

95

100
105
110
Link Subscription (%)

115

Fig. 7. Bandwidth cliff effect: measured TCP throughput vs. link subscription
rate for 100 thin clients sharing a 100-Mbps link. Each thin client repeatedly
downloads a segment every τ = 2 seconds.

Fig. 8. Illustration of vicious cycle of video bitrate oscillation. This plot is
obtained with 36 Smooth clients sharing a 100-Mbps link. For experimental
setup, refer to §VII-B.

downloads a segment every 2 seconds. We run 100 instances of
the thin client sharing a bottleneck link of 100 Mbps, each with
a starting time randomly selected from a uniform distribution
between 0 and 2 seconds. Fig. 7 plots the measured average
TCP throughput as a function of the link subscription rate.
We observe that when the link subscription is below 100%,
the measured throughput is about 3x the fair-share bandwidth
of ˜1 Mbps. When the link subscription is above 100%, the
measured throughput successfully predicts the fair-share bandwidth quite accurately. We refer to this sudden transition from
overestimation to fairly accurate estimation of the bandwidth
share at 100% subscription as the bandwidth cliff effect.
We summarize our findings as follows:
• Link oversubscription (even a slight one) converges to
fully overlapped bandwidth sharing and accurate bandwidth estimation.
• Link undersubscription (even a slight one) converges to
a bandwidth sharing pattern determined by the download
start times and bandwidth overestimation.
• In perfect link subscription, there exist unlimited bandwidth sharing modes, almost always leading to bandwidth
overestimation.

filter is applied to the measured TCP throughput (as in the
Smooth case), the resulting clients tend to demonstrate synchronized behavior; when the smoothing filter is weak (as in
HLS and HDS), the resulting clients tend to be more volatile,
producing more unpredictable bitrate shifts. Nevertheless, it
is the same underlying effect, i.e., the bandwidth cliff, that
drives the video bitrate to shift up and down.

B. Video Bitrate Oscillation
With an understanding of the bandwidth cliff effect, we
are now in a good position to explain the bitrate oscillation
observed in the Smooth clients, as shown in Fig. 1.
Fig. 8 illustrates this process. When the client buffer reaches
the maximum level Bmax , by (7), off-intervals start to emerge.
The link becomes undersubscribed, leading to bandwidth
overestimation (a). This triggers the upshift of requested video
bitrate (b). As the available bandwidth cannot keep up with
the video bitrate, the buffer falls below Bmax . By (7), the
client falls back to the buffer growing mode and the offintervals disappear, in which case the link again becomes
oversubscribed and the measured throughput starts to converge
to the fair-share bandwidth (c). Lastly, due to the quantization
effect, the requested video bitrate falls below the fair-share
bandwidth (d), and the client buffer starts growing again,
completing one oscillation cycle.
An intriguing question is, why neither the HLS nor the
HDS clients exhibit similar regular oscillation as the Smooth
clients? Our experience shows that when a strong smoothing

C. Fundamental Limitation
The bandwidth overestimation phenomenon reveals a more
general and fundamental limitation of the class of conventional reactive bandwidth estimation approaches discussed so
far. As video bitrates are chosen solely based on measured
TCP throughput from past segment downloads during the
on-intervals, such decisions completely ignore the network
conditions during the off-intervals. This leads to an ambiguity
of client knowledge of available network bandwidth during the
off-intervals, which, in turn, hampers the adaptation process.
To illustrate this point, consider two alternative scenarios as
depicted in Figs. 6 (f) and (a). In (f), the client downloading
the blue video segments occupies the link alone; in (a), it
shares the same link with a competing client downloading the
red video segments. Note that the on/off-intervals for all the
blue video segments follow exactly the same pattern in both
scenarios. Consequently, the client observes exactly the same
TCP throughput measurement over time. If the client would
obtain a complete picture of the network, it would know to
upshift its video bitrate in (f) but retain its current bitrate in (a).
In practice, however, an individual client cannot distinguish
between these two scenarios, hence, is bound to the same
behavior in both.
Note that as long as the off-intervals persist, such ambiguity
in client knowledge is inherent to the bandwidth measurement step in a network with competing streams. It cannot
be resolved or remedied by improved filtering, quantization,
or scheduling steps performed later in the client adaptation
algorithm. Moreover, the bandwidth cliff effect, as discussed
in Section V-A, suggests that the bandwidth overestimation
problem does not improve with more clients, and that it can
introduce large errors even with slight link undersubscription.
Instead, the client needs to take a more proactive approach
in adapting the video bitrate — whenever it is known that
the client knowledge is impaired, it must avoid using such
knowledge in bandwidth estimation. A way to distinguish the

LI et al.: PROBE AND ADAPT: RATE ADAPTATION FOR HTTP VIDEO STREAMING AT SCALE

case when the knowledge is impaired from when it is not, is to
probe the network subscription by small increment of its data
sending rate. We describe one algorithm that follows such an
alternative approach in the next section.
VI. P ROBE - AND -A DAPT A PPROACH
In this section, we introduce our proposed probe-and-adapt
approach to directly address the root cause of the conventional
algorithms’ problems. We begin the discussion by laying
out the design goals that a rate adaptation algorithm aims
to achieve. We then describe the PANDA algorithm as an
embodiment of the probe-and-adapt approach, and provide its
functional verification using experimental traces.
A. Design Goals
Designing an HAS rate adaptation algorithm involves tradeoffs among a number of competing goals. It is not legitimate
to optimize one goal (e.g., stability) without considering its
tradeoff factors. From an end-user’s perspective, an HAS rate
adaptation algorithm should be designed to meet these criteria:
• Avoiding buffer underrun. Once the playout starts, buffer
underrun (i.e., complete depletion of buffer) leads to a
playout stall. Empirical study [9] has shown that buffer
underrun may have the most severe impact on a user’s
viewing experience. To avoid it, some minimal buffer
level must be maintained at all times5 , and the adaptation algorithm must be highly responsive to network
bandwidth drops.
• High quality smoothness. In the simplest setting without
considering visual perceptual models, high video quality
smoothness translates into avoiding both frequent and
significant video bitrate shifts among available video
bitrate levels [14], [20].
• High average quality. High average video quality dictates that a client should fetch high-bitrate segments as
much as possible. Given a fixed network bandwidth, this
translates into high network utilization.
• Fairness. In the simplest setting, fairness translates
into equal network bandwidth sharing among competing
clients.
Note that this list above is non-exhaustive. Other criteria,
such as low playout startup latency, are also important factors
impacting user’s viewing experience.
B. PANDA Algorithm
In this section, we discuss the PANDA algorithm. PANDA
does not require more measurements than a conventional
scheme – all it needs are the TCP throughput as calculated in
(2) and the client buffer size. Furthermore, one only needs to
make modifications to two steps out of the four-step model as
discussed in Section III-B. Thus, it is fairly straightforward to
implement PANDA based upon modifying an existing scheme.
5 Note that, however, the buffer level must also have an upper bound, for a
few different reasons. In live streaming, the end-to-end latency from the realtime event to the event being displayed on user’s screen must be reasonably
short. In video-on-demand, the maximum buffered video must be limited to
avoid wasted network usage in case of an early termination of playback and
to limit memory usage.

725

Algorithm 2 PANDA
At the beginning of each downloading step n:
1) Estimate the bandwidth share x̂[n] by
x̂[n] − x̂[n − 1]
= κ·(w−max(0, x̂[n−1]−x̃[n−1]+w)).
T [n − 1]
(8)
2) Smooth out x̂[n] to produce filtered version ŷ[n] by
ŷ[n] = S({x̂[m] : m ≤ n}).

(9)

3) Quantize ŷ[n] to the discrete video bitrate r[n] ∈ R by
r[n] = Q (ŷ[n]; ...) .

(10)

4) Schedule the next download request via
T̂ [n] =

r[n] · τ
+ β · (B[n − 1] − Bmin ) .
ŷ[n]

(11)

Compared to the reactive bandwidth estimation used by
a conventional rate adaptation algorithm, PANDA uses a
more proactive probing mechanism. By probing, PANDA
determines a target average data rate x̂. This average data rate
is subsequently used to determine the video bitrate r to be
fetched, and the interval T̂ until the next segment download
request.
The PANDA algorithm is described in Algorithm 2, and
a block diagram interpretation of the algorithm is shown in
Fig. 9. Compared to the conventional algorithm in Algorithm
1, we only need to make modifications to the estimating and
scheduling steps – we replace (4) with (8) for estimating the
bandwidth share, and (7) with (11) for scheduling the next
download request. We now focus on elaborating each of these
two modifications.
In the estimating step, (8) is designed to directly address the
root cause that leads to the video bitrate oscillation/fluctuation
phenomenon. Based on the insights obtained from §V-A, when
the link becomes undersubscribed, the direct TCP throughput
estimate x̃ becomes inaccurate in predicting the fair-share
bandwidth, and thus should be avoided. Instead, the client
continuously increments the target average data rate x̂ by κ·w
per unit time as a probe of the available capacity. Here κ is
the probing convergence rate and w is the additive increase
rate. The algorithm keeps on monitoring the TCP throughput
x̃ according to (2) once per segment downloading cycle, and
compares it against the target average data rate x̂. Note that
the term w is present in x̂ − x̃ + w such that the target data
rate x̂ matches the TCP throughput x̃ at equilibrium (see
the Appendix for the equilibrium analysis). If x̃ > x̂ + w,
x̃ would not be informative, since in this case the link may
still be undersubscribed and x̃ may overestimate the fair-share
bandwidth. Thus, its impact is suppressed by the max(0, ·)
function. But if x̃ < x̂ + w, then TCP throughput cannot keep
up with the target average data rate indicates that congestion
has occurred. This is when the target data rate x̂ should back
off. The reduction imposed on x̂ is made proportional to
x̂ − x̃ + w. Intuitively, the lower the measured TCP throughput
x̃, the more reduction that needs to be imposed on x̂. This
design makes our rate adaptation algorithm very agile to
bandwidth changes.

726

IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS, VOL. 32, NO. 4, APRIL 2014

Fig. 9. Block diagram for PANDA (Algorithm 2). Module D represents delay of one adaptation step.

x̂o

= x̃o

ro

= ŷo
= Q(x̂o ; ...)


ro
τ
=
1−
· + Bmin ,
ŷo
β

Bo

(12)

(13)

where the subscript o denotes value of variables at equilibrium,
assuming ŷ is an unbiased estimator of x̂. Our stability
analysis shows that for the system to converge towards the
steady state, it is necessary to have:
κ

<

Δ ≥
6 Assuming

2
τ
0,

the underlying TCP is fair (e.g., equal RTTs).

(14)
(15)

TCP Throughput
Target Avg. Data Rate
Fetched Video Bitrate

6
4
2
0
0

50
40
Buffer (Sec)

8
Bitrate (Mbps)

PANDA’s probing mechanism shares similarities with
TCP’s congestion control [12], and has an additive-increasemultiplicative-decrease (AIMD) interpretation: κ · w is the
additive increase term, and −κ·max(0, x̂[n−1]− x̃[n−1]+w)
can be interpreted as the multiplicative decrease term. The
main difference is that in TCP, congestion is indicated by
packet losses (TCP Reno) or increased round-trip time (delaybased TCP), whereas in (8), congestion is indicated by the
reduction of measured TCP throughput. This AIMD property
ensures that PANDA is able to efficiently utilize the network
bandwidth, and in the presence of multiple clients, the bandwidth for each client eventually converges to fair-share status6 .
In the scheduling step, (11) aims to determine the target
inter-request time T̂ [n]. By right, T̂ [n] should be selected such
that the smoothed target average data rate ŷ[n] is equal to
r[n]·τ
. But additionally, the selection of T̂ [n] should also drive
T̂ [n]
the buffer B[n] towards a minimum reference level Bmin > 0,
so the second term is added to the right hand side of (11),
where β > 0 controls the convergence rate.
One distinctive feature of the PANDA algorithm is its
hybrid closed-loop/open-loop design. Refer to Fig. 9. In this
system, (8) forms a closed loop by itself that determines
the target average data rate x̂. (11) forms a closed loop by
itself that determines the target inter-request time T̂ . Overall,
the estimating, smoothing, quantizing and scheduling steps
together form an open loop (ignoring the network dynamics’
feedback). The main motivation behind this design is to
reduce the bitrate shifts associated with quantization. Since
quantization is excluded from the closed loop of x̂, it allows x̂
to settle in a steady state. Since r[n] is a deterministic function
of x̂[n], it can also settle in a steady state.
In the Appendix, we present an equilibrium and stability
analysis of PANDA. We summarize the main results as follows. Our equilibrium analysis shows that at steady state, the
system variables settle at

30
20
10

100

200
300
Time (Sec)

400

500

0
0

100

200
300
Time (Sec)

400

500

Fig. 10. A PANDA client adapts its video bitrate under a bandwidth-varying
link. The bandwidth is initially at 5 Mbps, drops to 2 Mbps at 200 seconds
and rises back to 5 Mbps at 300 seconds.

where Δ is a parameter associated with the quantizer Q(·),
referred to as the quantization margin, i.e., the selected
discrete rate r must satisfy
r[n] ≤ ŷ[n] − Δ.

(16)

C. Functional Verification
We verify the behavior of PANDA using experimental
traces. For detailed experiment setup (including the selection
of function S(·) and Q(·)), refer to §VII-B.
First, we evaluate how a single PANDA client adjusts its
video bitrate as the the available bandwidth varies over time.
In Fig. 10, we plot the TCP throughput x̃, the target average
data rate x̂, the fetched video bitrate r and the client buffer B
for a duration of 500 seconds, where the bandwidth drops
from 5 to 2 Mbps at 200 seconds, and rises back to 5
Mbps at 300 seconds. Initially, the target average data rate
x̂ ramps up gradually over time; the fetched video bitrate
r also ramps up correspondingly. After the initial ramp-up
stage, x̂ settles in a steady state. It can be observed that
at steady state, x̂ matches x̃, which is consistent with (12).
Similarly, the buffer B also settles in a steady state, and after
plugging in all the parameters, one can verify that the steady
state of buffer (13) also holds. At 200 seconds, when the
bandwidth suddenly drops, the fetched video bitrate quickly
drops to the desirable level. With this quick response, the
buffer hardly drops. This property makes PANDA favorable
for live streaming applications. When the bandwidth rises back
to 5 Mbps at 300 seconds, the fetched video bitrate gradually
ramps up to the original level.
Note that, in practical implementation, we can further add
startup logic to improve PANDA’s rate ramp-up speed at the
initial stage. Since off-intervals would not need to appear
before the buffer reaches the minimum reference level, a conventional rate adaptation algorithm based on TCP throughput
measurement would suffice.
The more intriguing question is whether PANDA could
effectively mitigate the bitrate fluctuation observed in the
conventional HAS players. We conduct an experiment with
the same setup as the experiment shown in Fig. 1, except that

LI et al.: PROBE AND ADAPT: RATE ADAPTATION FOR HTTP VIDEO STREAMING AT SCALE

100
80

400

500

600

700
800
Time (Sec)

900

1000

1100

0
200

1200

Fetched Bitrate of Individual Streams (Zoom In)

2

420

440

460

480

500
520
Time (Sec)

540

560

580

300

400

Fig. 11. 36 PANDA clients compete at a 100-Mbps link in steady state.

the PANDA player and the Smooth player use slightly different
video bitrate levels (due to different packaging methods). The
resulting fetched bitrates in aggregate and for each client are
shown in Fig. 11. From the plot of the aggregate fetched
bitrate, except for the initial fluctuation, the aggregate bitrate
closely tracks the available bandwidth of 100 Mbps. Zooming
in to the individual streams’ fetched bitrates, the fetched
bitrates are confined within two adjacent bitrate levels and the
number of shifts is much smaller than the Smooth client’s case.
This affirms that PANDA is able to achieve better stability than
the Smooth’s rate adaptation algorithm. In §VII, we perform
a comprehensive performance evaluation on each adaptation
algorithm.
To help the reader develop a better intuition on why PANDA
performs better than a conventional algorithm, in Fig. 12
we plot the trace of the measured TCP throughput and the
target average data rate for the same experiment as in Fig.
11. Note that the fair-share bandwidth for each client is
about 2.8 Mbps. From the plot, the TCP throughput not only
grossly overestimates the fair-share bandwidth, it also has a
large variation. If used directly, this degree of noisiness gives
the subsequent operations a very hard job to extract useful
information. For example, one may apply strong filtering
to smooth out the bandwidth measurement, but this would
seriously affect the responsiveness of the client. When the
network bandwidth suddenly drops, the client would not be
able to respond quickly enough to reduce its video bitrate,
leading to catastrophic buffer underrun. Moreover, the bias is
both large and difficult to predict, making any correction to
the mean problematic. In comparison, the target average data
rate estimated by the probing mechanism is neither biased nor
have a large variation, enabling the subsequent operations to
easily pick a video bitrate without sacrificing responsiveness.
In Fig. 13, we verify the stability criteria (14) and (15) of
PANDA. With τ = 2, the system is stable if κ < 1. This is
demonstrated by Fig. 13 (a), where we show the traces of the
target average rate x̂ for two κ values 0.9 and 1.1. In Fig. 13
(b), we show that when Δ = −w, the buffer cannot converge
towards the reference level of 30 seconds.
VII. P ERFORMANCE E VALUATION
In this section, we conduct a set of test bed experiments
to evaluate the performance of PANDA against other rate
adaptation algorithms.

600

700
800
Time (Sec)

900

1000

1100

1200

1000

1100

1200

5

0
200

600

500

Target Average Data Rate x̂

10

4

0
400

5

300

400

500

600

700
800
Time (Sec)

900

Fig. 12. The traces of the TCP throughput and the unfiltered target average
data rate of 36 PANDA clients compete at a 100-Mbps link in steady state.
The traces of the first five clients are plotted.
30

5
4

Buffer(Sec)

Fetched (Mbps)

6

300

Measured TCP Throughput x̃

10

Target (Mbps)

60
200

Target (Mbps)

Fetched (Mbps)

Throughput(Mbps)

Fetched Bitrate Aggregated over 36 Streams

120

727

3
2
κ=0.9
κ=1.1

1
0
0

50

100
150
Time (Sec)

(a) κ

200

20

10

0
0

Δ=0
Δ=−w
100
200
Time (Sec)

300

(b) Δ

Fig. 13. Experimental verification of PANDA’s stability criteria. In (a), one
PANDA client streams over a link of 5 Mbps. In (b), 10 PANDA clients
compete over a 10 Mbps link.

A. Evaluation Metrics
In §VI-A, we discussed four criteria that are most important
for a user’s watching experience – i) ability to avoid buffer
underruns, ii) quality smoothness, iii) average quality, and iv)
fairness. In this paper, we use buffer undershoot as the metric
for Criterion i), described as follows.
• Buffer undershoot: We measure how much the buffer
goes down after a bandwidth drop as a indicator of an
algorithm’s ability to avoid buffer underruns. The less the
buffer undershoot, the less likely the buffer will underrun.
Let Bo be a reference buffer level (30 seconds for all
players in this paper), and Bi,t the buffer level for player
i at time t. The buffer undershoot for player i at time t
max(0,Bo −Bi,t )
. The buffer undershoot for
is defined as
Bo
player i within a time interval T (right after a bandwidth
drop), is defined as the 90th-percentile value of the
distribution of buffer undershoot samples collected during
T.
We inherit the metrics defined in [14] – instability, inefficiency
and unfairness, as the metrics for Criteria ii), iii) and iv), respectively. We only make a slight modification to the definition
of inefficiency. Let ri,t be the video bitrate fetched by player
i at time t.
• Instability: The instability for player i at time t is
k−1
d=0 |ri,t−d −ri,t−d−1 |·w(d)
k−1
, where w(d) = k − d is a
d=0 ri,t−d ·w(d)
weight function that puts more weight on more recent
samples. k is selected as 20 seconds.
• Inefficiency: Let C be the available bandwidth. [14]

| i ri,t −C |
for player i at time t.
defines inefficiency as
C

TABLE II
PARAMETERS USED IN EXPERIMENTS
Algorithm
PANDA

Conventional

FESTIVE

•

Parameter
κ
w
α
β

Bmin
α

Bmax
Window
targetbuf

Default
0.14
0.3
0.2
0.2
0.15
26
0.2
0.15
30
20
30

Local

Values
0.04,0.07,0.14,0.28,0.42,0.56
0.05,0.1,0.2,0.3,0.4,0.5
0.5,0.4,0.3,0.2,0.1,0
0.01,0.04,0.07,0.1,0.15,0.2

HTTP
Server

100 or 10 Mbps
20 ms

Aggregation
Router

Local

l
ca
Lo

Home Loc
Router al

HAS
Client

IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS, VOL. 32, NO. 4, APRIL 2014

…
.
HAS
Client

728

Fig. 14. The network topology configured in the test bed. Local indicates
that the bitrate is effectively unbounded and the link delay is 0 ms.

20,15,10,6,3,1


But sometimes the sum of fetched bitrate i ri,t can be
greater than C. To avoid unnecessary penalty in
this case,

max(0,C− i ri,t )
we revise the inefficiency metric to
for
C
player i at time t.
Unfairness: Let JainF airt be the Jain fairness index
calculated on the rates ri,t at√
time t over all players. The
unfairness at t is defined as 1 − JainF airt .

B. Experimental Setup
HAS Player Configuration: The benchmark players that we
use to compare against PANDA are:
• Microsoft Smooth player [5], a commercially available
proprietary player. The Smooth players are of version
1.0.837.34 using Silverlight runtime version 4.0.50826.
To our best knowledge, the Smooth player as well as the
Apple HLS and the Adobe HDS players all use the same
TCP throughput measurement mechanism, so we picked
the Smooth player as a representative.
• FESTIVE player, which we implemented based on the
details specified in [14]. The FESTIVE algorithm is the
first known client-side rate adaptation algorithm designed
to specifically address the multi-client scenario.
• A player implementing the conventional algorithm (Algorithm 1), which differs from PANDA only in the
estimating and scheduling steps.
For fairness, we ensure that PANDA and the conventional
player use the same smoothing and quantizing functions. For
smoothing, we implemented a EWMA smoother of the form:
ŷ[n]−ŷ[n−1]
= −α · (ŷ[n − 1] − x̂[n]), where α > 0 is the
T [n−1]
convergence rate of ŷ[n] towards x̂[n]. For quantization, we
implemented a dead-zone quantizer r[n] = Q(ŷ[n], r[n − 1]),
defined as follows: Let the upshift threshold be defined as
rup := maxr∈R r subject to r ≤ ŷ[n] − Δup , and the
downshift threshold as rdown := maxr∈R r subject to
r ≤ ŷ[n] − Δdown, where Δup and Δdown are the upshift and
downshift safety margin respectively, with Δup ≥ Δdown ≥ 0.
The dead-zone quantizer updates r[n] as
⎧
⎪
r[n − 1] < rup ,
⎨rup ,
(17)
r[n] = r[n − 1], rup ≤ r[n − 1] ≤ rdown ,
⎪
⎩
rdown ,
otherwise.
The “dead zone” [rup , rdown ] created by setting Δup > Δdown
mitigates frequent bitrate hopping between two adjacent levels,
thus stabilizing the video quality (i.e. hysteresis control). For
the conventional player and PANDA, set Δup =  · ŷ and

Δdown = 0, where 0 ≤  < 1 is the multiplicative safety
margin.
Table II lists the default parameters used by each player, as
well as their varying values. For fairness, all players attempt
to maintain a steady-state buffer of 30 seconds. For PANDA,
Bmin is selected to be 26 seconds such that the resulting
steady-state buffer is 30 seconds (by (13)).
Server Configuration: The HTTP server runs Apache on
Red Hat 6.2 (kernel version 2.6.32-220). The Smooth player
interacts with an Microsoft IIS server by default, but we also
perform experiments of Smooth player interacting with an
Apache server on Ubuntu 10.04 (kernel version 2.6.32.21).
Network Configuration: As service provider deployment
over a managed network is our primary case of interest, our
experiments are configured to highly match the important
scenario where a number of HAS flows compete for bandwidth
within a DOCSIS bonding group. The test bed is configured
as in Fig. 14. The queueing policy used at the aggregation
router-home router bottleneck link is the following. For a link
bandwidth of 10 Mbps or below, we use random early detection (RED) with (min thr, max thr, p) = (30, 90, 0.25);
if the link bandwidth is 100 Mbps, we use RED with
(min thr, max thr, p) = (300, 900, 1). The video content
is chopped into segments of τ = 2 seconds, pre-encoded with
L = 10 bitrates: 459, 693, 937, 1270, 1745, 2536, 3758, 5379,
7861 and 11321 Kbps. For the Smooth player, the data rates
after packaging are slightly different.
C. Performance Tradeoffs
It would not be legitimate to discuss a single metric without
minding its impact on other metrics. In this section, we
examine the performance tradeoffs among the four metrics
of interest. We designed an experimental process under which
we can measure all four metrics in a single run. For each run,
five players (of the same type) compete at a bottleneck link.
The link bandwidth stays at 10 Mbps from 0 seconds to 400
seconds, drops to 2.5 Mbps at 400 seconds and stays there
until 500 seconds. We record the instability, inefficiency and
unfairness averaged over 0 to 400 seconds over all players, and
the buffer undershoot over 400 to 500 seconds averaged over
all players. Fig. 15 shows the tradeoff between stability and
each of the other criteria – buffer undershoot, inefficiency and
unfairness – for each of the types of player. Each data point
is obtained via averaging over 20 runs, and each data point
represents a different value for one of the parameters of the
corresponding algorithm, as indicated in the Values column of
Table II.
For the PANDA player, the three parameters that affect
instability the most are: the probing convergence rate κ, the

LI et al.: PROBE AND ADAPT: RATE ADAPTATION FOR HTTP VIDEO STREAMING AT SCALE

1

0.05

Undershoot

0.6
0.4

0.04
Instability

PANDA (Vary α)
PANDA (Vary k)
PANDA (Vary ε)
FESTIVE (Vary Win)
Conventional (Vary α)

0.8

0.2
0
0

729

PANDA
FESTIVE
Conventional
Smooth

0.03
0.02
0.01

0.02

0.04
Instability

0.06

0
0

0.08

5

(a)
0.35

0.15
0.1

PANDA
FESTIVE
Conventional
Smooth

0.3
0.25
Inefficiency

PANDA (Vary α)
PANDA (Vary k)
PANDA (Vary ε)
FESTIVE (Vary Win)
Conventional (Vary α)
Smooth

0.2

15

(a)

0.25

Inefficiency

10
Num. Clients

0.2
0.15
0.1

0.05
0
0

0.05
0.02

0.04
Instability

0.06

0
0

0.08

5

10

(b)

(b)
0.2

PANDA
FESTIVE
Conventional
Smooth

0.25

PANDA (Vary α)
PANDA (Vary k)
PANDA (Vary ε)
FESTIVE (Vary Win)
Conventional (Vary α)
Smooth

0.1
0.05
0
0

0.02

0.04
0.06
Instability

0.08

Unfairness

Unfairness

0.15
0.2
0.15

15

Num. Clients

0.1

0.05

0
0

5

10

15

Num. Clients

(c)

(c)

Fig. 15. The impact of varying instability on buffer undershoot, inefficiency
and unfairness for PANDA and other benchmark players.

Fig. 16. Instability, inefficiency and unfairness as the number of clients
increases. The link bandwidth is fixed at 10 Mbps.

smoothing convergence rate α and the safety margin . Fig.
15 (a) shows that as we vary these parameters, the tradeoff
curves mostly stay flat (except for at extreme values of these
parameters), implying that the PANDA player maintains good
responsiveness as the stability is improved. A few factors
contribute to this advantage of PANDA: First, as the bandwidth
estimation by probing is quite accurate, one does not need
to apply strong smoothing. Second, since after a bandwidth
drop, the video bitrate reduction is made proportional to the
TCP throughput reduction, PANDA is very agile to bandwidth
drops. On the other hand, for both the FESTIVE and the
conventional players, the buffer undershoot significantly increases as the scheme becomes more stable. Overall, PANDA
has the best tradeoff between stability and responsiveness to
bandwidth drop, outperforming the second best conventional
player by more than 75% reduction in instability at the same
buffer undershoot level. It is worth noting that the conventional
player uses exactly the same smoothing and quantization steps
as PANDA, which implies that the gain achieved by PANDA
is purely due to the improvement in the estimating and

scheduling steps. FESTIVE has the largest buffer undershoot.
We believe this is because the design of FESTIVE has mainly
concentrated on stability, efficiency and fairness, but ignored
responsiveness to bandwidth drops. As we do not have access
to the Smooth player’s buffer, we do not have its buffer
undershoot measure in Fig. 15 (a).
Fig. 15 (b) shows that PANDA has the lowest inefficiency
over all as we vary its instability. The probing mechanism
ensures that the bandwidth is most efficiently utilized. As
the instability increases, the inefficiency also increases moderately. This makes sense intuitively, as when the bitrate
fluctuates, the average fetched bitrate also decreases. The efficiency of the conventional algorithm underperforms PANDA,
but outperforms FESTIVE. Lastly, the Smooth player has the
highest inefficiency.
Lastly, Fig. 15 (c) shows that in terms of fairness, FESTIVE
achieves the best performance. This may be due to the
randomized scheduling strategy of FESTIVE. PANDA and
the conventional players have similar fairness; both of them
outperform the Smooth player.

730

IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS, VOL. 32, NO. 4, APRIL 2014

0.06

Instability

0.05
0.04

PANDA
FESTIVE
Conventional
Smooth

0.03
0.02
0.01
0
0

2

4
6
Num. Clients

8

10

(a)
0.4

PANDA
FESTIVE
Conventional
Smooth

Inefficiency

0.3

0.2

0.1

0
0

2

4
6
Num. Clients

8

10

(b)
0.25

Unfairness

0.2
0.15
0.1
PANDA
FESTIVE
Conventional
Smooth

0.05
0
0

2

4
6
Num. Clients

8

10

(c)
Fig. 17. Instability, inefficiency and unfairness as the number of clients
increases. The link bandwidth increases with the number of players, with
the bandwidth-player ratio fixed at 1 Mbps/player.

D. Increasing Number of Players
In this section, we focus on the question of how the number
of players affects instability, inefficiency and unfairness at
steady state. Two scenarios are of interest: i) we increase the
number of players while fixing the link bandwidth at 10 Mbps,
and ii) we increase the number of players while varying the
bandwidth such that the bandwidth/player ratio is fixed at 1
Mbps/player. Fig. 16 and Fig. 17 report results for these two
cases, respectively. Each data point is obtained by averaging
over 20 runs.
Refer to Fig. 16 (a). In the single-player case, all four
schemes are able to maintain their fetched video bitrate at a
constant level, resulting in zero instability. As the number of
players increases, the instability of the conventional player and
the Smooth player both increase quickly in a highly consistent
way. We speculate that they have very similar underlying
structure. The FESTIVE player is able to maintain its stability
at a lower level, due to the strong smoothing effect (smoothing
window at 20 samples by default), but the instability still

grows with the number of players, likely due to the bandwidth
overestimation effect. The PANDA player exhibits a rather
different behavior: at two players it has the highest instability,
then the instability starts to drop as the number of players
increases. Investigating into the experimental traces reveals
that this is related to the specific bitrate levels selected. More
importantly, via probing, the PANDA player is immune to the
symptoms of the bandwidth overestimation, thus it is able to
maintain its stability as the number of clients increases. The
case of varying bandwidth in Fig. 17 (a) exhibits behavior
fairly consistent with Fig. 16 (a), with PANDA and FESTIVE
exhibiting much less instability compared to the Smooth and
the conventional players.
Fig. 16 (b) and Fig. 17 (b) for the inefficiency metric both
show that PANDA consistently has the best performance as
the number of players grow. The conventional player and
FESTIVE have similar performance, both outperforming the
Smooth player by a great margin. We speculate that the
Smooth player has some large bitrate safety margin by design,
with the purpose of giving cross traffic more breathing room.
Lastly, let us look at fairness. Refer to Fig. 16 (c). We have
found that when the overall bandwidth is fixed, the unfairness
measure has high dependence on the specific bitrate levels
chosen, especially when the number of players is small. For
example, at two players, when the fair-share bandwidth is 5
Mbps, the two PANDA players end up in steady state with
5.3 Mbps and 3.7 Mbps, resulting in a high unfairness score.
At three players, when the fair-share bandwidth is 3.3 Mbps,
the three PANDA players each end up with 3.7, 3.7 and 2.5
Mbps for a long period of time, resulting in a lower unfairness
score. FESTIVE exhibits lowest unfairness overall, which is
consistent with the results obtained in Fig. 15 (c). In the
varying-bandwidth case in Fig. 17 (c), The unfairness ranking
is fairly consistent as the number of players grow: FESTIVE,
PANDA, the conventional, and Smooth.
E. Competing Mixed Players
One important question to ask is how PANDA will behave
in the presence of different types of other players? If it behaves
too conservatively and cannot grab enough bandwidth, then the
deployment of PANDA will not be successful. To answer this
question, we take the four types of players of interest and have
them compete on a 10-Mbps link. For the Smooth player, we
test it with both a Microsoft IIS server running on Windows
7, and an Apache HTTP server running on Ubuntu 10.04. A
single trace of the fetched bitrates for 500 seconds is shown
in Fig. 18. The plot shows that the Smooth player’s ability to
grab the bandwidth highly depends on the server it streams
from. Using the IIS server, which runs on Windows 7 with an
aggressive TCP, it is able to fetch video bitrates over 3 Mbps.
With the Apache HTTP server, which uses Ubuntu 10.04’s
conservative TCP, the fetched bitrates are about 1 Mbps. The
conventional, PANDA and FESTIVE players all run on the
same TCP (Red Hat 6), so their differences are due to their
adaptation algorithms. Due to bandwidth overestimation, the
conventional player aggressively fetches high bitrates, but the
fetched bitrates fluctuate. Both PANDA and FESTIVE are able
to maintain a stable fetched bitrate at about the fair-share level
of 2 Mbps.

4

8

3

6

Smooth (IIS)
Conventional
PANDA
FESTIVE
Smooth (Apache)

2
1
0
0

100

200

300
400
Time (Sec)

500

600

Bitrate (Mbps)

Fetched (Mbps)

LI et al.: PROBE AND ADAPT: RATE ADAPTATION FOR HTTP VIDEO STREAMING AT SCALE

Fig. 18. PANDA, Smooth (w/ IIS), Smooth (w/ Apache), FESTIVE and the
conventional players compete at a bottleneck link of 10 Mbps.

G. Summary of Performance Results
•

•

•

•

The PANDA player has the best stability-responsiveness
tradeoff, outperforming the second best conventional
player by 75% reduction in instability. PANDA also has
the best bandwidth utilization. In competing with longrunning TCP or the most aggressive HAS client, PANDA
shows fair competency.
The FESTIVE player has been tuned to yield high
stability, high efficiency and good fairness. However, it
underperforms other players in responsiveness to bandwidth drops.
The conventional player yields good efficiency, but lacks
in stability, responsiveness to bandwidth drops and fairness.
The Smooth player underperforms in efficiency, stability
and fairness. When competing against other players,
its ability to grab bandwidth is a consequence of the
aggressiveness of the underlying TCP stack.

PANDA Data Rate

TCP Data Rate

2

100

150

200
250
Time (Sec)

300

350

400

Fig. 19. PANDA competes with a long-running TCP. The link bandwidth is
kept at 7 Mbps. The long-running TCP starts at time 150 seconds.

F. Competing with Long-running TCP
In this experiment, we test PANDA’s competency against
a long-running TCP stream. Long-running TCP is not only
ubiquitous in today’s Internet, it can also be treated as the
aggressiveness upper bound of any HAS clients. Thus, the
experimental result also provides a sense of how PANDA
performs when competing with the most aggressive HAS
client. Fig. 19 shows the trace of PANDA competing with
one long-running TCP stream at a bottleneck link of 7 Mbps.
After the TCP starts at time 150 seconds, the PANDA stream
settles to steady state at fetched bitrate of 2.5 Mbps, while the
TCP settles at around 4 Mbps. This result shows that PANDA
is fairly competitive against the long-running TCP, or the most
aggressive HAS client possible.
There are a few reasons contributing to PANDA’s loss of 1
Mbps bitrate compared to the fair-share rate of 3.5 Mbps. First,
during the off-intervals, PANDA’s TCP congestion window
would stop growing, while a long-running TCP’s congestion
window would keep on growing. Second, both the discrete
nature of the video bitrate and the quantization safety margin
 (see Table II) would discount the competency of PANDA.
Note that these two factors also apply to a conventional HAS
client. As we are trying to build a better client algorithm
that behaves more properly and results in an improved performance, winning over an aggressive client algorithm is not
our primary goal. That said, we believe PANDA still show fair
competency against aggressive clients.

PANDA Fetched

4

0
50

700

731

VIII. C ONCLUDING R EMARKS
This paper identifies an emerging issue for HTTP adaptive
streaming, and lays out a solution direction that can effectively
address this issue and achieve significant improvements in
stability at no cost in responsiveness. Our main contributions
in this paper can be summarized as follows:
•

•

•

We have identified the bandwidth cliff effect as the root
cause of the bitrate oscillation/fluctuation phenomenon
and revealed the fundamental limitation of the conventional reactive measurement based rate adaptation algorithms.
We have envisioned a general probe-and-adapt principle
to directly address the root cause of the problems, and
designed and implemented PANDA, a client-based rate
adaptation algorithm, as an embodiment of this principle.
We have proposed a generic four-step model for an HAS
rate adaptation algorithm, based on which we have fairly
compared the proposed approach with the conventional
approach.

Our proposed PANDA algorithm not only is an effective
solution to address the bitrate oscillation problem, but can also
serve as a basis for more advanced video quality optimization
schemes, such as [16].
As a final remark, we would like to provide a briefly
justification for our design choice of admitting the on-off data
downloading pattern in our solution. An intriguing question
raised by our readers is why we would not simply eliminate
the off-intervals by always keeping the data sending on, hence
avoiding the bandwidth overestimation problem. We believe
that in HAS, the on-off pattern is a natural consequence of
the discrete video bitrate. Also, our experiments show that
more aggressive clients (e.g., clients without off-intervals by
completely filling the pipe) typically yield slower response to
network changes. Additionally, an HAS stream without offintervals would behave like a long-running TCP, which may
lead to bad interactions with poorly designed network queue
management equipments (e.g., the bufferbloat problem). Last
but not least, our solution demonstrates that it is possible to
address the rate oscillation problem from a pure applicationlayer perspective.
A PPENDIX
We perform an equilibrium and stability analysis of
PANDA. For simplicity, we only analyze the single-client case
where the system has an equilibrium point.

732

IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS, VOL. 32, NO. 4, APRIL 2014

Analysis of x̂
Assume the measured TCP download throughput x̃ is equal
to the link capacity C. Consequently, (8) can be re-written in
two scenarios (refer to Fig. 5) as:
x̂[n] − x̂[n − 1]
=
T [n − 1]

κ · w,
κ · w − κ · (x̂[n − 1] − C + w),

if x̂ < C,
otherwise.

(18)

At equilibrium, setting (18) to zero leads to x̂o = x̃o = C.
Considering the simplifying assumptions of ŷ = x̂ (no
smoothing) and a quantizer with a margin Δ such that
r = ŷ − Δ, we need ro = x̂o − Δ ≤ C at equilibrium.
Therefore, the quantization margin of the quantizer needs to
satisfy
Δ ≥ x̂o − C = 0,
so that the system stays on the multiplicative-decrease side of
the equation at steady state.
Note that close to equilibrium, the intervals between consecutive segment downloads match the segment playout duration
T [n−1] = τ , therefore, calculation of the estimated bandwidth
x̂ follows the simple form of a difference equation:
x̂[n] = a · x̂[n − 1] + b.
The two constants are: a = 1 − κ · τ and b = κ · (C + w) · τ .
The sequence converges if and only if |a| < 1. Hence, we
need: −1 < 1 − κ · τ < 1, leading to the stability criterion:
0<k<

2
.
τ

Analysis of T̂ and B
Assume no smoothing x̂ = ŷ. During transient states, when
T = T̂ > T̃ , update of T̂ pushes the playout buffer size in the
right direction: T̂ < r[n]·τ
x̂[n] leads to a steady growth in buffer
size when B < Bmin . At steady state, T̂o = τ = T̂o > T̃o . It
can be derived from (11) that:


ro
τ
·
Bo = Bmin + 1 −
x̂o
β
where Bo is playout buffer at equilibrium.
R EFERENCES
[1] History
of
Move
Networks.
Available
online:
http://www.movenetworks.com/history.html.
[2] S. Akhshabi, S. Narayanaswamy, Ali C. Begen, and C. Dovrolis. An
experimental evaluation of rate-adaptive video players over HTTP.
Signal Processing: Image Communication, 27:271–287, 2012.
[3] Saamer Akhshabi, Lakshmi Anantakrishnan, Constantine Dovrolis, and
Ali C. Begen. What happens when HTTP adaptive streaming players
compete for bandwidth. In Proc. ACM Workshop on Network and
Operating System Support for Digital Audio and Video (NOSSDAV’12),
Toronto, Ontario, Canada, 2012.
[4] Saamer Akhshabi, Lakshmi Anantakrishnan, Constantine Dovrolis, and
Ali C. Begen. Server-based traffic shaping for stabilizing oscillating
adaptive streaming players. In Proc. ACM Workshop on Network and
Operating System Support for Digital Audio and Video (NOSSDAV’13),
2013.
[5] Alex Zambelli.
IIS Smooth Streaming Technical Overview.
http://tinyurl.com/smoothstreaming.

[6] Luca De Cicco, Saverio Mascolo, and Vittorio Palmisano. Feedback
control for adaptive live video streaming. In Proc. ACM Multimedia
Systems Conference (MMSys’11), pages 145–156, San Jose, CA, USA,
February 2011.
[7] Cisco White Paper. Cisco visual networking index - forecast and
methodology, 2011-2016. http://tinyurl.com/ciscovni2011to2016 .
[8] Luca De Cicco and Saverio Mascolo. An experimental investigation
of the Akamai adaptive video streaming. In Proc. 6th International
Conference on HCI in Work and Learning, Life and Leisure: Workgroup
Human-computer Interaction and Usability Engineering, USAB’10,
pages 447–464, Berlin, Heidelberg, 2010. Springer-Verlag.
[9] F. Dobrian, V. Sekar, A. Awan, I. Stoica, D. Joseph, A. Ganjam, J. Zhan,
and H. Zhang. Understanding the impact of video quality on user
engagement. In Proc. ACM SIGCOMM 2011 conference, SIGCOMM
’11, pages 362–373, New York, NY, USA, 2011. ACM.
[10] Remi Houdaille and Stephane Gouache. Shaping HTTP adaptive
streams for a better user experience. In Proc. 3rd Multimedia Systems
Conference, pages 1–9, 2012.
[11] Te-Yuan Huang, Nikhil Handigol, Brandon Heller, Nick McKeown,
and Ramesh Johari. Confused, timid, and unstable: Picking a video
streaming rate is hard. In Proc. 2012 ACM conference on Internet
measurement, 2012.
[12] V. Jacobson. Congestion avoidance and control. In Symposium proceedings on Communications architectures and protocols, SIGCOMM ’88,
pages 314–329, New York, NY, USA, 1988. ACM.
[13] Dmitri Jarnikov and Tanir Ozcelebi. Client Intelligence for Adaptive
Streaming Solutions. EURASIP J. Signal Processing: Image Communication, Special Issue on Advances in IPTV Technologies, 26(7):378–389,
August 2011.
[14] Junchen Jiang, Vyas Sekar, and Hui Zhang. Improving fairness,
efficiency, and stability in HTTP-based adaptive video streaming with
FESTIVE. In Proc. 8th international conference on Emerging networking experiments and technologies (CoNEXT), 2012.
[15] F. P. Kelly, A. K. Maulloo, and D. K. H. Tan. Rate control for
communication networks: Shadow prices, proportional fairness and
stability. The Journal of the Operational Research Society, 49(3):237–
252, 1998.
[16] Zhi Li, Ali C. Begen, Joshua Gahm, Yufeng Shan, Bruce Osler, and
David Oran. Streaming video over HTTP with consistent quality. In
Proc. ACM Multimedia Systems Conference, 2014.
[17] Chenghao Liu, Imed Bouazizi, and Moncef Gabbouj. Rate Adaptation
for Adaptiv HTTP streaming. In Proc. ACM Multimedia Systems
Conference (MMSys’11), pages 169–174, San Jose, CA, USA, February
2011.
[18] Chenghao Liu, Imed Bouazizi, Miska M. Hannuksela, and Moncef
Gabbouj. Rate adaptation for dynamic adaptive streaming over HTTP in
content distribution network. Signal Processing: Image Communication,
27(4):288–311, April 2012.
[19] Konstantin Miller, Emanuele Quacchio, Gianluca Gennari, and Adam
Wolisz. Adaptation algorithm for adaptive streaming over HTTP. In
Proc. 2012 IEEE 19th International Packet Video Workshop, pages 173–
178, 2012.
[20] R.K.P. Mok, E.W.W. Chan, X. Luo, and R.K.C. Chang. Inferring the
QoE of HTTP video streaming from user-viewing activities. In Proc.
SIGCOMM W-MUST, 2011.
[21] Guibin Tian and Yong Liu. Towards agile and smooth video adaptation
in dynamic HTTP streaming. In Proc. 8th international conference on
Emerging networking experiments and technologies (CoNEXT), pages
109–120, 2012.
[22] Chao Zhou, Xinggong Zhang, Longshe Huo, and Zongming Guo.
A control-theoretic approach to rate adaptation for dynamic HTTP
streaming. In Proc. Visual Communications and Image Processing
(VCIP), pages 1–6, 2012.

LI et al.: PROBE AND ADAPT: RATE ADAPTATION FOR HTTP VIDEO STREAMING AT SCALE

Zhi Li is with the Video and Collaboration Group
at Cisco Systems. His current research interests
focus on improving video streaming experience for
consumers through system design, optimization and
control algorithms. He has broad interests in applying mathematical tools and analytical skills in
solving real-world engineering problems.
Zhi received his B.Eng. and M.Eng. degrees
in Electrical Engineering, both from the National
University of Singapore, Singapore, in 2005 and
2007, respectively, and a Ph.D. degree from Stanford
University, California, USA in 2012. His doctoral work focuses on source
and channel coding methods in multimedia networking problems. He was
a recipient of the Best Student Paper Award at IEEE ICME 2007 for
his work on cryptographic watermarking, and a co-recipient of Multimedia
Communications Best Paper Award from IEEE Communications Society in
2008 for a work on multimedia authentication.

Xiaoqing Zhu is a Technical Leader at the Enterprise Networking Lab at Cisco Systems Inc. She
received the B.Eng. degree in Electronics Engineering from Tsinghua University, Beijing, China. She
earned both the M.S. and Ph.D. degrees in Electrical
Engineering from Stanford University, California,
USA. Prior to joining Cisco, Dr. Zhu interned at
IBM Almaden Research Center in 2003, and at
Sharp Labs of America in 2006. She received the
best student paper award in ACM Multimedia 2007.
Dr. Zhu’s research interests span across multimedia applications, networking, and wireless communications. She has served
as reviewer, TPC member, and special session organizer for various journals,
magazines, conferences and workshops. She has contributed as guest editor
to several special issues in IEEE Technical Committee on Multimedia
Communications (MMTC) E-Letter, IEEE Journal on Selected Areas in
Communications, and IEEE Transactions on Multimedia.

Joshua Gahm is a Principal Engineer in the Service Provider Video Technology Group at Cisco.
During 15 years at Cisco and more than 25 years
overall in the areas of IP networking and video,
Joshua has led numerous product development and
applied research teams, spanning areas including
IP-TV, DOCSIS, HTTP Adaptive Bitrate Streaming, and MPLS. Joshua’s interests include network
QoS, transport protocols, IP video delivery systems,
caching algorithms, and real-time software design.
Joshua holds an A.B. in Mathematics from Harvard
University.

733

Rong Pan has been active in the networking field
for more than ten years while working at Cisco
Systems, Stanford University, Bell Laboratories, and
startups. Currently she is a Distinguished Engineer
at Cisco where she heads a team in ENG CTO office.
She is an author for 30+ technical papers and an
inventor of 30+ patents. Her work and innovations
have been recognized and adopted widely in the
industry. Currently, she is working on the buffer
bloat problem in the Internet and leading Ciscos
effort at IETF and DOCSIS on this topic. Rong
has won a best paper award and served as program committee members at
IEEE conferences, and she will serve as the technical co-chair at IEEE High
Performance Switching and Routing Conference 2014.
Hao Hu received the B.S. degree from Nankai University and the M.S. degree from Tianjin University
in 2005 and 2007 respectively, and the Ph.D degree
from Polytechnic Institute of New York University
in Jan. 2012.
He is currently with ENG Labs, Cisco Systems,
San Jose, CA. He interned in the Corporate Research, Thomson Inc., NJ in 2008 and Cisco Systems, CA in 2011. His research interests include
video QoE, video streaming and adaptation, and
distributed systems.

Ali C. Begen is with the Video and Content Platforms Research and Advanced Development Group
at Cisco. His interests include networked entertainment, Internet multimedia, transport protocols
and content delivery. Ali is currently working on
architectures for next-generation video transport and
distribution over IP networks, and he is an active
contributor in the IETF and MPEG in these areas.
Ali holds a Ph.D. degree in electrical and computer engineering from Georgia Tech. He received
the Best Student-paper Award at IEEE ICIP 2003, the Most-cited Paper
Award from Elsevier Signal Processing: Image Communication in 2008,
and the Best-paper Award at Packet Video Workshop 2012. Ali has been
an editor for the Consumer Communications and Networking series in the
IEEE Communications Magazine since 2011 and an associate editor for the
IEEE Transactions on Multimedia since 2013. He is a senior member of the
IEEE and a senior member of the ACM. Further information on Alis projects,
publications and presentations can be found at http://ali.begen.net.

David Oran is a Fellow at Cisco Systems. His
technical interests lie in the areas of Quality of
Service, Internet multimedia, routing, and security.
He was part of the original team that started Cisco’s
Voice-over-IP business in 1996 and helped grow it
into a multi-billion dollar revenue stream. He is
currently working on adaptive bitrate video over
IP networks, and advanced research on Information
Centric Networks.
Prior to joining Cisco, Mr. Oran worked in the
network architecture group at Digital Equipment,
where he designed routing algorithms and a distributed directory system. Mr.
Oran has led a number of industry standards efforts. He was a member of the
Internet Architecture Board, co-chair of the Speech Services working group,
and served a term as area director for Routing in the IETF. He was on the
board of the SIP Forum from its inception through 2008. He also serves on
the technical advisory boards of a number of venture-backed firms in the
networking and telecommunication sectors.
Mr. Oran has a B.A. in English from Haverford College.

Fast DTT – A Near Linear Algorithm for Decomposing a
Tensor into Factor Tensors
Xiaomin Fang

Rong Pan

∗

Department of Computer Science
School of Information Science and Technology
Sun Yat-sen University
Guangzhou, China

Department of Computer Science
School of Information Science and Technology
Sun Yat-sen University
Guangzhou, China

fangxmin@mail2.sysu.edu.cn

panr@sysu.edu.cn

ABSTRACT

1.

As tensors provide a natural representation for the higherorder relations, tensor factorization techniques such as Tucker
decomposition and CANDECOMP/PARAFAC decomposition have been applied to many fields. Tucker decomposition
has strong capacity of expression, but the time complexity is
unpractical for the large-scale real problems. On the other
hand, CANDECOMP/PARAFAC decomposition is linear in
the feature dimensionality, but the assumption is so strong
that it abandons some important information. Besides, both
of TD and CP decompose a tensor into several factor matrices. However, the factor matrices are not natural for the
representation of the higher-order relations. To overcome
these problems, we propose a near linear tensor factorization approach, which decompose a tensor into factor tensors
in order to model the higher-order relations, without loss
of important information. In addition, to reduce the time
complexity and the number of the parameters, we decompose each slice of the factor tensors into two smaller matrices. We conduct experiments on both synthetic datasets
and real datasets. The experimental results on the synthetic
datasets validate that our model has strong capacity of expression. The results on the real datasets show that our
approach outperforms the state-of-the-art tensor factorization methods.

Matrix factorization is used for modeling the second-order
relations. For example, for the item recommendation problem, we can construct a user-item matrix. The user-item
matrix is factorized into two factor matrices, representing
the latent features of the users and the items, respectively.
From the users’ view, matrix factorization assumes the items
are divided into D item-groups, where D denote the number
of the elements in the vector. For user u, there is a corresponding vector of the factor matrix of the users, describing
the relation between that user and the items. Each element
of that vector represents the relevance between that user and
a item-groups. From the items’view, the situation is similar.
Tensor is a higher-order extension of matrix and tensor
factorization is a higher-order extension of matrix factorization. Since tensor decomposition can model the higherorder relations, tensor decomposition techniques are applied
to many fields, such are psychology, chemometrics, computer
vision and data mining. Traditional tensor decomposition
techniques like Tucker decomposition (TD) [21] and CANDECOMP/PARAFAC (CP) [2, 9] decomposition factorize
a tensor into several factor matrices. Take the problem of
personalized tag recommendation for example. Traditional
tensor decomposition techniques decompose a user-item-tag
tensor into three factor matrices. Just like matrix factorization, from the users’ view, for user u, there is a corresponding vector of the factor matrix of the users, indicating the relation between that user, the items and the
tag, but we can not explain the meaning of each element
in that vector by the same way we analyzing matrix factorization, since vector is not natural for the representation
of the relations between that user, the item-groups and the
tag-groups. Moreover, both of Tucker decomposition and
CANDECOMP/PARAFAC have some drawbacks. Tucker
decomposition has strong capacity of expression but it requires lots of computation power, which is unpractical for
large-scale real problems. The other tensor factorization
technique CANDECOMP/PARAFAC decomposition is linear in the feature dimensionality, but it makes a strong assumption that limits its capacity of expression.
To tackle all these problems, we propose a novel tensor factorization approach in this paper. We decompose a tensor
into several factor tensors instead of factor matrices. From
the users’ view, for user u, there is a corresponding matrix
of the factor tensor of the users, indicating the relation between that user, the items and the tag, and each element of
that matrix can be seen as the relevance between that user,

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval—Information filtering; I.2.6 [Artificial
Intelligence]: Learning

Keywords
Tensor decomposition; DTT

∗Corresponding author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD’14, August 24–27, 2014, New York, NY, USA.
Copyright 2014 ACM 978-1-4503-2956-9/14/08 ...$15.00.
http://dx.doi.org/10.1145/2623330.2623713 .

967

INTRODUCTION

a item-group and a tag-group. Thus, the higher-order relations can be modeled by a more natural way. In addition, to
reduce the time complexity and the number of the parameters, we decompose each slice of the factor tensors into two
smaller matrices. Our proposed model is near linear and has
strong capacity of expression.
The contributions of our work are summarized as follows.

On the other hand, some studies employ CP decomposition (CANDECOMP/PARAFAC decomposition) [5, 23, 17].
Dunlavy, Kolda, and Acar [5] incorporate the time information for link predictions. The first two dimensions of the
tensor represent the entities and the third dimension represents the time slices. They decompose the tensor by CP.
Similarly, Xiong et al. [23] take the time into consideration
and add a special constraint on the time dimension. Work by
Rendle and Schmidt-Thieme [17] puts forward a new tensor
factorization technique, Pairwise Interaction Tensor Factorization (PITF) for personalized tag recommendation. PITF
can be seen as a special case of CP, modeling the pairwise interactions between the three dimensions of the user-item-tag
tensor. The CP-based models can be trained in linear time,
but it abandons some important information and restricts
the capacity of expression.
Furthermore, both TD and CP decompose a tensor into
several factor matrices, but the factor matrices are not natural for the representation of the higher-order relations, since
matrices can only represent the second-order relations. Therefore, we propose a novel tensor factorization approach in this
paper, which decomposes a tensor into several factor tensors
instead.

• We propose a novel tensor decomposition approach,
decomposing a tensor into factor tensors rather than
factor matrices, which is more natural for the representation of the higher-order relations
• Our proposed approach has strong capacity of expression and is near linear, which is practical for the largescale real problems.
• Our experimental results on the synthetic datasets verify that our proposed approach has strong capacity
of expression and our proposed approach significantly
outperforms other tensor factorization techniques on
the real datasets.
The rest of the paper is organized as follows. The related
work to tensor factorization techniques is depicted in the
next section. The notations and preliminaries of tensor and
tensor factorization are presented in section 3. Our proposed
tensor decomposition approach is described in section 4. The
experimental results on the synthetic datasets and the real
datasets are showed and analyzed in section 5. Finally, we
conclude our work in section 6.

3.

3.1
2.

NOTATIONS AND PRELIMINARIES

In this section, we first introduce tensor and the notations.
Then, we depict two most common tensor factorization techniques, Tucker decomposition and CANDECOMP/PARAFAC,
and analyze the advantages and the disadvantages of them.

RELATED WORK

Since tensors provide a natural representation for the higherorder relations, they are applied to many fields [12], including chemometrics [1], neuroscience [13], graph analysis [18,
6, 5, 24], personalized recommendation [19, 24, 11, 14, 17,
7, 20, 16, 23] and so on. Tucker decomposition (TD) [21]
and CANDECOMP/PARAFAC decomposition (CP) [2, 9]
are the most common techniques to decompose a tensor,
where Tucker decomposition has stronger capacity of expression and CANDECOMP/PARAFAC decomposition can be
trained in linear time.
Some researches are based on Tucker decomposition (TD)
[3, 4, 19, 11, 14]. Higher-Order Singular Value Decomposition (HOSVD) [3, 4], a generation of the matrix Singular
Value Decomposition (SVD), is introduced for computing
Tucker decomposition. Sun et al. [19] construct a userquery-URL tensor from the clickthrough data in the search
engines and employ HOSVD to capture the latent factors of
users, queries and URLs for personalized web search. Karatzoglou et al. [11] integrate the context information into the
traditional collaborative filtering models to improve the recommendation quality by making use of HOSVD as well.
However, one of the drawbacks of HOSVD is that it can
not deal with the missing data in the sparse tensors. It
treats the values of all the missing elements in the tensors
as zeros. To fix this problem, Rendle et al. [14] propose
a method called RTF, which exploits Bayesian Personalized
Ranking (BPR) [15] and learns from pairwise constraints,
for personalized tag recommendation. Although TD has a
strong capacity of expression for the higher-order relations,
the time complexity of which is not feasible for large-scale
real problems.

968

Tensor

A tensor is a multidimensional or multi-way array and
the order of a tensor is the number of the dimensions or
modes. Vectors are first-order tensors, matrices are secondorder tensors and tensors of order three or higher are called
higher-order tensors. Matrices have column and row, a column and a row of which are called mode-1 vector and mode-2
vector respectively. Similar to matrices, the third-order tensors have column, row and tube and a tube is a mode-3 vector. Since the constructions of the higher-order tensors are
similar, we focus on the third-order tensors and the thirdorder tensors are called tensors for short in the following
paper.
Matrices and tensors are denoted by boldface capital letters, e.g., X. Element (i, j) of a matrix X is denoted by xij
and element (i, j, k) of a tensor A is denoted by aijk .

3.2

Tensor Decomposition

Just like matrix factorization, tensor decomposition techniques factorize a tensor into several components. Tucker decomposition (TD) [21] and CANDECOMP/PARAFAC decomposition (CP) [2, 9] are the most common tensor decomposition techniques and can be seen as higher-order generations of the matrix factorization, Singular Value Decomposition (SVD) [8].
Tensor decomposition techniques have been applied to
personalized tag recommendation. For the sake of convenience of analysis, let’s take the personalized tag recommendation problem for example in the following paper. A
user-item-tag tensor A ∈ RI×J×K is constructed from the
social tagging data, where I, J, K denote the number of the
users, the items and the tags, respectively. Element aijk of
tensor A measures the probability of the i-th user annotat-

K

K

D3 D2

Z
D1

A

I

$

I

X

D2

Y
J

J

D1
D3

Figure 1: DTT: A novel tensor factorization model
ing the j-th item with the k-th tag. We treat all of the users,
the items and the tags as three different types of entities.
Tucker decomposition (TD) [21] was first introduced by
Tucker in 1966. It decomposes a tensor into a core tensor
multiplied by a matrix along each mode. Element aijk of
tensor A ∈ RI×J×K can be written as
aijk =

D1 D2 D3
X
XX

cpqr · xip · yjq · zkr ,

compare our proposed approach with other tensor factorization techniques on the time complexity, the space complexity
and the capacity of expression.

4.1

(1)

p=1 q=1 r=1

where X ∈ RI×D1 , Y ∈ RJ×D2 , Z ∈ RK×D3 are the factor
matrices and C ∈ RD1 ×D2 ×D3 is the core tensor. D1 , D2
and D3 are the numbers of the latent features. Given a factor
matrix, a vector of that matrix represents the latent features
of the corresponding entity. For user i, item j and tag k,
cpqr of the core tensor C indicates the relevance between the
p-th feature of user i, the q-th feature of item j and the r-th
feature of tag k.
CANDECOMP/PARAFAC decomposition (CP) [2, 9] decomposes a tensor into a sum of component rank-one tensors. ai,j,k can be written as
aijk =

D
X

xip · yjp · zkp ,

(2)

p=1

where X ∈ RI×D , Y ∈ RJ×D , Z ∈ RK×D are the factor
matrices and D is the number of latent features. For user i,
item j and tag k, CP makes a strong assumption that the pth feature of user i, xip is only relevant to the p-th feature of
item j, yjp and the p-th feature of tag k, zkp . In other words,
it assumes xip is independent of yjq and zkr , where p 6= q
and p 6= r. As generally D << I, J, K on real problems, the
assumption limits CP’s capacity of expression.
TD is more flexible and can model the higher-order relations better, but it requires lots of computation power.
Therefore, TD is not practical for large-scale real problems.
On the other hand, CP is linear in the feature dimensionality, but it gives up some important information. To overcome the drawbacks, we propose a near linear tensor decomposition technique, which is practical for large-scale problems, without lost of information.

4.

Tensor Factorization Model DTT

Singular Value Decomposition (SVD) factorize a matrix
into two smaller factor matrices and the factor matrices
present the the second-order relations. Tensor factorization
is a higher-order generation of matrix factorization. Traditional tensor decomposition techniques like TD and CP,
decomposing a tensor A ∈ RI×J×K into factor matrices,
but the factor matrices are not proper for presenting the
higher-order relations. Different from TD and CP, we decompose the tensor A into factor tensors, since the representation of a factor tensor is more natural for the higher-order
relations, compared with the representation of a factor matrix. We refer our proposed tensor factorization approach to
decomposing a tensor into tensors (DTT). The framework
of our proposed tensor factorization model is illustrated in
Figure 1. From the figure, we can see that tensor A is factorized into three smaller factor tensors X ∈ RI×D2 ×D3 ,
Y ∈ RJ×D3 ×D1 and Z ∈ RK×D1 ×D2 , where D1 , D2 and
D3 are parameters of the DTT model (e.g., taking personalized social tagging as an example, they can been seen as
the numbers of the user groups, the item groups and the tag
groups, respectively).
Since we decompose a tensor into factor tensors rather
than factor matrices, the latent features of an entity are
represented by a matrix rather than a vector. D2 × D3 is
the size of the factor matrix of a user, D3 × D1 is the size
of the factor matrix of an item and D1 × D2 is the size of
the factor matrix of a tag. From the users’ view, the factor
matrix of user i, the i-th slice in X, which is denoted by
Xi:: , contains D2 rows and D3 columns. It can be assumed
that for user i, all the items are divided into D2 item-groups
and the q-th row of Xi:: represents the characters of the qth item-group. Similarly, for user i, all the tags are divided
into D3 tag-groups and the r-th column of Xi:: represents
the characters of the r-th tag-group. Thus, xiqr means the
relevance between user i, the items in the q-th item-group
and the tags in the r-th tag-group. That is to say, matrix
Xi:: indicates the preference of user i under different conditions. The analysis from the views of the items and the tags
is similar.
Our proposed approach DTT models the higher-order relations by modeling the relations between the user-groups,

DECOMPOSING A TENSOR INTO FACTOR TENSORS

In this section, we explain our proposed tensor factorization approach and how to compute our model. Besides, we

969

user i

Xi::,a slice of X

tg1
tg2
tg3
tg4
tg5

ug1
ug2
ug3

item j

Zk::,a slice of Z
step 3:
dot product of
the matrices

Yj::,a slice of Y

step 1:
matrix product

ug1
ug2
ug3

ig1
step 2:
transpose
ig2
user i,item j
ig3
ig4

tag k

predic!on of
the element aijk

ug1
user i,item j ug2
ug3
ig1
ig2
ig3
ig4

ig1
ig2
ig3
ig4

ig1
ig2
ig3
ig4

tg1
tg2
tg3
tg4
tg5

ug1
ug2
ug3

input:

Figure 2: Reconstruction of the tensor elements from the three slices of the factor tensors
where X ∈ RI×D2 ×D3 , Y ∈ RJ×D3 ×D1 and Z ∈ RK×D1 ×D2
are the factor tensors. As the order of xiqr , yjrp and zkpq
has no influence on computing aijk in Eq. (4), the order of
the factor matrices of user i, item j and tag k in Figure 2
has no influence on computing the relevance score, which is
a good property as follows.

the item-groups and the tag-groups. For triplet (i, j, k), the
way how we predict the value of that triplet is illustrated in
Figure 2, where ug, ig and tg are short for the user-group,
the item-group and the tag-group.
• In step 1, we compute the matrix product of the factor
matrix of user i, Xi:: ∈ RD2 ×D3 and the factor matrix
of item j, Yj:: ∈ RD3 ×D1 . Thus, the factor matrices
of user i and item j are integrated into a new factor
matrix G(ij) ∈ RD2 ×D1 by their multiplication. That
is, G(ij) = Xi:: Yj:: .

Property 1. The prediction of tensor element aijk is well
defined. That is,
D
E
aijk =
(Xi:: Yj:: )T , Zk::
(5)
D
E
=
(Yj:: Zk:: )T , Xi::
(6)
D
E
=
(Zk:: Xi:: )T , Yj:: .
(7)

• In step 2, we transpose the new factor matrix G(ij) ∈
RD2 ×D1 to make its dimension be the same as the dimension of the factor matrix of tag k, Zk:: ∈ RD1 ×D2 .

4.2

• Finally, in step 3, we define the prediction of the tensor element aijk as the inner product of two matrices
(G(ij) )T ∈ RD1 ×D2 and the factor matrix of tag k,
Zk:: ∈ RD1 ×D2 . That is,
E D
E
D
aijk = (G(ij) )T , Zk:: = (Xi:: Yj:: )T , Zk:: , (3)
where the inner product of matrices X, Y ∈ Rm×n is
defined as
m X
n
X
hX, Yi =
xij yij .
i=1 j=1

Note that Eq. (3) is equivalent to
aijk =

D1 D2 D3
X
XX

xiqr · yjrp · zkpq ,

Fast DTT

Although DTT can capture the higher-order relations, the
time complexity to calculate Eq. (4) is O(D1 D2 D3 ), which
is unacceptable for the large-scale real problems. Besides,
there are too many parameters in DTT that it may lead
to overfitting, the space complexity of which is O(ID2 D3 +
JD3 D1 + KD1 D2 ), where I, J and K denote the numbers
of the users, the items and the tags, repectively. Note that,
for each entity, there is a factor matrix describe the features
of that entity. To reduce the number of parameters and the
time complexity, we decompose the factor matrix of each entity into two smaller matrices. For user i, the factor matrix
Xi:: ∈ RD2 ×D3 are decomposed into two smaller matrices
(l)
(r)
Xi:: ∈ RD2 ×d1 and Xi:: ∈ RD3 ×d1 . For item j, the facD3 ×D1
tor matrix Yj:: ∈ R
are decomposed into two smaller
(l)
(r)
matrices Yj:: ∈ RD3 ×d2 and Yj:: ∈ RD1 ×d2 . For tag k,
D1 ×D2
the factor matrix Zk:: ∈ R
are decomposed into two

(4)

p=1 q=1 r=1

970

Table 1: Comparison between DTT, Tucker decomposition (TD) and CANDECOMP/PARAFAC (CP)
Algorithm
Time Complexity
Space Complexity
Train
Predict
DTT
O(Dd2 N T )
O(Dd2 )
O(Dd2 I)
3
3
TD
O(D N T )
O(D )
O(D3 I)
CP
O(DN T )
O(D)
O(DI)
(l)

(r)

smaller matrices Zk:: ∈ RD1 ×d3 and Zj:: ∈ RD2 ×d3 . d1 , d2
and d3 are the numbers of latent features of the factor matrices Xi:: , Yj:: and Zk:: , respectively. Additionally, d1 , d2
and d3 should satisfy d1 ≤ min(D2 , D3 ), d2 ≤ min(D3 , D1 )
and d3 ≤ min(D1 , D2 ). d1 , d2 and d3 depend on the complexity of the pairwise relations. For example, if the relation
between the items and the tags is complex for the users, d1
should be large so as to reconstruct the factor matrices of
the users and if the relation between the items and the tags
is simple, d1 is small. Concretely, from the users’ view, for
(l)
user i, matrix Xi:: indicates the relations between user i
(r)
and the item-groups and matrix Xi:: indicates the relations
between user i and the tag-groups. Thus, the multiplication
(r)
(l)
of Xi:: and Xi:: , Xi:: , indicates the relations between user
i, the item-groups and the tag-groups.
After decomposing each factor matrix into two smaller
matrices, aijk can be rewritten as
aijk =

D1 D2 D3 d1
X
XXX

(r)

(l)

xiqu xiru

d2
X

(r)

(l)

yjrv yjpv

∂aijk
(l)

∂aijk
(r)

∂aijk
(l)
∂zkpw

where
xiqr =

∂aijk

=

∂aijk
(l) (r)
xiqu xiru ,

(r)
∂zkqw

(r)

(3)

(l)

(3)

(r)

(1)

(l)

(1)

(r)

(2)

(l)

φijk (u, v) · yjrv ,
φijk (u, v) · xiru ,

v=1

=

∂yjpv

(r)

d2
X
v=1

∂aijk

(8)
d1
X

=

d2
X

(l)

(2)

φijk (w, u) · zkqw ,

w=1

∂yjrv

zkpw zkqw ,

d3
X

∂xiru

(r)

(l)

=

∂xiqu

w=1

v=1

p=1 q=1 r=1 u=1

d3
X

Tensor decomposition techniques are employed to many
applications. As for different applications, the objective
function are different, let’s assume the objective function
is F . Gradient-base approaches are common to learn the
parameters of the models and we use gradient-based approaches to optimize the objective function F . The gradients are

d1
X

φijk (v, w) · zkpw ,

u=1

=

d1
X

φijk (v, w) · yjpv ,

u=1

=

d3
X

φijk (w, u) · xiqu ,

w=1

u=1

yjrp =

d2
X

where we define
(r)

(l)

yjrv yjpv ,

v=1
d3

zkpq =

X

(1)

d1
X

(2)

d2
X

φijk (v, w) =
(l)

φijk (w, u) =
(3)

(1)

(2)

(3)

ψjk (v, w) · ψki (w, u) · ψij (u, v),

φijk (u, v) =

(9)

D1
X

(r)

(2)

D2
X

4.3

(3)

D3
X

(2)

(r)

(l)

zkqw · xiqu ,
(r)

Comparisons with TD and CP

For the simplicity of the analysis, we assume D1 = D2 =
D3 = D, d1 = d2 = d3 = d and I = J = K, where I, J
and K denote the number of the users, the items and the
tags. Let N denote the number of the triplets in the training
set and T denote the number of iterations for training. The
comparison of the time complexity and space complexity is
summarized in Table 1.
For TD, it makes the assumption that the users, the items
and the tags are divided into several groups and uses a core
tensor to model the higher-order relations between the usergroups, the item-groups and the tag-groups. Given user i,
item j, tag k, the higher-order relation between them is or-

q=1

ψij (u, v) =

(1)

ψjk (v, w) · ψki (w, u).

(l)

yjpv · zkpw ,

p=1

ψki (w, u) =

d3
X

Thus, given aijk in A, the gradients can be calculated in
O(D1 d2 d3 + D2 d3 d1 + D3 d1 d2 ). With the gradients, we can
optimize the objective function F .

where we define
(1)

(1)

w=1

u=1 v=1 w=1

ψjk (v, w) =

(3)

ψij (u, v) · ψjk (v, w),

v=1

Note that, Eq. (8) is equivalent to
aijk =

(3)

u=1

(r)

zkpw zkqw .

w=1

d1 d2 d3
X
XX

(2)

ψki (w, u) · ψij (u, v),

(l)

xiru · yjrv .

r=1

Eq. (9) can be calculated in O(D1 d2 d3 + D2 d3 d1 + D3 d1 d2 )
and generally, d1 , d2 , d3 << D1 , D2 , D3 . Therefore, DTT is
feasible for large-scale real problems. Besides, the number of
parameters is reduced from O(ID2 D3 + JD3 D1 + KD1 D2 )
to O(Id1 (D2 + D3 ) + Jd2 (D3 + D1 ) + Kd3 (D1 + D2 )).

971

core tensor

rela!on tensor

user

item

tag

(a) Tucker decomposition

relaon tensor
user

item

tag

(b) DTT
Figure 3: Comparison of Tucker decomposition and DTT
ganized into a relation tensor. The framework of Tucker
decomposition is illustrated in Figure 3(a). The three vectors on the left of the arrow describe the factors of the user,
the item and the tag, respectively. The core tensor and the
three vectors are integrated into a relation tensor, which is
on the right of the tensor. Although Tucker decomposition
is flexible and expressive, it takes O(D3 N T ) to train the
data and O(D3 ) to predict a triplet. Besides, since the core
tensor is shared by all the triplets, it is not suitable to apply
parallel computing to training the data. Therefore, TD is
unpractical on large-scale datasets.
On the other hand, it takes O(DN T ) to trained a CPbased model and O(D) to predict a triplet. Thus, CP is
efficient. Moreover, CP divides the users, the items and the
tags into several groups as well. However, it assumes the
users in the p-th user-group are independent of the items
in the q-th item-group and the tags in the r-th tag-group,
where p 6= q and p 6= r. In fact, D << I when solving
real problems and it is possible that a user in the first usergroup might annotate an item in the first item-group with a
tag in the first tag-group and annotate another item in the
second item-group with another tag in the third tag-group.
Therefore, The assumption is too strong that it reduce the
capacity of expression.
For user i, item j, tag k, DTT exploits a special multiplication xiqr yjrp zkpq to model the higher-order relation
between them. Just like TD, it organizes the relation into
a relation tensor as well, the framework of which is showed
in Figure 3(b). The three matrices on the left of the arrow
are the factor matrices of the user, the item and the tag
and the factor matrices are integrated into a relation tensor, which is on the right of the arrow. Since DTT doesn’t
make the strong independent assumption like CP, it is more
expressive. Moreover, DTT requires O(Dd2 N T ) to train
and O(Dd2 ) to predict, where d << D. Thus, DTT is near
linear. According to the experimental results on the real
datasets, d = 1 is enough to capture the higher-order relations. Thus, DTT is efficient for large-scale problems. Fur-

thermore, compared with TD and CP, DTT exploits a more
natural way to represent the features of an entity, by using
a matrix to describe the features of the entity rather than a
vector.

5.

EXPERIMENTAL RESULTS

In this section, we compare our proposed model with other
tensor factorization techniques on the synthetic datasets and
the real datasets. The experiments on the synthetic datasets
are used to compare the capacity of expression and the experiments on the real datasets are used to compare the accuracy for predicting a triplet of a tensor.

5.1

Experimental Setup

As tensor decomposition techniques are applied to personalized tag recommendation, we use real tag annotation
datasets to evaluate the performance of DTT. In addition,
we generate synthetic datasets to compare different factorization models’ capacity of expression.
Let A denote the tensor constructed from the training set
and P denote the observed user-item posts in the training
+
−
set. Given post (u, i), T(u,i)
and T(u,i)
denote the set of
the positive tags and the set of the negative tags for that
post, respectively. We construct a candidate set for each
post (u, i). If triplet (u, i0 , t) is observed for some item i0 or
triplet (u0 , i, t) is observed for some user u0 , t is treated as
the candidate tag for post (u, i). For tag t in the candidate
set of post (u, i), if triplet (u, i, t) is observed, t is treated as
the positive tag for post (u, i), otherwise, the negative tag.
Following the work of [14], we optimize the pair-wise ranking function Bayesian Personalized Ranking (BPR) [15] for
personalized tag recommendation. The objective function

972

F is defined as
X
1
F =
+
−
|T
||T
u,i
u,i |
(u,i)∈P
+λ

X

X

X

where prec(i) is the precision of the cutoff rank list from the
first tag to the i-th tag and N is the size of the corresponding
candidate set.

σ(auit+ − auit− )

+
−
t+ ∈Tu,i
t− ∈Tu,i

||θ − µ||2F ,

5.3
(10)

θ∈Θ

where λ is the weight of the regularization term, Θ are the
model parameters and µ denotes the mean of the parameters. We use Stochastic Gradient Descent (SGD) to optimize
the objective function F .

5.2

Performance on Synthetic Datasets

To evaluate different tensor factorization models’ capacity
of expression, we generate 10 synthetic datasets. There are
100 users, 100 items and 100 tags in each synthetic dataset.
For each dataset, We first randomly divided the users into
10 user-groups, divide the items into 10 item-groups and divide the tags into 10 tag-groups. We assume the users in the
i-th user-group always annotate the items in the j-th itemgroups with the tags in the g(i, j)-th tag-group, where g(i, j)
is randomly generated in the range 1 - 10. Thus, to sample a triplet, we first randomly sample a user and an item,
and then sample a tag in the g(i, j)-th tag group, where
i, j denote the IDs of the user-group that user belongs to
and the item-group that item belongs to, respectively. For
each dataset, we randomly sample 6000 triplets and split
the triplets into a training set with 5000 triplets and a test
set with 1000 triplets. By this way, we can compare different tensor factorization techniques’ capacity of expression
and verify whether the independent assumption made by
CP limits its capacity.

Evaluation Methodology

We compare our approach with three categories of baselines, CP-based, TD-based and popularity-based. PITF [17],
CP are CP-based, RTF [14] and HOSVD [3] are TD-based.
For popularity-based, given a post (u, i), method popularity
ranks the tags based on the frequency item i and the tags
co-occur in the training set.
For convenience, we set D1 = D2 = D3 = D and d1 =
d2 = d3 = d. We performed all the experiments multiple times and tuned the hyper-parameters of all models to
achieve the best performance on the first training split for
each dataset. Most of the parameters of the tensor factorization models are drawn from normal distribution N (µ, 0.012 ).
1
. For all the
For all the parameters in DTT, µ = √Dd
parameters in PITF, µ = 0.0. For all the parameters in
1
CP, µ = √
. For the parameters of the factor matrices in
3

0.2
0.18
0.16

D
1
D

0.14

RTF, µ =
and the parameters of the core tensor in RTF
are drawn from normal distribution N (0, 0.1). Besides, the
weight of the regularization term λ = 0.0001 on the synthetic datasets and λ = 0.002 on the real datasets for DTT
and CP, λ = 10−6 on all the datasets for RTF, and λ = 0.001
on synthetic datasets and λ = 0.01 on the real datasets for
PITF.
We exploit two widely used metrics in information retrieval, Normalized Discounted Cumulative Gain (NDCG)
and Mean Average Precision (MAP), to evaluate the performance of different models.
Discounted Cumulative Gain (DCG) evaluates the gain of
a recommendation list based on the positions of the tags in
that list. The NDCG accumulated at a particular position
p is defined as
DCG@p =

p
X
2rel(i) − 1
,
log2 (i + 1)
i=1

NDCG

0.12
0.1
0.08
0.06
0.04
0.02
0
1

2

3

4

5

6

7

8

9

10

Top p
Fast DTT

PITF

CP

HOSVD

RTF

Figure 4: Performance on synthetic datasets
The experiments are repeated 10 times. We compare DTT
to other tensor factorization models and for all tensor factorization models, D = 2, and for DTT d = 1. The results
on the synthetic datasets are showed in Figure 4. HOSVD
works poorly, since it doesn’t handle the missing value in
the tensors. As the other TD-based model, RTF, exploits
the pair-wise ranking function Bayesian Personalized Ranking (BPR) [15], the performance is much better. Moreover,
as we expected, DTT and RTF outperforms the CP-based
models, PITF and CP, because the independent assumption
constraints CP-based models’ capacity. CP-based models
assumes the users in the i-th user group will only annotate
the items in the i-th item groups and annotate the items
with the tags in the i-th tag group, which is not always the
truth. Thus, DTT and TD-based models has stronger capacity of expression than CP-based models. Besides, DTT
and TD-based models like RTF model the higher-order relation by a relation tensor, but it seems the method DTT
exploiting to construct the relation tensor is better.

(11)

where rel(i) is 1 if the tag at position i i relevant and 0, otherwise. Normalized Discounted Cumulated Gain (NDCG) is
defined as
DCG@p
N DCG@p =
(12)
IDCG@p
where IDCG (Ideal Discounted Cumulated Gain) is the DCG
of the ideal ranked list, which is introduced to normalize the
DCG.
Mean Average Precision (MAP) computes the mean of
average precision (AP) over all posts in the test set, where
AP is the average of precisions computed at all positions
with a preferred tag and defined as
PN
i=1 prec(i) × rel(i)
AP =
,
(13)
PN
i=1 rel(i)

973

Dataset
Delicious
Last.fm
Movielens
Delicious-large

Users
1,681
1,348
456
433,791

category

algorithm

DTT
TD

CP

popularity

5.4

Items
29,540
6,927
1,973
1,241,772

Table 2: Data statistics
Tags
Posts
Triplets
7,251
58,114
283,030
2,132
59,849
162,047
1,222
15,210
27,0l26
125,455 30,538,733 71,709,640

Density
7.89 × 10−7
8.14 × 10−6
2.46 × 10−5
1.06 × 10−9

Table 3: Performance on the smaller real datasets
#feature
Delicious
Last.fm
D
d
MAP
NDCG@5
MAP
NDCG@5
DTT
32
1
0.1042
0.2317
0.4184
0.6446
64
1
0.1111
0.2395
0.4300
0.6561
RTF
32
64
HOSVD
32
64
PITF
32
0.1050
0.2381
0.4126
0.6463
64
0.1059
0.2398
0.4146
0.6482
CP
32
0.1054
0.2419
0.3808
0.6126
64
0.1040
0.2382
0.3782
0.6115
popularity
0.1004
0.1581
0.3151
0.4995

Performance on Real Datasets

Average Candidates
131.79
58.90
85.96
257.10

Movielens
MAP
NDCG@5
0.4608
0.6028
0.4764
0.6188
0.3285
0.4846
0.3474
0.5004
0.0685
0.1442
0.0790
0.1679
0.4464
0.6015
0.4503
0.6058
0.3920
0.5380
0.3944
0.5389
0.2195
0.3405

from the table that the MAP of DTT is significantly higher
than that of the other approaches on all the real dataset
and the NDCG@5 of DTT is significantly higher than that
of the other approaches on the real datasets Last.fm and
Movielens.
In the next experiment, we split Delicious-large into training (90%) and test (10%). As PITF is the most competitive
approach on the smaller datasets, we only compare DTT
with PITF on Delicious-large. The results can be found in
Table 4, which show that DTT outperform PITF on the
larger dataset as well.

In addition to the synthetic datasets, we evaluate the
performance of DTT on real datasets Delicoius 1 , Last.fm 2 ,
Movielens 3 and Delicious-large 4 [22]. The first three smaller
datasets come from the 2-nd International Workshop on Information Heterogeneity and Fusion in Recommender Systems (HetRec 2011). The larger dataset Delicious-large [22]
contains the complete bookmarking activity for almost 2
million users from the launch of the social bookmarking website in 2003 to the end of March 2011. For each real dataset,
we removed the infrequent users, items and tags by using a
core-based approach [10]. For the smaller datasets, we perform the removal until every user, item and tag occurred
in at least 5 triplets. For Delicious-large, as the original
dataset is too large, we first randomly sample 50% of the
users, the items and the tags, and then perform the corebased approach until every user, item and tag occurred in
at least 10 triplets. The statistics of the datasets after removal are described in Table 2. Average Candidates means
the average number of the candidate tags for each post in
each dataset.
We perform 10-fold cross-validation for the smaller datasets.
The comparison between DTT, PITF, CP, HOSVD, RTF
and popularity on the smaller real datasets are showed in
Table 3. As TD-based models require a lot of computation
power, we conducted experiments only on the smallest real
dataset Movielens for TD-based models, HOSVD and RTF.
Although TD-based models have stronger capacity of expression than CP-based models, they work poorer than the
CP-based models on the real datasets. Maybe the core tensor of TD is too complex that it leads to overfitting. Both of
TD and DTT model the higher-order relations into relation
tensors, but it seems DTT works much better. It can be seen

Table 4: Performance
algorithm #feature
D
d
DTT
64
1
PITF
64
-

on Delicious-large
Delicious-large
MAP NDCG@5
0.4674
0.6847
0.4612
0.6803

Finally, we investigate the influence of the number of features to DTT. The results with different number of features
on the real datasets are showed in Figure 5. It can be seen
from the figure that with the same D, the prediction quality
of the models with d = 1 is comparable with the models
with d = 2 on all the real datasets. d has no significant
influence to the accuracy of the models on the real datasets.
Thus, d = 1 is sufficient to capture the higher-order relations
on the personalized tag recommendation datasets. As the
time complexity of DTT to predict a triplet is O(Dd2 ) and
at most of the time d = 1, the time complexity is reduced
to O(D). Thus, DTT is near linear and is efficient for the
large-scale real problems.

6.

1

http://www.delicious.com
http://www.lastfm.com
3
http://www.grouplens.org
4
http://www.zubiaga.org/resources/socialbm0311
2

CONCLUSION AND FUTURE WORK

We propose a novel tensor factorization technique in this
paper. Different from the traditional tensor factorization
techniques which decompose a tensor into factor matrices,

974

0.28

0.72

0.66

0.27

0.7

0.64

0.68

0.62

0.26

0.24
0.23

0.6

0.64

0.58
NDCG

NDCG

NDCG

0.25

0.66

0.62

0.56

0.6

0.54

0.58

0.52

0.22
0.21

0.56

0.5

0.54

0.48
0.46

0.52

0.2
1

2

3

4

5

6

7

8

9

10

1

2

3

4

5

Top p

6

7

8

9

10

1

2

3

4

5

6

7

8

9

10

Top p

Top p

D=16,d=1

D=32,d=1

D=64,d=1

D=128,d=1

D=16,d=1

D=32,d=1

D=64,d=1

D=128,d=1

D=16,d=1

D=32,d=1

D=64,d=1

D=128,d=1

D=16,d=2

D=32,d=2

D=64,d=2

D=128,d=2

D=16,d=2

D=32,d=2

D=64,d=2

D=128,d=2

D=16,d=2

D=32,d=2

D=64,d=2

D=128,d=2

(a) Delicious

(b) Last.fm

(c) Movielens

Figure 5: Performance with different number of features on the real datasets
we decompose a tensor into factor tensors, since factor tensors can provide a more natural representation for the higherorder relations. Besides, we compare our proposed approach
with two commonly used tensor factorization techniques,
Tucker decomposition and CANDECOMP/PARAFAC. Our
proposed approach overcome some of the drawbacks of Tucker
decomposition and CANDECOMP/PARAFAC. It not only
has strong capacity of expression, but also is near linear,
which is feasible for the large-scale real problems. The experimental results on the synthetic datasets validate that
our approach has strong capacity of expression. The results
on the real datasets show that our approach outperforms
other tensor factorization techniques.
In the future, we plan to further reduce the time complexity and space complexity to compute our proposed approach,
by investigating its special case.

[5]

[6]

[7]

[8]

Acknowledgements
[9]

We would like to thank the many referees of the previous
version of this paper for their extremely useful suggestions
and comments. This work was supported by Huawei Innovation Research Program (HIRP) and National Science
Foundation of China (61033010).

7.

[10]

REFERENCES

[1] Carl J Appellof and ER Davidson. Strategies for
analyzing data from video fluorometric monitoring of
liquid chromatographic effluents. Analytical
Chemistry, 53(13):2053–2056, 1981.
[2] J.D. Carroll and J.J. Chang. Analysis of individual
differences in multidimensional scaling via an n-way
generalization of “eckart-young” decomposition.
Psychometrika, 35(3):283–319, 1970.
[3] Lieven De Lathauwer, Bart De Moor, and Joos
Vandewalle. A multilinear singular value
decomposition. SIAM journal on Matrix Analysis and
Applications, 21(4):1253–1278, 2000.
[4] Lieven De Lathauwer, Bart De Moor, and Joos
Vandewalle. On the best rank-1 and rank-(r 1, r 2,...,
rn) approximation of higher-order tensors. SIAM

[11]

[12]

[13]

975

Journal on Matrix Analysis and Applications,
21(4):1324–1342, 2000.
Daniel M Dunlavy, Tamara G Kolda, and Evrim Acar.
Temporal link prediction using matrix and tensor
factorizations. ACM Transactions on Knowledge
Discovery from Data (TKDD), 5(2):10, 2011.
Beyza Ermiş, Evrim Acar, and A Taylan Cemgil. Link
prediction via generalized coupled tensor factorisation.
arXiv preprint arXiv:1208.6231, 2012.
Dehong Gao, Renxian Zhang, Wenjie Li, and Yuexian
Hou. Twitter hyperlink recommendation with
user-tweet-hyperlink three-way clustering. In
Proceedings of the 21st ACM international conference
on Information and knowledge management, pages
2535–2538. ACM, 2012.
Gene H Golub and Christian Reinsch. Singular value
decomposition and least squares solutions. Numerische
Mathematik, 14(5):403–420, 1970.
Richard A. Harshman. Foundations of the PARAFAC
procedure: Models and conditions for an
“explanatory” multi-modal factor analysis. UCLA
working papers in phonetics, 16:1, 1970.
Robert Jäschke, Leandro Marinho, Andreas Hotho,
Lars Schmidt-Thieme, and Gerd Stumme. Tag
recommendations in social bookmarking systems. Ai
Communications, 21(4):231–247, 2008.
Alexandros Karatzoglou, Xavier Amatriain, Linas
Baltrunas, and Nuria Oliver. Multiverse
recommendation: n-dimensional tensor factorization
for context-aware collaborative filtering. In
Proceedings of the fourth ACM conference on
Recommender systems, pages 79–86. ACM, 2010.
Tamara G Kolda and Brett W Bader. Tensor
decompositions and applications. SIAM review,
51(3):455–500, 2009.
Fumikazu Miwakeichi, Eduardo Martınez-Montes,
Pedro A Valdés-Sosa, Nobuaki Nishiyama, Hiroaki
Mizuhara, and Yoko Yamaguchi. Decomposing eeg
data into space–time–frequency components using

[14]

[15]

[16]

[17]

[18]

parallel factor analysis. NeuroImage, 22(3):1035–1045,
2004.
Steffen Rendle, Leandro Balby Marinho, Alexandros
Nanopoulos, and Lars Schmidt-Thieme. Learning
optimal ranking with tensor factorization for tag
recommendation. In Proceedings of the 15th ACM
SIGKDD international conference on Knowledge
discovery and data mining, pages 727–736. ACM,
2009.
Steffen Rendle, Christoph Freudenthaler, Zeno
Gantner, and Lars Schmidt-Thieme. Bpr: Bayesian
personalized ranking from implicit feedback. In
Proceedings of the Twenty-Fifth Conference on
Uncertainty in Artificial Intelligence, pages 452–461.
AUAI Press, 2009.
Steffen Rendle, Christoph Freudenthaler, and Lars
Schmidt-Thieme. Factorizing personalized markov
chains for next-basket recommendation. In Proceedings
of the 19th international conference on World wide
web, pages 811–820. ACM, 2010.
Steffen Rendle and Lars Schmidt-Thieme. Pairwise
interaction tensor factorization for personalized tag
recommendation. In Proceedings of the third ACM
international conference on Web search and data
mining, pages 81–90. ACM, 2010.
Stephan Spiegel, Jan Clausen, Sahin Albayrak, and
Jérôme Kunegis. Link prediction on evolving data
using tensor factorization. In New Frontiers in Applied
Data Mining, pages 100–110. Springer, 2012.

[19] Jian-Tao Sun, Hua-Jun Zeng, Huan Liu, Yuchang Lu,
and Zheng Chen. Cubesvd: a novel approach to
personalized web search. In Proceedings of the 14th
international conference on World Wide Web, pages
382–390. ACM, 2005.
[20] Panagiotis Symeonidis, Alexandros Nanopoulos, and
Yannis Manolopoulos. Tag recommendations based on
tensor dimensionality reduction. In Proceedings of the
2008 ACM conference on Recommender systems,
pages 43–50. ACM, 2008.
[21] Ledyard R. Tucker. Some mathematical notes on
three-mode factor analysis. Psychometrika,
31:279–311, 1966.
[22] Robert Wetzker, Carsten Zimmermann, and Christian
Bauckhage. Analyzing social bookmarking systems: A
del.icio.us cookbook. In Mining Social Data (MSoDa)
Workshop Proceedings, pages 26–30, 2008.
[23] Liang Xiong, Xi Chen, Tzu-Kuo Huang, Jeff G
Schneider, and Jaime G Carbonell. Temporal
collaborative filtering with bayesian probabilistic
tensor factorization. In SDM, volume 10, pages
211–222, 2010.
[24] Nan Zheng, Qiudan Li, Shengcai Liao, and Leiming
Zhang. Flickr group recommendation based on tensor
decomposition. In Proceedings of the 33rd
international ACM SIGIR conference on Research and
development in information retrieval, pages 737–738.
ACM, 2010.

976

2013 IEEE 14th International Conference on High Performance Switching and Routing

PIE: A Lightweight Control Scheme to Address the
Bufferbloat Problem
Rong Pan, Preethi Natarajan, Chiara Piglione, Mythili Suryanarayana Prabhu,
Vijay Subramanian, Fred Baker and Bill VerSteeg
Advanced Architecture & Research Group, Cisco Systems Inc., San Jose, CA 95134, U.S.A.
{ropan, prenatar, cpiglion, mysuryan, vijaynsu, fred, versteb}@cisco.com

Abstract—Bufferbloat is a phenomenon where excess buffers in the
network cause high latency and jitter. As more and more interactive
applications (e.g. voice over IP, real time video conferencing and financial
transactions) run in the Internet, high latency and jitter degrade
application performance. There is a pressing need to design intelligent
queue management schemes that can control latency and jitter; and hence
provide desirable quality of service to users.
We present here a lightweight design, PIE (Proportional Integral
controller Enhanced), that can effectively control the average queueing
latency to a reference value. The design does not require per-packet extra
processing, so it incurs very small overhead and is simple to implement
in both hardware and software. In addition, the design parameters are
self-tuning, and hence PIE is robust and optimized for various network
scenarios. Simulation results, theoretical analysis and Linux testbed
results show that PIE can ensure low latency and achieve high link
utilization under various congestion situations.
Index Terms—bufferbloat, Active Queue Management (AQM), Quality
of Service (QoS), Explicit Congestion Notification (ECN)

An Example of Extreme Long Latency: RTTs measured using
ping messages sent overnight from a hotel in Ireland to San Jose, CA.
Fig. 1.

I. I NTRODUCTION
The explosion of smart phones, tablets and video traffic in the
Internet brings about a unique set of challenges for congestion
control. To avoid packet drops, many service providers or data center
operators require vendors to put in as much buffer as possible.
With rapid decrease in memory chip prices, these requests are easily
accommodated to keep customers happy. However, the above solution
of large buffers fails to take into account the nature of TCP, the
dominant transport protocol running in the Internet. The TCP protocol
continuously increases its sending rate and causes network buffers to
fill up. TCP cuts its rate only when it receives a packet drop or mark
that is interpreted as a congestion signal. However, drops and marks
usually occur when network buffers are full or almost full. As a result,
excess buffers, initially designed to avoid packet drops, would lead
to highly elevated queueing latency and jitter. The phenomenon was
detailed in 2009 [1] and the term, “bufferbloat” was introduced by
Jim Gettys in late 2010 [2].
Figure 1 shows an example of extremely long latencies that are
caused by the bufferbloat problem. Ping messages were sent overnight
from a hotel in Ireland to San Jose, CA on January 27, 2012. Figure
1 depicts frequent delays in the neighborhood of 8 to 9 seconds.
A review of the round trip time distribution, shown in Figure 2,
reveals that as many as eight copies of the same TCP segment, spaced
Retransmission Time Out (RTO) apart, were present at some time in
the hotel’s DSL service. This obviously reduces effective bandwidth;
any bandwidth used for an unnecessary retransmission is unavailable
for valuable data. It also reduces the Quality of Experience (QoE)
of users. If the service providers want to provide additional valueadd services, such as high volume video content delivery in these
networks, it is necessary to get control of the data buffered in
networks.
Active queue management (AQM) schemes, such as RED [3],
BLUE [4], PI [5], AVQ [6], etc, have been around for well over
a decade. AQM schemes could potentially solve the aforementioned

978-1-4673-4620-7/13/$31.00 ©2013 IEEE

Experiment RTT Distributions: number of occurrences as a
function of RTTs. There are spikes that are RTOs apart.
Fig. 2.

problem. RFC 2309 [7] strongly recommends the adoption of AQM
schemes in the network to improve the performance of the Internet.
RED is implemented in a wide variety of network devices, both in
hardware and software. Unfortunately, due to the fact that RED needs
careful tuning of its parameters for various network conditions, most
network operators do not turn RED on. In addition, RED is designed
to control the queue length which would affect delay implicitly. It
does not control latency directly.
Note that the delay bloat caused by poorly managed big buffer
is really the issue here. If latency can be controlled, bufferbloat,
i.e., adding more buffers for bursts, is not a problem. More buffer
space would allow larger bursts of packets to pass through as long

148

as we control the average queueing delay to be small. Unfortunately,
Internet today still lacks an effective design that can control buffer
latency to improve the quality of experience to latency-sensitive
applications. In addition, it is a delicate balancing act to design a
queue management scheme that not only allows short-term burst to
smoothly pass, but also controls the average latency when long-term
congestion persists.
Recently, a new AQM scheme, CoDel [8], was proposed to
control the latency directly to address the bufferbloat problem. CoDel
requires per packet timestamps. Also, packets are dropped at the
dequeue function after they have been enqueued for a while. Both of
these requirements consume excessive processing and infrastructure
resources. This consumption will make CoDel expensive to implement and operate, especially in hardware.
In this paper, we present a lightweight algorithm, PIE (Proportional
Integral controller Enhanced), which combines the benefits of both
RED and CoDel: easy to implement like RED while directly control
latency like CoDel. Similar to RED, PIE randomly drops a packet
at the onset of the congestion. The congestion detection, however, is
based on the queueing latency like CoDel instead of the queue length
like conventional AQM schemes such as RED. Furthermore, PIE also
uses the latency moving trends: latency increasing or decreasing, to
help determine congestion levels.
Our simulation and lab test results show that PIE can control
latency around the reference under various congestion conditions. It
can quickly and automatically respond to network congestion changes
in an agile manner. Our theoretical analysis guarantees that the PIE
design is stable for arbitrary number of flows with heterogeneous
round trip times under a predetermined limit.
In what follows, Section II specifies our goals of designing the
latency-based AQM scheme. Section III explains the scheme in
detail. Section IV presents simulation and lab studies of the proposed
scheme. In Section V, we present a control theory analysis of PIE.
Section VI concludes the paper and discusses future work.
II. D ESIGN G OALS
We explore a queue management framework where we aim
to improve the performance of interactive and delay-sensitive
applications. The design of our scheme follows a few basic criteria.
•

•

•

Low Latency Control. We directly control queueing latency
instead of controlling queue length. Queue sizes change with
queue draining rates and various flows’ round trip times. Delay
bloat is the real issue that we need to address as it impairs
real time applications. If latency can be controlled to be small,
bufferbloat is not an issue. As a matter of fact, we would allow
more buffers for sporadic bursts as long as the latency is under
control.
High Link Utilization. We aim to achieve high link utilization.
The goal of low latency shall be achieved without suffering
link under-utilization or losing network efficiency. An early
congestion signal could cause TCP to back off and avoid queue
buildup. On the other hand, however, TCP’s rate reduction
could result in link under-utilization. There is a delicate balance
between achieving high link utilization and low latency.
Simple Implementation. The scheme should be simple to
implement and easily scalable in both hardware and software.
The wide adoption of RED over a variety of network
devices is a testament to the power of simple random early
dropping/marking. We strive to maintain similar design

Fig. 3. Overview of the PIE Design. The scheme comprises three simple

components: a) random dropping at enqueuing; b) latency based drop
probability update; c) dequeuing rate estimation.

simplicity.
•

Guaranteed stability and Fast Responsiveness. The scheme
should ensure system stability for various network topologies
and scale well with arbitrary number streams. The system
also should be agile to sudden changes in network conditions.
Design parameters shall be set automatically. One only needs to
set performance-related parameters such as target queue delay,
not design parameters.

We aim to find an algorithm that achieves the above goals. It is
noted that, although important, fairness is orthogonal to the AQM
design whose primary goal is to control latency for a given queue.
Techniques such as Fair Queueing [9] or its approximate such as
AFD (Approximate Fair Dropping) [10] can be combined with any
AQM scheme to achieve fairness. Therefore, in this paper, we focus
on controlling a queue’s latency and ensuring flows’ fairness is not
worse than those under the standard DropTail or RED design.
III. T HE PIE S CHEME
In the section, we describe in detail the design of PIE and its
operations. As illustrated in Figure 3, our scheme comprises three
simple components: a) random dropping at enqueuing; b) periodic
drop probability update; c) dequeuing rate estimation.
The following subsections describe these components in further
detail, and explain how they interact with each other. At the end of
this section, we will discuss how the scheme can be easily augmented
to precisely control bursts.
A. Random Dropping
Like most state-of-the-art AQM schemes, PIE would drop packets
randomly according to a drop probability, p, that is obtained from
the “drop probability calculation” component. No extra step, like
timestamp insertion, is needed. The procedure is as follows:
Random Dropping:
Upon packet arrival

149

randomly drop a packet with a probability p.

3. Calculate drop probability as:

B. Drop Probability Calculation
The PIE algorithm updates the drop probability periodically as
follows:
•

•

•

p = p + α ∗ (cur del − ref del) + β ∗ (cur del − old del);
4. Update previous delay sample as:

estimate current queueing delay using Little’s law:
cur del = avgqlen
;
drate

old del = cur del.

calculate drop probability p as:
p = p+α∗(cur del −ref del)+β ∗(cur del −old del);
update previous delay sample as:
old del = cur del.

The average draining rate of the queue, avg drate, is obtained
from the “departure rate estimation” block. Variables, cur del and
old del, represent the current and previous estimation of the queueing
delay. The reference latency value is expressed in ref del. The
update interval is denoted as Tupdate . Parameters α and β are scaling
factors.
Note that the calculation of drop probability is based not only on
the current estimation of the queueing delay, but also on the direction
where the delay is moving, i.e., whether the delay is getting longer
or shorter. This direction can simply be measured as the difference
between cur del and old del. Parameter α determines how the
deviation of current latency from the target value affects the drop
probability; β exerts the amount of additional adjustments depending
on whether the latency is trending up or down. The drop probability
would be stabilized when the latency is stable, i.e. cur del equals
old del; and the value of the latency is equal to ref del. The relative
weight between α and β determines the final balance between latency
offset and latency jitter. This is the classic Proportional Integral
controller design [11], which has been adopted for controlling the
queue length before in [5] and [12]. We adopt it here for controlling
queueing latency. In addition, to further enhance the performance, we
improve the design by making it auto-tuning as follows:

We have discussed packet drops so far. The algorithm can be
easily applied to networks codes where Early Congestion Notification
(ECN) is enabled. The drop probability p could simply mean marking
probability.
C. Departure Rate Estimation
The draining rate of a queue in the network often varies either
because other queues are sharing the same link, or the link capacity
fluctuates. Rate fluctuation is particularly common in wireless
networks. Hence, we decide to measure the departure rate directly
as follows:
Departure Rate Calculation:
Upon packet departure
1. Decide to be in a measurement cycle if:
qlen > dq threshold;
2. If the above is true, update departure count dq count:
dq count = dq count + dq pktsize;
3. Update departure rate once dq count > dq threshold and
reset counters:

if p < 1%: α = α̃/8; β = β̃/8;
else if p < 10%: α = α̃/2; β = β̃/2;
else: α = α̃; β = β̃;
where α̃ and β̃ are static configured parameters. Auto-tuning would
help us not only to maintain stability but also to respond fast to
sudden changes. The intuitions are the following: to avoid big
swings in adjustments which often leads to instability, we would like
to tune p in small increments. Suppose that p is in the range of 1%,
then we would want the value of α and β to be small enough, say
0.1%, adjustment in each step. If p is in the higher range, say above
10%, then the situation would warrant a higher single step tuning,
for example 1%. The procedures of drop probability calculation can
be summarized as follows.
Drop Probability Calculation:
Every Tupdate interval
1. Estimation current queueing delay:
cur del =

qlen
.
avg drate

2. Based on current drop probability, p, determine suitable step
scales:
if p < 1%,

α=

α̃/8; β = β̃/8;

else if p < 10%,

α=

α̃/2; β = β̃/2;

else ,

α=

α̃; β = β̃;

dq int

=

dq rate

=

avg drate

=

now − start;
dq count
;
dq int
(1 − ε) ∗ avg drate + ε ∗ dq rate

start

=

now.

dq count

=

0;

From time to time, short, non-persistent bursts of packets result
in empty queues, this would make the measurement less accurate.
Hence we only measure the departure rate, dq rate, when there
are sufficient data in the buffer, i.e., when the queue length is over
a certain threshold, dq threshold. Once this threshold is crossed,
we obtain a measurement sample. The samples are exponentially
averaged, with averaging parameter ε, to obtain the average dequeue
rate, avg drate. The parameter, dq count, represents the number
of bytes departed since the last measurement. The threshold is
recommended to be set to 10KB assuming a typical packet size of
around 1KB or 1.5KB. This threshold would allow us a long enough
period, dq int, to obtain an average draining rate but also fast enough
to reflect sudden changes in the draining rate. Note that this threshold
is not crucial for the system’s stability.
D. Handling Bursts
The above three components form the basis of the PIE algorithm.
Although we aim to control the average latency of a congested queue,
the scheme should allow short term bursts to pass through the system
without hurting them. We would like to discuss how PIE manages
bursts in this section.

150

Bursts are well tolerated in the basic scheme for the following
reasons: first, the drop probability is updated periodically. Any short
term burst that occurs within this period could pass through without
incurring extra drops as it would not trigger a new drop probability
calculation. Secondly, PIE’s drop probability calculation is done
incrementally. A single update would only lead to a small incremental
change in the probability. So if it happens that a burst does occur
at the exact instant that the probability is being calculated, the
incremental nature of the calculation would ensure its impact is kept
small.
Nonetheless, we would like to give users a precise control of the
burst. We introduce a parameter, max burst, that is similar to the
burst tolerance in the token bucket design. By default, the parameter
is set to be 100ms. Users can certainly modify it according to their
application scenarios. The burst allowance is added into the basic
PIE design as follows:

(a) Light 5 TCP Flows

Burst Allowance Calculation:

(b) Heavy 50 TCP Flows

(c) Mix 5TCP + 2UDP Flows

Upon packet arrival
1. If burst allow > 0

Fig. 4. Queueing Latency Under Various Traffic Loads: a) 5 TCP flows;

b) 50 TCP flows; c) 5 TCP + 2 UDP flows. Queueing latency is controlled
at the reference level of 20ms regardless of the traffic intensity.

enque packet bypassing random drop;
Upon dq rate update
2. Update burst allowance:

their default values; i.e., delay ref = 20ms, Tupdate = 30ms, α̃ =
0.125Hz, β̃ = 1.25Hz, dq threshold = 10KB, max burst = 100ms.

burst allow = burst allow − dq int;
3. if p = 0; and both cur del and old del less than
ref del/2, reset burst allow,
burst allow = max burst;

The burst allowance, noted by burst allow, is initialized to
max burst. As long as burst allow is above zero, an incoming
packet will be enqueued bypassing the random drop process. Whenever dq rate is updated, the value of burst allow is decremented
by the departure rate update period, dq int. When the congestion
goes away, defined by us as p equals to 0 and both the current and
previous samples of estimated delay are less than ref del/2, we
reset burst allow to max burst.

(a) Light 5 TCP Flows

(b) Heavy 50 TCP Flows

IV. P ERFORMANCE E VALUATION
We evaluate the performance of the PIE scheme in both ns2 simulations and testbed experiment using Linux machines. As
the latest design CoDel is in Linux release, we compare PIE’s
performance against RED in simulations and against CoDel in testbed
evaluations.
A. Simulations Evaluation
In this section we present our ns-2 [13] simulations results. We first
demonstrate the basic functions of PIE using a few static scenarios;
and then we compare PIE and RED performance using dynamic scenarios. We focus our attention on the following performance metrics:
instantaneous queue delay, drop probability, and link utilization.
The simulation setup consists of a bottleneck link at 10Mbps
with a RTT of 100ms. Unless otherwise stated the buffer size is
200KB. We use both TCP and UDP traffic for our evaluations.
All TCP traffic sources are implemented as TCP New Reno
with SACK running an FTP application. While UDP traffic is
implemented using Constant Bit Rate (CBR) sources. Both UDP
and TCP packets are configured to have a fixed size of 1000B.
Unless otherwise stated the PIE parameters are configured to

(c) Mix 5TCP + 2UDP Flows
Fig. 5. Link Throughput Under Various Traffic Loads: a) 5 TCP flows; b)

50 TCP flows; c) 5 TCP + 2 UDP flows. High link utilization is achieved
regardless of traffic intensity, even under low multiplexing case.

Function Verification: We first validate the functionalities of PIE,
making sure it performs as designed using static traffic sources with
various loads.
1) Light TCP traffic: Our first simulation evaluates PIE’s performance under light traffic load of 5 TCP flows. Figure 4(a) shows the
queueing delay, and Figure 5(a) plots the instantaneous throughput
respectively. Figure 4(a) demonstrates that the PIE algorithm is able to
maintain the queue delay around the equilibrium reference of 20ms.
Due to low multiplexing, TCP’s sawtooth behavior is still slightly
visible in Figure 4(a). Nonetheless, PIE regulates the TCP traffic
quite well so that the link is close to its full capacity as shown in
Figure 5(a). The average total throughput is 9.82Mbps. Individual

151

PIE’s Burst Control: at simulation time 1s, the UDP flow starts
sending traffic. With a burst allowance of 100ms, the flow can either
pass through without incurring drops or starting incurring drops at 1.1sec.
With a burst allowance of 0ms, the flow would incur drops right from
the beginning. The longer the burst is, the higher the number of drops is.

Fig. 6.

flows’ throughputs are 1.86Mbps, 2.15Mbps, 1.80Mbps, 1.92Mbps
and 2.09Mbps respectively, close to their fair share.
2) Heavy TCP traffic: In this test scenario, we increase the number
of TCP flows to 50. With higher traffic intensity, the link utilization
reaches 100% as clearly shown in Figure 5(b). The queueing delay,
depicted in Figure 4(b), is controlled around the desired 20ms,
unaffected by the increased traffic intensity. The effect of sawtooth
fades in this heavy traffic case. The queueing delay fluctuates more
evenly around the reference level. The average throughput reaches
full capacity of 10Mbps as shown in Figure 5(b).
3) Mixture of TCP and UDP traffic: To demonstrate PIE’s performance under persistent heavy congestion, we adopt a mixture of
TCP and UDP traffic. More specifically, we have 5 TCP flows and
2 UDP flows (each sending at 6Mbps). The corresponding latency
plot can be found in Figure 4(c). Again, PIE is able to contain the
queueing delay around the reference level regardless the traffic mix
while achieving 100% throughput shown in Figure 5(c).
4) Bursty Traffic: As the last functionality test, we show PIE’s
ability to tolerate bursts, whose maximum value is denoted by
max burst. We construct a test where one short lived UDP traffic
sends at a peak rate of 25Mbps over a burst len period of time.
We set burst len to be 100ms and 200ms respectively. We also set
max burst values to be 0 (i.e., no burst tolerance) and 100ms. The
UDP flow starts sending at simulation time of 1s. Figure 6 plots
the number of dropped packets as a function of the simulation time
for four combinations of burst len and max burst. It is obvious
from the graph that if the burst len is less than max burst,
no packets are dropped. When burst len equals to 200ms, the
first 100ms of the burst are allowed into the queue and the PIE
algorithm starts dropping only afterword. Similarly, if we set
the PIE scheme to have no burst tolerance (i.e., max burst =
0), the algorithm starts dropping as soon as the queue starts filling up.
Performance Evaluation and Comparison: The functions of PIE
are verified above. This section evaluates PIE under dynamic traffic
scenarios, compares its performance against RED and shows how
PIE is better suited for controlling latency in today’s Internet. The
simulation topology is similar to the above. The core router runs
either the RED or PIE scheme. The buffer size is 2MB for the two
schemes. RED queue is configured with the following parameters:
minth = 20% of the queue limit, maxth = 80% of the queue limit,
maxp = 0.1 and q weight = 0.002. The PIE queue is configured
with the same parameters as in the previous section.

PIE vs. RED Performance Comparison Under Varying Link
Capacity: 0s-50s, link capacity = 100Mbps, 50s-100s, link capacity =
20Mbps, 100s-150s, link capacity = 100Mbps. By only controlling the
queue length, RED suffers long queueing latency when the queue draining
rate changes. PIE is able to quickly adapt to changing network conditions
and consistently control the queueing latency to the reference level of
20ms.
Fig. 7.

5) Varying Link Capacity: This test starts with the core capacity
of 100Mbps; 100 TCP flows start randomly in the initial second.
At 50s, the available bandwidth drops to 20Mbps and jumps back
to 100Mbps at 100s. The core routers queue limit is set to 2MB.
Figure 7 plots the queuing latency experienced by the RED and
PIE queues. The figure shows that both AQM schemes converge
to equilibrium after a couple of seconds from the beginning of the
simulation. The queues experience minimal congestion during the
first 50s. RED operates around minth , and the queue latency settles
around 30ms accordingly. Similarly, PIE converges to the equilibrium
value of 20ms. When available bandwidth drops to 20Mbps (at 50s),
the queue size and latency shoot up for both RED and PIE queues.
RED’s queue size moves from minth to maxth to accommodate
higher congestion, and the queuing latency stays high around 300ms
between 50s and 100s. On the other hand, PIE’s drop probability
quickly adapts, and in about three seconds, PIE is able to bring
down the latency around the equilibrium value (20ms). At 100s,
the available bandwidth jumps back to 100Mbps. Since congestion
is reduced, RED’s queue size comes down back to minth , and the
latency is reduced as well. PIE’s drop probability scales down quickly,
allowing PIE’s latency to be back at the equilibrium value. Note that
due to static configurations of RED’s parameters, it cannot provide
consistent performance under varying network conditions. PIE, on
the other hand, automatically adapts itself and can provide steady
latency control for varying network conditions.
6) Varying Traffic Intensity: We also verify both schemes’ performance under varying network traffic load with the number of TCP
flows ranging from 10 through 50. The simulation starts with 10
TCP flows and the number of TCP flows jumps to 30 and 50 at 50s
and 100s, respectively. The traffic intensity reduces at 150s and 200s
when number of flows drops back to 30 and 10, respectively. Figure 8
plots the queuing latency experienced under the two AQM schemes.
Every time the traffic intensity changes, RED’s operating queue size
and latency are impacted. On the other hand, PIE quickly adjusts the
dropping probability in a couple of seconds, and restore the control
of the queuing latency to be around equilibrium value.
B. Testbed Experiments
We also implemented PIE in Linux Kernel. In this section, we
evaluate PIE in the lab setup and compare it against CoDel whose
design is the most up to date in Linux. The current implementation
is on Kernel Version 3.5-rc1.

152

(a) Reference Delay = 5ms

(b) Reference Delay = 20ms

Fig. 11. Cdf of Queueing Delay Comparison Between PIE and CoDel: 5

PIE vs. RED Performance Comparison Under Varying Traffic
Intensity: 0s-50s, traffic load is 10 TCP flows; 50s-100s, traffic load is
30 TCP flows;100s-150s, traffic load is increased to 50 TCP flows; traffic
load is then reduced to 30 and 10 at 200s and 250s respectively. Due to
static configured parameters, the queueing delay increases under RED as
the traffic intensifies. The autotuning feature of PIE, however, allows the
scheme to control the queueing latency quickly and effectively.

Fig. 8.

Fig. 9. The Testbed Setup: our experiment testbed consists of four unique

Linux-based machines. The router implements AQM schemes.

Our experiment testbed consists of four unique Linux-based machines as shown in Figure 9. The sender is directly connected to the
router with the PIE implemention. Hierarchical token bucket (htb)
qdisc has been used to create a bandwidth constraint of 10Mbps. The
router is connected to the receiver through another server. The delay
is added in the forward direction using a delay emulator. Iperf tool is
run on the sender and receiver to generate traffic. All measurements
are done at the router. We obtain statistics and measure throughput
at the router through the tc interface.
For all our lab test scenarios, we use the following PIE parameters
PIE: α̃ = 0.125, β̃ = 1.25, buffer size = 200 packets, Tupdate = 30ms.
CoDel is used for comparison, whose parameters are set accordingly
to the default: interval = 100ms and queue limit = 200 packets.
Packet sizes are 1KB for both schemes.

TCP flows and 2 UDP flows. It is obvious that, under heavy congestion,
CoDel cannot control the latency to the the target values while PIE
behaves consistently according to the design.

reference delay is 5ms and 20ms respectively.1 Both schemes are
able to control the queueing delay reasonably well. When the target
delay is 5ms, more than 90% packets under both schemes experience
delays that are less than 20ms. It is clear that PIE performs better:
70% of the delays are less than 5ms while CoDel has only 30%. When
the reference delay equals to 20ms, the performance of both schemes
look similar. PIE still performs slightly better: 50% of packet delays
are less than 20ms while only 40% of packet delays are less than
20ms under CoDel. For the 20ms target delay case, the throughput
for PIE is 9.87Mbps vs. CoDel is 9.94Mbps. For the 5ms target delay
case, the throughput for PIE is 9.66Mbps vs. CoDel is 9.84Mbps.
CoDel’s throughput is slightly better than PIE.
2) Mixture of TCP and UDP Traffic: In this test, we show the
stability of both schemes in a heavily congested scenario. We setup 5
TCP flows and 2 UDP flows (each transmitting at 6Mbps). The 2 UDP
flows result in a 20% oversubscription on the 10Mbps bottleneck.
Figure 11 shows the cdf plot for the delay. Under the mixture of TCP
and UDP traffic, it is obvious that CoDel cannot control the latency
under the target values of 5ms and 20ms respectively. Majority of the
packets experience long delays over 100ms. PIE, on the other hand,
behaves consistently according to the design: with 70% less than the
target of 5ms, and 60% less than the target of 20ms respectively.
Vast majority of packets, close to 90%, do not experience delay that
is more than twice of the target value. In this test, when the target
delay equals to 20ms, the throughput for PIE is 9.88Mbps vs. CoDel
is 9.91Mbps. When the target delay equals to 5ms, the throughput for
PIE is 9.79Mbps vs. CoDel is 9.89Mbps. Throughputs are similar.
V. T HEORETICAL A NALYSIS
We formulate our analysis model based on the TCP fluid-flow and
stochastic differential equations originated in the work by Misra et
al. [14] and [15]. We consider multiple TCP streams passing through
a network that consists of a congested link with a capacity Cl . It is
shown in [14] that the TCP window evolution can be approximated
as

(a) Reference Delay = 5ms

dW (t)
dt
dq(t)
dt

(b) Reference Delay = 20ms

=
=

W (t)W (t − R(t))
1
−
p(t − R(t));
R(t)
2 R(t − R(t))
W (t)
N (t) − Cl ;
R(t)

(1)
(2)

Cdf of Queueing Delay Comparison Between PIE and CoDel:
20 TCP flows. When the reference delay is 5ms, 70% of the delays under
PIE vs. 30% of delays under CoDel are less than 5ms. PIE and CoDel
behave similarly when the reference delay is 20ms.

where W (t) and q(t) denote the TCP window size and the expected
queue length at time t. The load factor, number of flows, is indicated

1) TCP Traffic: We evaluate the performance of PIE and CoDel
in a moderately congested scenario. We use 20 NewReno TCP flows
with RTT = 100ms and run the test for 100 seconds. Figure 10 plots
the cdf curve of the queueing delay for PIE and CoDel when the

1 There is a subtle difference in the definition of terms: in CoDel,
parameter, target, represents the target latency, while del ref refers
reference equilibrium latency in PIE. For easy comparison, we would
”Reference Delay” to refer the target latency, target, in CoDel and
reference equilibrium latency, del ref , in PIE.

Fig. 10.

153

the
the
use
the

Fig. 14. An Illustration about Stability Margin for Different RTTs: for
Ro < R+ , the gain of the system reduces and poles are moved towards

high frequency. As a result, phase margin increases.
Fig. 12. The Feedback Loop of PIE: it captures TCP, Queue and PIE
dynamics; and also models the RTT delay.

We can approximate our system’s dynamics by their small-signal
linearization about an operating point based on small perturbations:
e.g. W = Wo + δW . Our system equations above lead to the
following:
δ q̇(t)

=

δ Ẇ (t)

=

δ ṗ(t)

Phase Margin as a Function of Parameters α and β : in order
for a system to be stable, we need a phase margin above 0◦ .

Fig. 13.

=

N
1
− δW (t) −
δq(t);
Ro
Ro
N
− 2 (δW (t) − δW (t − Ro ))
Ro Cl
Ro Cl2
δp(t − Ro );
−
2N 2
β + α/2
α
+
δq(t).
T Cl
Cl

≈

When the system reaches steady state, the operating point
(Wo , qo , po ) is defined by Ẇ = 0, q̇ = 0 and τ = τref so that
Wo2 po = 2 and Wo =

Ro Cl
.
N

(5)

(8)

Cl
((β + α/2)s + Tα ) e−sRo
− 2N
1
s
(s + R2N
2 C )(s + R )
o
o

A. Linearized Continuous System

(7)

From Equations (5) to (8), we can obtain the loop transfer function
in Laplace domain shown in Figure 12 as:
G(s)

as N (t). R(t) represents the round trip time including the queueing
delay. Note that R(t) denotes the harmonic mean of the flows’ round
trip times. The drop or mark probability is indicated as p(t).
AQM schemes determine the relationship between the drop or mark
probability p(t) and the queue length q(t). This relationship in PIE is
detailed in Section III-B. PIE increases its drop or mark probability
based on current queueing delay and the delay moving trend. Using
Bilinear Transformation, we can convert the PIE design into a fluid
model as follows:
q(t)
τ (t) =
;
(3)
Cl
dp(t)
τ (t) − τref
α dτ (t)
= α
+ (β + )
;
(4)
dt
T
2 dt
where τ and τref represent the queue latency and its equilibrium
reference value, and T is the update interval, which equals to Tupdate .
Note that although T does not show up directly in the discrete form
of the algorithm, it does play a role in the overall system’s behavior.
These fluid equations describe our control system’s overall behavior from which we can derive the equilibrium and dynamic
characteristics as follows.

(6)

l

κ(jω/z1 + 1)
e−jωRo
i.e. ≈ −
,
(9)
(jω/s1 + 1)(jω/s2 + 1) jω
√
where κ = αRo /(po T ), z1 = α/((β + α/2)T ), s1 = 2po /Ro
and s2 = 1/Ro . Note that the loop scales with C/N , which can be
derived from the drop probability po and Ro .
B. Determining Control Parameters α, β
Although many combinations of T , α and β would lead to system
stability, we choose our parameter settings according to the following
guideline. We choose T so that we sample the queueing latency two
or three times per round trip time. Then, we set the values of α and β
so that the Bode diagram analysis would yield enough phase margin.
Figure 13 shows what phase margin would be for various values of
α and β given that T = 30ms, R+ = 100ms and p− = 0.01%. It is
obvious that, in order to have system stability, we need to choose α
and β values so that we have phase margin above 0◦ .
Once we choose the values of α and β so that the feedback system
in Equation (9) is stable for R+ and p− . For all systems with Ro <
R+ and po > p− , p
the gain of the loop, |G(s)| = αRo /(po T ), <
αR+ /(p− T ), s1 > 2p− /R+ and s2 > 1/R+ . Hence, if we make
the system stable for R+ and p− , the system will be stable for all
Ro < R+ and po > p− . Figure 14 illustrates this point showing how
the phase margin improves from 29.5◦ to 105.0◦ when Ro is 50ms
instead of R+ of 100ms.
We could fix the values of α and β and still guarantee stability
across various network conditions. However, we need to choose

154

auto-tuning works well in varying congestion environment.
VI. C ONCLUSIONS AND F UTURE W ORK

√
Fig. 15. Phase Margin as a Function of po : three pairs of α : β settings.
Lower values of α and β gives higher phase margin. Autotuning picks
different pair for different po range to optimally tradeoff stability vs.

response time.

In this paper we have described PIE, a latency-based design for
controlling bufferbloat in the Internet. The PIE design based its
random dropping decisions not only on current queueing delay but
also on the delay moving trend. In addition, the scheme self-tunes
its parameters to optimize system performance. As a result, PIE is
effective across diverse range of network scenarios. Our simulation
studies, theoretical analysis and testbed results show that PIE can
ensure low latency under various congestion situations. It achieves
high link utilization while maintaining stability consistently. It is a
light-weight, enqueing based design that works with both TCP and
UDP traffic. The PIE design only requires low speed drop probability
update, so it incurs very small overhead and is simple enough to
implement in both hardware and software.
Going forward, we will study how to automatically set latency
references based on link speeds: set low latency references for high
speed links while being conservative for lower speed links. We will
also explore efficient methods to provide weighted fairness under PIE.
There are two ways to achieve this: either via differential dropping
for flows sharing a same queue or through class-based fair queueing
structure where flows are queued into different queues. There are pros
and cons with either approach. We will study the tradeoffs between
these two methods.
R EFERENCES

√

Loop Frequency as a Function of po : three pairs of α : β
settings. Lower values of α and β has slower response time. Autotuning
picks different pair for different po range to optimally tradeoff stability
vs. response time.
Fig. 16.

conservative values of α and β to achieve stability. For example,
we need to set α = 0.0156 and β = 0.156 in the above case to
guarantee a phase margin of 25◦ for po ≈ 0.01%. However, when
the drop probability increases, the system response time would take
√
a hit. Figure 15 shows the phase margin as a function of po 2
for α : β values of 0.125:1.25, (0.125/2):(1.25/2) = 0.0625:0.625,
(0.125/8):(1.25/8) = 0.0156:0.156, respectively. Their corresponding
loop bandwidths, which directly determine their response time, are
shown in Figure 16. As shown in Figure 15, if we choose α : β
values to be 0.0625:0.625 and 0.125:1.25, we don’t have enough
√
phase margin to ensure stability when po < 1%, i.e. po < 0.1.
On the other hand, these two higher value pairs would lead to faster
response time as depicted in Figure 16.
Auto-tuning in PIE tries to solve the above problem by adapting
its control parameters α and β based on congestion levels. How
congested a link is can be easily inferred from the drop probability
po . When the network is lightly congested, say under 1%, we choose
numbers that can guarantee stability. When the network is moderately
congested, say under 10%, we can increase their values to increase
system response time. When the network is heavily congested, we can
increase their values even further without sacrificing stability. While
the adjustment can be continuous, we choose discrete numbers for
simplicity. As demonstrated in Figure 15 and 16, the auto-tuning
design in PIE can improve the response time of the loop greatly
without losing stability. Our tests results in Section IV als shows that
2 We

choose

√

po instead of po because s1 scales with

√

[1] B. Turner, “Has AT&T Wireless Data Congestion Been Self-Inflicted?”
[Online]. Available: BroughTurnerBlog
[2] J. Gettys, “Bufferbloat: Dark buffers in the internet,” IEEE Internet
Computing, vol. 15, pp. 95–96, 2011.
[3] S. Floyd and V. Jacobson, “Random early detection gateways for
congestion avoidance,” IEEE/ACM Transactions on Networking, vol. 1,
no. 4, pp. 397–413, Aug. 1993.
[4] W. Feng, K. Shin, D. Kandlur, and D. Saha, “The blue active queue management algorithms,” IEEE/ACM Transactions on Networking, vol. 10,
no. 4, pp. 513–528, Aug. 2002.
[5] C. V. Hollot, V. Misra, D. Towsley, and W. bo Gong, “On designing
improved controllers for aqm routers support,” in Proceedings of IEEE
Infocom, 2001, pp. 1726–1734.
[6] S. Kunniyur and R. Srikant, “Analysis and design of an adaptive virtual
queue (avq) algorithm for active queue management,” in Proceedings of
ACM SIGCOMM, 2001, pp. 123–134.
[7] B. Braden, D. Clark, J. Crowcroft, and et. al., “Recommendations on
Queue Management and Congestion Avoidance in the Internet,” RFC
2309 (Proposed Standard), 1998.
[8] K. Nichols and V. Jacobson, “A Modern AQM is just one
piece of the solution to bufferbloat.” [Online]. Available: http:
//queue.acm.org/detail.cfm?id=2209336
[9] A. Demers, S. Keshav, and S. Shenker, “Analysis and simulaton of
a fair queueing algorihtm,” Journal of Internetworking Research and
Experience, pp. 3–26, Oct. 1990.
[10] R. Pan, B. Prabhakar, F. Bonomi, and R. Olsen, “Approximate fair bandwidth allocation: a method for simple and flexible traffic management,”
in Proceedings of 46th Annual Allerton Conference on Communication,
Control and Computing, 2008.
[11] G. Franklin, J. D. Powell, and A. Emami-Naeini, in Feedback Control
of Dynamic Systems, 1995.
[12] R. Pan, B. Prabhakar, and et. al., “Data center bridging - congestion
notification.” [Online]. Available: http://www.ieee802.org/1/pages/802.
1au.html
[13] “NS-2.” [Online]. Available: http://www.isi.edu/nsnam/ns/
[14] V. Misra, W.-B. Gong, and D. Towsley, “Fluid-based analysis of a
network of aqm routers supporting tcp flows with an application to red,”
in Proceedings OF ACM SIGCOMM, 2000, pp. 151–160.
[15] C. V. Hollot, V. Misra, D. Towsley, and W. bo Gong, “A control theoretic
analysis of red,” in Proceedings of IEEE Infocom, 2001, pp. 1510–1519.

po .

155

IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 13, NO. 5, OCTOBER 2005

975

SHRiNK: A Method for Enabling Scaleable
Performance Prediction and Efficient
Network Simulation
Rong Pan, Balaji Prabhakar, Senior Member, IEEE, Konstantinos Psounis, Member, IEEE, and Damon Wischik

Abstract—As the Internet grows, it is becoming increasingly difficult to collect performance measurements, to monitor its state,
and to perform simulations efficiently. This is because the size and
the heterogeneity of the Internet makes it time-consuming and difficult to devise traffic models and analytic tools which would allow
us to work with summary statistics.
We explore a method to side step these problems by combining
sampling, modeling, and simulation. Our hypothesis is this: if we
take a sample of the input traffic and feed it into a suitably scaled
version of the system, we can extrapolate from the performance of
the scaled system to that of the original.
Our main findings are as follows. When we scale an IP network
which is shared by short- and long-lived TCP-like and UDP flows
and which is controlled by a variety of active queue management
schemes, then performance measures such as queueing delay and
drop probability are left virtually unchanged. We show this in
theory and in simulations. This makes it possible to capture the
performance of large networks quite faithfully using smaller scale
replicas.
Index Terms—Network downscaling, performance extrapolation, small-scale network replica, traffic sampling.

I. INTRODUCTION

M

EASURING the performance of the Internet and predicting its behavior under novel protocols and architectures are important research problems. These problems are made
difficult by the sheer size and heterogeneity of the Internet: it
is very hard to simulate large networks and to pinpoint aspects
of algorithms and protocols relevant to their behavior. This has
prompted work on traffic sampling [6], [7]. Sampling certainly
reduces the volume of data, but it can be hard to work backward—to infer the performance of the original system.
A direct way to measure and predict performance is with
exhaustive simulation. If we record the primitive inputs to the
system, such as session arrival times and flow types, we can in

Manuscript received October 7, 2003; revised November 15, 2004; approved
by IEEE/ACM TRANSACTIONS ON NETWORKING Editor R. Srikant.
R. Pan was with Stanford University, Stanford, CA 94305 USA. She is now
with Cisco Systems, San Jose, CA, 95134 USA (e-mail: ropan@cisco.com;
rong.pan@gmail.com).
B. Prabhakar is with Stanford University, Stanford, CA 94305 USA (e-mail:
balaji@ee.stanford.edu).
K. Psounis was with Stanford University, Stanford, CA 94305 USA. He is
now with the Department of Electrical Engineering - Systems, University of
Southern California, Los Angeles, CA 90089 USA (e-mail: kpsounis@usc.edu).
D. Wischik was with Cambridge University, Cambridge, U.K. He is now
with the Department of Computer Science, University College London, Gower
Street, London WC1E 6BT, U.K. (e-mail: D.Wischik@cs.ucl.ac.uk).
Digital Object Identifier 10.1109/TNET.2005.857080

principle compute the full state of the system. Further, through
simulation, we can test the behavior of the network under new
protocols and architectures. But such large-scale simulation requires massive computing power.
Reduced-order models can go some way in reducing the
burden of simulation. In some cases [12], [30], one can reduce the dimensionality of the data, for example, by working
with traffic matrices rather than full traces, while retaining
enough information to estimate the state of the network. The
trouble is that this requires careful traffic characterization
and model-building. The heterogeneity of the Internet makes
this time-consuming and difficult, since each scenario might
potentially require a different model.
In this paper, we explore a way to reduce the computational
requirements of simulations and the cost of experiments and
hence simplify network measurement and performance prediction. We do this by combining simulations with sampling and
analysis. Our basic hypothesis, which we call SHRiNK (Smallscale Hi-fidelity Reproduction of Network Kinetics), is this: if
we take a sample of the traffic and feed it into a suitably scaled
version of the system, we can extrapolate from the performance
of the scaled system to that of the original.
This has two benefits. First, by relying only on a sample of
the traffic, SHRiNK reduces the amount of data we need to work
with. Second, by using samples of actual traffic, it short-cuts the
traffic characterization and model-building process while ensuring the relevance of the results.
This approach also presents challenges. At first sight, it appears optimistic. Might not the behavior of a large network with
many users and higher link speeds be intrinsically different from
that of a smaller network? Somewhat surprisingly, we find that,
in several essential ways, one can mimic a large network using
a suitably scaled-down version. The key is to find suitable ways
to scale down the network and extrapolate performance.
Our main results are as follows.
1) For networks in which flows arrive at random times and
whose sizes are heavy-tailed, performance measures
such as the distribution of the number of active flows
and of their normalized transfer times are left virtually
unchanged in the scaled system. In Section II, we verify
this using a theoretical argument. This argument reveals
that the method we suggest for “SHRiNKing” networks
in which flows arrive at random times will be widely
applicable (i.e., for a variety of topologies, flow transfer
protocols, and queue management schemes). These networks are representative of the Internet.

1063-6692/$20.00 © 2005 IEEE

976

IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 13, NO. 5, OCTOBER 2005

2) For networks which carry long-lived TCP-like flows arriving in clusters and which are controlled by a variety
of active queue management schemes, we find a different
scaling from that in Section II which leaves the queueing
delay and drop probability unchanged as a function of
time. In Section III, we verify this using the differentialequation-type models developed in [20]. (Such models
have been widely used in designing control algorithms
and for conducting control-theoretic analyses of network
behavior.) These networks are widely used in simulations,
e.g., [17], [18], and [29].
3) Finally, we apply SHRiNK to web server farms. Experimental results with multiple machines reveal that
a number of performance metrics remain virtually unchanged.
A motivating example: before continuing, we consider a
simple example which illustrates the key points—the
queue. Suppose jobs arrive at a queue according to a Poisson
process of rate and that service times are independent and
. Let
be the number of jobs in
exponential with rate
the system at time .
Now scale the system as follows. Sample the arriving jobs,
keeping each job with probability , independent of the others,
so that the sampled arrivals form a Poisson process of rate
.
Consider feeding the sampled arrivals to a separate queue whose
server runs slower than the first by a factor . This is equivalent
to multiplying the service times by a factor
(so that they are
rate
exponentials), and the second queue is also
. If
is the number of jobs in the slower queue at time , then
it is not hard to see that
in distribution. That
is, the evolution of the slower queue is statistically equivalent
to that of the original queue slowed down in time by a factor
. This is because the queue-size process in an
queue
is a birth–death chain. The birth and death rates in the original
queue are and , respectively, while they are
and
in
the slower queue.
As a consequence, in equilibrium, the marginal distributions
of the two queues are equal, i.e.,
. Thus, we have inferred the distribution of queue-size, and hence of delay, in the original high-speed
system by looking at a smaller scale version.
It is natural to be skeptical of the relevance of these results.
After all, they assume Poisson input traffic, whereas Internet
packet traffic exhibits long-range dependence. Even more, these
are open networks (the rate of arrivals is independent of current
network congestion), which is quite different from the window
flow-controlled Internet.
Nevertheless, we find in the coming sections that the
SHRiNK approach can be applied to IP networks, because
it relies on factors other than packet level statistics; indeed,
we shall see that it relies on certain fundamental scalability
properties of networks.

Fig. 1. Basic network topology and flow information.

that, since network sessions arrive as a Poisson process [9], [22],
[26], network flows are as if they were Poisson [15]. (In particular, the equilibrium distribution of the number of flows in
progress at any time can be obtained by assuming that flows
arrive as a Poisson process.) We take these observations into account and study the scaling behavior of IP networks carrying
heavy-tail distributed, Poisson flows. Such networks are a plausible representation of the Internet.
A. Sampling and Scaling
Due to the tremendous increase in the volume and speed of
network traffic, it is very expensive to sample packets. At the
other end of the spectrum, one may sample network sessions.
Here is an exmaple of a network session: an end user is browsing
the web to download pictures; a network session starts when
he/she starts web browsing and terminates when he/she stops.
Each download during this session corresponds to a flow. It is
hard to sample sessions in practice, because only end users have
enough information to distinguish different sessions. Hence, we
choose to sample network flows.1 This reduces the traffic we
have to deal with and is easy to implement in practice.
A second issue related to sampling is how and where are the
network flows sampled? Each flows is chosen with probability
, all choices being independent. Flows are sampled at network
entry points, e.g., at edge routers.
We shall now describe scaling—the procedure of obtaining a
small-scale replica of the original network. This is done as follows: 1) link capacities are reduced by a factor ; 2) propagation
delays are scaled up by a factor
; and 3) protocol timeouts
are also scaled up by the same factor. Informally, these steps aim
to slow down the speed of the network, which is a notion that
will be made more clear and precise in Section II-C.
B. Simulation Results

II. IP NETWORKS WITH SHORT AND LONG FLOWS

In this section, we use simulations to investigate the accuracy with which SHRiNK can predict the performance of IP networks using the network simulator ns [21].
To begin with, we consider the simple topology in Fig. 1.
There are three routers,
, and
, two links in tandem,
and three groups of flows, grp1, grp2, and grp3. The link speeds

It has been shown that the size distribution of flows on the
Internet is heavy-tailed [31]. Hence, Internet traffic consists of
a large fraction of short flows and a small fraction of long flows
that carry most of the traffic. Also, it has been recently argued

1In accordance with the usual practice [8], [13], [14], we say that packets
belong to the same flow if they have the same source and destination IP address
and source and destination port number. A flow is said to be “on” if its packets
arrive more frequently than a certain “timeout” number of some seconds. The
timeout is usually set to something less than 60 s in practice.

PAN et al.: SHRiNK: A METHOD FOR ENABLING SCALEABLE PERFORMANCE PREDICTION AND EFFICIENT NETWORK SIMULATION

Fig. 2. Distribution of number of active flows on the first link (uncongested
case).

are 10 Mb/s. Routers use either the random early detection
(RED) or the DropTail queue management schemes. The RED
, and
.
parameters are
When using DropTail, the buffer can hold 200 packets.
Within each group, flows arrive as a Poisson process with rate
. We vary to study both congested and uncongested network
scenarios. (We use built-in routines in ns to generate web sessions consisting of a single object each. This is what we call a
“flow” in the simulations.) Each flow consists of a Pareto-distributed number of packets with average size 12 packets and
shape parameter equal to 1.2. The packet size is set to 1000
bytes. The propagation delay of each flow of grp1, grp2, and
grp3, is 50, 100, and 150 ms, respectively.
We run the experiments for scale factors
and
and
compare the distribution of the number of active flows as well as
the histogram of the normalized delays of the flows in the original and the scaled system. (The normalized delays are the flow
transfer times multiplied by .) We also compare more detailed
performance measures such as the distribution of active flows
that are less than some size and belong to a particular group
and the distribution of the packet buffer occupancies. As will be
shown in Section II-C, the method can predict the marginal and
joint distributions of a large number of performance measures.
We start with the simple case of an uncongested network,
i.e., very few packet drops occur. The flow arrival rate is set to
45 flows/s within each group. Fig. 2 plots the distribution of the
number of active flows in the first link. The distributions at the
two different scales match. A similar result and conclusion is
obtained at the second link.
Fig. 3 plots the histogram of the normalized delays of the
flows of grp1. To generate the histogram, we use normalized
delay chunks of 10 ms each. There are 100 such delay chunks
in the plot, corresponding to flows having a normalized delay
of 0 –10 ms, 10–20 ms, and so on. The last delay chunk is for
flows that have a normalized delay of at least 1 s. The plot shows
that the distribution of the normalized delays at the two scales
match. The results for the other two groups of flows lead to the
same conclusion regarding the scalability of SHRiNK.
It is worth elaborating upon the specific nature of the plot. The
peaks are due to the TCP slow-start mechanism. The left-most

977

Fig. 3. Histogram of normalized delays of grp1 flows (uncongested case).

Fig. 4. Distribution of number of active flows on the first and second links
(RED).

peak corresponds to flows which send only one packet and face
no congestion. These flows only have to wait for the setup of the
TCP connection. (Hence, for example, in Fig. 3, where propagation delays are 50 ms, the normalized delay for these flows
is a bit more than 200 ms accounting for SYN, SYN-ACK, the
data packet, the ACK for the packet, and insignificant transmission and queueing delays.) The portion of the curve between
the first and second peaks corresponds to flows which send only
one packet and face some congestion (but no drops). The next
peak corresponds to flows which send two or three packets and
face no congestion. These flows have to wait for an additional
round-trip time for the acknowledgment for the first packet to
arrive. The third peak corresponds to flows which send between
four and seven packets and face no congestion, and so on.2
We now present results for the more realistic case of congested networks. Accordingly, flow arrival rates are set to
60 flows/s within each group. Flows experience drops that
account for up to 5% of the total traffic. We first present simulations where all three routers use RED.
2Recall that, during the slow-start phase of TCP, senders double their window
sizes upon receiving acknowledgment.

978

IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 13, NO. 5, OCTOBER 2005

Fig. 5.

Histogram of normalized delays of grp1 and grp2 flows (RED).

Fig. 6.

Histogram of normalized delays of grp3 flows (RED).

Fig. 7. Distribution of number of active grp3 flows with size less than 12
packets (RED).

Fig. 8.

Fig. 4 plots the distribution of the number of active flows in
the first and second links. The two distributions match in both
links.
Fig. 5 plots the histogram of the normalized delays of the
flows of grp1 and grp2. Notice that we use 150 and 200 delay
chunks for the grp1 and grp2 flows, respectively. Fig. 6 plots the
histogram of the normalized delays of the flows of grp3. Three
hundred delay chunks are used in this plot. In all three cases, the
delay histograms match.
What about more detailed performance measures? As an example, we compare the distribution of active flows belonging
to grp3 that are less than 12 packets long. Fig. 7 compares the
two distributions from the original and scaled system. Again,
the plots match.
We now present results when DropTail is used instead of
RED. Fig. 8 plots the distribution of the number of concurrently
active flows in the second link between routers
and
when
all routers use DropTail. It is evident from the plot that the two
distributions match as before. A similar scaling holds for the
other link.
Fig. 9 plots the histogram of the normalized delays of the
flows of grp2 when DropTail is employed. The distributions

Distribution of number of active flows on the second link (DropTail).

match as before. A similar scaling holds for the other two groups
of flows.
So far, the method has successfully predicted the distribution
of various performance measures at the flow level. Fig. 10 compares the distribution of the number of packets at the first queue,
which uses RED, in the original and scaled networks. As evident
from the plot, the method can also predict the distribution of the
queue occupancies.
C. Theory
Recall that flows arrive as a Poisson process, bearing sizes
drawn independently from a common (Pareto) distribution.3
By the state of the network at time , we mean the total
information that is needed to resume the evolution of the
network from time onwards, given input data (flow arrival
times and sizes) after time . For example, the state consists
of information about currently active flows, e.g., the number
of packets they have already transfered and where the packets
that are in transit are in the network. Write
for the state
3Note that, whereas flow sizes are independent, their delays (equal to their
total transfer times) are usually dependent.

PAN et al.: SHRiNK: A METHOD FOR ENABLING SCALEABLE PERFORMANCE PREDICTION AND EFFICIENT NETWORK SIMULATION

979

Fig. 11. Time evolution of (i) the original, (ii) the time-stretched, and (iii) the
scaled system.

Fig. 9.

Histogram of normalized delays of grp2 flows (DropTail).

It is a simple but far-reaching property of the Poisson process
, since sampling a proportion of the points of
that
a rate Poisson process will yield a rate
Poisson process.
Also, the independent nature of the sampling process does not
destroy the i.i.d. nature of the flow sizes.
Let
denote the state of the time-stretched
system at time . We shall show that the following identity is
satisfied at every time :
(1)

Fig. 10.

Distribution of number of packets in R1.

at time . If
denotes the input data to the system, then
is some function
of the input until time . Symbolically,
. We shall abbreviate this to
. Note that
is some complicated function
depending on transport protocols, queue management schemes,
and other network- and user-specific details.
Theorem 1: Consider a network where flows arrive as a
Poission process bearing sizes drawn independently from an arbitrary distribution. Let
be the state of the original network
at time and
be the state of the scaled network at time .
Then
, i.e., they are equal in distribution.
Proof: Let
and
be the inputs to the original and
scaled systems, respectively. Let
and
denote the functions corresponding to the original and scaled (slowed-down)
networks. Therefore,
and
.
Our method of proof consists of constructing a third system,
the “time-stretched system,” which is obtained by applying the
input
to the scaled system. That is, the input to
the time-stretched system is the same as the input to the original system stretched out in time by a factor . To elaborate
this point, suppose that a flow of size arrives to the original
system at time . Then, it arrives at time
(still possessing size
) to the time-stretched system. The converse is true as well.

Establishing this will complete our proof, since
.
We now establish the identity at (1). Consider the consequences of our method of scaling (slowing down) the original
network: reducing link speeds by a factor
will increase
queueing delays and transmission times by factor
and
increasing progagation delays by a factor
will increase
propagation times by
. Since the total delay of a packet is
the sum of its queueing, transmission, and propagation times,
we have effectively increased the delay of every packet by
.
This in turn increases the delay of every flow transfer time by a
factor
. It is now quite easy to see that much more is true:
since the networks are all discrete-event systems, clocked by
transmissions and acknowledgments of packets, every event
that occured in the original system at time will occur in the
time-stretched system at time
. Therefore
,
and the theorem is proved.
Remark 1: It is instructive to consider an illustration of the
three systems, as in Fig. 11. The time evolution of each of the
three systems is shown between an arbitrary source–destination pair. In each subfigure, the corresponding input process
is shown on the top line. The graph of an input process denotes flow arrival times and their corresponding sizes. The lines
going upwards denote acknowledgments. Finally, the big “X”s
denote packet drops. The original system has an input process
of
. For the time-stretched system, packets have larger transmission and propagation delays, denoted by “fatter” parallelograms and larger slopes, respectively; the input process
is
a time-stretched version of
. Notice that the time-stretched
system is just a device for the proof, and it does not exist. The
input of the scaled system
is just a subsample of the flows
of
. The unsampled flows of
are denoted by tiny “ ”s
on the top line of Fig. 11(iii).

980

IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 13, NO. 5, OCTOBER 2005

Remark 2: The theorem explains why the distributions of
various performance measures match in distribution. Further, it
shows that performance scaling involves speeding up the time,
and this is why we compare normalized delays rather than delays. The proof of the theorem only relies on the assumptions
about inputs (Poisson flow arrivals and i.i.d. sizes) and the fact
that the network evolves as a discrete-event system. Therefore,
when these assumptions are met,4 SHRiNK is widely applicable for marginal, joint, steady-state, and transient distributions
of a large family of performance measures, for any network
topology, transport protocol, and queue mechanism. Another
consequence of Theorem 1 is that SHRiNK works for any value
of . Thus, networks can be slowed down arbitrarily. However,
the smaller is, the slower the network is, and the longer it takes
for distributions to converge.
D. Applications
Since the method provides a way to deduce the performance
of a fast network from a slowed-down replica, it can be used to
reduce the cost of experiments. Imagine a test network with slow
network interfaces, slow switches and routers, and cheap links
that is fed with a sample of the actual network traffic.5 In this
network, one may experiment with new algorithms, protocols,
and architectures and extrapolate performance.
Another use of the method is the following. There has been
a recent development of research prototypes and products [5]
that record partial information about the network by sampling
incoming traffic. SHRiNK offers a systematic way to reproduce
offline the behavior of the network using this sample.

Fig. 12.

Basic network topology and flow information.

A. RED
The key features of RED are the following two equations,
which together specify the drop (or marking) probability. RED
maintains a moving average of the instantaneous queue size
and
is updated whenever a packet arrives, according to
the rule

where the
parameter determines the averaging window.
The average queue size determines the drop probability ,
according to

III. IP NETWORKS WITH LONG-LIVED FLOWS
In this section, we explore how SHRiNK can be applied to
IP networks used by long-lived TCP-like flows that arrive in
clusters and are controlled by queue management schemes like
RED. These networks are widely used to study the performance
of TCP and of various AQM schemes; see, for example, [17],
[18], and [29].
First, we explain in general terms how we sample traffic, scale
the network, and extrapolate performance.
Sampling is simple. We sample a proportion of the flows,
independently and without replacement.
We scale the network as follows: link speeds and buffer sizes
are multiplied by . The various AQM-specific parameters are
also scaled, as we will explain in Section III-A. The network
topology is unchanged during scaling. In the cases we study, we
find that performance measures such as average queueing delay
are virtually the same in the scaled and the unscaled systems.
Our main theoretical tool is the recent work on fluid models
for TCP networks [20]. While [20] shows these models to be
reasonably accurate in most scenarios, the range of their applicability is not yet fully understood. However, in some cases the
SHRiNK hypothesis holds even when the fluid model is not accurate, as shown in Section III-A3.
4We refer the reader to [15] and [4] for an interesting discussion of the M/GI
models and their role in generating the well-documented self-similar nature of
network traffic.
5This network should also have larger propagation delay than the original.
This can be achieved in software or with delay-loops.

if
if
if

(2)

We now explain how we scale the parameters
, and . We will multiply
and
by .
Recall that we are multiplying the buffer size by ; thus,
and
are fixed to be a constant fraction of the buffer size.
(This is in accord with the recommendations in [11].) We will
keep
fixed at 10%, so that the drop probability is kept
under 10% as long as the buffer is slightly congested. The
averaging parameter takes more thought. We shall multiply it
by
. The intuition is this: when the network is scaled down,
packets arrive less frequently, so
is updated less often; in
turn, this requires us to make the updates larger in magnitude.
We shall see that both simulation and theory show that this
choice of scaling is natural for extrapolating performance.
1) Basic Setup: We consider two congested links in tandem,
as shown in Fig. 12. There are three routers:
, and
,
and three groups of flows: grp1, grp2, and grp3. The link
speeds are 100 Mb/s and the buffers can hold 8000 packets.
The RED parameters are
, and
. For the flows: grp0 consists of 1200 TCP flows
each having a propagation delay of 150 ms, grp1 consists of
1200 TCP flows each having a propagation delay of 200 ms,
and grp2 consists of 600 TCP flows each having a propagation
delay of 250 ms. The flows switch on and off as shown in the

PAN et al.: SHRiNK: A METHOD FOR ENABLING SCALEABLE PERFORMANCE PREDICTION AND EFFICIENT NETWORK SIMULATION

Fig. 15.
Fig. 13. Basic setup: average queueing delay at Q1.

981

Basic setup: drop probability at Q1.

, where is the propagation
time . Here,
delay and
is the queue size at time . Let
be the drop
probability at time and
the average queue size used by
RED.
The fluid model describes how these quantities evolve or,
rather, since these quantities are random, the fluid model describes how their expected values evolve. Let be the expected
value of random variable . Then the fluid model equations are
(3)
(4)
(5)

Fig. 14. Basic setup: average queueing delay at Q2.

timing diagram in Fig. 12. Note that 75% of grp0 flows switch
off at time 150 s.
This network is scaled down by factors
and
,
and the parameters are modified as described above.
We plot the average queueing delay at Q1 and Q2 as a function of time in Figs. 13 and 14. The drop probability at Q1 is
shown in Fig. 15. Due to limited space, we omit the plot of drop
probability for Q2 since its behavior is similar to that of Q1.
We see that the queueing delay is almost identical at different
scales. (It is worth noting that it is the queueing delay which is
model it was
unchanged during scaling, whereas in the
the queue size distribution.)
Since the drop probability is also the same in the scaled and
unscaled systems, the dynamics of the TCP flows are the same.
In other words, an individual flow which survives the sampling
process essentially cannot tell whether it is in the scaled or unscaled system.
2) Theory: We now show that these simulation results are
supported by the recently proposed theoretical fluid model of
TCP/RED [20].
Consider
flows sharing a link of capacity . Let
and
be the window size and round-trip time of flow at

(6)
where
solves
is the
average packet inter-arrival time, and
is the same as in
(2).
Remarks: While the applicability of these equations is not
yet fully understood, [20] indicates that empirically they are
reasonably accurate. Also, note that we have the constant 1.5
in (3) and not 2 as in [20]. This change improves the accuracy
of the fluid model for reasons elaborated in [24]. Finally, note
that, while these equations describe a single link, the extension
to networks is straightforward and is given in [20].
Returning to the differential equations, suppose we have a
solution to these equations

Now, suppose the network is scaled and denote by
, etc.,
the parameters of the scaled system. When the network is scaled,
the fluid model equations change, and so the solution changes.
Let
be the solution of the scaled
system. In fact, we claim that

If our claim is established, we will obtain that the queueing delay
is identical to that in the unscaled system.

982

IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 13, NO. 5, OCTOBER 2005

Note also that the drop probability is the same in each case
. Thus, we will have theoretical support for the
observations in the previous section.
Establishing the claim. We will proceed through the fluid model
equations one by one. First consider (3). Note that
, so that
.
Hence

Next consider (4). Suppose for simplicity that all flows have
identical routes. Then the
are statistically identical, hence
the expectations
are all equal. So, we can rewrite the equation as

It is then easy to see that

Fig. 16. Fluid model predicts scaling behavior.

This extends to the case of multiple groups of flows with different routes, provided we sample a proportion from each
group.
Now consider (5). Recall that
and note that the average packet inter-arrival time increases as the number of flows
and the capacity decrease, in proportion
. Making the
approximation
, which is good
for small , we see that
and
hence that

In fact, we chose
so that this equation would be
satisfied, allowing us to scale properly.6
Finally, consider (6). Recall that
and that
and
. It is then clear that

This establishes the claim.
Fig. 16 presents the solution of the fluid model for the
queueing delay at Q1 under the scenario of Fig. 1 for the scale
parameters
and
. As can be seen, both of the solutions
are virtually identical, providing a numerical illustration of the
scaling property of the differential equations established above.
Remarks: It is worth remarking on a theoretical nicety related to the scaling property of these differential equations. If
they had been derived from a limiting procedure in which the
number of users, link capacities, and buffer sizes all increase
proportionally with , then the scaling behavior would have
been entirely expected (one has only to set equal to
before taking limits). However, they have been derived via a different route in [20]: by assuming that packet drops occur as a
Poisson process. Therefore, the scaling property they exhibit is
6It is true that w needs to be less than 1. However, this would not be a limiting
factor for the magnitude of scaling since w is generally set to a small value for
high-speed links: for example, 10 for a 1-Gb/s link.

Fig. 17.

With faster links: average queueing delay at Q1 (zoomed in).

rather stunning. It strongly suggests that, in fact, they describe
the behavior of the network in a large- limit.
We also draw attention to some interesting features of all of
the performance-related figures in this section. Note that transients are pretty well mimicked at the smaller scales. Also note
that the smaller scale plots look more jagged, as if they are a
noisy version of the original plots. The last point would be an
easy consequence of a limit theorem: if in the large- limit the
behavior of the network is describable using deterministic differential equations, then away from the limit (at smaller and
smaller scales) a corresponding central limit theorem would
suggest that the noise would be proportional to
.
3) With Faster and Slower Links: Suppose we alter the basic
setup, by increasing the link speeds to 500 Mb/s, while keeping
all other parameters the same. Fig. 17 (zoomed in to emphasize
the point) illustrates that, once again, scaling the network does
not alter the queueing delay. Note that, under these conditions,
the queue oscillates. There have been various proposals for stabilizing RED [18], [23]. We are not concerned with stabilizing
RED here: we mention this case to show that SHRiNK can work
whether or not the queue oscillates.

PAN et al.: SHRiNK: A METHOD FOR ENABLING SCALEABLE PERFORMANCE PREDICTION AND EFFICIENT NETWORK SIMULATION

983

Fig. 19. With web traffic: average queueing delay at Q1.
Fig. 18.

With slower links: average queueing delay at Q1.

Suppose we instead alter the basic setup by decreasing the
link speeds to 50 Mb/s, while keeping all other parameters
the same. Once again, scaling the network does not alter the
queueing delay. For such a simulation scenario, especially in
the time frame 100–150 s, the fluid model is not a good fit (see
Fig. 18). This is not unexpected [28]: actual window and queue
sizes are integer-valued whereas fluid solutions are real-valued;
rounding errors are nonnegligible when window sizes are small,
as is the case here. The range of applicability of the fluid model
is not our primary concern in this paper: we mention this case
to show that SHRiNK can work whether or not the fluid model
is appropriate.
4) With Web Traffic: So far, we have only considered longlived flows to which fluid models can be applied. We now introduce short-lived web flows to each flow group in the basic
setup. Each session consists of multiple requests, each request
being for a single file. The number of requests within a session
is random (we use the standard ns settings), and file sizes are
Pareto-distributed with an average of 12 packets and a shape
parameter of 1.2. In our experiment on the unscaled network,
20 000 web sessions were generated. In the scaled version, we
sample a proportion of these sessions independently. We also
sample a proportion of the original long-lived TCP flows, as
before.
Fig. 19 shows that scaling the network does not affect the
queueing delay much, even in the presence of web traffic. Note
that here the queueing delay is dominated by the behavior of
long-lived TCP flows which have reached steady state.
5) In a More Complex Network: As a further validation, we
test SHRiNK in a more complex network, shown in Fig. 20.
There are seven routers R1–R7. Links R1–R2, R2–R3, R1–R5,
R3–R5, and R4–R5 run at 150 Mb/s, links R1–R4 and R5–R6
run at 100 Mb/s, and all other links run at 50 Mb/s. The traffic is a
mixture of UDP and web flows and long-lived TCP, AIMD, and
Binomial [2] flows. These last types have the following common
form: on receiving an acknowledgment, increase the congestion
window
by
(TCP uses
) and, upon
(TCP uses
incurring a mark/drop, decrease by
). The parameters
describe each class.

Fig. 20.

More complex topology.

We omit a detailed description of all of the flows, except those
traversing link R5–R6 whose queueing dynamics are shown in
Fig. 21. Link R1 R5 carries 1000 long-lived flows, divided
into five groups: 200 normal TCP, 200 AIMD (1, 0;.1, 1), 200
AIMD (2, 0;.5, 1), 200 Binomial (1, 1;.5, 1), and 200 Binomial
(1.5, 1;.5, 1). The links are controlled by RED with
and
. As before, we see
that scaling the network does not affect the queueing delay.
B. Proportional-Integral (PI) Controller
A different AQM scheme is the PI controller [17], which
attempts to stabilize the queue size around a given target value.
The PI controller drops/marks packets with a probability
which is updated periodically by
(7)
Here, is the instantaneous queue size,
is the target queue
size, is the update timestep (fixed here at 0.01 s), and and
are arbitrary parameters.
We first explain how we will scale the network. As usual, let
, etc., denote the scaled parameters. We will sample a fraction of the flows and set
and
.
(This is in accordance with the design rules in [17].)

984

Fig. 21.

IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 13, NO. 5, OCTOBER 2005

In a more complex network: average queueing delay at R5–R6.

Fig. 23.

PI controller: drop probabilities at Q1 and Q2.

C. Adaptive Virtual Queue (AVQ)
Another type of active queue management scheme is the AVQ
[19], an extension of the virtual queue algorithm [16]. The idea
of AVQ is to adapt the marking probability to reach some given
target utilization. It does this by running a virtual queue in parallel with the actual queue and marking packets which arrive
when the virtual queue is full.
The easiest way to give more details about the algorithm is
via the fluid equations suggested in [19]. Let be the target
utilization, let
be the actual service rate of the queue, and
the service rate of the virtual queue is dynamically adjusted
according to
(8)
Fig. 22.

PI controller: average queueing delay at Q1.

We simulated the basic setup of Section III-A, replacing RED
by the PI controller. We use
and
, as suggested in [17]. We set
to be 1750
packets, which is half way between our
and
parameters from the last section.
Fig. 22 shows the average queueing delay at different scales
for Q1. We see that scaling the network does not affect queueing
delay, at least in steady state. There are some spikes when the
load changes abruptly, and the small-scale network shows
slightly larger spikes. Fig. 23 shows that the drop probability is
also not affected by scaling the network.
We can again use the fluid model to understand this behavior.
To obtain the fluid model for the PI controller, we simply replace
(5) and (6) in the fluid model by the fluid analog of (7): the
expected drop probability evolves according to

where
is the arrival rate at time and is an arbitrary gain
parameter.
How should the parameters and be scaled? Since is the
target link utilization which is independent of any specific link
speed, it is left unchanged. As a result, is also left unchanged
in order to properly scale (8).
The basic setup of Section III-A is simulated, replacing RED
by AVQ. The parameters
% and
are used at
both links. Figs. 24 and 25 show that scaling the network does
not affect essentially the link utilization or the virtual queueing
delay. Similar results hold for the marking probability.
This is also reflected in the fluid model for AVQ, which consists of (8) and the following equations:
(9)
(10)
(11)

As before, by our choice of scaling

Thus, the fluid model also scales.

The first equation is a modified version of (3), modified to remove queueing delay, as AVQ should keep the (actual) buffer
empty. The last two equations are from [19]. Recall that
is
the propagation delay for user .

PAN et al.: SHRiNK: A METHOD FOR ENABLING SCALEABLE PERFORMANCE PREDICTION AND EFFICIENT NETWORK SIMULATION

Fig. 24.

AVQ: link utilization.

Fig. 25.

AVQ: virtual capacity.

Fig. 26.

DropTail: average queueing delay at Q2.

Fig. 27.

CPU time comparison.

985

Now, suppose that
and
are a solution
to the fluid equations. Consider the fluid equations for the scaled
network. It is not difficult to check that
, and
solve these scaled equations.

correlated, the assumption that packet drops occur as a Poisson
process (see [20]) is violated and the differential equations become invalid. The connection between these two phenomena
(the failure of the scaling hypothesis and the invalidation of the
differential equation models) is explored in [25].

D. DropTail

E. Applications

In all of the examples we have studied in this section—with
heterogeneous end-systems, with different of active queue management policies, and with a range of system parameters—we
have found that basic performance measures such as queueing
delay are left unchanged, when we sample the input traffic and
scale the network parameters in proportion. This conclusion is
supported by the theory of fluid models and even holds where
the fluid models fail. A notable exception is provided by the
queue management scheme DropTail, as described next.
Consider the basic network setup of Section III-A and suppose that the routers use DropTail instead of RED. Fig. 26 shows
the average queueing delay at Q2. Clearly, the queueing delays
at different scales do not match. DropTail drops all of the packets
that arrive at a full buffer. As a result, it could cause a number
of consecutive packets to be lost. These bursty drops underlie
the failure of the scaling hypothesis in this case, as explained
in [25]. Separately, note that, when packet drops are bursty and

In this section, we find that, for certain IP networks supporting flows that arrive in clusters, SHRiNK can predict the
time-wise performance of a high-speed network using its properly scaled-down replica. Although in reality flows do not arrive
in clusters, this type of flow arrivals has been used extensively
in the design of AQM schemes and in the analysis of TCP’s performance [17], [10], [18]–[20], [29]. Most of this work demands
time-consuming ns simulations, especially for high-speed links.
Under these scenarios, the SHRiNK method offers an efficient
way of conducting packet-level simulations by drastically reducing the simulation time.
To illustrate the potential savings in resources, we report
the CPU time to run the simulations in Section III-A1 and
Section III-A3. As shown in Fig. 27, the CPU time rises
monotonically as increases. The reason behind this is the
fact that, for an event-driven simulator like ns, to simulate a
network with more packet arrivals would mean processing

986

IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 13, NO. 5, OCTOBER 2005

Fig. 28. Scaling a web server farm: (i) scaling the number of servers and
(ii) scaling the speed of the servers.

Fig. 29.

Average response time when sampling user-equivalents.

Fig. 30.

Server throughput when sampling user-equivalents.

more events. Naturally, one would expect that the simulation
time for
would be half the simulation time for
.
Surprisingly, we find that the reductions of the CPU time are
slightly more than half in all three cases shown in Fig. 27.
Generally, the slopes of increase are greater than
.7
IV. WEB SERVER FARMS
In this section, we briefly outline how SHRiNK may apply
to web server farms. Since a rapid growth in the size and capacity of web server farms makes it increasingly difficult to take
performance measurements and to evaluate new algorithms and
architectures, if SHRiNK applies to web server farms, it would
help reduce this difficulty significantly.
How should server farms be scaled? Consider a web server
farm with servers each having speed , as in Fig. 28.8 Sample
the requests for the original farm, retaining each independently
with probability . Feed the sampled traffic into a scaled-down
farm consisting of either: 1) a fraction of the original web
servers or 2) the same number of servers each having speed
[see (i) and (ii) of Fig. 28]. Of interest is the closeness of the average response time, the server throughput, and capacity (maximum throughput) in the scaled system to that in the original
system.
We conducted some preliminary experiments using eight
Linux machines configured with a Pentium III at 550 MHz and
384 MB of RAM, connected to a 100-Mb/s switch. A number
of the machines constitute the web farm and each hosts one
Apache 1.3.9 [1] web server. The rest of the machines act as
clients, each of which run Surge [3] to generate HTTP requests.
We report experimental results for the case where one scales
the number of servers [as illustrated in Fig. 28(i)]. This scaling
is very useful in practice since it reduces the size of the system.
In the first experiment, the original farm consists of four
machines. The clients use HTTP1.1, load-balancing is a simple
7We believe that the extra time saving comes from machine-related issues
such as memory requirements. This deserves to be investigated further.
8This is a simplified picture of a farm, since the application servers, the
databases, and the switches used to interconnect the various components are
absent.

round-robin scheme, and both load-balancing and sampling
take place at the user-equivalent level. (Surge uses the notion
of “user-equivalents” to generate sequences of requests similar
to those generated by web sessions that stay “on” throughout
the experiment.) The scaled system consists of a stand-alone
server.
Figs. 29 and 30 show the average response time and the
normalized server throughput as a function of the normalized
load. (Normalized quantities are quantities multiplied by
.)
Scaling the system leaves these quantities virtually unchanged.
Note that we treat the farm of the four servers as a single entity.
The normalized load is the total normalized load directed into
the farm, and the normalized throughput is the sum of the
normalized throughputs of the servers of the farm.
In the second experiment, the original farm consists of two
machines. The clients use HTTP1.0, load-balancing is again
achieved using a round-robin scheme that takes place at the userequivalent level, while sampling takes place at the HTTP request level. (We do not sample embedded requests but rather requests for whole documents.) The scaled system is a stand-alone
server.

PAN et al.: SHRiNK: A METHOD FOR ENABLING SCALEABLE PERFORMANCE PREDICTION AND EFFICIENT NETWORK SIMULATION

987

TABLE I
SHRINKING NETWORKS

V. CONCLUSION

Fig. 31.

Average response time when sampling document requests.

Fig. 32.

Server throughput when sampling document requests.

Figs. 31 and 32 show the average response time and the normalized server throughput as a function of the load.9
Again, these quantities remain virtually unchanged after
scaling. More experimental results can be found in [27].
The results of this section are encouraging. In paricular, we
have shown via experiments that a web farm consisting of a
round-robin load balancer and a number of web servers attached
to it can be scaled down when traffic sampling and load balancing occurs at the HTTP request or the web session level.
However, it should be noted that large web farms can have complex architectures whose topological scaling might be more involved than simply scaling the number of servers. More work
is needed to draw firm conclusions regarding the scalability of
server farms.

9The number of user-equivalents sending requests at the two systems is now
the same, hence the horizontal axis is not multiplied with  as before. It is
the number of requests directed at the two systems that differ due to document
sampling.

In this paper, we have described an approach, called
SHRiNK, for scaleable performance prediction and efficient
simulation of large networks.
Our first example concerned a network in which TCP flows
arrive at Poisson-like times and are heavy-tailed distributed.
This is a plausible representation of the Internet.10 To construct
the network replica, in addition to sampling flows and reducing
link speeds, we increased propagation delays and protocol timeouts. We showed that the distribution of a large number of performance measures of the original network can be accurately
predicted by the replica, irrespective of the network topology,
the protocols, and the AQM schemes used. This type of scaling
can be used to reduce the cost of experiments since all of the
hardware components will run slower. The cost to pay is time;
one needs to wait longer, in real time, for the distribution of the
various metrics to converge on the scaled system.
Our second example was a congested network of long-lived
TCP-like flows that arrive in clusters. This is a popular network
model for designing and testing new algorithms. To construct
the network replica, in addition to sampling flows and reducing
link speeds, we scaled down buffer sizes and AQM parameters.
We showed that various performance measures can be predicted
as a function of time, for a large class of networks. A notable exception is networks that use DropTail as an AQM scheme. This
type of scaling can be used in simulations to reduce execution
time. The cost to pay is accuracy; the smaller the scaling factor
the more noisy the predictions are. The above points are summarized in Table I.
Finally, we have proposed a way to apply SHRiNK to web
server farms. Our experimental testbed consisted of tens of machines; some generated HTTP traffic and some were organized
in a web farm replying to these requests. While the application of SHRiNK to networks leaves the network topology unchanged, in the web farm case we experimented with scaling
the topology too. Our results were encouraging.
REFERENCES
[1] The Apache Web-Server. [Online]. Available: http://httpd.apache.org
[2] D. Bansal and H. Balakrishnan, “Binomial congestion control algorithms,” in Proc. IEEE INFOCOM, 2001, pp. 631–640.
[3] P. Barford and M. Crovella, “Generating representative web workloads
for network and server performance evaluation,” in Proc. ACM SIGMETRICS, Jun. 1998, pp. 151–160.

10Since Internet sessions are Poisson [9], Internet flows can be considered as
if they were Poisson [15].

988

[4] T. Bonalds, A. Prutiere, G. Gegnie, and J. Roberts, “Insensitivity results
in statistical bandwidth sharing,” in Teletraffic Engineering in the Internet Era, Proc. ITC-17, Sep. 2001, pp. 125–136.
[5] Cisco. NetFlow services and applications. White paper (2000).
[Online]. Available: http://cisco.com/warp/public/cc/pd/iosw/ioft/neflet/tech/napps_wp.htm
[6] K. Claffy, G. Polyzos, and H.-W. Braun, “Applications of sampling
methodologies to network traffic characterization,” in Proc. ACM
SIGCOMM, San Francisco, CA, Sep. 1993, pp. 194–203.
[7] C. Estan and G. Varghese, “New directions in traffic measurement and
accounting,” in Proc. ACM SIGCOMM Internet Measurement Workshop, Pittsburgh, PA, 2002, pp. 323–338.
[8] W. Fang and L. Peterson, “Inter-AS traffic patterns and their implications,” in Proc. Global Telecommunications Conf. (GLOBECOM), 1999,
pp. 1859–1868.
[9] A. Feldmann, A. C. Gilbert, and W. Willinger, “Data networks as cascades: Investigating the multifractal nature of internet wan traffic,” in
Proc. ACM SIGCOMM, 1998, pp. 42–55.
[10] P. Fernando, W. Zhikui, S. Low, and J. Doyle, “A new TCP/AQM for
stable operation in fast networks,” in Proc. IEEE INFOCOM, 2003, pp.
96–105.
[11] S. Floyd and V. Jacobson, “Random early detection gateways for congestion avoidance,” IEEE/ACM Trans. Netw., vol. 1, no. 4, pp. 397–413,
Aug. 1991.
[12] Fluid Models for Large, Heterogeneous Networks. [Online]. Available:
http://www-net.cs.umass.edu/fluid/
[13] C. Fraleigh, C. Diot, B. Lyles, S. Moon, P. Owezarski, D. Papagiannaki, and F. Tobagi, “Design and deployment of a passive monitoring
infrastructure,” in Proc. Workshop Passive and Active Measurements
(PAM2001), Amsterdam, The Netherlands, Apr. 2001.
[14] C. Fraleigh, S. Moon, C. Diot, B. Lyles, and F. Tobagi, “Packet-Level
Traffic Measurements From a Tier-1 IP Backbone,”, Tech. Rep. TR01ATL-110101, Sprint ATL Tech. Rep., Nov. 2001.
[15] S. Ben Fredj, T. Bonalds, A. Prutiere, G. Gegnie, and J. Roberts, “Statistical bandwidth sharing: A study of congestion at flow level,” in Proc.
ACM SIGCOMM, Aug. 2001, pp. 111–122.
[16] R. Gibbens and F. Kelly, “Distributed connection acceptance control
for a connectionless network,” in Proc. 16th Int. Teletraffic Congress
(ITC16), Edinburgh, Scotland, 1999, pp. 941–952.
[17] C. V. Hollot, V. Misra, D. Towlsey, and W. Gong, “On designing improved controllers for AQM routers supporting TCP flow,” in Proc. IEEE
INFOCOM, 200l, pp. 1726–1734.
, “A control theoretic analysis of RED,” in Proc. IEEE INFOCOM,
[18]
2001, pp. 1510–1519.
[19] S. Kunniyur and R. Srikant, “Analysis and design of an adaptive virtual
queue (AVQ) algorithm for active queue management,” in Proc. ACM
SIGCOMM, San Diego, CA, 2001, pp. 123–134.
[20] V. Misra, W. Gong, and D. Towsley, “A fluid-based analysis of a network
of AQM routers supporting TCP flows with an application to RED,” in
Proc. ACM SIGCOMM, 2000, pp. 151–160.
[21] The Network Simulator – ns-2. [Online]. Available: http://www.isi.edu/
nsnam/ns
[22] C. J. Nuzman, I. Saniee, W. Sweldens, and A. Weiss, “A compound
model for TCP connection arrivals,” in Proc. ITC Seminar IP Traffic
Modeling, Monterey, CA, Sep. 2000.
[23] T. Ott, T. Lakshman, and L. Wong, “SRED: Stabilized RED,” in Proc.
IEEE INFOCOM, 1999, pp. 1346–1355.
[24] R. Pan, “Randomized algorithms for bandwidth partitioning and performance prediction in the Internet,” Ph.D. dissertation, Stanford Univ.,
Stanford, CA, Sep. 2002.
[25] R. Pan, B. Prabhakar, K. Psounis, and M. Sharma, “A study of the applicability of a scaling hypothesis,” in Proc. 4th Asia Control Conf., Singapore, 2002.
[26] V. Paxson and S. Floyd. (1995, Jun.) Wide area traffic: The failure
of poisson modeling. IEEE/ACM Trans. Netw. [Online], vol (3), pp.
226–244
[27] K. Psounis, “Probabilistic methods for web caching and performance
prediction of IP networks and web farms,” Ph.D. dissertation, Stanford
Univ., Stanford, CA, Dec. 2002.
[28] S. Shakkottai and R. Srikant, “How good are deterministic fluid models
of internet congestion control,” in Proc. IEEE INFOCOM, 2002, pp.
497–505.
[29] P. Tinnakornsrisuphap and A. Makowski, “Limit behavior of ECN/RED
gateways under a large number of TCP flows,” in Proc. IEEE INFOCOM, 2003, pp. 873–883.

IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 13, NO. 5, OCTOBER 2005

[30] J. Walrand, “A transaction-level tool for predicting TCP performance
and for network engineering,” in MASCOTS, 2000, [Online.] http://walrandpc.eecs.berkeley.edu/Papers/mascotsl.pdf.
[31] W. Willinger, M. S. Taqqu, R. Sherman, and D. V. Wilson, “Self-similarity through high-variability: Statistical analysis of ethernet LAN
traffic at the source level,” IEEE/ACM Trans. Netw., vol. 5, no. 1, pp.
71–86, Feb. 1997.

Rong Pan received the Ph.D. degree in electrical engineering from Stanford University, Stanford, CA, in
2002.
She is currently with Cisco Systems. Her research
interests are congestion control, active queue management, and TCP performance.

Balaji Prabhakar (M’00–SM’05) received the
Ph.D. degree from the University of California at
Los Angeles in 1994.
He has been at Stanford University, Stanford, CA,
since 1998, where he is an Assistant Professor of
Electrical Engineering and Computer Science. He
was a Post-Doctoral Fellow at Hewlett-Packard’s
Basic Research Institute in the Mathematical Sciences (BRIMS) from 1995 to 1997 and visited
the Electrical Engineering and Computer Science
Department at the Massachusetts Institute of Technology, Cambridge, from 1997 to 1998. He is interested in network algorithms
(especially for switching, routing and quality-of-service), wireless networks,
web caching, network pricing, information theory and stochastic network
theory.
Dr. Prabhakar is a Terman Fellow at Stanford University and a Fellow of the
Alfred P. Sloan Foundation. He has received the CAREER award from the National Science Foundation, the Erlang Prize from the INFORMS Applied Probability Society, and the Rollo Davidson Prize.

Konstantinos Psounis (S’97–M’02) received a
degree from the Department of Electrical and Computer Engineering, National Technical University of
Athens, Athens, Greece, in 1997 and the M.S. degree
in electrical engineering from Stanford University,
Stanford, CA, in 1999. He received the Ph.D. degree
from Stanford University in 2002.
His research concerns probabilistic, scalable algorithms for Internet-related problems. He has worked
mainly on web caching and performance, web traffic
modelling, congestion control, and performance prediction of IP networks and web farms.
Mr. Konstantinos has been a Stanford Graduate Fellow throughout his
graduate studies. He has received the Technical Chamber of Greece Award for
graduating first in his class.

Damon Wischik received the B.A. degree in mathematics in 1995 and the Ph.D. degree in 1999 from
Cambridge University, Cambridge, U.K. He held a
research fellowship at Trinity College, Cambridge,
until December 2004.
Since October 2004, he has held a University
Research Fellowship from the Royal Society, in the
Networks Research Group of the Department of
Computer Science at University College London,
London, U.K.

Probe and Adapt: Rate Adaptation for HTTP Video
Streaming At Scale
Zhi Li, Xiaoqing Zhu, Josh Gahm, Rong Pan, Hao Hu, Ali C. Begen, Dave Oran

I. I NTRODUCTION
Over the past few years, we have witnessed a major technology convergence for Internet video streaming towards a
new paradigm named HTTP-based adaptive streaming (HAS).
Since its inception in 2007 by Move Networks [1], HAS has
been quickly adopted by major vendors and service providers.
Today, HAS is employed for over-the-top video delivery by
many major media content providers. A recent report by Cisco
[7] predicts that video will constitute more than 90% of the
total Internet traffic by 2014. Therefore, HAS may become a
predominant form of Internet traffic in just a few years.
In contrast to conventional RTP/UDP-based video streaming, HAS uses HTTP/TCP – the protocol stack traditionally
used for Web traffic. In HAS, a video stream is chopped
into short segments of a few seconds each. Each segment is
pre-encoded and stored at a server in a number of versions,
each with a distinct video bitrate, resolution and/or quality.
After obtaining a manifest file with necessary information, a
client downloads the segments sequentially using plain HTTP

Fetched Bitrate Aggregated over 36 Streams
120
Fetched (Mbps)

Abstract—Today, the technology for video streaming over
the Internet is converging towards a paradigm named HTTPbased adaptive streaming (HAS), which brings two new features.
First, by using HTTP/TCP, it leverages network-friendly TCP
to achieve both firewall/NAT traversal and bandwidth sharing.
Second, by pre-encoding and storing the video in a number of
discrete rate levels, it introduces video bitrate adaptivity in a
scalable way so that the video encoding is excluded from the
closed-loop adaptation. A conventional wisdom is that the TCP
throughput observed by an HAS client indicates the available
network bandwidth, and thus can be used as a reliable reference
for video bitrate selection.
We argue that this is no longer true when HAS becomes
a substantial fraction of the total traffic. We show that when
multiple HAS clients compete at a network bottleneck, the
presence of competing clients and the discrete nature of the
video bitrates together result in difficulty for a client to correctly
perceive its fair-share bandwidth. Through analysis and test bed
experiments, we demonstrate that this fundamental limitation
leads to, for example, video bitrate oscillation that negatively
impacts the video viewing experience. We therefore argue that
it is necessary to design at the application layer using a “probeand-adapt” principle for HAS video bitrate adaptation, which
is akin to, but also independent of the transport-layer TCP
congestion control. We present PANDA – a client-side rate
adaptation algorithm for HAS – as practical embodiment of
this principle. Our test bed results show that compared to
conventional algorithms, PANDA is able to reduce the instability
of video bitrate selection by over 75% without increasing the
risk of buffer underrun.

100
80
60
200

300

400

500

600

700
800
Time (Sec)

900

1000

1100

1200

Fetched Bitrate of Individual Streams (Zoom In)
6
Fetched (Mbps)

arXiv:1305.0510v2 [cs.NI] 7 Jul 2013

Cisco Systems, San Jose, CA USA
{zhil2, xiaoqzhu, jgahm, ropan, hahu2, abegen, oran}@cisco.com

4
2
0
400

420

440

460

480

500
520
Time (Sec)

540

560

580

600

Fig. 1.
Oscillation of video bitrate when 36 Microsoft Smooth clients
compete at a 100-Mbps link. For more detailed experimental setup, refer to
§VI-B.

GETs, estimates the network conditions, and selects the video
bitrate of the next segment on-the-fly. A conventional wisdom
is that since the bandwidth sharing of HAS is dictated by
TCP, the problem of video bitrate selection can be resolved
straightforwardly. A simple rule of thumb is to approximately
match the video bitrate to the observed TCP throughput.
A. Emerging Issues
A major trend in HAS use cases is its large-scale deployment in managed networks by service providers, which
typically leads to aggregating multiple HAS streams in the
aggregation/core network. For example, an important scenario
is that within a single household or a neighborhood, several
HAS flows belonging to one DOCSIS1 bonding group compete
for bandwidth. In the unmanaged wide-area Internet, as HAS
is growing to become a substantial fraction of the total traffic,
it will also become more and more common to have multiple
HAS streams compete for available bandwidth at any network
bottlenecks.
While a simple rate adaptation algorithm might work fairly
well for the case where a single HAS stream operates alone
or shares bandwidth with non-HAS traffic, recent studies
[13], [3] have reported undesirable behaviors when multiple
HAS streams compete for bandwidth at a bottleneck link. For
example, while studies have suggested that significant video
quality variation over time is undesirable for a viewer’s quality
of experience [18], in [13] the authors reported unstable video
1 Data

over cable service interface specification.

bitrate selection and unfair bandwidth sharing among three
Microsoft Smooth clients sharing a 3-Mbps link. In our own
test bed experiments (see Figure 1), we observed significant
and regular video bitrate oscillation when multiple Microsoft
Smooth clients share a bottleneck link. We also found that
oscillation behavior persists under a wide range of parameter
settings, including the number of players, link bandwidth, start
time of clients, heterogeneous RTTs, random early detection
(RED) queueing parameters, the use of weight fair queueing
(WFQ), the presence of moderate web-like cross traffic, etc.
Our study shows that these HAS rate oscillation and instability behaviors are not incidental – they are simply symptoms
of a much more fundamental limitation of the conventional
HAS rate adaptation algorithms, in which the TCP downloading throughput observed by a client is directly equated
to its fair share of the network bandwidth. This fundamental
problem would also impact a HAS client’s ability to avoid
buffer underrun when the bandwidth suddenly drops. In brief,
the problem derives from the discrete nature of HAS video
bitrates. This makes it impossible to always match the video
bitrate to the network bandwidth, resulting in undersubscription of the network bandwidth. Undersubscription is typically
coupled with clients’ on-off downloading patterns. The offintervals then become a source of ambiguity for a client to
correctly perceive its fair share of the network bandwidth,
thus preventing the client from making accurate rate adaptation
decisions2 .
B. Overview of Solution
To overcome this fundamental limitation, we envision a
solution based on a “probe-and-adapt” principle. In this approach, the TCP downloading throughput is taken as an
input only when it is an accurate indicator of the fair-share
bandwidth. This usually happens when the network is oversubscribed (or congested) and the off-intervals are absent. In
the presence of off-intervals, the algorithm constantly probes3
the network bandwidth by incrementing its sending rate, and
prepares to back off once it experiences congestion. This
new mechanism shares the same spirit with TCP’s congestion
control, but it operates independently at the application layer
and at a per-segment rather than a per-RTT time scale. We
present PANDA (Probe AND Adapt) – a client-side rate
adaptation algorithm – as a specific implementation of this
principle.
Probing constitutes fine-tuning the requested network data
rate, with continuous variation over a range. By nature, the
available video bitrates in HAS can only be discrete. A main
challenge in our design is to create a continuous decision space
out of the discrete video bitrate. To this end, we propose to
fine-tune the intervals between consecutive segment download
requests such that the average data rate sent over the network
2 In [3], Akhshabi et al. have reached similar conclusions. But they identify
the off-intervals instead of the TCP throughput-based measurement as the
root cause. Their sequel work [4] attempts to tackle the problem from a very
different angle using traffic shaping.
3 By probing, we mean small trial increment of data rate, instead of sending
auxiliary piggybacking traffic.

Rate	
  

(a) PANDA

Time	
  

Steady	
  State	
  (Buﬀer	
  Full)	
  

Time	
  

Rate	
  

Buﬀer	
  Growing	
  

(b) Conventional Bimodal
Fig. 2.
Illustration of PANDA’s fine-granular request intervals vs. a
conventional algorithm’s bimodal request intervals.

Notation
w
κ
α
β

∆


τ
B
Bmin ; Bmax
T
T̂
T̃
x
x̂
ŷ
x̃
R
r
S(·)
Q(·)

Explanation
Probing additive increase bitrate
Probing convergence rate
Smoothing convergence rate
Client buffer convergence rate
Quantization margin
Multiplicative safety margin
Video segment duration (in video time)
Client buffer duration (in video time)
Minimum/maximum client buffer duration
Actual inter-request time
Target inter-request time
Segment download duration
Actual average data rate
Target average data rate (or bandwidth share)
Smoothed version of x̂
TCP throughput measured, x̃ := r·τ
T̃
Set of video bitrates R := {R1 , ..., RL }
Video bitrate available from R
Rate smoothing function
Video bitrate quantization function
TABLE I
N OTATIONS USED IN THIS PAPER

is a continuous variable (see Figure 2 for an illustrative comparison with the conventional scheme). Consequently, instead
of directly tuning the video bitrate, we probe the bandwidth
based on the average data rate, which in turn determines the
selected video bitrate and the fine-granularity inter-request
time.
There are various benefits associated with the probe-andadapt approach. First, it avoids the pitfall of inaccurate bandwidth estimation. Having a robust bandwidth measurement
to begin with gives the subsequent operations improved discriminative power (for example, strong smoothing of the
bandwidth measurement is no longer required, leading to
better responsiveness). Second, with constant probing via
incrementing the rate, the network bandwidth can be more
efficiently utilized. Third, it ensures that the bandwidth sharing
converges towards fair share (i.e., the same or adjacent video
bitrate) among competing clients. Lastly, an innate feature of
the probe-and-adapt approach is asymmetry of rate shifting –
PANDA is equipped with conservative rate level upshift but
more responsive downshift. Responsive downshift facilitates
fast recovery from sudden bandwidth drops, and thus can
effectively mitigate the danger of playout stalls caused by
buffer underrun.

Rate	
  

C. Paper Organization
In the rest of the paper, after formalizing the problem (§II),
we first introduce a method to characterize the conventional
rate adaptation algorithms (§III), based on which we analyze
the root cause of its problems (§IV). We then introduce our
probe-and-adapt approach (§V) to directly address the root
cause, and present the PANDA rate adaptation algorithm as
a concrete implementation of this idea. We provide comprehensive performance evaluations (§VI). We conclude the paper
with final remarks and discussion of future work (§VIII).
II. P ROBLEM M ODEL
In this section, we formalize the problem by first describing
a representative HAS server-client interaction process. We
then outline a four-step model for an HAS rate adaptation
algorithm. This will allow us to compare the proposed PANDA
algorithm with its conventional counterpart. Table I lists the
main notations used in this paper.
A. Process of HAS Server-Client Interaction
Consider that a video stream is chopped into segments of τ
seconds each. Each segment has been pre-encoded at L video
bitrates, all stored at a server. Denote by R := {R1 , ..., RL }
the set of available video bitrates, with 0 < R` < Rm for
` < m.
For each client, the streaming process is divided into sequential segment downloading steps n = 1, 2, .... The process
we consider here generalizes the process used by conventional
HAS clients by further incorporating variable durations between consecutive segment requests. Refer to Figure 3. At
the beginning of each download step n, a rate adaptation
algorithm:
• Selects the video bitrate of the next segment to be
downloaded, r[n] ∈ R;
• Specifies how much time to give for the current download, until the next download request (i.e., the interrequest time), T̂ [n].
The client then initiates an HTTP GET request to the server
for the segment of sequence number n and video bitrate
r[n], and the downloading starts immediately. Let T̃ [n] be
the download duration – the time required to complete the
download. Assuming that no pipelining of downloading is
involved, the next download step starts after time
T [n] = max(T̂ [n], T̃ [n]),

(1)

where T [n] is the actual inter-request time. That is, if the
download duration T̃ [n] is shorter than the target delay T̂ [n],
the client waits time T̂ [n] − T̃ [n] (i.e., the off-interval) before
starting the next downloading step (Scenario A); otherwise,
the client starts the next download step immediately after the
current download is completed (Scenario B).
Typically, a rate adaptation algorithm also measures its TCP
throughput x̃ during the segment downloading, via:
x̃[n] :=

r[n] · τ
.
T̃ [n]

Scenario	
  A	
  

Scenario	
  B	
  

C
τ ⋅ r[n]

τ ⋅ r[n +1]


T[n]
T̂[n]
Fig. 3.

τ ⋅ r[n + 2]

 +1]
T[n
T̂[n +1]

Time	
  

The HAS segment downloading process.

The downloaded segments are stored in the client buffer.
After playout starts, the buffer is consumed by the video
player at a natural rate of one video second per real second on
average. Let B[n] be the buffer duration (measured in video
time) at the end of step n. Then the buffer dynamics can be
characterized by:
B[n] = max (0, B[n − 1] + τ − T [n]) .

(2)

B. Four-Step Model
We present a four-step model for an HAS rate adaptation
algorithm, generic enough to encompass both the conventional
algorithms (e.g., [15], [19], [20], [17], [16]) and the proposed
PANDA algorithm. In this model, a rate adaptation algorithm
proceeds in the following four steps.
• Estimating. The algorithm starts by estimating the network bandwidth x̂[n] that can legitimately be used.
• Smoothing. x̂[n] is then noise-filtered to yield the
smoothed version ŷ[n], with the aim of removing outliers.
• Quantizing. The continuous ŷ[n] is then mapped to the
discrete video bitrate r[n] ∈ R, possibly with the help of
side information such as client buffer size, etc.
• Scheduling. The algorithm selects the target interval until
the next download request, T̂ [n].
III. C ONVENTIONAL A PPROACH
Using the four-step model above, in this section we introduce a scheme to characterize a conventional rate adaptation
algorithm, which will serve as a benchmark.
To the best of our knowledge, almost all of today’s commercial HAS players4 implement the measuring and scheduling
parts of the rate adaptation algorithm in a similar way, though
they may differ in their implementation of the smoothing
and quantizing parts of the algorithm. Our claim is based
on a number of experimental studies of commercial HAS
players [2], [10], [13]. The scheme described in Algorithm
1 characterizes their essential ideas.
First, the algorithm equates the currently available bandwidth share x̂[n] to the past TCP throughput x̃[n−1] observed
during the on-interval T̃ [n − 1]. As the bandwidth is inferred
reactively based on the previous downloads, we refer to this
as reactive bandwidth estimation.
The algorithm then obtains a filtered version ŷ[n] using a
smoothing function S(·) that takes as input the measurement
history {x̂[m] : m ≤ n}, as described in (4). Various filtering
methods are possible, such as sliding-window moving average,
4 In this paper, the terms “HAS player” and “HAS client” are used
interchangeably.

2
Rate

Rate

2
1
0
0

1

2

3

4

5

6

7

1
0
0

8

1

(a) Perfectly Subscribed, Round-Robin
Rate

Rate

5

6

7

8

1

2

3

4

5

6

7

8

5

6

7

8

1
0
0

1

(b) Perfectly Subscribed, Partially Overlapped

2

3

4

(e) Undersubscribed
2
Rate

2
Rate

4

2

1

1
0
0

3

(d) Oversubscribed

2

0
0

2

1

2

3

4

5

6

7

8

(c) Perfectly Subscribed, Fully Overlapped

1
0
0

1

2

3

4

5

6

7

8

(f) Single-Client

Fig. 4. Illustration of various bandwidth sharing scenarios. In (a), (b) and (c), the link is perfectly subscribed. In (d), the bandwidth sharing starts with
round-robin mode but then link becomes oversubscribed. In (e), the bandwidth sharing starts with fully overlapped mode when the link is oversubscribed.
Starting from the second round, the link becomes undersubscribed. In (f), a single client is downloading, and the downloading on-off pattern exactly matches
that of the blue segments in (a).

Algorithm 1 Conventional
At the beginning of each downloading step n:
1) Estimate the bandwidth share x̂[n] by equating it to the
measured TCP throughput:
x̂[n] = x̃[n − 1].

(3)

2) Smooth out x̂[n] to produce filtered version ŷ[n] by
ŷ[n] = S({x̂[m] : m ≤ n}).

(4)

3) Quantize ŷ[n] to the discrete video bitrate r[n] ∈ R by
r[n] = Q (ŷ[n]; ...) .

(5)

4) Schedule the next download request depending on the
buffer fullness:
(
0, B[n − 1] < Bmax ,
T̂ [n] =
(6)
τ, otherwise.

exponential weighted moving average (EWMA) or harmonic
mean [13].
The next step maps the continuous ŷ[n] to a discrete video
bitrate r[n] ∈ R using a quantization function Q(·). In general,
Q(·) can also incorporate side information, including the past
fetched bitrates {r[m] : m < n} and the buffer history {B[m] :
m < n}.
Lastly, the algorithm determines the target inter-request time
T̂ [n]. In (6), T̂ [n] is a mechanical function of the buffer
duration B[n − 1]. If B[n − 1] is less than a pre-defined
maximum buffer Bmax , T̂ [n] is set to 0, and by (1), the next
segment downloading starts right after the current download
is finished; otherwise, the inter-request time is set to the video
segment duration τ , to stop the buffer from further growing.
This creates two distinct modes of segment downloading –
the buffer growing mode and the steady-state mode, as shown
in Figure 2(b). We refer to this as the bimodal download
scheduling.

IV. A NALYSIS OF THE C ONVENTIONAL A PPROACH
In this section, we take a deep dive into the conventional
rate adaptation algorithms and study their limitations.
A. Bandwidth Cliff Effect
As we have seen in the previous section, conventional
rate adaptation algorithms use reactive bandwidth estimation
(3) that equates the estimated bandwidth share to the TCP
throughput observed during the on-intervals. In the presence
of competing HAS clients, however, the TCP throughput does
not always faithfully represent the fair-share bandwidth. In this
section, we present an intuitive analysis of this phenomenon,
by extending the one first presented in [3].5 A rigorous analysis
of this phenomenon is presented in Appendix A.
First, we illustrate with simple examples. Figure 4 (a) - (e)
show the various scenarios of how a link can be shared by two
HAS clients in steady-state mode. We consider three different
scenarios: perfect link subscription, link oversubscription and
link undersubscription. We assume ideal TCP behavior, i.e.,
perfectly equal sharing of the available bandwidth when the
transfers overlap.
Perfect Subscription: In perfect link subscription, the total
amount of traffic requested by the two clients perfectly fills the
link. (a), (b) and (c) illustrate three different modes of bandwidth sharing, depending on the starting time of downloads
relative to each other. Essentially, under perfect subscription,
there are unlimited number of bandwidth sharing modes.
Oversubscription: In (d), the two clients start with roundrobin mode and perfect subscription. Starting from the second
round of downloading, the bandwidth is reduced and the link
becomes oversubscribed, i.e., each client requests segments
larger than its current fair-share portion of the bandwidth.
This will result in unfinished downloads at the end of each
downloading round. Then, the unfinished segment will start
overlapping with segments of the next round. This repeats and
5 A main difference of our analysis compared to [3] is that we rigorously
prove the convergence properties presented in the bandwidth cliff effect.

With an understanding of the bandwidth cliff effect, we
are now in a good position to explain the bitrate oscillation
observed in Figure 1.
Figure 6 illustrates this process. When the client buffer
reaches the maximum level Bmax , by (6), off-intervals start
to emerge. The link becomes undersubscribed, leading to
bandwidth overestimation (a). This triggers the upshift of

TCP Throughput (Mbps)

3
2.5
2
1.5
1
90

95

100
105
110
Link Subscription (%)

115

12

130

TCP Throughput

10

120

8
6
4

Link Subscription

0

100

(a) (c)

90

(b) (d)

2

Requested Video Bitrate
450

110

500

550

Time (Sec)

600

80
650

Link Subscription (%)

B. Video Bitrate Oscillation

3.5

Fig. 5. Bandwidth cliff effect: measured TCP throughput vs. link subscription
rate for 100 thin clients sharing a 100-Mbps link. Each thin client repeatedly
downloads a segment every τ = 2 seconds.

Bitrate (Mbps)

the downloading will become more and more overlapped, until
all the clients enter the fully overlapped mode.
Undersubscription: In (e), initially the bandwidth sharing
is in fully overlapped mode, and the link is oversubscribed.
Starting from the second round, the bandwidth increases and
the link becomes undersubscribed. Then the clients start filling
up each other’s off-intervals, until a transmission gap emerges.
The bandwidth sharing will eventually converge to a mode
which is determined by the download start times.
In any case, the measured TCP throughput faithfully represents the fair-share bandwidth only when the bandwidth
sharing is in the fully overlapped mode; in all other cases the
TCP throughput overestimates the fair-share bandwidth. Thus,
most of the time, the bandwidth estimate is accurate when
the link is oversubscribed. Bandwidth overestimation occurs
when the link is undersubscribed or perfectly subscribed. In
general, when the number of competing clients is n, the
bandwidth overestimation ranges from one to n times the fairshare bandwidth.
Although the preceding simple examples assume idealized
TCP behavior which abstracts away the complexity of TCP
congestion control dynamics, it is easy to verify that similar
behavior occurs with real TCP connections. To see this,
we conducted a simple test bed experiment as follows. We
implemented a “thin client” to mimic an HAS client in the
steady-state mode. Each thin client repeatedly downloads a
segment every 2 seconds. We run 100 instances of the thin
client sharing a bottleneck link of 100 Mbps, each with a
starting time randomly selected from a uniform distribution
between 0 and 2 seconds. Figure 5 plots the measured average
TCP throughput as a function of the link subscription rate.
We observe that when the link subscription is below 100%,
the measured throughput is about 3x the fair-share bandwidth
of ~1 Mbps. When the link subscription is above 100%, the
measured throughput successfully predicts the fair-share bandwidth quite accurately. We refer to this sudden transition from
overestimation to fairly accurate estimation of the bandwidth
share at 100% subscription as the bandwidth cliff effect.
We summarize our findings as follows:
• Link oversubscription converges to fully overlapped
bandwidth sharing and accurate bandwidth estimation.
• Link undersubscription converges to a bandwidth sharing
pattern determined by the download start times and
bandwidth overestimation.
• In perfect link subscription, there exist unlimited bandwidth sharing modes, leading to bandwidth overestimation.

!

Fig. 6. Illustration of vicious cycle of video bitrate oscillation. This plot is
obtained with 36 Smooth clients sharing a 100-Mbps link. For experimental
setup, refer to §VI-B.

requested video bitrate (b). As the available bandwidth cannot
keep up with the video bitrate, the buffer falls below Bmax . By
(6), the client falls back to the buffer growing mode and the
off-intervals disappear, in which case the link again becomes
oversubscribed and the measured throughput starts to converge
to the fair-share bandwidth (c). Lastly, due to the quantization
effect, the requested video bitrate falls below the fair-share
bandwidth (d), and the client buffer starts growing again,
completing one oscillation cycle.
C. Fundamental Limitation
The bandwidth overestimation phenomenon reveals a more
general and fundamental limitation of the class of conventional reactive bandwidth estimation approaches discussed so
far. As video bitrates are chosen solely based on measured
TCP throughput from past segment downloads during the
on-intervals, such decisions completely ignore the network
conditions during the off-intervals. This leads to an ambiguity
of client knowledge of available network bandwidth during the
off-intervals, which, in turn, hampers the adaptation process.
To illustrate this point, consider two alternative scenarios as
depicted in Figures 4 (f) and (a). In (f), the client downloading
the blue (darker-shaded) video segments occupies the link
alone; in (a), it shares the same link with a competing client
downloading the green (lighter-shaded) video segments. Note
that the on/off-intervals for all the blue (darker-shaded) video
segments follow exactly the same pattern in both scenarios. Consequently, the client observes exactly the same TCP

throughput measurement over time. If the client would obtain
a complete picture of the network, it would know to upshift
its video bitrate in (f) but retain its current bitrate in (a).
In practice, however, an individual client cannot distinguish
between these two scenarios, hence, is bound to the same
behavior in both.
Note that as long as the off-intervals persist, such ambiguity
in client knowledge is inherent to the bandwidth measurement step in a network with competing streams. It cannot
be resolved or remedied by improved filtering, quantization,
or scheduling steps performed later in the client adaptation
algorithm. Moreover, the bandwidth cliff effect, as discussed
in Section IV-A, suggests that the bandwidth overestimation
problem does not improve with more clients, and that it can
introduce large errors even with slight link undersubscription.
Instead, the client needs to take a more proactive approach
in adapting the video bitrate — whenever it is known that
the client knowledge is impaired, it must avoid using such
knowledge in bandwidth estimation. A way to distinguish the
case when the knowledge is impaired from when it is not, is to
probe the network subscription by small increment of its data
sending rate. We describe one algorithm that follows such an
alternative approach in the next section.
V. P ROBE - AND -A DAPT A PPROACH
In this section, we introduce our proposed probe-and-adapt
approach to directly address the root cause of the conventional
algorithms’ problems. We begin the discussion by laying
out the design goals that a rate adaptation algorithm aims
to achieve. We then describe the PANDA algorithm as an
embodiment of the probe-and-adapt approach, and provide its
functional verification using experimental traces.
A. Design Goals
Designing an HAS rate adaptation algorithm involves tradeoffs among a number of competing goals. It is not legitimate
to optimize one goal (e.g., stability) without considering its
tradeoff factors. From an end-user’s perspective, an HAS rate
adaptation algorithm should be designed to meet these criteria:
• Avoiding buffer underrun. Once the playout starts, buffer
underrun (i.e., complete depletion of buffer) leads to a
playout stall. Empirical study [8] has shown that buffer
underrun may have the most severe impact on a user’s
viewing experience. To avoid it, some minimal buffer
level must be maintained at all times6 , and the adaptation algorithm must be highly responsive to network
bandwidth drops.
• High quality smoothness. In the simplest setting without
considering visual perceptual models, high video quality
smoothness translates into avoiding both frequent and
6 Note that, however, the buffer level must also have an upper bound, for a
few different reasons. In live streaming, the end-to-end latency from the realtime event to the event being displayed on user’s screen must be reasonably
short. In video-on-demand, the maximum buffered video must be limited to
avoid wasted network usage in case of an early termination of playback and
to limit memory usage.

Algorithm 2 PANDA
At the beginning of each downloading step n:
1) Estimate the bandwidth share x̂[n] by
x̂[n] − x̂[n − 1]
= κ·(w −max(0, x̂[n−1]− x̃[n−1])),
T [n − 1]
(7)
2) Smooth out x̂[n] to produce filtered version ŷ[n] by
ŷ[n] = S({x̂[m] : m ≤ n}).

(8)

3) Quantize ŷ[n] to the discrete video bitrate r[n] ∈ R by
r[n] = Q (ŷ[n]; ...) .

(9)

4) Schedule the next download request via
T̂ [n] =

r[n] · τ
+ β · (B[n − 1] − Bmin )
ŷ[n]

(10)

significant video bitrate shifts among available video
bitrate levels [13], [18].
• High average quality. High average video quality dictates
that a client should fetch high-bitrate segments as much as
possible. Given a fixed network bandwidth, this translates
into high network utilization.
• Fairness. In the simplest setting, fairness translates
into equal network bandwidth sharing among competing
clients.
Note that this list above is non-exhaustive. Other criteria,
such as low playout startup latency, are also important factors
impacting user’s viewing experience.
B. PANDA Algorithm
In this section, we discuss the PANDA algorithm. Compared
to the reactive bandwidth estimation used by a conventional
rate adaptation algorithm, PANDA uses a more proactive
probing mechanism. By probing, PANDA determines a target
average data rate x̂. This average data rate is subsequently
used to determine the video bitrate r to be fetched, and the
interval T̂ until the next segment download request.
The PANDA algorithm is described in Algorithm 2, and
a block diagram interpretation of the algorithm is shown in
Figure 7. Compared to the conventional algorithm in Algorithm 1, we only make modifications in the estimating and
scheduling steps – we replace (3) with (7) for estimating the
bandwidth share, and (6) with (10) for scheduling the next
download request. We now focus on elaborating each of these
two modifications.
In the estimating step, (7) is designed to directly address
the root cause that leads to the video bitrate oscillation phenomenon. Based on the insights obtained from §IV-A, when
the link becomes undersubscribed, the direct TCP throughput
estimate x̃ becomes inaccurate in predicting the fair-share
bandwidth, and thus should be avoided. Instead, the client
continuously increments the target average data rate x̂ by κ · w
per unit time as a probe of the available capacity. Here κ is the
probing convergence rate and w is the additive increase rate.

Network
Dynamics

x

Estimation

x̂

Smoothing

D
Fig. 7.

r

Scheduling

T̂

Network
Dynamics

B

D

Block diagram for PANDA (Algorithm 2). Module D represents delay of one adaptation step.

The algorithm keeps on monitoring the TCP throughput x̃, and
compares it against the target average data rate x̂. If x̃ > x̂,
x̃ would not be informative, since in this case the link may
still be undersubscribed and x̃ may overestimate the fair-share
bandwidth. Thus, its impact is suppressed by the max(0, ·)
function. But if x̃ < x̂, then TCP throughput cannot keep up
with the target average data rate indicates that congestion has
occurred. This is when the target data rate x̂ should back off.
The reduction imposed on x̂ is made proportional to x̂ − x̃.
Intuitively, the lower the measured TCP throughput x̃, the
more reduction that needs to be imposed on x̂. This design
makes our rate adaptation algorithm very agile to bandwidth
changes.
PANDA’s probing mechanism shares similarities with
TCP’s congestion control [11], and has an additive-increasemultiplicative-decrease (AIMD) interpretation: κ · w is the
additive increase term, and −κ·max(0, x̂[n−1]− x̃[n−1]) can
be interpreted as the multiplicative decrease term. The main
difference is that in TCP, congestion is indicated by packet
losses (TCP Reno) or increased round-trip time (delay-based
TCP), whereas in (7), congestion is indicated by the reduction
of measured TCP throughput. This AIMD property ensures
that PANDA is able to efficiently utilize the network bandwidth, and in the presence of multiple clients, the bandwidth
for each client eventually converges to fair-share status7 .
In the scheduling step, (10) aims to determine the target
inter-request time T̂ [n]. By right, T̂ [n] should be selected such
that the smoothed target average data rate ŷ[n] is equal to
r[n]·τ
. But additionally, the selection of T̂ [n] should also drive
T̂ [n]
the buffer B[n] towards a minimum reference level Bmin > 0,
so the second term is added to the right hand side of (10),
where β > 0 controls the convergence rate.
One distinctive feature of the PANDA algorithm is its
hybrid closed-loop/open-loop design. Refer to Figure 7. In
this system, (7) forms a closed loop by itself that determines
the target average data rate x̂. (10) forms a closed loop by
itself that determines the target inter-request time T̂ . Overall,
the estimating, smoothing, quantizing and scheduling steps
together form an open loop. The main motivation behind
this design is to reduce the bitrate shifts associated with
quantization. Since quantization is excluded from the closed
loop of x̂, it allows x̂ to settle in a steady state. Since r[n] is
a deterministic function of x̂[n], it can also settle in a steady
state.
In Appendix B, we present an equilibrium and stability analysis of PANDA. We summarize the main results as follows.
Our equilibrium analysis shows that at steady state, the system
7 Assuming

ŷ

Quantizing

the underlying TCP is fair (e.g., equal RTTs).

variables settle at
x̂o

= x̃o + w

(11)

= ŷo
ro
Bo

= Q(x̂o ; ...)


ro
τ
=
1−
· + Bmin ,
ŷo
β

(12)

where the subscript o denotes value of variables at equilibrium.
Our stability analysis shows that for the system to converge
towards the steady state, it is necessary to have:
κ

<

w

≤

2
τ
∆,

(13)
(14)

where ∆ is a parameter associated with the quantizer Q(·),
referred to as the quantization margin, i.e., the selected discrete
rate r must satisfy
r[n] ≤ ŷ[n] − ∆.

(15)

C. Functional Verification
We verify the behavior of PANDA using experimental
traces. For detailed experiment setup (including the selection
of function S(·) and Q(·)), refer to §VI-B.
First, we evaluate how a single PANDA client adjusts its
video bitrate as the the available bandwidth varies over time.
In Figure 8, we plot the TCP throughput x̃, the target average
data rate x̂, the fetched video bitrate r and the client buffer
B for a duration of 500 seconds, where the bandwidth drops
from 5 to 2 Mbps at 200 seconds, and rises back to 5 Mbps at
300 seconds. Initially, the target average data rate x̂ ramps up
gradually over time; the fetched video bitrate r also ramps up
correspondingly. After the initial ramp-up stage, x̂ settles in a
steady state. It can be observed that at steady state, the difference between x̂ and x̃ is about 0.3 Mbps, equal to w, which
is consistent with (11). Similarly, the buffer B also settles in
a steady state, and after plugging in all the parameters, one
can verify that the steady state of buffer (12) also holds. At
200 seconds, when the bandwidth suddenly drops, the fetched
video bitrate quickly drops to the desirable level. With this
quick response, the buffer hardly drops. This property makes
PANDA favorable for live streaming applications. When the
bandwidth rises back to 5 Mbps at 300 seconds, the fetched
video bitrate gradually ramps up to the original level.
Note that, in practical implementation, we can further add
a startup logic to improve PANDA’s ramp-up speed at the
stream startup stage, akin to the slow-start mode of TCP. The
idea is simple: since it is necessary to add off-intervals only
when the buffer duration B[n] exceeds the minimum reference

4

120
Fetched (Mbps)

6

40

2
0
0

Fetched Bitrate Aggregated over 36 Streams

50
TCP Throughput
Target Avg. Data Rate
Fetched Video Bitrate

Buffer (Sec)

Bitrate (Mbps)

8

30
20

100
80

10

100

200

300

400

0
0

500

60
200

100

200
300
Time (Sec)

400

500

300

400

500

600

700
800
Time (Sec)

900

1000

1100

1200

Fetched Bitrate of Individual Streams (Zoom In)

4

40

3

30

1
0
0

Startup Off
Startup On
20

40
Time (Sec)

60

2

Fig. 10.

20

420

440

460

480

500
520
Time (Sec)

540

560

580

600

36 PANDA clients compete at a 100-Mbps link in steady state.
Measured TCP Throughput x̃

10
0
0

Startup Off
Startup On
20

40
Time (Sec)

60

Fig. 9. Comparison of the startup behavior of a PANDA player with and
without the startup logic. The bandwidth is 5 Mbps.

level Bmin , we can use the conventional algorithm at startup
or after playout stall, until B[n] ≥ Bmin ; after that, we switch
to the main Algorithm 2. Without the presence of the offintervals, the conventional algorithm works fast enough and
reasonably well. Figure 9 shows the startup behavior of a
PANDA player with 5 Mbps link bandwidth, with and without
the startup logic. As can be seen, the startup logic allows the
video bitrate to ramp up efficiently, albeit at the expense of
somewhat dampened buffer growth.
The more intriguing question is whether PANDA could
effectively stop the bitrate oscillation observed in the Smooth
players. We conduct an experiment with the same setup as the
experiment shown in Figure 1, except that the PANDA player
and the Smooth player use slightly different video bitrate levels
(due to different packaging methods). The resulting fetched
bitrates in aggregate and for each client are shown in Figure
10. From the plot of the aggregate fetched bitrate, except for
the initial fluctuation, the aggregate bitrate closely tracks the
available bandwidth of 100 Mbps. Zooming in to the individual
streams’ fetched bitrates, the fetched bitrates are confined
within two adjacent bitrate levels and the number of shifts is
much smaller than the Smooth client’s case. This affirms that
PANDA is able to achieve better stability than the Smooth’s
rate adaptation algorithm. In §VI, we perform a comprehensive
performance evaluation on each adaptation algorithm.
To help the reader develop a better intuition on why PANDA
performs better than a conventional algorithm, in Figure 11
we plot the trace of the measured TCP throughput and the
target average data rate for the same experiment as in Figure
10. Note that the fair-share bandwidth for each client is
about 2.8 Mbps. From the plot, the TCP throughput not only
grossly overestimates the fair-share bandwidth, it also has a
large variation. If used directly, this degree of noisiness gives
the subsequent operations a very hard job to extract useful

Throughput(Mbps)

2

4

0
400

10

5

0
200

300

400

500

600

700
800
Time (Sec)

900

1000

1100

1200

1000

1100

1200

Target Average Data Rate x̂
10
Target (Mbps)

Buffer (Sec)

Bitrate (Mbps)

Fig. 8. A PANDA client adapts its video bitrate under a bandwidth-varying
link. The bandwidth is initially at 5 Mbps, drops to 2 Mbps at 200 seconds
and rises back to 5 Mbps at 300 seconds.

Fetched (Mbps)

6

5

0
200

300

400

500

600

700
800
Time (Sec)

900

Fig. 11. The traces of the TCP throughput and the target average data rate
of 36 PANDA clients compete at a 100-Mbps link in steady state. The traces
of the first five clients are plotted.

information. For example, one may apply strong filtering
to smooth out the bandwidth measurement, but this would
seriously affect the responsiveness of the client. When the
network bandwidth suddenly drops, the client would not be
able to respond quickly enough to reduce its video bitrate,
leading to catastrophic buffer underrun. Moreover, the bias is
both large and difficult to predict, making any correction to
the mean problematic. In comparison, although also biased, the
target average data rate estimated by the probing mechanism
is much less noisy than the TCP throughput. One can easily
correct the bias (via (15) and quantization) and select the right
video bitrate without sacrificing responsiveness.
In Figure 12, we verify the stability criteria (13) and (14)
of PANDA. With τ = 2, the system is stable if κ < 1. This
is demonstrated by Figure 12 (a), where we show the traces
of the target average rate x̂ for two κ values 0.9 and 1.1. In
Figure 12 (b), we show that when ∆ = 0, the buffer cannot
converge towards the reference level of 30 seconds.
VI. P ERFORMANCE E VALUATION
In this section, we conduct a set of test bed experiments
to evaluate the performance of PANDA against other rate
adaptation algorithms.

30

4

Buffer(Sec)

Target (Mbps)

5

3
2
κ=0.9
κ=1.1

1
0
0

50

100
150
Time (Sec)

(a) κ

200

20

10

0
0

∆=w
∆=0
100
200
Time (Sec)

300

(b) ∆

Fig. 12. Experimental verification of PANDA’s stability criteria. In (a), one
PANDA client streams over a link of 5 Mbps. In (b), 10 PANDA clients
compete over a 10 Mbps link.

A. Evaluation Metrics
In §V-A, we discussed four criteria that are most important
for a user’s watching experience – i) ability to avoid buffer
underruns, ii) quality smoothness, iii) average quality, and iv)
fairness. In this paper, we use buffer undershoot as the metric
for Criterion i), described as follows.
• Buffer undershoot: We measure how much the buffer
goes down after a bandwidth drop as a indicator of an
algorithm’s ability to avoid buffer underruns. The less the
buffer undershoot, the less likely the buffer will underrun.
Let Bo be a reference buffer level (30 seconds for all
players in this paper), and Bi,t the buffer level for player
i at time t. The buffer undershoot for player i at time t
max(0,Bo −Bi,t )
is defined as
. The buffer undershoot for
Bo
player i within a time interval T (right after a bandwidth
drop), is defined as the 90th-percentile value of the
distribution of buffer undershoot samples collected during
T.
We inherit the metrics defined in [13] – instability, inefficiency
and unfairness, as the metrics for Criteria ii), iii) and iv), respectively. We only make a slight modification to the definition
of inefficiency. Let ri,t be the video bitrate fetched by player
i at time t.
• Instability: The instability for player i at time t is
Pk−1
d=0 |ri,t−d −ri,t−d−1 |·w(d)
Pk−1
, where w(d) = k − d is a
d=0 ri,t−d ·w(d)
weight function that puts more weight on more recent
samples. k is selected as 20 seconds.
• Inefficiency: Let C be the available bandwidth. [13]
P
| i ri,t −C |
defines inefficiency as
for player
C
P i at time t.
But sometimes the sum of fetched bitrate i ri,t can be
greater than C. To avoid unnecessary penalty in
this case,
P
max(0,C− i ri,t )
we revise the inefficiency metric to
for
C
player i at time t.
• Unfairness: Let JainF airt be the Jain fairness index
calculated on the rates ri,t at√time t over all players. The
unfairness at t is defined as 1 − JainF airt .
B. Experimental Setup
HAS Player Configuration: The benchmark players that we
use to compare against PANDA are:
• Microsoft Smooth player [5], a commercially available
proprietary player. The Smooth players are of version

1.0.837.34 using Silverlight runtime version 4.0.50826.
To our best knowledge, the Smooth player as well as the
Apple HLS and the Adobe HDS players all use the same
TCP throughput measurement mechanism, so we picked
the Smooth player as a representative.
• FESTIVE player, which we implemented based on the
details specified in [13]. The FESTIVE algorithm is the
first known client-side rate adaptation algorithm designed
to specifically address the multi-client scenario.
• A player implementing the conventional algorithm (Algorithm 1), which differs from PANDA only in the
estimating and scheduling steps.
For fairness, we ensure that PANDA and the conventional
player use the same smoothing and quantizing functions. For
smoothing, we implemented a EWMA smoother of the form:
ŷ[n]−ŷ[n−1]
= −α · (ŷ[n − 1] − x̂[n]), where α > 0 is the
T [n−1]
convergence rate of ŷ[n] towards x̂[n]. For quantization, we
implemented a dead-zone quantizer r[n] = Q(ŷ[n], r[n − 1]),
defined as follows: Let the upshift threshold be defined as
rup := maxr∈R r subject to r ≤ ŷ[n] − ∆up , and the
downshift threshold as rdown := maxr∈R r subject to
r ≤ ŷ[n] − ∆down , where ∆up and ∆down are the upshift and
downshift safety margin respectively, with ∆up ≥ ∆down ≥ 0.
The dead-zone quantizer updates r[n] as


r[n − 1] < rup ,
rup ,
(16)
r[n] = r[n − 1], rup ≤ r[n − 1] ≤ rdown ,


rdown ,
otherwise.
The “dead zone” [rup , rdown ] created by setting ∆up > ∆down
mitigates frequent bitrate hopping between two adjacent levels,
thus stabilizing the video quality (i.e. hysteresis control). For
the conventional player, set ∆up = · ŷ and ∆down = 0, where
0 ≤  < 1 is the multiplicative safety margin. For PANDA,
due to (14) and (15), set ∆up = w +  · ŷ and ∆down = w 8 .
Table II lists the default parameters used by each player, as
well as their varying values. For fairness, all players attempt
to maintain a steady-state buffer of 30 seconds. For PANDA,
Bmin is selected to be 26 seconds such that the resulting
steady-state buffer is 30 seconds (by (12)).
Server Configuration: The HTTP server runs Apache on
Red Hat 6.2 (kernel version 2.6.32-220). The Smooth player
interacts with an Microsoft IIS server by default, but we also
perform experiments of Smooth player interacting with an
Apache server on Ubuntu 10.04 (kernel version 2.6.32.21).
Network Configuration: As service provider deployment
over a managed network is our primary case of interest,
our experiments are configured to highly match the imporant
scenario where a number of HAS flows compete for bandwidth
within a DOCSIS bonding group. The test bed is configured
as in Figure 13. The queueing policy used at the aggregation
router-home router bottleneck link is the following. For a link
bandwidth of 10 Mbps or below, we use random early detection (RED) with (min_thr, max_thr, p) = (30, 90, 0.25);
8 Note

that this will not give PANDA any unfair advantage.

Algorithm
PANDA

Conventional

FESTIVE

Parameter
κ
w
α
β

Bmin
α

Bmax
Window
targetbuf

Default
0.14
0.3
0.2
0.2
0.15
26
0.2
0.15
30
20
30

Values
0.04,0.07,0.14,0.28,0.42,0.56
0.05,0.1,0.2,0.3,0.4,0.5
0.5,0.4,0.3,0.2,0.1,0
0.01,0.04,0.07,0.1,0.15,0.2

20,15,10,6,3,1

HTTP
Server

100 or 10 Mbps
20 ms

Aggregation
Router

Local

l
ca
Lo

Home Loc
Router al

…
.
HAS
Client

Local

HAS
Client

TABLE II
PARAMETERS USED IN EXPERIMENTS

Fig. 13. The network topology configured in the test bed. Local indicates
that the bitrate is effectively unbounded and the link delay is 0 ms.

if the link bandwidth is 100 Mbps, we use RED with
(min_thr, max_thr, p) = (300, 900, 1). The video content is
chopped into segments of τ = 2 seconds, pre-encoded with
L = 10 bitrates: 459, 693, 937, 1270, 1745, 2536, 3758, 5379,
7861 and 11321 Kbps. For the Smooth player, the data rates
after packaging are slightly different.

C. Performance Tradeoffs
It would not be legitimate to discuss a single metric without
minding its impact on other metrics. In this section, we
examine the performance tradeoffs among the four metrics
of interest. We designed an experimental process under which
we can measure all four metrics in a single run. For each run,
five players (of the same type) compete at a bottleneck link.
The link bandwidth stays at 10 Mbps from 0 seconds to 400
seconds, drops to 2.5 Mbps at 400 seconds and stays there
until 500 seconds. We record the instability, inefficiency and
unfairness averaged over 0 to 400 seconds over all players, and
the buffer undershoot over 400 to 500 seconds averaged over
all players. Figure 14 shows the tradeoff between stability and
each of the other criteria – buffer undershoot, inefficiency and
unfairness – for each of the types of player. Each data point
is obtained via averaging over 10 runs, and each data point
represents a different value for one of the parameters of the
corresponding algorithm, as indicated in the Values column of
Table II.
For the PANDA player, the three parameters that affect
instability the most are: the probing convergence rate κ, the
smoothing convergence rate α and the safety margin . Figure
14 (a) shows that as we vary these parameters, the tradeoff
curves mostly stay flat (except for at extreme values of these
parameters), implying that the PANDA player maintains good
responsiveness as the stability is improved. A few factors

contribute to this advantage of PANDA: First, as the bandwidth
estimation by probing is quite accurate, one does not need
to apply strong smoothing. Second, since after a bandwidth
drop, the video bitrate reduction is made proportional to the
TCP throughput reduction, PANDA is very agile to bandwidth
drops. On the other hand, for both the FESTIVE and the
conventional players, the buffer undershoot significantly increases as the scheme becomes more stable. Overall, PANDA
has the best tradeoff between stability and responsiveness to
bandwidth drop, outperforming the second best conventional
player by more than 75% reduction in instability at the same
buffer undershoot level. It is worth noting that the conventional
player uses exactly the same smoothing and quantization
steps as PANDA, which implies that the gain achieved by
PANDA is purely due to the improvement in the estimating and
scheduling steps. FESTIVE has the largest buffer undershoot.
We believe this is because the design of FESTIVE has mainly
concentrated on stability, efficiency and fairness, but ignored
responsiveness to bandwidth drops. As we do not have access
to the Smooth player’s buffer, we do not have its buffer
undershoot measure in Figure 14 (a).
Figure 14 (b) shows that PANDA has the lowest inefficiency
over all as we vary its instability. The probing mechanism
ensures that the bandwidth is most efficiently utilized. As the
instability increases, the inefficiency also increases moderately.
This makes sense intuitively, as when the bitrate fluctuates,
the average fetched bitrate also decreases. The efficiency
of the conventional algorithm underperforms PANDA, but
outperforms FESTIVE. Lastly, the Smooth player has the
highest inefficiency.
Lastly, Figure 14 (c) shows that in terms of fairness,
FESTIVE achieves the best performance. This may be due to
the randomized scheduling strategy of FESTIVE. PANDA and
the conventional players have similar fairness; both of them
outperform the Smooth player.
D. Increasing Number of Players
In this section, we focus on the question of how the number
of players affects instability, inefficiency and unfairness at
steady state. Two scenarios are of interest: i) we increase
the number of players while fixing the link bandwidth at 10
Mbps, and ii) we increase the number of players while varying
the bandwidth such that the bandwidth/player ratio is fixed
at 1 Mbps/player. Figure 15 and Figure 16 report results for
these two cases, respectively. Each data point is obtained by
averaging over 10 runs.
Refer to Figure 15 (a). In the single-player case, all four
schemes are able to maintain their fetched video bitrate at a
constant level, resulting in zero instability. As the number of
players increases, the instability of the conventional player and
the Smooth player both increase quickly in a highly consistent
way. We speculate that they have very similar underlying
structure. The FESTIVE player is able to maintain its stability
at a lower level, due to the strong smoothing effect (smoothing
window at 20 samples by default), but the instability still
grows with the number of players, likely due to the bandwidth

Undershoot

0.8
0.6
0.4

PANDA (Vary α)
PANDA (Vary k)
PANDA (Vary ε)
FESTIVE (Vary Win)
Conventional (Vary α)
Smooth

0.15

0.1

0.05

0.2
0
0

0.2

0.02

0.04
Instability

0.06

0.08

0
0

0.2
0.15
PANDA (Vary α)
PANDA (Vary k)
PANDA (Vary ε)
FESTIVE (Vary Win)
Conventional (Vary α)
Smooth

0.1
0.05

0.02

(a)
Fig. 14.

0.25

Unfairness

PANDA (Vary α)
PANDA (Vary k)
PANDA (Vary ε)
FESTIVE (Vary Win)
Conventional (Vary α)

Inefficiency

1

0.04
Instability

0.06

0.08

0
0

0.01

0.02

(b)

0.03
0.04
Instability

0.05

0.06

0.07

(c)

The impact of varying instability on buffer undershoot, inefficiency and unfairness for PANDA and other benchmark players.

overestimation effect. The PANDA player exhibits a rather
different behavior: at two players it has the highest instability,
then the instability starts to drop as the number of players
increases. Investigating into the experimental traces reveals
that this is related to the specific bitrate levels selected. More
importantly, via probing, the PANDA player is immune to the
symptoms of the bandwidth overestimation, thus it is able
to maintain its stability as the number of clients increases.
The case of varying bandwidth in Figure 16 (a) exhibits
behavior fairly consistent with Figure 15 (a), with PANDA
and FESTIVE exhibiting much less instability compared to
the Smooth and the conventional players.
Figure 15 (b) and Figure 16 (b) for the inefficiency metric
both show that PANDA consistently has the best performance
as the number of players grow. The conventional player and
FESTIVE have similar performance, both outperforming the
Smooth player by a great margin. We speculate that the
Smooth player has some large bitrate safety margin by design,
with the purpose of giving cross traffic more breathing room.
Lastly, let us look at fairness. Refer to Figure 15 (a). We
have found that when the overall bandwidth is fixed, the
unfairness measure has high dependence on the specific bitrate
levels chosen, especially when the number of players is small.
For example, at two players, when the fair-share bandwidth
is 5 Mbps, the two PANDA players end up in steady state
with 5.3 Mbps and 3.7 Mbps, resulting in a high unfairness
score. At three players, when the fair-share bandwidth is 3.3
Mbps, the three PANDA players each end up with 3.7, 3.7
and 2.5 Mbps for a long period of time, resulting in a lower
unfairness score. FESTIVE exhibits lowest unfairness overall,
which is consistent with the results obtained in Figure 14 (c).
In the varying-bandwidth case in Figure 16 (c), The unfairness
ranking is fairly consistent as the number of players grow:
FESTIVE, PANDA, the conventional, and Smooth.
E. Competing Mixed Players
One important question to ask is how PANDA will behave
in the presence of different type of players? If it behaves too
conservatively and cannot grab enough bandwidth, then the
deployment of PANDA will not be successful. To answer this
question, we take the four types of players of interest and have
them compete on a 10-Mbps link. For the Smooth player, we
test it with both a Microsoft IIS server running on Windows
7, and an Apache HTTP server running on Ubuntu 10.04. A

single trace of the fetched bitrates for 500 seconds is shown
in Figure 17. The plot shows that the Smooth player’s ability
to grab the bandwidth highly depends on the server it streams
from. Using the IIS server, which runs on Windows 7 with an
aggressive TCP, it is able to fetch video bitrates over 3 Mbps.
With the Apache HTTP server, which uses Ubuntu 10.04’s
conservative TCP, the fetched bitrates are about 1 Mbps. The
conventional, PANDA and FESTIVE players all run on the
same TCP (Red Hat 6), so their differences are due to their
adaptation algorithms. Due to bandwidth overestimation, the
conventional player aggressively fetches high bitrates, but the
fetched bitrates fluctuate. Both PANDA and FESTIVE are able
to maintain a stable fetched bitrate at about the fair-share level
of 2 Mbps.
F. Summary of Performance Results
•

•

•

•

The PANDA player has the best stability-responsiveness
tradeoff, outperforming the second best conventional
player by 75% reduction in instability. PANDA also has
the best bandwidth utilization.
The FESTIVE player has been tuned to yield high stability, high efficiency and good fairness. However, it underperforms other players in responsiveness to bandwidth
drops.
The conventional player yields good efficiency, but lacks
in stability, responsiveness to bandwidth drops and fairness.
The Smooth player underperforms in efficiency, stability
and fairness. When competing against other players,
its ability to grab bandwidth is a consequence of the
aggressiveness of the underlying TCP stack.
VII. R ELATED W ORK

AIMD Principle: The design of the probing mechanism in
PANDA shares similarity with Jacobson’s AIMD principle for
TCP congestion control [11]. Kelly’s framework on network
rate control [14] provides a theoretical justification for the
AIMD principle, and proves its stability in the general network
setup.
HAS Measurement Studies: Various research efforts have
focused on understanding the behavior of several commercially deployed HAS systems. One such example is [2],
where the authors characterize and evaluate HTTP streaming
players such as Microsoft Smooth Streaming, Netflix, and

0.25
PANDA
FESTIVE
Conventional
Smooth

0.3
0.25
Inefficiency

Instability

0.04

0.35
PANDA
FESTIVE
Conventional
Smooth

0.03
0.02

0.2
0.15

PANDA
FESTIVE
Conventional
Smooth

0.2
Unfairness

0.05

0.15
0.1

0.1
0.01

0.05

0.05

0
0

5

10

0
0

15

5

Num. Clients

10

(a)
Fig. 15.

0.2

0.25
0.2
0.15

0.15
Unfairness

PANDA
FESTIVE
Conventional
Smooth

0.3

0.02

0.1
0.01

0.1
PANDA
FESTIVE
Conventional
Smooth

0.05

0.05
2

15

(c)

0.35
PANDA
FESTIVE
Conventional
Smooth

Inefficiency

Instability

10
Num. Clients

(b)

0.03

0
0

5

Instability, inefficiency and unfairness as the number of clients increases. The link bandwidth is fixed at 10 Mbps.

0.05
0.04

0
0

15

Num. Clients

4
6
Num. Clients

8

10

0
0

2

4
6
Num. Clients

(a)

8

0
0

10

2

(b)

4
6
Num. Clients

8

10

(c)

Fig. 16. Instability, inefficiency and unfairness as the number of clients increases. The link bandwidth increases with the number of players, with the
bandwidth-player ratio fixed at 1 Mbps/player.

VIII. C ONCLUSIONS
This paper identifies an emerging issue for HTTP adaptive
streaming, which is expected to become the predominant form

4
Fetched (Mbps)

Adobe OSMF via experiments in controlled settings. The first
measurement study to consider HAS streaming in the multiclient scenarios is [3]. The authors identify the root cause of
the player’s rate oscillation problem as the existence of on-off
patterns in HAS traffic. In [10], the authors measure behavior
of commercial video streaming services, i.e., Hulu, Netflix,
and Vudu, when competing with other long-lived TCP flows.
The results revealed that inaccurate estimations can trigger a
feedback loop leading to undesirably low-quality video.
Existing HAS Designs: To improve the performance of
adaptive HTTP streaming, several rate adaptation algorithms
[15], [19], [20], [17], [16] have been proposed, which, in
general, fit into the four-step model discussed in Section II-B.
In [12], a sophisticated Markov Decision Process (MDP) is
employed to compute a set of optimal client strategies in order
to maximize viewing quality. The MDP requires the knowledge of network conditions and video content statistics, which
may not be readily available. Control-theoretical approaches,
including use of a PID controller, are also considered by
several works [6], [19], [20]. A PID controller with appropriate
parameter choice can improve streaming performance. Serverside mechanisms are also advocated by some works [9], [4].
Two designs have been considered to address the multi-client
issues: in [4], a rate-shaping approach aiming to eliminate the
off-intervals, and in [13], a client rate adaptation algorithm
design implementing a combination of randomization, stateful
rate selection and harmonic mean based averaging.

3
Smooth (IIS)
Conventional
PANDA
FESTIVE
Smooth (Apache)

2
1
0
0

100

200

300
400
Time (Sec)

500

600

700

Fig. 17. PANDA, Smooth (w/ IIS), Smooth (w/ Apache), FESTIVE and the
conventional players compete at a bottleneck link of 10 Mbps.

of the Internet traffic, and lays out a solution direction that can
effectively address this issue. Our main contributions in this
paper can be summarized as follows:
•

•

•

We have identified the bandwidth cliff effect as the root
cause of the bitrate oscillation phenomenon and revealed
the fundamental limitation of the conventional reactive
measurement based rate adaptation algorithms.
We have envisioned a general probe-and-adapt principle
to directly address the root cause of the problems, and
designed and implemented PANDA, a client-based rate
adaptation algorithm, as an embodiment of this principle.
We have proposed a generic four-step model for an HAS
rate adaptation algorithm, based on which we have fairly
compared the proposed approach with the conventional
approach.

The probe-and-adapt approach and our PANDA realization
thereof achieve significant improvements in stability of HAS
systems at no cost in responsiveness. Given this framework,
we plan to explore further improvements in our future work.

R EFERENCES
[1] History
of
Move
Networks.
Available
online:
http://www.movenetworks.com/history.html.
[2] S. Akhshabi, S. Narayanaswamy, Ali C. Begen, and C. Dovrolis. An
experimental evaluation of rate-adaptive video players over HTTP.
Signal Processing: Image Communication, 27:271–287, 2012.
[3] Saamer Akhshabi, Lakshmi Anantakrishnan, Constantine Dovrolis, and
Ali C. Begen. What happens when HTTP adaptive streaming players
compete for bandwidth. In Proc. ACM Workshop on Network and
Operating System Support for Digital Audio and Video (NOSSDAV’12),
Toronto, Ontario, Canada, 2012.
[4] Saamer Akhshabi, Lakshmi Anantakrishnan, Constantine Dovrolis, and
Ali C. Begen. Server-based traffic shaping for stabilizing oscillating
adaptive streaming players. In to appear in NOSSDAV13, 2013.
[5] Alex Zambelli.
IIS Smooth Streaming Technical Overview.
http://tinyurl.com/smoothstreaming.
[6] Luca De Cicco, Saverio Mascolo, and Vittorio Palmisano. Feedback
control for adaptive live video streaming. In Proc. ACM Multimedia
Systems Conference (MMSys’11), pages 145–156, San Jose, CA, USA,
February 2011.
[7] Cisco White Paper. Cisco visual networking index - forecast and
methodology, 2011-2016. http://tinyurl.com/ciscovni2011to2016.
[8] F. Dobrian, V. Sekar, A. Awan, I. Stoica, D. Joseph, A. Ganjam, J. Zhan,
and H. Zhang. Understanding the impact of video quality on user
engagement. In Proceedings of the ACM SIGCOMM 2011 conference,
SIGCOMM ’11, pages 362–373, New York, NY, USA, 2011. ACM.
[9] Remi Houdaille and Stephane Gouache. Shaping http adaptive streams
for a better user experience. In Proceedings of the 3rd Multimedia
Systems Conference, pages 1–9, 2012.
[10] Te-Yuan Huang, Nikhil Handigol, Brandon Heller, Nick McKeown, and
Ramesh Johari. Confused, timid, and unstable: Picking a video streaming
rate is hard. In Proceedings of the 2012 ACM conference on Internet
measurement conference, 2012.
[11] V. Jacobson. Congestion avoidance and control. In Symposium proceedings on Communications architectures and protocols, SIGCOMM ’88,
pages 314–329, New York, NY, USA, 1988. ACM.
[12] Dmitri Jarnikov and Tanir Ozcelebi. Client Intelligence for Adaptive
Streaming Solutions. EURASIP Journal on Signal Processing: Image
Communication, Special Issue on Advances in IPTV Technologies,
26(7):378–389, August 2011.
[13] Junchen Jiang, Vyas Sekar, and Hui Zhang. Improving fairness,
efficiency, and stability in http-based adaptive video streaming with
festive. In Proceedings of the 8th international conference on Emerging
networking experiments and technologies (CoNEXT), 2012.
[14] F. P. Kelly, A. K. Maulloo, and D. K. H. Tan. Rate control for
communication networks: Shadow prices, proportional fairness and
stability. The Journal of the Operational Research Society, 49(3):237–
252, 1998.
[15] Chenghao Liu, Imed Bouazizi, and Moncef Gabbouj. Rate Adaptation
for Adaptiv HTTP streaming. In Proc. ACM Multimedia Systems
Conference (MMSys’11), pages 169–174, San Jose, CA, USA, February
2011.
[16] Chenghao Liu, Imed Bouazizi, Miska M. Hannuksela, and Moncef
Gabbouj. Rate adaptation for dynamic adaptive streaming over HTTP in
content distribution network. Signal Processing: Image Communication,
27(4):288–311, April 2012.
[17] Konstantin Miller, Emanuele Quacchio, Gianluca Gennari, and Adam
Wolisz. Adaptation algorithm for adaptive streaming over http. In
Proceedings of 2012 IEEE 19th International Packet Video Workshop,
pages 173–178, 2012.
[18] R.K.P. Mok, E.W.W. Chan, X. Luo, and R.K.C. Chang. Inferring the
QoE of HTTP video streaming from user-viewing activities. In Proc.
SIGCOMM W-MUST, 2011.
[19] Guibin Tian and Yong Liu. Towards agile and smooth video adaptation in dynamic http streaming. In Proceedings of the 8th international conference on Emerging networking experiments and technologies
(CoNEXT), pages 109–120, 2012.
[20] Chao Zhou, Xinggong Zhang, Longshe Huo, and Zongming Guo. A
control-theoretic approach to rate adaptation for dynamic http streaming.
In Proceedings of Visual Communications and Image Processing (VCIP),
pages 1–6, 2012.

A PPENDIX A
BANDWIDTH C LIFF E FFECT: T HEORETICAL A NALYSIS
A. Problem Formulation
Consider that K clients share a bottleneck link of capacity
C. The streaming process of each client consists of discrete
downloading steps n = 1, 2, ..., where during each step one
video segment is downloaded.
Fixed requested video bitrate. As we are interested in how
the measured TCP throughput causes HAS clients to shift their
video bitrates requested, we analyze the stage of the dynamics
where the rate shift has not occurred yet. Thus, in our model,
we assume that each HAS client does not change its requested
video bitrate over the time interval of analysis. For k = 1, ...K,
each k-th client requests a video segment of fixed size rk · τ
at each downloading step, where rk ∈ R is the video bitrate
selected.
Segment requesting time and downloading duration. Denote
by Rk (t) the instantaneous data downloading rate of the k-th
client at time t (note that Rk and rk are different). Denote by
tk [n] the time that the k-th client requests its n-th segment
(which, for simplicity, is also assumed to be the time that
the downloading starts). For each client, assume that the
requesting time of the first segment tk [1] is given. For n ≥ 1,
the requesting time of the next segment is determined by:
tk [n + 1] = tk [n] + max(τ, T˜k [n]),

(17)

where T˜k [n] is the actual duration of the k-th client downloading its n-th segment. This is a reasonable assumption where the
buffer level of a HAS client has reached the maximum level.
The actual duration of downloading, T˜k [n], can be related to
rk by
ˆ tk [n]+T˜k [n]
Rk (t) · dt = rk · τ.
(18)
t=tk [n]

The TCP throughput measured by the k-th client for down·τ
. In a
loading the n-th segment, is defined as x̃k [n] := Tr˜k[n]
k
conventional HAS algorithm, the TCP throughput is used as
an estimator of a client’s fair-share bandwidth, which, ideally,
C
.
is equal to K
Idealized TCP behavior. We assume that the network obeys
a simplified bandwidth sharing model, where we do not
consider the effects such as TCP slow-start restart and heterogenous RTTs.9 At any moment t ≥ 0 when there are
A(t) ≥ 1 active TCP flows sharing the bottleneck link, each
C
active flow will receive a fair-share data rate of Rk (t) = A(t)
instantaneously, and the total traffic rate in the link is C; at
any moment when there is no active TCP flows, or A(t) = 0,
the total traffic rate in the link is 0. For this case, we have the
following definition:
Definition 1. A gap is an interval within which the total traffic
rate of all clients is 0.
9 It

is trivial to extend the analysis to the case of heterogenous RTTs.

Client initial state. We assume that each client may have
some arbitrary initial state, including:

Lemma 4. Within the interval [t+ , t+ + τ ), each client must
request one and only one segment.

Time of requesting the first segment tk [1], k = 1, ..., K,
as forementioned.
Initial data downloading rate, i.e., it may be that Rk (t) >
0 for t < tk [1], k = 1, ..., K, where the rate may be due to
requesting a segment earlier than the first segment being
considered. In practice, this may correspond to the cases
where the clients have already started downloading but
may be in a different state, before the first segment being
considered (e.g., link is oversubscribed before it becomes
undersubscribed).

Proof: First, we show that each client must request at least
one segment. Invoking Lemma 2 and the fact that [t− , t+ ) is
a gap, the request times immediately preceding and following
[t− , t+ ) must be spaced exactly by τ . This can never hold if
no segment is requested within [t+ , t+ + τ ).
Second, we show that each client can request at most one
segment within [t+ , t+ +τ ). This directly follows applying (17)
to any interval of duration τ , similar to the proof of Lemma
3.
Since exactly one segment is requested by each client within
[t+ , t+ + τ ), for convenience, let us re-label these segments
using a new index n0 .

•
•

B. Link Undersubscription
The link is undersubscribed by the K HAS clients if the
sum of the requested
video bitrates is less than the link
PK
r
< C. We would like to show that
capacity, i.e.,
k
k=1
even the slightest undersubscription of the link would lead
to convergence into a state where each client has a TCP
C
.
throughput greater than its fair-share bandwidth K
To begin with, we prove a set of lemmas. We first show
that any two adjacent requesting times tk [n] and tk [n + 1] are
spaced by exactly τ if there exists a gap between them.
Lemma 2. tk [n + 1] = tk [n] + τ if there exists a gap [t− , t+ )
with tk [n] ≤ t− and t+ ≤ tk [n + 1].
Proof: By (17), the only case that tk [n + 1] 6= tk [n] + τ is
when T˜k [n] > τ . But this cannot hold, since otherwise there
cannot be a gap [t− , t+ ) with tk [n] ≤ t− and t+ ≤ tk [n + 1].
The
PKrest of the lemmas in this section make the assumption
of k=1 rk < C. First, we show that at least one gap must
emerge after some time.
Lemma 3. There exists a gap [t− , t+ ), where t−
maxk tk [1].

>

Proof: By (17), within [t, t + τ ) for any t, each client can
download at most one segment, or data of size at most rk · τ .
Consider within an interval [maxk tk[1] , maxk tk[1] + m · τ )
for some m ∈ N . The maximum size of the data
that can
PK
be downloaded by the K clients is Bres + m · k=1 rk τ ,
where Bres is the total size of the residue data from segments
requested prior to maxk tk [1], including those prior to the
first segments being considered, as discussed in Section A-A,
client initial state. By the idealized TCP behavior, at any
instant, the total traffic rate can be either C or 0. If zero
total traffic rate does not happen, the total downloaded data
size within the interval being considerd is mCτ . Therefore,
a sufficient condition for a gap [t− , t+ ) to occur P
is to have
K
t− > max
t
+
m
·
τ
,
where
mCτ
>
B
+
m
·
k
res
k=1 rk τ ,
l k[1]
m
PK
or m = Bres /τ · (C − k=1 rk ) .
The next lemma shows that within an interval of duration τ
immediately following this gap, each client must request one
and only one segment.

Lemma 5. [t− + τ, t+ + τ ) is a gap.
Proof: First, invoking Lemma 2 and the fact that [t− , t+ )
is a gap, we have tk [n0 − 1] = tk [n0 ] − τ for k = 1, ..., K. In
other words, the starting time patterns within intervals [t+ −
τ, t+ ) and [t+ , t+ + τ ) exactly repeat.
Second, consider the data traffic within [t+ − τ, t+ ) and
+ +
[t , t + τ ). The only difference is that within [t+ − τ, t+ ),
there might be unfinished residue data from the previous
segments whereas within [t+ , t+ + τ ), there is no such
unfinished residue data due to the gap [t− , t+ ). By (18),
the exact starting times and the idealized TCP behavior, the
downloading completion time can only get delayed with the
extra residue data, thus we must have T̃k [n0 − 1] ≥ T̃k [n0 ],
k = 1, ..., K. Therefore, the data traffic within [t+ , t+ + τ )
must finish no later than t− + τ , and [t− + τ, t+ + τ ) must
also be a gap.
The following theorem shows that in the case of link
undersubscription, regardless of the client initial states, the
banwidth sharing among the clients will eventually converge
to a periodic pattern.
PK
Theorem 6. If
k=1 rk < C, the data downloading rate
Rk (t) of each client will converge to a periodic pattern with
period τ , i.e., there exists some t0 ≥ 0 such that for all t ≥ t0 ,
Rk (t + τ ) = Rk (t), k = 1, ..., K.
Proof: The interval [t+ , t+ + τ ) has no residue data
from the previous unfinished segments, because of the gap
[t− , t+ ). Using this fact and the idealized TCP behavior, the
data downloading rates Rk (t), k = 1, ..., K for t ∈ [t+ , t+ +τ )
is a deterministic function of the starting times tk [n0 ], k =
1, ..., K. The same argument applies to Rk (t), k = 1, ..., K
for t ∈ [t+ + τ, t+ + 2τ ) and tk [n0 + 1], k = 1, ..., K.
By Lemma 2 and Lemma 5, we have tk [n0 + 1] = tk [n0 ] + τ
for k = 1, ..., K. In other words, other than the offset τ , the
starting time patterns within [t+ , t+ + τ ) and [t+ + τ, t+ + 2τ )
are the same. Invoking the deterministic function argument, we
can show Rk (t+τ ) = Rk (t), k = 1, ..., K for t ∈ [t+ , t+ +τ ).
Taking t0 = t+ and repeatly applying the argument above to
the intervals [t+ + mτ, t+ + (m + 1)τ ), m = 2, 3, ... complete
the proof.

Note that by the idealized TCP behavior, no client’s TCP
C
throughput would overestimate its fair-share bandwdith K
only if the number of active flows A(t) = K during all
the active time. After the bandwith sharing pattern converges
to periodic, this happens only when the starting times of
all clients are all aligned and their segment data size are
equal. The following corollary is an immediate consequence
of Theorem 6:
PK
Corollary 7. If k=1 rk < C, and tk [n0 ] 6= tl [n0 ] for some
k 6= l, then for some clients the TCP throughput will converge
to a value that is greater than its fair-share bandwdith, i.e.,
C
for n ≥ n0 . In particular, if
there exists k 0 with x̃k0 [n] > K
C
r1 = r2 = ... = rK , then for all k 0 = 1, ..., K, x̃k0 [n] > K
for n ≥ n0 .
Note that the start times pattern tk [n0 ], k = 1, ..., K will dictate exactly by how much the TCP throughput overestimates
k [n]
the fair-share bandwidth, with the range x̃C/K
∈ [1, K].
C. Link Oversubscription
We next consider the caseP
that the link is oversubscribed
K
by the K HAS clients, i.e., k=1 rk > C. We first show a
sufficient condition under which the TCP throughput would
correctly predict the fair-share bandwidth.
C
Theorem 8. If rk > K
for all k = 1, ..., K, all clients’ TCP
throughput converges to the fair-share bandwidth, i.e., there
C
exists n0 > 0 such that x̃k [n] = K
, k = 1, ..., K for n ≥ n0 .

Proof: We first want to show that there exists n0 > 0 such
that T̃k [n] > τ for all k = 1, ..., K for n ≥ n0 . Assume that the
opposite is true, i.e., there exists at least a client k 0 such that,
C
T̃k0 [n] ≤ τ for all n ≥ n0 . Since rk0 > K
, this would hold only
if within the active intervals of client k 0 , at least another client
+
k 00 must be inactive. Denote by [t−
k00 , tk00 ) the first such inactive
−
interval. Consider within an interval [t−
k00 , tk00 +m·τ ) for some
m ∈ N . By the idealistic TCP behavior, the total number of
bits that can be downloaded by the K − 1 clients (other than
k 0 ) must be ≤ (K − 1)/K · mτ C. This contradicts the fact
that T̃k [n] > τ , for k 6= k 0 , n ≥ n0 . Thus, the assumption is
invalid.
Then, by (17), for n ≥ n0 , all clients are active all the
C
time. By the idealistic TCP behavior, we have x̃k [n] = K
,
0
k = 1, ..., K for n ≥ n .
By slightly extending the above argument, a sufficient
and necessary condition for the correct fair-share bandwidth
estimation of all clients can be found.
PK
Theorem 9. If
k=1 rk > C, all clients’ TCP throughput
C
converges to the fair-share bandwidth, if and only if rk ≥ K
C
0
for all k = 1, ..., K and there exists k such that rk0 > K .
A PPENDIX B
A NALYSIS OF PANDA
We perform an equilibrium and stability analysis of
PANDA. For simplicity, we only analyze the single-client case
where the system has an equilibrium point.

A. Analysis of x̂
Assume the measured TCP download throughput x̃ is equal
to the link capacity C. Consequently, (7) can be re-written in
two scenarios (refer to Figure 3) as:
(
κ · w,
if x̂ < C,
x̂[n] − x̂[n − 1]
=
T [n − 1]
κ · w − κ · (x̂[n − 1] − C), otherwise.
(19)
At equilibrium, setting (19) to zero leads to w = x̂o − C,
hence, x̂o = C + w > x̃o = C.
Considering the simplifying assumptions of ŷ = x̂ (no
smoothing) and a quantizer with a margin ∆ such that
r = ŷ − ∆, we need ro = x̂o − ∆ < C at equilibrium.
Therefore, the quantization margin of the quantizer needs to
satisfy
∆ > x̂o − C = w,
so that the system stays on the multiplicative-decrease side of
the equation at steady state.
Note that close to equilibrium, the intervals between consecutive segment downloads match the segment playout duration
T [n−1] = τ , therefore, calculation of the estimated bandwidth
x̂ follows the simple form of a difference equation:
x̂[n] = a · x̂[n − 1] + b.
The two constants are: a = 1 − κ · τ and b = κ · (C + w) · τ .
The sequence converges if and only if |a| < 1. Hence, we
need: −1 < 1 − κ · τ < 1, leading to the stability criterion:
0<k<

2
.
τ

B. Analysis of T̂ and B
Assume no smoothing x̂ = ŷ. During transient states, when
T = T̂ > T̃ , update of T̂ pushes the playout buffer size in the
right direction: T̂ < r[n]·τ
x̂[n] leads to a steady growth in buffer
size when B < Bmin . At steady state, T̂o = τ = T̂o > T̃o . It
can be derived from (10) that:


ro
τ
Bo = Bmin + 1 −
·
x̂o
β
where Bo is playout buffer at equilibrium.

APPECT: An Approximate Backbone-Based Clustering
Algorithm for Tags
Yu Zong1,2, Guandong Xu3, Ping Jin1,*, Yanchun Zhang3,
4
EnHong Chen2, and Rong Pan
1

Department of Information and Engineering, West Anhui University, Luan, 237012, China
Department of Computer Science and Technology, University of Science and Technology,
Hefei, 230036, China
3
Center for Applied Informatics, Victoria University, PO Box 14428, VIC 8001, Australia
4
Department of Computer Science, Aalborg University, Denmark
Nick.zongy@gmail.com, jinping@wxc.edu.cn

2

Abstract. In social annotation systems, users label digital resources by using
tags which are freely chosen textual descriptions. Tags are used to index, annotate and retrieve resource as an additional metadata of resource. Poor retrieval
performance remains a major problem of most social tagging systems resulting
from the severe difficulty of ambiguity, redundancy and less semantic nature of
tags. Clustering method is a useful tool to address the aforementioned difficulties. Most of the researches on tag clustering are directly using traditional clustering algorithms such as K-means or Hierarchical Agglomerative Clustering on
tagging data, which possess the inherent drawbacks, such as the sensitivity of
initialization. In this paper, we instead make use of the approximate backbone
of tag clustering results to find out better tag clusters. In particular, we propose
an APProximate backbonE-based Clustering algorithm for Tags (APPECT).
The main steps of APPECT are: (1) we execute the K-means algorithm on a tag
similarity matrix for M times and collect a set of tag clustering results
Z={C1,C2,…,Cm}; (2) we form the approximate backbone of Z by executing a
greedy search; (3) we fix the approximate backbone as the initial tag clustering
result and then assign the rest tags into the corresponding clusters based on the
similarity. Experimental results on three real world datasets namely MedWorm,
MovieLens and Dmoz demonstrate the effectiveness and the superiority of the
proposed method against the traditional approaches.
Keywords: Approximate backbone, Tag clustering, Social annotation systems.

1

Introduction

With the development of Web2.0 application services, tag-based services, e.g.,
Del.icio.us 1, Last.fm2, and Flickr3, have undergone tremendous growth in the past
*

Corresponding author.
http://del.icio.us
2
http://www.last.fm
3
http://flickr.com
1

J. Tang et al. (Eds.): ADMA 2011, Part I, LNAI 7120, pp. 175–189, 2011.
© Springer-Verlag Berlin Heidelberg 2011

176

Y. Zong et al.

years. Tags are simple, uncontrolled and ad-hoc labels that are assigned by users to
describe or annotate any kind of resource [1]. The low technical barrier of social tagging systems and the easy use of tagging have attracted a large amount of research
interest. The user-contributed tags are not only an effective way to facilitate personal
organization but also provide a possibility for users to search for needed information.
Recently tagging has been widely used in social annotation systems for many applications [2-4]. The common usage of tags in these systems is to add the tagging
attribute as an additional feature to re-model users or resources over the tag vector
space, and in turn, making tag-expanded collaborative filtering recommendation or
personalized recommendation. However, as the tags are of syntactic nature, in a free
style and do not reflect sufficient semantics, the problems of redundancy, ambiguity
and less semantics of tags are often incurred in all kinds of social tagging systems.
For example, for one resource, different users will use their own words to describe
their feeling of likeness, such as “favorite, preference, like” or even the plural form of
“favorites”; and another obstacle is that not all users are willing to annotate the tags,
resulting in the severe problem of sparseness. In order to deal with these difficulties,
recently clustering methods have been introduced into social tagging systems to find
meaningful information conveyed by tag aggregates. The aim of tag clustering is to
reveal the coherence of tags from the perspective of how resources are annotated and
how users annotate in the tagging behaviors. Undoubtedly, the tag cluster form is able
to deliver user tagging interest or resource topic information in a more concise and
semantic way, which, in some extent, to handle the problems of tag sparseness and
redundancy, in turn, facilitating the tag-based recommender systems. Thus this demand mainly motivates the research of tag clustering in social annotation systems. In
general, the tag clustering algorithm could be described as to: (1) define a similarity
measure of tags and construct a tag similarity matrix; (2) execute a traditional clustering algorithm such as K-means [5], or Hierarchical Agglomerative Clustering [6] on
this similarity matrix to generate the clustering results; (3) abstract the meaningful
information from each cluster and do recommendation. Although the experimental
results have shown the success of these algorithms, the inherent drawbacks of the
traditional clustering algorithm, such as the sensitivity of initialization and high computational cost etc, are the main reason, which affects the performance of the proposed tag clustering algorithms.
In order to alleviate the inherent drawbacks of tag clustering algorithms, in this paper, we propose an APProximate backbonE-based Clustering algorithm for Tags
(APPECT). The motivation of our method is based on the phenomenon that the common part of several clustering results, derived by executing a traditional clustering
algorithm for several times, captures the most significant entities within a clustering
process from the perspective of optimization. Using this common part, particularly
coined Approximate Backbone is able to improve the clustering results [7]. In the
context of tag clustering, we especially extract the approximate backbone, i.e., the
core tags to form the initial tag clusters and conduct the tag clustering.
The main steps of APPECT are: we firstly define a similarity measure for tags by
considering the resource and user aspects simultaneously and construct a similarity
matrix SM; secondly, we execute a traditional clustering algorithm on SM for M times

APPECT: An Approximate Backbone-Based Clustering Algorithm for Tags

177

to obtain a collection of clustering results Z ={C1,C2,…,CM}; thirdly, we design an
algorithm, FIND_AB based on greedy search method to find out the approximate
backbone from Z; eventually, we fix the approximate backbone as the initial clustering results, and assign the rest tags to the most appropriate clusters based on the similarity. In order to evaluate the effectiveness of the proposed approach, we conduct
experiments on three real world tagging datasets. The contributions of our work are as
follows:
• we define a similarity measure for tags by considering the resource and users aspects simultaneously;
• we introduce the concept of approximate backbone for tags that can capture the
core tags to form various tag clusters from the multiple execution of traditional
clustering, and devise a greedy search algorithm to form the approximate backbone;
• we propose a new approximate backbone based clustering algorithm for tags, in
which we make use of the approximate backbone of tags;
• we conduct comparative experiments on three real world datasets to evaluate the
effectiveness of the proposed algorithm.
The remainder of this paper is organized as follows. We review the related work in
Section 2 and introduce the preliminaries in Section 3. The details of APPECT algorithm are discussed in Section 4. Experimental evaluation results are reported in Section 5. Section 6 concludes this paper and outlines the future work.

2

Related Work

In the past years, many studies have been carried out on tagging clustering. [5, 6]
demonstrated how tag clusters serving as coherent topics can aid in the social recommendation of search and navigation. Astrain et al. firstly combines a syntactic similarity measure based in a fuzzy automaton with ε -moves and a cosine relatedness
measure, and then design a clustering algorithm for tags to find out the short length
tags [8]. In general, tags lack organizational structure limiting their utility for navigation. Simpson proposes a hierarchical divisive clustering algorithm to release these
influence of the inherent drawback of tag data [9]. In [10], an approach that monitors
users’ activity in a tagging system and dynamically quantifies associations among
tags is presented and the associations are then used to create tags clusters. Zhou et al.
propose a novel method to compute the similarity between tag sets and use it as the
distance measure to cluster web documents into groups [11].
In [12] topic relevant partitions are created by clustering resources rather than tags.
By clustering resources, it improves recommendations by distinguishing between
alternative meanings of query. While in [13], clusters of resources are shown to improve recommendation by categorizing the resources into topic domains. A framework named Semantic Tag Clustering Search, which is able to cope with the syntactic
and semantic tag variations, is proposed in [14]. P. Lehwark et al. use Emergent-SelfOrganizing Maps (ESOM) and U-Map techniques to visualize and cluster tagged data

178

Y. Zong et al.

and discover emergent structures in collections of music [15]. State-of-the-art methods suffice for simple search, but they often fail to handle more complicated or
noisy Web page structures due to the key limitations. Miao et al. propose a new method for record extraction that captures a list of objects in a more robust way based on
a holistic analysis of a Web page [16]. In [17], a co-clustering approach is employed,
that exploits joint groups of related tags and social data resources, in which both social and semantic aspects of tags are considered simultaneously. The common characteristic of aforementioned tagging clustering algorithm is that they use a traditional
clustering algorithm on tag dataset to find out the similar tag groups. In [18], however, the authors introduce FolksEngine, a parametric search engine for folksonomies
allowing specifying any tagging clustering algorithm. In the similar way, Jiang et al,
make use of the concept of ensemble clustering to find out a consensus tag clustering
results of a given topic and propose tag groups with better quality [19].
The efficient way which improves tag clustering result is to use the common parts
of several tag clustering results. Approximate Backbone, the intersection of different
solutions of a dataset, is often used to investigate the characteristic of a dataset [2022]. Zong et al. use approximate backbone to deal with the initialization problem of
heuristic clustering algorithm [7]. In this paper, we firstly define approximate backbone to capture the common tags among the tag clustering results derived by executing a traditional clustering algorithm several times on a tag similarity matrix. And
then fix approximate backbone as the initial tag clustering result, and assign the rest
tags into the one with highest similarity.

3

Preliminaries

3.1

Social Tagging System Model

In this paper, our work is to deal with the tagging data. A typical social tagging system has three types of objects, users, tags and resources which are interrelated with
one another. Social tagging data can be viewed as a set of triples [23]. Each triple (u,
r, t) represents a user u annotates tag t to resource r. A social tagging system can
described as a four-tuple, where exists a set of users, U; a set of tags, T ; a set of resources, R; and a set of annotations, AN. We denote the data in the social tagging system as D and define it as D =< U,R,T,AN >. The annotations are represented as a set
of triples contains a user, tag and resource defined as: AN ⊆ <u, r, t> : u ∈ U, r ∈ R,
t ∈ T . Therefore a social tagging system can be viewed as a tripartite hyper-graph [24]
with users, tags and resources represented as nodes and the annotations represented as
hyper-edges connecting users, resources and tags.
3.2

Similarity Measure

In the framework of tag clustering, the tag similarity plays an important role. In real
applications, in order to compute the similarity, we usually start from the tripartite
graph of social annotations to compose a two-dimensional, e.g., the resource-tag

APPECT: An Approximate Backbone-Based Clustering Algorithm for Tags

179

matrix by accumulating the frequency of each tag along users. In this expression, each
tag is described by a set of resources, to which this tag has been assigned, i.e.,
t = ( wr , wr ,..., wr ) , where wr denotes the weight on resource dimension r of tag
t , which could be determined by the occurrence frequency. In this manner, the similarity between any two tags on resource is defined as Definition 1.
i

i1

i2

im

ik

k

i

Definition 1. Given two tags t = ( wr , wr ,..., wr ) and t = ( wr , wr ,..., wr ) , the simii

i1

i2

j

im

j1

j2

jm

larity is defined as the Cosine function of angle between two vectors t and t :
j

i

Simr (t , t ) =
i

j

t ⋅t
i

(1)

j

|| t || ⋅ || t ||
i

j

Definition 1 only considers the relationship between tags and resources, however the
relation between users and tags was neglected. Tags are freely used by users to reflect
their opinions or interests on various resources. If we simultaneously consider the
similarity of tags from both resource and user perspective, a complete relationship
between tags will be captured.
To realize this, we build another user-tag matrix from the tripartite graph by using
the similar approach, i.e., t = ( wu , wu ,..., wu ) , where wu denotes the weight on
user dimension u of tag t , which could be determined by the occurrence frequency. Definition 2 gives the similarity of tags on users.
i

k

i1

i2

in

ik

i

Definition 2. Given two tags t = ( wu , wu ,..., wu ) and t = ( wu , wu ,..., wu ) , the
i

i1

i2

j

in

j1

j2

jn

similarity is defined as the Cosine function of angle between two vectors t and t :
i

Simu (t , t ) =
i

j

t ⋅t
i

(2)

j

|| t || ⋅ || t ||
i

j

j

Furthermore, the similarity of tags is defined as the line combination of
Simr and Simu , as defined in Definition 3.
Definition 3. Given two tags t and t , Simr (t , t ) ,and Simu (t , t ) , the overall simii

larity between

i

i

j

i

j

ti and t j is defined as:

Sim(t , t ) = α ⋅ Simr (t , t ) + (1 − α ) ⋅ Simu (t , t )
i

j

i

j

i

j

(3)

where 0 ≤ α ≤ 1 is a tuning factor for Sim(t , t ) . If α = 0 , the similarity only accounts
i

j

the importance from the user aspect, otherwise, the contribution of resource aspect
dominates the similarity, i.e. when α = 1 . For whole tags, we calculate the similarity
between each tag pair according to Definition 1-3, and a similarity matrix SM is
constructed.
N×N

180

3.3

Y. Zong et al.

Approximate Backbone of Tag Clustering Results

Most tag clustering algorithms utilize traditional clustering algorithms, such as Kmeans, on the tag similarity matrix to conduct the tag clustering result C. As known,
clustering is an unsupervised learning process. One execution of the traditional clustering algorithm on tag set could be regard as one knowledge extraction process, and
the clustering result is the learning result of tag data. If we use the common information of several clustering results derived by running a traditional clustering algorithm
on tag dataset to initialize the clustering algorithm, the more robust clustering result
will be obtained. In this section, we introduce the concept of approximate backbone to
capture the common information from several tag clustering results. Assume that a
traditional clustering algorithm is run for M times on tag similarity matrix and a tag
clustering result collection Z = {C , C ,..., C } is generated, where, C , m = 1,..., M
denotes the tag clustering result of the m-th clustering, the common part of Z is defined as the approximate backbone of tag clustering results.
1

2

M

m

Definition 4. Given a collection of tag clustering results Z = {C , C ,..., C } , where
C = {C , C ,..., C } denotes the m -th clustering result, and C , k = 1,..., K denotes
the k -th cluster in the m -th clustering result. The approximate backbone of Z is
defined as AP( Z ) = {ap , ap ,..., ap } , where ap k = 1,..., K satisfies the following conditions:
(1) | ap |≥ 2 and (2) all elements in ap k = 1,..., K must be co-occurred among
Z for M times.
According to Definition 4, the common part of Z is captured.
1

m

m

1

m

m

m

2

K

k

1

2

K

k

4

2

M

k

k

The Details of APPECT

In this section, we first propose an algorithm with greedy search for capturing the
approximate backbone from the tag clustering collection Z, and then we discuss the
details of APPECT, at last, we analyses the time cost of APPECT.
4.1

Capture the Approximate Backbone

In order to capture the approximate backbone of tag clustering result collection Z , we
propose an algorithm named Find_AB (Find Approximate Backbone by using Set
intersection operation). We assume that each cluster C ⊂ Z is consisted by tag id,
e.g., C ={#2,#5,…,#338}. In the tag clustering result collection, there are C possible combinations of clusters. Each cluster C must be intersected with the other clusters in C , m ' = 1,..., M , m ≠ m ' , so the whole traversal search method is very time
consuming. In this section, we thus design the algorithm Find_AB via greedy search.
Algorithm 1 shows the main steps of Find_AB.
m

k

m

K

k

M

m

k

m'

APPECT: An Approximate Backbone-Based Clustering Algorithm for Tags

181

Algorithm 1. Find_AB
Input: tag clustering result collection Z
Output: approximate backbone of Z , AP ( Z ) = {ap , ap ,..., ap }
1

2

K

(1) AP ( Z ) ← ∅ ;
m

(2) Randomly select a baseline C from Z ;
(3) for k = 1,..., K
(3.1) intersect C

m
k

m'

with C in C

m'

k'

of Z ,where m ' = 1,..., M , m ≠ m ' and k ' = 1,..., K ;

(3.2) if | C  C |≥ 2 { ap = C  C
m

m'

k

k'

k

m

m'

k

k'

;

AP ( Z ) = AP ( Z )  ap ;}
k

(4) return AP( Z ) ;

Algorithm Find_AB shown in algorithm 1 gives the main steps of finding the
approximate backbone set AP( Z ) . The first step is to randomly select a baseline partition from Z . Each cluster of the baseline partition will be intersected with all the
clusters in the rest partitions, as shown in step 3, and the data objects co-occurring in
the same cluster are find out. The intersection of data objects in each cluster will be
regard as the AP( Z ) and then returned.
4.2

Details of APPECT

After the approximate backbone of tag clustering result collection Z is generated,
we fix them as the initial clustering result, that is, C ← AP( Z ) . We define the tags in
cluster C ⊂ C , k = 1,..., K as the core tags, because they are co-occurred in
M times clustering. And eventually, we assign the rest tags into the corresponding
clusters based on the highest similarity to the cluster centers.
The main steps of APPECT are shown in algorithm 2.
init

k

init

Algorithm 2. APPECT
Input: the tag similarity matrix, SM , executing times, M ;
Output: the tag clustering result, C ;
(1) generate tag clustering result collection, Z , by executing a traditional clustering
algorithm for M times;
(2) find the approximate backbone AP( Z ) from Z by running Find_AB;
(3) fix approximate backbone of Z
C ← AP( Z ) ;

as the initial clustering result, that is,

init

(4) Assign the rest tags into the corresponding clusters in C
similarity;
(5) return C ← C

init

;

init

based on the highest

182

Y. Zong et al.

Algorithm 2 consists of three main steps: firstly, we generate the tag clustering result collection Z by running a traditional clustering algorithm, such as, K-means, on
the similarity matrix SM. The main aim of this step is to prepare sufficient tag clustering result for finding the substantially co-occurred tags. Secondly, according to Definition 4, we find out the co-occurred tags which are conveyed by the approximate
backbone AP(Z). In this step, we use a greedy search method to capture the cooccurred tags by running the Find_AB algorithm. At last, we first fix the approximate
backbone AP(Z) as the initial clusters, and then assign the rest tags into the corresponding clusters with the highest similarity.
4.3

Time Analysis of APPECT

The first step of APPECT executes a traditional clustering algorithm for M times to
generate the tag clustering result collection Z . Assume that the running time of a traditional clustering algorithm is O (⋅) , it needs M × O(⋅) time cost for the M times
running. In the second step of APPECT, the approximate backbone of Z is derived
by executing Find_AB and its time cost is O( MK ) . At last, we assign the rest tags
into corresponding clusters with the highest similarity and the time cost at most is
O ( NK ) .
According to the above discussion, the total time cost of APPECT is
M × O(⋅) + O( MK ) + O( NK ) , where M is the number of running time, N is the number
of tags, K is the number of clusters and K << N , M << N . The last two parts of the
time cost have the linear relationship with M , N and K , so the main time cost of
APPECT depends on the first part, i.e., M × O(⋅) . In real experiments, we execute the
K-means algorithm to generate Z. In this situation, the time cost of APPECT is
M × O(tNK ) + O( MK ) + O( NK ) , where t << N is the iterative number of K-means.

5

Experiments and Evaluations

To evaluate our approach, we conducted experiments on three real world datasets:
MedWorm4, MovieLens5 and Dmoz6. We performed the experiments using an Intel
Core 2 Duo CPU (2.4GHz) workstation with 4G memory, running Windows XP. All
algorithms were implemented in Matlab 7.0.
5.1

Data Sets

The first dataset is extracted from the crawled MedWorm article repository during
April 2010. After stemming out the entity attributes from the data, four data files,
namely user, resource, tags and quads, are obtained as the source datasets. Here we
only use the fourth data, which presents the social annotations where each row
4
5
6

http://www.medworm.com
http://www.movielens.org
http://www.michael-noll.com/dmoz100k06/

APPECT: An Approximate Backbone-Based Clustering Algorithm for Tags

183

denotes a user u annotates resource r by using tag t. The second dataset is MovieLens
which is provided by GroupLens. This dataset consists of three files - movies.dat,
rating.dat and tags.dat. The tags.dat has the same format as the quads in MedWorm
dataset, which we utilize to conduct the experiments. The last dataset dmoz100k06 is
a large research dataset about document metadata based on a random sample of
100,000 web documents from the Open Directory combined with data retrieved from
Delicious.com/ Yahoo!, Google, and ICRA. For our study, we use the tagging data
available at del.icio.us, one of the most popular social bookmarking services. The data
from del.icio.us was collected over a period of three weeks in December 2006. This
dataset contains 13,771 documents with 25,311 tags by 5,016 users. The statistical
results of these three datasets are listed in Table 1. These three datasets are preprocessed to filter out some noisy and extremely sparse data subjects to increase the
data quality. Lin [25] has indicated the fact that the distributions of tags, resources
and users are subject to power distribution. This observation is justified by the statistical results of these three datasets shown in Table 1.
Table 1. Statistics of Experimental Datasets
Property
Number of users
Number of resources
Number of tags
Total entries
Average tags per user
Average tags per resource

5.2

MedWorm
949
261,501
13,507
1,571,080
132
5

MovieLens
4,009
7,601
16,529
95,580
11
9

Dmoz
5,016
13,771
25,311
97,587
123
11

Evaluation Metrics

The aim of tag clustering is to assign the tags serving for the similar function into the
same cluster. Here we assume that a better tag cluster is composed of similar tags,
which are dissimilar to tags belonging to other different tag clusters. In particular, we
use Similarity and Dissimilarity to validate our method.
Definition 5. Given the tag clustering result C = {C1 , C2 ,..., CK } , its similarity is defined as
SimiM (C ) =

1 K 2 ⋅ Sim(ti , t j )
, ti , t j ∈ Ck

K k =1 | Ck | ⋅(| Ck | −1)

(4)

Definition 6. Given the tag clustering result C = {C1 , C2 ,..., CK } , its dissimilarity is
defined as
DissimiM (C ) =

1 K
Dism(k )

K k =1 | Ck | ⋅(| T | − | Ck |)

(5)

where Dism(k ) =  k '=1 Sim(ti , t j ), ti ∈ Ck , t j ∈ Ck ' , k ≠ k ' , and | T | is the total number of
K

tags.

184

Y. Zong et al.

According to the requirement of tag clustering, it is obvious that the higher Similarity value and smaller Dissimilarity value indicate better tag clustering results.
5.3

Experimental Results and Discussions

The first step of APPECT is to generate a tag clustering result collection by running a
traditional clustering algorithm on the similarity matrix for M times. The number of M
will have a direct impact on the quality of tag clustering results. In order to reveal this
relationship, we conduct experiments on three datasets shown in Table 1 and the experimental results are shown in Fig. 1(a), (b). From the SimiM(C) plot in Fig. 1(a), we
can find that the SimiM(C) plots of three datasets significantly change when M increases from 2 to 5 with step 1, while these plots gradually stabilize with M varying
from 6 to 10. Likewise, in Fig. 1(b), the changes of DissimiM(C) of these three datasets exhibit the similar varying trends with SimiM(C), that is, three curves are dramatically descending with M increasing from 2 to 5, and they change slightly after M>5.
The change trends of SimiM(C) and DissimiM(C) have shown that the quality of tag
clustering result does not heavily rely on the running times of K-means algorithm
after an appropriate execution number is reached. In the other words, when the approximate backbone of Z has captured enough common information of tag clusters,
the capability of approximate backbone on improving the tag clustering becomes
diminished. According to the above discussion, we should select a proper M value
which not only guarantees the quality of tag clustering result, but also, optimizes the
time cost of APPECT. In the rest experiment, we set M as 5 based on the results of
Fig. 1.

(a) SimiM(C)

(b) DisimiM(C)

Fig. 1. The SimiM(C) and DisimiM(C) changes with the increase of M

In order to illustrate the relationship between α and the clustering results, i.e., SimiM(C) and DissimiM(C), we conduct experiments on three real world data sets under α changing from 0 to 1 by step 0.1. The clustering results are shown in Fig. 2.
From Fig. 2(a), we can find that the SimiM(C) values of three datasets are reaching
higher when α changing between [0.4 0.6] than in other ranges. And the same phenomenon on DisimiM(C) is observed in Fig. 2(b). This observation implies that the
user and resource feature contribute almost evenly to the similarity measure, i.e., α
varying in the range of [0.4, 0.6]. According to the experimental result, in this paper,

APPECT: An Approximate Backbone-Based Clustering Algorithm for Tags

(a) SimiM(C)

185

(b) DisimiM(C)

Fig. 2. The SimiM(C) and DisimiM(C) changes with the tuning parameter of α

we set α =0.5. Our proposed tag clustering algorithm aims to capture the tag groups
with higher SimiM(C) value and lower DissimiM(C) value. We conduct the comparative studies of K-means, Single-link [26] and APPECT on three real world datasets in
terms of SimiM(C) and DissimiM(C). Since we use a combined similarity measure
which takes the resource aspect and user aspect to define the similarity of tags, we
also carry out comparisons of three clustering algorithms on different similarity
measure matrices, e.g., K-means(Simr) denotes that we run K-means algorithm on the
similarity matrix derived by Definition 1, and K-means(Sim) indicates that we execute K-means on the similarity matrix generated by using Definition 3, and so on.
For each tag dataset, we first execute the K-means algorithm on two different similarity measures for M=5 times to obtain the tag clustering result collection, Z; and
then, we calculate the SimiM(C) and DissimiM(C) according to Define 4 and 5, respectively and the tag clustering result with the highest SimiM(C) and the lowest DissimiM(C) value in Z is saved as the tag clustering result of K-means; at last, we form
the approximate backbone of Z, and use it to find the tag clustering result of APPECT.
For single-link clustering, we execute it on two different similarity matrix of three tag
dataset to generate the corresponding K clusters and then calculate the clustering result quality in terms of SimiM(C) and DissimiM(C). The experimental results are
shown in Fig. 3.

(a) SimiM(C)

(b) DisimiM(C)

Fig. 3. The comparison of SimiM(C) and DissimiM(C) of three clustering algorithms with
different tag similarity measures on three datasets

186

Y. Zong et al.

For each clustering algorithm, we see that the quality of tag clustering results using
Simr is consistently lower than those of Sim, i.e., the SimiM(C) values using Simr are
smaller than those using Sim for each compared clustering algorithm, while the DissimiM(C) values are larger than those of Sim. The reason to this is because that the
combined similarity measure is able to better capture the overall similarity for the tag
clustering. Furthermore, the tag clustering result of APPECT on the three datasets are
better than those of K-means and Single-link. As such we can conclude that the combined similarity measure benefits the similarity computation and the common core
tags captured by the approximate backbone have a significant effort on improving the
clustering quality of tags.
Fig. 4(a) and (b) give the improvement comparisons of SimiM(C) and DissimiM(C), respectively. We can find from Fig. 4(a) that the SimiM(C) of APPECT has
increased by at least 9.7% in comparison to those of K-means on three datasets. In
particular, the improvement on MoiveLens dataset is most significant. Similarly, the
SimiM(C) of APPECT has an almost 10% increase against those of Single-link. Fig.
4(b) shows the improvements of DissimiM(C) of APPECT –K-means and APPECTSingle-link. The results shown in Fig. 4 reveal that the total quality of APPECT has a
nearly 35% overall (considering both the improvement of SimiM(C) and DissimiM(C)) improvement than those of K-means and Single-link on three datasets.

(a) SimiM(C)

(b) DisimiM(C)

Fig. 4. The improvement comparisons of SimiM and DissimiM between APPECT and Kmeans, Single-link on three datasets

In order to evaluate the efficiency of three compared clustering algorithms, we
compared the CPU time of them, and the results are shown in Fig. 5. According to the
CPU time lines in Fig. 5, the K-means algorithm has the lowest time cost, which depends on the tag set size. For our algorithm, we first need generate Z by running Kmeans for M times and then find out the approximate backbone. So the time cost of
APPECT is nearly M times than K-means. There have two ways to deal with the time
cost of APPECT: (1) If we have the tag clustering result Z archived in offline stage,
then APPECT could run more faster; (2) using the parallel computation method is
another option for reducing the time cost of APPECT. The time cost of single-link
algorithm is O ( N ) , so it is the most computational expensive one.
2

APPECT: An Approximate Backbone-Based Clustering Algorithm for Tags

187

Fig. 5. Comparisons of CPU time of three clustering algorithm on three datasets

6

Conclusion and Future Work

Under social tagging systems, a typical Web2.0 application, users label digital data
sources by using tags which are freely chosen textual descriptions. Tags are used to
index, annotate and retrieve resource as an additional metadata of resource. The poor
retrieval performance remains a major problem of most social tagging systems resulting from the severe difficulty of ambiguity, redundancy and less semantic nature of
tags. Clustering is a useful tool to deal with these difficulties. Inspired by the common
information of tag clustering results collection Z derived by running a traditional clustering algorithm for several times has the potential in improving the quality clustering
results, we propose a new clustering algorithm for tags, named APPECT. We first
define the approximate backbone to describe the common information of Z; and then,
devise an algorithm, named Find_AB based on the greedy search to capture the approximate backbone from Z to form the initial clusters; at last, the complete tag clusters are built up by assigning the rest tags into the corresponding initial clusters, based
on the highest similarity. Experimental results on three real world datasets have
shown that APPECT has the superiority in improving the quality of tag clustering.
Future work could focus on reducing the time of cost of the proposed algorithm.
Acknowledgements.This work has been supported by the Nature Science Foundation
of china under the Grant No 60775037and 60933013. the Ph.D. Programs Foundation
of Ministry of Education of China under Grant No. 20093402110017. the Nature
Science Foundation of Anhui Education Department under Grant No. KJ2009A54,
KJ2011Z321.

References
[1] Zong, Y., Xu, G.D., Jin, P., et al.: A local information passing clustering algorithm for
tagging systems. In: The Second Workshop on Social Networks and Social Media Mining
on the Web, Hong Kong, pp. 333–343 (2011)

188

Y. Zong et al.

[2] Durao, F., Dolog, P.: Extending a hybrid tag-based recommender system with personalization. In: SAC 2010: Proceedings of the 2010 ACM Symposium on Applied Computing,
pp. 1723–1727. ACM, New York (2010)
[3] Jäschke, R., Marinho, L., Hotho, A., Schmidt-Thieme, L., Stumme, G.: Tag Recommendations in Folksonomies. In: Kok, J.N., Koronacki, J., Lopez de Mantaras, R., Matwin, S.,
Mladenič, D., Skowron, A. (eds.) PKDD 2007. LNCS (LNAI), vol. 4702, pp. 506–514.
Springer, Heidelberg (2007)
[4] Tso-Sutter, K.H.L., Marinho, L.B., Schmidt-Thieme, L.: Tag-aware recommender systems by fusion of collaborative filtering algorithms. In: SAC 2008: Proceedings of the
2008 ACM Symposium on Applied Computing, pp. 1995–1999. ACM, New York (2008)
[5] Gemmell, J., Shepitsen, A., Mobasher, M., Burke, R.: Personalization in folksonomies
based on tag clustering. In: Proceedings of the 6th Workshop on Intelligent Techniques
for Web Personalization and Recommender Systems (July 2008)
[6] Shepitsen, A., Gemmell, J., Mobasher, B., Burke, R.: Personalized recommendation in
social tagging systems using hierarchical clustering. In: Proceedings of the 2008 ACM
Conference on Recommender Systems, pp. 259–266. ACM (2008)
[7] Zong, Y., Jiang, H., Li, M.C.: Approximate backbone guided reduction clustering algorithm. Journal of Electronics and Information Technology 31(2), 2953–2957 (2009)
[8] Astrain, J.J., Echarte, F., Córdoba, A., Villadangos, J.: A Tag Clustering Method to Deal
with Syntactic Variations on Collaborative Social Networks. In: Gaedke, M., Grossniklaus, M., Díaz, O. (eds.) ICWE 2009. LNCS, vol. 5648, pp. 434–441. Springer, Heidelberg (2009)
[9] Simpson, E.: Clustering tags in enterprise and web folksonomies. HP Labs Technical Reports, citeulike: 2545406 (2008)
[10] Boratto, L., Carta, S., Vargiu, E.: RATC: A Robust Automated Tag Clustering Technique. In: Di Noia, T., Buccafurri, F. (eds.) EC-Web 2009. LNCS, vol. 5692, pp. 324–
335. Springer, Heidelberg (2009)
[11] Zhou, J.L., Nie, X.J., Qin, L.J., et al.: Web clustering based on tag set similarity. Journal
of Computers 6(1), 59–66 (2011)
[12] Matteo, N.R., Peroni, S., Tamburini, F., et al.: A parametric architecture for tags clustering in folksonomic search engines. In 9th international Conference on Intelligent Systems
Design and Applications, Pisa, Italy, pp. 279–282 (2009)
[13] Chen, H., Dumais, S.: Bringing order to the web: Automatically categorizing search results. In: Proceedings of the SIGCHI Conference on Human Factors in Computting Systems, pp. 145–152. ACM (2000)
[14] van Dam, J., Vandic, D., Hogenboom, F., Frasincar, F.: Searching and browsing tagspaces using the semantic tag clustering search framework. In: 2010 IEEE Fourth International
Conference on Semantic Computing (ICSC), pp. 436–439. IEEE (2010)
[15] Lehwark, P., Risi, S., Ultsch, A.: Visualization and clustering of tagged music data. Data
Analysis, Machine Learning and Applications, 673–680 (2008)
[16] Miao, G., Tatemura, J., Hsiung, W., Sawires, A., Moser, L.: Extracting data records from
the web using tag path clustering. In: Proceedings of the 18th International Conference on
World Wide Web, pp. 981–990. ACM (2009)
[17] Giannakidou, E., Koutsonikola, V., Vakali, A., Kompatsiaris, Y.: Co-clustering tags and
social data sources. In: The Ninth International Conference on Web-Age Information
Management, pp. 317–324. IEEE (2008)
[18] Nicola, R.D., Silvio, P., Fabio, T., et al.: Of mice and terms: Clustering algorithms on
ambiguous terms in folksonomies. In: Proceeding of the 2010 ACM Symposium on Applied Computing SAC 2010, pp. 844–848 (2010)

APPECT: An Approximate Backbone-Based Clustering Algorithm for Tags

189

[19] Jiang, Y.X., Tang, C.J., Xu, K.K., et al.: Core-tag clustering for web2.0 based on multisimilarity measurements. In: The Joint International Conference on Asia-Pacific Web
Conference (APWeb) and Web-Age Information Management (WAIM), Suzhou, China,
pp. 222–233 (2009)
[20] Zou, P., ZHou, Z.H., Chen, G.L.: Approximate backbone guided fast ant algorithm to
QAP. Journal of Software 16(10), 1691–1698 (2005)
[21] Jiang, H., Zhang, X.C., Chen, G.L.: Exclusive overall optimal solution of graph bipartition problem and backbone compute complexity. Chinese Science Bulletin 52(17), 2077–
2081 (2007)
[22] Jiang, H., Zhang, X.C., Chen, G.L.: Backbone analysis and algorithm design of QAP.
Chinese Science 38(01), 1–14 (2008)
[23] Guan, Z., Wang, C., Bu, J., Chen, C., Yang, K., Cai, D., He, X.: Document recommendation in social tagging services. In: Proceedings of the 19th International Conference on
World Wide Web, pp. 391–400. ACM (2010)
[24] Mika, P.: Ontologies Are US: A Unified Model of Social Networks and Semantics. In:
Gil, Y., Motta, E., Benjamins, V.R., Musen, M.A. (eds.) ISWC 2005. LNCS, vol. 3729,
pp. 522–536. Springer, Heidelberg (2005)
[25] Lin, X., Guo, L., Zhao, Y.E.: Tag-bsed social interest discovery. In: Proceeding of the
17th International World Wide Web Conference (2008)
[26] Sibson, R.: SLINK: An optimally efficient algorithm for single-link cluster method.
Computer Journal 16(1), 30–34 (1973)

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE INFOCOM 2010 proceedings
This paper was presented as part of the Mini-Conference at IEEE INFOCOM 2010

Layered Internet Video Engineering (LIVE):
Network-Assisted Bandwidth Sharing and Transient Loss
Protection for Scalable Video Streaming
Xiaoqing Zhu, Rong Pan, Nandita Dukkipati, Vijay Subramanian and Flavio Bonomi
Advanced Architecture & Research Group, Cisco Systems Inc., San Jose, CA 95134, U.S.A.
{xiaoqzhu, ropan, ndukkipa, vijaynsu, flavio}@cisco.com

Abstract—This paper presents a novel scheme, Layered Internet Video
Engineering (LIVE), in which network nodes feed back virtual congestion
levels to video senders to assist both media-aware bandwidth sharing and
transient loss protection. The video senders respond to such feedback
by adapting the rates of encoded H.264/SVC streams based on their
respective video rate-distortion (R-D) characteristics. The same feedback
is employed to calculate the amount of forward error correction (FEC)
protection for combating transient losses. Simulation studies show that
LIVE can minimize the total distortion of all participating video streams
and hence maximize their overall quality. At steady state, video streams
experience no queueing delays or packet losses. In face of transient
congestion, the network-assisted adaptive FEC scheme effectively protects
video packets from losses while minimizing overhead. Our theoretical
analysis further guarantees system stability for an arbitrary number of
streams with heterogenous round trip delays below a prescribed limit.
Finally, we show that LIVE streams can coexist with TCP flows within
the existing explicit congestion notification (ECN) framework.
Index Terms—scalable video streaming, media-aware bandwidth
sharing, forward error correction (FEC), explicit congestion notification (ECN)

I. I NTRODUCTION
Recent years have seen a rapid growth of video traffic over the
Internet. According to [1], Internet video is now approximately onethird of all consumer Internet traffic, and will account for over 60
percent by year 2013. In addition to the sheer volume increase in
traffic, video streaming applications also require persistently high
bandwidth and timely packet delivery to ensure continuous media
playout. The compressed video streams are also sensitive to packet
losses. Most research that address these challenges have adopted endto-end schemes in which the video senders infer network condition
from estimated packet delay and loss statistics [2]. Such schemes can
only react to ongoing network congestion or packet losses, and suffer
from long queuing delays and persistent packet losses even at steady
state. Moreover, they usually cannot adapt agilely enough to abrupt
changes in traffic or network conditions, e.g., sudden arrival of new
streams in a fully utilized network.
The benefit of active network participation for streaming video
has long been recognized. Most existing designs are based on the
bandwidth reservation model [3]. Such an approach requires network
nodes to maintain per-flow states and incurs high implementation
complexity, especially at high link speeds. On the other hand, simpler
forms of explicit network feedback have been widely explored, for
instance by means of explicit congestion notification (ECN) [4]. Most
earlier works along this direction are designed for general Internet
traffic, not for streaming video.
In this paper, we study how simple network participation can
benefit video streaming in a novel framework. Our system design
follows a few basic criteria. First, an ideal bandwidth sharing scheme
should result in no standing queues at network nodes, so as to
avoid queueing delays and persistent packet losses at steady state.

Second, when multiple video streams compete for shared network
bandwidth, it is desirable that their allocated rates reflect their
respective rate-distortion (R-D) characteristics, while maintaining
comparable rate share for background non-video traffic. Furthermore,
the loss protection mechanism should provide sufficient protection
against transient losses, yet avoid overhead when losses are unlikely.
Finally, the system should be stable for arbitrary network topologies,
scale with arbitrary number of streams, and react fast to changes.
We present the design, analysis, and performance evaluation of
such a scheme, named Layered Internet Video Engineering (LIVE).
Under LIVE, network nodes feed back locally calculated congestion
information to video senders. Such information guides the video
sender in calculating both the optimal allocated rates and the percentage of forward error correction (FEC) protection. The streams are
pre-encoded using the scalable video coding (SVC) extension of the
H.264/AVC standard, allowing on-the-fly rate adaptation according
to the allocated rates and recommended FEC percentage.1
Our simulation evaluation shows that LIVE’s simple network
feedback allows the video streams to adjust to network congestion in
a proactive manner, without incurring large queuing delays or packet
losses. The feedback also facilitates fast convergence of the video
rates. LIVE’s media-aware bandwidth sharing scheme leads to more
balanced qualities among video streams than conventional fair-rate
allocation. LIVE’s network-assisted FEC scheme effectively protects
video packets from transient congestion without compromising the
video quality at steady state. Our theoretical analysis guarantees that
the LIVE system is stable for arbitrary number of video streams
with heterogeneous round trip times below a prescribed limit. Our
proposed ECN-based implementation of LIVE further assures coexistence of video streams with competing TCP flows.
In what follows, Section II reviews prior research related to
LIVE. Section III explains the scheme in detail. Section IV presents
simulation study of the proposed scheme. In Section V, we present
an ECN-based solution for coexistence of LIVE and TCP streams.
Section VI concludes the paper and discusses future work.
II. R ELATED W ORK
A. Multi-Stream Bandwidth Sharing
For bandwidth sharing among multiple streams, TCP-friendly
rate control (TFRC) [5] is commonly used for guiding the video
rate adaptation per stream. Most such schemes rely on end-to-end
packet statistics, and achieve the same TCP-friendly rate allocation
irrespective of the streams’ R-D characteristics [6]. Previous work
such as ECN [4] and MaxNet [7] has explored the benefit of explicit
network feedback, without specifically catering to streaming video.
1 The basic LIVE scheme may also accommodate other forms of video rate
adaptation such as bitstream switching, transcoding, or packet pruning from
a non-scalable stream. We leave such explorations for future work.

978-1-4244-5837-0/10/$26.00 ©2010 IEEE

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE INFOCOM 2010 proceedings
This paper was presented as part of the Mini-Conference at IEEE INFOCOM 2010

optimal rate
calculation

adaptive FEC
calculation

video R-D target
parameters rate

SVC video
source

FEC
percentage

SVC stream
rate adaptation

video
packets

Rate
Fig. 2.

update of virtual
congestion level

video sender

Fig. 1.

video
playout
echo

Cost

virtual congestion level
feedback

network node

Illustration of the optimal rate calculation.

video receiver

Overview of the LIVE system.

The importance of R-D optimized rate allocation has been recognized for multiplexing multiple video streams over a common
bottleneck link [8]. Most such schemes are centralized in nature,
and require knowledge of video R-D information of all participating
streams at a common entity. A mathematical framework for distributed rate allocation has been presented by Kelly et al. [9]. This
framework is combined with video R-D information for reducing
quality fluctuation in an end-to-end scheme [10]. It is also extended to
rate allocation for wireless video streaming [11]. LIVE incorporates
explicit network feedback within Kelly’s framework, and augments
its design for streams with heterogeneous round trip times.
B. FEC Protection for Layered Video
A vast body of literature has explored how to protect video streams
against packet losses using FEC. Most work studies the application of
FEC in settings with no feedback between the senders and receivers,
as in multicast and broadcast video [12]. In LIVE, on the other hand,
the FEC scheme is designed for systems with feedback. The second
category of work describes how to adapt the FEC protection level in
a unicast setting based on end-to-end packet loss rates measured at
the sender [13].The losses are usually due to transmission errors, e.g.,
over wireless links, whereas LIVE applies network-assisted FEC to
protect against congestion-induced losses.
Previous work has also investigated how to optimally allocate FEC
protection among different layers of the video stream bearing different importance [12]. These studies are complementary to LIVE’s
approach, and can be applied to enhance the current scheme with
media-aware FEC protection.
III. T HE LIVE S CHEME
We now introduce the LIVE system as shown in Fig. 1. Each
network node periodically calculates its virtual congestion level
(VCL). The sender calculates the optimal rate based on its own video
R-D parameters, as well as the maximum VCL value reported from
the receiver. The VCL information is also used to adjust the FEC
protection percentage, which determines the final SVC streaming rate.
Next, we describe each system component in greater detail. We
keep our descriptions of the scheme at a generic, conceptual level,
and defer discussion on deployment-related issues till Section V.

minimize the total distortion of all streams while achieving target
utilization at the bottleneck link. This can be formulated as:

(1)
min
i di (ri )
{ri }

(2)
s.t. yl = rl + i:l∈i ri ≤ γcl
where di (ri ) denotes the R-D function of each video stream. The
capacity of the bottleneck is denoted as cl ; the total rate over
the link is yl , comprising of the rates of all video streams ri ’s
and rate of all non-video streams rl traversing that link; the target
utilization γ is chosen to be slightly less than unity. We adopt the
parametric model from [14] for characterizing video R-D tradeoff
curves: di (ri ) = d0i + θi /(ri − ri0 ). The parameters d0i , ri0 and θi
are fitted from empirical R-D points of a pre-encoded video stream
for each group of pictures (GOP).
The optimization problem in (1)-(2) can be solved in a distributed
manner, following Kelly’s framework [9]. At the network node, the
virtual congestion level ql (t) for Link l is updated as q˙l (t) =
κ(yl (t)/cl −γ), where κ denotes the update scaling factor. The initial
value of the virtual congestion level is chosen as ql (0) = 0.
At the video sender, the optimal rate is calculated based on the
maximum congestion level along its path: q̃i (t) = maxl∈i ql (t).
The optimal rate depends on the video R-D
 function di (ri ), as:
ri∗ (t) = argminri [di (ri ) + q̃i (t)ri ] = ri0 + θi /q̃i (t). As illustrated
in Fig. 2, the optimal allocation balances between the competing
needs of increasing rate to reduce video distortion, and decreasing
rate to avoiding network congestion. A higher virtual congestion level
leads to a lower allocated rate whereas a lower virtual congestion level
encourages a higher video rate. In addition, it can be noted that the
same value of q̃i leads to different optimal rates for video streams
with different R-D parameters.
We now build upon this basic solution a practical design to guarantee system stability with streams of arbitrary round trip times (RTTs),
while remaining simple and scalable. At the network node, calculation
of the congestion level is updated once every time interval τ , so as
to limit the extra processing burden imposed on the network nodes.
The procedures at the network nodes are summarized as below.
Network Nodes:
Every update interval
Calculate the virtual congestion level as:
ql (t) = ql (t − τ ) + κ(

yl (t)
− γ)τ.
cl

(3)

Upon packet arrival
For LIVE video packet, update the virtual congestion level
packet header field as q˜i = max[ql (t), q˜i ].

A. Media-Aware Bandwidth Sharing
Conventional bandwidth sharing schemes typically aim at allocating equal rate among competing flows, assuming the same rate utility
function for each flow. In LIVE, we propose to share the bandwidth
among competing video streams in a media-aware fashion, i.e., to

At the video sender, the update of video rate is modified to
compensate for the effect of heterogeneous RTTs experienced by
each stream. First, each video sender predicts the current bottleneck
congestion level both from the past sample q̃i (t − τi ), and the

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE INFOCOM 2010 proceedings
This paper was presented as part of the Mini-Conference at IEEE INFOCOM 2010

freshly received sample q̃i (t). This predicted congestion information,
q̂i (t), is used for optimal rate calculation. Second, we make gradual
rate updates to approach the target optimal rate ri∗ (t). The update
step sizes are inversely proportional to the RTTs, to avoid big rate
swings for streams with long RTTs. The steps at video senders are
summarized as below.
Video Senders:
Upon receiving an ACK packet
1. Predict current congestion level
(q̃i (t) − q̃i (t − τi ))
.
(4)
τi
In (4), τi denotes the rate update interval; τ̃i is the estimated
round trip time; α designates the reference time for congestion
level prediction.
2. Calculate the optimal target rate as:

θi
∗
0
.
(5)
ri (t) = ri +
q̂i (t)
q̂i (t) = q̃i (t) + α

3. Gradually approach the target rate with step sizes tuned by
RTTs:
r∗ (t) − ri (t − τi )
τi .
(6)
ri (t) = ri (t − τi ) + i
ητ̃i
In (6), η is the rate update scaling factor.
The following theorem formally establishes stability of the proposed practical design:
Theorem: Let rttmax be the maximum round trip time in the system,
dmin be the minimum distortion and rmax be the corresponding
maximum video rate in the system. Further assume that α  rttmax
and η  1. Then, given
κ<

πη(dmin − d0 )
,
γα(rmax − r0 )

(7)

the overall system is stable for any number of streams with round
trip times less than rttmax .
Proof: see [15].
B. Network-Assisted Adaptive FEC
The simplest protection against transient network congestion is to
always add a fixed amount of FEC within the optimal rate budget
as calculated in the above section. However, fixed FEC protection
unnecessarily introduces overhead during steady state, when congestion is unlikely. Instead, we propose to leverage the virtual congestion
level feedback and to adapt the FEC percentage in a proactive and
efficient manner.
We apply (n, k) Reed-Solomon (RS) erasure codes across k video
packets within each frame to generate n − k parity packets.2 The
parameters n and k are adjusted on the fly for each video frame
based on past and current congestion levels. This protects against
any n−k lost packets within the same frame, introduces an overhead
percentage of (n − k)/n, and incurs an additional delay comparable
to frame interval.
The protection percentage fa is calculated based on the virtual
congestion feedback qi observed for each stream, as:
⎧
⎪
, Δq̃i < 0
⎨0
fa = fmax
(8)
, Δq̃i > Δq̃max .
⎪
⎩ Δq̃i
f
,
otherwise
Δq̃max max
2 We choose RS code due to its optimality for erasure protection, while the
adaptive algorithm is general enough to accommodate other channel codes.

In (8), Δq̃i denotes the difference between two samples of the
congestion level observed at the sender. We choose the time interval
between the two observations to be 200ms, so as to avoid overreacting
to temporary local fluctuations in the observation. Full FEC protection
at fmax is invoked when the difference Δq̃i exceeds Δq̃max . The
value of Δq̃max can be chosen empirically by learning from RTT
statistics.3
If the recommended FEC amount suddenly falls to zero, the scheme
maintains the last positive value for at least three RTTs before
following the recommendation. The adaptive scheme also dictates full
FEC protection at the start of a stream, before sufficient information is
collected from the network. In addition, no FEC packets are injected
unless the recommended amount is greater than 5%, so as to reduce
false alarms.
C. SVC Rate Adaptation
The final step in the LIVE system is to determine the rate of the
SVC stream. We consider video streaming with pre-encoded contents
in this work. In H.264/SVC, each video frame is encoded into
multiple video packets corresponding to multiple quality layers [16].
The video packets are classified as base layer (BL) and enhancement
layer (EL) packets. The video frames are organized into multiple
temporal layers, in that a frame from the (m + 1)th temporal layer
is encoded via bi-directional prediction from adjacent reconstructed
frames in the mth temporal layer. On-the-fly rate adaptation can
therefore be achieved by sequentially omitting EL packets according
to their temporal layers. A stream with M temporal layers hence has
M + 1 available rate points. The video R-D parameters are fitted
from a discrete set of available rates and qualities, and are stored as
meta data along with the streams.
Given a target rate r calculated from media-aware bandwidth
sharing and a recommended FEC percentage fa , the SVC stream
rate rsvc is determined as:
rsvc = rm , rm ≤ (1 − fa )r < rm+1 , 0 ≤ m ≤ M,

(9)

where the set of rates rm ’s denote available rate points for the
stream. The rest of the optimal rate is padded with FEC packets,
as rf ec = r − rsvc . In practice, this rate can only be approximated
when transmitting each video frame comprising n network packets,
by adding k = nrf ec /r FEC packets.
IV. P ERFORMANCE E VALUATION
A. Simulation Setup
We evaluate performance of the LIVE scheme in ns-2 simulations. At the network node, the target utilization is γ = 95%; the
virtual congestion level update interval is τ = 10ms; the update
scaling factor is κ = 0.1MSE/Kbits. At the video sender, the
reference time for virtual congestion level prediction is α = 250ms
and the video rate update scaling factor is η = 4.0.
Two video sequences are used in the simulations: Harbor and
City. They have a spatial resolution of 704 × 576 pixels per frame,
and a temporal resolution of 30 frames per second. Each stream is
encoded using the H.264/SVC reference codec [17] with two quality
layers and a GOP length of 32 frames, corresponding to 6 temporal
layers and 7 rate points. The video packets are further segmented
into network packets with a size of 1500 bytes for transmission.
Upon receipt of every network packet, the receiver feeds back an
acknowledgement (ACK) packet containing the maximum VCL value
along the forwarding path.
3 In our current implementation, it corresponds to an oversubscription limit
by 50% of link capacity for the entire duration of 200ms.

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE INFOCOM 2010 proceedings
This paper was presented as part of the Mini-Conference at IEEE INFOCOM 2010
Harbor vs. City, 4Mbps

10

20

10

30

20

stream 1: Harbor
stream 2: City
10

40

30

20

50

40

30
Time (s)

60

50

40

60

50

60

Avg. Video Quality in PSNR (dB)

VCL
Video Rate
Link Rate
(Mbps) (MSE/Mbps) (Mbps)

5
4
3
2
1
0
0
15
10
5
0
0
4
3
2
1
0
0

34
33.5
33
32.5
32
0

0.5

Fig. 3. Transient behavior of the media-aware bandwidth sharing scheme

admissible combination
fair−rate allocation
LIVE
reference from LIVE
1
1.5
2
2.5
3
3.5
4
Rate of Stream 2: City (Mbps)

from LIVE, when two streams share a bottleneck link.
Fig. 5. Optimality of LIVE’s media-aware bandwidth sharing scheme at

steady state, with respect to all admissible rate combinations.
LIVE

TFRC

PSNR (dB)

45
40
35
30
25
20
0

Queue Size
(# of pkts)

0
0

60
50
40
30
20
10
0
0

20

20

40

40

60

60

80

80

100

100

40

60
80
Time (s)

100

1

120

0
0

120

45
40
35
30
25
20
0
60
50
40
30
20
10
0
0

qlen
qlim
20

2

PSNR (dB)

1

120

Stream 1:Harbor
Stream 2:City

3

20

40

60

80

100

120

Transient

34
PSNR (dB)

2

No−FEC

40

60

80

100

120

28
1

2

40

60
80
Time (s)

100

Stream ID

3

4

3

4

Steady State

32
30
28
26

20

Max−FEC

30

34

20

Network−Assisted FEC

32

26

PSNR (dB)

Rate (Mbps)

4

3

Queue Size
(# of pkts)

Rate (Mbps)

4

1

2

Stream ID

120

Fig. 4. In comparison with the reference TFRC scheme, LIVE achieves
fast convergence, media-aware allocation of the video rates, and an empty
queue at steady state.

Fig. 6. Comparison of No-FEC, Network-Assisted FEC, and Max-FEC
schemes in terms of decoded video quality in PSNR during the transient
period and at steady states.

C. Effect of Media-Aware Bandwidth Sharing
B. Study of Transient Behavior
We first illustrate the basic dynamics of the media-aware bandwidth
sharing scheme in a simple scenario. The Harbor and City streams
share a bottleneck link with a capacity of 4Mbps. Figure 3 shows
traces of total traffic rate, virtual congestion level (VCL) values, and
allocated video rates. When only Harbor is active, the link can easily
accommodate the maximum rate of the stream, and the congestion
level remains at zero. When City starts streaming at t = 20s, the
instantaneous traffic rate over the link exceeds its capacity, leading
to a sharp increase in the congestion level and drives both streams
to reduce their rates. Note that the more complex Harbor streams a
higher rate than City. When Harbor stops streaming at t = 40s,
the congestion level quickly drops to zero, thereby allowing the
remaining City stream maximum rate and quality.
As a reference, Fig. 4 compares the LIVE scheme against TCPfriendly rate control (TFRC) [5]. Given the virtual congestion level
information from network nodes, rate allocation in LIVE converges
much faster than in TFRC. The TFRC scheme further suffers from
standing queues and occasional packet losses at steady state, resulting
in drastic drops in ongoing video quality and higher packet delivery
delay. The LIVE scheme, in contrast, manages to avoid packet losses
completely in this scenario. It yields an empty queue at steady state,
thereby reducing the packet delivery delay. When both streams are
active in the network, the allocated video rates from LIVE reflect
differences in the video R-D characteristics of the two streams.
Allocation from TFRC, on the other hand, result in equal rate
allocation among the two streams and a larger gap in their video
qualities.

Next, we examine the optimality of LIVE’s media-aware bandwidth sharing scheme. For the simple case of two streams sharing
a single link, we plot the average video quality of both streams as
achieved by LIVE, by fair-rate allocation, and by all other admissible
allocations. The latter refers to various rate combinations of both
streams that do not exceed the target link utilization. Figure 5 shows
such comparisons for the sequence pair Harbor vs. City over a
bottleneck links of capacity 4Mbps. It can be noted that LIVE
achieves the highest average video quality of both streams among
all admissible rate allocations. It outperforms fair-rate allocation by
0.57 dB in PSNR of the average video quality.

D. Effect of Network-Assisted FEC
We now study the network-assisted FEC scheme in a demanding
scenario. Three additional video streams simultaneously arrive at a
single link previously occupied by a single stream. The link capacity
is 10Mbps; all 4 video streams experience an RTT of 240ms and
have the same content of Harbor. Figure 6 compares the networkassisted FEC scheme against two other heuristics operating at extreme
measures: the No-FEC scheme never injects FEC packets; the MaxFEC scheme always streams the video at base-layer quality only,
padding the rest of the rate budget with FEC packets. During the transient period, both the Max-FEC scheme and the proposed networkassisted adaptive scheme are successful in recovering most packet
losses, therefore both achieve a higher video quality than the NoFEC scheme. At steady state, video quality from the network-assisted
adaptive scheme is only slightly lower than the No-FEC scheme. It
outperforms the Max-FEC scheme, as the latter compromises video
quality at steady state with constant FEC overhead.

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE INFOCOM 2010 proceedings
This paper was presented as part of the Mini-Conference at IEEE INFOCOM 2010
5

Rate (Mbps)

4

TCP stream
LIVE stream: City
LIVE stream: Harbor
total rate
target utilization

3
2
1
0
0

20

40

60
Time (s)

80

100

120

Fig. 7. Coexistence of LIVE streams and TCP streams within the ECN
framework.

V. C OEXISTENCE WITH TCP F LOWS
So far we have described and evaluated the LIVE scheme from a
clean-slate perspective. In reality, however, success of this scheme
also hinges on how well it can coexist with legacy systems. We
address this crucial question in this section, and present a solution
for LIVE’s coexistence with TCP flows while retaining most of its
benefits.
We achieve this goal by leveraging the existing explicit congestion
notification (ECN) mechanism [4]. A random marking probability p is
calculated as p = pmax q/qmax , based on the current congestion level
q, the maximum congestion level qmax , and the maximum marking
probability pmax . With a probability of p, the network node then
marks the Congestion-Experienced (CE) bit within the IP header of
traversing packets, as specified by [4].
The parameters qmax and pmax are globally defined, so that
the receiver can recover the congestion level based on estimated
p. The value of qestm
packet marking ratio p, as qestm = pqmax
max
is reported to the video sender in an application-layer packet header
field. Subsequently, the sender calculates the optimal rate according to
(4) - (6), substituting qestm for q̃. Since qestm is essentially a lowpass filtered observation of q̃, the scheme is expected to converge
more slowly, without affecting the final allocation result.
No modification is required for the TCP flows, as long as the
TCP sender and receiver react to the ECN markings according to
the standard specifications [4]. The average TCP throughput can be
expressed as:
kB
(10)
Rtcp = √ ,
τ̃ p
with scaling factor k, average packet size B, and round trip time
τ̃ [5]. The average video rate can be rewritten as:


θpmax /qmax
θpmax
≈
.
(11)
Rvideo = r0 +
√
qmax p
p
In (11), the value of r0 is typically much smaller than Rvideo , hence
it can be omitted. Note the same square root dependence on the
marking probability p in (10) and (11). This allows a TCP flow to
compete for bandwidth within the LIVE framework with a video-like
rate utility function.
Figure 7 illustrates the effectiveness of the proposed solution. In
this simulation, a link with capacity of 4 Mbps initially accommodates
a single City stream at the maximum rate of 1.7 Mbps. An FTP
flow over TCP starts at t = 30s and obtains the remaining available
bandwidth over the link, driving the total traffic rate to the target
utilization of 95% of link capacity. Shortly after the second stream
Harbor arrives at the link, the rates of both the TCP flow and the
City stream are reduced in response. It can be noted that the TCP
flow receives a bandwidth share between the more complex Harbor
sequence and the less demanding City sequence.

VI. C ONCLUSIONS
In this paper we have described LIVE, a new architecture for
streaming video in the Internet. Our simulation studies and theoretical
analysis show that video streams benefit immensely from simple explicit network feedback. The media-aware bandwidth sharing scheme
in LIVE allows the bottleneck link to be shared in an efficient
manner, minimizing total distortion of all competing video streams.
Rate allocation adapts to transient events with fast convergence, while
avoiding steady-state queuing delays or persistent packet losses. With
the network-assisted FEC scheme, a video stream can proactively
shield itself from transient network congestion before its quality is
impacted. Going forward, we will explore how real networks can
benefit from LIVE.
R EFERENCES
[1] “Cisco visual networking index: Forecast and methodology, 2008-2013,”
Cisco White Paper. [Online]. Available: http://newsroom.cisco.com/dlls/
index.html
[2] D. Wu, Y. T. Hou, W. Zhu, Y.-Q. Zhang, and J. Peha, “Streaming video
over the Internet: approaches and directions,” IEEE Trans. on Circuits
and Systems for Video Technology, vol. 11, no. 3, pp. 282–300, Mar.
2001.
[3] M. Krunz, “Bandwidth allocation strategies for transporting variable-bitrate video traffic,” IEEE Communications Magazine, vol. 36, no. 1, pp.
40–46, Jan. 1999.
[4] K. K. Ramakrishnan, S. Floyd, and D. Black, The Addition of Explicit
Congestion Notification (ECN) to IP, RFC 3168 (Proposed Standard),
Sept. 2001.
[5] S. Floyd, M. Handley, J. Pahdye, and J. Widmer, “TCP Friendly
Rate Control (TFRC): Protocol Specification,” RFC 5348 (Proposed
Standard), Sept. 2008.
[6] N. Wakamiya, M. Miyabayashi, M. Murata, and H. Miyahara, “MPEG4 video transfer with TCP-friendly rate control,” Proc. 4th IFIP/IEEE
International Conference on Management of Multimedia Networks and
Services (MMNS’01), vol. 2216, pp. 29–42, 2001.
[7] L. H. Andrew, K. Jacobsson, S. H. Low, M. Suchara, R. Witt, and
B. P. Wydrowski, “MaxNet: Theory and implementation,” Technical
Report, Netlab, CalTech, Tech. Rep., 2006. [Online]. Available:
http://netlab.caltech.edu/maxnet/maxnet papers.htm
[8] J. Chakareski and P. Frossard, “Rate-distortion optimized distributed
packet scheduling of multiple video streams over shared communication
resources,” IEEE Trans. on Multimedia, vol. 8, no. 2, pp. 207–218, Apr.
2006.
[9] F. Kelly, A. Maulloo, and D. Tan, “Rate control in communication
networks: shadow prices, proportional fairness and stability,” Journal
of the Operational Research Society, vol. 49, no. 3, pp. 237–252, Mar.
1998.
[10] M. Dai, D. Loguinov, and H. M. Radha, “Rate-distortion analysis
and quality control in scalable internet streaming,” IEEE Trans. on
Multimedia, vol. 8, no. 6, pp. 1135–1146, Dec. 2006.
[11] X. Zhu and B. Girod, “Distributed media-aware rate allocation for
wireless video streaming,” in Picture Coding Symposium (PCS’09),
Chicago, IL, USA, May 2009.
[12] W.-T. Tan and A. Zakhor, “Video multicast using layered FEC and
scalable compression,” IEEE Trans. on Circuits and Systems for Video
Technology, vol. 11, no. 3, pp. 373–386, Mar. 2001.
[13] S. Kang and D. Loguinov, “Impact of FEC overhead on scalable video
streaming,” in Proc. International Workshop on Network ad Operating
Systems Support for Digital Audio and Video (NOSSDAV’05), Stevenson,
WA, USA, 2005, pp. 123–128.
[14] K. Stuhlmüller, N. Färber, M. Link, and B. Girod, “Analysis of video
transmission over lossy channels,” IEEE Journal on Selected Areas in
Communications, vol. 18, no. 6, pp. 1012–32, June 2000.
[15] X. Zhu, R. Pan, N. Dukkipati, V. Subramanian, and F. Bonomi, “Layered
internet video engineering (LIVE): Network-assisted bandwidth sharing
and transient loss protection for scalable video streaming,” Cisco
Technical Report. [Online]. Available: https://sites.google.com/site/
liveexternal/
[16] H. Schwarz, D. Marpe, and T. Wiegand, “Overview of the scalable video
coding extension of H.264/AVC,” IEEE Trans. on Circuits and Systems
for Video Technology, vol. 17, no. 9, pp. 1103–1120, Sept. 2007.
[17] ITU-T Recommendation H.264 - ISO/IEC 14496-10(AVC), Advanced
Video Coding for Generic Audiovisual services, Amendment 3: Scalable
Video Coding, ITU-T and ISO/IEC JTC 1, 2005.

1

Tag-Weighted Topic Model
For Large-scale Semi-Structured Documents

arXiv:1507.08396v1 [cs.CL] 30 Jul 2015

Shuangyin Li, Jiefei Li, Guan Huang, Ruiyang Tan, and Rong Pan
Abstract—To date, there have been massive Semi-Structured Documents (SSDs) during the evolution of the Internet. These
SSDs contain both unstructured features (e.g., plain text) and metadata (e.g., tags). Most previous works focused on modeling
the unstructured text, and recently, some other methods have been proposed to model the unstructured text with specific tags.
To build a general model for SSDs remains an important problem in terms of both model fitness and efficiency. We propose a
novel method to model the SSDs by a so-called Tag-Weighted Topic Model (TWTM). TWTM is a framework that leverages both
the tags and words information, not only to learn the document-topic and topic-word distributions, but also to infer the tag-topic
distributions for text mining tasks. We present an efficient variational inference method with an EM algorithm for estimating the
model parameters. Meanwhile, we propose three large-scale solutions for our model under the MapReduce distributed computing
platform for modeling large-scale SSDs. The experimental results show the effectiveness, efficiency and the robustness by
comparing our model with the state-of-the-art methods in document modeling, tags prediction and text classification. We also
show the performance of the three distributed solutions in terms of time and accuracy on document modeling.
Index Terms—semi-structured documents, topic model, tag-weighted, variational inference, large-scale, parallelized solutions

✦

1

I NTRODUCTION

I

N the evolution of the Internet, there have been
a huge amount of documents in many web applications. Such kinds of documents with both plain
text data and document metadata (tags, which can be
viewed as features of the corresponding document)
are called the Semi-Structured Documents (SSDs).
How to characterize the semi-structured document
data becomes an important issue addressed in many
areas, such as information retrieval, artificial intelligence and data mining etc. The tags can be more
important than the text data in document mining.
For example, in IMDB 1 , the world’s most popular
and authoritative source for movie, TV and celebrity
content, each movie has lots of tags, like director,
writers, stars, country, language and so on, and a
storyline as text data. Given a movie with a tag “Dick
Martin”, we may have an idea that it has a higher
chance to be a comedy, without read the full text of
its storyline or watch it. Another example is that in
a collection of scientific articles, each document has a
list tags(authors and keywords). Before read the main
text of paper, we would know what it talks about after
we see the authors or the keywords that the paper
provides.
Many solutions have been proposed to deal with
the semi-structured documents (e.g., SVD, LSI), and
shown to be useful in document mining [11], [25], [35],

• Shuangyin Li, Jiefei Li, Guan Huang, Ruiyang Tan, and Rong Pan’s
E-mails: shuangyinli@cse.ust.hk, {lijiefei@mail2., huangg6@mail2.,
tanry@mail2., panr@}sysu.edu.cn. Submitted and reviewed by IEEE
Transactions on Knowledge and Data Engineering (TKED).
1. http://www.imdb.com

[33], e.g., text classification and structural information exploiting. For document modeling, topic models
have been used to be a powerful method of analyzing
and modeling of document corpora, using Bayesian
statistics and machine learning to discover the thematic contents of untagged documents. Topic models
can discover the latent structures in documents and
establish links between them, such as latent Dirichlet
allocation (LDA) [9]. However, as an unsupervised
method, only the words in the documents are modeled in LDA. Thus, LDA could only treat the tags as
word features rather than a new kind of information
for document modeling.
To model semi-structured documents needs to consider the characteristics of different kinds of objects,
including word, topic, document, and tag, and the
relationship among them. In this problem, topic is
a kind of hidden objects, and the other three are
the observations. Relative to tag, word and document are objective; tag can be either objective (e.g.,
author and venue information of publications) and
subjective (e.g., tags in social bookmark marked by
people). Similar to the topic models, we should consider binary relationships between the pairs of the
objects, including topic-word and document-topic. In
addition, we may consider the binary relationships,
like tag-word, tag-topic, tag-document, and tag-tag.
The tag-document relationship implies that we should
consider the weights of the tags in each document.
The tag-topic and tag-tag relationships can be more
complicated, thus are difficult to model. Some earlier
works consider certain tags. For example, the authortopic model in [31] considers the authorship information of the documents to be modeled. In this work, we

2

don’t limit the types and number of the tags in each
document. In an extreme case, where there is no tag
in any document, the new model may degenerates
into LDA. On the other hand, since the tags can be
created by some people, they should be relevant to
topics of the documents; however, some of them may
be correlated, redundant, and even noisy. Therefore,
the tag-topic relationships should be general enough
and we should also model the weights of the tags in
each document.
In the past few years, researchers have proposed approaches to model documents with tags or labels [26],
[29], [30]. For example, Labeled LDA [29] assumes
there is no latent topics and each topic is restricted
to be associated with the given labels. PLDA assumes
that each topic is associated with only one label [30].
However, both Labeled LDA and PLDA have implicit
assumptions that the given labels should be strongly
associated with the topics to be modeled or the labels
are independent to each other.
Another problem is that we would get into trouble when we need to deal with large-scale semistructured documents. A variety of algorithms have
been used to estimate the parameters of these proposed topic models for mining documents, such as
Monte Carlo Markov chain (MCMC) sampling techniques [1], [19], variational methods [3] and others
methods [2], [32]. For sampling methods, actually, we
may have to appeal to a tailored solution of MCMC
[7] for a particular model, which would impede the
requirement of convergence properties and speed, especially when the corpus comprise millions of words.
Variational methods as approximation solutions to
some extent improve the learning speed. However,
it would also be ineffective on learning speed and
model accuracy when it comes to a large-scale corpus.
In this paper, we propose a framework of TagWeighted Topic Model (TWTM) to represent the text
data and the various tags with weights to evaluate the importance of the tags. Besides learning the
topic distributions of documents and generating the
topic distributions over words, the framework also
infers the topic distributions of tags. The weights
of observed tags in each document, which we infer
from the dataset, give us an opportunity to provide a
method to rank the tags.
In many web applications, not all the documents
in the corpora have tags. There are lots of documents
only consist of words without any tags which maybe
removed after data preprocessing for denoising. Only
consider the weights among tags would not hold
this case. To address this problem, we also propose
a more flexible model called Tag-Weighted Dirichlet Allocation (TWDA) as an extended model. It is
based on TWTM, and learns the weights among a
Dirichlet prior and the given tags, not just among
the tags. Therefore, TWDA handles not only the
semi-structured documents, but also the unstructured

documents. For the unstructured documents, TWDA
degenerates into latent Dirichlet allocation (LDA).
For hybrid corpora which consist of both the semistructured documents and unstructured documents,
TWDA can handle this complex type of corpora more
effectively and easily.
For the challenge of modeling large-scale corpora,
we propose three distributed schemes for the framework of TWTM model in MapReduce programming
framework [16]. The proposed model has four principal contributions.
1) It is a novel topic modeling method to model
the semi-structured documents, not only generating the topic distributions over words, but also
inferring the topic distributions of tags.
2) The TWTM leverages the weights among the
observed tags in a document to evaluate the
importance of the tags using a function of tagweighted topic assignment process. The weights
are associated with the observed tags in a document providing a way to rank the tags. In
addition, this could be used to predict latent tags
in the document.
3) The framework of tag-weighted process is easy
to extend for many different real world applications. For example, with the extended model
TWDA, we can handle both the multi-tag documents and non-tag documents simultaneously,
which is very useful to process some complicated web applications.
4) Three distributed solutions for TWTM have been
proposed that focus on challenges of working
at a large-scale semi-structured documents in
MapReduce programming framework.
The rest of the paper is organized as follows. In Section 2, we first analyze and discuss related works. In
Section 3, after introducing the notations, we present
the novel topic modeling framework of TWTM, and
give the methods of learning and inference. In Section 4, we show the extended model TWDA, and give
the process of learning and inference. In Section 5,
we will give the theoretical analysis to discuss the
differences between TWTM and TWDA, comparing
the other topic models. In Section 6, we propose three
distributed solutions of TWTM for a large-scale semistructured documents. In Section 7, we present the
experimental results on three domains to show the
performance of the proposed method in document
modeling, text classification and the effectiveness and
efficiency of the three large-scale solutions on a large
scale semi-structured documents modeling. We end
the paper in Section 8.

2

R ELATED WORKS

Topic models provide an amalgam of ideas drawn
from mathematics, computer science, and cognitive
science to help users understand unstructured data.

3

There are many topic models proposed and shown
to be powerful on document analyzing, such as in
[28], [20], [9], [8], [10], [14], which have been applied
to many areas, including document clustering and
classification [12], and information retrieval [34]. They
are extended to many other topic models for different
situation of applications in analyzing text data [21],
[23], [36]. However, most of these models only consider the textual information and can only treat the
tag information as plain text as well.
TMBP [17] and cFTM [15] propose the methods to
make use of the contextual information of documents
for topic modeling. TMBP is a topic model with biased
propagation to leveraging contextual information, the
authors and venue. TMBP needs to predefine the
weights of the author and venue information on
word assignment, which limits the usefulness in real
applications. The method of cFTM has a very strong
assumption that each word is associated with only
one tag, either author or venue. In many applications,
this assumption may not hold.
Several models have been proposed to take advantage of tags or labels, such as Labeled LDA [29],
DMR [26] and PLDA [30], or modeling relationships
among several variables, such as Author-Topic Model
[31]. Labeled LDA [29] get the topic distribution for
a document through picking out the several hyperparameter components that correspond to its labels,
and draw the topic components by the new hyperparameter without inferring the topic distribution of
labels. Labeled LDA does not assume the existence
of any latent topics [30]. PLDA [30] provides another way of modeling the tagged text data, which
assumes the generation topics assignment is limited
by only one of the given tags for one word, and in
the training process, PLDA assumes that each topic
takes part in exactly one label, and may optionally
share global label present on every document. In
Author Topic Model, it obtains the topic distributions
of authors, without giving the importance weights
among the given authors in each document. DMR
[26] is a Dirichlet-multinomial regression topic model
that includes a log-linear prior on the documenttopic distributions, which is an exponential function
of the given features of the document. However, DMR
doesn’t output the tag weights either [31], which is
useful for tag ranking.
So in this work, we propose a tag-weighted topic
modeling framework which leverages the tag information given in a document by a list of weight
values to model the topic distribution of the document. Meanwhile, for a mixture collection of semistructured documents and unstructured documents,
we present an extended model called tag-weighted
Dirichlet Allocation which considers both a Dirichlet prior and the tags by the weight values among
them. Based on the framework of Tag-Weighted Topic
Model, we also show three large-scale solutions under

the MapReduce distributed computing platform for
large-scale semi-structured documents.

3

TWTM M ODEL

AND

A LGORITHMS

In this section, we will mathematically define the
tag-weighted topic model (TWTM), and discuss the
learning and inference methods.
3.1

Notation

Similar to LDA [9], we formally define the following
terms. Consider a semi-structured corpus, a collection of M documents. We define the corpus D =
{(w1 , t1 ), . . . , (wM , tM )}, where each 2-tuple (wd , td )
denotes a document, the bag-of-word representation
d
wd = (w1d , . . . , wN
), td = (td1 , . . . , tdL ) is the document
tag vector, each element of which being a binary tag
indicator, and L is the size of the tag set in the corpus
D. For the convenience of the inference in this paper,
td is expanded to a ld × L matrix T d , where ld is
the number of tags in the document d. For each row
number i ∈ {1, . . . , ld } in T d , Ti·d is a binary vector,
where Tijd = 1 if and only if the i-th tag of the
document d2 is the j-th tag of the tag set in the corpus
D. In this paper, we wish to find a probabilistic model
for the corpus D that assigns high likelihood to the
documents in the corpus and other documents alike
utilizing the given tag information.
3.2

Tag-Weighted Topic Model

TWTM is a probabilistic graphical model that describes a process for generating a semi-structured
document collection. In the previous topic models, a
document d is typically characterized by a multinomial distribution over topics, θd , and each topic k is
represented by ψk , over words in a vocabulary. Take
LDA [9] as an example, the generative process of topic
distribution of document d is assumed as follows.
Choose θd ∼Dirichlet(α),
and choose zni ∼Multinomial(θd ),
where α is the hyperparameter of θd . In LDA, the topic
distribution θd is drawn from a hyperparameter α,
without considering the given tags. However, the tag
information should be more useful for the generation
of θd than a Dirichlet prior.
In this paper, we use ϑd , instead of θd , to denote the
topic distribution of document d as shown in Figure 1.
Let θ represent a L × K topic distribution matrix
over the tag set, where K is the number of topics.
Let ψ represent a K × V distribution matrix over
words in the dictionary, where V is the number of
words in the dictionary of D. Similar to LDA, TWTM
models the document d as a mixture of underlying
2. Note that we can sort the tags of the document d by the index
of the tag set of the corpus D.

4

ψ

θ
L×K

K×V

εd
π

ϑd

z

w

3.3

N

η

t

paper is the tag’s weight topic assignment by which
ϑd is generated through εd , T d , and θ, which provides
an effective and direct method to infer the weights of
the tags.

L
D

Fig. 1. Graphical model representation for TWTM,
where θ is distribution matrix of the whole tags, ψ is
distribution matrix of words, ǫd represents the weight
vector of the tags, and ϑd indicates the topic components for each document. π is a Dirichlet prior and η is
a Bernoulli prior.
topics and generates each word from one topic. The
topic proportions ϑd of the document d is a mixture
of tag-topic distributions, not only controlled by a
hyperparameter described as in LDA.
The generative process for TWTM is given in the
following procedure:
1) For each topic k ∈ {1, . . . , K}, draw ψk ∼ Dir(β) ,
where β is a V dimensional prior vector of ψ.
2) For each tag t ∈ {1, . . . , L}, draw θt ∼ Dir(α), where
α is a K dimensional prior vector of θ.
3) For each document d:
a) For each l ∈ {1, . . . , L}, draw tdl ∼ Bernoulli(ηl ).
b) Generate T d by td .
c) Draw εd ∼ Dir(T d × π).
d) Generate ϑd = (εd )T × (T d × θ).
e) For each word wdi :
i) Draw zdi ∼Mult(ϑd ).

Tag-Weighted Topic Assignment

As we assume that all the observed tags in the document d make contributions to infer the topic distribution ϑd of the document, it is expected that different
tags works corresponding to their own weights. For
example, in some blog application, a blog has tags of
an author, a blog’s date, a blog category and a blog’s
url. Clearly, compared to other tags, the tag of the
author plays the most important role in constituting
the topic components of the blog.
The function of how to leverage the tag information
or contextual to infer topic distribution of a document
is defined as follows:
ϑ ←− f (t1 , · · · , tl ),

where f (·) is the way of making use of the tag
information. Topic models using tag information or
contextual take advantage of the different f (·) in
the past. In TWTM, we assume that ϑd is made up
by all the observed tags with their own weights.
Figure 1 shows that how TWTM works in a probabilistic graphical model. As shown in Figure 1, ϑd
is controlled by two sides, the topic distributions
over tags θ, and the weights of the given tags of
the document d. It is important to distinguish TWTM
from the Author-Topic Model [31]. In the author-topic
model, the words w is chose only by one of the given
tags’ distribution, while in TWTM, for word w, all
the observed tags in the document would make the
contributions.
The f (·) in the proposed model is assumed as this,
for the document d,
f (ϑd ) = (εd )T × T d × θ,

ii) Draw wdi ∼Mult(ψzdi ).

In this process, Dir(·) designates a Dirichlet distribution, Mult(·) is a multinomial distribution, and π
is a L × 1 column vector, a Dirichlet prior. Note that
εd indicates the weight vector of the observed tags in
constituting the topic proportions of the document d,
and (εd )T is the transpose of εd . Furthermore, εd is
drawn from a Dirichlet prior which obtained by the
matrix multiplication of T d × π. Clearly, the result of
T d × π will be a (ld × 1) vector whose dimension is
depended on the number of the observed tags in the
document d.
In Step 3, for one document d, we first generate the
document’s tags tdl using a Bernoulli coin toss with a
prior probability ηl , as shown in step (a). After draw
out the εd , we generate the ϑd through εd , T d and θ.
The remaining part of the generative process is just
familiar with LDA [9]. As shown above, in TWTM,
we introduce a novel way to model the topic proportions of semi-structured document by documentspecial tags and text data. The key discussed in this

where the linear multiplication of (εd )T , T d and
PK
d
θ maintains the condition of
k=1 ϑk = 1 without
d
d
normalization of ϑ , since ε and θ satisfy
d

l
X

εdi = 1,

i=1

K
X

θlk = 1.

k=1

Firstly, we pick out the topic distributions of the given
tags in the document d from θ by T d × θ, where T d
is a ld × L matrix and θ is a L × K matrix. Here we
define
Θd = T d × θ,

where the Θd is a ld × K topic distribution matrix of
the given tags in d as sub-components of θ. Secondly,
εd is the weight vector of the observed tags in d,
and each dimension of εd represents the weight or
importance associated to the corresponding tag. Thus,
ϑd is mixed by Θd with corresponding weight values.
d

d

d

i=1

i=1

i=1

l
l
l
X
X
X
ϑd = (
εdi Θdi1 , . . . ,
εdi Θdij , . . . ,
εdi ΘdiK ).

5

ξ

following fully factorized distribution:

γd

q(εd , z1:N |ξ1:L , γ1:K ) = q(εd |ξ)
z

εd

N
Y

q(zi |γi ).

i=1
N
M

Fig. 2. Graphical model representation of the variational distribution used to approximate the posterior in
TWTM.
With ϑd , TWTM generates all the words in the
document d with the assumption of bag-of-words.
Based on the above framework, we can define a
special topic assignment function f (·) in an extended
model for a real world application.

The dimension of parameter ξ is changed with different documents. It could be difficult to compute the
expected log probability of a topic assignment by the
way of tag-weighted topic assignment used in TWTM.
Then, we maximize the lower bound L(·) with
respect to the variational parameters ξ and γ, using a
variational expectation-maximization(EM) procedure
as follows.
3.4.1 Variational E-step
We first maximize L(·) with respect to ξi for the
document d. Maximize the terms which contain ξ:
d

3.4 Inference for TWTM
In the topic models, the key inferential problem that
we need to solve is to compute the posterior distribution of the hidden variables given a document d.
Given the document d, we can easily get the posterior
distribution of the latent variables in the proposed
model, as:
p(εd , z|wd , T d , θ, η, ψ, π) =

p(εd , z, wd , T d |θ, η, ψ, π)
.
p(wd , T d |θ, η, ψ, π)

L[ξ] =

·

N X
K
Y

Z

+

p(zid |(εd )T × T d × θ)p(wid |zid , ψ1:K ) dεd .

i=1 z d =1
i

In this work, we make use of mean-field variational
EM algorithm [4] to efficiently obtain an approximation of this posterior distribution of the latent
variables. In the mean-field variational inference, we
minimize the KL divergence between the variational
posterior probability and the true posterior probability through by maximizing the evidence lower bound
(ELBO) L(·) [8]. For a single document d, we obtain
the L(·) using Jensen’s inequality:
L(ξ1:ld , γ1:K ; η1:L ,π1:L , θ1:L , ψ1:K )
= E[log p(T1:ld |η1:L )] + E[log p(εd |T d × π)]
+

N
X

E[log p(zi |(εd )T × T d × θ)]

i=1

+

N
X

E[log p(wi |zi , ψ1:K )] + H(q),

N X
K
X

′

j =1

d

γik ·

i=1 k=1

l
X

d

(j)

log θk ξj /

j=1

l
X

ξj ′

′

j =1

d

d

i=1

i=1

(2)

l
l
X
X
− log Γ(
ξi ) +
log Γ(ξi )

(1)



p εd |(T d × π)

il

i=1 l′ =1

−

In Eq. (1), integrating over ε and summing out z, we
easily obtain the marginal distribution of d:
p(wd , T d |η, θ, ψ, π) = p(td |η)

d

l
L
l
X
X
X
(
πl′ T d′ − 1)(Ψ(ξi ) − Ψ(
ξj ′ ))

ld
X

d

(ξi − 1)(Ψ(ξi ) − Ψ(

i=1

l
X

ξj ′ )),

′

j =1

where Ψ(·) denotes the digamma function, the first
derivative of the log of the Gamma function. Here we
use gradient descent method to find the ξ to make the
maximization of L[ξ] .
Next, we maximize L(·) with respect to γik . Adding
the Lagrange multipliers to the terms which contain
γik , taking the derivative with respect to γik , and
setting the derivative to zero yields, we obtain the
update equation of γik :
d

γik ∝ ψk,vwi exp{

l
X

j=1

ξj
(j)
log θk P d
l

j ′ =1 ξj ′

},

(3)

where v wi denotes the index of wi in the dictionary.
In E-step, we update the ξ and γ for each document
with the initialized model parameters. For the reason
of different document with different number of tags,
we have to keep all the ξ updated by each document
for the M-step estimation.
3.4.2

M-step estimation

i=1

where ξ is a ld -dimensional Dirichlet parameter vector and γ is 1×K vector, both of which are variational
parameters of variational distribution shown in Figure 2, and H(q) indicates the entropy of the variational
distribution:
H(q) = −E[log q(εd )] − E[log q(z)].

Here the exception is taken with respect to a variational distribution q(εd , z1:N ), and we choose the

The M-step needs to update four parameters: η, the
tagging prior probability, π, the Dirichlet prior of the
tags’ weights, θ, the topic distribution over all tags in
the corpus, and ψ, the probability of a word under
a topic. Because each document’s tag-set is observed,
the Bernoulli prior η is unused included for model
completeness. For a given corpus, the ηi is estimated
by adding up the number of i-th tag which appears
in the corpus.

6

ψ

θ

For the document d, the terms that involve the
Dirichlet prior π:

L×K



L[π] = log Γ 
+

d

l
X
i=1

ld 
X
i=1



(T d × π)i  −


i=1



log Γ (T d × π)i


(T d × π)i − 1 Ψ(ξi ) − Ψ(

ld
X

j=1



εd
(4)

ξj ) ,

and
ψkj ∝

D X
N
X

ξld tdl
d
,
γik
PL
d d
l=1 (ξl tl )
d=1 i=1
D X
N
X

d
γik
(w d )ji .

ϑd

π

z

w
N

λ

Pd P
d
where (T d × π)i = li=1 L
l=1 πl Til . We use gradient
descent method by taking derivative of Eq. (4) with
respect to πl on the corpus to find the estimation of
π.
To maximize with respect to θ and ψ, we obtain the
following update equations:
θlk ∝

K×V

d

l
X

η

t

L
D

µ

Fig. 3. Graphical model representation for TWDA,
where µ is a Dirichlet prior of λ.

(5)

(6)

d=1 i=1

We provide a detailed derivation of the variational
EM algorithm for TWTM in Appendix A. And we
show the variational expectation maximization (EM)
procedure of TWTM in Algorithm 1.
Algorithm 1 The variational expectation maximization (EM) algorithm of TWTM
1: Input: a semi-structured corpora including totally
V unique words, L unique tags, and the expected
number K of topics.
2: Output: Topic-word distributions ψ, Tag-topic distributions θ, π, topic distribution ϑd and weight
vector εd of each training document.
3: initialize π, and initialize θ and ψ with the conPK
PV
straint of k=1 θlk equals 1 and i=1 ψki equals
1.
4: repeat
5:
for each document d do
6:
update ξ d with Eq. (2) using gradient descent
method.
7:
update γik with Eq. (3).
8:
end for
9:
update π with Eq. (4) using gradient descent
method.
10:
update θ by Eq. (5).
11:
update ψ by Eq. (6).
12: until convergence

the weights among the observed tags. Our proposed
solution to the problem is to add a Dirichlet prior to
the topic distribution ϑd , which means that we learn
the weights among the Dirichlet prior and the given
tags, not just among the tags. We call this solution TagWeighted Dirichlet Allocation (TWDA). When handling the unstructured documents in a hybrid corpus,
TWDA degenerates into LDA [9] which just draws
the topic proportions for a document from a Dirichlet
distribution.
As an extended model of TWTM, TWDA uses the
same parameter notations. Unlike TWTM, for the
convenience of the inference in TWDA, td is expanded
to a ld × (L + 1) matrix T d , where ld is one more than
the number of the given tags in the document d (For
example, if the document d has five tags, ld is six). For
each row number i ∈ {1, . . . , ld } in T d , Ti·d is a binary
vector, where Tijd = 1 if and only if the i-th tag of the
document d is the j-th tag of the tag set in the corpus
D. Note that, we set the last dimension of the last row
in T d to 1, and the other dimensions of the last row
equal to 0 for all documents. The detail of the above
setting will be shown later.
TWDA defines a Dirichlet prior µ over a latent topic
distribution of a document, and mixes the latent topic
proportion with these topic distributions of the given
tags by importance or weight (tag-weighted) to form
the final topic distribution of the document. Figure 3
shows the graphical model representation of TWDA,
and the generative process for TWDA is given in the
following procedure:
1) For each topic k ∈ {1, . . . , K}, draw ψk ∼ Dir(β) ,
where β is a V dimensional prior vector of ψ.
2) For each tag t ∈ {1, . . . , L}, draw θt ∼ Dir(α), where
α is a K dimensional prior vector of θ.

4

TAG -W EIGHTED D IRICHLET A LLOCATION

In a real world application, a corpus is very likely
to contain both semi-structured documents and unstructured documents. Many documents in the corpus
have no tags, just with unstructured text data. In
this case, TWTM does not work, which generates
the topic distribution of a document by leveraging

3) For each document d:
a) Draw λ ∼ Dir(µ).
b) Generate T d by td .
c) Draw εd ∼ Dir(T d × π).
d) Generate ϑd = (εd )T × T d × ( λθ ) .
e) For each word wdi :

7

i) Draw zdi ∼Mult(ϑd ) .

ξ

ρ

γd

εd

λ

z

ii) Draw wdi ∼Mult(ψzdi ) .

Note that, L is the number of tags appeared in the
corpora and K is the number of topics. Different from
TWTM, here π is a (L+1)×1 column vector and µ is a
K × 1 column vector. Both of them are Dirichlet prior.
λ is a 1 × K row vector which is drawn from µ. (εd )T
is the transpose of εd , and εd is drawn from a Dirichlet
prior which obtained by the matrix multiplication of
T d × π. Clearly, the result of T d × π will be a (ld × 1)
vector whose dimension is depended on the number
of the observed tags in the document d. Note that, ld
is one more than the number of tags given in d as we
described above.
In other words, we treat the λ as a topic distribution
of one latent tag, the Dirichlet prior µ. Each document
is controlled by a latent tag, that is the same idea
both TWDA and Latent Dirichlet Allocation (LDA).
The form of ( λθ ) is the augmented matrix of θ and
λ, which represents that we add the vector λ to the
matrix θ as the last row, so ( λθ ) becomes a (L + 1) × K
matrix. As we show above, T d is the matrix form of
the given tags in the document d, and the last row of
T d is a binary vector, of which only the last dimension
equals to 1 and the others equal 0. Here we define
θ
Θd = T d × ( ).
λ

Clearly, Θd is a ld × K matrix, whose last row is
λ. Actually, the purpose of Θd is to pick out the rows
corresponded to the tags appeared in d from tag-topic
distribution matrix θ.
The key idea of tag-weighted Dirichlet allocation
is to model the topic proportions of semi-structured
documents by document-special tags and text data.
Different from LDA, the topic proportion of one document assumed in this model is controlled not only by
a Dirichlet prior µ, but also by all the observed tags.
The way to generate the normalized topic distribution
of the document d is that we mix both Dirichlet allocation and tags information through a weight vector
εd . Thus, we use the function f (·) of topic assignment
to obtain the topic distribution of d by

N
M

Fig. 4. Graphical model representation of the variational distribution used to approximate the posterior in
TWDA.
4.1 Inference for TWDA
In TWDA, we treat π, µ, η, θ and ψ as unknown
constants to be estimated. Similar to TWTM, the
marginal distribution of d is not efficiently computable
as follows:
p(wd , T d |η, θ, ψ, π, µ) = p(td |η)
· p(λ|µ)

Z



p εd |(T d × π)

N X
K
Y

i=1 z d =1
i

·

p(wid |zid , ψ1:K )

4.1.1 Variational inference
In TWDA, we use the following fully factorized distribution as shown in Figure 4:
q(εd , λd , z1:N |ξ1:L , ρ1:K , γ1:K ) = q(εd |ξ)q(λd |ρ)

H(q) = −E[log q(εd )] − E[log q(λ)] − E[log q(z)].

For the variational parameter ξ, we take the terms
which contain ξ out of the evidence lower bound
(ELBO) L(·) of TWDA to form L[ξ] , and we use
gradient descent method to find the ξ to make the
maximization of L[ξ] :
d

L[ξ] =

d

L+1
l
l
X
X
X
(
πl′ T d′ − 1)(Ψ(ξi ) − Ψ(
ξj ′ ))
il

i=1 l′ =1

+

N X
K
X

′

j =1

d

γik ·

i=1 k=1
d

l
X

j=1

ξj
(j)
Ck P d
l

i=1

εdi = 1,

k=1

θlk = 1, and

′

(7)

l
l
X
X
− log Γ(
ξi ) +
log Γ(ξi )
i=1

−

λk = 1.

l
X

i=1

d

(ξi − 1)(Ψ(ξi ) − Ψ(

i=1

k=1

Therefore, the linear multiplication of (εd )T , T d , θ
P
d
and λ maintains the condition of K
k=1 ϑk = 1 without
d
d
normalization of ϑ . With ϑ , the topic proportions of
the document d, the remaining part of the generative
process is just familiar with LDA.

ξ

′
j =1 j

d

d

K
X

q(zi |γi ),

and the entropy of the variational distribution will
be

It is worth to note that the ε is draw by a Dirichlet
prior π, each row of θ is draw by a Dirichlet prior
α, and λ is draw by a Dirichlet prior µ, so εd and θ
satisfy
K
X

N
Y

i=1

d

X

dεd .

In this case, We also use a variational expectationmaximization (EM) procedure to carry out approximate maximum likelihood estimation of TWDA.

θ
f (ϑd ) = (εd )T × T d × ( ).
λ

ld

θ
p(zid |(εd )T × T d × ( ))
λ

l
X

ξj ′ )),

′

j =1

where
(j)
Ck

(
(j)
log θk
P
=
Ψ(ρk ) − Ψ( K
ρ ′)
′
j =1 j

j ∈ {1, · · · , ld − 1}
,
j = ld

and Ψ(·) denotes the digamma function, the first
derivative of the log of the Gamma function.

8

In particular, by computing the derivatives of the
L(·) and setting them equal to zero, we obtain the
following pair of update equations for the variational
parameters ρd and γik :
ρi ∝ µi +

N
X

n=1

ξd
γni · P dl
l

d

γik ∝ ψk,vwi exp{

l
X

j=1

wi

j=1 ξj

,

ξj
(j)
Ck P d
l

j ′ =1 ξj ′

(8)

},

4.1.2 Model Parameter Estimation
There are four model parameters that need to estimate
in M-step, π, the Dirichlet prior of the tags’ weights,
θ, the topic distribution over all tags in the corpus,
ψ, the probability of a word under a topic, and µ, a
Dirichlet prior of model. In TWDA, we can estimate
π, θ and ψ as same as in TWTM.
Different from TWTM, TWDA has an extra Dirichlet
prior µ. The involved terms of µ are:
D
X

(log Γ(

+

µj ) −

j=1

d=1
K
X

K
X

(µi −

1)(Ψ(ρdi )

i=1

K
X

log Γ(µi )

i=1

− Ψ(

K
X

(10)
ρdj ))).

j=1

We can invoke the linear-time Newton-Raphson algorithm to estimate µ as same as the Dirichlet parameter
described in LDA [9].
In the variational expectation maximization (EM)
procedure of TWDA, we update the variational parameters ξ d , ρ and γik with Eqs. (7), (8) and (9)
respectively in the E-step. In the M-step, besides the
update of π, θ and ψ, we also update µ with Eq. (10) by
Newton-Raphson algorithm. The detailed derivation
of the model parameter estimation in TWDA is shown
in Appendix B.

5

A NALYSIS

OF

θ
ϑd = (εd )T × T d × ( )
λ

(9)

where v denotes the index of wi in the dictionary.
In the E-step, we update the variational parameters
ξ, ρ and γ for each document with the initialized
model parameters. We show the detailed derivation
of the variational parameters for TWDA in Appendix
B.

L[µ] =

function that how to generate the topic distribution of
a document, or, in other words, the assumption that
what distribution the topic of a document follows.
In TWDA, the topic proportions ϑd for a document
d is obtained by the following function:

TWDA

In TWDA, we introduce a better way to directly model
the semi-structured documents and unstructured documents by adding a latent tag to each documents,
which the topic distribution of a document is controlled by the observed tags and one latent tag. In
LDA, the topic distribution of a document is drawn
from a hyperparameter, without considering the given
tags, and while in TWTM, the topic distribution is
controlled by a list of given tags with corresponding
weight values. The main difference among the models
which handle the unstructured text (e.g., LDA and
CTM [7]) or the semi-structured documents (e.g., ATM
[31], Label-LDA [29], DMR [26] and PLDA [30]) is the

When we ignore the tags in a document, the T d
in Eq. (5) becomes a binary row vector and the last
dimension equals to 1 and the others are 0. In this
case, ( λθ ) is simplified to λ:
θ
ϑd = (εd )T × T d × ( )
λ
= λ.
The topic distribution of d is simplified to λ, and as
we shown above, λ is draw by a Dirichlet prior µ. It
means that the topic proportions for the document
d as a draw from a Dirichlet distribution which is
the basic assumption of LDA [9]. In others words,
when handling the unstructured documents, TWDA
degenerates into LDA.
In other words, the topic distribution of a document in TWTM is the weighted average of the topic
distributions of the given tags, and to some extent,
it is a linear relation between the topic distribution
of a document and the tags. While, in TWDA, with
the addition of the Dirichlet prior µ, which is equal to
generate a latent tag for each document with a special
topic distribution, it is a non-linear topic generation
procedure in each document.

6

L ARGE S CALE S OLUTIONS

Currently, many web applications appear with large
scale tagged documents, and highlight the issues of
large scale semi-structured documents in many areas.
In this paper, we propose and compare three different distributed methods based on the framework
of TWTM, which focus on the challenge of working
at a large scale, in the MapReduce programming
framework.
Solution I
The first solution is a tailored parallel algorithm for
TWTM. The learning and inference of the proposed
model are based on variational method with an EM
algorithm. Thus, we design a parallel algorithm for
TWTM using MapReduce programming framework.
As shown above, we need to update the global
parameters π, θ, and ψ for a corpus. Every document
has associated with the corresponding variational parameters ξ and γ. The mapper computes these variational parameters for each document and uses them to
generate the sufficient statistics to update π, θ, andψ.

9

And the reducer updates the global parameters π, θ,
and ψ.
1) Mapper: For each document d, we compute γ d
using the update equation Eq. (3) and obtain ξ d
by Eq. (2). The sufficient statistics are kept for
each document.
2) Reducer: The Reduce function adds the value
to the global parameters θ and ψ using the
sufficient statistics as in Eqs. (5), and (6).
3) Driver: The driver program marshals the entire
inference process. At the beginning, the driver
initializes all the model parameters K, L, θ, ψ,
and π. The topic number K is user specified; the
number of tags L is determined by the data; the
initial value of π is given by the user, θ and ψ
is randomly initialized. After each MapReduce
iteration, the driver normalizes the global θ and
ψ.
Note that, because π is a global parameter over the
corpus, we have to update π at the end of each
iteration in driver. However, this will lead to a large
scale data migration to compute the π by Eq. (4),
since π is associated with each document and different
documents have different tags which affect the different dimensions in π. The whole corpus data would
migrate to the single driver node. This could generate
a bottleneck in the driver.
Solution II
On account of the bottleneck in Solution I, we optimize the calculation of π through an approximate
method as the Solution II. The MapReduce procedure
of Solution II is as follows.
1) Mapper: For each document d, we compute γ d
and ξ d by Eqs. (2) and (3) and the sufficient
statistics for updating θ and ψ. Different with
Solution I, we obtain a π s for each map data
split s by Eq. (4).
2) Reducer: The Reduce function adds the value
to the global parameters θ and ψ using the
sufficient statistics as in Eqs. (5), and (6).
3) Driver: In the driver function, we only need to
compute an average of π s , s ∈ (1, · · · , S) where
S is the total number of mapper in the cluster.
The driver also normalizes the global θ and ψ
for next iteration.
Solution II is an approximate solution of TWTM,
which computes the πs for each mapper and takes
their average as the solution of π to avoid the large
scale data migration.
Solution III
As shown in Eq. (4), πl , l ∈ (1, · · · , L) is only associated with the documents who contain the lth
tag. Thus, before running TWTM, we can cluster the
documents into several clusters with the condition

that the documents which contain one or a plurality
of the same tag should be in the same cluster. It
means that the documents are divided into the mutually independent space by the tags. We show the
detailed process of the clustering in Appendix C. The
MapReduce procedure of Solution III is the following
procedure.
1) Mapper: The input of mapper is clusters. For
each cluster, we obtain a π c for the cluster c,
c ∈ (1, · · · , C), where C is the number of document clusters, which is the sufficient statistics
for updating θ and ψ.
2) Reducer: The Reduce function adds the value
to the global parameters θ and ψ using the
sufficient statistics as in Eqs. (5), and (6).
3) Driver: In the driver, we update θ and ψ. Note
that there is no need to recompute π, and we
combine all the π c to obtain the final π for
current iteration.
Solution III is an exact solution for TWTM, and it is
equivalent to Solution I when the documents are all
belong to one cluster. However, Solution III provides
a more efficient method than Solution I, and this
depends on the result of document clustering, which
would be anther bottleneck in some real applications.
Although Solution II is an approximate method for
modeling the semi-structured documents, it effectively avoids the bottleneck brought by Solution I
and Solution III. The experiment results in Section 7
show that Solution II works better than Solution I and
Solution III.
It is worth note that all the solutions need to
iterate the MapReduce procedure in driver function
until convergence or maximum number of iterations
is reached. In Section 7, we show the experimental
results about the comparisons of the three solutions
on document modeling and efficiency.

7

E XPERIMENTAL A NALYSIS

7.1

Experiment Settings

In the experiments of this work, we used three
semi-structured corpora. The first document collection
is the data from Internet Movie Database (IMDB).
The data set includes 12,091 movie storylines, 52,274
words after removing stop words, and 3,654 tags.
These movies belong to 29 genres. And the tags we
used contain directors, stars, time, and movie keywords. The second one consists of technical papers of
the Digital Bibliography and Library Project (DBLP)
data set3 , which is a collection of bibliographic information on major computer science journals and proceedings. In this paper, we use a subset of DBLP that
contains abstracts of D=27,435 papers, with W =70,062
words in the vocabulary and L=6,256 unique tags.
The tags we used in DBLP include authors and
3. http://www.informatik.uni-trier.de/∼ley/db/

10

CTM

LDA

TWDA

ATM

TWTM

CorrLDA

CTM

DMR

LDA

TWDA

TWTM

4000
PLDA

8000

TWDA

TWTM

12500

7000

10000

Perplexity

Perplexity

Perplexity

3000

2000

7500

1000

6000

5000

10 20

50

100

150

200

10 20

50

# of Topics

100

150

200

# of Topics

0
1021

2041

# of Topics

(a) TWTM, TWDA, LDA and CTM (b) TWTM, TWDA, DMR, ATM,
CorrLDA, LDA and CTM

(c) TWTM, TWDA and PLDA

Fig. 5. Perplexity results of different models on IMDB corpora. LDA and CTM only use the words when training
in (a), and add the tags as the word features during the training process in (b).

keywords. The last corpus we used contains about
967,012 Wordpress blog posts4 from Kaggle5 . In the
corpus, there are 163,504 tags and 2,592,562 words.
We used this corpus to test the effectiveness and
performance of TWTM over a large scale dataset. We
implemented the three distributed methods of TWTM
using Hadoop 1.1.1 and ran all experiments on a
cluster containing 7 physical nodes; each node has 4
cores and 8 threads, and could be configured to run a
maximum of 7 mappers and 7 reducers of tasks. With
the configuration, we build different scales distributed
environments by setting the maximum of mappers
used in each node.
We have released the codes on GitHub6 including
TWTM, TWDA and the three distributed solutions
using the Hadoop platform.
7.2 Results on Documents Modeling
In order to evaluate the generalization capability of
the model, we use the perplexity score that described
in [9]. For a test set of D documents, the perplexity is:
( P
)
D
d log p(wd )
perplexity = exp −
,
PD
d Nd

where a lower perplexity score represents better document modeling performance.
There are two parts of the experiments. First, We
trained four latent variable models including LDA
[9], CTM [7], TWTM and TWDA, on the corpora of
a set of movie documents in IMDB, to compare the
generalization performance of the four models. In this
part, LDA and CTM trains text data without taking
advantage of tag information. We removed the stop
words and conducted experiments using 5-fold crossvalidation. Figure 5(a) demonstrates the perplexity
results on the IMDB data set. Clearly, TWTM and
TWDA excel both CTM and LDA significantly and
consistently.
4. http://wordpress.com
5. http://www.kaggle.com/c/predict-wordpress-likes/data
6. https://github.com/Shuangyinli

Second, in order to compare the performance of
TWTM and TWDA with other topic models which
take advantage of the tag information, we trained
TWTM, TWDA, DMR7 , PLDA8 , Author Topic Model
(ATM) [31], CorrLDA[6], CTM, and LDA on the set
of movie documents in IMDB and computed the
perplexity on test data set. Since CTM and LDA could
not handle corpus with tags easily, in this experiment,
we treated the given tags as word features for them.
In CorrLDA, we used the tags in each document to
represent the image segments, so that the CorrLDA
can handle the SSDs. Figure 5(b) demonstrates the
perplexity results of the six models on the IMDB
data. The experiment results shows that TWTM and
TWDA are better than the other models, and when
T increases, CorrLDA, DMR, CTM and LDA are
running into over-fitting, while the trend of TWTM
and TWDA keeps going down and the perplexity is
significantly lower than those of the baselines.
As PLDA [30] assumes that one of tags may optionally denote as a tag “latent” present on every
document d, thus, we trained PLDA, TWTM and
TWDA over 1021 and 2041 topics on IMDB data set
with 1020 tags, since in PLDA, each latent topic takes
part in exactly one tag in a collection. As shown in
[30], PLDA builds on Labeled LDA [29], and when it
set one latent topic and one topic for each tag, it is approximately equivalent to Labeled LDA. For this case,
we trained PLDA over 1021 topics. Figure 5(c) shows
the perplexity results of TWTM, TWDA and PLDA.
Note that TWDA has less mean squared error (MSE)
than TWTM. As the results of Figure 5 shown, TWTM
and TWDA both work well compared with the other
topic models which make use of tag information.
7.3

Results on Tags prediction

In this section we use TWDA to demonstrated the
performance of our works on the tags prediction by
7. We used the Mallet code (http://mallet.cs.umass.edu/).
8. We used the code in Stanford Topic Modeling Toolbox
(http://www-nlp.stanford.edu/software/tmt/tmt-0.4/).

1.00

1.00

0.75

0.75

Recall

0.50

TABLE 1
Classification results of different features on F1-score
F1-score
TFIDF
LDA+TFIDF
TWDA
TWDA+TFIDF

0.50

0.25

0.25

ATM

CorrLDA

DMR

TWDA

ATM

CorrLDA

DMR

@1
0.5
0.5
0.57
0.58

@3
0.41
0.42
0.5
0.5

@5
0.39
0.39
0.47
0.47

TWDA

0.00

0.00
100

400

8001000

1500

2000

2500

3000 3300

100

Rank

400

8001000

1500

2000

2500

3000 3300

Rank

(a) K=100

(b) K=200

Fig. 6. Prediction results of TWDA, DMR and ATM for
authors on DBLP corpora. We set the number of topic
in the corpora to be 100 in (a) and 200 in (b).

process the paper collection in DBLP. In addition to
predicting the tags given a document, we evaluate
the ability of the proposed model, compared with
ATM, DMR and CorrLDA, to predict the tags of the
document conditioned on words in the document.
In this part, we treat the authors of each paper as
the tags, and the abstract as the word features, and
we predict the authors of one paper by modeling
the paper abstract document data using ATM, DMR,
CorrLDA, and TWDA. For each model, we evaluate
the likelihood of the authors given the word features
in a document, and rank each possible author by
the likelihood function of the author. First, for each
model, we can get the topic distribution over a test
document dtest given one author a. Then, we evaluate
the p(dtest |a) for dtest over each author a in the
tags(authors) set by
N X
Y
(
p(z|a)p(wi |z)).
p(dtest |a) =
i

z

For CorrLDA, we let authors represent image regions,
and used p(dtest |region) shown in [6] to evaluate
the likelihood of a author given a document. For
DMR and ATM, the method which define p(dtest |a)
is shown as [26]. Note that the likelihoods for a
given author over a document are not necessarily
comparable among the topic models, however, what
we are interested in is the ranking as same as [26].
We trained the three models on DBLP data set using
5-fold cross-validation and shows the recall when the
topic in the corpora is set to be 100 and 200. Results
are shown in Figure 6(a) and Figure 6(b). TWDA ranks
authors consistently higher than the other models.

on tf-idf word features, features induced by a 30topic LDA model and tf-idf word features, features
generated by a TWDA model with the same number
of topics, and features induced by a 30-topic TWDA
model and tf-idf word features respectively.
In these experiments, we conducted multi-class
classification experiments using the IMDB data set,
which contains 29 genres. We calculated the evaluation metrics @1, @3 and @5 with the provided class
tags of movies’ genres, using 5-fold cross-validation.
We report the movie classification performance of the
different methods in Figure 7, where we see that there
is significant improvement in classification performance when using LDA and TWDA comparing with
only using tf-idf features, and TWDA outperforms
both LDA and tf-idf in terms of @1, @3 and @5.
In order to show the classification performance
better, we also calculated the evaluation metrics FMeasure (F1-score). The results of F-Measure is reported in Table 1. TWDA provides substantially better
performance on F-Measure.
7.5

Results on Model Robustness

We demonstrated the performance of our work on
model robustness in this part of experimental analysis. In this part, we measured and compared the
perplexity when we added noise tags information to
the test documents using DBLP data set. Respectively,
we randomly added 20%, 40%, 50%, 80% and 100%
noise tags into a test document and then calculated
the perplexity. For example, if a paper document in
DBLP has five authors, adding 20% noise is that we
randomly selected one author from the author set of
0.8

LDA+TFIDF

TFIDF

TWDA

TWDA+TFIDF

0.7

Precision

Recall

11

0.6

7.4 Results on Feature Construction for Classification
0.5

The next experiment is to test the classification performance utilizing feature sets generated by TWDA and
other baselines. For the base classifier, we use LIBSVM
[13] with Gaussian kernel and the default parameters.
For the purpose of comparison, we trained four SVMs

@1

@3

@5

Fig. 7. Classification results of different features on
@1, @3 and @5 with 5-fold cross-validation.

12

the DBLP corpora and added into the paper as a noise
author.
In some real-world applications, the noise tags appeared in a document may have some relevance to
the real tags. So in this experiment, we selected the
noise tags from the author-tag set to meet the real
applications to some extent. In this experiment, the
DBLP corpora contains more than 6,000 tags, the noise
tags we added into a test document would be very
sparse for the whole tag set in the corpora. So, we
added the different percentages noise tags into the
test document to show the trend of perplexity as
the noise content increases. Figure 8 shows that both
TWDA and ATM have a more steady trend as the
noise level increases, compared with DMR. Table 2
shows some examples about the weights between the
original tags and noise tags. The red tags are the
noise added into the test data, and the values behind
are the weights among the tags we inference from
the TWDA model. Note that, we showed the weight
values after normalized. As the results shown, TWDA
has a good performance on model robustness, for the
weight values of the noise tags are much smaller than
the other original tags. In some applications, we can
use the proposed model to rank the tags given in a
document, which would be a good approach to tag
recommendation and annotation.
7.6 Results on Large-scale Datasets
We demonstrated the performance of the three proposed parallelized solutions of TWTM for a largescale dataset from training time and accuracy on
document modeling, which are suitable for TWDA as
well.
Firstly, we measured and compared the training
time of Solutions I, II and III using the Wordpress blog
data set with the same system setting and model parameters. We used a doc-indexed sparse storage mode
for the matrix of ξ-document, for the matrix would
be very huge over a large scale data set. Figure 9(a)
shows the performance on the average training time

TABLE 2
Some examples of the normalized weights among the
original tags and noise tags. The noise tags are in red,
and the numbers are the corresponding weight values.
“Bug isolation via remote program sampling [24]”
Ben Liblit: 0.185
Alex Aiken: 0.2257
aAlice X. Zheng: 0.228
Michael I. Jordan: 0.349
K. G. Shin: 0.01
“Web question answering: is more always better? [18]”
Susan Dumais: 0.986
Michele Banko: 0.0032
Eric Brill: 0.0038
Jimmy Lin: 0.0038
Andrew Ng: 0.0024
R. Katz: 0.00018
“Contextual search and name disambiguation in email
using graphs [27]”
Einat Minkov: 0.425
William W. Cohen: 0.342
Andrew Y. Ng: 0.128
J. Ma: 0.033
D. Ferguson: 0.07
“A Sparse Sampling Algorithm for Near-Optimal Planning
in Large Markov Decision Processes [22]”
Michael Kearns: 0.296
Yishay Mansour:0.166
Andrew Y. Ng: 0.31
J. Blythe: 0.089
B. Adida: 0.027
P. J. Modi: 0.1
“The nested Chinese restaurant process and bayesian
nonparametric inference of topic [5]”
David M. Blei:0.46
Thomas L. Griffiths:0.186
Michael I. Jordan:0.225
B. Clifford:0.031
R. Szeliski:0.048
X. Wang:0.05

per iteration of the three solutions compared with
the standard TWTM as the baseline, when we set the
number of topic K = 10, 20 and 50 respectively.
Secondly, We sampled the training dataset from
the Wordpress corpus with different sample ratios,
0.1, 0.3, 0.6, 0.8 and 1.0, to show the performance
of running time by different size of training dataset.
In addition, we limited the maximum number of
Mappers in the configuration when we trained the

12500
16000
3000
Solution I

7500

Solution II

10000

Solution III

ATM

5000

DMR

8000

TWDA

ATM

DMR

Perplexity

standalone

Running Time [s]

Perplexity

Perplexity

12000

7500

5000

2500

Solution I
Solution II

TWDA

Solution III

2000
2500

standalone

4000

2500

1500
0
0.0

0.2

0.4

0.5

% of noise

(a) K=100

0.8

1.0

0.0

0.2

0.4

0.5

0.8

1.0

% of noise

(b) K=200

Fig. 8. The Results of adding noise to different models(ATM, DMR and TWDA). (a) set K=100, and (b) set
K=200. Steady trending means a good performance on
model robustness.

10

20

50

# of Topics

(a)

10

20

50

# of Topics

(b)

Fig. 9. (a) The average training time per iteration
for Solution I, II, III with different number of topics
compared with the standard TWTM. (b) The perplexity
results for Solution I, II, III, and the standard TWTM.

13

3000
Solution I

Solution I

Solution II

Solution II

6000

Solution III

Solution III
standalone

Running Time [s]

Running Time [s]

standlone
2000

4000

1000
2000

0
0.1

0.3

0.6

0.8

1.0

0.1

0.3

Sampling Ratio

0.6

0.8

1.0

Sampling Ratio

(a) K = 20

(b) K = 50

Fig. 10. The average training time per iteration on the
Wordpress corpus with different number of sampling
radios for Solution I, II, III.
1200
3000
Solution I

Solution I

Solution II

Solution II

Solution III

1000

Solution III

Running Time [s]

Running Time [s]

2500

800

As described in Section 6, in Solution I, it would
spend a great deal of time on data migration to
update π in Driver process, and in Solution III, a
lot of resources are taken on the clustering process
in each Mapper, especially when the corpus is nonhomogeneous which leads to uneven loading of each
Mapper. While, Solution II avoids these problems by
a approximation method.
Lastly, we measured the generalization capability of
the three solutions using the perplexity and conducted
experiments. We held out 20% of the data for test and
trained the three solutions on the remaining 80%. We
observe that there is relatively little difference among
the solutions compared with the standard TWTM in
terms of perplexity as shown in Figure 9(b) when
the number of topic increases. That is, all the three
solutions are good approximations in terms of model
fitness. It is worthy to note that Solution II has almost
the same performance as Solution I and Solution III.

2000

8
600

C ONCLUSION

1500

6

12

18

24

30

6

12

# of Mappers

18

24

30

# of Mappers

(a) K = 20

(b) K = 50

Fig. 11. The average training time per iteration on the
Wordpress corpus with different number of Mappers for
Solution I, II, III. Note that the horizontal axis repesents
the maximum number of Mappers used in a training
task.

model as described in Section 7.1, to demonstrate
the comparison performance of the three solutions
under the restricted resources. Figure 10 and Figure 11
show the results about the average training time
per iteration of the three solutions using different
sample ratios and Mappers of dataset when training,
by setting the number of topic K = 10, 20 and 50
respectively. From this part of experiments, we find
that Solution II has a better performance of efficiency
than Solution I and III.
Meanwhile, in order to compare with other model,
such as PLDA, we used the Wordpress dataset with
1,000 tags to train a PLDA model with Kl = 1 (we
used the code from Stanford Topic Modeling Toolbox).
We trained TWTM by Solution II with K = 5. Table 3
shows the comparison of PLDA and TWTM by Solution II.

With the tag-weighted topic model proposed in the
paper, we provide and analyze a probabilistic approach for mining semi-structured documents. Meanwhile, three distributed solutions for TWTM are presented to handle the large scale problems. Besides,
TWTM is able to obtain the topics distribution of tags
in the corpus, which is very useful for text classification, clustering and other data mining applications. At
the same time, we propose a novel framework of processing the tagged text with a high extensibility, and
uses a novel function of tag-weighted topic assignment of documents. As an extended model, TWDA
shows the capability on handling the mixture corpora
of semi-structured documents and unstructured documents. The second benefit of the tag-weighted topic
model is that it allows one to incorporate different
types of tags in modeling documents, and provides
a general framework for multi-tag modeling at not
only the level of tags but also the level of documents.
It helps provide a different approach in classification,
clustering, recommendation, and so on. For large scale
semi-structured documents, the proposed solutions
are shown to be effective and efficient for some
complex web applications. In the future, we plan to
apply TWTM to different practical areas (e.g., image
classification and annotation, video retrieval).

R EFERENCES
[1]

TABLE 3
The average training time (second) per iteration for
Solution II and PLDA

[2]
[3]

Sampling radio
PLDA
Solution II

0.1
66.6
77.6

0.3
114.8
88.6

0.6
193.4
104.8

0.8
250.6
116.2

1.0
276.4
120.8

[4]

Christophe Andrieu, Nando de Freitas, Arnaud Doucet, and
Michael I. Jordan. An introduction to mcmc for machine
learning. Machine Learning, 50(1-2):5–43, 2003.
Arthur U. Asuncion, Max Welling, Padhraic Smyth, and
Yee Whye Teh. On smoothing and inference for topic models.
In UAI, pages 27–34, 2009.
Hagai Attias. A variational baysian framework for graphical
models. In NIPS, pages 209–215, 1999.
Christopher M. Bishop and Nasser M. Nasrabadi.
Pattern Recognition and Machine Learning. J. Electronic Imaging,
16(4):049901, 2007.

14

[5]

[6]
[7]
[8]
[9]
[10]
[11]
[12]
[13]
[14]
[15]
[16]
[17]

[18]

[19]
[20]
[21]
[22]

[23]
[24]
[25]
[26]
[27]

[28]

[29]

David M. Blei, Thomas L. Griffiths, and Michael I. Jordan. The
nested chinese restaurant process and bayesian nonparametric
inference of topic hierarchies. J. ACM, 57(2):7:1–7:30, February
2010.
David M. Blei, Michael I, David M. Blei, and Michael I.
Modeling annotated data. In In Proc. of the 26th Intl. ACM
SIGIR Conference, 2003.
David M. Blei and John D. Lafferty. Correlated topic models.
In NIPS, 2005.
David M. Blei and Jon D. McAuliffe. Supervised topic models.
In NIPS, 2007.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent
dirichlet allocation. Journal of Machine Learning Research, 3:993–
1022, 2003.
Jordan L. Boyd-Graber and David M. Blei. Syntactic topic
models. CoRR, abs/1002.4665, 2010.
Andrej Bratko and Bogdan Filipic. Exploiting structural information for semi-structured document categorization. Information Processing and Management, 42(3):679 – 694, 2006.
Deng Cai, Qiaozhu Mei, Jiawei Han, and Chengxiang Zhai.
Modeling hidden topics on document manifold. In CIKM,
pages 911–920, 2008.
Chih-Chung Chang and Chih-Jen Lin. Libsvm: A library for
support vector machines. ACM TIST, 2(3):27, 2011.
Jonathan Chang and David M. Blei. Relational topic models
for document networks. Journal of Machine Learning Research Proceedings Track, 5:81–88, 2009.
Xu Chen, Mingyuan Zhou, and Lawrence Carin. The contextual focused topic model. In KDD, pages 96–104, 2012.
Jeffrey Dean and Sanjay Ghemawat. Mapreduce: simplified
data processing on large clusters. Communications of the ACM,
51(1):107–113, 2008.
Hongbo Deng, Jiawei Han, Bo Zhao, Yintao Yu, and
Cindy Xide Lin. Probabilistic topic models with biased propagation on heterogeneous information networks. In KDD, pages
1271–1279, 2011.
Susan Dumais, Michele Banko, Eric Brill, Jimmy Lin, and Andrew Ng. Web question answering: is more always better? In
Proceedings of the 25th annual international ACM SIGIR conference
on Research and development in information retrieval, SIGIR ’02,
pages 291–298, New York, NY, USA, 2002. ACM.
Thomas L. Griffiths and Mark Steyvers. Finding scientific
topics. In PNAS, pages 449–455, 2004.
Thomas Hofmann. Probabilistic latent semantic indexing. In
SIGIR, pages 50–57, 1999.
Tomoharu Iwata, Takeshi Yamada, and Naonori Ueda. Modeling social annotation data with content relevance using a topic
model. In NIPS, pages 835–843, 2009.
Michael Kearns, Yishay Mansour, and Andrew Y. Ng. A sparse
sampling algorithm for near-optimal planning in large markov
decision processes. Mach. Learn., 49(2-3):193–208, November
2002.
Simon Lacoste-Julien, Fei Sha, and Michael I. Jordan. Disclda:
Discriminative learning for dimensionality reduction and classification. In NIPS, pages 897–904, 2008.
Ben Liblit, Alex Aiken, Alice X. Zheng, and Michael I. Jordan.
Bug isolation via remote program sampling. SIGPLAN Not.,
38(5):141–154, May 2003.
Pierre-Francois Marteau, Gildas Ménier, and Eugen Popovici.
Weighted naive bayes model for semi-structured document
categorization. CoRR, abs/0901.0358, 2009.
David M. Mimno and Andrew McCallum. Topic models
conditioned on arbitrary features with dirichlet-multinomial
regression. In UAI, pages 411–418, 2008.
Einat Minkov, William W. Cohen, and Andrew Y. Ng. Contextual search and name disambiguation in email using graphs.
In Proceedings of the 29th annual international ACM SIGIR
conference on Research and development in information retrieval,
SIGIR ’06, pages 27–34, New York, NY, USA, 2006. ACM.
James Petterson, Alexander J. Smola, Tibério S. Caetano,
Wray L. Buntine, and Shravan Narayanamurthy. Word features for latent dirichlet allocation. In NIPS, pages 1921–1929,
2010.
Daniel Ramage, David Hall, Ramesh Nallapati, and Christopher D. Manning. Labeled lda: A supervised topic model for
credit attribution in multi-labeled corpora. In EMNLP, pages
248–256, 2009.

[30] Daniel Ramage, Christopher D. Manning, and Susan Dumais.
Partially labeled topic models for interpretable text mining. In
Proceedings of the 17th ACM SIGKDD international conference on
Knowledge discovery and data mining, KDD ’11, pages 457–465,
New York, NY, USA, 2011. ACM.
[31] Michal Rosen-Zvi, Thomas L. Griffiths, Mark Steyvers, and
Padhraic Smyth. The author-topic model for authors and
documents. In UAI, pages 487–494, 2004.
[32] Issei Sato and Hiroshi Nakagawa. Rethinking collapsed variational bayes inference for lda. In ICML, 2012.
[33] Markus Tresch, Neal Palmer, and Allen Luniewski. Type
classification of semi-structured documents. In VLDB, pages
263–274, 1995.
[34] Xing Wei and W. Bruce Croft. Lda-based document models for
ad-hoc retrieval. In Proceedings of the 29th annual international
ACM SIGIR conference on Research and development in information
retrieval, SIGIR ’06, pages 178–185, New York, NY, USA, 2006.
ACM.
[35] Jeonghee Yi and Neel Sundaresan. A classifier for semistructured documents. In KDD, pages 340–344, 2000.
[36] Jun Zhu, Amr Ahmed, and Eric P. Xing. Medlda: maximum
margin supervised topic models for regression and classification. In ICML, page 158, 2009.
Shuangyin Li received the Master degree in
School of Information Science and Technology, Sun Yat-sen University, China, in 2011.
During the Master’s program, he focused on
the research of large scale image retrieval
system on Hadoop platform. Currently, he is
active within the field of Text Mining and Artificial Intelligence, and continues his research
in a PhD track at Sun Yat-sen University. His
PhD research focuses on Topic Model and
Deep Neural Networks, and he has published
several research mainly focused on the semi-structured documents
modeling.
Jiefei Li received the Bachelor’s degree
in Department of Computer Science, Sun
Yat-sen University, in 2011. Currently, he is
studying for a master’s degree in Sun Yatsen University. His research focuses on Topic
Model.

Guan Huang received the Bachelor’s degree
in Department of Computer Science, Sun
Yat-sen University, in 2009. Currently, he is
studying for a master ’s degree in Sun Yatsen University in the filed of word embedding
and topic model, learning to rank.

Ruiyang Tan is studying for a Bachelor’s
degree in Department of Computer Science,
Sun Yat-sen University. He has participated
in ACM/ICPC twice times and won two Asia
regional champions.

Rong Pan received the BSc and PhD degrees in applied mathematics from Sun Yatsen University, China, in 1999 and 2004,
respectively. He was a postdoctoral fellow
at the Hong Kong University of Science
and Technology (2005 2007) and HP Labs
(2007 2009). Since then, he has been a faculty member of Department of Computer Science in Sun Yat-sen University. His research
interest includes text mining, recommender
systems, data mining, and machine learning.

15

A PPENDIX A
TAG -W EIGHTED TOPIC M ODEL
In the topic models, the key inferential problem that we need to solve is to compute the posterior distribution
of the hidden variables given a document d. Given the document d, we can easily get the posterior distribution
of the latent variables in the proposed model, as:
p(εd , z|wd , T d , θ, η, ψ, π) =

p(εd , z, wd , T d|θ, η, ψ, π)
.
p(wd , T d |θ, η, ψ, π)

Integrating over ε and summing out z, we easily obtain the marginal distribution of d:
d

d

d

p(w , T |η, θ, ψ, π) = p(t |η)

Z

N X
K
 Y
p(zid |(εd )T × T d × θ)p(wid |zid , ψ1:K ) dεd .
p εd |(T d × π) ·
i=1 z d =1
i

In this work, we make use of mean-field variational EM algorithm to efficiently obtain an approximation
of this posterior distribution of the latent variables. In the mean-field variational inference, we minimize
the KL divergence between the variational posterior probability and the true posterior probability through
by maximizing the evidence lower bound (ELBO) L(·). For a single document d, we obtain the L(·) using
Jensen’s inequality:
L(ξ1:ld , γ1:K ; η1:L , π1:L , θ1:L , ψ1:K ) = E[log p(T1:ld |η1:L )] + E[log p(εd |T d × π)]
+

N
X

d T

d

E[log p(zi |(ε ) × T × θ)] +

i=1

N
X

E[log p(wi |zi , ψ1:K )] + H(q),

i=1

where ξ is a ld -dimensional Dirichlet parameter vector and γ is 1 × K vector, both of which are variational
parameters of variational distribution. H(q) indicates the entropy of the variational distribution:
H(q) = −E[log q(εd )] − E[log q(z)].
Here the exception is taken with respect to a variational distribution q(εd , z1:N ), and we choose the following
fully factorized distribution:
q(εd , z1:N |ξ1:L , γ1:K ) = q(εd |ξ)

N
Y

q(zi |γi ),

i=1

where the dimension of parameter ξ is changed with different documents.
In the L(·),
d T

d

E[log p(zi |(ε ) × T × θ)] =

K
X

γik E[log((εd )T × T d × θ)k ].

k=1

To preserve the lower bound on the log probability, we upper bound the log normalizer in E[log((εd )T ×
T × θ)k ] using Jensen’s inequality again:
d

d

E[log((ε ) × T × θ)k ] = E[log

l
X

d

d

d

d T

(i)
εdi θk ]

≥ E[

l
X

εdi

(i)
log θk ]

(i)

log θk E[εdi ],

i=1

i=1

i=1

=

l
X

where the expression of θ(i) , i ∈ {1, · · · , ld }, means the i-th tag’s topic assignment vector, corresponding to
the i-th row of Θd . Note that the expectation of Dirichlet random variable is E[εdi ] = Pldξi .
j=1

Thus, for the document d,
N
X
i=1

d T

d

E[log p(zi |(ε ) × T × θ)] =

N X
K
X

i=1 k=1

d

γik ·

l
X
j=1

ξj
(j)
log θk Pld

′
j ′ =1 ξj

.

ξj

16

Finally, we expand L(·) in terms of the model parameters (η, π, θ, ψ) and the variational parameters (ξ, γ)
as follows:
L
X

L(ξ, γ; η, π, θ, ψ) =

(tdl log ηld + (1 − tdl ) log(1 − ηld ))

l=1



ld
ld
ld
ld
X
X
X
X

ξj )
((T d × π)i − 1) Ψ(ξi ) − Ψ(
log Γ (T d × π)i +
(T d × π)i ) −
+ log Γ(
+

γik

i=1 k=1

ld
X
j=1

ld
X

− log Γ(

ξi ) +

ξj
(j)
log θk Pld

j =1 ξj
′

ld
X

log Γ(ξi ) −

+
′

ld
X
i=1

i=1

i=1

j=1

i=1

i=1

i=1

N X
K
X

N X
K X
V
X

γik (wd )ji log ψkj

i=1 k=1 j=1



d

(ξi − 1) Ψ(ξi ) − Ψ(

l
X
′

j =1



ξj ) −
′

N X
K
X

d
d
γik
log γik
.

i=1 k=1

Then, we maximize the lower bound L(ξ, γ; η, π, θ, ψ) with respect to the variational parameters ξ and γ,
using a variational expectation-maximization(EM) procedure as follows.
A.1 Variational E-step
A.1.1

ξ

We first maximize L(·) with respect to ξi for the document d. Maximize the terms which contain ξ:


L
ld
ld X
N X
K
ld
ld
X
X
X
X
X
ξ
j
(j)
d
(
πl′ Til′ − 1) Ψ(ξi ) − Ψ(
log θk Pld
ξj ′ ) +
L[ξ] =
γik ·
ξi )
− log Γ(
′
′
ξ
′
i=1 l′ =1
i=1
j=1
i=1
k=1
j
j
=1
j =1


d
d
ld
l
l
X
X
X
(ξi − 1) Ψ(ξi ) − Ψ(
ξj ′ ) ,
log Γ(ξi ) −
+
i=1

i=1

j ′ =1

where Ψ(·) denotes the digamma function, the first derivative of the log of the Gamma function, and (T d × π)i
Pld PL
= i=1 l=1 πl Tild . The derivative of L[ξ] with respect to ξi is
′

′

L (ξi ) = Ψ (ξi )(

L
X



Pd
(i) P d
(j)
N X
ld X
K
L
X
X
log θk ( lj=1 ξj ) − lj=1 log θk ξj
d
d
.
γi′ k · 
− ξi ) − Ψ (
πl Til − ξi ) +
ξj ) ·
(
2
Pl d
′
′
ξ
j=1
i=1 l=1
k=1
′
j
i =1
j =1
d

πl Tild

l=1

′

l
X

Here we use gradient descent method to find the ξ to make the maximization of L[ξ] .
A.1.2

γ

Next, we maximize L(·) with respect to γik . Adding the Lagrange multipliers to the terms which contain γik ,
we get the following equation:
L[γ] =

N X
K
X

i=1 k=1

d

γik

l
X
j=1

ξj
(j)
log θk Pld

′
j ′ =1 ξj

+

N X
K X
V
X

γik (wd )ji log ψkj −

i=1 k=1 j=1

N X
K
X

i=1 k=1

d
d
γik
log γik
+

N
X
i=1

K
X
λi (
γik − 1).
k=1

By taking the derivative with respect to γik , and setting the derivative to zero yields, we obtain the update
equation of γik :
ld
X
ξj
(j)
log θk Pld
γik ∝ ψk,vwi exp{
},
j=1
j ′ =1 ξj ′
where v wi denotes the index of wi in the dictionary.

A.2 M-step estimation
The M-step needs to update four parameters: η, the tagging prior probability, π, the Dirichlet prior of the tags’
weights, θ, the topic distribution over all tags in the corpus, and ψ, the probability of a word under a topic.

17

A.2.1 η
For a given corpus, the ηi is estimated by adding up the number of ith label which appears in all documents. It
does not depend any parameter in the proposed model, except itself. By maximizing the terms which contain
η, we have
PD d
t
ηl = d l ,
D
where D is the size of corpus. Because each document’s tags-set is observed, the Bernoulli prior η is unused,
which is included for model completeness.
A.2.2 π
For the document d, the terms that involve the Dirichlet prior π:
L[π]



ld
ld
ld
ld
X
X
X
X


ξj ) .
(T d × π)i − 1 Ψ(ξi ) − Ψ(
log Γ (T d × π)i +
(T d × π)i ) −
= log Γ(
j=1

i=1

i=1

i=1

We use gradient descent method by taking derivative of L[π] with respect to πl on the whole corpus to find
the estimation of π. Taking derivatives with respect to πl on the corpus, we obtain:


ld
D X
ld
L
D X
ld
ld
D
ld X
L
X
X
X
X
X
X
X
′
Ψ(ξi ) − Ψ( ) · Tild .
Ψ(
πl′ · Tild′ ) · Tild +
Tild −
Ψ(
πl′ · Tild′ ) ·
L[πl ] =
d=1

d=1 i=1

i=1

i=1 l′ =1

d=1 i=1

l′ =1

j=1

A.2.3 θ
The only term that involves θ is:
L[θ] =

K
N X
D X
X

d

γik

d=1 i=1 k=1

l
X
j=1

ξj
(j)
log θk Pld

′
j ′ =1 ξj

,

where ξj , j ∈ {1, · · · , ld } in the document d needs to be extended to tdl · ξld , l ∈ {1, · · · , L} for convenient to
simplify L[θ] . With the Lagrangian of the L[θ] , which incorporate the constraint that the K-components of θl
P
PK
sum to one, adding L
l=1 λl (
k=1 θlk − 1) to L[θ] , taking the derivative with respect to θlk , and setting the
derivative to zero yields, we obtain the estimation of θ over the whole corpus,
θlk ∝

D X
N
X

ξld tdl
d
γik
.
PL
d d
l=1 (ξl tl )
d=1 i=1

A.2.4 ψ
To maximize with respect to ψ, we isolate corresponding terms and add Lagrange multipliers:
L[ψ] =

D X
N X
K X
V
X

γik (wd )ji log ψkj +

d=1 i=1 k=1 j=1

K
X

k=1

Take the derivative with respect to ψkj , and set it to zero, we get:
ψkj ∝

D X
N
X
d=1 i=1

d
γik
(wd )ji .

v
X
ψkj − 1).
λk (
j=1

18

A PPENDIX B
TAG -W EIGHTED D IRICHLET A LLOCATION
In TWDA, we treat π, µ, η, θ and ψ as unknown constants to be estimated, and use a variational expectationmaximization (EM) procedure to carry out approximate maximum likelihood estimation as TWTM. Given the
document d, we can easily get the posterior distribution of the latent variables in the TWDA model, as:
p(εd , λd , z, wd , T d|θ, η, ψ, π, µ)
.
p(wd , T d |θ, η, ψ, π, µ)

p(εd , λd , z|wd , T d , θ, η, ψ, π, µ) =

As with TWTM, it is not efficiently computable. We maximize the evidence lower bound(ELBO) L(·)using
Jensen’s inequality, and for a document d we have the form:
L(ξ1:ld , γ1:K , ρ1:K ; η1:L , π1:L , µ1:K , θ1:L , ψ1:K ) = E[log p(T1:ld |η1:L )] + E[log p(εd |T d × π)] + E[log p(λd |µ)]
N
X
θ
E[log p(wi |zi , ψ1:K )]
E[log p(zi |(ε ) × T × ( ))] +
+
λ
i=1
i=1
N
X

d T

d

+ H(q),

d

where ξ is a l -dimensional Dirichlet parameter vector, ρ is a 1 × K vector and γ is 1 × K vector, all of which
are variational parameters of variational distribution. Unlike the TWTM, ld in TWDA is one more than the
number of the observed tags in the document d. H(q) indicates the entropy of the variational distribution:
H(q) = −E[log q(εd )] − E[log q(λ)] − E[log q(z)].
Here the exception is taken with respect to a variational distribution q(εd , q(λd ), z1:N ), and we choose the
following fully factorized distribution:
d

d

d

d

q(ε , λ , z1:N |ξ1:L , ρ1:K , γ1:K ) = q(ε |ξ)q(λ |ρ)

N
Y

q(zi |γi ).

i=1

The term of the expected log probability of the topic assignment:
K

X
θ
θ
E[log p(zi |(εd )T × T d × ( ))] =
γik E[log((εd )T × T d × ( ))k ],
λ
λ
k=1

which could be difficult to compute, because of tag-weighted topic assignment which is used in TWDA. Thus
we use Jensen’s inequality:
d

lX
−1
θ
(i)
εdi θk + εdld λk )]
E[log((εd )T × T d × ( ))k ] = E[log(
λ
i=1

≥ E[

d
lX
−1

(i)

εdi log θk + εdld · log λk ]

i=1

=

d
lX
−1

(i)

log θk E[εdi ] + E[εdld · log λk ],

i=1

(i)

d

where the expression of θ , i ∈ {1, · · · , l − 1}, means the i-th tag’s topic assignment vector, corresponding
to the i-th row of Θd .
because the variational distribution is fully factorized, so we can get:
d

lX
−1
θ
(i)
E[log((ε ) × T × ( ))k ] =
log θk E[εdi ] + E[εdld ] · E[log λk ],
λ
i=1
d T

where

d

d

E[εdld ]

= ξld /

l
X

ξj ,

j=1

E[log λk ] = Ψ(ρk ) − Ψ(

K
X
′

j =1

ρj ′ ).

19

ξi
P ld

With E[εdi ] =

j=1

ξj

, Thus, for the document d,
d

N
X

K
N X
K
lX
−1
X
X
θ
ξj
ξd
(j)
E[log p(zi |(εd )T × T d × ( ))] =
log θk Pld
].
+ (Ψ(ρk ) − Ψ(
ρj ′ )) Pldl
γik · [
λ
′
ξ
′
j
i=1
i=1 k=1
j=1
j ′ =1 ξj
j=1
j =1

Then we expand the L(·) of TWDA as follows:
L(ξ, γ, ρ; η, π, µ, θ, ψ) =

L
X

(tdl log ηld + (1 − tdl ) log(1 − ηld ))

l=1



ld
ld
ld
ld
X
X
X
X
ξj )
((T d × π)i − 1) Ψ(ξi ) − Ψ(
log Γ((T d × π)i ) +
(T d × π)i ) −
+ log Γ(
j=1

i=1

i=1

i=1





K
K
K
K
X
X
X
X
ρdj )
(µi − 1) Ψ(ρdi ) − Ψ(
log Γ(µi ) +
µj ) −
+ log Γ(

+

N X
K
X

d

γik ·

i=1 k=1

− log Γ(

ξi ) +

ξj
(j)
Ck Pld
j

ld
X
i=1

i=1

−

l
X
j=1

ld
X

N X
K
X

i=1

i=1

j=1

′

′
=1 ξj

log Γ(ξi ) −

+

N X
K X
V
X

j=1

γik (wd )ji log ψkj

i=1 k=1 j=1

ld
X
i=1



d

(ξi − 1) Ψ(ξi ) − Ψ(

l
X
′

j =1



ξj ′ )

γik · log γik

i=1 k=1



K
K
K
K
X
X
X
X
ρj ) .
(ρi − 1) Ψ(ρi ) − Ψ(
log Γ(ρi ) −
ρj ) +
− log Γ(
i=1

j=1

where

(j)
Ck

and

j=1

i=1

(
(j)
log θk
=
PK
Ψ(ρk ) − Ψ( j ′ =1 ρj ′ )
(T d × π)i =

L+1
X

j ∈ {1, · · · , ld − 1}
,
j = ld
πl Tild.

l=1

B.1 Variational E-step
For a single document d, the variational parameters include ξ d , ρd and γik . First, we maximize L(·) with
respect to the variational parameters to obtain an estimate of the posterior.
B.1.1 Optimization with respect to ξ
We first maximize L(·) with respect to ξi for the document d. Maximize the terms which contain ξ:


ld
ld L+1
N X
K
ld
ld
X
X
X
X
X
X
ξj
(j)
d


′
′
(
πl Til′ − 1) Ψ(ξi ) − Ψ(
Ck Pld
ξj ) +
L[ξ] =
γik ·
ξi )
− log Γ(
′
i=1 l′ =1
i=1 k=1
j=1
i=1
j ′ =1 ξj
j ′ =1


ld
ld
ld
X
X
X
(ξi − 1) Ψ(ξi ) − Ψ(
ξj ′ ) ,
log Γ(ξi ) −
+
i=1

i=1

j ′ =1

The derivative of L[ξ] with respect to ξi is
′

′

L+1
X

L (ξi ) = Ψ (ξi )(

l=1



P ld
(j)
(i) Pld
N X
K
ld L+1
ld
X
X
X
X
C
ξ
ξ
)
−
C
(
j
j
j=1 k
j=1
k
.
(
πl Tild − ξi ) +
γid′ k · 
ξj )
πl Tild − ξi ) − Ψ (
P ld
2
′
(
)
ξ
′
i=1 l=1
j=1
j ′ =1 j
i =1 k=1
′

Here we use gradient descent method to find the ξ to make the maximization of L[ξ] .

20

Optimization with respect to ρ

B.1.2

Next, we maximize L(·) with respect to ρ. The terms that involve the variational Dirichlet ρ are:




K
K
K
K
K
K
X
X
X
X
X
X
ρj )
(ρi − 1) Ψ(ρi ) − Ψ(
log Γ(ρi ) −
ρj ) +
ρj ) − log Γ(
(µi − 1) Ψ(ρi ) − Ψ(
L[ρ] =
+

K X
N
X
k=1 i=1

ξd
γik · Pldl

j=1 ξj



K
X

· Ψ(ρk ) − Ψ(

This simplifies to:


K
K
X
X
Ψ(ρi ) − Ψ(
ρj ) ·
L[ρ] =

j=1



ρj ) .

µi − ρ i +

N
X

n=1

j=1

i=1

j=1

i=1

i=1

j=1

j=1

i=1

ξld

γni · Pld

j=1 ξj

!

K
K
X
X
log Γ(ρi ).
ρj ) +
− log Γ(
j=1

i=1

Taking the derivative with respect to ρi and setting it to zero, we obtain a maximum at:
ρ i = µi +

N
X

n=1

ξd
γni · Pldl

j=1 ξj

.

Optimization with respect to γ

B.1.3

The terms that contain γ are:
L[γ] =

N X
K
X

i=1 k=1

d

γik

l
X
i=1

ξi
(i)
Ck · Pld

j=1 ξj

+

N X
K X
V
X

γik wij log ψk,vwi −

N X
K
X

γik · log γik

i=1 k=1

i=1 k=1 j=1

Adding the Lagrange multipliers to the terms which contain γik , taking the derivative with respect to γik ,
and setting the derivative to zero yields, we obtain the update equation of γik :
d

γik ∝ ψk,vwi exp{

l
X
j=1

ξj
(j)
Ck Pld

j ′ =1 ξj ′

},

where v wi denotes the index of wi in the dictionary.
In E-step, we update the ξ, ρ and γ for each document with the initialized model parameters.
B.2 M-step estimation
The M-step needs to update five parameters: η, the tagging prior probability, π, the Dirichlet prior of the tags’
weights, θ, the topic distribution over all tags in the corpus, ψ, the probability of a word under a topic, and
µ, a Dirichlet prior of model. It is worthy to note that we update η with the same method as in TWTM.
Optimization with respect to π

B.2.1

For the document d, the terms that involve the Dirichlet prior π:
L[π]



ld
ld
ld
ld
X
X
X
X
ξj ) ,
((T d × π)i − 1) Ψ(ξi ) − Ψ(
log Γ((T d × π)i ) +
(T d × π)i ) −
= log Γ(
j=1

i=1

i=1

i=1

PL+1
where (T d × π)i = l=1 πl Tild . We use gradient descent method by taking derivative of L[π] with respect to
πl on the corpus to find the estimation of π. Taking derivatives with respect to πl on the whole corpus, we
obtain:


ld
D X
ld
L+1
D X
ld
ld
D
ld L+1
X
X
X
X
X
X
X
X
′
Ψ(ξi ) − Ψ(
ξj ) · Tild .
Ψ(
πl′ · T d′ ) · Tild +
Tild −
L[π ] =
Ψ(
πl′ · T d′ ) ·
il

il

l

d=1

i=1 l′ =1

i=1

d=1 i=1

l′ =1

d=1 i=1

j=1

21

B.2.2 Optimization with respect to θ
The only term that involves θ is:
L[θ] =

K
N X
D X
X

d

γik

d=1 i=1 k=1

l
X
j=1

ξj
(j)
log θk Pld

′
j ′ =1 ξj

,

where ξj , j ∈ {1, · · · , ld } in the document d needs to be extended to tdl · ξld , l ∈ {1, · · · , L + 1} for convenient
to simplify L[θ] . With the Lagrangian of the L[θ] , which incorporate the constraint that the K-components of
θl sum to one, we obtain the estimation of θ over the whole corpus,
θlk ∝

D X
N
X

ξld tdl
d
γik
PL+1 d d .
l=1 (ξl tl )
d=1 i=1

B.2.3 Optimization with respect to ψ
To maximize with respect to ψ, we isolate corresponding terms and add Lagrange multipliers:
L[ψ] =

D X
N X
K X
V
X

γik (wd )ji log ψkj +

d=1 i=1 k=1 j=1

K
X

k=1

v
X
ψkj − 1).
λk (
j=1

Take the derivative with respect to ψkj over the whole corpus, and set it to zero, we get:
ψkj ∝

D X
N
X

d
γik
(wd )ji .

d=1 i=1

B.2.4 Optimization with respect to µ
For the Dirichlet parameters µ, the involved terms are:


K
K
K
D
K
X
X
X
X
X
log Γ(
ρdj )) .
(µi − 1)(Ψ(ρdi ) − Ψ(
log Γ(µi ) +
µj ) −
L[µ] =
d=1

i=1

j=1

j=1

i=1

Taking the derivative with respect to µi gives:




D
K
K
X
X
X
′
Ψ(ρdi ) − Ψ(
ρdj )
µi ) − Ψ(µi ) +
L[µi ] = D Ψ(
j=1

d=1

j=1

We can invoke the linear-time Newton-Raphson algorithm to estimate µ as same as in LDA.

22

A PPENDIX C
C LUSTER A LGORITHM

IN

S OLUTION III

As shown in Eq. 4, πl , l ∈ (1, · · · , L) is only associated with the documents who contain the lth tag. Thus, before
running TWTM, we can cluster the documents into several clusters with the condition that the documents
which contain the same tags should be in the same cluster. It means that the documents are divided into the
mutually independent space by the tags. We show a simple example as shown in Figure 12, left panel.

d1
d2

t1

t2

t3

1

1

1
1

t4

t6

t7

Cluster 1

1
π

d3

1

d4
d5

t5

1
1

Cluster 2
b

Cluster c
b

b

1

L

1

1

Fig. 12. Left: An example of the clustering result. Each row represents a document d in a corpora D, and Each
column represents a tag t. Dij = 1 means that tj is given in di . The documents in the red circle belong to one
cluster, and the documents in the blue circle belong to another cluster. Right: The illustration to update π by
combine the different parts.
After document clustering, the tags contained in one cluster are not appeared to any other clusters. In this
case, we could assign each cluster to different computed nodes. When update the π, we just simply combine
the π c where c ∈ (1, · · · , C) and C is the number of document clusters, just as shown as in Figure 12, right
panel. We show the cluster process of Solution III in Algorithm 2.
Algorithm 2 The cluster process of Solution III
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:

Input: a semi-structured corpora D = {(w1 , t1 ), . . . , (wM , tM )} and the tag set T of the corpora.
Output: a cluster set C that contains all the clusters, and each cluster c in C contains a set of documents.
create a cluster set C = {}.
create a document cluster c = {}.
create pre added docs = {} to store the documents which are ready to add into cluster c.
create a tag set scanned tags = {} to store the tags which have been scanned.
add c into C.
for each tag t in T do
if t is not in scanned tags then
add t into scanned tags;
create a new cluster c, and add c into C;
else
continue;
end if
add the documents which own t into pre added docs;
repeat
for each d in the pre added docs do
if d is not in c then
add d into c;
end if
for each tag td in d do
add td into scanned tags;
add the documents which have td and not in c into pre added docs;
end for
remove d from pre added docs;
end for
until pre added docs is empty.
end for
return C

2009 11th IEEE International Symposium on Multimedia

A Network-assisted Scheme for Bandwidth Allocation Among Video Streams
Mythili Suryanarayana Prabhu, Xiaoqing Zhu, Rong Pan, Vijay Subramanian and Flavio Bonomi
Advanced Architecture & Research Group, Cisco Systems Inc., San Jose, CA 95134, U.S.A.
{mysuryan, xiaoqzhu, ropan, vijaynsu, flavio}@cisco.com

Abstract

echo max virtual
congestion level

We present a network-assisted scheme for media-aware
bandwidth sharing among multiple video streaming sessions. Departing from the conventional paradigm of fairrate allocation among data traffic flows, our scheme allocates the bottleneck bandwidth among the video streams according to their rate-distortion (R-D) characteristics, with
the objective of minimizing the total video distortion of
all streams. This demonstration shows that our scheme
achieves the optimal rate allocation with fast convergence,
efficient bottleneck utilization, and balanced video qualities
among the competing streams.

1

video rate
adaptation

virtual congestion level
update
video sender

sharing scheme.

achieving a target utilization at the bottleneck link. This can
be formally expressed as:
P
min
(1)
i di (ri )
{ri }
P
s.t. yl = i:l∈i ≤ γcl
(2)

Introduction

where di (ri ) denotes the R-D tradeoff function of a video
stream i. The capacity of the bottleneck is denoted as cl ;
the total rate over the link is yl , comprising of the rates of
all video streams traversing that link; the target utilization γ
is chosen to be slightly less than unity.
Figure 1 provides an overview of our network-assisted
media-aware bandwidth sharing scheme. Each video packet
carries a header field into which a network node can insert
its congestion information, virtual congestion level. This
field is initialized to zero at the sender and can be modified by any network node if its congestion level is greater
than the one already in the header. By the time a packet
reaches the receiver, its header carries the maximum congestion level along the forward path. The receiver then
echoes this information back to sender in the video acknowledgement (ACK) packet header. The congestion information is then used to calculate the optimal rate based
on the video R-D parameters. Note that the intermediate
network nodes are oblivious of the video R-D information
while each video sender only needs the end-to-end maximum virtual congestion level along the path of its stream.

Scheme Description

The goal of our media-aware bandwidth sharing algorithm is to minimize the total distortion of all streams, while
978-0-7695-3890-7/09 $26.00 © 2009 IEEE
DOI 10.1109/ISM.2009.75

video receiver

Figure 1. Overview of the network-assisted bandwidth

As video traffic increases in the Internet and competes
for limited bandwidth resources, it is important to design
bandwidth sharing schemes that account for video characteristics, beyond the traditional paradigm of fair-rate allocation among data flows. Ideally, when multiple video streams
compete for a shared network bandwidth, it is desirable that
their allocated rates reflect their respective rate-distortion
(R-D) characteristics. For instance, a video stream containing scenes from a dynamic action movie needs a higher
rate than a stream containing static head-and-shoulder news
clips to achieve the same visual quality.
We present a novel scheme for media-aware bandwidth
sharing among multiple video streaming sessions. The proposed scheme leverages explicit congestion feedback from
the network to aid the sender of each stream in calculating its optimal rate allocation for each video stream. Our
demonstration shows that the proposed scheme is simple to
implement in a real system, and achieves desirable visual
qualities for each video streaming session.

2

network node

442

Standard header fields

Rate (Mbps)

stream 1: City
stream 2: Mother & Daughter

2

1

0
0

10

20

30
Time (s)

40

50

(a)

Extension header fields

60

15
Virtual Congestion Level
(MSE/Mbps)

3

bit
0-1 2 3 4-7 8 9-15
16-31
offset
0
V=2 P X CC M PT
sequence number
32
timestamp
64
synchronization source (SSRC) identifier
96
contributing source (CSRC) identifiers
…
…
profile specific identifier
length
virtual congestion level

10

5

0
0

10

20

30
Time (s)

40

50

60

(b)

Figure 3. Recorded traces of (a) allocated video rates
Figure 2. RTP header extension.

and (b) virtual congestion level at the bottleneck link.

Details of algorithms steps are described in [1], where we
also prove the optimality and stability of the system.

3

Demonstration

We implement the network node as a Click Router module [2] hosted in a Linux environment. It continuously estimates the traversing traffic rate, and periodically updates the
virtual congestion level. Upon relaying each video packet,
it stamps the updated virtual congestion level into the an
RTP extension header field, if its value is greater than the
existing value in the header field.
The video sender and receiver are implemented as application agents in Linux. The receiver extracts the value
of maximum virtual congestion level along the path, and
reports such information in the header of an acknowledgment packet. The sender then adjust the outgoing rate of the
video stream accordingly, by dynamically tuning the interval of adjacent packet transmissions. For the purpose of rate
adaptation, each video sequence is pre-encoded into multiple quality versions using x264 [3], a fast implementation
of the H.264/AVC standard [4].
In our demonstration scenario, two different streams,
City and Mother & Daughter, share a bottleneck link with
a capacity of 2.5Mbps. The target utilization is chosen at
95%. City starts streaming at time t = 0 second and Mother
& Daughter starts at time t = 18 second. Figure 3 shows
the recorded traces of allocated rates at the senders, as well
as traces of the virtual congestion level at the network node.
Initially when only the City sequence is present in the network, its allocated rate stabilizes at 2.35 Mbps; with the
virtual congestion level stabilizing at a corresponding value.
Immediately after the Mother & Daughter stream enters the
network, the virtual congestion level at the network node
quickly increases to a new equilibrium, leading to decreased
allocated rate for City and increased allocation for the new
stream. Figure 4 shows the visual quality of both sequences,
before and after the allocation has converged. It can be
noted that while the rate reduction of 0.5 Mbps from City introduced negligible reduction in its visual quality, it greatly
improves the quality of the new Mother & Daughter stream.

(a) before convergence

(b) after convergence

Figure 4. Visual quality of the two competing video
streams, before and after the rate allocation has converged.

4

Conclusions

We have demonstrated a network-assisted, media-aware
sharing scheme for video streaming. Our results show
that the bottleneck bandwidth is shared among competing
streams in a media-aware fashion, achieving minimum total video distortion of all streams; and the scheme adapts
to transient events with fast convergence, while avoiding
steady-state queuing delays or persistent packet losses.

References
[1] X. Zhu, R. Pan, N. Dukkipati, V. Subramania, and
F. Bonomi, “Layered internet video engineering (LIVE):
Network-assisted bandwidth sharing and transient loss
protection for scalable video streaming.” [Online]. Available:
https://sites.google.com/site/liveexternal/
[2] [Online]. Available: http://read.cs.ucla.edu/click/
[3] [Online]. Available: http://developers.videolan.org/x264.html
[4] Advanced Video Coding for Generic Audiovisual services,
ITU-T Recommendation H.264 - ISO/IEC 14496-10 (AVC),
ITU-T and ISO/IEC JTC 1, 2003.

443

Approximate Fairness Through Differential Dropping

(summary)
Rong Pan
Stanford University

Lee Breslau
AT&T Labs-Research

Balaji Prabhakar
Stanford University

Scott Shenker
ACIRI

1

J 0.,

|

i

i

:

:

i

! ............. i . . . . . . . . .

.........

i

'

......

........

! .......

i
....

- - / - L f

0.6

°-'

.......

................. r

$00-

, .....
. . . . .

.............

o., ......
o

1e-g6

-

....................
,

':.......
'

18-05

......

i: . . . . . . . . . . . . . . .

! ........... [/

100

...... I.............

_~_._-~.::~"~

, " ~ s ..........

0.0001
0.001
0.01
Fr~tlnn nf 1,-SecondR m s

0.1

R.BD

APID-SB

~D-FT

Fig. 2. MbtedTraffic - throughput

Fig. 1. ComplementaryDislfilmtionof 1-Se.condRates
Many researchers have argued that the Internet architecture would be
more robust and more accommociating of heterogeneity if reuters allocated bandwidth fairly. Most of the mechanisms proposed to accomplish this fall into two general categories, each with their own drawbacks. The first category, which includes Fair Queueing (FQ) and its
many variants, involves complicated packet scheduling algorithms that
may not be inexpensively implementable at extremely high speeds. The
algorithms in the second category, active queue management schemes
with modification to improve fairness (e.g., FRED, SFB), am easy to
implement and are much fairer than the original RED design, but are
not intended to provide max-rain fairness among a large population of
flows. Our research focuses on the design of a router mechanism that
achieves approximately fair bandwidth allocations with relatively low
implementation complexity.t
There are three basic goals of our design. First, the resulting bandwidth allocations should be approximately max-rain fair. We evaluate fairness over relatively long time scales, on the order of several
roundtrip times. Second, the design should be easily implemeatablc at
high speeds. We limit ourselves to FIFO packet scheduling algorithms
with probabilistic drop-on-arrival, and use only a small amount of state
(compared to the packet buffers) to make these dropping decisions. Finally, the algorithm must have active queue management.
To achieve these goals, we propose an algorithm called Approximate
Fair Dropping (AFD). It uses a FIFO queue with probabilistic drop-onarrival like RED. However, these dropping decisions are based not only
on the queue size but also on the flow's current sending rate ri. Under
congestion, a flow's packet is discarded with a probability de = ( I -r/=;,
)+, which limits each flow's throughput to the fair share ryalr.
r;
Thus, dropping is not applied uniformly across flows but is applied
differentially based on an estimate of a flow's current sending rate. The
fair rate r/=i, is estimated implicitly based on the length of the FIFO
queue, and so requires little state or complexity. Providing reasonably
accurate estimates of the flow rates ri is the key technical challenge.
The state required to accurately estimate each flow's rate is quite
large, growing linearly with the number of flows, and thus is infeasible.
However, we only need to keep state for those flows that are sending
at or above the fair share, since those are the only flows whose packets
will be dropped. Recent trace data has suggested that the distribution
of flow rates is lung-tailed and that most bytes are sent by relativelyfast
I A J~Cent paper de.~libea the RED-PD algmithm, which has similar but not ideafical goals In what we
pmpese here.

ACM SIGCOMM

~

72

o.2-

i
O.15

0
~

A.~D-SIi

A.I=D.-Fr

Fig. 3. MixedTraffic- drop probability

flows. For example, Figure I shows the cumulative distributions of the
1-second flow rates for three traces; in these datasets, 10% of the flows
represent between 60% to 90% of the total bytes. Thus, a sampling
of the recent traffic will be heavily dominated by the faster flows and,
in typical cases, these are the only flows at or above the fair share.
Our design uses a sampling of recently arrived packets to estimate the
flow rates. The state required is roughly proportional to the number
of fast flows, not the total number of flows, and thus is much more
manageable. Back-of-the-envelope calculations indicate that this state
will be much less than that already kept in the packet buffers.
A direct implementation of A F D algorithm, which we refer to as
AFD-SB, requires two data structures: a shadow buffer which stores
the recent history of all packet arrivals (header only) and a flow table
which keeps the packet count of each flow that appeared in the shadow
buffer. A randomized approximation of APD, which we call AFD-FT,
uses only a flow table and, as a result, it needs much less state than
AFD-SB.
We have evaluated AFD in a variety of scenarios using simulations.
One typical simulation is presented in Figures 2 and 3, in which 7 TCP
flow groups (5 flows each) with different congestion control mechanisms and R'I'rs compete for a congested link bandwidth of 10Mbps.
Figure 2 shows the average throughput that each flow group gets under
four algorithms: BED, FRED, AFD-SB and AFD-FF. The coxresponding drop probability of each flow group is depicted in Figure 3. This
and other data suggests that, in a wide range scenarios, AFD provides
a good approximation to fair bandwidth allocation, typically providing
bandwidth allocations within +1-15% of the fair share.

Computer Communication Review

Case Retrieval
Using Nonlinear Feature-Space Transformation
Rong Pan1 , Qiang Yang2, and Lei Li1
1

Software Engineering Institute
Zhngshan University
Guangzhou, China
gzpanrong@etang.com,lncsri07@cs.zsu.edu.cn
2
Department of Computer Science
Hong Kong University of Science and Technology
Clearwater Bay, Kowloon Hong Kong, China
qyang@cs.ust.hk

Abstract. Good similarity functions are at the heart of effective case-based reasoning. However, the similarity functions that have been designed so far have
been mostly linear, weighted-sum in nature. In this paper, we explore how to
handle case retrieval when the case base is nonlinear in similarity measurement,
in which situation the linear similarity functions will result in the wrong solutions. Our approach is to first transform the case base into a feature space using
kernel computation. We perform correlation analysis with maximum correlation
criterion(MCC) in the feature space to find the most important features through
which we construct a feature-space case base. We then solve the new case in the
feature space using the traditional similarity-based retrieval. We show that for
nonlinear case bases, our method results in a performance gain by a large margin. We show the theoretical foundation and empirical evaluation to support our
observations.
Keywords: Similarity, Case Base Transformation, Nonlinear Case Bases.
Paper type: Research.

1

Introduction

Case-based reasoning (CBR) is a problem-solving strategy that uses previous cases to
solve new problems ([5], [6]). Over the years, CBR has enjoyed tremendous success
as a technique for solving problems related to knowledge reuse. Several practical systems and applications [15] highlight the use of similarity based functions to find relevant cases from case bases. In building a case base, important descriptors of the case,
which distinguish between the cases, are singled out and represented as features. The
features are typically combined in some numerical computation for similarity. When a
new problem is input, its features will be extracted to compute its similarity measure to
other cases in the case base. The cases with the most similar measure will be retrieved
for further analysis and adaptation ([5], [6]).
The quality of the retrieved case in a CBR system depends heavily on how to use
the features to compute similarity measures. Various methods have been proposed to
P. Funk and P.A. González Calero (Eds.): ECCBR 2004, LNAI 3155, pp. 361–374, 2004.
c Springer-Verlag Berlin Heidelberg 2004


362

Rong Pan, Qiang Yang, and Lei Li

compute the similarity ([2], [1], [6], [12], [13], [14]), where most approaches rely on
linear combination of features to perform this function. However, when the nature of
the case base is nonlinear, where similar cases cannot be found by a linear combination
of the features, such a method will fail to deliver the most relevant cases. In this paper,
we present a solution to solving this problem.
As an example, suppose that in a problem domain there are N different features.
If the similarity in the domain is based on a high-order polynomial function of the
features’ values, then the similarity of the features cannot be explained by a simple
weighted sum of the input features alone. A real world example of this nature is when
we define the similarity of two boxes by their weight. Suppose the input features given
are the boxes’ three dimensions x1 , x2 and x3 and the density d of the material that
makes up the boxes. Then the computation of the boxes’ weight which defines the
similarity function is not a linear weighted sum of the three dimensions; instead, it
involves the multiplication of the four features x1 , x2 ,x3 and d.
One can argue that in the above example, one can input the nonlinear features such
as x1 ∗ x2 ∗ x3 directly as part of an input feature, but we cannot expect the designer
of case bases to have this insight for every domain that he encounters. We would rather
have the system find out these nonlinear features directly using an automatic method.
This issue is the focus of our paper.
In this paper, we present a kernel-based method by which we transform a case base
from the original space to a feature space with the kernel trick. For a nonlinear target
case base, we propose nonlinear feature-extraction methods with a Maximum Correlation Criterion(MCC). With this criterion, one can find in feature space those features
that have the highest correlation to target solution. We call this method the Kernel Case
Correlation Analysis (KCCA). Our empirical results show that for many nonlinear domains, our KCCA method outperforms the traditional linear similarity functions applied
in the original case space.

2

Transformation of a Case Base to Feature Case Space

In this paper, we focus on a dot-product formulation of the similarity computation.
Consider a given case base D = {(xi , yi ), i = 1, . . . , M, xi ∈ RN , yi ∈ R}, where R
is the real domain, xi is a vector of input attributes (features), and yi is the case-solution
which corresponds to a target variable. For generality, we assume that the target variable
is a continuous variable; discrete variables that are ordinal can also be converted to
continuous ones. Then a popular method for computing the similarity between two
→
→
cases is as follows: for an input problem −
c , the similarity between a case −
x in the case
base and the input case is computed as the S function:
→
−
→
→
w · (−
c −−
x)
→
→
S(−
c ,−
x) =
→
−
|w |
→
where −
w is a weight vector. Then, the cases with the largest value of the above similarity function are chosen as a candidate case. These cases are adapted to obtain a new
solution. In this paper, we consider a simplified situation where we choose a highest

Case Retrieval Using Nonlinear Feature-Space Transformation

363

ranked case by the similarity function and use the target value of that case as a recommended solution for the input case. This corresponds to using a 1-NN method for case
retrieval. Our work can be easily extended to k-NN computations. In cases where the
case solution is a compound structure, such as in the case of planning [3], our solution
corresponds to predicting a solution index for the corresponding target case.
Given a case base D, we now consider how to transform the cases to the feature
space. Our intuition is illustrated by the following example.
Consider a problem domain where the target z = x2 + 2y 2 , where x, y are the
attributes. In the original space (R2 ), we cannot find a direction which correlates well
with z, where the correlation coefficient is defined in [−1, 1]. Thus, if we use an 1-NN
in the original space, we are not going to get good result.
Now consider the case in a nonlinear space induced by a 2-degree polynomial kernel
[10]. The corresponding nonlinear map of the kernel is:


φ : ([x], [y]) → [x]2 , [y]2 , [x][y], [y][x]
With this kernel function, there exists u = [x]2 + 2[y]2 , which is a linear transformation
in the nonlinear feature space. We can see that u completely correlates to the target z.
We can now solve the nonlinear case-base retrieval problem better by considering the
correlation in a nonlinear feature space.
We now consider the general case. Let φ(x) be the nonlinear function which maps
the input data into feature space, F . Then in F , we can define a matrix, in terms of a
dot product in that space i.e. K(i, j) = φ(xi ), φ(xj ). Typically we select the matrix
K based on our knowledge of the properties of the matrix rather than any knowledge of
the function φ(). The kernel trick allows us to define every operation in feature space in
terms of the kernel matrix rather than the nonlinear function, φ().
Much research has been done in machine learning on feature selection and feature transformation in nonlinear feature space; some examples are Principal Component Analysis(PCA), single value decomposition(SVD)([4]), Kernel PCA, Sparse Kernel Feature Analysis, Kernel Projection Pursuit ([9], [10], [11]). However, in case-based
reasoning, it is important to relate between the input and target variables and these
works do not address this issue directly. In order to draw this relationship, we turn to
Kernel Fisher Discriminant Analysis (KFDA)([7], [8]) which takes the class label of
target into consideration. However, KFDA restricts the target to be of discrete values.
In this paper, we present a novel nonlinear feature transformation method, by which
we consider the correlation of input features with a continuous valued target variable
in the feature space. Our questions are: first, for a given case base, how do we tell if a
transformation to a feature space will give better result? Second, how do we perform
feature selection in the feature space to result in maximal retrieval accuracy?

3

Kernel Correlation Analysis in the Feature Space

3.1 Review of Correlation Coefficient
In multivariate statistics, the correlation coefficient is used to measure the linear dependency between two random variables. Suppose that Y1 and Y2 are random variables

364

Rong Pan, Qiang Yang, and Lei Li

with means µ1 and µ2 and with standard deviations σ1 and σ2 , the correlation coefficient between Y1 and Y2 is defined as
M
(Y1 − µ1 ) (Y2 − µ2 )
(1)
ρ= 1
σ1 σ2
It is easy to prove that the value of correlation coefficient ranges from −1 to 1. The
larger the absolute value of ρ, the greater the linear dependence between Y1 and Y2 .
Positive values indicate that Y1 increases with Y2 ; negative values indicate that Y1
decreases with Y2 . A zero value indicates that there is no linear dependency between
Y1 and Y2 . (see Fig.1.) If we normalize Y1 and Y2 as
Y1 =

Y1 − µ1
σ1

Y2 =

Y2 − µ2
σ2

and

and define two vectors as follows

and




, Y12
, . . . , Y1M
)
x1 = (Y11

(2)




, Y22
, . . . , Y2M
)
x2 = (Y21

(3)

then, x1 , x2 are identity vectors (whose 2-norms are equal to one) and the correlation
coefficient is the inner product of x1 , x2
ρ = x1 , x2 
(see the right one of fig.1) On the left of the figure are two sets of scatter points(circles

-

Y1

x1

+

x2

Y2

+

Fig. 1. Illustration of the correlation coefficient.

and dots) corresponding to Y1 and Y2 as they center around the mean point (µ1 , µ2 ).
If the scatter points mainly distribute in the 1-st and 3-rd quadrants(circle points), the

Case Retrieval Using Nonlinear Feature-Space Transformation

365

correlation coefficient is positive; if the scatter points mainly distribute in the 2-nd and
4-th quadrants, the correlation coefficient is negative(dots). If the scatter points equally
distribute in the four quadrants, the correlation coefficient trends to zero. On the right of
the figure are two vectors x1 and x2 as defined in (2) and (3), where θ is their angle. The
correlation coefficient equals cos θ, where θ = 0 means that they positively correlate,
θ = π means that they completely negatively correlate, and θ = π/2 means that they
do not correlate.
3.2 Correlation Analysis on Input Case Base
We now propose a new feature extraction method similar to Fisher Linear Discriminant
Analysis (FDA), extended to handle continuous target values. First, we consider the
case in the input case space. Given an original case base with M cases:
D = {(xi , yi ), i = 1, . . . , M, xi ∈ RN , yi ∈ R}
We assume that the attributes are centered around the origin and yi is also normalized
(assuming continuous attributes):
M

i=1

xi = 0 ,

M


yi = 0 ,

i=1

M


yi2 = 1

(4)

i=1

The correlation coefficient between the j-th coordinate x(j) and y is defined as follows:
M (j)
M (j)


i=1 xi yi
i=1 xi yi

=
cor x(j) , y = 



2 
M
M  (j) 2
(j)
M
2
x
y
i
i=1
i=1 i
i=1 xi
We now consider how to find features that best describe the correlation between attributes and the target. For many problems, there does not exist an independent variable
whose correlation coefficient with the target variable is either 1 or −1. In such cases, we
wish to find a new direction w in which the correlation coefficient between the projection of all cases on this direction and the target variable is maximized (absolute value
maximizing). This new direction will serve as a new feature in the feature space and
be used for computing case similarities. Suppose that zw is the coordinate on the new
direction w when a case x is projected on w,
zw = w, x = wτ x
Then the correlation coefficient of z and the target variable y is:
M
M
i=1 w, xi  yi
i=1 w, yi xi 

cor (zw , y) = 
= 
M
2 M
M
2
2
i=1 w, xi 
i=1 yi
i=1 w, xi 
To increase the correlation between zw and y is equivalent to maximizing the absolute value of the correlation coefficient between zw and y. We know the following:
2

arg max |cor (zw , y)| = arg max (cor (zw , y))
w

w

366

Rong Pan, Qiang Yang, and Lei Li

Thus, we can get
M

(

2

(cor (zw , y)) =

If we define µ =



yi xi , C =



2

w,xi yi )

i=1

=
=

i=1
M

w,

w,xi 

M

M
i=1

2

2
i=1 yi xi
τ
w τ xi xi w




M
τ
wτ ( M
i=1 yi xi )(
i=1 yi xi ) w

M
τ
wτ ( i=1 xi xi )w

xi xτi , we can get a new Rayleigh coefficient:

J(w) =

(wτ µ)2
wτ Cw

Finally, to obtain the important directions which mostly correlate with the target
variable and be used as the new feature, we compute arg max J(w). We call this the
w

Maximum Correlation Criterion(abr. MCC).
To provide some intuition on how Correlation Analysis generates new feature, we
show an experiment with an artificial 3-d linear target-function case base in Fig. 2. In
this example, the input variables’ x, y-values are elliptically distributed as the righthand
figure shows. The target z-values are generated from z = x − 2y + ξ, where ξ is the
white noise with a standard deviation of 0.2. The lefthand figure shows the 3-d coordinate of the case base. The right hand figure is the projection on the x − y plane and
illustrates the Principle Component Analysis (PCA) and Correlation Analysis for this
case base. PCA does not consider the target variable and simply returns the direction
of statistical maximum variance as the first eigenvector. Correlation Analysis, on the
other hand, returns a direction (1, −2) that correlate to the continuous target variable
the most.

Z = x - 2y
3

W PCA

2

Y

1

0

-1

W CORR

-2

-3
-3

-2

-1

0

1

2

3

X

Fig. 2. 2-dimension input and 1-dimension target artificial example, with 500 cases generated.

Case Retrieval Using Nonlinear Feature-Space Transformation

367

3.3 Transformation of Case Base by KCCA
In a nonlinear target domain, it is often difficult to find one or several directions that correlates well with the target variable. With the “kernel trick”, we now consider projecting
a case base into a nonlinear feature space. Then, we attempt to transpose the linear correlation analysis to the nonlinear case base also with MCC. We call this method Kernel
Case Correlation Analysis (abr. KCCA). In the next subsection, we give some examples
of an artificial domain to demonstrate the merit of KCCA with 1-NN.
Given an original case base with M centered observations, the new case base can
be obtained in a feature space F S by:
Φ(D) = {(φ(xi ), yi ), i = 1, . . . , M, xi ∈ RN , yi ∈ R}
where φ(x) and yi are centered on the origin:
M


φ(xi ) = 0 ,

i=1

M


yi = 0 ,

i=1

M


yi2 = 1

i=1

We project our input case in the new direction w in the feature space. Like the Kernel
PCA [9], we assume that zw is a new coordinate:

w=
αi φ(xi ) and zw = w, φ(x)
i

Then the correlation coefficient of zw and y is:
 
j αj φ(xi ),φ(xj )yi
 i 
2
i w,φ(xi )
(
i
j αj φ(xj ),φ(xi ))
τ
τ
τ
α
Ky
α
Ky
α
Ky
√ τ 2 =  
= √ατ KK τ α
ατ
(Ki K τ )α
i (α Ki )


w,yi φ(xi )
=
(cor(zw , y)) = √i
2

=

i

i

τ

where K is the Kernel Matrix, and α = (α1 , . . . , αM ) .
Next, we consider the Rayleigh coefficient
2

J (α) = (cor (z, y)) =

2

(ατ Ky)
ατ KK τ α

where K is the kernel matrix and y = (y1 , . . . , yM )τ . Let µ = Ky, M = µµτ , and
N = KK τ . Finally we obtain an expression for
J (α) =

2

ατ Mα
(ατ µ)
= τ
τ
α Nα
α Nα

(5)

[10] presents several equivalent ways of the similar problems of maximizing Equation (5). One method is to solve the generalized eigenvalue problem and then selecting
eigenvectors α with maximal eigenvalues λ, as follows:
Mα = λN α

(6)

368

Rong Pan, Qiang Yang, and Lei Li

Like Kernel PCA, we can compute the projection on the eigenvectors wk in the
feature space as follows:

  k
 k
αi (φ (xi ) , φ (x)) =
αki K (xi , x)
(7)
w , φ (x) =
Each eigenvector then corresponds to an attribute that we can select in the feature
space for defining the cases. Let < X1 , X2 , . . . , Xn > be the selected attributes in the
feature space, where the target value remains the same. We can then build a featurespace case base for case based reasoning. In particular, our feature-space case-based
reasoning algorithm is shown as follows:
Algorithm
Step1.
Step2.

Step3.
Step4.
Step5.

Kernel Case Correlation Analysis(KCCA)
Transform the case base by solving the Eq.(6) and computing the
selected attributes for the case base with Eq.(7).
For an input case c, transform c to the feature space using the Eq.(7).
The weight is determined by the correlation coefficient between the
nonlinear feature and the target.
Compute the weighted similarity between c and every case in the Case Base
Select the case xi with the largest similarity value.
Return the target value y of xi as the solution.

The KCCA algorithm is based on an 1-NN computation in the feature space. However, it would be straightforward for us to extend it to a k-NN algorithm.
3.4 An Example for KCCA
To give some intuition on how KCCA generates new case base and the merit of KCCA,
we show in Fig. 3 an experiment on an artificial case base with two input dimensions
and one target dimension using a polynomial kernel of degree two.
In this example, we have 500 data randomly generated cases in the following way:
the input variable’s (x,y) values are distributed in a circle, and the target z-values are
generated from z = 100x2 + 200y 2 + ξ, where ξ is the white noise with a standard
deviation 0.2. The top left figure shows the 3-d coordinate of the case base. The top
right one is the result of our KCCA on this case base. V1 , V2 are the first two directions
with which the linear regression plane (the hexagonal plane) is a good fit for the actual
values. The table at the bottom shows the result of a segment with 6 cases in 500 cases
before and after applying KCCA. The case numbers are also marked in the top figures.
In this table, we can find that the overall MAE (Mean Absolute Error) of 1-NN with
KCCA is about 40% lower than the overall MAE of 1-NN with original case base.
Moreover, we can find that the nearest neighbors in the original case base of case no.
148 and case no. 348 are no. 204 and no. 149. The errors are respectively 108.03 and
50.289. In contrast, KCCA put these cases (symmetrical in original case base) together,
so that the errors reduce to 7.139.

4

Experimental Analysis

We claim that KCCA benefits from superior efficiency and performance in terms of
retrieval accuracy. We test our algorithms on a number of case bases. Our experiments

Case Retrieval Using Nonlinear Feature-Space Transformation
Z = 100x

2

+ 200y

z

369

2

z

-300
-200
300

Case No.

x

80
148
149
204
328
348

1.246799
1.339216
-1.35932
1.225457
1.274692
-1.35658

-400

-600

200

Original Case Base
KCCA
1-NN
1-NN Absolute
Target
1-NN
1-NN Absolute
Error
V1
V2
Case No. Output
Error
y
Value Case No. Output
…
…
1.69054 723.6435
328
704.506
19.138 275.286 189.7407
328
704.506
19.138
7.139
-1.92838 918.2133
204
810.183 108.03 341.6025 240.2621
348
911.074
1.975726 961.3629
348
911.074
50.289 365.5675 257.0333
148
918.213
43.15
-1.81911 810.1831
148
918.213
108.03 261.0379 215.4208
80
723.644
86.539
1.648425 704.506
80
723.644
19.138 274.8932 168.949
80
723.644
19.138
7.139
1.912332 911.0737
149
961.363 50.289 344.0176 230.023
148
918.213
…
…
MAE:
MAE:
27.901
16.75

Fig. 3. A 2-dimension input and 1-dimension target artificial example.

are performed on an artificial case base and several publicly available case bases; in
this paper, the case bases are: Wine Recognition, Boston House, bank, travel and compactiv1 . For each application domain, we validate our KCCA with linear regression and
1-NN respectively.
We first used an artificial domain where there are three attributes and a numerical
target value. This domain is designed to be a nonlinear one, and is aimed at showing the
validity of our KCCA algorithm to show that for nonlinear domains where the linear
regression technique cannot produce good results, our KCCA method can indeed make
a dramatic improvement. For each system the retrieval similarity criterion is based on a
cosine-similarity threshold; an input problem was successfully solved if the similarity
between the problem and the retrieved case exceeded the threshold.
4.1

Testing KCCA

Artificial Domain Experiments. We now conduct experiments to test the effect of
KCCA in several domains, to demonstrate how effective the new algorithm is. The first
1

The Wine and Boston domains are available in
http://www.ics.uci.edu/∼mlearn/MLSummary.html. The bank and comp-activ domains are
available in http://www.cs.utoronto.ca/∼delve/data/datasets.html. For the Travel domain, we
thank Lorraine McGinty and Barry Smyth for the data.

370

Rong Pan, Qiang Yang, and Lei Li

experiment is concerned with evaluating the KCCA method in an artificially generated
domain(same domain of fig.3). The common feature of these problem domains is that
they all exhibit nonlinear nature in terms of feature descriptions. To demonstrate this
point, we first show, in Figures (4) and (5), the result of linear regression, 1-NN and 1NN with KCCA in these problem domains. “Actual” means the true value of the target
function. Thus, the closer a method is to the actual value, the better the method is. As
we can see, linear regression performs poorly in these domains.

Mean Absolute Err.

700
600
Linear Regression in
original case base
1-NN in original case

500
400
300

1-NN in KCCA case
base

200
100
0
80

124 149 204 328 348

Case No.
Fig. 4. Comparison of KCCA with linear regression and 1-NN in the original space.
900
800
700

Linear Regression in
original case base
1-NN in original case base

600
500

1-NN in KCCA case base

400
Min of the above 3

300
200
100
0
Max Absolute Error

Mean Absolute Error

Standard Deviation

Fig. 5. The retrieval result of 6 cases from the artificial data set, when compared with Linear
Regression and 1-NN in the original space and 1-NN with KCCA.

To test the efficiency of the KCCA method, we plotted the mean absolute error of
KCCA as a function of the number of features. The result is shown in Figure (6). As we
can see, the first several eigenvectors found by KCCA are in fact much better features
than the rest in the feature space. This gives us confidence as to the use of KCCA in
case-based reasoning.
Public Domain Experiments. We used the Bank domain, the com-activ domain and
the Wine domain (available from the UCI Machine Learning Repository). The result is

Case Retrieval Using Nonlinear Feature-Space Transformation

371

The Error Trend of 1-NN with KCCA
Mean Absolute Err.
(1:0.0001)

0.505
0.5
0.495
0.49
0.485
0.48
0.475
0.47
0.465
13

27

55

84

116 151 189 233 285 351

Number of features
Fig. 6. The trend of average error as a function of different number of features computed by
KCCA.
Bank Domain

0.1

Mean Absolute Err.

1-NN
KCCA

0.09

0.08

0.07

0.06
128

256

512

1024

2048

4096

Size of Case Base

Fig. 7. Test KCCA on the Bank Base. The figure shows the average error as a function of different
sizes of the case base.

shown in Figure (7), where we compare the mean absolute error KCCA and 1-NN in the
original space as a function of different case-base size. As can be seen, using the KCCA
uniformly outperforms 1-NN in terms of the MAE, and the difference in the error rate
is the largest when the case base size reaches 512. Figure (8) shows the test result on
the Travel database used often in case-based reasoning testing [14], where the objective
is defined on the Price attribute, and Figure (9) shows the result of the com-activ base.
As can be seen from both bases, the KCCA method outperforms 1-NN in the original
space.
Table (1) shows a similar comparison with different kernels for the KCCA. As we
can see, the MAE for the 1-NN method in the original space is as large as 172.944,
whereas for the Gaussian kernel with the appropriately chosen parameters the MAE
can be made smaller. One interesting fact is that the polynomial kernel in fact results in
larger MAE error; this indicates to us that the Wine domain is in fact a linear domain,
and thus 1-NN in the original space will perform just fine. It also indicated to us that
the performance of the KCCA method is sometimes sensitive to the choice of kernel
functions.

372

Rong Pan, Qiang Yang, and Lei Li
Travel Domain

620

Mean Absolute Err.

600

1-NN
KCCA

580
560
540
520
500
480
64

128

256

Size of Case Base

512

1022

Mean Absolute Err.

Fig. 8. Test KCCA on the Travel Database. The figure shows the average error as a function of
different sizes of the case base.
Com-Activ Domain

4
3.8
3.6
3.4
3.2
3
2.8
2.6
2.4
2.2
2

1-NN
KCCA

128

256

512

1024

2048

4096

Size of Case Base

Fig. 9. Test KCCA on the Computer Database. The figure shows the average error as a function
of different sizes of the case base.

We also noted that the time for the KCCA computation involves building the feature
space case base and case correlation analysis. This is a one time cost. Once done, we
can use the result to measure the similarity of each new case. This latter computation
has the same time complexity as the original case based retrieval cost.

5

Conclusions and Future Work

In this paper we proposed a new solution to case base retrieval using a new nonlinear
similarity function, when the nature of the problem domain is nonlinear. We used an
FDA for finding the best attributes to compute a new case base in the feature space.
We noted that the FDA cannot handle the continuous target case. We then proposed
a new correlation analysis in the feature space, where we designed a new case based
reasoning algorithm we call KCCA. Our approach is to first transform the case base
into a feature space using kernel computation. We perform correlation analysis with
maximum correlation criterion(MCC) in the feature space to find the most important
features through which we construct a feature-space case base. We solve the new case
in the feature space using the traditional similarity-based retrieval. We then empirically

Case Retrieval Using Nonlinear Feature-Space Transformation

373

Table 1. Test KCCA on the Wine data set. We compare the Mean Absolute Error with different
kernels.
Original Space
172.944
KCCA with polynomial kernels of different parameters
degree
2
3
4
5
6
7
MAE
217.96
165.6
217.9
179.8
267.5
191
KCCA with Gaussian kernels of different parameters
Gamma
0.05
0.1
0.2
0.3
0.4
0.5
MAE
166.48
178.1
167.3
163.8
168.9
176
MAE

tested the KCCA for artificially generated data and for UCI data sets. Our result supports our initial claim that in nonlinear domains the KCCA will be more appropriate
measure of similarity.
In the future we wish to extend this work to other methods for the construction
of case bases. One important subject is to design kernel functions for the purpose of
selecting cases from raw datasets, so that the CBR solution can be carried out. Another
direction is to apply the kernel computation to more sophisticated kinds of target values,
instead of just a single real value.

Acknowledgment
We thank Lorraine McGinty and Barry Smyth for the travel data. Rong Pan and Lei
Li are supported by Zhong Shan University. Qiang Yang is supported by a Hong Kong
RGC grant and an ITF grant, and a Chinese National 973 project.

References
1. A. Aamodt and E. Plaza. Case-based reasoning: Foundational issues, methodological variations, and system approaches. AI Communications, 7(1):39–52, 1994.
2. D. Aha, D. Kibler, and M. Albert. Instance-based learning algorithms. Machine Learning,
6(1):37–66, 1991.
3. K. Hammond. Case-Based Planning: Viewing Planning as a Memory Task. Academic Press,
San Diego, 1989.
4. I. T. Jolliffe. Principal Component Analysis. Springer Verlag, New York, 2002.
5. J. Kolodner. Case-Based Reasoning. Morgan Kaufmann, San Mateo, CA, 1993.
6. D. Leake, A. Kinley, and D. Wilson. Case-based similarity assessment: Estimating adaptability from experience. In Proceedings of the Fourteenth National Conference on Artificial
Intelligence. AAAI Press, 1997.
7. S. Mika, G. Rätsch, J. Weston, B. Schölkopf, and K.-R. Müller. Kernel fisher discriminant
analysis. In Neural Networks for Signal Processing 9 – Proceedings of the 1999 IEEE Workshop, New York, 1999. IEEE.
8. V. Roth and V. Steinhage. Nonlinear discriminant analysis using kernel functions. In S.A.
Solla, T.K. Leen, and K.-R. Müller, editors, Advances in Neural Information Processing
Systems, volume 12, pages 568–574. MIT Press, 1999.
9. B. Schölkopf, A. Smola, and K.-R. Müller. Nonlinear component analysis as a kernel eigenvalue problem. Neural Computation, 10:1299–1319, 1998.

374

Rong Pan, Qiang Yang, and Lei Li

10. B. Schölkopf and A. J. Smola. Learning with Kernels. MIT Press, 2002.
11. A. Smola, O. Mangasarian, and B. Schölkopf. Sparse kernel feature analysis, 1999.
12. B. Smyth and M. Keane. Remembering to forget: A competence-preserving case deletion
policy for case-based reasoning systems. In Proceedings of the Thirteenth International Joint
Conference on Artificial Intelligence, pages 377–382, San Francisco, August 1995. Morgan
Kaufmann.
13. B. Smyth and M. Keane. Adaptation-guided retrieval: Questioning the similarity assumption
in reasoning. Artificial Intelligence, 102(2):249–293, 1998.
14. B. Smyth and E. McKenna. Footprint-based retrieval. In Proceedings of the Third International Conference on Case-Based Reasoning, pages 343–357, Berlin, 1999. Springer Verlag.
15. I. Watson. Applying Case-Based Reasoning: Techniques for Enterprise Systems. Morgan
Kaufmann, San Mateo, CA, 1997.

1638

IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 15, NO. 7, NOVEMBER 2013

Proxy-Based Multi-Stream Scalable Video
Adaptation Over Wireless Networks
Using Subjective Quality and Rate Models
Hao Hu, Xiaoqing Zhu, Yao Wang, Fellow, IEEE, Rong Pan, Jiang Zhu, and Flavio Bonomi

Abstract—Despite growing maturity in broadband mobile
networks, wireless video streaming remains a challenging task,
especially in highly dynamic environments. Rapidly changing
wireless link qualities, highly variable round trip delays, and
unpredictable traffic contention patterns often hamper the performance of conventional end-to-end rate adaptation techniques
such as TCP-friendly rate control (TFRC). Furthermore, existing
approaches tend to treat all flows leaving the network edge equally,
without accounting for heterogeneity in the underlying wireless
link qualities or the different rate utilities of the video streams.
In this paper, we present a proxy-based solution for adapting the
scalable video streams at the edge of a wireless network, which
can respond quickly to highly dynamic wireless links. Our design
adopts the recently standardized scalable video coding (SVC)
technique for lightweight rate adaptation at the edge. Leveraging
previously developed rate and quality models of scalable video
with both temporal and amplitude scalability, we derive the
rate-quality model that relates the maximum quality under a
given rate by choosing the optimal frame rate and quantization
stepsize. The proxy iteratively allocates rates of different video
streams to maximize a weighted sum of video qualities associated
with different streams, based on the periodically observed link
throughputs and the sending buffer status. The temporal and amplitude layers included in each video are determined to optimize
the quality while satisfying the rate assignment. Simulation studies
show that our scheme consistently outperforms TFRC in terms of
agility to track link qualities and overall subjective quality of all
streams. In addition, the proposed scheme supports differential
services for different streams, and competes fairly with TCP flows.
Index Terms—Scalable video coding (SVC), subjective video
quality model, video rate adaptation, wireless video streaming.

I. INTRODUCTION

R

ECENT years have seen a proliferation of smart phones
and constant bandwidth upgrades in broadband mobile
networks. These two factors combined have fueled the rapid

Manuscript received November 09, 2011; revised September 25, 2012; accepted December 25, 2012. Date of publication June 04, 2013; date of current
version October 11, 2013. This work was supported in part by a gift award from
Cisco Systems, Inc. The associate editor coordinating the review of this manuscript and approving it for publication was Dr. Zhihai (Henry) He.
H. Hu, X. Zhu, R. Pan, J. Zhu, and F. Bonomi are with the Advanced Architecture and Research Group, Cisco Systems, San Jose, CA 95134 USA (e-mail:
hahu2@cisco.com; xiaoqzhu@cisco.com; ropan@cisco.com; jiangzhu@cisco.
com; flavio@cisco.com).
Y. Wang is with the Department of Electrical and Computer Engineering,
Polytechnic Institute of NYU, Brooklyn, NY 11201 USA (e-mail: yao@poly.
edu).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TMM.2013.2266092

growth of mobile media traffic. The study in [1] predicts that by
2015, two-thirds of world’s mobile data will be video. On the
other hand, mobile media streaming remains a daunting task,
especially for users in a highly dynamic environment. The presence of heterogeneous access networks and high user mobility
contribute to the wide fluctuations of wireless link qualities
in terms of their throughputs and latencies. As multiple video
streaming sessions share the same access node (e.g., a cellular
bases station or a WiFi access point), the system also needs to
allocate wireless channel resources wisely among competing
traffic flows. It is therefore of crucial importance to have an
effective video rate adaptation scheme to strive for the best
possible viewing experience of individual users in face of wide
link quality fluctuations and dynamic network traffic patterns.
The challenges are multifold. First, rate adaptation for
streaming video needs to closely track fluctuations in the available wireless link bandwidth. Conventional techniques such
as TCP-friendly rate control (TFRC) [2], however, typically
rely on end-to-end packet statistics and fall behind abrupt
changes in the underlying network conditions. Second, existing
approaches achieve fairness by allocating equal rates to all
competing flows, whereas video streams naturally differ in
their utilities of rate depending on their contents. For instance,
it would be desirable for an action movie sequence to be
streamed at a higher rate than a head-and-shoulder news clip
competing over the same bottleneck wireless link. Such content-aware allocation is missing in today’s systems. Thirdly,
clients connecting to the same access node may experience
different throughputs over their respective wireless links, due
to factors such as distance and channel fading characteristics. Without proper in-network information, rate adaptation
decisions made at the senders can easily lead to inefficient
resource sharing. More specifically, packet transmissions over
a low-quality wireless link can block the access node from
adequately serving other streams over higher-quality links, a
problem commonly known as head-of-line blocking [3].
In this paper, we address the above issues in a novel rate
adaptation scheme for streaming video over a highly dynamic
environment. Our design introduces a proxy at the edge of the
network, right where congestion over the wireless links occurs.
This allows the rate adaptation module to constantly monitor
the bottleneck buffer level, which, in turn, reflects variations
in the throughput and delay of wireless links for all receivers.
To strike a balance between computational complexity and efficiency, we adopt the latest H.264/SVC standard [4] for lightweight in-network rate adaptation. Although the standard of-

1520-9210 © 2013 IEEE

HU et al.: PROXY-BASED MULTI-STREAM SCALABLE VIDEO ADAPTATION

fers spatial, temporal and amplitude scalability, we only make
use of temporal and amplitude scalability in the present work.
Compared with nonscalable H.264 coding, the relative bit rate
increase at the same fidelity for amplitude scalability can be as
low as 10% for all supported rate points when spanning a bitrate range with a factor of 2–3 between the lowest and highest
supported rate point [5]. To provide a wider selection of rate
points while maintaining high coding efficiency, temporal scalability can be considered, as providing temporal scalability usually does not have any negative impact on coding efficiency [5].
By combined usage of temporal scalability and amplitude scalability, a wide bitrate range (with a factor of more than 10) is
allowed. The resulting scalable video stream can be decoded at
different frame rates (FR) and quantization stepsizes (QS). We
further leverage the parametric models from our prior work [6]
to explicitly account for the impact of FR and QS on rate and
subjective quality of the scalably encoded stream. These models
enable our system to choose the best combination of FR and QS,
and correspondingly the temporal and amplitude layers, given a
rate constraint for each stream. The adaptation of both FR and
QS supports video delivery over a wide range of rates, which is
important because of the wide ranging channel conditions over
wireless networks.
The goal of the video adaptation module at a proxy node is
to maximize the overall viewing experience of all traversing
streams. We show that the problem can be decomposed into two
steps: i) to allocate the video rate for each stream based on their
respective rate-quality relations and wireless link throughputs
and the common bottleneck buffer level; and ii) to extract video
packets belonging to the appropriate temporal and amplitude
layers from each scalable video stream based on the allocated
rate. Given the optimal rate-quality tradeoff derived from the
original rate and quality models, the first subproblem of multistream rate allocation is solved by maximizing the weighted
sum of user qualities under a total network utilization constraint.
We propose an iterative solution, whereby the per-stream rate is
calculated based on periodic updates of bottleneck buffer level
and relative link throughputs. The second subproblem can be
solved offline, by using the original rate and quality parametric
models to pre-order the video temporal and amplitude layers so
that each additional layer offers maximum quality improvement
for the rate increment.
The main contributions of this study include: i) We derived
a novel analytical rate-quality model, that relates the maximum
achievable quality for a given video rate, for scalable video with
both temporal and amplitude scalability; the model is highly accurate for a variety of video content and does not require any
content-dependent parameters except the rate of the complete
scalable stream; ii) We presented an efficient method for pre-ordering the temporal and amplitude layers of a scalable stream to
achieve rate-quality optimality; iii) We proposed a proxy-based
video adaptation architecture for multi-stream video streaming
to wireless nodes, which can react quickly to changes in the
link conditions of the wireless nodes; iv) We derived an iterative
multi-stream rate allocation scheme at the proxy, that can maximize a weighted sum of received video quality at all receivers,
given the link bandwidths of all receivers. Extensive simulation studies confirm that the proposed scheme consistently out-

1639

performs conventional TFRC-based rate adaptation used in the
Datagram Congestion Control Protocol (DCCP) [7].
The rest of the paper is organized as follows. The next
section presents a review of related work. Then, we provides
a background on scalable video coding and our prior work
in modeling the rate and subjective quality of SVC streams.
In Section IV, we derive the optimal rate-quality tradeoff
based on the prior subjective quality and rate models, and
present an algorithm for ordering the temporal and amplitude
layers to achieve rate-quality optimality while considering
the layer-dependency in SVC. In Section V, we describe the
framework for maximizing a weighted sum of the subjective
quality for all participating streams, subject to the channel
constraint and video coding constraints. A iterative rate allocation scheme is proposed to solve the quality maximization
problem. In Section VI we address the practical system designs.
Section VII compares performance of the proposed scheme
against TFRC-based rate adaptation in DCCP under various
network scenarios. Our contributions and future works are
summarized in Section VIII.
II. RELATED WORK
A. Subjective Quality and Rate Modeling
There have been quite extensive research exploring the impact of frame rate and quantization step size, individually and
jointly, on the perceptual quality [6], [8]–[14]. However, there
is not a widely adopted quality model that considers explicitly
the effect of both FR and QS. In this work, we choose to use the
quality model developed in [6], [14], as this model was shown
to be highly accurate not only over the authors’ own dataset,
but also several other datasets. In addition, this model is analytically simple, with only two parameters that can be estimated
accurately from features computed from the original video [14].
To the best of our knowledge, no prior work except [6] has
proposed rate models that considers the impact of FR and QS explicitly. By using both the quality and rate models in [6], we develop in this work a rate-quality model. Using both the rate and
quality models, we also develop an efficient algorithm to preorder the temporal and amplitude layers to achieve rate-quality
optimality.
B. Video Adaptation and Rate Allocation
It has been long agreed that the video streaming rate needs
some form of adaptation to match the time-varying wireless
channel capacity [15], to provide a better user experience. At
the encoder end, techniques such as adaptive encoder rate control [16], [17], transcoding [18], and bitstream switching [19]
are proposed to dynamically adjust video rate. A viable alternative to this is scalable video coding, whereby a stream only
needs to be encoded once yet can be flexibly decoded at several different target rates [20]. Such a design greatly facilitates
on-the-fly adaptation of the spatial resolution, temporal rate,
and frame quality (controlled by QS) of the transmitted video
stream. The recently standardized SVC extension in H.264, in
particular, has succeeded in achieving comparable coding efficiency as the non-scalable encoding in H.264/AVC [5]. Hence

1640

IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 15, NO. 7, NOVEMBER 2013

it is especially appealing for video streaming in a mobile environment [21]. However, given a target rate, there are many
possible combinations of SVC spatial, temporal and amplitude
layers that can lead to different perceptual quality. Therefore,
adaptation of SVC streams subject to a rate constraint is not a
trivial problem. Using the rate and quality models in [6], we can
pre-order the temporal and amplitude layers in a SVC stream to
reach rate-quality optimality, greatly simplifying the SVC adaptation problem.
In terms of underlying rate control protocol/algorithm, many
conventional schemes rely on equation-based TCP-friendly
rate control (TFRC) [2] for regulating the rate of each stream
[22], [23]. However, these end-to-end schemes often suffer
from slow convergence when the bottleneck link bandwidth
changes rapidly, and lead to allocation results oblivious of
video content characteristics. [24] proposed a TCP-friendly
video transport protocol targeting for wireless environment,
but it is still content-agnostic. [25]–[28] rely on using a video
rate-distortion model to solve network resource allocation
while providing video content-awareness. The model is however only applicable for videos at a fixed frame rate. Cross-layer
design is also investigated in the literature to improve the video
adaptation over wireless [29]–[32]. In a very recent work [33],
proxy-assisted video adaptation is considered for the case
where multiple streams share a common backbone network and
the video rate-distortion model is used. Our work stands apart
from existing approaches by combining the rate and quality
adaptation capability of H.264/SVC with a rate-quality tradeoff
model that considers effect of both frame rate and quantization
stepsize on the rate and quality. In addition, link bandwidth
heterogeneity is considered in the rate allocation. The proposed
algorithm enables a fast converging, quality optimized rate
allocation at the proxy node. The proposed system is capable of
both closely following dynamics in the wireless link bandwidth
and tailoring the rate allocation for each stream based on its
own rate-quality tradeoff and the effective link bandwidth, and
choosing the optimal combination of frame rate and quantization stepsize that maximizes the quality for a given rate.

Fig. 1. Structure of a group of pictures (GOP) in an H.264/SVC stream encoded
with the coarse granularity scalablity (CGS) approach. The GOP length is 8
frames in this example. The stream supports 3 amplitude layers and 4 temporal
layers.

amplitude scalabilities. Each frame in the video sequence is enand )
coded into multiple amplitude layers (labeled as ,
with decreasing QS . Inter-frame prediction among the pictures
in each amplitude layer follows a dyadic pattern, leading to several temporal layers (labeled as , , etc.) with increasing FR
. This approach, also known as coarse granularity scalability
(CGS), allows the stream to be flexibly decoded at various combinations of amplitude and temporal levels without introducing
any mismatch error in the decoding process. In the example
of Fig. 1, for instance, the stream supports 3 amplitude layers
and 4 temporal layers, thereby allowing 12 rate-quality tradeoff
points. If at certain point, the server or proxy decides to send
, then all chunks with label
will
be extracted for sending, e.g. those shaded chunks in the GOP.
In the bitstream, each chunk represents a network abstract layer
(NAL) unit, containing bits at that layer from a single video
frame.
With CGS, video adaptation is allowed at GOP boundaries.
In the same example, suppose the highest framerate is 30 frames
per second (fps), then the video rate can be switched every 8/30
seconds. There is a tradeoff between granularity of the adaptation interval and coding efficiency since smaller GOP size
leads to lower compression ratio. The typical choice of GOP
size ranges between 8 to 32 for video streaming.
B. Modeling Subjective Quality and Rate of SVC

III. SVC BACKGROUND AND PRIOR WORK
IN QUALITY AND RATE MODELING
A. SVC Coding Scheme and Rate Adaptation
Scalable video coding schemes have been advocated for
video adaptation to network and terminal capabilities due to its
low complexity and flexibility [20]. This approach eliminates
computationally demanding transcoding processes at video
servers or intermediate proxies by simply extracting appropriate bitstreams according to network or terminal constraint.
It is shown that the latest SVC standard [4] can achieve comparable coding efficiency as the state-of-the-art H.264/AVC
non-scalable coding [34].
In SVC, the motion-compensated transform coding architecture is extended to achieve a wide range of spatio-temporal and
amplitude scalabilities. Fig. 1 illustrates the typical structure of
a group of pictures (GOP) that implements only temporal and

In [6], [14], the authors studied the impact of FR and QS on
the subjective quality and bitrate of scalable video. Based on
the mean opinion score (MOS) obtained from subjective quality
tests, it is observed that the impact of FR and that of QS on
the MOS is separable. In other words, the video quality can be
quantified as the product of a metric that accesses the quality of a
quantized video at the highest frame rate, based on the QS , and
a temporal correction factor which scales the quality assessment
according to the actual FR . The normalized subjective quality
at any QS and FR can be written as:
(1)
where
are content-dependent model parameters;
dedenotes the maximum FR. Here
notes the minimum QS;
quality is normalized with respect to the quality achievable at

HU et al.: PROXY-BASED MULTI-STREAM SCALABLE VIDEO ADAPTATION

1641

TABLE I
PARAMETERS FOR RATE MODEL, QUALITY MODEL, AND RATE-QUALITY MODEL AND THE RMSE FOR RATE-QUALITY MODEL

. Note that, the parameter indicates how fast
the quality drops with increasing , with a larger suggesting
a faster drop. On the other hand, the parameter determines
how fast the quality reduces when decreases, with a smaller
incurring a faster drop. The parameter values depend on the
motion and texture characteristics of the video. The parameter
values for 7 test sequences used in [14] are given in Table I. As
can be seen, ranges between 0.05–0.2, while varies between
5–9, for these sequences which cover a large range of motion
and texture characteristics.
For the same set of encoded sequences, their bitrates are
recorded and the influence of and on the bitrate is analyzed.
Following the same decomposition approach, it is shown that
the bitrate can also be modeled as the product of two functions
of and , respectively. The overall rate model is:

To find the optimal FR
which maximizes
for a
given , we solve for
. Together with (2),
we have the following relation between the optimal
and
the rate contraint ,

(2)

(4)

are content-dependent model parameters and
where
corresponds to the bitrate at
. Here, characterizes how fast the bitrate reduces when increases. On the other
hand, controls the bitrate decaying speed when decreases.
Table I shows that varies over 1.0–1.3, while ranges between
0.4–0.8.
based on the
How to estimate the values of
video characteristics can be found in [35]. Basically, two content features, i.e., average frame difference and average motion vector magnitude, when linearly combined, suffice to give
an acceptable prediction accuracy. These features and consequently the model parameters can be computed along with the
encoding operation at the video server, with a complexity no
more than that of motion estimation.

(5)

IV. RATE-QUALITY MODELING AND QUALITY
OPTIMIZED ORDERING OF SVC LAYERS
A. Deriving Rate-Quality Model
As decreasing FR or increasing QS will lead to decreasing
bitrate and vice versa, there might be multiple combinations
of FR and QS that satisfy a given bitrate constraint . But
the associated quality is different. It is then desirable to find
the combination that gives the best quality while satisfying .
By finding the optimal
and the corresponding maximum
achievable normalized quality for each possible , we arrive
at the rate-quality model, to be denoted as
.

First, we define some notations that will be used later: normal, normalized FR
and
ized bitrate
relative QS
. Given a rate constraint
, it
can be derived from (2) that
, which is equivalent to
. Then, we can rewrite (1) in terms of
as

(3)

Note here, as
and
, if the resulting
is less
than 1, the
constraint is active and then
is clipped to
1; a new
is calculated using (5). The above equations give a
criteria for choosing the FR and QS under any rate constraint.
It is easy to verify that
and
so
that
is unique and is monotonically increasing while
is monotonically decreasing as increases. Then, the best
normalized quality
can be derived from
by using
(1). Although it is hard to derive closed-form relations between
the rate constraint and the optimal
,
, and
,
we can easily compute , , and
for any given based on
(4) and (5) numerically (for example, using the “solve” function
in MATLAB). For notational simplicity, in the sequel, we will
ignore the superscript in
, and use
to denote the
optimal rate-quality tradeoff. Fig. 2 shows the
and the
corresponding
versus bitrate for two sequences, FOREMAN
and FOOTBALL.
Fig. 3 shows the numerically computed optimal rate-quality
tradeoff curves in terms of normalized rate for seven sequences
with various video characteristics. We found that these curves
can be closely approximated with the following exponential
function
(6)

1642

IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 15, NO. 7, NOVEMBER 2013

still video-sequence dependent. We can rewrite the
function of the absolute rate as

as a
(7)

B. Rate-Quality Optimized Ordering of SVC Layers

Fig. 2. Optimal FR and QS (left axis) and the corresponding optimal normalized quality (right axis) versus bitrate. These results assume that both FR
and
and QS can take on any value in their respective ranges, i.e.,
. (a) FOREMAN; (b) FOOTBALL.

Fig. 3. Normalized optimal Rate-Quality tradeoff curves for seven sequences:
AKIYO, CITY, CREW, FOOTBALL, FOREMAN, ICE and WATERFALL.
The solid line gives a unified rate-quality model.

where
are model parameters.
controls the overall
quality dropping rate, with larger indicating faster drop.
impacts the quality gain over bitrate increment. In other words,
larger implies less bits increment for the same quality gain.
In order to estimate and , we have to first determine a set
of the optimal
points by using (3) and (4), then employ standard non-linear regression method. For pre-encoded
video, this can be done offline. For real-time live video, this is
still feasible since we only need to re-estimate the parameters
every few seconds or longer. Table I summarizes the model parameters for each sequence and the model accuracy in terms of
the root-mean-square error (RMSE). The fitting curve for individual sequence matches with its data very accurately and hence
are not shown separately in Fig. 3.
Because the optimal
curves for different sequences in
Fig. 3 are very close to each other, we further propose to use
a unified model with the same parameter set for all sequences.
Using least squares fitting to the
data from all sequences,
we found
and
. This unified model is
also shown in Fig. 3 and summarized in Table I. As will be
shown later, the rate allocation results obtained using the unified model are very similar to those obtained using content-dependent values for and . Note that although we propose to
use the same parameters and , the maximum rate
is

To efficiently stream a pre-coded scalable video where the
target bit rate is changing dynamically, it is desirable to preorder the SVC layers in a rate-quality optimized manner, so
that each additional layer yields the maximum possible quality
improvement. With such a pre-ordered SVC stream, the proxy
can simply keep sending additional layers, until the rate target
is reached. Noting that each SVC layer (together with its previous layers) corresponds to a feasible
pair, the problem
is equivalent to ordering the feasible
pairs, subject to decoding dependency constraint. In this section, we discuss how
to employ the rate and quality models given in Section IV-A to
optimize the ordering of
pairs.
We have shown that (4) and (5) can be employed to determine
the optimal
which gives the best quality under a rate
constraint. However, the resulting
might not always be
feasible. For example, if a SVC stream is encoded with a dyadic
temporal prediction structure, the feasible FR is doubled every
time from the lowest to the highest FR. Similarly, there are only
a small number of amplitude layers in a typical SVC stream,
corresponding to a few discrete levels of QS . In the following,
we discuss how to take into account such practical limitations.
Suppose there are
temporal layers and
amplitude
layers, the corresponding feasible choices of FR and QS are
and
, respectively. We can
construct a table which gives all possible combinations, each
indicated by a quadruplet
. If some combination
in this table has higher but lower than at least one other
combination, then it is clearly not rate-quality optimal. We
can eliminate these points and order the remaining points in
increasing rates with two steps. The first step is to sort all points
in terms of their rates from low to high. Then starting from
the point with the second lowest rate to the end (the first point
corresponds to the base layer), we compare the quality of the
current point to that of the previous kept point; we remove the
current point if the quality is less or equal, otherwise keep the
current point. We denote the table that contains the remaining
entries as
. The complexity of generating this table is
.
The points in
has the property that as the rate increases,
the quality also increases. Some of the points in
may not
be optimal in the sense that they do not provide the maximum
possible quality improvement for the incurred rate increment.
Furthermore, some points in
may not satisfy the SVC decoding dependency. For example, a current point may have a
FR that is lower than the previous point, or a QS that is higher
than the previous point. There are multiple ways to remove
the non-feasible points in
based on the rationale that the
next feasible point can be either increasing in FR or decreasing
in QS or both. We use the following Algorithm 1 to remove
the non-optimal and non-feasible points in
, and create the
table
. The associated complexity is also
.

HU et al.: PROXY-BASED MULTI-STREAM SCALABLE VIDEO ADAPTATION

ENTRIES OF

FOR

1643

TABLE II
SEVEN SEQUENCES. EACH ENTRY IS A QUADRUPLET OF

Algorithm 1 Generate rate-quality optimized table
put

into table

;

while

.

do
for
do
if
last point in

has lower FR or higher QS than
then

continue;

Fig. 4. Quality optimized table for the sequence FOOTBALL. (a) rate-quality
and
; (b) corresponding
values for points
tradeoff of points in
.
in

end if
calculate

from points

and

;
end for
find

which gives the highest

put

into table

value;

;

;
end while
Fig. 4 shows the rate-quality relations of points contained in
table
and
and the corresponding
for each
point obtained using Algorithm 1. Clearly, many feasible points
are not optimal and are removed to get
. A few points in
are still not optimal or do not satisfy the decoding dependency, which are removed to get
. Note that the removed
points in
tend to have a much smaller quality improvement compared to a neighbor point with slightly larger rate. As
the target rate increases, the FR monotonically increases while
the QS monotonically decreases, thereby satisfying dependency
across the layers. The points in
follow the concave shape
of the rate-quality curves in Fig. 3 very closely, indicating that
ordering SVC layers based on these points yields near-optimal
rate-quality tradeoff. Table II summarizes the entries in
for
all seven sequences.

V. QUALITY MAXIMIZATION FRAMEWORK AND ALGORITHMS
FOR RATE ALLOCATION
In this section, we develop a subjective quality maximization
framework for rate allocation among multiple wireless receivers
under the same wireless access node. The framework is based
on the rate-quality model in (7) and can be easily adapted to
accommodate other models.
A. Problem Formulation
Consider a set of video receivers sharing a common access
node. For each receiver
experiencing a wireless link throughput of , the interested video can be adapted
to have an equivalent coding parameter setting indicated by
, which results in subjective quality of
and
video bitrate of
. In addition, suppose there are a set
of background flows, each generates rate
.
From system-wide of view, the optimal network utilization
is achieved by solving the following utility maximization
problem.
(8)
(9)
(10)

1644

IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 15, NO. 7, NOVEMBER 2013

Here
and
denote the possible choice of QS and FR for
the video requested by receiver , respectively. The objective
function (8) is the weighted sum of subjective quality over all
video receivers where important receivers will be assigned with
a larger weight. The constraint (9) ensures that the aggregated
channel utilization time is below the maximal system utilization
ratio (which we assume to be 1 here, but can in general be less
than 1). (10) specifies the coding parameter constraints.
The computational complexity involved in solving the optimization problem increases with the dimension of the coding
parameter set and the number of video streams. Besides, these
coding parameters are usually integers. Thus, the problem becomes combinatorial hence computationally expensive. Suppose there are
possible choices of ,
possible choices of
and video streams, the number of possible combinations
would be
.
We, instead, propose to solve a relaxed form of this
problem by allowing continuous choices of video rates, and
by leveraging the optimal rate-quality tradeoff developed in
Section IV-A. After obtaining candidate sending rates from the
first step, we can then simply pump pre-ordered video packets
into the network as described in Section IV-B.
To solve for the candidate sending rate for each video receiver, we reformulate the problem as follows.

, is concave. As the weighted summation in (11) preserves
the concavity, the overall objective function is concave.
Concavity of the objective function (11) ensures that the local
maximum is also the global maximum. We propose to use an iterative algorithm to approach the optimal point. The reason behind this is trifold: 1) if the bottleneck link is highly dynamic
with time-varying bandwidth and delay, the results by directly
solving problem (11)–(13) given past observation (or estimation) of
,
and
could be highly skewed; 2) directly
solving problem (11)–(13) incurs much larger complexity and
overhead; 3) directly solving problem (11)–(13) requires an accurate observation (or estimation) of ,
and , otherwise,
the results can be invalid even if computed at a high frequency.
Iterative solutions, on the other hand, are generally more robust
against variations of the network conditions and measurement
errors [36].
Denote
and
the instantaneous incoming rate and the required
serving time at the access point, respectively. Then
is the instantaneous effective link outgoing rate. Following the
same idea of the primal-dual algorithm in network rate control
[37], [38], we propose the following two iterative steps:

(11)

(15)

(12)
(13)
where

denotes the base layer bitrate for video .

B. Iterative Solution
Claim 1: In practical video rate regions, the problem
(11)–(13) is a concave maximization problem.
a) Proof: To justify this claim, we show the concave region for
, or equivalently,
.

As
, then
gives the concave region of
. And the concave region is
. At the lower boundary
, the corresponding normalized quality
. With the parameters shown in Table I,
we have the normalized quality always less than 0.1 and
. As a normalized quality of 0.1 (e.g., 1 on a 10
rating scale) is considered very annoying and unacceptable, the
video stream should never be extracted at a normalized rate
lower than this bound. With a SVC video, the lowest rate is
the base layer rate. In our test videos, the base layers all have
normalized rate above 0.016 with normalized quality above
0.13, as indicated in Table II. Therefore, we claim that for
the practically meaningful range of rate,
or equivalently

(14)

Here
of

are two scaling factors;
is the first derivative
w.r.t . denotes the price of using the link and
.
Intuitively, the value of increases when the network is temporarily over-congested, leading to a negative or slower increment of , whereas temporarily underutilization of the network
results in decreased and consequently higher
from all contributing streams.
Theorem 1: The iterative algorithm of (14), (15) will
converge to an equilibrium point which solve the problem
(11)–(13) in a time-sharing wireless network under static
channel conditions.
b) Proof: The constrained optimization problem of
(11)–(13) can be converted to maximizing the following objective function

where
are Lagrange multipliers. The Karush-KuhnTucker (KKT) conditions are as follows:

(16)
(17)
(18)
(19)

HU et al.: PROXY-BASED MULTI-STREAM SCALABLE VIDEO ADAPTATION

1645

Note that the algorithm of (14), (15) is essentially gradient
descending algorithm, which stabilizes at certain point
,
i.e. when
and
. Next, we show that the equilibrium
point
achieved by the algorithm of (14), (15) solves
the problem (11)–(13) by examining the satisfaction of its KKT
conditions.
Case 1: if
and
, the point
satisfies

Then, we have the corresponding
KKT condition.
Case 2: if
and
and
for the KKT condition.
Case 3: if
and
and
for the KKT condition.
Case 4: if
, then
we have
,
and
condition.

for the
for some , we have

for some , we have

according to (14). So,
for the KKT

VI. PRACTICAL SYSTEM DESIGN
Previously, we have developed the mathematical framework
for solving the system-wide quality maximization problem. In
this section, we focus on how to implement the proposed iterative solution in a practical wireless video streaming system.
A. Architecture Overview
In a wireless video streaming system, the video server maintains the original video bistreams. Upon requests for certain
video contents, the server will send the (sub)stream through IP
network to the wireless access node via which the (sub)stream is
served to end-users. The wireless access node is usually the bottleneck where the congestion is likely to occur as it is shared by
many users and has relatively low bandwidth capacity compared
to the IP network. In order to track the wireless link status, an
end-to-end feedback mechanism would be expected to inform
the video adapter if it is at the server. However, the end-to-end
delay is typically large in a wireless environment.
To agilely trace the link status, we envision a proxy node
colocated with the access node. It is in charge of tracking the
time-varying status of the wireless access link, while dynamically adapting the traversing scalable video streams. Fig. 5
provides an architectural overview of the proxy-based video
streaming system. The benefit of such a design is multifold.
First, it requires no additional modifications at either the video
servers or the mobile clients; video adaptation is performed at
the proxy and is agnostic to both ends. Second, since the proxy
node is located right at the bottleneck wireless node, it can
react much more agilely than end-to-end adaptation schemes in
face of abrupt changes over the wireless hop. Furthermore, the
proxy node has knowledge and control of all traffic traversing
the bottleneck wireless link, therefore is well-positioned to

Fig. 5. Architecture overview of the adaptive video streaming system. A proxy
node at the edge of the network performs video adaptation before relaying video
streams to mobile clients.

optimize the allocated rate across the competing streams in a
more holistic manner.
The main drawback of this architecture is the potentially
large wasted bandwidth on the IP network. By the time adaptation happens at the proxy, the required video (sub)streams
are expected to be arrived. Therefore, the server needs to
send the whole bitstream to the proxy or send over-provisioned
(sub)streams based on some prediction algorithms. Considering
that the bandwidth is abundant in the core network and the
prevalent deployment of caching servers in the network, this is
acceptable.
B. Implementing the Algorithm at the Proxy
The iterative algorithm consists of two processes. The first
process, which follows (14), updates the streaming rate given
the observation of and , which is the vector containing the
observed link throughputs of all video receivers. The second
process, with (15), determines a new link price given the observation of the sending rates in the last update interval from all
users and the effective serving rate . Note here, depends
on both and . Suppose the proxy adapts video stream every
seconds, we write (15) in a discretized form,
(20)
is in fact the evolution
The middle term in (20), i.e.
of queue length at the access node. At time index
, the new
stream rate for video is calculated as

(21)
and according to the rate-quality model in (7),
by

is given

1646

IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 15, NO. 7, NOVEMBER 2013

Fig. 6. Main components in proxy-based adaptation architecture.

We implement (20) and (21) in two separate modules: link
buffer monitor and video adapter both at the proxy. Fig. 6 shows
the diagram of the two modules and the signaling in between.
The link buffer monitor checks the bottleneck queue length
once every
seconds. It is also responsible for estimating
the link throughput
for each receiver . In our system, the
packets’ inter-departure time at the interface queue is inspected
and it is used to derive the instantaneous throughput of the link
that transports the packet under consideration (via dividing the
packet length by the inter-departure time for that packet). Then,
the link throughput
can be estimated by averaging over a
number of packets.
The optimal rate allocation module will calculate the new
stream rate based on the feedback from the link buffer monitor
and the video rate-quality parameters embedded in the SVC
stream. Then, the SVC stream is adapted to the new rate by
simply sending video packets up to the target rate assuming
stream is pre-ordered in the quality-optimized manner, as discussed in Section IV-B. Note that the pre-ordering should ideally
be done at video encoder so that the video streams arriving at
the proxy are already in optimal orders. However, it is possible
to have the proxy to order the SVC layers if the video servers do
not have pre-ordered streams and are agnostic of the rate-quality
model adopted by the proposed proxy-based adaptation system.
C. Discussions
1) Feedback Interval: For CGS video streams, the rate adaptation can only be carried out at the boundary of GOPs. For example, with a CGS encoded video stream with GOP size of 16,
the rate switching can only be done every
seconds assuming 30 frames per second. On the other hand, for the
proxy to accurately track the link status, it should run the iterative algorithm of (20), (21) at a much faster update frequency.
To cope with this situation, we run the iterative algorithm with
a short update interval, but only adapt the video rate at the beginning of each new GOP. A filter may be applied to obtain a
smoother sending rate. In our simulation, the smoothed sending
rate at the beginning of each GOP is calculated as a weighted
sum of the current sending rate determined from (21) and the
smoothed sending rate for the previous GOP with coefficients
0.8 and 0.2 respectively. Then, the video rate is adapted according to the most recent smoothed sending rate.
2) Iterative Algorithm vs. Exhaustive Search: We carried out
some numerical case studies and found that the iterative algorithm incurs at most 5% efficiency loss (in terms of the total

Fig. 7. Performance comparison of the iterative algorithm with the exhaustive
search. Three sequences (AKIYO, FOREMAN and FOOTBALL) are requested
by three receivers and all the receivers have the same link throughput, ranging
from 300 kbps to 3 Mbps. Five amplitude layers and five temporal layers are
generated, thereby allowing 25 discrete quality-rate points. But only one of those
points given in Table II is chosen for each allocated rate for a sequence.

utility at the same rate) compared with the brute-force exhaustive search approach as shown in Fig. 7. The loss is largely
due to the discrete nature of the feasible rate points. An allocated rate (which is determined assuming any rate is achievable) is not always fully utilized. The complexity for exhaustive
search is on the order of
, while the iterative algorithm is
at each iteration. The corresponding running time for the
MATLAB scripts is 500 ms and 0.17 ms, respectively. Through
our extensive simulations, we found that the discrete nature of
the feasible rate points does not impact the algorithm convergence speed much.
3) Limitation of the Effective Link Bandwidth Estimation
Method: As we assume there is no cross-layer information,
i.e. the information from MAC layer and below, exposed to
the proxy, the proxy is agnostic to the packet loss below the
link layer. This limitation will result in inaccurate bandwidth
estimation, especially when there are severe packet losses.
For example, in WiFi system, a packet may undergo several
retransmissions and finally be dropped at the MAC layer,
therefore the instantaneous link throughput is 0. However, the
proxy had only observed the packet sojourn time at the MAC
layer, and calculated the instantaneous link throughput as the
packet size divided by the sojourn time (which is equivalent
to the packet inter-departure time at the interface queue if
other processing overheads are negligible). Thus, the resulting
estimation does not consider potential packet loss after the
maximum retransmission limit is reached.
4) Equalizing Receivers’ Subjective Qualities: In the formulation (11)–(13), the objective is to maximize the aggregated
weighted system utility. When all the weights are equal, the
quality at all receivers are generally not equal. By appropriately choosing the weights, we can equalize the quality of all
receivers, or making some receivers enjoying higher quality.
Without detailed derivations, we can show that setting
for each receiver leads to equalized subjective qualities at the receivers. We conducted some experiments to verify
this and the results can be found in Section VII-C-3.
5) Sensitivity to Parameters and in the Rate-Quality
Model: We have conducted a theoretical analysis, investigating
the sensitivity of the rate allocation results to the accuracy of the
parameters and . This study showed that the relative error in
the allocated rate is less than 15%, when we use the fixed parameters in the unified model for all sequences. This analysis is not

HU et al.: PROXY-BASED MULTI-STREAM SCALABLE VIDEO ADAPTATION

1647

Fig. 8. Simulation topology setup. a) Topology for the first set of simulations; b) topology for the second set of simulations.

included here due to space limitation. We also conducted some
of the experiments described in Section VII-C-1 both with the
content-dependent parameters and fixed parameters, and found
that the results are very close. One specific comparison is given
in Figs. 11(a) and 11(b), to be discussed later. Both the analysis and simulation results suggest that in practice it is viable
to use the unified Rate-Quality model, while performing rate
allocation.
VII. PERFORMANCE EVALUATION
In this section, we evaluate our system design with extensive simulations based on the
[39] simulator. We implemented video adaptation agents and a video player emulator
which generates video playback trace. We conducted two sets
of simulations. The first set, targeting for evaluating the effectiveness of the proxy-based design in adapting the sending
rate based on the time-varying channel condition, is driven by
a real-world wireless measurement trace. The impact on background traffic (e.g. TCP) is also evaluated. The second set of
simulations investigate the effectiveness of the proposed approach in rate allocation among multiple receivers based on
both channel conditions and video characteristics. We assume
the channel conditions of all receivers are static so as to isolate
the effect of channel dynamics.
A. Common Simulation Settings
We use real video packets traces to drive the simulations. Two
typical video sequences are used in the simulations: FOREMAN
and FOOTBALL, representing a slow-to-medium motion clip
and a highly intense motion clip, respectively. Both sequences
have a spatial resolution of 352 288 pixels (CIF) and temporal
rate of 30 fps. We encode the sequences with JSVM version 9.12
[40] to generate SVC streams with 5 CGS layers and 5 temporal
layers, and then pre-order the packets in a quality-optimized
manner as explained in Section IV-B. The feasible rate points
are as shown previously in Table II. Note that the maximum
rates needed to achieve the highest quality are very different
for the two videos, 0.8 Mbps and 2.1 Mbps, respectively. This
means that to achieve similar quality, FOOTBALL should be
allocated much higher rate.
We choose TFRC as a comparison rate control mechanism,
as it is considered suitable for media streaming. We applied
the Datagram Congestion Control Protocol (DCCP) patch for

.1 which contains a TFRC implementation. DCCP does
not require reliable in-order delivery of packets and the stream
rate will be determined by the TFRC solely.
In our simulations, we choose
,
where denotes total number of receivers, and
for
every receiver unless otherwise stated. The proxy senses the
interface queue every 50 ms and estimates the effective bandwidth for each link by averaging over 16 most recent packets.
The original candidate sending rate for each video is updated
using (21). A smoothed candidate sending rate is calculated as
discussed in Section VI-C-1 and the actual video rate is only
changed at the beginning of each GOP. The rate point among
those in Table II that is closest to the smoothed candidate rate
from below is chosen. The packet size is set to 500 bytes and if a
video NAL unit size is larger than that, it will be segmented into
several packets. The access point queue size is set to 75 packets
as suggested in [41].
We set the video playback delay to be 2 seconds which means
the player will start 2 seconds later after receiving the first video
packet. If any packet belonging to a NAL unit is lost during
transmission, the entire NAL unit is discarded as well as all other
NAL units that depend on it. Remaining NAL units that meet
the playback deadline are used to determine the highest decodable temporal layer and amplitude layer, which correspond to
a certain FR and QS . This
pair is used to derive the
normalized quality using (1).
B. One Receiver With a Dynamic Wireless Environment
In this set of simulation, we are interested in the responsiveness of our system design to the dynamic behavior of the wireless link. The most important aspect is how fast the proposed
design can tract and react to the wireless link quality changes.
Besides, it is also important to study how video traffic interacts with background TCP flows. Only FOOTBALL sequence
is used in this part.
1) Setup: Fig. 8(a) illustrates the simulation topology setup,
where the video server is attached to node N1 and the proxy
is located at the AP. A video client agent and a video player
agent are attached to node M0. There is also a background TCP
traffic generated by a FTP session from node N2 to node M0,
traversing the same AP. The wired link between node N0 and
AP is of 100 Mbps and 2 ms delay; the wired link between N1
1http://eugen.dedu.free.fr/ns2/dccp-ns2.34.patch

1648

IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 15, NO. 7, NOVEMBER 2013

Fig. 9. Wireless SNR and PHY rate traces from a real-world measurement. The trace was collected while driving around Mountainview, CA and the average
speed of the vehicle was around 20 mph. (a) Signal-to-noise ratio; (b) PHY rate.

Fig. 10. Performance comparison of proxy-based adaptation and TFRC. The average sending rates for proxy-based adaptation and TFRC are 1601 kbps and 1072
kbps respectively. The average playback rates over time for proxy-based adaptation and TFRC are 1333 kbps and 919 kbps respectively (not shown here). The average normalized qualities over time for proxy-based adaptation and TFRC are 0.66 and 0.47 respectively. (a) Sending rate; (b) normalized quality; (c) background
TCP throughput.

and N0 has 10 Mbps and 18 ms delay while the wired link between N2 and N0 has 10 Mbps and 10 ms delay; the wireless
link between AP and M0 is driven by a real-world measurement
trace obtained when driving around Mountainview, CA. The average speed of the vehicle was around 20 mph. Fig. 9 shows
the signal to noise ratio (SNR) (in dB) and PHY rate (in Mbps)
over time. These two traces are used to generate PER rate and
calculate transmission time. Note here, the effective link rate is
always less than the PHY rate due to retransmission at the MAC
layer up to a preset retransmission limit. The video server starts
streaming at time 0 while FTP session starts at time 80. Both
flows end at time 200.
2) Proxy Adaptation vs TFRC: Fig. 10 shows the performance comparison for proxy based adaptation and TFRC
scheme. From Fig. 10(a), it can be seen that when the channel
condition is good and stable, e.g., around time 50, 90 and 150,
TFRC achieves good performance. However, when the channel
quality changes dynamically, TFRC can not react agilely
experiencing a slow convergence speed, for example during
time 75–85 and 125 to 140. On the other hand, proxy-based
adaptation can adapt the sending rate quickly, and the resulting video playback quality is significantly improved over
the TFRC scheme. When the channel condition changes, the
proxy-based adaptation can agilely follow up and stabilize
at a high streaming rate. In terms of playback quality, the
proxy-based adaptation provides higher quality than TFRC
in most time, and the average quality over the entire duration
is much higher, as shown in Fig. 10(b). It is worth noting
here that since the proxy does not have information from the
MAC layer, it does not know whether a packet is dropped or
not and the link throughput estimation may not be accurate at
severe packet loss. This can lead the proxy-based adaptation

react improperly when continuous packet losses happen. For
example, around time 110–120, the proxy is still sending out
video packets which are all dropped (which can be inferred
from the zero quality during that time period in Fig. 10(b)).
When there are other flows destined to good wireless channels,
the system will unnecessarily lower the rate assigned to those
flows. To alleviate this problem, either cross-layer information
exchange or user feedback can be considered so that the proxy
can estimate the link throughput more accurately. We defer
this to our future study. Fig. 10(c) shows the throughput of
the background TCP traffic over time. TCP throughputs are
very similar under both types of competing video streams. This
indicates that the proxy-based adaptation is also TCP-friendly.
C. Multiple Receivers With Static Behavior
In this second part, we isolate the randomness of the wireless
network and use fixed wireless links to drive the simulator. The
topology setup is given in Fig. 8(b) where two or more servers
stream videos via individual wireless links to the receivers depends on the specific scenario. In each scenario, we set different
link conditions for different receivers. In this set of simulations,
there is no background traffic injected.
1) Content-Aware Rate Allocation Among Two Users: We
consider two nodes M1 and M2 share the same AP node. The
link between AP and M1 has a PHY rate of 1 Mbps while
the link between AP and M2 has a PHY rate of 11 Mbps. M1
streams the FOREMAN sequence starting from time 0 and lasts
for 200 seconds, whereas M2 starts to stream FOOTBALL at
time 20 seconds and finishes at time 250 seconds. Fig. 11 shows
the sending rate for the two competing video streams. As can be
seen in Fig. 11(a), with the proxy-adapt approach (with equal
weights for the two videos) the rate allocation of two streams

HU et al.: PROXY-BASED MULTI-STREAM SCALABLE VIDEO ADAPTATION

1649

Fig. 11. Comparison of video playback rate achieved by TFRC, and the proposed proxy-based scheme. In this simulation, the FOREMAN sequence and the
FOOTBAL sequence share a base station with PHY rate of 1 Mbps and 11 Mbps respectively. FOREMAN lasts between 0–200 sec, FOOTBALL between 20–250
sec. Same weights are used for both receivers. (a) Proxy-based (non-unified model); (b) proxy-based (unified model); (c) TFRC.

Fig. 12. Normalized quality and video rate seen by users. The number of users ranges from 4 to 36 where half of them receive FOREMAN and the others receive
FOOTBALL. In addition, half of the members within each group have 6 Mbps PHY rate and the others have 54 Mbps PHY rate. (a) Average video rate for each
category of users: TFRC case; (b) average video rate for each category of users: Proxy-adapt case; (c) average normalized quality for each category of users: TFRC
case; (d) average normalized quality for each category of users: Proxy-adapt case.

converges in a short time period with FOOTBALL being allocated more rate. After session 1 ends, session 2 quickly jumps
to the full video rate. On the other hand, TFRC converges to a
equilibrium point which gives similar rates to both sessions as
can be seen in Fig. 11(c). Some rate variations are observed for
the second stream (FOOTBALL) due to the limited rate choices.
TFRC also requires more time to converge, e.g. when the first
stream ends, TFRC takes 3 more seconds to converge to the full
video rate for the second stream.
The results shown in Fig. 11(a) were obtained with content
dependent parameter values for
and
in the rate-quality
model. To examine the sensitivity of the rate allocation results
to these parameters, we also run the simulations with the unified
model and the results are shown in Fig. 11(b). Comparing these
two figures, we can see that the rate allocation stabilizes at the
same level for both settings. This suggests that it is viable to use
fixed parameters for the Rate-Quality model, while performing
rate allocation.
2) Performance With Varying Number of Users: In this scenario, an AP is shared by multiple users, each receives one video

stream. The number of concurrent users ranges from 4 to 36
with half of them receiving FOREMAN and the others receiving
FOOTBALL. In both groups of receivers, half of them have a
good link condition of 54 Mbps PHY rate and the others have a
fair link condition of 6 Mbps PHY rate. Equal weights are used
for all receivers and they all start at time 0. The total simulation
time is 100 seconds. Results are averaged over a duration of 60
seconds after the system has reached convergence.
Fig. 12 shows the average video rate and the resulting
normalized quality for each category of receivers. Comparing
Fig. 12(a) and Fig. 12(b), we can easily notice the effectiveness
the content-aware and link throughput-aware rate adaptation
of the proxy-based approach. Fig. 12(a) shows that, with
TFRC, all users are receiving similar video rate regardless
of their video characteristics and link status. This also leads
to the head-of-line blocking especially when the system is
overloaded with large number of receivers. As can be seen
in Fig. 12(a), the video rate for all receivers are similar to
the receiver with 6 Mbps PHY rate. Fig. 12(b) confirms that
with proxy-based scheme, FOOTBALL users will receive

1650

IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 15, NO. 7, NOVEMBER 2013

TABLE III
WEIGHT SETTINGS FOR DIFFERENTIAL SERVICES

or differentiated quality as it attempts to allocate same rates to
all users. As a result, while FOREMAN users enjoy a higher
quality, all FOOTBALL users experience much lower quality.
VIII. CONCLUSIONS AND FUTURE WORK

Fig. 13. Users’ receiving rate and quality under the Proxy-based and TFRC
schemes with heterogeneous weight assignment. (a) Rate; (b) quality.

higher rate than FOREMAN users if their channel conditions
are the same, whereas users with poor channel quality will
receive lower rates than those with good channel quality. The
resulting average normalized qualities for both scenarios are
shown in Fig. 12(c) and Fig. 12(d). With TFRC, more complex
video (FOOTBALL) is delivered with a lower quality. Also,
same video content receivers have similar quality even though
their link throughputs are different. On the other hand, with
proxy-based scheme, received quality not only depends on the
video content but also depends on the link throughput. It is also
clear that proxy-based scheme gives larger aggregated quality
than TFRC.
3) Differential Services: Previous results for the proxy-based
approach are all obtained with equal weights for all users. Although the sending rates are content-aware, with higher rates
allocated to more complex video, the received qualities are not
equalized. By assigning different weights to different users,
the proxy-based scheme provides the capability of differential
services or equalizing receivers’ qualities. We consider an
AP being shared by 9 users divided into three groups. The
first group, denoted as G1, receives FOREMAN; the second
group, G2, receives FOOTBALL and the third group G3, also
receives FOOTBALL. Two sets of weightings are investigated
as shown in Table III. The first one (denoted as Proxy-1),
targeting for equal subjective quality, has weights 1 and 2.6
and 2.6 respectively for the three groups. These weights are
chosen as discussed in Section VI-C-4. The second set (denoted
as Proxy-2) has weights 1, 2.6 and 10 respectively for the
three groups, so that G1 and G2 receive similar quality and
G3 receives a higher quality. In practice, the second set may
represent the case where one group of users paid premium
price for better video services. All users have link PHY rate
12 Mbps. As shown in Fig. 13, with proxy-based scheme and
the first set of weights (Proxy-1), all FOOTBALL receivers
obtain higher rates than FOREMAN receivers and all receivers
receive very similar quality. With the second set of weights
(Proxy-2), the premium users are able to receive a much higher
quality than the other users. TFRC can not provide either equal

We proposed a proxy-based subjective-quality aware content-adaptation solution targeting for wireless edge network. We
first derived a novel analytical rate-quality model which can be
utilized for quality-aware network optimization, followed by a
video chunk pre-ordering method that achieves rate-quality optimality. Then, we investigated an iterative multi-stream rate
allocation scheme at the proxy, that can maximize a weighted
sum of received video quality at all receivers, given the link
bandwidths of all receivers. The algorithm explicitly accounts
for heterogeneity in both the video streams and the underlying
wireless link capacities experienced by different users. It can
converge very fast and accurately track the available network resources. Requiring only the queue length at the congested access
point and the throughputs of individual outgoing video streams,
it does not require any additional feedback from the receivers.
The proposed scheme also readily supports differentiated service for users of different relative importance levels, yet competes fairly against conventional TCP flows.
We note that the rate adaptation framework proposed in this
work can potentially be extended for other video streaming scenarios, with or without the support of scalable video coding.
For instance, one can adopt the same notion of maximizing
overall viewing experience for adaptive HTTP video streaming,
in which case the proxy either prefetches multiple pre-encoded
versions of the same video segment, or generate them on-the-fly
from a received high-quality single-layer video. Furthermore,
the proposed iterative rate-allocation scheme is not limited to
our rate-quality model; rather it is applicable to any concave
rate-quality function. It can be extended to accommodate the
spatial scalability as well.
REFERENCES
[1] Cisco, Visual Networking Index: Global Mobile Data Traffic Forecast
Update, 2010–2015, Feb. 2011.
[2] S. Floyd, M. Handley, J. Pahdye, and J. Widmer, “Tcp friendly rate control (TFRC): Protocol specification,” RFC 5348 (Proposed Standard),
Sep. 2008.
[3] S. Lu, V. Bharghavan, and R. Srikant, “Fair scheduling in wireless
packet networks,” IEEE/ACM Trans. Netw., vol. 7, no. 4, pp. 473–489,
Aug. 1999.
[4] ITU-T Recommendation H.264-ISO/IEC 14496-10(AVC), Advanced
Video Coding for Generic Audiovisual Services, Amendment 3: Scalable Video Coding, ITU-T and ISO/IEC JTC 1, 2005.
[5] H. Schwarz, D. Marpe, and T. Wiegand, “Overview of the scalable
video coding extension of H.264/AVC,” IEEE Trans. Circuits Syst.
Video Technol., vol. 17, no. 9, pp. 1103–1120, Sep. 2007.
[6] Y. Wang, Z. Ma, and Y.-F. Ou, “Modeling rate and perceptual quality
of scalable video as functions of quantization and frame rate and its
application in scalable video adaptation,” in Proc. Packet Video, May
2009, pp. 1–9.
[7] E. Kohler, M. Handley, and S. Floyd, “Datagram congestion control
protocol (DCCP),” RFC 4340, Mar. 2006.
[8] H. Chen and J. E. Thropp, “Review of low frame rate effects on human
performance,” IEEE Trans. Syst., Man, Cybern. A: Syst. Humans, vol.
37, no. 6, pp. 1063–1076, Nov. 2007.
[9] Y. Wang, S.-F. Chang, and A. C. Loui, “Subjective preference of
spatio-temporal rate in video adaptation using multi-dimensional
scalable coding,” in Proc. IEEE Int. Conf. Multimedia and Expo, Jun.
2004, pp. 1719–1722.

HU et al.: PROXY-BASED MULTI-STREAM SCALABLE VIDEO ADAPTATION

[10] K.-C. Yang, C. C. Guest, K. El-Maleh, and P. K. Das, “Perceptual temporal quality metric for compressed video,” IEEE Trans. Multimedia,
vol. 9, no. 7, pp. 1526–1535, Nov. 2007.
[11] Q. Huynh-Thu and M. Ghanbari, “Temporal aspect of perceived quality
in mobile video broadcasting,” IEEE Trans. Broadcast., vol. 54, no. 3,
pp. 641–651, Sep. 2008.
[12] S. H. Jin, C. S. Kim, D. J. Seo, and Y. M. Ro, “Quality measurement
modeling on scalable video applications,” in Proc. IEEE Int. Workshop
Multimedia Signal Processing, Oct. 2007, pp. 131–134.
[13] K. Yamagishi and T. Hayashi, “QRP08-1: Opinion model for estimating video quality of videophone services,” in Proc. IEEE Global
Communications Conf., Nov. 2006, pp. 1–5.
[14] Y.-F. Ou, Z. Ma, T. Liu, and Y. Wang, “Perceptual quality assessment
of video considering both frame rate and quantization artifacts,” IEEE
Trans. Circuits Syst. Video Technol., vol. 21, no. 3, pp. 286–298, Mar.
2011.
[15] Q. Zhang, W. Zhu, and Y. Zhang, “End-to-end QoS for video delivery
over wireless internet,” Proc. IEEE, vol. 93, no. 1, pp. 123–134, Jan.
2005.
[16] J. Cabrera and A. Ortega, “Stochastic rate control of video coders for
wireless channels,” IEEE Trans. Circuits Syst. Video Technol., vol. 12,
no. 6, pp. 496–510, Jun. 2002.
[17] L. Haratcherev, J. Taal, K. Langendoen, R. Lagendijk, and H. Sips,
“Optimized video streaming over 802.11 by cross-layer signaling,”
IEEE Commun. Mag., vol. 44, no. 1, pp. 115–121, Jan. 2006.
[18] P. van Beek and M. U. Demircin, “Delay-constrained rate adaptation
for robust video transmission over home networks,” in Proc. IEEE Int.
Conf. Image Processing, Sep. 2005, pp. 173–176.
[19] T. Stockhammer, M. Walter, and G. Liebl, “Optimized h. 264-based
bitstream switching for wireless video streaming,” in Proc. IEEE Int.
Conf. Multimedia and Expo, Jul. 2005, pp. 1396–1399.
[20] J. R. Ohm, “Advances in scalable video coding,” Proc. IEEE, vol. 93,
no. 1, pp. 42–56, Jan. 2005.
[21] T. Schierl, T. Stockhammer, and T. Wiegand, “Mobile video transmission using scalable video coding,” IEEE Trans. Circuits Syst. Video
Technol., vol. 17, no. 9, pp. 1204–1217, Sep. 2007.
[22] M. Chen and A. Zakhor, “Rate control for streaming video over wireless,” in Proc. IEEE Int. Conf. Computer Communications, Mar. 2004,
pp. 1181–1190.
[23] H. Luo, D. Wu, S. Ci, H. Sharif, and H. Tang, “TFRC-based rate
control for real-time video streaming over wireless multi-hop mesh
networks,” in Proc. IEEE Int. Conf. Communications, Jun. 2009,
pp. 1–5.
[24] G. Yang, T. Sun, M. Gerla, M. Y. Sanadidi, and L.-J. Chen, “Smooth
and efficient real-time video transport in the presence of wireless errors,” ACM Trans. Multimedia Comput., Commun., Applicat., vol. 2,
no. 2, pp. 109–126, May 2006.
[25] J. Chakareski and P. Frossard, “Rate-distortion optimized distributed
packet scheduling of multiple video streams over shared communication resources,” IEEE Trans. Multimedia, vol. 8, no. 2, pp. 207–218,
April 2006.
[26] L. Zhou, X. Wang, W. Tu, G. Muntean, and B. Geller, “Distributed
scheduling scheme for video streaming over multi-channel multi-radio
multi-hop wireless networks,” IEEE J. Select. Areas Commun., vol. 28,
no. 3, pp. 409–419, Apr. 2010.
[27] X. Zhu, R. Pan, N. Dukkipati, V. Subramanian, and F. Bonomi, “Layered internet video engineering (LIVE): Network-assisted bandwidth
sharing and transient loss protection for scalable video streaming,” in
Proc. IEEE Int. Conf. Computer Communications Mini-Conference,
Mar. 2010.
[28] X. Zhu and B. Girod, “Distributed media-aware rate allocation for
wireless video streaming,” IEEE Trans. Circuits Syst. Video Technol.,
vol. 20, no. 11, pp. 1462–1474, Jan. 2010.
[29] S. Khan, S. Duhovnikov, E. Steinbach, and W. Kellerer, “MOS-based
multiuser multiapplication cross-layer optimization for mobile multimedia communication,” Adv. Multimedia, vol. 2007, no. 1, pp. 1–11,
Jan. 2007.
[30] A. Ksentini, M. Naimi, and A. Gueroui, “Toward an improvement of
H.264 video transmission over IEEE 802.11e through a cross-layer architecture,” IEEE Commun. Mag., vol. 44, no. 1, pp. 107–104, Jan.
2006.
[31] M. van der Schaar and N. Shankar, “Cross-layer wireless multimedia
transmission: Challenges, principles and new paradigms,” IEEE Wireless Commun., vol. 12, no. 4, pp. 50–58, Aug. 2005.

1651

[32] H. Zhang, Y. Zheng, M. Khojastepour, and S. Rangarajan, “Cross-layer
optimization for streaming scalable video over fading wireless networks,” IEEE J. Select. Areas Commun., vol. 28, no. 3, pp. 344–353,
Apr. 2010.
[33] J. Chakareski, “In-network packet scheduling and rate allocation: A
content delivery perspective,” IEEE Trans. Multimedia, vol. 13, no. 5,
pp. 1092–1102, Oct. 2011.
[34] M. Wien, H. Schwarz, and T. Oelbaum, “Performance analysis of
SVC,” IEEE Trans. Circuits Syst. Video Technol., vol. 17, no. 9, pp.
1194–1203, Sep. 2007.
[35] Z. Ma, M. Xu, Y.-F. Ou, and Y. Wang, “Modeling rate and perceptual
quality of video as functions of quantization and frame rate and its
applications,” IEEE Trans. Circuits Syst. Video Technol., vol. 22, no.
5, pp. 671–682, May 2012.
[36] S. Ulukus and R. D. Yates, “Stochastic power control for cellular radio
systems,” IEEE Trans. Commun., vol. 46, no. 6, pp. 784–798, Jun.
1998.
[37] F. Kelly, A. Maulloo, and D. Tan, “Rate control in communication networks: Shadow prices, proportional fairness and stability,” J. Oper.
Res. Soc., vol. 49, no. 3, pp. 237–252, Mar. 1998.
[38] R. Srikant, The Mathematics of Internet Congestion Control. Cambridge, MA, USA: Birkhauser, 2003.
[39] Ns-2 Network Simulator. [Online]. Available: http://www.isi.edu/
nsnam/ns/.
[40] JSVM SVC Reference Software. [Online]. Available: http://ip.hhi.de/
imagecom_G1/savce/downloads/.
[41] Cisco, QoS on Wireless LAN Controllers and Lightweight APs
Configuration Example. [Online]. Available: http://www.cisco.
com/en/US/tech/tk722/tk809/technologies_configuration_example09186a00807e9717.shtml.

Hao Hu received the B.S. degree from Nankai University and the M.S. degree from Tianjin University
in 2005 and 2007 respectively, and the Ph.D. degree
from Polytechnic Institute of New York University in
January 2012.
He is currently with the Advanced Architecture
and Research group, Cisco Systems, San Jose, CA.
He interned in the Corporate Research, Thomson
Inc., NJ in 2008 and Cisco Systems, CA in 2011.
His research interests include video QoE, video
streaming and adaptation.

Xiaoqing Zhu is currently a member of the Advanced Architecture & Research Group at Cisco
Systems Inc. She received the B.Eng. degree in
Electronics Engineering from Tsinghua University,
Beijing, China, in 2001. She received both the M.S.
and Ph.D. degrees in Electrical Engineering from
Stanford University, CA, USA, in 2002 and 2009,
respectively. She interned with the IBM Almaden
Research Center in 2003, and was at Sharp Labs
of America in the summer of 2006. Dr. Zhu was
awarded the Stanford Graduate Fellowship from
2001 to 2005. She was recipient of the best student paper award in ACM
Multimedia 2007.
Dr. Zhu’s research interests lie at the intersection of multimedia signal
processing, wireless communications, and networking. She has served
as reviewer for many journals and magazines, including IEEE JOURNAL
ON SELECTED AREAS IN COMMUNICATIONS, IEEE TRANSACTIONS ON
WIRELESS COMMUNICATIONS, IEEE TRANSACTIONS ON MULTIMEDIA, IEEE
COMMUNICATIONS MAGAZINE, and IEEE NETWORK MAGAZINE. She has
also helped organize various conferences and workshops, such as IEEE
GLOBECOM, IEEE International Conference on Computing, Networking and
Communication (ICNC), and SPIE Visual Communications and Image Processing (VCIP). She served as guest editor for IEEE Technical Committee on
Multimedia Communications (MMTC) E-Letter, IEEE JOURNAL ON SELECTED
AREAS IN COMMUNICATIONS, and IEEE TRANSACTIONS ON MULTIMEDIA.

1652

Yao Wang (M’90–SM’98–F’04) received the B.S.
and M.S. degrees in electronic engineering from Tsinghua University, Beijing, China, in 1983 and 1985,
respectively, and the Ph.D. degree in electrical and
computer engineering from the University of California, Santa Barbara, in 1990.
Since 1990, she has been with the Electrical and
Computer Engineering Faculty, Polytechnic University, Brooklyn, NY (now Polytechnic Institute of
New York University). She is the leading author of
the textbook Video Processing and Communications
(Prentice-Hall, 2001). Her current research interests include video coding and
networked video applications, medical imaging, and pattern recognition.
Dr. Wang has served as an Associate Editor for the IEEE TRANSACTIONS
ON MULTIMEDIA and the IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR
VIDEO TECHNOLOGY. She received the New York City Mayor Award for Excellence in Science and Technology in the Young Investigator Category in 2000.
She was a co-winner of the IEEE Communications Society Leonard G. Abraham
Prize Paper Award in the Field of Communications Systems in 2004. She received the Overseas Outstanding Young Investigator Award from the National
Natural Science Foundation of China in 2005 and was named the Yangtze River
Lecture Scholar by the Ministry of Education of China in 2007.

Rong Pan received her Ph.D. degree from the
Electrical Engineering Department at Stanford
University in 2002. Currently, she is a Distinguished
Engineer/Senior Director at Cisco where she heads
a team in the advanced architecture and research
division. She is an author for more than 30 technical
papers and an inventor of 27 patents. Her work and
innovations have been widely recognized. She is a
key-inventor of the QCN algorithm, which is now an
IEEE 802.1 standard on congestion notification for
Data Center Ethernet. Her other algorithms, such as
AFD (a simple, approximate scheme to Fair Queueing), have had major impact
on multiple Cisco’s flagship products with a combined revenue of more than
$10 B. The CHOKe algorithm that she developed as part of her Ph.D. thesis has
become a standard QoS feature in Linux Kernel since version 3.2.2. Currently,
she is working on the buffer bloat problem in the Internet and leading Cisco’s
effort at IETF on this topic. She has won a best paper award and served as
program committee members at IEEE conferences, and she will serve as the
technical chair at IEEE High Performance Switching and Routing Conference
2014.

IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 15, NO. 7, NOVEMBER 2013

Jiang Zhu is a senior technical leader in Advanced
Architecture and Research group at Cisco Systems,
Inc. He has over 15 years of industrial experience
building large-scale distributed media systems. His
research focuses on adaptive content networking,
large-scale data systems, software defined networking (SDN), cloud service orchestrations and
applications of data mining and machine learning
in these fields. He did his doctoral study focusing
on SDN and OpenFlow in High Performance Networking Group at Stanford University. He also
received his M.S. degrees in Electrical Engineering and in Management Science & Engineering from Stanford University. Before Stanford, he obtained his
M.S. in Computer Science from DePaul University and B.Eng. in Automation
from Tsinghua University.

Flavio Bonomi is a Cisco Fellow, Vice President,
and is the Head of the Advanced Architecture and
Research Organization at Cisco Systems, in San
Jose, CA. Over the past years, he has led a number
of Cisco’s Advanced Architecture activities, and
contributed to the establishment of Cisco’s virtual,
distributed Research Organization, including architects and researchers embedded in a wide number of
organizations across Cisco, and collaborating with a
growing network of industry and university partners.
He received his Ph.D. in Electrical Engineering in
1985, and a Master in Electrical Engineering in 1981 from Cornell University
in Ithaca, NY. He received his Electrical Engineering Degree from Pavia University, in Italy.

Using Semantic Web technology in Multi-Agent Systems:
a case study in the TAGA trading agent environment
Youyong Zou, Tim Finin, Li Ding, Harry Chen, Rong Pan
U.Maryland Baltimore County
1000 Hilltop Circle
Baltimore, MD, 21250
410-455-3971

{yzou1,finin,dingli1,hchen4 }@cs.umbc.edu
ABSTRACT
Travel Agent Game in Agentcities (TAGA) is the framework that
extends and enhances the Trading Agent Competition (TAC)
scenario to work in Agentcities, an open multi agent environment
based on FIPA compliant pla tforms. TAGA uses the semantic web
languages and tools (RDF and OWL) to specify and publish the
underlying common ontologies; as a content language within the
FIPA ACL messages; as the basis for agent knowledge bases via
XSB-based reasoning tools; to describe and reason about services.
TAGA extends the FIPA protocols to support open market auctions
and enriches the Agentcities with auction services. The introducing
of the semantic web languages improves the interoperability among
agents. TAGA is intended as a platform for research in multi-agent
systems, the semantic web and automated trading in dynamic
markets as well as a self-contained application for teaching and
experimentation with these technologies.

Keywords
Agentcities, FIPA, Multi Agent System, OWL, Semantic Web,
Trading Agent Competition.

1. INTRODUCTION
The Trading Agent Competition (TAC) [Wellman, 2002] was a test
bed for intelligent software agents that interact through simultaneous
auctions to obtain services for customers. The trading agents
operated within the travel market scenario, buying and selling goods
to best serve their given travel clients. TAC was designed to
promote and encourage research in markets involving auction and
autonomous trading agents and had proven to be successful after
three consecutive year’s competitions.
Although TAC’s framework, infrastructure and game rules had
evolved over the past three competitions [Stone, 2000] [Greenwald,
2001] [Wellman, 2002], the assumptions and approach of TAC
limited its usefulness as a realistic test bed for agent based
automated commerce. TAC used centralized market server as the

sole mechanism for service discovery, communication, coordination,
commitment, and control among the participating software agents.
The trading agents communicate with the central auction server
through network socket interface, exchanging pre-defined XMLbased messages. In real world, the auction servers (for example,
priceline.com and hotwire.com) and service providers are
distributed among the massive open Internet and have distinct
service descriptions and diverse service access interfaces. The
abstractness and simplicity of the TAC approach helped to launch it
as a research vehicle for studying bidding strategies, but are now
perceived as a limiting factor for exploring the wide range of issues
inherent in automated trading in open environment.
Agentcities [Dale, 2002] is the international initiative designed to
explore the commercial and research potential of agent-based
applications by constructing an open distributed network of
platforms to host diverse agents and services. The ultimate goal is to
enable the dynamic, intelligent and autonomous composition of
services to achieve user and business tasks, therefore creating
compound services to address changing needs. In such an open and
distributed environment, the need of standard mechanisms and
specifications is crucial for ensuring interoperability of distinct
systems. The Foundation for Intelligent Physical gents (FIPA)
produces such standards for heterogeneous and interacting agents
and agent-based systems. In the production of these standards,
FIPA promotes the technologies and interoperability specifications
that facilitate the end-to-end inter-working of intelligent agent
systems in modern commercial and industrial settings.
Inspired by TAC, we have developed Travel Agent Game in
Agentcities (TAGA) on the foundation of FIPA technology and the
Agentcities infrastructure. The agents and services use FIPA
supported languages, protocols and service interfaces to create the
travel market framework and provide stable communication
environment where messages expressed in semantic languages can
be exchanged. The travel market is the combination of auctions and
varying markets including service registries, service brokerage,
wholesalers, peer-to-peer transactions, bilateral negotiation, etc.
This provides a richer test bed for experimenting with agents and
web services as well as a interesting scenario to test and challenge
agent technology. TAGA is running as a continuous open game at
http://taga.umbc.edu/ and source code is available for research and
teaching purposes.
The next section introduces the TAGA game and six types of
agents. The details of using semantic web technology are presented
in Section three. We discuss TAGA’s features and our research

Copyright is held by the author/owner.
ICEC 2003. Pittsburgh, PA
ACM 1-58113-788-5/03/09

95

contributions in Section four and suggest the future works in Section
five.

2. TAGA GAME AND AGENTS
We design TAGA as a general framework for running agent-based
market simulations and games. Our first use of TAGA has been to
build a travel competition along the lines that used in the last three
year‘s TACs. In the competition, customers travel from City A to
City B and spend several days before flying back. A travel
package includes a round-trip flight ticket, corresponding hotel
accommodation and tic kets to entertainment events. A travel agent
(an entrant to the game) competes with other travel agents in
making contracts with customers and purchasing the limited travel
services from the Travel Service Agents. Customer selects the
travel agent with best travel itinerary. The objective of the travel
agent is to acquire more customers, fulfill the customer’s travel
package, and maximize the profit.
TAGA provides a flexible framework to run the travel market
game. Figure 1 show the structure of TAGA. The collaboration and
competition among six types of agents who play different market
roles simulate the real world travel market. We find that basing our
implementation on FIPA compliant agent platforms has made the
framework extremely flexible. We’ll briefly describe the different
agents in our initial TAGA game.

A Bulletin Board Agent (BBA) provides a mechanism helping
customer agents find and engage one or more travel agents.
A Customer Agent (CA) represents an individual customer who
has particular travel constraints and preferences. Its goal is to
engage one or more TAs, negotiate with them over travel packages,
and select one TA that is able to acquire all needed travel service
units.
The Market Oversight Agent monitors the game and updates the
financial model after each reported transaction and finally
announces the winning TA when the game is over.
The basic cycle of the TAGA game has the following five stages:
•
•

•

•

•

Figure 1: TAGA Architecture
The Auction Service Agent (ASA) operates all of the auctions in
TAGA. Supported auction types include English and Dutch auctions
as well as other dynamic markets similar to Priceline.com and
Hotwire.com.

A customer-generating agent creates a new customer with
particular travel constraints and preferences chosen from a
certain distribution.
The CA sends the customer’s travel constraints and
preferences to the BBA in the form of a CFP (call for
proposal) message. The BBA forwards the CA’s CFP
message to each of the TAs that has registered with it. Each
TA considers the CA's CFP independently and decides
whether and how to respond.
When deciding to propose a travel package, The TA contacts
the necessary ASAs and SAs and assembles a travel itinerary.
Note that the TA is free to implement a complex strategy using
both aggregate markets (ASAs) as well as direct negotiation
with SAs. The proposal to the CA includes the travel itinerary,
a set of travel units, the total price and the penalty to be
suffered by the TA if it is fail to complete the transaction.
The CA negotiates with the TAs ultimately selecting one from
which to purchase an itinerary based on its constraints,
preferences and purchasing strategy (which might, for
example, depend on a TA’s reputation).
Once the TA has a commitment from the CA, it attempts to
purchase the units in the itinerary from the ASAs and SAs.
There are two possible outcomes: the TA acquires the units
and completes the transaction resulting in a satisfied CA and a
profit or loss for the TA, or the TA is unable or unwilling to
purchase all of the units, resulting in an aborted transaction and
the invocation of the penalty (which can involve both a
monetary and a reputation component).

3. AGENT COMMUNICATION
3.1 Agent Communication Model

A Service Agent (SA) offers travel related service units such as
airline tickets, lodging and entertainment tickets. Each class of travel
related service has multiple providers with different service quality
level and with limited service units. It allows other agents to query
its description (e.g. service type, service quality, location) and its
inventory (the availability or price of a certain type of service unit).
Other agents may directly buy the service units through published
service interface. SA also bids intentionally in the auctions to sell its
good, e.g. listing its goods in auction and wait for the proper buyer.

The previous TACs used a straightforward client-server
architecture in which a single TAC server managed all travel
service suppliers as well as the customers. Game participants wrote
travel agency (TA) agents that connected as clients to the central
TAC server. Moreover, these TA agents could only interact with
service providers through centralized auction markets. While this
architecture greatly simplifies both the development of the TAC
infrastructure and the programming of a TAC client, it is a poor
model for commerce in the real world.

A Travel Agent (TA) is a business that helps customers acquire
travel service units and organize travel plan. The units can be
bought either directly from the service agents, or through an auction
server.

Peer-to-peer or multi-agent systems offer a more realistic model
where customers, service providers and various kinds of
“middlemen”, including market providers, operate as autonomous
peer agents. Moreover, agents can develop complex strategies,
which involved a combination of direct transactions (e.g., TA buy

96

direct from hotel agent) as well as auction-mediated transactions of
various kinds. Finally, adopting a multi-agent systems approach
integrated all aspects of commerce (service discovery, information
seeking, negotiation, decision making, commitment, transaction
execution et.) in a more natural manner.
The FIPA standards offer mature specifications for multi-agent
systems communication, interactions and infrastructure with an
emphasis on agent communication languages (ACLs) and protocols.
We found the FIPA framework to be a good one for TAGA when
augmented with the semantic web languages RDF [zou, 2003] and
OWL. In the remainder of this section we will describe the choices
made for the content la nguages.

These examples were used as an expressive test for a candidate
FIPA content language and compared the result of encoding these
in SL, KIF, ebXML, Prolog and DAML. Clearly OWL is less
expressive than SL, KIF or Prolog, but the OWL version of these
test cases given in Table 1 show that it’s up to most of tasks it might
be asked to serve.
Table 1: OWL Expre ssivity Test
Expressio
n

Representation

Comment

“Schröding
er’s Cat is
alive”

<Cat rdf:ID=“schrödinger-s_cat”>
<owner>Shrodinger</owner>
<status> alive </status>
</Cat>

“Cats are
animals”

<owl:Class rdf:ID=“cat”>
<rdfs:subClassOf
rdf:resource=“#animal”>
</owl:Class>
<fipaowl:Action
rdf:ID=”tea_action1”>
<fipaowl:act>making-tea
</fipaowl:act>
<fipaowl:actor>you</fipaowl:actor>
<fipaowl:Action>
<Behavior rdf:ID=“drinktoomuch”>
<hasBehavior>excessive_drinking</
hasBehavior>
<healthy>bad</healthy>
</Behavior >

There is a
live cat in
the world
whose
owner is
Shrodinger.
Cat is
subclass of
animal

3.2 OWL as Content Language
The content language is a language used to express the content of
messages exchanged between agents. The FIPA communication
infrastructure allows agents to communicate using any mutually
understandable content language as long as it satisfies a few
minimal criteria as a FIPA compliant content language. Published
FIPA specifications provide a library of registered FIPA compliant
content la nguage, including FIPA-SL, XML and RDF. A good
content la nguage should be able to express rich forms of content
and can be efficiently processed and fit well with existing
technology. XML, used by the TAC system, is adequate as a low
level language for encoding information but falls short as a language
in which to express information at the knowledge level, even when
augmented by more recent components such as XML Schema, XSL
or through applications such as WSDL.
Our TAGA system uses OWL [Dean, 2002] as the content
language for agent communic ation. Compared with RDF that used
on our previous TAGA work [Zou, 2003], OWL has a well-defined
model-theoretic semantics as well as an axiomatic specification that
determines the intended interpretations of the language. OWL is
unambiguously computer-interpretable, thus making it amenable to
agent interoperability and automated reasoning techniques. The
benefit of adopting a stronger semantically rich content language
like OWL is that it facilitates a higher-level of interoperability
between agents. By agreeing on how meaning is conveyed, it is
simpler for applications to share meaningful content.
We have defined the OWL ontology for use as a FIPA-compliant
content language. In addition to the basic required classes (e.g.,
Agent, ACLMessage, Service, etc.) and necessary expressive
requirement (such as Proposition, Action, and Reification), our
ontology provides supports for expressing rules, queries and
responses to queries. We believe that OWL is a good choice as a
general ACL content language for four reasons. First, its
expressive power as a knowledge representation language seems to
be adequate for many if not most needs of current agent based
systems. Second, it offers better support for using terms drawn
from multiple ontologies than do current popular ACL content
languages. Third, as a semantic web la nguage, it is designed to fit
into and integrate with web-based information and service systems.
Fourth, OWL has the potential to be a widely accepted and used
representation language, enhancing the potential for interoperability
among many systems. We will touch briefly on the first two points
and leave the others as exercises for the reader.
To demonstrate that OWL is an adequate la nguage for ACL
content we consider a list of test cases presented in [Bothelo 2002].

97

“You
making the
tea”

“Drinking
too much
is bad for
you”

“All red
things”

“Any color
a car might
have”

“All things
are hot”

<owl:Class rdf:ID="allredthing ">
<owl:intersectionOf
rdf:parseType="Collection">
<owl:Classrdf:about="#Thing"/>
<owl:Restriction>
<owl:onProperty
rdf:resource="#hasColor" />
<owl:hasValue
rdf:resource="#Red" />
</owl:Restriction>
</owl:intersectionOf>
</owl:Class>
<owl:Class rdf:ID="anycarcolor">
<rdfs:subClassOf>
<owl:Restriction>
<owl:onProperty
rdf:resource="#color" />
<owl:allValuesFrom
rdf:resource="#CarColor " />
</owl:Restriction>
</rdfs:subClassOf>
</owl:Class>

<owl:Class rdf:about= “#Thing”>
<rdfs:subClassOf>

There is a
making-tea
action,
“you” are
the actor.
The
behavior of
drinking too
much is bad
for your
health.
The things
whose color
are red.

The color
that limits
the color
property
value in the
car colors.
This can
also be a
query:
“Select
color where
color in Car
Color”
All things’s
temperature

“Somethin
g is cold”

“Herring
or Perch”

“Vodka
and
Tonic”.

“Not
cricket”

“Success
implies
Payment”

“Luis has
the
persistent
goal that
W”
“Steve
Believes
X”

“Jonathan
Desires
Y”

“Matthias
Intends Z”

<owl:Restriction>
<owl:onProperty
rdf:resource="#tempterature"/>
<owl:hasValue rdf:resource="#hot"
/>
</owl:Restriction>
</rdfs:subClassOf>
</owl:Class>
<owl:Thing rdf:ID= “cold_thing”>
<temperature>cold</temperature>
</owl:Thing>

<owl:oneOf
rdf:parseType="Collection">
<owl:Thing rdf:about="#Vokda "/>
<owl:Thing rdf:about="#Perch"/>
</owl:oneOf>
<owl:union Of
rdf:parseType="Collection">
<owl:Class rdf:about="#Vodka " />
<owl:Class rdf:about="#Tonic" />
</owl:unionOf>
<owl:Class rdf:ID="Noncricket">
<owl:complementOf
rdf:resource="#Cricket " />
</owl:Class>
<fipaowl:Rule> <fipaowl:Implies >
<fipaowl:head >
Payment</fipaowl:head >
<fipaowl:body >Success
</fipaowl:body >
</fipaowl:Implies> </fipaowl:Rule>
<Person rdf:ID= “Luis”>
<hasPersistentGoals> W
</hasPersistentGoals>
</Person>

are hot.
Compared with other ACL content languages, OWL provides much
better support in modeling, maintaining, and sharing ontologies.
Standard content languages such as SL and KIF offer no explicit
mechanisms for ontology support. FIPA inherited the simple
mechanism for ontology specification first used in KQML that
essentially required that all content terms in a particular message be
tagged as coming from a single ontology. Although variations and
“work arounds” to this constraint have been proposed, implemented
and used, none have been formally adopted as part of the stable
FIPA specification. OWL supports multiple namespaces and
ontologies and, in fact, is a large part of its raison d'etre. Large
scale and open multi-agent systems will benefit from OWL’s
abilities to integrate information from different ontologies.
Moreover, OWL and other semantic web languages, will better
support other services essential to large scale open systems, such as
the capability to translate or map information from one ontology to
another and to negotiate meaning or otherwise resolve differences
between ontologies.

There exist
something
whose
temperature
is cold.

3.3 Understanding Messages
When an agent receives an incoming ACL message, it computes
the meaning of the message from the ACL semantics, the protocols
in effect, the content language and the conversational context. The
agent’s subsequent behavior, both internal (e.g., updating its
knowledge base) and external (e.g., generating a response) depends
on the correct interpretation of the message’s meaning. Thus, a
sound and, if possible, complete understanding the semantics of the
key communication components (ACL, protocol, ontologies, content
language, context) is extremely important. In TAGA, the service
providers are independent and autonomous entities, which makes
enforcing a design decision that all use exactly the same ontology or
protocol difficult, if not impossible. For example, the Delta Airline
service agent may has its own view of travel business and uses
class and property terms that extend an ontology used in the
industry as a whole. This situation parallels that for the semantic
web as a whole – some amount of diversity is inevitable and must
be panned for lest our systems become impossibly brittle.

The rule :
Payment :Success.

<Person rdf:ID=”steve”>
< hasProposition>
< Belief rdf:ID=”stevebelief1”>
< believe>true</believe>
< Statement > X</Statement>
</Belief> </hasProposition>
</Person >
<Person rdf:ID=”Jonathan”>
<hasProposition>
<Desire rdf:ID=”jonthandesire11”>
<desire >true</desire>
<Statement > Y </Statement>
</Desire> </hasProposition>
</Person >
<Person rdf:ID=”Matthias”>
< hasProposition>
<Intend rdf:ID=”Matthiasintend1”>
<intend>true</intend>
<Statement > Z </Statement>
</Intend> </hasProposition>
</Person >

The ontologies in TAGA are distributed and managed by multiple
parties. This distributed model is a better fit for deployment in an
open web environment. There is no centralized site or agent that has
to understand every ontologies. Ontologies and rules are designed
and implemented by service owners to reflect their business models
and meet their requirements; tan agent belonging to a service owner
is responsible for answering the question related to the ontologies it
uses. Ontologies store in local and may access only by local agent.
We could define personalized ontologies and rules. It would help
resolving the problem of security and trust.
Many of the agents we have implemented in the TAGA system use
FOWL (Flora OWL) to represent and reason about content
presented in RDF or OWL. FOWL is a flora-2 [Yang 2000]
program that interprets RDF and OWL represented as a collection
of RDF triples. Flora-2 is itself a compiler that compiles from a
dialect of f-logic into XSB, taking advantage of the tabling, HiLog
and well-founded semantics for negation features found in XSB. On
receiving an ACL message with content in RDF or OWL, a TAGA
agent parses the content into triples, which are then loaded into the
XSB engine for processing.

98

The message’s meaning (communicative act, protocol, content
language, ontologies and context) all play a part in the interpretation.
For example, receiving a query message using query protocol, the
agent searches its knowledge base for matching answers and
returns an appropriate inform message. TAGA uses multiple models
to reflect the multiple namespace and ontologies in the system. The
agent treats each ontology as an independent Model in XSB engine.
The support of ontology sharing and exchanging is achieved by
defining a set of ontology related actions:
•

NewInstance: this message creates an instance using the
specified ontology and the provided instance data;

•

OntologyQuery: this message queries other agents about the
terms defined in their ontology;

•

OntologyShare: this inform message is about the ontology
definition, which include Class/Property definition, ClassSubclass relation and Class-Property relation.

•

OntologyRelation: this message is about the conversion and
relations among class or property term defined different
ontologies. For example, agent A informs agent B that the
class Person is same class as the class Human used by agent
B. The relations include extension, identical and equivalent.
This message can be an inform message informing other
agents about the relation, or query message asking to confirm
the relation, or request message asking to translate the
ontology term used in multiple ontologies.

3.4 Query Support
Among the most important communicative acts used by agents are
those designed to support querying. The FIPA ACL has a very
simple query model supporting just two acts -- query-if and queryref – but allows a more complicated query to be encoded as a
request act. In order to use semantic web la nguages for ACL
content, we have experimented with the integration of a number of
RDF based approaches, including DQL, RQL, RDQL, Triple, and
TAP. Since a consensus query system has not yet emerged, we
have adopted an approach in which agents can use any of several
query systems and associated protocols. An agent specifies the
query languages and protocols it understands as part of its basic
service description. Other agents who intend to submit query to this
agent are expected to encode the query string in one of the support
languages. Table 2 is a query and answer example using RDQL
language.
Table 2: Query and answer example
Query:
<fipaowl:Query rdf:ID=”query1”>
<fipaowl:queryLanguage>rdql</fipaowl:queryLanguage>
<fipaowl:question>
“SELECT ?x,?y
FROM <people.rdf>
WHERE (?x,<dt:friend>,?y),(?y,<dt:friend>,?x)
AND ?x<^gt;?y
USING dt for <http://foo.org#>, rdf for
http://www.w3.org/1999/02/22-rdf-syntax-ns#”
</fipaowl:question>

99

<fipaowl:result_number>10</fipaowl:result_number>
</fipaowl:Query >
Answer:
<fipaowl:Query rdf:about=”query1”>
<fipaowl:queryLanguage>rdql</fipaowl:queryLanguage>
<fipaowl:result_number>1</fipaowl:result_number>
<fipaowl:answer>

“Array ( [0] => Array ( [?x] =>
http://foo.org/persons/Carl
[?y]
=>
http://foo.org/persons/Peter ) [1] => Array
( [?x] => http://foo.org/persons/Peter [?y]
=> http://foo.org/persons/Carl ) )”
</fipaowl:answer>
</fipaowl:Query>

We have found that the basic framework of FIPA standards
support this approach well by having a good set of primitive
communicative acts, a way for agents to define communication
protocols, and a sound mechanism by which agents can describe
their capabilities and the supporting services. We are planning to
experiment with adding mediator agents to TAGA that offer a query
translation service. Such an agent would be able to handle several
kinds of query languages permitting it to act as a proxy. For
example, agent A might wish to ask a DQL query of agent B, which
only understands RQL. A query translation service able to process
both DQL and RQL could provide the mediation service – receiving
a DQL query from A, sending appropriate RQL queries to B,
accepting the response, and reformulating to fit the DQL protocol.

4. DISCUSSION
In this section we will briefly discuss several additional design issues
we have addressed in TAGA.
Ontologies. In addition to the FIPA content la nguage ontology, we
have defined two domain ontologies in OWL. The first is a travel
ontology that covers the basic concepts of traveling needed in
TAGA, include the travel itinerary, customers, travel services and
service reservations. The second ontology is one for auctions. This
ontology is used to define the different kinds of auctions, the roles
the participants play in them, and the protocols used.
Service description and matching. FIPA agents are associated
with one or more FIPA platforms, each of which offers a set of
agent services including a Directory Facility (DF) agent that handles
service registration, deregistration and matching. When an agent
registers a service in a DF, it provides service information like the
service type and owner. However, more specific service
information may also be useful when searching for agent services.
For example, a customer may want a booking in a hotel with at least
three star rating, is close to public transportation, offers breakfast,
and accepts VISA card payments. This can be achieved with the
use of DAML-S [DAML-S, 2002] profile. In TAGA, every travel
service provider describes its service process model with DAML-S
language and publishes it as a web page. This covers basic service
information like address, phone number and service interface
information. For example, a hotel may describe booking service as:
customer name, payment methods, travel date as input; reserve

number as output; the effect of booking is one room occupied at the
travel date. The travel agent, who is responsible for organizing
travel package, is able to contact with customer agent and related
service agents and finds the best match. First the travel agent loads
the DAML-S parsing rule and planning rules into its XSB reasoning
engine. It then loads service agents’ DAML-S profiles and
customer’s personal profile. The best matching service providers
are selected and a most profitable travel package is composed
dynamically.
Implementation comments.
The original Trading Agent
Competitions relied on a few centralized market servers to handle
all interactions and coordination, including service discovery, agent
communication, coordination, and game control. In contrast, the
TAGA framework uses a distributed peer-to-peer approach based
on standard agent languages, protocols and infrastructure
components (FIPA, Agentcities), emerging standards for
representing ontologies, knowledge and services (RDF, OWL,
DAML-S) and web infrastructure (e.g., Sun’s Java Web Start).
Several FIPA pla tform implementations are currently used within
TAGA, including Jade and AAP (April Agent Platform),
demonstrating agent interoperability. Our current demonstration
system allows new users to dynamically join a running game at any
time. A dummy agent implemented in JADE can be downloaded
and run to instantiate a new TA agent. A simple GUI allows the
user to set operating parameters or the java code can be modified or
extended. A set of web based monitoring services allow one to see
the status of a game, examine messages being sent, lookup the
reputation of agents, etc.
Contribution. We see two main contributions in our work. First,
TAGA provides a rich framework for exploring agent-based
approaches to e-commerce like applications.
Our current
framework allows users to create their own agent (perhaps based
on our initial prototype) to represent a TA or SA and to include it in
a running game where it will compete with other system provided
and user defined agents. We hope that this might be a useful
teaching and learning tool, not only for multi-agent systems
technology, but also for the semantic web languages RDF and
OWL and their use in agent based systems. Secondly, we hope that
TAGA will be seen as a flexible, interesting and rich environment
for simulating agent-based trading in dynamic markets. Agents can
be instantiated to represent customers, aggregators, wholesalers,
and service provides all of which can make decisions about price
and purchase strategies based on complex strategies and market
conditions. Moreover, simulations like TAGA encourage exploring
aspects of e-commerce that go beyond auction theory. TA agents
might compete on their ability to better understand the descriptions
of services sought and services offered and the basic models of the
preferences of their users in order to best satisfy the needs of their
clients. These descriptions, of course, will be in a semantic web
language like OWL.

5. Conclusions and future work
Travel Agent Game in Agentcities (TAGA) is a framework that
extends and enhances the Trading Agent Competition (TAC)
system to work in Agentcities, an open multiagent systems
environment of FIPA compliant systems. We hope that TAGA will
serve as an experimental test-bed for several communities of users.

100

First, it provides an environment, which can be used to explore
aspects of multiagent systems technology based on the mature,
published FIPA standards. Research on multiagent systems
technology is best done with in a rich yet easily understood problem
domain. We have found that the travel agent scenario as originally
put forth by TAC provides both the richness as well as accessibility,
especially when opened up to be peer-to-peer. We are using
TAGA as a test-bed for research on the use of semantic web
languages (e.g., RDF and OWL) as content languages and as
service description languages. Future work is planned in adding
more sophisticated negotiation and ontology mapping to our TAGA
environment.
Second, we hope that TAGA could serve as an interesting
framework and test-bed for experiments with automated markets
and trading. By adding autonomous service provide agents (e.g., for
hotels) one could experiment with a dynamic market with both
“shopbots” and “pricebots” or investigate the role of intermediation
in the form of agents performing a wholesale function.
Third, we hope that others will find TAGA useful as a test,
demonstration and teaching environment, both in technology classes
focused multi-agent systems, FIPA standards or the semantic web
and in business or e-commerce classes focused on automating
commerce and trading, auctions or agent-based simulations.
The Agentcities project is exploring the delivery and use of agentbased services in an open, dynamic and international setting. We
are working to increase the integration of TAGA and emerging
Agentcities components and infrastructure and will include agents
running on handheld devices using LEAP.

6. REFERENCES
[Bothelo 2002] L. Bothelo, S. Willmott, T. Zhang, J. Dale, A review
of Content Languages Suitable for Agent-Agent Communication,
EPFL I&C Technical Report #200233.
[Dale, 2002] J. Dale, S. Willmot, and B.Burg: Agentcities:
Challenges and Deployment of Next-Generation Service
Environments. Proc. Pacific Rim Intelligent Multi-Agent Systems,
Tokyo, Japan, August 2002.
[DAML-S, 2002] The DAML Services Coalition (alphabetically
Anupriya Ankolenkar, Mark Burstein, Jerry R. Hobbs, Ora Lassila,
David L. Martin, Drew McDermott, Sheila A. McIlraith, Srini
Narayanan, Massimo Paolucci, Terry R. Payne and Katia Sycara):
DAML-S: Web Service Description for the Semantic Web, The
First International Semantic Web Conference (ISWC), Sardinia
(Italy), June, 2002.
[Dean, 2002] M. Dean and Guus Schreiber (eds): OWL Web
Ontology Language 1.0 Reference. W3C Working Draft.
[Greenwald, 2001] Amy Greenwald and Peter Stone: Autonomous
Bidding Agents in the Trading Agent Competition, IEEE Internet
Computing, March/April 2001.
[Greenwald, 2003] Amy Greenwald (ed.). The 2002 trading agent
competition: An overview of agent strategies. AI Magazine, to
appear
[Sagonas, 1994] Kostantinos Sagonas, Terrance Swift, and David S.
Warren: XSB as an efficient deductive database engine, In ACM
Conference on Management of Data (SIGMOD), 1994.

[Stone, 2000] Peter Stone and Amy Greenwald: The First
International Trading Agent Competition: Autonomous Bidding
Agents, Electronic Commerce Research Journal pp1-36, 2000.
[Wellman, 2002] Michael P. Wellman, Amy Greenwald, Peter
Stone, and Peter R. Wurman: The 2001 Trading Agent Competition,
Fourteenth Innovative Applications of Artificial Intelligence
Conference (IAAI-2002), pp935-941, Edmonton, August 2002.
[Willmott, 2001] Willmott, S., Dale, J., Burg, B., Charlton, P. and
O'Brien, P., Agentcities: A Worldwide Open Agent Network. In:
AgentLink News, Issue 8, November 2001.

101

[Yang, 2000] Guizhen Yang and Michael Kifer. FLORA:
Implementing an efficient DOOD system using a tabling logic
engine. Proceedings of Computational Logic --- CL-2000, number
1861 in LNAI, pp 1078--1093. Springer, July 2000.
[Zou, 2003] Youyong Zou, Tim Finin, Li Ding, Harry Chen, and
Rong Pan, TAGA: Trading Agent Competition in Agentcities,
Workshop on Trading Agent Design and Analysis, held in
conjunction with the Eighteenth International Joint Conference on
Artificial Intelligence, Monday, 11 August, 2003, Acuulco MX.

Query-Adaptive Ranking with Support Vector
Machines for Protein Homology Prediction
Yan Fu1 , Rong Pan2 , Qiang Yang3 , and Wen Gao4
1

Institute of Computing Technology and Key Lab of Intelligent Information
Processing, Chinese Academy of Sciences, Beijing 100190, China
2
School of Information Science and Technology,
Sun Yat-sen University, Guangzhou 510275, China
3
Department of Computer Science and Engineering,
Hong Kong University of Science and Technology, Hong Kong, China
4
Institute of Digital Media, Peking University, Beijing 100871, China
yfu@ict.ac.cn, panr@mail.sysu.edu.cn, qyang@cse.ust.hk, wgao@pku.edu.cn

Abstract. Protein homology prediction is a crucial step in templatebased protein structure prediction. The functions that rank the proteins
in a database according to their homologies to a query protein is the
key to the success of protein structure prediction. In terms of information retrieval, such functions are called ranking functions, and are often
constructed by machine learning approaches. Diﬀerent from traditional
machine learning problems, the feature vectors in the ranking-function
learning problem are not identically and independently distributed, since
they are calculated with regard to queries and may vary greatly in
statistical characteristics from query to query. At present, few existing
algorithms make use of the query-dependence to improve ranking performance. This paper proposes a query-adaptive ranking-function learning
algorithm for protein homology prediction. Experiments with the support vector machine (SVM) used as the benchmark learner demonstrate
that the proposed algorithm can signiﬁcantly improve the ranking performance of SVMs in the protein homology prediction task.
Keywords: Protein homology prediction, information retrieval, ranking
function, machine learning, support vector machine.

1

Introduction

A good ranking function is crucial for a successful information retrieval system
[1]. A ranking function is based on the measurement of the relevance of database
items to a query. Usually, there are multiple ways to measure the relevance of


This work was supported by the Research Initiation Funds for President Scholarship Winners of Chinese Academy of Sciences (CAS), the National Natural Science
Foundation of China (30900262, 61003140 and 61033010), the CAS Knowledge Innovation Program (KGGX1-YW-13), and the Fundamental Research Funds for the
Central Universities (09lgpy62).

J. Chen, J. Wang, and A. Zelikovsky (Eds.): ISBRA 2011, LNBI 6674, pp. 320–331, 2011.
c Springer-Verlag Berlin Heidelberg 2011


Query-Adaptive Ranking with SVMs

321

database items. An important issue is how to automatically and intelligently
combine these relevance measures into a powerful single function using machine
learning technologies [2,3,4,5].
Protein structures play an important role in biological functions of proteins.
Experimental approach to protein structure determination is both slow and expensive. Since homologous proteins (evolved from the same ancestor) usually
share similar structures, predicting protein structures based protein homologies
has been one of the most important problems in bioinformatics [6,7,8,9]. Protein
homology prediction is a key step of protein structure prediction and is a typical
ranking problem[10]. In this problem, the database items are protein sequences
with known three-dimensional structures, and the query is a protein sequence
with unknown structure. The objective is to ﬁnd those proteins in the database
that are homologous to the query protein so that the homologous proteins can
be used as structural templates.
The homology between two proteins can be captured from multiple views,
such as sequence alignment, sequence proﬁle and threading [11]. In this paper,
we will not focus on these homology measures or features, but on the machine
learning algorithms that integrate these features into a single score, i.e., a ranking
function, in order to rank the proteins in a database. Since the proposed algorithm is, in principle, applicable to general ranking-function learning tasks, we
will discuss the problem and describe the algorithm in a somewhat general manner. For example, when we say a ’query’ or ’database item’, it corresponds to a
’protein’ in our protein homology prediction problem, and ’relevant’/’relevance’
means ’homologous’/’homology’.
In ranking-function learning, the items (proteins in our case) in a database
are represented as vectors of query-dependent features, and the objective is to
learn out a function that can rank the database items in order of their relevances
to the query. Each query-dependent feature vector corresponds to a query-item
pair. Training data also consist of relevance (homology in our case) judgments for
query-item pairs, which can be either absolute (e.g., item A is relevant, item B is
not, while item C is moderate, etc.) or relative (e.g., item A is more relevant than
item B). The relevance judgments can be acquired from the manual annotation
of domain experts.
Algorithms for ranking-function learning mainly diﬀer in the form of training
data (e.g., absolute or relative relevance judgments), the type of ranking function (e.g., linear or nonlinear), and the way to optimize coeﬃcients. In early
years, various regression models were used to infer probability of relevance from
binary judgments, e.g., the polynomial regression [3] and the logistic regression [12,13]. Information retrieval can also be viewed as a binary classiﬁcation
problem: given a query, classify all database items into two classes - relevant
or irrelevant[2,14]. An advantage of viewing retrieval as a binary classiﬁcation
problem is that powerful discriminative models in machine learning, e.g., SVM,
can be directly applied and the resultant ranking function is discriminative.
Between regression and classiﬁcation is the ordinal regression. Ordinal regression diﬀers from conventional regression in that the targets are not continuous

322

Y. Fu et al.

but ﬁnite and diﬀers from classiﬁcation in that the ﬁnite targets are not nominal but ordered [15,16]. Methods were also proposed to directly learn to rank
things instead of learning the concept of relevance. For example, Joachims addressed the ranking-function learning problem in the framework of large margin
criterion, resulting in the Ranking SVM algorithm [5]. Learning to rank has
drawn more and more attention from the machine learning ﬁeld in recent years
(e.g., [17,18]).
A major characteristic of ranking-function learning is that the feature vector
of each database item is computed with regard to a query. Therefore, all feature
vectors are partitioned into groups by queries (each group of data associated
with a query is called a block in this paper). Unlike traditional learning tasks,
e.g., classiﬁcation and regression, in which data are assumed to be independently
and identically distributed, the ranking data belonging to the same block are correlated via the same query. We have observed that the data distributions may
vary greatly from block to block [19]. The same value of a feature may indicate
relevance in one block but irrelevance in another block. In the pure ranking algorithms that take preference judgments as input and do not aim to estimate
relevance, e.g., Ranking SVM [5], training is performed so that rankings are only
consistent within training queries. In this case, the diﬀerence between queries
does not pose an obstacle to learning a pure ranking function. However, few efforts have so far been devoted to explicitly making use of the diﬀerence between
queries to improve the generalization performance of learned ranking functions.
Since queries in practice diﬀer in various ways, no single ranking function performs well for all queries. A possible way to improve ranking performance is to
use diﬀerent ranking functions for diﬀerent queries [20].
In this paper, we describe a query-adaptive ranking-function learning
algorithm for the protein homology prediction task. Our approach, called KNearest-Block Ensemble Ranking, is motivated by the intuitive idea of learning
a ranking function for a query using its similar queries in training data instead of using all available training data. Note that by similar we do not mean
sequence similarity, but rather similarity in distributions of query-dependent
data. To avoid online training, we employ an ensemble method. On each data
block (corresponding to a query protein) in training data, an individual ranking
model is trained oﬄine in advance. Given the data block derived from a new
query, the k ranking models trained on the k most similar blocks to the given
block are applied separately to the given block and the k groups of ranks are
aggregated into a single rank. In this way, incremental learning is also supported. As support vector machines (SVMs) [21] have been extensively studied
in recently years for ranking-function learning and have been shown to have
excellent performance (see, e.g., [5,15,14,17]), we use the SVM as the benchmark learner in this work. Experiments on a public dataset of protein homology
prediction show that the proposed algorithm performs excellently in terms of
both ranking accuracy and training speed, signiﬁcantly improving the ranking
performance of SVMs.

Query-Adaptive Ranking with SVMs

2

323

Algorithm

In this section, we describe our K-Nearest-Block (KNB) approach to ranking.
Below is the terminology used in this paper.
Query-Dependent Feature Vector. Given a query (a protein sequence here),
a database item (a protein sequence with known structure) is represented as
a vector of (query-dependent) features that measure the relevance (homology) of the database item to the query. Each query-dependent feature vector
corresponds to a query-item pair.
Block. A block B is a group of instances of query-dependent feature vectors
associated with the same query. Each block corresponds to a query. A block
usually includes several hundreds or thousands of feature vectors, which are
computed from the most homologous proteins in a database according to
some coarse scoring function.
Training Block. A training block is a block with relevance judgements for all
of the feature vectors in it.
Test Block. A test block is a block in which the relevance judgments are unavailable and are to be made.
Block Distance. A block distance D(Bi , Bj ) is a mapping from two blocks
Bi and Bj to a real value that measures the dissimilarity between the two
blocks.
k Nearest Blocks. Just as the name implies, the k nearest blocks to a block
are the k training blocks that are most similar to this blocks according to a
block distance deﬁnition.
The block structure of data is a unique feature of the ranking problem. We believe
that the diﬀerences among blocks, if appropriately used, can be very valuable
information for training more accurate ranking models. One straightforward idea
is that given the test block corresponding to a new query, all n training blocks
should not be used to learn a ranking model, but only the k(n) most similar
training blocks to the test block should be used. This is the block-level version of
the traditional K-Nearest Neighbors method for classiﬁcation or regression and
thus can be called the K-Nearest Block (KNB) approach to ranking.
Three important sub-problems in the KNB approach are:
1. How to ﬁnd the k nearest training blocks?
2. How to learn a ranking model using the k nearest blocks?
3. How to choose the value of k?
Diﬀerent resolutions to the above three sub-problems lead to diﬀerent implementations of the KNB method for ranking. High speed is a most crucial factor
for a real-world retrieval system. Generating a new ranking model for each new
query seems to apparently conﬂict with the above criterion. Therefore, the second sub-problem is especially important and needs to be carefully addressed.

324

2.1

Y. Fu et al.

K Nearest Blocks (KNB)

To ﬁnd the k nearest blocks to a given block, a distance between blocks must
be deﬁned in advance. In general, the block distance can be deﬁned in various
ways, either domain-dependent or independent.
The most intuitive way is to represent each block as a vector of block features.
Block features are variables that characterize a block from some views. For example, block features can be the data distribution statistics of a block. Given a
vector representation of blocks, any vector-based distance can serve as a block
distance, such as Euclidean distance or Mahalanobis’ distance.
For simplicity and generality, we employ a domain-independent vector presentation of blocks and use Euclidean distance in this paper. Each block is represented by the statistics of each feature in the block, that is,
Φ(Bi ) = μi1 , σi1 , μi2 , σi2 , · · · , μid , σid ,

(1)

where μik and σik are the mean and the standard deviation of the k-th feature
in block Bi , respectively. The distance between two blocks Bi and Bj then is

D(Bi , Bj ) =  Φ(Bi ) − Φ(Bj ) 2 .
(2)
2.2

KNB Ensemble Ranking

Given the k nearest blocks, the next step is to learn a ranking model from the
selected training data. The most simple resolution is to train a global model
on the union of all selected blocks. However, as we pointed out previously, a fatal
drawback of doing this is that a training process has to be conducted online for
each new query while the training time to be needed is totally unknown. This is
unacceptable for a practical retrieval system.
To overcome the diﬃculty of online training, we propose to use an ensemble
model. First, on each training block, a ranking model (called a local model)
is trained oﬄine. All local models are saved for future use. When a new query
comes, a test block is generated and is compared to all training blocks. The k
nearest training blocks to the test block are identiﬁed and the k corresponding
local models are separately applied to the test block. Then, the k groups of
relevance predictions are aggregated together to generate the ﬁnal single rank for
the query. Figure 1 gives the ﬂowchart of the KNB Ensemble Ranking method.
In principle, any ranking model can serve as the local model in the KNB
Ensemble Ranking method. Since SVMs have recently been extensively explored
for ranking, we choose the classiﬁcation SVM as the base learner and compare the
resulted KNB ensemble SVM with other SVM-based ranking methods. Another
reason for using SVMs is that previous best results on the data set used in this
paper (see next section for detail) were mostly obtained with SVMs [19,22,23,24].
For aggregation, we simply sum over all the predictions made by selected local
models with block distances as weights; that is,

1
M odeli (B ∗ ),
(3)
Relevance(B ∗ ) =
D(B ∗ , Bi )
∗
i∈I

Query-Adaptive Ranking with SVMs

325

New query

Generate the test block B*

Find the k nearest training

Training blocks

blocks to B*

Train on separate blocks
Predict for B* using the k
corresponding local models

Local Models

Aggregate the k groups of

Off-line part

predictions on block B*

Rank

Fig. 1. Flowchart of the KNB Ensemble Ranking method

where I ∗ is the index of the k nearest training blocks to the test block B ∗ and
M odeli (B ∗ ) denotes the predictions made by the local model trained on block Bi .
Compared to the global model, the ensemble model used in the KNB ranking
method has several advantages:
– Firstly, training is fast. The time needed for training a learner (e.g. SVM)
often increases nonlinearly with the number of training examples. Training
on separate blocks is a divide-and-conquer strategy and thus is faster.
– Secondly, test is fast. KNB-based global model must be trained online for
each new query, while local models can be trained oﬄine in advance.
– Thirdly, incremental learning is supported. When a new training block comes,
a new local model can be trained on it and be added into the repository of
local models.
– Fourthly, a training block can be easily weighted according to its distance
from the test block.
– Finally, an ensemble model often outperforms a global model in complex
learning problems.

3

Experiments

In this section, we apply the KNB ensemble ranking algorithm to the protein
homology prediction problem and demonstrate that the algorithm is superior to
other SVM-based ranking methods in both ranking accuracy and training speed.

326

Y. Fu et al.

Table 1. Statistics of the protein homology data set

Training data
Test data

3.1

#Queries Block size #Examples #Features
153
145,751
∼ 1000
74
150
139,658

Data Set

Protein homology search is a routine task in current biology and bioinformatics researches. The task of protein homology prediction is to rank/predict the
homologies of database proteins to a query protein. In this paper, we use the
KDDCUP2004 data set of protein homology prediction [25]. In this data set,
each database protein is characterized by 74 features measuring its homology to
the query protein. The data are generated by the program LOOPP (Learning
Observing and Outputting Protein Patterns), a protein fold recognition program
[26]. The homology features include length of alignment, percentage of sequence
identity, z-score for global sequence alignment, etc. [11]. On this data set, we have
obtained the Tied for First Place Overall Award in the KDDCUP2004 competition. In the original winning solution, we successfully developed and used the
intra-block data normalization and support-vector data sampling technologies
for ranking-function learning [19].
The statistics of the data are summarized in Table 1. They include 153 training queries and 150 training queries. For each query, the examples (candidate
homologous proteins) were obtained from a preliminary scoring/ranking function. The labels (homology judgments) are in binary form (0 for homology and
1 for non-homology) and are available for training data. Labels for test data
are not published. The predictions for test data can be evaluated online at the
competition web site. This provides a relatively fair manner for researchers to
test and compare their methods.
For computing the block distance, we globally normalize the query-dependent
features so that the mean is zero and the variance is one in the whole training
data. For training local SVMs, we locally normalize the features so that the
mean is zero and the variance is one within each block. We found that this
kind of intra-block normalization resulted in improved prediction performance
compared to the global normalization [19].
3.2

Performance Evaluation

Four metrics are used to evaluate the performance of a ranking method. They
are TOP1, RKL (average rank of the last relevant item), APR (mean average
precision), and RMS (root mean squared error). Each of the four metrics is ﬁrst
computed on individual blocks, and then averaged over all blocks.
TOP1. (maximize) TOP1 is deﬁned as the fraction of blocks with a relevant
item ranked highest. It measures how frequently a search engine returns a
relevant item to the user at the top position.

Query-Adaptive Ranking with SVMs

327

RKL. (minimize) RKL is deﬁned as the average rank of the last relevant item.
It measures how many returned items have to be examined sequentially in
average so that all relevant items can be found. If the purpose is to ﬁnd out
all relevant items, then RKL is a more suitable metric than TOP1.
APR. (maximize) APR is deﬁned as the average of a kind of average ranking
precision on each block. The average precision on single block is quite similar
to the AUC (Area Under presision/recall Curve) metric. APR provides an
overall measurement of the ranking quality.
RMS. (minimize) RMS is the average root mean square error. It evaluates how
accurate the prediction values are if they are used as estimates of relevance
(1 for absolute relevance and 0 for absolute irrelevance).
The ﬁrst three metrics exclusively depend on the relative ranking of the items in
a block while RMS needs relevance estimates. Since the target values are binary,
we found that to a large extent RMS seems to rely on a good normalization of
the prediction values more than on a good ranking or classiﬁcation. Therefore,
we place emphasis on the ﬁrst three metrics, although we have obtained the best
result of RMS on test data.
To evaluate a learning algorithm and perform model selection, cross validation
is the most widely used strategy. Since the performance measures for ranking
are calculated based on blocks, it becomes natural to divide the training data by
blocks for cross validation. We extend the traditional cross validation method
Leave-One-Out (LOO) to a block-level version which we call Leave-One-BlockOut (LOBO). Given a training data set, the LOBO cross validation puts one
block aside as the validation set at a time and uses other blocks for training. After
all blocks have their turns as the validation set, an averaged performance measure
is computed at the end. The LOBO cross validation is diﬀerent from both the
traditional LOO and the n-fold cross validation. It is a graceful combination
of these two common cross validation strategies, taking advantage of the block
structure of ranking data.
3.3

Results

Four algorithms are compared, including the standard classiﬁcation SVM, the
Ranking SVM, the KNB global SVM, and the KNB ensemble SVM. For Ranking
SVM, relative relevance judgments are derived from binary labels. In all experiments, the SVMlight package [27] is used. In most cases, the linear kernel is used
for SVM training, based on the following considerations:
– High speed is crucial for a real-word retrieval system and linear ranking
function are more eﬃcient than nonlinear ones. Especially, a linear SVM
can be represented by a weight vector while a nonlinear SVM has to be
represented by a group of support vectors, the number of which is in general
uncontrollable.
– Nonlinear kernels introduce additional parameters, thus increasing the diﬃculty of model selection. In our case, experiments have shown that training
with nonlinear kernels, e.g., RBF kernel, on the entire data set we used is
extremely slow and does not show better results.

328

Y. Fu et al.

Table 2. Cross-validation performance on training data set (the results of Standard
SVM and Ranking SVM were obtained after intra-block data normalization, a method
we previously proposed [19]; otherwise they would perform much worse. It is the same
with Table 3).
TOP1
(maximize)
Standard SVM
0.8758
Ranking SVM
0.8562
KNB global SVM
0.8889
KNB ensemble SVM 0.8889
Method

RKL
(minimize)
51.94
36.83
45.18
39.50

APR
(maximize)
0.8305
0.8257
0.8560
0.8538

RMS
(minimize)
0.0367
N/A
0.0354
0.0357

Table 3. Performance on test data set
TOP1
(maximize)
Standard SVM
0.9133
Ranking SVM
0.9000
KNB global SVM
0.9067
KNB ensemble SVM 0.9067
Method

RKL
(minimize)
59.21
45.80
45.90
40.50

APR
(maximize)
0.8338
0.8369
0.8475
0.8476

RMS
(minimize)
0.0357
N/A
0.0379
0.0364

– In the KNB SVM ensemble approach, the linear local SVMs can be easily
combined into a single linear function before prediction, thus decreasing the
online computational burden. Moreover, it has been shown that in ensemble
machine learning, a good generalization ability is often achieved using weak
(e.g., linear) base learners rather than strong (e.g., nonlinear) ones.
– The use of linear kernel is fair for all the SVM-based methods compared.
The only parameter in the linear SVM is the parameter C, the tradeoﬀ between
training error and learner complexity. Another parameter in the KNB-based
methods is k, the number of selected local models. To do model selection and
performance evaluation on the training data, we use the LOBO cross-validation
as described above. Table 2 gives the best results obtained using various methods.
It shows that on the TOP1, APR and RMS metrics, KNB-based methods are
superior to the other two methods. On the RKL metric, the Ranking SVM
obtains the best result. On all metrics, the KNB ensemble SVM is comparable
or superior to the KNB global SVM.
On the test data, predictions, made by models trained with the parameter
values optimized for each metric, were evaluated on online. Table 3 gives the test
results. The KNB ensemble SVM obtains the best results on the RKL and APR
metrics among the four methods. In fact, they are the best known results on
these two metrics (up to the time that this paper is submitted). On the other two
metrics, KNB ensemble SVM does not perform best. However, the diﬀerences are
very small and once again we argue that the RMS metric is very sensitive to the
normalization of prediction values. On average, the solution of KNB ensemble
SVM is the best result among all the original and subsequent submissions to the
competition (http://kodiak.cs.cornell.edu/cgi-bin/newtable.pl?prob=bio).

Query-Adaptive Ranking with SVMs

329

50
Our Result
Previous Best Result

RKL (minimize)

48
46
44
42
40
38
10

12
14
16
18
k in KNB Ensemble Ranking

20

Fig. 2. RKL performance on test data set vs. k in KNB ensemble SVM in comparison
with previous best result
Table 4. Training speed comparison
Method
Standard SVM
Ranking SVM
KNB global SVM
KNB ensemble SVM

Training mode Training time (seconds)
Oﬄine
95
Oﬄine
32255
Online
dependent on k
Oﬄine
9

It is also found that the KNB ensemble SVM is not sensitive to k, the number of
selected nearest blocks. Figure 2 shows the RKL result of KNB ensemble SVM on
the test data with the k as a variable. It can be seen that between a large range of
the value of k (from 10 to 20), the RKL is rather stable and is considerably better
than the previous best result obtained by Foussette et al. [22].
Besides the ranking accuracy, the training speed is another important factor.
Table 4 lists the training mode and training time of the four methods. Standard
classiﬁcation SVM and Ranking SVM are trained on all available blocks. KNB
global SVM is trained online on selected k nearest blocks, and the training time
is dependent on k and is unpredictable in practice. For KNB ensemble SVM,
local SVMs are trained oﬄine on separate blocks. We can see that the oﬄine
training of KNB ensemble SVM only costs 9 seconds, 3600 times faster than
ranking SVM. These experiments were performed on a Solaris/SPARC Server
with 8 Sun Microsystems Ultra-SPARC III 900Mhz CPUs and 8GB RAM.

4

Conclusion and Future Work

In this paper, we have proposed a K-Nearest-Blocks (KNB) ensemble ranking
algorithm with SVMs used as the base learners, and applied it to the protein
homology prediction problem. Experiments show that compared to several other
SVM-based ranking algorithms, the proposed one is signiﬁcantly better on most
performance evaluation metrics and meanwhile is extremely fast in training

330

Y. Fu et al.

speed. Here, we have used a public data set. It is possible to develop more
measures of protein homology to improve the accuracy of protein homology prediction. On the other hand, since the method is domain-independent, it is in
principle applicable to general ranking problems. A potential problem with the
KNB approach is that when the number of training blocks becomes very large,
ﬁnding the k nearest blocks may be computationally expensive. However, all
methods for expediting the traditional K-Nearest Neighbor method can also be
used for the KNB method. In addition, we used a very simply deﬁnition of block
distance in this paper. In fact, it can be improved in various ways, for example,
reﬁnement of block features, feature selection, distance learning, etc. We will try
to address some of these aspects in our future work.

References
1. Baeza-Yates, R., Ribeiro-Neto, B.: Modern Information Retrieval. Addison-WesleyLongman, Harlow (1999)
2. Robertson, S.E., Sparck Jones, K.: Relevance weighting of search terms. Journal
of American Society for Information Sciences 27, 129–146 (1976)
3. Fuhr, N.: Optimal polynomial retrieval functions based on the probability ranking
principle. ACM Transactions on Information Systems 7, 183–204 (1989)
4. Cohen, W., Shapire, R., Singer, Y.: Learning to order things. Journal of Artiﬁcial
Intelligence Research 10, 243–270 (1999)
5. Joachims, T.: Optimizing Search Engines Using Clickthrough Data. In: 8th ACM
Conference on Knowledge Discovery and Data Mining, pp. 133–142. ACM Press,
New York (2002)
6. Baker, D., Sali, A.: Protein structure prediction and structural genomics. Science 294, 93–96 (2001)
7. Zhang, Y., Skolnick, J.: The protein structure prediction problem could be solved
using the current PDB library. Proc. Natl. Acad. Sci. USA 102, 1029–1034 (2005)
8. Ginalski, K.: Comparative modeling for protein structure prediction. Current Opinion in Structural Biology 16, 172–177 (2006)
9. Zhang, Y.: Progress and challenges in protein structure prediction. Current Opinion
in Structural Biology 18, 342–348 (2008)
10. Soding, J.: Protein homology detection by HMMCHMM comparison. Bioinformatics 2, 951–960 (2005)
11. Teodorescu, O., Galor, T., Pillardy, J., Elber, R.: Enriching the sequence substitution matrix by structural information. Proteins: Structure, Function and Bioinformatics 54, 41–48 (2004)
12. Cooper, W., Gey, F., Chen, A.: Information retrieval from the TIPSTER collection:
an application of staged logistic regression. In: 1st NIST Text Retrieval Conference, pp. 73–88. National Institute for Standards and Technology, Washington, DC
(1993)
13. Gey, F.: Inferring Probability of Relevance Using the Method of Logistic Regression. In: 17th Annual International ACM Conference on Research and Development
in Information Retrieval, Dublin, Ireland, pp. 222–231 (1994)
14. Nallapati, R.: Discriminative Models for Information Retrieval. In: 27th Annual
International ACM Conference on Research and Development in Information Retrieval, pp. 64–71. ACM Press, New York (2004)

Query-Adaptive Ranking with SVMs

331

15. Herbrich, R., Obermayer, K., Graepel, T.: Large margin rank boundaries for ordinal regression. In: Smola, A.J., Bartlett, P., Schölkopf, B., Schuurmans, C. (eds.)
Advances in Large Margin Classiﬁers, pp. 115–132. MIT Press, Cambridge (2000)
16. Crammer, K., Singer, Y.: Pranking with ranking. In: Advances in Neural Information Processing Systems, vol. 14, pp. 641–647. MIT Press, Cambridge (2002)
17. Chapelle, O., Keerthi, S.S.: Eﬃcient algorithms for ranking with SVMs. Information Retrieval Journal 13, 201–215 (2010)
18. McFee, B., Lanckriet, G.: Metric Learning to Rank. In: 27th International Conference on Machine Learning, Haifa, Israel (2010)
19. Fu, Y., Sun, R., Yang, Q., He, S., Wang, C., Wang, H., Shan, S., Liu, J., Gao,
W.: A Block-Based Support Vector Machine Approach to the Protein Homology
Prediction Task in KDD Cup 2004. SIGKDD Explorations 6, 120–124 (2004)
20. Fu, Y.: Machine Learning Based Bioinformation Retrieval. Ph.D. Thesis, Institute
of Computing Technology, Chinese Academy of Sciences (2007)
21. Vapnik, V.N.: The Nature of Statistical Learning Theory. Springer, New York
(1995)
22. Foussette, C., Hakenjos, D., Scholz, M.: KDD-Cup 2004 - Protein Homology Task.
SIGKDD Explorations 6, 128–131 (2004)
23. Pfahringer, B.: The Weka Solution to the 2004 KDD Cup. SIGKDD Explorations 6,
117–119 (2004)
24. Tang, Y., Jin, B., Zhang, Y.: Granular Support Vector Machines with Association
Rules Mining for Protein Homology Prediction. Special Issue on Computational
Intelligence Techniques in Bioinformatics, Artiﬁcial Intelligence in Medicine 35,
121–134 (2005)
25. Caruana, R., Joachims, T., Backstrom, L.: KDD Cup 2004: Results and Analysis.
SIGKDD Explorations 6, 95–108 (2004)
26. Tobi, D., Elber, R.: Distance dependent, pair potential for protein folding: Results
from linear optimization. Proteins, Structure Function and Genetics 41, 16–40
(2000)
27. Joachims, T.: Making large-Scale SVM Learning Practical. In: Schölkopf, B.,
Burges, C., Smola, A. (eds.) Advances in Kernel Methods - Support Vector Learning, pp. 115–132. MIT Press, Cambridge (1999)

A Bayesian Network Approach to Ontology Mapping
Rong Pan, Zhongli Ding, Yang Yu, and Yun Peng
Department of Computer Science and Electrical Engineering,
University of Maryland, Baltimore County,
Baltimore, Maryland, USA
{pan.rong, zding1, yangyu1, ypeng}@umbc.edu

Abstract. This paper presents our ongoing effort on developing a principled
methodology for automatic ontology mapping based on BayesOWL, a probabilistic framework we developed for modeling uncertainty in semantic web. In this
approach, the source and target ontologies are first translated into Bayesian
networks (BN); the concept mapping between the two ontologies are treated as
evidential reasoning between the two translated BNs. Probabilities needed for
constructing conditional probability tables (CPT) during translation and for
measuring semantic similarity during mapping are learned using text classification techniques where each concept in an ontology is associated with a set of
semantically relevant text documents, which are obtained by ontology guided
web mining. The basic ideas of this approach are validated by positive results
from computer experiments on two small real-world ontologies.

1 Introduction
Uncertainty concerns every aspect of semantic web ontologies. In many applications,
overlapping between concepts/classes cannot be represented logically by OWL constructs. Even if they can, the degree of overlapping is not represented (e.g., how close
a class A is to its super class B?). A description about an unknown concept or object
input to an OWL reasoner may be uncertain (e.g., x is an instance of class A and is
moderately likely to have property p related with class B). In a previous work, we
have developed a Bayesian network based framework BayesOWL, to address representation and reasoning with uncertainty within a single ontology ([5], [6]).
Uncertainty becomes more prevalent in concept mapping between two ontologies
where it is often the case that a concept defined in one ontology can only find partial
matches to one or more concepts in another ontology. Semantic similarities between
concepts are difficult, if not impossible to be represented logically, but can easily be
represented probabilistically. This has motivated recent development of ontology
mapping taking probabilistic approaches (GLUE [7], CAIMAN [11], OntoMapper
[19], and OMEN [13]) (See [14] for a survey of existing approaches to ontology mapping, including those based on logical translation, syntactical and linguistic analysis).
However, these existing approaches fail to completely address uncertainty in mapping. For example, GLUE captures similarity between two concepts onto1:A and
onto2:B by joint probability distribution P(A, B) obtained by text classification of
exemplars (semantically relevant text documents) to each concept. Then onto1:A is
mapped to onto2:C whose similarity to onto1:A, measured by, say their Jaccard coefY. Gil et al. (Eds.): ISWC 2005, LNCS 3729, pp. 563 – 577, 2005.
© Springer-Verlag Berlin Heidelberg 2005

564

R. Pan et al.

ficients [21] (computed from the joint distribution), passes a threshold and is highest
among all concepts in onto2. Here, onto1:A is taken as (semantically) equivalent to
onto2:C, the degree of similarity between them will not be considered in future reasoning (e.g., subsumption within onto2). Also ignored are the other concepts that are
also similar to onto1:A (albeit at smaller degree).
The work reported in this paper extends BayesOWL in a number of significant
ways so that uncertainty in ontology mapping can be dealt with properly. As depicted
in Figure 1 below, this new framework consists of three components: 1) a text classification based learner to learn from web data the probabilistic ontological information
within individual ontologies and between concepts in two different ontologies; 2) a
BayesOWL module to translate given ontologies (together with the learned uncertain
information) into BNs; and 3) a concept mapping module which takes a set of learned
raw similarities as input and finds mappings between concepts from two different
ontologies based on evidential reasoning across two BNs.
Before describing the BN Mapping module and the learner in detail (Sections 3
and 4), we first provide some background information in Section 2. This includes a
brief summary of BayesOWL, and introductions to Jeffrey’s rule and iterative proportional fitting procedure (IPFP), two techniques used in this work. Methods and results
of computer experiments with two small ontologies are given in Section 5. The paper
concludes with discussions and directions of future research in Section 6.

Fig. 1. The framework

2 Background
As background, we briefly introduce Jeffrey’s rule, IPFP, and BayesOWL here.
2.1 Techniques for Updating Probability Distributions
Two techniques for updating a probability distribution by another distribution used in
this work are briefly described below.

A Bayesian Network Approach to Ontology Mapping

565

Jeffrey's rule, also known as rule of probability kinematics or J-conditioning, was
proposed by Richard Jeffrey [9] to revise a probability measure (e.g., a joint distribution P(x) ) by another probability function (e.g., a prior Q ( xi ) in another distribution).
The rule can be written as follows in this context: if P( xi ) , our belief on X i ∈ X , is
changed to Q ( xi ) , then the beliefs of other variables X j ≠i ∈ X shall be changed to

Q( x j ) = ∑ P( x j | X i = xi )Q( X i = xi )

(2.1)

xi

if P( x j | xi ) is invariant with respect to Q ( xi ) .
Jeffrey’s rule can be used as a mechanism to update a distribution by soft evidence, represented as a distribution such as Q ( xi ) . The rule then can be written as
P ( xi | se) = Q( xi ) , and

Q( x j ≠i ) = P( x j | se)
= ∑ P( x j | X i = xi ) P( X i = xi | se)
xi

= ∑ P ( x j | X i = x i )Q ( X i = x i )

(2.2)

(2.3)

xi

Pearl ([16], [17]) has shown that the virtual evidence, a method widely adopted in
Bayesian network (BN) inference, can be viewed as formally equivalent to the likelihood ratio version of Jeffrey’s rule. This is done by adding a virtual node vei which
has X i as its only parent in the BN, related by likelihood ratio :
L( X i ) =

P(vei | X i ) P( X i )Q ( X i )
=
P(vei | X i ) Q( X i ) P ( X i )

(2.4)

when X i is binary. Soft evidence update (eqs. 2.2 and 2.3) can be realized by BN
belief update with vei instantiated to true. It can be shown that L( X i ) for multivalued variables can also be calculated from P( xi ) and Q ( xi ) [17].
As will be seen shortly, we use Jeffrey’s rule to propagate probabilistically beliefs on
variables between two BNs that are translated from two ontologies during mapping.
IPFP. (Iterative Proportional Fitting Procedure) is a computational procedure that
updates a given distribution Q0 ( x) to satisfy a set of probability constraints
R = {Ri ( y i )} where each Ri ( y i ) is a distribution over Y i ⊆ X [10]. Roughly speaking,
IPFP iterates over constraints in {Ri ( y i )} in cycle, at each iteration, the current distribution is updated by one constraint according to
Qk ( x) = Qk −1 ( x) ⋅

Ri ( y i )
Qk −1 ( y i )

(2.5)

It has been proved based on I-divergence geometry ([4], [22]) that IPFP converges
to an unique distribution Q * ( x) , which 1) satisfies all Ri ( y i ) in R, i.e., Q * ( y i ) =
Ri ( y i ) for Ri ∈ R , and 2) has the smallest Kullback-Leibler distance (or I-divergence)
to Q0 ( x) among all distributions Q (x) that satisfy all constraints in R, i.e.,
I (Q * || Q( 0) ) = ∑ Q * ( x) log
x

Q* ( x)
Q( 0) ( x)

(2.6)

566

R. Pan et al.

is minimized. Q * ( x) is called I1-projection of Q0 ( x) on R. Bock [1] and Cramer [2]
extended IPFP to conditional IPFP (CIPFP) to allow constraints with the form of
conditional probability distributions and proved its convergence.
If we consider Q( y i ) as soft evidence on a collection of variables Y i , then IPFP
can be considered as another mechanism of processing soft evidence [20]. The difference between Jeffrey’s rule and IPFP in this regard is that the former requires the
invariance of domain knowledge (i.e., P( x j | xi ) remains unchanged in Q (x) ) while
the latter requires minimizing I-divergence which in general destroys the invariance in
the updated Q * ( x ) . How to combine these two techniques together when used in
ontology to BN translation and in concept mapping will be given in Subsection 2.2
and Section 3.
2.2 BayesOWL

BayesOWL ([5], [6]) is a framework which augments and supplements OWL for representing and reasoning with uncertainty based on Bayesian networks. This framework provides a set of rules and procedures for direct translation of an OWL ontology
into a BN structure (a directed acyclic graph or DAG) and a method based on IPFP
that utilizes available probability constraints about classes and interclass relations in
constructing the conditional probability tables (CPTs) of the BN. The translated BN,
which preserves the semantics of the original ontology and is consistent with the
probabilistic constraints, can support ontology reasoning, both within and across
ontologies, as Bayesian inferences.
Structural translation. The general principle underlying the structural translation
rules is that all classes (specified as “subjects” and “objects” in RDF triples of the
OWL file) are translated into nodes in BN, and an arc is drawn between two nodes in
BN if the corresponding two classes are related by a “predicate” in the OWL file, with
the direction from the superclass to the subclass.
The model-theoretic semantics of OWL treats the domain as a non-empty collection of individuals. If class A represents a concept, the node it is translated to is
treated as a binary random variable of two states a and a , and we interpret P( A = a )
as the prior probability or one’s belief that an arbitrary individual belongs to class A ,
and P(a | b) as the conditional probability that an individual of class B also belongs
to class A . Similarly, for P(a ) , P(a | b) , P(a | b ) , and P(a | b ) , we interpret the
negation as “not belonging to”.
Control nodes are created during the translation to facilitate modeling relations
among class nodes that are specified by OWL logical operators, and there is a converging connection from each of the concept nodes involved in this logical relation to
its specific control node. There are five types of control nodes in total corresponding
to the five types of logical relations: “and” (owl:intersectionOf), “or” (owl:unionOf),
“not” (owl:complementOf), “disjoint” (owl:disjointWith), and “same as”
(owl:equivalentClass).

A Bayesian Network Approach to Ontology Mapping

567

Constructing CPTs. The nodes in the DAG obtained from the structural translation
step can be divided into two disjoint groups: XR, regular nodes representing concepts
in ontology, and XC, control nodes for bridging logical relations. The CPT for a control node in XC can be determined by the logical relation it represents so that when its
state is “True”, the corresponding logical relation holds among its parent nodes. When
all the control nodes’ states are set to “True” (denote this situation as CT), all the
logical relations defined in the original ontology are held in the translated BN. The
remaining issue is then to construct the CPTs for node in XR so that P(XR|CT), the
joint distribution of all regular nodes in the subspace of CT, is consistent with all the
given probabilistic constraints about classes and relations between classes. These
constraints include, most likely, priors for classes P(C), conditionals P(C|D) for relations between classes C and D. Several suggestions have been made to encode probability constraints in semantic web languages (e.g., [6] with OWL, and [8] with RDF).
These constraints can be obtained from the ontology designers or learned from data
(an approach that learns these constraints from web is described in Section 4).
In principle, IPFP can be applied to construct CPTs to satisfy all the given probabilistic constraints. Two difficulties exist. First, as we mentioned earlier, direct application of IPFP may destroy the existing interdependencies between variables (i.e.,
the given DAG becomes invalid). Secondly, IPFP is computationally very expensive
since every entry in the joint distribution of the BN must be updated at each iteration.
To overcome these difficulties, we developed an algorithm named D-IPFP that decomposes IPFP so that each iteration only updates a small portion of the BN that are
directly involved with the chosen constraint, and the update is done only to CPTs
while keeping the DAG of the network intact [18]. In particular, when each of the
given constraints involves only one variable Ci and a set of zero or more of its parents Li , (2.5) of IPFP becomes [5]
Q(ci | Li )
⎧
⎪Qk (ci | π i ) = Qk −1 (ci | π i ) ⋅
Qk −1 (ci | Li )
⎨
⎪⎩Qk (c j | π j ) = Qk −1 (c j | π j )
∀j ≠ i

(2.7)

The BayesOWL framework can support common ontology reasoning tasks as probabilistic inferences in the translated BN. For example, given a concept description e, it
can answer queries about concept satisfiability (whether P(e|CT) = 0), about concept
overlapping (how close e is to a concept C as P(e|C,CT)), and about concept subsumption (find the concept which is most similar to e) by defining some similarity
measures such as Jaccard coefficient [21].

3 Concept Mapping Between Ontologies Using BN Mapping
It is often the case when attempting to map concept A defined in Ontology 1 to Ontology 2, there is no concept in Ontology 2 that is semantically identical to A. Instead, A
is similar to several concepts in Ontology 2 with different degree of similarity. A
solution to this so-called one-to-many problem, as suggested by [19] and [7], is to
map A to the target concept B which is most similar to A by some measure. This simple approach would not work well because 1) the degree of similarity between A and
B is not reflected in B and thus will not be considered in reasoning after the mapping;

568

R. Pan et al.

2) potential information loss because other similar concepts are ignored in the mapping; 3) it cannot handle the situation where A itself is uncertain; and 4) it does not
work well when more than one concepts need to be mapped. To see the last point,
consider a situation where concept x defined as intersection of A and B in onto1 is to
be mapped to onto2. Suppose the most similar concepts to A in onto2 are C and D,
and those to are B are E and D, it would be difficult to determine which of the three
(C, D, and E) x should be mapped to.
These difficulties in ontology mapping can be dealt with properly in our framework. We assume that pair-wise similarity measures are available between any concepts in two ontologies onto1 and onto2 (or between variables in BN1 and BN2, respectively). We take mapping as update on probability distribution of variables in
BN2 by distributions of variables in BN1 in accordance to the similarity measures
between these variables. Further inferences (e.g., finding the most probable subsumer
in onto2 for a concept defined in onto1) can be drawn by Bayesian inference with the
updated distribution of BN2. We present our approach starting with the basis: 1) a
notion of probabilistic semantic linkage between a pair of concepts/variables; 2) the
“1 to n” mapping (one variable in BN1 mapped to multiple similar ones in BN2); and
3) the “m to n” mappings where multiple variables in BN1 need to be mapped.
3.1 Pair-Wise Probabilistic Semantic Linkage

We assume the similarity information between variable A in BN1 and B in BN2 is
captured by the joint distribution P(A, B). This distribution is in a probability space,
denoted as PS 1,2 , which is related but different from the spaces for A and B, denoted
as PS 1 and PS 2 , respectively. Moreover, since this measure is based on the semantic
similarity intrinsic to the meanings of these two variables, P(A, B) is assumed invariant with respect to changes in PS 1 and PS 2 . That is, beliefs on variables in A and B
may change when evidence is presented but not that of P(A, B) in PS 1,2 .
Probabilistic semantic linkage between A and B, which serves as a basis mapping
mechanism between similar variables, is defined as
SL1A, 2,B = < PS 1 , PS 2 , A, B, P(A, B)>,

where A ∈ PS 1 , and B ∈ PS 2 , and P(A, B) measures the semantic similarity between A and B. Then the influence to B by A via the single linkage SL1A,2,B changes
P(B) to Q(B) by P(A). This update can be viewed as twice applications of Jeffrey’s
rule across these three spaces, first from PS 1 to PS 1,2 , then PS 1,2 to PS 2 , as depicted
in Figure 2 below. Since A in PS 1 is identical to A in PS 1,2 , P(A) in PS 1 becomes soft
evidence Q(A) to PS 1,2 by (2.2), the distribution of B in PS 1,2 is updated by (2.3) to
Q( B ) = ∑ A P( B | A)Q ( A) ,

(3.1)

Q(B) is then applied as soft evidence from PS 1,2 to node B in PS 2 , updating distribution of other variables C in PS 2 by (2.3) as
Q(C) = ∑B P(C | B)Q( B) = ∑ B P(C | B) ∑ A P( B | A)P( A) .

(3.2)

A Bayesian Network Approach to Ontology Mapping

569

Fig. 2. Mapping concept A to B via semantic linkage SL1A, 2,B

3.2 Multiple Semantic Linkages

Usually, A in onto1 may be semantically similar to more than one concept in onto2.
For, example, if A is fairly similar to B in onto2, it would also be similar to all super
concepts and also some sub-concepts of B, possibly with different similarity measures. In other words, mapping A to BN2 amounts mapping it through all semantic
linkages that initiate from A and end at each similar concept BJ in BN2. Probabilistically, BN2 can be seen as receiving n soft evidences, one for a linkage from A to BJ
for each concept BJ in BN2. This requires 1) all similarity measures P(A, BJ) remain
invariant, and 2) conditional dependencies among variables in BN2 also remain invariant. This “1 to n” mapping can be carried out by a process that combines both
Jeffrey’s rule and IPFP. Like IPFP, this process is iterative over these linkages in a
cycle until convergence.
This process can be realized by generalizing Pearl’s virtual evidence approach for
soft evidence update [15]. In this method of ours, each node BJ is attached a virtual
evidence node. At iteration step k, if linkage from A to BJ is chosen, then we first
calculate likelihood LK ( B J ) for virtual evidence node ve J that will be used to simulate soft evidence Q( B J ) by
LK ( B J ) =

QK −1 ( B J )Q( B J )
,
Q( B J )QK −1 ( B J )

(3.3)

and then apply Jeffrey’s rule of (3.1) and (3.2) with the modified likelihood to update
variable beliefs in BN2. Note that (3.3) is the same as (2.4) except for Qk −1 ( B J ) , the
new distribution obtained at step k-1 is used rather than the initial P( B J ) . Also note
that this process does not explicitly modify the joint distribution of BN2 as the standard IPFP would do, instead, it modifies the likelihood associated with each virtual
evidence node ve J while keep the joint distributions P(A, BJ) and CPT’s in BN2
unchanged. It can be shown that when the process converges, beliefs on variables in
BN2 are consistent with all similarity measures P(A, BJ) and P(A), the belief of A in
BN1.
Mapping Reduction. Using all n linkages in “1 to n” type of mapping, as described
above, is computationally very expensive because the IPFP process takes a number of
iterations to converge, and each iteration involves belief update of BN2, which itself
is exponential to the size of BN2. The problem gets worse for “m to n” type of mapping where what needs to be mapped is a composite concept that is defined as a conjunction (intersection) of several variables or their negations in BN1.

570

R. Pan et al.

Fortunately, satisfying a given probabilistic relation P(A, B) does not always require the use of a linkage from A to B or even know what the linkage looks like. Several probabilistic relations may be satisfied by one linkage. Consider a simple example in Figure 3 with variables A and B in BN1, C and D in BN2, and similarity (joint
probabilities) between every pair as below:

P (C , A ) = ⎛⎜ 0 . 3 0 ⎞⎟ , P( D, A) = ⎛⎜ 0.33
⎝ 0 .1 0 .6 ⎠
⎝ 0.07
0 ⎞,
⎛ 0.348
P (C , B) = ⎛⎜ 0.3
⎟ P ( D, B ) = ⎜ 0.112
⎝ 0.16 0.54 ⎠
⎝

0.18 ⎞ ,
0.42 ⎟⎠
0.162 ⎞
0.378 ⎟⎠

Fig. 3. Mapping Reduction Example

However, we do not need to set up linkages for all these relations. As Figure 3 depicts, when we have a linkage from A to C, all these relations are satisfied (the other
three linkages are thus redundant). This is because not only beliefs on C, but also
beliefs on D are properly updated by the mapping A to C.
Several experiments with large BNs have shown that only a very small portion of
all n1 ⋅ n2 linkages are needed in satisfying all probability constraints. This, we suspect, is due to the fact that some of these constraints can be derived from others based
on the probabilistic interdependencies among variables in the two BNs. We are currently actively working on developing a set of rules that examine the BN structures
and CPTs so that redundant linkages can be identified and removed.

4 Learning Probabilities from Web Data
In this work, we use prior probability distributions P(C) to capture the uncertainty about
concepts (i.e., how likely an arbitrary individual belongs to class C), conditional distributions P(C|D) for relations between C and D in the same ontology (e.g., how likely an
arbitrary individual in class D is also in D’s subclass C), and joint probability distributions P(A,B) for semantic similarity between concepts C and D from different ontologies. Often these kinds of probabilistic information are not available and are difficult to
obtain from domain experts. Our solution is to learn these probabilities using text classification technique ([3], [12]) by associating a concept with a group of sample text
documents called exemplars. The idea is inspired by those machine learning based semantic integration approaches such as [7], [11], and [19], where the meaning of a concept is implicitly represented by a set of exemplars that are relevant to it.

A Bayesian Network Approach to Ontology Mapping

571

Learning the probabilities for semantic similarity between concepts in two ontologies is straightforward, assuming we have sufficient exemplars of good quality associated with each concept. First, we can build a model (classifier) for each concept in
Ontology 1 according to the statistical information in that concept’s exemplars using a
text classifier such as Rainbow1 or Bayesian text classifier dbacl2. Then concepts in
Ontology 2 are classified into classes of Ontology 1 by feeding their respective exemplars into the models of Ontology 1 to obtain a set of probabilistic scores. These
scores showing the inter-concept similarity in a probability form. Concepts in Ontology 1 can be classified in the same way into classes of Ontology 2. This crossclassification process (Figure 4) helps find a set of raw mappings between Ontology 1
and Ontology 2. Similarly, we can obtain prior or conditional probabilities related to
concepts in a single ontology through self-classification with the models learned for
that ontology.

Fig. 4. Cross-classification using Text Classifiers on Web Data

The quality of these text classification based methods is highly dependent on the
quality of text exemplars to each concept, which together should well capture the
meaning of the concept. Two criteria are seen to be crucial in assessing the quality of
exemplars: each exemplar (at least most of them) should be relevant to the meaning
of the concept, and that these exemplars together should well cover all aspects of that
concept. For example, articles on computer games are very relevant to the concept of
“computer applications”, but they alone hardly cover all computer applications.
The need to find sufficiently many relevant exemplars for a large number of concepts greatly reduces the attractiveness and applicability of these machine learning
based approaches. It would be a very time-consuming task for knowledge workers to
find high quality text exemplars manually, as apparently the case for GLUE [7]. Our
approach is to use search engines such as Google3 to retrieve text exemplars for each
concept node automatically from WWW, the richest information resource available
nowadays. The goal is to search for documents in which the concept is used in its
1

http://www-2.cs.cmu.edu/~mccallum/bow/rainbow
http://www.lbreyer.com/
3
http://www.google.com
2

572

R. Pan et al.

intended semantics. The rationale is that the meaning of a concept can be described or
understood by the way it is used.
To find out what documents are relevant to a term, one cannot simply use the words
in the name of the term as keywords to query the search engine. This because a word
may have multiple meanings (word senses) and a query using only the name of the term
in attention may return documents related to a meaning different from the intended
semantics of the term. For example, in an ontology for “food”, a concept named “apple”
is a subconcept of “fruit”. If one only uses “apple” as the keyword for query, documents
showing how to make an apple pie and how to use an iPod may both be returned.
Clearly, the documents using “apple” for its meaning in computer field is irrelevant to
“apple” as a fruit. Fortunately, since we are dealing with concepts in well defined ontologies, the semantics of a term is to a great extent specified by the other terms used in
defining this concept in the ontology, including names of its super and subconcept
classes and the properties of this concept and its super classes. This semantic information can thus be used to guide the web search with increased relevancy. There are a
number of ways the semantic information can be used to help search. The simplest one,
and the one we have experimented so far is to form search query for one concept by
combining all the terms on the path from root to that concept node in the taxonomy. In
the “apple” example, the query would then become “food fruit apple”, and documents
about iPod and Apple computers would not be returned.
In the experiments, for each concept A, we search the web to obtain two sets of exemplars: UA+ containing exemplars that support (or positively related to) A; and UA-,
containing exemplars that support the negation of (or negatively related to ) A. Exemplars in UA+ are obtained by searching the web for pages that contain A and all names
of A’s ancestors on the taxonomy, while that for UA- are obtained by search pages that
contain all names of A’s ancestors but not A.
With all these documents, we can obtain joint probabilities of A and B by text classification, similar to what is done in GLUE [7]: applying the classifiers of concepts A
and B to all text documents in U, and classify them into four categories: UA+B+, UA+B-,
UA-B+, and UA-B-. Then the joint probabilities can be obtained by counting the items in
each category, e.g., P(A, B)= |UA+B+| / |U|. If we only search for positive exemplars
UA+ and UB+, then only conditional probability P(B|A) can be obtained (by applying
B’s classifier to A’s supportive exemplars to obtain UA+B+ and compute P(B|A) =
|UA+B+| / |U A+|). The first approach is the one that works for our purpose.

5 Experiments
We have performed computer experiments on two small-scale real-world ontologies.
Our goals are to find how good the learning can be with the exemplars mined from
the web, and how the uncertainty inference across multiple Bayesian networks could
help ontology mapping.
Translating Taxonomies to BNs. We took the Artificial Intelligence sub-domain
from ACM Topic Taxonomy4 and DMOZ5 (Open Directory) hierarchies and pruned
4
5

http://www.acm.org/class/1998/
http://dmoz.org/

A Bayesian Network Approach to Ontology Mapping

573

some concepts to form two ontologies, both of which have a single root node Artificial Intelligence. All other concepts in the hierarchies are sub categories of AI. These
two hierarchies differ in both terminologies and modeling methods. DMOZ categorizes concepts by popularities of web pages to facilitate people’s easy access to these
pages, while ACM topic hierarchy categorizes concepts from super to sub to structure
a classification primarily for academics.
Table 1. Statistics of the expirements
Taxonomies
ACM AI
DMOZ AI

#
Nodes
15
25

Depth
3
3

Total Exemplar
size
19.7 MB
29.2 MB

Avg. Exemplar Size
698 KB
612 KB

# Exemplar
24533
35148

Avg. #
Exp./node
1636
1406

For every concept, except the root, we obtained exemplars by querying Google as
described in the previous section. The statistics of these web pages is listed in Table 1.
We used Bayesian text classifier dbacl to create a model for each non-root concept X
and obtained the pair-wise conditional probability P(X | Parent(X)). The root nodes
were assigned a prior probability as (0.5, 0.5).
Then, using BayesOWL’s translation rules, the two ontologies were translated into
two BNs as shown in Figure 5.
Learning uncertainty mappings. Raw mappings P(A, B) were computed for each
pair of concepts of the two BNs. The similarity between A and B were measured by
their Jaccard coefficient, computed from the joint probability. Table 2 lists the five
most similar concepts and five most different concepts in the learning result. The
top three most similar concepts are actually identical concepts. However, besides
these three, another pair of identical concepts is not measured as highly related.
They are /Learning/Connectionism & Neural Net in ACM topic and /Machine
Learning/Neural Network in DMOZ. Their similarity is only 0.61. We speculate
this is because the term “connectionism” is not as popular as when ACM topic hierarchy was constructed, and thus is not used along with “Neural Network” in most
web pages.
Inference with BN Mappings. Treating ontology mapping as Bayesian network
mapping as described here allows us to conduct probabilistic reasoning far beyond
finding the best concept match. We are currently actively investigating this issue and
developing related algorithms. To illustrate our point, consider the example of finding
a description of DMOZ’s /Knowledge Representation/Semantic Web (dmoz.sw) in
ACM topic. There is no ACM concept that is identical to dmoz.sw, it must be described by a composite expression involving multiple ACM concepts. The two most
semantically similar concepts to dmoz.sw in ACM are /Knowledge Representation
and Formalism Method/Relation System (acm.rs) and /Knowledge Representation and
Formalism Method/Semantic Network (acm.sn) with the joint distributions

574

R. Pan et al.

Fig. 5. Bayesian network for ACM topics’ AI sub-domain and DMOZ’s AI sub-domain

⎛ 0.58 0.13 ⎞
⎛ 0.60 0.12 ⎞
⎟⎟ ,
⎟⎟ and P(dmoz.sw, acm.sn) = ⎜⎜
P(dmoz.sw, acm.rs ) = ⎜⎜
⎝ 0.25 0.04 ⎠
⎝ 0.21 0.07 ⎠
and respective Jaccard coefficients J(dmoz.sw, acm.rs) = 0.64, and J(dmoz.sw,
acm.sn) = 0.61.
From the two joint probabilities, we can see that dmoz.sw is not a subconcept of either acm.rs or acm.sn, but had a sizable overlap with each of them. From the following joint probabilities
P (acm.rs, acm.sn) = ⎛⎜ 0.2612 0.0498 ⎞⎟ ,
⎝ 0.0323 0.6557 ⎠

A Bayesian Network Approach to Ontology Mapping

575

we can see that acm.rs and acm.sn also overlap with each other. Figure 6 illustrates
the overlap of these three concepts.
Table 2. Five most similar concepts and most different concepts in the learning result. The root
concept’s name is omitted.
ACM topic
DMOZ
/Knowledge Representation & Formalism Method
/Knowledge Representation
/Natural Language Processing
/Natural Language
/Learning
/Machine Learning
/Learning
/Knowledge Representation
/Applications & Expert System
/Knowledge Representation
……
/Fuzzy
/Learning/Analog
/Learning/Induction
/Learning/Game
/Deduction & Theorem Proving
/Programming Language/Declarative
/Learning/Induction
/Application
/Learning/Analogy
Agent

Similarity
0.96
0.90
0.88
0.81
0.79
0.03
0.02
0.02
0.01
0.01

Fig. 6. The Venn diagram for dmoz.sw, acm.rs, and acm.sn

This leads to a conjecture that dmoz.sw may be described in terms of acm.rs and
acm.sn. To validate this conjecture, we need to have the conditional probability
P(acm.rs= true, acm.sn = true| dmoz.sw = true). This can be obtained as follows.
1. Using learned probabilities P(dmoz.sw, acm.rs) and P(dmoz.sw, acm.sn), two semantic linkage were created, from dmoz.sw to acm.rs and to acm.sn, respectively.
2. Instantiate dmoz.sw as true, and compute the likelihoods for the two virtual evidence nodes associated with acm.rs and acm.sn.
3. Compute P(acm.rs= true, acm.sn = true| dmoz.sw = true) by any Bayesian network
inference algorithm with the two virtual evidence nodes set to true.
In our experiment, this probability was computed to be 0.851. From this we could
conclude that intersection of acm.rs and acm.sn is the highly probable subsumer of
dmoz.sw. More detailed analysis may require having the joint distribution of the three
concept nodes (in two ontologies/BNs) or distribution involving additional relevant
ACM concepts (with similarity measure lower than those of acm.rs and acm.sn).
These distributions can be computed in the similar fashion.

576

R. Pan et al.

6 Discussion and Future Work
This paper describes our ongoing research on developing a probabilistic framework
for automatic ontology mapping. In this framework, ontologies (or parts of them) are
first translated into Bayesian networks, and then the concept mapping is realized as
evidential reasoning between the two BNs by Jeffrey’s rule. The probabilities needed
in both translation and mapping can be obtained by using text classification programs,
supported by associating to individual concepts relevant text exemplars retrieved from
the web.
We are currently actively working on each of these components. In searching for
relevant exemplars, we are attempting to develop a measure of relevancy so that less
relevant documents can be removed. We are also investigating how semantic information can be utilized to post-process text documents mined from the web so that less
relevant ones can be identified and excluded. We are expanding the ontology to BN
translation from taxonomies to include properties, and develop algorithms to support
common ontology-related reasoning tasks. As for a general BN mapping framework,
our current focus is on linkage reduction. We are also working on the semantics of
BN mapping and examining its scalability and applicability. Future work also includes developing methods to properly deal with inconsistent probability constraints
in IPFP process.

Acknowledgement
This work was supported in part by DARPA contract F30602-97-1-0215 and NSF
award IIS-0326460.

References
1. Bock, H. H. 1989. A Conditional Iterative Proportional Fitting (CIPF) Algorithm with Applications in the Statistical Analysis of Discrete Spatial Data. Bull. ISI, Contributed Papers
of 47th Session in Paris, 1: 141-142.
2. Cramer, E. 2000. Probability Measures with Given Marginals and Conditionals: Iprojections and Conditional Iterative Proportional Fitting. Statistics and Decisions, 18:
311-329.
3. Craven, M.; DiPasquo, D.; Freitag, D.; McCallum, A.; Mitchell, T.; Nigam, K.; and
Slattery, S. 2000. Learning to Construct Knowledge Bases from the World Wide Web. Artificial Intelligence, 118(1-2): 69-114.
4. Csiszar, I. February 1975. I-divergence Geometry of Probability Distributions and Minimization Problems. The Annuals of Probability, 3(1): 146-158.
5. Ding, Z.; Peng, Y.; and Pan, R. November 2004. A Bayesian Approach to Uncertainty
Modeling in OWL Ontology. In Proceedings of 2004 International Conference on Advances in Intelligent Systems - Theory and Applications (AISTA2004). LuxembourgKirchberg, Luxembourg.
6. Ding, Z.; and Peng, Y. January 2004. A Probabilistic Extension to Ontology Language
OWL. In Proceedings of the 37th Hawaii International Conference on System Sciences
(HICSS-37). Big Island, Hawaii.

A Bayesian Network Approach to Ontology Mapping

577

7. Doan, A.; Madhavan, J.; Domingos, P.; and Halevy, A. 2004. Ontology Matching: A Machine Learning Approach. Handbook on Ontologies in Information Systems, S. Staab and
R. Studer (eds.), Springer-Velag, 2004. Invited paper. P397-416.
8. Fukushige, Y. October 2004. Representing Probabilistic Knowledge in the Semantic Web.
Position paper for the W3C Workshop on Semantic Web for Life Sciences. Cambridge,
MA, USA.
9. Jeffery, R. 1983. The logic of Decisions 2nd Edition, University of Chicago Press.
10. Kruithof, R. Telefoonverkeersrekening, De Ingenieur 52, E15-E25, 1937.
11. Lacher, M.; and Groh, G. May 2001. Facilitating the Exchange of Explicit Knowledge
through Ontology Mappings. In Proceedings of the 14th International FLAIRS Conference. Key West, FL, USA.
12. McCallum, A.; and Nigam, K. 1998. A Comparison of Event Models for Naive Bayes Text
Classification. AAAI-98 Workshop on “Learning for Text Categorization”.
13. Mitra, P.; Noy, N. F.; and Jaiswal, A. R. 2004. OMEN: A Probabilistic Ontology Mapping
Tool. In Workshop on Meaning Coordination and Negotiation at the Third International
Conference on the Semantic Web (ISWC-2004). Hisroshima, Japan.
14. Noy, N. 2004. Semantic integration: A survey of ontology-based approaches. SIGMOD
Record.
15. Pan, R.; and Peng, Y.;. 2005. A Framework for Bayesian Network Mapping. (Extend Abstract). Accepted by AAAI-05.
16. Pearl, J. 1988. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufman, San Mateo, CA.
17. Pearl, J. 1990. Jeffery’s rule, passage of experience, and neo-Bayesianism. In H.E. et al.
Kyburg, Jr., editor, Knowledge Representation and Defeasible Reasoning, pages 245-265.
18. Peng, Y.; and Ding, Z. July 2005. Modifying Bayesian Networks by Probability Constraints. Proceedings of the 24th Conference on Uncertainty in AI (UAI 2005). Edinburgh,
Scotland.
19. Prasad, S.; Peng, Y.; and Finin, T. 2002. A Tool For Mapping Between Two Ontologies
(Poster), International Semantic Web Conference (ISWC02).
20. Valtorta, M.; Kim, Y.; and Vomlel, J. 2002. Soft Evidential Update for Probabilistic Multiagent Systems. International Journal of Approximate Reasoning, 29(1): 71-106.
21. van Rijsbergen, C. J. 1979. Information Retrieval. London: Butterworths. Second Edition.
22. Vomlel J. 1999. Methods of Probabilistic Knowledge Integration. PhD thesis, Department
of Cybernetics, Faculty of Electrical Engineering, Czech Technical University.

Realizing Advanced Video Optimized Wireless Networks

QoE-Based Multi-Stream Scalable Video
Adaptation over Wireless Networks with Proxy
Hao Hut, Xiaoqing Zhu", Yao Wangt, Rong Pan", Jiang Zhu" and Flavio Bonomi*
tElectrical & Computer Engineering, Polytechnic Institute of NYU, Brooklyn, NY 11201
* Advanced Architecture & Research, Cisco Systems, San Jose, CA 95134
Abstract-In this paper, we present a proxy-based solution for
adapting the scalable video streams at the edge of a wireless
network, which can respond quickly to highly dynamic wireless links. Our design adopts the scalable video coding (SVC)
technique for lightweight rate adaptation at the edge. We derive
a QoE model, Le., rate-quality tradeoff model, that relates the
maximum subjective quality under a given rate by choosing
the optimal frame rate and quantization stepsize. The proxy
iteratively allocates rates of different video streams to maximize a
weighted sum of video qualities associated with different streams,
based on the periodically observed link throughputs and the
sending butler status. Simulation studies show that our scheme
consistently outperforms TFRC in terms of agility to track
link qualities and overall quality of aU streams. In addition,
the proposed scheme supports differential services for difTerent
streams, and competes fairly with TCP flows.
I. INTRODUCTION

Recent years have seen a proliferation of smart phones and
constant bandwidth upgrades in broadband mobile networks.
These two factors combined have fueled the rapid growth of
mobile media traffic. The study in [1] predicts that by 2015,
two-thirds of world's mobile data will be video. On the other
hand, mobile media streaming remains a daunting task, especially for users in a highly dynamic environment. The challenges are multifold. First, rate adaptation for streaming video
needs to closely track fluctuations in the available wireless
link bandwidth. Conventional techniques such as TCP-friendly
rate control (TFRC) [2], however, typically rely on end-toend packet statistics and fall behind abrupt changes in the
underlying network conditions. Second, existing approaches
achieve fairness by allocating equal rates to all competing
flows, whereas video streams naturally differ in their utilities
of rate depending on their contents. For instance, it would
be desirable for an action movie sequence to be streamed at a
higher rate than a head-and-shoulder news clip competing over
the same bottleneck wireless link. Such content-aware allocation is missing in today's systems. Thirdly, clients connecting
to the same access node may experience different throughputs
over their respective wireless links, due to factors such as
distance and channel fading characteristics. Without proper
in-network information, rate adaptation decisions made at the
senders can easily lead to inefficient resource sharing, e.g. the
occurrence of head-of-line blocking.
In this paper, we address the above issues in a novel rate
adaptation scheme for streaming video over a highly dynamic
environment. As shown in Fig. 1, our design introduces a

978-1-4577-2053-6/12/$31.00 ©2012 IEEE

Fig. 1. Architecture overview of the adaptive video streaming system.

proxy at the edge of the network, right where congestion
over the wireless links occurs. This allows the rate adaptation module to constantly monitor the bottleneck buffer
level, which, in turn, reflects variations in the throughput and
delay of wireless links for all receivers. To strike a balance
between computational complexity and efficiency, we adopt
the latest H.264/SVC standard [3] for lightweight in-network
rate adaptation. It can also improve the content caching
performance [4]. By combined usage of temporal scalability
and amplitude scalability, a wide bitrate range (with a factor of
more than 10) is allowed. The resulting scalable video stream
can be decoded at different frame rates (FR) and quantization
stepsizes (QS). Based on the parametric models from our
prior work [5], we derive a QoE model that relates the video
subjective quality under a given rate by choosing the optimal
FR and QS. This model (also called Rate-Quality model)
works as a utility function for the rate adaptation module to
maximize the overall viewing experience of all streams.
We solved two specific problems: i) to allocate the video rate
for each stream based on their respective rate-quality relations
and wireless link throughputs and the common bottleneck
buffer level; and ii) to extract video packets belonging to the
appropriate temporal and amplitude layers from each SVC
stream based on the allocated rate. The first problem is solved
by an iterative solution, whereby the per-stream rate is calculated based on periodic updates of bottleneck buffer level and
relative link throughputs. The second problem can be solved
offline, by pre-ordering the video temporal and amplitude layers in a rate-quality optimized way so that each additional layer
offers maximum quality improvement for the rate increment.
Extensive simulation studies confirm that our scheme adapts
more swiftly to the dynamic wireless environment than TFRC.
The proposed scheme provides content-aware and channel
condition-aware network resource allocation. Furthermore, it
is flexible enough to support differentiated service for video
streams with different user-specified importance levels.

7088

TABLE I
PARAMETERS FOR R ATE -Q UALITY M ODEL AND THE RMSE

A. Related Work
It has been long agreed that the video streaming rate needs
some form of adaptation to match the time-varying wireless
channel capacity [6], to provide a better user experience. [7]
proposed a TCP-friendly video transport protocol targeting
for wireless environment, but it is content-agnostic. [8]–[10]
rely on using a video rate-distortion model to solve network
resource allocation while providing video content-awareness.
The model is however only applicable for videos at a ﬁxed
frame rate. Cross-layer design is also investigated in the
literature to improve the video adaptation over wireless [11],
[12]. A simulation study [13] reported the performance of
streaming SVC over DCCP. It is conﬁrmed that TFRC is suitable for adaptive SVC streaming. In a very recent work [14],
proxy-assisted video adaptation is considered for the case
where multiple streams share a common backbone network
and the video rate-distortion model is used. Our work stands
apart from existing approaches by combining the rate and
quality adaptation capability of H.264/SVC with a rate-quality
tradeoff model that considers effect of both frame rate and
quantization stepsize on the rate and quality. In addition, link
bandwidth heterogeneity is considered in the rate allocation.
II. R ATE -Q UALITY M ODELING AND Q UALITY O PTIMIZED
O RDERING OF SVC LAYERS
A. Deriving Rate-Quality Model
We use the pair (q, f ) to designate a scalably encoded video
stream with QS q and FR f . A stream at the full rate Rmax
and quality Qmax corresponds to a minimum QS q min and a
maximum FR f max . The quality and bitrate for a substream
with q ≥ q min and f ≤ f max can be modeled as [5]:
−c

q
q min

f

1 − e−d f max
e−c
1 − e−d
q
f
R(q, f ) = Rmax ( min )−a ( max )b
q
f

Q(q, f ) = Q

max e

(1)
(2)

Here a, b, c, d are model parameters and can be estimated
based on the video characteristics [5].
It can be veriﬁed that, given a bitrate constraint R, there is
a unique (q, f ) resulting in maximum achievable quality Q.
Based on (1) and (2), we can drive the optimal rate-quality
tradeoff numerically. To simplify the notations, we deﬁne
normalized bitrate as R̃ = R/Rmax , and normalized quality
as Q̃ = Q/Qmax . Fig. 2 shows the numerically computed
optimal rate-quality tradeoff curves in terms of normalized
bitrate for seven sequences with various video characteristics.
We found that these curves can be closely approximated by
Q̃(R̃) = e−αR̃

−β

+α

(3)

where α, β are model parameters. The model parameters for
each sequence can be found by least squares ﬁtting into the
numerically calculated (R̃, Q̃) data for that sequence. Table I
summarizes the model parameters for each sequence and
the model accuracy in terms of the root-mean-square error
(RMSE). The ﬁtting curve for individual sequence matches

Seq. Name
AKIYO
CITY
CREW
FOREMAN
FOOTBALL
ICE
WATERFALL
UNIFIED

α
0.12
0.14
0.20
0.15
0.13
0.17
0.14
0.16

Rmax (kbps)
163.2
658.3
1381.4
805.5
2154.1
693.0
462.9
N/A

β
0.69
0.69
0.59
0.68
0.63
0.67
0.68
0.66

RMSE
0.003
0.003
0.003
0.003
0.005
0.004
0.006
0.041

with its data very accurately. As shown in Fig. 2, a uniﬁed
model with α = 0.16 and β = 0.66 can also provide a good
ﬁtting across all sequences. Note that the maximum rate Rmax
is still video-sequence dependent. We can rewrite the Q̃(R̃)
as a function of the absolute rate as
R

Q̃(R) = e−α( Rmax )

−β

+α

(4)

B. Rate-Quality Optimized Ordering of SVC Layers
To efﬁciently stream a pre-coded scalable video targeting
for dynamic environment, it is desirable to pre-order the SVC
layers in a rate-quality optimized manner, so that each additional layer yields the maximum possible quality improvement.
With such a pre-ordered SVC stream, the proxy can simply
keep sending additional layers, until the rate target is reached.
Suppose there are M temporal layers and N amplitude
layers, the corresponding feasible choices of FR and QS are
{f1 , f2 , ..., fM } and {q1 , q2 , ..., qN }, respectively, where FR
is increasingly ordered and QS is decreasing ordered. We
also deﬁne the gradient of a quality enhancement layer as the
quality gain over the rate increase, i.e. ΔQ/ΔR. Starting from
the base layer encoded with (q1 , f1 ), we use Algorithm 1 to
generate the rate-quality optimized table Topt . The associated
complexity is O(max(M, N )).
Algorithm 1 Generate rate-quality optimized table Topt
put (q1 , f1 ) into table Topt ; qid ← 1; f id ← 1.
while qid < N || f id < M do
check the gradients of enhancement layers (qqid+1 , ff id ),
(qqid , ff id+1 ), (qqid+1 , ff id+1 ) if feasible.
put the enhancement layer with the largest gradient into
Topt ; update qid and f id accordingly;
end while
The rate-quality optimized pre-order table is generated for
sequence FOREMAN as an example. Fig. 3 shows the ratequality relations of points in Topt and the corresponding
(q, f ). Clearly, many feasible points are not optimal and are
removed, as can be seen in Fig 3(a). The residual points
(Topt ) follow the rate-quality model (4) very closely, indicating
that ordering SVC layers based on these points yields nearoptimal rate-quality tradeoff. As the target rate increases,
the FR monotonically increases while the QS monotonically
decreases, thereby satisfying dependency across the layers,

7089

Fig. 2.
Normalized optimal RateQuality tradeoff curves for seven sequences: Akiyo, City, Crew, Foreman,
Football, Ice, Waterfall. The solid line
gives a uniﬁed rate-quality model.

Fig. 3.

as shown in Fig 3(b). Topt is also compared with two other
preordering schemes as shown in Fig 3(c), where one method
always increases FR if possible and the other method always
decreases QS if possible. Clearly, Topt outperforms the other
two methods that it typically gives a higher quality under the
same rate constraint, especially in medium rate region.
III. Q UALITY M AXIMIZATION F RAMEWORK AND
A LGORITHMS F OR R ATE A LLOCATION
In this section, we develop a subjective quality maximization framework for rate allocation among multiple wireless
receivers under the same wireless access node. The framework
is based on the rate-quality model in (4) and can be easily
adapted to accommodate other models.
A. Problem Formulation
Consider I video receivers sharing a common access node
with J background ﬂows. For each receiver i ∈ {1, 2, ..., I}
experiencing a wireless link throughput of Ci , we determine
the sending rate for each video receiver by solving the following constrained optimization problem:
max

R

s.t.

I

i=1
J


wi Q̃i (Ri )
Rjb

Cb
j=1 j
Rimin ≤

+

(5)

I

Ri
i=1

Ci

≤1

Ri ≤ Rimax

(6)
∀i = 1, . . . , I

(7)

B. Iterative Solution
It can be easily veriﬁed that Q̃i (R̃i ) is concave if R̃i ≥
1/β
, which is around 0.015 given the parameters in
Table I. The resulting quality at such low rate is unacceptable,
therefore, we claim that, in practical video rate regions, Q̃i (R̃i )
is concave, as shown in Fig. 2. We propose to use the following
iterative algorithm to approach the local optimum, which is
αβ
β+1

(c) Topt vs FR or QS-ﬁrst
preordering

Quality optimized table for the sequence FOREMAN.

also the global optimum given the concavity of the objective
function (5). 
I
J
J
Denote x := j=1 Rjb + i=1 Ri and y := j=1 Rjb /Cjb +
I
i=1 Ri /Ci the instantaneous incoming rate and the required
serving time at the access point, respectively. Then C̃ := x/y
is the instantaneous effective link outgoing rate. Following
the same idea of the primal-dual algorithm in network rate
control [15], we propose the following two iterative steps:



p
τ
(8)
ΔRi = θRi wi Q̃i (Ri ) −
Ci
x
Δp = φ (y − 1) τ = φ(x − C̃)τ
(9)
y


Here θ, φ are two scaling factors; Q̃i (Ri ) is the ﬁrst derivative
of Q̃i (Ri ) w.r.t Ri , which can be analytically calculated from
(4). p denotes the “shadow price” of using the link and τ is the
update time interval. Intuitively, the value of p increases (Δp
is positive) when the network is temporarily over-congested,
leading to a smaller ΔRi thus negative or slower increment of
Ri , whereas temporarily underutilization of the network results
in decreased p (Δp is negative) and consequently higher Ri
from all contributing streams. It is also clear that p is in fact
bottleneck queue length scaled by φ according to (9).
C. Implementing the Iterative Algorithm at the Proxy

where Rimin denotes the base layer bitrate for video i; Rjb and
Cjb denote the trafﬁc rate and link throughput of j th background ﬂow. The objective function (5) is the weighted sum
of subjective quality over all video receivers. The constraint (6)
ensures that the aggregated channel utilization time is below
the maximal system utilization ratio (which we assume to be
1 here, but can in general be less than 1). (7) speciﬁes the
video rate limits for each receiver.



(b) corresponding (q, f )
values for points in Topt

(a) rate-quality tradeoff of
points in Topt

We implement the iterative algorithm (8)-(9) at the proxy in
two separate modules: link buffer monitor and video adapter
both at the proxy. Fig. 4 shows the diagram of the two modules
and the signaling in between. The link buffer monitor checks
the bottleneck queue length once every τ seconds. It is also
responsible for estimating the link throughput Ci for each
receiver i. In our system, the packets’ inter-departure time at
the interface queue is inspected and it is used to derive the
instantaneous throughput of the link that transports the packet
under consideration (via dividing the packet length by the
inter-departure time for that packet). Then, the link throughput
Ci can be estimated by averaging over a number of packets.
The optimal rate allocation module will calculate the new
stream rate based on the feedback from the link buffer monitor
and the video rate-quality parameters (α, β, Rmax ) embedded
in the SVC stream. Then, the SVC stream is adapted to the
new rate by simply sending video packets up to the target
rate assuming stream is pre-ordered in the quality-optimized
manner, as discussed in Sec. II-B. Note that the pre-ordering
should ideally be done at video encoder so that the video

7090

Fig. 4. Main components in proxy-based adaptation architecture.

Fig. 5. Simulation topology setup. (a) Topology for the first set of simulation; (b) Topology for the
second set of simulation.

Fig. 6. Wireless SNR and PRY rate traces from a real-world measurement.

streams arriving at the proxy are already in optimal orders.
However, it is possible to have the proxy to order the SVC
layers if the video servers are agnostic of the rate-quality
model adopted by the proposed system.
IV.

PERFORMANCE EVALUATION

In this section, we evaluate our system design with extensive
simulations based on the ns-2simulator. We implemented
video adaptation agents and a video player emulator which
generates video playback trace. We conducted two sets of
simulations to investigate the dynamic and static behavior of
the proposed design. Two typical video sequences are used
in the simulations: FOREMAN and FOOTBALL; both have a
spatial resolution of 352x288 pixels (CIF) and temporal rate
of 30 fps. We encode the sequences with JSVM version 9.12
to generate SVC streams with 5 COS layers and 5 temporal
layers, and then pre-order the packets in a quality-optimized
manner as explained in Sec. IT-B. The rate-quality model
parameters are given in Table I.
We choose TFRC as a comparison rate control mechanism,
as it is targeting for media streaming. For our adaptation
scheme, we choose 0=1, ¢=5I(1/Mbps) where I denotes
number of receivers, and wi=l for every receiver i unless
otherwise stated. The proxy senses the interface queue length
every 50ms and estimates the effective bandwidth for each link
by averaging over 16 most recent packets. The packet size is
set to 500 bytes and the access point queue size is set to 75
packets [16]. The initial playback delay is 2 seconds.
A. One Receiver with a Dynamic Wireless Environment
In this set of simulation, we are interested in the responsiveness of our system design to the dynamic behavior of
the wireless link. Fig. 5(a) illustrates the simulation topology
setup. The background TCP traffic is generated by a FTP
session, traversing the same AP as the video stream. The
wireless link between AP and MO is driven by a real-world
WiFi measurement trace obtained when driving around Mountainview, CA. The average speed of the vehicle was around
20mph. Fig. 6 shows the signal to noise ratio (SNR) and PHY

Fig. 7. Performance comparison of proxy-based adaptation and TFRC. The
average sending rates for proxy-based adaptation and TFRC are 1601kbps and
1072kbps respectively. The average playback rates are 1333kbps and 919kbps
respectively (not shown here). The resulting average normalized qualities are
0.66 and 0.47 respectively (not shown here).

rate over time. The video server starts streaming at time 0
while FTP starts at time 80. Both flows end at time 200.
Figure 7 shows the performance comparison for proxy
based adaptation and TFRC scheme. From Fig. 7(a), it can
be seen that when the channel condition is good and stable,
e.g., around time 50, 90 and 150, TFRC achieves good
performance. However, when the channel quality changes
dynamically, TFRC can not react agilely experiencing a slow
convergence speed, for example during time 75-85 and 125 to
140. On the other hand, proxy-based adaptation can adapt the
sending rate quickly when channel condition changes, and the
resulting video playback quality is significantly improved over
the TFRC scheme. The playback quality follows similar trend
and are not shown separately. Fig. 7(b) shows the throughput
of the background TCP traffic over time. TCP throughputs are
very similar under both types of competing video streams. This
indicates that the proxy-based adaptation is also TCP-friendly.
B. Multiple Receivers with Static Behavior

In this second part, we isolate the randomness of the
wireless network and use fixed wireless links to drive the
simulator. The topology setup is given in Fig. 5(b) where
several servers stream videos via individual wireless links to
the receivers. There is no background traffic injected.
1) Performance with Varying Number of Users: The number of concurrent users ranges from 4 to 36 with half of them
receiving FOREMAN and the others receiving FOOTBALL.
In both groups of receivers, half of them have a good link
condition of 54Mbps PHY rate and the others have a fair link
condition of 6Mbps PHY rate. All users start at time 0 and
the total simulation time is 100 seconds. Results are averaged
over the last 60 seconds.
Figure 8 shows the average video rate for each category
of receivers. Comparing Fig. 8(a) and Fig. 8(b), we can easily
notice the effectiveness the content-aware and link throughputaware rate adaptation of the proxy-based approach. Fig. 8(a)

7091

(a) TFRC
Fig. 8.

(b) Proxy-adapt

Video rate received by each group of users.

(b) Quality

(a) Rate

Fig. 9. Users’ receiving rate and quality under the Proxy-based and TFRC
schemes with heterogeneous weight assignment.

shows that, with TFRC, all users are receiving similar video
rate regardless of their video characteristics and link status.
This also leads to the head-of-line blocking especially when
the system is overloaded with large number of slow receivers.
Fig. 8(b) conﬁrms that with proxy-based scheme, FOOTBALL
users will receive higher rate than FOREMAN users if their
channel conditions are the same, whereas users with poor
channel quality will receive lower rates than those with good
channel quality. Clearly, with proxy-based scheme, received
rate and thus video quality not only depends on the video
content but also depends on the link throughput.
2) Differential Services: We consider an AP being shared
by 9 users divided into three groups. The ﬁrst group, denoted
as G1, receives FOREMAN; the second group, G2, receives
FOOTBALL and the third group G3, also receives FOOTBALL. Two sets of weightings are investigated. The ﬁrst one
(denoted as Proxy-1), targeting for equal subjective quality, has
weights 1 and 2.6 and 2.6 respectively for the three groups.
The second set (denoted as Proxy-2) has weights 1, 2.6 and 10
respectively for the three groups, so that G1 and G2 receive
similar quality and G3 receives a higher quality. In practice,
the second set may represent the case where one group of
users paid premium price for better video services.
All users have link PHY rate 12Mbps. As shown in Fig. 9,
with proxy-based scheme and the ﬁrst set of weights (Proxy1), all FOOTBALL receivers obtain higher rates than FOREMAN receivers and all receivers receive very similar quality.
With the second set of weights (Proxy-2), the premium users
are able to receive a much higher quality than the other users.
TFRC can not provide either equal or differentiated quality as
it attempts to allocate same rates to all users. As a result, while
FOREMAN users enjoy a higher quality, all FOOTBALL users
experience much lower quality, so it is not content-aware.
V. C ONCLUSIONS
The main contributions of this study include: i) We derived
a novel analytical rate-quality model, that relates the maximum
achievable quality for a given video rate, for scalable video

with both temporal and amplitude scalability; ii) We presented an efﬁcient method for pre-ordering the temporal and
amplitude layers of a scalable stream to achieve rate-quality
optimality; iii) We proposed a proxy-based video adaptation
architecture for multi-stream video streaming to wireless nodes
sharing the same access point, which can react quickly to
changes in the link conditions of the wireless nodes; iv) We
derived an iterative multi-stream rate allocation scheme at the
proxy, that can maximize a weighted sum of received video
quality at all receivers. The scheme bases on local observation
of buffer queue length as well as packet sojourn time; it
does not require feedback from receivers. The allocation result
explicitly accounts for heterogeneity in both the video streams
and the underlying wireless link capacities experienced by
different users. The proposed scheme also readily supports
differentiated service for users of different relative importance
levels, yet competes fairly against conventional TCP ﬂows.
R EFERENCES
[1] Cisco, “Visual networking index: global mobile data trafﬁc forecast
update, 2010-2015,” Febrary 2011.
[2] S. Floyd, M. Handley, J. Pahdye, and J. Widmer, “Tcp friendly rate
control (TFRC): protocol speciﬁcation,” RFC 5348 (Proposed Standard),
September 2008.
[3] ITU-T recommendation H.264-ISO/IEC 14496-10(AVC), advanced video
coding for generic audiovisual services, amendment 3: scalable video
coding, ITU-T and ISO/IEC JTC 1, 2005.
[4] Y. Sanchez, T. Schierl, C. Hellge, T. Wiegand, D. D. Vleeschauwer,
W. V. Leekwijck, D. Hong, and Y. L. Louedec, “iDASH: improved
dynamic adaptive streaming over http using scalable video coding,” in
Proceedings of ACM MMsys, Feb. 2011, pp. 257–264.
[5] Z. Ma, M. Xu, Y.-F. Ou, and Y. Wang, “Modeling rate and perceptual
quality of video as functions of quantization and frame rate and its
applications,” accepted to IEEE Trans. on CSVT, 2011.
[6] Q. Zhang, W. Zhu, and Y. Zhang, “End-to-end QoS for video delivery
over wireless internet,” Proceedings of the IEEE, vol. 93, no. 1, pp.
123–134, January 2005.
[7] G. Yang, T. Sun, M. Gerla, M. Y. Sanadidi, and L.-J. Chen, “Smooth
and efﬁcient real-time video transport in the presence of wireless errors,”
ACM Transactions on Multimedia Computing, Communications, and
Applications, vol. 2, no. 2, pp. 109–126, May 2006.
[8] J. Chakareski and P. Frossard, “Rate-distortion optimized distributed
packet scheduling of multiple video streams over shared communication
resources,” IEEE trans. on Multimedia, vol. 8, no. 0, pp. 207–218, April
2006.
[9] L. Zhou, X. Wang, W. Tu, G. Muntean, and B. Geller, “Distributed
scheduling scheme for video streaming over multi-channel multi-radio
multi-hop wireless networks,” IEEE Journal on Selected Areas in
Communications, vol. 28, no. 3, pp. 409–419, April 2010.
[10] X. Zhu and B. Girod, “Distributed media-aware rate allocation for
wireless video streaming,” IEEE Trans. on CSVT, vol. 20, no. 11, pp.
1462–1474, January 2010.
[11] M. van der Schaar and N. Shankar, “Cross-layer wireless multimedia
transmission: challenges, principles and new paradigms,” IEEE Wireless
Communications, vol. 12, no. 4, pp. 50–58, August 2005.
[12] H. Zhang, Y. Zheng, M. Khojastepour, and S. Rangarajan, “Crosslayer optimization for streaming scalable video over fading wireless
networks,” IEEE Journal on Selected Areas in Communications, vol. 28,
no. 3, pp. 344–353, April 2010.
[13] B. Gorkemli and A. M. Tekalp, “Adaptation strategies for streaming svc
video,” in Proceedings of IEEE ICIP, Sept. 2010, pp. 2913–2916.
[14] J. Chakareski, “In-network packet scheduling and rate allocation: a
content delivery perspective,” IEEE trans. on Multimedia, vol. 13, no. 5,
pp. 1092–1102, October 2011.
[15] R. Srikant, The mathematics of internet congestion control. Birkhauser,
2003.
[16] Cisco. QoS on wireless LAN controllers and lightweight APs conﬁguration example.

7092

Proceedings of the Twenty-First International Conference on Automated Planning and Scheduling

Cross-Domain Action-Model Acquisition
for Planning Via Web Search
Hankz Hankui Zhuoa and Qiang Yangb and Rong Pana and Lei Lia
a

b
Department of Computer Science
and Engineering, Hong Kong University
of Science and Technology, Hong Kong
qyang@cse.ust.hk

Department of Computer Science,
Sun Yat-sen University,
Guangzhou, China
{zhuohank,panr,lnslilei}@mail.sysu.edu.cn
Abstract

models simultaneously for HTN planning, assuming that a
set of hierarchical structures was given, just to name a few.
Despite the success of the previous systems, they are all
based on the assumption that there are enough training examples for learning high-quality action models.
In this paper, we consider the problem of learning action
models with only limited amount of data. Given just a limited amount of training data, we make use of action-models
already created beforehand in other related domains, which
are called source domains, to help us learn actions in a target
domain. The motivation for this problem is that, in many situations, it is usually difficult or expensive to collect a large
amount of training data to learn action models in the target
domain, while it is easier to find related knowledge or information in some auxiliary domains. If we can find a bridge
between these domains, transfer learning can then be used
to help learn the action models in the target domain. For
instance, in the NASA rovers planning domain, it is not an
esay task to collect a large number of plan traces for learning
action models for the robotic rovers to explore Mars. However, there may exist one or more related domains, such as
robots exploring in a coal mine, or driving truck driverlog1 ,
where action models have already been created. These domains provide helpful knowledge for learning action models
in the rovers planning domain. In this paper, we aim at identifying similar parts of these source domains to help improve
the learning of the target domains.
A key challenge in this problem is how to bridge the target
and source domains. We observe that although the actions
in the source domain and the target domain are different,
some of them are similar in their semantics. For instance,
the action “navigate” in the rovers1 domain (viewed as the
target domain), is similar to the action “walk” in the driverlog1 domain (viewed as the source domain). The former
action indicates that a rover navigates from one position to
another, while the latter indicates that a driver walks from
one location to another. Both of them describe the action of
changing locations, and this relationship may have already
been recorded by human editors on some Web sites. As a result, these two actions are linked through some Web pages,
which allow us to build a mapping between these domains
via Web search. In this paper, we propose to bridge the tar-

Applying learning techniques to acquire action models is an
area of intense research interest. Most previous works in this
area have assumed that there is a significant amount of training data available in a planning domain of interest, which we
call target domain, where action models are to be learned.
However, it is often difficult to acquire sufficient training data
to ensure that the learned action models are of high quality.
In this paper, we develop a novel approach to learning action models with limited training data in the target domain
by transferring knowledge from related auxiliary or source
domains. We assume that the action models in the source
domains have already been created before, and seek to transfer as much of the the available information from the source
domains as possible to help our learning task. We first exploit a Web searching method to bridge the target and source
domains, such that transferrable knowledge from source domains is identified. We then encode the transferred knowledge together with the available data from the target domain as constraints in a maximum satisfiability problem, and
solve these constraints using a weighted MAX-SAT solver.
We finally transform the solutions thus obtained into highquality target-domain action models. We empirically show
that our transfer-learning based framework is effective in several domains, including the International Planning Competition (IPC) domains and some synthetic domains.

Introduction
AI planning techniques often require a given set of action
models as input. Creating action models, however, is a difficult task that costs much manual effort. The problem of
action-model acquisition has drawn a lot of interest from researchers in the past. For instance, McCluskey et al. (Blythe
et al. 2001; McCluskey, Liu, and Simpson 2003) designed
a system to interact with a human expert to generate action
models. Amir (Amir 2005) introduced a tractable and exact technique for learning action models known as Simultaneous Learning and Filtering, where the state observations
were needed for learning. Yang et al. (Yang, Wu, and Jiang
2007) presented a framework for automatically discovering
STRIPS (Fikes and Nilsson 1971) action models from a set
of successfully observed plans; Zhuo et al. (Zhuo et al.
2009) proposed to learn method preconditions and action
c 2011, Association for the Advancement of Artificial
Copyright 
Intelligence (www.aaai.org). All rights reserved.

1

298

http://planning.cis.strath.ac.uk/competition/

connect two domains. In the past, some previous research
works considered learning common sense knowledge from
the Web to assist model training, which was inspired by the
research of Etzioni et al. (Etzioni et al. 2004). Bollegala
et al. (Bollegala, Matsuo, and Ishizuka 2009) proposed a
relational similarity measure, using a Web search engine, to
compute the similarity between semantic relations implied
by two pairs of words. Perkowitz et al. (Perkowitz et al.
2004) proposed to mine the natural language descriptions of
activities from ehow.com as labeled data, and translate them
into probabilistic collection of object terms. Zheng et al.
(Zheng, Hu, and Yang 2009) developed a mapping between
activities in two domains by learning a similarity function
via Web search.
The MAX-SAT problem (Borchers and Furman 1998) for
a CNF formula φ is the problem of finding an assignment
of values to propositional variables that minimizes the number of unsatisfied clauses (or equivalently, that maximizes
the number of satisfied clauses). In propositional logic a
variable xi may take values false or true. A literal li is a
variable xi or its negation x̄i . A clause is a disjunction of
literals, and a CNF formula φ is a conjunction of clauses.
The length of a clause is the number of its literals. The size
of φ, denoted by |φ|, is the number of all its clauses. An assignment of truth values to the propositional variables satisfies a literal xi if xi takes the value true and satisfies a literal
x̄i if xi takes the value false, satisfies a clause if it satisfies
at least one literal of the clause, and satisfies a CNF formula if it satisfies all the clauses of the formula. An empty
clause, denoted by , contains no literals and cannot be satisfied. An assignment for a CNF formula φ is complete if
all the variables occurring in φ have been assigned; otherwise, it is partial. MaxSatz (LI, Manya, and Planes 2007;
LI et al. 2009) implements a lower bound computation
method that consists of incrementing the lower bound by one
for every disjoint inconsistent subset that can be detected
by unit propagation. Moreover, the lower bound computation method is enhanced with failed literal detection. The
variable selection heuristics takes into account the number
of positive and negative occurrences in binary and ternary
clauses.

get domain and a related source domain by searching Web
pages related to the target domain and the source domain,
and then building a mapping between them by calculating
the similarity between their corresponding Web pages.
In this paper, we present a novel algorithm to Learn
Action models with transferring knowledge from a related
source domain via Web search; this system is called LAWS.
We focus on learning STRIPS action models (Fikes and
Nilsson 1971) rather than the full PDDL action models (Fox
and Long 2003). In our algorithm LAWS, we build a similarity function between two sets of Web pages, and then calculates the similarity between the target domain and a related
source domain using the similarity function after searching
Web pages related to them. After that, we build a set of
weighted constraints, which we call web constraints, based
on the similarity calculated. We also build other constraints
based any available example plan traces in the target domain, which we call state constraints, action constraints
and plan constraints, respectively. We solve all the constraints (web/state/action/plan constraints) using a weighted
MAX-SAT solver (Borchers and Furman 1998), and generate target-domain action models based on the solution to the
constraint satisfaction problem.
We organize the paper as follows. We introduce the related work in the next section, and then give the formulation
of our learning problem. After that, we present our algorithm LAWS and evaluate it in the experiment section. Finally, we conclude the paper together with the future work.

Related Work
Yang et al. presented the ARMS system (action-relation
modeling system) (Yang, Wu, and Jiang 2007) for automatically discovering STRIPS (Fikes and Nilsson 1971)
action models from a set of successfully observed plans.
The ARMS system automates the process of knowledge
acquisition for action model learning in previous works,
where a computer system interacted with human experts
to generate the needed action models (Blythe et al. 2001;
McCluskey, Liu, and Simpson 2003). Amir (Amir 2005)
presented a tractable and exact technique for learning action models known as Simultaneous Learning and Filtering,
where the state observations were needed for learning. Zhuo
et al. (Zhuo et al. 2010) proposed an algorithm called LAMP
to learn complex action models with quantifiers and implications using Markov Logic Networks (MLNs) (Richardson
and Domingos 2006). Zettlemoyer et al. (Zettlemoyer, Pasula, and Kaelbling 2005) investigated into learning a model
of the effects of actions in noisy stochastic worlds. Despite
the success of these previous learning systems, they learn
action models in one domain with the assumption that there
are large enough training data available in the task domain.
Zhuo et al. (Zhuo, Yang, and Li 2009) studied the way to
learn action models by building mappings between source
domains and the target domain, which focused on syntaxlevel mapping. In contast, in this paper, we aim to explore
the semantic mapping using Web searching for action-model
learning.
Our work is related to transfer learning (Pan and Yang
2010; Caruana 1997). We exploit Web search technology to

Problem Formulation
A planning problem can be described as a triple P =
(Σ, s0 , g), where s0 is an initial state, g is a goal, and Σ
is defined by Σ = (S, A, γ), where S is a set of states,
A is a set of action models, and γ is a transition function defined by γ : S × A → S. A solution to a planning problem is an action sequence (or a plan) denoted as
(a1 , a2 , . . . , an ), where ai is an action. An action model
is defined as (a, PRE, ADD, DEL), where a is an action
name with zero or more parameters, which is called action
schema, PRE is a precondition list specifying the condition
under which the action a can be applied, ADD is an adding
list and DEL is a deleting list. Notice that in this paper we
focus on the STRIPS action model description.
A plan trace t is an action sequence with partially observed states, i.e., t = (s0 , a1 , s1 , . . . , an , sn ), where si can
be a partially observed state and ai is an action. We define

299

IN PU T: action models from a source domain driverlog.
(:action walk
:parameters (?driver - driver ?loc-from - location ?loc-to - location)
:precondition (and (at ?driver ?loc-from) (path ?loc-from ?loc-to))
:effect (and (not (at ?driver ?loc-from)) (at ?driver ?loc-to))))
...
IN PU T: action schemas, predicates and plan traces from the target
domain rovers.
Action schemas:
navigate (?x - rover ?y - waypoint ?z - waypoint)
sample_ soil (?x - rover ?s - store ?p - waypoint)
...
Predicates:
(at ?x - rover ?y - waypoint) (empty ?x - store) (full ?x - store) ...
Plan traces:
{ (at rover0, waypoint0), (empty store0), Ă}
navigate(rover0, waypoint0, waypoint1) sample_ soil(rover0, store0, waypoint1)
{ (at rover0, waypoint1) (full store0)}
...
OU TPU T: action models in the target domain rovers.
(:action navigate
:parameters (?x - rover ?y - waypoint ?z - waypoint)
:precondition (and (can_ traverse ?x ?y ?z) (available ?x) (at ?x ?y) (visible ?y ?z))
:effect (and (not (at ?x ?y)) (at ?x ?z)))
...

Figure 1: An example of inputs and outputs
Algorithm 1 An overview of our LAWS algorithm
input: (1) A set of action models As of a source domain,
(2) an action schema set A and a predicate set P in the target
domain, (3) a set of plan traces T from the target domain.
output: A set of action models At of the target domain.
1: build the similarity function build sim fun(As , A, P );
2: generate weighted constraints according to the similarity function;
3: build weighted constraints according to T ;
4: solve all the weighted constraints;
5: convert the solving result to At ;
6: return At ;

our learning problem as: given a set of action schemas, a set
of predicates, a small set of plan traces T from the target
domain Dt , and a set of action models As from a source domain Ds , how do we construct a set of action models in the
target domain Dt ? We assume that action models from Ds
were already created before, which means they are available
to be used to help learn the unknown action models in the
target domain. Here is an example of our learning problem
in Figure 1 for illustration.

Our Transfer Learning Framework
In this section, we present our algorithm LAWS to learn action models. We show an overview of LAWS in Algorithm 1.
We first build a similarity function to bridge a source domain
and the target domain, and generate a set of weighted constraints according to the similarity function (steps 1 and 2 of
Algorithm 1). After that, we build a set of weighted constraints from the plan traces from the target domain (step 3).
Finally, we solve all weighted constraints with a weighted
MAX-SAT solver, and convert the solving result to a set of
action models (steps 4 and 5).
We will give the detailed description of each step of Algorithm 1 in the following.

how to build weighted constraints based on the similarity
function (step 2) in the following two subsections.
Building the Similarity Function For each predicate p ∈
P and each action a ∈ A, if PARA(p) ⊆ PARA(a) holds,
then p is probably a precondition or an effect of a, where
PARA(p) (or PARA(a)) denotes a set of parameters of p (or
a). Thus, we build a set of predicate-action pairs from the
target domain

Building Constraints via Web Searching
In steps 1 and 2 of Algorithm 1, we aim to build constraints
from a source domain via Web searching. We will first describe how to build the similarity function (step 1), and then

P At = {hp, ai|p ∈ P ∧ a ∈ A ∧
(PARA(p) ⊆ PARA(a))}.

300

, with each yi as a tf-idf vector.
After extracting the Web data Ds and Dt , we measure the
similarity between the action-predicate pairs s and t. Note
that a possible choice to calculate the similarity between two
data distributions is using the Kullback-Leibler (KL) divergence (Kullback and Leibler 1951). However, generally the
Web text data are high-dimensional and it is hard to model
the distributions over the two different data sets. Therefore,
we propose to use the Maximum Mean Discrepancy (MMD)
(Borgwardt et al. 2006) to calculate the similarity, which can
directly measure the distribution distance without the density estimation.
Definition 1: Let F be a class of functions f : X →
R. Let p and q be Borel probability distributions, and let
X = (x1 , . . . , xm ) and Y = (y1 , . . . , yn ) be samples composed of independent and identically distributed observations drawn from distributions p and q respectively. Then
the Maximum Mean Discrepancy (empirical estimation) is

Furthermore, we also build sets of predicate-action pairs
from the source domain
P Apre
= {hp, ai|a ∈ As ∧ p ∈ PRE(a)},
s
P Aadd
= {hp, ai|a ∈ As ∧ p ∈ ADD(a)},
s
P Adel
s = {hp, ai|a ∈ As ∧ p ∈ DEL(a)}.
For each predicate-action pair t ∈ P At and s ∈ P Apre
s
del
(or s ∈ P Aadd
s , s ∈ P As ), we can exploit Web searching
to extract Web pages related to them. For instance, using the
example in Figure 1, for a pair hat, navigatei from the target domain rovers, where at is a predicate and navigate is an
action, we can search with query “navigate and at”. Then we
can get a list of search results on the page. By clicking all
search results, we can get a set of Web pages. Likewise, we
can extract Web pages by searching a pair hat, walki from
the source domain driverlog with query “walk and at”. Note
that we ignore the difference of the searching results from
different predicate-action orders, such as “at and walk” and
“walk and at”. Although the Web pages contain a lot of
information, only a small amount of it is related to the semantics of the searched query. So we apply the information retrieval to retrieve the useful information for each Web
page.
In particular, for each Web page, we first extract the plain
text as a document di . Such a document di can be further
processed as a vector xi , each dimension of which is the
term frequency-inverse document frequency (tf-idf) (Jones
1972) of each word w of di :
ni,w
|{di }|
tf -idfi,w = P
× log
,
n
|{d
i : w ∈ di }|
l i,l
where ni,w is the number of occurrences of the word w in
document di . Besides, |{di }| is the total number of collected
documents, and |{di : w ∈ di }| is the number of documents
n
where the word w appears. The first term P i,w
of the tfl ni,l
idf equation is called term frequency, which denotes the
frequency of the word w that appears in the document di .
If the word w appears more frequently in the document di ,
then ni,w is larger, and thus the whole term is larger. The
second term
|{di }|
log
|{di : w ∈ di }|
is called inverse document frequency, which denotes the
inverse document frequency for the word w. If the word w
appears in more documents of the corpus, then |{di : w ∈
di }| is larger, and thus the whole term is smaller. For example, for the word “the”, it is used in almost all the documents,
but it is a stop word without any meaning. Hence, its inverse
document frequency will vanish to zero, thus the tf-idf value
of the word approaches to zero. It means that such a word
does not encode any semantics of the searched keyword, so
it can be removed from the Web data’s feature vector.
Therefore, for a predicate-action pair s, e.g., hat, walki,
we can search it and get a set of documents
Ds = {xi |i = 1, . . . , ms }
, with each xi as a tf-idf vector. Similarly, for another pair t,
e.g., hat, navigatei, we can get another set of documents
Dt = {yi |i = 1, . . . , mt }

1X
1 X
f (xi ) −
f (yi )).
M M D[F , X, Y ] = sup (
n i=1
f ∈F m i=1
m

n

Considering the universal reproducing kernel Hilbert spaces
(RKHS), we can interpret the function f as the feature mapping function of a Gaussian kernel (Borgwardt et al. 2006),
and we have the following result given by (Borgwardt et al.
2006)
X
1
k(xi , xj ) +
m(m − 1)
m

M M D2 [F , X, Y ] =
1
n(n − 1)

n
X

k(yi , yj ) −

i6=j

i6=j
m,n
X

2
k(xi , yj ),
mn i,j=1

where k(xi , yj ) is defined as
k(xi , yj ) = exp(−

kxi − yj k2
),
2σ 2

where σ is the kernel width for the Gaussian kernel function.
Given the Web data Ds and Dt , we can finally have the
similarity function between s and t defined as
similarity(s, t) := M M D2 [F , Ds , Dt ].

Generating Web Constraints
After defining the similarity function, we present the process of generating the constraints based on the function in
step 2 of Algorithm 1. We call this kind of constraints web
constraints.
For each predicate-action pair s = hps , as i in P Apre
s , and
each pair t = hpt , at i in P At , we generate a constraint,
pt ∈ PRE(at ), and associate it with similarity(s, t) as its
weight. Intuitively, similarity(s, t) measures the degree of
the constraint, that pt is a precondition of at , being satisfied,
since ps is a precondition of as in the source domain. Likewise, we can generate weighted constraints pt ∈ ADD(at )
and pt ∈ DEL(at ) with respect to P Aadd
and P Adel
s
s .

301

Building STRIPS Semantics Constraints

We also require that this kind of constraints to be maximally satisfied, and assign the largest value of the weights of
state constraints as the weights of this kind of constraints.

Besides the constraints extracted from the source domain,
we also build constraints based on the plan traces from
the target domain, which is presented by step 3 of Algorithm 1. We will build three kinds of constraints, i.e., state
constraints, action constraints and plan constraints, each of
which will be presented in the following.

Solving All Constraints
In step 4, we solve all the weighted constraints built by steps
2 and 3 using a weighted MAX-SAT solver (Borchers and
Furman 1998). Before feeding the weighted constraints to
the weighted MAX-SAT solver, we adjust the weights of
web constraints by replacing the original weights (denoted
as wo (0 ≤ wo ≤ 1), which is calculated by the similarity
function), with
γ
× wm × wo ,
1−γ
where wm , as the scale factor for wo , is the maximal value
of weights of state constraints, and γ is a parameter to vary
the importance of web constraints. By varying γ from 0 to
1, we can adjust the weights of web constraints from 0 to
∞. We will show the experiment result of varying γ in the
next section. After adjusting the weights, we can solve the
all the weighted constraints using the weighted MAX-SAT
solver. The solving result is an assignment of all the atoms,
e.g., the atom of p ∈ PRE(a) is assigned as true. After that,
we will directly convert the solving result to action models
At in step 4 of Algorithm 1. For instance, if p ∈ PRE(a) is
true, then p will be converted to a precondition of a.

State constraints By observation, we find that if a predicate p frequently appears just before an action a is executed,
then p is probably a precondition of a. We formulate this
idea as the constraint
PARA(p) ⊆ PARA(a) ⇒ p ∈ PRE(a),
where PARA(p) (PARA(a)) means a set of parameters of p
(a), the condition of PARA(p) ⊆ PARA(a) is required by
the STRIPS description, that the action should contain all
the parameters of its preconditions or effects. Likewise, if a
predicate p frequently appears just after an action a is executed, then p is probably an effect of a. We also formulate
this idea as the constraint
PARA(p) ⊆ PARA(a) ⇒ p ∈ ADD(a).
We calculate the weights of this kind of constraints with
occurrences of them in the plan traces. In other words, if a
predicate p has occurred just before a is executed for three
times in the plan traces, and p’s parameters are included by
a’s parameters, then the weight of the constraint that p ∈
PRE(a) is 3.

Experiments
We test our learning algorithm LAWS in these domains
rovers1 , driverlog1 , zenotravel1 , laundry and dishwashing.
laundry and dishwashing are created based on the description of these two domains in the MIT PLIA1 dataset2 . We
use rovers, zenotravel and laundry as target domains, and
driverlog, and dishwashing as source domains. We collect
75 plan traces from each target domain as training data.
We evaluate our LAWS algorithm by comparing its learned
action models with the artificial action models which are
viewed as the ground truth. We define the error rate of the
learning result by calculating the missing and extra predicates of the learned action models. Specifically, for each
learned action model a, if a precondition of a does not exist
in the ground-truth action model, then the number of errors
increases by one; if a precondition of the ground-truth action
model does not exist in a’s precondition list, then the number of errors also increases by one. As a result, we have the
total number of errors of preconditions with respect to a. We
define the error rate of preconditions (denoted as Errpre (a))
as the proportion of the total number of errors among all the
possible preconditions of a, that is,

Action constraints We would like to ensure that the action models learned are consistent, i.e., an action a should
not generate two conflict conditions such as p and ¬p at the
same time. Such an idea can be formulated by the following
constraint,
p ∈ ADD(a) ⇒ p 6∈ DEL(a)
Since we would like to require that this kind of constraints
to be satisfied maximally, we assign the weights of this kind
of constraints with the largest value of the weights of state
constraints.
Plan constraints Each plan trace in the target domain provides us the information that it can be executed successfully
from the first action to the last one. In other words, actions
in a plan trace are all executable, i.e., their preconditions are
all satisfied before they are executed. This information can
be represented by the following constraint,
p ∈ PRE(ai ) ⇒ p ∈ EXE(i − 1)
where p ∈ EXE(i − 1) means p either exists in the
initial state and is not deleted by the action sequence
ha1 , . . . , ai−1 i, or is added by some action a′ prior to ai and
is not deleted by actions between a′ and ai .
Furthermore, consider an observed state oi , which is composed of a set of predicates. We require that each predicate
in oi should either be newly added by actions or exist in the
initial state. Likewise, we formulate the idea with the following constraint,

Errpre (a) =

the total number of errors of preconditions
.
all the possible precondition of a

Likewise, we can calculate the error rates of adding effects
and deleting effects of a, and denote them as Erradd (a) and
Errdel (a) respectively. Furthermore, we define the error
rate of all the action models A (denoted as Err(A)) as the
2

p ∈ oi ⇒ p ∈ EXE(i − 1)

302

http://architecture.mit.edu/house n/data/PlaceLab/PLIA1.htm

average of Errpre (a), Erradd (a) and Errdel (a) for all the
actions a in A, that is,
1 X1
(Errpre (a) +
Err(A) =
|A|
3

the same, (3) the accuracy of the learned action models with
the full constraint set, defining their weights by the similarity function, (4) the accuracy of the learned action models
with respect to different number of states in plan traces, setting γ = 0.5 and the percentage of propositions in each state
as 50%. The results are shown in Figures 3-4
From Figure 3, we can see that the accuracy of Case III
is generally higher of the other cases. This suggests that
we can get higher accuracies when we exploit the similarity
function, which is consistent with our motivation of measuring the correlation between domains by building the similarity function. Furthermore, the accuracy of Case II is generally better than the one of Case I, which indicates that the
web constraints is helpful in improving the learning result.
We can also find that the accuracies generally increase when
the number of plan traces increases.

a∈A

Erradd (a) + Errdel (a)),
and define the accuracy as Acc = 1 − Err(A).

Comparison among LAWS, t-LAMP and ARMS
We repeat our LAWS five time calculating the average of accuracy. Each time we randomly select one of each five sequential intermediate partial states being observed in plan
traces leaving other states empty, and each partial state is
selected by 50% of propositions in the corresponding full
state (a state is assumed to be represented by a set of propositions). We compare our LAWS algorithm to the previous
learning system t-LAMP (Zhuo, Yang, and Li 2009) and
ARMS (Yang, Wu, and Jiang 2007), where t-LAMP learns
action models by transferring information from source domains via building syntax mappings between the target and
source domains; and ARMS learns action models without any
information about the source domains. We show the experimental results in Figure 2, where “driverlog ⇒ rovers”
suggests we learn action models in the domain rovers by
transferring knowledge from the domain driverlog, likewise
for “driverlog ⇒ zenotravel” and “dishwashing ⇒ laundry”.
The parameter γ, which is introduced in the previous section, is set as 0.5 when running our algorithm LAWS.
From Figure 2, we can see that the accuracy of our algorithm LAWS is higher than the other two, which indicates
that exploiting the semantic information from Web performs
better than building syntax mappings (as t-LAMP does) and
learning without knowledge transfer (as ARMS does). We
can also observe that when the number of plan traces is
small, the difference between our algorithm LAWS and tLAMP (or ARMS) is larger. However, when the number
of plan traces becomes large, the gap shrinks. This phenomenon indicates that our algorithm LAWS provides better effect on learning the action models when we don’t have
enough plan traces, since when the number of plan traces becomes larger, there will be more knowledge available from
the plan traces themselves, which can be enough to learn
the action models. The result also reveals that even when
the number of plan traces is very small (e.g., 15), the learning accuracy of our algorithm LAWS will be no less than
75%, which means that exploiting the knowledge from Web
searching can really help learning action models.

1

0.98

0.96

Acc

0.94

0.92

0.9
driverlog=>rover
dishwashing=>laundry
driverlog=>zenotravel

0.88

0.86
0.1

0.2

0.4

0.6
ratio of states

0.8

1

1.1

Figure 4: The accuracy with respect to different ratios of
states
In order to see the accuracy of the learned action models
with respect to different number of states available in plan
traces, we test different ratios of states by setting γ = 0.5.
We perform this test with 60 plan traces and 50% as the ratio
of propositions in each state of plan traces. From Figure 4,
we can see that the accuracy generally increases when the
ratio increases. This is consistent with our intuition, since
the information increases and could help improve the learning process when the ratio increases.
We varied the value of the parameter γ from 0 to 1 to
see the trend of the accuracy, by fixing the number of plan
traces to 60. We show the results in Figure 5. From Figure 5, we can see that when γ increases from 0 to 0.5, the
accuracy increases, which exhibits that when the effect of
the knowledge from Web searching enlarges, the learning
accuracy gets higher. However, when γ is larger than 0.5
γ
(note that when γ = 0.5, 1−γ
will be equal to 1, which
means the weights of web constraints remain unchanged),
the accuracy becomes lower when γ increases. This is because the impact of the knowledge from the plan traces is

Ablation Study
Furthermore, we would like to study the interaction between
the impact of the knowledge extracted from Web searching
in the learning framework and the impact of the knowledge
only from the plan traces. We perform this study by calculating the accuracies of the following three cases: (1) the
accuracy of the learned action models just from STRIPS semantics constraints, (2) the accuracy of the learned action
models with the STRIPS semantics constraints and the web
constraints, setting the weights of the web constraints to be

303

driverlog => rovers

dishwashing => laundry

driverlog => zenotravel

0.95

0.95

0.95

0.9

0.9

0.9

0.85

0.85

LAWS→

0.85
LAWS→

LAWS→

0.7

← t−LAMP

0.75

0.8
← t−LAMP

Acc

0.75

0.8
Acc

Acc

0.8

← ARMS

0.7

0.7

0.65

0.65

0.65

0.6

0.6

0.6

0.55
1015

← t−LAMP

0.75

← ARMS

30
45
60
number of plan traces

0.55
1015

7580

30
45
60
number of plan traces

← ARMS

0.55
1015

7580

30
45
60
number of plan traces

7580

Figure 2: The comparison of LAWS, t-LAMP and ARMS
driverlog => rovers

driverlog => zenotravel

dishwashing => laundry

1

1

1

0.95

0.95

0.95

0.9

0.9
Case III→

0.9
Case III→

Case III→

0.8

0.75

0.8

← Case II

7580

0.65
1015

← Case I

0.75

← Case I

0.7

30
45
60
number of plan traces

← Case II
0.8

Case II→

0.75

← Case I

0.7

0.65
1015

Acc

0.85

Acc

0.85

Acc

0.85

0.7

30
45
60
number of plan traces

7580

0.65
2015

30
45
60
number of plan traces

7580

Figure 3: Comparison of different cases (Case I: the accuracy with STRIPS constraints (γ = 0); Case II: The accuracy with
STRIPS & web constraints where weights of web constraints are all the same (γ = 0.5 and wo = 1); Case III: The accuracy
with STRIPS & web constraints where web constraints are defined by the similarity function (γ = 0.5))
relatively reduced when γ becomes very large, and implies
that the knowledge from the plan traces is also important
for learning high-quality action models. In summary, with
the knowledge of the current limited plan traces, exploiting
knowledge from Web searching does help improve the learning accuracy.

network quality.

Conclusion
In this paper, we proposed a novel cross-domain actionmodels acquisition algorithm LAWS to learn action models
by transferring the knowledge from related source domains
to a target domain via Web search. We first built a similarity
function to measure the relation between two domains and
generate web constraints associated with weights defined by
the similarity function. We then built STRIPS semantics
constraints from plan traces and solve all the constraints using a weighted MAX-SAT solver. From our experiments, we
can see that our LAWS algorithm can learn the action models

We do not have space to show the running time of our
algorithm LAWS. Suffice it to say that the CPU time of the
learning process is quite reasonable. The maximal time of
our LAWS algorithm is smaller than 1,000 seconds for learning action models in our experiment on a typical 2 GHZ PC
with 1GB memory. However this time did not include the
Web searching time, since it mainly depends on the specific

304

Caruana, R. 1997. Multitask learning. Machine Learning
28(1):41–75.
Etzioni, O.; Cafarella, M. J.; Downey, D.; Popescu, A. M.;
Shaked, T.; Soderland, S.; Weld, D. S.; and Yates, A.
2004. Methods for domain-independent information extraction from the web: An experimental comparison. In Proceedings of AAAI, 391–398.
Fikes, R., and Nilsson, N. J. 1971. STRIPS: A new approach
to the application of theorem proving to problem solving.
Artificial Intelligence Journal 189–208.
Fox, M., and Long, D. 2003. Pddl2.1: An extension to
pddl for expressing temporal planning domains. Journal of
Artificial Intelligence Research (JAIR) 20:61–124.
Jones, K. S. 1972. A statistical interpretation of term specificity and its application in retrieval. Journal of Documentation 28:11–21.
Kullback, S., and Leibler, R. A. 1951. On information and
sufficiency. Annals of Mathematical Statistics 22(1):79–86.
LI, C. M.; Manya, F.; Mohamedou, N.; and Planes, J. 2009.
Exploiting cycle structures in Max-SAT. In In proceedings
of 12th international conference on the Theory and Applications of Satisfiability Testing (SAT-09), 467–480.
LI, C. M.; Manya, F.; and Planes, J. 2007. New inference
rules for Max-SAT. Journal of Artificial Intelligence Research 30:321–359.
McCluskey, T. L.; Liu, D.; and Simpson, R. M. 2003. GIPO
II: HTN planning in a tool-supported knowledge engineering environment. In Proceedings of ICAPS, 92–101.
Pan, S. J., and Yang, Q. 2010. A survey on transfer learning.
In IEEE Transactions on Knowledge and Data Engineering
(IEEE TKDE) 22(10):1345–1359.
Perkowitz, M.; Philipose, M.; Fishkin, K.; and Patterson, D.
2004. Mining models of human activities from the web. In
Proceedings of WWW, 573–582.
Richardson, M., and Domingos, P. 2006. Markov logic
networks. Machine Learning 1-2(62):107–136.
Yang, Q.; Wu, K.; and Jiang, Y. 2007. Learning action
models from plan examples using weighted MAX-SAT. Artificial Intelligence Journal 171:107–143.
Zettlemoyer, L. S.; Pasula, H. M.; and Kaelbling, L. P. 2005.
Learning planning rules in noisy stochastic worlds. In Proceedings of AAAI.
Zheng, V. W.; Hu, D. H.; and Yang, Q. 2009. Cross-domain
activity recognition. In Proceedings of UbiComp, 61–70.
Zhuo, H. H.; Hu, D. H.; Hogg, C.; Yang, Q.; and MunozAvila, H. 2009. Learning htn method preconditions and
action models from partial observations. In Proceedings of
IJCAI, 1804–1810.
Zhuo, H. H.; Yang, Q.; Hu, D. H.; and Li, L. 2010. Learning
complex action models with quantifiers and logical implications. Artificial Intelligence 174(18):1540–1569.
Zhuo, H.; Yang, Q.; and Li, L. 2009. Transfer learning
action models by measuring the similarity of different domains. In Proceedings of PAKDD.

0.95

0.9

Acc

0.85

0.8

0.75

0.7

driverlog => rover
dishwashing => laundry
driverlog => zenotravel
0

0.25

0.5
γ

0.75

1

Figure 5: The accuracy with respect to different γ
with the help of the source domains, which is more accurate than other algorithms that do not exploit any knowledge
from other domains. We also showed that exploiting knowledge from Web search is helpful in building a mapping function between the domains.
In the future, we would like to study the feasibility of applying our transferring framework to learning more expressive action models, such as PDDL, rather than STRIPS models, and study the feasibility of transferring knowledge from
multiple domains.

Acknowledgement
Hankz Hankui Zhuo thanks China Postdoctoral Science
Foundation funded project(Grant No.20100480806) and
Macao FDCT 013/2010/A for support of this research.
Qiang Yang thanks Hong Kong RGC/NSFC grant N
HKUST624/09 for supporting this research.

References
Amir, E. 2005. Learning partially observable deterministic
action models. In Proceedings of IJCAI, 1433–1439.
Blythe, J.; Kim, J.; Ramachandran, S.; and Gil, Y. 2001.
An integrated environment for knowledge acquisition. In
Proceedings of IUI, 13–20.
Bollegala, D.; Matsuo, Y.; and Ishizuka, M. 2009. Measuring the similarity between implicit semantic relations from
the web. In Proceedings of WWW.
Borchers, B., and Furman, J. 1998. A two-phase exact algorithm for max-sat and weighted max-sat problems. Journal
of Combinatorial Optimization 2(4):299–306.
Borgwardt, K.; Gretton, A.; Rasch, M.; Kriegel, H.;
Scholkopf, B.; and Smola, A. 2006. Integrating structured
biological data by kernel maximum mean discrepancy. In
Proceedings of ISMB, 49–57.

305

APPROXIMATE FAIR DROPPING FOR
VARIABLE-LENGTH PACKETS
THE CHOKE ROUTER ALGORITHM PROVIDES AN APPROXIMATELY FAIR
BANDWIDTH ALLOCATION AT A LOW IMPLEMENTATION COST. THE AUTHORS
CONSIDER PERFORMANCE AND IMPLEMENTATION ISSUES ASSOCIATED WITH

CHOKE, WITH PARTICULAR EMPHASIS ON VARIABLE-LENGTH PACKETS.

Konstantinos Psounis
Rong Pan
Balaji Prabhakar
Stanford University

48

To provide high-quality service
under heavy user loads, the Internet depends
on congestion avoidance mechanisms implemented in the transport-layer such as the
transmission-control protocol (TCP). However, many TCP implementations don’t
include—either deliberately or by accident—
a congestion avoidance mechanism. Moreover, a growing number of user datagram
protocol (UDP)-based applications running
on the Internet don’t back off properly when
they receive congestion indications. As a
result, these applications aggressively use more
bandwidth than other TCP-compatible flows.
Therefore, it’s necessary to have router mechanisms to shield responsive flows from unresponsive or aggressive flows and to provide
good quality of service.1
All of the router algorithms (scheduling and
queue management) discussed in the “Router
algorithms” box have either provided fairness
or were simple to implement, but not both
simultaneously. In this article, we explore the
CHOKe 2 (choose and keep for responsive
flows, choose and kill for unresponsive flows)
algorithm, which combines fairness and simplicity, and we address approximating byteby-byte fairness and implementation issues of
the algorithm.

Background
CHOKe uses the observation that the
FIFO-buffer contents form sufficient statistics about incoming traffic to penalize misbehaving flows in a simple fashion. The state,
taken to be the number of active flows and the
flow identification of each of the packets, is
assumed to be unknown to the algorithm.
The only observable data for the algorithm is
the total occupancy of the buffer.
Specifically, CHOKe calculates the average
occupancy of the FIFO buffer using an exponential moving average window exactly as in
the random early detection (RED) algorithm.3
It marks two thresholds on the buffer, a minimum threshold, minth and a maximum threshold, maxth. When the average queue size is less
than minth, every arriving packet is queued into
the FIFO buffer. When the average queue size
is larger than minth, each arriving packet is compared with a randomly selected packet, called
a drop candidate packet, from the FIFO buffer.
Packets with the same flow identification are
both dropped; otherwise, the randomly chosen packet remains in the buffer, and the arriving packet is dropped with a probability
dependent on queue size. The drop probability is computed exactly as in RED. In particular, when the average queue size is greater than

0272-1732/01/$10.00  2001 IEEE

Router algorithms
The two types of router algorithms for congestion control are scheduling and queue management.1 The well-known fair queuing (FQ) algorithm exemplifies the scheduling algorithm class. FQ requires partitioning
the buffer at each router output into separate queues, each of which
buffers each flows’ packets.2 Because of per-flow queuing, packets
belonging to different flows are essentially isolated from each other, and
one flow cannot degrade the quality of another. However, it’s well known
by researchers that this approach requires complicated per-flow state
information, making it too expensive to be widely deployed.
To reduce the cost of maintaining flow state information, the recently
proposed scheduling algorithm called core stateless FQ3 divides routers
into two categories: edge and core. An edge router keeps per-flow state
information and estimates each flow’s arrival rate. These estimates are
inserted into the packet headers and passed on to the core routers. A core
router simply maintains a stateless FIFO queue and, during periods of congestion, drops a packet randomly based on the rate estimates. Even though
this scheme reduces the core router’s design complexity, the edge router’s
design is still complicated. Also, core routers have to extract packet information differently from traditional routers, which further increases the
complexity of the scheme.
Another notable scheme, which aims to approximate FQ at a smaller
implementation cost, is stochastic fair queuing.4 SFQ classifies packets
into a smaller number of queues than FQ using a hash function. Although
this reduces FQ’s design complexity, SFQ still requires approximately 1,000
to 2,000 queues in a typical router to approach FQ’s performance.5
Scheduling algorithms can allocate fairly, but they are often too complex for high-speed implementations and don’t scale well to a large number of users. Conversely, queue management algorithms are usually
simple. Given their simplicity, the hope is to approximate fairness. Random early detection (RED)6 exemplifies this class of algorithms. A router
implementing RED maintains a single FIFO shared by all the flows and
drops an arriving packet at random during periods of congestion. The drop
probability increases with the congestion level. By keeping the average
queue size small, RED reduces the delays experienced by most flows.
However, RED can’t penalize unresponsive flows.
To improve RED’s ability for penalizing unresponsive users, a few variants—such as RED with penalty box7 and flow random early drop8—have
been proposed. However, these variants incur extra implementation overhead since they collect certain types of state information. Another inter-

maxth, every arriving packet is dropped. This
moves the queue occupancy back to below
maxth. Figure 1 (next page) shows a flow chart
of the algorithm.
An intuitive reason for why this scheme
penalizes unresponsive flows is that the FIFO
buffer is more likely to have packets belonging to a misbehaving flow, and hence, the
algorithm is more likely to choose these packets for comparison. Further, packets belong-

esting variant is stabilized RED (SRED).9 SRED evens out the occupancy of
the FIFO buffer independently of the number of active flows. More interestingly, it estimates the number of active connections and finds candidates for misbehaving flows. It achieves this by maintaining a data
structure, called the Zombie list, which serves as a proxy for information
about recent flows. Although SRED identifies misbehaving flows, it doesn’t
propose a simple router mechanism for penalizing misbehaving flows.

References
1. B. Braden et al., “Recommendations on Queue Management
and Congestion Avoidance in the Internet,” Internet
Engineering Task Force (IETF) RFC (Informational) 2309, Apr.
1998; http://www.rfc-editor.org/rfc.html.
2. A. Demers, S. Keshav, and S. Shenker, “Analysis and
Simulation of a Fair Queuing Algorithm,” J. Internetworking
Research and Experience, pp. 3-26, Oct. 1990. Also Proc.
ACM Sigcomm, ACM, New York, 1989, pp. 3-12.
3. I. Stoica, S. Shenker, and H. Zhang, “Core-Stateless Fair
Queuing: Achieving Approximately Fair Bandwidth Allocations
in High Speed Networks,” Proc. ACM Sigcomm, ACM, New
York, 1998.
4. P. McKenny, “Stochastic Fairness Queuing,” Proc. IEEE
Infocom, IEEE Press, Piscataway, N.J., 1990, pp. 733-740.
5. A. Manin and K. Ramakrishnan, “Gateway Congestion Control
Survey,” IETF RFC (Informational) 1254, Aug. 1991;
http://www.rfc-editor.org/rfc.html.
6. S. Floyd and V. Jacobson, “Random Early Detection Gateways
for Congestion Avoidance,” IEEE/ACM Trans. Networking,
vol. 1, no. 4, 1993, pp. 397-413.
7. S. Floyd and K. Fall, “Router Mechanisms to Support End-toEnd Congestion Control,” Lawrence Berkeley Laboratories
tech. report, 1997.
8. D. Lin and R. Morris, “Dynamics of Random Early Detection,”
Proc. ACM Sigcomm, ACM, New York, 1997, pp. 127-137.
9. T. Ott, T. Lakshman, and L. Wong, “SRED: Stabilized RED,”
Proc. IEEE Infocom, IEEE Press, Piscataway, N.J., 1999, pp.
1346-1355.

ing to a misbehaving flow arrive more numerously and are more likely to trigger comparisons. The intersection of these two
high-probability events is precisely the event
that causes dropping of packets belonging to
misbehaving flows. Therefore, packets from
misbehaving flows are dropped more often
than packets from well-behaved flows.
In general, the algorithm can choose m > 1
packets from the buffer, compare all of them

JANUARY–FEBRUARY 2001

49

FAIR DROPPING SCHEME

Arriving packet

AvgQ size ≤ minth?

y

n
Draw a packet at
random from queue

Admit new packet

End

Both packets
from same flow?

y

Drop both packets

n
AvgQ size ≤ minth?

y

End

n

Admit packet with
a probability p

Drop new packet

End

End

average buffer occupancy is
between intth and maxth, it sets
m = 2. More generally, we can
introduce multiple thresholds
that partition the interval
between minth and maxth into
k regions R1, R2, …, Rk and
choose different values of m,
depending on the region in
which the average buffer
occupancy falls. Obviously,
we need to let m increase
monotonically with the average queue size.

Constant- and variablelength packets

Simulation results of
CHOKe’s performance in
penalizing misbehaving flows
enable approximating fair bandwidth allocation, under both constant- and variable-length
packets.

Figure 1. The CHOKe algorithm flow chart. Nonshaded boxes represents the RED algorithm.

S(1)
S(2)

S(1)
10 Mbps

10 Mbps

S(2)

TCP
sources

TCP
sinks
S(m )

S(m +1)

R1

1 Mbps

R2

S(m)
S(m+1)

UDP
sources

UDP
sinks

S(m +n)

S(m +n)

Figure 2. Network configuration: m = TCP sources, n = UDP sources.

with the incoming packet, and drop the packets that have the same flow identification as
the incoming packet. Not surprisingly, choosing more than one drop candidate packet
improves CHOKe’s performance. This is especially true when there are multiple unresponsive flows. Indeed, as the number of
unresponsive flows increases, it’s necessary to
choose more candidate packets for dropping.
One way to automate the process so that the
algorithm chooses the proper value of m ± 1 is
to introduce an intermediate threshold, intth,
which partitions the interval between minth
and maxth into two regions. When the average
buffer occupancy is between minth and intth,
the algorithm can set m = 1, and when the

50

IEEE MICRO

Constant-length packets
Figure 2 illustrates the network configuration
of CHOKe’s performance when there is a single congested link. We simulate a particular scenario where the 1-Mbps link between routers
R1 and R2 is shared by 1-UDP and 32-TCP
sources. The UDP source sends packets at a rate
of 2,000 Kbps. Using the Network Simulator
Version 2.0,4 we plot in Figure 3 the throughput of the UDP flow under different router
algorithms: DropTail (packets are accepted at
the queue if the queue is not full, and dropped
otherwise), RED, and CHOKe. It’s evident that
while both DropTail and RED are unable to
prevent the UDP source from consuming up
to 95% of the link capacity, CHOKe limits the
UDP source’s throughput to about 25% of the
link capacity. This prevents the TCP sources
from being shut out by the UDP source.
When there are many UDP flows in the
network, CHOKe approximates fairness by
drawing more than one sample from the
queue. We set up a simulation configuration
with 32-TCP and 5-UDP sources using the
basic network topology shown in Figure 2. All
UDP sources are assumed to have the same
arrival rate, which varies simultaneously from
100 Kbps to 10,000 Kbps. Figure 4 gives the
simulation results for the CHOKe algorithm.

Variable-length packets

1,000

800
Throughput (Kbps)

In this simulation we used the automated
process previously described to decide on the
number of samples to draw. In particular, we
divided the region between minth and maxth
into three subregions and the number of samples drawn in a region is set to 2 × i – 1 (i = 1,
2, 3). By drawing up to five samples, CHOKe
manages to protect responsive flows.

DropTail
RED
CHOKe

600

400

120

Throughput (Kbps)

One problem with the basic CHOKe algo200
rithm is that it treats all packets the same
regardless of their size. As a result, flows with
larger packet sizes get more bandwidth than
0
flows with smaller packet sizes. More precise20
40
60
80
100
ly, let F1 and F2 denote two flows with equal
Time (seconds)
arrival rates (measured in bytes/sec) but different packet sizes, equal to S1 and S2 = 2 × S1 Figure 3. UDP throughput comparison.
respectively. Since the two flows have the same
arrival rate, the former will
send, on average, twice the
1,000
Total UDP throughput with
number of packets than the
mark for UDP drop percentage
latter. Thus, F1 packets will
Total TCP throughput
800
trigger more comparisons and
occupy a larger proportion of
600
the FIFO buffer, resulting in
more drops compared to the
61.1%
82.3%
packets of F2. Let T1 and T2 be
400
54.0%
the throughputs of F1 and F2
respectively in bytes/sec. It’s
38.3%
93.7%
200
easy to see that T1 < T2.
22.2%
97.9%
Figure 5 (next page)
6.6%
depicts this phenomenon.
0
The simulation setup for this
100
1,000
figure is based on the network
Total UDP arrival rate (Kbps)
topology of Figure 2. In this
case the bottleneck link with Figure 4. Self-adjusting CHOKe: throughput for 32-TCP and 5-UDP configuration.
bandwidth 1,000 Kbps is
shared by 2-UDP and 15TCP sources. Both UDP flows have a rate of basis doesn’t lead to fairness at the byte level,
1,000 Kbps, but their packet sizes are 800 and and hence basic CHOKe cannot ensure fair400 bytes respectively. TCP packet sizes equal ness among flows with variable-length pack400 bytes. It’s evident from the figure that basic ets. Thus, there is a need to penalize flows
CHOKe fails to provide fairness between the according to the number of bytes, rather than
two UDP flows. The throughput of the UDP packets, sent by them. To accomplish this, the
flow with the large packets equals 424 Kbps algorithm would ideally need to sample a ranwhile the throughput of the other UDP flow dom byte from the buffer and determine the
equals only 230 Kbps. The rest of the band- packet, and hence the flow to which this byte
width is almost equally divided among the belongs. If, as a result of the comparison, it’s
TCP flows.
determined that the byte should be dropped,
a dilemma occurs: Although it’s bytewise fair,
Chopping the packets
it’s impracticable since individual bytes canCleary, random sampling on a per-packet not be dropped from within a packet. Some-

JANUARY–FEBRUARY 2001

99.3%
10,000

51

FAIR DROPPING SCHEME

450
400

1-15 TCP flows
16 UDP flow, packet size 800 bytes
17 UDP flow, packet size 400 bytes

Throughput (Kbps)

350
300
250
200
150
100
50
0

2

4

6

8

10

12

14

16

18

Flow identifications

Figure 5. Throughput in a single congested link of a 1,000-Kbps bandwidth.

350

300

1-15 TCP flows
16 UDP flow, packet size 800 bytes
17 UDP flow, packet size 400 bytes

Analysis

Throughput (Kbps)

250

200

150

100

50

0

2

4

6

8

10

12

14

16

18

Flow identifications

Figure 6. Throughput in a single congested link of a 1,000-Kbps bandwidth.
Rate of both UDP flows is 1,000 Kbps. Each 800-byte UDP packet is chopped
to two 400-byte packets.

what surprisingly, we find that it suffices to
drop the whole packet. That is, it’s enough for
the sampling process to take place at the byte
level, while the dropping process can drop
entire packets.

52

Since byte-level sampling is not practical,
we seek a mechanism to approximate it. One
mechanism for this is to use a data structure
for drawing samples. The number of data
structure entries corresponding to each packet are proportional to its size. If Smin is the size
of the smallest packet in the queue, a packet
of size S will have S mod Smin entries in the
data structure. Uniformly sampling the data
structure entries is a good approximation of
byte-level sampling.
In a system, packets are stored in memory
at arbitrary places, and a linked list is maintained to indicate the order of the packets in
the FIFO queue. Thus, sampling can take
place in this list. To make the sampling process
byte-level fair, it suffices to fragment the packets, since this will insert in the list the proper
number of entries corresponding to each packet. We refer to this fragmentation as chopping.
Actually, some implementations do segment packets into fixed-size cells anyway. In
particular, this is common practice among
high-performance switch designs because it
makes memory management easier and more
efficient. In these cases we get the chopping
for free as a part of the process.

IEEE MICRO

Byte-by-byte fairness in the CHOKe
scheme using packet chopping can be
explained by comparing the number of drops
faced by two flows whose arrival rates are
equal, while their packet sizes differ.
In particular, assume flows F1 and F2 have
packet sizes S1 and S2 respectively, and identical arrival rates (measured in bytes/sec). Let S1
= K × S2. To simplify analysis, assume that all
the packets of a particular flow are the same
size. (For flows with variable-packet sizes, the
analysis can be extended by averaging over a
large number of packets.) According to the
chopping scheme, packets of flow F1 will be
chopped into K parts. We aim to compare the
expected number of bytes dropped from flows
F1 and F2, denoted by N1 and N2 respectively,
over a period of time during which S1 new bytes
arrive at the front of the queue from each flow.
Since the packets of size S1 will be chopped
into K parts of size S1/K, upon arrival, the probability of match pm for both flows will be the
same. It’s easy to see why this is true when there
are no packet drops. For this probability to

N1 = (S1 + K × S1/K) × pm = 2S1 × pm.

For flow F2, we examine the aggregate effect
of K arrivals to compare with the previous
case. If we consider that the number of packets of the flow in the queue is much larger than
K—this is reasonable since the flow is supposed to be misbehaving and causing congestion—then the number of packets out of K
that cause packet drops follow binomial distribution with parameters K and pm. Every
time there is a match, the newly arrived packet of size S2 = S1/K and the matched packet of
the same size will be dropped. Thus

700

600

Dashed lines
Solid lines
o
+

No chopping
Chopping used
Packet size 800 bytes
Packet size 400 bytes

500
Throughput (Kbps)

remain approximately the same when packet
drops take place, the packet drops faced by both
flows should be approximately the same.
For flow F1, every time there is a match, the
newly arrived packets of size S1 and all the
items of size S1/K that correspond to the
matched chopped packet will be dropped.
Thus

400

300

200

100

0 2
10

103
UDP rate (Kbps)

104

Figure 7. UDP throughput in a single congested link of a 1,000-Kbps bandwidth with one sample drawn from the queue.

N2 = Kpm × (2 × S1/K) = N1.

300

Simulation results
250

Throughput (Kbps)

To evaluate the performance of the chopping scheme, we ran various simulations in a
single congested link. We compared the
throughput used by flows with the same
arrival rate but different packet sizes.
Figure 6 plots the throughput obtained by
2-UDP and 16-TCP flows, when the simulation scenario is the same as in Figure 5. The
two UDP flows use up the same amount of
bandwidth, despite their packet size difference, while the TCP flows are not affected by
the chopping mechanism.
In Figures 7 and 8 we vary the rate of both
UDP flows simultaneously from 100 Kbps
to 10,000 Kbps. The rest of the simulation
parameters are the same as in Figure 5. In Figure 7 only one sample (drop candidate) is
drawn from the queue. Since one sample is
not enough to sufficiently penalize the unresponsive UDP flows, Figure 8 shows the
bandwidth allocation when two samples are
drawn. Note that the UDP with a large packet size gets a lot of bandwidth under basic
CHOKe while chopping packets equalizes
the performance of the two UDPs without
taking precious bandwidth from the TCP

200

150

100

50

0 2
10

Dashed lines
Solid lines
o
+

No chopping
Chopping used
Packet size 800 bytes
Packet size 400 bytes
103
UDP rate (Kbps)

104

Figure 8. UDP throughput in a single congested link of a 1,000 Kbps bandwidth with two samples drawn from the queue.

flows. The conclusion is that chopping packets seems to work both for any UDP rate and
under sufficient and insufficient numbers of
drop candidates.
A UDP flow that chooses a large packet size,

JANUARY–FEBRUARY 2001

53

FAIR DROPPING SCHEME

400
350

1-32
33
White bars
Black bars

TCP flows
UDP flow
No chopping
Chopping used

Throughput (Kbps)

300
250
200
150
100
50
0

5

10

15
20
Flow identifications

25

30

35

Figure 9. Throughput in a single congested link of a 1,000-Kbps bandwidth.
UDP flow rate is 1,000 Kbps.

can have an advantage over both other UDP
flows and of TCP flows. Figure 9 compares
the throughput obtained by 1-UDP and 32TCP flows with packet sizes of 800 and 400
bytes respectively. It’s evident that chopping
reduces the throughput obtained by the UDP
flow, and thus protects TCP flows from UDP
flows that try to use up more bandwidth by
choosing large packet sizes.
Packet chopping can be applied to any
flow, including TCPs. However, the TCP rate
is dictated by their back-off mechanism and
the degree of congestion. Thus, the analysis
of the chopping mechanism does not hold
for TCP flows. Given that UDP flows consume most of the available bandwidth and
TCP flows in general back off properly, it’s
not critical to investigate the problem of different packet sizes among TCP flows. We
mainly used TCP flows in the previous simulations to introduce randomness and make
the scenarios more realistic, and not to study
their throughput, which we’ve done extensively elsewhere.2

Implementation issues
Roughly speaking, packet switches are
stored in main memory (SDRAM) at arbitrary places. A linked list is maintained in a

54

IEEE MICRO

different, faster physical memory (SRAM) to
indicate the packet order in the FIFO queue.
The list items contain pointers to the memory pages where the actual packets are stored.
One way to implement basic CHOKe is to
randomly sample an address from the memory where the linked list is stored. In case of a
match, a single bit flag of the current item of
the linked list can be set to one, to indicate
that the corresponding packet should be
dropped. Marking the item is preferred over
removing it because removal is an expensive
operation since it requires breaking the list.
To save memory space, the system can immediately move (at the time of marking) the
respective memory address where the actual
packet is stored into the free list. (The free list
keeps track of all available memory spaces for
new-packet storage.) At the head of the queue,
packets corresponding to linked list items
whose flag bits are zero are sent to the outgoing line, while packets whose flag bits equal
one are ignored, see Figure 10a.
When CHOKe draws multiple drop candidates from the queue, the algorithm needs
to perform this operation more than once.
This makes the whole process slower. However, there are cases where due to the linked
list’s large size that it is stored in more than
one physical SRAM. In these cases, the algorithm may sample from each distinct memory in parallel to accelerate the procedure, see
Figure 10b.
Cases where the same memory has multiple FIFOs may require drawing samples until
a sample is drawn from the correct FIFO.
This assumes it’s possible to tell which FIFO
a sample belongs to, which can be accomplished by using a couple of bits to distinguish between FIFOs.
Packet chopping, which the enhanced version of CHOKe supports, adds complexity.
Packet chopping may require marking more
than one consecutive item on the linked list.
This is necessary when a large packet that corresponds to more than one item in the list
should be dropped. In such a case, the flag of
each of these items should be set to one.
Therefore, it may be necessary to traverse the
list both forward and backward, since the random sample can be any of the items that correspond to the large packet. However, if
possible memory waste can be tolerated by the

1

0

1

0

Outgoing 0

0

0

1

Outgoing

0

Dropped
1 Dropped

1
Sample in parallel

1 Drop 0 Send

(a)

(b)

Figure 10. Implementation for single packet dropping (a) and multiple packet dropping (b).

Front CHOKe

300

250
UDP throughput (Kbps)

system, there is no need to
mark the rest of the cells and
free the respective memory
occupied by all the cells of the
chosen packet. At the defragmentation phase, any packet
with one or more of its cells
marked can just be ignored.

Front CHOKe
Back CHOKe

200

150

100

A different approach to
avoid these implementation
50
challenges is to simply not use
random sampling. In partic0
ular, all systems keep track of
100
1,000
the head and the tail of a
UDP arrival rate (Kbps)
FIFO queue. Therefore, it’s
very easy to always choose, for Figure 11. Throughput performance: Front CHOKe versus Back CHOKe.
comparison, the identification of the packet at the
queue’s head or tail. We call these variations network topology shown in Figure 2. We varof the basic algorithm Front CHOKe and ied the UDP source rate from 100 Kbps to
10,000 Kbps. Figure 11 compares bandwidth
Back CHOKe.
Front CHOKe aids bursty traffic. Indeed, allocation of the UDP source under basic
if a flow sends bursts of lengths less than the CHOKe and Front CHOKe. When the
queue size, and separates them by silent peri- UDP rate is low, the two schemes perform
ods longer than the time it takes to service all nearly the same. As the UDP rate increases
packets of a burst, then the flow is not subject from 500 Kbps to 2,000 Kbps, Front
to drops. On the other hand, Back CHOKe CHOKe assigns a larger proportion of the
penalizes bursty traffic. Bursts will result in bandwidth to TCP flows since it aids bursty
drops, while packets that are spread out in a traffic. For larger UDP rates—more than
uniform manner over time will prevent drops. 2,000 Kbps—Front CHOKe controls the
Since TCP traffic is bursty while UDP is not, UDP bandwidth, but performs worse than
Front CHOKe is more appropriate. Front basic CHOKe. However, since these rates are
CHOKe avoids random sampling without twice the bottleneck bandwidth and therecompromising performance in terms of TCP fore unrealistic, Front CHOKe could be used
successfully. Multiple drops could be incorthroughput.
To examine Front CHOKe’s performance, porated to Front CHOKe by choosing as
we set up a simulation configuration with 32- drop candidates the m packets that are closTCP and 1-UDP sources, using the basic er to the head of the queue. A generalization

10,000

JANUARY–FEBRUARY 2001

55

FAIR DROPPING SCHEME

of that idea is to spread out the m drop candidate positions along the queue. This has the
advantage of penalizing large bursts, but it
requires that the hardware maintain more
pointers.

W

e are currently studying the implementation of the algorithm and some variants in a commercial high-speed router. MICRO
Acknowledgments
A Stanford Graduate Fellowship and a Terman Fellowship supported this research in
part.
References
1. S. Floyd and K. Fall, “Promoting the Use of
End-to-End Congestion Control in the
Internet,” IEEE/ACM Trans. Networking,
Aug. 1999.
2. R. Pan, B. Prabhakar, K. Psounis, “CHOKe,
A Stateless Active Queue Management
Scheme for Approximating Fair Bandwidth
Allocation,” Proc. IEEE Infocom, IEEE,
Piscataway, N.J., 2000, pp. 942-951.
3. S. Floyd and V. Jacobson, “Random Early
Detection Gateways for Congestion
Avoidance,” IEEE/ACM Trans. Networking,
vol. 1, no. 4, 1993, pp. 397-413.
4. Network Simulator Version 2.0; http://www.
isi.edu/nsnam/ns.

Konstantinos Psounis is a PhD candidate in
the Electrical Engineering Department at
Stanford University. He received an MS from
Stanford in electrical engineering. Psounis
graduated (his first degree) from the Electrical
Engineering and Computer Science Department of the National Technical University of
Athens, Greece. He researches probabilistic,
scalable algorithms for Internet-related problems. He has worked mainly in Web caching
and performance, and congestion control.
Rong Pan is a PhD candidate in the Electrical
Engineering Department at Stanford University. Prior to Stanford, she was with Bell Labs,
Lucent Technologies as a member of the technical staff, where she worked on Ethernet
switch and wireless ATM designs. Her research
interests are active-queue management
schemes at network routers. She is interested in
both router algorithm design and modeling.

56

IEEE MICRO

Balaji Prabhakar is an assistant professor of
electrical engineering, computer science and
(by courtesy) of engineering-economic systems and operations research at Stanford University. He received his PhD from the
Electrical Engineering Department at the
University of California, Los Angeles. He has
also served as a postdoctoral fellow at the Basic
Research Institute in the Mathematical Sciences (BRIMS), Hewlett-Packard Labs, England. He was a research scientist and lecturer
at the Electrical Engineering and Computer
Science Department at MIT. Prabhakar’s
research interests include high-speed computer and wireless networks, simple packet
switch schedulers and router mechanisms for
Internet quality of service, Web caching and
accelerating content delivery, computer network pricing mechanisms, and ad hoc wireless network algorithms. His theoretical
interests include stochastic network theory
and its relationship to information theory,
randomized algorithms arising in networking,
and probability theory. He has been awarded
the NSF Career award, the Alfred Sloan Fellowship, an Okawa Foundation Research
Grant, a Terman Fellowship from Stanford
University, and the Erlang Prize.
Direct comments about this article to Konstantinos Psounis, Stanford University,
Department of Electrical Engineering, Barnes
4B, Stanford, CA 94305; kpsounis@leland.
stanford.edu.

New product
announcements and past
issues of articles, columns,
and departments are
available at the
IEEE Micro Web site:
http://computer.org/micro

Switching Technology
IEEE Micro helps practitioners and academics understand the likely direction of strategic technologies in chips, systems, software, and applications.
These industry professionals turn to IEEE Micro for the inside story about
new designs and how they evolved, design trade-offs, performance, and implementation issues, as well as current electronic, legal, and standards issues, books
and software reviews, and key industry trends.

Readership
IEEE Micro reaches the managers, engineers, and designers of small, highperformance systems who influence system designs in their companies and
determine market winners from losers.

Editorial Calendar for 2001
January-February
Hot Interconnects

May-June
Mobile/Wearable computing

This issue focuses on the hardware
and software architecture and implementation of high-performance interconnections on chips. Topics include
network-attached storage; voice and
video transport over packet networks;
network interfaces, novel switching and
routing technologies that can provide
differentiated services; and active network architecture.
Ad close date: 2 January

The new generation of cell phones
and powerful PDAs has made mobile
computing practical. Wearable computing will soon be moving into the deployment stage.
Ad close date: 1 May

March-April
Hot Chips
An extremely popular annual issue,
Hot Chips presents the latest developments in microprocessor chip and system
technology used to construct highperformance workstations and systems.
Ad close date: 1 March

0272-1732/01/$10.00  2001 IEEE

July-August
General Interest
IEEE Micro gathers together the latest details on new developments in
chips, systems, and applications.
Ad close date: 1 July

September-October
Embedded Fault-Tolerant Systems
To avoid loss of life, certain computer systems—such as those in automobiles, railways, satellites, and other vital
systems—cannot fail. Look for articles

that focus on the verification and validation of complex computers, embedded computing system design, and
chip-level fault-tolerant designs.
Ad close date: 1 September

November-December
RF-ID and noncontact smart card applications
Equipped with radio-frequency signals, small electronic tags can locate and
recognize people, animals, furniture, and
other items.
Ad close date: 1 November

IEEE Micro is a bimonthly publication of the IEEE Computer Society.
Authors should submit paper proposals
to micro-ma@computer.org; include
author name(s) and full contact information.

57

Time Optimized Algorithm for Web Document
Presentation Adaptation
Rong Pan and Peter Dolog
IWIS – Intelligent Web and Information Systems,
Department of Computer Science, Aalborg University,
Selma Lagerlöfs Vej 300, DK-9220 Aalborg, Denmark
{rpan,dolog}@cs.aau.dk

Abstract. Currently information on the web is accessed through different
devices. Each device has its own properties such as resolution, size, and capabilities to display information in different format and so on. This calls for
adaptation of information presentation for such platforms. This paper proposes
content-optimized and time-optimized algorithms for information presentation
adaptation for different devices based on its hierarchical model. The model is
formalized in order to experiment with different algorithms.
Keywords: adaptation, hierarchical model, web document.

1 Introduction
Currently the mobile multimedia network is developing fast, and many more choices
are being provided for people to access Internet resources. A large number of resources are available on the web. However, one fixed page size fits all approach does
not fit mobile web very well mainly because of their different capabilities to exhibit
information comparing with personal computer (PC) monitors, such as the size, resolution and the color quality of a device. For example, displaying a web page on a PC
might turn out a mess on the screen of a PDA or a mobile phone: texts are jam-packed
due to the limited screen size. Pictures are fragmented and presented with inappropriate resolution, requiring scrolling both horizontally and vertically necessary to read
the whole page. With the development of various interconnected devices, the situation
is worsening.
An adaptive presentation approach of a web document might relieve such a problem. That is, the mobile device should be able to choose the best way to display the
content automatically. Our former work [11] proposed a method of retaining some
redundancy on the web documents by repeating the same information in different
scales of detail like size and color, to allow the terminal (which means the PDA, mobile phone’s screen) to choose the most suitable scale for it. In the previous example,
a traditional web page can be repeated in different grains of detail, together with some
meta-tags for the terminal to identify. When it is browsed on a PDA, the quality of
pictures will be reduced to fit the screen. If it is shown on an ordinary mobile phone,
D. Dicheva and D. Dochev (Eds.): AIMSA 2010, LNAI 6304, pp. 81–90, 2010.
© Springer-Verlag Berlin Heidelberg 2010

82

R. Pan and P. Dolog

only some text-based descriptions will be provided to facilitate the even smaller display. To sum up, the web document will permit the devices to display auto-adaptive
according to their abilities, by choosing the content’s appropriate form, and hence
alleviate the misrepresentation.
This work is a progress of a series of former works [7, 9, 10, 11]. Previously a
simpler hierarchical model was introduced as a special case of the one defined in Pan
et al [11], in which the model is generalized from binary trees to more practical unranked ones. Building on this ongoing work, this paper’s main contributions are as
follows:
For the hierarchical model, it provides an algorithm for time-optimized solutions
which is more feasible with its detailed analyses.
z The comparison among other two algorithms for content-optimized solutions and
time-optimized solutions; discussion about the advantages and disadvantages of
each solution.
z

The remainder of this paper is organized as follows. In Section 2 we discuss related
work. Section 3 will recall the hierarchical model and the formal problem related to it,
then Section 4 will propose the algorithms for such problem, analyze it and compare
the algorithms. Section 5 introduces the evaluation for three algorithms, and Section 6
presents the conclusion and future works.

2 Related Works
The ontology or knowledge based method [1, 2, 6] is a main approach to creating an
adaptive or pervasive computing environment. The knowledge to be presented is
structured upon its formal semantics and reasoned about using some reasoning tools,
based upon certain axioms and rules. This is suitable to derive deeper facts beyond
known knowledge and their exhibition. Compared with their way, the work presented
in this paper is more pages implementation-related and fit for simple format-based
hypertext to be displayed. In [13], traditional bipartite model of ontology is extended
with the social dimension, leading to a tripartite model of actors, concepts and instances, and illustrates ontology emergence by two case studies, an analysis of a large
scale folksonomy system and a novel method for the extraction of community-based
ontology from web pages.
[3] presents a statemachine based approach to design and implement adaptive
presentation and navigation. The work in this paper can be used as a target model for
a generator from a state machine.
Phanouriou [14] and Ku et al [5] also proposed markup languages to interconnect
different devices. The former one proposed a comprehensive solution to the problem
of building device-independent (or multi-channel) user interfaces promoting the separation of the interface from the application logic. It introduced an interface model to
separate the user interface from the application logic and the presentation device. It
also presented User Interface Markup Language 2 to realize the model. The latter
proposed the device-independent markup language that generated automatically and
thus unified the interfaces of home appliances, while interfaces generated by the
proposed transformation engine would also take into account interfaces previously

Time Optimized Algorithm for Web Document Presentation Adaptation

83

generated for the user and create single combined interfaces for multiple connected
appliances. These two mainly focus on both user and application interfaces.
Er-Jongmanee [4] took XML and XSLT to create user interfaces which is also a
possible way of implementation, but her work concentrates on design time adaptation
while the work presented in this paper focus runtime adaptation.
Wang and Sajeev [15] provided a good conclusion for the state-of-the-art in such
field. They studied abstract interface specification languages as the approach for device-independent application interface design. Then they classified their design into
three groups, discussed their features and analyzed their similarities and differences.

3 Hierarchical Model
This section aims at a brief recollection of the hierarchical model defined in Pan et al
[11] to make this paper more self-contained. Detailed descriptions of this model can
be found in [11].
Informally, our approach of web document presentation adaptation is to provide different abstraction levels of the content, by the following cutting-and-condensing
method. First, divide a web document into N0 segments. Then condense neighboring
segments into a briefer one by abstracting words and/or reducing picture quality, viz.
losing some type of content details. This is performed recursively to form different
levels of more general segments. Each segment corresponds to several adjacent ones in
a lower level, forming a complete tree structure with all leaves on the same level. In the
achieved tree structure, each segment serves as a node, and the relationship of abstraction forms parents and children. At last, the whole document is condensed as one
segment, that is, the root of the tree structure (the title of the web page, for example).
Suppose abstracted segments never need more capability of the terminal (in the light
that they never take more area on the terminal) than their children, then our problem can
be formalized as finding proper nodes on the tree to display, whose capability consumption suits the terminal’s limit. Let us now describe formally the model.
First is a mapping to assign a serial number for each segment
f : Infoparts → N

in which we make a sequence of all the segments of all levels, from the n-th level
(most condensed; root level) to the 0-th level (most detailed; leaf level). We assign 0
as the root’s mapping and denote the number of nodes on the i-th level as Ni. Hence
each segment’s mapping can be deducted according to its position in the tree.
Definition 1. For m, n ∈ N , denote m as n’s parent if f -1(m) is a directly compressed
part of f -1(n) and some of its neighbors; if m is n’s parent, then n is m’s child.
From Definition 1 we obtain two mappings ch and pa:
ch : N → 2N , mapping a number to all its children;
pa : N → N , mapping a number to its parent.

And for the convenience of statement, we indicate a lifting of ch as

CH : 2N → 2N , S a {ch(n) | n ∈ S}.

84

R. Pan and P. Dolog

Definition 2. For m, n ∈ N , denote m as n’s ancestor if a sequence m, m1, m2, …, mk, n
exists, such that in each adjacent pair, the left is the parent of the right; if m is n’s
ancestor, then n is m’s descendant.
Based on Definition 2, another two mappings de and an are defined:
de : N → 2N , mapping a number to all its descendants;
an : N → 2N , mapping a number to all its ancestors.

We can define the weight of each node as its exhibited size decided by the layout
manager of the browser. When the layout manager generates the nodes, each node has
its own size, which can be calculated as the weight of the consumed resources.
Definition 3. Denote wi as i’s weight, where wi is defined by an existing mapping
we : N → N , and wi = we(i).
With these definitions, the problem can be changed to a new form: with a given sequence of weights w0, w1, … wN, where wi ≤ ∑ w,j construct a set S⊆ {0, 1, …, N},
j∈ch ( i )
satisfying that for any a, N − N0 ≤ a ≤ N , ∃b ∈ S , b = a or b∈an(a) , and ∑wi<w , where w is
i∈S

given.

Definition 4. Denote a tree T as a hierarchical model, if each node of T has been
assigned a natural number as an identifier (the assignment is in breadth-first order and
begins at zero), and each node i has unique weight wi, where any parent node i and its
children set ch(i) have relationship wi ≤ ∑ w j for their weights.
j∈ch (i )

Definition 5. Set S is defined to be a cover set of a hierarchical model T, if
∀i ∈ S, i ∈T holds for any node i, and for any leaf node i ( N − N 0 ≤ i ≤ N ) in T, on the
path from it to the root node 0, there exists one and only one node s, such that s ∈ S .
(For any empty hierarchical model, its cover set is defined as empty set.)
The problem of auto-adaptation of web document can be formalized as:
Definition 6. For certain hierarchical model T and constant w, their hierarchical
problem is to find a cover set S of T, such that provided the sum of weights of the
nodes in S does not exceed w, |S| should be maximized.
For any given hierarchical model T and constant w, a natural means to find a best (or
largest) cover set S is to try every cover set of T, and choose the one with the most
nodes and not exceeding the weight limit w.
The cover set containing only the root, namely, {0}, is a cover set. Besides this
one, there are still many others, which all follow the rule that each of them is built up
by some smaller cover sets of the root’s children trees. It is not difficult to prove that
only these two types of cover sets exist. So an equivalent definition may be stated as
follows.
Definition 7. Set S is defined to be a cover set of a hierarchical model T, if S = {0},
N n−1

orS = U Si, whereSi L S Nn−1 are cover sets of 0’s child trees, respectively. (For any empi =1

ty hierarchical model, its cover set is defined as empty set.)

Time Optimized Algorithm for Web Document Presentation Adaptation

85

This recursive definition reveals the essence of cover sets. It also shows a direct way
to find all the cover sets of a hierarchical model: first find all the cover sets of all
root’s children trees, then make a combination with each cover set of each child tree,
plus the one containing only the root.

4 Algorithms for the Hierarchical Model
4.1 Two Related Algorithms for Content-Optimized Solutions
In Pan et al [11] we discussed two algorithms for the hierarchical model; and the
content-optimized solutions are to reveal as much content as possible (which might be
infeasible in computation) while time-optimized plays the other role. One of the algorithms is based on direct search which has both a local set to catch the current cover
set and a global repository to store all of them. It first tries to add each whole level of
nodes into the current cover set, then saves the cover set into the repository, and attempts to replace some of the nodes with the children next.
Another algorithm is based on dynamic plan. The goal is to find and store the
number of nodes in the best cover set of each node as the root of a small hierarchical
model, which is part of the original one, and combine these cover sets with the numbers stored to form the solution to the original problem.
The first algorithm’s time complexity is O (m N ). The second one’s time complexity is O((wm−−11 )wN ) , with N as the number of nodes in the hierarchical model, for each
node has exactly m children, each of which has a weight at most w .
4.2 An Algorithm for Time-Optimized Solutions
For the hierarchical problem of a model, there is no formal definition for what a timeoptimized solution is; hence it can be achieved in many different ways. Here the
greedy principle is adopted to conduct a local search for a possible solution to the
problem. It restricts the search within two adjacent levels of the model, namely, for
given weight limit w and hierarchical model T, denoting CHi as the i-th composition
of CH, if

∑

i∈CH n−k −1 ({0})

wi ≤w≤∑ i∈CH n−k ({0}) wi

holds, say, the k-th level of T has a sum of weight no less than w and the (k+1)-th
level has a weight sum no more than w, then the search will base on all the nodes on
the k-th level, whose total weight just exceeds the limit, and some of these nodes will
be replaced with their parents on the (k+1)-th level to reduce the total weight to a
value below the limit.
The procedure of our algorithm can be divided into two steps. The first is to find a
proper level as the search base, and the second is to replace some nodes on the base
level with their parents. In the second step another greedy strategy is used, that on the
search base level, the weight which is the sum of children’s weight subtracting their
parents’, and divided by the number of the children minus one, is sorted descendently.
The children with larger quotients above are replaced to enforce a faster decrease of

86

R. Pan and P. Dolog

total weight. The greedy strategy used here is to prevent the selection of replacing
order from drowning in a KNAPSACK-like problem. At last, also it can be seen that
the solutions are restricted in the two adjacent levels.
procedure TIME-OPTIMIZED (h_model, w) returns the time-optimized cover set of
h_model
inputs: h_model, the hierarchical model
w, the weight limit
i ← n; result ← {n0}
while 0 ≤ i
weight ← the total weight of the nodes on leveli
if weight ≤ w then
result ← {all nodes on leveli}
else if i=n then
result ← empty; exit while
else
DESC_SORT(SA = {(∑k∈ch(j)wk – wj) / (|ch(k)| – 1) | j∈CHn-i({0}) })
j←0
for j ← 0 to Ni-1 – 1 step 1
REPLACE(result, ch (j-th node of SA), j-th node of SA)
RENEW(weight)
if weight ≤ w then
exit for and while
end if
end for
end if
i← i – 1
end while
return result
end TIME-OPTIMIZED
The greedy idea restricts the search within the nodes: each level is just scanned once
at most, and even each node is also considered no more than once. So the complexity
of the algorithm is highly improved, that is, O (N log N) (where N is the number of
nodes). This is feasible enough for most applications.
However, the solution gained by this means is generally a possible one. If we take
the number of nodes in the cover set as the only standard to judge the quality of a
solution, then sometimes this solution can have a considerable gap from the best one,
due to the imbalance of the weights on different branches of trees. Below is a somewhat extreme example to illustrate this.
Example 1. Suppose the nodes of a hierarchical model have their weights listed as
below, and the weight limit is 7. The time-optimized solution algorithm for this will
find t result S ' = {n1 , n2 , n8 , n9 } , shown in Figure 1 (with the numbers beside nodes to
represent their weights).
This algorithm’s time complexity perfectly fits the needs of web document presentation in runtime, especially in the mobile multimedia network environment. Meanwhile, the nodes that this algorithm returns are placed on the two adjacent levels of

Time Optimized Algorithm for Web Document Presentation Adaptation

87

2
1

2

1

2

2

2

1

1

2

2 2 2 22 21 12 1 1 1 1 1
Fig. 1. An Example of the Time-optimized Solutions

the model, so they are similar in abstraction scale of the web document, and are thus
more convenient for display, in support of this practical algorithm.

5 Evaluation
For this time-optimized algorithm and the two for content-optimized solutions in Pan
et al [9], we have performed experiments over many different sorts of web documents
and done statistics with their models.
5.1 Motivation
As far as evaluation is concerned, we focus on two aspects, viz. time consumption of
the algorithms and quality of nodes in the cover sets they provide, respectively. Here
“quality of nodes” consists of two criteria, i.e. the sum of nodes’ weight and the similarity of levels the nodes are in. Hence we have three perspectives of observation for
our experiments: time consumption, sum of nodes’ weight, and level variance. We
will compare and explain these three aspects in the following sections.
5.2 Method
The idea of the experiment is as follows. 1. Select a group of HTML pages as a basic
test page. 2. Process the various elements in the page as the nodes in the hierarchical
model. 3. Assign the weight of each node based on the consumed resources, where the
terminal maximum weight limit is based on the issuance capacity. 4. Use the above
algorithm for the cover sets. 5. Finally, the nodes in the model obtained are organized
as a new re-mix, and make comparison among the three algorithms according to the
returned nodes set and the time consumption. Thus the original web page is completed for adaptive information issuance.
5.3 Dataset
As for experiment dataset, we have built respective 1,000 and 10,000 hierarchical
models from two sets of web documents. When we build those models, the web documents’ layout is removed and we only consider the number of characters to calculate
the weight of each node. We want to show the differences of results when choosing

88

R. Pan and P. Dolog

the various numbers of hierarchical models. For both sets of 1,000 and 10,000 test
cases we still have two dimensions as scales of the data: level numbers of models and
weight limits of each node. The level numbers are 3, 4 and 5 and the weight limits are
3, 4 and 6, dividing each dataset as 9 groups. We also want to show the stability of the
algorithm in time consuming and the quality of cover sets by choosing the different
number of levels and weight limits. The observed time consumption is based on the
average time of executing the same experiment ten times over the same dataset (to
obtain more accurate result).
5.4 Experimental Result
Of the entire 10,000 hierarchical models constructed, in the condition of the maximal
weight limit is 6 and for the 5-levels, the two algorithms for content-optimized
solution both return 57,981 nodes, while the one for time-optimized gave 53,277; the
ratio of the two types of solutions is about 1.09:1. It is found that, on a very large
hierarchical model (which is the only one), the content-optimized solution has 10
more nodes than the time-optimized one; in other situations the difference in nodes
of two types of solutions has not exceeded 5, and mostly there are only one in
difference.
With a little gap in nodes quantity, these three algorithms have distinct time performance. For all the 10,000 cases, the direct search algorithm took 399.487 seconds,
the dynamic plan version is done in 0.257 seconds, and it is just 0.018 for the timeoptimized one. The first is over 22,194 times more than the last one. The comparison
on time consumption is shown in Table 1.
Another observation on our experiments is the other evaluation of cover set quality: the level variance of nodes, which is the sum of all cases’ square roots of variance
concerning the level difference of nodes in each cover set. The smaller the value is,
the nearer levels are the nodes in the cover sets on. Since the time-optimized solution
algorithm always finds solutions in adjacent levels, its level variance is just about half
of the former two’s. On this aspect the time-optimized algorithm performs better than
the other two due to its greedy essence. On average, the time-optimized algorithm
finds cover sets with slightly more than half level variance of that of the results from
the other two methods, say 2043.37 compared with 3571.19 (57.22% in proportion).
This trend is more evident for hierarchical models with more levels, as this data is
626.01 vs. 988.74 (63.31%) for 3-level hierarchical models whereas 731.13 vs.
1428.43 (51.18%) for 5-level ones. This might be attributed to the fact that the nodes’
levels tend to vary more in models with more levels (and hence the other two algorithms tend to find worse results albeit they have more total nodes). We conjecture
this trend would be more obvious for more complicated hierarchical models where
time-optimized algorithm would yield more practical results for issuance.
Through the comparison, the direct search algorithm should be eliminated from
practical use. The dynamic plan can find better cover sets within not long time, but
the abstraction degree of nodes is not as good as the one for time-optimized solutions,
whose execution time is also the least. To conclude, in a practical environment, these
two algorithms should be evaluated more precisely according to the application’s real
needs.

Time Optimized Algorithm for Web Document Presentation Adaptation

89

Table 1. Comparison of the three algorithms On Time Consumption
1000 Cases

10000 Cases

Weight

3-

4-

5-

Weight

3-

4-

5-

Limit

Levels

Levels

Levels

Limit

Levels

Levels

Levels

3

0.637

3.473

64.893

3

2.339

9.690

381.250

4

0.781

3.561

67.815

4

2.351

10.310

390.120

6

0.820

3.667

69.375

6

2.397

11.375

399.487

3

0.000

0.001

0.016

3

0.031

0.072

0.156

4

0.001

0.002

0.032

4

0.046

0.087

0.188

6

0.016

0.002

0.038

6

0.049

0.110

0.257

3

0.001

0.001

0.001

3

0.006

0.007

0.009

4

0.001

0.001

0.001

4

0.006

0.008

0.010

6

0.001

0.001

0.002

6

0.006

0.016

0.018

Algorithm

Direct
Search

Dynamic
Plan

TimeOptimized

6 Conclusion and Future Works
In this paper we proposed a time-optimized algorithm based on the HTML hierarchical model for adaptive web document presentation, and compared it with our former
works. The comparison indicates that our previous content-optimized solutions can
find more comprehensive cover sets; however, the time-optimized solution algorithm
finds solutions in more adjacent levels and has great advantage in time complexity,
which is therefore regarded better.
Our future works include designing an adaptive extension of XHTML recommendation finding both other strategies for solutions to the problem, and more potential
facets of the device’s capabilities to help to extend the XHTML. Meanwhile, as the
algorithms rely on a weighting of the nodes in the hierarchical model, we will study
different weighting algorithms for document modeling in the future work.

References
1. Cui, G., Sun, D., et al.: WebUnify: An Ontology-based Web Site Organization and
Publication Platform for Device Adaptation. In: Proceedings of SNPD 2004 International
Conference (July 2004)
2. Dolog, P., Nejdl, W.: Semantic Web Technologies for the Adaptive Web. In: Brusilovsky,
P., Kobsa, A., Nejdl, W. (eds.) The Adaptive Web: Methods and Strategies for Web
Personalization, pp. 697–719. Springer, Heidelberg (2007)

90

R. Pan and P. Dolog

3. Dolog, P., Nejdl, W.: Using UML and XMI for Generating Adaptive Navigation Sequences in Web Based Systems. In: Stevens, P., Whittle, J., Booch, G. (eds.) UML 2003.
LNCS, vol. 2863, pp. 205–219. Springer, Heidelberg (2003)
4. Er-Jongmanee, T.: XML-Driven Device Independent User Interface–build Rich Client
Applications Using XML. Master thesis, Lehrstuhl für Informatik V (2005)
5. Ku, T., Park, D., Moon, K.: Device Independent Authoring Language. In: Fourth Annual
ACIS International Conference on Computer and Information Science, pp. 508–512 (2005)
6. Lewis, D., Conlan, O., et al.: Managing Adaptive Pervasive Computing using Knowledgebased Service Integration and Rule-based Behavior. In: Proceedings of IFIP/IEEE Network Operations and Management Systems, Seoul, Korea (April 2004)
7. Luo, C., Pan, R., Wang, S.: A Hierarchical Model for Auto-adjusting of Information
Issuance. International Journal of Computer Science and Network Security, IJCSNS
(December 2006)
8. Ma, W., Bedner, I., Chang, G., et al.: Framework for adaptive content delivery in heterogeneous network environments. In: Proceedings of Multimedia Computing and Networking, San Jose (January 2000)
9. Pan, R.: The PLCH Binary Tree Model of the Auto-adaptation of Web Information
Issuance. Journal of Computer Applications (May 2005)
10. Pan, R.: A Mathematical Model for the Hierarchization of Web Information and Its
Second-Best Algorithm. Computer Engineering and Science (October 2005)
11. Pan, R., Wei, H., Wang, S., Luo, C.: Auto-adaptation of Web Content: Model and Algorithm. In: ICWMMN 2008, Beijing, China, October 12-15, 2008, pp. 507–511 (2008)
12. Pemberton, S., et al. (eds.): XHTML 1.0 Specification, W3C Recommendation (August
2002), http://www.w3.org/TR/xhtml1/
13. Mika, P.: Ontologies are us: A unified model of social networks and semantics. Web
Semantics: Science, Services and Agents on the World Wide Web 5(1), 5–15 (2007)
14. Phanouriou, C.: UIML: A Device-Independent User Interface Markup Language, PhD
thesis, Virginia Polytechnic Institute and State University (2000)
15. Wang, L., Sajeev, A.S.M.: Abstract Interface Specification Languages for Device Independent Interface Design: Classification, Analysis and Challenges. In: Proceedings of First
International Symposium on Pervasive Computing and Applications, Xinjiang, China,
August, pp. 241–246. IEEE Press, Los Alamitos

Large-Scale Parallel Collaborative Filtering for
the Netﬂix Prize
Yunhong Zhou, Dennis Wilkinson, Robert Schreiber, and Rong Pan
HP Labs, 1501 Page Mill Rd, Palo Alto, CA, 94304
{yunhong.zhou,dennis.wilkinson,rob.schreiber,rong.pan}@hp.com

Abstract. Many recommendation systems suggest items to users by
utilizing the techniques of collaborative ﬁltering (CF) based on historical records of items that the users have viewed, purchased, or rated.
Two major problems that most CF approaches have to contend with
are scalability and sparseness of the user proﬁles. To tackle these issues,
in this paper, we describe a CF algorithm alternating-least-squares with
weighted-λ-regularization (ALS-WR), which is implemented on a parallel
Matlab platform. We show empirically that the performance of ALS-WR
(in terms of root mean squared error (RMSE)) monotonically improves
with both the number of features and the number of ALS iterations. We
applied the ALS-WR algorithm on a large-scale CF problem, the Netﬂix Challenge, with 1000 hidden features and obtained a RMSE score of
0.8985, which is one of the best results based on a pure method. In addition, combining with the parallel version of other known methods, we
achieved a performance improvement of 5.91% over Netﬂix’s own CineMatch recommendation system. Our method is simple and scales well to
very large datasets.

1

Introduction

Recommendation systems try to recommend items (movies, music, webpages,
products, etc) to interested potential customers, based on the information available. A successful recommendation system can signiﬁcantly improve the revenue
of e-commerce companies or facilitate the interaction of users in online communities. Among recommendation systems, content-based approaches analyze the
content (e.g., texts, meta-data, features) of the items to identify related items,
while collaborative ﬁltering uses the aggregated behavior/taste of a large number of users to suggest relevant items to speciﬁc users. Collaborative ﬁltering is
popular and widely deployed in Internet companies like Amazon [16], Netﬂix [2],
Google News [7], and others.
The Netﬂix Prize is a large-scale data mining competition held by Netﬂix
for the best recommendation system algorithm for predicting user ratings on
movies, based on a training set of more than 100 million ratings given by over
480,000 users to 17,700 movies. Each training data point consists of a quadruple
(user, movie, date, rating) where rating is an integer from 1 to 5. The test
dataset consists of 2.8 million data points with the ratings hidden. The goal is
R. Fleischer and J. Xu (Eds.): AAIM 2008, LNCS 5034, pp. 337–348, 2008.
c Springer-Verlag Berlin Heidelberg 2008


338

Y. Zhou et al.

to minimize the RMSE (root mean squared error) when predicting the ratings
on the test dataset. Netﬂix’s own recommendation system (CineMatch) scores
0.9514 on the test dataset, and the grand challenge is to improve it by 10%.
The Netﬂix problem presents a number of practical challenges. (Which is
perhaps why, as yet, the prize has not been won.) First, the size of the dataset
is 100 times larger than previous benchmark datasets, resulting in much longer
model training time and much larger system memory requirements. Second,
only about 1% of the user-movie matrix has been observed, with the majority of
(potential) ratings missing. This is, of course, an essential aspect of collaborative
ﬁletering in general. Third, there is noise in both the training and test dataset,
due to human behavior – we cannot expect people to be completely predictable,
at least where their feelings about ephemera like movies is concerned. Fourth,
the distribution of ratings per user in the training and test datasets are diﬀerent,
as the training dataset spans many years (1995-2005) while the testing dataset
was drawn from recent ratings (year 2006). In particular, users with few ratings
in the training set are well represented in the test set. Intuitively, it is hard to
predict the ratings of a user who is sparsely represented in the training set.
In this paper, we introduce the problem in detail. Then we describe a parallel algorithm, alternating-least-squares with weighted-λ-regularization. We use
parallel Matlab on a Linux cluster as the experimental platform, and our core
algorithm is parallelized and optimized to scale up well with large, sparse data.
When we apply the proposed method to the Netﬂix Prize problem, we achieve
a performance improvement of 5.91% over Netﬂix’s own CineMatch system.
The rest of the paper is organized as follows: in Section 2 we introduce the
problem formulation. In Section 3 we describe our novel parallel AlternativeLeast-Squares algorithm. Section 4 describes experiments that show the eﬀectiveness of our approach. Section 5 discusses related work and Section 6 concludes
with some future directions.

2

Problem Formulation

Let R = {rij }nu ×nm denote the user-movie matrix, where each element rij
represents the rating score of movie j rated by user i with its value either being a
real number or missing, nu designates the number of users, and nm indicates the
number of movies. As in most recommendation systems our task is to estimate
some of the missing values in R based on the known values. (The Netﬂix dataset
consists of nm = 17770 movies, nu = 488000 users, and nr ≈ 100 million known
ratings.)
We start with a low-rank approximation of the ratings matrix R. This approach models both users and movies by giving them coordinates in a low dimensional feature space. Each user and each movie has a feature vector, and
each rating (known or unknown) of a movie by a user is modeled as the inner
product of the corresponding user and movie feature vectors. More speciﬁcally,
let U = [ui ] be the user feature matrix, where ui ∈ Rnf , i = 1 . . . nu , denotes
the ith column of U , and let M = [mj ] be the movie feature matrix, where

Large-Scale Parallel Collaborative Filtering for the Netﬂix Prize

339

mj ∈ Rnf , j = 1 . . . nm , is the j th column of M . Here nf is the dimension of the
feature space, that is, the number of hidden variables in the model. It is a system
parameter that can be determined by a hold-out dataset or cross-validation. If
user ratings were fully predictable and nf suﬃciently large, we could expect that
rij = < ui , mj >, ∀ i, j. In practice, however, we minimize a loss function (of
U and M ) to obtain the matrices U and M . In this paper, we study the meansquare loss function. The loss due to a single rating is deﬁned as the squared
error:
L2 (r, u, m) = (r− < u, m >)2 .
(1)
Then we can deﬁne the empirical, total loss (for a given pair U and M ) as
the summation of loss on all the known ratings in Eq. (2).
Lemp (R, U, M ) =

1  2
L (rij , ui , mj ),
n

(2)

(i,j)∈I

where I is the index set of the known ratings and n is the size of I.
We can formulate the low-rank approximation problem as follows.
(U, M ) = arg min Lemp (R, U, M ).

(3)

(U,M)

where U and M are real, have nf rows, but are otherwise unconstrained.
In this problem, (Eq. (3)), there are (nu + nm ) × nf free parameters to be
determined. Our results show that allowing nf to be quite large, 1000 in our tests,
improves the quality of the results. Thus, for Netﬂix we have over 480, 000, 000
model parameters, more data than there are points in the training set I. For this
reason, problem 3 is underdetermined. Indeed, very few users will have rated nf
movies when nf is greater than a few hundred, yet we must assign each user a
point in an nf dimensional feature space.
Thus, when nf is relatively large, solving the problem Eq. (3) overﬁts the data.
To avoid overﬁtting, a common method appends a Tikhonov regularization [22]
term to the empirical risk function:
emp
(R, U, M ) + λ(U ΓU 2 + M ΓM 2 ),
Lreg
λ (R, U, M ) = L

(4)

for a certain suitably selected Tikhonov matrices ΓU and ΓM .1 We will discuss
the details in the next section.

3

Our Approaches

In this section, we describe an iterative algorithm, alternating-least-squares with
weighted-λ-regularization (ALS-WR), to solve the low rank approximation problem. Then we develop a parallel implementation of ALS-WR based on a parallel
Matlab platform.
1

Throughout the paper, X denotes the Frobenius norm of the matrix X.

340

3.1

Y. Zhou et al.

ALS with Weighted-λ-Regularization

As the rating matrix contains both signals and noise, it is important to remove
noise and use the recovered signal to predict missing ratings. Singular Value
Decomposition (SVD) is a natural approach that approximates the original usermovie rating matrix R by the product of two rank-k matrices R̃ = U T × M . The
solution given by the SVD minimizes the Frobenious norm of R − R̃, which is
equivalent to minimizing the RMSE over all elements of R. However, as there
are many missing elements in the rating matrix R, standard SVD algorithms
cannot ﬁnd U and M .
In this paper, we use alternating-least-squares (ALS) to solve the low-rank
matrix factorization problem as follows:
Step 1. Initialize matrix M by assigning the average rating for that movie as
the ﬁrst row, and small random numbers for the remaining entries.
Step 2. Fix M , Solve for U by minimizing the objective function (4);
Step 3. Fix U , solve for M by minimizing the objective function similarly;
Step 4. Repeat Steps 2 and 3 until a stopping criterion is satisﬁed.
Observe that when the regularization matrices Γ(U,M) are nonsingular, each of
the problems of Steps 2 and 3 of the algorithm has a unique solution, which
we derive below. Note that the sequence of achieved errors Lreg
λ (R, U, M ) is
monotone nonincreasing and bounded below, hence this sequence converges.
Rather than going all the way to convergence, we use a stopping criterion
based on the observed RMSE on the probe dataset. The probe dataset is provided
by Netﬂix, and it has the same distribution as the hidden test dataset. After one
round of updating both U and M , if the change in RMSE on the probe dataset
is less than 1 bps2 , the iteration stops and we use the obtained U, M to make
ﬁnal predictions on the test dataset.
As we mention in Section 2, there are many free parameters. Without regularization, ALS might lead to overﬁtting. A common ﬁx is to use Tikhonov
regularization, which penalizes large parameters. We tried various regularization matrices, and eventually found the following weighted-λ-regularization to
work the best, as it never overﬁts the test data (empirically) when we increase
the number nf of features or the number of ALS iterations.

f (U, M ) =


(i,j)∈I

⎛
(rij − uTi mj )2 + λ ⎝


i

nui ui 2 +



⎞
nmj mj 2 ⎠ , (5)

j

where nui and nmj denote the number of ratings of user i and movie j respec√
tively. This corresponds to Tikhonov regularization (4), where ΓU = diag( nui )
√
and ΓM = diag( nmj ). 3
2
3

1 bps equals 0.0001.
The same objective function was used previously by Salakhutdinov et al. [20] and
solved using gradient descent. We will discuss more on their approach in Section 5.2.

Large-Scale Parallel Collaborative Filtering for the Netﬂix Prize

341

Now we demonstrate how to ﬁnd the matrix U when M is given. Let IiU denote
the set of indices of movies that user i rated, so that nui is the cardinality of
IiU ; similarly IjM denotes the set of indices of users who rated movie j, and nmj
is the cardinality of IjM . A given column of U , say ui , is determined by solving
a regularized linear least squares problem involving the known ratings of user i,
and the feature vectors mj of the movies j ∈ IiU that user i has rated.
1 ∂f
= 0, ∀i, k
2 ∂uki

⇒
(uTi mj − rij )mkj + λnui uki = 0,
j∈IiU

⇒



mkj mTj ui + λnui uki =

j∈IiU



∀i, k

mkj rij ,

∀i, k

j∈IiU



⇒ MIiU MITU + λnui E ui = MIiU RT (i, IiU ),
i

⇒ ui = A−1
i Vi ,

∀i

∀i

where Ai = MIiU MITU + λnui E, Vi = MIiU RT (i, IiU ), and E is the nf × nf
i

identity matrix. MIiU denotes the sub-matrix of M where columns j ∈ IiU are
selected, and R(i, IiU ) is the row vector where columns j ∈ IiU of the i-th row of
R are selected.
Similarly, when M is updated, we can compute individual mj ’s via a regularized linear least squares solution, using the feature vectors of users who rated
movie j, and their ratings of it, as follows:
mj = A−1
j Vj ,

∀j,

where Aj = UIiM UITM + λnmj E and Vj = UIiM R(IiM , j). UIiM denotes the subi

matrix of U where columns i ∈ IiM are selected, and R(IiM , j) is the column
vector where rows i ∈ IiM of the j-th column of R is taken.
Let nr denote the total number of ratings, then we have nr  nu nm for
a sparse rating matrix. We can easily derive the running time of the above
algorithm based on standard matrix operations:


Theorem 1. For ALS-WR, each step of updating U takes O n2f (nr + nf nu )


time while each step of updating M takes O n2f (nr + nf nm ) . If ALS-WR takes


totally nt rounds to stop, it runs in time O n2f (nr + nf nu + nf nm )nt .
3.2

Parallel ALS with Weighted-λ-Regularization

We parallelize ALS by parallelizing the updates of U and of M . We are using
the latest verstion of Matlab, which allows parallel Matlab computation in which
several separate copies of Matlab, each with its own private workspace, and each

342

Y. Zhou et al.

running on its own hardware platform, collaborate and communicate to solve
problems. Each such running copy of Matlab is referred to as a “lab”, with its
own identiﬁer (labindex) and with a static variable (numlabs) telling how many
labs there are. Matrices can be private (each lab has its own copy, and their values
diﬀer), replicated (private, but with the same value on all labs) or distributed
(there is one matrix, but with rows, or columns, partitioned among the labs).
Distributed matrices are a convenient way to store and use very large datasets,
too large to be stored on one processor and its associated local memory. In our
case, we use two distributed copies of the ratings matrix R, one distributed
by rows (i.e., by users) and the other by columns (i.e., by movies). We will
compute distributed, updated matrices U and M . In computing U we will require
a replicated version of M , and vice versa. Thus, our labs communicate to make
replicated versions of U and of M from the distributed versions that are ﬁrst
computed. Matlab’s “gather” function performs the inter-lab communication
needed for this.
To update M , we require a replicated copy of U , local to each lab. We use
the ratings data distributed by columns (movies). The distribution is by blocks
of equal numbers of movies. The lab that stores the ratings of movie j will, naturally, be the one that updates the corresponding column of M , which is movie
j’s feature vector. Each lab computes mj for all movies in the corresponding
movie group, in parallel. These values are then “gathered” so that every node
has all of M , in a replicated array. To update U , users are partitioned into equalsize user groups and each lab just update user vectors in the corresponding user
group, using the ratings data partitioned by rows. The following Matlab snippet
implements the procedure of updating M given U :
function M = updateM(lAcols, U)
lamI = lambda * eye(Nf);
lM = zeros(Nf,Nlm); lM = single(lM);
for m = 1:Nlm
users = find(lAcols(:,m));
Um = U(:, users);
vector = Um * full(lAcols(users, m));
matrix = Um * Um’ + locWtM(m) * lamI;
X = matrix \ vector;
lM(:, m) = X;
end
M = gather(darray(lM));
end
For the above Matlab code, lAcols is the local copy of R distributed by columns
(movies), locWtM is the vector of nmj for all movies in the partitioned movie
group, and Nlm is the number of movies in the movie group. Nf and lambda
correspond to nf and λ, and they are the only tunable parameters of ALS-WR.
The gather operation is the only place where we incur communication cost
due to using a distributed (as opposed to a shared-memory) algorithm. For
our method, it takes less than 5% of the total run time. Therefore, the parallel

Large-Scale Parallel Collaborative Filtering for the Netﬂix Prize

343

algorithm achieves a nearly linear speedup. As an example, for nf = 100, it
takes about 2.5 hours to update M and U once with a single processor; with 30
processors, it takes about 5 minutes for one iteration, and the converged solution
(with 30 ALS iterations) can be computed in 2.5 hours.

4

Performance for the Netﬂix Prize Problem

We ran on a 30-processor Linux cluster of HP ProLiant DL380 G4 machines.
All processors are Xeon 2.8GHz and every four processors share 6GB of RAM.
For each ﬁxed nf , we run between 10 to 25 rounds of ALS-WF and stop when
one round of U, M update improves by less than 1 bps the RMSE score on the
probe dataset. The optimal value of λ is determined by trial and error.4 The
test RMSE is obtained by submission to the Netﬂix prize website5 .
The probe dataset is a subset of the training dataset. It consists of 1,408,395
ratings from the year 2006. In it, users are selected at random with uniform
probability, and at most 9 ratings are included for each selected user. The test
dataset is hidden by Netﬂix but the distribution of the test dataset is the same as
the distribution of the probe dataset. For model building and parameter tuning,
we exclude the probe dataset from the training dataset (this is how we determine
the set I, above.) We use the probe for convergence criterion in ALS-WR and
for testing.
4.1

Postprocessing

For postprocessing of the prediction results, we ﬁrst apply a global bias correction
technique to the prediction solution. Given a prediction P , if the mean of P is
not equal to the mean of the test dataset, we can shift all predicted values by a
ﬁxed constant τ = mean(test) − mean(P ), thus improving the RMSE. We also
use convex combinations of several predictors to obtain a better predictor. For
example, given two predictors P0 and P1 , we can obtain a family of predictors
Px = (1 − x)P0 + xP1 , x ∈ [0, 1], and use linear regression to ﬁnd x∗ minimizing
RMSE(Px ). Therefore we obtain Px∗ which is at least as good as P0 or P1 .
4.2

Experimental Results for ALS

The most important discovery we made is that ALS-WR never overﬁts the data
if we either increase the number of iterations or the number of hidden features.
As Figure 1 shows, with ﬁxed nf and λ, each iteration improves the RMSE
score of the probe dataset, and it converges after about 20 rounds. Diﬀerent λ
values give diﬀerent ﬁnal score, and we normally need to try a small number of
4

5

Empirically we found that for ﬁxed nf , the convergence RMSE score is a convex
function of λ, and the optimal value of λ is monotone decreasing with respect to nf .
Based on these observations, we are able to ﬁnd the best value of λ for each nf with
only 2-3 experiments.
See http://www.netflixprize.com/rules for the detailed rules of the competition.

344

Y. Zhou et al.

λ=0.03
λ=0.04
λ=0.05
λ=0.06
λ=0.075
λ=0.065

0.965
0.96

RMSE

0.955
0.95
0.945
0.94
0.935

5

10

15

20

25

30

35

40

45

50

Number of Iterations

Fig. 1. Comparisons of diﬀerent λ values for ALS-WR with nf = 8. The best performer
with 25 rounds is λ = 0.065. For this ﬁxed λ, after 50 rounds, the RMSE score still
improves but only less than 0.1 bps for each iteration afterwards.

λ 0.03
λ 0.04
λ 0.05

0.96
0.955

RMSE

0.95
0.945
0.94
0.935
0.93
0.925
2

4

6

8

10

12

14

16

18

20

Number of hidden features (n )
f

Fig. 2. Performance of ALS-WR with ﬁxed λ and varying nf

λ values to get a good RMSE score. Figure 2 shows the performance of ALS-WR
with ﬁxed λ value and varying number of hidden features (nf ranges from 2 to
20). For each experiment, ALS-WR iterations continue until the RMSE over the
probe dataset improves less than 1 bps. From the ﬁgure we can tell that the
RMSE monotonically decreases with larger nf , even though the improvement
diminishes gradually.
Next we conduct experiments with real submissions using large values of nf .
For ALS with simple λ regularization (Γu = Γm = E), we obtain a RMSE of
0.9184. For ALS with weighted-λ-regularization, we obtained a RMSE of 0.9114
with nf = 50, 0.9066 with nf = 150. With nf = 300 and global bias correction, we obtain a RMSE of 0.9017; with nf = 400 and global bias correction,
a score of 0.9006 was obtained; with nf = 500 and global bias shift, a score of
0.9000 was obtained. Ultimately, we experimented with nf = 1000 and obtained

Large-Scale Parallel Collaborative Filtering for the Netﬂix Prize

345

a RMSE score of 0.8985.6 Given that 6 bps improvement is obtained from 400 to
500 features, and assuming diminishing (equal-decrease) return with increasing
number of features, moving from 500 to 1000 features improves approximately
5 + 4 + 3 + 2 + 1 = 15 bps. Therefore, 0.8985 is likely the limit we can achieve
using ALS with Weighted-λ-Regularization. A RMSE score of 0.8985 translates
into a 5.56% improvement over Netﬂix’s CineMatch, and it represents one of the
top single-method performance according to our knowledge.
4.3

Other Methods and Linear Blending

We also implement parallel versions of two other popular collaborative ﬁltering techniques as described in this section. In each case, the speedup as compared to a single-processor version is roughly a factor of n on a cluster of n
processors.
The Restricted Boltzmann Machine (RBM) is a kind of neural network where
there are visible states and hidden states, and undirected edges connecting each
visible state to each hidden state. There are no connections among visible states
or among hidden states, thus the name “restricted.” RBM was previously demonstrated to work well for the Netﬂix challenge [20]. We implemented RBM using
Matlab, and converted it to Pmode. For a model with 100 hidden units, it takes
about 1 hour for one iteration without Pmode; using Pmode with 30 labs, it
takes 3 minutes for one iteration.
The k-nearest neighbor (kNN) method is also popular for prediction. With a
properly deﬁned distance metric, for each data point needed for prediction, the
weighted average of the ratings of its k closest neighbors is used to predict the
rating of this point. Since there are so many user-user pairs for us to handle in
reasonable time and space, we use a simpliﬁed approach with only movie-movie
similarities. Again, we parallelize kNN by partitioning users into user groups so
that each lab processes one user group.
For RBM itself, a score of 0.9181 is obtained. For kNN with k = 21 and a
good similarity function, a RMSE of 0.9270 is obtained. Linear blending of ALS
with kNN and RBM yields a RMSE of 0.8952 (ALS + kNN + RBM), and it
represents a 5.91% improvement over Netﬂix’s CineMatch system.

5

Related Work

There is a lot of academic and industrial work on recommendation systems, lowrank matrix approximation, and the Netﬂix prize. In the following we brieﬂy
discuss related work most relevant to ours.
6

The experiment with nf = l000 is technically challenging as U takes 2G memory
with single precision entries for each processor. We managed to run the procedure of
updateM in two batches, while in each batch only two processors for each server are
active and U is only replicated in these processors. This avoids memory thrashing
using the 6G shared memory for each server. And ALS-WR stops in 10 rounds while
each rounds takes 1.5 hours.

346

5.1

Y. Zhou et al.

Recommendation Systems

Recommendation systems can be mainly categorized into content-based and collaborative ﬁltering, and are well-studied in both academia and industry [16,2,7].
Content-based recommendation systems analyze the content (e.g., texts, metadata, features) of the item to identify related items, with exemplary systems
InfoFinder [12], NewsWeeder [14]. Collaborative Filtering uses aggregated behavior/taste of a large number of users to suggest relevant items to speciﬁc
users. Recommendations generated by CF are based solely on the user-user
and/or item-item similarities, with exemplary systems GroupLens [19] and Bellcore Video Recommender [11]. Eﬀorts to combine both content-based approach
and collaborative ﬁltering include the Fab system [3] and uniﬁed probabilistic
framework [18].
5.2

The Netﬂix Prize Approaches

For the Netﬂix prize, Salakhutdinov et al. [20] used Restricted Boltzmann Machines (RBM), obtaining an RMSE score of slightly below 0.91. They also presented a low-rank approximation approach using gradient descent. Their lowrank approximation obtained an RMSE score slightly above 0.91 using between
20-60 hidden features.7 The objective function of their SVD approach is the same
as our ALS-WR, however we use alternating least squares instead of gradient
descent to solve the optimization problem, and we are able to use a much large
number of features (1000 rather than 40) to obtain signiﬁcant improvement in
RMSE score.
Among many other approaches to the Netﬂix problem, Bell et al. [5] proposed
a neighborhood-based technique which combines k-nearest-neighbor (kNN) and
low-rank approximation to obtain signiﬁcantly better results compared to either
technique alone. Their team won the progress prize in October 2007, obtaining
an RMSE score on the qualifying dataset of 0.8712, improving the CineMatch
score by 8.5%. However, their solution [4] is a linear combination of 107 individual solutions, while multiple solutions are derived by variants of three classes of
solutions (ALS, RBM, and kNN). For ALS alone, their best result was obtained
using 128 hidden features with an RMSE score above 0.9000. For a comprehensive treatment of various approaches for the Netﬂix prize, see the individual
papers presented in KDD Cup & Workshop 2007 [21,17,15,13,23].
5.3

Low-Rank Approximation

When a fully speciﬁed matrix is to be approximated by a low-rank matrix factorization, variants of singular value decomposition are used, for example in
information retrieval (where SVD techniques are known as latent semantic indexing [9]). Other matrix factoring methods, for example nonnegative matrix
factorization and maximum margin matrix factorization have also been proposed
for the Netﬂix prize [23].
7

We obtained similar results and got 0.9114 with nf = 50.

Large-Scale Parallel Collaborative Filtering for the Netﬂix Prize

347

For a partially speciﬁed matrix, the SVD is not applicable. To minimize the
sum of squared diﬀerences between the known elements and the corresponding
elements of the factored low rank matrix, ALS has proven to be an eﬀective
approach. It provides non-orthogonal factors, unlike SVD. The SVD can be
computed one column at a time, whereas for the partially speciﬁed case, no such
recursive formulation holds. An advantage of ALS is its easy parallelization. Like
Lanczos for the sparse, fully speciﬁed case, ALS preserves the sparse structure
of the known matrix elements and is therefore storage-eﬃcient.

6

Concluding Remarks

We introduced a simple parallel algorithm for large-scale collaborative ﬁltering
which, in the case of the Netﬂix prize, performed as well as any single method
reported in the literature. Our algorithm is designed to be scalable to very large
datasets. Moderately better scores can be obtained by reﬁning the RBM and
kNN implementation or using more complicated blending schemes. ALS-WR
in particular is able to achieve good results without using date or movie title
information. The fast runtime achieved through parallelization is a competitive
advantage for model building and parameter tuning in general. It will be interesting to develop a theory to explain why ALS-WR never overﬁts the data.
As the world shifts into Internet computing and web applications, large-scale
data intensive computing becomes pervasive. Traditional single-machine, singlethread computing is no longer viable, and there is a paradigm shift in computing
models. Parallel and/or distributed computing becomes an essential component
for any computing environment. Google, the leading Internet company, is building its own proprietary parallel/distributed computing infrastructure, based on
MapReduce [8], Google File System [10], Bigtable [6], etc. Most technology companies do not have the capital and expertise to develop an in-house large-scale
parallel/distributed computing infrastructure, and prefer instead to use readily
available solutions to solve computing infrastructure problems. Hadoop [1] is
an open-source project sponsored by Yahoo!, which tries to replicate the Google
computing infrastructure with open-source development. We have found parallel
Matlab to be ﬂexible and eﬃcient, and very straightforward to program. Thus,
from our experience, it seems to be a strong candidate for widespread, easily
scalable parallel/distributed computing.

References
1. The Hadoop Project, http://lucene.apache.org/hadoop/
2. Netﬂix CineMatch, http://www.netflix.com
3. Balabanovi, M., Shoham, Y.: Fab: content-based, collaborative recommendation.
Communications of the ACM 40(3), 66–72 (1997)
4. Bell, R., Koren, Y., Volinsky, C.: The bellkor solution to the netﬂix prize. Netﬂix
Prize Progress Award (October 2007),
http://www.netflixprize.com/assets/ProgressPrize2007 KorBell.pdf

348

Y. Zhou et al.

5. Bell, R., Koren, Y., Volinsky, C.: Modeling relationships at multiple scales to improve accuracy of large recommender systems. In: Proc. KDD 2007, pp. 95–104
(2007)
6. Chang, F., et al.: Bigtable: A distributed storage system for structured data. In:
Proc. of OSDI 2006, pp. 205–218 (2006)
7. Das, A., Datar, M., Garg, A., Rajaram, S.: Google news personalization: Scalable
online collaborative ﬁltering. In: Proc. of WWW 2007, pp. 271–280 (2007)
8. Dean, J., Ghemawat, S.: Mapreduce: Simpliﬁed data processing on large clusters.
In: Proc. OSDI 2004, San Francisco, pp. 137–150 (2004)
9. Deerwester, S., Dumais, S.T., Furnas, G.W., Landauer, T.K., Harshman, R.: Indexing by latent semantic analysis. J. Amer. Soc. Info. Sci. 41(6), 391–407 (1999)
10. Ghemawat, S., Gobioﬀ, H., Leung, S.-T.: The Google File System. In: Proc. of
SOSP 2003, pp. 29–43 (2003)
11. Hill, W., Stead, L., Rosenstein, M., Furnas, G.: Recommending and evaluating
choices in a virtual community of use. In: Proc. of CHI 1995, Denver (1995)
12. Krulwich, B., Burkey, C.: Learning user information interests through extraction
of semantically signiﬁcant phrases. In: Proc. AAAI Spring Symposium on Machine
Learning in Information Access, Stanford, CA (March 1996)
13. Kurucz, M., Benczur, A.A., Csalogany, K.: Methods for large scale SVD with missing values. In: Proc. KDD Cup and Workshop (2007)
14. Lang, K.: NewsWeeder: Learning to ﬁlter Netnews. In: Proc. ICML 1995, pp. 331–
339 (1995)
15. Lim, Y.J., Teh, Y.W.: Variational bayesian approach to movie rating prediction.
In: Proc. KDD Cup and Workshop (2007)
16. Linden, G., Smith, B., York, J.: Amazon.com recommendations: Item-to-item collaborative ﬁltering. IEEE Internet Computing 7, 76–80 (2003)
17. Paterek, A.: Improving regularized singular value decomposition for collaborative
ﬁltering. In: Proc. KDD Cup and Workshop (2007)
18. Popescul, A., Ungar, L., Pennock, D., Lawrence, S.: Probabilistic models for uniﬁed
collaborative and content-based recommendation in Sparse-Data Environments. In:
Proc. UAI, pp. 437–44 (2001)
19. Resnick, P., Iacovou, N., Suchak, M., Bergstrom, P., Riedl, J.: GroupLens: an open
architecture for collaborative ﬁltering of Netnews. In: Proc. the ACM Conference
on Computer-Supported Cooperative Work, Chapel Hill, NC (1994)
20. Salakhutdinov, R., Mnih, A., Hinton, G.E.: Restricted boltzmann machines for
collaborative ﬁltering. In: Proc. ICML, pp. 791–798 (2007)
21. Takacs, G., Pilaszy, I., Nemeth, B., Tikk, D.: On the gravity recommendation
system. In: Proc. KDD Cup and Workshop (2007)
22. Tikhonov, A.N., Arsenin, V.Y.: Solutions of Ill-posed Problems. John Wiley, New
York (1977)
23. Wu, M.: Collaborative ﬁltering via ensembles of matrix factorizations. In: Proc.
KDD Cup and Workshop (2007)

Public Review for

NetShare and Stochastic NetShare:
Predictable Bandwidth
Allocation for Data Centers
Vinh The Lam, Sivasankar Radhakrishnan, Amin Vahdat,
George Varghese, and Rong Pan
Tenants in datacenters desire performance isolation from each other. Such isolation for the network
has been difficult to achieve without sacrificing utilization. This paper presents a set of techniques
that together could achieve such isolation without requiring hardware changes in switches. The system is evaluated on a testbed of Fulcrum switches.
The techniques employed are as follows. On each switch, on each outbound link, a separate DRR
queue is configured for each class of service. Tenants are clustered into these classes, and the weight
of each class is the sum of the weights of the tenants. These weights are assigned by an operator
when a tenant is provisioned. The traffic for each tenant is labeled so that it lands in the right queue.
To handle UDP, each host needs a rate throttling shim. A centralized bandwidth allocator measures
the rates of flows and then decides on new rates that are enforced using token bucket rate limiters
at hosts or ingress switch ports.
There is a lot to absorb in this paper and the reviewers craved more details. One reviewer was concerned about how the system scales down to a small number of tenants because of a potential for
bandwidth stealing, or how it scales to fast churn in tenants. Another was more concerned about the
speed with which switch configurations could be updated. All the reviewers liked the paper. It is
timely and the topic is important. The implementation on Fulcrum switches impressed them.
A general question worth pondering is what type of isolation the datacenter operator wants to offer,
and what type tenants desire, and are those two in conflict? I suspect that one wants to offer proportional sharing of bandwidth, while the other wants minimum guaranteed bandwidths.
Public review written by

Sharad Agarwal
Microsoft Research

acm

sigcomm

ACM SIGCOMM
SIGCOMM Computer
Computer Communication
Communication Review
Review
ACM

xx
5

•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•

Volume
Volume 42,
42, Number
Number 3,
3, July
July 2012
2012

NetShare and Stochastic NetShare: Predictable Bandwidth
Allocation for Data Centers
Vinh The Lam, Sivasankar
Radhakrishnan, Amin Vahdat, and
George Varghese

Rong Pan
Cisco Systems

ropan@cisco.com

University of California, San Diego
{vtlam,sivasankar,vahdat,varghese}@cs.ucsd.edu

ABSTRACT

momentum, there is a growing demand to provide performance isolation between different services and tenants. While isolation can
be achieved by strict rate limits, this leads to inefficient use of the
expensive data center network because traffic is often bursty.
We propose a new mechanism for data center networks called
NetShare that provides predictable bandwidth allocation, bandwidth
isolation, and high utilization — and can be implemented without any changes to existing switches. NetShare does so using hierarchical weighted max-min fair sharing in which the bisection
bandwidth of the network is first allocated to services according to
weights and the bandwidth of each service is then allocated equally
among its TCP connections. Hierarchical max-min fair sharing
generalizes hierarchical fair sharing of links [7] to networks. We
also generalize stochastic fair queuing [12] to stochastic weighted
max-min fair queuing.
If Internet QoS did not succeed, why hope for data center QoS?
First, Internet QoS issues are often solved by overprovisioning, but
overprovisioning core links in data centers from say 10 Gbps to
40 Gbps is very expensive. Current core links are indeed oversubscribed [4]. Second, users have begun to notice latency degradation
when VM traffic from different services 1 interfere [8]. Third, a reason for the failure of QoS was that there was no simple policy for
setting QoS parameters. NetShare uses a simple set of per-service
weights which can be set automatically based on VM placement,
or set manually by a manager based on the revenue or cost of each
service analogous to VMware’s ESX server shares [8].
NetShare can also be viewed as a way to virtualize (i.e., statistically multiplex) a data center network among multiple services.
Together with virtualized CPUs and disks, it allows managers to
create “virtual data centers” with performance isolation. While one
can argue whether our model of network virtualization is right, NetShare is perhaps the simplest starting point.

Application performance in cloud data centers often depends crucially on network bandwidth, not just the aggregate data transmitted
as in typical SLAs. We describe a mechanism for data center networks called NetShare that requires no hardware changes to routers
but allows bandwidth to be allocated predictably across services
based on weights. The weights are either specified by a manager,
or automatically assigned at each switch port based on a virtual
machine heuristic for isolation. Bandwidth unused by a service is
shared proportionately by other services, providing weighted hierarchical max-min fair sharing. On a testbed of Fulcrum switches,
we demonstrate that NetShare provides bandwidth isolation in various settings, including multipath networks.

Categories and Subject Descriptors
C.2 [Computer-Communication Networks]: Network Architecture and Design

Keywords
data center networks, bandwidth virtualization

1. INTRODUCTION
Cloud services and enterprise networks are hosted by data centers that concurrently support many distinct services — e.g., search
and email for cloud services, or accounting and engineering for an
enterprise data center. The services use a shared data center because physical equipment is expensive, costing over 100 M a year
to maintain [4] and because statistical multiplexing using Virtual
Machines (VMs) is effective. However, the economics also require
two other characteristics of the network, both of which are imperfectly provided today. First, to be profitable, the networks must
have high utilization. Second, many services have stringent performance service-level agreement (SLA) that must be met to keep
customers satisfied; thus the network should also ideally provide
bandwidth guarantee to each service. Any new mechanism to provide these should not require hardware changes to existing switches
so that providers do not have to retrofit their networks.
Service-level agreements today specify network requirement in
terms of dollar cost per Gigabyte transferred and not in terms of network bandwidth. But the performance of application frameworks
such as MapReduce depends greatly on network performance. With
current SLAs, a user may pay for 10 hours for a number of VMs
only to find that the VMs are mostly idle waiting for slow network
transfers. The user job may complete in one hour with a faster network and the user may be willing to pay more for the higher bandwidth. In addition, as cloud computing and shared data centers gain

ACM SIGCOMM Computer Communication Review

2. NETSHARE ALGORITHMS
The generalization of fair sharing to multiple resources such as a
network is called Pareto Optimality (in economics) or max-min fair
sharing (in networking). However, max-min fair sharing at the TCP
level is not the appropriate model for sharing data center services.
First, services that open up multiple connections get an unfair share
of bandwidth. Second, the network administrator cannot allocate
more bandwidth to certain services based on their importance.
In this paper, we propose generalizing the above connectionlevel fairness concept to services. In particular, the NetShare framework presents an abstraction for service-level weighted hierarchical max-min fairness as follows. First, the weights for different
services are specified. The network demand for a particular VM
1

6

We use the terms application and service interchangeably.

Volume 42, Number 3, July 2012

H2

TCP at H2
limits to 1

The argument sketch is as follows. We adopt the standard waterfilling algorithm [3] with some modifications to accommodate the
hierarchical allocation. It starts by finding the weighted bottleneck.
NetShare will emulate this by DRR [16] at the bottleneck link to
give each application its weighted share. Next, we assume that the
TCP flows of each application share the bottleneck link equally.
While this is not strictly true if the TCP flows have very different RTTs, we assume this is true in data centers. Hence, each of
these TCP flows cannot increase any further. Just as in the standard water-filling algorithm, we remove these TCP flows and their
bandwidths, and recurse to find the new bottlenecks.
Concretely, for every switch and outbound link, we configure a
separate fair queuing queue for each service class with the respective weight. In our hardware testbed, we used DRR [16] to configure fair queuing and ToS bits to distinguish services. Note that this
is not the same as reservation. If a service is inactive or is routed
on a different path it will not consume bandwidth on this link.

A2 (weight 1)
H1

10

E1

TCP at H1
limits to 1

10
A3 (weight 1)

H3

C1

10

E2

A1 (weight 4)

TCP at H3
grows to 9

Figure 1: Simple fair queuing at switches at the service level together with TCP achieves hierarchical
max-min fair sharing of services.
is a function of the application and can be set in the SLA policy,
just as application developers specify CPU cores and memory requirement in each VM for the service. Next, there is a mechanism
that allocates network bandwidth in weighted max-min fair fashion
among these services. The bandwidth assigned to each service is
then divided recursively (again in max-min fair fashion) among the
individual flows for that service.
By adopting weighted max-min fair sharing, NetShare allocates
bandwidth based on relative weights, and does not strictly guarantee minimum bandwidth to a service. Minimum bandwidth policies
can be added to the NetShare controller (Section 2.4), but the controller would report no feasible solution after the rate computation
if all policies could not be met with the existing VM assignment
and traffic matrix. Hence we note that NetShare can be augmented
with a VM placement and admission control strategy to guarantee
a minimum bandwidth. The admission control agent should reject
a request for VM instances when the requirement could not be met.
We present three mechanisms to implement NetShare. The first
mechanism (Section 2.1) relies on TCP and fair queuing, only requires configuration changes, and responds to changes in a few
round trip delays. We show how this can scale to a larger number
of applications using the idea of Stochastic NetShare (Section 2.2).
The second mechanism (Section 2.3) augments the first mechanism
to handle UDP. Finally, the third mechanism (Section 2.4) uses centralized allocation to provide more general bandwidth allocations.

2.2

2.1 Group Allocation Leveraging TCP
Our starting point is a classic result by Hahne [9].
Proposition 1 [9]: Flow control with large sliding window at
sources plus fair queuing achieves max-min fair allocation.
Proposition 1 is applicable to every sliding window based flow
control (hence TCP in particular). A corollary of Proposition 1 is
that TCP congestion control together with fair queueing achieves
max-min allocation.
In NetShare, we wish to allocate bandwidth in hierarchical maxmin fashion first at the service level and only then at the TCP connection level. As shown in Figure 1, let’s consider three services
A1, A2, and A3 with weights A1 : A2 : A3 = 4 : 1 : 1 and
suppose A2 has two TCP flows (from H1 and H2) while A1 and
A3 have one TCP flow each. Fair queuing at TCP connection level
does not achieve hierarchical max-min fair sharing: on link C1 to
E2, the TCP connection from A1 is allocated 4/6th of the bandwidth and thus gets only 6.6 Gbps instead of 8 Gbps.
However, if we do fair queuing at the service level, both connections belonging to service A2 are treated identically at core router
C1 (i.e., mapped to the same queue). Assuming the fair mechanism
gives both the TCP connections from H1 and H2 equal bandwidth,
both limit themselves to 1 Gbps, which then allows TCP at H3 to
grow to 9 Gbps. Thus we state the following proposition.
Proposition 2: Window flow control plus fair queuing at the service level achieves hierarchical max-min allocation.

ACM SIGCOMM Computer Communication Review

Stochastic NetShare

Since the switches may support limited DRR queues, NetShare
scales to a large number of application classes by stochastic weighted
max-min fair sharing, which is a generalization of McKenney’s
Stochastic Fair Queuing [12]. Concretely, applications are randomly hashed to specific DRR queues at each switch port. Each
DRR group is assigned weight equal to the sum of weights of the
individual applications that are hashed on to the DRR queue. The
DRR grouping of applications in each switch can be different and
can also be different at each switch port. Also, the grouping of
applications onto DRR queues is changed periodically to avoid intermittent hashing imbalance. Note that most current switches do
not support direct hashing to queues based on packet headers, but
instead only allow mapping from header fields to queues via ACLs.
We simulate hashing by having a central allocator provide labels to
services; these labels can be randomized and changed periodically.
Since queue sharing allows a small weight service to steal more
bandwidth, we propose a mixture of random and weight-based allocation as follows. First, we group services based on weight classes
(say all services of weight 1, all of weight 2, all of weight 4, etc.).
Then we map each weight class into a set of queues and randomly
assign services to a specific queue within each set. In practice,
given the small number of existing queues, we suggest grouping
large weight services and low weight services into two classes, and
randomly assigning within each set of queues. Clearly, this introduces errors due to weight aggregation and these errors can cascade. Nevertheless, it provides a solution to the difficult problem
of combining scalability together with tunability.

2.3 Rate Throttling for UDP
In this section, we augment the TCP-based group allocation in
Section 2.1 to handle aggressive UDP flows and misbehaving applications. Each host is instrumented with a rate throttling shim layer
just below UDP. As illustrated in Figure 2, suppose H1 sends traffic at 10 Gbps to another host H4. The shim layer at H4 measures
received traffic of 1 Gbps from H1. This is sent back to the corresponding rate throttling layer at H1 which rate-limits the traffic at
close to 1 Gbps. Furthermore, to allow legitimate rate growth (e.g.,
H1 could grow to 2 Gbps if H2 disappears), we set the throttled
rate to somewhat more than the measured rate to allow ramp-up.
The throttling mechanism is described in Algorithm 1. The receiver measures received throughput in some period T (e.g., 50
msec in our experiments) and sends a control (e.g., ICMP) message
to the sender with the current measured rate C every T msec. The
sender then executes Algorithm 1 to set the throttled rate R. The

7

Volume 42, Number 3, July 2012

H2

rate throttler
limits to 1

each service is measured at either the switches (using ACLs) or at
the hosts (using a shim layer) in intervals of T seconds and used to
predict a demand for the next interval.
2. Rate Reporting: The predicted rates are sent to a centralized bandwidth allocator (implemented on a server in the network)
that is also supplied with the service weights and the topology via
routing updates.
3. Centralized Calculation: The centralized allocator calculates rates for each flow and each service and sends back rate updates to the switches or hosts.
4. Rate Enforcement: Token bucket rate-limiters are used at
the hosts or ingress switch ports to limit the rates to the calculated
rates. Note that we rate-limit at the service level by the rate computation in Step 3. As in rate-throttling, each flow (especially TCP
flows) must be allocated say 10% higher than its optimal centralized allocation to allow it to grow.
We have designed and implemented such a centralized allocator.
The predictor in Step 1 is a standard least squares predictor using
the last five measurements of traffic demand. The algorithm in Step
3 is a variant of the standard water-filling algorithm [3] which starts
by finding the weighted bottleneck. We implemented the centralized algorithm on several large simulated 2-tier data center topologies. To approximate the solution to a large number of flows, we
aggregated flows of the same weight and edge-to-edge path during
the computation. On a simulated topology with 16 cores and 128
edge switches, the algorithm took less than 100 msec on a standard
Intel Core2Duo 3GHz desktop.
Since the centralized bandwidth allocator includes a feedback
loop, we also developed a formal feedback control framework for
a rigorous proof of stability and convergence [6].

A2 (weight 1)
10

H1
rate throttler
limits to 1

E1

10

C1

A3 (weight 1)

10
A1 (weight 4)

H4

E2

rate measurement
measures 1 Gbps

TCP at H3
H3 grows to 8
feedback received rate via control message

Figure 2: Simple fair queuing at switches at the service
level together with rate measurement and rate throttling implements hierarchical max-min fair even with
UDP.
Algorithm 1 Compute NetShare Rates at Rate Throttler
Performance tuning knobs:
d: threshold of rate difference
rI : factor for increasing flows
rD : factor for decreasing flows
rO : factor for overshooting flows

Measurement parameters:
L: last measured rate
C: current measured rate at receiver
f : flag indicating that the flow increased on last iteration.
R: current rate limit
if (|(L − C)/L| ≥ d) then {is rate change substantial?}
if C − L > 0 then {increasing, allow overshoot by rI }
R ← C · (1 + rI )
f ← true
end if
if C − L < 0 then {decreasing, allow overshoot by rD }
R ← C · (1 + rD )
end if
else
if f = true then {limit overshoot by rO }
R ← C · (1 + rO )
f ← f alse
end if
end if
if R < T h then {do not lower below threshold}
R ← Th
end if
L←C

2.5

values rI and rD are performance tuning knobs, indicating how
much network bandwidth can be wasted. If rI or rD are large, then
any newly freed up bandwidth can be acquired fast by the UDP
traffic class. Note that very large difference between rI and rD
would also cause instability and would make it harder for the rate
to stabilize. In our experiments, we achieved good performance
with rI = 20% and rD = 10%. The flag f detects if the sender
did increase in the last iteration so that the rate is set to rO (typically rO < rI , say rO = 10%) higher than the measured rate C.
This limits the amount of overshoot committed previously by the
higher rI . Also, we do not let the rate to fall below a threshold to
avoid long ramp-up of small flows.

2.4

Centralized Bandwidth Allocator

In this section, we describe a centralized bandwidth allocator
to allow advanced bandwidth allocation policies beyond max-min
fairness. For instance, a more general policy can allow some connections between important servers to be allocated higher bandwidth. Another example is a policy on reallocation of excess bandwidth [6]. There are four steps.
1. Rate Measurement: The rate of each flow (TCP or UDP) for

ACM SIGCOMM Computer Communication Review

Discussion

NetShare can be integrated to a centralized management framework (such as Openflow / SDN controller) to update switch configurations (e.g., assigning weights during VM allocation and migration). Furthermore, the switch configuration process can be amortized into the launch and migration of VMs. Newly launched services are detected when VMs are allocated to them. The VM admission controller would also update the network controller, which
then reconfigures switch weights accordingly. This does not limit
NetShare scalability since the time to configure some switches in
the network (in parallel) is less than the time to allocate and bring
up a new VM. To identify an application or service class, we can
use a combination of Type-of-Service (ToS) bits, IP options, etc.,
which are tagged on outgoing packets by the hypervisor and ACL
rules in the network.
Modern switches typically support rate-limiting at finer granularity than port rate-limiting (QoS class, VLAN, ACL matching etc).
Otherwise, rate-limiting can be offloaded to the VMM layer on the
host and hence scales well to a large number of flows. Note that the
number services hosted by servers locating at an edge switch can
be much less than the total number of services in the whole data
center. Therefore, with Stochastic NetShare we expect to scale to
enterprise-level data centers with tens to hundreds of services. For
a much larger scale with thousands of services, an approach such
as Approximate Fair Dropping (AFD) [13] can reduce the approximation error of stochastic queue sharing in Stochastic NetShare.
Future routers are expected to be equipped with a few thousand
AFD queues for 16 DRR queues [13]. Note that AFD scales better
because it uses a counter for each class as opposed to a queue.
Table 1 shows the tradeoffs between the three NetShare algorithms: group allocation, rate throttling, and centralized allocation.
The group allocation mechanism has high responsiveness but relies

8

Volume 42, Number 3, July 2012

Group Allocation
Rate Throttling
Centralized Allocation

Table 1: Comparison of different NetShare mechanisms
Deployment
Responsiveness
Generality
Configuration at routers
< 1 msec
Only TCP flows
Only Hierarchical max-min
Configuration at routers
10-50 msec
Only Hierarchical max-min
Added endnode or router software
Centralized allocation software
10 - 100 msec
More general allocation policies
Added router software

on TCP, and so it can be augmented with the UDP rate throttling to
handle non-TCP traffic. The centralized allocator avoids the need
for the UDP rate throttling in general, at the cost of higher complexity and lower responsiveness due to the control loop. Note that
increasing generality must be paid for by smaller responsiveness
and more software deployment.
(a) Single path (1 core)

3.

AUTOMATIC WEIGHT ASSIGNMENT

Figure 3: Testbed topologies

We propose a technique to perform service isolation by assigning weights to different services at each port in the network automatically. This helps avoid the need for network administrators to
manually determine what weights different services must use. In
an enterprise or cloud data center, when resources are provisioned
for an application or customer, the customer usually requests some
number of servers or VMs (instances) each with some number of
CPUs, RAM and disk. Besides this, each instance must also be
allocated some units of network bandwidth. For example, if each
server has a 10Gbps NIC, we could place up to 10 VMs on the
server with each allocated 1Gbps of bandwidth.
We leverage two fundamental ideas. First, we use per switchport weights i.e., weights per application can vary from switch to
switch, and even from one switch port to another. Second, we assign weights based on VM placement. We compute both the downstream and upstream sums of the bandwidths assigned to all VMs
allocated to application A with respect to switch port P . Then the
weight assigned to A at P is the smaller of the two.
As an example, suppose there is an accounting application with 2
servers connected to an edge switch and each server has 4 instances
of an accounting application. The uplink of the edge switch is connected to a core switch and from there to other servers with 8 instances of the accounting application. Assume each VM instance is
allocated 1 Gbps. Then we set the accounting application’s weight
at each of the 2 downlink ports of the edge switch to be 4 (smaller of
4 and 12), while we set its weight at the uplink port to be 8 (smaller
of 8 and 8). Note that taking the minimum makes sense because
even if there are 8 VMs upstream that can transmit at 8 Gbps, there
are only 4 VMs downstream that can receive only at an aggregate
capacity of 4 Gbps.
We make the following assumptions. First, VM bandwidths at
the servers are enforced using mechanisms like Linux HTB qdisc.
Second, we have knowledge of the complete topology and placement of each VM instance. Third, in a multirooted tree network,
forwarding is based on ECMP. We assume that each egress port in
a switch either forwards traffic upwards from a server towards the
core layer (up-facing ports) and the rest of the fabric or forwards
traffic down towards a server (down-facing ports). This is a technique that simplifies the routing while also avoiding routing loops.
Finally, in this setup, each egress port on the switch has a definite
role in terms of which server’s traffic flows through it. For example in a two level multirooted tree network, a down facing port
on a core switch can forward traffic to servers in a particular edge
switch from all other edge switches while an up-facing port on an
edge switch can forward traffic from the servers on that edge switch

ACM SIGCOMM Computer Communication Review

(b) Multipath (2 cores)

to all servers in other edge switches. Servers to which the particular
port forwards traffic are called downstream servers and the servers
from which this traffic could be coming are called upstream servers
of that port.
While we have used global weights for the bulk of this paper, we
note that extending the definitions to per-port weights is straightforward. For example, the standard water-filling algorithm [3] can be
modified to use the weight of each application/service at the current
bottleneck link as opposed to a global weight.

4. EVALUATION
We implemented NetShare on a small scale data center testbed
consisting of a 24-port Fulcrum Monaco 10GigE switch [1] – a
commercial switch with an extensive programming API for customizations. Out of the 24 switch ports, 12 were directly connected to the servers. Each server had two quad core Intel Xeon
E5520 2.26GHz processors, 24 GB of RAM, and 16 local hard
disks with 8 TB of total capacity. The remaining 12 ports were used
to setup loopbacks (through a Glimmerglass optical MEMS switch)
for partitioning the original 24-port physical switch into several virtual switches using VLANs and creating multi-switch data center
topologies. Figure 3 shows our topologies. Multipathing was based
on Equal-Cost Multipath (ECMP).
We evaluate the performance by investigating application completion times as the overall performance metric for the applications.
Since the network is not the only factor, we also plot bandwidth utilization to demonstrate the ground truth. In this section, we show
several key results to demonstrate the effectiveness of NetShare in
sharing real data center applications, providing both bandwidth isolation and statistical multiplexing. Comprehensive experimental results are presented in [6].

4.1

Multipath Experiments

In this section, we demonstrate NetShare effectiveness by showing that it truly divides the bisection bandwidth (both core links) on
demand with the topology in Figure 3b. Note that the core switches
were the bottlenecks with an oversubscription factor of 2:1 for traffic between different edge switches. We used two Hadoop Sort
applications A1 with 96 maps and 96 reducers, and A2 with 96
maps and 48 reducers.
Concretely, we first generated 96GB of data for each instance
using the Hadoop RandomWriter application (8 maps per slave ×
12 slaves). We subsequently ran two Hadoop Sort jobs in the two
Hadoop instances A1 and A2. A1 used a total of 96 maps (8 per

9

Volume 42, Number 3, July 2012

1000

App 1
App 2
App 3

Throughput (Mbps)

800

600

Figure 8: Topologies for Stochastic DRR experiments

400

cation can be suboptimal. In Figure 4, each application received its
weighted share of the network resources. For instance, during the
period 5-35s, A2 got 750 Mbps and A1 got 250 Mbps as they were
sharing the bottleneck E1, C in the ratio 3:1 of their weights. However, from t=95-125s A1’s TCP flow got close to 725Mbps, which
exceeded the share allocated by its application weight, but since
A2’s UDP flow had a downstream bottleneck on the link C, E3
only 250 Mbps of the UDP flow was “useful” (that is the throughput of the UDP flow that actually reached the receiver H9). So in
this case, A2 was rate-limited at the ingress switch to 275 Mbps
(250 * 1.1) which results in A1 getting close to 725 Mbps. Without rate throttling A1 would have sent at much higher rates and got
dropped at C.
2. No NetShare: We demonstrate that without a bandwidth isolation mechanism, a bandwidth-aggressive application can acquire
much more than fair share. In Figure 5, when A1 and A2 were both
active from t=5-35s, A1’s TCP flow was overwhelmed by A2’s
UDP flow and received zero throughput. Note that from t=65-95s,
A1’s throughput did not reach 1 Gbps although its path from H1
to H5 was not affected by A3’s UDP flow. However, the ACKs
from H5 to H1 shared a link with A3’s UDP flow; some of the
ACKs were dropped, this resulted in A1’s throughput dropping to
sometimes as low as 750 Mbps.
3. Group Allocation Only: Figure 6 shows the impact of omitting Rate Throttling. In the period t=95-125s, A2 and A3 shared
the bandwidth of their shared bottleneck link in the ratio of their
application weights (3:9). Thus A2 only received 250Mbps. Unfortunately, A1 also received only 250Mbps because A2 continued
to send greedily at 750Mbps on the E1, C link of which 500Mbps
got dropped at C.
4. Rate Throttling Only: In Figure 7, the behavior was similar to Case 1 from t=5-95s. However from t=95-125s, A1 only
achieved 450-500Mbps. This is because A2 was rate-limited at E1
to a little over 500Mbps, so A1 was able to use the remaining bandwidth on the E1, C link. Thus rate throttling and fair queuing are
orthogonal and complementary mechanisms.

200

0

0

30

60
90
time (sec)

120

150

Figure 4: NetShare with Group Allocation (DRR) +
Rate Throttling
Time(s) A1 A2 A3
Bottlenecks
5-35


X
E1, C
35-65
X


C, E3
65-95

X

E2, C
95-125


 All of the above
Table 2: Traffic pattern that indicates times during
which different flows were active.
slave) and 96 reducers (8 per slave) while A2 used 96 maps (8 per
slave) and 48 reducers (4 per slave). The Hadoop Distributed File
System (HDFS) was configured with a default replication factor of
3 and the HDFS block size was set to 256 MB.
First we ran the sort jobs without NetShare in the network. In this
case, A1 used twice the bandwidth (summed over all core links, the
“bisection bandwidth”) when compared to A2 because it opens up
nearly twice the connections. Next, we set up NetShare by configuring DRR with equal weights for the 2 applications. Note that the
bandwidths on the various core links are not shared as uniformly
because of hashing effects and because the sort job does not saturate all links consistently.
Using NetShare, A1 completed sorting in 1633s while A2 completed sorting the data in 1810s. As a comparison, we also ran
A1 and A2 in the same fashion, but using the single core topology
shown in Figure 3a. In this case, we found that A1 and A2 finished
sorting in 3070s and 3212s respectively. After factoring out the
500s for the map phase (that is unaffected by the extra bandwidth),
the bisection bandwidth appears to be nearly equally shared between the two “services” and both were sped up by nearly a factor
of 2 in the multipath topology. Some difference is not surprising
because A1 has more connections, and thus its use of ECMP load
balancing is likely more effective than A2.

4.3

4.2 How Effective is Rate Throttling?
We deployed three applications in the testbed in Figure 3a: A1
generated a TCP flow from host H1 to host H5; A2 generated a
UDP flow from host H2 to host H9; and A3 generated a UDP flow
from host H6 to host H10. The weights of the applications A1,
A2, A3 were set to 1:3:9 respectively.
Table 2 shows the traffic pattern. During the time 5-35s, A3 was
inactive and thus the TCP flow A1 (weight 1) contended with the
UDP flow A2 (weight 3) for the core link E1, C. From time 3565s, the two UDP applications A2 and A3 (with weights 3 and 9)
contended for the core link C, E3. From time 65-95s, the TCP
application A1 contended with the high weight UDP application
but only on the link from edge router E2 to core router C. Thus the
UDP application could only interfere with TCP acknowledgements
for A1 destined to Host H1.
We evaluate the following scenarios.
1. Group Allocation and Rate Throttling: We show that if
we just set static weights locally, then with UDP, bandwidth allo-

ACM SIGCOMM Computer Communication Review

Scalability of Stochastic NetShare

A concern with Group Allocation is that it requires a number of
queues equal to the number of applications. To scale beyond 16
queues commonly available today and the 1000’s available shortly
with AFD-based routers [13], we proposed Stochastic NetShare
in Section 2.2. Due to restrictions on the physical queues in the
switch, we had a simulation setup as in Figure 8. The topology had
one core switch C0, four edge switches E1 to E4, and eight servers
S1 to S8 (two servers per edge switch). Note that all links had
equal capacity with an oversubscription factor of 2 at the core. We
had 32 applications and one instance of each application on each
server creating an all-to-all traffic pattern. One application was
“bad”, i.e. with low priority weight and competing aggressively for
bandwidth by opening ten times the number of connections. The
link capacity was B = 100 Mbps. We evaluated the scalability of
Stochastic DRR by varying the number of DRR queues per switch
port Q = 4, 8, 16.
Table 3 shows the application bandwidth at one typical server.
All DRR queues were assigned the same weights independent of
the number of applications being hashed into them. Due to the

10

Volume 42, Number 3, July 2012

600

400

200

600

400

200

0

30

60
90
time (sec)

120

150

Q=4
B̄ = 2.3
Q=8
B̄ = 2.7
Q=16
B̄ = 2.9

Bad app
Good app
Bad app
Good app
Bad app
Good app

T=5
(13.6, 3.3)
(2.2, 1.5)
(8.9, 2.1)
(2.8, 2.2)
(6.7, 1.5)
(2.8, 1.9)

T=10
(14.3, 2.6)
(2.2, 1.3)
(8.9, 1.9)
(2.3, 1.7)
(6.6, 1.6)
(3.1, 2.2)

30

60
90
time (sec)

120

Figure 6: NetShare with
Group Allocation Alone
T=20
(12.4, 2.8)
(2.4, 1.4)
(9.0, 2.0)
(2.5, 1.8)
(6.7, 1.7)
(3.3, 2.5)

periodic rehashing process of Stochastic NetShare, the application
rates oscillated, so our evaluation relied on mean and variance of
the rate measurements. As shown in Table 3, the isolation performance of NetShare degraded gracefully with decreasing number of
queues. Also, the impact of the bad application declined with additional queues in the system. The mean was close to our prediction
(the ideal bandwidth is B/N which was around 3.1 Mbps, together
with a degradation of Q−1
, where Q is the number of queues).
Q
Note that periodic rehashing of applications onto DRR queues
reduced variance. Clearly the rehashing period T should neither be
too small (for good stability and minimizing out-of-order packets)
nor too large (for good bias correction).

7.

400

150

0

30

60
90
time (sec)

120

150

Figure 7: NetShare with Rate
Throttling Alone

REFERENCES

[1] Fulcrum Monaco http://www.fulcrummicro.com/.
[2] Hitesh Ballani, Paolo Costa, Thomas Karagiannis, and Ant Rowstron.
Towards predictable data center networks. In Proc. SIGCOMM’11.
[3] D. Bertsekas and R. Gallager. Data Networks. P. H., 1992.
[4] A. Greenberg et al. VL2: A Scalable and Flexible Data Center
Network. In SIGCOMM’09.
[5] C. Guo et al. SecondNet: A Data Center Network Virtualization
Architecture with Bandwidth Guarantees. In Proc. ACM CoNEXT
’10.
[6] Lam et al. NetShare and Stochastic NetShare: Predictable Bandwidth
Allocation for Data Centers. In UCSD Tech Report 2011. http://
cse.ucsd.edu/users/vtlam/netshare-TR-2011.pdf.
[7] S. Floyd et. al. Link-sharing and resource management models for
packet networks. IEEE/ACM Trans. Netw.’95.
[8] Ajay Gulati, Arif Merchant, and Peter Varman. mClock: Handling
Throughput Variability for Hypervisor IO Scheduling. In Proc. OSDI
’10.
[9] Ellen L. Hahne. Round-robin scheduling for max-min fairness in data
networks. IEEE J. Comms, 1991.
[10] W. Kim and et al. Automated and scalable qos control for network
convergence. In USENIX INM/WREN, 2010.
[11] T. Lam, S. Radhakrishnan, A. Vahdat, and G. Varghese. NetShare:
Virtualizing Data Center Networks across Services. In UCSD Tech
Report 05/2010. http://csetechrep.ucsd.edu/Dienst/
UI/2.0/Describe/ncstrl.ucsd_cse/CS2010-0957.
[12] P. McKenney. Stochastic fairness queueing. In Internetworking,1991.
[13] Rong Pan, Balaji Prabhakar, Flavio Bonomi, and Bob Olsen.
Approximate Fair Bandwidth Allocation: A Method for Simple and
Flexible Traffic Management. In 46th Allerton Conf., 2008.
[14] Rob Sherwood, Glen Gibb, Kok-Kiong Yap, Guido Appenzeller,
Martin Casado, Nick McKeown, and Guru Parulkar. Can the
Production Network Be the Testbed? In Proc. OSDI’10.
[15] Alan Shieh, Srikanth Kandula, Albert Greenberg, and Changhoon
Kim. Seawall: Performance Isolation in Cloud Datacenter Networks.
In Proc. HotCloud’10.
[16] M. Shreedhar and G. Varghese. Efficient fair queueing using deficit
round robin. In In the Proceedings of SIGCOMM’95.

RELATED WORK

The need for QoS in data centers has become apparent in several
recent papers [11]. Seawall [15] performs isolation by enforcing
VM-to-VM rates for VMs belonging to one application/customer
in the hypervisor using congestion feedback. SecondNet [5] proposes a heuristic to map virtual data center specifications into the
physical data center infrastructure with constraints on resource demands. SecondNet uses reservations and hence is complementary
to NetShare. Flowvisor [14] virtualizes a testbed network to allow
multiple experiments to run concurrently but does so using suboptimal hop-by-hop allocation. The HP QoS Framework [10] allows
network QoS to be implemented centrally but is only a framework
that can, in fact, be used to implement NetShare. Oktopus [2] discusses a VM placement policy based on the network requirements
for each customer. Bandwidth is reserved for each customer’s VMs
and the fair share for each flow is computed by a centralized controller for that customer. Multiplexing across customers requires
coordination among controllers of different customers or a single
central controller similar to NetShare.

6. CONCLUSIONS
NetShare allows managers to use weights to tune the relative
bandwidth allocation for different services, providing isolation and
statistical multiplexing without changing routers. Managers can

ACM SIGCOMM Computer Communication Review

600

use NetShare with Virtual Disks and Virtual Machines to create
Virtual Data Centers. While NetShare is based on a simple packaging of existing ideas (max-min fair share, stochastic fair queuing,
UDP rate throttling), no such mechanism is used today.
Group allocation works well with only configuration changes at
routers; it can be extended to scale to more applications than the
number of DRR queues available today either using AFD [13] or
stochastic methods. Rate throttling handles UDP applications and
may be simpler than deploying TCP-friendly UDP by modifying
applications. Finally, centralized allocation can implement arbitrary bandwidth allocation policies, and can provide stability. We
suggest a simple automatic weight assignment algorithm based on
finding the number of VMs upstream and downstream from a port.

Table 3: Scalability of Stochastic DRR: application
bandwidth at one typical server in (mean, stddev) over
B
time. All queues had equal weights. B̄ = N
· Q−1
Q
is the expected bandwidth per application. Ideal bandB
width is N
= 3.1 Mbps. T is rehashing period (in
seconds).

5.

800

0

0

Figure 5: No NetShare mechanisms

App 1
App 2
App 3

200

0

0

1000

App 1
App 2
App 3

800

Throughput (Mbps)

800

Throughput (Mbps)

1000

App 1
App 2
App 3

Throughput (Mbps)

1000

11

Volume 42, Number 3, July 2012

IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 61, NO. 2, FEBRUARY 2015

1139

(m, n, 3, 1) Optical Orthogonal Signature Pattern
Codes With Maximum Possible Size
Rong Pan and Yanxun Chang

Abstract— Kitayama proposed a novel code-division multipleaccess (CDMA) network for image transmission called spatial
CDMA. Optical orthogonal signature pattern codes (OOSPCs)
have attracted wide attention as signature patterns of spatial CDMA. An (m, n, k, λ)-OOSPC is a set of m × n (0, 1)matrices with Hamming weight k and maximum correlation
value λ. Let (m, n, k, λ) be the largest possible number
of codewords among all (m, n, k, λ)-OOSPCs. In this paper,
we concentrate on the calculation of the exact value of
(m, n, 3, 1) and the construction of an (m, n, 3, 1)-OOSPC
with (m, n, 3, 1) codewords. As a consequence, we show that
(m, n, 3, 1) =  mn−1
6  − 1 when mn ≡ 14, 20 (mod 24), or
mn ≡ 8, 16 (mod 24) and gcd(m, n, 4) = 2, or mn ≡ 2 (mod 6)
and gcd(m, n, 4) = 4, and (m, n, 3, 1) =  mn−1
6 
otherwise.
Index Terms— Optical orthogonal signature pattern code,
relative difference family, strictly G-invariant packing.

I. I NTRODUCTION

A

N OPTICAL orthogonal signature pattern code
(OOSPC) is a family of (0, 1)-matrices with good autoand cross-correlation. Its study has been motivated by an
application in a novel code-division multiple-access (CDMA)
network for image transmission called spatial CDMA. The
spatial CDMA network has promoted the development
of high-speed multiple access network applications,
especially image applications such as supercomputer
visualizations, medical image access, and distribution
and digital video broadcasting. Comparing with the
traditional CDMA, the spatial CDMA provides higher
throughput. For more details, interested readers may refer
to [18], [19], and [28].
Let m, n, k and λ be positive integers. An (m, n, k, λ)
optical orthogonal signature pattern code (briefly,
(m, n, k, λ)-OOSPC) is a family C of m × n (0, 1)-matrices
(called codewords) of Hamming weight k satisfying the
following two correlation properties:
Manuscript received December 22, 2013; revised December 1, 2014;
accepted December 2, 2014. Date of publication December 18, 2014; date
of current version January 16, 2015. Y. Chang was supported by the NSFC
under Grant 11431003.
R. Pan is with the Department of Mathematics, Beijing Jiaotong University,
Beijing 100044, China (e-mail: 10118358@bjtu.edu.cn).
Y. Chang is with the Department of Mathematics, Beijing Jiaotong University, Beijing 100044, China, and also with the Beijing Center for Mathematics
and Information Interdisciplinary Sciences, Beijing 100048, China (e-mail:
yxchang@bjtu.edu.cn).
Communicated by K. Yang, Associate Editor for Sequences.
Digital Object Identifier 10.1109/TIT.2014.2381259

(1) The auto-correlation property: For any (ai j ) ∈ C and
every (s, t) ∈ Z m × Z n \ {(0, 0)},
n−1
m−1


ai, j ai⊕s, j ⊕
t ≤ λ;

i=0 j =0

(2) The cross-correlation property: For any distinct
(ai j ), (bi j ) ∈ C and every (s, t) ∈ Z m × Z n ,
n−1
m−1


ai, j bi⊕s, j ⊕
t ≤ λ,

i=0 j =0

where Z l is the additive group of the residual-class ring of
 are, respectively,
integers module l. The additions ⊕ and ⊕
reduced modulo m and n.
For any (0, 1)-matrix A = (ai j ) ∈ C, whose rows are
indexed by Z m and columns are indexed by Z n , we define
X A = {(i, j ) ∈ Z m × Z n : ai j = 1}. Then, F = {X A : A ∈ C}
is a set-theoretic representation of the (m, n, k, λ)-OOSPC.
Conversely, for any subset X ∈ Z m × Z n , we construct a
(0, 1)-matrix A = (ai j ) such that ai j = 1 if and only if
(i, j ) ∈ X, where the weight of A equals |X|. Let F be a set of
k-subsets of Z m × Z n . We say that F is an (m, n, k, λ)-OOSPC
if the following two correlation properties are satisfied:
(1’) The auto-correlation property: For any X ∈ F and every
(s, t) ∈ Z m × Z n \ {(0, 0)},
|X ∩ (X + (s, t))| ≤ λ;
(2’) The cross-correlation property: For any distinct
X, Y ∈ F and every (s, t) ∈ Z m × Z n ,
|X ∩ (Y + (s, t))| ≤ λ,
where the addition “+” performs in Z m × Z n . Throughout this
paper, we shall always use the set-theoretic notation to list the
codewords of a given OOSPC.
The number of codewords in an OOSPC is called the size of
the OOSPC. For given integers m, n, k and λ, let (m, n, k, λ)
be the largest possible size among all (m, n, k, λ)-OOSPCs.
An (m, n, k, λ)-OOSPC with size (m, n, k, λ) is said to be
maximum. Research on OOSPCs mainly focuses on the calculation of the exact value of (m, n, k, λ) and the construction
of an (m, n, k, λ)-OOSPC with maximum size (m, n, k, λ).
The most general upper bound on (m, n, k, λ), which is
presented on the basis of the Johnson bound [17] for constant
weight codes, is
(m, n, k, λ) ≤ J (mn, k, λ)

0018-9448 © 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

(1)

1140

where

IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 61, NO. 2, FEBRUARY 2015

By extensive studies on these combinatorial structures, a wide
class of (m, n, k, λ)-OOSPCs can be derived and we will quote
J (mn, k, λ) =
···
···
partially
known results when we need.
k k−1
k −2
k −λ
Now,
we
give the definition of packings.
and x denotes the largest integer not exceeding x.
Let
v,
k
and
t be positive integers such that t < k < v.
When m and n are coprime, it has been shown in [28] that
A
t-(v,
k,
1)
packi
ng is a pair D = (V, B) where V is a
an (m, n, k, λ)-OOSPC is actually a 1-dimensional (mn, k, λ)
finite
set
of
v
points
and B is a collection of k-subsets (called
optical orthogonal code (briefly, 1-D (mn, k, λ)-OOC).
blocks)
of
V
such
that
every t-subset of V appears in at most
For the formal definition of 1-D OOCs, the reader can refer
one
block.
to [10]. Similarly, a 1-D OOC is said to be maximum if there
An automorphism of D is a permutation σ on V such that
is no 1-D OOC of larger size with the same parameters. So far,
σ
keeps B invariant, i.e., B σ = {B σ = {x σ : x ∈ B} :
many efforts have been done on constructing 1-D OOCs
with maximum size and many results have been made, B ∈ B} = B. The collection of all automorphisms of D
for instance, [1], [4]–[10], [12]–[16], [20], [21], [30], [31]. forms a group under composition, called the full automorphism
On the other hand, when m and n are not coprime, very group, and any of its subgroups is called an automorphism
little has been done about an (m, n, k, λ)-OOSPC with group of D. Throughout this paper when we say that D is
maximum size. To our knowledge, the construction of an G-invariant, we mean that D admits G as an automorphism
(m, n, k, λ)-OOSPC with maximum size J (mn, k, λ) has been group acting sharply transitively on the points. Suppose that
D is G-invariant. For any block B ∈ B, the G-orbit of B
settled for the following cases:
is the set OrbG (B) of all distinct images of B under G,
(1) [28] λ = 1 and m, n are primes such that m = n ≡ 1
that is, OrbG (B) = {B σ : σ ∈ G}. Evidently B can be
(mod k(k − 1));
partitioned into some G-orbits. A set of base blocks of D
(2) [26] λ = 1, k = 3 and m, n ≡ 1 (mod 2), except for m,
is a complete set of representatives for the G-orbits of B.
n ≡ 5(mod 6);
Moreover, when we say that D is strictly G-invariant, it means
(3) [3] λ = 1, k = 3, m = 3 and n ≡ 0 or 6 (mod 24);
that D is G-invariant with all G-orbits of its blocks of full
(4) [26] λ = 1, k = 3, m = 3 and n is a positive integer;
length |G|.
(5) [22] λ = 1, k = 4, gcd(m, 18) = 3 and n ≡ 0
As the terminology suggests, given all the base blocks
(mod 12);
of a strictly Z m × Z n -invariant t-(mn, k, 1) packing
ε
(6) [25] λ = 2, k = 4, m = 2 x and n = y, where ε ∈ {1, 2},
(t-(m × n, k, 1)-SP in short), we can obtain all its blocks
and each factor of x, y is a prime less than 500000 and
by successively adding (i, j ) to each base block, where
congruent to 53 or 77 modulo 120 or belongs to S =
(i, j ) ∈ Z m × Z n . Let A(m × n, k, t) be the largest pos{5, 13,17, 25, 29, 37, 41, 53, 61, 85, 89, 97, 101, 113, 137,
sible number of base blocks among all t-(m × n, k, 1)-SPs.
149, 157, 169, 173, 193, 197, 229, 233, 289, 293, 317}.
A t-(m × n, k, 1)-SP with A(m × n, k, t) base blocks is
said to be maximum. It is obvious that A(m × n, k, t) ≤
A. Equivalent Description
J (mn, k, t − 1). When t = 2, 2-(m × n, k, 1)-SP and A(m ×
An OOSPC is a particular type of 2-dimensional optical n, k, 2) are abbreviated as (m × n, k, 1)-SP and A(m × n, k)
orthogonal codes (briefly 2-D OOCs) [29] and closely related respectively.
to a 1-D OOC [10]. The following result was shown in [28].
Sawa [25] established the following relation between an
Lemma 1.1 [28, Construction 1]: Suppose that there exists (m, n, k, λ)-OOSPC and a (λ + 1)-(m × n, k, 1)-SP.
a 1-D (m, k, λ)-OOC with size N. Then for any integer
Lemma 1.4 [25]: An (m, n, k, λ)-OOSPC with maximum
factorization m = m 1 m 2 satisfying gcd(m 1 , m 2 ) = 1, there size (m, n, k, λ) is equivalent to a strictly Z m × Z n -invariant
exists an (m 1 , m 2 , k, λ)-OOSPC with the same size N.
(λ + 1)-(mn, k, 1) packing (i.e., (λ + 1)-(m × n, k, 1)-SP) with
Remark 1.2: Similar to Lemma 1.1, for any integers A(m × n, k, λ + 1) base blocks. Furthermore, (m, n, k, λ) =
m = m 1 m 2 and n = n 1 n 2 satisfying gcd(m 1 n 2 , m 2 n 1 ) = 1, A(m × n, k, λ + 1).
an (m, n, k, λ)-OOSPC with size N can be regarded as an
Suppose that B is the set of all the base blocks of
(m 1 n 1 , m 2 n 2 , k, λ)-OOSPC with the same size N.
an (m × n, k, 1)-SP. Without loss of generality we can
Based on Lemma 1.1 and Remark 1.2, several known always assume that each base block B ∈ B contains
results on 1-D OOCs and OOSPCs are restated as the element (0, 0), i.e., B is of the form {(x 0 , y0 ) =
follows.
(0, 0), (x 1 , y1 ), . . . , (x k−1 , yk−1 )}. Define
Lemma 1.3: There exists an (m, n, 3, 1)-OOSPC with

B = {(x i , yi ) − (x j , y j ) : 0 ≤ i, j ≤ k − 1, i 	= j }
J (mn, 3, 1) codewords for any positive integers m and n
satisfying one of the following conditions:
as the 
list differences of B. Then the list differences of B is
(1) [1, Th. 1.5] gcd(m, n) = 1 and mn 	≡ 14, 20 (mod 24);

B = B∈B 
B. Note that 
B covers each non-zero element
(2) [26, Th. 3.1] m and n are odd except for m, in Z m × Z n at most once since the (m × n, k, 1)-SP is strictly
Z m × Z n -invariant. Define the difference leave of B (briefly,
n ≡ 5 (mod 6);
DL(B)) as the set of all the non-zero elements in Z m × Z n
(3) [26, Lemma 3.1] m = 3m 0 and gcd(m 0 , 3n) = 1.
Moreover, an OOSPC is closely related to a certain com- which are not covered by 
B. B is said to be (s × t)-regular
binatorial structure, called a packing. In a particular case, (briefly, (m × n, s × t, k, 1)-SP) if DL(B) along with {(0, 0)}
it is nothing else than a relative difference family [2]. forms an additive subgroup S × T of Z m × Z n , where S and T
 1  mn − 1  mn − 2 

 mn − λ 



PAN AND CHANG: (m, n, 3, 1) OOSPC WITH MAXIMUM POSSIBLE SIZE

are, respectively, the additive subgroups of order s in Z m and
order t in Z n .
If an (m, n, k, 1)-OOSPC is equivalent to an (m × n, s × t,
k, 1)-SP, then the OOSPC is said to be (s, t)-regular.
Evidently, an (s, t)-regular (m, n, k, 1)-OOSPC consists of
(mn−st)/(k(k−1)) codewords and all its codewords constitute
a (Z m × Z n , ms Z m × nt Z n , k, 1) relative difference family.
In particular, a (1 × 1)-regular (m, n, k, 1)-OOSPC
is just an (mn, k, 1) difference family on Z m × Z n .
The following is a variant of a known result on difference
families in [27].
Lemma 1.5 [27, Th. 7]: Let p > 3 be a prime. Then there
exists a (1, 1)-regular ( p, p, 3, 1)-OOSPC.
B. Main Results
In this paper, we are concerned about maximum
(m, n, 3, 1)-OOSPCs and determine the exact value of
(m, n, 3, 1) for any positive integers m and n. We are to
prove the following main result of this paper.
Theorem 1.6: Let m and n be positive integers. Then
⎧
J (mn, 3, 1) − 1,
⎪
⎪
⎪
⎪
if mn ≡ 14, 20 (mod 24),
⎪
⎪
⎪
⎪
or mn ≡ 8, 16 (mod 24)
⎨
and gcd(m, n, 4) = 2,
(m, n, 3, 1) =
⎪
⎪
or mn ≡ 2 (mod 6)
⎪
⎪
⎪
⎪
and gcd(m, n, 4) = 4;
⎪
⎪
⎩
J (mn, 3, 1),
otherwise.
With a special attention, the case of k = 3 is the only
situation for which the exact value of (m, n, k, 1) is completely determined. While the cases of k ≥ 4 have not been
fully figured out.
The rest of this paper is organized as follows. In Section II
we improve the upper bound on (m, n, 3, 1) which is tighter
than the well-known Johnson bound. In Section III an auxiliary
design is introduced to quote a recursive construction for
(m, n, k, 1)-OOSPCs with general k. In Section IV based on
the new upper bound on (m, n, 3, 1), we completely solve
the construction problem of (m, n, 3, 1)-OOSPCs with maximum size for any positive integers m and n by using direct
constructions and the recursive construction which is given in
Section III. Moreover, we give the proof of Theorem 1.6 in
this section. Finally, Section V gives a brief conclusion.
II. I MPROVED U PPER B OUND ON (m, n, 3, 1)
In this section, we shall give a new upper bound on
(m, n, 3, 1), which is tighter than the well-known Johnson
bound.
Lemma 2.1: Let mn
≡
14, 20 (mod 24) and
gcd(m, n, 2) = 1. Then (m, n, 3, 1) ≤ J (mn, 3, 1) − 1.
Proof: Without loss of generality, we assume that
m ≡ 0 (mod 2) and n ≡ 1 (mod 2). By the Johnson
bound, (m, n, 3, 1) ≤ J (mn, 3, 1). Suppose that there exists
an (m, n, 3, 1)-OOSPC with size J (mn, 3, 1) = (mn − 2)/6.
Such an OOSPC corresponds to an (m × n, 2 × 1, 3, 1)-SP.
Let B be the set of all the base blocks of the corresponding
packing. Then DL(B) = {(m/2, 0)}. For each base block

1141

B = {(0, 0), (x 1 , y1 ), (x 2 , y2 )} ∈ B, define
B = {0, x 1 (mod 2), x 2 (mod 2)}.
Let B = {B : B ∈ B}. Then all the triples in B
must be one of the following types: T1 = {0, 0, 0} and
T2 = {0, 0, 1}. Moreover, 
B contains 1 exactly mn−2
or
2
mn
times
according
to
whether
mn
is
congruent
to
14
or
20
2
modulo 24. On the other hand, let x be the multiplicity of
T2 in B. Then 
B covers 1 exactly 4x times. Hence, we have
mn − 2 = 8x ≡ 0 (mod 8) when mn ≡ 14 (mod 24), and
mn = 8x ≡ 0 (mod 8) when mn ≡ 20 (mod 24), which are
impossible. Therefore, (m, n, 3, 1) ≤ J (mn, 3, 1) − 1.
Lemma 2.2: Let b ≥ 2. Let mn ≡ 1 (mod 6) when b is odd,
and mn ≡ 5 (mod 6) when b is even. Then (2m, 2b n, 3, 1) ≤
J (2b+1 mn, 3, 1) − 1.
Proof:
Now we have 2b+1 mn ≡ 4 (mod 6).
Suppose that there exists a (2m, 2b n, 3, 1)-OOSPC with size
J (2b+1 mn, 3, 1) = (2b mn − 2)/3 and let B be the set of all
the base blocks of its corresponding (2m × 2b n, 3, 1)-SP. It is
easy to know that DL(B) = {(m, 0), (0, 2b−1 n), (m, 2b−1 n)}.
Similar to Lemma 2.1, let B = {B : B ∈ B}. Let
x be the multiplicity of T2 in B. By calculating the multiplicity of 1 in 
B, we have 4x = 2b mn − 2. It is
followed by 2x = 2b−1 mn − 1 ≡ 0 (mod 2) which is
impossible when b ≥ 2. Therefore, (2m, 2b n, 3, 1) ≤
J (2b+1 mn, 3, 1) − 1.
Lemma 2.3: Let a and b be positive integers. Let
mn ≡ 1 (mod 6) when a+b is odd, and mn ≡ 5 (mod 6) when
a + b is even. Then (2a m, 2b n, 3, 1) ≤ J (2a+b mn, 3, 1) − 1.
Proof: Note that there are at least four elements
(0, 0), (0, 2b−1 n), (2a−1 m, 0), (2a−1 m, 2b−1 n) not appearing
in the list differences of a (2a m×2b n, 3, 1)-SP. By Lemma 1.4,
we have
(2a m, 2b n, 3, 1) = A(2a m × 2b n, 3)
 2a+b mn − 4 
≤
= J (2a+b mn, 3, 1) − 1.
6
Then the conclusion follows.
Indeed, an (m, n, 3, 1)-OOSPC can be viewed as an
(n, m, 3, 1)-OOSPC. We therefore have (m, n, 3, 1) =
(n, m, 3, 1) and get the following theorem by combining the
Johnson bound with Lemmas 2.1-2.3.
Theorem 2.4: Let m and n be positive integers. Then
⎧
J (mn, 3, 1) − 1,
⎪
⎪
⎪
⎪
if mn ≡ 14, 20 (mod 24),
⎪
⎪
⎪
⎪
or mn ≡ 8, 16 (mod 24)
⎨
and gcd(m, n, 4) = 2,
(2)
(m, n, 3, 1) ≤
⎪
⎪
or mn ≡ 2 (mod 6)
⎪
⎪
⎪
⎪
and gcd(m, n, 4) = 4;
⎪
⎪
⎩
J (mn, 3, 1),
otherwise.
Proof: Based on the Johnson bound, we always have
(m, n, 3, 1) ≤ J (mn, 3, 1). Without loss of generality, we
assume that m = 2a m  and n = 2b n  , where 0 ≤ a ≤ b and
m  n  ≡ 1 (mod 2).
When mn ≡ 14, 20 (mod 24) and gcd(m, n, 2) = 1, the
conclusion follows immediately from Lemma 2.1.

1142

IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 61, NO. 2, FEBRUARY 2015

When mn ≡ 20 (mod 24) and gcd(m, n, 2) = 2, which
corresponds to a = b = 1 and m  n  ≡ 5 (mod 6), the
conclusion follows from Lemma 2.3.
When mn ≡ 8, 16 (mod 24) and gcd(m, n, 4) = 2, which
corresponds to b > a = 1 and m  n  ≡ 1, 5 (mod 6), the
conclusion follows from Lemmas 2.2 and 2.3.
When mn ≡ 2 (mod 6) and gcd(m, n, 4) = 4, which
corresponds to m  n  ≡ 1 (mod 6) and a + b ≡ 1 (mod 2),
or m  n  ≡ 5 (mod 6) and a + b ≡ 0 (mod 2), the conclusion
follows from Lemma 2.3.
III. AUXILIARY C OMBINATORIAL T OOLS
Recall the introduction in Section I-A, an (s, t)-regular
(m, n, k, 1)-OOSPC is actually a (Z m × Z n , ms Z m × nt Z n , k, 1)
relative difference family. In view of design theory, the following results are straightforward.
Lemma 3.1: If 1 ≤ st ≤ k(k − 1), then there exists an
(s, t)-regular (m, n, k, 1)-OOSPC (or an (m × n, s ×
t, k, 1)-SP) attaining the Johnson bound.
Lemma 3.2 (Filling Construction): Suppose that there
exists:
(1) an (s, t)-regular (m, n, k, 1)-OOSPC with b1 codewords;
(2) an (s, t, k, 1)-OOSPC with b2 codewords.
Then there exists an (m, n, k, 1)-OOSPC with b1 + b2
codewords. Moreover, if a (g, h)-regular (s, t, k, 1)-OOSPC
with b2 codewords is given, then a (g, h)-regular (m, n, k, 1)OOSPC with b1 + b2 codewords is obtained.
In order to quote more recursive constructions for OOSPCs,
we first introduce the notion of difference matrices.
Let G be an additive group (not necessarily abelian) of
order v. A (G, k, λ) difference matrix (briefly, (G, k, λ)-DM)
is a k ×λv matrix D = (di j ) with entries from G such that for
any distinct rows x and y, the multiset {dxi −d yi : 1 ≤ i ≤ λv}
contains each element of G exactly λ times. If G = Z v ,
the difference matrix is called cyclic and denoted by
(v, k, λ)-CDM.
Difference matrices play an important role in our constructions of (m, n, 3, 1)-OOSPCs. Much work on difference matrices has been done (see [11]). Here we only
list partially known results on difference matrices for later
use.
Lemma 3.3 [2, Th. 2.13]: Let G be a finite group and let
k be the least prime factor dividing |G|. Then there exists a
(G, k, 1)-DM.
Lemma 3.4 [23, Lemma 3.6]: Let x and y be positive
integers. Then there exists a (Z 2x × Z 2 y , 3, 1)-DM.
In the rest of this paper, the (Z m × Z n , 3, 1)-DM is used
many times, and its existence is guaranteed by Lemma 3.3 or
Lemma 3.4. Hence, we will not repeatedly say that “the needed
(Z m × Z n , 3, 1)-DM exists by Lemma 3.3 (or Lemma 3.4)”.
The following result in [23], which is actually an immediate
corollary of [2, Corollary 5.8], can be used to improve the
previous results on (s, t)-regular (m, n, k, 1)-OOSPCs.
Theorem 3.5: Let m, n, x and y be positive integers.
Suppose that there exists:
(1) an (s, t)-regular (m, n, k, 1)-OOSPC;
(2) a (Z x × Z y , k, 1)-DM.
Then there exists an (sx, t y)-regular (mx, ny, k, 1)-OOSPC.

Sketch of Proof:
• Step 1: Start from an (s, t)-regular (m, n, k, 1)OOSPC C, which is constructed on Z m × Z n . Let
D = (γi j )i=1,...,k; j =1,...,x y be a (Z x × Z y , k, 1)-DM,
where each γi j = (ai j , bi j ) ∈ Z x × Z y .
• Step 2: For any codeword C = {α1 , . . . , αk } ∈ C, let AC
consist of the following codewords:
{α1 + (a1 j m, b1 j n), . . . , αk + (akj m, bkj n)}

•

with j = 1, . . . , x y, where the addition “+” performs in
Z mx × Z ny .
Step 3: Take

A=
AC .
C∈C

Then A forms the desired (sx, t y)-regular (mx, ny, k, 1)OOSPC on Z mx × Z ny .
Furthermore, as a corollary of Theorem 3.5, we have the
following result.
Corollary 3.6: Let x and y be positive integers. If an
(s, t)-regular (m, n, 3, 1)-OOSPC exists, then so does a
(2x s, 2 y t)-regular (2x m, 2 y n, 3, 1)-OOSPC.
IV. E XISTENCE S PECTRUM FOR M AXIMUM
(m, n, 3, 1)-OOSPC S
In this section, the main purpose is to construct an
(m, n, 3, 1)-OOSPC attaining the bound in (2). The construction of a maximum (m, n, 3, 1)-OOSPC in the case of
mn ≡ 0 (mod 2) is more difficult than that in the case of
mn ≡ 1 (mod 2). To simplify the construction, we give some
(2a 3α , 2b 3β , 3, 1)-OOSPCs firstly.
Lemma 4.1: There exists an (s, t)-regular (m, n, 3, 1)OOSPC for each (m, n, s, t) ∈ {(4, 4, 2, 2), (4, 8, 2, 4),
(8, 8, 4, 4), (8, 16, 4, 8)}.
Proof: By computer search, there is a (2, 2)-regular
(4, 4, 3, 1)-OOSPC with 2 codewords:
{(0, 0), (0, 1), (1, 0)}, {(0, 0), (1, 1), (2, 3)},
and a (2, 4)-regular (4, 8, 3, 1)-OOSPC with 4 codewords:
{(0, 0), (0, 1), (1, 0)}, {(0, 0), (0, 3), (1, 1)},
{(0, 0), (1, 2), (2, 5)}, {(0, 0), (1, 4), (2, 1)}.
Moreover, start from such a (2, 2)-regular (4, 4, 3, 1)-OOSPC.
Applying Corollary 3.6 with (x, y) = (1, 1) and (1, 2)
respectively, we obtain a (4, 4)-regular (8, 8, 3, 1)-OOSPC and
a (4, 8)-regular (8, 16, 3, 1)-OOSPC.
Lemma 4.2: Let integers a, b and j satisfy one of the
following conditions:
(1) a = 0, b ≥ 3, b > j > 0 and b ≡ j (mod 2);
(2) a = 1, b ≥ 4, b > j ≥ 2 and b ≡ j (mod 2).
Then there exists a (2a , 2 j )-regular (2a , 2b , 3, 1)-OOSPC.
Proof: For Condition (1), use induction on b−2 j . When
b− j
= 1 (i.e., j = b − 2), we construct the desired
2
(1, 2 j )-regular (1, 2b , 3, 1)-OOSPC on Z 2b which consists
of the following codewords: {0, 2i + 1, 2b−1 − 2i − 1}
with 0 ≤ i < 2b−3 . When b−2 j > 1, suppose that a

PAN AND CHANG: (m, n, 3, 1) OOSPC WITH MAXIMUM POSSIBLE SIZE

(1, 2 j +2 )-regular (1, 2b , 3, 1)-OOSPC and a (1, 2 j )-regular
(1, 2 j +2 , 3, 1)-OOSPC exist. Then the desired (1, 2 j )-regular
(1, 2b , 3, 1)-OOSPC is obtained by Lemma 3.2.
For Condition (2), there is a (1, 2 j −1 )-regular
(1, 2b−1 , 3, 1)-OOSPC from Condition (1). We therefore
obtain the desired (2, 2 j )-regular (2, 2b , 3, 1)-OOSPC by
applying Corollary 3.6 with (x, y) = (1, 1).
Lemma 4.3: Let b ≥ a ≥ 2 and i ∈ {1, 2}.
Then there exists a (2i , 2 j )-regular (2a , 2b , 3, 1)-OOSPC,
where i + j ≡ a +b (mod 2) and j ∈ {1, 2} or {2, 3} according
to whether i = 1 or 2.
Proof: Consider the case of i = 2.
If a ∈ {2, 3}, use induction on b. It is trivial when
(a, b) = (2, 2), (2, 3). When (a, b) = (2, 4), there is
a (1, 2)-regular (1, 8, 3, 1)-OOSPC from Lemma 4.2. We
therefore obtain the desired (4, 4)-regular (4, 16, 3, 1)-OOSPC
by applying Corollary 3.6 with (x, y) = (2, 1). When
(a, b) = (3, 3), (3, 4), the conclusion holds by Lemma 4.1.

When b > 4, start from a (1, 2b −2 )-regular (1, 2b−2 , 3, 1)OOSPC, which exists by Lemma 4.2, where b ∈ {3, 4} and
b ≡ b (mod 2). Applying Corollary 3.6 with (x, y) = (a, 2),

we obtain a (2a , 2b )-regular (2a , 2b , 3, 1)-OOSPC. Finally the
conclusion holds by applying Lemma 3.2 with the conclusion
of the case of a ∈ {2, 3} and b ∈ {3, 4}.

If a ≥ 4, there is a (2a −1 , 1)-regular (2a−1 , 1, 3, 1)OOSPC, which is from Lemma 4.2, where a  ∈ {2, 3} and
a  ≡ a (mod 2). Then the conclusion holds by applying
Corollary 3.6 with (x, y) = (1, b) and Lemma 3.2 with the
conclusions of the cases of a = 2, 3.
For the case of i = 1, there is a (2, 2)-regular
(4, 4, 3, 1)-OOSPC and a (2, 4)-regular (4, 8, 3, 1)OOSPC from Lemma 4.1. Hence, the conclusion holds
by applying Lemma 3.2 with the assertion of the case
of i = 2.
Lemma 4.4: Let g ∈ {1, 2, 4}. There exists an (s, tg)regular (m, ng, 3, 1)-OOSPC for each (m, n, s, t)
∈
{(1, 27, 1, 3), (3, 9, 1, 3), (9, 3, 1, 3)}.
Proof: All the codewords of the desired (s, tg)-regular
(m, ng, 3, 1)-OOSPC are listed in Appendix A.
Lemma 4.5: Let α ≥ 0 and β > 0. There exists an
(s, t)-regular (3α u, 3β v, 3, 1)-OOSPC for the following types
of parameters:
(1) u = 1, v ∈ {1, 2, 4} and (s, t) belongs to {(1, 3v),
(1, 9v), (3, 3v)};
(2) u = 2, v ∈ {2, 4, 8} and (s, t) belongs to {(2, 6), (2, 12),
(2, 18), (2, 24), (2, 36)}.
Proof: For Type (1), use induction on α.
When α = 0, use induction on β. It is trivial for β = 1, 2.
For β = 3, the conclusion follows from Lemma 4.4. For
β ≥ 4, start from a (1, 3v)- or (1, 9v)-regular (1, 3β−1 v, 3, 1)OOSPC, which exists by the induction hypothesis. Applying
Theorem 3.5 with (x, y) = (1, 3), we obtain a (1, 9v)or (1, 27v)-regular (1, 3β v, 3, 1)-OOSPC. Finally, the desired
(1, 3v)- or (1, 9v)-regular (1, 3β v, 3, 1)-OOSPC is obtained
by Lemma 3.2.
When α ≥ 1, it is trivial for (α, β) = (1, 1). For
(α, β) = (1, 2), (2, 1), the conclusion follows from
Lemma 4.4. For the remaining cases, start from an

1143

(s, t)-regular (3α−1 , 3β v, 3, 1)-OOSPC, which exists by the
induction hypothesis, where (s, t) belongs to {(1, 3v), (1, 9v),
(3, 3v)}. Applying Theorem 3.5 with (x, y) = (3, 1) and
Lemma 3.2 with the conclusion of the case of (α, β) ∈
{(1, 1), (1, 2), (2, 1)}, we then obtain the desired (s, t)-regular
(3α u, 3β v, 3, 1)-OOSPC.
For Type (2), it is trivial for (α, β) = (0, 1), or
(α, β) = (0, 2) with v = 2, 4. For (α, β) = (0, 2) with v = 8,
or (α, β) = (1, 1), all the codewords of the desired (s, t)regular (3α u, 3β v, 3, 1)-OOSPC are listed in Appendix B.
For the remaining cases, there is an (s  , t  )-regular
(3α , 3β , 3, 1)-OOSPC, whose existence is guaranteed by
Type (1), where (s  , t  ) belongs to {(1, 3), (1, 9), (3, 3)}.
Hence, we obtain the desired (s, t)-regular (3α u, 3β v, 3, 1)OOSPC by applying Theorem 3.5 with (x, y) = (u, v)
and Lemma 3.2 with the conclusion of the case of
(α, β) ∈ {(0, 1), (0, 2), (1, 1)}.
Lemma 4.6: Let g ∈ {2, 4}. Let mn ≡ 3 (mod 6) and
gcd(m, n) = 3i , where integer i ≥ 0. Then there exists an
(m, gn, 3, 1)-OOSPC with J (gmn, 3, 1) codewords.
Proof: Without loss of generality, set m = 3α m  and
n = 3β n  , where β 	= 0 and gcd(m  n  , 6) = gcd(m  , n  ) = 1.
Start from an (s, gt)-regular (3α , 3β g, 3, 1)-OOSPC, which
is from Lemma 4.5, where (s, t) belongs to {(1, 3), (1, 9),
(3, 3)}. Applying Theorem 3.5 with (x, y) = (m  , n  ), we
obtain an (m  s, gn  t)-regular (m, gn, 3, 1)-OOSPC. Note
that, for each (s, t) ∈ {(1, 3), (1, 9), (3, 3)}, there
exists an (m  s, gn  t, 3, 1)-OOSPC with J (gm  n  st, 3, 1) codewords by Lemma 1.3 (1) and (3). Hence, the desired
(m, gn, 3, 1)-OOSPC with J (gmn, 3, 1) codewords exists
by Lemma 3.2.
Next, according to the values of m and n, we distinguish four cases to consider the constructions of maximum
(m, n, 3, 1)-OOSPCs.

A. mn ≡ 1 (mod 2)
In this subsection, to solve the construction problem of an
(m, n, 3, 1)-OOSPC attaining the bound in (2), our main task
is to construct it in the cases of m, n ≡ 5 (mod 6).
Lemma 4.7: Let m, n ≡ 5 (mod 6) be integers. Then there
exists an (m, n, 3, 1)-OOSPC with J (mn, 3, 1) codewords.
Proof: Set m = pm  and n = qn  , where p,
q ≡ 5 (mod 6) are primes and m  , n  ≡ 1 (mod 6).
If m  = n  = 1, i.e., both m and n are primes, by
Lemma 1.3 (1) and Lemma 1.5, there exists a (1, 1)-regular
(m, n, 3, 1)-OOSPC with J (mn, 3, 1) codewords. Otherwise,
start from a (1, 1)-regular ( p, q, 3, 1)-OOSPC, which is from
the first case. Applying Theorem 3.5 with (x, y) = (m  , n  ),
we obtain an (m  , n  )-regular (m, n, 3, 1)-OOSPC. Since an
(m  , n  , 3, 1)-OOSPC with J (m  n  , 3, 1) codewords exists by
Lemma 1.3 (2), we obtain the desired (m, n, 3, 1)-OOSPC
with J (mn, 3, 1) codewords by Lemma 3.2.
Combining Lemma 1.3 (2) with Lemma 4.7, the following
theorem is straightforward.
Theorem 4.8: Let mn ≡ 1 (mod 2). Then there exists an
(m, n, 3, 1)-OOSPC attaining the bound in (2).

1144

IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 61, NO. 2, FEBRUARY 2015

B. mn ≡ 0 (mod 2) and gcd(m, n, 2) = 1
In this subsection, we shall create an infinite family of
maximum (m, n, 3, 1)-OOSPCs in which one of integers
m and n is even and the another is odd. To achieve
our purpose successfully, we first give some (s, t)-regular
(m, n, 3, 1)-OOSPCs with special parameters by using direct
constructions.
Lemma 4.9: Let p > 3 be a prime. Then there exists a
(1, v)-regular ( p, pv, 3, 1)-OOSPC for each v ∈ {2, 4, 8, 16}.
Proof:
We construct the desired (1, v)-regular
( p, pv, 3, 1)-OOSPC on (Fp2 , +) × Z v , which is isomorphic
to Z p × Z pv , where (Fp2 , +) is the additive group of the
finite field Fp2 .
When v = 2, 4, let ω and ε be a primitive element and
a cube primitive root of Fp2 respectively. Then the desired
(1, v)-regular ( p, pv, 3, 1)-OOSPC consists of the following
codewords:
v=2:

{(0, 0), (1, 0), (ε + 1, 0)} · (ωi , 1),
i ∈ {1, 3, . . . , ( p 2 − 7)/6},
{(0, 0), ((ω − 1)2 , 0), (ω(ω − 1), 1)} · (ω j , 1),
j ∈ {0, 2, . . . , ( p 2 − 5)/2};

v =4:

{(0, 0), (1, 0), (ε + 1, 0)} · (ωi , 1),
{(0, 0), (1, 1), (ε + 2, 2)} · (ωi , 1),
{(0, 0), (ε, 1), (ε − 1, 2)} · (ωi , 1),
{(0, 0), (ε2 , 1), (ε2 − ε, 2)} · (ωi , 1),
i ∈ {0, 1 . . . , ( p 2 − 7)/6}.

When v = 8, 16, all the codewords of the desired
(1, v)-regular ( p, pv, 3, 1)-OOSPC can be divided into
two parts C1 and C2 , where C1 is the set of all
the codewords of a (1, v4 )-regular ( p, v4p , 3, 1)-OOSPC on
(Fp2 , +) × 4Z v which has just been constructed directly,

is
{(0,
0),
(s,
1),
(2s,
3)}
:
s
∈
F
or
and
C
2 \ {0}
2
p

{(0, 0), (s, 1), (2s, 7)}, {(0, 0), (s, 2), (2s, 5)} : s ∈ Fp2 \ {0}
according to whether v = 8 or 16.
Next, applying Theorem 3.5 or Lemma 3.2 with
Lemma 4.9, we give the main result of this subsection step
by step.
Lemma 4.10: Let d be a positive integer such that
gcd(d, 6) = 1. Then there exists a (1, v)-regular (d, dv, 3, 1)OOSPC for each v ∈ {2, 4, 8, 16}.
Proof: Let d = p1α1 p2α2 · · · prαr be the factorization of d, where pi > 3 is a prime and αi ≥ 1
is an integer for each i . For each prime pi , there is
a (Z pi × Z pi , 3, 1)-DM from Lemma 3.3 and a (1, v)regular ( pi , pi v, 3, 1)-OOSPC from Lemma 4.9. Start from a
(1, v)-regular ( p1 , p1 v, 3, 1)-OOSPC. Applying Theorem 3.5
with (x, y) = ( pi , pi ), we obtain a ( pi , pi v)-regular
( p1 pi , p1 pi v, 3, 1)-OOSPC. Then applying Lemma 3.2
with a (1, v)-regular ( pi , pi v, 3, 1)-OOSPC, we obtain a
(1, v)-regular ( p1 pi , p1 pi v, 3, 1)-OOSPC. Hence, the desired
(1, v)-regular (d, dv, 3, 1)-OOSPC can be obtained by repeating this process.
Lemma 4.11: Let g ∈ {2, 4} and m 0 n 0 ≡ 1 (mod 2). Then
there exists an (m 0 , gn 0 , 3, 1)-OOSPC attaining the bound
in (2).

Proof: Let m 0 = dm  and n 0 = dn  , where gcd(d, 6) = 1,
gcd(m  , n  ) = 3i and integer i ≥ 0. Start from a (1, g)regular (d, dg, 3, 1)-OOSPC, which exists by Lemma 4.10.
Applying Theorem 3.5 with (x, y) = (m  , n  ), we obtain
an (m  , gn  )-regular (m 0 , gn 0 , 3, 1)-OOSPC. Note that there
exists an (m  , gn  , 3, 1)-OOSPC with J (gm  n  , 3, 1) − 1 codewords if m  n  ≡ 7 (mod 12) and g = 2, or m  n  ≡ 5 (mod 6)
and g = 4 by the construction of a 1-D (n, 3, 1)-OOC in
[10, Sec. IV-E], or with J (gm  n  , 3, 1) codewords otherwise
by Lemma 4.6. Hence, by Lemma 3.2, we obtain the desired
(m 0 , gn 0 , 3, 1)-OOSPC with J (gm 0 n 0 , 3, 1) − 1 codewords if
m 0 n 0 ≡ 7 (mod 12) and g = 2, or m 0 n 0 ≡ 5 (mod 6) and
g = 4, or with J (gm 0 n 0 , 3, 1) codewords otherwise.
Theorem 4.12: Let mn ≡ 0 (mod 2) and gcd(m, n, 2) = 1.
Then there exists an (m, n, 3, 1)-OOSPC attaining the bound
in (2).
Proof: Without loss of generality, we assume that m is odd
and n = 2b n 0 where n 0 is odd and b ≥ 1. When b ∈ {1, 2},
the conclusion holds by Lemma 4.11. When b = 3, 4, we have
two cases.
Case 1: mn 0 ≡ 7 (mod 12) with b = 3, or mn 0 ≡ 5 (mod 6)
with b = 4.
Let m = dm  and n 0 = dn  , where gcd(m  , n  ) = 1. Start
from a (1, 2b )-regular (d, 2b d, 3, 1)-OOSPC, which exists
by Lemma 4.10. Applying Theorem 3.5 with (x, y) =
(m  , n  ), we obtain an (m  , 2b n  )-regular (m, n, 3, 1)-OOSPC.
Note that an (m  , 2b n  , 3, 1)-OOSPC with J (2b m  n  , 3, 1)
codewords exists by Lemma 1.3 (1). We therefore obtain
the desired (m, n, 3, 1)-OOSPC with J (mn, 3, 1) codewords
by Lemma 3.2.
Case 2: mn 0 	≡ 7 (mod 12) with b = 3, or mn 0 	≡ 5 (mod 6)
with b = 4.
Start from a (1, 2b−2 )-regular (1, 2b , 3, 1)-OOSPC, which
exists by Lemma 4.2. We then obtain the desired (m, n, 3, 1)OOSPC with J (mn, 3, 1) codewords by applying Theorem 3.5
with (x, y) = (m, n 0 ) and applying Lemma 3.2 with
Lemma 4.11.
When b ≥ 5, start from a (1, 2i )-regular (1, 2b , 3, 1)OOSPC, which exists by Lemma 4.2, where i ∈ {3, 4} and
i ≡ b (mod 2). Then the desired (m, n, 3, 1)-OOSPC with
J (mn, 3, 1) codewords is obtained by applying Theorem 3.5
with (x, y) = (m, n 0 ) and Lemma 3.2 with the assertion of
the case of b ∈ {3, 4}.
C. mn ≡ 0 (mod 4) and gcd(m, n, 4) = 2
In this subsection, we discuss the construction of an
(m, n, 3, 1)-OOSPC with maximum size for any even integers
m and n such that gcd(m, n, 4) = 2. To our knowledge, very
little has been done on maximum (m, n, 3, 1)-OOSPCs with
m, n ≡ 0 (mod 2).
Lemma 4.13: Let g
∈
{6, 12, 18, 24, 36}. Let m
be an odd integer such that gcd(m, 3) = 1. Then
there exists a (2, gm, 3, 1)-OOSPC with J (2gm, 3, 1)
codewords.
Proof:
When m = 1, the desired (2, gm, 3, 1)OOSPC with J (2gm, 3, 1) codewords exists for each
g ∈ {6, 12, 18, 24, 36}, which lists as follows.

PAN AND CHANG: (m, n, 3, 1) OOSPC WITH MAXIMUM POSSIBLE SIZE

t
6
12
18

24

36

Codewords
{(0, 0), (0, 1), (1, 2)};
{(0, 0), (0, 1), (0, 3)},
{(0, 0), (0, 4), (1, 1)},
{(0, 0), (0, 5), (1, 7)};
{(0, 0), (0, 1), (0, 3)},
{(0, 0), (0, 4), (1, 1)},
{(0, 0), (0, 5), (1, 7)},
{(0, 0), (0, 6), (1, 10)},
{(0, 0), (0, 7), (1, 12)};
{(0, 0), (0, 1), (0, 3)},
{(0, 0), (0, 4), (0, 9)},
{(0, 0), (0, 6), (1, 1)},
{(0, 0), (0, 7), (1, 3)},
{(0, 0), (0, 8), (1, 15)},
{(0, 0), (0, 10), (1, 16)},
{(0, 0), (0, 11), (1, 13)};
{(0, 0), (0, 1), (1, 16)},
{(0, 0), (0, 2), (0, 6)},
{(0, 0), (0, 3), (0, 11)},
{(0, 0), (0, 5), (0, 12)},
{(0, 0), (0, 9), (1, 1)},
{(0, 0), (0, 10), (1, 4)},
{(0, 0), (0, 13), (1, 22)}, {(0, 0), (0, 14), (1, 3)},
{(0, 0), (0, 15), (1, 5)},
{(0, 0), (0, 16), (1, 29)},
{(0, 0), (0, 17), (1, 34)}.

When m > 1 and gcd(m, 6) = 1, there exists a (1, 3)regular (1, 3m, 3, 1)-OOSPC and a (1, 3)-regular (1, 9m, 3, 1)OOSPC which are equivalent to a cyclic STS(3m) and a cyclic
STS(9m) in [24]. Applying Corollary 3.6 with appropriate
(x, y), we can obtain a (2, t)-regular (2, gm, 3, 1)-OOSPC for
each g ∈ {6, 12, 18, 24, 36}, where t belongs to {6, 12, 24}.
Then, by applying Lemma 3.2 with the conclusion of the case
of m = 1, we obtain the desired (2, gm, 3, 1)-OOSPC with
J (2gm, 3, 1) codewords.
Lemma 4.14: Let m ≡ 1, 5 (mod 6). Then there exists a
(2m, 2, 3, 1)-OOSPC attaining the bound in (2).
Proof: The desired (2m, 2, 3, 1)-OOSPC is constructed
on Z m × Z 2 × Z 2 . Let C1 be the set of all the
J (m, 3, 1) codewords of a maximum (m, 1, 3, 1)-OOSPC on
Z m ×{0}×{0}, which exists by Lemma 1.3 (1). And let C2 consist of the following codewords: {(0, 0, 0), (i, 0, 1), (2i, 1, 0)}
with i ∈ {1, 2, . . . , (m−1)/2}. It is readily checked that C1 ∪C2
forms the desired (2m, 2, 3, 1)-OOSPC, where |C1 ∪ C2 | =
J (4m, 3, 1) or J (4m, 3, 1) − 1 according to whether m ≡ 1
or 5 (mod 6).
Lemma 4.15: Let m ≡ 1, 5 (mod 6). Then there exists a
(2m, 4, 3, 1)-OOSPC attaining the bound in (2).
Proof: We construct the desired (2m, 4, 3, 1)-OOSPC with
J (8m, 3, 1) − 1 codewords on Z m × Z 2 × Z 4 . Let N = {0, 2}
be the additive subgroup of order 2 in Z 4 .
When m ≡ 1 (mod 6), start from a (1, 1)-regular
(m, 1, 3, 1)-OOSPC, which exists by Lemma 1.3 (1). Applying Corollary 3.6 with (x, y) = (1, 2), we obtain a
(2, 4)-regular (2m, 4, 3, 1)-OOSPC. Since there is a trivial
(2, 4, 3, 1)-OOSPC without codeword, we then obtain
a (2m, 4, 3, 1)-OOSPC with J (8m, 3, 1) − 1 codewords
by Lemma 3.2.
When m ≡ 5 (mod 6), let C1 be the set of the J (2m, 3, 1)
codewords of a maximum (m, 2, 3, 1)-OOSPC on Z m ×
{0} × N, which exists by Lemma 1.3 (1). And let C2 consist
of the following codewords: {(0, 0, 0), (2i, 0, 1), (3i, 1, 2)},
{(0, 0, 0), (−2i, 0, 1), (−i, 1, 1)} with i ∈ {1, 2, . . . , (m −
1)/2}. It is readily checked that C1 ∪ C2 forms the desired
(2m, 4, 3, 1)-OOSPC with J (8m, 3, 1) − 1 codewords.

1145

Lemma 4.16: Let m ≡ 1 (mod 6). Then there exists a
(2, 8)-regular (2m, 8, 3, 1)-OOSPC. Moreover, there exists a
(2m, 8, 3, 1)-OOSPC attaining the bound in (2).
Proof: Start from a (1, 1)-regular (m, 1, 3, 1)-OOSPC,
which exists by Lemma 1.3 (1). We then get the desired (2, 8)regular (2m, 8, 3, 1)-OOSPC by applying Corollary 3.6 with
(x, y) = (1, 3).
Moreover, there is a (2, 8, 3, 1)-OOSPC with 1 codeword:
{(0, 0), (0, 1), (0, 3)}. Hence, we get a (2m, 8, 3, 1)-OOSPC
with J (16m, 3, 1) − 1 codewords by Lemma 3.2.
Lemma 4.17: Let m ≡ 5 (mod 6). Then there exists a
(2m, 8, 3, 1)-OOSPC attaining the bound in (2).
Proof: We first prove the existence of (2 p, 8, 3, 1)OOSPC with J (16 p, 3, 1) − 1 codewords by constructing its
corresponding (2 p × 8, 3, 1)-SP with J (16 p, 3, 1) − 1
base blocks for any prime p
≡
5 (mod 6).
By Lemmas 1.3 (1) and 1.4, let B1 be the set of all
the J ( p, 3, 1) base blocks of a maximum ( p × 1, 3, 1)-SP on
Z p × {0} × {0} with {±(( p − 1)/2, 0, 0)} ⊆ DL(B1 ). And
construct B2 on Z p × Z 2 × Z 8 as follows:
{(0, 0, 0), (2i, 0, 1), (4i + 1, 0, 2)},
{(0, 0, 0), (4i − 1, 0, 2), (2i, 1, 1)},
{(0, 0, 0), (i, 0, 4), (2i, 1, 0)},
i ∈ {1, 2, 3, . . . , ( p − 1)/2},
{(0, 0, 0), ( j, 0, 3), (2 j, 1, 5)},
j ∈ {2, 3, . . . , p − 1} \ {( p − 1)/2, ( p + 1)/2},
{(0, 0, 0), (( p + 1)/2, 0, 0), (0, 0, 3)},
{(0, 0, 0), (( p + 1)/2, 0, 3), (0, 1, 5)},
{(0, 0, 0), (1, 0, 1), (1, 1, 2)},
{(0, 0, 0), (1, 0, 2), (2, 1, 5)},
{(0, 0, 0), (1, 0, 3), (1, 1, 5)}.
It is readily verified that B1 ∪ B2 forms a (2 p × 8, 3, 1)-SP
with J (16 p, 3, 1) − 1 base blocks.
Next, set m = pm  , where m  ≡ 1 (mod 6) and
p ≡ 5 (mod 6) is a prime. Start from a (2, 8)-regular
(2m  , 8, 3, 1)-OOSPC, which exists by Lemma 4.16. Applying
Theorem 3.5 with (x, y) = ( p, 1), we have a (2 p, 8)regular (2m, 8, 3, 1)-OOSPC. Then applying Lemma 3.2 with
the claim, we obtain the desired (2m, 8, 3, 1)-OOSPC with
J (16m, 3, 1) − 1 codewords.
Lemma 4.18: Let g ∈ {2, 4, 8} and mn ≡ 1 (mod 2). Then
there exists a (2m, gn, 3, 1)-OOSPC attaining the bound in (2).
Proof: Let m = 3α dm  and n = 3β dn  , where α, β ≥ 0
and gcd(m  , n  ) = gcd(dm  n  , 6) = 1. Start from a (1, 1)regular (d, d, 3, 1)-OOSPC, which exists by Theorem 4.8.
When α = β = 0, applying Theorem 3.5 twice with
(x, y) = (2, g) and (x, y) = (m  , n  ), we obtain a (2m  , gn  )regular (2m, gn, 3, 1)-OOSPC. Note that a (2m  , gn  , 3, 1)OOSPC attaining the bound in (2) is equivalent to a
(2m  n  , g, 3, 1)-OOSPC attaining the bound in (2), which
exists by Lemmas 4.14-4.17. We therefore obtain the
desired (2m, gn, 3, 1)-OOSPC attaining the bound in (2) by
Lemma 3.2.

1146

IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 61, NO. 2, FEBRUARY 2015

When (α, β) 	= (0, 0), without loss of generality, we
can assume that β 	= 0. We first obtain a (2 · 3α , 3β g)regular (2 · 3α d, 3β dg, 3, 1)-OOSPC by applying Theorem 3.5
twice with (x, y) = (2, g) and (x, y) = (3α , 3β ). From
Lemma 4.5, there is a (2, h)-regular (2·3α , 3β g, 3, 1)-OOSPC,
where h belongs to {6, 12, 18, 24, 36}. Then, a (2, h)-regular
(2 · 3α d, 3β dg, 3, 1)-OOSPC exists by Lemma 3.2. Next,
applying Theorem 3.5 with (x, y) = (m  , n  ), we have a
(2m  , hn  )-regular (2m, gn, 3, 1)-OOSPC. Finally, the desired
(2m, gn, 3, 1)-OOSPC with J (2gmn, 3, 1) codewords can be
obtained by applying Lemma 3.2 with Lemma 4.13.
The following is a theorem in this subsection.
Theorem 4.19: Let mn ≡ 0 (mod 4) and gcd(m, n, 4) = 2.
Then there exists an (m, n, 3, 1)-OOSPC attaining the bound
in (2).
Proof: Without loss of generality, we can assume that
m = 2m  and n = 2b n  , where m  , n  are odd and b ≥ 1.
For each b ∈ {1, 2, 3}, the conclusion immediately follows from Lemma 4.18. For the case b ≥ 4, there is a
(2, 2i )-regular (2, 2b , 3, 1)-OOSPC from Lemma 4.2, where
i ∈ {2, 3} and i ≡ b (mod 2). We then obtain the desired
(m, n, 3, 1)-OOSPC attaining the bound in (2) by applying
Theorem 3.5 with (x, y) = (m  , n  ) and Lemma 3.2 with
Lemma 4.18.
D. m, n ≡ 0 (mod 4)
By the bound in (2), we observe that a maximum
(m 0 , n 0 , 3, 1)-OOSPC can not attain the Johnson bound
for each (m 0 , n 0 ) ∈ {(4, 8), (1, 4 p), (2, 2 p), (2, 4 p)},
but a maximum (4, 8 p, 3, 1)-OOSPC can do that, where
p ≡ 5 (mod 6) is a prime. Hence, in the subsection, we can not complete the existence of (m, n, 3, 1)OOSPCs attaining the bound in (2) readily by using
Theorem 3.5 and the difficulty naturally concentrates on
the construction of a (4, 8 p, 3, 1)-OOSPC with J (32 p, 3, 1)
codewords.
First of all, to obtain the desired (4, 8 p, 3, 1)-OOSPC, we
introduce a property of a maximum (2 × p, 3, 1)-SP.
Lemma 4.20: Let p ≡ 5 (mod 6) be a prime. Then there
exists a (2 × p, 3, 1)-SP with J (2 p, 3, 1) base blocks whose
difference leave contains {±(0, 1)} or {±(1, 1)} according to
whether p ≡ 5 or 11 (mod 12).
Proof: The existence of a (2× p, 3, 1)-SP with J (2 p, 3, 1)
base blocks is guaranteed by Lemmas 1.3 (1) and 1.4.
Denoted by B the set of all its base blocks, we just need
to point out the structure of DL(B). It is readily checked
that DL(B) must be {(1, 0), ±(i, α)}, where i = 0 or 1
and α 	= 0. Since there exists an element β ∈ Z p such
that αβ = 1, without loss of generality we can assume that
DL(B) = {(1, 0), ±(i, 1)}. Let x be the number of the base
blocks of the form {(0, 0), (0, ∗), (1, ∗)} in B and k be the
number of the elements in DL(B) which is of the form (1, ∗).
We then have p − k = 4x by calculating the number of the
differences of the form (1, ∗). It is easy to know that k must
be 1 if p ≡ 5 (mod 12), or 3 if p ≡ 11 (mod 12). That is,
DL(B) contains {±(0, 1)} if p ≡ 5 (mod 12), or {±(1, 1)} if
p ≡ 11 (mod 12).

Next, we give a direct construction to create a
(2, 2)-regular (4, 8 p, 3, 1)-OOSPC, which attains the bound
in (2).
Lemma 4.21: Let p ≡ 5 (mod 6) be a prime. Then there
exists a (2, 2)-regular (4, 8 p, 3, 1)-OOSPC.
Proof: We obtain the desired (2, 2)-regular (4, 8 p, 3, 1)OOSPC by constructing its corresponding (4×8 p, 2×2, 3, 1)SP on Z 4 × Z 8 × Z p . First, by Lemma 4.20, let B1 be the set of
the ( p−2)/3 base blocks of a (2× p, 3, 1)-SP on {0}×{0, 4}×
Z p with DL(B1 ) = {(0, 4, 0), ±(0, 0, 1)} if p ≡ 5 (mod 12),
or DL(B1 ) = {(0, 4, 0), ±(0, 4, 1)} if p ≡ 11 (mod 12). Next,
let B2 consist of
{(0, 0, 0), (0, 0, 1), (1, 5, 0)},
{(0, 0, 0), (1, 1, 0), (2, 2, 1)},
{(0, 0, 0), (1, 2, −1), (2, 6, 1)}
if p ≡ 5 (mod 12), or
{(0, 0, 0), (0, 4, 1), (1, 1, 1)},
{(0, 0, 0), (1, 1, 0), (2, 6, −1)},
{(0, 0, 0), (1, 2, −1), (2, 6, 1)}
if p ≡ 11 (mod 12). Finally, let B3 consist of
{(0, 0, 0), (0, 1, 1), (1, 0, 2)} · (1, 1, s),
{(0, 0, 0), (0, 2, 1), (2, 3, 2)} · (1, 1, s),
{(0, 0, 0), (0, 3, 1), (1, 6, 2)} · (1, 1, s), s ∈ Z p ,
{(0, 0, 0), (1, 1, i ), (2, 6, 2i − 1)}, i ∈ Z p \ {0, 1},
{(0, 0, 0), (1, 2, j ), (2, 4, 2 j + 2)},
j ∈ {4t, 4t + 1 : t = 0, 1, . . . , ( p − 5)/4}
i f p ≡ 5 (mod 12),
j ∈ {4t + 1, 4t + 2 : t = 0, 1, . . . , ( p − 7)/4} ∪ { p − 2}
i f p ≡ 11 (mod 12),
{(0, 0, 0), (1, 4, k), (2, 0, 2k + 4)},
k ∈ {8t + a : t = 0, 1, . . . ( p − 13)/8,
a = 1, 4, 6, 7} ∪ { p − 4, p − 1}
i f p ≡ 5 (mod 24),
k ∈ {8t + a : t = 0, 1, . . . , ( p − 9)/8, a = 3, 4, 5, 6}
i f p ≡ 17 (mod 24),
k ∈ {8t + a : t = 0, 1, . . . ( p − 11)/8,
a = 0, 3, 5, 6} ∪ { p − 3}
i f p ≡ 11 (mod 24),
k ∈ {8t + a : t = 0, 1, . . . ( p − 7)/8,
a = −1, 0, 1, 6} \ { p − 1}
i f p ≡ 23 (mod 24).
Then B1 ∪ B2 ∪ B3 forms the (4 × 8 p, 2 × 2, 3, 1)-SP.
Finally, we establish the main theorem in this subsection.
Theorem 4.22: Let m, n ≡ 0 (mod 4). Then there exists an
(m, n, 3, 1)-OOSPC attaining the bound in (2).
Proof: Without loss of generality, let m = 2a m  and
n = 2b n  where m  n  ≡ 1 (mod 2) and b ≥ a ≥ 2.
We then have two cases.
Case 1: m  n  ≡ 5 (mod 6) and a + b ≡ 1 (mod 2).
If m  ≡ 1 (mod 6) and n  ≡ 5 (mod 6), set n  = pn 0 ,
where n 0 ≡ 1 (mod 6) and p ≡ 5 (mod 6) is a prime. Start
from a (4, 8)-regular (2a , 2b , 3, 1)-OOSPC which exists by
Lemma 4.3. We then obtain a (2, 2)-regular (2a , 2b p, 3, 1)OOSPC by applying Theorem 3.5 with (x, y) = (1, p) and

PAN AND CHANG: (m, n, 3, 1) OOSPC WITH MAXIMUM POSSIBLE SIZE

Lemma 3.2 with Lemma 4.21. Finally, applying Theorem 3.5
with (x, y) = (m  , n 0 ) and Lemma 3.2 with Lemma 4.18,
we obtain the desired (m, n, 3, 1)-OOSPC attaining the bound
in (2). If m  ≡ 5 (mod 6) and n  ≡ 1 (mod 6), set m  = pm 0 ,
where m 0 ≡ 1 (mod 6) and p ≡ 5 (mod 6) is a prime. The
proof is similar to that of the first case.
Case 2: m  n  ≡ 1, 3 (mod 6) and a + b ≡ 1 (mod 2), or

m n  ≡ 1 (mod 2) and a + b ≡ 0 (mod 2).
Start from a (2, 2 j )-regular (2a , 2b , 3, 1)-OOSPC where
j ∈ {1, 2} and j ≡ a + b − 1 (mod 2), which
exists by Lemma 4.3. We then obtain a (2m  , 2 j n  )regular (m, n, 3, 1)-OOSPC by applying Theorem 3.5 with
(x, y) = (m  , n  ). Finally, the desired (m, n, 3, 1)-OOSPC is
obtained by Lemma 3.2, where the needed (2m  , 2 j n  , 3, 1)OOSPC attaining the bound in (2) is from Lemma 4.18 for
each j ∈ {1, 2}.
Proof of Theorem 1.6: Combining Theorems 4.8, 4.12,
4.19 and 4.22, we know that (m, n, 3, 1) attains the bound
in (2). Hence, the conclusion follows.

V. C ONCLUSION
In this paper, we have completely determined the exact
value of the size of a maximum (m, n, 3, 1)-OOSPC (say
(m, n, 3, 1)) for any positive integers m and n. In conclusion,
we point out that (m, n, 3, 1) equals J (mn, 3, 1) − 1 if
mn ≡ 14, 20 (mod 24), or mn ≡ 8, 16 (mod 24) and
gcd(m, n, 4) = 2, or mn ≡ 2 (mod 6) and gcd(m, n, 4) = 4,
or J (mn, 3, 1) otherwise.
Now the reader may want to know the gap between the maximum sizes of 1-D OOCs and OOSPCs. Let 
(m, k, 1) be the
largest possible size among all 1-D (m, k, 1)-OOCs by using
the notation in [10]. From [1, Th. 1.5] and [10, Sec. IV-E],
we know that 
(m, 3, 1) is  m−1
6 −1 if m ≡ 14, 20 (mod 24),
or  m−1

otherwise.
In
comparison
with the exact value of
6
(m, n, 3, 1) in Theorem 1.6, it is observed that the following
formula is a gap between the maximum sizes of 1-D OOCs and
OOSPCs.
⎧

(mn, 3, 1) − 1,
⎪
⎪
⎪
⎪
⎪
if mn ≡ 8, 16 (mod 24)
⎪
⎪
⎪
⎨
and gcd(m, n, 4) = 2,
(m, n, 3, 1) =
⎪
or mn ≡ 2 (mod 6)
⎪
⎪
⎪
⎪
⎪
and gcd(m, n, 4) = 4,
⎪
⎪
⎩

(mn, 3, 1),
otherwise.
On the other hand, a natural question is why we are concerned about maximum optical orthogonal signature pattern
codes with weight 3. We know that research on OOSPCs
mainly focuses on calculating the exact value of (m, n, k, λ)
and constructing an (m, n, k, λ)-OOSPC with maximum size
(m, n, k, λ). Generally speaking, calculating the exact value
of (m, n, k, λ) is difficult and hitherto it has only been
completely settled for k = 3 and λ = 1. Although the case of
k = 3 is too small for practical application, it is significant in
theory and it may help to study the other larger cases.

1147

A PPENDIX A
Some (s, tg)-regular (m, ng, 3, 1)-OOSPCs of Lemma 4.4.
g (m, n, s, t)

Codewords

{(0, 0), (0, 1), (0, 3)},
{(0, 0), (0, 5), (0, 15)},
{(0, 0), (0, 1), (1, 0)},
1 (3, 9, 1, 3)
{(0, 0), (0, 4), (1, 2)},
{(0, 0), (1, 0), (2, 2)},
(9, 3, 1, 3)
{(0, 0), (2, 0), (5, 1)},

{(0, 0), (0, 4), (0, 11)},
{(0, 0), (0, 6), (0, 14)};
{(0, 0), (0, 2), (1, 6)},
{(0, 0), (1, 1), (2, 4)};
{(0, 0), (1, 1), (4, 1)},
{(0, 0), (2, 1), (5, 0)};

{(0, 0), (0, 1), (0, 3)},
{(0, 0), (0, 5), (0, 16)},
{(0, 0), (0, 8), (0, 25)},
{(0, 0), (0, 13), (0, 28)},
{(0, 0), (0, 1), (0, 5)},
{(0, 0), (0, 7), (1, 1)},
2 (3, 9, 1, 3)
{(0, 0), (1, 2), (2, 7)},
{(0, 0), (1, 7), (2, 3)},
{(0, 0), (1, 0), (2, 4)},
{(0, 0), (1, 2), (4, 0)},
(9, 3, 1, 3)
{(0, 0), (2, 1), (5, 3)},
{(0, 0), (2, 3), (4, 2)},

{(0, 0), (0, 4), (0, 10)},
{(0, 0), (0, 7), (0, 30)},
{(0, 0), (0, 12), (0, 32)},
{(0, 0), (0, 14), (0, 35)};
{(0, 0), (0, 2), (1, 0)},
{(0, 0), (0, 8), (1, 3)},
{(0, 0), (1, 4), (2, 10)},
{(0, 0), (1, 9), (2, 1)};
{(0, 0), (1, 1), (2, 0)},
{(0, 0), (1, 3), (5, 1)},
{(0, 0), (2, 2), (5, 5)},
{(0, 0), (3, 0), (6, 1)};

{(0, 0), (0, 1), (0, 5)},
{(0, 0), (0, 3), (0, 59)},
{(0, 0), (0, 7), (0, 46)},
{(0, 0), (0, 10), (0, 38)},
(1, 27, 1, 3)
{(0, 0), (0, 14), (0, 30)},
{(0, 0), (0, 21), (0, 43)},
{(0, 0), (0, 25), (0, 51)},
{(0, 0), (0, 32), (0, 66)},
{(0, 0), (0, 1), (0, 8)},
{(0, 0), (0, 4), (0, 14)},
{(0, 0), (0, 11), (1, 34)},
{(0, 0), (0, 16), (1, 1)},
4 (3, 9, 1, 3)
{(0, 0), (1, 4), (2, 9)},
{(0, 0), (1, 7), (2, 3)},
{(0, 0), (1, 11), (2, 23)},
{(0, 0), (1, 15), (2, 5)},
{(0, 0), (1, 0), (2, 1)},
{(0, 0), (1, 3), (5, 5)},
{(0, 0), (1, 5), (6, 1)},
(9, 3, 1, 3)
{(0, 0), (1, 7), (3, 1)},
{(0, 0), (1, 10), (3, 2)},
{(0, 0), (2, 0), (4, 10)},
{(0, 0), (2, 5), (4, 0)},
{(0, 0), (2, 11), (5, 4)},

{(0, 0), (0, 2), (0, 15)},
{(0, 0), (0, 6), (0, 50)},
{(0, 0), (0, 8), (0, 19)},
{(0, 0), (0, 12), (0, 53)},
{(0, 0), (0, 17), (0, 37)},
{(0, 0), (0, 23), (0, 47)},
{(0, 0), (0, 29), (0, 60)},
{(0, 0), (0, 33), (0, 68)};
{(0, 0), (0, 2), (2, 0)},
{(0, 0), (0, 5), (1, 25)},
{(0, 0), (0, 13), (2, 33)},
{(0, 0), (0, 17), (2, 18)},
{(0, 0), (1, 6), (2, 14)},
{(0, 0), (1, 9), (2, 19)},
{(0, 0), (1, 14), (2, 6)},
{(0, 0), (1, 19), (2, 7)};
{(0, 0), (1, 2), (4, 5)},
{(0, 0), (1, 4), (5, 1)},
{(0, 0), (1, 6), (2, 3)},
{(0, 0), (1, 8), (3, 4)},
{(0, 0), (1, 11), (4, 6)},
{(0, 0), (2, 2), (5, 11)},
{(0, 0), (2, 9), (5, 9)},
{(0, 0), (3, 6), (6, 2)}.

(1, 27, 1, 3)

(1, 27, 1, 3)

A PPENDIX B
Some (s, t)-regular (m, n, 3, 1)-OOSPCs of Lemma 4.5 (2).
For each (m, n, s, t) ∈ {(2, 72, 2, 12), (6, 6, 2, 6), (6, 12, 2, 12),
(6, 24, 2, 12)}, an (s, t)-regular (m, n, 3, 1)-OOSPC is constructed by
listing all its codewords as follows:
(m, n, s, t)

Codewords

{(0, 0), (0, 1), (0, 23)},
{(0, 0), (0, 3), (0, 38)},
{(0, 0), (0, 5), (0, 32)},
{(0, 0), (0, 8), (1, 11)},
{(0, 0), (0, 10), (1, 19)},
(2, 72, 2, 12)
{(0, 0), (0, 13), (1, 26)},
{(0, 0), (0, 15), (1, 29)},
{(0, 0), (0, 17), (1, 34)},
{(0, 0), (0, 20), (1, 45)},
{(0, 0), (0, 31), (1, 32)},

{(0, 0), (0, 2), (0, 28)},
{(0, 0), (0, 4), (0, 29)},
{(0, 0), (0, 7), (1, 2)},
{(0, 0), (0, 9), (1, 16)},
{(0, 0), (0, 11), (1, 21)},
{(0, 0), (0, 14), (1, 22)},
{(0, 0), (0, 16), (1, 31)},
{(0, 0), (0, 19), (1, 39)},
{(0, 0), (0, 21), (1, 44)},
{(0, 0), (0, 33), (1, 37)};

(6, 6, 2, 6)

{(0, 0), (1, 0), (2, 1)},
{(0, 0), (1, 3), (2, 2)},

{(0, 0), (1, 2), (2, 0)},
{(0, 0), (2, 3), (4, 1)};

(6, 12, 2, 12)

{(0, 0), (1, 0), (2, 7)},
{(0, 0), (1, 2), (2, 10)},
{(0, 0), (1, 4), (2, 9)},
{(0, 0), (2, 0), (4, 4)},

{(0, 0), (1, 1), (2, 11)},
{(0, 0), (1, 3), (2, 2)},
{(0, 0), (1, 6), (2, 3)},
{(0, 0), (2, 1), (4, 6)};

{(0, 0), (0, 1), (1, 17)},
{(0, 0), (0, 5), (5, 10)},
{(0, 0), (0, 9), (5, 11)},
{(0, 0), (1, 0), (5, 13)},
{(0, 0), (1, 2), (2, 23)},
(6, 24, 2, 12)
{(0, 0), (1, 4), (2, 3)},
{(0, 0), (1, 6), (3, 11)},
{(0, 0), (1, 8), (3, 17)},
{(0, 0), (1, 10), (2, 1)},
{(0, 0), (2, 0), (4, 8)},

{(0, 0), (0, 3), (2, 18)},
{(0, 0), (0, 7), (4, 17)},
{(0, 0), (0, 11), (2, 13)},
{(0, 0), (1, 1), (2, 21)},
{(0, 0), (1, 3), (3, 1)},
{(0, 0), (1, 5), (3, 9)},
{(0, 0), (1, 7), (3, 3)},
{(0, 0), (1, 9), (3, 19)},
{(0, 0), (1, 12), (2, 6)},
{(0, 0), (2, 12), (4, 5)}.

1148

IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 61, NO. 2, FEBRUARY 2015

ACKNOWLEDGMENT
The authors would like to thank the anonymous referees and
Prof. Kyeongcheol Yang, the Associate Editor for Sequences,
for their constructive comments and suggestions that greatly
improve the quality of this paper.
R EFERENCES
[1] R. J. R. Abel and M. Buratti, “Some progress on (v, 4, 1) difference
families and optical orthogonal codes,” J. Combinat. Theory, Ser. A,
vol. 106, no. 1, pp. 59–75, 2004.
[2] M. Buratti, “Recursive constructions for difference matrices and relative
difference families,” J. Combinat. Designs, vol. 6 no. 3, pp. 165–182,
1998.
[3] M. Buratti, “1-rotational Steiner triple systems over arbitrary groups,”
J. Combinat. Designs, vol. 9 no. 3, pp. 215–226, 2001.
[4] M. Buratti, “Cyclic designs with block size 4 and related optimal optical
orthogonal codes,” Designs, Codes Cryptograph., vol. 26, nos. 1–3,
pp. 111–125, 2002.
[5] M. Buratti and A. Pasotti, “Further progress on difference families
with block size 4 or 5,” Designs, Codes Cryptograph., vol. 56, no. 1,
pp. 1–20, 2010.
[6] Y. Chang, R. Fuji-Hara, and Y. Miao, “Combinatorial constructions
of optimal optical orthogonal codes with weight 4,” IEEE Trans. Inf.
Theory, vol. 49, no. 5, pp. 1283–1292, May 2003.
[7] Y. Chang and L. Ji, “Optimal (4up, 5, 1) optical orthogonal codes,”
J. Combinat. Designs, vol. 12, no. 5, pp. 346–361, 2004.
[8] Y. Chang and Y. Miao, “Constructions for optimal optical orthogonal
codes,” Discrete Math., vol. 261, nos. 1–3, pp. 127–139, Jan. 2003.
[9] Y. Chang and J. Yin, “Further results on optimal optical orthogonal
codes with weight 4,” Discrete Math., vol. 279, nos. 1–3, pp. 135–151,
Mar. 2004.
[10] F. Chung, J. A. Salehi, and V. K. Wei, “Optical orthogonal codes:
Design, analysis and applications,” IEEE Trans. Inf. Theory, vol. 35,
no. 3, pp. 595–604, May 1989.
[11] C. J. Colbourn, “Difference matrices,” in CRC Handbook of Combinatorial Designs, C. J. Colbourn and J. H. Dinitz, Eds. Boca Raton, FL,
USA: CRC Press, 2007, pp. 411–419.
[12] T. Feng, Y. Chang, and L. Ji, “Constructions for rotational Steiner
quadruple systems,” J. Combinat. Designs, vol. 17, no. 5, pp. 353–368,
Sep. 2009.
[13] T. Feng, Y. Chang, and L. Ji, “Constructions for strictly cyclic
3-designs and applications to optimal OOCs with λ = 2,” J. Combinat.
Theory, Ser. A, vol. 115, no. 8, pp. 1527–1551, Nov. 2008.
[14] R. Fuji-Hara and Y. Miao, “Optical orthogonal codes: Their bounds and
new optimal constructions,” IEEE Trans. Inf. Theory, vol. 46, no. 7,
pp. 2396–2406, Nov. 2000.
[15] R. Fuji-Hara, Y. Miao, and J. Yin, “Optimal (9v, 4, 1) optical orthogonal
codes,” SIAM J. Discrete Math., vol. 14, no. 2, pp. 256–266, 2001.
[16] G. Ge and J. Yin, “Constructions for optimal (v, 4, 1) optical orthogonal codes,” IEEE Trans. Inf. Theory, vol. 47, no. 7, pp. 2998–3004,
Nov. 2001.
[17] S. Johnson, “A new upper bound for error-correcting codes,” IRE Trans.
Inf. Theory, vol. 8, no. 3, pp. 203–207, Apr. 1962.
[18] K. Kitayama, “Novel spatial spread spectrum based fiber optic CDMA
networks for image transmission,” IEEE J. Sel. Areas Commun., vol. 12,
no. 4, pp. 762–772, May 1994.

[19] W. C. Kwong and G.-C. Yang, “Image transmission in multicore-fiber
code-division multiple-access networks,” IEEE Commun. Lett., vol. 2,
no. 10, pp. 285–287, Oct. 1998.
[20] S. Ma and Y. Chang, “A new class of optimal optical orthogonal
codes with weight five,” IEEE Trans. Inf. Theory, vol. 50, no. 8,
pp. 1848–1850, Aug. 2004.
[21] S. Ma and Y. Chang, “Constructions of optimal optical orthogonal codes
with weight five,” J. Combinat. Designs, vol. 13, no. 1, pp. 54–69, 2005.
[22] R. Pan and Y. Chang, “Combinatorial constructions for maximum optical
orthogonal signature pattern codes,” Discrete Math., vol. 313, no. 24,
pp. 2918–2931, Dec. 2013.
[23] R. Pan and Y. Chang, “Further results on optimal (m, n, 4, 1) optical
orthogonal signature pattern codes,” Sci. Sin. Math., vol. 44, no. 11,
pp. 1141–1152, 2014.
[24] R. Peltesohn, “Eine Lösung der beiden Heffterschen differenzenprobleme,” Compos. Math., vol. 6, pp. 251–257, 1938.
[25] M. Sawa, “Optical orthogonal signature pattern codes with maximum
collision parameter 2 and weight 4,” IEEE Trans. Inf. Theory, vol. 56,
no. 7, pp. 3613–3620, Jul. 2010.
[26] M. Sawa and S. Kageyama, “Optimal optical orthogonal signature
pattern codes of weight 3,” Biometrical Lett., vol. 46, no. 2, pp. 89–102,
2009.
[27] R. M. Wilson, “Cyclotomy and difference families in elementary abelian
groups,” J. Number Theory, vol. 4, no. 1, pp. 17–47, Feb. 1972.
[28] G.-C. Yang and W. C. Kwong, “Two-dimensional spatial signature
patterns,” IEEE Trans. Commun., vol. 44, no. 2, pp. 184–191, Feb. 1996.
[29] G.-C. Yang and W. C. Kwong, “Performance comparison of multiwavelength CDMA and WDMA + CDMA for fiber-optic networks,” IEEE
Trans. Commun., vol. 45, no. 11, pp. 1426–1434, Nov. 1997.
[30] J. Yin, “Some combinatorial constructions for optical orthogonal codes,”
Discrete Math., vol. 185, nos. 1–3, pp. 201–219, Apr. 1998.
[31] J. Yin, “A general construction for optimal cyclic packing designs,”
J. Combinat. Theory, Ser. A, vol. 97, no. 2, pp. 272–284, Feb. 2002.

Rong Pan was born in Yunnan, China, in 1988. She is currently a
Ph.D. student at Beijing Jiaotong University, China. Her research interests
include Combinatorial Design Theory and Coding Theory.

Yanxun Chang was born in Hebei, China, in 1962. He received the
B.S. degree and the M.S. degree from Hebei Normal University, China, in
1985 and 1988 respectively, and the Ph.D. degree from Suzhou University,
China, in 1995, all in mathematics.
From 1988 to 1995, he was with Hebei Normal University as a
Teaching- Research Assistant (1988-1990), a Lecturer (1990-1992) and then
an Associated Professor (1993-1995). From 1996 to 1997, he was an
Associated Professor at the Department of Mathematics, Beijing Jiaotong
University, China. Since 1998, he has been a Full Professor at the Department
of Mathematics, Beijing Jiaotong University, China. His research interests
include Combinatorial Design Theory, Coding Theory, Cryptography, and their
interactions.
Dr. Chang received several national and provincial awards from China.

Industrial and Government Track Short Paper

Domain-Constrained Semi-Supervised Mining of Tracking
Models in Sensor Networks
Rong Pan1 , Junhui Zhao2 , Vincent Wenchen Zheng1 , Jeffrey Junfeng Pan1 ,
Dou Shen1 , Sinno Jialin Pan1 and Qiang Yang1
1

2

Hong Kong University of Science & Technology
{panrong,vincentz,panjf,dshen,sinnopan,
qyang}@cse.ust.hk

ABSTRACT

NEC Labs, China, Zhongguancun
East Road, Beijing 100084, China
zhaojunhui@research.nec.com.cn

unique advantages of being lightweight, distributed, environmentaware and network-based. Object tracking, event-detection and
activity recognition can now be realized in sensor networks using probabilistic algorithms[7, 11]. It is a fundamental task for
many of these applications to locate mobile client devices using
collected wireless signals (in terms of radio-signal-strength, RSS,
values) from different sensor nodes that act as beacons.
In the past, some conventional data mining technologies have
been applied for solving the localization problem [7, 9]. Generally, some statistical models are obtained offline which can map
signals to locations. These models are then used online to predict
the client locations based on the real-time signal values. Among
the past works, many researchers have developed ranging-based algorithms for localizing mobile nodes in a wireless sensor network.
One approach is multilateration (e.g., [9]), consisting of two main
steps. It first transforms the sensor readings into a distance measure. It then attempts to recover the coordinate locations in terms
of relative distance to the beacon nodes. This approach relies on an
ideal signal propagation model and extensive hardware support. It
suffers from low accuracy problem since RSSs do not follow ideal
propagation patterns, especially in complex environments.
In this paper, we address this problem using a semi-supervised
statistical relational learning approach based on conditional random
fields (Semi-CRF). We assume that a mobile sensor node moves
in a sensor network environment. The RSS of the mobile node
can be received by several sensors in the network, which are then
forwarded to a processor for tracking. It can also happens in the
way that all sensors in the network are sending signals to the mobile
sensor node, which performs the localization itself. In either case,
we have a sequence of multi-dimensional vectors that corresponds
to a trace. Each vector along the trace can be labeled with a physical
location coordinate, or unlabelled.
This paper makes the following contributions. First, we identify and solve a major bottleneck in the application of data mining
technologies in sensor networks. Second, we present a novel semisupervised learning method for mobile-node tracking and localization by utilizing both labeled and unlabelled RSS trace data. Third,
we introduce domain-driven heuristics for reducing the complexity
of the learning procedure, which greatly improve the scalability of
the statistical models. Finally, we validate the proposed methods
through the experiments over a real sensor network.

Accurate localization of mobile objects is a major research problem in sensor networks and an important data mining application.
Specifically, the localization problem is to determine the location of
a client device accurately given the radio signal strength values received at the client device from multiple beacon sensors or access
points. Conventional data mining and machine learning methods
can be applied to solve this problem. However, all of them require large amounts of labeled training data, which can be quite expensive. In this paper, we propose a probabilistic semi-supervised
learning approach to reduce the calibration effort and increase the
tracking accuracy. Our method is based on semi-supervised conditional random fields which can enhance the learned model from
a small set of training data with abundant unlabeled data effectively. To make our method more efficient, we exploit a Generalized EM algorithm coupled with domain constraints. We validate our method through extensive experiments in a real sensor network using Crossbow MICA2 sensors. The results demonstrate the
advantages of methods compared to other state-of-the-art objecttracking algorithms.

Categories and Subject Descriptors
I.2.6 [Artificial Intelligence]: Learning; H.2.8 [Database Management]: Database Applications—Data mining

General Terms
Algorithms

Keywords
Localization, Calibration, Tracking, Sensor Networks, EM, CRF

1. INTRODUCTION
Recently, wireless sensor networks have attracted great interests
in several related research fields and industries. Many tasks such
as context-aware computing [4] and environmental monitoring can
be realized with the help of wireless sensor networks, which offer

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
KDD’07, August 12–15, 2007, San Jose, California, USA.
Copyright 2007 ACM 978-1-59593-609-7/07/0008 ...$5.00.

2. THE Semi-CRF ALGORITHM FOR
TRACKING AND LOCALIZATION
2.1

Problem Statement

Consider a two-dimensional tracking problem. Our objective is

1023

Industrial and Government Track Short Paper
the Markov assumption, however, Z(x) (as well as the node marginal p(yt |x) and the Viterbi labeling) can be calculated efficiently
by variants of the standard dynamic programming algorithms used
in HMM.
We assume that potentials factorize themselves according to a set
of features fk that are given and fixed, so that
!
X
Φ(yt−1 , yt , x) = exp
λk fk (yt−1 , yt , x, t) .
(3)

to determine the location of a mobile object yt = (ut , vt ) ∈ C
as it moves in a sequence, given the observed signal vectors xt .
Figure 3 shows an example of the floor in one of our experimental
test beds, which consists of N = 8 beacon nodes and one mobile
unknown node. The localization problem can be converted to a supervised classification problem if we had sufficient labeled data for
each location. However, when the labeled data are insufficient at
each location, we wish to make the best use of some partially labelled or totally unlabeled RSS sequences as well in our prediction.
Now let us formally introduce the notation of training data.
In our study, the training data consist of a set of fully labelled sequences Df = {(X1 , Y1 ), . . . , (XM , YM )}, and a
set of partially labeled (or totally unlabeled) sequences Dp =
{(XM +1 , YM +1 ), . . . , (XM +L , YM +L )}, where Xi is a sequence
of signal vectors xi1 , . . . , ximi , i = 1, . . . , M + L, and Yi is a sequence of corresponding locations yi1 , . . . , yimi , i = 1, . . . , M +
L. Some values of yij are unknown for M + 1  i  M + L. A
mobile robot can be employed to collect these unlabelled data by
simply wandering around.

2.2
2.2.1

k

The model parameters are a set of real weights Λ = {λk }, one
for each feature (to be defined below). The feature functions can
describe any aspect of a transition from yt−1 to yt as well as yt and
the global characteristics of x. For example, fk may have value 1
when the distance between yt−1 and yt is smaller than 50cm.

2.2.2

The parameters Λ can be estimated through a maximum likelihood procedure using the training data. That is, we can estimate
them by maximizing the conditional log-likelihood of the labeled
sequences in the training data Ψ = {(X1 , Y1 ), . . . , (XM , YM ))},
which is defined as:

Linear-chain CRF
The CRF Model

In this paper, we propose a statistical relational learning approach using CRF to exploit the relationship between RSS readings at two neighboring time points in terms of their corresponding
physical distance. As a mobile object moves around, a sequence of
RSS values can be received, with each corresponding to a certain
location. This process can be modeled by an 1-D Linear-chain CRF
as introduced in the following section, where the states correspond
to the location labels and the inputs or observations correspond to
the RSS readings.
Linear-chain CRF models have been widely used to model
sequential data.
These models can be roughly viewed as
conditionally-trained finite state machines [5]. A linear-chain CRF,
as shown in Figure 1, defines a distribution over state sequence
y = y1 , y2 , . . . , yT given an input sequence x = x1 , x2 , . . . , xT
by making a first-order Markov assumption on states, where T is
Unobserved Yi

y1

y2

yT

Observed Xi

x1

x2

xT

L(Λ) =

M
X

log(P (Yi |Xi ; Λ)),

(4)

i=1

where M is the number of sequences. As discussed in Sutton et
al. in [10], L(Λ) is concave
in light of the convexity of the kind of
P
functions g(x) = log i exp xi .

2.2.3

Inference

Given the conditional probability of the state sequence defined
by a CRF in Equation (1) and the parameters Λ, the most probable
labeling sequence can be obtained as
Y ∗ = arg max P (Y |X; Λ),
Y

(5)

which can be efficiently calculated using the Viterbi algorithm [8].
The marginal probability of states at each position in the sequence
can be computed by a dynamic programming inference procedure
similar to the forward-backward procedure for HMM [3]. We can
define the “forward values” αt (y|x) by setting α1 (y|x) equal to
the probability of starting with state y and then iterate as follows:
X
`
´
αt (y|x) =
αt−1 (y  |x)exp Λt (y  , y, x) ,
(6)
y

Figure 1: Diagram of Linear-chain CRF

where Λt (y  , y, x) is defined by:

the length of the sequence. These Markov assumptions imply that
the distribution over sequences factorizes in terms of potential functions Φt (yt−1 , yt , x) as:
Q
Φt (yt−1 , yt , x)
,
(1)
p(y|x) = t
Z(x)

Λt (y  , y, x) =

X

λk fk (yt−1 = y  , yt = y, x)

(7)

k

P
The “backward values”
Then Z(x) equals to
y αt (y|x).
βi (y|x) can be defined similarly.
X
`
´
βt+1 (y  |x)exp Λt (y, y  , x) .
(8)
βt (y|x) =

where the partition function Z(x) is the normalization constant that
makes the probability of all state sequences sum to one. It is defined
as follows:
XY
Z(x) =
Φt (yt−1 , yt , x).
(2)
y

Parameter Estimation

y 

After that, we calculate the marginal probability of each location
given the observed signal sequence:

t

The potential functions Φt (yt−1 , yt , x) can be interpreted as the
cost of making a transition from state yt−1 to state yt at time t,
similar to a transition probability in an HMM.
Computing the partition function Z(x) requires summing over
the exponentially many possible state sequences y. By exploiting

P (yt = g|x) =

αt (g|x) ∗ βt (g|x)
.
Z(x)

(9)

So far, we have introduced a linear-chain CRF model for unknown mobile-node tracking. We can see that fk (yt−1 , yt , x) (in

1024

Industrial and Government Track Short Paper
To incorporate the domain constraints mentioned above, we use
a so-called parameter tying technique that is designed for speech
recognition [6] to combine similar parameters. Our assumption is
that the characteristics of two transitions with the same distance are
alike. Intuitively, in Figure 2, we observe that the transitions g1 ↔
g2 and g8 ↔ g9 should happen with similar frequencies as they
both transit by one grid in terms of Euclidean distance. Similarly,
the transitions g24 ↔ g35 and g35 ↔ g46√should happen similarly
as the their Euclidean distances are both 2 grids.
From this observation, we can tie the parameters of the transition feature functions so long as they have the same transition
distance, which is defined as follows: The value of the transition feature function fk (yt−1 , yt , x) equals one if and only if
dis(yt−1 , yt ) = k, where the dis defines the distance between the
two points. As expected, the number of parameters is greatly reduced by using this constraint.

Equation (3)) is an arbitrary feature function over the entire observed sequences and the states at positions t and t − 1. In our
problem, the locations are two-dimensional continuous values. The
number of possible locations are infinite large. Therefore, it is extremely difficult to compute the feature of two arbitrary locations.
Fortunately, the tracking area is known in advance usually. One
solution is to discretize a 2-D location space into grids. For instance, in a 5m × 4m area, we can divide it into 10 × 8 grids with
each grid being 50 × 50cm2 . This example is shown in Figure 2.
In this way, we can convert the known locations into such grids. In
the test phase, if a mobile object is located at grid gi , we can use
the coordinates of the center point in gi to represent the location of
the mobile object. After limiting the location space, it is possible to
use the linear-chain CRF approach for tracking problem. However,
a major issue is how to determine the size of the grid. This problem can be solved in two ways. First, the size often is determined
by the nature of the problem itself, which is decided by the precision requirement posed by application users. Another approach is
to study the problem empirically, as we will do in the experimental
section.
g1

g2

g8

2.3

g9

g24
g35
g46

g80

=

L(Λ; Λt )
PM
log P (Yi |Xi ; Λ)+
Pi=1
P
(u)
(o)
(u)
(o)
M +L
P (Yi |Xi , Yi ; Λt ) log P (Yi , Yi |Xi ; Λ)
i=M +1
Y

Figure 2: A demo of reduction of locations to grids

2.2.4

The Semi-CRF Algorithm

In this section, we introduce how to incorporate sequences whose
labels are fully or partially observed in the parameter estimation of
CRF.
An efficient method for parameter estimation with incomplete
data can be derived by the extension of EM algorithm [2]. In this
paper, we use a Generalized Expectation Maximization (GEM) algorithm to learn the parameters Λ of CRF with both fully and partially observed data [1]. In the GEM algorithm, the probabilistic
optimization problem is divided into two-step iterations. The unobserved data are estimated in the E-step with the parameters obtained in the last iteration and the parameters of CRF are optimized
in the M-step. We first compute the log likelihood of Equation (4)
with expectation over the unobserved data as follows:

(u)

PM P i 
PM +L
=
Z(Xi )+
t (y , y, Xi ) −
i=1 logP
tΛ
Pi=1
P
(u)
(o)
M +L
t

P
(Y
|X
,
Y
;
Λ
)
i
i
i
i=M +1
t Λt (y , y, Xi )

Incorporating Domain Constraints

(u)

Yi

After reducing locations to grids, we can specialize the feature
functions for each possible transition among different grids. That
is, we can define fk (yt−1 = g, yt = h, x) by f(g,h) (t − 1, t, x).
However, the number of the transition feature functions, as well
as the corresponding parameters, reaches n2 (n is the number of
grids), which can be quite large. For instance, in the above example in Figure 2, n = 80, then in the CRF learning, we need
to estimate 6400 parameters for the potential fk (yt−1 , yt , x). Although we can still estimate the values of the parameters with large
n, it will certainly increase the computational cost and run the risk
of overfitting. What is worse, learning CRF with more parameters requires more training data, which will increase the labelling
costs. In addition, we also need to trade off the complexity of the
model and its generalization capability. If we increase the grid size
to reduce the computational cost, we will sacrifice the estimation
accuracy.
In this paper, we incorporate the domain constraints in the data
mining process to reduce the number of parameters that need be
learned. In particular, we note that a mobile object in a sensor network typically moves around in the same way, such that the likelihood of transiting between two neighboring points are roughly
same. The likelihood of traveling between two distant points will
also be roughly the same, although the value will be much smaller.
Such a domain constraint is supported by our experiments.

(u)

In this equation, Yi is the unobserved locations of the i-th se(o)
quence, Yi is the observed counterpart,
X
Λt (y  , y, Xi ) =
λk fk (y  , y, Xi ),
k

and
Z(Xi ) =

X
y

exp

“X
t

”
Λt (yt−1 , yt , Xi ) .

Similar to Equation (4), L(Λ; Λt ) is also concave. We can use
the same method to optimize it. The only problem left is how to
infer for partially observed sequences. We need to change Equations (6) and (8) for some cases. If yt = j is observed, we directly
assign 1 to αt (y = j|x) and βt (y = j|x) and assign 0 to the other
values of αt and βt . If yt is not observed, we follow Equations (6)
and (8). The new inference formulae are summarized in Equations
(10) to (12).
8
>
<0
αt (y = j|x) = 1
>
:P α (y  |x)exp (Λ (y  , y, x))
t
y  t−1

1025

yt =
 j
yt = j
yt unseen
(10)

Industrial and Government Track Short Paper
8
>
<0
βt (y = j|x) = 1
>
:P β (y  |x)exp (Λ (y  , y, x))
t
y  t+1

an experimental test-bed of 5.0 meters by 4.0 meters. In Figure 3,
|P1 P3 | = |P4 P6 | = 5.0m and |P1 P4 | = |P3 P6 | = 4.0m. There
are three main components of our setup:

yt =
 j
yt = j
yt unseen
(11)
(12)

• Wireless Sensor Networks. We use CrossBow MICA2 and
MICA2Dot to construct a wireless sensor network. We program these sensor nodes to broadcast and detect beacon
frames periodically so that they could measure the RSS of
each other.

We now summarize the Semi-CRF learning algorithm in Table
1. There are several ways of parameter initialization. The common
one is to randomly assign them values from 0 to 1. To speed up
the convergence, we use an alternative that preliminarily estimates
parameters with labeled data. As to the number of iterations, we
will discuss it in the experimental section.

• Mobile Robots. We try different kinds of robots that can run
freely around the floor at different speeds, such as a Sony
AIBO dogs, LEGO Mindstorms and off-the-shelf toy cars.
Figure 3 shows that a sensor node is attached on top of a toy
car which can be remotely controlled by radio at the speed of
0.4 m/s.

Table 1: The training algorithm for CRF with both fully and
partially observed data.
Algorithm Semi-CRF
Input: The fully and partially observed data Df , Dp
Output: The parameters Λ of CRF

• A Camera Array is used to record the ground truth locations
of the mobile robots for our training and test data.

P (yt = k|x) =

αt (y = k|x) ∗ βt (y = k|x)
.
Zx

We use two performance measurements to evaluate the original
CRF and the CRF model using parameter tying (denoted by CRFPT) localization algorithms. The first metric is the mean errordistance values between estimated and true locations. The second
measurement is the accuracy in percentage. Given an error-distance
threshold θ, the accuracy rate is the probability that the distance between the estimated and true locations is smaller than θ. Two more
baselines in our experiments include (1) Logistic Regression (LR),
(2) Support Vector Regression (SVR). We control a mobile robot
to run and stop around the test area (Figure 3) for collecting data
with sampling interval 0.5s. The data set formed a trace of length
about 600m with 3, 000 examples. For every experiment below,
we randomly select a subset of the data as fully observed training
data, a subset of data as partially observed training data by randomly removing the locations associated with them, and evaluate
the performance on the rest. To reduce the statistical variability,
we repeated the experiments for 30 times and reported the average
results.

Initialize parameters Λ0 of CRF.
Λt = Λ0 .
while log-likelihood has not converged
or the max number of iterations is not reached, do
% ====== E-step ======
Compute the expectations of the all unobserved locations,
by Equations (10) to (12).
% ====== M-step ======
Optimize Λ using L-BFGS.
Λt = Λ.
endwhile
return Λ

MICA2Dot

3.2

Camera-2

Camera-1

One question about the the Semi-CRF algorithm is the convergence of the EM iterations. In this experiment, we use 10 fully
labelled and 50 partially labelled sequential data to train the CRF,
where the length of each sequence is 5 and only one node is labelled
in the partially labelled data. Figure 4 shows the convergence rate
of Semi-CRF. We can see that about 4 iterations are enough. In the
experiments of this paper, the maximum number of iterations of the
Semi-CRF is set to 10.

Camera-3

Camera-4

Sony AIBO
MICA2

Projected Plane
6

3
8

5

7
Mobile Node

Real Plane

Beacon Node
4

1

LEGO Mindstorms

180

Summation of Log Likelihooh

2

Figure 3: Experimental Test-Bed

3.

Convergence of Semi-CRF

EXPERIMENTAL EVALUATION

3.1 Experimental Setup
We test the effectiveness and robustness of our location tracking algorithm for mobile sensor nodes in a sensor network based
on the RSS signals. Our experiments are performed in the Pervasive Computing Laboratory (Figure 3) in the Department of Computer Science and Engineering at Hong Kong University of Science and Technology. The room is large enough for us to set up

170
160
150
140
130
120
0

2

4

6

8

10

Number of Iterations

Figure 4: Convergence rate of Semi-CRF

1026

Industrial and Government Track Short Paper

3.3

Semi-CRF vs. Baselines

4. CONCLUSION AND FUTURE WORKS

In the following experiments, we fix the training data size at 550,
and tune the ratio of the labelled data from 0.33 to 1. In Figure
5, we show the mean error performance of the four algorithms described above. As can be seen, Semi-CRF consistently outperforms
the other algorithms in terms of mean error distance, while CRF
beats the remaining two baselines. One important reason is they
both effectively leverage the sequential information of the mobile
node. Moreover, as Semi-CRF can also learn from the unlabelled
data, it gains much better performance when there are a lot of such
data with a small portion of labelled ones. We list some more information of these experiments including the accuracy performance in
Table 2.

We have presented a new approach to reducing the calibration
effort when tracking mobile sensor nodes in a wireless sensor network. Our approach made extensive use of the sequential information of moving sensor’s trajectory. These sequences provided unlabelled examples which can be used to train CRF together with the
manually labelled RSS values. We introduce a Semi-CRF model
to utilize such partially labelled data. By using parameter tying
techniques we significantly improve the performance of Semi-CRF
algorithm while reducing calibration effort. A sensor network was
set up based on Crossbow MICA2 and MICA2Dot nodes which
are used as both beacon and mobile nodes. Experimental results
showed that the proposed method could achieve a better performance with relatively fewer number of labelled examples.
In the future, we plan to continue to test the Semi-CRF based
framework in a large scale sensor network. We are also interested
in introducing different factors, such as changing time and space,
to see how the knowledge learned in one setting can be applied to
another.

110
LR
SVR
CRF
GEM−CRF

Mean Error (cm)

100
90
80

5.

70
60
50
0.33

0.5

0.667

0.83

1

proportion of labelled instances

Figure 5: Vary the ratio of training set size
Table 2: Performance of the tested approaches
Approach
Mean(cm) Accuracy at 100cm
Semi-CRF
56.9267
85.67%
CRF
60.9268
83.67%
SVR
82.7318
67.33%
LR
82.3059
69.85%

3.4

Impact of Grid Sizes

The grid size may affect the performance of the localization algorithms. In this experiment, we fix the ratio of labelled data at 5%
and vary the side length of the grids from 10cm to 100cm. Figure 6 shows the experimental results of CRF and Semi-CRF . From
the figure we can see that when the grid size ranges from 20cm to
50cm, the performance of both the two methods is less sensitive
than that with the grid size of 10cm and 100cm.

Mean Error (cm)

85
80
75
70

CRF
GEM−CRF

65
60
10

20

25

50

REFERENCES

[1] H. L. Chieu, , S. W. Lee, and P. K. Leslie. Activity
recognition from physiological data using conditional
random fields, January 2006.
[2] A. P. Dempster, N. M. Laird, and D. Rubin. Maximum
likelihood from incomplete data via the em algorithm.
Journal of the Royal Statistical Society, 1(39):1–38, 1977.
[3] J. Lafferty, A. McCallum, and F. Pereira. Conditional
random fields: Probabilistic models for segmenting and
labeling sequence data. In Proc. 18th International Conf. on
Machine Learning, pages 282–289. Morgan Kaufmann, San
Francisco, CA, 2001.
[4] J. Lester, T. Choudhury, N. Kern, G. Borriello, and
B. Hannaford. A hybrid discriminative/generative approach
for modeling human activities. In IJCAI, pages 766–772,
2005.
[5] A. McCallum. Efficiently inducing features of conditional
random fields. In C. Meek and U. Kjærulff, editors, UAI,
pages 403–410. Morgan Kaufmann, 2003.
[6] C. Neukirchen, D. Willett, and G. Rigoll. Soft State-Tying
for HMM-Based Speech Recognition. In 5th International
Conference on Spoken Language Processsing (ICSLP),
pages 2999–3002, Sydney, 1998.
[7] X. Nguyen, M. I. Jordan, and B. Sinopoli. A kernel-based
learning approach to ad hoc sensor network localization.
ACM Transactions on Sensor Networks, 1(1):134–152, 2005.
[8] L. R. Rabiner. A tutorial on hidden markov models and
selected applications in speech recognition. Proceedings of
the IEEE, 77(2):257–286, 1989.
[9] A. Savvides, C. Han, and M. B. Strivastava. Dynamic
fine-grained localization in ad-hoc networks of sensors. In
Proceedings of the 7th Annual International Conference on
Mobile Computing and Networking, pages 166–179, Rome,
Italy, 2001.
[10] C. Sutton and A. McCallum. An introduction to conditional
random fields for relational learning. In L. Getoor and
B. Taskar, editors, Introduction to Statistical Relational
Learning. MIT Press, 2006.
[11] J. Yin, X. Chai, and Q. Yang. High-level goal recognition in
a wireless LAN. In Proceedings of the Nineteenth National
Conference on Artificial Intelligence, pages 578–584, San
Jose, CA, USA, July 2004.

100

Grid Size (cm)

Figure 6: Vary the grid size (ratio of labelled data is 5%.)

1027

KNN-Based Clustering for Improving Social
Recommender Systems
Rong Pan1 , Peter Dolog1 , and Guandong Xu2
1

Department of Computer Science, Aalborg University, Denmark
{Rpan,Dolog}@cs.aau.dk
2
Centre for Applied Informatics, Victoria University, Australia
Guandong.Xu@vu.edu.au

Abstract. Clustering is useful in tag based recommenders to reduce
sparsity of data and by doing so to improve also accuracy of recommendation. Strategy for the selection of tags for clusters has an impact on the
accuracy. In this paper we propose a KNN based approach for ranking
tag neighbors for tag selection. We study the approach in comparison to
several baselines by using two datasets in diﬀerent domains. We show,
that in both cases the approach outperforms the compared approaches.
Keywords: Tag Neighbors, Clustering, Personalization, Recommender
Systems, Social Tagging.

1

Introduction

A large number of social tagging sites, such as Delicious, Last.fm, Flickr, CiteUlike and Digg have undergone tremendous growth in the past few years.
Tagging, as a labeling of items with speciﬁc lexical information, plays a crucial
role in such social collaborative tagging systems. The common usage of tags in
these systems is to add the tagging attribute as an additional feature to re-model
users or resources over the tag vector space, and in turn, making tag-based collaborative ﬁltering recommendation or personalized recommendation. The usercontributed tags are not only an eﬀective way to facilitate personal organization
but also provide a possibility for users to search for needed information.
With the help of tagging data, user annotation preference and document topical tendency are substantially coded into the proﬁles of users or documents.
However, the redundancy, ambiguity and syntactic nature of tags are often incurred in all kinds of social tagging systems and remain still the main problems.
These problems also impact on the accuracy of recommendation.
In our previous research [20], we have proposed a clustering algorithm which
uses tag neighbors. This has improved the precision of the Recommendations;
however, the result of selection of tag neighbors for the individual tag in the realworld dataset is not satisﬁed. So we aim at improving this selection by ranking
tag neighbors.


This research has been partially supported by the EU FP7 ICT project M-Eco:
Medical Ecosystem Personalized Event-based Surveillance (No.247829).

L. Cao et al.: ADMI 2012, LNAI 7607, pp. 115–125, 2013.
c Springer-Verlag Berlin Heidelberg 2013


116

R. Pan, P. Dolog, and G. Xu

Due to above problems our motivation in this paper is to expand the previous work [19,23,20,24] with KNN algorithm in order to enhance recommendation precision. The approach is based on KNN neighbor directed graph of tags
which is used to drive the expansion of tag expression with top-K frequently tag
neighbors. By doing so we are able to facilitate the organization of information
documents in search and navigation.
The main contributions made in this paper are:
– We propose an approach of KNN algorithm with tag clustering algorithm to
ﬁlter the “noisy tags”.
– We show that our approach outperform the baselines in experiments on two
real world datasets.
The rest of the paper is organized as follows: Section 2 presents the related
work in the ﬁeld of tag neighbors and tag clustering. In section 3, we discuss
the preliminaries of the tagging data model. Section 4 describe the previous
work for the detailed process of extending the tag neighbors. The experiment
design, evaluation measures and the comparison of the results are in Section 5.
We conclude the paper in section 6.

2

Related Work

Tags are used to index, annotate and retrieve resource as an additional metadata of resource. Poor retrieval performance remains a major problem of most
social tagging systems resulting from the severe diﬃculty of ambiguity, redundancy and less semantic nature of tags. Clustering method is a useful tool to
address the aforementioned diﬃculties. We review the related literatures from
the perspectives of tag expansion and tag clustering.
The authors in [11] formalize the notion of resource recommendation in social
annotation systems. A linear-weighted hybrid framework for making recommendations is proposed and shown to be eﬀective. Some of the hybrid recommender
systems [4] have been shown to be an eﬀective method of drawing out the best
performance among several independent component algorithms. Our work can
be also considered as a hybrid recommender system where we utilize tags to
understand both, users and documents.
K. R. Bayyapu and P. Dolog in [1] try to solve the problems of sparse data and
low quality of tags from related domains. They suggest using tag neighbors for
tag expression expansion. The tag neighbors are based on the location within
documents while in this paper the tag neighbors are understood as similarity
neighbors in vector space model along users and documents.
Recommenders can assist users by suggesting resources, tags or even other
users. Authors in [16] have demonstrated that an integrated approach which exploits all three dimensions of the data (users, resources, tags) perform superior
results in tag recommendation. They extend the resource recommendation approach and propose an approach for designing weighted linear hybrid resource
recommenders. In this paper, we diﬀer in calculating weights and in a way which
we apply clustering.

KNN-Based Clustering for Improving Social Recommender Systems

117

The authors in [24] focus on the sensitivity of initialization instead and make
use of the approximate backbone of tag clustering results to ﬁnd out better tag
clusters. By proposing an APProximate backbone-based Clustering algorithm for
Tags (APPECT), they ﬁx the approximate backbone as the initial tag clustering
result and then assign the rest of the tags into the corresponding clusters based
on the similarity.
Existing data mining tools use workﬂow to capture user requirements. In
agent-based distributed data mining (ADDM), agents are an integral part of the
system and can seamlessly incorporate with workﬂows. The authors [18] describe
a mechanism to use workﬂow in descriptive and executable styles to incorporate between workﬂow generators and executors. They show that agent-based
workﬂows can improve ADDM interoperability and ﬂexibility, and also demonstrates the concepts and implementation with a supporting the argument, a
multi-agent architecture and an agent-based workﬂow model are demonstrated.
And the authors [17] investigate an interaction mechanism between agents and
data mining, and focus on agent-enhanced mining. The workﬂow enactment can
be improved with a suitable underlying execution layer, which is a Multi-Agent
System (MAS). They propose a strategy to obtain an optimal MAS conﬁguration
from a given workﬂow when resource access restrictions and communication cost
constraints are concerned, which is essentially a constraint optimization problem.
The authors in [6,5] specify ﬁve types of ubiquitous intelligence: data intelligence,
human intelligence, domain intelligence, network and web intelligence, organizational intelligence, and social intelligence. They deﬁne and illustrate them, and
discuss techniques for involving them into agents, data mining, and agent mining
for complex problem-solving. Further investigation on involving and synthesizing
ubiquitous intelligence into agents, data mining, and agent mining will lead to a
disciplinary upgrade from methodological, technical and practical perspectives.
In [10,12,13,7] topic relevant partitions are created by clustering resources
rather than tags. By clustering resources, the improvement of recommendations
is made by distinguishing between alternative meanings of a query. In [3], an
interesting approach is proposed to model the documents in social tagging systems by document graphs. The relevance of tag propagated along edges of the
documents graph is determined via a scoring scheme, with which the tag prediction was carried out. In [2], an approach that monitors users activity in a
tagging system and dynamically quantiﬁes associations among tags is presented.
The associations are then used to create tag clusters. In our work we focus on
tag clustering instead.
Zhou et al. propose a novel method to compute the similarity between tag sets
and use it as the distance measure to cluster web documents into groups. Major
steps in such method include computing a tag similarity matrix with set-based
vector space model, smoothing the similarity matrix to obtain a set of linearly
independent vectors and compute the tag set similarity based on these vectors.
[15]. In this paper we propose a diﬀerent enhanced approach which utilizes tag
neighbors, the KNN graph and clustering to compute recommendations.

118

R. Pan, P. Dolog, and G. Xu

The purpose of tag clustering is the ability of aggregating tags into topic domains. [22] demonstrate how tag clusters serving as coherent topics can aid in the
social recommendation of search and navigation. They present a personalization
algorithm for recommendation in folksonomies which relies on hierarchical tag
clusters. Their basic recommendation framework is independent of the clustering method. They use a context-dependent variant of hierarchical agglomerative
clustering which takes into account the user’s current navigation context in cluster selection. We employ clustering in diﬀerent stage and utilize KNN graph to
calculate neighbors. A framework named Semantic Tag Clustering Search, which
is able to cope with the syntactic and semantic tag variations is proposed in [8].
In our work we do not consider the semantics of tags as it is an additional computation step. We show, that even without the consideration of semantics the
performance of a recommender is reasonable.

3

Preliminaries

We will review the preliminaries based on our previous work in this section
[19,23,20,24].
3.1

Folksonomy

The folksonomy is a three-dimensional data model of social tagging behaviors of
users on various documents. It reveals the mutual relationships between these
three-fold entities, i.e. user, document and tag. A folksonomy F according to
[14] is a tuple F = (U, T, D, A), where U is a set of users, T is a set of tags,
D is a set of web documents, and A ⊆ U × T × D is a set of annotations.
The activity in folksonomy is tijk ⊆ {(ui , dj , tk ) : ui ∈ U, dj ∈ D, tk ∈ T }, where
U = {U1 , U2 , · · · , UM } is the set of users, D = {D1 , D2 , · · · , DN } is the set of
documents, and T = {T1 , T2 , · · · , TK } is the set of tags. tijk = 1 if there is an
annotation (ui , dj , tk ); otherwise tijk = 0.
3.2

User Profile and Document Profile

The constructed folksonomy data model is actually a three-dimensional array
(or called tripartite hyper-graph). In real applications, we often decompose the
tagging data into two two-dimensional matrices, i.e., user proﬁle and document
proﬁle. User proﬁle can be used to store the descriptive tags of the user’s characteristics and preferences. The document proﬁle is represented by the tags generated by the group of users tagging the documents. In the context of social
tagging systems, the user proﬁles and document proﬁles thus are expected to be
represented by the representative tags. Therefore the process of user and document proﬁling is to capture the signiﬁcant tags from a large volume of tagging
data in a social collaborative environment. We will utilize them based on our
previous work [19].

KNN-Based Clustering for Improving Social Recommender Systems

3.3

119

Similarity Measure for Tags

The similarity is a quantity that reﬂects the strength of relationship between
two objects. In the previous part, each user proﬁle and document proﬁle can be
represented by the pair of tags and frequencies. In this manner, we perform the
transformation of above two matrices and utilize the cosine function to measure
the similarity between two tags. Its value ranges from 0 to 1, the higher value of
the similarity, the more similar the objects are [19].

4

Tag Clustering with Tag Neighbors

The basic idea of the proposed approach is to utilize the tag neighbors to extend
the users’ or documents’ proﬁles which are represented by the tags. In the tag
similarity matrix, each tag has diﬀerent similarity weights with other tags, we
assume the higher the weight, the more similar the tags are to the target tag.
To realize the task of expanding the tag, the major diﬃculty is how to deﬁne
the tag neighbors and how to locate them from the total tags. Here we adapt
a statistical deﬁnition of tag neighbor - the tags which are co-occurred most
frequently or they have the higher similarity weight in the similarity matrices to
the target tag, are the neighbors for each other. So the N tags according to the
top-N similarity weight can be deﬁned as the tag neighbors of an individual tag.
After such steps, each tag will have an additional neighboring tag set which will
help to improve the quantity of the tag expression.
Given an arbitrary tag T, its neighboring tags are deﬁned as:
N b(Ti ) = {Tj : Tj ∈ T opN {SM (Ti, Tj )}}
where T opN {SM (Ti, Tj )} is the tags which possess the top-N highest similarity
values to tag Ti .
However, not all social tagging systems proposed so far maintain high quality
of tag data. Even when the tag elements in tag expression can be expanded, such
tag expression could contain a lot of inappropriate tags. We particularly call them
as “noisy tags”. From the previous paper, we can get the basic idea of ﬁltering:
the neighboring tags from the same tag cluster contribute collaboratively to
speciﬁc topic, being kept as the appropriate tag neighbors for tag expression
expansion. The next processing step is to ﬁlter out the noisy tags according to
the discovered tag clusters. Each tag has an expanded set of tag neighbors, which
can belong to diﬀerent clusters. To ensure all neighboring tags are from the same
tag cluster, each tag in the expanded neighbors set will be compared with all
the tags from the tag cluster where the target tag is assigned. If the expanded
neighbor appears in the same cluster, it can be considered as the appropriate
neighbor of the tag, making it kept in the expanded tag set; otherwise, it should
be ﬁltered out. After such steps above, the left elements could be deﬁned as
the tag neighbors for the target tag, and the quality of the tag neighbor will
be accordingly improved. Also in such way the density in the integrated taguser-document matrix could be increased substantially. For the detailed process,
please refer to our previous work[20].

120

5

R. Pan, P. Dolog, and G. Xu

Tag Neighbors Filtering Based on KNN Algorithm

In this section we use the k -nearest neighbor algorithm Algorithm to pre-process
the tagging data for clustering to optimize the tag clustering algorithm.
The k -nearest neighbor algorithm (KNN) is a method for classifying objects
based on closest training examples in the feature space. It’s the Memory-based
without the training model where the function is only approximated locally and
all computation is deferred until classiﬁcation. The k -nearest neighbor algorithm
can generate the maximum likelihood estimation of the class posterior probabilities.The basic idea is: an object is classiﬁed by a majority vote of its neighbors,
with the object being assigned to the class most common amongst its k nearest
neighbors (k is a positive integer, typically small). If k = 1, then the object is
simply assigned to the class of its nearest neighbor. The best choice of k depends upon the data; the larger values of k reduce the eﬀect of noise on the
classiﬁcation, but make boundaries between classes less distinct[9,21].
According to subsection of Similarity Measure for Tags, the annotation between the user and resource can create a similarity matrix S to indicate the
aﬃnity of tags. The k -nearest neighbors for each tag can be created according
to S, then we can construct a KNN directed graph G based on the relationships
among the tags. So we can deﬁne the KNN directed graph G and its adjacency
matrix A as [23]:
G =< V, E > , where V is the node set of tags and E is the directed edge
set between each pair of tags, < p, q >∈ E denotes that tag q is one of the
KNN-neighbors of tag p.
The adjacency matrix is deﬁned as A, where A(p, q) = 1, if the directed arch
< p, q > exists, and A(p, q) = 0, otherwise.
Especially in the context of social tagging systems, we know each tag can be
expressed as a column vector of user proﬁle and document proﬁle, i.e., Ti = T Ui
and Ti = T Di . Then we stack up T Ui and T Di and form a new tag vector over
users and documents, Ti = T Ui ∪ T Di . At last, we employ KNN algorithm and
spectral clustering on the integrated tag vectors to get tag clusters. The pseudo
codes of the spectral tag clustering are listed in Algorithm 1.

6

Experimental Evaluations

We evaluate our approach through extensive experiments. The experiments are
conducted on the real world dataset: “MovieLens” dataset and the “M-Eco system” datasets. Our experiments focus on results conducted on the implemented
tag-based recommender system with tag neighbor expansion. The goal of such
experiments is to show how can we reduce sparsity of data and improve the
accuracy of recommendation tag based recommenders to reduce sparsity.
6.1

Dataset and Experimental Setup

The “MovieLens” dataset is provided by GroupLens. We utilize part of the
“MovieLens” dataset, which contains tags provided by users on movies. All users

KNN-Based Clustering for Improving Social Recommender Systems

121

Algorithm 1. KNN Spectral Tag Clustering
Input: The tag-user matrix and tag-document matrix, i.e.,
T U = {T Ui , i = 1, · · · , K}, T D = {T Di , i = 1, · · · , K}
Output: A list of top-N documents for the candidate user and a set of C tag
clusters T C = {T Cc , c = 1, · · · , C} such that the cut of C -partitioning
of the bipartite graph is minimized
1
2
3
4
5
6
7
8
9
10
11
12
13

14

Pre-process the tag-user-document matrix by stacking up the above two matrix,
Ti = T Ui ∪ T Di according to the a Folksonomy model;
Obtain the matrixes of user proﬁles or document proﬁles by making a
two-dimensional projection of the URT matrix;
Calculate all of cosine similarities to score the resemblance of resources to those
found in the user proﬁle and document proﬁle;
Construct a KNN directed graph;
Calculate the diagonal matrices D of T ;
Form a new matrix RT = D−1/2 T D−1/2 ;
Perform SVD (Singular value decomposition) operation on RT , and obtain k
singular vectors Ls to create a new projection matrix;
Execute a clustering algorithm and return clusters of tags in the integrated
vectors: T C = {T Cc , c = 1, · · · C};
Collect the top-N tags according to the highest values in the similarity matrix
for each tag;
Partition tags into diﬀerent clusters by spectral clustering algorithm;
Check all the tag neighbors generated in the previous steps whether they belong
to the same cluster with the original tag, and ﬁlter out noisy tag neighbors;
Update the tag vectors of user proﬁles and document proﬁles with tag neighbors;
Calculate the similarity between the candidate user’s tag vector and the
document tag vector, and rank the documents according to the similarity values
in a descending order;
Select the top-N documents with the ﬁrst N highest similarities as the
recommendions to the candidate user.

selected had rated at least 50 movies with rank from 4. We also involve a part of
the “M-Eco system” dataset, which provided from M-Eco project. It has users
for each medical condition mapped with tags generated for documents matching
to a particular medical condition. The “MovieLens” data includes 97 users, 1201
documents and 1712 tags. And the “M-Eco system” has 35 diﬀerent users, 101
documents and 619 tags with 2017 tagging annotations. Following the common
protocol in information retrieval domain which chooses 20-30% data for the
testing data, we use 75% of data as the training data and the remaining 25% as
the testing data to evaluate our approach.
6.2

Precision Evaluation

We calculate the precision with the same process in our previous work [20].
In the experiment, we examine the diﬀerent algorithms’ precision of recommending documents to the individual user. We calculate the precision in the

122

R. Pan, P. Dolog, and G. Xu

following steps: The recommendation can be created by various approaches by
ranking the similarity values. Each one can generate the top-N documents which
will be recommended to the user. Then we can compare them with the existing
documents in the test data. If there k documents appear out of the N recommended documents in the test data, the number of test documents existing for
each user is Ki , the precision for the individual user is deﬁned as t :
t=

Ki
× 100%
N

We assume that the existed documents are the preference of each user. The
high value of t the better recommendation we got. There are 307 existing documents in the “MovieLens” data and 36 existing documents in the “M-Eco” data
for testing. We calculated the precision for each algorithms to evaluate the approach. So that we discussed diﬀerent strategies of our approach against existing
approaches. The “Pure Tag Approach” is to calculate the directly similarity between the user proﬁle and the document proﬁle in the tag vector, and the system
will recommend the N documents to the user according to the top-N similarity values. “TagNeighbor Approach” is to calculate such similarity based on the
naive tag neighbor expansion by collaborative ﬁltering approach. “TagNeighbor
with Clustering Approach” is to ﬁlter the noisy tags based on the “TagNeighbor
Approach”. The “Collaborative Filtering Approach” is set as the benchmark of
the experiment. Our proposed approach “KNN-based Clustering TagNeighbor”
is to utilize the KNN algorithm combining with clustering to ﬁlter out the noisy
tags to improve the accuracy of tag neighbors to get the better performance of
recommendation.

Fig. 1. Precision for conditional recommended documents in “MovieLens”

KNN-Based Clustering for Improving Social Recommender Systems

123

Fig. 2. Precision for conditional recommended documents in “M-Eco”

We average the whole precision for all of the users, and compare the recommendations from top 1 to 40 documents to the users in the experiments. We denote these ﬁve approaches as “KNN-based Clustering TagNeighbor”, “TagNeighbor with Clustering”, “TagNeighbor”, “Collaborative Filtering” and the “Pure
Tag”. For the “M-Eco” dataset, the average precision of “KNN-based Clustering
TagNeighbor” is 73.9, “TagNeighbor with Clustering” is 69.1, the “TagNeighbor”
is 65.3, the “Collaborative Filtering” is 51.9, and the “Pure Tag” is 29.4. For the
“Moivelens system” dateset, the average precision of “KNN-based Clustering TagNeighbor” is 87.1, “TagNeighbor with Clustering” is 82.4, the “TagNeighbor” is
73.3, the “Collaborative Filtering” is 62.1, and the “Pure Tag” is 37.9. The precision comparisons of ﬁve approaches for the top 40 recommendations in two real
world dataset are shown in Fig. 1 and Fig. 2. In summary, the evidence demonstrates the advantage of our approach in recommendations.

7

Conclusion

Social annotations systems enable users to annotate resources with tags. Under
social tagging systems, a typical Web2.0 application, users label digital data
sources by using tags which are freely chosen textual descriptions. Tags are used
as one kind of speciﬁc lexical information that is user-generated metadata with
uncontrolled vocabulary, plays a crucial role in such social collaborative tagging
systems.
In this work, We aim on the major problem of most social tagging systems
resulting from the severe diﬃculty of ambiguity, redundancy and less semantic
nature of tags, we have proposed a method to utilize the KNN algorithm combining with the clustering approach for expanding tag neighbors and improve

124

R. Pan, P. Dolog, and G. Xu

the accuracy of recommendation. We improved our previous work by involving
the KNN algorithm. We compared the preliminary experiments on the realworld dataset: “MovieLens” and “M-Eco” to evaluate the performance of our
proposed approach. The experimentally result demonstrates that our approach
could considerably improve the performance of recommendations.

References
1. Bayyapu, K.R., Dolog, P.: Tag and Neighbour Based Recommender System for
Medical Events. In: Proceedings of MEDEX 2010: The First International Workshop on Web Science and Information Exchange in the Medical Web Colocated
with WWW 2010 Conference (2010)
2. Boratto, L., Carta, S., Ratc, V.E.: A robust automated tag clustering technique.
In: Proceedings of the 10th International Proceedings on E-Commerce and Web
Technologies, pp. 324–335 (2009)
3. Budura, A., Michel, S., Cudré-Mauroux, P., Aberer, K.: Neighborhood-Based Tag
Prediction. In: Aroyo, L., Traverso, P., Ciravegna, F., Cimiano, P., Heath, T.,
Hyvönen, E., Mizoguchi, R., Oren, E., Sabou, M., Simperl, E. (eds.) ESWC 2009.
LNCS, vol. 5554, pp. 608–622. Springer, Heidelberg (2009)
4. Burke, R.: Hybrid recommender systems: Survey and experiments. In: User Modeling and User Adapted Interaction, pp. 331–370. Springer, Heidelberg (2002)
5. Cao, L., Gorodetsky, V., Mitkas, P.: Agent mining: The synergy of agents and data
mining. IEEE Intelligent Systems 24(3), 64–72 (2009)
6. Cao, L., Luo, D., Zhang, C.: Ubiquitous Intelligence in Agent Mining. In: Cao, L.,
Gorodetsky, V., Liu, J., Weiss, G., Yu, P.S. (eds.) ADMI 2009. LNCS, vol. 5680,
pp. 23–35. Springer, Heidelberg (2009)
7. Chen, H., Dumais, S.: Bringing order to the web: automatically categorizing search
results. In: CHI 2000: Proceedings of the SIGCHI Conference on Human Factors
in Computing Systems, pp. 145–152. ACM, New York (2000)
8. van Dam, J.-W., Vandic, D., Hogenboom, F., Frasincar, F.: Searching and browsing
tag spaces using the semantic tag clustering search framework. In: Proceedings of
the 2010 IEEE Fourth International Conference on Semantic Computing, ICSC
2010, pp. 436–439. IEEE Computer Society, Washington, DC (2010)
9. Dasarathy, B.V.: Nearest Neighbor (NN) Norms: NN Pattern Classiﬁcation Techniques (1991)
10. Di Matteo, N.R., Peroni, S., Tamburini, F., Vitali, F.: A parametric architecture for
tags clustering in folksonomic search engines. In: Proceedings of the 2009 Ninth
International Conference on Intelligent Systems Design and Applications, ISDA
2009, pp. 279–282. IEEE Computer Society, Washington, DC (2009)
11. Gemmell, J., Schimoler, T., Mobasher, B., Burke, R.: Tag-Based Resource Recommendation in Social Annotation Applications. In: Konstan, J.A., Conejo, R.,
Marzo, J.L., Oliver, N. (eds.) UMAP 2011. LNCS, vol. 6787, pp. 111–122. Springer,
Heidelberg (2011)
12. Guan, Z., Wang, C., Bu, J., Chen, C., Yang, K., Cai, D., He, X.: Document recommendation in social tagging services. In: Proceedings of the 19th International
Conference on World Wide Web, WWW 2010, pp. 391–400. ACM, New York (2010)
13. Hayes, C., Avesani, P.: Using tags and clustering to identify topic-relevant blogs.
In: International Conference on Weblogs and Social Media (March 2007)

KNN-Based Clustering for Improving Social Recommender Systems

125

14. Hotho, A., Jäschke, R., Schmitz, C., Stumme, G.: Folkrank: A ranking algorithm
for folksonomies. In: LWA, pp. 111–114 (2006)
15. Zhou, J., Nie, X., Qin, L., Zhu, J.: Journal of Computers
16. Gemmell, J., Schimoler, T., Mobasher, B., Burke, R.: Resource Recommendation
in Collaborative Tagging Applications. In: Buccafurri, F., Semeraro, G. (eds.) ECWeb 2010. LNBIP, vol. 61, pp. 1–12. Springer, Heidelberg (2010)
17. Moemeng, C., Wang, C., Cao, L.: Obtaining an Optimal MAS Conﬁguration
for Agent-Enhanced Mining Using Constraint Optimization. In: Cao, L., Bazzan,
A.L.C., Symeonidis, A.L., Gorodetsky, V.I., Weiss, G., Yu, P.S. (eds.) ADMI 2011.
LNCS, vol. 7103, pp. 46–57. Springer, Heidelberg (2012)
18. Moemeng, C., Zhu, X., Cao, L.: Integrating Workﬂow into Agent-Based Distributed
Data Mining Systems. In: Cao, L., Bazzan, A.L.C., Gorodetsky, V., Mitkas, P.A.,
Weiss, G., Yu, P.S. (eds.) ADMI 2010. LNCS, vol. 5980, pp. 4–15. Springer, Heidelberg (2010)
19. Pan, R., Xu, G., Dolog, P.: User and Document Group Approach of Clustering
in Tagging Systems. In: Proceeding of the 18th Intl. Workshop on Personalization
and Recommendation on the Web and Beyond, LWA 2010 (2010)
20. Pan, R., Xu, G., Dolog, P.: Improving recommendations in tag-based systems with
spectral clustering of tag neighbors. In: Proceedings of The 3rd FTRA International
Conference on Computer Science and its Applications (CSA 2011): Computer Science and Convergence. LNEE, vol. 114, Part I, pp. 355–364. Springer, Heidelberg
(2011)
21. Shakhnarovish, D., Indyk: Nearest-Neighbor Methods in Learning and Vision. The
MIT Press (2005)
22. Shepitsen, A., Gemmell, J., Mobasher, B., Burke, R.: Personalized recommendation
in social tagging systems using hierarchical clustering. In: Proceedings of the 2008
ACM Conference on Recommender Systems, RecSys 2008, pp. 259–266. ACM, New
York (2008)
23. Xu, G., Zong, Y., Pan, R., Dolog, P., Jin, P.: On Kernel Information Propagation for Tag Clustering in Social Annotation Systems. In: König, A., Dengel, A.,
Hinkelmann, K., Kise, K., Howlett, R.J., Jain, L.C. (eds.) KES 2011, Part II.
LNCS, vol. 6882, pp. 505–514. Springer, Heidelberg (2011)
24. Zong, Y., Xu, G., Jin, P., Zhang, Y., Chen, E., Pan, R.: APPECT: An Approximate
Backbone-Based Clustering Algorithm for Tags. In: Tang, J., King, I., Chen, L.,
Wang, J. (eds.) ADMA 2011, Part I. LNCS, vol. 7120, pp. 175–189. Springer,
Heidelberg (2011)

IEEE TRANSACTIONS ON RELIABILITY, VOL. 62, NO. 2, JUNE 2013

527

A Novel Approach to Optimal Accelerated Life Test
Planning With Interval Censoring
Tao Yang and Rong Pan, Member, IEEE

Abstract—Accelerated life testing (ALT) is widely used in
industry to obtain the lifetime estimate of a product which is
expected to last years or even decades. It is important to find an
effective experimental design of ALT with the consideration of certain optimality criteria. In this paper, we discuss a new approach
to designing ALT test plans when readout data (i.e., interval
censoring) are collected. We utilize the proportional hazard (PH)
model for a failure time distribution, and formulate a generalized
linear model (GLM) for censored data. The optimal design is
obtained such that the prediction variance of the expected product
lifetime at the product’s use condition is minimized.

probability of failure of the
inspection interval

conditional probability of the
test unit failed in
the
inspection interval given that it survived at

Index Terms—Accelerated life testing, generalized linear model,
-optimal
proportional hazard model, sensitivity analysis,
design.

Accelerated Life Testing

GLM

Generalized Linear Model

PH

Proportional Hazard

AFT

Accelerated Failure Time

IWLS

Iterative Weighted Lease Square

ES

Equal Spaced

EP

Equal Probability

test

an indicator variable for whether or not the
unit has survived by the time

test

baseline reliability function
linear predictor for the

test unit

I. INTRODUCTION
A. Background and Motivation

N

NOTATION
design matrix
an experimental design
sample size
index of test units
index of time intervals
test condition of the

an indicator variable for whether or not the
unit is failed in the
inspection interval

reliability function

ACRONYMS
ALT

test unit in the

test unit

use condition
weight matrix
inspection time

Manuscript received February 13, 2012; revised September 08, 2012; accepted November 26, 2012. Date of publication April 12, 2013; date of current
version May 29, 2013. Associate Editor: R. H. Yeh.
The authors are with the School of Computing, Informatics, and Decision
Systems Engineering, Arizona State University, Tempe, AZ 85287 USA
(e-mail: rong.pan@asu.edu).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TR.2013.2257053

EW products will be tested to check their functionality,
safety, and reliability before they are released to the
market. However, most products these days can last years or
even decades, so regular life testing is not an option. Accelerated life testing (ALT) is introduced to shorten the product’s
lifetime. In ALT, certain stress variables, such as temperature,
humidity, and voltage, are set to the higher-than-normal stress
level so that experimenters can expect more failures from test
units within a limited testing period. Readout data are very
common in ALTs. Due to measurement availability, costs, and
other constraints, experimenters are often unable to monitor test
units continuously. Instead, they often inspect them periodically according to an inspection plan. Therefore, product failure
times are interval censored; i.e., we know a test unit failed in
a certain time interval, but we do not know the exact failure
time. The ALT with interval censoring is easy to implement,
but interval censoring is a loss of information, affecting the test
planning and data analysis. This loss may reduce the accuracy
of the product lifetime prediction, and complicate the task
of finding the expected information matrix for constructing
optimal test plans.
The conventional way of developing an optimal ALT plan
is to formulate the likelihood function, and then derive the expected information matrix for test planning. When interval censoring is applied, the likelihood function of an interval censored
failure time is the probability of observing a failure between the
lower and upper bounds of the time interval. This result causes

0018-9529/$31.00 © 2013 IEEE

528

the total likelihood function to be complicated, and its expected
information matrix hard to obtain.
In this paper, we propose to use the proportional hazard (PH)
model for a failure time distribution with stress variables (experimental factors). In general, the PH model is semi-parametric;
thus, it is more flexible than the traditional failure time regression model. When the baseline hazard function is fully specified, this model may include a wide range of distributions, such
as a Weibull distribution or a gamma distribution, which are
commonly used in reliability data analysis. Given the proportional hazard rate form of the PH model, we can formulate the
total likelihood function of the censored data using a generalized linear model (GLM) formulation. GLM was developed by
Nelder and Wedderburn [1], and its details can be found in McCullagh and Nelder [2], and in Myers et al. [3]. Typically, there
are three components in a GLM formulation:
• a distribution from the exponential family,
• a linear predictor, and
• a link function.
With GLM formulation, the information matrix can be easily
derived, and then it can assist experimenters to find the optimal
test plan, and to evaluate the optimal plan.
To plan an optimal ALT, some statistical efficiency criteria
need to be defined. For example, we may want to maximize the
determinant of the expected information matrix (as the overall
variance of model parameter estimators will be minimized); in
this case it is a D-optimal design. Other optimal criteria include
A-optimality, G-optimality, V-optimality, I-optimality, etc. (see
Myers et al. [4]).
In this paper, we are interested in the asymptotic variance of
the expected product lifetime at the product’s use condition. A
test plan that minimizes the product lifetime prediction variance
at its use condition is called the -optimal design in [5]. As the
-optimality minimizes prediction variance, this optimal plan
gives us more confidence in predicting a product’s lifetime at its
use condition.
In conclusion, compared with the conventional ALT optimal
plans which are generated under the accelerated failure time
(AFT) model, our PH model assumes that stress variables
accelerate the failure rate. However, it is well-known that
through re-parameterizations these two models are equivalent
for Weibull failure time distributions (and the exponential
distribution as a special case). The benefit of applying the PH
model is that we can formulate the problem using a generalized linear model (GLM), so the statistical inference of the
regression coefficients can be easily obtained. By using this
formulation, we are ready to plan an ALT with more than one
stress variable, and with interactions of stress variables, which
cannot be conveniently done by the conventional method.
B. Previous Work
The literature on optimal ALT designs is vast. In this section,
we mention some important developments in this field, particularly for a test with censoring, but it is not intended to be a
comprehensive review of ALT. For that, readers may refer to
Nelson [6], [7]. Nelson and Kielpinski [8] discussed the theory
of optimal ALT plans for estimating a simple linear relationship
between a stress factor and product lifetime that has a normal or

IEEE TRANSACTIONS ON RELIABILITY, VOL. 62, NO. 2, JUNE 2013

lognormal distribution, when censoring is considered. Nelson
and Meeker [9], and Meeker and Nelson [10] applied the maximum likelihood theory for designing optimal ALT plans while
assuming the product lifetime has a Weibull or smallest extreme value distribution. Tang et al. [11] presented the method
for finding optimal ALT plans for censored two-parameter exponential distributions. However, these papers only discussed
ALTs with a single stress factor. Escobar and Meeker [12] introduced ALT planning with censoring for two stress factors.
Park and Yum [13], and Sitter and Torsney [14] presented the
optimal test plans with two stress factors. In addition, Xu and
Fei [15], and Li and Fard [16] consider step-stress accelerated
life testing plans for two stress variables. To deal with interval
censoring, Finkelstein and Wolfe [17], and Finkelstein [18] developed a PH regression model for analyzing interval censored
data. However, we found very few papers directly addressing
optimal test plans with the consideration of readout data. Islam
and Ahmad [19], and Yum and Choi [20] developed optimal
designs under periodic inspection and type-I censoring. Seo and
Yum [21] extended the optimal ALT plans under intermittent inspection schemes. Tse and Ding and Yang [22] provided optimal
ALT designs under interval censoring with random removals. In
Ng and Chan and Balakrishnan [23], the ways of finding optimal
test plans for different optimal criteria by deriving the expected
Fisher information matrix were discussed in detail. Other extensions of ALT plans can be found in Pascual [24], [25], and Liu
and Qiu [26]. They derived plans for ALT with -independent
competing risks.
All previous work in optimal ALT designs follows the direct
approach of deriving the expected information matrix from the
likelihood function of failure times. Recently, Monroe et al.
[27], and Pan and Yang [28] proposed to use the generalized
linear model (GLM) approach to approximate the information
matrix for finding optimal ALT plans. Their methods have been
applied to ALT with type-I censoring. We will modify and
improve this approach in the paper to plan ALTs with interval
censoring. In Woods et al. [29], the reasons why exact (optimal)
design is more efficient than traditional factorial designs in
GLM experimental designs were discussed. Aitkin and Clayton
[30] showed how to fit the regression model with censored survival data while using exponential, Weibull, and extreme value
distributions in GLM. In addition, Barbosa and Colosimo and
Louzada-Neto [31] analyzed ALT results by using a piecewise
exponential distribution and the GLM approach. To see the
difference between the optimal experimental designs for GLM
and those for typical linear models, one may refer to Chipman
and Welch [32], in which the authors used the asymptotic covariance matrix to develop an analogous D-optimality criterion,
and made the comparison of GLM D-optimal design to linear
regression D-optimal design. Dror and Steinberg [33] proposed
a heuristic method, based on clustering a set of local optimal
designs, for constructing robust designs for multivariate GLMs,
and they also discussed sequential experimentation methods.
Moreover, Yang and Zhang and Huang [34] developed a new
approach to identifying optimal designs for multi-factor logistics and probit models under different optimal criteria.
In this paper, we will apply the proportional hazard model on
failure times to transform the problem of ALT planning with

YANG AND PAN: A NOVEL APPROACH TO OPTIMAL ACCELERATED LIFE TEST PLANNING WITH INTERVAL CENSORING

interval censoring to the problem of experimental design for
GLM.
II. METHODOLOGY
A. Optimality Criterion
First, we introduce the optimality criterion for finding the optimal experimental design in this paper. The benefits of applying
optimal experimental design principles on ALT planning had
been demonstrated in Monroe and Pan [35]. In this paper, we
consider the -optimality, which is to seek the smallest asymptotic variance of the failure time prediction at the product’s use
condition among all test plans.
Define a moment matrix as (Myers, et al. [3])
(1)
The design matrix

has the form

529

, where

. Furthermore, let
, and
. If a test unit has failed in the
interval, then its
failure time is
, for
. Let
be the
probability of failure of the
test unit in the
interval, and
let
be the conditional probability of the
test unit failed in
the
interval given that it survived at
, i.e.,

and

with
that

, and

. It is shown in [36]
(3)

..
.

..
.

..

.

..
.

where there are covariates, and each row in the design matrix
stands for a testing condition (a point in the experimental design
region).
For a linear model, the inverse of M contains the variance
and covariance values of regression coefficients. The asymptotic variance of the response prediction at a point inside the
design region is determined by
, where is
the vector of the covariates at this specific point. In ALT, we
are interested in the lifetime prediction under the product’s actual use condition, so it is the point
. However, notice that
this point typically locates outside of the experimental design
region, and extrapolation needs to be exercised to compute the
prediction variance.
Moreover, regression models for lifetime data are not linear
models. In particular, nonlinearity is introduced into the model
when failure time censoring occurs. Thus, the above formula
needs to be modified for the lifetime prediction variance estimation. If we can reformulate the lifetime regression problem
as a GLM problem, then the -optimality becomes the task of
solving the optimization

for
, with
.
We let
be an indicator variable for whether or not the
test unit is failed in the
interval, and let
be another indicator variable for whether or not the
test unit survives by
time , i.e.,
when
otherwise
and
when
otherwise,
then

for
.
The sample likelihood of

Substituting

by (3), the likelihood function becomes

(2)
where
is a weight matrix associated with the variance of the
response variable in the GLM formulation.
B. Model Derivation
In this section, we present the GLM approach to finding optimal designs of ALT with interval censoring. The GLM formulation of interval censored survival data was initially discussed
in [36]. We summarize the formulation with our notation and derive the adjusted weight matrix for optimal experimental design.
Suppose that all test units enter a test at time 0, and during
the course of the test they are inspected at times
until

is

(4)
As

, (4) becomes
(5)

The above likelihood function has the same likelihood function form as from -independent random variables
such that
. Because we have units and
intervals in a test, there are
corresponding binomial variables. Based on the definitions of
and , their sum will be

530

IEEE TRANSACTIONS ON RELIABILITY, VOL. 62, NO. 2, JUNE 2013

1 if the
test unit survives to time
, i.e.,

, and 0 if it fails before

when
otherwise.
Therefore, we have
(11)
(6)
Note that
(7)
When the proportional hazard model is applied, (7) can be
written as
(8)
where
is the baseline reliability function, and is a linear
predictor, i.e.,
. Taking the logarithm twice, we find
that

(9)
where
, and it is irrelevant to
stress factors. Therefore, if we apply the GLM formulation on
the binomial likelihood function, the link function is a complementary log-log link function.
The iterative weighted least square (IWLS) procedure is used
to find the maximum likelihood estimation of a GLM (Myers
and Montgomery and Vining [3]). The weight matrix
is a
diagonal matrix, and the diagonal elements can be derived by
finding the approximate variance of the linear predictor, which
is also a diagonal matrix formed as
(10)
where
, and is the variance of
. The
parameter
is the natural location parameter of the distributions from the exponential family. The
term is introduced
into the weight matrix because the log-log link function is not a
canonical link function for the binomial distribution. As
and
are random variables, we use their expectations to replace
these variables in . We derive the weight matrix from (10) as

because we use the exThe weight matrix here becomes
pected sample size of the binomial distribution,
from (6), to replace the actual sample size.
Now, based on -optimality, the prediction variance can be
written as
(12)
where is the design matrix, and
is the vector of the use
condition.
We apply the nonlinear optimization procedure in SAS software to find the -optimal design. Readers need to pay attention to the format of the design matrix . If we have test
units, and time intervals, then the dimension of
will be
, where the p is the number of stress factors used in
ALT. For a specific test unit, its testing conditions are the same
in different time intervals. In other words, the ALT only have
different test units, but because there are intervals, these
test units are treated as
virtual units. Now, we have developed a general method that can handle any lifetime distribution
with interval censoring.
C. Weibull and Exponential Distributions
Assuming a Weibull distribution for a product’s lifetime distribution, in this section we show the formulation of the adjusted
weight matrix for finding the optimal test plan, given an inspection plan of the test. The failure function of the Weibull distribution is

where , and are the shape, and scale parameters, respectively.
According to the PH model, the stress factors will affect only
through a log-linear function, i.e.,

where is a linear predictor, and
a test unit, we have

The reliability function is
(7),

are stress factors. Thus, for

; by placing it into

(13)

YANG AND PAN: A NOVEL APPROACH TO OPTIMAL ACCELERATED LIFE TEST PLANNING WITH INTERVAL CENSORING

Assume the shape parameter
have

is known. Let

; then we

531

TABLE I
-OPTIMAL DESIGN WITH TYPE-I CENSORING

,

TABLE II
-OPTIMAL DESIGN WITH TYPE-I CENSORING

,

(14)
With (11), (12), and (14), we can do the optimization and find
the -optimal test plan.
Now consider a special case of
, i.e., the failure time
distribution becomes an exponential distribution, then

Suppose all the time intervals have the same length,
(13) can be written as

. Then

(15)
and the

matrix can be derived as

(16)
III. EXAMPLES, COMPARISON, AND SENSITIVITY ANALYSIS
We first use a simple example from Tse and Ding and Yang
[22], and compare our -optimal test plan from the GLM approach to their equal spaced (ES) and equal probability (EP)
test plans. We then demonstrate the capabilities of our method
by applying it to a multiple-stress factor ALT and performing
the sensitivity study of the optimal plan to model parameter
assumptions.
A. Comparison With the Traditional Method
Tse and Ding and Yang [22] used the second derivative of the
total likelihood function to obtain the design information matrix, and to develop the equal spaced (ES) and equal probability
(EP) deigns for -optimal test plans. Only one stress factor was
considered in [22], and the lifetime distribution was given as a
Weibull distribution with the shape parameter being 2.
To make comparisons between the optimal design generated
from a GLM approach and the optimal designs under ES and
EP as in [22], we use the same distribution assumption from
their paper and apply the ES inspection scheme to obtain the
optimal design. The comparison is given in Tables I and II. In
these two tables, , and
respectively stand for the percentile
of failure at the use condition, and the high stress condition; and
is the number of intervals in the test. Notice here that, in this
comparison, the coded stress variable
of the use condition,
and the high stress level condition, are set as 0, and 1 to match
the terms in [22].
One can see that the results from our GLM method, and Tse’s
ES and EP methods, are very similar. The small differences in

test plans come from the computation rounding errors. However, when there are more than one stress factors, the traditional method requires one to reformulate the information matrix, which becomes very complicated, while the GLM approach
can easily address the problem by adding new stress variables
in the linear predictor. Moreover, the interaction effect of stress
factors can be investigated by the GLM approach without further difficulty.
B. An example With Two Stress Factors
Suppose an electronic part has an exponential lifetime distribution, and its lifetime is affected by temperature and humidity.
The use condition of this electronic part is set to 30 , and 25%
humidity. Under the ALT test, the testing temperature ranges
from 60
to 110 , and the testing relative humidity level
ranges from 60% to 90%. The natural stress of temperature, and
humidity can be presented as
with temperature
in degrees Kelvin, and
with relative humidity as
a percentage. Following the notation used in Monroe et al. [5],
we let the design space of this experimentation be a unit square,
and the use condition be located at the first quadrant. The transformation is given by
, where
, and
are the high, and low level of stress j respectively.
Thus, the highest stress level is transformed to (0, 0), and the
lowest stress level is transformed to (1, 1). The linear predictor
is written below, while we note that an interaction effect of temperature and humidity is considered in this model.
(17)
Let the total testing time be 30 hours, and set equal-length
inspection intervals during the test, where
, 5, 10,
and 30. We program the optimal design for GLM in SAS,
based on the method from Atkinson and Donev and Tobias
[37]. Tables III to VI show the optimal test plans for these
experimentations. In these tables, four ALT testing conditions

532

IEEE TRANSACTIONS ON RELIABILITY, VOL. 62, NO. 2, JUNE 2013

TABLE III
-OPTIMAL DESIGN WITH 2 INTERVALS

TABLE IV
-OPTIMAL DESIGN WITH 5 INTERVALS

Fig. 1. Contour plot of

-optimal design with 2 intervals.

Fig. 2. Contour plot of

-optimal design with 10 intervals.

TABLE V
-OPTIMAL DESIGN WITH 10 INTERVALS

TABLE VI
-OPTIMAL DESIGN WITH 30 INTERVALS

are selected, and their temperature and humidity settings are
given. The allocation column lists the number of test units to
be allocated at each testing condition, with the assumption of
100 test units in total. The last two columns are the failure
probability at the end of the testing period, and the lifetime
prediction variance at the use condition. Fig. 1 to Fig. 4 provide
the contour plots of prediction variance under these optimal test
plans. The unit square region in the lower-left corner of each
graph is the experimental design region. The selected testing
conditions are marked by circles, and the circle diameter is

corresponding to the sample size allocation at such condition.
The use condition is marked by a rectangle, which is located
outside of the experimental design region, and the locations
with equal prediction variance are outlined by contour lines.
From these tables and figures, one can see that, with increasing number of inspection intervals, the lifetime prediction
variance at the use condition is decreasing. This result is expected, as more intervals means that more precise information
of failure times can be obtained. Notice that the number of test

YANG AND PAN: A NOVEL APPROACH TO OPTIMAL ACCELERATED LIFE TEST PLANNING WITH INTERVAL CENSORING

533

TABLE VII
-OPTIMAL DESIGN WITH TYPE-I CENSORING

Fig. 3. Contour plot of

-optimal design with 10 intervals.

Also, fewer intervals (or longer interval lengths) causes the
problem to become more nonlinear, thus the highest testing
stress condition may not be located at the low-left corner of the
design region as it often appears on the optimal experimental
design for linear models. In fact, the actual number of testing
conditions found by our algorithm are more than four; however,
to have an easier implementation of any optimal test plan, we
round up the coded test location coordinates to 0.001, which
aggregates these locations to four distinct points. Moreover, in
all of these plans, one can see that most test units are allocated at
the upper-right corner of the design region, which corresponds
to the lowest testing stress level. This happens because 1) it
is the closest point in the design region to the use condition
point, so more failure information at this point will help reduce
the prediction variance at the use condition; and 2) as fewer
failures are expected at a low stress level, we need to increase
the sample size.
Now we compare the interval censoring case with a right
censoring case with the same model assumption, as shown in
Table VII. One can see that the optimal test plan for interval censoring becomes closer to the optimal plan for right censoring as
the number of intervals increases. Actually, when we run an extreme case with 120 intervals during a 30 hour testing period, the
optimal test plan is exactly the same as that of a right censoring
case. It validates the test plan generated for interval censoring.
C. Sensitivity Study

Fig. 4. Contour plot of

-optimal design with 30 intervals.

units being allocated at the location of the lowest combination
in coded variables and (or, the highest stress level) shows
a decreasing trend when the number of inspection intervals
increases, i.e., 31 at (0.280, 0.097), 21 at (0.124, 0.000), 15
at (0.000, 0.000), and 12 at (0.000, 0.000) in Tables III to VI,
respectively. This trend happens because, with fewer intervals,
readout data contain less failure time information, and correspondingly, less test units will be allocated at higher stress
levels, and more units will be allocated at lower stress levels.

The linear predictor in (17) in our model plays a very important role in determining the optimal test plan. The coefficients
specified in the linear predictor are from previous experimental
results and engineering experience. However, the true model
may not be the same as the model pre-specified; therefore, a
study of the sensitivity of the optimal test plan to a misspecified
model is necessary. Suppose the original coefficient values in
(17) are used for finding an optimal test plan. However, the real
values of these coefficients are not the same as those in (17). Assuming that the real value of each stress’s coefficient may vary
20% from the used value, we have 9 combinations of these coefficient values, or 9 true linear predictor models. We will compare the performance of the assumed optimal test plan to the true
optimal plan, and discuss the impact of the misspecified model.
In this study, we let the number of inspection intervals
,
and the shape parameter
. The results are presented in
Table VIII, where the column of ideal PV gives the lifetime prediction variance if the right model is specified, and the column

534

IEEE TRANSACTIONS ON RELIABILITY, VOL. 62, NO. 2, JUNE 2013

TABLE VIII
PV OF DESIGNS WITH DIFFERENT COMBINATION FOR STRESS COEFFICIENT

of actual PV gives the prediction variance when the predictor
model is misspecified.
In Table VIII, the ideal PV is always smaller than the actual
PV because the ideal PV is calculated from the true optimal test
plan derived from the true predictor function, while the actual
PV is calculated from the assumed optimal test plan from the
misspecified predictor function. One can see that their differences are relatively small ( 8%) in 6 out of 8 cases, which indicates that, under these cases, even if the model is misspecified
the resulted test plan still gives a robust performance. However,
for the cases that the coefficient of
changes in the positive
direction when the coefficient of
is kept the same or moving
to the same direction, the PV differences are larger than 10%.
This result can be explained from the predictor function, in (17),
as one can see that
has a much larger influence on . The result implies that, at the test planning stage, we should pay more
attention to the coefficient of temperature, and specify it as accurately as possible.
The above example shows how the wrongly estimated coefficients in the linear predictor will affect ALT results. Therefore, a reliability engineer may prefer a test plan that is robust to a group of possible models, instead of a single model.
It is common that, at the test planning stage, the engineer assumes that the coefficients used in the predictor function may
take values from an interval with a certain distribution, or it
can be discretized to a set of values using probability weights.
For example, let the coefficient of
be chosen from 4.486,
4.286, 4.086, 3.886, and 3.686 with the probabilities of
0.1, 0.2, 0.4, 0.2, and 0.1, respectively; and the coefficient of
be chosen from 1.876, 1.676, 1.476, 1.276, and 1.076
with the probabilities of 0.1, 0.2, 0.4, 0.2, and 0.1 respectively.
Then, we can seek a test plan that can minimize the average prediction variance over the range of the coefficient values. We call
it a weighted average -optimal test plan. The optimal criterion
of the weighted average -optimal design can be written as
(18)
where
and
are the index of the available values for the
coefficients, and is the prior probability for each coefficient
combination.
Continuing our example, we compute the weighted average
prediction variance based on the 25 coefficient combinations.

WEIGHTED AVERAGE

Fig. 5. Weighted average

TABLE IX
-OPTIMAL DESIGN WITH 5 INTERVALS

-optimal design with 5 intervals.

Also, assume that the shape parameter is 1, and there are 5 inspection intervals in the test. The weighted average -optimal
test plan is given in Table IX, and the contour plot is in Fig. 5.
Suppose the coefficient values ( 4.086, 1.476) are the true
values; then given the weighted average -optimal plan from
Table IX, the real prediction variance at the use condition is
8.356, which is close to the predicted value, 8.59.

YANG AND PAN: A NOVEL APPROACH TO OPTIMAL ACCELERATED LIFE TEST PLANNING WITH INTERVAL CENSORING

TABLE X
COMPARISONS AMONG WEIGHTED AVERAGE

In addition, the comparisons among the weighted average
-optimal plans with different probability distributions are presented in Table X. With the same discretizing schemes for
and as above, we compare plans for three different weighting
schemes, equal weighting (0.2, 0.2, 0.2, 0.2, 0.2), symmetric
weighting (0.1, 0.2, 0.4, 0.2, 0.1), and concentrated weighting,
or fixed coefficient value, (0, 0, 1, 0, 0), respectively. The last
case is assumed using the true coefficient values. We also vary
the number of intervals from 2 to 10. From this table, we can
see that Case B, which has larger weight on the true coefficient values, provides a better test plan than Case A, where
all possible coefficient combinations have the equal weight. In
other words, when the prior probabilities of these coefficients
are more accurate, the test plan will be closer to the true optimal
plan. At the same time, when the number of inspection intervals
increases, the prediction variance will decrease monotonically.
IV. CONCLUSION
In this paper, we develop a GLM approach to constructing
optimal ALT test plans when the failure times of test units are
expected to be interval censored. The optimal criterion is selected to minimize the prediction variance of the product’s expected lifetime at its use condition. A PH regression model is
assumed for the failure time distribution, as it allows the GLM
formulation to be derived from the total likelihood function of
readout data. This assumption indeed encompasses a wide range
of failure time distributions, with Weibull and exponential distributions as two special cases. Comparing with the conventional approach to ALT test planning, our GLM approach has

535

-OPTIMAL TEST PLANS

the advantages on both the ease of computation and the ability
to handle more complex models, such as a model with more than
one stress factor and with the interaction effect of two stress factors, or the sensitivity study of a test plan to its model parameter
specification, etc. These models have been demonstrated in the
examples above.
REFERENCES
[1] J. A. Nelder and R. W. M. Wedderburn, “Generalized linear models,”
J. Royal Statist. Soc. Series A (General), vol. 135, no. 3, pp. 370–384,
1972.
[2] P. McCullagh and J. A. Nelder, Generalized Linear Models, 2nd ed.
Boca Raton, FL, USA: CRC, 1989.
[3] R. H. Myers, D. C. Montgomery, and G. G. Vining, Generalized Linear
Models: With Applications in Engineering and the Sciences. Wiley Series in Probability and Statistics, 2nd ed. Hoboken, NJ, USA: Wiley,
2010, c2010.
[4] R. H. Myers, D. C. Montgomery, and C. M. Anderson-Cook, Response
Surface Methodology: Process and Product Optimization Using Designed Experiments, 3rd ed. Hoboken, NJ, USA: Wiley, 2009.
[5] E. M. Monroe, R. Pan, C. Anderson-Cook, D. C. Montgomery, and C.
M. Borror, “Sensitivity analysis of optimal designs for accelerated life
testing,” J. Quality Technol., vol. 42, no. 2, pp. 121–135, 2010.
[6] W. B. Nelson, “A bibliography of accelerated test plans,” IEEE Trans.
Rel., vol. 54, no. 2, pp. 194–197, Jun. 2005.
[7] W. B. Nelson, “A bibliography of accelerated test plans part ii—References,” IEEE Trans. Rel., vol. 54, no. 3, pp. 370–373, Sep. 2005.
[8] W. B. Nelson and T. J. Kielpinski, “Theory for optimum censored accelerated life tests for normal and lognormal life distributions,” Technometrics, vol. 18, no. 1, pp. 105–114, 1976.
[9] W. B. Nelson and W. Q. Meeker, “Theory for optimum accelerated
censored life tests for weibull and extreme value distributions,” Technometrics, vol. 20, no. 2, pp. 171–177, 1978.
[10] W. Q. Meeker and W. B. Nelson, “Optimum accelerated life-tests for
the weibull and extreme value distributions,” IEEE Trans. Rel., vol.
R-24, no. 4, pp. 321–332, Dec. 1975.

536

IEEE TRANSACTIONS ON RELIABILITY, VOL. 62, NO. 2, JUNE 2013

[11] L. Tang, T. Goh, Y. Sun, and H. Ong, “Planning accelerated life tests
for censored two-parameter exponential distributions,” Naval Res. Logist. (NRL), vol. 46, no. 2, pp. 169–186, 1999.
[12] L. A. Escobar, J. Meeker, and Q. William, “Planning accelerated life
tests with two or more experimental factors,” Technometrics, vol. 37,
no. 4, pp. 411–427, 1995.
[13] J.-W. Park and B.-J. Yum, “Optimal design of accelerated life tests with
two stresses,” Naval Res. Logist. (NRL), vol. 43, no. 6, pp. 863–884,
1996.
[14] R. R. Sitter and B. Torsney, “Optimal designs for binary response
experiments with two design variables,” Statistica Sinica, vol. 5, pp.
495–419, 1995.
[15] H.-Y. Xu and H.-L. Fei, “Planning step-stress accelerated life tests with
two experimental variables,” IEEE Trans. Rel., vol. 56, pp. 569–579,
Sept. 2007.
[16] C. Li and N. Fard, “Optimum bivariate step-stress accelerated life test
for censored data,” IEEE Trans. Rel., vol. 56, pp. 77–84, March 2007.
[17] D. M. Finkelstein and R. A. Wolfe, “A semiparametric model for regression analysis of interval-censored failure time data,” Biometrics,
vol. 41, no. 4, pp. 933–945, 1985.
[18] D. M. Finkelstein, “A proportional hazards model for interval-censored
failure time data,” Biometrics, vol. 42, no. 4, pp. 845–854, 1986.
[19] A. Islam and N. Ahmad, “Optimal design of accelerated life tests for the
weibull distribution under periodic inspection and type I censoring,”
Microelectron. Rel., vol. 34, no. 9, pp. 1459–1468, 1994.
[20] B.-J. Yum and S.-C. Choi, “Optimal design of accelerated life tests
under periodic inspection,” Naval Res. Logist. (NRL), vol. 36, no. 6,
pp. 779–795, 1989.
[21] S.-K. Seo and B.-J. Yum, “Accelerated life test plans under intermittent inspection and type-i censoring: The case of weibull failure distribution,” Naval Res. Logist. (NRL), vol. 38, no. 1, pp. 1–22, 1991.
[22] S.-K. Tse, C. Ding, and C. Yang, “Optimal accelerated life tests under
interval censoring with random removals: The case of weibull failure
distribution.,” Statist., vol. 42, no. 5, pp. 435–451, 2008.
[23] H. K. T. Ng, P. S. Chan, and N. Balakrishnan, “Optimal progressive
censoring plans for the weibull distribution,” Technometrics, vol. 46,
no. 4, pp. 470–481, 2004.
[24] F. Pascual, “Accelerated life test planning with independent weibull
competing risks with known shape parameter,” IEEE Trans. Rel. , vol.
56, no. 1, pp. 85–93, Mar. 2007.
[25] F. Pascual, “Accelerated life test planning with independent weibull
competing risks,” IEEE Trans. Rel., vol. 57, no. 3, pp. 435–444, Sep.
2008.
[26] X. Liu and W. S. Qiu, “Modeling and planning of step-stress accelerated life tests with independent competing risks,” IEEE Trans. Rel.,
vol. 60, no. 4, pp. 712–720, Dec. 2011.
[27] E. M. Monroe, R. Pan, C. Anderson-Cook, D. C. Montgomery, and C.
M. Borror, “A generalized linear model approach to designing accelerated life test experiments,” Quality Rel. Eng. Int., vol. 27, no. 4, pp.
595–607, 2011.

[28] R. Pan and T. Yang, “A glm approach to optimal alt test plans for
weibull distribution with type-I censoring,” in The Proc. SAE 2011
World Congress (SAE), Apr. 2011.
[29] D. C. Woods, S. M. Lewis, J. A. Eccleston, and K. G. Russell, “Designs for generalized linear models with several variables and model
uncertainty,” Technometrics, vol. 48, no. 2, pp. 284–292, 2006.
[30] M. Aitkin and D. Clayton, “The fitting of exponential, weibull and
extreme value distributions to complex censored survival data using
glim,” J. Roy. Statist. Soc. Series C (Applied Statistics), vol. 29, no. 2,
pp. 156–163, 1980.
[31] E. Barbosa, E. Colosimo, and F. Louzada-Neto, “Accelerated life tests
analyzed by a piecewise exponential distribution via generalized linear
models,” IEEE Trans. Rel., vol. 45, no. 4, pp. 619–623, Dec. 1996.
[32] H. Chipman and H. Welch, D-Optimal Design for Generalized Linear
Models. 1996, unpublished.
[33] H. A. Dror and D. M. Steinberg, “Robust experimental design for multivariate generalized linear models,” Technometrics, vol. 48, no. 4, pp.
520–529, 2006.
[34] M. Yang, B. Zhang, and S. Huang, “Optimal designs for generalized
linear models with multiple design variables,” Statistica Sinica, vol.
21, pp. 1415–1430, 2011.
[35] E. M. Monroe and R. Pan, “Experimental design considerations for accelerated life tests with nonlinear constraints and censoring,” J. Quality
Technol., vol. 40, no. 4, pp. 355–367, 2008.
[36] D. Collett, Modelling Survival Data in Medical Research, 2nd ed.
Boca Raton, FL, USA: Chapman and Hall/CRC, 2003.
[37] A. Atkinson, A. Donev, and R. Tobias, Optimum Experimental Designs, With SAS. Oxford Statistical Science Series. London, U.K.:
Oxford Univ. Press, 2007.

Tao Yang is currently a Ph.D. candidate in the School of Computing, Informatics, and Decision Systems Engineering at Arizona State University. He received the M.S. degree in Industrial Engineering from Arizona State University,
and the B.Eng degree in Logistics & Systems Engineering from Huazhong University of Science and Technology. His research interests include experimental
design, quality, and reliability engineering.

Rong Pan is an Associate Professor in the School of Computing, Informatics,
and Decision Systems Engineering at Arizona State University. He received the
Ph.D. in Industrial Engineering from Penn State University in 2002. His research interests include failure time data analysis, design of experiments, multivariate statistical quality control, time series analysis, and control. He is the
recipient of 2008 and 2011 Stan Ofsthun Awards, given by the Society of Reliability Engineers. He is a senior member of ASQ and IIE, and a member of SRE
and IEEE.

626

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,

VOL. 18, NO. 5,

MAY 2006

Test-Cost Sensitive Classification on Data
with Missing Values
Qiang Yang, Senior Member, IEEE, Charles Ling, Xiaoyong Chai, and Rong Pan
Abstract—In the area of cost-sensitive learning, inductive learning algorithms have been extended to handle different types of costs to
better represent misclassification errors. Most of the previous works have only focused on how to deal with misclassification costs. In
this paper, we address the equally important issue of how to handle the test costs associated with querying the missing values in a test
case. When an attribute contains a missing value in a test case, it may or may not be worthwhile to take the extra effort in order to
obtain a value for that attribute, or attributes, depending on how much benefit the new value will bring about in increasing the accuracy.
In this paper, we consider how to integrate test-cost-sensitive learning with the handling of missing values in a unified framework that
includes model building and a testing strategy. The testing strategies determine which attributes to perform the test on in order to
minimize the sum of the classification costs and test costs. We show how to instantiate this framework in two popular machine learning
algorithms: decision trees and naive Bayesian method. We empirically evaluate the test-cost-sensitive methods for handling missing
values on several data sets.
Index Terms—Cost-sensitive learning, decision trees, naive Bayes.

æ
1

INTRODUCTION

I

NDUCTIVE learning techniques, such as the naive Bayesian
and decision tree algorithms, have met great success in
building classification models with an aim to minimize the
classification errors [1], [2]. However, much previous
inductive learning research has only focused on how to
minimize classification costs such as the cost of false
positive (FP) and the cost of false negative (FN). The
classification errors are useful in deciding whether a
learned model tends to make correct decisions on assigning
class labels for new cases and is useful for dealing with data
with unbalanced classes. However, misclassification costs
are not the only costs to consider in practice. When
performing classification on a new case, values for some
attributes may be missing. In such a case, we may have the
option of performing additional tests in order to obtain a
value for these attributes. However, performing these
additional tests may incur more costs, where some costs
are in the form of lengthy waiting time and others include
monetary payment. Still, some tests are worthwhile to
perform because having the additional values might greatly
increase the classification accuracy. Thus, we often must
consider the “test cost” when missing values must be
obtained through physical “tests” which incur costs

. Q. Yang and R. Pan are with the Department of Computer Science, Hong
Kong University of Science and Technology, Clear Water Bay, Kowloon,
Hong Kong. E-mail: {qyang, panrong}@cs.ust.hk.
. C. Ling is with the Department of Computer Science, The University of
Western Ontario, London, Ontario N6A 5B7, Canada.
E-mail: cling@csd.wuo.ca.
. X. Chai is with the Department of Computer Science, Purdue University,
250 N. University Street, West Lafayette, IN 47907.
E-mail: chai@cs.purdue.edu.
Manuscript received 17 May 2005; revised 9 Oct. 2005; accepted 27 Oct.
2005; published online 17 Mar. 2006.
For information on obtaining reprints of this article, please send e-mail to:
tkde@computer.org, and reference IEEECS Log Number TKDE-0192-0505.
1041-4347/06/$20.00 ß 2006 IEEE

themselves. These costs are often as important as the
misclassification costs.
As an example, consider the task of a medical practice
that examines incoming patients for a certain illness (see
Table 1). Suppose that the doctors’ previous experience has
been compiled into a classification model such as a naive
Bayesian classifier. When dealing with an incoming patient,
it is often the case that certain information for this patent
may not yet be known; for example, the blood tests or the
X-ray test may not have been done yet. At this point, the
doctor (that is, the classification model) must exercise its
judgement appropriately: Performing these tests will incur
certain extra costs, but some tests may provide useful
informational benefits toward reducing the classification
costs. In the end, it is the balancing act of the two types of
costs—namely, the classification costs and the test costs
—that will determine which tests will be done.
Tasks that incur both misclassification and test costs
associated with missing values abound in industrial
practice ranging from medical diagnosis to scientific
research and to drug design. In the past, inductive
learning methods that consider a variety of costs are
often referred to as cost-sensitive learning [3], [4]. In this
paper, we present a unified framework for how to
integrate both cost-sensitive learning and the treatment
of missing values in test data. To distinguish from
methods that only consider misclassification costs, we call
this method test-cost-sensitive learning methods.
Our test-cost-sensitive learning framework, which is
called the TCSL framework, consists of two parts. First, in
part one, when training a model, we consider both the
misclassification costs and the potential test costs associated
with the attributes in order to minimize the potential future
total costs. Then, in part two, we design test strategies that
are tailored for each individual test example in order to
Published by the IEEE Computer Society

YANG ET AL.: TEST-COST SENSITIVE CLASSIFICATION ON DATA WITH MISSING VALUES

627

TABLE 1
An Example of a New Case Containing Missing Values and Their Associated Costs for Getting a Value

MC is the misclassification cost, and T C is the test cost.

exploit the known information and propose a plan to
acquire the unknown.
For training a model, we apply our TCSL framework to
two machine learning algorithms, decision trees and naive
Bayesian, and evaluate their relative merits. For testing,
when a new case contains missing values for its attributes,
decisions must be made whether to obtain values of these
attributes through tests. In this paper, we consider two test
strategies: a sequential test strategy and a batch test
strategy. The former takes tests for missing values sequentially. Decisions on whether an additional test is needed or
which attribute with missing value should be tested next
are made based on the outcome of the previous test results.
The batch test strategy requires several new tests to be done
all together rather than in a sequential manner.
The novelty of our work can be seen from several angles,
as follows:
1.

2.

2

Most previous work on cost-sensitive learning has
mostly considered how to increase the classification
accuracy by considering the false positive and
negative costs. In our TCSL framework, we additionally consider the test cost, by minimizing the
sum of the classification and test costs together.
We not only consider how to build a TCSL model
from the training data, but we also consider how to
handle the test cases by considering the sequential
and batch test strategies for obtaining the missing
values.

RELATED WORK

Much work has been done in machine learning on
minimizing the classification errors. This is equivalent to
assigning the same cost to each type of classification errors
(for example, FP and FN), and then minimizing the total
misclassification costs. In Turney’s survey papere [3], a
whole variety of costs in machine learning are analyzed,
and the test cost is singled out as one of the least considered
areas in machine learning. In particular, [3] considered the
following types of costs in machine learning:
.

.

Misclassification costs: These are the costs incurred
by misclassification errors. Works such as [5], [4], [6]
considered machine learning with nonuniform misclassification costs.
Test costs: These are the costs incurred for obtaining
attribute values. Some previous work such as [7], [8]
considered the test cost alone without incorporating
misclassification cost. As pointed out in [3], it is
obviously an oversight.

As far as we know, the only works that considered both
misclassification and test costs include [9], [10], [11], [12]. Of
these works, [9] explicitly considered how to directly
incorporate both types of costs in decision tree building
processes and in determining the next attribute to test,
should the attribute contain a missing value. An advantage
of the minimal-test-cost decision tree method is that it
naturally extends the decision tree construction algorithm
by considering both the misclassification costs and the test
cost in a local search framework. It additionally considers
whether a test should be conducted and how to select the
next attribute to test. Through experimentation with this
method, however, we have also found some shortcomings.
Because decision trees are aimed at serializing attribute
tests along the path of a tree, it is not well-suited for
performing batch tests that involve a number of tests to be
done together. Furthermore, because it places different
levels of importance on the attributes by the natural
organization of the tree, it cannot be easily fitted to make
decisions on testing strategies that select attributes for the
tests. In contrast, the naive Bayesian-based algorithms
overcome these difficulties more naturally. As we will see,
the performance offered by the test-cost sensitive naive
Bayesian is significant over its decision-tree counterpart.
In [11], the cost-sensitive learning problem is cast as a
Markov Decision Process (MDP), and an optimal solution is
given as a search in a state space for optimal policies. For a
given new case, depending on the values obtained so far,
the optimal policy can suggest a best action to perform in
order to both minimize the misclassification and the test
costs. While related to our work, their research adopts an
optimal strategy, which may take very high computational
cost to conduct the search process. In contrast, we adopt the
local search algorithm the concept of a utility gain, which is
more efficient to compute and, as we will show, attains a
high level of classification accuracy. Thus, our algorithm
follows the direction of approximation rather than optimal
algorithms.
Similarly, Greiner et al. in the interest in constructing an
optimal learner, [12] studied the theoretical aspects of active
learning with test costs using a PAC learning framework.
Turney [10] presented a system called ICET, which uses a
genetic algorithm to build a decision tree to minimize the
cost of tests and misclassification. As mentioned above,
because our algorithm essentially adopts the conditional
probability-based framework, which requires only a linear
scan through the data set, our algorithm is expected to be
more efficient than Turney’s genetic algorithm-based
approach.

628

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,

In the past, test costs have also been separately
considered in [9], where decision-tree-based testing strategies were explored along with several testing strategies, and
in [13], where a Naive Bayesian-based method is proposed
with sequential strategies. However, these methods have
never been placed under a unified test-cost-sensitive
learning framework, nor have they been compared thoroughly together. With the unified framework, it is now
possible to compare not only sequential test strategies, but
also batch test strategies where several tests are selected to
be done together.

3

TEST-COST SENSITIVE LEARNING

3.1 Problem Formulation
The Test-Cost-Sensitive classification learning problem can
be formulated as follows:
Description of Algorithm TCSL
Given: ðD; R; T Þ, where
D is a data set consisting of N samples ðx1 ; x2 ;
   ; xN Þ from P classes ðc1 ; c2 ;    ; cP Þ. Each sample
xi is described by M attributes ða1 ; a2 ;    ; aM Þ
among whom there can be missing values.
. R is a misclassification cost matrix. Each entry
4
rij ¼ Rði; jÞ specifies the cost of classifying
one sample from class ci as class cj ð1  i; j  P Þ.
Usually, Rii ¼ 0 ð1  i  P Þ.
4
. T is a test cost vector. Each entry Tk ¼ T ðkÞ specifies
the cost of taking a test on attribute ak .
Build: a classifier for minimizing the total cost of
classification that includes both the test cost and the
misclassification cost for the training examples, and a test
strategy for deciding, given a test case, an order in which to
obtain the missing values.
Our aim is to minimize the sum of classification costs
Cmc and test costs for every test case, Ctest .
Subject to: constraints such as the total cost is within a
user specified upper bound.
The above formulation provides a more general framework than the traditional cost-sensitive learning formulations. Actually, the latter is just a special case of the TCSL
framework where the test cost is sufficiently large so that no
test will be performed. Also, the conditional risk [14] can be
equivalently implemented by setting the misclassification
cost matrix properly.
TCSL provides a more general framework than the
traditional supervised learning frameworks. Standard supervised learning is just a special case of the TCSL because
no missing values are acquired in the traditional methods.
That is, if we set the test costs T to positive infinite so that
no test will be performed, then the TCSL retrogresses to a
standard supervised learning algorithm which makes
prediction based only on the known attributes. For
example, the conditional risk [14] can be equivalently
implemented by setting the misclassification cost matrix R.
By imposing different constraints on how to obtain the
attributes, we can realize different test strategies. An
important constraint is the available resource constraint
used to limit the total number of test costs. As an example,
consider the task of a medical practice that patients have a
.

VOL. 18, NO. 5,

MAY 2006

limited amount of money for doing medical tests, each test
having a different test cost. Therefore, the total test fees
cannot exceed the money limitation. When the money
limitation is reached, the patients cannot afford any more
tests, even if there are some tests that can reduce the risk of
false diagnosis (misclassification). Then, the problem of the
doctors is to design an optimal sequential test strategy
within the test cost constraint. The strategy is sequential
since the decision on the next test to perform can be
dependent on the outcome of the previous one.
In addition to doing the test in sequence, in practice,
there is also a great need for batch tests. In medical
diagnosis, doctors cannot afford to wait for the result of the
first test before other tests can be done. They normally order
a set of tests to be done at one shot. This kind of constraint
can be viewed as limiting the times of doing tests in
addition to the resources available. It is more practical and
decisions must be made altogether before any test result
comes out. In this situation, the problem is to design an
optimal batch test strategy instead of doing tests one by one
in a sequential manner. It is interesting how to design such
an “Optimal Batch Test” for the TCSL. We will discuss the
batch test strategy in detail in Section 5.4.
Note that a main contribution of our work is the
unification of attribute test and misclassification costs.
However, in many real-world domains, these two costs
carry different meanings and, therefore, how to combine
them is a domain-dependent issue. For simplicity, however,
in this work, we simply assume that they are comparable
and can be added together to get the total cost.

4

TEST-COST-SENSITIVE DECISION TREES

4.1 Model Construction
The test-cost-sensitive learning framework can include
different classification algorithms. In this section, we review
the test-cost-sensitive decision tree algorithm [9] and model
it under the general test-cost-sensitive learning framework.
We assume that the training data may consist of some
missing values (whose values cannot be obtained). We also
assume the test costs are associated with attributes (that is,
to perform a CAT Scan in a medical case, the cost to the
patient is the same regardless of the outcome). Furthermore,
we assume that the test cost and the misclassification cost
have been defined on the same cost scale, such as the dollar
cost incurred in a medical diagnosis. For simplicity, we
consider discrete attributes and binary class labels; extensions to other situations, such as numerical classes, can be
made similarly. We assume that F P is the cost of one false
positive example and F N is the cost of one false negative
example.
Our decision-tree learning algorithm uses a new splitting
criterion of minimal total cost on training data, instead of
minimal entropy, to build decision trees. This cost measure
is equivalent to the expected total cost measure used in the
works of [3], [11], [12]. More specifically, at each step, rather
than choosing an attribute that minimizes the entropy (as in
C4.5), our algorithm chooses an attribute that reduces and
minimizes the total cost, which is the sum of the test cost and
the misclassification cost, for the split. With the total cost

YANG ET AL.: TEST-COST SENSITIVE CLASSIFICATION ON DATA WITH MISSING VALUES

629

formula, similar to C4.5, our algorithm chooses a locally
optimal attribute without backtracking. Another similarity
is the way it treats missing values in the training data set.
Thus, even though the resulting tree may not be globally
optimal, the efficiency of the tree-building algorithm is
generally high. A concrete example is given later in this
section.
An important point is how the leaves are labeled. In
traditional decision tree algorithms, the majority class is
used to label the leaf node. In our case, as the decision tree
is used to make predictions in order to minimize the total
cost, the leaves are labeled also to minimize the total cost.
That is, at each leaf, the algorithm labels the leaf as either
positive or negative (in a binary decision case) by minimizing the misclassification cost. More specifically, suppose
that the leaf has P positive examples and N negative
examples. If P  F N > N  F P (i.e., the cost of predicting
negative is greater than the cost of predicting positive), then
the leaf is labeled as positive (þ); otherwise, it is labeled as
negative (). Therefore, the label of a leaf not only depends
on the majority class of the leaf, but also on the cost of
misclassification.
Algorithm csDT-learn() (see Algorithm 1) lists the
general input and output required to learn a model in
test-cost-sensitive decision-tree learning. Once a model is
built on the training data, the model can then be applied to
a test case x, as shown in Algorithm 2 in the next section.

possibly building a subtree. If P  F N > N  F P , then, if
no subtree is built, the set would be labeled as positive
(+). Thus, the total misclassification cost is T ¼ N  F P .
Suppose that an attribute A with a test cost C is
considered for a potential splitting attribute. Assume that
an attribute A has two values, and there are P 1 and N1
positive and negative examples with the first value,
respectively, and P 2 and N2 positive and negative
examples with the second value, respectively. Then, the
total test cost would be ðP 1 þ N1 þ P 2 þ N2Þ  C.
Assume that the first branch will be labeled as positive
(as P 1  F N > N1  F P ), and the second branch will be
labeled as negative, then the total misclassification cost of
the two branches would be N1  F P þ P 2  F N. Based on
these considerations, the total cost of choosing A as a
splitting attribute, when processing the training examples,
would be:

Algorithm 1 csDT-learnðD; A; CL; R; T ; T CF Þ
Input:
D—a data set of samples fx1 ; x2 ; . . . ; xN g,
A—a set of attributes fA1 ; A2 ; . . . ; AM g, where
Am 2 fvm;1 ; vm;2 ; . . . ; vm;jAm j g,
CL—predefined classes fc1 ; c2 ; . . . ; cP g,
R—misclassification cost matrix (for binary class
problems, this defines the F P and F N costs),
T —a test cost vector, which is an array listing the
cost value for each attribute,
T CF —a total cost formula for each outcome.
Output:
ModelðcÞ—the learned model that predicts the class
value of a new case c with a probability measure.

4.2 Test Strategy on New Examples
After the minimal-cost decision tree is built, the next
interesting question is how this tree can be used to deal
with testing examples with many missing values in order to
predict the class of the testing examples with the minimal
total cost for this case. A test strategy determines an order to
obtain missing values for each particular testing test case.
Different test strategies can result in different total costs. In
this paper, we consider a simple sequential strategy: to
simply follow the tree built in the previous section.
The testing strategy we consider in this paper is to
simply follow the tree (see Algorithm 2) because the
decision trees have already specified an order in which to
perform the tests. During the process of classifying a single
example, should there be any missing values for an
attribute, we always do a test on that attribute in order to
obtain a value. This process continues until a leaf node is
reached.

Step 1: Let T be the minimum misclassification cost of
stopping at D with a class label cD , among all
possible cD 2 CL;
Step 2: For each attribute Ai ; i ¼ 1; 2; . . . ; m in D, calculate
the total cost Ti of testing and splitting D on Ai ’s
values;
Step 3: If T  Ti for all i, then stop at D with a class label cD
from Step 2;
Step 4: Select the Ai with the minimum total cost Ti among
all Ai in D, and split the data set D according the
values of Ai , producing data sets Dij ; j ¼ 1; 2; . . . k;
Step 5: Apply TCSL-learn algorithm to each Dij
recursively.
Consider a concrete example to illustrate the calculation of the total cost. Assume that, during the tree
building process, there is a set of P and N positive and
negative examples, respectively, to be further classified by

TA ¼ ðP 1 þ N1 þ P 2 þ N2Þ  C þ N1  F P þ P 2  F N:
If TA < T , where T ¼ N  F P , then splitting on A would
reduce the total cost of the original set, and we will choose
such an attribute with the minimal total cost as a splitting
attribute. We will then apply this process recursively on
examples falling into branches of this attribute. If TA  T for
all remaining attributes, then no further subtree will be
built, and the set would become a leaf with a positive label.

Algorithm 2 csDT-testðModel; C; T ; Ttotal ; xÞ
Input:
ModelðÞ—a test-cost-sensitive classification model,
R—a misclassification cost matrix,
T —a test cost vector,
Ttotal —the total resources available,
x—a testing example.
Output:
Label—the predicted class,
Ttest —the test cost for the example.
Step 1: If a batch-test strategy is followed, then obtain all
missing values of x with tests, and then apply the
decision tree classification to x; output the class
label and total test cost Ttest ðxÞ, and stop.

630

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,

Step 2: If a sequential-test strategy is followed, then apply
the tree to x starting from the root node;
Step 3: Repeat: at any node N, if the corresponding value of
x is missing, then perform a test on this attribute if
the total test cost so far is less than Ttotal ; if exceeding
Ttotal already, then stop and report the minimal cost
class label at node N;
Step 4: If a leaf node is met, then report the class label C
with the total test cost so far Ttest ðxÞ and stop.
The decision-tree-based sequential test strategy, while
optimal based on the minimal cost of the training set, is
sequential in nature. A batch test strategy can be modeled in
our decision tree as well, although, as we will see in the next
section, not as natural as in a Naive Bayesian algorithm. The
basic idea is that, when a testing case is stopped at the first
attribute whose value is unknown, all unknown values
under that attribute must be obtained. Clearly, this batch
strategy will return the same prediction as the decision-treebased test strategy (i.e., same misclassification cost), but it
would incur a higher test cost in general because some test
results are not used in classification.

5

TEST-COST-SENSITIVE NAIVE BAYES

5.1 Costs in Naive Bayesian Classification
Decision trees determine a natural sequence of tests to be
done, but may be too strict in some cases. To allow a more
flexible testing strategy, we now turn to Naive Bayesian
classifiers, which are shown to perform very well in practice
[15]. Below, we consider the cost-sensitive Naive Bayesian
Learning [13] under our test-cost sensitive learning framework. For classification, the standard naive Bayes algorithm
computes the posterior probability P ðcj jxÞ of sample x
belonging to class cj according to Bayes’ rule:
P ðcj jxÞ ¼

P ðxjcj ÞP ðcj Þ
;
P ðxÞ

where
P ðxjcj Þ ¼

jAj
Y

P ðAm ¼ vm;k jcj Þ

m¼1

is the product of jAj likelihoods. jAj is the number of attributes
describing x and vm;k is one possible value of attribute Am .
Then, sample x is predicted to belong to class j which has the
highest value P ðcj jxÞ. When there exist missing values in
sample x, the corresponding attributes are simply left out in
likelihood computation and the posterior probability is
e
computed only based
Q on the known attributes A  A.
Therefore, P ðxjcj Þ ¼
e P ðAm ¼ vm;k jcj Þ.
Am 2A

The standard naive Bayesian algorithm can be extended
to take into account the misclassification cost. Suppose that
Cij is the cost of predicting an example of class i as
belonging to class j. In this situation, the expected total cost
(that is, the Ttotal in Algorithm 1) of predicting a single
sample x as a class j is known
as the conditional risk [14] and
P
is defined as: RðjjxÞ ¼ i Cij  P ðijxÞ, where P ðijxÞ is the
posterior probability. Then, sample x is predicted to belong
to the class j which has the minimal conditional risk RðjjxÞ.

VOL. 18, NO. 5,

MAY 2006

TABLE 2
Likelihoods of Attributes

Before considering the test costs, let us consider an
example. Suppose that, in the medical diagnosis of a
hepatitis case, 21 percent of patients are positive (have
hepatitis, c1 ) and 79 percent of patients are negative
(healthy, c2 ). Therefore, the priors are P ðc1 Þ ¼ 21 percent
and P ðc2 Þ ¼ 79 percent, respectively. Assume the misclassification costs (conditional risk) are C12 ¼ 400, C21 ¼ 100,
and C11 ¼ C12 ¼ 0.
There are four attributes to characterize a patient, and the
test cost of each attribute and the likelihoods are listed in
Table 2.
When diagnosing a new patient, the doctor faces the
decision of whether a test should be performed and, if so,
which one. Since each test on an unknown attribute has its
own discriminating power on disease and also brings a
certain amount of cost, decisions must be made by
considering both factors. Later, we will see that the attribute
“ascites” is the best first choice since it trades off the
misclassification cost and test costs. However, if its test cost
is raised from 45 to 47, the attribute “liver firm” will be
more preferable for testing first. This example shows that
the testing strategy is related to the test costs.
In practice, the problem is even more complicated when
more attributes are involved and some tests are with
delayed results. For example, the blood tests are usually
shipped to a laboratory and the results are sent back to
doctors the next day. In this case, for the sake of patients,
doctors often ask for a batch of tests simultaneously.
Therefore, the test strategy must consider more than
one test to be done, under different constraints.

5.2 Model Construction in csNB
The procedure of learning a csNB classifier is basically an
estimation of the distribution parameters as in standard
naive Bayes. Algorithm 3 outlines the learning procedure.
Algorithm 3 csNB-learnðD; A; CLÞ
Input:
D—a data set of samples fx1 ; x2 ; . . . ; xN g,
A—a set of attributes fA1 ; A2 ; . . . ; AM g, where
Am 2 fvm;1 ; vm;2 ; . . . ; vm;jAm j g,
CL—predefined classes fc1 ; c2 ; . . . ; cP g.
Output:
P^ðcj Þ—the estimated prior probabilities,
P^ðAm ¼ vm;k jcj Þ—the estimated likelihoods, where
0 < k <¼ jAm j.

YANG ET AL.: TEST-COST SENSITIVE CLASSIFICATION ON DATA WITH MISSING VALUES

Steps:
1: for eachP
class
cj do
N
P ðcj jxi Þ
2: P^ðcj Þ ¼ i¼1 N
3: end for
4: for each class cj do
5: for each attribute Am do
6:
for each value vm;k of attribute Am in class cj do
7:
P^ðAm ¼ vm;k jcj Þ
¼

þN
i¼1 NumðAm ¼vm;k ;xi ÞP ðcj jxi Þ
jA j

NumðAm ¼vm;l ;xi ÞP ðcj jxi Þ
jAm jþl¼1m N
i¼1

8:
end for
9: end for
10: end for
In the above equations,  is the smoothing factor.  ¼ 1 is
known as the Laplacian smoothing which we use in our
experiments. The NumðAm ¼ vm;k ; xi Þ is a function counting
the number of times that attribute Am has the value vm;k .
Moreover, P ðcj jxi Þ 2 f0; 1g, j 2 f1;    ; P g; if xi belongs to
the class j, the value is 1; otherwise, the value is 0.

5.3 Sequential Test Strategies
After a naive Bayes classifier is built, an interesting question
now is how the classifier performs tests when the testing
examples have missing values. Instead of treating test costs
Ctest and misclassification costs Cmc separately, we offer
testing strategies for minimizing the sum of Ctest and Cmc
for different situations. In this section, we consider the
sequential test strategy and leave the batch test strategy to
Section 5.4.
The sequential test strategy is as follows: During the
process of classification, decisions are made sequentially on
whether a test on an unknown attribute (an attribute with
missing value) should be performed based on the results of
previous tests, and, if so, which attribute is selected.
Formally, let S ¼< D1 ; D2 ;    > denote the strategy of a
sequence of decisions where Di ¼ Aj represents the
ith decision of selecting an unknown attribute Aj for
testing. Sequential test means that decision Diþ1 is made
dependent on the result of decision Di , more specifically,
the outcome of the test.
Suppose that x ¼ ða1 ; a2 ;    ; aM Þ is a testing example.
e
Each attribute ai can be either known or unknown. Let A
denote the set of all known attributes among attributes A
and A the unknown attributes. The misclassification cost of
classifying x as class cj is:
Rðcj jxÞ ¼

P
X

Cij  P ðcj jxÞ;

1  j  P;

ð1Þ

i¼1
P ðxjc ÞP ðc Þ

where P ðcj jxÞ ¼ P jðxÞ j is the posterior probability
obtained using Bayes’ rule. The class cj with the minimum
cost is then viewed as the predicted label and the value
Rðcj jxÞ is the misclassification cost Cmc with the current
e.
values of attributes in A
e. However, a test or a
Prediction can be made based on A
sequence of tests on some unknown attributes may be more
preferable to reduce the misclassification cost while minimizing the total cost. To decide whether a further test is
needed and, if so, which attribute Ai 2 A to select, we

631

introduce the utility of testing an unknown attribute Ai as
follows:
UtilðAi Þ ¼ GainðA; Ai Þ  Ctest ðAi Þ:

ð2Þ

Ctest ðAi Þ is the test cost of Ai given by Ti . GainðA; Ai Þ is the
reduction in misclassification cost obtained from knowing
Ai ’s true value, which is given by:
e; Ai Þ ¼ Cmc ðA
eÞ  Cmc ðA
e [ Ai Þ:
GainðA

ð3Þ

eÞ ¼ minj Rðcj jA
eÞ is easily obtained using (1). HowCmc ðA
e [ Ai Þ since
ever, it is not trivial as the calculation of Cmc ðA
the value of unknown Ai is not revealed until the test is
performed. We calculate it by taking expectation over all
possible values of Ai as follows:



e [ Ai Þ ¼ E min Rðcj jA
e [ Ai ÞjA
e
Cmc ðA
j

¼

jjA
i jj 
X

eÞ
P ðAi ¼ vi;k jA

ð4Þ

k¼1


e; Ai ¼ vi;k Þ :
 min Rðcj jA
j

ð5Þ

In (4), the expected minimum value of misclassification cost
e known so far. In
is dependent on the values of attributes A
the expended form (5), the minimum misclassification
given Ai ¼ vi;k is weighted by the conditional probability
eÞ which can be obtained using Bayes’ rule.
P ðAi ¼ vi;k jA
Overall, by using (2) to calculate all the utilities of testing
unknown attributes in A, we can decide whether a test is

needed (9i UtilðAi Þ > 0) and which attribute Ai should be
tested (arg maxi UtilðAi Þ).

After the attribute Ai is selected and tested, the set of
e
e [ fA g and the
known attributes A is extended to A
i

corresponding A is reduced to A=fAi g. Such a selection
process is repeated until all the utilities of testing unknown
attributes left is negative (unworthy) or there is no
unknown attribute left.
Finally, a class label is predicted based on the extended
e. The misclassification cost Cmc is
unknown attribute set A
then Cij if the example of true class ci is predicted as cj .
All the costs brought by the attribute tests are comprised
of the test costs Ctest . Consequently, the total cost Ctotal ¼
Cmc þ Ctest can be obtained. The details of the csNBsequential prediction are given in Algorithm 4. As the
output, the algorithm gives the prediction of a testing
example x as well as the test costs Ctest involved for some
unknown attributes.
Algorithm 4 csNB-sequential-predictðNBC; R; T; xÞ
Input:
NBC—a cost-sensitive naive Bayes classifier,
R—a misclassification cost matrix,
T —a test cost vector,
x—a testing example.
Output:
Label—the predicted class,
Ctest —the test costs.

632

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,

Steps:
e and A denote the set of known attributes and the
1: Let A
set of unknown attributes of x.
2: Set Ctest ¼ 0.
3: while A is not empty do
4:
for all Ai 2 A do
5:
Calculate UtilðAi Þ by using (2);
6:
end for
7:
if not 9i UtilðAi Þ > 0 then
8:
break;
9:
end if

10: Ai ¼ maxi UtilðAi Þ
11: Reveal Ai ’s value v.
12: Ctest ¼ Ctest þ TA
i

e
e [ fAi ¼ vg
13: A
A

A=fAi g
14: A
15: end while
eÞ by (1).
16: Calculate the misclassification costs Rðcj jA
eÞ.
17: Label = arg minj Rðcj jA
Back to the example in Section 5.1, the utilities of the four
attributes are 9.6, 8.6, 9.0, and 11.1, respectively. Therefore,
the unknown attribute “ascites” will be selected in the first
place. Through further calculation, a test sequence can be
obtained.
A desirable property is that even though all the test costs
are zero, the csNB may not do tests for all missing
attributes. One reason is that the gain from knowing a
new attribute value Ai is not always positive. According to
e [ Ai Þ is equal to or
(4), if the misclassification cost Cmc ðA
eÞ, the gain is
even larger than the original one Cmc ðA
nonpositive. This creates a paradox: Adding new features
(especially unrelated features) to a naive Bayes classifier
may actually lead to a worse prediction accuracy. The basic
source can be traced back to the wrong independent
assumption of naive Bayes [14]. For the same reason,
adding these features to the csNB can increase the
misclassification costs and is therefore not preferred. Also,
another reason is that the characteristics of some misclassification cost matrix can affect the testing process dramatically. As an example, suppose the entries Cij0 in the j0 th
column of misclassification matrix C is much smaller than
other entries in C, so that the minimizing function
eÞ and (5) always have j0 returned. In this case,
minj Rðcj jA
the gain from any unknown attribute Ai is always zero and
csNB will not do any tests even if their costs are zero.

5.4 Batch Test Strategies
The sequential test strategy is optimal in the sense that 1) it
takes expectation on all possible outcomes of attribute tests
and 2) the decisions are made in a sequential manner such
that the next test is dependent on the previous one.
However, in many situations, tests are required to be done
at once due to some practical constraints, such as time. In
these situations, unknown attributes are predetermined for
testing in a batch manner.
Specifically, the batch test strategy is different from the
sequential test strategy mainly in two facets. First, tests on
unknown attributes must be determined in advance before

VOL. 18, NO. 5,

MAY 2006

any one of them is carried out (time constraints). Second,
because of the limitation on available resources, it is
impossible to test all the unknown attributes and, consequently, selection must be made among those attributes
(resource constraints). Therefore, under the paradigm of
batch test, a strategy S ¼ fD1 ; D2 ;    ; Dt g is no longer a
sequence of, but a set of decisions. Still, Di ¼ Aj represents
the selection of unknown attribute Aj for testing. While
two decisions are correlated due to the constraints imposed,
one decision is no longer dependent on the outcome of the
other as in the sequential test strategy.
While finding the optimal batch test strategy S by
examining all possible subset of unknown attributes A is
computationally difficult, we assume the conditional independence assumption of attributes and map the problem
of finding strategy S as a knapsack problem [18]. Given a
knapsack and a set of items, each with a volume and a
value, the problem is to determine the number of each item
to include in the knapsack so that the total volume is less
than the volume of the knapsack and the total value is as
large as possible.
The equivalent knapsack problem is defined as follows:
Each unknown attribute Ai 2 A is viewed as an item in the
currently missing item set A. The volumes of each item Ai is
the test cost TAi of it given by the test cost vector T . The gain
of Ai given by (3) is cast as its value. Furthermore, the
resource constraint Ttotal is viewed as the whole volume of
the knapsack. Then, the problem is to find the most valuable
set of items (a subset of A) that fit in the knapsack of fixed
volume Ttotal which can be solved using dynamic programming in fjAjTtotal g.The resulting algorithm is given in
Algorithm 5.
Algorithm 5 csNB-batch-predictðNBC; R; T; Ttotal ; xÞ
Input: NBC—a cost-sensitive naive Bayes classifier,
R—a misclassification cost matrix,
T —a test cost vector,
Ttotal —the total resources available,
x—a testing example.
Output:
Label—the predicted class,
Ctest —the test costs.
Steps:
e and A denote the set of known attributes and the
1: Let A
set of unknown attribute of x.
2: Set Ctest ¼ 0.
3: Set V alues ¼ fg and V olumes ¼ fg.
4: for all Ai 2 A do
e [ Ai Þ by (3)
5:
Calculate GainðA
6:
if GainðAi Þ > Ctest ðAi Þ ¼ TAi then
7:
V alues
V alues [ fGainðAi Þg
8:
V olumes
V olumes [ fTAi g
9:
end if
10: end for

11: ðfAi g; Ctest Þ ¼ KNAPSACKðV alues; V olumesÞ

12: Reveal the values of attributes in fAi g.


e
e
13: A
A [ fAi ¼ vi;k g
eÞ by (1).
14: Calculate the misclassification costs Rðcj jA
e
15: Label = arg minj Rðcj jAÞ.

YANG ET AL.: TEST-COST SENSITIVE CLASSIFICATION ON DATA WITH MISSING VALUES

633

TABLE 3
Data Sets Used in the Experiments

In Step 11 of the csNB-batch algorithm, the outputs of the

function KNAPSACKðÞ are the selected items Ai (unknown attributes) and the total test costs Ctest  Ttotal .
Similarly, the misclassification cost Cmc can be calculated in
the same way as in csNB-sequential and, subsequently,
Ctotal can be obtained.

6

EXPERIMENTS

In order to evaluate the performance of the TCSL framework for both decision-trees and naive Bayesian methods,
in both sequential and batch test manners, several experiments were carried out on data sets from the UCI ML
repository [15] as well as a real-world data set from an
insurance company. The eight data sets used are listed in
Table 3. These data sets were chosen because they have
discrete attributes, binary class, and a sufficient number of
examples. We only consider binary class problems (positive
and negative) in the following experiments, although our
TCSL algorithms can be used in multiple class problems
naturally. The numerical attributes in data sets were
discretized using minimal entropy method [18].
We ran a three-fold cross validation on these data sets.
For the testing examples, a certain percentage (missing rate)
of attributes are randomly selected and marked as
unknown. If during testing, an algorithm decides to
perform a test on an unknown attribute, the real value of
the attribute is revealed and the test cost Ctest is accumulated. Finally, the misclassification cost Cmc can be obtained
by comparing the predicted label with the true class label.
The performance of the algorithms is therefore measured in
terms of the total cost Ctotal , the sum of Ctest and Cmc . To the
two-class problems, let c1 be the positive class and c2 the
negative class. The misclassification matrix was set as R12 ¼
R21 ¼ 400 and R11 ¼ R22 ¼ 0, where R12 can be interpreted
as false negative and R21 false positive. Test costs of each
attributes in T are set randomly between 0 and 100.
For algorithm comparison, two variations of traditional
naive Bayes classifiers were used as the baselines. The first
one is the naive Bayes classifier augmented to minimize the
misclassification cost (conditional risk), as given in [14].
This classifier is termed Lazy Naive Bayes (LNB) since it
simply predicts class labels based on the known attributes
and requires no further tests to be done on unknown ones.
The second variation is the naive Bayes classifier extended
further from LNB. It requires all the missing values to be
made up before prediction. Since this classifier allows no

Fig. 1. Average total cost comparisons of four methods on data sets:
Ecoli, Breast, Heart, and Thyroid.

missing values, it is therefore termed Exacting Naive Bayes
(ENB). Comparisons were also made between csNB and the
Test-Cost-Sensitive Decision Trees (csDT) proposed in
Section 4.
In summary, four methods were examined:
LNB—Lazy naive Bayes,
ENB—Exacting naive Bayes,
csNB—Cost-sensitive naive Bayes, and
csDT—test-cost-sensitive decision trees with the
sequential test strategy (follow the tree).
We note that the decision tree algorithm we use here is a
rather simple minded one: It simply follows the tree
sequentially to know which test to perform in order to obtain
a next missing value. Other more sophisticated methods can
be further developed, which is our ongoing work.
.
.
.
.

6.1 Comparing Decision Tree and Naive Bayesian
We first compared the cost-sensitive decision-tree algorithm
with sequential test strategy (follow the tree) with the

Fig. 2. Average total cost comparisons of four methods on data sets:
Australia, Cars, Voting, and Mushroom.

634

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,

VOL. 18, NO. 5,

MAY 2006

Fig. 3. Comparisons with varying missing rates.

Fig. 4. Comparisons on the Breast data set with varying test costs.

different cost-sensitive naive Bayesian algorithms. Fig. 1
and Fig. 2 show the results of different algorithms in
sequential test strategy on all the eight data sets. In these
experiments, the percentage of unknown attributes (missing
rate) was 40 percent. Each group of four bars represents the
runs of four algorithms on one particular data set. The
height of a bar represents the average total cost and,
therefore, the lower the better. Each bar consists of
two parts: The lower dark portion stands for the misclassification cost while the upper light portion stands for
the test costs.
There are several interesting observations from these
experiments. First, although the misclassification costs of
the ENB method are almost always the lowest among the
four methods, the average total costs are the highest. This is
because the low misclassification costs are achieved at the
cost of testing all unknown attributes, which is costly when
the missing rate is high.
Second, despite its lazy nature, the LNB method performs surprisingly well, even better than the csDT method.
This can be explained by the fact that, while csDT uses the
splitting criterion of minimal total costs for attribute
selection in tree building, whenever trees are built, all tests
are fixed. Only the values of those attributes associated with
tree nodes along the paths are examined and the others are
omitted. However, the other attributes that are on other
branches of a tree can still be informative for classification.
LNB, on the other hand, is capable of making use of these
attributes. For the example in Section 5.1, suppose that the
attributes “ascites” and “Spiders” are known. csDT may
choose “ascites” as a splitting node and leave “Spider” out,
while csNB still utilizes both attributes.
To investigate the impact of the percentage of unknown
attributes on the total costs, experiments were carried out on
average total cost with the increasing percentage of unknown
attributes. Fig. 3 shows the results on the Mushroom data set
(other figures are spared for space). As we can see, when the
percentage increases (> 40 percent), the average total cost of
LNB increases significantly and surpasses that of csDT.
Again, csNB is better than the other two over the whole range.

Another set of experiments was conducted to compare
two cost-sensitive algorithms csDT and csNB in test costs.
The misclassification costs are still fixed at 400 (FP) and
400 (FN). Fig. 4 and Fig. 5 are the results on the Breast and
Mushroom data sets with both the missing rate 20 percent
and 60 percent. One can see that, when the test costs are
small (below 20), csDT wins overall. However, as the test
costs increase, csNB outperforms csDT. One thing to note is
that csNB is less sensitive to the test costs than csDT. This
reveals that the csNB algorithm is better at balancing the
misclassification and test costs.

6.2

Comparison to Other Cost-Sensitive Learning
Systems
In addition, we consider comparing with three main
previous works in cost sensitive learning. Incorporating
attribute costs into decision trees is an established research
direction in coping with cost-sensitive learning. As we
reviewed in Section 2, several approaches have been
proposed that have mainly focused on fixing existing
decision tree-based algorithms. They include ICET [19],

Fig. 5. Comparisons on the Mushroom data set with varying test costs.

YANG ET AL.: TEST-COST SENSITIVE CLASSIFICATION ON DATA WITH MISSING VALUES

635

Fig. 6. Comparisons of CSDT and CSNB against other cost-sensitive algorithms: EG2, CS-ID3, IDX, and ICET on eight UCI data sets. (a) Ecoli.
(b) Breast. (c) Heart. (d) Cars. (e) Australia. (f) Thyroid. (g) Voting. (h) Mushroom.

EG2 [20], CS-ID3 [21], and IDX [22]. In this section, we
compare with these systems empirically.

EG2 [20] is a top-down decision-tree induction algorithm
that uses the Information Cost Function (ICF ðAi Þ) for

636

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,

VOL. 18, NO. 5,

MAY 2006

TABLE 4
Attribute Names and Test Costs of the Insurance Data Set

selecting an attribute Ai , instead of the information gain
used in C4.5. The selection score for the attribute Ai is
defined as
ICF ðAi Þ ¼

2IðAi Þ  1
:
ðcostðAi Þ þ 1Þ!

ð6Þ

We implemented EG2 using this equation. In this equation,
the value of ! needs to be determined. In the EG2 system,
the value of ! is set to a constant value of one. In ICET [19],
this value is learned through a genetic algorithm. We have
implemented both EG2 and ICET for the comparison.
In addition, we have implemented and compared with
the CS-ID3 system of [21]. Our implementation uses the
following formula for attribute selection:
CSID3ðAi Þ ¼

IðAi Þ2
:
costðAi Þ

ð7Þ

Finally, the IDX [22] system uses a similar but simpler
heuristic
IDXðAi Þ ¼

IðAi Þ
:
costðAi Þ

ð8Þ

We have done a comprehensive set of experiments to
compare both our test-cost sensitive decision tree and naive
Bayesian algorithms with these previous works. Furthermore, we have tested our system on a new real-world
domain. The results are shown in Fig. 6.
Fig. 6 shows the comparison against previous methods.
As can be seen, our methods csDT and csNB both preform
much better on the UCI domains Ecoli, Heart, Australia,
Thyroid, and so on, for increasing noise ratios. csNB
performs the best in almost all the testing domains. For
these tests, we used different settings of FP and FN costs:
F P ¼ 400 and F N ¼ 200. The attribute costs are the same as

Fig. 7. Comparisons on the Insurance data set.

that assumed in Section 4.1. Our conclusion is that both of
our proposed algorithms work well on different domains
with different levels of noise.

6.3 Evaluation on Real Data
In order to test the system’s ability to handle real-world
data, we have also tested all algorithms on an insurance
data set collected with an insurance company in Canada. It
consists of more than 900 records for customers who have
the status of “stay” or “leave” the insurance company. Our
target is to determine whether the customer will continue to
stay with the insurance company or not. We will refer to
them as positive and negative, respectively. The data set is
described by more than 10 attributes that are associated
with costs. The FP cost is 400 and the FN cost is 200. Some
attributes and their costs are listed in Table 4. The idea is
that, when some attributes have missing value, it costs the
insurance company extra resources to obtain their values.
Thus, in order to reduce costs, it is natural to apply testcost-sensitive learning. In the test, we used three-fold cross
validation.
The result of the empirical tests are shown in Fig. 7. As
can be seen, both of our methods again wins by a large
margin over the previous methods. This result demonstrates the effectiveness of csDT and csNB in their ability to
reduce the total costs in a real-world setting.
6.4 Batch Test Strategy
Batch test is also investigated by conducting several
experiments. In order to compare CSNB with CSDT in
terms of their abilities in batch test strategies, set the
parameters Ttotal given in Algorithm 5.

Fig. 8. Comparisons in batch test on all the eight data sets.

YANG ET AL.: TEST-COST SENSITIVE CLASSIFICATION ON DATA WITH MISSING VALUES

637

REFERENCES
[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]
Fig. 9. Comparisons in batch test with varying missing rates.

The results of average total cost on the eight data sets with
20 percent missing rate are shown in Fig. 8. The resource
constraint Ttotal is set to 100. Overall, csNB-batch outperforms csDT-batch greatly. This reveals that, although both
algorithms aim to minimize the total cost, csNB trades off the
misclassification cost and the test costs much better than
csDT, especially when resource constraints are imposed.
Fig. 9 shows the two runs on the Breast and Mushroom data sets with the variation of percentage of
unknown attributes. As we can see, csNB-batch is less
sensitive to the missing-value rate and performs much
better than csDT-batch.

[10]
[11]
[12]
[13]
[14]
[15]
[16]
[17]

7

CONCLUSIONS

AND

FUTURE WORK

In this paper, we proposed a test-cost-sensitive earning
framework for designing classifiers that minimize the sum
of the misclassification cost and the test costs. In the
framework of TCSL, attributes are selected for testing
intelligently to get both the sequential test strategy and the
batch test strategy. Experiments show that our method
outperforms other competing algorithms. We observe that
the decision tree algorithm we use here is a rather simpleminded one in that it simply follows the tree sequentially to
obtain a next missing value. In the future, we plan to
consider more sophisticated test strategies for decision
trees. We will also consider the conditional test costs [3] in
which the cost of a certain test is conditional on the other
attributes. For example, in medical diagnosis, the cost of an
exercise stress test on a patient may be conditional on
whether the patient has heart disease or not. Another
direction is to consider group tests where the cost of
performing tests on a group of samples are cheaper than the
sum of the costs spent individually.

ACKNOWLEDGMENTS
The authors would like to thank Hong Kong RGC for their
support under grant No. HKUST 6187/04E. They also thank
the anonymous referees for their comments.

[18]
[19]
[20]
[21]
[22]

T.M. Mitchell, Machine Learning. McGraw Hill, 1997.
J.R. Quinlan, C4.5: Programs for Machine Learning. Morgan
Kaufmann Publishers, 1993.
P.D. Turney, “Types of Cost in Inductive Concept Learning,” Proc.
Workshop Cost-Sensitive Learning at the 17th Int’l Conf. Machine
Learning, 2000.
C. Elkan, “The Foundations of Cost-Sensitive Learning,” Proc. 17th
Int’l Joint Conf. Artificial Intelligence, pp. 973-978, 2001.
P. Domingos, “Metacost: A General Method for Making Classifiers Cost-Sensitive,” Knowledge Discovery and Data Mining, pp. 155164, 1999.
M.T. Kai, “Inducing Cost-Sensitive Trees Via Instance Weighting,”
Principles of Data Mining and Knowledge Discovery, Second European
Symp., pp. 139-147, 1998.
M. Nunez, “The Use of Background Knowledge in Decision Tree
Induction,” Machine Learning, vol. 6, pp. 231-250, 1991.
M. Tan, “Cost-Sensitive Learning of Classification Knowledge and
Its Applications in Robotics,” Machine Learning J., vol. 13, pp. 7-33,
1993.
C. Ling, Q. Yang, J. Wang, and S. Zhang, “Decision Trees with
Minimal Costs,” Proc. 2004 Int’l Conf. Machine Learning, 2004.
P.D. Turney, “Cost-Sensitive Classification: Empirical Evaluation
of a Hybrid Genetic Decision Tree Induction Algorithm,”
J. Artificial Intelligence Research, vol. 2, pp. 369-409, 1995.
V.B. Zubek and T.G. Dietterich, “Pruning Improves Heuristic
Search for Cost-Sensitive Learning,” Proc. 19th Int’l Conf. Machine
Learning, pp. 27-34, 2002.
R. Greiner, A. Grove, and D. Roth, “Learning Cost-Sensitive
Active Classifiers,” Artificial Intelligence J., vol. 139, no. 2, pp. 137174, 2002.
X. Chai, L. Deng, Q. Yang, and C.X. Ling, “Test-Cost Sensitive
Naive Bayesian Classification,” Proc. 2004 IEEE Int’l Conf. Data
Mining (ICDM ’04), Nov. 2004.
R.O. Duda, P.E. Hart, and D.G. Stork, Pattern Classification, second
ed. Wiley and Sons, Inc., 2001.
P. Domingos and M. Pazzani, “On the Optimality of the Simple
Bayesian Classifier Under Zero-One Loss,” Machine Learning,
vol. 29, pp. 103-130, 1997.
T. Cormen, C. Leiserson, R. Rivest, and C. Stein, Introduction to
Algorithms, second ed. McGraw Hill and MIT Press, 2001.
C.L. Blake and C.J. Merz, “UCI Repository of Machine Learning
Databases,” http://www.ics.uci.edu/~mlearn/MLRepository.
html, 1998.
U.M. Fayyad and K.B. Irani, Multi-Interval Discretization of
Continuous-Valued Attributes for Classification Learning, pp. 10221027. Morgan Kaufmann, 1993.
P.D. Turney, “Cost-Sensitive Classification: Empirical Evaluation
of a Hybrid Genetic Decision Tree Induction Algorithm,”
J. Artificial Intelligence Research (JAIR), vol. 2, pp. 369-409, 1995.
M. Núñez, “Economic Induction: A Case Study,” Proc. European
Working Session on Learning, pp. 139-145, 1988.
M. Tan, “Cost-Sensitive Learning of Classification Knowledge and
Its Applications in Robotics,” Machine Learning, vol. 13, pp. 7-33,
1993.
S.W. Norton, “Generating Better Decision Trees,” Proc. Int’l Joint
Conf. Artificial Intelligence, pp. 800-805, 1989.

Qiang Yang received the PhD degree from the
University of Maryland, College Park. He is a
faculty member in the Hong Kong University of
Science and Technology’s Department of Computer Science. His research interests are AI
planning, machine learning, case-based reasoning, and data mining. He is a senior member of
the IEEE and an associate editor for the IEEE
Transactions on Knowledge and Data Engineering and IEEE Intelligent Systems. His homepage
is http://www.cs.ust.hk/~qyang.

638

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,

Charles Ling received the MSc and PhD
degrees from the Department of Computer
Science at the University of Pennsylvania in
1987 and 1989, respectively. Since then, he has
been a faculty member of computer science at
the University of Western Ontario, Canada. His
main research areas include machine learning
(theory, algorithms, and applications), cognitive
modeling, and AI in general. He has published
more than 100 research papers in journals (such
as Machine Learning, JMLR, JAIR, IEEE Transactions on Knowledge
and Data Engineering, and Cognition) and international conferences
(such as IJCAI, ICML, and ICDM). He has been an associate editor for
IEEE Transactions on Knowledge and Data Engineering and a guest
editor for several journals. He is also the director of Data Mining Lab,
leading data mining development in CRM, Bioinformatics, and the
Internet. He has managed several data mining projects for major banks
and insurance companies in Canada. See http://www.csd.uwo.ca/
faculty/cling for more info.

VOL. 18, NO. 5,

MAY 2006

Xiaoyong Chai received the MPhil degree in
computer science from the Hong Kong University of Science and Technology in 2005.
Currently, he is a PhD student in the Department
of Computer Sciences at Purdue University. His
research interests include artificial intelligence
and data mining.

Rong Pan received the BSc and PhD degrees in
applied mathematics from Zhongshan University, China, in 1999, and 2004, respectively. He
is a postdoctoral fellow at the Hong Kong
University of Science and Technology. His
research interest includes machine learning,
data mining, and case-based reasoning. His
homepage is http://www.cs.ust.hk/~panrong.

. For more information on this or any other computing topic,
please visit our Digital Library at www.computer.org/publications/dlib.

arXiv:1611.01982v2 [cs.CV] 16 Nov 2016

Chinese/English mixed Character Segmentation as Semantic Segmentation
Huabin Zheng
Sun Yat-sen University
Guangzhou, China

Jingyu Wang
iPIN
Shenzhen, China

Zhengjie Huang
Sun Yat-sen University
Guangzhou, China

zhhuab@mail2.sysu.edu.cn

wangjingyu@ipin.com

hzhengj@mail2.sysu.edu.cn

Yang Yang
iPIN
Shenzhen, China

Rong Pan
Sun Yat-sen University
Guangzhou, China

yangyang@ipin.com

panr@sysu.edu.cn

Abstract

mixed case is especially difficult due to the coexistence of
touching characters and Chinese disconnected structure, as
shown in Figure 1. For those ignorant of both languages, it’s
confusing that a Chinese character with disconnected structure (e.g. those in Figure 1) should not be splitted apart,
but a pair of touching neighboring English characters (e.g.
“DL” or “AI”) should be splitted apart. Traditional projection based method will falsely break up an intact Chinese
character with disconnected structure. A more advanced
method [13] with a connected regions merging phase, tends
to falsely take “DL” as an intact character. In order to correctly perform segmentation, a model should implicitly or
explicitly remember all valid characters in both languages.
Moreover, in order to deal with various possible font types
and sizes, the model should automatically learn necessary
features to recognize a valid character since it is cumbersome to specifically design features for every font.
Nowadays we know the ability of deep neural networks
to perform automatic feature learning on raw data has significantly advanced the research in various fields of computer vision. Semantic segmentation is among these fields.
Fortunately, the problem of multilingual character segmentation can be reframed as the problem of two-class semantic
segmentation. To be specific, given a text line image, we
classify each horizontal pixel into two categories: splitting
point or not. With this problem re-defined, we can utilize
those successful deep architectures in recent progress of semantic segmentation. Among them we choose fully convolutional networks (FCN).
In this paper, we reframe OCR character segmentation
as semantic segmentation and propose a FCN architecture
to solve it. We train our FCN model on synthesized samples
with simulated random disturbance and show that it is able
to

OCR character segmentation for multilingual printed
documents is difficult due to the diversity of different linguistic characters. Previous approaches mainly focus on
monolingual texts and are not suitable for multilinguallingual cases. In this work, we particularly tackle the Chinese/English mixed case by reframing it as a semantic segmentation problem. We take advantage of the successful architecture called fully convolutional networks (FCN) in the
field of semantic segmentation. Given a wide enough receptive field, FCN can utilize the necessary context around
a horizontal position to determinate whether this is a splitting point or not. As a deep neural architecture, FCN can
automatically learn useful features from raw text line images. Although trained on synthesized samples with simulated random disturbance, our FCN model generalizes well
to real-world samples. The experimental results show that
our model significantly outperforms the previous methods.

1. Introduction
Character segmentation plays an important role in optical character recognition (OCR) pipeline [18]. One major reason for poor recognition accuracy in OCR system
is the error in character segmentation. Some previous researches [1, 2, 5, 6, 13, 16, 17, 19, 23] achieve high performance on monolingual texts, but rely on feature engineering
specific to single character style. Other researches [4, 10,
24, 25] work on multilingual cases but introduce complex
processing pipelines. Actually, it’s difficult to manually design a set of features suitable for multilingual scene. Thus
a mixture of multiple languages presents a challenge for
existing character segmentation methods. Chinese/English

1. significantly outperform previous methods on Chi1

Figure 1. The main challenge of Chinese/English mixed character segmentation arises from the coexistence of disconnected structure in
Chinese and touching characters. Correct and wrong segments are colored as blue and red, respectively.

nese/English mixed printed document images;

it is easy to control the output size of FCN via deconvolution. Therefore, FCN is also widely used in tasks where
both input and output are images. For example, Simo-Serra
et al. [20] use FCN to simplify sketch drawing.

2. generalize well from simulated disturbance to realword disturbance introduced by photographing;
3. generalize well across different text content styles;
4. generalize well across different font styles in most
cases;
5. nicely handle disconnected structure and touching
characters.

(a) Several training samples

2. Related Work
Previous Approaches Projection based method is
among the simplest approach for OCR segmentation. It calculates the average grey value for each pixel column then
split every blank region in the middle, making it vulnerable to disconnected structure and touching characters. Recently, improved methods have been proposed but are only
specific for single language [1, 2, 5, 6, 13, 16, 17, 19, 23].
Other researches exploit complex processing pipelines and
hand-crafted rules to tackle multilingual cases [4, 10, 24,
25]. There are also researches on handwritten character
segmentation [26, 22, 9, 3]. Compared with printed characters, handwritten characters often require nonlinear splitting
paths rather than vertical splitting lines, which is not necessary for regular font types in normal printed documents.
Semantic Segmentation Semantic segmentation is a
sub-field of computer vision. Compared with recognition
problem, it progresses from coarse to fine inference by making a prediction directly at every pixel [12, 15]. To this end
it requires the output size of model to match original input
size. However, normal convolutional layer used in recognition only maintains or reduces the size of feature maps, so
comes the deconvolution layer.
Devonlutional Layers Deconvolution is also called upconvolution [20] or fractionally-strided convolution [8]. It
is typically used for expanding the size of feature maps in
FCN architecture.
Fully Convolutional Networks FCN is prevalent in the
research of semantic segmentation and object detection.
The key feature that distinguishes FCN from CNN is that

(b) Zoom in to a region

Figure 2. (a) Several training samples and corresponding output
ground truths. Notice that the model outputs a vector of length W
rather than a matrix of shape H × W . (b) For each character in
the image, the left and right margins are given as splitting points.
Each splitting point is visualized as a vertical blue lines.

3. Proposed Approach
We firstly define the training task in Section 3.1. In Section 3.2 we propose the detailed architecture of FCN. In
Section 3.3 we describe the post-processing phase when using the trained model to crop segments. Training data synthesizing process is described in Section 3.4. To deal with
imbalanced classes problem, we use a dynamic weighted
binary cross entropy loss, which is defined in Section 3.5.

3.1. Training Task Definition
In semantic segmentation form, our model is to classify
each horizontal pixel position into two classes: splitting
point as positive class and non-splitting point as negative
class. Formally, given an image of height H and width W
as input, a FCN outputs a probability vector p of length W ,
2

where
pi =


1


0

if the image should be splitted
at the i-th column;
otherwise.

(1)

pi = 0, i = m + 1, m + 2, . . . , n − 1.

Hidden Size

image

1 x 48 x 2048

down-conv, reduce 2x2,   channel 32 32 x 24 x 1024

For each character in the image, the left and right margins are given as splitting points. For example, if a character has an extending range from the m-th column to the n-th
column, then we have
pi = 1, i = m, n,

Blocks

down-conv, reduce 2x2,   channel 64

64 x 12 x 512

down-conv, reduce 2x2,   channel 128

128 x 6 x 256

(2)
down-conv, reduce 2x2,   channel  256

256 x 3 x 128

See Figure 2 for several input images and corresponding
output ground truths. In this paper we have H = 48 and
W = 2048.

down-conv, reduce 3x2,   channel 512

512 x 1 x 64

3.2. Architecture

up-conv, expand 1x2， channel 512

512 x 1 x 128

up-conv, expand 1x2， channel 256

256 x 1 x 256

up-conv, expand 1x2， channel 128

128 x 1 x 512

up-conv, expand 1x2， channel 64

64 x 1 x 1024

up-conv, expand 1x2， channel 1

1 x 1 x 2048

flatten

2048

In a typical FCN architecture, a down-convolution block
and an up-convolution block are to reduce and expand
the size of feature maps, respectively [20]. Each downconvolution is composed of a convolutional layer, a batch
normalization layer [7], a max-pooling layer and an activation layer. Each up-convolution is composed of a deconvolutional layer, a batch normalization layer and an activation
layer. In research of semantic segmentation, FCN is used
to restore both the width and height of original input images [12, 15].
Our FCN architecture is almost the same as typical ones
except that only the width of input images need to be restored. As defined in Section 3.1, Eq. (1), it simply outputs
a vector of length W , which is equivalent to an image of
shape 1×W . To this end, deconvolutional layers in our FCN
expand the width of feature maps but maintain the height.
See Figure 3 for details.
All activation layers except the last one are the ReLU
layer [14]. The last one is a sigmoid layer producing a probability output vector.

probability output

Figure 3. Our FCN architecture is composed of 5 downconvolution blocks and 5 up-convolution blocks. The activation
layer of the last down-convolution block is a sigmoid layer.

vector output p of FCN is converted to binary vector according to a threshold (e.g. 0.5). Secondly, for each contiguous positive segments, the center point is selected as
the splitting point. Thirdly, each pair of adjacent splitting
points form a candidate segments. Finally, we discard the
blank segments and output a list of bounding line segments.
Complete pipeline is shown in Figure 4.

3.3. Post-processing for Cropping
As described in Section 3.1, during training phase only
two splitting points are given as positive ground truth for
each character. During prediction, however, points in adjacent region of a true positive point are usually classified to
be positive as well. It’s inevitable because neural networks
cannot fit the data exactly. In fact, those surrounding points
could also be valid splitting points.
Another problem arises when we want to really crop out
characters for downstream recognition, because a segment
bounded by two adjacent splitting points could contain a
character or just the blank between characters. The blank
segments should be discarded.
Therefore, we propose a simple post-processing procedure to deal with the issues above. Firstly, the probability

3.4. Synthesizing Training Data
We focus on camera photographs of Chinese/English
mixed printed documents with various fonts. Compared
with using scanner, camera photographing introduces much
more noises, blurring, rotation and distortion, thus makes
the problem more challenging. To approximate these realworld disturbance, we first plot clean texts onto blank image
with a size of 2048 × 48 then successively apply four simulated random disturbance: rotation, erosion, dilation and
3

we define a weighted binary cross entropy loss function
X
X
L(p, q) = −α
log pi − β
log(1 − pi ), (4)

(a) Partial input image

i,qi =1

i,qi =0

where α + β = 1.
We initialize α to 0.9 and β to 0.1. After each iteration
we use a heuristic rule to dynamically adjust the weights
according to the average positive accuracy accpos and the
average negative accuracy accneg of the last mini-batch,
where
X
X
accpos =
1(pi > 0.5)/
1,
(5)

(b) FCN’s output probability

(c) Step 1: thresholding

(d) Step 2: select middle points

i,qi =1

accneg =

X

i,qi =1

1(pi < 0.5)/

i,qi =0

(e) Step 3: get candidate segments

Figure 4. Post-processing steps. (a) Partial input image to FCN.
(b) Corresponding part of FCN’s output probability vector. (c)
Step 1: convert the probability vector into binary vector according
to a threshold. (d) Step 2: for each contiguous positive segments,
select the center point as splitting point. (e) Step 3: each pair
of adjacent splitting points form a candidate segment. (f) Step 4:
discard the blank segments and finally output a list of bounding
line segments.

3.5. Dynamic Weighted Binary Cross Entropy
Our model performs binary classification at each of the
2048 horizontal positions. Therefore, given FCN’s output
probability vector p and the ground truth vector q, we can
simply define the binary cross entropy loss function as
L(p, q) = −

i,qi =1

log pi −

log(1 − pi ).

(6)

Algorithm 1 Heuristic Rules for Dynamic Loss
1: α ← 0.9
2: β ← 0.1
3: while training do
4:
get mini-batch data for this iteration
5:
update model towards minimizing Eq. (4)
6:
compute accpos according to Eq. (5)
7:
compute accpos according to Eq. (6)
8:
if accpos < accneg then
9:
δ ← min(β, 0.001)
10:
α←α+δ
11:
β ←β−δ
12:
else
13:
δ ← min(α, 0.001)
14:
α←α−δ
15:
β ←β+δ
16:
end if
17: end while

Gaussian blurring. Finally we binarize the grey-scale images according to threshold 160. We keep track of the left
and right margins of each character in process above and
finally convert them into corresponding binary mask vector
as ground truth. Several training input images and output
vectors are visualized in Figure 2.

X

1.

i,qi =0

If accpos < accneg then we increase α and decrease β, otherwise we increase β and decrease α. Our strategy can
balance the model performance on positive and negative
classes throughout training and speed up convergence. See
Algorithm 1 for details.

(f) Step 4: discard blank segments

X

X

4. Experiments
4.1. Datasets

(3)

i,qi =0

In this section, we describe the datasets for experiments,
including those built by photographing printed documents
and those synthesized as described in Section 3.4.

However, the ratio of positive and negative ground truths
is not balanced. Most of the ground truths are negative as
shown in Figure 2. In our experiments unbalanced classes
slow down model convergence or even make the model
stuck in a local optimum. When stuck, the model predicts
negatively at each horizontal position. To tackle this issue,

4.1.1

Photographed Dataset

The first dataset is built by photographing as follows.
Firstly, text contents are randomly extracted from Baidu
4

Pi matches Tj if and only if

Baike corpus, printed with various font types, and photographed with normal phone camera. Secondly, we apply
a series of traditional OCR techniques of denoising, binarization, line segmentation etc. to collect a set of text line
images. Finally, we hand label the bounding line segment
annotations for each character.
Because each text line image sample typically contains
several dozens of characters, it takes a long time to annotate
even one sample. Thus only 50 text line image samples are
finally collected. Nevertheless, they totally contains 2710
characters, which are enough for reliable evaluation. In the
following sections, we refer this dataset as “Photo-Normal”.
Several samples are shown in Figure 5.

min(ui,1 , ui,2 , . . . , ui,N ) = ui,j ,

(7)

max(ci,1 , ci,2 , . . . , ci,N ) = ci,j ,

(8)

ui,j < t1 ,

(9)

ci,j > t2 ,

(10)

max(ci,1 , ci,2 , . . . , ci,j−1 , ci,j+1 , . . . , ciN ) < t3 ,

(11)

where t1 , t2 , t3 are thresholds.
Taking t1 = t3 = 0 is equivalent to requiring Pi exactly
matched with Tj . However, exact matching is not necessary in practice due to some blank space between characters. Therefore we take t1 = 8, t2 = 0 and t3 = 5 in our
experiments for all the methods we compare, which is fair.
Given the number of matched pairs K, we define the segmentation accuracy as
acc =

Figure 5. Several samples of Photo-Normal Dataset.

4.1.2

K
.
max(M, N )

(12)

4.3. Hyperparameters
We use mini-batch stochastic gradient descent for 50000
iterations with batch size 8 and momentum coefficient 0.9.
For each iteration a mini-batch of samples are randomly selected from training samples. Learning rate is initialized to
0.0001 and divided by 10 at the 20000-th and 40000-th iteration. Training takes approximately 50 minutes on a single
GPU (NVIDIA GeForce GTX TITAN X).

Synthesized Datasets

The quantity and diversity of Photo-Normal dataset is somewhat limited. Moreover, it can only evaluate our model’s
generalization ability from simulated disturbance to realworld disturbance. To further evaluate its generalization
ability, we synthesize a series of datasets with the combination of two text content styles and 36 font types (specified
in next paragraph). Each combination is splitted into training and evaluation parts, producing totally 144 (2 × 2 × 36)
datasets. Among them, one training set contains 3000 samples and one evaluation set contains 30 samples. Total number of characters is more than 10 million.
As for text content styles, the first style called “normal”
simply refers to normal content in Baidu Baike corpus, and
the second style called “chaotic” is acquired by randomly
shuffling normal text characters. As for font styles, see Table 3 for totally 36 font types used in our experiments.
In the following sections, we refer a dataset of normal text content and font SIMYOU for training as “TrainNormal-SIMYOU”, and so on.

4.4. Quantitative and Qualitative Evaluation
In this section, we quantitatively evaluate whether our
model can generalize
1. from simulated disturbance to real-word disturbance,
2. between normal text content and chaotic text content,
3. among different font types.
We also qualitatively evaluate its performance on the
hardest part of Chinese/English mixed case: coexistence of
disconnected components and touching characters.
The generalization over font sizes is trivial and already
included throughout these experiments, thus not specifically
evaluated.

4.2. Evaluation Metric
We evaluate segmentation accuracy by matching predictions and ground truths. Given a text line image, FCN and
post-processing procedure output a list of M bounding line
segments. They are aligned with the ground truth, which is
a list of N bounding line segments. For each predicted segment Pi and each true segment Tj , we denote the number of
Tj ’s horizontal pixels covered by Pi as ci,j and the number
of Tj ’s horizontal pixels not covered as ui,j . Then we say

4.4.1

Generalization from Simulation to Real-World

Our FCN is trained on synthesized data because it’s difficult
to collect a large amount of real word samples with segment
annotations. However, simulated disturbance in synthesized
samples is definitely not identical to real-world disturbance.
FCN must generalize well beyond simulation to deal with
photographed printed document images.
5

To verify this, we train three FCN instances on three
datasets and evaluate them on Photo-Normal dataset. Training sets are

Training Set
Train-Normal-All
Train-Chaotic-All
Train-All-All

1. Train-Normal-All: training samples of normal text,
2. Train-Chaotic-All: training samples of chaotic text,

Table 2. Evaluate generalization ability between normal and
chaotic text content styles.

3. Train-All-All: union of both above.
Sample counts are 108000, 108000 and 216000, respectively. Each of them contains all the 36 font types.
In this experiment we compare our approach with four
baselines: the traditional projection based method, the
region-merging based method designed for Chinese [13],
the connected component based method designed for English [2] and Tesseract [21], an open source OCR engine
still in active development1 . In the following sections they
are referred to as PROJ, CN, EN and Tesseract, respectively.
The results are shown in Table 1. Our FCN instances significantly outperform the baselines. Among the first three
baselines, PROJ is the simplest model but outperforms CN
and EN, because it is not specifically designed for one single language. As a morden OCR engine targeting various
languages, Tesseract achieves a decent accuracy but is still
outperformed by a large margin. Among the three FCN instances, the one trained on Train-Normal-All achieves the
best result, because it has the most similar text content style
with Photo-Normal dataset.
All FCN instances achieve over 98% accuracy. Thus we
conclude that they generalize well from simulated disturbance to real-world disturbance.
Model
PROJ
CN [13]
EN [2]
Tesseract [21]
Train-All-All
Train-Normal-All
Train-Chaotic-All

FCN overfits certain style of text content. For example, if
character A is always surrounded by B and C in training
text content, FCN may fit such pattern. When it comes to
testing text content in which A is surrounded by D end E,
FCN will probably make a mistake. We want to know how
serious this problem is.
In this section, five datasets are used for training and
evaluation:
1. Train-Normal-All: training samples of normal text,
2. Train-Chaotic-All: training samples of chaotic text,
3. Train-All-All: union of above,
4. Eval-Normal-All: evaluation samples of normal text,
5. Eval-Chaotic-All: evaluation samples of chaotic text.
Sample counts are 108000, 108000, 216000, 1080 and
1080, respectively.
The results are shown in Table 2. The best performance
on Eval-Normal-All and Eval-Chaotic-All are achieved by
training on Train-Normal-All and Train-Chaotic-All, respectively. The first two rows show that our model generalizes well from chaotic style to normal style and only
slightly worse from normal style to chaotic style.
This experiment suggests that in practice, if the text content style we want to finally work on can be accessed, it is
optimal to train on the same content style. If not, training
on chaotic text content still works well.
Note that all of the results in Table 2 are worse than results in Table 1 because the our simulated disturbance setup
is actually more difficult than real-world disturbance.

Accuracy
83.5
72.7
76.0
90.4
98.1
98.6
98.2

Table 1. Segmentation accuracy on Photo-Normal dataset. “TrainXXX-All” refers to which dataset our FCN model is trained on.
Our model significantly outperforms baselines. The best result
is acquired by training on Train-Normal-All, which has the most
similar text content style with Photo-Normal dataset.

4.4.2

Evaluation Set
Eval-Normal-All Eval-Chaotic-All
97.8
96.0
97.3
97.4
97.5
97.2

4.4.3

Generalization across Font Types

Real-world documents contain various font types. In practice we can include as much font types as possible in training sets to improve generalization. However, some particular fonts of interest may still not be included, which requires
the model to generalize across different font types.
To evaluate this generalization ability, three groups of
datasets are used in this section:

Generalization across Text Content Styles

FCN has the advantage to utilize a wide receptive field on
input image to predict at a horizontal pixel. This advantage
could be a disadvantage, because it introduces the risk that
1 https://github.com/tesseract-ocr/tesseract

1. Train-Normal-All: training samples of normal text,
6

Font
AdobeFangsongStd-Regular
AdobeHeitiStd-Regular
AdobeKaitiStd-Regular
AdobeSongStd-Light
Baoli
Hannotate
Hanzipen
Hiragino-Sans-GB-W3
Hiragino-Sans-GB-W6
Kaiti
Lantinghei
Libian
SIMLI
SIMYOU
STCAIYUN
STFANGSO
STHUPO
STHeiti-Light
STKAITI
STLITI
STSONG
STXIHEI
STXINGKA
STXINWEI
Songti
WawaSC-Regular
WeibeiSC-Bold
Yuanti
YuppySC-Regular
msyh
msyhbd
simfang
simhei
simkai
simsun
stzhongs

Exclude
98.7
98.6
98.1
99.0
97.5
98.4
95.9
98.7
98.9
98.6
96.9
97.9
98.8
99.2
51.9
98.0
93.2
98.3
98.8
74.0
98.2
96.4
72.2
98.2
98.6
95.2
98.9
97.2
97.6
96.8
97.1
99.7
98.5
98.3
98.7
99.6

All
99.5
98.6
99.1
99.4
98.1
98.8
96.3
98.9
98.3
98.3
97.6
98.1
99.3
99.3
94.7
98.2
96.9
97.7
99.3
83.8
98.8
96.7
91.6
98.6
98.8
97.1
98.6
98.0
98.2
98.7
98.5
99.9
99.1
98.4
98.9
99.8

train FCN on dataset that does not include this font type and
dataset that does, corresponding to the second column and
third column. The second column shows that FCN generalizes well on unseen font types for the most cases. The
third column shows that including corresponding font type
during training further improves accuracy.
Nevertheless, there are several bad cases highlighted in
the table: STCAIYUN, STLITI and STXINGKA. Their
font styles are illustrated in Figure 6.

(a) STCAIYUN

(b) STLITI

(c) STXINKA

Figure 6. Partial samples of three particular font types on which
FCN generalizes badly.

Table 3. Evaluate generalization ability across font styles. For each
font XXX in the first column, we train a FCN instance on TrainNormal-exclude-XXX dataset and another on Train-Normal-All
dataset. Then we evaluate both on corresponding Eval-Normalonly-XXX dataset. Results are shown in the second and third column, respectively. Bad cases are highlighted.

(a) STLITI

(b) STXINKA

3. Eval-Normal-only-XXX: evaluation samples of normal text, containing only one font type XXX.

Figure 7. Partial segmentation results on STLITI and STXINGKA. Bounding line segments above and below are predictions
and ground truths, respectively. Both FCN instances are trained on
Train-Normal-All dataset but still perform illy. Most errors arise
from italic English characters.

Sample counts of each dataset in the three groups are
108000, 105000 and 1080, respectively.
The results are shown in Table 3. For each font type, we

The first font type, STCAIYUN, is completely different from others because of its hollow structure. However,
when it is included in training set, segmentation accuracy

2. Train-Normal-exclude-XXX: training samples of normal text, containing font types except XXX,

7

Figure 8. Comparison of projection-based approach and our approach. In each sample there are three rows of bounding line segments,
corresponding to ground truths, projection-based method’s outputs and our model’s outputs, respectively. Correct and wrong bounding
line segments are colored as blue and red, respectively. In the last sample FCN falsely splits character “m” apart probably because FCN
recognizes it as two touching “n”. Nevertheless, our FCN handles disconnected components and touching characters well in most cases.

5. Conclusion and Future Work

increases from 51.9% back to 94.7%. In this case, FCN
generalizes badly on particular font type but can restore decent accuracy once it is included again.
However, on the second and third font types, accuracy
cannot be restored even when they are included in training
set. As shown in Figure 7, Most segmentation errors arise
from English characters. This is because both STLITI and
STXINGKA have italic English character style.To properly
segment italic characters, the model should predict oblique
lines rather than vertical splitting lines, which is impossible
in our FCN architecture.
This experiment shows that our FCN generalizes well on
most cases except those with completely different styles or
italic styles. The first issue can be fixed by including such
special font types in training sets. As for the second issue,
we will discuss the possible solution in Section 5.
4.4.4

In this paper we tackle Chinese/English character segmentation for printed document images. By reframing it as
a two-class semantic segmentation problem, we take advantage of the successful deep neural architecture called fully
convolutional networks (FCN) in the field of semantic segmentation. Trained on synthesized samples with simulated
random disturbance, FCN can accurately perform binary
classification at each horizontal position on text line images
to decide whether this position should be a splitting point
or not. Our approach significantly outperforms traditional
methods on segmentation accuracy. Experiments show that
it is able to generalize from simulated disturbance to realworld disturbance, generalize between normal and chaotic
text content styles, generalize among various font types and
properly handle the coexistence of disconnected structure
and touching characters.

Handling Difficult Cases

The main challenges of Chinese/English mixed character
segmentation are two-fold: first, various character widths
inside and across languages and second, the coexistence of
disconnected structure and touching characters.
Without the first challenge, we can calculate the widths
of each connected components in a text line image then take
the mode as unified character width, which is used in traditional OCR techniques. Without the second challenge,
we can either tune the threshold in projection-based methods to tackle touching characters, or use a region-merging
phase [13] to tackle disconnected structure.
Nevertheless, our FCN architecture handles these difficulties well by automatically utilizing useful features. Typical samples are shown in Figure 8.

The experimental result in Section 4.4.3 shows that our
approach performs badly on characters of italic font type
because FCN simply predicts vertical splitting lines rather
than oblique splitting lines. In addition, there exist even
more difficult cases where two characters are so close that
they can only be splitted by curved lines. A possible solution and step forward is to reframe character segmentation
as an instance segmentation problem. Instance segmentation is also called simultaneous detection and segmentation.
In this task, instance-level information and pixel-wise accurate mask for objects are to be estimated [11]. Ideally, with
instance segmentation every single characters can be curved
out exactly and cleanly. In the future we will work on this
possible solution.
8

References

[15] H. Noh, S. Hong, and B. Han. Learning deconvolution network for semantic segmentation. In Proceedings of the IEEE
International Conference on Computer Vision, pages 1520–
1528, 2015.
[16] S. Nomura, K. Yamanaka, O. Katai, H. Kawakami, and
T. Shiose. A novel adaptive morphological approach for degraded character image segmentation. Pattern Recognition,
38(11):1961–1975, 2005.
[17] P. P. Roy, U. Pal, J. Lladós, and M. Delalandre. Multioriented touching text character segmentation in graphical
documents using dynamic programming. Pattern Recognition, 45(5):1972–1983, 2012.
[18] P. Sahare and S. B. Dhok. Review of text extraction algorithms for scene-text and document images. IETE Technical
Review, pages 1–21, 2016.
[19] D. Senapati, S. Rout, and M. Nayak. A novel approach to
text line and word segmentation on odia printed documents.
In Computing Communication & Networking Technologies
(ICCCNT), 2012 Third International Conference on, pages
1–6. IEEE, 2012.
[20] E. Simo-Serra, S. Iizuka, K. Sasaki, and H. Ishikawa. Learning to simplify: fully convolutional networks for rough
sketch cleanup. ACM Transactions on Graphics (TOG),
35(4):121, 2016.
[21] R. Smith and G. Inc. An overview of the tesseract ocr engine. In Proc. 9th IEEE Intl. Conf. on Document Analysis
and Recognition (ICDAR, pages 629–633, 2007.
[22] J. Tan, W.-X. Wang, M.-S. Feng, and X.-X. Zuo. A new
approach based on ncut clustering algorithm for signature
segmentation. AASRI Procedia, 1:14–20, 2012.
[23] J. Tse, C. Jones, D. Curtis, and E. Yfantis. An ocrindependent character segmentation using shortest-path in
grayscale document images. In Machine Learning and Applications, 2007. ICMLA 2007. Sixth International Conference on, pages 142–147. IEEE, 2007.
[24] K. Wang, J. Jin, and Q. Wang. High performance chinese/english mixed ocr with character level language identification. In 2009 10th International Conference on Document Analysis and Recognition, pages 406–410. IEEE, 2009.
[25] K. Wang, J.-M. Jin, W.-M. Pan, G.-S. Shi, and Q.-R. Wang.
Mixed chinese/english document auto-processing based on
the periodicity. In Machine Learning and Cybernetics, 2004.
Proceedings of 2004 International Conference on, volume 6,
pages 3616–3619 vol.6, Aug 2004.
[26] S. Wshah, Z. Shi, and V. Govindaraju. Segmentation of arabic handwriting based on both contour and skeleton segmentation. In 2009 10th International Conference on Document
Analysis and Recognition, pages 793–797. IEEE, 2009.

[1] Y.-K. Chen and J.-F. Wang. Segmentation of single-or
multiple-touching handwritten numeral string using background and foreground analysis. IEEE Transactions on pattern analysis and machine intelligence, 22(11):1304–1317,
2000.
[2] A. Choudhary, R. Rishi, and S. Ahlawat. A new approach
to detect and extract characters from off-line printed images
and text. Procedia Computer Science, 17:434–440, 2013.
[3] N. Dave. Segmentation methods for hand written character
recognition. International Journal of signal processing, image processing and pattern rcognition, 8:154–164, 2015.
[4] U. Garain and B. B. Chaudhuri. Segmentation of touching characters in printed devnagari and bangla scripts using
fuzzy multifactorial analysis. IEEE Transactions on Systems,
Man, and Cybernetics, Part C (Applications and Reviews),
32(4):449–459, 2002.
[5] A. R. Himamunanto and A. R. Widiarti. Javanese character
image segmentation of document image of hamong tani. In
Digital Heritage International Congress (DigitalHeritage),
2013, volume 1, pages 641–644. IEEE, 2013.
[6] W. L. Hwang and F. Chang. Character extraction from documents using wavelet maxima. Image and Vision Computing,
16(5):307–315, 1998.
[7] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift.
In D. Blei and F. Bach, editors, Proceedings of the 32nd
International Conference on Machine Learning (ICML-15),
pages 448–456. JMLR Workshop and Conference Proceedings, 2015.
[8] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for
real-time style transfer and super-resolution. arXiv preprint
arXiv:1603.08155, 2016.
[9] A. Kaur, S. Rani, and P. Singh. Segmentation of isolated
and touching characters in handwritten gurumukhi word using clustering approach. 2014.
[10] S.-W. Lee, D.-J. Lee, and H.-S. Park. A new methodology
for gray-scale character segmentation and recognition. IEEE
Transactions on Pattern Analysis and Machine Intelligence,
18(10):1045–1050, 1996.
[11] S. Liu, X. Qi, J. Shi, H. Zhang, and J. Jia. Multi-scale patch
aggregation (mpa) for simultaneous detection and segmentation. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
[12] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pages 3431–3440, 2015.
[13] Y. Mei, X. Wang, and J. Wang. A chinese character segmentation algorithm for complicated printed documents. International Journal of Signal Processing, Image Processing
and Pattern Recognition, 6(3):91–100, 2013.
[14] V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In J. Frnkranz and T. Joachims,
editors, Proceedings of the 27th International Conference on
Machine Learning (ICML-10), pages 807–814. Omnipress,
2010.

9

NeuroImage 59 (2012) 2298–2306

Contents lists available at SciVerse ScienceDirect

NeuroImage
journal homepage: www.elsevier.com/locate/ynimg

A prior feature SVM-MRF based method for mouse brain segmentation
Teresa Wu a,⁎, Min Hyeok Bae a, Min Zhang a, Rong Pan a, Alexandra Badea b
a
b

School of Computing, Informatics and Decision Systems Engineering, Arizona State University, Tempe, Arizona 85287-5906, USA
Center for In Vivo Microscopy, Box 3302, Duke University Medical Center, Durham, NC 27710, USA

a r t i c l e

i n f o

Article history:
Received 1 June 2011
Revised 26 August 2011
Accepted 22 September 2011
Available online 1 October 2011
Keywords:
Automated segmentation
Magnetic resonance microscopy
Markov Random Field
Mouse brain
Support Vector Machine

a b s t r a c t
We introduce an automated method, called prior feature Support Vector Machine-Markov Random Field
(pSVMRF), to segment three-dimensional mouse brain Magnetic Resonance Microscopy (MRM) images.
Our earlier work, extended MRF (eMRF) integrated Support Vector Machine (SVM) and Markov Random
Field (MRF) approaches, leading to improved segmentation accuracy; however, the computation of eMRF is
very expensive, which may limit its performance on segmentation and robustness. In this study pSVMRF
reduces training and testing time for SVM, while boosting segmentation performance. Unlike the eMRF
approach, where MR intensity information and location priors are linearly combined, pSVMRF combines
this information in a nonlinear fashion, and enhances the discriminative ability of the algorithm. We validate
the proposed method using MR imaging of unstained and actively stained mouse brain specimens, and
compare segmentation accuracy with two existing methods: eMRF and MRF. C57BL/6 mice are used for training and testing, using cross validation. For formalin ﬁxed C57BL/6 specimens, pSVMRF outperforms both eMRF
and MRF. The segmentation accuracy for C57BL/6 brains, stained or not, was similar for larger structures like
hippocampus and caudate putamen, (~ 87%), but increased substantially for smaller regions like susbtantia
nigra (from 78.36% to 91.55%), and anterior commissure (from ~ 50% to ~ 80%). To test segmentation robustness against increased anatomical variability we add two strains, BXD29 and a transgenic mouse model of
Alzheimer's disease. Segmentation accuracy for new strains is 80% for hippocampus, and caudate putamen,
indicating that pSVMRF is a promising approach for phenotyping mouse models of human brain disorders.
© 2011 Elsevier Inc. All rights reserved.

Introduction
Precise delineation of human neuroanatomical structures helps in
the early diagnosis of a variety of neurodegenerative and psychiatric
disorders (Fischl et al., 2002). The importance of human brain
segmentation has given great momentum to the development of segmentation methods, and considerable progress has been made. In the
meantime, the study of mouse models has also drawn substantial
attention of the biomedical community due to the close evolutionary
relationship between humans and mice, which enables scientists to
use mouse mutants as models of human neurological disease, and
to understand structural and functional changes of human brains
(Bock et al., 2006; Kovacevic et al., 2005). For example, transgenic
mouse models which mimic neurodegenerative diseases were
investigated to study the functions of particular genes or other
defects, and to test novel therapeutic interventions (McDaniel et al.,
2001). However, developing automated segmentation methods for
mouse brain MR images is a difﬁcult task. First, the MR signal is proportional to the voxel volume (Edelstein et al., 1986), around 1 mm 3
for the human brain, but more than 100,000 times smaller in higher

⁎ Corresponding author. Fax: +1 480 965 2751.
E-mail address: teresa.wu@asu.edu (T. Wu).
1053-8119/$ – see front matter © 2011 Elsevier Inc. All rights reserved.
doi:10.1016/j.neuroimage.2011.09.053

resolution (21.5 μm) mouse brain images necessary to resolve detailed anatomical features. Improvements in imaging technology,
complemented with the use of T1 shortening contrast agents (Badea
et al., 2007; Dorr et al., 2008; Johnson et al. 2002) have allowed the
segmentation of more than 30 mouse brain structures based on MR
images (Kovacevic et al., 2005; Ma et al., 2005; Badea et al., 2007;
Dorr et al., 2008). These large image arrays (e.g. 1024 × 512 × 512 voxels, Badea et al., 2007) pose increasing computational demands.
Second, most studies using mouse models require large numbers of
animals to achieve statistical power for detecting subtle variations in
neuroanatomy. This requirement translates into a pressing need for
the development of high-throughput segmentation methods for 3D
brain images. The segmentation results should be robust, consistent
and with acceptable computational time. To handle these challenges,
we need to develop an automated mouse brain image segmentation
method that is accurate, reliable and fast.
Previous research on developing automated segmentation methods
for human and mouse brain images includes atlas based segmentation,
probabilistic information based segmentation, and machine learning
based segmentation. The atlas based segmentation method can involve
nonlinear registration of a manually labeled atlas image to a new image
set. The label of each voxel in the atlas image is elastically matched to
the image being segmented. The segmentation performance can be improved by using an average atlas obtained from multiple subjects

T. Wu et al. / NeuroImage 59 (2012) 2298–2306

instead of a single subject (Rohlﬁng et al., 2004). Most existing methods
for mouse brain segmentation have used the atlas based segmentation.
Ma et al. (2005) used six-parameter rigid-body transformation, and
nonlinear registration to segment T2*-weighted MRM images of
C57BL/6 mouse brains into 20 structures using an atlas image of a single
mouse brain. Kovacevic et al. (2005) used an average atlas for atlas
based segmentation of the MR images of 129S1/SvImJ mouse brain.
The probabilistic atlas based segmentation incorporates different
kinds of probabilistic information based on multi-spectral MR signals
(Fischl et al., 2002). The probabilistic information on MR intensity is
modeled as a Gaussian distribution. The prior probability of a label at
one voxel location in the 3D image provides the location prior, and
the pairwise probability of a labeling, given the labels of neighboring
voxels is deﬁned by the MRF theory. Ali et al. (2005) adapted Fischl's
method to segment T2, Proton Density (PD) and diffusion-weighted
MRM images of the C57BL/6 mouse brain into 21 neuroanatomical
structures.
Machine learning based segmentation was used in human brain
segmentation, and uses various classiﬁers to assign each voxel to a
number of classes. For example, Artiﬁcial Neural Network (ANN)
was used to segment MR images into three tissues types: white
matter, gray matter and cerebrospinal ﬂuid based on T1, T2 and PDweighted MR signal intensity (Reddick et al., 1997). Powell et al.
(2008) used probability map values, spherical coordinates, T1 and
T2-weighted MR signal intensity as input features for ANN and SVM
to segment MR images of human brains into eight structures. They
showed that machine learning based segmentation outperforms the
atlas or probability based segmentation methods. In our previous
work (Bae et al., 2010), we segmented MRM images of the C57BL/6
mouse brain into 21 neuroanatomical structures using an enhanced
SVM model, called Mix-Ratio sampling-based SVM (MRS-SVM),
which relieved the data imbalance problem in multiclass classiﬁcation. Only the location and MR intensity are used as features for the
SVM model. The results showed much improved performance compared to the atlas-based method and comparable classiﬁcation
performance to the probabilistic information based method for larger
structures (Bae et al., 2010).
Each segmentation method has its drawbacks. In the case of the
atlas based segmentation, registration errors can severely hurt the
overall segmentation performance (Sharief et al., 2008) since a poor
registration can cause structure mismatches and boundary blurring.
The probability information based segmentation uses MR intensity
information and contextual information based on neighbors' labels,
as well as location information, which depend on the registration
quality. The additional information — MR intensity information and
contextual information, could make up for the loss of segmentation
performance resulting from imperfect registration. Therefore, the
probability information based segmentation is less affected by the
registration quality than the atlas based segmentation. This is why
MRF, a class of probability theory modeling contextual dependencies
has been widely applied for image segmentation (Li, 2009). However,
the probability information based segmentation methods use a weak
classiﬁer, multivariate Gaussian distribution, to model the MR intensity information (Ali et al., 2005; Fischl et al., 2002). The contribution
of MR intensity information to the segmentation is undermined due
to the poor discriminative power of the classiﬁer. We proposed a
hybrid of probability information–machine learning based segmentation, termed eMRF (Bae et al., 2009) where SVM is employed to replace the weak classiﬁer in the probability information based
method. In the eMRF method, the overall segmentation performance
was improved by employing SVM to model the MR intensity information instead of Gaussian distribution. Using manual labeling as gold
standard, the eMRF method overall provides 10.05% higher percentage voxel overlap (VOP) and 23.84% less label volume difference
(VDP) compared with the atlas based segmentation, and 2.79% higher
percentage voxel overlap and 12.71% less label volume difference

2299

compared with the probability information based segmentation.
Note for labeling overlap, higher is better, for label volume difference,
less is better. While the machine learning based segmentation improves the segmentation accuracy, it requires enormous computation
time. The long training and testing time and the difﬁculty in model
parameter selection limit the practical application of the method to
large data-sets and large samples. Powell et al. (2008) reported that
it took a day to train a neural net for the classiﬁcation of one structure
from others even though they used a random sampled data
(500,000 voxels per structure) instead of using the whole data set.
It is known that the training time for SVM is approximated as O(N 4)
where N is the total number of training data points. For mouse
MRM images (128 × 128 × 256), N is over 16 million. Hence, in the
eMRF study, it took ~ 7 days for training and the 4.82 h for testing
using a 3.4-GHz PC. The classiﬁcation performance of classiﬁers largely depends on the selection of model parameters (e.g. kernel
functions and related parameters for SVM). To ﬁnd the best model
parameters for a data set, additional large number of runs with different parameter settings should be conducted. However, the long
training and testing time for the brain image segmentation make it
prohibitive to run large number of experiments, which implies that
the best performance of the machine learning based segmentation
would be difﬁcult to obtain due to computing concerns. The robustness of the algorithm for mutant mice which has large anatomical
variability is also difﬁcult to be assessed.
In this study, we develop a new algorithm that samples fewer
voxels, enabling the identiﬁcation of optimal parameters for the machine learning classiﬁer. This new algorithm is called prior feature
SVM-MRF (pSVMRF) which is robust and computationally efﬁcient.
pSVMRF integrates the good classiﬁcation ability of SVM into the MRF
image segmentation framework. Both voxel location prior and MR
intensity are used as input features for the training and testing of
SVM. Adding the location prior as the input features is inspired by
the previous work of Powell et al. (2008). The probabilistic outputs of
the prior feature SVM (pSVM) are treated as inputs to the MRF segmentation formula. The contribution of the SVM and contextual information is controlled with two model parameters. This is different from
the eMRF method, where the MR intensity information and location
prior are combined linearly by weights that are tuned by grid-search.
Since in the new approach the training sample size is small in each
experiment, we can easily run a large number of experiments to ﬁnd
the best SVM parameters to give the best and robust classiﬁcation
performance.
We assess segmentation performance and compare the new segmentation method with two other methods: MRF (Ali et al. 2005),
and eMRF (Bae et al., 2009) for the segmentation of MRM brain images
of adult C57BL/6 mice. To test the robustness of the algorithm when
faced with increased anatomical variability, we add two different
strains: BXD29, a recombinant inbred strain derived from an intercross
between C57BL/6 and DBA/2J, and a double transgenic mouse model of
Alzheimer's Disease (AD), overexpressing mutant amyloid precursor
protein (Jankowsky et al., 2005).
Methods
MRF based image segmentation
The contextual dependency is a general and meaningful way to
model the spatial property (Zhang et al., 2001). MRF theory is a
class of probability theory for modeling the contextual dependencies
of physical phenomena such as image pixels and correlated features. It
has become increasingly popular in many image segmentation
problems and image reconstruction problems. In the ﬁeld of medical
image segmentation, MRF has been used for brain tissue segmentation
(Awate et al., 2006; Held et al., 1997; Zhang et al., 2001), neuroanatomical structure segmentation (Ali et al., 2005; Fischl et al., 2002),

2300

T. Wu et al. / NeuroImage 59 (2012) 2298–2306

detection of microcalciﬁcation in digital mammograms (Yu et al., 2006)
and detection of multiple sclerosis lesions in MR images (Khayatia et
al., 2008), etc.
Let S = {1, 2, …, n} be the set of sites in a image, X be a vector of site's
signal, and Y be the associated labeling vector, that is, X = {xi, i ∈ S} and
Y = {yi, i ∈ S}. Let N be a neighborhood system deﬁned as N = {Ni, i ∈ S}
where Ni denotes the set of sites neighboring site i. Y is said to be a
MRF on S with respect to a neighborhood system N if and only if
P ðY ÞN0 and P ðyi jyS−fig Þ ¼ P ðyi jyNi Þ

ð1Þ

for brain tumor recognition (Luts et al., 2007), brain states classiﬁcation
of functional MRI (Mourao-Miranda et al., 2005), breast cancer detection in dynamic contrast-enhanced MRI (Levman et al., 2008), and
knee bone segmentation in MR images (Bourgeat et al., 2007).
The basic idea of SVM is to construct an optimal hyperplane which
gives maximum separation margin between two classes. Assuming a binary classiﬁcation problem with a n dimensional training set xi ∈ Rn
with its label set yi ∈ {+1,−1}, where i = 1, 2,…, m. The hyperplane f
(x), that separates the given data, is deﬁned as:
T

where S-{i} denotes the set difference. The condition of (1) means that
only neighboring labels have direct interaction with each other, and the
joint probability P(Y) can be uniquely determined by its local conditional probabilities.
According to Hammersley–Clifford theorem (Li, 2009), the probability P(Y) of an MRF can be equivalently speciﬁed by a Gibbs distribution as follows:
P ðY Þ ¼



1
exp − ∑ Vc ðY Þ
Z
c∈C

ð2Þ

where Z is a normalizing constant and Vc(Y) is a clique potential function over all cliques c ∈ C. A clique c is a subset of sites in S that are all
neighbors of each other, and C is a set of cliques or the neighborhood
of the clique under study. The value of Vc(Y) depends on a certain conﬁguration of labels on the clique c. For the image segmentation problem,
the posterior probability of the label of a site, given speciﬁc signal, can
be formulated using Bayesian theorem and the Hammersley–Clifford
theorem as:
P ðY jXÞ ¼



1
exp ∑ logP ðxi jyi Þ þ ∑ Vc ðY Þ :
Z
c∈C
i∈S

ð3Þ

∑ logP ðxi jyi Þ in the right hand side in (3) is the sum of the log
i∈S

likelihood function of the given labeling for the site's signal. Usually
a multivariate Gaussian distribution is used for modeling P(X|Y) (Ali
et al., 2005; Fischl et al., 2002; Held et al., 1997; Zhang et al., 2001),
which is based on the assumption of Gaussian relationship between
features and labels. This assumption is too restrictive to model the
complex dependencies between features and labels in some cases.
By employing a machine learning classiﬁer, such as SVM, the performance of the MRF based image segmentation was improved since
SVM is generally better at modeling the complex dependencies due
to the virtue of the non-linear transformation (Bae et al., 2009; Lee
et al., 2005). The well-accepted generalization ability of SVM is
explained in next section.
Segmentation enhancement by SVM
SVM has received a lot of attention from the machine learning and
pattern recognition community due to the following reasons (Abe,
2005). First, SVM works well for classifying objects which are not
linearly separable. The objects are mapped from their input space into
a high-dimensional feature space by kernel transformations; thus
SVM can separate objects which are not linearly separable. Secondly,
SVM has good generalization ability. SVM attempts to maximize the
separation margin between the classes, so the generalization performance does not drop signiﬁcantly even when the training data are
scarce. In addition, SVM can achieve a global optimal solution because
it is solved with quadratic programming. Because of the generalization
ability of SVM, it has accomplished great success in a variety of
applications including fault detection, fraud detection, handwritten
character recognition, object detection and recognition, and text classiﬁcation. In the ﬁeld of medical image classiﬁcation, SVM has been used

f ðxÞ ¼ w ΦðxÞ þ b

ð4Þ

where w is the n dimensional normal vector perpendicular to the hyperplane, b is a bias term and Φ(xi) is a non-linear transformation
which maps the samples into a higher-dimensional dot-product space
called the feature space. The optimal hyperplane is obtained by solving
the following optimization problem:
m
1 T
Min w w þ C ∑ξi
2
i¼1


T
s:t: yi w ⋅Φðxi Þ þ b ≥1−ξi ; i ¼ 1; …; m:

ξi ≥0;

ð5Þ

i ¼ 1; …; m

where ξ = {ξ1, …, ξm} is a slack variable and C is the penalty parameter
which controls the balance between the model complexity and classiﬁcation error. The proper value of penalty parameter (C) is determined
by the training set to avoid overﬁtting. The non-negative slack variable
(ξi) allows (5) to always yield feasible solutions by relieving the
constraint of maximum margin.
The constrained optimization problem in (5) can be converted to an
unconstrained optimization problem by introducing the non-negative
Lagrangian multipliers αi, and the unconstrained optimization problem
is converted to a Lagrangian dual problem by introducing the Karush–
Kuhn–Tucker (KKT) condition. The optimal solution αi * of the dual
problem (Abe, 2005) yields the following optimal hyperplane:
m

!

f ðxÞ ¼ sign ∑ yi α⁎i K ðx;xi Þ þ b⁎

ð6Þ

i¼1

where xi are support vectors and K ðx; xi Þ is a kernel function deﬁned as
K ðx; xi Þ ¼ ΦðxÞT Φðxi Þ. The kernel function performs the nonlinear
mapping implicitly so that we can avoid the complexity of mapping
and the curse of dimensionality resulted from the nonlinear mapping.
Commonly used kernel functions are linear, polynomial and RBF
among which nonlinear kernel function, RBF has been recommended
in many studies. For example, a comparative study on SVM using fMRI
to decipher brain patterns concludes RBF outperforms linear SVM
signiﬁcantly (Song et al., 2011). Our study on Alzheimer's disease
(AD) diagnosis using MRI imaging indicates that RBF kernel outperforms both linear and polynomial kernels for differentiating AD patients
with normal individuals (Zhang et al., 2008). Therefore, in this study we
used the Radial Basis Function (RBF) kernel, deﬁned as follows:


2
K ðx; xi Þ ¼ exp −γ‖x−xi ‖ ; γN0

ð7Þ

where γ in (7) is a parameter related to the span of an RBF kernel.
Brain image segmentation by eMRF
In our previous research (Bae et al., 2009) we proposed eMRF method, in which we integrate three different types of information – MR
intensity, voxel location and contextual relationship with neighboring
voxels' labels – to improve the overall segmentation performance, and

T. Wu et al. / NeuroImage 59 (2012) 2298–2306

the MR intensity information is modeled by an enhanced SVM, which
takes different sampling ratio for different brain structures. The three
pieces of information are linearly combined with the model parameters
which control their relative contributions. The model parameters are
determined through a training process to maximize the segmentation
performance. The experimental results from using the eMRF method
showed that the integration of the probability information based
segmentation and the machine learning based segmentation can improve the overall segmentation performance, compared with the atlas
based segmentation method and the MRF method (Ali et al., 2005).
This is because it takes advantage of the classiﬁcation ability of machine
learning classiﬁers, in addition to the virtue of the location information
and the contextual information of the probability information based
segmentation, which are critical information for classifying each voxel
in a 3D image into the multiple classes.
Even though employing machine learning classiﬁers for brain image
segmentation improves the overall segmentation performance, computation time remains a big challenge. As stated earlier, the eMRF method
requires long training and testing times due to the difﬁculty in selection
of SVM parameters. These drawbacks are mainly associated with the
large data size. The number of data points in a 3D MRM images with
the matrix of 128 × 128 × 256 is more than 4 million. Multiplied by the
number of training sets this number is ~16 million. The number of
data for training and testing directly affects the training and testing
time of SVM. Therefore it is desirable to use the minimum number of
data necessary to produce comparable classiﬁcation performance. The
key to solve the problem of the large data sets is to reduce the number
of the training data while maintaining the classiﬁcation performance of
classiﬁers. The pSVMRF method proposed in the next section is built
based on the eMRF method, but tries to reduce the training and testing
time while maintaining the segmentation performance.
The proposed segmentation method: pSVMRF
Let S = {1, 2, …, n} be the set of voxels in a 3D MR image, X be a
vector of voxels signal intensity, and Y be the associated labeling vector, that is, X = {xi, i ∈ S} and Y = {yi, i ∈ S}. A location prior vector of a
voxel, li, has m elements, where m is the number of structures to be
k
segmented and ∑m
k¼1 li ¼ 1. Let K be the set of the classes to which
voxels will be assigned, i.e. K = {1, 2, …, m}, and L = {li} is the collection of location prior vectors. The k th element of the location prior
vector of the voxel i is deﬁned as:
q

∑ of voxels labeled as k at location r ðiÞ
k

li ¼

j¼1

ð8Þ

q

where q is the number of the images in the training set and r(i) is
location function which informs us the location of the voxel i in the
3D image. Using Hammersley–Clifford theorem and the assumption
of P(Y) N 0 and P(X,L) N 0, the posterior probability of having a label
conﬁguration Y given a MR intensity vector X and a location prior vector L is formulated as follows:



P ðY jX; L Þ∝ exp w1 ∑ logAi ðyi ; xi ; li Þ þ w2 ∑ Vi yi ; yNi
i∈S

ð9Þ

i∈S

s:t: w1 þ w2 ¼ 1
where w1 and w2 are model parameters which control the contribution
of the two terms in (9) to the posterior probability P(Y|X,L). Based on
the MRF theory, the prior probability of having a label at a given site i
is determined by the label conﬁguration of the neighborhood of the
site i. The Hammersley–Clifford theorem enables us to calculate the
joint probability P(Y) as a sum of the clique potential functions. We
use a ﬁrst order neighborhood system of a 3D image as a clique,
which consists of the adjacent six voxels in the four cardinal directions

2301

in a plane and the front and back directions through the plane. The clique potential function Vi(yi,yNi), called contextual potential function, in
(9) will have a higher value when the number of neighbors that have
the same label increases. This function is, thus, deﬁned as



 ∑j∈N δ yi ; yj

  1 if y ¼ y
i
i
j
where δ yi ; yj ¼
Vi yi ; yNi ¼
0 if yi ≠yj
nðNi Þ

ð10Þ

where n(Ni) is the number of voxels in a neighborhood of site i.
The location information of a voxel in a 3D image after registration is
important for classiﬁcation of the voxel into the neuroanatomical
structures. Fischl et al. (2002) pointed out that if the image registration
does well, only small numbers of neuroanatomical structures are available at a given location in a 3D brain atlas and the location information
have anatomical meaning so that it can help in classiﬁcation. Powell et
al. (2008) included probability map values as input features for the
machine learning based segmentation. The probability map was created
by calculating the probability of a neuroanatomical structure being
located at a voxel location in the atlas space across all subjects in a training set. For example, given four subjects, if one out of four subjects labeled voxel i as structure k, and three out of four subjects labeled
voxel i as structure l, in the probability map for structure k, the value
for voxel i is 25% while the probability map for structure l will have
75% for voxel i. Separate probability maps for each structure were calculated and included as one of elements in the input vectors for the binary
classiﬁcation of ANN and SVM in Powell et al.'s experiment. Taking a
similar approach, pSVMRF employs a prior feature SVM (pSVM), which
includes the features from MR intensity and location prior vectors, for
simultaneously modeling the MR intensity information and location information. Similar to eMRF, OAO (One-Against-One) method is applied
to train SVM for classifying the kth class against the l th class since it is
more efﬁcient on large datasets than OAA (One-Against-All) and AAO
(All-At-Once) (Hsu and Lin, 2002). Thus, overall n*(n-1)/2 models will
be trained. In each SVM training, the location prior being derived from
the speciﬁc probability map with respect to the speciﬁc structure together with MR intensity for each voxel are being the input features
for the model.
Since pSVM performs a multiclass classiﬁcation of SVM, the number of elements in the location prior vector is identical to the number
of structures to be segmented. Each element of the location prior
vector represents the number of times a particular structure occurs
at a given location in all the brain images of the training set. The
location prior vector deﬁned in (8) can model all the probabilities of
the m structures being located at a speciﬁc location and be used as a
feature for the multiclass classiﬁcation. As explained earlier, SVM
can enhance linear separation by mapping the original input space
into a high-dimensional feature space using the nonlinear transformation. By adding the MR intensity information and the location
prior vector as input features, we anticipate that pSVM can boost the
separability with the power of nonlinear transformation of SVM.
The ﬁrst term, Ai(yi, xi, li), in (9) is called as the observation potential
function that models the MR intensity information and the location
information. To be incorporated into pSVMRF, the decisions made by
pSVM need to be probabilistic output. Platt (2000) proposed a method
for mapping the SVM outputs into posterior probability by applying a
sigmoid function. The observation potential function for voxel i is
deﬁned as follows, for class k:
Ai ðyi ; xi ; li Þ ¼ Pi ðyi ¼ 1jxi ; li Þ ¼

1
1 þ expðα fk ðxi ; li Þ þ βÞ

ð11Þ

where fk(xi,li) is the SVM decision function for class k, α and β are the
parameters estimated from the training data. That is, SVM model is
trained using the MR intensity (xi) and location (li) to determine
the belonging of voxel i to class k (yi = − 1, 1). Let us deﬁne a new

2302

T. Wu et al. / NeuroImage 59 (2012) 2298–2306

training set (ti, xi, li), where ti here is the target probabilities deﬁned
as:
ti ¼

yi þ 1
2

ð12Þ

The parameters α and β can be found by solving the following
minimization problem (Platt 2000):
Min− ∑ ti logðpi Þ þ ð1−ti Þ logð1−pi Þ

ð13Þ

i

where pi is deﬁned in (11). We used MATLAB (Natick, MA) to solve
the optimization problem (13) and obtain the values of α and β
values for each class. The next step is to ﬁnd the label conﬁguration
Y⁎ that maximizes the posterior probability P(Y|X,L) in (11), i.e.,
Y  ¼ arg maxY P ðY jX; LÞ. This is known as the maximum a posterior
(MAP) solution. Because of the highly complicated interactions
among multiple labels, it is very difﬁcult to ﬁnd the optimal solution of the joint probability P(Y|X,L). We adopt a local search method called iterated conditional modes (ICM), which maximize local
conditional probabilities iteratively by using the greedy search in
the local optimization. It is expressed as


yi ¼ arg max P ðyi jxi ; li Þ
yi ∈Y

ð14Þ

The ICM algorithm sequentially updates yi(t) into yi(t + 1) by switching the different labels to ﬁnd the maximum value of P(yi|xi, li). We use
the MAP solution based on the location prior vector as the initial estimator y (0) of the ICM algorithm. In this study, the algorithm continues
until no improvement is made and the iteration which gives the best
solution and terminates the algorithm is the optimal terminating
point. We estimate the optimal terminating point from the training
process and apply the terminating points for predicting labels of
new testing data.
Results and discussion
Performance measurements
To estimate the performance of segmentation methods, we use
the two performance metrics: volume overlap percentage (VOP)
and volume difference percentage (VDP) (Ali et al., 2005; Fischl et
al., 2002). They are calculated by comparing the automated labeling
with the manual labeling (gold standard) of each voxel. Denote LA
and LM as labeling of the structure k by automated and manual segmentation respectively, and V(L) as a function which calculates the
volume of the labeling. VOP and VDP for a structure k are deﬁned as:
VOPk ðLA ; LM Þ ¼
¼

V ðLA ∩LM Þ
×100 and VDPk ðLA ; LM Þ
ðV ðLA Þ þ V ðLM ÞÞ=2

ð15Þ

jV ðLA Þ−V ðLM Þj
×100
ðV ðLA Þ þ V ðLM ÞÞ=2

VOP is the larger the better, VDP is the smaller the better. VOP is
more sensitive to the spatial difference of the two labels than the
volumetric difference, but VDP is more sensitive to the volumetric
difference. To estimate the overall segmentation performance of a
particular method, we use average VOP (AVOP) and average VDP
(AVDP), which are calculated by dividing the sum of VOP or VDP for
all structures by the number of structures.
Implementation of the segmentation method
We assessed the performance of pSVMRF using MRM images of
mouse brains acquired by the Center for In Vivo Microscopy, at Duke

University Medical Center, and previously used in Ali et al. (2005);
Sharief et al. (2008); Badea et al. (2010). T2-weighted MRM mouse
brain images from ﬁve formalin-ﬁxed C57BL/6 male mice (approximately 9 weeks in age) were used. Image acquisition parameters
were: TE/TR = 30/400 ms, bandwidth 62.5 kHz, ﬁeld of view=
12× 12× 24 mm and matrix size= 128 × 128 × 256, 86 μm isotropic
resolution. A 9-parameter afﬁne registration was applied to each
image. 21 manual labels were used as gold standard to evaluate
segmentation accuracy. Table 1 presents the 21 neuroanatomical
structures and abbreviations used in this study.
Two different strains were introduced to test the segmentation, a
BXD29 and an AD mouse models (Jankowsky et al., 2005). These
mice and an additional set of ﬁve C57BL/6 were actively stained
(Johnson et al., 2002), and imaged as described in Sharief et al.
(2008). Imaging consisted of two protocols: T1 weighted (3D spin
warp, TE/TR 5.2/50 ms, ﬁeld of view 11 × 11 × 22 mm, matrix size
512 × 512 × 1024), and MEFIC enhanced T2 weighted acquisitions
(3D CMPG, TR 400 ms, echo spacing 7 ms, 7 echoes, ﬁeld of view
11 × 11 × 22 mm, matrix size 256 × 256 × 512) (Sharief and Johnson,
2006) were used to provide intensity priors. T1 images were resampled to match the resolution (43 microns) of the T2 weighted
image set. Using both image channels, 33 manual labels were produced for the C57BL/6 brains, and a set of 7 labels was traced to test
the BXD and AD segmentation.
The implementation of the pSVMRF method consists of two steps.
The ﬁrst step is to build the pSVM models and test the models. The
pSVM models were trained using the randomly sampled training set,
consisting of 300 randomly selected data points from each of the
structures (all neuroanatomical structures to be deﬁned, and one
added miscellaneous structure). Each of the training and testing
data has the input feature vector, which consists of one feature for
T2-w MR signal intensity and additional N features (N = 22 for the
formalin ﬁxed, N = 34 for actively stained specimens) for the location
prior. As mentioned earlier, the selection of the penalty parameter
(C) and RBF kernel parameter (γ) greatly affects on the classiﬁcation
accuracy and the generalization ability of SVM. We conducted a grid
search to ﬁnd the best penalty parameter and the best RBF kernel
parameter using the ﬁve-fold cross validation, which can help in
avoiding the overﬁtting problem and estimating the generalization
ability. Each of the trained pSVM models was tested on each of the
testing data to calculate the observation potential function in (11).
The training and testing time of the pSVM models for the randomly
sampled mouse brain dataset (formalin ﬁxed) were 1.12 min, and
14.24 min respectively, using a 3.4-GHz PC and LibSVM for MATLAB
(Chang and Lin, 2001). Since SVM can transform the linear combination of the MR intensity and location prior vector to a nonlinear combination that can help in classiﬁcation, pSVM could train a better
model using a small number of data (6600 voxels for each formalin
mouse, 10,200 voxels for each actively stained mouse). In the eMRF
method (Bae et al., 2009), a large size of training set (472,100 voxels
per a mouse), which is over-sampled from some classes, was needed
for the SVM training. That results in a very long training and testing
time: 7.56 days for training and 4.82 h for testing.

Table 1
List of the 21 neuroanatomical structures and abbreviations.
Cerebral cortex (CORT)
Cerebral peduncle (CPED)
Hippocampus (HC)
Caudate putamen (CPU)
Globus pallidus (GP)
Internal capsule
Periacqueductal gray
(PAG)

Inferior colliculus
(INFC)
Medulla oblongata (MED)
Thalamus (THAL)
Midbrain (MIDB)
Anterior commissure (AC)
Cerebellum (CBLM)
Ventricular system (VEN)

Pontine nuclei (PON)
Substantia nigra (SNR)
Interpeduncular nuclues
(INTP)
Olfactory bulb (OLFB)
Optic tract (OPT)
Trigeminal tract (TRI)
Corpus callosum (CC)

T. Wu et al. / NeuroImage 59 (2012) 2298–2306

The second step was to implement the ICM algorithm to calculate
the contextual potential function in (10) and the posterior probability
P(Y|X,L) in (9). We did a grid search over the range W ¼ f0:01≤
wi ≤0:99; ∑i wi ¼ 1 and i ¼ 1; 2g to ﬁnd the best model parameters,
which were chosen as w1 = 0.89 and w2 = 0.11 for observation and
contextual functions, respectively. During the grid search for model
parameters, a large number of the ICM implementations with the
different parameter values were performed. Each of the ICM implementations run until there was no change in labels assignment, but
the best solution was achieved at the ﬁrst iteration from every ICM
implementation. In this grid search, we tried to ﬁnd the model that
has the maximum AVOP and the minimum AVDP. Since one model
has the maximum AVOP and the other model has the minimum
AVDP, we could not ﬁnd a best model which satisﬁed both criteria.
Therefore, we calculated the margins of AVOP and AVDP compared
with those of the MRF method (Ali et al., 2005). Total margin,
which is sum of the two margins, was used as the criterion for
selecting the best model. Fig. 1 provides the plot of the total margin
vs. iterations of the ICM algorithm, with the best pSVMRF model of
w1 = 0.89 and w2 = 0.11. The maximum of the total margin was
achieved at the ﬁrst iteration within 11.86 min using a 3.4-GHz PC.
Therefore, we chose the ﬁrst iteration as the optimal terminating
point for this pSVMRF model. This optimal terminating point will
then be used for testing new mouse brain images. Since the ICM
algorithm is a local optimization algorithm and does not guarantee
a global optimal, the optimal terminating point should be determined
based on the model and data set. Estimating and using the optimal
termination point enables the ICM algorithm to converge much faster
at better solution.
Validation of the segmentation method
To validate the proposed method, pSVMRF, we ﬁrst test on MR images of ﬁve C57BL/6 mice using a ﬁve-fold cross validation. Each of
the ﬁve mice was used as the testing set while the remaining four
mice were used as the training set. The results are compared with
two existing methods: the MRF method (Ali et al., 2005) and the
eMRF method (Bae et al., 2009). In Table 2, the segmentation
performances of the three automated mouse brain image segmentation
method are estimated based on VOP and VDP. The performance
estimates in Table 2 are based on the average values from testing all
the mice using the ﬁve-fold cross validation. The upper rows include
VOP and the lower rows include VDP. A ‘+’ sign always means that
pSVMRF method outperforms the other methods for the speciﬁc

2303

structure and ‘−’ means that pSVMRF underperforms. pSVMRF
outperformed eMRF in 16 structures, there was no change in one
structure and a slight underperformance in 4 structures. Major improvements in segmentation performance were noted for the olfactory
bulbs (from 83% to 91%), pons (from 80% to 86%), and trigeminal tract
(from 74% to 82%). In comparison to MRF, pSVMRF outperformed in 14
structures, most notable in the cases of optic tract (53% to 73%), trigeminal tract (from 64% to 82%), and pons (73% to 86%). Table 3 presents
the comparisons of the overall segmentation performance and the
computation time of the three automated segmentation methods.
Overall pSVMRF outperforms the two existing methods. AVOP and
AVDP of pSVMRF are improved by 2.55% and 9.57% compared with
eMRF, and by 5.41% and 21.07% compared with MRF. The total testing
time of pSVMRF in MATLAB, which includes the testing time of pSVM
testing and the ICM algorithm, was 26.10 min, which is improved by
92.85% compared with the testing time of eMRF (364.4 min). The testing time of MRF (15 min) is less than pSVMRF. However, pSVMRF can
produce 26.48% (total margin from MRF) more accurate segmentation
than MRF by spending 16 min more. The proposed method, pSVMRF,
gives better segmentation than eMRF and MRF, within a short testing
time.
Even though pSVMRF outperforms eMRF in 16 structures out of 21,
eMRF is still better than pSVMRF in some small structures such as GP,
PAG, OPT and TRI. This results from the fact that eMRF use Mix-ratio
sampling based SVM (MRS-SVM; Bae et al., 2010) and an over-sampled
training set for some smaller structures to improve the classiﬁcation
performance for these structures. In contrast pSVMRF uses the same
number of training data from each of the structures regardless their
size. MRF is also better than pSVMRF in some small structures such as
GP, PAG, AC and INTP even though pSVMRF is better for most structures.
That is because MRF relies more on the contextual information, which
enhances the identiﬁcation of the smaller structures, for segmentation
than pSVMRF. The proposed method, pSVMRF, still needs to be improved
for the smaller structures.
The combination of higher resolution imaging and higher contrast
given by active staining boosted segmentation accuracy, relative to
that obtained for the formalin ﬁxed brains, as illustrated in Fig. 2 for
adult C57BL/6 mice. The percent voxel overlap (VOP) increased substantially for smaller structures like the anterior commissure (from
50.76% to 83.5%), corpus callosum (from 65.59% to 85.44%), substantia nigra (78.36 to 91.55%) and ventricles (72.56 to 81.72%). For hippocampus and caudate putamen the VOP values were more similar.
VOP changed from 87.07 to 87.67% for hippocampus, and increased
from 97.67% to 90.87% for caudate putamen.

Fig. 1. Convergence of the ICM algorithm with w1 = 0.89 and w2 = 0.11.

2304

T. Wu et al. / NeuroImage 59 (2012) 2298–2306

Table 2
Comparison of segmentation performance of the pSVMRF, eMRF and MRF methods based on VOP and VDP for 21 structures. A. pSVMRF vs. eMRF based on VOP; B. pSVMRF vs. eMRF
based on VDP; C. pSVMRF vs. MRF based on VOP; D. pSVMRF vs. MRF based on VDP.
A
VOP

CORT.

CPED

HC

CPU

GP

ICAP

PAG

INFC

MED

THAL

MIDB

pSVMRF
eMRF
+/−

94.08
91.10
3.27

74.22
73.15
1.47

87.07
86.20
1.01

87.67
87.62
0.05

79.49
79.64
− 0.19

73.33
73.14
0.26

90.16
90.26
− 0.11

88.40
85.32
3.61

93.27
91.93
1.45

94.18
93.53
0.69

93.81
93.27
0.58

VOP

AC

CBLM

VEN

PON

SNR

INTP.

OLFB.

OPT

TRI

CC

pSVMRF
eMRF
+/−

50.76
50.76
0.00

96.45
92.68
4.06

72.56
71.83
1.01

86.04
80.03
7.50

78.36
78.98
− 0.77

72.41
71.61
1.12

90.99
82.50
10.29

73.62
68.67
7.20

82.19
73.83
11.33

65.59
65.75
− 0.23

B
VDP

CORT.

CPED

HC

CPU

GP

ICAP

PAG

INFC

MED

THAL

MIDB

pSVMRF
eMRF
+/−

3.69
3.34
− 10.52

2.31
8.32
72.27

5.13
5.56
7.72

3.57
3.98
10.19

9.34
7.54
− 23.85

11.03
11.05
0.14

4.01
3.55
− 12.93

4.93
5.26
6.33

7.62
7.12
− 7.05

2.34
2.48
5.82

3.08
2.97
− 3.72

VDP

AC

CBLM

VEN

PON

SNR

INTP.

OLFB.

OPT

TRI

CC

pSVMRF
eMRF
+/−

25.16
25.55
1.53

3.34
3.73
10.50

14.26
16.83
15.29

7.28
12.49
41.68

9.05
10.16
10.90

19.71
26.09
24.47

7.30
10.79
32.32

12.52
5.67
− 120.91

9.87
7.33
− 34.65

15.67
20.57
23.86

C
VOP

CORT.

CPED

HC

CPU

GP

ICAP

PAG

INFC

MED

THAL

MIDB

pSVMRF
eMRF
+/−

94.08
90.77
3.65

74.22
67.69
9.65

87.07
87.69
− 0.71

87.67
88.46
− 0.90

79.49
78.46
1.31

73.33
73.85
− 0.70

90.16
85.38
5.59

88.40
83.08
6.41

93.27
86.15
8.26

94.18
93.08
1.19

93.81
90.77
3.35

VOP

AC

CBLM

VEN

PON

SNR

INTP.

OLFB.

OPT

TRI

CC

pSVMRF
eMRF
+/−

50.76
55.38
− 8.35

96.45
93.08
3.63

72.56
70.77
2.53

86.04
73.08
17.73

78.36
68.46
14.47

72.41
76.92
− 5.87

90.99
84.62
7.53

73.62
53.08
38.70

82.19
63.85
28.73

65.59
71.54
− 8.31

D
VDP

CORT.

CPED

HC

CPU

GP

ICAP

PAG

INFC

MED

THAL

MIDB

pSVMRF
eMRF
+/−

3.69
4.35
15.16

2.31
12.17
81.04

5.13
6.96
26.25

3.57
6.96
48.66

9.34
7.83
− 19.38

11.03
13.04
15.43

4.01
2.61
− 53.80

4.93
4.35
− 13.39

7.62
10.43
26.95

2.34
3.48
32.73

3.08
4.35
29.14

VDP

AC

CBLM

VEN

PON

SNR

INTP.

OLFB.

OPT

TRI

CC

pSVMRF
eMRF
+/−

25.16
19.13
− 31.50

3.34
3.48
4.03

14.26
19.13
25.48

7.28
17.39
58.11

9.05
15.65
42.17

19.71
13.91
− 41.66

7.30
16.52
55.79

12.52
14.78
15.34

9.87
10.43
5.44

15.67
22.61
30.71

To test the robustness of the pSVMRF on mutant mice which has
large anatomical variability, we examined the performance of the segmentation in two new strains, BXD29 the APP/TTA mouse model of
AD, and contrasted it with the baseline accuracy for stained C57BL/6
mice, imaged using the same protocol, and using a full sampling strategy for training/classiﬁcation. We evaluated the segmentation qualitatively (Fig. 3) and quantitatively (Fig. 4).
Segmenting the overall brain is a very accurate process (N90%
VOP), even in strains other than the C57BL6 used for generating priors.
However, the increased anatomical variability introduced by new
strains resulted in overall decreased performance for a subset of structures including: hippocampus, caudate putamen, anterior commisure,
corpus callosum, substantia nigra and ventricles. When using 7 labels

Table 3
Comparisons of overall segmentation performances and computation time for the
pSVMRF, eMRF and MRF methods.

pSVMRF
eMRF
MRF

AVOP

AVDP

Testing time (min)

82.13
80.09
77.91

8.63
9.54
10.93

26.1
364.4
15.0

only during training, the larger structures such as hippocampus and
caudate putamen could be segmented with accuracy of ~ 80% and
greater. The hippocampus VOP was 94.11 ± 0.73% for C57BL6, vs.:
85.81 ± 1.19% for the other two strains, while for caudate putamen
VOP was 92.21 ± 0.71% for C57BL/6, and 83.48 ± 5.93% for the new
strains. Smaller white matter tracts and nuclei, and especially the
widely variable ventricles remain challenging for the automated segmentation task. VOP for the corpus callosum was 86.11 ± 10.06% for
C57BL/6, but 59.54 ± 6.06% for the additional strains, while for the
substantia nigra VOP was 64.31 ± 2.12% for C57BL/6, and 61.81 ±
10.0% in the other strains.
Multiple avenues exist to increase accuracy of the segmentation.
Improved registration, together with a denser sampling strategy,
has the potential to increase segmentation accuracy, while increasing
computational demands. For example the use of a full sampling strategy on C57BL/6 mice yielded VOP values of 92.38 ± 0.20% for Hc, versus 87.67 ± 0.86 for under-sampled data. Similarly for CPu the VOP
was 94.58 ± 0.9%, versus 90.87 ± 0.16%. However the VOP for other
structures, including ventricles did not increase using this strategy,
e.g. VOP for ventricles was 81.72 ± 0.19% for under-sampled strategy
but only 75.92 ± 4.33 for the full sampling strategy. We noted that a
denser parcellation of the brain yields in general better segmentation

T. Wu et al. / NeuroImage 59 (2012) 2298–2306

Fig. 2. Increased segmentation accuracy was obtained for the higher resolution, actively stained sets, relative to the formalin ﬁxed sets, particularly in smaller structures like
the anterior commissure (ac: from 50.76% to 83.5%), corpus callosum (cc: 65.59% to
85.44%), substantia nigra (SN: 78.36 to 91.55%) and ventricles (VS: 72.56 to 81.72%).
For hippocampus and caudate putamen the values are more similar (~ 87% for Hc,
and increased from 87.67 to 90.87% for CPu).

results, compared to a reduced set of labels, embedded in the larger
brain area, perhaps by more accurately constraining individual regions deﬁnition.
Conclusion
Given recent imaging technology development, we can acquire
higher resolution mouse brain images which have eight times larger
data than the current data. Hence, there has been a pressing need

2305

Fig. 4. Segmenting strains other than the one used for generating the priors (C57BL/6)
is a more challenging task, as illustrated by the examples of a BXD29 and an APP/TTA
mouse model of AD. Using a full sampling strategy, but only a subset of 7 labels, yields
VOP for hippocampus, ranging from 94.11 ± 0.73% in the C57BL/6 (for the 5 specimens)
to 86.65% for the BXD29 and 84.97% for APP/TTA mouse. For the caudate putamen VOP
ranges from 92.21 ± 0.71% for C57BL6, to 87.68% for BXD29 and 79.28% for APP/TTA.
However smaller white matter tracts and nuclei, and especially the ventricles remain
challenging for automated segmentation (e.g. VOP for corpus callosum 86.11 ± 2.12%
in C57BL/6, 55.25% in BXD29, and 63.83% in APP/TTA).

for computationally efﬁcient segmentation method. We have presented an automated method for mouse brain images, pSVMRF,
which is computationally efﬁcient. It integrates pSVM and MRF for a
more accurate and faster segmentation by modeling the three kinds
of information which are critical for the brain image segmentation.
Even though eMRF produced a more accurate delineation of the
mouse brain MRM images than the atlas based segmentation and
the probability information based segmentation by integration of
SVM and MRF, eMRF suffers from the long training and testing time

Fig. 3. Visual assessment of comparable coronal levels through the brains C57BL/6, BXD29 and APP/TTA mouse model of AD, overlaid with automatically generated labels. The labeled regions are: anterior commisure (ac), corpus callosum (cc), caudate putamen (CPu), hippocampus (Hc), susbtantia nigra (SN) and the ventricular system (VS).

2306

T. Wu et al. / NeuroImage 59 (2012) 2298–2306

due to the use of SVM which requires of the long training and testing
time. To reduce the training and testing time of SVM, we use pSVM
which relies on location priors as well as MR intensity information
as input features. By the virtue of nonlinear transformation of these
two critical pieces of information, pSVM can train better models with
a small size of training sets and reduce the testing time by 92.85% compared with the SVM testing of eMRF. By using the optimal termination
point for the ICM implementation, the ICM algorithm converges much
faster with the better solution. The AVOP and AVDP of pSVMRF are improved by 2.55% and 9.57% compared with eMRF, and by 5.41% and
21.07% compared with MRF. In the future, we will make efforts to improve the performance of smaller structures in which pSVMRF still
produces poorer performance than the other two methods.
The C57BL/6 is a widely used mouse strain, at the basis of a large
number of derived strains, and therefore was chosen to create priors,
and training the classiﬁer. There is a wide interest in segmenting
other mouse strains, many of them having a C57BL/6 background, to
identify anatomical phenotypes. While more studies on larger groups
of animals from different strains are required to validate and optimize
a more general segmentation/anatomical phenotyping task in the
future, we have shown the initial applicability of the method to
other strains as well, including a recombinant inbred mouse strain
derived from parental C57BL/6 and DBA2 (BXD29), and a model of
Alzheimer's disease (APP/TTA). The improvements in accuracy while
reducing the computational time will allow us to address the issue
of brain segmentation in larger population studies, and higher resolution images, therefore facilitating image based phenotyping of mouse
models of neurological and psychiatric conditions.

Acknowledgments
The authors would like to thank Dr. Yutong Liu and Mariano G.
Uberti in the Department of Radiology of University of Nebraska,
and Sally Zimney at CIVM, Duke University Medical Center. Images
were provided by the Duke Center for In Vivo Microscopy (CIVM),
supported by NIH grants (NCRR P41 RR005959/ NCI U24
CA092656). CIVM has also received support from the Biomedical Informatics Research Network (mBIRN) (U24 RR021760).

References
Abe, S., 2005. Support Vector Machines for Pattern Classiﬁcation (Advances in Pattern
Recognition). Springer-Verlag New York, Inc., Secaucus, NJ.
Ali, A.A., Dale, A.M., Badea, A., Johnson, G.A., 2005. Automated segmentation of neuroanatomical structures in multispectral MR microscopy of the mouse brain. Neuroimage 27 (2), 425–435.
Awate, S.P., Tasdizen, T., Foster, N., Whitaker, R.T., 2006. Adaptive Markov modeling for
mutual-information-based, unsupervised MRI brain-tissue classiﬁcation. Med.
Image Anal. 10, 726–739.
Badea, A., Ali-Sharief, A.A., Johnson, G.A., 2007. Morphometric analysis of the C57BL/6J
mouse brain. Neuroimage 37 (3), 683–693 Sep 1.
Badea, A., Johnson, G.A., 2010. Remote sites of structural atrophy predict later amyloid
formation in a mouse model of Alzheimer's disease. Neuroimage 50 (2), 416–427.
Bae, M.H., Pan, R., Wu, T., Badea, A., 2009. Automated segmentation of mouse brain images using extended MRF. Neuroimage 46, 717–725.
Bae, M.H., Wu, T., Pan, R., 2010. Mix-ratio sampling: classifying multiclass imbalanced mouse
brain images using support vector machine. Expert Syst. Appl. 37 (7), 4955–4965.
Bock, N.A., Kovacevic, N., Lipina, T.V., Roder, J.C., Ackerman, S.L., Henkelman, R.M., 2006.
In vivo magnetic resonance imaging and semiautomated image analysis extend the
brain phenotype for cdf/cdf mice. J. Neurosci. 26 (17), 4455–4459.
Bourgeat, P., Fripp, J., Stanwell, P., Ramadan, S., Ourselin, S., 2007. MR image segmentation
of the knee bone using phase information. Med.Image Anal. 11, 325–335.
Chang, C., Lin, C., 2001. LIBSVM: A Library for Support Vector MachinesSoftware
available at http://www.csie.ntu.edu.tw/~cjlin/libsvm 2001.

Dorr, A.E., Lerch, J.P., Spring, S., Kabani, N., Henkelman, R.M., 2008. High resolution
three-dimensional brain atlas using an average magnetic resonance image of 40
adult C57Bl/6J mice. Neuroimage 42 (1), 60–69 Aug 1.
Edelstein, W.A., Glover, G.H., Hardy, C.J., Redington, R.W., 1986. The intrinsic signal-tonoise ratio in NMR imaging. Magn. Reson. Med. 3, 604–618.
Fischl, B., Salat, D.H., Busa, E., Albert, M., Dieterich, M., Haselgrove, C., Van der Kouwe,
A., Killiany, R., Kennedy, D., Klaveness, S., et al., 2002. Whole brain segmentation:
automated labeling of neuroanatomical structures in the human brain. Neuron
33, 341–355.
Held, K., Kops, E.R., Krause, B.J., Wells III, W.M., Kikinis, R., Muller-Gartner, H.W., 1997.
Markov random ﬁeld segmentation of brain MR images. IEEE Trans. Med. Imaging
16 (6), 878–886.
Hsu, C.W., Lin, C.J., 2002. A comparison of methods for multi-class support vector machines. IEEE Trans. Neural Netw. 13 (2), 415–425.
Jankowsky, J.L., Slunt, H.H., Gonzales, V., Savonenko, A.V., Wen, J.C., Jenkins, N.A., Copeland, N.G., Younkin, L.H., Lester, H.A., Younkin, S.G., Borchelt, D.R., 2005. Persistent
amyloidosis following suppression of Aβ production in a transgenic model of Alzheimer disease. PLoS Med. 2 (12), 1318–1333 Dec.
Johnson, G.A., Cofer, G.P., Gewalt, S.L., Hedlund, L.W., 2002. Morphologic phenotyping
with magnetic resonance microscopy: the visible mouse. Radiology 222 (3),
789–793.
Khayatia, R., Vafadusta, M., Towhidkhaha, F., Nabavib, M., 2008. Fully automatic segmentation of multiple sclerosis lesions in brain MR FLAIR images using adaptive
mixtures method and Markov random ﬁeld model. Comput. Biol. Med. 38,
379–390.
Kovacevic, N., Henderson, J.T., Chan, E., Lifshitz, N., Bishop, J., Evans, A.C., Henkelman, R.
M., Chen, X.J., 2005. A three-dimensional MRI atlas of the mouse brain with
estimates of the average and variability. Cereb. Cortex 15 (5), 639–645.
Lee, C.H., Schmidt, M., Murtha, A., Bistritz, A., Sander, J., Greiner, R., 2005. Segmenting
brain tumors with conditional random ﬁelds and support vector machines. Lect.
Notes Comput. Sci. 3765, 469–478.
Levman, J., Leung, T., Causer, P., Plewes, D., Martel, A.L., 2008. Classiﬁcation of dynamic
contrast-enhanced magnetic resonance breast lesions by support vector machines.
IEEE Trans. Med. Imaging 27 (5), 688–696.
Li, S.Z., 2009. Markov Random Field Modeling in Image Analysis, 3rd Edition. Springer.
Luts, J., Heerschap, A., Suykens, J.A.K., Huffel, S.V., 2007. A combined MRI and MRSI
based multiclass system for brain tumour recognition using LS-SVMs with class
probabilities and feature selection. Artif. Intell. Med. 40, 87–102.
Ma, Y., Hof, P.R., Grant, S.C., Blackband, S.J., Bennett, R., Slatest, L., Mcguigan, M.D., Benveniste, H., 2005. A three-dimensional digital atlas database of the adult C57BL/6J
mouse brain by magnetic resonance microscopy. Neuroscience 135 (4),
1203–1215.
McDaniel, B., Sheng, H., et al., 2001. Tracking brain volume changes in C57BL/6J and
ApoE-deﬁcient mice in a model of neurodegeneration: a 5-week longitudinal
micro-MRI study. Neuroimage 14 (6), 1244–1255.
Mourao-Miranda, J., Bokde, A.L.W., Born, C., Hampel, H., Stetter, M., 2005. Classifying
brain states and determining the discriminating activation patterns: Support
Vector Machine on functional MRI data. Neuroimage 28, 980–995.
Platt, J., 2000. Probabilistic outputs for support vector machines and comparison to
regularized likelihood methods. Advances in Large Margin Classiﬁers. MIT Press,
Cambridge, MA.
Powell, S., Magnotta, V.A., Johnson, H., Jammalamadaka, V.K., Pierson, R., Andreasen, N.
C., 2008. Registration and machine learning-based automated segmentation of
subcortical and cerebellar brain structures. Neuroimage 39, 238–247.
Reddick, W.E., Glass, J.O., Cook, E.N., Elkin, T.D., Deaton, R.J., 1997. Automated
segmentation and classiﬁcation of multispectral magnetic resonance images
of brain using artiﬁcial neural networks. IEEE Trans. Med. Imaging 16,
911–918.
Rohlﬁng, T., Brandt, R., Menzel, R., Maurer Jr., C.R., 2004. Evaluation of atlas selection
strategies for atlas-based image segmentation with application to confocal microscopy images of bee brains. Neuroimage 21 (4), 1428–1442.
Sharief, A.A., Johnson, G.A., 2006. Enhanced T2 contrast for MR histology of the mouse
brain. Magn. Reson. Med. 56 (4), 717–725 Oct.
Sharief, A.A., Badea, A., Dale, A.M., Johnson, G.A., 2008. Automated segmentation of the
actively stained mouse brain using multi-spectral MR microscopy. Neuroimage 39,
136–145.
Song, S., Zhan, A., Long, A., Zhang, J., Yao, L., 2011. Comparative study of SVM methods
combined with voxel selection for object category classiﬁcation on fMRI data. PLoS
One 6 (2), e17191. doi:10.1371/journal.pone.0017191.
Yu, S.N., Li, K.Y., Huang, Y.K., 2006. Detection of microcalciﬁcations in digital mammograms using wavelet ﬁlter and Markov random ﬁeld model. Comput. Med. Imaging
Graph. 30 (3), 163–173.
Zhang, Y., Smith, S., Brady, M., 2001. Segmentation of brain MR images through a
hidden Markov random ﬁeld model and the expectation–maximization algorithm.
IEEE Trans. Med. Imaging 20, 45–57.
Zhang, H., Wu, T., Bae, M., Chen, K., Reiman, E., Alexander, G.E., 2008. Diagnosing
Alzheimer disease using artiﬁcial neural network and support vector machines
classiﬁers. ICAD 2008: Alzheimer's Association International Conference on
Alzheimer's Disease.

Fair Scheduling in Input-Queued Switches under
Inadmissible Traffic
Neha Kumar, Rong Pan, Devavrat Shah
Departments of EE & CS
Stanford University
{nehak, rong, devavrat}@stanford.edu
Abstract—In recent years, several high-throughput low-delay scheduling
algorithms have been designed for input-queued (IQ) switches, assuming
admissible traffic. In this paper, we focus on queueing systems that violate
admissibility criteria.
We show that in a single-server system with multiple queues, the Longest
Queue First (LQF) policy disallows a fair allocation of service rates 1 . We
also describe the duality shared by LQF’s rate allocation and a fair rate
allocation. In general, we demonstrate that the rate allocation performed
by the Maximum Weight Matching (MWM) scheduling algorithm in overloaded IQ switches is unfair. We attribute this to the lack of coordination
between admission control and scheduling, and propose fair scheduling algorithms that minimize delay for non-overloaded queues.
Keywords— Congestion Control, Quality of Service and Scheduling,
Stochastic Processes and Queueing Theory, Switches and Switching, Resource Allocation

achieve 100% throughput. These results focus on admissible
traffic conditions however, when in practice traffic is frequently
inadmissible. Herein lies our motivation for this paper, where
we study scheduling policies for inadmissible traffic conditions
in IQ switches.
Under admissible traffic, stable scheduling algorithms grant
every flow its desired service, and there does not arise a need
for fairness in rate allocation 4 . Under inadmissible traffic, not
all flows can receive desired service. We observe the rate allocations performed by LQF and MWM in such a scenario, and
prove that they lack fairness. This motivates our search for a
scheduling policy that performs a fair rate allocation, given inadmissibility. We now summarize our results.

I. I NTRODUCTION
The input-queued (IQ) switch architecture is widely used in
high-speed switching. This is due to its low memory bandwidth
requirements compared to those of output-queued and sharedmemory architectures, making it the preferred choice.
In an N × N IQ switch, we assume fixed-size cells (packets).
Each input has N FIFO virtual output queues (VOQs), one for
each output 2 . Packets queue up at the inputs, arriving at input i
for output j at an average rate λij . In each time slot, at most one
packet can arrive at each input and at most one can be transferred
to an output. Consider these conditions for Λ = [λij ]:
N


λij < 1, ∀i

j=1

N


λij < 1, ∀j

i=1

The first condition is enforced by the line-rate constraints
at the inputs. Incoming traffic is called admissible when the
second condition is satisfied and inadmissible otherwise. The
switch scheduling problem reduces to a matching problem in a
weighted bipartite graph with N inputs and N outputs 3 .
A. Background and Motivation
The primary performance metric of an IQ switch scheduling
algorithm is the throughput it delivers. The Maximum Weight
Matching (MWM) algorithm has been shown to achieve 100%
throughput when arriving traffic is admissible and obeys the
Strong Law of Large Numbers. Practical heuristics to approximate MWM [7] [4] too have been proposed. In a single-server
queueing system, Longest Queue First (LQF) is also known to
1 The

B. Outline and Results
In section II, we formalize the notion of fairness that we will
use. We then commence our study of IQ switch-scheduling under inadmissible traffic conditions by observing the performance
of LQF in a system with multiple queues and an over-subscribed
server in section III. As an interesting side observation, we also
present the duality shared by the rate allocations determined by
LQF and Max-Min fairness. In section IV, we provide FairLQF, an algorithm that incorporates fairness into LQF to perform a provably fair rate allocation.
In section V, we extend our study to the N × N switch and
observe the rate allocation performed by MWM. We show that
it lacks fairness, and propose the Fair-MWM algorithm in section VI, conjecturing that this too performs a fair rate allocation.
Our conclusions follow in section VII.
II. FAIRNESS IN S CHEDULING
To define a fair rate allocation for flows, we use the established notion of Max-Min fairness [1].
Definition 1 (Max-Min Fairness) Let there be n flows arriving at a server of capacity C with rates λ1 , · · · , λn respectively.
A rate
 allocation r = (r1 , · · · , rn ) is called Max-Min fair iff
(i) n ri ≤ C, ri ≤ λi , and
(ii) any ri can be increased only by reducing rj s.t. rj ≤ ri .
Definition 2 (Fairness in a Switch) There exist N 2 flows in
an N × N switch. We call R = [rij ] a fair allocation for
Λ = [λij ] iff it is Max-Min fair for every output.

formal definition of fairness is provided later.

2 The Virtual Output Queueing architecture improves performance by prevent-

ing Head-of-Line blocking [5].
3 In this paper, we take the weight of the edge (i, j) as the size of V OQ .
ij

IEEE Communications Society
Globecom 2004

4 In this paper, each VOQ defines one flow. This can easily be generalized to
accommodate the arrival of multiple flows at an input that are intended for the
same output.

1713

0-7803-8794-5/04/$20.00 © 2004 IEEE

B. Rate Allocation under LQF
When traffic is inadmissible, queues that do not receive deWe first consider a system of N queues and a server of unit
capacity. The arrival rate for queue i, 1 ≤ i ≤ N , is denoted by sired service grow unboundedly. Since LQF is a non-idling polλi , and the cumulative number of arrivals until time n is denoted icy, it services a queue in every time slot. For large enough t,
by Ai (n), where Ai (0) = 0. We assume that the arrivals obey ri (t) = Ti (t)/t denotes the service rate allocated to queue i
by LQF. The fluid model equations (2) – (5) characterize LQF’s
the Strong Law of Large Numbers (SLLN) that states:
rate allocation for arrival rates λ1 , · · · , λn .
Ai (n)
For admissible traffic, techniques such as Lyapunov analysis
= λi ∀i w.p.1
(1)
lim
n→∞
n
may be used to show that LQF services every queue at its arrival
Let Qi (n) denote the size of queue i at time n, Di (n) the rate. For inadmissible traffic, we state the following theorem to
number of departures from queue i until time n (Di (0) = 0), describe LQF’s rate allocation:
Theorem 1: Let λi represent the arrival rate for queue i, 1 ≤
and Ti (n) the number of time slots queue i is scheduled for seri
≤
N . Assume λ1 ≥ λ2 ≥ · · · ≥ λN ≥ λN +1 = 0. Let
vice in [0, n]. LQF always serves the longest queue, breaking
(t)
represent the rate allocated by LQF to queue i in time slot
r
i
ties arbitrarily. For n ≥ 0, the equations below describe queue
t.
Then
∀t ≥ 0:
sizes, departures and the busyness of the server over time.

(i) 0 ≤ rk (t) ≤ λk s.t. k rk (t) = 1
Qi (n) = Qi (0) + Ai (n) − Di (n)
(ii) Let 1 ≤ l ≤ N be the smallest index such that:
l
Di (n) = Di (n − 1) + (Ti (n) − Ti (n − 1))(Qi (n − 1) > 0)
 
λi − 1)/l > λl+1
∆=(
N
i=1

Ti (n) = n
Ti (·) is non-decreasing and
Then ∀k, rk (t) = (λk − ∆)+ . That is, all queues served at
i=1
a positive rate grow at the rate ∆, while the rest grow at their
The dynamics of the system are completely described by respective arrival rates 6 .
S(n) = (Qi (n), Di (n), Ti (n))N
i=1 .
III. L ONGEST Q UEUE F IRST AND FAIRNESS

WATER

WATER

λ4

A. Fluid Model for a Queue
We employ the fluid model technique to study LQF [3]. To
do this, we define a sequence of systems indexed by r =
1, 2, 3 . . . with the associated description vectors S r (t) =
r
(Qri (n), Dir (n), Tir (n))N
i=1 . The relationship between S (t) and
S(·) is described by:
S r (t) = S(rt)/r

λ1

λ2

r4= λ 4

r1 = r2 = r3

λ

λ3

1

WATER

λ2

λ3

GRAVITY

r3
WATER

GRAVITY

r2
SOLID

r1

The fluid model solution of the system is characterized by
S̄(t) such that:
S̄(t) = lim S r (t)

(a) LQF

SOLID

(b) Max−Min Fair

Fig. 1. Comparison of LQF and Max-Min Fairness.

r→∞

S r (t) is a non-deterministic quantity. We would like to show
that the limiting quantity S̄(t) is a deterministic solution to a set
of differential equations. We proceed by showing the existence
of a limiting quantity and its characterization.
If for all i, |Dir (t) − Dir (t )| ≤ |t − t |, |Tir (t) − Tir (t )| ≤
|t − t |, and limr→∞ |Ari (t) − λi t| = 0 then S̄(t) exists [2] and
satisfies the fluid model equations 5 as follows:
Q̄i (t)
dD̄i (t)
dt

=

Q̄i (0) + λi t − D̄i (t)
dT̄i (t)
=
if Q̄i (t) > 0
dt
N

T̄i (t) = t
T̄i (·) is non-decreasing and

(2)
(3)
(4)

i=1

We can check that our system satisfies these conditions if we
assume that arriving traffic satisfies condition (1).
By serving only the longest queue at all times, LQF further
imposes the following condition on the evolution of T̄i (·):
If ∃j s.t. Q̄i (t) < Q̄j (t) then
5 For

dT̄i (t)
= 0.
dt

(5)

C. LQF vs. Max-Min Fairness
We momentarily digress to present an interesting duality that
exists between the rates allocated by LQF and those allocated by
Max-Min fairness. It is illustrated in Figure 1, where the shaded
area represents a solid surface and the rest is empty space. There
are N steps such that step i has depth λi and unit width, i.e.
there are N columns with volumes λ1 , · · · , λN respectively and
column i represents queue i with arrival rate λi .
We begin our experiment by pouring water (of unit volume)
through the hole. In Figure 1(a) gravity acts downwards, causing
the water to fill the columns deepest-first. In Figure 1(b) gravity
acts upwards, so that the shallowest column fills up first. When
the pouring is complete, the water level in column i indicates the
service rate allocated to queue i by LQF in 1(a) and Max-Min
fairness in 1(b).
We observe that downwards-acting gravity causes the water to
fill up the deepest column first, just as LQF services the longest
queue at the expense of the less-demanding flows. Conversely,
in Figure 1(b), the water first fills all columns equally. When it
reaches a depth of λN , the additional water fills the remaining
6 The

an example, see [3].

IEEE Communications Society
Globecom 2004

1714

proof can be found at http://simula.stanford.edu/∼neha/FMWM.pdf.

0-7803-8794-5/04/$20.00 © 2004 IEEE

N − 1 columns equally and so on. After the water has been
poured, all partially filled columns have equal depth. Moreover,
the water level in column i indicates the Max-Min fair rate allocation to queue i.
This experiment reveals the duality shared by the rates allocated by LQF and those allocated by Max-Min fairness. We
now propose an algorithm that incorporates Max-Min fairness
into LQF to provide a fair rate allocation.
IV. FAIR -LQF
The Fair-LQF algorithm combines the advantages of MaxMin fairness and LQF. Based on individual queue sizes and a
pre-specified threshold value, it maintains a congested list of
queues whose sizes exceed the threshold 7 and an uncongested
list of all other queues. In the limiting case of this threshold
going to ∞, a congested queue is one that is served at less than
its desired rate.
A. The Algorithm
If nc represents the number of congested queues and nuc represents the number of non-empty, uncongested queues, for the
next nc time slots, Fair-LQF serves every congested queue exactly once. For the remainder nuc time slots, it mimics LQF.
The pseudocode for Fair-LQF is as follows:

V. M AXIMUM W EIGHT M ATCHING AND FAIRNESS
We now extend our study to the N × N switch and observe
MWM’s rate allocation in an overloaded switch to show that it
lacks fairness.
An input-output matching is represented by a permutation
matrix π = [πij ]i,j≤N where πij = 1 iff it matches input i to
output j. A scheduling algorithm S must determine a matching
π(n) for each time slot n.
The arrival rate for V OQij , 1 ≤ i, j ≤ N , is denoted by λij ,
and the cumulative number of arrivals until time n is denoted by
Aij (n), where Aij (0) = 0. We assume that the arrivals obey the
Strong Law of Large Numbers (SLLN) that states:
lim

n→∞

= λij ∀i, j w.p.1

(6)

Let Qij (n) denote the size of V OQij at time n, Dij (n) the
number of departures from V OQij until time n (Dij (0) = 0),
and W (n) = [Wij (n)], where Wij (n) denotes the weight of the
edge (i, j) at time n in the switch bipartite graph 11 . Then the
weight of a matching π is defined thus:

W π (n) =
πij Wij (n) = πW (n)
i,j

Definition 3 (Maximum Weight Matching) A maximum weight
matching algorithm is one that determines a matching π w (n) at
time n such that:

//Information Collection
if (queue_size(queue_ID) >= threshold)
add_to_congested_list(queue_ID);
// Scheduling
// Step1: scheduling congested ports
m = number-of-congested-queues;
while ( m != 0) {
// Round-Robin
schedule-congested-queues();
m--;
}
//Step2: scheduling uncongested ports
m = number-of-nonempty-uncongested-queues;
while ( m != 0) {
LQF-schedule-uncongested-queues();
m-- ;
}

π w (n)

= arg max{W π (n)}
π

(7)

We now consider the dynamics of the discrete-time
switch.

Let MWM be our scheduling algorithm and
be
a
collection

of all possible matchings 12 . For any π ∈ , let Tπ (n) be the
cumulative amount of time that a permutation π is
scheduled in
the time interval [0, n]. Again, Tπ (0) = 0, ∀π ∈ . For n ≥ 0,
the equations below describe queue sizes, departures, and the
busyness of the server over time:

Fair-LQF deviates from LQF as it proactively limits the
throughput of a congested queue. Under admissible traffic, FairLQF behaves identically to LQF, since the limiting size of the
queues is 0. Under inadmissible traffic, it does a provably MaxMin fair rate allocation.
Theorem 2: Let λ1 ≥ · · · ≥ λN be the arrival rates of N
queues and the server capacity be 1. Let R1 , · · · , RN denote the
Max-Min fair rate allocation, and r1 , · · · , rN denote the FairLQF rate allocation. Then ∀k, rk = Rk , i.e. Fair-LQF is MaxMin fair 89 .
B. Performance Evaluation
We studied the performance of LQF and Fair-LQF via simulations and compared it to Max-Min fairness. The simulations
attest that the Fair-LQF rate allocation is Max-Min fair and provides lower delay for uncongested queues 10 .

Qij (n)
Dij (n)

=

Qij (0) + Aij (n) − Dij (n)
n
 

=


π∈ l=1

πij 1Qij (l)>0 (Tπ (l) − Tπ (l − 1))

Tπ (·) is non-decreasing and



π∈

Tπ (n) = n

(8)
(9)
(10)

A. Fluid Model for a Switch
We use the fluid model technique again to study MWM. By
appropriately scaling the discrete-time switch, we can obtain the
fluid model for a switch [3]. Based on equations (8) – (10),
it has been shown [3] that under condition (6), the continuous,
deterministic fluid model of a switch is as follows:

7 The threshold may be chosen freely and does not affect the long-term performance of the algorithm.
8 The proof can be found at http://simula.stanford.edu/∼neha/FMWM.pdf.
9 This theorem holds regardless of the threshold value.
10 The simulation results are at http://simula.stanford.edu/∼neha/FMWM.pdf.

IEEE Communications Society
Globecom 2004

Aij (n)
n

1715

11 In

Qij (t)

=

Ḋij (t)

=

Qij (0) + λij t − Dij (t) ≥ 0, t ≥ 0

πij Ṫπ (t), if Qij (t) > 0 , t ≥ 0


π∈

(11)
(12)

this paper, we let Wij
(n) be Qij (n).
an N × N switch, | | = N !.

12 For

0-7803-8794-5/04/$20.00 © 2004 IEEE

Tπ (·) is non-decreasing, and



π∈

Tπ (t) = t, t ≥ 0

(13)

For a function f , f˙(t) denotes its derivative at t, if one exists.
Assuming Tπ (t) is differentiable and ∃ π  such that: W π (t) <

W π (t), then MWM dictates that Ṫπ (t) = 0.
B. Rate Allocation under MWM
The service rate 13 sij (t) for V OQij is defined as follows:
sij (t) =

1 
πij Tπ (t)
t 
π∈

It has been shown [3] that under admissible traffic for MWM, all
queue-sizes Qij (t) = 0, ∀t ≥ 0. From equations (11) – (13) for
admissible traffic, we can infer that sij (t) ≥ λij , ∀t ≥ 0. We
now characterize the rates s(t) = [sij (t)] under inadmissible
traffic using the notion of a cycle.
Definition 4 (Cycle) In an N × N bipartite graph, a cycle of length l is an ordered set of input-output pairs
{(i1 , o1 ), (i2 , o2 ),· · · , (il , ol )} comprised of edges connecting
these pairs.
Theorem 3: Let Λ = [λij ] s.t. λij > 0, ∀i, j. Let s = [sij ]
represent the rates allocated by MWM. Create an N × N bipartite graph with edge weights wij = (λij − sij )+ . For any cycle
C , if sij > 0 for all edges in C , the following holds 14 :



wij ,oj = wi1 ,ol +

j=1

−1


wij+1 ,oj

A. The Algorithm
Fair-MWM combines the advantages of MWM and Max-Min
fairness to provide fair treatment to all V OQs, ensuring a small
delay for uncongested queues. While all well-behaved 15 flows
receive desired service, the offending flows are allotted an even
fraction of the leftover bandwidth.
When a queue exceeds a pre-specified threshold, say a percentage of the buffer size 16 , it is considered congested. Once a
congested queue V OQij is served, it gets blocked 17 for nj time
slots, where nj is the number of non-empty queues containing
packets headed for output j. In the scenario that multiple outputs have congested queues, nj may be different for every oversubscribed output j. Once the blocking is accounted for, the
matching is determined by MWM as before. The pseudocode
for Fair-MWM is as follows:
//Information Collection
if (voq_size(voq_ID) >= threshold)
add_to_congested_list(voq_ID);
// Scheduling
// Step1: MWM
MWM_schedule_unblocked_voqs();
//Step2: Blocking information
for (i=0; i<N; i++) {
for (j=0; j<N; j++) {
if (voq[i][j] is matched and congested)
cycles_to_block[i][j] =
number-of-nonempty-voqs-to-output-j();
else if (cycles_to_block[i][j] > 0)
cycles_to_block[i][j]--;
}

(14)

j=1

}

C. MWM vs. Max-Min Fairness
Theorem 3 does not give a simple closed-form rate allocation
as in Theorem 1. However, we can use this result to show that
MWM’s rate allocation is not fair. For example, consider a 2 × 2
switch with the following arrival and permutation matrices:






1 0
0 1
0.8 0.1
π2 =
Λ=
π1 =
0 1
1 0
0.3 0.5
Suppose that MWM schedules π1 for α ∈ [0, 1] fraction of the
time and π2 for the remaining (1 − α) fraction of the time. From
Theorem 3, it follows that:
(0.8 − α)+ + (0.5 − α)+ =
(0.3 − (1 − α))+ + (0.1 − (1 − α))+

(15)

MWM’s departure rates and the Max-Min fair rate allocation
below show that MWM lacks fairness.




0.75 0.1
0.7 0.1
Rmwm =
Rmmf =
0.25 0.5
0.3 0.5

B. Performance Evaluation
In this section, we study the rates allocated by the MWM and
Fair-MWM algorithms by simulating various inadmissible traffic scenarios. The results show that Fair-MWM is indeed fair.
We verified, in addition, that when traffic is admissible, the rate
allocation is identical to that of MWM.
We consider a 4 × 4 example of an overloaded switch. Time
is slotted and arriving traffic is Bernoulli IID. The performance
of each algorithm is evaluated under various overloaded arrival
traffic patterns (with multiple overloaded outputs). Simulations
are run long enough to obtain the equilibrium results.
We first compare the performance of MWM and Fair-MWM
with Max-Min fairness, when arriving traffic causes one output
port to be overloaded. Consider the following arrival matrix:


1.0
 0.15
Λ(1) = 
0.15
0.15



13 The service rate may be higher than the arrival rate, as we assume that
MWM schedules a complete matching every time.
14 The proof can be found at http://simula.stanford.edu/∼neha/FMWM.pdf.

IEEE Communications Society
Globecom 2004

0.0
0.2
0.2
0.2


0.0
0.2
0.2
0.2

The Max-Min fair and MWM rate allocations for Λ(1) are:

VI. FAIR -MWM
Analogous to Fair-LQF, we now present Fair-MWM, an algorithm that aims to incorporate fairness into MWM to ensure
that at every output of the N × N switch, the rate allocation is
Max-Min fair.

0.0
0.2
0.2
0.2

Rmmf

0.55
 0.15
=
0.15
0.15

0.0
0.2
0.2
0.2

0.0
0.2
0.2
0.2



0.0
0.88
0.2
 0.04
R
=
0.2 mwm  0.04
0.2
0.04

0.0
0.2
0.2
0.2

0.0
0.2
0.2
0.2


0.0
0.2
0.2
0.2

15 By well-behaved flows, we mean flows that ask for less than their fair share
of service.
16 The threshold may be chosen freely and does not affect the long-term performance of the algorithm.
17 Blocking V OQ is equivalent to assigning it a weight of 0.
ij

1716

0-7803-8794-5/04/$20.00 © 2004 IEEE

Compared to the rate of 0.55 in Rmmf (1), MWM allocates a
service rate of 0.88 to the greedy queue V OQ11 and only 0.04
to the remaining VOQs (V OQ21,31,41 ) destined for output 1.
The Fair-MWM algorithm gives a rate allocation identical to
that given by Rmmf .
0.9
Fair-MWM
MWM

This behavior of MWM is analogous to what we observed previously: greedy flows are served far more than their fair share.
Fair-MWM solves this problem by allocating Max-Min fair
rates: Rf −mwm = Rmmf .
We now test the robustness of Fair-MWM in a more complicated scenario, when there are multiple offending flows for
multiple outputs. The arrival matrix is:

0.8



0.6
 0.6
Λ(4) = 
0.0

0.7

S11

0.6

0.2

0.5

0.0
0.2
0.6
0.6

0.2
0.0
0.0
0.0


0.1
0.1
0.1

0.1

0.4

The MWM and Max-Min fair rate allocations for Λ(4) are:

0.3



0.2

0.1
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

Rmwm

1

λ11

0.06

Fig. 2. MWM & Fair-MWM: Comparison of Rate Allocation

Instead of fixing λ11 at 1.0, we now vary it from 0.1 to 1.0
and evaluate the performance of Fair-MWM. Figure 2 depicts
that Fair-MWM successfully limits the service rate of V OQ11
to 0.55, once λ11 exceeds the fair share. Under MWM, however, the service rate for V OQ11 grows monotonically as λ11
increases. Since the service rates for other queues destined for
the same output are not protected, they become much smaller
than the demand rates.
When there exist multiple offending flows destined for the
same output, Fair-MWM first satisfies the service requirement
for the good flows to that output and then splits the remaining
service capacity evenly among the offenders. For instance, view
the following example:


0.5
 0.5
Λ(2) = 
0.1
0.1

0.1
0.1
0.1
0.1

0.1
0.1
0.1
0.1


0.1
0.1
0.1
0.1

The Max-Min fair and MWM rate allocations for Λ(2) are:


Rmmf

0.4
 0.4
=
0.1
0.1

0.1
0.1
0.1
0.1



0.1
0.45
0.1
 0.45
R
=
0.1 mwm  0.05
0.1
0.05

0.1
0.1
0.1
0.1

0.1
0.1
0.1
0.1

0.1
0.1
0.1
0.1


0.1
0.1
0.1
0.1

The Fair-MWM and Max-Min fair rate allocations are identical.
When there exist offending flows destined for multiple outputs, Fair-MWM limits their service rates independently while
protecting other non-offending flows. For example, consider the
arrival matrix Λ(3):


0.8
 0.3
Λ(3) = 
0.0
0.2

0.0
0.3
0.8
0.2

0.0
0.1
0.0
0.2

Rmwm

0.7
 0.2
=
0.0
0.1

0.0
0.2
0.7
0.1

0.0
0.1
0.0
0.2



0.0
0.5
0.1
 0.3
Rmmf = 

0.0
0.0
0.2
0.2

IEEE Communications Society
Globecom 2004

0.1

0.2

0.0
0.2
0.4
0.4

0.2
0.0
0.0
0.0


0.1
0.1
0.1

0.1

R EFERENCES
[1]
[2]

[6]


0.0
0.1
0.0
0.2



0.1
0.4
0.1
 0.4
R
=
 0.0
0.1 mmf

In practice, traffic is often inadmissible and queues are often
overloaded. The inadmissible traffic scenario, however, has thus
far been neglected. In this paper, we showed that the LQF and
MWM algorithms considered ideal for admissible traffic perform poorly when traffic is inadmissible. Their allocation of service rates to input queues is biased in favor of offending (overloaded) traffic flows, penalizing the well-behaved flows unjustly.
We addressed this problem by proposing algorithms based on
the notion of Max-Min fairness, also highlighting the duality
shared by Max-Min fairness and LQF. Finally, we observed that
our algorithms, Fair-LQF and Fair-MWM, effectively perform a
fair rate allocation with low delay, using the knowledge of queue
sizes alone.
The techniques we employed in this paper are of a general
nature and can easily be adapted to study the performance of
other scheduling algorithms under inadmissible traffic.

[5]

0.0
0.1
0.0
0.2

0.2
0.0
0.0
0.0

VII. C ONCLUSIONS

[4]

0.0
0.3
0.5
0.3

0.0
0.06
0.47
0.47

Again, our algorithm performs a rate allocation that is Max-Min
fair, i.e. Rf −mwm = Rmmf .
This concludes our study of fairness in switch-scheduling as
we complete the analysis of Fair-MWM, having demonstrated
the benefits it adds to the MWM scheduling algorithm.

[3]


0.0
0.1
0.0
0.2

The MWM and Max-Min fair rate allocations for Λ(3) are:


0.47
 0.47
=
0.0

[7]

1717

D. Bertsekas, R. Gallager, “Data Networks”, Prentice Hall, 1992, pp. 526.
J.G. Dai, “Stability of fluid and stochastic networks”, Miscellanea Publication, No. 9, Center for Mathematical Physics and Stochastics, Denmark,
http://www.maphysto.dk, Jan. 1999.
J.G. Dai, B. Prabhakar, “The Throughput of Data Switches with and without Speedup” in Proceedings of IEEE INFOCOM, Mar. 2000, pp. 556-564.
P. Giaccone, B. Prabhakar, D. Shah, “Switching under Energy Constraints”
in Asilomar Conference on Signals, Systems and Computers, Nov. 2002.
N. McKeown, A. Mekkittikul, V. Anantharam, J. Walrand, “Achieving
100% throughput in an input-queued switch” in IEEE Transactions on
Communications, vol. 47, n. 8, Aug. 1999, pp. 1260-1267.
D. Shah, D. Wischik, “Switch under Heavy Traffic”, under preparation,
2004.
L. Tassiulas, “Linear Complexity Algorithms for Maximum Throughput
in Radio Networks and Input-Queued Switches” in Proceedings of IEEE
INFOCOM, Apr. 1998, pp. 533-539.

0-7803-8794-5/04/$20.00 © 2004 IEEE

Discrete Mathematics 313 (2013) 2918–2931

Contents lists available at ScienceDirect

Discrete Mathematics
journal homepage: www.elsevier.com/locate/disc

Combinatorial constructions for maximum optical
orthogonal signature pattern codes✩
Rong Pan, Yanxun Chang ∗
Institute of Mathematics, Beijing Jiaotong University, Beijing 100044, PR China

article

info

Article history:
Received 2 July 2013
Received in revised form 6 September 2013
Accepted 6 September 2013
Available online 26 September 2013
Keywords:
Optical orthogonal signature pattern code
Maximum
Strictly invariant packing
Perfect strictly invariant packing
Incomplete different matrix

abstract
An (m, n, k, λa , λc ) optical orthogonal signature pattern code (OOSPC) is a family C of
m × n (0, 1)-matrices of Hamming weight k satisfying two correlation properties. OOSPCs
find application in transmitting two-dimensional image through multicore fiber in CDMA
networks. Let Θ (m, n, k, λa , λc ) denote the largest possible number of codewords among
all (m, n, k, λa , λc )-OOSPCs. An (m, n, k, λa , λc )-OOSPC with Θ (m, n, k, λa , λc ) codewords
is said to be maximum. For the case λa = λc = λ, the notations (m, n, k, λa , λc )-OOSPC
and Θ (m, n, k, λa , λc ) can be briefly written as (m, n, k, λ)-OOSPC and Θ (m, n, k, λ). In
this paper, some direct constructions for (3, n, 4, 1)-OOSPCs, which are based on skew
starters and an application of the Theorem of Weil on multiplicative character sums, are
given for some positive integer n. Several recursive constructions for (m, n, k, 1)-OOSPCs
are presented by means of incomplete different matrices and group divisible designs. By
utilizing those constructions, the number of the codewords of a maximum (m, n, 4, 1)OOSPC is determined for any positive integers m, n such that gcd(m, 18) = 3 and n ≡
0 (mod 12). It is established that Θ (m, n, 4, 1) = (mn − 12)/12 for any positive integers
m, n such that gcd(m, 18) = 3 and n ≡ 0 (mod 12).
© 2013 Elsevier B.V. All rights reserved.

1. Introduction
The technology of parallel transmission and simultaneous access of two-dimensional (2-D) images in optical codedivision multiple-access (CDMA) network using multicore fiber, called spatial CDMA, was first introduced by Kitayama [19].
In spatial CDMA network, each pixel in a 2-D image is encoded into a signature pattern, which is called an optical orthogonal
signature pattern code (OOSPC). For detailed background we refer to [19,29]. Now we introduce the formal definition of an
optical orthogonal signature pattern code.
Definition 1.1. Let m, n, k, λa and λc be positive integers. An (m, n, k, λa , λc ) optical orthogonal signature pattern code
(briefly, (m, n, k, λa , λc )-OOSPC ) is a family C of m × n (0, 1)-matrices (called codewords) of Hamming weight k satisfying
the following two correlation properties:
(1) The auto-correlation property:
m−1 n−1



xi,j xi⊕s,j⊕
 t ≤ λa

i=0 j=0

for any (xij ) ∈ C and every (s, t ) ∈ Zm × Zn \ {(0, 0)};
✩ Supported by NSFC grant Nos. 61071221, 11271042 and the Fundamental Research Funds for the Central Universities 2011JBZ012.

∗

Corresponding author.
E-mail address: yxchang@bjtu.edu.cn (Y. Chang).

0012-365X/$ – see front matter © 2013 Elsevier B.V. All rights reserved.
http://dx.doi.org/10.1016/j.disc.2013.09.005

R. Pan, Y. Chang / Discrete Mathematics 313 (2013) 2918–2931

2919

(2) The cross-correlation property:
m−1 n−1



xi,j yi⊕s,j⊕
 t ≤ λc

i=0 j=0

for any distinct (xij ), (yij ) ∈ C and every (s, t ) ∈ Zm × Zn ,

 are, respectively, reduced modulo m and n. When
where Zl denotes the additive group of module l. The additions ⊕ and ⊕
λa = λc = λ, the notation (m, n, k, λa , λc )-OOSPC can be briefly written as (m, n, k, λ)-OOSPC. When m and n are coprime
(that is Zm × Zn is isomorphic to Zmn ), (m, n, k, λa , λc )-OOSPC is actually the classical (mn, k, λa , λc ) optical orthogonal code
(briefly (mn, k, λa , λc )-OOC). For the formal definition of OOC, the reader can refer to [10].
j) : (x, y) ∈ X }. By identifying all codewords of an
Let X be a k-subset of Zm × Zn and define X + (i, j) = {(x ⊕ i, y⊕
OOSPC with k-subsets of Zm × Zn representing the subscripts of the nonzero positions, then the notation of OOSPC can be
reformulated by using set-theoretical terminology.
Definition 1.2. Let m, n, k, λa and λc be positive integers. An (m, n, k, λa , λc ) optical orthogonal signature pattern code is a
family C of k-subsets of Zm × Zn satisfying the following two correlation properties:

(1′ ) The auto-correlation property:
|X ∩ (X + (s, t ))| ≤ λa
for any X ∈ C and every (s, t ) ∈ Zm × Zn \ {(0, 0)};
(2′ ) The cross-correlation property:

|X ∩ (Y + (s, t ))| ≤ λc
for any distinct X , Y ∈ C and every (s, t ) ∈ Zm × Zn .
For given positive integers m, n, k, λa and λc , let Θ (m, n, k, λa , λc ) denote the largest possible number of codewords
among all (m, n, k, λa , λc )-OOSPCs. An (m, n, k, λa , λc )-OOSPC with Θ (m, n, k, λa , λc ) codewords is said to be maximum
(or optimal). When λa = λc = λ, Θ (m, n, k, λa , λc ) can be briefly written as Θ (m, n, k, λ). Based on the Johnson bound [18]
for constant weight codes, Θ (m, n, k, λ) is upper bounded by

 
1

mn − 1

k

k−1



 
 
mn − λ
···
···
:= J (mn, k, λ)
k−2
k−λ

mn − 2

where ⌊x⌋ denotes the largest integer not exceeding x.
It is worth mentioning that a maximum (m, n, k, λa , λc )-OOSPC always exists from the definition. Thus, the research on
OOSPCs mainly focus on determining the exact value of Θ (m, n, k, λa , λc ) and constructing a maximum (m, n, k, λa , λc )OOSPC. In [29], it has been shown that a maximum (m, n, k, λa , λc )-OOSPC is equivalent to a maximum (mn, k, λa , λc )-OOC
when m and n are coprime. In this case, we can obtain a number of maximum OOSPCs from rich results on maximum
OOCs (for example, see [1,2,4–10,12–17,22–24,27,28,30]). However, when m and n are not coprime, some constructions
of maximum (m, n, k, λa , λc )-OOSPCs have been
 already known for very specific conditions. Yang and Kwong [29] have
shown that Θ (m, n, k, λa , λc ) ≤ ⌊λa



mn−1

λc

determining the value Θ (n, n, k, λa , 1) =
(1) (n, k, λa ) = (p,

p−1
2

qt +1 −1



k



k−1

2
⌊ λak((kn−−1)1) ⌋

λc


⌋ when λa > λc and presented three algebraic constructions for

for the following parameters:

, p−4 3 ) where p ≡ 3 (mod 4) be a prime;
qt −1

qt −1 −1

(2) (n, k, λa ) = ( q−1 , q−1 , q−1 ) where q be a prime power and t be a positive integer;
(3) (n, k, λa ) = (p, k, 1) where p ≡ 1 (mod k(k − 1)) be a prime.
In [26], Sawa and Kageyama have constructed an (m, n, 3, 1)-OOSPC with Θ (m, n, 3, 1) = ⌊ mn6−1 ⌋ codewords for any
odd integers m, n such that either m or n is not congruent to 5 modulo 6. They have also given a new upper bound of
Θ (m, n, 3, 2, 1) and presented two algebraic constructions for maximum (m, n, 3, 2, 1)-OOSPCs which attain the new
bound. In [25], Sawa has shown a close relationship between OOSPCs and combinatorial packings, and constructed a
(2ε x, y, 4, 2)-OOSPC with J (2ε xy, 4, 2) codewords, which is maximum, where ε ∈ {1, 2} and x, y are positive integers,
whose each factor being a prime less than 500 000 and congruent to 53 or 77 modulo 120 or belonging to S =
{5, 13, 17, 25, 29, 37, 41, 53, 61, 85, 89, 97, 101, 113, 137, 149, 157, 169, 173, 193, 197, 229, 233, 289, 293, 317}.
Recently, there are many studies on multiple-weight OOSPC, which is a generalization of constant-weight OOSPC. It was
proposed by Kwong and Yang [20] for spatial CDMA with multiple performance requirement. The reader may refer to [20,21]
for details. In this paper, we focus our attention on constant-weight OOSPC. In order to construct more (m, n, 4, 1)-OOSPCs
with Θ (m, n, 4, 1) = ⌊ mn12−1 ⌋ codewords for some integers m, n, we first need some terminologies from design theory.
Assume that v ≥ k ≥ 2 are positive integers. A (v, k, 1) packing is a pair (V , B ), where V is a v -set of points and B is a collection of k-subsets (called blocks) of V , such that every pair of distinct points from V occurs in at most one block. Suppose that

2920

R. Pan, Y. Chang / Discrete Mathematics 313 (2013) 2918–2931

σ is a permutation on V . For any block B = {b0 , b1 , . . . , bk−1 } define Bσ = {bσ0 , bσ1 , . . . , bσk−1 }. If B σ = {Bσ : B ∈ B } = B ,
then σ is called an automorphism of the (v, k, 1) packing (V , B ). The collection of all automorphisms of (V , B ) forms group
under composition, called the full automorphism group, and any of its subgroup is called an automorphism group of (V , B ).
Suppose that G be an automorphism group of (V , B ). For any B ∈ B , the stabilizer of B under G is the subgroup of G consisting of all elements σ ∈ G such that Bσ = B. And the orbit of B under G is the collection OrbG (B) of all distinct images of
B under G, i.e.,
OrbG (B) = {Bσ : σ ∈ G}.
It is clear that B can be partitioned into some orbits under G. An arbitrary set of representatives for each orbit of B is called
the base blocks of the packing. (V , B ) is said to be G-invariant if it admits G as a point-regular automorphism group, that is,
G is an automorphism group such that for any x, y ∈ V , there exists exactly one element σ ∈ G such that xσ = y. Moreover,
a G-invariant packing (V , B ) is said to be strictly, if the stabilizer of each B ∈ B under G equals the identity of G, that is, the
orbit of B under G consists |G| blocks.
As the terminology suggests, given an arbitrary family of all base blocks of a strictly Zm × Zn -invariant (mn, k, 1) packing
((m × n, k, 1)-SP in short), we can obtain the packing by successively add (i, j) to each base block, where (i, j) ∈ Zm × Zn . Let

A(mn, k) be the largest possible number of base blocks among all (m × n, k, 1)-SPs. An (m × n, k, 1)-SP is said to be maximum
−1
⌋. Suppose that (Zm × Zn , B ) be an (m × n, k, 1)-SP
if it contains 
A(mn, k) base blocks. It is obvious that 
A(mn, k) ≤ ⌊ kmn
(k−1)

 be the family of all its base blocks. For any base block
and B
,
B = {(x0 , y0 ), (x1 , y1 ), . . . , (xk−1 , yk−1 )} ∈ B
we define the list differences of B by

yj ) : 0 ≤ i, j ≤ k − 1, i ̸= j}
△B = {(xi ⊖ xj , yi ⊖
 is
 are, respectively, reduced modulo m and n, and the differences of B
where the subtractions ⊖ and ⊖

=
△B
△B.

B∈B

 cover each non-zero element in Zm × Zn at most once since (Zm × Zn , B ) is strictly. The difference leave of
It is clear that △B
 is the set of all non-zero elements in Zm × Zn which are not covered by △B
, denoted by DL(B
). The packing is said to be
B
) along with {(0, 0)} forms a subgroup S × T of Zm × Zn , where S and T are, respectively, the
an (m × n, s × t , k, 1)-SP if DL(B
−st
base blocks. The following
subgroups of order s in Zm and order t in Zn . In this case, an (m × n, s × t , k, 1)-SP contains kmn
(k−1)
result is presented in [25].

Lemma 1.3 ([25]). A maximum (m, n, k, 1)-OOSPC is equivalent to a maximum strictly Zm × Zn -invariant (mn, k, 1) packing
(i.e., (m × n, k, 1)-SP).
Therefore, instead of constructing maximum (m, n, k, 1)-OOSPC directly, we need only to construct the corresponding
maximum (m × n, k, 1)-SP. In view of design theory, the following results are clear.
−st
Lemma 1.4. If 1 ≤ st ≤ k(k − 1), then an (m × n, s × t , k, 1)-SP is maximum and hence Θ (m, n, k, 1) = kmn
.
(k−1)

Lemma 1.5. If both an (m × n, s × t , k, 1)-SP with b1 base blocks and an (s × t , k, 1)-SP with b2 base blocks exist, then so
does an (m × n, k, 1)-SP with b1 + b2 base blocks. Moreover, if an (s × t , g × h, k, 1)-SP with b2 base blocks is given, then an
(m × n, g × h, k, 1)-SP with b1 + b2 base blocks is obtained.
In this paper, we focus our attention on constructing a maximum (m, n, 4, 1)-OOSPC (i.e. (m × n, 4, 1)-SP with ⌊ mn12−1 ⌋
base blocks) for any positive integers m, n such that gcd(m, 18) = 3 and n ≡ 0 (mod 12). In Section 2, we present some
direct constructions by utilizing skew starters and an application of the Theorem of Weil on multiplicative character sums,
and obtain a (3 × gu, 3 × g , 4, 1)-SP for g = 12 with u a prime greater than 3 and for g = 24, 36 with u a positive integer such
that gcd(u, 150) = 1 or 25. In Section 3, recursive constructions for (m × n, k, 1)-SPs are presented by means of incomplete
different matrices and group divisible designs. In Section 4, combining all previous results, we obtain an (m, n, 4, 1)OOSPC with (mn − 12)/12 codewords, which is maximum, for any positive integers m, n such that gcd(m, 18) = 3 and
n ≡ 0 (mod 12).
2. Direct constructions
In this section, we shall give some direct constructions for (3 × gu, 3 × g , 4, 1)-SP, which are based on skew starters
and an application of the Theorem of Weil on multiplicative character sums, for g = 12 with u a prime greater than 3, and
g = 24, 36 with u a positive integer such that gcd(u, 150) = 1 or 25. To develop our constructions, we first require the
notion of skew starters from design theory.


1
Let (G, +) be an abelian group of order u > 1. A skew starter in G is a set of unordered pairs S = (xi , yi ) : 1 ≤ i ≤ u−
2

which satisfies the following three properties:

R. Pan, Y. Chang / Discrete Mathematics 313 (2013) 2918–2931

2921

1
(1) {xi : 1 ≤ i ≤ u−
} ∪ {yi : 1 ≤ i ≤ u−2 1 } = G \ {0};
2
1
(2) {±(xi − yi ) : 1 ≤ i ≤ u−
} = G \ {0};
2
1
} = G \ {0}.
(3) {±(xi + yi ) : 1 ≤ i ≤ u−
2

1
According to the definition, a skew starter in G can exist only if u is odd. Furthermore, if we write X = {xi : 1 ≤ i ≤ u−
}
2

1
and Y = {yi : 1 ≤ i ≤ u−
}. Then we may assume, without loss of generality, that X = −Y , and hence we have
2
X ∪ Y = X ∪ (−X ) = Y ∪ (−Y ) = G \ {0}.
Skew starters have been useful in constructions of Room squares and other combinatorial designs. Skew starters in Zu
have special applications to constructions of Hamiltonian path balanced tournament designs and optical orthogonal codes
[5,9,17]. The following known result on skew starters is very useful.

Lemma 2.1 ([9]). There exists a skew starter in Zu for each positive integer u such that gcd(u, 150) = 1 or 25. There does not
exist any skew starter in Zu if u ≡ 0 (mod 3).
Now we give our constructions.
Lemma 2.2. Let u be a positive integer such that gcd(u, 150) = 1 or 25. Then there exists a (3 × 24u, 3 × 24, 4, 1)-SP.
Proof. By Lemma 2.1, there exists a skew starter Su in Zu . Since gcd(u, 24) = 1, then the desired (3 × 24u, 3 × 24, 4, 1)-SP
can be obtained by listing the following 6(u − 1) base blocks over Z3 × Z24 × Zu , which is isomorphic to Z3 × Z24u :

{(0, 0, 0), (0, 0, x − y), (0, 5, x), (1, 1, −y)},
{(0, 0, 0), (0, 12, x − y), (1, 2, −y), (2, 22, x)},
{(0, 0, 0), (0, 1, δ x), (0, 7, δ y), (0, 11, δ(x + y))},
{(0, 0, 0), (0, 16, δ x), (1, 5, δ y), (2, 0, δ(x + y))},
{(0, 0, 0), (1, 10, δ x), (1, 12, δ y), (2, 3, δ(x + y))},
{(0, 0, 0), (1, 11, δ x), (2, 15, δ y), (2, 18, δ(x + y))},
{(0, 0, 0), (1, 22, δ x), (2, 1, δ y), (2, 16, δ(x + y))},
where δ ∈ {1, −1}, and (x, y) runs over all pairs of the skew starter Su in Zu .
It is readily calculated in Z3 × Z24 × Zu that the differences from the initial base blocks can be partitioned into the following
six classes depending on the value in the third coordinate:
(1) {(a, b, δ x) : (x, y) ∈ Su } where (a, b) = ±(0, 1), ±(0, 3), ±(0, 4), ±(0, 8), ±(0, 9), ±(1, 10), ±(1, 11), ±(1, 15),
±(1, 19), ±(1, 22);
(2) {(a, b, δ y) : (x, y) ∈ Su } where (a, b) = ±(0, 7), ±(0, 10), ±(1, 5), ±(1, 7), ±(1, 9), ±(1, 12), ±(1, 16), ±(1, 17),
±(1, 18), ±(1, 23);
(3) {(a, b, δ(x + y)) : (x, y) ∈ Su } where (a, b) = ±(0, 11), ±(1, 0), ±(1, 6), ±(1, 8), ±(1, 20), ±(1, 21);
(4) {(a, b, δ(x − y)) : (x, y) ∈ Su } where (a, b) = (0, 0), ±(0, 2), ±(0, 6), (0, 12), ±(1, 3), ±(1, 4), ±(1, 13);
(5) {(a, b, x), (a, b, y) : (x, y) ∈ Su } where (a, b) = (0, 5), (2, 10), (2, 22), (2, 23);
(6) {(a, b, −x), (a, b, −y) : (x, y) ∈ Su } where (a, b) = (0, 19), (1, 1), (1, 2), (1, 14).
Since Su is a skew starter in Zu , it is easy to see that each element in (Z3 × Z24 × Zu ) \ (Z3 × Z24 × {0}) occurs in the above
list of differences exactly once, while any element in the additive subgroup Z3 × Z24 × {0} is not covered at all. The assertion
then follows immediately. 
Lemma 2.3. Let u be a positive integer such that gcd(u, 150) = 1 or 25. Then there exists a (3 × 36u, 3 × 36, 4, 1)-SP.
Proof. By Lemma 2.1, there exists a skew starter Su in Zu . The desired (3 × 36u, 3 × 36, 4, 1)-SP can be obtained by listing
the following 9(u − 1) base blocks over Z3 × Z36 × Zu :

{(0, 0, 0), (0, 0, x − y), (0, 1, x), (0, 3, −y)},
{(0, 0, 0), (0, 2, x + y), (0, 10, y − x), (0, 28, 2y)}
{(0, 0, 0), (0, 6, δ x), (0, 25, δ y), (1, 9, δ(x + y))},
{(0, 0, 0), (0, 13, δ x), (1, 5, δ y), (2, 1, δ(x + y))},
{(0, 0, 0), (1, 14, δ x), (1, 18, δ y), (1, 23, δ(x + y))},
{(0, 0, 0), (1, 19, δ x), (2, 0, δ y), (2, 29, δ(x + y))},
{(0, 0, 0), (1, 29, δ x), (2, 14, δ y), (2, 35, δ(x + y))},
{(0, 0, 0), (1, 30, δ x), (2, 10, δ y), (2, 34, δ(x + y))},
{(0, 0, 0), (1, 33, δ x), (2, 5, δ y), (2, 21, δ(x + y))},
{(0, 0, 0), (1, 34, δ x), (2, 11, δ y), (2, 25, δ(x + y))},
where δ ∈ {1, −1}, and (x, y) runs over all pairs of the skew starter Su in Zu .
Since Su is a skew starter in Zu , it is readily checked that each element in (Z3 × Z36 × Zu ) \ (Z3 × Z36 × {0}) occurs in the
differences from the above list of base blocks exactly once, while any element in the additive subgroup Z3 × Z36 × {0} is not
covered at all. The assertion then follows immediately. 

2922

R. Pan, Y. Chang / Discrete Mathematics 313 (2013) 2918–2931

The construction for (3 × 12u, 3 × 12, 4, 1)-SP is a little different from above constructions. Cyclotomic cosets play an
important role in direct constructions for (3 × 12u, 3 × 12, 4, 1)-SP. We first need to explain some notions. For a given prime
p−1
p ≡ 1 (mod n) and a primitive element ω ∈ Zp , C0n will denote the multiplicative subgroup {ωin : 0 ≤ i ≤ n − 1} of the
nth powers modulo p, while Cjn (0 ≤ j ≤ n − 1) will denote the coset of C0n in Zp∗ represented by ωj , i.e.,



Cjn = ωin+j : 0 ≤ i ≤

p−1
n


−1 .

The following lemma is an application of the Theorem of Weil on multiplicative character sums, which can be also seen as
a corollary of Theorem 2.2 in [3].

√

Lemma 2.4 ([6, Theorem 3.2]). Let p ≡ 1 (mod n) be a prime with p − [ i=0 i (s − i − 1)(n − 1)s−i ] p − sns−1 > 0. Then,
for any given s-tuple (j1 , j2 , . . . , js ) ∈ {0, 1, 2, . . . , n − 1}s and any given s-tuple (c1 , c2 , . . . , cs ) of pairwise distinct elements
of Zp , there exists an element x ∈ Zp such that x + ci ∈ Cjni for each i.

s−2  s 

Lemma 2.5. If p ≡ 1 (mod 4) is a prime and p > 5, then there exists an element x ∈ Zp satisfying x + 1 ∈ C02 , {x, x − 1} ⊆ C12 .
If p ≡ 3 (mod 4) is a prime and p > 11, then there exists an element x ∈ Zp satisfying {x − 2, x + 1} ⊆ C02 , {x, x − 1} ⊆ C12 .
Proof. Suppose that p ≡ 1 (mod 4) is a prime. Applying Lemma 2.4 with n = 2 and s = 3, an element x satisfying the
condition always exists in Zp for any prime p ≡ 1 (mod 4) and p ≥ 53. For the remaining cases 5 < p < 53, such an element
x ∈ Zp is listed below: (p, x) = (13, 8), (17, 7), (29, 3), (37, 6), (41, 7).
Suppose that p ≡ 3 (mod 4) is a prime. Applying Lemma 2.4 with n = 2 and s = 4, an element x satisfying the condition
always exists in Zp for any prime p ≡ 3 (mod 4) and p ≥ 351. For the remaining cases 11 < p < 351, a computer search
shows that such an element x ∈ Zp always exists. We list our results below:
p

19

23

31

43

47

59

67

71

79

83

x

3

11

27

3

11

11

3

14

7

6

p
x
p
x

103 107 127 131
6

18

24

51

139 151 163 167 179 191 199 211 223 227 239 251 263 271
3

7

3

35

11

22

12

3

6

6

14

11

15

13

283 307 311 331 347
3

3

23

3

51



Lemma 2.6. Let p ≡ 1 (mod 4) be a prime. Then a (3 × 12p, 3 × 12, 4, 1)-SP exists.
Proof. First, we construct (3 × 60, 3 × 12, 4, 1)-SP with the following 12 base blocks over Z3 × Z60 :
{(0, 0), (0, 1), (1, 9), (1, 52)}, {(0, 0), (0, 2), (0, 16), (1, 49)}, {(0, 0), (0, 3), (1, 21), (1, 44)},
{(0, 0), (0, 4), (1, 11), (1, 17)}, {(0, 0), (0, 7), (0, 36), (2, 33)}, {(0, 0), (0, 8), (1, 12), (1, 24)},
{(0, 0), (0, 9), (1, 2), (1, 28)}, {(0, 0), (0, 11), (1, 48), (2, 14)}, {(0, 0), (0, 18), (1, 32), (1, 54)},
{(0, 0), (0, 19), (0, 47), (2, 18)}, {(0, 0), (0, 21), (1, 43), (2, 22)}, {(0, 0), (0, 27), (1, 23), (2, 29)}.
Note that −1 ∈ C02 for p ≡ 1 (mod 4). When p > 5 is prime, by Lemma 2.1 there exists a skew starter Sp in Zp since
gcd(p, 150) = 1. Then all base blocks of the desired (3 × 12p, 3 × 12, 4, 1)-SP can be divided into two parts over Z3 × Z12 × Zp ,
which is isomorphic to Z3 × Z12p . The first part consists of the following (p − 1)/2 base blocks:

C = {{(0, 0, 0), (0, 0, x − y), (0, 1, x), (0, 7, −y)} : (x, y) ∈ Sp }.
It is readily checked that

△C = {±(0, 0, x − y), ±(0, 1, x), ±(0, 1, y), ±(0, 5, x), ±(0, 5, y), ±(0, 6, x + y) : (x, y) ∈ Sp }

=
α × Zp \ {0}
α∈X

where X = {(0, 0), ±(0, 1), ±(0, 5), (0, 6)}. The second part consists of 5(p − 1)/2 base blocks. By Lemma 2.5 there exists
an element z ∈ Zp such that z + 1 ∈ C02 and {z , z − 1} ⊆ C12 . Consider the following base blocks:

B = {{(0, 0, 0), (0, 2, 1), (1, 10, z ), (2, 10, z + 1)}, {(0, 0, 0), (0, 3, z + 1), (1, 6, 1), (2, 5, z )},

{(0, 0, 0), (0, 3, z ), (1, 8, 1), (2, 8, z + 1)}, {(0, 0, 0), (0, 4, z ), (1, 1, z − 1), (1, 3, −1)},
{(0, 0, 0), (0, 4, 1), (1, 6, z ), (2, 7, z + 1)}}.
It is readily calculated that

△B =


α∈Y

α × Rα

R. Pan, Y. Chang / Discrete Mathematics 313 (2013) 2918–2931

2923

where X and Y form a partition of Z3 × Z12 , and Rα is a complete system of representatives for the cosets of C02 in Zp∗ for each
α ∈ Y . Let

F =



B · (1, 1, r ).

r ∈C02

We can have

△F =



α × Zp∗ .

α∈Y

It is readily checked that | F ∪ C |= 3(p − 1) and each element in (Z3 × Z12 × Zp ) \ (Z3 × Z12 × {0}) occurs in △(F ∪ C )
exactly once, while any element in the additive subgroup Z3 × Z12 × {0} is not covered at all. The assertion then follows
immediately. 
Lemma 2.7. Let p ≡ 3 (mod 4) be a prime and p > 3. Then a (3 × 12p, 3 × 12, 4, 1)-SP exists.
Proof. Note that −1 ∈ C12 for p ≡ 3 (mod 4). When p > 11 is prime, by Lemma 2.5 there exists an element x ∈ Zp satisfying
that {x − 2, x + 1} ⊆ C02 and {x, x − 1} ⊆ C12 . The desired (3 × 12p, 3 × 12, 4, 1)-SP can be obtained by listing the following
3(p − 1) base blocks over Z3 × Z12 × Zp :

{(0, 0, 0), (0, 0, x − 1), (0, 1, −1), (0, 7, x)} · (1, 1, r ),
{(0, 0, 0), (0, 2, x + 1), (1, 10, x), (2, 10, 1)} · (1, 1, r ),
{(0, 0, 0), (0, 3, 1), (1, 8, x + 1), (2, 8, x)} · (1, 1, r ),
{(0, 0, 0), (0, 3, x − 1), (1, 6, −1), (2, 5, x − 2)} · (1, 1, r ),
{(0, 0, 0), (0, 4, 1 − x), (1, 1, −x), (1, 3, −1)} · (1, 1, r ),
{(0, 0, 0), (0, 4, −x − 1), (1, 6, −x), (2, 7, −1)} · (1, 1, r ),
where r ∈ C02 . It is readily checked that each element in (Z3 × Z12 × Zp ) \ (Z3 × Z12 × {0}) occurs in the differences from the
above list of base blocks exactly once, while any element in the additive subgroup Z3 × Z12 × {0} is not covered at all. The
assertion then follows immediately. When p = 7, 11, the desired (3 × 12p, 3 × 12, 4, 1)-SP can be obtained by listing the
following 3(p − 1) base blocks over Z3 × Z12 × Zp , which is isomorphic to Z3 × Z12p .
p = 7:

{(0, 0, 0), (0, 0, 2), (0, 1, 1), (0, 7, 3)} · (1, 1, r ),
{(0, 0, 0), (0, 2, 1), (1, 10, 3), (2, 10, 4)} · (1, 1, r ),
{(0, 0, 0), (0, 3, 1), (1, 8, 3), (2, 8, 2)} · (1, 1, r ),
{(0, 0, 0), (0, 3, 5), (1, 6, 2), (2, 5, 3)} · (1, 1, r ),
{(0, 0, 0), (0, 4, 3), (1, 1, 1), (1, 3, 6)} · (1, 1, r ),
{(0, 0, 0), (0, 4, 2), (1, 6, 3), (2, 7, 1)} · (1, 1, r ),
where r ∈ {1, 2, 4};
p = 11:

{(0, 0, 0), (0, 0, 1), (0, 1, 3), (0, 7, 2)} · (1, 1, r ),
{(0, 0, 0), (0, 2, 1), (1, 10, 3), (2, 10, 2)} · (1, 1, r ),
{(0, 0, 0), (0, 3, 1), (1, 8, 3), (2, 8, 8)} · (1, 1, r ),
{(0, 0, 0), (0, 3, 2), (1, 6, 1), (2, 5, 3)} · (1, 1, r ),
{(0, 0, 0), (0, 4, 1), (1, 1, 2), (1, 3, 4)} · (1, 1, r ),
{(0, 0, 0), (0, 4, 2), (1, 6, 8), (2, 7, 6)} · (1, 1, r ),
where r ∈ {1, 3, 4, 5, 9}.



3. Recursive constructions
In this section, we shall give some recursive constructions for (m × n, k, 1)-SPs. We first introduce some auxiliary designs.
Let (G, ·) be a finite group of order v and H be a subgroup of order h in G. An H-regular (v, k, λ)-incomplete difference
matrix over G is a k × λ(v − h) matrix D = (dij ), 0 ≤ i ≤ k − 1, 1 ≤ j ≤ λ(v − h), with entries from G, such that for any
1
0 ≤ i < j ≤ k − 1 the multiset {dil · d−
: 1 ≤ l ≤ λ(v − h)} contains every element of G \ H exactly λ times. When G
jl
is an abelian group, typically additive notation is used, so that the differences dil − djl are employed. In what follows, we
assume that G = Zv and H is a subgroup of order h in Zv . Then H = {iv/h : 0 ≤ i ≤ h − 1}. We usually denote an H-regular
(v, k, λ)-incomplete difference matrix over Zv by h-regular ICDM(k, λ; v) if |H | = h. When H = ∅ or h = 0, an H-regular
ICDM(k, λ; v) is termed as CDM(k, λ; v). When λ = 1, write h-regular ICDM(k, 1; v) (or CDM(k, 1; v)) briefly as h-regular
ICDM(k; v) (or CDM(k; v), respectively).
Difference matrices have been studied extensively as a consequence of their uses in the constructions of optical
orthogonal codes, orthogonal arrays, generalized Steiner triple systems, constant weight codes, and so on. For related details,
the reader can refer to [11] and the references therein. Here we list some results which are very useful to our recursion.

2924

R. Pan, Y. Chang / Discrete Mathematics 313 (2013) 2918–2931

Lemma 3.1 ([11]). Let v and k be positive integers such that gcd(v, (k − 1)!) = 1. Let dij ≡ ij (mod v) for i = 0, 1, . . . , k − 1
and j = 0, 1, . . . , v − 1. Then D = (dij ) is a CDM(k; v). In particular, if v is an odd prime number, then there exists a CDM(k; v)
for any integer k ≤ v.
Lemma 3.2. There exists a 2-regular ICDM(4; v) over Zv for any positive integer v of the following types:
(1) [7, Lemma 3.6] v = 2n for any integer n ≥ 3;
(2) [5, Theorem 5.8] v = 3 · 2n for any integer n ≥ 2.
Construction 3.3. Let m, n, and v be positive integers. Suppose that there exist:
(1) an (m × n, s × t , k, 1)-SP;
(2) a CDM(k; v).
Then there exists an (m × nv, s × t v, k, 1)-SP (or (mv × n, sv × t , k, 1)-SP).
Proof. Let m = sm0 and n = tn0 . Suppose that B be the family of all base blocks of the given (m × n, s × t , k, 1)-SP over
Zm × Zn , where △B = (Zm × Zn )\(m0 Zm × n0 Zn ). Suppose that D = (dij ) be a CDM(k; v) where dij ∈ Zv for i = 0, 1, . . . , k − 1,
j = 1, 2, . . . , v such that the multiset {dil − djl : l = 1, 2, . . . , v} = Zv where 0 ≤ i, j ≤ k − 1 and i ̸= j. For any
B = {(x0 , y0 ), (x1 , y1 ), . . . , (xk−1 , yk−1 )} ∈ B , construct the following collection of k-subsets of Zm × Znv





AB = {(x0 , y0 + nd0j ), (x1 , y1 + nd1j ), . . . , (xk−1 , yk−1 + ndk−1,j )} : j = 1, 2, . . . , v .
It is readily calculated that

△AB = {(xi − xj , yi − yj + n(dil − djl )) : 0 ≤ i, j ≤ k − 1, i ̸= j, l = 1, 2, . . . , v}
= {(α, β + nγ ) : (α, β) ∈ △B, γ ∈ Zv }.

Let C = B∈B AB . Then we can have

△AB = {(α, β + nγ ) : (α, β) ∈ △B , γ ∈ Zv }
△C =
B∈B

= {(α, β + nγ ) : (α, β) ∈ (Zm × Zn ) \ (m0 Zm × n0 Zn ), γ ∈ Zv }.
It is readily checked that each element in (Zm × Znv ) \ (m0 Zm ) × (n0 Zn
v ) occurs in △C exactly once, while any element in the
additive subgroup (m0 Zm ) × (n0 Znv ) is not covered at all. Then C = B∈B AB is the required (m × nv, s × t v, k, 1)-SP. 
Construction 3.4. Let m, n, and v be positive integers such that gcd(n, v) = 1. Suppose that there exist:
(1) an (m × n, s × t , k, 1)-SP;
(2) an h-regular ICDM(k; v);
(3) an (m × nh, s × th, k, 1)-SP.
Then there exists an (m × nv, s × t v, k, 1)-SP.
Proof. Since gcd(n, v) = 1, we then can construct the required (m × nv, s × t v, k, 1)-SP over Zm × Zn × Zv which is isomorphic
to Zm × Znv . Let m = sm0 and n = tn0 . Suppose that B be the family of all base blocks of the given (m × n, s × t , k, 1)-SP
over Zm × Zn , where △B = (Zm × Zn ) \ (m0 Zm × n0 Zn ). Suppose that D = (dij ) be an h-regular ICDM(k; v) where dij ∈ Zv
for i = 0, 1, . . . , k − 1, j = 1, 2, . . . , v − h such that the multiset

{dil − djl : l = 1, 2, . . . , v − h} = Zv \ {0, v/h, . . . , (h − 1)v/h}
where 0 ≤ i, j ≤ k − 1 and i ̸= j. For any B = {(x0 , y0 ), (x1 , y1 ), . . . , (xk−1 , yk−1 )} ∈ B , construct





AB = {(x0 , y0 , d0j ), (x1 , y1 , d1j ), . . . , (xk−1 , yk−1 , dk−1,j )} : j = 1, 2, . . . , v − h .
It is readily calculated that

△AB = {(xi − xj , yi − yj , dil − djl ) : 0 ≤ i, j ≤ k − 1, i ̸= j, l = 1, 2, . . . , v − h}
= {(α, β, γ ) : (α, β) ∈ △B, γ ∈ Zv , γ ̸= 0, v/h, . . . , (h − 1)v/h}.

Let C = B∈B AB . Then

△C =
{(α, β, γ ) : (α, β) ∈ △B, γ ∈ Zv , γ ̸= 0, v/h, . . . , (h − 1)v/h}
B∈B

= {(α, β, γ ) : (α, β) ∈ △B , γ ∈ Zv , γ ̸= 0, v/h, . . . , (h − 1)v/h}
= {(α, β, γ ) : (α, β) ∈ (Zm × Zn ) \ (m0 Zm × n0 Zn ), γ ∈ Zv , γ ̸= 0, v/h, . . . , (h − 1)v/h}.

R. Pan, Y. Chang / Discrete Mathematics 313 (2013) 2918–2931

2925

Suppose that F be the family of all base blocks of the given (m × nh, s × th, k, 1)-SP over Zm × Zn × Zh , where △F =
{(α, β, γ ) : (α, β) ∈ (Zm × Zn ) \ (m0 Zm × n0 Zn ), γ ∈ Zh }. Let F ′ be defined over Zm × Zn × Zv as follows:


F ′ = {(x0 , y0 , z0 · v/h), . . . , (xk−1 , yk−1 , zk−1 · v/h)} : {(x0 , y0 , z0 ), . . . , (xk−1 , yk−1 , zk−1 )} ∈ F .
It is readily calculated that



△F ′ = (α, β, γ ) : (α, β) ∈ (Zm × Zn ) \ (m0 Zm × n0 Zn ), γ ∈ {0, v/h, . . . , (h − 1)v/h} .
Then



△(C ∪ F ′ ) = (α, β, γ ) : (α, β) ∈ (Zm × Zn ) \ (m0 Zm × n0 Zn ), γ ∈ Zv .
It is readily checked that each element in (Zm × Zn × Zv ) \ (m0 Zm × n0 Zn × Zv ) occurs in △(C ∪ F ′ ) exactly once, while any
element in the additive subgroup (m0 Zm )×(n0 Zn )× Zv is not covered at all. Then C ∪ F ′ is the required (m × nv, s × t v, k, 1)SP. 
In order to improve our recursive construction, we need to introduce a new concept so called perfect. Let t be a positive
divisor of n such that n = tn0 . Suppose that C = {Bi : 1 ≤ i ≤ r } is an (m × nh, s × th, k, 1)-SP, where (0, 0) ∈ Bi for
1 ≤ i ≤ r . Define
ele(C , x) = {y : (0, 0) ̸= (x, y) ∈ B, B ∈ C }
for each x ∈ Zm . Then the (m × nh, s × th, k, 1)-SP is said to be (−, h) perfect, if



ele(C , x) ⊆ a + bn : 0 ≤ a ≤

n
2


, a ̸= 0, n0 , . . . , (t − 1)n0 ; b = 0, 1, . . . , h − 1

for each x ∈ {0, m/s, . . . , (s − 1)m/s} and



ele(C , x) ⊆ a + bn : 0 ≤ a ≤

n
2


; b = 0, 1, . . . , h − 1

for each x ∈ Zm \ {0, m/s, . . . , (s − 1)m/s}.
Lemma 3.5. There exist (−, 1) perfect (3 × n, 3 × g , 4, 1)-SP for (g , n) = (2, 18), (4, 24).
Proof. The all base blocks of the desired (−, 1) perfect (3 × n, 3 × g , 4, 1)-SP’s are listed below:
(g , n) = (2, 18):
{(0, 0), (0, 1), (0, 7), (1, 5)}, {(0, 0), (0, 2), (1, 8), (2, 7)}, {(0, 0), (0, 3), (0, 8), (2, 6)},
{(0, 0), (0, 4), (1, 7), (2, 8)}.
(g , n) = (4, 24):
{(0, 0), (0, 1), (1, 5), (1, 10)}, {(0, 0), (0, 2), (2, 4), (2, 11)}, {(0, 0), (0, 3), (0, 11), (2, 8)},
{(0, 0), (0, 4), (1, 1), (1, 11)}, {(0, 0), (0, 9), (1, 2), (2, 10)}. 
Lemma 3.6. There exist (−, 2) perfect (3 × n, 3 × g , 4, 1)-SP for (g , n) = (4, 36), (8, 48).
Proof. The all base blocks of the desired (−, 2) perfect (3 × n, 3 × g , 4, 1)-SP’s are listed below:
(g , n) = (4, 36):
{(0, 0), (0, 1), (0, 23), (1, 26)}, {(0, 0), (0, 2), (0, 6), (0, 26)}, {(0, 0), (0, 3), (2, 7), (2, 22)},
{(0, 0), (0, 5), (1, 2), (2, 26)}, {(0, 0), (0, 7), (1, 20), (2, 6)}, {(0, 0), (0, 8), (1, 7), (2, 2)},
{(0, 0), (0, 19), (1, 23), (2, 8)}, {(0, 0), (0, 25), (1, 8), (2, 20)}.
(g , n) = (8, 48):
{(0, 0), (0, 1), (0, 10), (1, 35)}, {(0, 0), (0, 2), (1, 33), (2, 4)}, {(0, 0), (0, 3), (0, 35), (1, 10)},
{(0, 0), (0, 7), (0, 26), (2, 34)}, {(0, 0), (0, 8), (1, 5), (2, 9)}, {(0, 0), (0, 11), (0, 25), (2, 10)},
{(0, 0), (0, 27), (2, 7), (2, 11)}, {(0, 0), (0, 28), (1, 9), (2, 26)}, {(0, 0), (0, 33), (1, 11), (1, 28)},
{(0, 0), (1, 3), (1, 8), (2, 35)}. 
The following is an improved recursive construction which is based on incomplete difference matrices.
Construction 3.7. Let m, n, and v be positive integers. Suppose that there exist:
(1) a (−, 1) perfect (m × n, s × t , k, 1)-SP;
(2) a (−, h) perfect (m × nh, s × th, k, 1)-SP;
(3) an h-regular ICDM(k; v).
Then there exists a (−, v) perfect (m × nv, s × t v, k, 1)-SP.

2926

R. Pan, Y. Chang / Discrete Mathematics 313 (2013) 2918–2931

Proof. Let m = sm0 , n = tn0 and v = hv0 . Suppose that B = {(0, 0) ∈ Bi : 1 ≤ i ≤ r } be the family of all base blocks of the
given (−, 1) perfect (m × n, s × t , k, 1)-SP over Zm × Zn , where ele(B , x) ⊆ {y : 0 ≤ y ≤ ⌊n/2⌋, y ̸= 0, n0 , . . . , (t − 1)n0 }
for each x ∈ m0 Zm , ele(B , x) ⊆ {y : 0 ≤ y ≤ ⌊n/2⌋} for each x ∈ Zm \ (m0 Zm ), and △B = (Zm × Zn ) \ (m0 Zm × n0 Zn ).
Suppose that D = (dij ) be an h-regular ICDM(k; v) where dij ∈ Zv for i = 0, 1, . . . , k − 1, j = 1, 2, . . . , v − h such
that the multiset {dil − djl : l = 1, 2, . . . , v − h} = Zv \ {0, v0 , . . . , (h − 1)v0 } where 0 ≤ i, j ≤ k − 1 and
i ̸= j. For any B = {(x0 , y0 ) = (0, 0), (x1 , y1 ), . . . , (xk−1 , yk−1 )} ∈ B , without loss of generality we can assume that
0 ≤ y1 ≤ y2 ≤ · · · ≤ yk−1 ≤ ⌊n/2⌋, construct

AB = {(0, 0), (x1 , y1 + (d1l − d0l )n), . . . , (xk−1 , yk−1 + (dk−1,l − d0l )n)} : 1 ≤ l ≤ v − h .





It is readily calculated that

△AB = ±{(xj − xi , yj − yi + (djl − dil )n) : 0 ≤ i < j ≤ k − 1, 1 ≤ l ≤ v − h}
= ±{(xj − xi , yj − yi + γ n) : 0 ≤ i < j ≤ k − 1, γ ∈ Zv \ {0, v0 , . . . , (h − 1)v0 }}
= ±{(α, β + γ n) : (α, β) ∈ △B, 0 ≤ β ≤ ⌊n/2⌋, γ ∈ Zv \ {0, v0 , . . . , (h − 1)v0 }}.

Let C = B∈B AB . It is easy to see that


n
, a ̸= 0, n0 , . . . , (t − 1)n0 , b ∈ Zv \ {0, v0 , . . . , (h − 1)v0 }
ele(C , x) ⊆ a + bn : 0 ≤ a ≤
2

for each x ∈ m0 Zm ,



ele(C , x) ⊆ a + bn : 0 ≤ a ≤

n
2

, b ∈ Zv \ {0, v0 , . . . , (h − 1)v0 }



for each x ∈ Zm \ (m0 Zm ), and

△C =



△AB

B∈B

=



±{(α, β + γ n) : (α, β) ∈ △B, 0 ≤ β ≤ ⌊n/2⌋, γ ∈ Zv \ {0, v0 , . . . , (h − 1)v0 }}

B∈B

= ±{(α, β + γ n) : (α, β) ∈ △B , 0 ≤ β ≤ ⌊n/2⌋, γ ∈ Zv \ {0, v0 , . . . , (h − 1)v0 }}
= ±{(α, β + γ n) : (α, β) ∈ (Zm × Zn ) \ (m0 Zm × n0 Zn ), 0 ≤ β ≤ ⌊n/2⌋, γ ∈ Zv \ (v0 Zv )}.
Suppose that F = {(0, 0) ∈ Fi : i = 1, 2, . . . , r ′ } be the family of all base blocks of the given (−, h) perfect
(m × nh, s × th, k, 1)-SP over Zm × Znh , where ele(F , x) ⊆ {a + bn : 0 ≤ a ≤ ⌊n/2⌋, a ̸= 0, n0 , . . . , (t − 1)n0 , b =
0, 1, . . . , h − 1} for each x ∈ m0 Zm , ele(F , x) ⊆ {a + bn : 0 ≤ a ≤ ⌊n/2⌋, b = 0, 1, . . . , h − 1} for each x ∈ Zm \ (m0 Zm ).
Note that

△F = (Zm × Znh ) \ (m0 Zm × n0 Znh )
= {(α, β + γ n) : (α, β) ∈ (Zm × Zn ) \ (m0 Zm × n0 Zn ), γ ∈ Zh }
= ±{(α, β + γ n) : (α, β) ∈ (Zm × Zn ) \ (m0 Zm × n0 Zn ), 0 ≤ β ≤ ⌊n/2⌋, γ ∈ Zh }.
For any F = {(0, 0), (x1 , a1 + b1 n), . . . , (xk−1 , ak−1 + bk−1 n)} ∈ F , where 0 ≤ a1 ≤ · · · ≤ ak−1 ≤ ⌊n/2⌋ and
bi ∈ {0, 1, . . . , h − 1} for each i, let
F ′ = {(0, 0), (x1 , a1 + b1 v0 n), . . . , (xk−1 , ak−1 + bk−1 v0 n)}.
Then bi v0 ∈ {0, v0 , . . . , (h − 1)v0 } for each i. Let F ′ = {F ′ : F ∈ F }. It is readily checked that

△F ′ = ±{(α, β + γ n) : (α, β) ∈ (Zm × Zn ) \ (m0 Zm × n0 Zn ), 0 ≤ β ≤ ⌊n/2⌋, γ ∈ v0 Zv }.
Then

△(C ∪ F ′ ) = ±{(α, β + γ n) : (α, β) ∈ (Zm × Zn ) \ (m0 Zm × n0 Zn ), 0 ≤ β ≤ ⌊n/2⌋, γ ∈ Zv }
= {(α, β + γ n) : (α, β) ∈ (Zm × Zn ) \ (m0 Zm × n0 Zn ), γ ∈ Zv }.
It is readily checked that each element in (Zm × Znv ) \ (m0 Zm × n0 Znv ) occurs in △(C ∪ F ′ ) exactly once, while any element
in the additive subgroup (m0 Zm ) × (n0 Znv ) is not covered at all. Its (−, v) perfectness can be checked straightforwardly. This
completes the proof. 
Lemma 3.8. If a 2-regular ICDM(4; v) exists, then so do a (3 × 18v, 3 × 2v, 4, 1)-SP and a (3 × 24v, 3 × 4v, 4, 1)-SP.
Proof. It follows immediately from Lemma 3.5, Lemma 3.6 and Construction 3.7.



R. Pan, Y. Chang / Discrete Mathematics 313 (2013) 2918–2931

2927

Let v , k be positive integers. A group divisible design k-GDD is a triple (V , G, B ) satisfying the following properties:
(1)
(2)
(3)
(4)

V is a v -set of points;
G is a partition of V into subsets called groups;
B is a collection of k-subsets of V called blocks, such that a group and a block contain at most one common point;
every pair of points from distinct groups occurs in exactly one block.

The group type of the GDD is the list (|G| : G ∈ G). The usual exponential notation will be used to describe types. Thus, a
GDD of type t1 u1 t2 u2 · · · tl ul is one in which there are ui groups of size ti for each i.
Given positive integers n and u, define Iu = {1, 2, . . . , u} and V = Iu × Zn . The elements of V are denoted by (i, a), where
i ∈ Iu and a ∈ Zn . A k-GDD of type nu based on points set V having group set G = {{i} × Zn : i ∈ Iu } and block set B is said
to be semi-cyclic, if for any B ∈ B , adding 1 ∈ Zn successively to the second coordinate of each point of B ∈ B modulo n
always gives n distinct blocks of B (such a GDD is also called a GD∗ (k, 1, n; nv) in [30]). Assume that B ∗ is the family of all
base blocks of a semi-cyclic k-GDD of type nu . Define the multiset

△ij (B ∗ ) = {a − b(mod n) : (i, a), (j, b) ∈ B, (i, a) ̸= (j, b), B ∈ B ∗ }.
When i = j, △ii (B ∗ ) is the multiset of all pure (i, i)-differences of B ∗ . When i ̸= j, △ij (B ∗ ) is the multiset of all mixed
(i, j)-differences of B ∗ . For any (i, j) ∈ Iu × Iu , it is easy to verify that △ij (B ∗ ) = Zn if i ̸= j or ∅ if i = j. According to
the definition, it is easy to see that an n-regular CP(k, 1; nu) is a semi-cyclic k-GDD of type nu . Now we present an another
recursive construction.
Construction 3.9. Let m, n, and u be positive integers. Suppose that there exist:
(1) an (m × n, s × t , l, 1)-SP;
(2) a semi-cyclic k-GDD of type v l .
Then there exists an (m × nv, s × t v, k, 1)-SP (or (mv × n, sv × t , k, 1)-SP).
Proof. Let m = sm0 and n = tn0 . Suppose that B and F ∗ be, respectively, the family of all base blocks of the given (m × n, s ×
t , l, 1)-SP and semi-cyclic k-GDD on Il × Zv with group set {{x}× Zv : x ∈ Il }. For any B = {(x1 , y1 ), (x2 , y2 ), . . . , (xl , yl )} ∈ B
and F = {(i0 , z0 ), (i1 , z1 ), . . . , (ik−1 , zk−1 )} ∈ F ∗ , construct

AB (F ) = {(xi0 , yi0 + z0 n), (xi1 , yi1 + z1 n), . . . , (xik−1 , yik−1 + zk−1 n)}.
Let AB =



F ∈F ∗

AB (F ). It is readily calculated that

△AB = {(xia − xib , yia − yib + (za − zb )n) : (ia , za ), (ib , zb ) ∈ F ∈ F ∗ , 0 ≤ a, b ≤ k − 1, a ̸= b}
= {(xia − xib , yia − yib + γ n) : ia , ib ∈ Il , ia ̸= ib , γ ∈ △ia ib (F ∗ )}
= {(xia − xib , yia − yib + γ n) : ia , ib ∈ Il , ia ̸= ib , γ ∈ Zv }
= {(α, β + γ n) : (α, β) ∈ △B, γ ∈ Zv }.

Let C = B∈B △AB . Then we have
△C = {(α, β + γ n) : (α, β) ∈ △B , γ ∈ Zv }
= {(α, β + γ n) : (α, β) ∈ (Zm × Zn ) \ (m0 Zm × n0 Zn ), γ ∈ Zv }.
It is readily known that each element in (Zm × Znv ) \ (m0 Zm × n0 Znv ) occurs in △C exactly once, while any element in the
additive subgroup (m0 Zm ) × (n0 Znv ) is not covered at all. Hence, C is the required (m × nv, s × t v, k, 1)-SP. 
Lemma 3.10. There exists an (m × n, s × t , 5, 1)-SP for (m, n, s, t ) ∈ {(1, 81, 1, 1), (1, 162, 1, 2), (3, 48, 3, 8)}.
Proof. From [17, Lemmas 3.4 and 3.7], we have a CB(5, 1; 81) and a 2-regular CP(5, 1; 162), i.e., a (1 × 81, 1 × 1, 5, 1)-SP
and a (1 × 162, 1 × 2, 5, 1)-SP. For (m, n, s, t ) = (3, 48, 3, 8), the base blocks of the desired (3 × 48, 3 × 8, 5, 1)-SP over
Z3 × Z48 are listed as follows:
{(0, 0), (0, 1), (1, 4), (1, 26), (2, 41)}, {(0, 0), (0, 2), (0, 10), (0, 19), (2, 9)},
{(0, 0), (0, 3), (1, 16), (1, 20), (1, 31)}, {(0, 0), (0, 5), (1, 19), (1, 40), (2, 15)},
{(0, 0), (0, 7), (0, 20), (2, 21), (2, 46)}, {(0, 0), (0, 14), (1, 11), (1, 43), (2, 16)}. 
From [30, Example 5.1], there exists a semi-cyclic 4-GDD of type 35 . Applying Construction 3.9 and Lemma 3.10, we are
able to obtain the following result.
Lemma 3.11. There exists a (3 × n, 3 × g , 4, 1)-SP for (g , n) ∈ {(1, 81), (2, 162), (24, 144)}.
Lemma 3.12. There exists a (3 × n, 3 × g , 4, 1)-SP for any positive integers g, n of the following types:
(1) (g , n) = (2t −1 , 3 · 2t ) for any integer t ≥ 3;
(2) (g , n) = (2t , 9 · 2t ) for any integer t ≥ 3;
(3) (g , n) = (3 · 2t , 27 · 2t ) for any integer t ≥ 3.

2928

R. Pan, Y. Chang / Discrete Mathematics 313 (2013) 2918–2931

Proof. We first consider the type (3). By Lemma 3.2, there exists a 2-regular ICDM(4; 3 · 2t −1 ) for any positive integer t ≥ 3.
Applying Lemma 3.8 with v = 3 · 2t −1 where t ≥ 3, we obtain a (3 × 27 · 2t , 3 × 3 · 2t , 4, 1)-SP for any integer t ≥ 3.
For the type (2), there exists a 2-regular ICDM(4; 2t −1 ) for any positive integer t ≥ 4 from Lemma 3.2. Applying
Lemma 3.8 with v = 2t −1 where t ≥ 4, we obtain a (3 × 9 · 2t , 3 × 2t , 4, 1)-SP for any integer t ≥ 4. When t = 3,
there exists a (3 × 72, 3 × 8, 4, 1)-SP which is given by the following base blocks over Z3 × Z72 :
{(0, 0), (0, 1), (0, 4), (0, 61)}, {(0, 0), (0, 2), (0, 8), (1, 66)}, {(0, 0), (0, 5), (0, 24), (1, 17)},
{(0, 0), (0, 7), (1, 59), (2, 19)}, {(0, 0), (0, 10), (0, 38), (2, 33)}, {(0, 0), (0, 13), (1, 33), (1, 55)},
{(0, 0), (0, 14), (1, 62), (2, 51)}, {(0, 0), (0, 16), (1, 30), (2, 38)}, {(0, 0), (0, 17), (0, 43), (1, 28)},
{(0, 0), (0, 20), (1, 22), (1, 71)}, {(0, 0), (0, 21), (1, 25), (2, 49)}, {(0, 0), (0, 25), (1, 26), (1, 68)},
{(0, 0), (0, 31), (1, 41), (2, 56)}, {(0, 0), (0, 32), (1, 29), (2, 66)}, {(0, 0), (0, 33), (1, 7), (2, 2)},
{(0, 0), (0, 35), (1, 3), (2, 16)}.
For the type (1), there exists a 2-regular ICDM(4; 2t −3 ) for any positive integer t ≥ 6 from Lemma 3.2. Applying
Lemma 3.8 with v = 2t −3 where t ≥ 6, we obtain a (3 × 3 · 2t , 3 × 2t −1 , 4, 1)-SP for any integer t ≥ 6. The cases
t = 3, 4 are covered by Lemmas 3.5 and 3.6. When n = 5, there exists a (3 × 96, 3 × 16, 4, 1)-SP which is given by the
following base blocks over Z3 × Z96 :
{(0, 0), (0, 1), (1, 32), (1, 93)}, {(0, 0), (0, 2), (1, 28), (1, 57)}, {(0, 0), (0, 3), (1, 67), (2, 23)},
{(0, 0), (0, 4), (1, 15), (1, 37)}, {(0, 0), (0, 5), (1, 22), (1, 85)}, {(0, 0), (0, 7), (0, 41), (1, 46)},
{(0, 0), (0, 8), (1, 7), (1, 16)}, {(0, 0), (0, 10), (1, 1), (1, 53)}, {(0, 0), (0, 11), (1, 51), (1, 70)},
{(0, 0), (0, 13), (0, 53), (2, 75)}, {(0, 0), (0, 14), (1, 23), (1, 91)}, {(0, 0), (0, 15), (1, 25), (1, 71)},
{(0, 0), (0, 16), (1, 20), (1, 79)}, {(0, 0), (0, 17), (1, 3), (2, 38)}, {(0, 0), (0, 20), (1, 49), (1, 81)},
{(0, 0), (0, 21), (1, 14), (1, 83)}, {(0, 0), (0, 23), (1, 50), (1, 88)}, {(0, 0), (0, 25), (1, 38), (1, 69)},
{(0, 0), (0, 26), (1, 45), (1, 94)}, {(0, 0), (0, 39), (1, 41), (1, 86)}.
This completes the proof. 
4. Some new maximum OOSPCs
In this section we construct a maximum (m, n, 4, 1)-OOSPC (i.e. (m × n, 4, 1)-SP with (mn − 12)/12 base blocks) for
any positive integers m, n such that gcd(m, 18) = 3 and n ≡ 0 (mod 12) step by step. We first need to quote some known
results on OOCs as follows. For the formal definition of g-regular OOC, the reader can refer to [7]. A g-regular (gu, k, 1)-OOC
is actually equivalent to a (g1 u1 × g2 u2 , g1 × g2 , k, 1)-SP where g = g1 g2 , u = u1 u2 and gcd(g1 u1 , g2 u2 ) = 1.
Lemma 4.1 ([5, Lemma 6.4]). There exists a g-regular (3 · 2a , 4, 1)-OOC for any integer a ≥ 3 where g = 24, or 48, or 96.
Lemma 4.2 ([5, Theorem 6.12]). There exists a (v, 4, 1)-OOC with (v − 12)/12 codewords for any positive integer v ≡
0 (mod 24).
To enlarge the results on maximum (m × n, 4, 1)-SPs, we need to construct some examples with small orders.
Lemma 4.3. There exists a (3 × n, 3 × g , 4, 1)-SP for each (g , n) ∈ {(8, 40), (8, 72), (8, 80), (16, 96), (12, 108), (20, 120),
(20, 160), (20, 180), (24, 216), (8, 288), (36, 324), (8, 432), (8, 648), (16, 864), (12, 972)}.
Proof. When (g , n) = (8, 40), (8, 80), (20, 160), the required (3 × n, 3 × g , 4, 1)-SP (i.e., 3g-regular (3n, 4, 1)-OOC) exists
by [17, Lemma 3.9], [5, Lemma 4.5], [5, Theorem 6.10].
When (g , n) = (8, 72), (16, 96), (24, 216), the conclusions follow from Lemma 3.12.
When (g , n) = (12, 108), 24 base blocks of a (3 × 108, 3 × 12, 4, 1)-SP are listed below over Z3 × Z108 :
{(0, 0), (0, 1), (1, 8), (1, 96)}, {(0, 0), (0, 2), (1, 25), (1, 102)}, {(0, 0), (0, 3), (1, 42), (1, 79)},
{(0, 0), (0, 4), (1, 47), (2, 28)}, {(0, 0), (0, 5), (1, 37), (2, 42)}, {(0, 0), (0, 6), (1, 21), (1, 44)},
{(0, 0), (0, 7), (1, 3), (1, 62)}, {(0, 0), (0, 8), (1, 10), (1, 49)}, {(0, 0), (0, 10), (1, 58), (1, 69)},
{(0, 0), (0, 12), (1, 26), (2, 1)}, {(0, 0), (0, 13), (0, 55), (2, 51)}, {(0, 0), (0, 14), (0, 47), (1, 82)},
{(0, 0), (0, 15), (0, 58), (2, 89)}, {(0, 0), (0, 16), (0, 86), (2, 30)}, {(0, 0), (0, 19), (1, 65), (1, 106)},
{(0, 0), (0, 21), (1, 22), (1, 73)}, {(0, 0), (0, 24), (0, 52), (2, 68)}, {(0, 0), (0, 25), (1, 31), (2, 47)},
{(0, 0), (0, 26), (0, 60), (1, 93)}, {(0, 0), (0, 29), (0, 91), (1, 103)}, {(0, 0), (0, 30), (1, 13), (2, 88)},
{(0, 0), (0, 32), (1, 29), (2, 80)}, {(0, 0), (0, 35), (1, 24), (1, 88)}, {(0, 0), (0, 40), (1, 17), (2, 10)}.
When (g , n) = (20, 120), (20, 180), start from a (3 × 24, 3 × 4, 4, 1)-SP and a (3 × 36, 3 × 4, 4, 1)-SP, which exist by
Lemmas 3.5 and 3.6. Apply Construction 3.3 with v = 5 to obtain the required (3 × n, 3 × g , 4, 1)-SP, where the needed
CDM(4; 5) is from Lemma 3.1.
When (g , n) = (8, 288), apply Lemma 3.8 with v = 12 and Lemma 1.5 to obtain the required (3 × 288, 3 × 8, 4, 1)-SP,
where the needed 2-regular ICDM(4; 12) is from Lemma 3.2, the needed (3 × 48, 3 × 8, 4, 1)-SP is from Lemma 3.6.
When (g , n) = (36, 324), apply Lemma 3.8 with v = 18 to obtain the required (3 × 324, 3 × 36, 4, 1)-SP, where the
needed 2-regular ICDM(4; 18) is from [7, Lemma 3.4].
When (g , n) = (8, 432), (16, 864), start from a (3 × 27 · 2t , 3 × 3 · 2t , 4, 1)-SP and a (3 × 3 · 2t , 3 × 2t −1 , 4, 1)-SP for
t = 4, 5, which exist from types (3) and (1) of Lemma 3.12. Apply Lemma 1.5 to obtain the required (3 × n, 3 × g , 4, 1)-SP.

R. Pan, Y. Chang / Discrete Mathematics 313 (2013) 2918–2931

2929

When (g , n) = (8, 648), apply Construction 3.4 with v = 8 to obtain the required (3 × 648, 3 × 8, 4, 1)-SP, where the
needed 2-regular ICDM(4; 8) is from Lemma 3.2, and the needed (3 × 81, 3 × 1, 4, 1)-SP and (3 × 162, 3 × 2, 4, 1)-SP are
from Lemma 3.11.
When (g , n) = (12, 972), apply Lemma 3.8 with v = 54 and Lemma 1.5 to obtain the required (3 × 972, 3 × 12, 4, 1)-SP,
where the needed 2-regular ICDM(4; 54) from [7, Lemma 3.4], and the needed (3 × 108, 3 × 12, 4, 1)-SP was constructed
above. 
Lemma 4.4. There exists an (m × n, 4, 1)-SP with (mn − 12)/12 base blocks, which is maximum, for each (m, n) ∈
{(3, 12), (3, 24), (3, 36), (3, 60), (15, 20), (15, 24), (15, 36)}.
Proof. When (m, n) = (3, 12), a (3 × 12, 4, 1)-SP with 2 base blocks is constructed over Z3 × Z12 as follows:
{(0, 0), (0, 1), (0, 3), (1, 0)}, {(0, 0), (0, 4), (1, 2), (1, 7)}.
When (m, n) = (3, 24) (3, 36), the conclusion follows by Lemmas 3.5, 3.6 and 1.4.
When (m, n) = (3, 60), start from a (3 × 60, 3 × 12, 4, 1)-SP with 12 base blocks, which exists from Lemma 2.6. Apply
Lemma 1.5 to obtain the required (3 × 60, 4, 1)-SP with 14 base blocks, where the needed (3 × 12, 4, 1)-SP with 2 base
blocks was constructed above.
When (m, n) = (15, 20), a (15 × 20, 4, 1)-SP with 24 base blocks is constructed over Z15 × Z20 as follows:
{(0, 0), (0, 1), (2, 5), (9, 10)}, {(0, 0), (0, 2), (2, 14), (6, 1)}, {(0, 0), (0, 3), (0, 8), (3, 9)},
{(0, 0), (0, 4), (4, 18), (9, 18)}, {(0, 0), (0, 6), (1, 11), (3, 11)}, {(0, 0), (0, 7), (4, 17), (7, 17)},
{(0, 0), (0, 9), (2, 7), (6, 3)}, {(0, 0), (1, 0), (2, 6), (10, 15)}, {(0, 0), (1, 1), (3, 4), (8, 0)},
{(0, 0), (1, 2), (5, 8), (8, 6)}, {(0, 0), (1, 3), (3, 2), (8, 12)}, {(0, 0), (1, 4), (5, 13), (9, 8)},
{(0, 0), (1, 7), (3, 17), (6, 4)}, {(0, 0), (1, 8), (6, 7), (7, 6)}, {(0, 0), (1, 9), (2, 1), (6, 0)},
{(0, 0), (1, 10), (3, 19), (5, 1)}, {(0, 0), (1, 13), (4, 8), (10, 16)}, {(0, 0), (1, 14), (5, 7), (11, 0)},
{(0, 0), (1, 15), (4, 5), (8, 8)}, {(0, 0), (1, 16), (3, 13), (8, 19)}, {(0, 0), (1, 17), (3, 8), (13, 5)},
{(0, 0), (1, 18), (4, 1), (8, 13)}, {(0, 0), (2, 8), (4, 4), (12, 6)}, {(0, 0), (2, 13), (5, 9), (11, 18)}.
When (m, n) = (15, 24), (15, 36), note that a (15 × 24, 15 × 4, 4, 1)-SP and a (15 × 36, 15 × 4, 4, 1)-SP are, respectively,
equivalent to a (3 × 120, 3 × 20, 4, 1)-SP and a (3 × 180, 3 × 20, 4, 1)-SP which exist from Lemma 4.3. Then apply Lemma 1.5
to obtain the required (m × n, 4, 1)-SP with (mn − 12)/12 base blocks where the needed (15 × 4, 4, 1)-SP with 4 base blocks
(i.e., (60, 4, 1)-OOC with 4 codewords) is from [27, Theorem 4.2]. 
Lemma 4.5. There exists a (3 × n, 3 × g , 4, 1)-SP for g = 8, or 16, or 32 and any positive integer n of the following types:
(1) n = 3 · 2a+1 for any integer a ≥ 3;
(2) n = 9 · 2a for any integer a ≥ 3;
(3) n = 27 · 2a+1 for any integer a ≥ 3.
Proof. Take a ≥ 3 be a integer and g = 8, or 16, or 32. Note that a (3 × 2a , 3 × g , 4, 1)-SP (i.e. 3g-regular (3 · 2a , 4, 1)-OOC)
exists from Lemma 4.1 since gcd(3, 2a ) = 1. The conclusion then follows immediately by Lemma 1.5, where the needed
(3 × 3 · 2a+1 , 3 × 2a , 4, 1)-SP, (3 × 9 · 2a , 3 × 2a , 4, 1)-SP and (3 × 27 · 2a+1 , 3 × 3 · 2a+1 , 4, 1)-SP are from Lemma 3.12. 
Lemma 4.6. There exists a (3 × 4 · 3b , 3 × g , 4, 1)-SP for any positive integer b ≥ 1, where g = 12 or 36.
Proof. Using induction on b. It is obvious when b = 1, 2. The cases b = 3, 4, 5 are covered by Lemma 4.3. When b ≥ 5,
suppose that there exists a (3 × 4 · 3t , 3 × g , 4, 1)-SP for any positive integer t ≤ b, where g = 12 or 36. Then there exist
both a (3 × 4 · 3b−2 , 3 × g , 4, 1)-SP where g = 12 or 36 by the assumption and a CDM(4; 27) from [7, Lemma 3.7]. Applying
Construction 3.3 with v = 27, we obtain a (3 × 4 · 3b+1 , 3 × 27g , 4, 1)-SP where g = 12 or 36. Apply Lemma 1.5 with
Lemma 4.3 to obtain the required (3 × 4 · 3b+1 , 3 × g , 4, 1)-SP, where g = 12 or 36. 
Lemma 4.7. There exists a (3 × 2a 3b , 3 × g , 4, 1)-SP for any positive integers a ∈ {3, 4, 5} and b ≥ 1, where g = 8, or 16,
or 24.
Proof. Using induction on b. It is obvious when (a, b) = (3, 1). For each a ∈ {3, 4, 5}, the cases b = 1, 2, 3, 4 are covered
by Lemmas 3.6, 3.11 and 4.3 with the exception of (a, b) = (3, 1), (4, 4), (5, 4). When a = 3 and b ≥ 4, or a ∈ {4, 5} and
b ≥ 3, the proof is similar to that of Lemma 4.6. 
Lemma 4.8. There exists a (3 × g v, 3 × g ′ , 4, 1)-SP for each g ∈ {8, 12, 16, 24, 32, 36} and any positive integer v such that
gcd(v, 6) = 1, where g ′ = g, or 8, or 20.
Proof. When g = 8, there exists a 24-regular (24v, 4, 1)-OOC from [17, Lemma 5.1], which is equivalent to a (3 × 8v, 3 ×
8, 4, 1)-SP since gcd(3, 8v) = 1.
α α
When g = 12, let v = p1 1 p2 2 · · · pαr r be the factorization of v where each pi ≥ 5 be prime and each integer αi ≥ 1. For
each prime pi , there exist a CDM(4; pi ) from Lemma 3.1 and a (3 × 12pi , 3 × 12, 4, 1)-SP from Lemmas 2.6 and 2.7. Start from
a (3 × 12p1 , 3 × 12, 4, 1)-SP. Applying Construction 3.3 with a CDM(4; pi ), we can obtain a (3 × 12p1 pi , 3 × 12pi , 4, 1)-SP.

2930

R. Pan, Y. Chang / Discrete Mathematics 313 (2013) 2918–2931

Then apply Lemma 1.5 with a (3 × 12pi , 3 × 12, 4, 1)-SP to obtain a (3 × 12p1 pi , 3 × 12, 4, 1)-SP. Repeat the process to
obtain the required (3 × 12v, 3 × 12, 4, 1)-SP.
When g ∈ {16, 24, 32, 36}, let v = 5δ v ′ where δ ∈ {0, 1} and gcd(v ′ , 150) = 1 or 25. If δ = 0, there exist a
(3 × 16v, 3 × 8, 4, 1)-SP or (3 × 16v, 3 × 16, 4, 1)-SP (i.e. 24- or 48-regular (48v, 4, 1)-OOC) from [5, Lemma 6.9] and
[17, Lemma 3.9], a (3 × 32v, 3 × 32, 4, 1)-SP (i.e. 96-regular (96v, 4, 1)-OOC) from [5, Lemma 3.9], a (3 × g v, 3 × g , 4, 1)-SP
for g = 24, 36 from Lemmas 2.2 and 2.3. Then the conclusion follows. If δ = 1, start from a (3 × g v ′ , 3 × r , 4, 1)-SP for each
g ∈ {16, 24, 32, 36}, which exists from the case δ = 0, where r = g or 8. Apply Construction 3.3 with a CDM(4; 5) to obtain
a (3 × 5g v ′ , 3 × 5r , 4, 1)-SP (i.e. (3 × g v, 3 × 5r , 4, 1)-SP) where the needed CDM(4; 5) is from Lemma 3.1. Then apply
Lemma 1.5 to obtain the required (3 × g v, 3 × g ′ , 4, 1)-SP where g ′ = 8 or 20, and the needed (3 × 5r , 3 × g ′ , 4, 1)-SP for
each r ∈ {8, 16, 24, 32, 36} is from Lemma 4.3. 
Lemma 4.9. There exists a (3u × g , 4, 1)-SP with (gu − 4)/4 base blocks for each g ∈ {8, 12, 16, 20, 24, 32, 36} and any
positive integer u such that gcd(u, 6) = 1.
Proof. When g ∈ {8, 16, 32}, we have 3gu ≡ 0 (mod 24) and gcd(3u, g ) = 1. The conclusion follows by Lemma 4.2.
When g = 12, note that a (3u × 12, 3 × 12, 4, 1)-SP, which is equivalent to a (3 × 12u, 3 × 12, 4, 1)-SP since gcd(u, 6) = 1,
exists from Lemma 4.8. Then apply Lemma 1.5 to obtain the required (3u × 12, 4, 1)-SP with 3u − 1 base blocks, where the
needed (3 × 12, 4, 1)-SP with 2 base blocks is from Lemma 4.4.
When g ∈ {20, 24, 36}, let u = 5δ u′ where δ ∈ {0, 1} and gcd(u′ , 150) = 1 or 25. If δ = 0, there exist a 60-regular
(60u, 4, 1)-OOC over Zu × Z60 from [5, Lemma 3.6], which is equivalent to a (3u × 20, 3 × 20, 4, 1)-SP over Z3u × Z20 since
gcd(3, 20) = 1 and gcd(3, u) = 1, and a (3u × g , 3 × g , 4, 1)-SP (i.e., (3 × gu, 3 × g , 4, 1)-SP) for g = 24, 36 from
Lemmas 2.2 and 2.3. Then apply Lemma 1.5 to obtain the required (3u × g , 4, 1)-SP with (gu − 4)/4 base blocks for each
g ∈ {20, 24, 36}, where the needed (3 × 20, 4, 1)-SP with 4 base blocks (i.e., (60, 4, 1)-OOC with 4 codewords) is from
[27, Theorem 4.2], and the needed (3 × g , 4, 1)-SP with (g − 4)/4 base blocks for g = 24, 36 are from Lemma 4.4. If δ = 1,
start from a (3u′ × g , 3 × g , 4, 1)-SP for each g ∈ {20, 24, 36}, which exists from the case δ = 0. Apply Construction 3.3
with a CDM(4; 5) to obtain a (15u′ × g , 15 × g , 4, 1)-SP (i.e., a (3u × g , 15 × g , 4, 1)-SP), where the needed CDM(4; 5) is
from Lemma 3.1. Then apply Lemma 1.5 to obtain the required (3u × g , 4, 1)-SP with (gu − 4)/4 base blocks, where the
needed (15 × g , 4, 1)-SP with (5g − 4)/4 base blocks for each g ∈ {20, 24, 36} is from Lemma 4.4. 
Lemma 4.10. There exists a (3u × g v, 4, 1)-SP with (guv − 4)/4 base blocks for each g ∈ {8, 12, 16, 24, 32, 36} and any
positive integers u, v such that gcd(u, 6) = 1 and gcd(v, 6) = 1.
Proof. Take g ∈ {8, 12, 16, 24, 32, 36}, start from a (3 × g v, 3 × g ′ , 4, 1)-SP, which exists from Lemma 4.8, where g ′ = g, or
8, or 20. Applying Construction 3.3 with a CDM(4; u), we obtain a (3u × g v, 3u × g ′ , 4, 1)-SP, where the needed CDM(4; u)
is from Lemma 3.1 since gcd(u, 6) = 1. Then apply Lemma 1.5 to obtain the required (3u × g v, 4, 1)-SP with (guv − 4)/4
base blocks, where the needed (3u × g ′ , 4, 1)-SP with (g ′ u − 4)/4 base blocks for each g ′ ∈ {8, 12, 16, 20, 24, 32, 36} is
from Lemma 4.9. 
Lemma 4.11. There exists a (3u × 2a 3b v, 4, 1)-SP with 2a−2 3b uv − 1 base blocks, which is maximum, for any positive integers
a ∈ {2, 3, 4, 5}, b ≥ 1 and u, v such that gcd(u, 6) = 1 and gcd(v, 6) = 1.
Proof. For any positive integer b ≥ 1, start from a (3 × 2a 3b , 3 × g , 4, 1)-SP where a = 2 and g = 12 or 36, or
a ∈ {3, 4, 5} and g = 8, or 16, or 24, which exists from Lemmas 4.6 and 4.7. Apply Construction 3.3 twice to obtain a
(3u × 2a 3b v, 3u × g v, 4, 1)-SP where the needed CDM(4; u) and CDM(4; v) are from Lemma 3.1 since gcd(u, 6) = 1 and
gcd(v, 6) = 1. Then applying Lemma 1.5, we obtain the required (3u × 2a 3b v, 4, 1)-SP with 2a−2 3b uv − 1 base blocks, where
the needed (3u × g v, 4, 1)-SP with (guv − 4)/4 base blocks for each g ∈ {8, 12, 16, 24, 36} is from Lemma 4.10. 
Lemma 4.12. There exists a (3u × 2a 3b v, 4, 1)-SP with 2a−2 3b uv − 1 base blocks, which is maximum, for any integers
a ≥ 6, b ≥ 1, and u, v such that gcd(u, 6) = 1 and gcd(v, 6) = 1.
′

Proof. For any integers a ≥ 6, b′ ∈ {1, 2, 3}, start from a (3 × 2a 3b , 3 × g , 4, 1)-SP where g = 8, or 16 or 32,
which exists from Lemma 4.5. For any integer b ≥ 1, it can be written as b = b′ + 3i where integer i ≥ 0. Apply
′
Construction 3.3 with a CDM(4; 27) i times and with a CDM(4; v) just once to obtain a (3 × 2a 3b +3i v, 3 × 33i g v, 4, 1)a b
3i
SP (i.e. a (3 × 2 3 v, 3 × 3 g v, 4, 1)-SP), where g = 8, or 16, or 32, the needed CDM(4; 27) is from [7, Lemma 3.7], and the
needed CDM(4; v) is from Lemma 3.1. For the remaining process, we divide it into two cases to consider.
Case (1) When i = 0, there exists a (3 × g v, 3 × r , 4, 1)-SP for each g ∈ {8, 16, 32} from Lemma 4.8, where r = g,
or 8, or 20. Start from the (3 × 2a 3b v, 3 × g v, 4, 1)-SP, where g = 8, or 16 or 32. Applying Lemma 1.5, we obtain
a (3 × 2a 3b v, 3 × r , 4, 1)-SP where r = 8, or 16, or 20, or 32. Apply Construction 3.3 with a CDM(4; u) to obtain a
(3u × 2a 3b v, 3u × r , 4, 1)-SP, where the needed CDM(4; u) is from Lemma 3.1. Finally, apply Lemma 1.5 to obtain the
required (3u × 2a 3b v, 4, 1)-SP with 2a−2 3b uv − 1 base blocks, where the needed (3u × r , 4, 1)-SP with (ru − 4)/4 base
blocks for each r ∈ {8, 16, 20, 32} is from Lemma 4.9.

R. Pan, Y. Chang / Discrete Mathematics 313 (2013) 2918–2931

2931

Case (2) When i ≥ 1, start from the (3 × 2a 3b v, 3 × 33i g v, 4, 1)-SP, where g = 8, or 16, or 32. Applying Construction 3.3
with a CDM(4; u), we obtain a (3u × 2a 3b v, 3u × 33i g v, 4, 1)-SP. Then apply Lemma 1.5 to obtain the required (3u ×
2a 3b v, 4, 1)-SP with 2a−2 3b uv − 1 base blocks, where the needed (3u × 33i g v, 4, 1)-SP with (33i guv − 4)/4 base blocks for
each integers i ≥ 1 and g ∈ {8, 16, 32} is from Lemma 4.11.
This completes the proof. 
Now, we are able to state our main result of this paper.
Theorem 4.13. There exists an (m, n, 4, 1)-OOSPC with (mn − 12)/12 base blocks, which is maximum, for any positive integers
m, n such that gcd(m, 18) = 3 and n ≡ 0 (mod 12).
Proof. For any positive integers m, n, which satisfy gcd(m, 18) = 3 and n ≡ 0 (mod 12), can be written as m = 3u and
n = 2a 3b v where a ≥ 2, b ≥ 1, gcd(u, 6) = 1 and gcd(v, 6) = 1. The conclusion follows immediately by Lemma 1.3 with
Lemmas 4.11 and 4.12. 
From Theorem 4.13, we have Θ (m, n, 4, 1) = (mn − 12)/12 for any positive integers m, n such that gcd(m, 18) = 3 and
n ≡ 0 (mod 12). Hence, Theorem 4.13 can be restated as follows.
Theorem 4.14. Θ (m, n, 4, 1) = (mn − 12)/12 for any positive integers m, n such that gcd(m, 18) = 3 and n ≡ 0 (mod 12).
Acknowledgments
The authors would like to thank the anonymous referees for their helpful comments and suggestions.
References
[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]
[10]
[11]
[12]
[13]
[14]
[15]
[16]
[17]
[18]
[19]
[20]
[21]
[22]
[23]
[24]
[25]
[26]
[27]
[28]
[29]
[30]

M. Buratti, Cyclic designs with block size 4 and related optimal optical orthogonal codes, Des. Codes Cryptogr. 26 (2002) 111–125.
M. Buratti, K. Momihara, A. Pasotti, New results on optimal (v, 4, 2, 1) optical orthogonal codes, Des. Codes Cryptogr. 58 (2011) 89–109.
M. Buratti, A. Pasotti, Combinatorial designs and the theorem of Weil on multiplicative character sums, Finite Fields Appl. 15 (2009) 332–344.
M. Buratti, A. Pasotti, D. Wu, On optimal (v, 5, 2, 1) optical orthogonal codes, Des. Codes Cryptogr. 68 (2013) 349–371.
Y. Chang, R. Fuji-Hara, Y. Miao, Combinatorial constructions of optimal optical orthogonal codes with weight 4, IEEE Trans. Inform. Theory 49 (5)
(2003) 1283–1292.
Y. Chang, L. Ji, Optimal (4up, 5, 1) optical orthogonal codes, J. Combin. Des. 12 (5) (2004) 346–361.
Y. Chang, Y. Miao, Constructions for optimal optical orthogonal codes, Discrete Math. 261 (2003) 127–139.
Y. Chang, J. Yin, Further results on optimal optical orthogonal codes with weight 4, Discrete Math. 279 (2004) 135–151.
K. Chen, G. Ge, L. Zhu, Starters and related codes, J. Statist. Plann. Inference 86 (2000) 379–395.
F.R.K. Chung, J.A. Salehi, V.K. Wei, Optical orthogonal codes: design, analysis, and applications, IEEE Trans. Inform. Theory 35 (3) (1989) 595–604.
C.J. Colbourn, Difference matrices, in: C.J. Colbourn, J.H. Dinitz (Eds.), CRC Handbook of Combinatorial Designs, CRC Press, Boca Raton, 2007,
pp. 411–419.
T. Feng, Y. Chang, L. Ji, Constructions for rotatinal Steiner quadruple systems, J. Combin. Des. 17 (5) (2009) 353–368.
T. Feng, Y. Chang, L. Ji, Constructions for strictly cyclic 3-designs and applications to optimal OOCs with λ = 2, J. Combin. Theory Ser. A 115 (2008)
1527–1551.
R. Fuji-Hara, Y. Miao, Optical orthogonal codes: their bounds and new optimal constructions, IEEE Trans. Inform. Theory 46 (7) (2000) 2396–2406.
R. Fuji-Hara, Y. Miao, J. Yin, Optimal (9v, 4, 1) optical orthogonal codes, SIAM J. Discrete Math. 14 (2001) 256–266.
G. Ge, Y. Miao, X. Sun, Perfect difference families, perfect difference matrices, and related combinatorial structures, J. Combin. Des. 18 (2010) 415–449.
G. Ge, J. Yin, Constructions for optimal (v, 4, 1) optical orthogonal codes, IEEE Trans. Inform. Theory 47 (7) (2001) 2998–3004.
S.M. Johnson, A new upper bound for error-correcting codes, IEEE Trans. Inform. Theory 8 (1962) 203–207.
K. Kitayama, Novel spatial spread spectrum based fiber optic CDMA networks for image transmission, IEEE J. Sel. Areas Commun. 12 (4) (1994)
762–772.
W.C. Kwong, G.C. Yang, Image transmission in multicore-fiber code-division multiple-access networks, IEEE Commun. Lett. 2 (10) (1998) 285–287.
W.C. Kwong, G.C. Yang, Double-weight signature pattern codes for multicore-fiber code-division multiple-access networks, IEEE Commun. Lett. 5 (5)
(2001) 203–205.
S. Ma, Y. Chang, A new class of optical orthogonal codes with weight five, IEEE Trans. Inform. Theory 50 (8) (2004) 1848–1850.
S. Ma, Y. Chang, Constructions of optimal optical orthogonal codes with weight five, J. Combin. Des. 13 (1) (2005) 54–69.
K. Momihara, M. Buratti, Bounds and constructions of optimal (n, 4, 2, 1) optical orthogonal codes, IEEE Trans. Inform. Theory 55 (2009) 514–523.
M. Sawa, Optical orthogonal signature pattern codes with maximum collision parameter 2 and weight 4, IEEE Trans. Inform. Theory 56 (7) (2010)
3613–3620.
M. Sawa, S. Kageyama, Optimal optical orthogonal signature pattern codes of weight 3, Bio. Lett. 46 (2) (2009) 89–102.
X. Wang, Y. Chang, Further results on (v, 4, 1)-perfect difference families, Discrete Math. 310 (2010) 1995–2006.
X. Wang, Y. Chang, Further results on optimal (v, 4, 2, 1)-OOCs, Discrete Math. 312 (2) (2012) 331–340.
G.C. Yang, W.C. Kwong, Two-dimensional spatial signature patterns, IEEE Trans. Commun. 44 (2) (1996) 184–191.
J. Yin, Some combinatorial constructions for optical orthogonal codes, Discrete Math. 185 (1998) 201–219.

