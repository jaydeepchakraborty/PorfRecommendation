A System Architecture for Affective Meta Intelligent
Tutoring Systems
Javier Gonzalez-Sanchez1, Maria Elena Chavez-Echeagaray1, Kurt VanLehn1,
Winslow Burleson1, Sylvie Girard2, Yoalli Hidalgo-Pontet1, and Lishan Zhang1
1

Arizona State University, Tempe, AZ, USA
{javiergs,mchaveze,kurt.vanlehn,winslow.burleson,
yoalli.hidalgopontet,lishan.zhang}@asu.edu
2
University of Birmingham, Birmingham, UK
s.a.girard@bham.ac.uk

Abstract. Intelligent Tutoring Systems (ITSs) constitute an alternative to expert
human tutors, providing direct customized instruction and feedback to students.
ITSs could positively impact education if adopted on a large scale, but doing
that requires tools to enable their mass production. This circumstance is the key
motivation for this work. We present a component-based approach for a system
architecture for ITSs equipped with meta-tutoring and affective capabilities. We
elicited the requirements that those systems might address and created a system
architecture that models their structure and behavior to drive development
efforts. Our experience applying the architecture in the incremental
implementation of a four-year project is discussed.
Keywords: architecture, component-based, tutoring, meta-tutoring, affect.

1

Introduction

Intelligent Tutoring Systems (ITSs) seem capable of becoming untiring and
economical alternatives to expert human tutors. This possibility has proven difficult to
achieve, but significant progress has been made. The use of ITSs has become more
common, and there is significant work about their pedagogical and instructional
design but not about their technological implementation. ITSs are software products
and, as for any other software product, their implementation on a massive scale relies
on the principle of assembly instead of crafting them as one-of-a-kind systems.
Component-based software engineering [1] is an appropriate approach for handling
mass production. Component-based software engineering addresses the development
of systems as an assembly of parts (components), with the development of these parts
as reusable entities and with the maintenance and upgrading of systems through
customizing and replacing such parts.
Following a component-based approach, we have defined a system architecture to
drive the development of ITSs equipped with affective and meta-tutoring capabilities,
called affective meta intelligent tutoring systems (AMTs). Defining a system architecture
S. Trausan-Matu et al. (Eds.): ITS 2014, LNCS 8474, pp. 529–534, 2014.
© Springer International Publishing Switzerland 2014

530

J. Gonzalez-Sanchez et al.

is the first step in creating a component-based software framework to implement AMTlike applications. This system architecture takes advantage of previous experiences with
ITS implementations; most of that previous experience was extracted from the analysis
made on existing ITS behavior described in [2], as well as from previous experience in
the development of real-time affective companions, mainly by the work described in [3].
This paper is organized as follows: Section 2 provides the terminology and
background for system architectures, ITS behavior, and affect recognition; Section 3
describes the AMT system architecture; Section 4 describes the implementation of an
application following the AMT system architecture and discusses its software metrics;
and Section 5 provides a conclusion.

2

Terminology and Background

The following terminology and background summary contextualizes the work
described in this paper:
System Architecture. A system is a group of interacting, interrelated or independent
modules forming a complex whole. Modules are self-contained entities that carry out
a specific function; they are implemented as a set of parts called components. A
system architecture is a conceptual model that describes the modules and components
of a system and how they interconnect with each other; it becomes a software design
model by mapping each component to a set of classes following software engineering
methodologies. The system architecture is essential for realizing the system's quality
attributes [4].
ITS Behavioral Description. ITSs are typically used to assign tasks to students; tasks
are composed of steps that the student must accomplish. The structure of this kind of
ITSs, called step-based, is described in [2] and can be summarized as follows: (1) the
group of tasks known by the ITS conforms its Knowledge Base; (2) a Task Selector
chooses from the Knowledge Base the Task that the student must solve by considering
the student’s previous performance reported by an Assessor; (3) a User Interface (a
tool or an environment) provides the space in which the interaction between the tutor
and the student occurs; (4) a Step Analyzer methodically examines the student’s steps
and determines whether they are correct or incorrect and then reports that information
to a Pedagogical Module and to an Assessor; (5) a Pedagogical Module provides
support (hints and feedback); the provided support depends on current steps and the
student’s previous performance; and (6) an Assessor measures the performance of the
student (requested hints, time used to go from one step to another, etc.).
Affect Recognition Strategies. Research shows that learning is enhanced when
affective support is present [5]. To provide that support, ITSs need to recognize
students’ affect. Diverse strategies exist for affect recognition; the one we are
considering for this work uses sensing devices to read students’ physiological responses;
this strategy uses, among others, brain-computer interfaces, eye-tracking systems, facebased emotion recognition systems, and diverse sensors to measure skin conductance
(arousal), posture, and finger pressure [6].

A System Architecture for Affective Meta Intelligent Tutoring Systems

3

531

System Architecture

The system architecture was engineered [7] on the principles of encapsulation, low
coupling, centralized shared data, and layering. Functionality is encapsulated in
simple components; components that are complex and/or serve diverse purposes are
split into several collaborative components. Components are low-coupled to facilitate
replacement, i.e., to increase modifiability. A centralized data-sharing mechanism is
used to pass data among modules to reduce latency. Components are organized in a
three-layer structure in which the bottom layer encodes utility services for data
management and communication responsibilities; the middle layer encodes the
business logic; and the topmost layer encloses the user interface, which handles the
interaction with the user. Since the user interface is particular to a specific system, it
is not described here. Fig. 1 shows modules (boxes), components (gray boxes), and
their relationships (arrows) as follows:
Tutor Module. It encapsulates the ITS behavior. Its components and relationships are
summarized in Section 2.
Meta Tutor Module. It encapsulates the logic for providing meta-tutoring
recommendations and promoting meta-skills in the student. The Meta Tutor module
has two components: (1) an Inspector that reads Tutor events (populated in the Shared
Repository) and filters those that suggest an intervention is needed; and (2) an Engine
that provides intelligence to the Meta Tutor; the Engine is notified by the Inspector of
compelling events and it infers the type of intervention that must be done.
Interventions consist of showing a message or disabling channels of user interaction.
The Engine implements the policies about how and when interventions must be done.
It communicates the interventions to the User Interface for its execution.
Affective Companion Module. It encapsulates the logic for generating affective
interventions. The Affective Companion has two components: (1) an Event Selector
reads the data for Tutor events and affective states (populated in the Shared
Repository) and filters combinations that suggest an intervention is needed; and (2) an
Affective Engine that implements the affective intelligence; the Affective Engine is
notified by the Event Selector of compelling combinations of Tutor events and
affective state data and infers the type of intervention that must be done. Interventions
consist of motivational messages. The Affective Engine implements the policies about
how and when interventions must be done. It communicates the interventions to the
User Interface for its execution.
Shared Repository. It is a centralized means for passing data among the other
modules, which are running concurrently. The Shared Repository module follows a
blackboard architectural model, in which a common data repository, “the blackboard,”
is updated by some modules and read by others. The Tutor posts events to the
blackboard and the Emotion Recognition Subsystem posts affective state reports.
The Meta Tutor and Affective Companion observe the blackboard, looking for data that
triggers an action on their side.

532

J. Gonzalez-Sanchez et al.

Emotion Recognition Subsystem. It is a facade that provides a simplified interface
to a source of affective state data, such as a third-party system, framework, or library.
The system architecture prioritizes the quality attributes of modifiability, extensibility,
and integrability. Modifiability refers to the ease with which a component can be
modified for use in applications or environments other than those for which it was
specifically designed; affective and cognitive intelligence require this quality since they
are implemented in different ways. Extensibility refers to being prepared for extension
into unforeseen contexts since not all application requirements can be determined in
advance; our system architecture required this quality to make feasible the addition of
new tutoring, meta-tutoring, or affective support capabilities. Integrability is the process
of combining software subsystems to assemble an overall system; AMT system
architecture requires the integration of a third-party system or code (1) for affect
recognition to support the functionality of the Affective Companion module and (2)
for decision-making (machine-learning algorithm implementation) to support the
functionality of the Affective Companion and Meta Tutor modules.

Fig. 1. AMT System Architecture

4

Usage and Discussion

The AMT system architecture has been used as a reference during a four-year project
focused on developing an AMT application [8]. The AMT application was
implemented in Java with Swing components. The final version is composed of 16
packages, 120 classes, 1507 methods, 1810 attributes, and 37,374 lines of code. A
production-rule system and a third-party implementation of emotion recognition
algorithms were used to support the application development. A detailed description
of moving from AMT system architecture to software design is outside the scope of

A System Architecture for Affective Meta Intelligent Tutoring Systems

533

this paper due to space limitations; nevertheless, a description of mapping the ITS
module to a software design can be found in [9]. The four-year implementation
process was managed using a revision control system and comprises 1,643 revisions
and 8 released versions. Differences between released versions include, among others,
changes in requirements, enhancements of decision-making strategies, and bug fixing.
A total of 15 developers were involved in the different stages of the project, and a
team of at least four developers was working concurrently in every stage.
The results of applying the system architecture were measured indirectly by
evaluating the structural software quality of the systems developed under its influence
using software metrics. Due to space limitations we report the evaluation of four AMT
application releases, one from each development year, as follows: (1) Release 742
implemented the first deployed Tutor; it was focused on the User Interface (a tool) and
had limited tutoring capabilities; coding the skeleton of the system was the primary goal
during this year. (2) Release 1277 refashioned the User Interface and implemented an
enhanced Tutor. (3) Release 1545 included a Meta Tutor, continued refashioning the
User Interface, and enhanced the Tutor module. (4) Release 1643 added the Affective
Companion capabilities, enhanced the Meta Tutor, and refactored the User Interface
and Tutor. The metrication of structural qualities, shown in Table 1, includes measures
for size, complexity, and coupling as follows: number of packages (P), number of
classes (F), number of functions (Fn), number of lines of code (LoC), number of
comments (LoCm), average cyclomatic complexity (AvC), maximum afferent coupling
(MaxAC), and maximum efferent coupling (MaxEC) [10].
Table 1. Comparison of software metrics for modules in diverse AMT application releases
Release
742
1277
1545
1643

Release
1545
1643

Release
1643

Date
07/2010
07/2011
07/2012
07/2013

P
5
9
11
14

F
24
42
55
62

Fn
347
650
885
936

Tutor
LoC
10656
20839
24542
25189

LoCm
2861
4127
4654
4816

AvgC
3.11
3.61
3.03
2.96

MaxAC
4
8
9
12

MaxEC
5
9
9
10

Date
07/2012
07/2013

P
1
1

F
22
22

Fn
202
248

Meta Tutor
LoC
3346
4210

LoCm
437
458

AvgC
2.68
3.05

MaxAC
4
5

MaxEC
4
7

Date
07/2013

P
3

F
36

Affective Companion
Fn
LoC
LoCm
323
7975
1403

AvgC
2.59

MaxAC
9

MaxEC
6

Even though we had a high turnover in the development team, the size, complexity,
and coupling remained at acceptable values. Size measurements (P, F, Fn, LoC, and
LoCm) show a correspondence of the requirements implemented in each release and
the size of the application, as well as a balance in its granularity. The average
complexity (AvgC) at the module level remains within acceptable ranges (below
five); at a fine-grain level (classes), not shown in the table, those values are not
always acceptable. The decrease in average complexity in the latest versions of Tutor
shows the refactoring outcome (functionality was fixed and developers focused on
code improvements). Lower values in coupling measures (MaxAC and MaxEC) are

534

J. Gonzalez-Sanchez et al.

better since they are a sign of independence; the high values of coupling in Tutor can
be justified because they belong to the User Interface (highly connected); Meta Tutor
values are acceptable, but Affective Companion values suggest that a refactoring
would be required in the implementation of this module.

5

Conclusions

In this paper, we have presented the AMT system architecture, the first step for
creating a component-based software framework to implement AMT-like
applications. We have defined its requirements and qualities and have shown how the
AMT system architecture addresses them to support large-scale reuse. Software
metrics for different releases of one AMT application show how the system
architecture provided a flexible partition of the system that facilitates modifiability,
extensibility, and integrability. With this proposed system architecture, we aim to
share our experience, looking forward to making the development of AMT-like
systems an easier, faster, and standardized process.
Acknowledgments. This material is based upon work supported by the National
Science Foundation under Grant No. 0910221.

References
1. Crnkovic, I.: Component-Based Software Engineering - New Challenges in Software
Development. Software Focus 2(4), 127–133 (2001)
2. VanLehn, K.: The Behavior of Tutoring Systems. International Journal of Artificial
Intelligence in Education 16(3), 227–265 (2006)
3. Burleson, W.: Affective Learning Companions: Strategies for Empathetic Agents with
Real-Time Multimodal Affective Sensing to Foster Meta-Cognitive Approaches to
Learning, Motivation, and Perseverance. MIT PhD thesis (2006)
4. Bass, L., Clements, P., Kazman, R.: Software Architecture in Practice, 2nd edn. AddisonWesley, Boston (2003)
5. Lehman, B., D’Mello, S.K., Person, N.: All Alone with Your Emotions. In: 9th
International Conference on Intelligent Tutoring Systems. Springer (2008)
6. Gonzalez-Sanchez, J., Chavez-Echeagaray, M.E., Atkinson, R., Burleson, W.: Multimodal
Detection of Affective States: A Roadmap Through Diverse Technologies. In: ACM
SIGCHI Conference on Human Factors in Computing Systems. ACM (2014)
7. Firesmith, D.G., Capell, P., Falkentha, D., Hammons, C.B., Latimer IV, D.T., Merendino,
T.: The Method Framework for Engineering System Architectures. CRC Press (2008)
8. VanLehn, K., Burleson, W., Girard, S., Chavez- Echeagaray, M.E., Gonzalez-Sanchez, J.,
Hidalgo-Pontet, Y., Zhang, L.: The Affective Meta-Tutoring project: Lessons Learned. In:
15th International Conference on Intelligent Tutoring Systems. Springer (2014)
9. Gonzalez-Sanchez, J., Chavez-Echeagaray, M.E., VanLehn, K., Burleson, W.: From
Behavioral Description to A Pattern-Based Model for Intelligent Tutoring Systems. In:
18th International Conference on Pattern Languages of Programs. ACM (2011)
10. Pressman, R.S.: Software Engineering, 7th edn. McGraw-Hill, Boston (2009)

NeuroImage 58 (2011) 675–686

Contents lists available at ScienceDirect

NeuroImage
j o u r n a l h o m e p a g e : w w w. e l s ev i e r. c o m / l o c a t e / y n i m g

The neural correlates of strategic reading comprehension: Cognitive control and
discourse comprehension
Jarrod Moss a,⁎, Christian D. Schunn b, Walter Schneider b, Danielle S. McNamara c, Kurt VanLehn d
a

Department of Psychology, Mississippi State University, USA
Learning Research and Development Center, University of Pittsburgh, USA
Department of Psychology, University of Memphis, USA
d
School of Computing, Arizona State University, USA
b
c

a r t i c l e

i n f o

Article history:
Received 5 October 2010
Revised 25 April 2011
Accepted 13 June 2011
Available online 29 June 2011
Keywords:
Reading comprehension
Reading strategies
fMRI

a b s t r a c t
Neuroimaging studies of text comprehension conducted thus far have shed little light on the brain
mechanisms underlying strategic learning from text. Thus, the present study was designed to answer the
question of what brain areas are active during performance of complex reading strategies. Reading
comprehension strategies are designed to improve a reader's comprehension of a text. For example, selfexplanation is a complex reading strategy that enhances existing comprehension processes. It was
hypothesized that reading strategies would involve areas of the brain that are normally involved in reading
comprehension along with areas that are involved in strategic control processes because the readers are
intentionally using a complex reading strategy. Subjects were asked to reread, paraphrase, and self-explain
three different texts in a block design fMRI study. Activation was found in both executive control and
comprehension areas, and furthermore, learning from text was associated with activation in the anterior
prefrontal cortex (aPFC). The authors speculate that the aPFC may play a role in coordinating the internal and
external modes of thought that are necessary for integrating new knowledge from texts with prior
knowledge.
© 2011 Elsevier Inc. All rights reserved.

Introduction
The importance and difﬁculty of comprehending expository text is
evident to anyone who has attempted to learn about a new ﬁeld of
science by reading a textbook. Comprehension is not a simple process
of accessing word meanings and then combining them. The process of
comprehension involves the construction of a mental representation
of a text, which is referred to as a situation model (e.g., Kintsch, 1998;
Zwaan and Radvansky, 1998). The construction of a situation model
requires lexical processes to access word meanings, memory retrieval
to elaborate on the text and form connections to prior knowledge, and
inference processes to help integrate the current sentence with prior
sentences and knowledge.
The complexity of text comprehension processes results in large
individual differences in the strategies that students utilize to
understand texts as well as what students learn from texts (e.g., Chi
et al., 1989; Just and Carpenter, 1992; McNamara, 2004). Although
there have been neuroimaging studies of text comprehension (e.g.,
Ferstl and von Cramon, 2001; Xu et al., 2005; Yarkoni et al., 2008a,
2008b), these studies have not examined the differences in brain
⁎ Corresponding author at: Department of Psychology, PO Box 6161, Mississippi
State, MS 39762, USA. Fax: +1 662 325 7212.
E-mail address: jarrod.moss@msstate.edu (J. Moss).
1053-8119/$ – see front matter © 2011 Elsevier Inc. All rights reserved.
doi:10.1016/j.neuroimage.2011.06.034

activity associated with different reading strategies. Understanding
the neural correlates of different types of strategic reading comprehension processes should help us to better understand the brain
mechanisms underlying comprehension.
Strategic reading comprehension
There are a number of theoretical frameworks that describe the
cognitive processes underlying text comprehension (Kintsch, 1988,
1998; McNamara and Magliano, 2009; Zwaan et al., 1995). Many of
these theories propose that the reader constructs a situation model
that is a representation of text content that abstracts away from the
written form of the sentences composing the text and includes
knowledge not contained directly in the text. Constructing a coherent
situation model requires that the reader form a textbase on the basis
of the propositions contained directly in the text itself, and elaborate
on this information by using prior knowledge through inference
processes (Kintsch, 1988, 1998; Zwaan, 1999; Zwaan and Radvansky,
1998).
The quality of the situation model depends on how successful the
reader is at representing the propositions of the text, providing
information missing from the text from prior domain-general and
domain-speciﬁc knowledge, and forming coherent representations by
drawing inferences across phrases in the text (Kintsch, 1998;

676

J. Moss et al. / NeuroImage 58 (2011) 675–686

McNamara et al., 1996). Characteristics of both the reader and the text
inﬂuence success at forming a good situation model. For some readers,
construction of a situation model is more difﬁcult because they have
little or no prior knowledge about the content of the text (Voss and
Silﬁes, 1996). For example, low domain knowledge readers learn more
from highly cohesive texts while high domain knowledge readers
learn more from low cohesion text (McNamara and Kintsch, 1996;
McNamara et al., 1996). Low domain knowledge readers are
presumably unable to make the necessary inferences from low
cohesion texts, whereas the low cohesion text forces the high domain
knowledge readers to engage in inferencing processes resulting in a
good situation model.
Reading comprehension strategies improve readers' comprehension
of text, and while some readers use strategies naturally, others beneﬁt
from being provided with strategy instruction (McNamara, 2007). Selfexplanation is one reading strategy that has been shown to be highly
effective (Bielaczyc et al., 1995; Chi, 2000; Chi et al., 1989, 1994;
McNamara, 2004). The self-explanation strategy was developed by
observing what good students do naturally when studying worked
examples in physics texts (Chi et al., 1989). Later studies on selfexplanation found that training poor students to self-explain improved
their comprehension and problem solving (e.g., Bielaczyc et al., 1995;
Chi et al., 1994; McNamara, 2004).
Because instructing readers to self-explain most often beneﬁts
readers who are skilled self-explainers more than less skilled selfexplainers (Chi et al., 1994), McNamara (2004) developed SelfExplanation Reading Training (SERT) in which students are provided
with instruction and practice on using reading strategies while selfexplaining texts. This approach combined the technique of selfexplanation with ﬁve reading strategies with demonstrated effectiveness: comprehension monitoring, paraphrasing, elaboration,
bridging, and prediction. Comprehension monitoring is being aware
of whether the text is being successfully understood while reading.
Paraphrasing is putting the text into one's own words in order to help
activate relevant semantic knowledge in long-term memory and
prepare the reader to make further inferences. Inferences are
necessary in text comprehension situations because texts do not
state all relevant pieces of information explicitly (Kintsch, 1998).
Elaboration involves making inferences that aid in understanding the
text by using knowledge from memory. Bridging involves making
inferences across sentence boundaries to aid in understanding the
text. Prediction is making predictions at the end of a sentence or
paragraph about what information will be contained in the next
section of the text.
Collectively, these strategies help the reader to process challenging, unfamiliar material by scaffolding the comprehension process.
The process of self-explaining externalizes the comprehension
process by helping the reader to understand the text (i.e., using
paraphrasing and comprehension monitoring) and go beyond the text
by generating inferences (i.e., using elaboration, bridging, and
prediction). The study presented in this paper uses an intelligent
tutoring system, iSTART (McNamara et al., 2004), to teach the ﬁve
SERT strategies so that the neural correlates of reading comprehension strategies can be examined during comprehension of expository
texts.
Neuroimaging studies of reading comprehension
There have been a number of neuroimaging studies that have
investigated text comprehension (Ferstl and von Cramon, 2001, 2002;
Ferstl et al., 2005; Friese et al., 2008; Hasson et al., 2007; Maguire et al.,
1999; Mar, 2004; Siebörger et al., 2007; Xu et al., 2005; Yarkoni et al.,
2008b). In a recent meta-analysis of neuroimaging studies of text
processing, Ferstl et al. (2008) identiﬁed a set of areas common to
many studies of text processing including the anterior temporal lobe
(aTL), areas along the superior temporal sulcus, inferior temporal

gyrus (ITG), inferior frontal gyrus (IFG), inferior frontal sulcus, presupplementary motor area (pSMA), and the cerebellum. In addition,
they also identiﬁed a set of regions that are associated with coherence
building processes including aTL, posterior superior temporal sulcus,
middle temporal gyrus (MTG), IFG, dorsal and ventral medial
prefrontal cortex (dmPFC and vmPFC), and precuneus. These latter
set of areas as well as the angular gyrus and posterior cingulate cortex
(PCC) are active in studies examining coherence building processes
such as inferencing and linking text content with global themes and
other information in memory (Ferstl and von Cramon, 2001, 2002;
Kuperberg et al., 2006; Maguire et al., 1999; Mellet et al., 2002).
Other discourse comprehension studies have attempted to map
processes such as situation model construction and updating on to
brain regions (e.g., Yarkoni et al., 2008b). In particular, Yarkoni et al.
examined areas that showed a linear increase in activation during
reading that might be associated with maintaining and integrating
information into a situation model as the reader proceeds through the
text. These areas include bilateral aTL, bilateral IFG, bilateral ITG, left
precentral gyrus, bilateral posterior parietal cortex (PPC), left fusiform
gyrus, and right precuneus. In addition, they also found that bilateral
dmPFC was activated exclusively in the story condition. Yarkoni et al.
argue that this dmPFC activation may reﬂect processes of integrating
information into a coherent situation model or that activity in this
area may reﬂect perspective-taking or theory-of-mind processes
associated with the narrative rather than more general comprehension processes. Situation model construction and updating are exactly
the kind of processes that a reading strategy such as self-explanation
is thought to enhance. Thus, it is likely many of these areas would also
be active when self-explaining.
Areas such as dmPFC, the angular gyrus, and the precuneus that
are involved in discourse comprehension are also considered part of
the brain's default network that is active when people are not engaged
in an external task (Buckner et al., 2008; Gusnard et al., 2001; Raichle
et al., 2001). Some studies of discourse processing have noted this
partial overlap between the default network and areas active during
comprehension (Xu et al., 2005; Yarkoni et al., 2008b). The default
network has been associated with self-referential processing and the
mental generation of a coherent scene through the retrieval and
integration of information (Hassabis and Maguire, 2007). These
cognitive processes should be involved in both comprehension and
reading strategies as the goal is to form a coherent representation of
the text being studied, and therefore one hypothesis is that areas such
as dmPFC, the angular gyrus, and the precuneus will be active during
the use of self-explanation.
Expository texts are designed to communicate knowledge often
including technical ideas and terms with which the reader is
unfamiliar embedded in low coherence text (Graesser et al., 2003).
Due to these properties of expository text, implicit and explicit
inference processes are likely to be needed more when processing
expository text than when processing narrative text. The effectiveness
of reading strategies has mostly been examined using expository
texts. However, most neuroimaging studies of discourse processing
have used narrative texts. The comprehension of narrative texts is
thought to be similar to but also different from expository texts
(Graesser et al., 2003; Kintsch, 1998). In particular, it could be
expected that theory-of-mind processes play less of a role in
expository text comprehension while casual and elaborative inferences play a larger role. Examining the brain areas associated with
using reading strategies should provide more information about the
role these areas play in the coherence building processes that are
essential for expository text comprehension.
Current study
The current study examines the brain areas active during performance of reading comprehension strategies that vary in complexity and

J. Moss et al. / NeuroImage 58 (2011) 675–686

effectiveness. Participants were taught self-explanation using iSTART, an
intelligent tutoring system previously found to teach self-explanation
effectively using the SERT strategies (McNamara et al., 2007). Paraphrasing a text to put it into one's own words is another reading strategy that
could be used to aid comprehension, and it is one of the ﬁve SERT
included in iSTART self-explanation training (McNamara et al., 2009).
Finally, a commonly used reading strategy is to simply reread the
material. Rereading is known to be less useful than self-explanation and
is often used as a control condition to evaluate the effectiveness of selfexplanation training (e.g., Chi et al., 1994). Participants were asked to
reread, paraphrase, and self-explain three different expository texts on
biology topics in a block design fMRI study. The comparisons of interest
were between the relative activation of brain areas during performance
of these three reading strategies. Learning was also assessed via
improvement from pretest to posttest. Pre–post data allowed for
veriﬁcation of the expected effectiveness of the reading strategies as
well as an analysis of the brain areas that correlated with measurable
learning.
Because self-explanation is an intentional strategy that enhances a
reader's existing comprehension processes, then it can be expected to
involve areas of the brain that are normally involved in reading
comprehension along with areas that are involved in strategic control
processes. A network of brain areas including dorsolateral prefrontal
cortex (DLPFC), anterior cingulate cortex/pre-supplementary motor
area (ACC/pSMA), dorsal pre-motor cortex (dPMC), anterior insular
cortex (AIC), inferior frontal junction (IFJ), and PPC have been shown
to be active in a variety of tasks involving executive control (Brass et
al., 2005; Chein and Schneider, 2005; Cole and Schneider, 2007;
Dosenbach et al., 2006; Schneider and Chein, 2003; Wager et al.,
2004). These areas also show high functional connectivity (Cole and
Schneider, 2007), and the amount of controlled processing necessary
for a task is related to the degree of activation in these areas (Chein
and Schneider, 2005). To aid in localizing the executive control
network, a variant of the line orientation search task used by Cole and
Schneider (2007) was used as a functional localizer to deﬁne regions
of interest (ROIs) for each subject.
Because reading strategies such as self-explanation are effortful
and complex, we hypothesize that this executive control network will
be active during self-explanation. We also expect lower levels of
activation in this network for less complex reading strategies that do
not involve as much effort and management of complex information,
such as paraphrasing or rereading. It was also predicted that more
complex strategies would show more activation of areas that previous
studies have associated with discourse comprehension. It is an open
question whether strategy effectiveness is primarily a function of
more engagement (as measured by activation of the executive control
network) or primarily a function of speciﬁc text comprehension
processes beyond the executive control components.
Method
Participants
Twenty-two right-handed, native English speakers were recruited
from the University of Pittsburgh and Carnegie Mellon University
communities (14 female, M age = 20.7; SD = 2.4; range = 18–28).
None of the participants were biology majors. One participant was
excluded from analysis due to excessive head motion (more than
9 mm) during the scanning session.
Materials
Three biology texts that were matched on length (approximately
580 words) were selected along with a set of short-answer questions.
Text and question difﬁculty were equated using data from a pilot
study in which another group of participants answered the questions

677

before and after reading and self-explaining the texts. The three texts
discussed the following topics: the process of cell mitosis, the
structure and function of DNA, and the circulatory system's role in
heat transportation. The texts were from different topic areas to
minimize transfer between them. Approximately half of the questions
for each text were text-based, meaning that they could be answered
given information from one sentence in the text. The answers for the
other half of the questions required bridging information across
multiple sentences in the text. Each text was separated into 12
paragraphs, with each paragraph containing 2–4 sentences, so that
they could be presented one paragraph at a time during the study.
Design
Each participant performed all three reading strategies: rereading,
paraphrasing, and self-explaining. Each participant was instructed to
use a given reading strategy to read all of a given text. The assignment
of reading strategies to texts was counterbalanced across participants.
The order in which participants performed the reading strategies was
randomized.
Each text was broken up into three sections consisting of four
paragraphs each. Each of these four-paragraph sections was presented
in a single data acquisition run. Because strategies were assigned to
texts, participants were always performing a single strategy during
each acquisition run. One four-paragraph section of each of the three
texts was presented before the next four-paragraph section of each
text. For example, this organization implies that the ﬁrst (second) and
second (third) blocks of paragraphs from a particular text were
separated by a block of each of the other two texts (e.g., Text1–Block1,
Text2–Block1, Text3–Block1, Text1–Block2, …). The blocks were
presented in this fashion so that each reading strategy would be
performed once in each third of the acquisition session in order to
help control for potential confounding effects (e.g., fatigue).
Procedure
This study took place over two sessions, separated by 2–5 days,
with fMRI data collected only during the second session.
Session 1
During the ﬁrst session, participants were given up to 30 min to
complete a pretest including all of the questions for each of the three
texts. Participants then completed an iSTART session, which provided
instruction on how to self-explain using reading strategies.
iSTART, described in greater detail by McNamara et al. (2004, 2006,
2007), provides students with instruction and practice on how to selfexplain texts using the ﬁve SERT reading strategies: comprehension
monitoring, paraphrasing, elaboration, bridging, and prediction.
iSTART uses animated agents to introduce each of the ﬁve strategies
by having a student agent receive instruction on the strategy by a
teacher agent, and then the student agent uses the strategy. Following
this introduction, for each strategy, the system asks the participant a
set of questions about each strategy and has the participant identify
each strategy in a set of example self-explanations. The participant
then reads one expository text and practices each of the ﬁve strategies
by typing in self-explanations and receiving feedback from the iSTART
system on the content and quality of the self-explanations. iSTART
training took approximately 90 min.
After iSTART training, the participants were provided with task
practice in an MRI simulator. The MRI simulator was designed to
closely simulate the physical conditions of the MRI scanner and
included a magnetic tracking system to track and present feedback to
the participant regarding head movement. The simulator practice was
done to screen for claustrophobia, to train participants to perform the
experiment (especially talking) without excessive head motion, and
to provide them with practice on the experimental task using the

678

J. Moss et al. / NeuroImage 58 (2011) 675–686

same button response system they would use during the scanning
session. In the simulator, participants were presented with 14
paragraphs from two practice texts that were of a similar expository
nature but contained different content than the texts in the
experiment. Before each block of paragraphs, participants read
instructions on the screen indicating the reading strategy they were
to use for that block.
The title of the text was centered on the top of the screen with the
paragraph appearing on the center of the screen. Along the bottom of
the screen was a prompt reminding the participant of the current
strategy. Participants were instructed to read the paragraph aloud
once, and then to press a button on a response glove. Once they did so,
the color of the paragraph's text changed from black to blue which
served as a cue that they were to perform the given reading strategy
aloud. The participants then reread, paraphrased, or self-explained
the text and pressed a button to move to reading the next paragraph.
The paraphrasing and self-explanation strategies had been introduced within iSTART, and thus, participants were provided only brief
instructions on how to either paraphrase or self-explain out loud each
sentence in the text. In the paraphrase condition, participants were
told to put each sentence in the paragraph into their own words
without using any of the other SERT strategies. In the self-explanation
condition, participants were instructed to self-explain each paragraph
using the reading strategies covered in iSTART. For the rereading
condition, they were told to read and then reread each paragraph out
loud until the computer indicated it was time to move to the next
paragraph of text. A prompt, which ﬂashed at the bottom of the screen,
instructed the participant to stop rereading and move on to the next
paragraph. The rereading condition was designed this way in order to
roughly equate the amount of time spent rereading with the amount of
time spent paraphrasing and self-explaining. The amount of time
allotted for rereading was 45 s, which was determined from a pilot
study in which participants applied the three strategies to the same
texts. Paraphrasing and self-explanation were self-paced with the
constraint that the participant could take no longer than 60 s.
Participants were prompted to move on using the same ﬂashing
prompt if they reached 60 s.

Session 2
The second session occurred 2–5 days after the ﬁrst session in
order to reduce the chance that participants would read the passages
with the pretest questions in mind. This session began with an iSTART
practice session lasting at most 30 min, which gave the participants
additional practice self-explaining. This practice session was similar to
the ﬁnal part of the initial iSTART training in which participants read
and self-explained an expository text while receiving feedback on the
self-explanations from iSTART. fMRI data was collected for the
remainder of the session. All tasks were presented using E-Prime
(Schneider et al., 2002) on a Windows PC for task presentation and
response collection. To verify strategy use within each condition,
verbal responses were collected using an active noise canceling
microphone system (Psychology Software Tools, Inc., Pittsburgh, PA),
which almost entirely removed the scanner background noise.
Participants were reminded of the instructions for the experiment
before and after being placed in the scanner. The only difference from
the MRI simulator procedure was that a 30-second rest period was
placed before and after each block of four paragraphs. A ﬁxation cross
was presented in the middle of a white screen for the rest period.
Participants were told to relax and to try not to think about anything
during this time. The participants completed a total of 9 fMRI runs
with each run consisting of 4 paragraphs (3 runs while performing
each of the 3 strategies). Following these 9 learning runs, participants
were presented with a posttest for each text. Although the posttest
was collected in the scanner, we do not examine the posttest imaging
data in this paper.

After the posttest runs, participants were presented with a line
search task that served as a functional localizer to localize activity in
control areas (Saxe et al., 2006). A version of this task has been used in
prior research on executive control (Cole and Schneider, 2007).
Participants received instructions on how to complete this task just
before the start of fMRI data acquisition. The task involved detecting a
target line orientation of 65° by monitoring lines of differing
orientation in four locations on the screen (see Fig. 1). There were
three angles of distractor lines: 85°, 45°, and 155°. The line in each of
the four locations changed orientation every 2 s. Only one location
changed at a time, and the orientation changes proceeded in a
clockwise fashion every 500 ms. Targets appeared at least 2 s apart.
The participants' task was to press a button when the target was
present. A control task was also presented with almost identical visual
stimuli except that the participants' task was to press a button every
time the central ﬁxation cross blinked. The central ﬁxation cross
blinked the same number of times as there were targets in the line
search task while all other stimuli were static. Each participant
completed one to two runs of this task depending on time constraints,
and each run consisted of 4 blocks of each task with blocks alternating
between the line and control tasks. Each block of the tasks began with
6 s of encoding, followed by 30 s of the task (control or line search),
followed by a 6 s delay before the next block began.
In order to increase statistical power in the pretest/posttest
comparison across reading strategy conditions while constraining
the number of fMRI participants, a second group of 14 behavioral
participants was run using the same reading strategy paradigm. The
only differences between the groups were that the behavioral group
was run in front of a computer instead of in the scanner and did not
complete the line search functional localizer task.

Data acquisition and analysis
Structural and functional images were collected on a whole body
Siemens Trio 3 T scanner at the Magnetic Resonance Research Center
of the University of Pittsburgh Medical Center during a 2-hour
scanning session. The scanning session began with the acquisition of
structural images, which included scanner-speciﬁc localizers and
volume anatomical series. The volume anatomical scan was acquired
in a sagittal plane (1 mm 3) using the Siemens MP-RAGE sequence and
the functional data were co-registered to these images. The functional
runs were acquired as 39 oblique-axial slices parallel to the AC–PC
plane using a T2*-weighted echo-planar imaging (EPI) pulse sequence
(TE = 25 ms, TR = 2000 ms, FOV = 21, slice thickness = 3.5 mm with
no gap, ﬂip angle = 76, in-plane resolution = 3.28 mm 2).
The raw neuroimaging data were preprocessed and analyzed using
the AFNI software package (Cox, 1996). Preprocessing included slice
scan time correction, three-dimensional motion correction, and
spatial smoothing. All functional images were realigned to the ﬁrst
image of each run, which were aligned to the ﬁrst run of each subject.
The signal for each voxel was spatially smoothed (7 mm FWHM). Each
subject's MP-RAGE anatomical images were co-registered to their
functional images by applying a transformation to the anatomical
images. The structural and functional images were then transformed
into a canonical Talairach space (Talairach and Tournoux, 1988).
Analyses of the fMRI data used voxel-based statistical techniques.
Unless otherwise speciﬁed, all results were corrected for multiple
comparisons using family-wise error (FWE) cluster size thresholding to
an FWE corrected p-value of less than .05 (Forman et al., 1995). Cluster
sizes were determined using AFNI's Alphasim, which allows for
determination of cluster size using Monte Carlo simulations. At the
individual subject level, general linear models were ﬁt to the data using
a set of boxcar functions convolved with a standard hemodynamic
response function (Boynton et al., 1996). Separate regressors for
reading, rereading, paraphrasing, and self-explaining were included in

J. Moss et al. / NeuroImage 58 (2011) 675–686

679

Fig. 1. Line search task. The top row is the control condition, and the bottom row is the search condition.

the model. Each group-level analysis used a mixed effects model with
subjects as a random factor.
The line search task was used as a functional localizer to deﬁne
subject-speciﬁc ROIs corresponding to the six bilateral areas of the
executive control network (DLPFC, ACC/pSMA, dPMC, AIC, IFJ, PPC).
The line search fMRI data were not spatially smoothed for this
analysis. ROIs corresponding to the control net regions were deﬁned
on the basis of each subject's FWE-corrected statistical map for the
contrast of the line search and control conditions. A corrected p-value
of .05 was obtained by using the combination of a voxel-based p-value
of .01 with a cluster threshold of 6 contiguous voxels. Local peaks of
activation corresponding to the anatomical location of the control net
areas were used to identify each ROI for each subject, and then all
statistically signiﬁcant voxels within a sphere of radius 15 mm from
the peak were included in the ROI.
Results
Behavioral results
The proportions correct on the pretest and posttest were used to
calculate a learning gain score, where gain = (posttest− pretest) / (1−
pretest). This gain score adjusts for the fact that questions already
answered correctly on the pretest cannot be improved upon on the
posttest (Cohen et al., 1999). Due to technical difﬁculties, the recordings
from a portion of two participants' posttests were not available to be
scored. These missing scores corresponded to the paraphrase strategy
for one participant and the self-explanation strategy for another.
The gain scores for the behavioral and imaging participants did not
differ on any of the three conditions (for all comparisons, p N .3), so the
data for these two groups were combined for the analysis of the effect
of strategy on learning. Planned comparisons showed that rereading
gain (M = .41, SD = .26) did not differ from paraphrasing (M = .42,
SD = .22), t b 1. As expected, self-explanation led to greater learning
(M = .51, SD = .19) than paraphrasing, t(32) = 2.41, p = .02, Cohen's
d = 0.4, and rereading, t(33) = 2.03, p = .05, Cohen's d = 0.4.
All participants in the imaging portion of the study performed the
line search task well; d′ was greater than 2 for all participants.
Imaging results
Analysis of areas that were more active in the line search task than
in the control condition conﬁrmed that the task served well as a
functional localizer. As can be seen in Fig. 2, this task activated the
expected set of six bilateral ROIs consistent with prior work on a
domain-general control network (e.g., Chein and Schneider, 2005).

Average percent signal change in the control network ROIs for each of
the three reading strategies relative to the rest condition is presented
in Fig. 3. For each ROI, an ANOVA was run to test for differences
between the three strategies. Bonferroni corrections were used
because 12 separate ANOVAs were conducted. For ANOVAs indicating
a signiﬁcant difference, a series of planned comparisons was used to
determine whether certain strategies activated the control regions
more than other strategies in a particular ROI. The 12 ROIs fell into two
groups. One group did not show any differential activation for the
three strategies. This group included right AIC, right IFJ, and right
DLPFC. The second group, consisting of the remaining 9 control
network ROIs, all showed greater activation for the paraphrase and
self-explanation strategies relative to the reread strategy but no
difference in activation between the paraphrase and self-explanation
strategies. Overall, the results indicate that with the exception of 3
ROIs in the right hemisphere the control network was more active
during performance of paraphrasing and self-explanation, but
the control network did not differentially activate for these two
strategies.
In order to directly examine differences in activation between the
different strategies, a voxel-wise ANOVA with strategy (reread,
paraphrase, self-explain) as a within-subjects factor was conducted
followed by three planned contrasts (paraphrase–reread, selfexplain–reread, and self-explain–paraphrase). Contrasts were done
using the strategy participants had been instructed to perform as well
as using a self-explanation coding process to determine whether they
had indeed self-explained each paragraph. The self-explanation
strategy training consisted of ﬁve separate techniques: comprehension monitoring, paraphrasing, bridging, elaboration, and prediction.
The verbal protocols from both the behavioral and imaging participants were transcribed, and the self-explanation for each paragraph
was coded for whether it contained each of the ﬁve techniques
comprising self-explanation using a coding scheme based on prior
self-explanation research (McNamara, 2004). Inter-rater agreement
between two independent coders was good (89% agreement; Cohen's
kappa = .66). If the self-explanation for a paragraph did not contain
any self-explanation strategy other than paraphrasing, then that selfexplanation was classiﬁed as being in the paraphrase condition. This
reclassiﬁcation resulted in an average of 1.7 out of 12 selfexplanations per participant being reclassiﬁed as paraphrases. The
fMRI results were similar for both versions of this analysis with the
reclassiﬁed data generally showing slightly more signiﬁcant local
maxima, therefore only the reclassiﬁed analysis is reported.
For the contrasts between the reading strategies, activation in the
line search task was examined to identify clusters of activation that fell
both inside and outside of the control net. Tables 1 and 2 show for each

680

J. Moss et al. / NeuroImage 58 (2011) 675–686

Fig. 2. Statistical map for group analysis of areas active in line search functional localizer task projected on to cortical surface (p b .001, minsize = 490 mm3). Statistical maps projected
on to cortical surface. Corresponds to table of regions in supplementary materials. For all ﬁgures, left hemisphere lateral and medial views are on the left of the ﬁgure.

peak whether or not the peak fell within a control net region or not. The
areas more active for the paraphrase condition compared to rereading
are shown in Table 1. Areas outside of the control net included left
pSMA, left IFG, right lingual gyrus, right cerebellum, and bilateral areas
of the basal ganglia. The self-explanation–reread contrast yielded many
of the same regions as the paraphrase–reread contrast as can be seen in
Table 1 and Fig. 4 (see supplementary materials for an image of the
paraphrase–reread contrast). In addition to the areas outside of the
control net seen in the paraphrase–reread contrast, regions of
activation included left dmPFC, left superior frontal gyrus, left
precuneus, left MTG, and the thalamus. Given that many of the peaks
overlapped with the control network, these results are consistent with
the hypothesis that there is engagement of a domain-general control
network with the use of complex reading strategies.

However, the contrast between the self-explanation and paraphrase conditions shows a different pattern of results as seen in
Table 2 and Fig. 5. None of the regions are part of the control network,
and they include bilateral activations in prefrontal cortex, PCC,
precuneus, and the angular gyrus.
An additional analysis was conducted to examine whether the
contrasts between the learning strategies may be explained in part by
production processes that differ across the three reading strategies
rather than comprehension processes. Coh-Metrix (Graesser et al.,
2004) was used to examine the transcribed utterances produced by
participants. Coh-Metrix analyzes text and provides a number of
variables related to the content of the texts being analyzed including
syntactic variables. The variables that Coh-Metrix reported were
examined to see if they differed across the reading strategies.

Fig. 3. Mean signal change and standard error in each executive control network ROI for each reading strategy.

681

J. Moss et al. / NeuroImage 58 (2011) 675–686
Table 1
Local maxima of regions showing positive activation in paraphrase–reread and self-explanation–reread contrasts (p b .001, minsize = 490 mm3).
Regions

Control
net

BA

Frontal cortex
L dPMC
L ACC/pSMA
L ACC
R ACC
R pSMA
L IFJ
L inferior frontal g
L inferior frontal g
L superior frontal g
L superior frontal g
L insula
L inferior frontal g
R dPMC

Partial
Partial
Partial
Yes
Yes
Yes
No
No
No
No
Yes
No
Yes

Parietal cortex
L superior parietal
L precuneus
L parietal/occipital
L inferior parietal
R superior parietal

Self-explanation–reread

Paraphrase–reread

Cluster size (mm3)

x

Peak t

Cluster size (mm3)

x

6
6,32
32
32
6
6,9,44
44,45
13,47
6,8
8
13,45
10,46
6

57,956
–

− 33
− 10

6
13

52
52

11.25
10.61

19
10
13
13
29
33
49
26
36
−7

38
56
31
10
0
52
42
3
17
56

7.06
6.43
8.93
6.68
6.44
5.5
4.6
6
5.28
5.18

47,631
–
–
–
–
–
–
–

− 30
− 10
−7
3
3
− 39
− 49
− 39

0
13
23
19
10
6
16
23

56
52
35
38
56
35
17
−1

10.54
10.77
7.99
6.6
5.97
8.78
8.08
3.34

–
–
–
–
–
–
–
–
–
980

3
3
− 36
− 46
− 43
− 10
− 10
− 26
− 43
20

753
1394

− 33
26

33
0

13
55

5.04
5.43

Yes
No
Yes
Yes
Yes

7
7
7,19
40
7

9345
–
–

− 23
−3
− 26

− 69
− 66
− 66

49
42
31

8.41
7.24
6.9

9646

− 13

− 69

49

10.25

–
3052

− 33
26

− 43
− 69

35
38

6.4
7.4

Temporal cortex
L middle temporal g

No

21,37

2788

Occipital cortex
R lingual g
R middle occipital g

No
Yes

18
19

16
33

− 79
− 79

−8
10

4.86
4.51

Cerebellum/subcortical
R cerebellum
R cerebellum
R cerebellum
R cerebellum
L caudate
L globus pallidus
L midbrain
L thalamus
R globus pallidus
R caudate

No
Yes
No
No
No
No
No
No
No
No

23
39
39
16
− 16
− 13

− 59
− 59
− 46
− 72
10
−4

− 29
− 25
− 46
− 39
14
3

7.62
7.11
4.35
6.37
6.63
8.64

13

0

3

5.35

y

z

− 56

− 46

−4

6.92

980

13

− 82

−8

7.25

528
565

12,548
–
–

23
33
39

− 66
− 49
− 56

− 25
− 29
− 46

9.24
8.11
5.23

− 13
−3
−7
16
16

−4
− 23
− 17
−4
6

3
− 11
17
3
21

9.89
4.71
4.66
8.55
5.11

10,099
–
–
–
4521
–

9345
–
–
3617
–

641

y

z

Peak t

Note. All regions within a connected cluster are presented on consecutive lines. The ﬁrst row within a cluster contains the cluster size, and all regions within the same cluster contain
a ‘–’ for cluster size.

Verbalizations during rereading had fewer verbs and a lower
Flesch Reading Ease score than paraphrases and self-explanations.
Because rereading was just a repetition of the texts, these differences
indicate that verbalizations composed by participants did differ from
the original texts. Paraphrases also differed from rereadings on
variables related to cohesion including noun-stem overlap, temporal
cohesion, and incidence of intentional actions/participles. Selfexplanations had higher frequency words, more adverbs, more
connectives, and a higher proportion of causal participles to causal
verbs than did rereadings.
Self-explanations differed in a number of ways from paraphrasing.
Syntactic differences included more adverbs, a higher proportion of
function words, higher lexical diversity, lower syntactic similarity
across sentences, fewer modiﬁers per noun phrase, and nouns with
lower hypernym value for self-explanations than paraphrases.
Measures of cohesion that differed included lower noun-stem overlap
and more causal verbs/participles for self-explanations than paraphrases. Also, self-explanations had a lower incidence of intentional
actions/events but a higher ratio of intentional participles to
intentional actions/events indicating that intentional cohesion was
higher for self-explanations.
Any variable that differed signiﬁcantly across the strategies was
included as a covariate in a group analysis of the imaging data that
replicated the contrasts reported above. Inclusion of the covariates did

not alter the signiﬁcance or location of any of the peaks reported for
the strategy contrasts.
The previous contrasts examine areas that were more active when
participants were self-explaining. However, another approach to
examining self-explanation is to examine those times when it led to
measurable learning. Thus, a separate analysis was conducted to
examine whether there were brain regions that had activity parametrically modulated by successful learning. This analysis was conducted by
using an amplitude-modulated regressor in addition to the strategy
regressor for the self-explanation runs. The amplitude of this regressor
was based on the gain score for a particular paragraph. The gain score for
each paragraph was calculated by ﬁrst determining for each question on
the pre/posttests in which paragraph the information to answer the
question was presented. Some paragraphs may have mapped to
multiple questions. In this case, the average gain across all questions
mapping to that paragraph was calculated. The regressor for the analysis
was formed by convolving a boxcar function whose amplitude was
determined by the gain score with a hemodynamic response function.
The mean gain score for each subject was subtracted from the
amplitudes to yield a regressor that was used to identify brain areas
exhibiting a linear relation to gain scores (e.g., Buchel et al., 1998).
This learning analysis identiﬁed a set of bilateral prefrontal areas
that were positively associated with learning gains. These areas are
shown in Fig. 6 and Table 3. There were no areas negatively associated

682

J. Moss et al. / NeuroImage 58 (2011) 675–686

Table 2
Local maxima of regions showing positive activation in self-explain–paraphrase
contrast (p b .001, minsize = 490 mm3).
Regions
Frontal cortex
L orbital g
R orbital g
L superior frontal g
R superior frontal g
L anterior cingulate
Parietal Cortex
L posterior cingulate
L posterior cingulate
L precuneus
R posterior cingulate
R precuneus
L angular g
R angular/middle
temporal g

Cluster size
(mm3)

Control
net

BA

942
–
942
–
490

No
No
No
No
No

10
10
9,10
9,10
10,32

−3
3
−7
3
−7

59
49
59
56
49

7
−1
24
28
3

4.77
4.51
5.59
3.98
4.91

12,360
–
–
–
–
4333
2223

No
No
No
No
No
No
No

23,31
23,31
7,31
23,31
7,31
39
37,39

−7
−7
−3
3
7
− 49
46

− 33
− 49
− 69
− 49
− 66
− 66
− 66

35
28
28
28
24
24
10

7.62
6.76
5.32
6.53
5.36
6.1
5.95

− 16

− 43

− 18

5.03

Cerebellum/Subcortical
L cerebellum
565

No

x

y

z

Peak t

Note. All regions within a connected cluster are presented on consecutive lines. The ﬁrst
row within a cluster contains the cluster size, and all regions within the same cluster
contain a ‘–’ for cluster size.

with learning gain. In addition to the areas that were active during
self-explanation, these anterior prefrontal areas were more active
during self-explanation trials during which material was learned well
enough to be answered better on the posttest than the pretest.
Discussion
The results provide evidence that complex reading strategies
engage executive control regions, semantic/comprehension regions,
and bilateral aPFC. The behavioral learning results conﬁrmed that the
three reading strategies differed in effectiveness as hypothesized.
With a relatively short learning period for complex science materials
and a short delay between learning and test, these moderately-sized
learning differences were as expected. With longer delays, there

would likely be further differentiation of results between paraphrasing and rereading as well as between self-explanation and
paraphrasing.
Comparing the least complex strategy, rereading, with the next
most complex strategy, paraphrasing, showed that predominantly
areas known to be involved in executive control were more active for
the more complex strategy. This ﬁnding is consistent with our initial
hypothesis that more complex strategies would require more
engagement and cognitive control. In addition to the control network,
areas of activation identiﬁed in a recent meta-analysis of language
processing included left pSMA and left IFG (Ferstl et al., 2008). The
other active non-control regions included right lingual, right
cerebellum, and portions of the basal ganglia, which have previously
been seen in studies of word and sentence reading (e.g., Joubert et al.,
2004; Xu et al., 2005). Based on these results, it appears that
paraphrasing activates the control network and a portion of the
language processing network more than rereading does.
Self-explanation when contrasted with rereading activated the
same regions as paraphrasing as well as additional areas including left
superior frontal gyrus near the dorsal median wall, left precuneus, left
MTG, and the thalamus. Many of these areas including premotor
cortex and the thalamus are known to be active during word and
sentence processing (e.g., Xu et al., 2005). Areas such as dmPFC, the
precuneus, and MTG have been linked to coherence building
processes including inferencing (Ferstl and von Cramon, 2001,
2002; Ferstl et al., 2008; Friese et al., 2008). In particular, Maguire et
al. (1999) found the same area of the precuneus to be more active
during the second reading of a narrative passage, and they
hypothesized that this area might be associated with the processing
of episodic memories while further developing a mental model of the
text. The self-explanation strategy was designed to promote coherence building processes, and these results support the link between
these brain regions and coherence building cognitive processes.
The control network was not more active for self-explanation than
it was for paraphrasing. The beneﬁts of self-explanation over
paraphrasing were clear in the behavioral learning results, but were
associated with areas outside of the control network. Because these
areas are deﬁned as control areas by the fact that they show practicerelated decreases as more automatic processing occurs (Chein and
Schneider, 2005), then the activation of the control network may be

Fig. 4. Statistical map for group analysis of areas more active in self-explanation than reread projected on to cortical surface (p b .001, minsize = 490 mm3). Corresponds to list of
regions in Table 3. Activation map is very similar to paraphrase–reread contrast map (see supplementary materials).

J. Moss et al. / NeuroImage 58 (2011) 675–686

683

Fig. 5. Statistical map for group analysis of areas more active in self-explanation than paraphrase projected on to cortical surface (p b .001, minsize = 490 mm3). Corresponds to list of
regions in Table 2.

seen as an indication of the amount of controlled processing required.
The effectiveness of self-explanation was never expected to be solely
due to the controlled effort involved, but it is interesting that the more
effective complex reading strategy requires a similar amount of effort
as a less effective one.
The contrast of self-explanation with paraphrase yielded activation in bilateral vmPFC (anterior cingulate and orbital gyri), bilateral
dmPFC (superior frontal gyrus), bilateral precuneus, and left PCC
which were all identiﬁed in a meta-analysis of studies contrasting
coherent with incoherent text (Ferstl et al., 2008). Also, the bilateral
angular gyrus activation found in this contrast is close to the superior
temporal sulcus region found in the same meta-analysis. The overlap
between this contrast and the meta-analysis shows that the regions
more active in self-explanation than paraphrasing are the same
regions known to be involved in coherence building processes while
reading. Most of the studies included in the meta-analysis used
narrative texts or sentences, so this overlap also indicates that the
processing of expository text involves similar brain regions as the
coherence building processes that occur for narrative texts. The only
area identiﬁed by Ferstl et al. (2008) that was not seen in this contrast
is the aTL. The lack of aTL activation is also consistent with other
studies that have examined inferencing in discourse comprehension
(Kuperberg et al., 2006). Ferstl et al. (2008) hypothesize that this
region may be associated with producing a semantic propositional

representation, and it could be that this process is equally important
for rereading, paraphrasing, and self-explaining which is why it was
not seen in our results.
The angular gyrus, PCC, and precuneus have been associated with
relating text to prior knowledge and the use and manipulation of
mental models (Maguire et al., 1999; Mellet et al., 2002; Xu et al.,
2005). The areas active in the MTG in self-explanation are also similar
to areas that have been found when people draw inferences during
text comprehension (Virtue et al., 2006). These are exactly the kinds
of cognitive processes that a reading strategy such as self-explanation
is assumed to engage to support deep comprehension of the text.
An open question is whether there is a special role for right
hemisphere language processing regions in comprehending discourse. Some neuroimaging and neuropsychological studies have
found that the right hemisphere may be more important for discourse
comprehension and making inferences than the left hemisphere (e.g.,
Beeman and Chiarello, 1998; Jung-Beeman, 2005; Lehman-Blake and
Tompkins, 2001; Mason and Just, 2004; St George et al., 1999). The
evidence is mixed as some studies, including a recent meta-analysis,
have not found a differential level of activity in the right hemisphere
during discourse comprehension (e.g., Ferstl and von Cramon, 2001,
2002; Ferstl et al., 2008; Kuperberg et al., 2006). Visual inspection of
Figs. 4 and 5 also shows that if anything activity is left lateralized. In
many cases, regions are activated bilaterally, but the right hemisphere

Fig. 6. Statistical map for areas linearly related to measurable learning gains during self-explanation projected on to cortical surface (p b .01, minsize = 1496 mm3). Corresponds to
list of regions in Table 3.

684

J. Moss et al. / NeuroImage 58 (2011) 675–686

Table 3
Local maxima of regions showing activation for learning regressor (p b .01,
minsize = 1496 mm3).
Regions

Cluster Size (mm3)

BA

x

y

z

Peak t

R inferior frontal gyrus
R superior frontal gyrus
R superior orbital gyrus
R middle frontal gyrus
L superior frontal gyrus
L middle frontal gyrus

4560
–
–
–
2148
–

46
10
10
9
10
9,10

35
23
23
31
− 18
− 33

28
53
41
45
43
39

18
15
0
31
21
28

5.11
4.86
4.21
4.10
5.05
4.86

Note. All regions within a connected cluster are presented on consecutive lines. The ﬁrst
row within a cluster contains the cluster size, and all regions within the same cluster
contain a ‘–’ for cluster size.

was not differentially activated for self-explanation than for either of
the other two strategies even though self-explanation should lead to
more inferences than the other strategies. The evidence in the
literature for a special role of the right hemisphere in inferencing is
mixed, but our results are consistent with other work on inferencing
in discourse comprehension (e.g., Kuperberg et al., 2006) as well as
the meta-analysis by Ferstl et al. (2008) that do not show differential
right hemisphere activity.
While the activation shown while performing self-explanation
seems to be associated with coherence building processes as
expected, it is interesting to note that the contrast between selfexplanation and paraphrase is not a subset of the regions active for the
self-explanation–reread contrast. This pattern of results indicates that
activation of many of the regions in the self-explanation–paraphrase
contrast was similar to the reread condition. Many of the regions in
the self-explanation–paraphrase contrast are part of the default
network (Buckner et al., 2008; Raichle et al., 2001). There are a
number of possible interpretations for the highly consistent pattern of
activity that deﬁnes the default network, but many of these
explanations focus on an internal mode of thought that is stimulus
independent self-guided thought (Buckner et al., 2008). These
stimulus-independent thoughts have been associated with lapses in
attention (Weissman et al., 2006) and mind wandering (Christoff et
al., 2009), but this mode of thought is also thought to have adaptive
purposes (Bar, 2007; Hassabis and Maguire, 2007). One explanation
for our results is that during rereading participants were engaging this
same network for the purposes of self-directed thought or mind
wandering instead of processing text. Rereading is not a particularly
demanding task especially because our participants repeated the
same paragraph two or more times in a row so that they spent the
same amount of time rereading as self-explaining and paraphrasing.
This less demanding strategy could have left enough time and
attention free that mind-wandering occurred to some degree while
rereading.
In support of this explanation of increased mind wandering during
rereading, we have data from a recent fMRI study on mind wandering
during reading strategies using a similar methodology as the current
study (Moss et al., 2011). In this follow-up study, participants rated
the frequency of mind wandering while performing the reading
strategies after each short paragraph. Mind wandering ratings were
signiﬁcantly higher for rereading than for self-explanation (p b .05),
and marginally higher for rereading than for paraphrasing (p b .06).
This data suggests that some of the differences between the rereading
contrasts and the self-explanation–paraphrase contrast may be due to
mind wandering.
Rereading is also different from paraphrasing and self-explanation
because it does not require the generation and production of new
sentences as the other two strategies do. While the inclusion of
covariates related to syntactic complexity did not alter the results, the
covariate analysis does not completely rule out production planning
and other production-related differences in the contrasts between

rereading and the other strategies. In fact, the generation of new
sentences beyond those contained in the text is an inherent difference
between rereading and the more effective strategies. The design of
this study does not permit the separation of the reread contrast results
into comprehension versus production related regions. This limitation
provides the basis for future work on understanding the neural
correlates of strategic reading comprehension.
It has been found that the default network is anti-correlated with
attentional and executive control areas (Fox et al., 2005). Effective
reading strategies appear to strongly activate both executive control
areas as well as default mode areas. These default mode areas likely
perform similar functions during rest and during comprehension. One
possibility is that effective reading strategies are explicit strategies
that involve intentionally carrying out a sequence of actions, but that
these strategies intentionally involve functions like memory retrieval,
mental simulation, and information integration that are performed
during mind wandering and other forms of self-directed thought as
well.
The analysis of the areas that were correlated with the amount
learned during self-explanation mainly included bilateral aPFC. That
is, in addition to the activity in executive control and text
comprehension areas associated with self-explanation, the aPFC was
more active during self-explanation of paragraphs where measurable
learning took place. Maguire et al. (1999) also found that a similar
region of the left aPFC was associated with the number of idea units
recalled after reading a narrative, and it was also active while listening
to a second repetition of the story. They hypothesized that this area is
associated with retrieval success. Alternatively, a recent theory of
aPFC function refers to it as a router or gateway between modes of
thought (Burgess et al., 2005, 2007). One of these modes of thought is
one in which external representations (i.e., objects in the environment) drive thought, and the other mode is one in which internal
representations drive thought. This gateway hypothesis might help to
explain the correlation of the aPFC with learning in this study. The
aPFC might be helping to coordinate the reading and processing of the
text presented on the screen with the internal retrieval of memories
and construction of situation models. It may also reﬂect the
coordination of an explicit strategy with the types of internal thought
normally associated with the default network. Self-explanation may
be most effective in aiding learning when there is a good deal of
strategic processing of internal representations.

Conclusions
This initial exploration of the neural correlates of strategic reading
comprehension has shown that networks of areas associated with
executive control and the manipulation of internal representations
and memories underlie the effectiveness of these strategies. Selfexplanation produced greater learning gains than the other two
strategies, and performing self-explanation led to greater activation in
areas associated with executive control as well as discourse comprehension areas involved in the maintenance and manipulation of
internal representations to build coherent situation models. The
results show that the beneﬁts of self-explanation are not solely due
to increased engagement of the executive control network because
paraphrasing activated the control network to a similar degree.
Instead, co-activation of the control network and discourse comprehension areas distinguished self-explanation from the less effective
strategies. In addition, aPFC activation was associated with learning
gains while performing self-explanation. Future work should explore
the role of aPFC in reading strategies as well as whether these results
will generalize to other texts and other types of texts, such as
narratives.
Supplementary materials related to this article can be found online
at doi:10.1016/j.neuroimage.2011.06.034.

J. Moss et al. / NeuroImage 58 (2011) 675–686

Acknowledgments
This work was supported by The Defense Advanced Research
Projects Agency (NBCH090053). The views, opinions, and/or ﬁndings
contained in this article are those of the authors and should not be
interpreted as representing the ofﬁcial views or policies, either
expressed or implied, of the Defense Advanced Research Projects
Agency or the Department of Defense. The authors would like to thank
Melissa Thomas, Kevin Jarbo, and Adrienne McGrail for their
assistance with data collection.

References
Bar, M., 2007. The proactive brain: using analogies and associations to generate
predictions. Trends Cogn. Sci. 11, 280–289.
Beeman, M.J., Chiarello, C., 1998. Complementary right- and left-hemisphere language
comprehension. Curr. Dir. Psychol. Sci. 7, 2–8.
Bielaczyc, K., Pirolli, P.L., Brown, A.L., 1995. Training in self-explanation and selfregulation strategies: investigating the effects of knowledge acquisition activities
on problem solving. Cogn. Instr. 13, 221–252.
Boynton, G.M., Engel, S.A., Glover, G.H., Heeger, D.J., 1996. Linear systems analysis of
functional magnetic resonance imaging in human V1. J. Neurosci. 16, 4207–4221.
Brass, M., Derrfuss, J., Forstmann, B., Cramon, D.Y., 2005. The role of the inferior frontal
junction area in cognitive control. Trends Cogn. Sci. 9, 314–316.
Buchel, C., Holmes, A.P., Rees, G., Friston, K.J., 1998. Characterizing stimulus–response
functions using nonlinear regressors in parametric fMRI experiments. NeuroImage
8, 140–148.
Buckner, R.L., Andrews-Hanna, J.R., Schacter, D.L., 2008. The brain's default network:
anatomy, function, and relevance to disease. Ann. N. Y. Acad. Sci. 1124, 1–38.
Burgess, P.W., Simons, J.S., Dumontheil, I., Gilbert, S.J., 2005. The gateway hypothesis of
rostral PFC function. In: Duncan, J., Phillips, L., McLeod, P. (Eds.), Measuring the
Mind: Speed Control and Age. Oxford University Press, Oxford, pp. 215–246.
Burgess, P.W., Dumontheil, I., Gilbert, S.J., 2007. The gateway hypothesis of rostral
prefrontal cortex (area 10) function. Trends Cogn. Sci. 11, 290–298.
Chein, J.M., Schneider, W., 2005. Neuroimaging studies of practice-related change: fMRI
and meta-analytic evidence of a domain-general control network for learning.
Brain Res. Cogn. Brain Res. 25, 607–623.
Chi, M.T.H., 2000. Self-explaining: the dual processes of generating inference and
repairing mental models. In: Glaser, R. (Ed.), Advances in Instructional Psychology.
Lawrence Erlbaum Associates, Mahwah, NJ, pp. 161–238.
Chi, M.T.H., Bassok, M., Lewis, M.W., Reimann, P., Glaser, R., 1989. Self-explanations:
how students study and use examples in learning to solve problems. Cogn. Sci. 13,
145–182.
Chi, M.T.H., Deleeuw, N., Chiu, M.H., Lavancher, C., 1994. Eliciting self-explanations
improves understanding. Cogn. Sci. 18, 439–477.
Christoff, K., Gordon, A.M., Smallwood, J., Smith, R., Schooler, J.W., 2009. Experience
sampling during fMRI reveals default network and executive system contributions
to mind wandering. Proc. Natl. Acad. Sci. U. S. A. 106, 8719–8724.
Cohen, P., Cohen, J., Aiken, L.S., West, S.G., 1999. The problem of units and the
circumstance for POMP. Multivariate Behav. Res. 34, 315–346.
Cole, M.W., Schneider, W., 2007. The cognitive control network: integrated cortical
regions with dissociable functions. NeuroImage 37, 343–360.
Cox, R.W., 1996. AFNI: software for analysis and visualization of functional magnetic
resonance neuroimages. Comput. Biomed. Res. 29, 162–173.
Dosenbach, N.U., Visscher, K.M., Palmer, E.D., Miezin, F.M., Wenger, K.K., Kang, H.C.,
Burgund, E.D., Grimes, A.L., Schlaggar, B.L., Petersen, S.E., 2006. A core system for
the implementation of task sets. Neuron 50, 799–812.
Ferstl, E.C., von Cramon, D.Y., 2001. The role of coherence and cohesion in text
comprehension: an event-related fMRI study. Brain Res. Cogn. Brain Res. 11,
325–340.
Ferstl, E.C., von Cramon, D.Y., 2002. What does the frontomedian cortex contribute to
language processing: coherence or theory of mind? NeuroImage 17, 1599–1612.
Ferstl, E.C., Rinck, M., von Cramon, D.Y., 2005. Emotional and temporal aspects of
situation model processing during text comprehension: an event-related fMRI
study. J. Cogn. Neurosci. 17, 724–739.
Ferstl, E.C., Neumann, J., Bogler, C., von Cramon, D.Y., 2008. The extended language
network: a meta-analysis of neuroimaging studies on text comprehension. Hum.
Brain Mapp. 29, 581–593.
Forman, S.D., Cohen, J.D., Fitzgerald, M., Eddy, W.F., Mintun, M.A., Noll, D.C., 1995.
Improved assessment of signiﬁcant activation in functional Magnetic Resonance
Imaging (fMRI): use of a cluster-size threshold. Magn. Reson. Med. 33, 636–647.
Fox, M.D., Snyder, A.Z., Vincent, J.L., Corbetta, M., Van Essen, D.C., Raichle, M.E., 2005.
The human brain is intrinsically organized into dynamic, anticorrelated functional
networks. Proc. Natl. Acad. Sci. U. S. A. 102, 9673–9678.
Friese, U., Rutschmann, R., Raabe, M., Schmalhofer, F., 2008. Neural Indicators of
inference processes in text comprehension: an event-related functional magnetic
resonance imaging study. J. Cogn. Neurosci. 20, 2110–2124.
Graesser, A.C., McNamara, D.S., Louwerse, M.M., 2003. What do readers need to learn in
order to process coherence relations in narrative and expository text. In: Sweet, A.P.,
Snow, C.E. (Eds.), Rethinking Reading Comprehension. Guilford Publications, New
York, NY, pp. 82–98.

685

Graesser, A.C., McNamara, D.S., Louwerse, M.M., Cai, Z., 2004. Coh-Metrix: analysis of
text on cohesion and language. Behav. Res. Methods 36, 193–202.
Gusnard, D.A., Akbudak, E., Shulman, G.L., Raichle, M.E., 2001. Medial prefrontal cortex
and self-referential mental activity: relation to a default mode of brain function.
Proc. Natl. Acad. Sci. U. S. A. 98, 4259–4264.
Hassabis, D., Maguire, E.A., 2007. Deconstructing episodic memory with construction.
Trends Cogn. Sci. 11, 299–306.
Hasson, U., Nusbaum, H.C., Small, S.L., 2007. Brain networks subserving the extraction of
sentence information and its encoding to memory. Cereb. Cortex 17, 2899.
Joubert, S., Beauregard, M., Walter, N., Bourgouin, P., Beaudoin, G., Leroux, J., Karama, S.,
Lecours, A.R., 2004. Neural correlates of lexical and sublexical processes in reading.
Brain Lang. 89, 9–20.
Jung-Beeman, M., 2005. Bilateral brain processes for comprehending natural language.
Trends Cogn. Sci. 9, 512–518.
Just, M.A., Carpenter, P.A., 1992. A capacity theory of comprehension: individual
differences in working memory. Psychol. Rev. 99, 122–149.
Kintsch, W., 1988. The role of knowledge in discourse comprehension: a construction–
integration model. Psychol. Rev. 95, 163–182.
Kintsch, W., 1998. Comprehension: A Paradigm for Cognition. Cambridge University
Press, Cambridge.
Kuperberg, G.R., Lakshmanan, B.M., Caplan, D.N., Holcomb, P.J., 2006. Making sense of
discourse: an fMRI study of causal inferencing across sentences. NeuroImage 33,
343–361.
Lehman-Blake, M.T., Tompkins, C.A., 2001. Predictive inferencing in adults with right
hemisphere brain damage. J. Speech Lang. Hear. Res. 44, 639–654.
Maguire, E.A., Frith, C.D., Morris, R.G.M., 1999. The functional neuroanatomy of comprehension and memory: the importance of prior knowledge. Brain 122, 1839–1850.
Mar, R.A., 2004. The neuropsychology of narrative: story comprehension, story
production and their interrelation. Neuropsychologia 42, 1414–1434.
Mason, R.A., Just, M.A., 2004. How the brain processes causal inferences in text. Psychol.
Sci. 15, 1–7.
McNamara, D.S., 2004. SERT: self-explanation reading training. Discourse Process 38,
1–30.
McNamara, D.S., 2007. Reading Comprehension Strategies: Theory, Interventions, and
Technologies. Erlbaum, Mahwah, NJ.
McNamara, D.S., Kintsch, W., 1996. Learning from texts: effects of prior knowledge and
text coherence. Discourse Process 22, 247–288.
McNamara, D.S., Magliano, J., 2009. Towards a comprehensive model of comprehension. In: Ross, B.H. (Ed.), The Psychology of Learning and Motivation. Academic
Press, New York, pp. 297–384.
McNamara, D.S., Kintsch, E., Songer, N.B., Kintsch, W., 1996. Are good texts always
better? Interactions of text coherence, background knowledge, and levels of
understanding in learning from text. Cogn. Instr. 14, 1–43.
McNamara, D.S., Levinstein, I.B., Boonthum, C., 2004. iSTART: interactive strategy
training for active reading and thinking. Behav. Res. Methods Instrum. Comput. 36,
222–233.
McNamara, D.S., O'Reilly, T.P., Best, R.M., Ozuru, Y., 2006. Improving adolescent
students' reading comprehension with iSTART. J. Educ. Comput. Res. 34, 147–171.
McNamara, D.S., O'Reilly, T., Rowe, M., Boonthum, C., Levinstein, I., 2007. iSTART: a web-based
tutor that teaches self-explanation and metacognitive reading strategies. Reading
Comprehension Strategies: Theories, Interventions, and Technologies, pp. 397–421.
McNamara, D.S., Boonthum, C., Kurby, C.A., Magliano, J., Pillarisetti, S.P., Bellissens, C.,
2009. Interactive paraphrase training: the development and testing of an iSTART
module. Proceedings of the 2009 Conference on Artiﬁcial Intelligence in Education:
Building Learning Systems that Care: From Knowledge Representation to Affective
Modeling. IOS Press, pp. 181–188.
Mellet, E., Bricogne, S., Crivello, F., Mazoyer, B., Denis, M., Tzourio-Mazoyer, N., 2002.
Neural basis of mental scanning of a topographic representation built from a text.
Cereb. Cortex 12, 1322–1330.
Moss, J., Schunn, C.D., Schneider, W., McNamara, D.S., 2011. An fMRI study of zoning out
during strategic reading comprehension. Proceedings of the Thirty-third Annual
Conference of the Cognitive Science Society. Austin, TX: Cognitive Science Society.
Raichle, M.E., MacLeod, A.M., Snyder, A.Z., Powers, W.J., Gusnard, D.A., Shulman, G.L.,
2001. A default mode of brain function. Proc. Natl. Acad. Sci. U. S. A. 98, 676–682.
Saxe, R., Brett, M., Kanwisher, N., 2006. Divide and conquer: a defense of functional
localizers. NeuroImage 30, 1088–1096.
Schneider, W., Chein, J.M., 2003. Controlled & automatic processing: behavior, theory,
and biological mechanisms. Cogn. Sci. 27, 525–559.
Schneider, W., Eschman, A., Zuccolotto, A., 2002. E-Prime User's Guide. Psychology
Software Tools Inc., Pittsburgh, PA.
Siebörger, F.T., Ferstl, E.C., von Cramon, D.Y., 2007. Making sense of nonsense: an fMRI
study of task induced inference processes during discourse comprehension. Brain
Res. 1166, 77–91.
St George, M., Kutas, M., Martinez, A., Sereno, M.I., 1999. Semantic integration in
reading: engagement of the right hemisphere during discourse processing. Brain
122, 1317–1325.
Talairach, J., Tournoux, P., 1988. Co-planar Stereotaxic Atlas of the Human Brain.
Thieme, New York.
Virtue, S., Haberman, J., Clancy, Z., Parrish, T., Jung Beeman, M., 2006. Neural activity of
inferences during story comprehension. Brain Res. 1084, 104–114.
Voss, J.F., Silﬁes, L.N., 1996. Learning from history text: The interaction of knowledge
and comprehension skill with text structure. Cogn. Instr. 14, 45.
Wager, T.D., Jonides, J., Reading, S., 2004. Neuroimaging studies of shifting attention: a
meta-analysis. NeuroImage 22, 1679–1693.
Weissman, D.H., Roberts, K.C., Visscher, K.M., Woldorff, M.G., 2006. The neural bases of
momentary lapses in attention. Nat. Neurosci. 9, 971–978.

686

J. Moss et al. / NeuroImage 58 (2011) 675–686

Xu, J., Kemeny, S., Park, G., Frattali, C., Braun, A., 2005. Language in context: emergent
features of word, sentence, and narrative comprehension. NeuroImage 25, 1002–1015.
Yarkoni, T., Speer, N.K., Balota, D.A., McAvoy, M.P., Zacks, J.M., 2008a. Pictures of a
thousand words: investigating the neural mechanisms of reading with extremely
rapid event-related fMRI. NeuroImage 42, 973–987.
Yarkoni, T., Speer, N.K., Zacks, J.M., 2008b. Neural substrates of narrative comprehension and memory. NeuroImage 41, 1408–1425.

Zwaan, R.A., 1999. Situation models: the mental leap into imagined worlds. Curr. Dir.
Psychol. Sci. 8, 15–18.
Zwaan, R.A., Radvansky, G.A., 1998. Situation models in language comprehension and
memory. Psychol. Bull. 123, 162–185.
Zwaan, R.A., Langston, M.C., Graesser, A.C., 1995. The construction of situation
models in narrative comprehension: an event-indexing model. Psychol. Sci. 6,
292–297.

International Journal of Artificial Intelligence in Education, 10(4), 2010.

The Effect of Self-Explaining on Robust Learning
Robert G. M. Hausmann, Carnegie Learning, Inc., Pittsburgh, PA, USA
bhausmann@carnegielearning.com
Kurt VanLehn, School of Computing and Informatics, Arizona State University,
Tempe, AZ, USA
kurt.vanlehn@asu.edu
Abstract. Self-explaining is a domain-independent learning strategy that generally leads to a robust
understanding of the domain material. However, there are two potential explanations for its effectiveness. First,
self-explanation generates additional content that does not exist in the instructional materials. Second, when
compared to comprehension, generation of content increases understanding and recall. An in vivo experiment
was designed to distinguish between these potentially orthogonal hypotheses. Students were instructed to use
one of two learning strategies, self-explaining and paraphrasing, to study either a completely justified example or
an incomplete example. Learning was assessed at multiple time points and levels of granularity. The results were
consistent, favoring the generation account of self-explanation. This suggests that examples should be designed
to encourage the active generation of missing content information.

Keywords. Self-explanation, physics, worked-out examples, problem solving

INTRODUCTION
An important domain-independent learning strategy is self-explaining, which is defined as the sensemaking process that an individual uses to gain a greater understanding of some instructional material,
including texts, worked-out examples, diagrams, and other multimedia materials by explaining it to
themselves (as opposed to another person) (Roy & Chi, 2005). Self-explaining has consistently been
shown to produce learning gains in several domains, including physics (Chi & Bassok, 1989; Chi,
Bassok, Lewis, Reimann, & Glaser, 1989), the human circulatory system (Butcher, 2006; Chi,
DeLeeuw, Chiu, & LaVancher, 1994; Hausmann & Chi, 2002), geometry (V. A. W. M. M. Aleven &
Koedinger, 2002) and many others (Atkinson, Renkl, & Merrill, 2003; McNamara, 2004; McNamara,
Levinstein, & Boonthum, 2004; Moreno, 2006; Renkl, 1997). Moreover, self-explaining has been used
in several different learning contexts, including in the laboratory (Butcher, 2006; Chi, et al., 1989), in
the classroom (McNamara, 2004; McNamara, et al., 2004), with prompting from humans (Chi, et al.,
1994) and from computers (V. A. W. M. M. Aleven & Koedinger, 2002; Conati & VanLehn, 2000;
Hausmann & Chi, 2002).
Although the effect has been widely replicated, it is still not clear why self-explanation works.
Two potential explanations will be addressed in the paper that follows. The first explanation is that
differences in the content are responsible for the increased learning gains. That is, self-explaining
generates additional content that is not present in the instructional materials. The second explanation is
that it is the process, rather than the product, that matters. That is, the process of generating the extra

content is more important for learning than the content itself. Let us provide names for the hypotheses:
the Coverage hypothesis and the Generation hypothesis.
As an illustration, and a brief glimpse into the electrodynamics task domain used in the present
experiment, suppose an example is being presented as a video. The video starts with a problem that
shows an electric field (represented by parallel arrows; see Appendix B) and a positively charged
particle (represented by a dot). The example video draws an arrow with its tail on the dot and says,
“We draw a vector representing the electric force on the particle due to the electric field.” This is a
step in the example, so let us illustrate the process of self-explaining by considering two possible
student responses:
1. Suppose the student says, “OK, so there’s a force on the particle parallel due to the field.”
This does not count as a self-explanation; it is merely a paraphrase of the step.
2. On the other hand, suppose the student says, “The charge is positive, so the force is in the
same direction as the field; if it had been negative, it would be in the opposite direction.”
This does count as a self-explanation because it goes well beyond what the example states.
However, suppose the example video itself said, “The charge is positive, so the force is in the same
direction as the field; if it had been negative, it would be in the opposite direction.” and the student
read it and even paraphrased it. Would simply being aware of this extra content have the same benefits
as generating it? The Coverage hypothesis says that it would, but the Generation hypothesis says that
it would not.
Because examples are often addressed in Cognitive Load Theory (Paas, Renkl, & Sweller, 2003),
it is worth a moment to discuss the theory’s predictions. The theory defines three types of cognitive
load: intrinsic cognitive load is due to the content itself; extraneous cognitive load is due to the
instruction and harms learning; germane cognitive load is due to the instruction and helps learning.
Renkl and Atkinson (2003) note that self-explaining increases measurable cognitive load and also
increases learning, so it must be a source of germane cognitive load. This is consistent with both of our
hypotheses. The Coverage hypothesis suggests that the students are attending to more content, and this
extra content increases both load and learning. The Generation hypothesis suggests that load and
learning are higher when generating content than when comprehending it. In short, Cognitive Load
Theory is consistent with both hypotheses and does not help us discriminate between them.
The two hypotheses are pragmatically important. If the Coverage hypothesis is correct, then
instruction should include examples that are as completely explained as possible—this is what many
instructors tend to do in lectures and texts. On the other hand, if the Generation hypothesis is correct,
then instructors should provide sparse examples and somehow motivate the students to fill in the
explanations. Thus, the Coverage and Generation hypotheses have important implications for the
design of instruction.
Explaining self-explaining
This research focuses on self-explanation of examples. An example is a solved problem, where the
solution is derived in a series of steps (VanLehn, 1996); however, examples are typically incomplete.
Although each step is produced by applying one or more knowledge components, neither the
knowledge components nor the details of their application are mentioned in an incomplete example

(Zhu & Simon, 1987). The student is left to infer them instead. For instance, in the illustration above,
the example asserted that a force existed in a certain direction, and the student had to apply the
definition of electric fields to figure out why. Self-explaining an example consists mostly of applying
knowledge components in order to justify and connect steps in an incomplete solution (Chi &
VanLehn, 1991; VanLehn & Jones, 1993; VanLehn, Jones, & Chi, 1992). The self-explanation effect
is the by-now common finding that students who self-explain incomplete examples learn more than
students who do not (V. Aleven & Koedinger, 2000; Pirolli & Bielaczyc, 1989; Reimann & Neubert,
2000; Renkl, 1997; Renkl, Stark, Gruber, & Mandl, 1998). Although most examples found in
textbooks are incomplete, it is possible to construct complete examples. A complete example justifies
every step in terms of domain principles, definitions, and other knowledge components. This assumes
that the domain has well-defined and accepted principles, definitions, etc. That is arguably the case for
introductory physics, which is the task domain used in this study.
Several discussions of the self-explanation effect also mention the generation effect (Jacoby,
1978; Slamecka & Graf, 1978), which is a well-known finding in the memory literature, wherein
subjects who participate in the generation of paired-associates have a higher probability of recall than
participants who are merely presented with pairs (i.e., the read-only condition). It is quite a leap to
apply a hypothesis about low-level recall to a whole cognitive skill, and its truth is not a foregone
conclusion (for preliminary evidence, see deWinstanley, 1995; deWinstanley & Bjork, 2004;
Peynircioglu & Mungan, 1993). As mentioned earlier, we call this conjecture the Generation
hypothesis. To put it precisely, students learn more when they generate the explanation for example
steps than when they merely read and comprehend the explanations. Thus, complete examples should
be less effective than incomplete examples provided that students self-explain all the steps in the
incomplete examples.
Lovett (1992) tested the Coverage and Generation hypothesis with permutation and combination
problems. Lovett’s 2 x 2 design crossed the source of the solution (subject vs. experimenter) with the
source of the explanation for the solution (subject vs. experimenter). Thus, the experimenter-subject
condition had students study an incomplete example without any prompting for self-explanation. The
experimenter-experimenter condition had students study a complete example. Lovett found that the
subject-experimenter condition was so confusing to the students that they required six times as many
hints—thus, the fact that it produced low learning is not surprising and irrelevant to testing the
hypotheses. Lovett analyzed errors and found that outcome differences among the remaining three
conditions was mostly due to a single knowledge component, called numerator-starting-value (NSV)
in her cognitive task analysis. Students in the experimenter-experimenter condition always heard about
NSV during the experimenter’s explanation, and they mostly applied it successfully during the posttest. About half the students in the experimenter-subject condition mentioned NSV during their
explanations, and only those students applied it during testing. Students in the subject-subject
condition probably had to apply NSV in order to generate the first step in their solutions, and they
almost all applied it successfully again during testing. Thus, it appears that learning occurs if and only
if NSV is either mentioned in experimenter’s explanation, mentioned in students’ explanation, or
applied by the student. This is consistent with the Coverage hypothesis and not the Generation
hypothesis. The Generation hypothesis would predict that when the experimenter mentions NSV,
students are not likely to learn it.
Brown and Kane (1988) found that explanations provided by children between the ages of four
and seven, either spontaneously or in response to prompts, were much more effective at promoting
transfer than those provided by the experimenter. On the face of it, this is consistent with the

Generation hypothesis and not the Coverage hypothesis. However, the students who were told the rule
may not have paid much attention to it, according to Brown and Kane. If so, then the experiment is not
a good test of the hypotheses because the Coverage hypothesis requires that students attend to the
explanations they are given.
Stark (1999) found that students who studied incomplete examples demonstrated stronger
performance on near- and medium-transfer problems than students who studied complete examples.
However, the benefit for incomplete problems did not reach traditional levels of statistical significance
for far-transfer problems (summarized in Renkl, 2002, p. 533). Thus, the evidence from this study is
mixed. The null result for far-transfer problems is consistent with the Coverage hypothesis only. The
positive result for near- and medium-transfer problems is consistent with the Generation hypothesis
only.
Manipulating the completeness of the examples is analogous to studies that compare learning
from differentially elaborated texts. Across several experiments, McNamara et al. (McNamara, 2001;
1996) manipulated the completeness of a text and found that the learning outcomes depended on the
prior knowledge of the students. Low-knowledge students learned best from elaborated texts. This is
consistent with both the Coverage hypothesis and the Generation hypothesis because low-knowledge
students are unlikely to generate the key connecting inferences left unsaid by the unelaborated text. On
the other hand, the high-knowledge students learned better from the unelaborated texts. This is
consistent with the Generation hypothesis and not with the Coverage hypothesis, which would predict
a null result because students read the knowledge components in the elaborated text and they
generated the knowledge components (during self-explanation) for the unelaborated texts. However,
another possible explanation for this result is that the high prior-knowledge students’ reading
strategies were disrupted by the elaborated texts. Because the text supplied the knowledge
components, they simply became passive readers and often failed to attend to the knowledge
components presented by the text. Being passive thereby hurt their comprehension of the text. It also
salvages the Coverage hypothesis, which requires that students attend to the presented knowledge
components, and passive readers often do not do so.
Support for the passive-reader interpretation can be found in a study by Gilabert, Martinez, and
Vidal-Abarca (2005). They designed elaborations for a text that simultaneously increased coherence,
provided causal inferences, and encouraged active processing. They found that both high- and lowprior knowledge students learned more from the elaborated text than the original text. These results are
consistent with the Coverage hypothesis and not with the Generation hypothesis. Moreover, this
finding supports the passive-reader interpretation of the McNamara et al. results, thus undercutting its
support for the Generation hypothesis.
In summary, the experimental record is mixed. Some studies (e.g., Lovett; Gilabert et al.) support
the Coverage hypothesis; some studies (e.g., Brown & Kane; McNamara et al.) support the Generation
hypothesis; and others studies (e.g., Stark et al.) support both. Although the main manipulation was
providing incomplete versus complete examples and texts, other variables surfaced. Students with low
prior knowledge may be unable to self-explain an incomplete text or an example. Students may not
attend to the explanations in complete examples or texts.
Thus, to test the Coverage and the Generation hypotheses, an experiment is needed that controls
for both the students’ prior knowledge and their engagement in self-explaining or comprehending the
examples. The experiments reviewed earlier seem not to have had adequate control of these important
variables.

DESIGN AND PREDICTIONS
The ideal experiment would compare Generation versus comprehension of the same explanations for
example steps. The first challenge is to get students in the generation condition to actually generate
most of the explanations. To do this, we prompted for self-explanations of steps in incomplete
examples because such prompting has been shown to vastly increase the frequency of self-explanation
(Chi, et al., 1994; Renkl, 1997). The second challenge is to get students in the comprehension
condition to attend to the presented justifications, while simultaneously not self-explaining them. To
do this, we had students paraphrase complete examples because paraphrasing provides an overt,
behavioral indication that the individual has at least attended to the instructional materials and tends to
reduce the frequency of self-explanation (Hausmann & Chi, 2002). Thus, the two main conditions are
self-explanation of incomplete examples and paraphrase of complete examples. Let us use SEincomplete and P-complete as the names for these conditions.
If compliance of the participants with the instructions was the same in both conditions, then the
Generation hypothesis predicts that the learning gains of SE-incomplete should be larger than Pcomplete, and the Coverage hypothesis predicts a tie in learning gains. That is, if students in SEincomplete condition generated 90% of the content that they should, and the students in the Pcomplete condition comprehended 90% of the content that they should, then we would expect a tie
(Coverage hypothesis) or a difference in favor of the SE-incomplete condition (Generation
hypothesis). However, suppose it is less likely for students to self-explain than to comprehend, as
seems plausible. That is, suppose students in the SE-incomplete condition generate 75% of the content
that they should, and students in the P-complete condition comprehend 90% of the content that they
should. Then the Coverage hypothesis predicts that the SE-incomplete condition would learn less than
the P-complete condition. Written symbolically, the predictions are:
1. Generation hypothesis: P-complete < SE-incomplete
2. Coverage hypothesis: SE-incomplete ≤ P-complete
As a manipulation check, we included two more conditions: P-incomplete and SE-complete. In
the P-incomplete condition, students are given an incomplete example and prevented from selfexplaining it by being prompted to paraphrase it. Thus, they should neither generate nor comprehend
the target content, so both hypotheses predict that students in the P-incomplete conditions will learn
the least. In the SE-complete conditions, students were prompted to self-explain a complete example.
The self-explanation prompting should act like the active engagement manipulation of Gilabert et al.
(2005), so we’d expect the same amount of self-explanation in SE-complete as in SE-incomplete.
Thus, the Generation hypothesis predicts SE-complete = SE-incomplete. We’d also expect students
who are prompted to self-explain a complete example to comprehend at least as much as students
prompted to paraphrase it. Thus, the Coverage hypothesis predicts P-complete ≤ SE-complete. Here is
a summary of all the predictions:
1. Generation hypothesis:
2. Coverage hypothesis:

P-incomplete
P-incomplete

< P-complete < SE-incomplete
< SE-incomplete ≤ P-complete

= SE-complete
≤ SE-complete

In the results section, we will report measures for all four conditions, with a special focus on
comparing P-complete to SE-incomplete (italicized above), as they are the conditions where the
hypotheses’ predictions differ.
METHOD
LearnLab: Knowledge components
The data were collected in the Pittsburgh Science of Learning Center’s physics LearnLab.1 A
LearnLab is a course that is designed for conducting rigorous, in vivo experiments on issues relating to
robust learning. Robust learning is defined in three parts. First, learning is considered robust when the
knowledge is retained over significant periods of time (i.e., long-term retention). Second, robust
learning is the opposite of inert knowledge in the sense that students are able to broadly apply their
knowledge to other problems within the same class of problems, as well as across different classes of
problems (i.e., far transfer). Finally, robust learning expedites the acquisition of new information (i.e.,
acceleration of future learning). The benefit of collecting data in this environment, as opposed to the
laboratory, is that the realism of the classroom increases the generalizability of the results, without
sacrificing randomization or control over other extraneous variables. Therefore, the results obtained in
a LearnLab classroom should be easily assimilated into non-LearnLab classrooms.
One assumption made by the Pittsburgh Science of Learning Center is that knowledge can be
decomposed into smaller units, which will be referred to as knowledge components (VanLehn, 2006).
A knowledge component is defined as any piece of knowledge that can be learned and applied
independent of other knowledge components. An example of a knowledge component for the domain
of electrodynamics is, “If a charged particle is in an electric field, then there will be an electric force
on the particle due to the field.” Another knowledge component is, “If a particle is positively charged,
then its electric force is parallel to the electric field; otherwise, the electric force is anti-parallel to the
field.” Because the first piece of knowledge can be known without knowing the second, they are both
treated as knowledge components.

Participants
To test the learning of these knowledge components, students were recruited from five, secondsemester, calculus-based physics courses (Physics II: Electricity and Magnetism) taught at the U.S.
Naval Academy. Of the 113 available students, 106 students volunteered and provided informed
consent. Two students, who provided informed consent, were absent the day of the experiment;
therefore, the total sample size was N = 104 students. The student volunteers were given course credit
for their participation.

1

http://www.learnlab.org/learnlabs/physics

Materials
The materials used for the experiment were developed in association with one of the LearnLab
instructors and two other physicists. The domain covered during the experiment was electrodynamics,
with an emphasis on the forces acting on a charged particle located in a region with an electric field. A
complete specification of the problems can be found in Appendix A.
The problems were solved by the participants using the Andes Physics Tutor (Gertner &
VanLehn, 2000; VanLehn, et al., 2005). Andes is an intelligent tutoring system designed to replace the
paper-and-pencil problems located at the end of each chapter (see Appendix B for a screenshot of
Andes). Students are expected to enter each solution step, which is analogous to instructors expecting
that students “show all of their work.” The benefit of using Andes for homework is that students
receive feedback on each step of the solution, as well as on-demand help. The feedback is in the form
of red and green flags that alert the student to incorrect and correct entries, respectively. The assistance
Andes provides is either in response to the student asking what’s wrong with an incorrect entry (i.e.,
what’s wrong help) or the student asking what step should be taken next (i.e., next-step help).
In addition to solving problems with Andes, the students also studied examples. The examples
were videos of a screen-logging program that captured the actions of an expert solving the problems in
Andes. In addition, the examples’ completeness was manipulated by using an audio track that
described each action either with or without a justification for the action. Excluding justifications as a
manipulation of example completeness has been used with success in other studies [33, 40-42]. For
instance, in a complete example, the voice-over included a statement of why the definition of an
electric field equation (F = qE) was used in the solution (see Appendix D).
The second manipulation was the type of study strategy that the students were instructed to use at
the beginning of the experiment and prompted throughout the study. For the self-explaining
conditions, students were given a description of what self-explaining entails, as well as an example of
a hypothetical student self-explaining a concept from first-semester physics (see Appendix C). For the
paraphrasing conditions, they received almost identical instructions, with the major change being a
description of what paraphrasing is and an example of a student paraphrasing the same physical
concept.
In addition to the given descriptions and examples, students were prompted throughout the
experiment to engage in their respective study strategies. The annotated, worked-out examples were
broken into seven to ten segments, depending on the complexity of the problem. At the end of each
segment, the voice-over prompted the student to either, “Please begin your self-explanation” or,
“Please begin your paraphrase.” Below the window where the video was replayed, there were four
prompts that were taken from the instructions (see the bulleted lists in Appendix C). These written
prompts served as an additional cue for the student to engage in a particular study strategy.
The materials were designed with two constraints in mind. First, every example was isomorphic
to the previously solved problem. The only difference between the two was the surface features of the
problem, such as the values of the givens and, in some cases, the directions along which the motion or
force was directed (i.e., the vertical or the horizontal axis). Second, the problems were designed such
that they grew in conceptual complexity (see the right-most column in Appendix A). That is, the
principles from the first problem were also addressed by the second problem, and the second
problem’s principles were included by the third. More importantly, each subsequent problem added a
new principle to the previous problem. This nested structure of problems allowed the experimenters to
track performance on specific knowledge components over time. The implication is that the current

experiment does not conform to the conventional pretest-intervention-posttest design used in most
educational research. Instead, it takes individual knowledge components as the unit of analysis and
tracks the student’s performance over several opportunities to apply them.
Design
The experiment was a 2 x 2 x 3 mixed-factorial design. Two independent variables were crossed.
Study strategy (paraphrase vs. self-explain) and example type (complete vs. incomplete) were
between-subjects variables; whereas problem (PROB1, PROB2, vs. PROB3) was a within-subjects
variable.
Students were block-randomized into condition. That is, care was taken to ensure that students
were randomly assigned to condition under the constraints that grade point average (GPA), prior
Andes usage, major, and the letter grade from the previous physics course (i.e., General Physics I)
were equally represented in each experimental condition. The sample size for each condition was: Pcomplete: paraphrasing complete examples (n = 26), P-incomplete: paraphrasing incomplete examples
(n = 23), SE-complete: self-explaining complete examples (n = 27), and SE-incomplete: selfexplaining incomplete examples (n = 28).
Procedure
As stated previously, participants were recruited from five sections of second-semester physics at the
U.S. Naval Academy (i.e., General Physics II). The experiment took place during one of the class
periods, which was approximately 110 minutes in duration. As students logged into the system at the
start of class, they were randomly assigned to an experimental condition. The system then introduced
students to the experiment and displayed the learning strategy instructions that corresponded with their
experimental condition (see Appendix C). Upon reading the instructions, the students were then
prompted to solve the first problem. The first problem was relatively easy and only required one
principle application (see the practice problem in Appendix A). When the first problem was
completed, the students were then instructed to play the first example video (i.e., EX1). This process,
alternating between solving problems and studying video examples, repeated for three cycles so that,
by the end of the experiment, four problems were solved and three examples were studied. The
procedure of iterating between studying examples and solving problems was analogous to the
Alternating Example condition in Trafton and Reiser (1993).
It should be noted, however, that the student solved a problem first, and then they studied an
isomorphic example afterwards. This sequencing of problems and examples departs from traditional
methods used in prior research (Catrambone & Yuasa, 2006; Sweller & Cooper, 1985; Trafton &
Reiser, 1993). We inverted the isomorphic pairs so that we could use learning curves to plot increasing
competence. That is, for every knowledge component introduced to the students during the
experiment, it first occurred in a problem, then in an isomorphic example. The problem played the role
of a pre-test and furnished the first point on the learning curve. The problem that followed the example
played the role of a post-test or mid-test, and furnished the next point on the learning curve.
While the students were studying the examples, they were prompted to either paraphrase or selfexplain at the end of each segment. To capture their verbalizations, each student wore a pair of
headphones equipped with a close-talk, noise-cancelling microphone (Andrea ANC-750 CTI Stereo
Headset). The headphones and noise cancellation were necessary because each classroom had

approximately 24 students, each studying and solving problems simultaneously. The audio was
digitally recorded and stored on the local machines. In addition to audio, all of the on-screen activity
was recorded using a built-in, screen-logging facility. The following data-streams were created for
each student: a.) an audio track of their verbalizations; b.) a movie of their on-screen activities; and c.)
a text-only log file of each action in the Andes interface; d.) homework log files from the entire
semester; and e.) exam performance on electrodynamics.
The training period ended when the student logged off of the Andes system. Students were free to
leave the training at any time; however, only nine students left early due to scheduling conflicts. The
remaining students left either after the last problem was solved, when the class period was over (i.e.,
after 110 minutes had elapsed), or whichever occurred first.
RESULTS
Equivalent groups check
Before turning to the problem-solving and learning results, it was necessary to ensure that the
randomization procedure was effective in producing equivalent groups. Participants were blockrandomized such that there were initially equal sample sizes for each of the four conditions (i.e., Pincomplete, P-complete, SE-incomplete, SE-complete). Five variables were checked to ensure equal
groups. The variables included GPA (based on a four-point system), major (broken down into three
categories: Engineering, Science, and Other), previous Andes usage (i.e., a subset of the students used
Andes in Physics I: SP211), the letter grade for Physics I, and performance on the practice problem, as
measured by the normalized assistance score.
According to an Analysis of Variance (ANOVA), there was not a statistically reliable difference
between experimental conditions for GPA, F(3, 100) < 1. Furthermore, a chi-squared analysis revealed
that there were no differences between conditions for major (χ2(6, N = 104) = 3.70, p = .72), previous
Andes usage (χ2(3, N = 104) = 1.50, p = .68), nor the first semester physics grade (χ2(12, N = 104) =
11.49, p = .49). An ANOVA on practice problem assistance scores revealed no reliable differences,
F(3, 99) = .37, p = .78. Based on these five metrics, it is reasonable to assume that the experiment
started with equivalent groups. Therefore, the statistical analyses that follow were conducted without
controlling for any of these variables.
When the log files from the homework assignments were analyzed, it was discovered that some
of the students had done homework problems on electric fields before the experiment started;
however, these students turned out to be equally distributed among the conditions, χ2(3, N = 104) =
4.96, p = .18.

Manipulation check
In addition to prior knowledge and ability, we also evaluated whether or not the students followed the
directions outlined at the beginning of the experiment. The students were instructed to alternate
between two activities. They were asked to first solve a practice problem using Andes. Then they were
asked to watch a short video on how to solve an isomorphic problem. The problems were represented
in a list, starting with the first activity at the top and the next activity below it. The results of this study
are based on three assumptions.

First, it is assumed that the students would read the instructions at the beginning of the
experiment. To check this assumption, an analysis of the students’ reading duration of the instructions
was undertaken. It was found that this assumption was essentially correct. Ninety-two percent (96 /
104 = 92%) of the students read the instructions.
Second, it was expected that the students worked on their problems and watch the videos in order.
About one-third of the students (35 / 104 = 34%) completely solved all of the problems and watched
all of the videos in the exact order prescribed by the instructions.
Finally, it was assumed that the students would watch the whole video. Approximately one-third
(101 / 312 = 32%) of the videos were viewed in their entirety. Based on these analyses, the
participants may not have acted in accordance to our assumptions. Therefore, our conclusions drawn
from the data analyses will require a caveat that not all of the videos were completely viewed by all
participants at each step of the experiment.

Example study time and time-to-solution
The amount of time dedicated to studying the examples did not differ between conditions, F(3, 100) =
.31, p = .82. Likewise, the amount of time taken to solve the problems did not differ between
conditions, F(3, 100) = .81, p = .49. The lack of difference in both the time-to-study and the time-tosolution argues against a time-on-task interpretation of the results.

Normalized assistance score
A normalized assistance score was used as our dependent measure to gauge the impact of the
experimental conditions on learning. The normalized assistance score was defined as the sum of the
help requests and errors per problem, divided by the number of entries made in solving that problem.
That is, we count the amount of assistance students receive (summing help requests and immediate
feedback on errors) normalized by the number of opportunities for such assistance. Thus, lower
assistance scores indicate that the student derived a solution while making fewer mistakes and getting
less help, and thus demonstrating better performance and understanding.
Normal learning

Problems solved during the experimental session
As a measure of normal learning (as opposed to robust learning, which was defined earlier),
normalized assistance scores were averaged over individuals for all three problems in the training set
(see Fig. 1). Notice that the y-axis in this figure is inverted from the customary display, so that higher
bars represent less learning. The bars are ordered left-to-right in accord to the predictions of the
Generation hypothesis (see top portion of Table 1). If the Generation hypothesis holds, then the bars
should get shorter as they move rightward. The predictions for the Coverage hypothesis are listed in
the bottom portion of Table 1. If the Coverage hypothesis holds, then the heights of the two middle
bars should be reversed. To test these hypotheses statistically, contrast-coded predictors (found in
parentheses) were created and tested as single degree of freedom, planned comparisons within the
overall repeated-measures ANOVA.

Table 1
The predictions and contrast codes for the two rival hypotheses.

Generation Hypothesis

Contrast Codes
Coverage Hypothesis

Pincomplete

<

(-4)
Pincomplete

P-complete

<

(-2)
<

SEincomplete

SEincomplete

=

(3)
≤

P-

SEcomplete
(3)

≤

SEcomplete

complete
Contrast Codes

(-4)

(-2)

(3)

(3)

The pattern of means for the normalized assistance scores is consistent with the predictions of the
Generation hypothesis (see Fig. 1). A repeated-measures ANOVA confirmed that the visually apparent
difference between the P-incomplete and P-complete conditions were higher than the SE-incomplete
and SE-complete conditions. This relationship was statistically reliable with a medium effect size, F(1,
73) = 6.17, p = .02, ηp2 = .078.2

Partial eta squared (ηp ) is an effect-size measure, which is equivalent to R2 or the variance accounted for by
the predictor variable. Traditional interpretations of partial eta squared are as follows: >.2 is a large effect size;
>.1 is a medium effect size; >.05 is a small effect size (Cohen, 1988).
2

2

Normalized Assistance Score
(Hints + Errors) / Entries

1.20
1.00
0.80
0.60
0.40
0.20
0.98

0.90

0.67

0.67

P-Incomplete

P-Complete

SE-Incomplete

SE-Complete

0.00

Experimental Condition

Fig. 1. Mean normalized assistance scores (± standard error), collapsing across the three training problems.

Andes help usage: Next-step help and bottom-out hints
As stated in a previous section (see Materials, p. 7), the Andes tutoring system offers several different
types and levels of help. For instance, when a user is uncertain as to which step to take next, she is
then able to ask Andes for a hint (i.e., next-step help). Thus, next-step help requests are a measure of
the students’ confusion or uncertainty during problem solving. To assess the extent to which students
relied on the next-step help, we counted the total number of next-step hint requests and divided by the
number of entries made per problem (i.e., the hint rate).
The pattern of results was consistent with the Generation hypothesis (see Fig. 2). To test the
reliability of this pattern, an repeated-measure ANOVA confirmed that the SE-complete and SEincomplete were equal to each other, but requested reliably fewer next-step hints per entry than both of
the paraphrase conditions, F(1, 73) = 8.70, p = .004, ηp2 = .11.

0.60

Hint Rate
(Hints / Entries)

0.50
0.40
0.30
0.20
0.10
0.42

0.35

0.18

0.10

P-Incomplete

P-Complete

SE-Incomplete

SE-Complete

0.00

Experimental Condition

Fig. 2. Mean next-step hint rate (± standard error) across the three training problems.

Once a student has asked for help on a step, the student can then proceed to ask for hints until
Andes tells the user directly what to enter. This last hint will be referred to as a bottom-out hint. Note
that both next-step help and what’s wrong help include bottom-out hints. There was a reliable main
effect of study strategy on bottom-out help usage, which collapsed across both types of hints (i.e.,
what’s wrong and next-step help). The students in the P-complete and P-incomplete conditions
requested marginally more bottom-out hints than the students in the self-explain conditions, F(1, 73) =
5.47, p = .02, ηp2 = .07 (see Fig. 3).

0.16
Bottom-out Hint Rate
(BOH / Entries)

0.14
0.12
0.10
0.08
0.06
0.04
0.02

0.12

0.10

0.06

0.05

P-Incomplete

P-Complete

SE-Incomplete

SE-Complete

0.00

Experimental Condition

Fig. 3. The mean (±SE) frequency of the bottom-out hint rate per experimental condition.

Again, this pattern of results was more consistent with the Generation hypothesis than the
Coverage hypothesis. Furthermore, the results from the next-step help and bottom-out help usage
replicates a finding reported in Catrambone and Yuasa (2006). They found evidence that participants
who were in the “active learning” condition (i.e., those asked to self-explain examples) asked for
fewer hints than the students in the “passive learning” condition (i.e., those asked to study examples).
Robust learning

Far transfer and retention
On an average of 29 days after the completion of the experiment, the students were administered an
exam that covered a subset of the material from the experiment. One exam question was similar to one
of the problems used in the experiment (see “PROB2” and “Chapter Exam” from Appendix A). The
exam question is considered a far-transfer assessment item because the problems from the training set
were in one dimension, while the exam problem included motion in two dimensions. Thus, the chapter
exam question served as both a far-transfer assessment and a retention test.
To analyze the effect of the experimental conditions on exam performance, an ANOVA was
conducted on the midterm exam question, and the score was expressed as a percentage of the total
possible points (20 point). Unfortunately, not every section in the experiment used Andes throughout
the duration of the semester. Because Andes is known to affect learning (VanLehn et al, 2005),
analyses of exam scores were restricted to the three out of the five sections (N = 63 participants) that
used Andes.
Activity and problem type were entered as between-subjects variables. The results were
consistent with neither the Generation (ηp2 = .036) nor the Coverage (ηp2 = .001) hypotheses.

However, a Fischer's least significant difference (LSD) post-hoc analyses revealed a trend with the
SE-complete group (M = 90.83, SD = 9.96) demonstrating a marginally higher score on the chapter
exam question than the P-complete group (M = 73.00, SD = 22.51), (LSD: p = 0.06, ηp2 = .08).
A second measure of retention was the students’ performance on an isomorphic homework
problem that was completed well after the training session (see “Homework: Exam isomorph” in
Appendix A). Although some students complete their homework by the date it is due, others do their
homework later, typically just before an exam. This provides an opportunity for LearnLab researchers
to measure the students’ performance on the homework items at different points in the semester, albeit
knowing that students self-selected when they would do their homework.
We analyzed the students’ performance on a homework problem that was isomorphic to the
chapter exam, in the sense that they shared an identical deep structure (i.e., both analyzed the motion
of a charged particle moving in two dimensions – see Appendix A). The homework problem was
solved after the training session. There was a statistically reliable effect and a medium to large effect
size favoring the Generation hypothesis (see Fig. 4). The SE-incomplete and SE-complete conditions
demonstrated lower normalized assistance scores than both of the paraphrase conditions, F(1, 27) =
4.81, p = .04, ηp2 = .15. This pattern of results replicates and strengthens the finding from the chapter
exam.

Normalized Assistance Score
(Hints + Errors) / Entries

1.40
1.20
1.00
0.80
0.60
0.40
0.20
1.04

0.69

0.45

0.37

P-Incomplete

P-Complete

SE-Incomplete

SE-Complete

0.00

Experimental Condition

Fig. 4. Mean normalized assistance scores (± standard error) for an isomorphic homework problem.

Accelerated future learning
To assess the acceleration of future learning, the chapter on magnetism was selected because some of
its concepts overlap those of the electrodynamics chapter. For example, both include a charged
particle, a force, and a field (either magnetic or electric). A deep understanding of electrodynamics
might accelerate students’ learning of magnetism.

There were significant differences between experimental conditions on the magnetism homework
problem that was most similar to the electrodynamics problems (see Fig. 5). The pattern of data
supported the Generation hypothesis. The students in the P-complete and P-incomplete conditions
demonstrated higher normalized assistance scores than the SE-complete and SE-incomplete
conditions, F(1, 46) = 3.70, p = .06, ηp2 = .075. Once again, this pattern of results was more consistent
with the Generation hypothesis than the Coverage hypothesis.

Normalized Assistance Score
(Hints + Errors) / Entries

1.40
1.20
1.00
0.80
0.60
0.40
0.20
0.91

1.17

0.68

0.75

P-Incomplete

P-Complete

SE-Incomplete

SE-Complete

0.00

Experimental Condition

Fig. 5. Mean normalized assistance scores (± standard error) for a homework problem on magnetism.

Knowledge component analysis
To obtain a finer-grained analysis of learning over time, we focused on knowledge components that
occurred in all three problems.3 Four knowledge components met this criterion, which included: KC1:
applying the definition of the electric field (F = qE); KC2: drawing an electric-field (E-field) vector;
KC3: drawing an electric-force vector; and KC4: defining the charge on a particle. The most important
knowledge component was applying the definition of the electric field (KC1) because it was the main
principle taught in the chapter on electric fields. However, not every student was able to complete all
three problems. Therefore, the data were restricted to those who applied the knowledge components
across all three problems.
To measure performance on each of these knowledge components, we used a repeated-measures
ANOVA on the assistance scores for each of the four knowledge components across the three
problems. Opportunity was entered as the repeated, within-subjects factor, and the contrast between

3

The data from this experiment can be accessed from the Pittsburgh Science of Learning Center DataShop,
which can be found here: https://learnlab.web.cmu.edu/datashop/

the P-complete and SE-incomplete was evaluated at each level of Opportunity. In order to simplify the
analysis, we compared only these two conditions because they are the ones where the Generation
hypothesis and the Content hypothesis make different predictions.

KC1. Applying the definition of the electric field
For the most important knowledge component, KC1, the assistance score decreased for all of the
experimental conditions (Hotelling’s trace; F(2, 29) = 5.26, p = .01). Moreover, the assistance score,
as a function of opportunity, differed between conditions (see Fig. 6). Collapsing across opportunities,
there was a reliable difference between the P-complete and SE-incomplete conditions, F(1, 30) = 4.40,
p = .05, ηp2 = .13. The difference between conditions was largely driven by a reliable difference for
the second opportunity, F(1, 30) = 14.32, p = .001, ηp2 = .32. This pattern of results replicated the
normalized assistance scores findings reported earlier, which were measured at the level of the
experimental session (i.e., collapsing across knowledge components and opportunities). The findings
at this fine-grain size were also consistent with the Generation hypothesis.

KC1 = Apply Def. of Electric Field
4.00
3.75
3.50

(incorrects + hints)

Assistance Score

3.25
3.00
2.75
2.50
2.25
2.00
1.75
1.50
1.25
1.00
0.75
0.50
0.25
0.00

Problem 1

Problem 2

Problem 3

Opportunity
P-Complete

SE-Complete

P-Incomplete

SE-Incomplete

Fig. 6. Assistance score per opportunity to apply KC1, the definition of an electric field.

KC2. Drawing the electric force vector
The pattern of results for the second knowledge component (i.e., KC2: Drawing an electric-force
vector) was slightly different (see Fig. 7). Unfortunately, there was no evidence that the assistance
scores decreased over time (Hotelling’s trace; F(2, 54) = .47, p = .63). However, similar to KC1, the
assistance score differed between conditions when performance was collapsed across opportunities
(see Fig. 6). There was a marginal difference between the P-complete and SE-incomplete conditions
(F(1, 55) = 3.31, p = .07, ηp2 = .06), and the difference was largely driven by a reliable difference for
the first opportunity, F(1, 55) = 4.40, p = .04, ηp2 = .07. This was also consistent with the Generation
hypothesis.

(incorrects + hints)

Assistance Score

KC2 = Draw Electric Force Vector
5.00
4.75
4.50
4.25
4.00
3.75
3.50
3.25
3.00
2.75
2.50
2.25
2.00
1.75
1.50
1.25
1.00
0.75
0.50
0.25
0.00

Problem 1

Problem 2

Problem 3

Opportunity
P-Complete

SE-Complete

P-Incomplete

SE-Incomplete

Fig. 7. Assistance score per opportunity to apply KC2: drawing the electric-force vector.

KC3. Drawing an electric-field vector
A similar main effect was found for drawing the electric-field vector. There was a strong withinsubjects effect of time, with students in all conditions becoming increasingly competent over time

(Hotelling’s trace; F(2, 72) = 26.57, p < .001, ηp2 = .43; see Fig. 8). This is a marked contrast with
KC2, drawing an electric force vector, where the learning curves were more flat. Both knowledge
components involve drawing a vector, which is a complex act that requires filling out a multi-entry
dialogue box. However, the different shapes of their learning curves may differ due to the fact that
drawing a mechanical force vector (e.g., gravity, tension, and friction) is familiar to these students,
whereas drawing a field vector is a more novel concept.
The difference between conditions was consistent with the previous two knowledge components.
The assistance score differed between the P-complete and SE-incomplete conditions when
performance was collapsed across opportunities (F(1, 73) = 4.82, p = .03, ηp2 = .06; see Fig. 6). The
difference was largely driven by a reliable difference for the first opportunity, F(1, 73) = 7.80, p =
.007, ηp2 = .10. Again, the difference between the P-complete and SE-incomplete conditions was
consistent with the Generation hypothesis.

KC3 = Draw Electric Field Vector
6.5
6.0
5.5

(incorrects + hints)

Assistance Score

5.0
4.5
4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0
Problem 1

Problem 2

Problem 3

Opportunity
P-Complete

SE-Complete

P-Incomplete

SE-Incomplete

Fig. 8. Assistance score per opportunity to apply KC3, drawing an electric-field vector.

KC4. Defining the charge on a particle
Finally, for the last knowledge component, defining the charge on a particle, there were no reliable
differences between conditions (all Fs < 1). The assistance scores were, however, lower for KC4 than
the average assistance score for all the other KCs, F(1, 32) = 76.59, p < 0.001, ηp2 = .70.

The reason why this knowledge component was unaffected by the experimental manipulations
was because the students were committing very few errors, even on the first opportunity (M = .59, SD
= .17). Defining the charge on a particle is extremely easy because all of the information is given in
the problem statement. Therefore, there is little surprise why there was a large effect between
knowledge components, yet no differences between experimental conditions.
Analysis of classroom verbal data
To better understand the results, an analysis of the verbal protocols was undertaken. While the
students studied the worked-out examples, they were asked to either self-explain or paraphrase the
content of the instructional materials. As stated previously, every student was outfitted with a closetalk, noise-canceling microphone so that their verbalizations could be recorded, transcribed, and
coded. The data were coded according to the procedure outlined in Chi (1997).

Data selection
Because of the volume of data that verbal protocols generate, we reduced the data to only those
students who watched all the video-based examples and focused on only the verbalizations that
occurred during the example-studying phase of the experiment. That is, we did not analyze the
verbalizations made while solving the problems. Restricting our analyses to these data, 994 episodes
were hand-coded, which constituted approximately eight percent of the overall sample (994 / 12,749 =
7.80%).

Segmentation and coding scheme
The data were segmented according to idea units, which roughly correspond to individual sentences.
Coding was conducted in three passes through the verbal data (see Fig. 9). Within studying examples,
the talk was coded as either a self-explanation or paraphrase (Pass 1). Self-explanations and
paraphrases were decomposed by their topic (Pass 2). One topic was the user interface. The Andes
user interface is nontrivial, and both the videos and the students sometimes discuss how to use it. In
contrast, students’ talk was coded as reflecting content when it was directly tied to the instructional
content either as a part of the student’s prior knowledge, or embedded in the video-based example.
Finally, self-explanations were further categorized as being either meta-cognitive or justificationbased (Pass 3). The purpose for the final coding category was to explore differences between
conditions that might be due to the meta-cognitive prompts used in the self-explanation condition (see
Appendix C).

Example Studying
(n = 994)
Self-explanation

Paraphrase

(n = 280)

(n = 714)

Pass 1

User Interface

Content

User Interface

Content

(n = 30)

(n = 250)

(n = 260)

(n = 454)

Metacognitive

Justificationbased

(n = 194)

(n = 86)

Pass 2

Pass 3

Fig. 9. The coding scheme used to categorize student utterances while studying examples.

For the first pass (Pass 1), the coding scheme used the following four criteria to distinguish selfexplanations from paraphrases (see Table 2). First, if the segment’s content went beyond the
information presented in the current step of the worked-out problem, it was coded as self-explanation.
Second, segments with meta-cognitive comments were also counted as a self-explanation. Third, if the
student raised a question during the segment in response to the content of the step, the segment was
coded as self-explanation. Finally, the segment was coded as a self-explanation if the student made
some integrative statement with either their prior domain-relevant knowledge or previously presented
material. Finally, if none of the above criteria were met, and the statement merely repeated what was
in the example, then the segment was coded as a paraphrase.

Table 2
The definitions and examples for the third level of coding.

Code

Definition

Example

Self-explanation
(user-interface)

A meta-cognitive statement or
justification that goes beyond the
information provided in the example
that deals with the interface.

So he just put 'E' out in the middle of
nowhere. That's interesting. I guess it
really kind of doesn't really matter. I
don't know. Huh.

Self-explanation
(content)

A meta-cognitive statement or
justification of the physics content.

I have no earthly idea what goes...
what's going on with Andes right at this
point. So, I'm still a little confused.

Paraphrase
(user-interface)

A restatement of the example step that
deals primarily with how to
accomplish an action in the user
interface.

So, I read the problem and assign a
variable, which is 'P.' Click on the little
dot in the left-hand corner. Drag it
down, left click, a box comes up. You
select particle, click 'OK.'

Paraphrase

A restatement of the physics content.

'F' equals 'Q E' and solve for 'F.' Force
is defined on the body... uh... the same
direction that 'E' is.

(content)

Coding-scheme results
The results for the coding scheme are presented in the next three sections, corresponding to each of the
three passes through the data. For each pass, the results are reported in a table that summarizes the
descriptive statistics using the proportions of each code.4 On the other hand, to test for differences
4

For example, the 994 segments that comprise the data of Level 1 (see Table 3, bottom row) were first
partitioned according to condition (e.g., there were 213 segments in the P-complete condition; see Table 3, top
row) and then the counts for each code in that condition were divided by the total number of segments in that
condition.

between experimental conditions, the amount of total talk was controlled statistically using an
ANCOVA. The reason for reporting the results as proportions in tabular formats is because they are
easier to interpret than the estimated marginal means that are generated by the ANCOVA.
Pass 1: Paraphrasing vs. Self-explaining. In an effort to detect if the experimental manipulation
had its intended effects, the average number of paraphrase and self-explanation segments were
contrasted for each of the four conditions. A post-hoc analysis revealed that the SE-incomplete
condition produced more self-explanation segments than both the P-incomplete (LSD: p = .05) and Pcomplete (LSD: p < .03) conditions (see Table 3). This suggests that the SE vs. P manipulation was
successful.
Moreover, the SE-complete condition produced marginally more paraphrases than the SEincomplete condition, F(1, 73) = 3.38, p = 0.07. This suggests that students in both SE conditions
articulated justifications of steps as intended, but because those justifications were mentioned in the
complete examples, such articulations were counted as paraphrases for the SE-complete students and
as self-explanations for the SE-incomplete students. This is consistent with the intended effect of the
complete vs. incomplete manipulation. In short, the Pass 1 coding suggests that both experimental
manipulations were operating as intended.
Table 3
The mean number of paraphrases and self-explanation episodes for each experimental condition.

n

Paraphrase

Self-explanation

P-Incomplete

23

165 / 213 = 0.77a

48 / 213 = 0.23a

P-Complete

26

244 / 321 = 0.76a

77 / 321 = 0.24a

SE-Incomplete

28

125 / 219 = 0.57b

94 / 219 = 0.43b

SE-Complete

27

180 / 241 = 0.75

61 / 241 = 0.25

Total

104

714 / 994 = 0.72

280 / 994 = 0.28

Note. Means in columns with different subscripts differ reliably at p < .05 by Fisher’s least significant
difference test.

Pass 2: User-interface vs. Content. For the second pass through the verbal data, paraphrases and
self-explanations were partitioned into user-interface vs. content statements. There were two reliable
effects. First, a post-hoc analysis revealed that the SE-incomplete condition produced more userinterface self-explanation segments than both the P-incomplete (LSD: p < .04) and P-complete (LSD:
p = .003) conditions (see Table 4). If details of the user interface are easily overlooked, which seems
likely, then asking students to self-explain may bring the user-interface details to the students’
attention, whereas asking students to paraphrase may not increase the salience of such details.
Second, across all conditions, there were more content self-explanations than user interface selfexplanations (250 vs. 30). This suggests that, although the Andes user interface is nontrivial, the
physics content was even more challenging. This supports our assumption that students are mostly
learning physics in this experiment, and that user-interface learning was a minor source of variance.
Table 4
The mean number of user interface and content paraphrases and self-explanation episodes
for each experimental condition.

SE

SE

Paraphrase

Paraphrase

n

User Interface

Content

User Interface

Content

P-Incomplete

23

5 / 48 = 0.10a

43 / 48 = 0.90

66 / 165 = 0.40

99 / 165 = 0.60

P-Complete

26

4 / 77 = 0.05a

73 / 77 = 0.95

69 / 244 = 0.28

175 / 244 = 0.72

SE-Incomplete

28

15 / 94 = 0.16b

79 / 94 = 0.84

50 / 125 = 0.40

75 / 125 = 0.60

SE-Complete

27

6 / 61 = 0.10

55 / 61 = 0.90

75 / 180 = 0.42

105 / 180 = 0.58

Total

104

30 / 280 = 0.11

250 / 280 = 0.89 260 / 714 = 0.36 454 / 714 = 0.64

Note. Means with different subscripts differ reliably at p < .05 by Fisher’s least significant difference
test.
Pass 3: Meta-cognitive vs. Justification-based. For the final pass through the data, only the selfexplanations segments were decomposed and coded as being either meta-cognitive or justificationbased (see Table 5). There were a total of 86 justification episodes and 194 meta-cognitive episodes
across the four conditions. A post-hoc analysis revealed that the SE-incomplete condition produced
more justification-based self-explanation episodes than the P-complete (LSD: p = .02) and marginally
more than the P-incomplete (LSD: p = .11) conditions. This suggests that the students in the SEincomplete were in fact actively trying to fill in the missing information from the examples, as we
intended that they should. The SE-incomplete group also produced more justification-based selfexplanations segments than the SE-complete group (LSD: p = .02). In short, these results suggest
again that the students’ verbalizations are consistent with the activities that they were requested to
perform.

Table 5
The mean meta-cognitive and justification-based self-explanation episodes that
focused on user-interface elements.

SE-

SE-

n

Justification

Meta-cognitive

P-Incomplete

23

17 / 86 = .198

31 / 194 = .160

P-Complete

26

23 / 86 = .267

54 / 194 = .278

SE-Incomplete

28

34 / 86 = .395

60 / 194 = .309

SE-Complete

27

12 / 86 = .140

49 / 194 = .253

Total

104

86 / 86 = 1.00

194 / 194 = 1.00

None of the pair-wise contrasts were statistically reliable for the meta-cognitive self-explanations.
However, the overall results from the third pass (the “Total” line of Table 5) suggest that the focus of
most self-explanations was targeted more toward meta-cognitive reflections of one’s current state of
understanding and comprehension (194 episodes) than filling in the missing justifications (86
episodes). This makes sense because mentioning justifications was suppressed in one condition (Pincomplete) and counted as paraphrasing in two conditions (SE-complete and P-complete).
Coding scheme summary
The pattern of results for the verbal protocols suggests a few conclusions. First, it may have been more
challenging to self-explain than paraphrase the material. If we collapse across experimental conditions,
we find that the average number of paraphrase segments (M = 6.87, SD = 8.50) is higher than the
average number of self-explanation segments (M = 2.69, SD = 5.19). It may be that the instructions to
self-explain were too difficult or that paraphrasing is the first step toward producing high quality selfexplanations.
Second, when students successfully self-explained, the focus of their statements was targeted
toward physics content. This was encouraging because some students were new to the Andes interface.
Given that most self-explanation segments were about the content may be one of the reasons why they
performed well on the problem-solving tasks (e.g., the training set, homework problems, and the exam
question).

DISCUSSION
Self-explanation can be viewed both as a process (i.e., the activity of generation) and as an outcome
(i.e., new content). Although many believe that active learning strategies, such as self-explanation,
necessarily increase learning from worked-out examples, a confound is introduced for self-explanation
in that it makes students aware of content that is not contained in the original example. In order to
tease apart these two explanations, we defined two hypotheses. The Generation hypothesis claims that
generating content makes is better understood and easier to recall than comprehending it. The
Coverage hypothesis claims that generation vs. comprehension makes no difference; the benefits of
self-explanation are due to increasing the content experienced by self-explainers compared to non-selfexplainers.
To test these hypotheses, we had students to engage in two different learning strategies. The first
was self-explaining, which is a classic active learning strategy. The second was paraphrasing, which
might also be considered an active learning strategy because the student is at least attempting to put
into his or her own words the content of the example. Yet paraphrasing, by definition, does not
produce any new content.
Orthogonal to the learning strategy, we manipulated the content of what was studied. The
examples that the students studied were either completely or incompletely justified. The completely
justified examples articulated the reasons for taking each problem-solving step. The incompletely
justified examples, on the other hand, omitted the necessary reasons for the step and merely explained
the mechanics of taking the step in the learning environment (i.e., Andes).
Ideally, students who paraphrased complete examples should experience the same set of
justifications as the students who self-explained incomplete examples. Thus, these two groups would
become aware of the same content, but one would have generated it while the other would have
comprehended it. If the Coverage hypothesis holds, then these two groups should have the same
learning gains. If the Generation hypothesis holds, then the students who self-explain incomplete
examples should learn more than the students who paraphrase complete examples.
The results were remarkably consistent across two grain sizes of analysis (i.e., problem-set vs.
knowledge-component), across situations (i.e., experimental session, homework, and exams), across
assessments (i.e., near transfer, far transfer, and accelerated future learning topics), and across
dependent measures (i.e., normalized assistance scores, errors, and help usage). The results usually
supported the Generation hypothesis. Students in the experimental condition who were instructed and
prompted to self-explain while studying the examples demonstrated lower errors and requested fewer
hints than the students who merely paraphrased the information. This suggests that generation is an
important feature of self-explaining. In particular, students who paraphrased completely justified
examples did not learn more than students who self-explained incompletely justified examples, which
would occur if the Coverage hypothesis were true and so were certain plausible assumptions about the
relative frequency of paraphrasing and self-explaining (see the Introduction).
Limitations of the study
There are a few limitations to the current study. First, students were not required to watch every frame
of every video, and only about a third of them did so. Perhaps this occurred because the new content
tended to occur at the beginning of the set of videos, so some students stopped watching when the
content became familiar. Another reason might be due to the students’ preferred method of learning.

For instance, some students may prefer to try a problem themselves first. That is, they prefer to learn
on their own and from their own mistakes. Alternatively, other students prefer to attempt to solve a
problem only after they have observed a worked example. The materials were structured such that the
examples came after the problems were solved. This may have violated their traditional study
methods.
The prompts given to the students were meta-cognitive in nature, and did not explicitly remind
students to produce justifications for steps. The prompts were selected from Chi et al. (1994) because
they successfully induced self-explanation of a biology text. The prompts mainly focused on the
reflection of knowledge, instead of the production of justifications. In a follow-up study, we plan to
contrast meta-cognitive prompts with justification prompts in order to investigate if they produce
different behaviors.
Finally, one qualification should be made with respect to the completeness manipulation. The
students could flesh out the incompletely justified examples by using the help system during problem
solving. That is, Andes presents the hints in a graded fashion. The first hint is supposed to serve as a
reminder for which step to apply. If the student asks for a second hint on the same step, it provides a
reason for taking the step. The last hint in the sequence directly tells the student which action to take.
Therefore, the second-level of help may have washed out our completeness manipulation. This
possibility will be the focus of future analyses.
Conclusions
The results of the current experiment were remarkably consistent across problems, time, and
granularity. The evidence favors the Generation hypothesis, which states that the robust learning that
results from self-explanation is largely due to the active generation of the missing information in the
form of justifications for the application of each problem-solving step. These results suggest that new
learning technologies should be designed to encourage students to actively generate missing
information.
ACKNOWLEDGEMENTS
Funding for this research is provided by the National Science Foundation, Grant Number SBE0354420 to the Pittsburgh Science of Learning Center (PSLC, http://www.learnlab.org). Portions of
the results were presented at the 13th International Conference on Artificial Intelligence in Education
Conference. The authors would like to thank Anders Weinstein and Dale Walters for their help
integrating the screen-capturing software into Andes; Michael A. Ringenberg and Min Chi for their
expertise in setting up the databases; Anders Weinstein, Adaeze Nwaigwe, Alida Skogsholm, and
Benjamin Billings for their help in migrating Andes log files into the DataShop, Robert Shelby and
Brett van de Sande for their assistance in developing the materials; and Donald J. Treacy, John J.
Fontanella, and Mary C. Wintersgill for graciously allowing us to collect data in their classroom.

REFERENCES
Aleven, V., & Koedinger, K. R. (2000). Limitations of student control: Do students know when they need help?
In G. Gauthier, C. Frasson & K. VanLehn (Eds.), Proceedings of the 5th International Conference on
Intelligent Tutoring Systems, ITS 2000 (pp. 292-303). Berlin: Springer Verlag.
Aleven, V. A. W. M. M., & Koedinger, K. R. (2002). An effective metacognitive strategy: Learning by doing
and explain with a computer-based Cognitive Tutor. Cognitive Science, 26, 147-179.
Atkinson, R. K., Renkl, A., & Merrill, M. M. (2003). Transitioning from studying examples to solving problems:
Effects of self-explanation prompts and fading worked-out steps. Journal of Educational Psychology,
95(4), 774-783.
Brown, A. L., & Kane, M. J. (1988). Preschool children can learn to transfer: Learning to learn and learning
from example. Cognitive Psychology, 20(4), 493-523.
Butcher, K. R. (2006). Learning from text with diagrams: Promoting mental model development and inference
generation. Journal of Educational Psychology 98(1), 182-197.
Catrambone, R., & Yuasa, M. (2006). Acquisition of procedures: The effects of example elaborations and active
learning exercises. Learning and Instruction, 16(2), 139-153.
Chi, M. T. H. (1997). Quantifying qualitative analysis of verbal data: A practical guide. The Journal of the
Learning Sciences, 6(3), 271-315.
Chi, M. T. H., & Bassok, M. (1989). Learning from examples via self-explanations. In L. B. Resnick (Ed.),
Knowing, learning, and instruction: Essays in honor of Robert Glaser (pp. 251-282). Hillsdale, NJ:
Lawrence Erlbaum Associates, Inc.
Chi, M. T. H., Bassok, M., Lewis, M. W., Reimann, P., & Glaser, R. (1989). Self-explanations: How students
study and use examples in learning to solve problems. Cognitive Science, 13, 145-182.
Chi, M. T. H., DeLeeuw, N., Chiu, M.-H., & LaVancher, C. (1994). Eliciting self-explanations improves
understanding. Cognitive Science, 18, 439-477.
Chi, M. T. H., & VanLehn, K. A. (1991). The content of physics self-explanations. The Journal of the Learning
Sciences, 1(1), 69-105.
Cohen, J. (1988). Statistical power analysis for the behavioral sciences (2 ed.). Hillsdale, NJ: Lawrence
Earlbaum Associates, Inc.
Conati, C., & VanLehn, K. (2000). Toward computer-based support of meta-cognitive skills: A computational
framework to coach self-explanation. International Journal of Artificial Intelligence in Education, 11, 398415.
deWinstanley, P. A. (1995). A generation effect can be found during naturalistic learning. Psychological Bulletin
& Review, 2(4), 538-541.
deWinstanley, P. A., & Bjork, E. L. (2004). Processing strategies and the generation effect: Implications for
making a better reader. Memory & Cognition, 32(6), 945-955.
Gertner, A., & VanLehn, K. (2000). Andes: A coached problem solving environment for physics. In Gauthier,
Frasson & K. VanLehn (Eds.), Intelligent Tutoring Systems: 5th International Conference (pp. 133-142).
Berlin: Springer.
Gilabert, R., Martinez, G., & Vidal-Abarca, E. (2005). Some good texts are always better: Text revision to foster
inferences of readers with high and low prior background knowledge. Learning and Instruction, 15(1), 4568.

Hausmann, R. G. M., & Chi, M. T. H. (2002). Can a computer interface support self-explaining? Cognitive
Technology, 7(1), 4-14.
Jacoby, L. L. (1978). On interpreting the effects of repetition: Solving a problem versus remembering a solution
Journal of Verbal Learning and Verbal Behavior, 17(6), 649-667.
Lovett, M. C. (1992). Learning by problem solving versus by examples: The benefits of generating and receiving
information Proceedings of the Fourteenth Annual Conference of the Cognitive Science Society (pp. 956961). Hillsdale, NJ: Erlbaum.
McNamara, D. S. (2001). Reading both high-coherence and low-coherence texts: Effects of text sequence and
prior knowledge. Canadian Journal of Experimental Psychology, 55(1), 61-62.
McNamara, D. S. (2004). SERT: Self-explanation reading training. Discourse Processes, 38(1), 1-30.
McNamara, D. S., Kintsch, E., Songer, N., & Kintsch, W. (1996). Are good texts always better? Interactions of
text coherence, background knowledge, and levels of understanding in learning from text. Cognition and
Instruction, 14(1), 1-43.
McNamara, D. S., Levinstein, I. B., & Boonthum, C. (2004). iSTART: Interactive strategy training for active
reading and thinking. Behavioral Research Methods, Instruments, and Computers, 36, 222-233.
Moreno, R. (2006). When worked examples don't work: Is cognitive load theory at an Impasse? Learning and
Instruction, 16(2), 170-181.
Paas, F., Renkl, A., & Sweller, J. (2003). Cognitive load theory and instructional design: Recent developments.
Educational Psychologist, 38(1), 1-4.
Peynircioglu, Z. F., & Mungan, E. (1993). Familiarity, relative distinctiveness, and the generation effect.
Memory & Cognition, 21(3), 367-374.
Pirolli, P., & Bielaczyc, K. (1989). Empirical analyses of self-explanation and transfer in learning to program. In
C. M. Olson & E. E. Smith (Eds.), Proceedings of the 11th Annual Conference of the Cognitive Science
Society (pp. 450-457). Hillsdale, NJ: Erlbaum Associates, Inc.
Reimann, P., & Neubert, C. (2000). The role of self-explanation in learning to use a spreadsheet through
examples. Journal of Computer Assisted Learning, 16, 316-325.
Renkl, A. (1997). Learning from worked-out examples: A study on individual differences. Cognitive Science,
21(1), 1-29.
Renkl, A. (2002). Worked-out examples: Instructional explanations support learning by self-explanations.
Learning and Instruction, 12(5), 529–556.
Renkl, A., & Atkinson, R. K. (2003). Structuring the transition from example study to problem solving in
cognitive skill acquisition: A cognitive load perspective. Educational Psychologist, 38(1), 15-22.
Renkl, A., Stark, R., Gruber, H., & Mandl, H. (1998). Learning from worked-out examples: The effects of
example variability and elicited self-explanations. Contemporary Educational Psychology, 23(1), 90-108.
Roy, M., & Chi, M. T. H. (2005). The self-explanation principle in multimedia learning. In R. E. Mayer (Ed.),
The Cambridge handbook of multimedia learning (pp. 271-286). Cambridge: Cambridge University Press.
Slamecka, N. J., & Graf, P. (1978). The generation effect: Delineation of a phenomenon. Journal of
Experimental Psychology: Human Learning and Memory, 4(6), 592-604.
Stark, R. (1999). Lernen mit Lösungsbeispielen. Einfluß unvollständiger Lösungsbeispiele auf
Beispielelaboration, Motivation und Lernerfolg Bern, Switzerland: Huber.
Sweller, J., & Cooper, G. A. (1985). The use of worked examples as a substitute for problem solving in learning
algebra. Cognition and Instruction, 2(1), 59-89.

Trafton, J. G., & Reiser, B. J. (1993). The contributions of studying examples and solving problems to skill
acquisition Proceedings of the Fifteenth Annual Conference of the Cognitive Science Society (pp. 10171022). Hillsdale, NJ: Erlbaum.
VanLehn, K. (1996). Cognitive skill acquisition. In J. T. Spence, J. M. Darly & D. J. Foss (Eds.), Annual Review
of Psychology (pp. 513-539). Palo Alto, Ca: Annual Reviews, Inc.
VanLehn, K. (2006). The behavior of tutoring systems. International Journal of Artificial Intelligence in
Education, 16, 227-265.
VanLehn, K., & Jones, R. M. (1993). Learning by explaining examples to oneself: A computational model. In S.
Chipman & A. L. Meyrowitz (Eds.), Foundations of knowledge acquisition: Cognitive models of complex
learning (pp. 25- 82). Boston: Kluwer.
VanLehn, K., Jones, R. M., & Chi, M. T. H. (1992). A model of the self-explanation effect. The Journal of the
Learning Sciences, 2(1), 1-59.
VanLehn, K., Lynch, C., Schultz, K., Shapiro, J. A., Shelby, R., Taylor, L., et al. (2005). The Andes physics
tutoring system: Lessons learned. International Journal of Artificial Intelligence and Education, 15(3),
147-204.
Zhu, X., & Simon, H. A. (1987). Learning mathematics from examples and by doing. Cognition and Instruction,
4(3), 137-166.

APPENDIX A
Electrodynamics problem statements used for problem solving, example studying, homework and an
in-class chapter exam.
Activity

Problem Statement

Practice

SOLVE

EX1

STUDY

PROB1

SOLVE

A charged particle is in a region where there is an electric
field E of magnitude 17.8 V/m at an angle of 37 degrees
above the positive x-axis. If the charge on the particle is
3.4 C, find the magnitude of the force on the particle P
due to the electric field E.
A charged particle is in a region where there is an electric
field E of magnitude 14.3 V/m at an angle of 22 degrees
above the positive x-axis. If the charge on the particle is 7.9 C, find the magnitude of the force on the particle P
due to the electric field E.
An electron (qe = -1.60e-19 C; me = 9.11e-31 kg) is in a
region where there is a uniform electric field E. The force
on the electron due to the electric field exactly cancels its
weight near the Earth's surface. If the y-component of the
net force on the particle near Earth due to the electric
field and gravity is zero, what is the magnitude (and
show the direction) of the electric field E?
A charged particle (q = 52.0 mC) is in a region where
there is a uniform electric field E of magnitude 120 N/C
at an angle of 90 degrees above the positive x-axis. If the
y-component of the net force on the particle near Earth
due to the electric field and gravity is zero, what is the
mass of the particle?
An electron (qe = -1.60e-19 C; me = 9.11e-31 kg) is in a
region, between two parallel charged plates, that produce
a uniform electric field E of magnitude 2.0e+4 N/C. The
separation between the plates is 1.5 cm. The electron
undergoes a constant acceleration from rest near the
negative plate and passes through a tiny hole in the
positive plate (see Figure below). Find the velocity of the
electron as it leaves the hole. In this problem, gravity can
be ignored.

Problem

EX2

PROB2

STUDY

SOLVE

Principle
Fe = qE

Fe = qE

Fe = qE
Fw = mg
Fe + Fw = 0

Fe = qE
Fw = mg
Fe + Fw = 0

Fe = qE
Fe = ma
v12 = v02 + 2ad

EX3

PROB3

Chapter
Exam

STUDY

SOLVE

SOLVE

Question
Homework: SOLVE
Exam
isomorph

Homework: SOLVE
Magnetism

A proton (qp = 1.6e-19 C; mp = 1.7e-27 kg) is in a
region where there is a uniform electric field E of
magnitude 320 N/C, directed along the positive x-axis.
The proton accelerates from rest and reaches a speed of
1.20e+5 m/s. How long does it take the proton to reach
this speed? In this problem, gravity can be ignored.
An electron (qe = -1.60e-19 C; me = 9.11e-31 kg) is in a
region where there is a uniform electric field E of
magnitude 4.0e-12 N/C, directed along the negative yaxis. The electron is moving in the positive y-direction at
an initial velocity of 4.3 m/s. How far will the electron
travel before it comes to rest? In this problem, please
include gravity.
An electron enters a region of space where the vector
electric field is E = (2i + 3j)x 10^4 N/C. If the initial
velocity of the electron when it entered the field was V0
= 4x10^6i m/s what is its velocity 6E-6s after entering
the field?
A fully ionized alpha particle with a charge of 3.2 E-19 C
and a mass of 6.64 E-27 kg is initially moving in the x
direction with a velocity of 1.5 E3 m/s. The particle
enters a region where there is a uniform electric field in
the positive y direction with a magnitude of 2.5 E2 N/C.
What will be the velocity of the particle after it has
moved 0.5 m in the x direction?
A particle with a charge of -3.2e-6 C enters a region with
a velocity given by v = (56 m/s) i. The uniform magnetic
field in the region is given by B = -(0.15 T) k, and there
is a uniform electric field pointing downward in the
vertical direction. If the particle is to keep moving in a
straight line, what must be the magnitude of the electric
field?

Fe = qE
Fe = ma
v12 = v02 + 2ad

Fe = qE
Fw = mg
Fe + Fw = mg
v12 = v02 + 2ad
Fe = qE
Fe = ma
v1 = v0 + at
Fe = qE
Fe = ma
v1 = v0 + at

FB = vqB
Fe + FB = 0
Fe = qE

APPENDIX B
A screenshot of the Andes Physics Tutor

Flag Feedback

Equation Cheat Sheet

Bottom-out Hint

Skill Est. Score

APPENDIX C
Self-explanation and paraphrase instructions and examples
Self-explanation Instructions
In this experiment, we would like you to study a few worked-out examples. The examples are
presented one step at a time so that you will have time to really think about what information
each step provides and how this relates to what you've already seen.
We would like you to watch and listen to each step and then explain what it means to you.
That is:


What new information does each step provide for you?



How does it relate to what you've already seen?



Does it give you a new insight into your understanding of how to solve the problems?



Does it raise a question in your mind?

Tell us whatever is going through your mind - even if it seems unimportant.

Self-explanation Example
In this case, a hypothetical student is studying how to solve a statics problem. The example
reads: "Consider the knot at the junction of the three strings to be 'the body'" and is
accompanied by the following diagram:

At first, this confuses the student
because she initially thought that the
block should be the body. She says:
"I thought the block would be the
body. The weight force is acting
down and the strings are pulling up.
Oh I see, the sum of the forces is zero
at the knot, that's why it should be the
body."
The student in this example provides
a reason or explanation for choosing
the knot. This is the essence of selfexplaining.

Paraphrase Instructions
In this experiment, we would like you to study a few worked-out examples. The examples are
presented one step at a time so that you will have time to really think about what information
each step provides and how this relates to what you've already seen.
We would like you to watch and listen to each step and then paraphrase what it means in your
own words. That is:



How can I put this in my own words?



How can I say this in a different way?



In other words:_____________________.



What's another way that I could put this?

Tell us whatever is going through your mind - even if it seems unimportant.
Paraphrase Example
In this case, a hypothetical student is studying how to solve a statics problem. The example
reads: "Consider the knot at the junction of the three strings to be 'the body'" and is
accompanied by the following diagram:

The student tries to restate that the
knot should be the body. She says:
"The knot would be the body. It is in
the middle of the three strings."
The student in this example restates
the reasoning for choosing the knot.
This is the essence of paraphrasing.

APPENDIX D
Completely-justified and Incompletely-justified Examples

The completely justifications are in bold, which were omitted from the incompletely justified
examples. This appendix only includes the first (of three) examples that were presented to the
students.

This is example number one.

We are going to start by reading the problem statement:

A charged particle is in a region where there is an electric field E of magnitude 14.3 V/m at an angle of
22 degrees above the positive x-axis. If the charge on the particle is -7.9 C, find the magnitude of the
force on the particle P due to the electric field E.

The first step in solving this problem is to choose a body. In this case the body of interest is the charged
particle. To draw the body, we select the Body Tool from the Diagram Toolbar, place the cursor in a
convenient position in the Diagram Window and left-click. This brings up a dialog box in which we
select our body. We choose the charged particle because both of the forces mentioned in the
problem affect the particle. Next, we assign it a name. In this case, we will use “p” to refer to the
particle.

[ PROMPT ]

The second step is to draw a coordinate system. To draw the axis, we select the Axis Tool from the
Diagram Toolbar and we click and drag. This brings up a dialog box in which we select the Orientation
of our axis. In this case, we will use a zero degree orientation. We choose an un-rotated axis because
all of the motion is going to be straight up and down.

[ PROMPT ]

For the next few steps, we are going to enter the scalar and then the vector givens mentioned in the
problem statement. The only scalar given is the charge on the particle. To assign a charge on the
particle, we right-click in the Variable Window and select “Add new variable”. This brings up a
submenu with several options. Because we are defining the charge on the particle, we select
“charge.” This brings up a dialog box. Here we select the body on which the charge is located. In this
case, the charge is on the particle, and we use the default name, “q” as the name for the charge. We also
know the value because it is mentioned in the problem statement, so we enter that in the next field [
paste: -7.9C ].

[ PROMPT ]

The next given is the magnitude and direction of the electric field. An electric field is a vector, so we
need to indicate the direction in the free-body diagram. To draw the electric field, select the Electric
Field tool from the vector tools in Diagram toolbar, and we place the cursor in the diagram window,
click and drag. This brings up a dialog box. The electric field is in a region and it is due to some
unspecified source. It is unspecified because the source of the electric field is not mentioned in the
problem statement. The field is acting at only one time point, and the problem statement says that the
orientation is 22 degrees in the plane. We will use the default name “E” to refer to the electric field. We
also know the magnitude, so we can assigned it by typing it in the Equation Window [ paste:
E=14.3N/C ]

[ PROMPT ]

Now that all the given information has been entered, we need to apply our knowledge of physics to
solve the problem.

One way to start is to ask ourselves, “What quantity is the problem seeking?” In this case, the answer is
the magnitude of the force on the particle due to the electric field.

We know that there is an electric field. If there is an electric field, and there is a charged particle
located in that region, then we can infer that there is an electric force on the particle. The
direction of the electric force is in the opposite direction as the electric field because the charge
on the particle is negative.

We use the Force tool from the vector tool bar to draw the electric force. This brings up a dialog box.
The force is on the particle and it is due to some unspecified source. We do know, however, that the
type of force is electric, so we choose “electric” from the pull-down menu. For the orientation, we need
to add 180 degrees to 22 degrees to get a force that is in a direction that is opposite of the direction of
the electric field. Therefore we put 202 degrees. Finally, we use “Fe” to designate this as an electric
force.

[ PROMPT ]

Now that the direction of the electric force has been indicated, we can work on finding the magnitude.
We must choose a principle that relates the magnitude of the electric force to the strength of the
electric field, and the charge on the particle. The definition of an electric field is only equation
that relates these three variables. We write this equation, in the equation window.

[ PROMPT ]

Now that all of the equations have been entered, we can solve for the unknown. To do so, right click
anywhere in the equation window and select the sought for quantity. In this case, we’re solving for the
electric force and Andes gives us a value so we can cut and paste the answer in the answer field. When
the answer turns green, we know that we have the answer correct.

That is the conclusion of Example 1

DO MICRO-LEVEL TUTORIAL DECISIONS
MATTER: APPLYING REINFORCEMENT
LEARNING TO INDUCE PEDAGOGICAL
TUTORIAL TACTICS

by
Min Chi
B.S., Xi’an Jiaotong University, 1999
M.S., University of Pittsburgh, 2006

Submitted to the Graduate Faculty of
the Intelligent Systems Program in partial fulfillment
of the requirements for the degree of
Doctor of Philosophy

University of Pittsburgh
2009

UNIVERSITY OF PITTSBURGH
INTELLIGENT SYSTEMS PROGRAM

This dissertation was presented
by

Min Chi

It was defended on
Nov 20th 2009
and approved by
Diane Litman, Professor, Intelligent Systems Program & Department of Computer Science
Kurt VanLehn, Professor, Department of Computer Science and Engineering, ASU
Peter Brusilovsky, Associate Professor, Intelligent Systems Program & School of Library
and Information Science
Marek Druzdzel, Associate Professor, Intelligent Systems Program & School of Library and
Information Science
Jack Mostow, Professor, Robotics Institute & Department of Machine Learning
Department, CMU
Dissertation Director: Diane Litman, Professor, Intelligent Systems Program &
Department of Computer Science

ii

DO MICRO-LEVEL TUTORIAL DECISIONS MATTER: APPLYING
REINFORCEMENT LEARNING TO INDUCE PEDAGOGICAL TUTORIAL
TACTICS
Min Chi, PhD
University of Pittsburgh, 2009

In this dissertation, I investigated applying a form of machine learning, reinforcement learning, to induce tutorial tactics from pre-existing data collected from real subjects. Tutorial
tactics are policies as to how the tutor should select the next action when there are multiple
ones available at each step. In order to investigate whether micro-level tutorial decisions
would impact students’ learning, we induced two sets of tutorial tactics: the “Normalized
Gain” tutorial tactics were derived with the goal of enhancing the tutorial decisions that
contribute to the students’ learning while the “Inverse Normalized Gain” ones were derived
with the goal of enhancing those decisions that contribute less or even nothing to the students’ learning. The two sets of tutorial tactics were compared on real human participants.
Results showed that when the contents were controlled so as to be the same, different tutorial
tactics would indeed make a difference in students’ learning gains. The “Normalized Gain”
students out-performed their “Inverse Normalized Gain” peers. This dissertation sheds some
light on how to apply reinforcement learning to induce tutorial tactics in natural language
tutoring systems.

iii

TABLE OF CONTENTS

PREFACE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xvii
1.0 INTRODUCTION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.1

RESEARCH QUESTIONS . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.1.1

1.1.2

1
7

Question 1: Do Micro-level Pedagogical Tutorial Decisions Affect
Students’ Learning? . . . . . . . . . . . . . . . . . . . . . . . . . . .

7

1.1.1.1

Background on Pedagogical Tutorial Tactics . . . . . . .

7

1.1.1.2

Elicit/Tell . . . . . . . . . . . . . . . . . . . . . . . . . .

11

1.1.1.3

Justify/Skip-Justify . . . . . . . . . . . . . . . . . . . . .

14

Question 2: Is Reinforcement Learning a Feasible Method to Induce
Tutorial Tactics? . . . . . . . . . . . . . . . . . . . . . . . . . . . .

17

1.1.2.1

Previous research about applying RL in ITSs . . . . . . .

17

1.1.2.2

Applying RL to Dialogue Systems vs. Natural Language
Tutoring Systems . . . . . . . . . . . . . . . . . . . . . .

1.1.2.3

19

Whether RL Is Able To Induce Effective Tutorial Tactics
Is Still An Open Question. . . . . . . . . . . . . . . . . .

21

GENERAL APPROACH . . . . . . . . . . . . . . . . . . . . . . . . . . . .

24

2.0 CORDILLERA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

27

1.2

2.1

STUDENT INTERFACE

. . . . . . . . . . . . . . . . . . . . . . . . . . .

28

2.2

WIZARD INTERFACE . . . . . . . . . . . . . . . . . . . . . . . . . . . .

29

2.3

AN EXAMPLE SCRIPT . . . . . . . . . . . . . . . . . . . . . . . . . . . .

30

3.0 REINFORCEMENT LEARNING PROCEDURE . . . . . . . . . . . . .

34

3.1

REINFORCEMENT LEARNING FOR TUTORIAL TACTICS . . . . . .
iv

35

3.2

ISSUE 1: TRAINING CORPUS . . . . . . . . . . . . . . . . . . . . . . . .

36

3.3

ISSUE 2: KNOWLEDGE COMPONENTS . . . . . . . . . . . . . . . . . .

37

3.3.1

Identified KCs in the Selected Domain . . . . . . . . . . . . . . . .

38

3.3.2

Tutorial Dialogue Annotation . . . . . . . . . . . . . . . . . . . . .

39

KC-BASED MDPS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

42

3.4.1

Issue 3: State Representation . . . . . . . . . . . . . . . . . . . . .

42

3.4.1.1

Sub-Issue 1: Feature Choices . . . . . . . . . . . . . . . .

43

3.4.1.2

Sub-Issue 2: Feature Discretization . . . . . . . . . . . .

43

3.4.1.3

Sub-Issue 3: Feature Selection . . . . . . . . . . . . . . .

43

3.4.1.4

Sub-Issue 4: Maximum Number of Features . . . . . . . .

43

3.4.2

KC-based Action . . . . . . . . . . . . . . . . . . . . . . . . . . . .

44

3.4.3

Issue 4: KC-based Reward . . . . . . . . . . . . . . . . . . . . . . .

44

3.5

INDUCE KC-GENERAL POLICIES . . . . . . . . . . . . . . . . . . . . .

44

3.6

TETREAULT AND LITMAN’S RL TOOLKIT . . . . . . . . . . . . . . .

45

3.6.1

Expected Cumulative Reward (ECR) . . . . . . . . . . . . . . . . .

46

3.6.2

Confidence Interval . . . . . . . . . . . . . . . . . . . . . . . . . . .

47

3.6.3

An Example to Illustrate ECR and CI . . . . . . . . . . . . . . . .

48

3.7

ISSUE 5: CONFLICTING POLICIES . . . . . . . . . . . . . . . . . . . .

49

3.8

DISCUSSION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

50

3.4

4.0 STUDY 1: EXPLORATORY CORPUS
4.1

. . . . . . . . . . . . . . . . . . .

52

METHODS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

52

4.1.1

Participants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

52

4.1.2

Materials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

53

4.1.2.1

32 Knowledge Components . . . . . . . . . . . . . . . . .

53

4.1.2.2

Physics Textbook . . . . . . . . . . . . . . . . . . . . . .

53

4.1.2.3

Pre- and Posttest . . . . . . . . . . . . . . . . . . . . . .

53

4.1.2.4

Domain Principles . . . . . . . . . . . . . . . . . . . . . .

54

4.1.2.5

Seven Training Problems . . . . . . . . . . . . . . . . . .

55

4.1.2.6

Training Scripts . . . . . . . . . . . . . . . . . . . . . . .

55

4.1.2.7

Random-Cordillera . . . . . . . . . . . . . . . . . . . . .

56

v

4.1.2.8

Human Wizards . . . . . . . . . . . . . . . . . . . . . . .

4.1.2.9

Some Clarification On The Number Of KCs Appearing In

56

This Dissertation . . . . . . . . . . . . . . . . . . . . . .

56

4.1.3

Procedure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

56

4.1.4

Grading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

57

4.1.5

Measures

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

58

RESULTS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

60

4.2.1

Time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

60

4.2.2

Learning Results . . . . . . . . . . . . . . . . . . . . . . . . . . . .

61

4.2.3

Exploratory Corpus . . . . . . . . . . . . . . . . . . . . . . . . . . .

63

4.2.3.1

Overall Characteristics . . . . . . . . . . . . . . . . . . .

63

4.2.3.2

KC-based Characteristics . . . . . . . . . . . . . . . . . .

64

DISCUSSION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

66

5.0 STUDY 2: DICHOTIC GAIN (DICHGAIN) GROUP . . . . . . . . . .

67

4.2

4.3

5.1

5.2

APPLY RL TO INDUCE DICHGAIN POLICIES . . . . . . . . . . . . . .

68

5.1.1

Training Corpus . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

68

5.1.2

Knowledge Components . . . . . . . . . . . . . . . . . . . . . . . .

68

5.1.3

KC-based Reward . . . . . . . . . . . . . . . . . . . . . . . . . . . .

69

5.1.4

State Representation . . . . . . . . . . . . . . . . . . . . . . . . . .

69

5.1.4.1

Feature Choices . . . . . . . . . . . . . . . . . . . . . . .

70

5.1.4.2

Maximum Number of Features . . . . . . . . . . . . . . .

76

5.1.4.3

Feature Discretization . . . . . . . . . . . . . . . . . . . .

76

5.1.4.4

Feature Selection . . . . . . . . . . . . . . . . . . . . . .

77

5.1.5

Conflicting Policies . . . . . . . . . . . . . . . . . . . . . . . . . . .

78

5.1.6

Summary: Procedure of Inducing Tutorial Tactics in Study 2 . . . .

78

METHODS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

80

5.2.1

Participants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

80

5.2.2

Materials & Procedures . . . . . . . . . . . . . . . . . . . . . . . .

81

5.2.3

Grading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

81

5.2.4

Measures

81

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
vi

5.3

RESULTS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

83

5.3.1

Compare Pre- and Post-test . . . . . . . . . . . . . . . . . . . . . .

83

5.3.2

Post-hoc Comparison: DichGain vs. Exploratory . . . . . . . . . .

84

5.3.2.1

Post-hoc Comparison: DichGain vs. Exploratory On Training Time . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.3.2.2

5.3.3

Post-hoc Comparison: DichGain vs. Exploratory On Learning Performance . . . . . . . . . . . . . . . . . . . . . . .

85

Post-hoc Comparison: DichGain vs. Exploratory Tutorial Corpora .

88

5.3.3.1

Post-hoc Comparison: DichGain vs. Exploratory On Overall Tutorial Decisions . . . . . . . . . . . . . . . . . . . .

5.3.3.2

5.4

84

88

Post-hoc Comparison: DichGain vs. Exploratory On Individual KCs . . . . . . . . . . . . . . . . . . . . . . . . .

88

DISCUSSION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

94

6.0 APPLYING RL TO INDUCE NORMALIZED GAIN (NORMGAIN)
AND INVERSE NORMALIZED GAIN (INVNORMGAIN) TUTORING TACTICS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

96

6.1

TRAINING CORPUS . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

97

6.2

KNOWLEDGE COMPONENTS . . . . . . . . . . . . . . . . . . . . . . .

98

6.3

KC-BASED REWARD . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

99

6.4

STATE REPRESENTATION . . . . . . . . . . . . . . . . . . . . . . . . . 101
6.4.1

Sub-issues 1: Feature Choices . . . . . . . . . . . . . . . . . . . . . 102
6.4.1.1

Autonomy — five features . . . . . . . . . . . . . . . . . 103

6.4.1.2

Temporal Situation — three features . . . . . . . . . . . 104

6.4.1.3

Problem Solving Contextual — fifteen features . . . . . . 104

6.4.1.4

Performance — twelve features . . . . . . . . . . . . . . . 107

6.4.1.5

Background — five features . . . . . . . . . . . . . . . . 109

6.4.1.6

Student Dialogue — ten features . . . . . . . . . . . . . . 110

6.4.1.7

Simplified Example of Deriving Fifty Features from Log
Files. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111

6.4.2

Sub-issues 2: Maximum Number of Features . . . . . . . . . . . . . 112
vii

6.4.3

Sub-issues 3: Feature Discretization . . . . . . . . . . . . . . . . . . 112

6.4.4

Sub-issues 4: Feature Selection . . . . . . . . . . . . . . . . . . . . 114
6.4.4.1

RL-based Feature Selection . . . . . . . . . . . . . . . . . 115

6.4.4.2

PCA-based Feature Selection . . . . . . . . . . . . . . . . 117

6.4.4.3

PCA and RL-based Feature Selection . . . . . . . . . . . 120

6.4.4.4

Random Feature Selections . . . . . . . . . . . . . . . . . 122

6.5

CONFLICTING POLICIES . . . . . . . . . . . . . . . . . . . . . . . . . . 124

6.6

SUMMARY: INDUCTION of TUTORIAL TACTICS in STUDY 3 . . . . 125

6.7

INDUCED POLICIES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
6.7.1

Source Training Corpus . . . . . . . . . . . . . . . . . . . . . . . . 128

6.7.2

Number of Features . . . . . . . . . . . . . . . . . . . . . . . . . . . 129

6.7.3

Feature Choices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131

6.7.4
6.8

6.7.3.1

Autonomy Features . . . . . . . . . . . . . . . . . . . . . 131

6.7.3.2

Temporal Situation Features . . . . . . . . . . . . . . . . 132

6.7.3.3

Problem Solving Contextual Features . . . . . . . . . . . 133

6.7.3.4

Performance Features . . . . . . . . . . . . . . . . . . . . 134

6.7.3.5

Background Features . . . . . . . . . . . . . . . . . . . . 134

6.7.3.6

Student Dialogue Features . . . . . . . . . . . . . . . . . 134

Feature Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136

SUMMARY: RL in STUDY 3 . . . . . . . . . . . . . . . . . . . . . . . . . 137

7.0 STUDY 3: NORMALIZED GAIN (NORMGAIN) VS. INVERSE NORMALIZED GAIN (INVNORMGAIN) . . . . . . . . . . . . . . . . . . . . 139
7.1

7.2

METHODS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
7.1.1

Participants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139

7.1.2

NormGain-Cordillera and InvNormGain-Cordillera . . . . . . . . . 139

7.1.3

Materials & Procedures . . . . . . . . . . . . . . . . . . . . . . . . 140

7.1.4

Grading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140

7.1.5

Measures

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140

RESULTS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
7.2.1

Time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
viii

7.2.2

Learning Performance . . . . . . . . . . . . . . . . . . . . . . . . . 141
7.2.2.1

Compare NormGain vs. InvNormGain conditions: Overall Learning Performance . . . . . . . . . . . . . . . . . . 141

7.2.2.2

Compare NormGain vs. InvNormGain Conditions: KCbased Learning Performance . . . . . . . . . . . . . . . . 143

7.2.2.3
7.2.3

7.3

Summary of Learning . . . . . . . . . . . . . . . . . . . . 147

Log Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
7.2.3.1

Overall Tutorial decision Steps . . . . . . . . . . . . . . . 149

7.2.3.2

Comparing I-ratio Across Primary KCs . . . . . . . . . . 149

7.2.3.3

Comparing J-ratio Across Primary KCs . . . . . . . . . . 150

7.2.3.4

Summary of Log Analysis . . . . . . . . . . . . . . . . . . 151

DISCUSSION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151

8.0 GENERAL DISCUSSION AND CONCLUSIONS . . . . . . . . . . . . . 153
8.1

8.2

POST-HOC COMPARISON . . . . . . . . . . . . . . . . . . . . . . . . . . 153
8.1.1

STUDY VARIATIONS . . . . . . . . . . . . . . . . . . . . . . . . . 153

8.1.2

LEARNING PERFORMANCE . . . . . . . . . . . . . . . . . . . . 155

8.1.3

LEARNING PERFORMANCE ACROSS THE FOUR GROUPS . 162

8.1.4

LOG ANALYSIS . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
8.1.4.1

I-Ratio . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163

8.1.4.2

Justify Ratio . . . . . . . . . . . . . . . . . . . . . . . . . 164

REVISITING THE TWO RESEARCH QUESTIONS . . . . . . . . . . . . 166
8.2.1

Question 1: Micro-level Pedagogical Tutorial Decisions Affect Students’ Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166

8.2.2

Question 2: Reinforcement Learning is a Feasible Method to Induce
Tutorial Tactics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167

9.0 CONTRIBUTIONS AND FUTURE WORK . . . . . . . . . . . . . . . . 169
9.1

CONTRIBUTION TO COGNITIVE & LEARNING SCIENCE . . . . . . 169

9.2

CONTRIBUTIONS TO AI&ED, ITS & EDM . . . . . . . . . . . . . . . . 172

9.3

FUTURE WORK . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173

APPENDIX A. KNOWLEDGE COMPONENTS . . . . . . . . . . . . . . . . 176
ix

APPENDIX B. GRADING . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
B.1 GRADING PROCEDURE . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
B.2 INTER-GRADER AGREEMENT . . . . . . . . . . . . . . . . . . . . . . . 186
APPENDIX C. BACKGROUND SURVEY . . . . . . . . . . . . . . . . . . . . 187
C.0.1 Instructions: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
C.0.2 Questions: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
APPENDIX D. TEXTBOOK . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
D.0.2.1

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 191

D.0.2.2

Physical Quantities . . . . . . . . . . . . . . . . . . . . . 192

D.0.2.3

Mass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193

D.0.2.4

Displacement . . . . . . . . . . . . . . . . . . . . . . . . 194

D.0.2.5

Velocity . . . . . . . . . . . . . . . . . . . . . . . . . . . 195

D.0.2.6

Acceleration . . . . . . . . . . . . . . . . . . . . . . . . . 196

D.0.2.7

Gravitational Acceleration . . . . . . . . . . . . . . . . . 197

D.0.2.8

Force . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198

D.0.2.9

Weight Force . . . . . . . . . . . . . . . . . . . . . . . . . 199

D.0.2.10 Normal Force . . . . . . . . . . . . . . . . . . . . . . . . 200
D.0.2.11 Friction Force . . . . . . . . . . . . . . . . . . . . . . . . 201
D.0.2.12 Introduction to energy . . . . . . . . . . . . . . . . . . . 202
D.0.2.13 Kinetic energy . . . . . . . . . . . . . . . . . . . . . . . . 203
D.0.2.14 Potential energy . . . . . . . . . . . . . . . . . . . . . . . 204
D.0.2.15 Gravitational potential energy . . . . . . . . . . . . . . . 205
D.0.2.16 Spring potential energy . . . . . . . . . . . . . . . . . . . 206
D.0.2.17 Total mechanical energy . . . . . . . . . . . . . . . . . . 208
D.0.2.18 Isolated and non-isolated systems . . . . . . . . . . . . . 210
D.0.2.19 Conservation of Mechanical Energy for isolated systems . 211
D.0.2.20 Internal forces vs. external forces . . . . . . . . . . . . . 213
D.0.2.21 Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214
D.0.2.22 Net work . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
x

D.0.2.23 Conservation of Mechanical Energy for systems whose nonisolation is due to forces . . . . . . . . . . . . . . . . . . 216
D.0.2.24 Different choices of system . . . . . . . . . . . . . . . . . 218
D.0.2.25 Conservative and non-conservative forces . . . . . . . . . 219
D.0.2.26 Internal forces must be conservative . . . . . . . . . . . . 220
D.0.2.27 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
APPENDIX E. PRE- AND POSTTEST QUESTIONS. . . . . . . . . . . . . 222
APPENDIX F. PRE- AND POST-TEST KCS . . . . . . . . . . . . . . . . . . 236
APPENDIX G. TRAINING PROBLEMS . . . . . . . . . . . . . . . . . . . . . 238
APPENDIX H. AN EXAMPLE OF STUDENT-CORDILLERA LOG FILE 241
APPENDIX I. AN EXAMPLE TUTORIAL SCRIPT . . . . . . . . . . . . . 263
APPENDIX J. STUDY 2: TUTORIAL FEATURES . . . . . . . . . . . . . . 315
APPENDIX K. STUDY 2: DICHGAIN TUTORIAL TACTICS . . . . . . . 317
APPENDIX L. STUDY 3: EXAMPLE LOG 50 FEATURES . . . . . . . . . 329
APPENDIX M. STUDY 3: NORMGAIN AND INVNORMGAIN TUTORIAL TACTICS (FEATURES) . . . . . . . . . . . . . . . . . . . . . . . . . 333
APPENDIX N. STUDY 3: NORMAGAIN TUTORIAL TACTICS (POLICIES) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336
APPENDIX O. STUDY 3: INVNORMGAIN TUTORIAL TACTICS (POLICIES) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 344
BIBLIOGRAPHY . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353

xi

LIST OF TABLES

2.1 A Sample Cordillera Script . . . . . . . . . . . . . . . . . . . . . . . . . . . .

31

3.1 Example of Tutorial Dialogue with KC Labels . . . . . . . . . . . . . . . . .

40

4.1 Major Principles of Work and Energy . . . . . . . . . . . . . . . . . . . . . .

54

4.2 Seven Training Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

55

4.3 Exploratory Students’ Learning Performance . . . . . . . . . . . . . . . . . .

62

4.4 Overall Characteristics On Tutorial Decisions in Exploratory Corpus . . . . .

63

4.5 KC-based Exploratory Corpus . . . . . . . . . . . . . . . . . . . . . . . . . .

65

5.1 A Simplified Example of Part of Student Log on Training Problem P4 . . . .

75

5.2 Autonomy Features Updated . . . . . . . . . . . . . . . . . . . . . . . . . . .

76

5.3 Compare DichGain Tactics With Tutorial Tactics Under New Feature Selection Methods On Eight Primary KCs . . . . . . . . . . . . . . . . . . . . . .

78

5.4 DichGain Students’ Pre- vs. Post-test Performance . . . . . . . . . . . . . . .

83

5.5 DichGain vs. Exploratory Scores: Pre vs. Post-Test (No Q20 ) . . . . . . . . .

86

5.6 Overall Tutorial Decision Characteristics: DichGain vs. Exploratory Corpora

89

5.7 Tutorial Decisions Per KC. . . . . . . . . . . . . . . . . . . . . . . . . . . . .

90

5.8 I-Ratio Between DichGain vs. Exploratory on a per-KC basis. . . . . . . . .

92

5.9 Justify Ratio Differences on a per-KC Basis. . . . . . . . . . . . . . . . . . .

93

6.1 Compare Three Corpus on Eight Primary KCs . . . . . . . . . . . . . . . . . 100
6.2 An Example PCA Feature Set from the Exploratory Corpus Induced for A
KC-general Tutorial Tactics . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
6.3 Issue-by-Issue Comparison of Studies 2 and 3 . . . . . . . . . . . . . . . . . . 127
6.4 The Source Training Corpus Of the Inducing 34 Tutorial Tactics . . . . . . . 129
xii

6.5 The Complexity of the 34 Induced Tutorial Tactics . . . . . . . . . . . . . . . 130
6.6 Distribution of Policy Sizes. . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
6.7 Occurrence of Autonomy Features in The Final Tutorial Tactics . . . . . . . 131
6.8 Occurrence of Temporal Situation Features in The Final Tutorial Tactics . . 132
6.9 Occurrence of Problem Solving Contextual Features in The Final Tutorial
Tactics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
6.10 Occurrence of Performance Features in The Final Tutorial Tactics . . . . . . 135
6.11 Occurrence of Student Dialogue Features in The Final Tutorial Tactics . . . . 136
6.12 Applying 11 Feature Selection Methods to Induce 34 Tutorial Tactics . . . . 137
7.1 NormGain vs. InvNormGain on Pre- and Post-Test . . . . . . . . . . . . . . 144
7.2 KC-based Pre- and Post-Test Test Scores . . . . . . . . . . . . . . . . . . . . 145
7.3 Between-Group Comparison by KC-based Pre- and Post-Test Scores . . . . . 146
7.3 Between-Group Comparison by KC-based Pre- and Post-Test Scores . . . . . 147
7.4 Overall Characteristics of Tutorial Decisions in Exploratory Corpus . . . . . 148
7.5 Compare NormGain vs. InvNormGain on I-ratio Across Eight Primary KCs

150

7.6 Compare NormGain vs. InvNormGain on J-ratio across Eight Primary KCs

151

8.1 Compare Four Groups Under the Overall Grading Criteria . . . . . . . . . . 157
8.1 Compare Four Groups Under the Overall Grading Criteria . . . . . . . . . . 158
8.1 Compare Four Groups Under the Overall Grading Criteria . . . . . . . . . . 159
8.2 Compare Four Groups Under the Cumulative KC-based Grading Criteria . . 160
8.2 Compare Four Groups Under the Cumulative KC-based Grading Criteria . . 161
8.2 Compare Four Groups Under the Cumulative KC-based Grading Criteria . . 162
8.3 Pairwise Comparison Among Four Groups On I-ratio . . . . . . . . . . . . . 164
8.4 Pairwise Comparison Among Four Groups On J-ratio . . . . . . . . . . . . . 165
8.5 Pairwise Comparison Among Four Groups On Number of Justification Steps

165

A1 Individual (not net) forces . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
A2 Choosing a system for COE . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
A3 Individual (not net) work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
A4 Net work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180
A5 Individual (not net) mechanical energies . . . . . . . . . . . . . . . . . . . . . 181
xiii

A6 COE, TME and isolated/non-isolated . . . . . . . . . . . . . . . . . . . . . . 182
A7 Kinematics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
C1 Questions

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188

C2 High School . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
C3 Advanced Placement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
C4 College-level Math . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
C5 College-level physics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
F1 KC Occurrences by Pre- & Post-test problem. . . . . . . . . . . . . . . . . . 237
H1 An Example of Student Log on Training Problem P4 . . . . . . . . . . . . . 241
H1 An Example of Student Log on Training Problem P4 . . . . . . . . . . . . . 242
H1 An Example of Student Log on Training Problem P4 . . . . . . . . . . . . . 243
H1 An Example of Student Log on Training Problem P4 . . . . . . . . . . . . . 244
H1 An Example of Student Log on Training Problem P4 . . . . . . . . . . . . . 245
H1 An Example of Student Log on Training Problem P4 . . . . . . . . . . . . . 246
H1 An Example of Student Log on Training Problem P4 . . . . . . . . . . . . . 247
H1 An Example of Student Log on Training Problem P4 . . . . . . . . . . . . . 248
H1 An Example of Student Log on Training Problem P4 . . . . . . . . . . . . . 249
H1 An Example of Student Log on Training Problem P4 . . . . . . . . . . . . . 250
H1 An Example of Student Log on Training Problem P4 . . . . . . . . . . . . . 251
H1 An Example of Student Log on Training Problem P4 . . . . . . . . . . . . . 252
H1 An Example of Student Log on Training Problem P4 . . . . . . . . . . . . . 253
H1 An Example of Student Log on Training Problem P4 . . . . . . . . . . . . . 254
H1 An Example of Student Log on Training Problem P4 . . . . . . . . . . . . . 255
H1 An Example of Student Log on Training Problem P4 . . . . . . . . . . . . . 256
H1 An Example of Student Log on Training Problem P4 . . . . . . . . . . . . . 257
H1 An Example of Student Log on Training Problem P4 . . . . . . . . . . . . . 258
H1 An Example of Student Log on Training Problem P4 . . . . . . . . . . . . . 259
H1 An Example of Student Log on Training Problem P4 . . . . . . . . . . . . . 260
H1 An Example of Student Log on Training Problem P4 . . . . . . . . . . . . . 261
H1 An Example of Student Log on Training Problem P4 . . . . . . . . . . . . . 262
xiv

J1

Representing Sample Dialogue in Table 5.2 Using the 18 Feature Variables
From Study 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316

L1 Student Autonomy Features . . . . . . . . . . . . . . . . . . . . . . . . . . . 329
L2 Problem Solving Context . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 330
L3 Background Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 330
L4 Student Dialogue Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . 330
L5 Temporal Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331
L6 Performance Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 332
M1 NormGain Tutorial Tactics: . . . . . . . . . . . . . . . . . . . . . . . . . . . . 334
M2 InvNormGain Tutorial Tactics: . . . . . . . . . . . . . . . . . . . . . . . . . . 335

xv

LIST OF FIGURES

1.1 A Training Problem: P4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4

1.2 An example Dialog with Cordillera

. . . . . . . . . . . . . . . . . . . . . . .

6

1.3 Elicit vs. Tell . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

13

1.4 Justify vs. Skip-justify . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

16

2.1 Student Interface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

28

2.2 Students’ Response Classification Window in Wizard Interface . . . . . . . .

30

3.1 ECR and CI Sample Learned policies . . . . . . . . . . . . . . . . . . . . . .

49

3.2 General RL Procedure For Inducing KC-based Tutorial Tactics . . . . . . . .

51

4.1 An Example of Three Grading Criteria . . . . . . . . . . . . . . . . . . . . .

59

4.2 Average Time Spent Per Training Problem By the Exploratory Group . . . .

61

4.3 Learning Performance on Exploratory Group . . . . . . . . . . . . . . . . . .

62

5.1 The Induced Policy πDichGain (KC21 , ET ): Gravitational Potential Energy . .

79

5.2 Learning Performance of Exploratory Group . . . . . . . . . . . . . . . . . .

84

5.3 Per Problem Time Comparison: DichGain vs. Exploratory Group . . . . . .

85

7.1 Compare Time Between NormGain vs InvNormGain Groups On Training
Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
8.1 Compare Four Groups Learning Performance under Overall Grading . . . . . 155

xvi

PREFACE

First and foremost I would like to thank my advisor Kurt VanLehn. There are so many
things I have learned from him. It would take a life time to thank him. A Chinese proverb
says: “once a mentor, lifetime like a father,” which is what I want to express here. Kurt is
not just a mentor or advisor for me. He is like a father to me especially because, like my
father, every time I thought he was wrong, it turned out I was wrong. I thank him for his
wisdom, sagacious guidance, unconditional support, and frequent encouragement over the
years. I feel especially lucky to have worked in the strong research environment Kurt and
Professor Micki Chi have built. I have met so many brilliant people over all these years and
received so much help and advice from them.
I also want to give my special thanks to Professor Diane Litman. I want to thank her for
agreeing to be my committee chair. Despite the disadvantage of not having Kurt around,
she has taken the role of being a full advisor for me during the past year. Without her
astute knowledge, guidance, great help, and wonderful information, I do not think that this
year would have gone so smoothly for me. This project would not have happened without
help from the whole VanLehn group and ITSPOKE group. I especially owe a lot to Dr.
Pamela Jordan, who helped me to form the research questions and plans. She has been like
a mentor to me for all these years. Pam and the old ITR group deserve a lot of credit for
this project. I want to thank her for all the great advice and suggestions over all these years.
She volunteered to help run my subjects and presented the paper for me during the summer.
I would also like to thank the rest of my committee for taking the time out of their busy
lives to help me do this and for always being so willing to negotiate times, places, etc. I
really appreciate their insightful comments and questions. My other committee members
are Dr. Peter Brusilovsky, Dr. Marek Druzdzel, and Dr. Jack Mostow. Additionally, I want
xvii

to give my special thanks to Professor Kevin Ashely. He appears in almost every important
moment in my life at University of Pittsburgh and has given me so much valuable advice and
suggestions. Quite a lot of people have helped with my project in particular: Art Ward (who
helped greatly when I was running subjects and writing my dissertation); Scott Silliman and
Moses Hall (who helped me greatly on setting up the labs and system and more importantly,
helped me to relax); Z and Linn Taylor (I feel so lucky to have them in my life; they helped
me in so many aspects that it would take the whole page to list); Joel Tetreault (who helped
to set up the code for this project); Bob Hausmann and Leslie Hausmann (who have given me
so much help, encouragement and support); Michael Ringenberg (I enjoyed our discussion
on cognitive science and learning science, and I learned greatly from him); Hua Ai (who
has helped me with some of the important citations and gave me encouragements); and Xie
Wei and Yuan Changhe (who have been my friends for all these years and have constantly
supported my work). I also owe a lot to Kurt’s other graduate students: Chad Lane, Noboru
Matsuda, Chas Murray, and Stephanie Siler. I really missed our weekly graduate meetings.
Even though they graduated several years ago, whenever we meet, we still feel like brothers
and sisters. Thank you for all the help and support you have provided to me for all these
years. There are also many people in the Intelligent System Program and Learning Research
Development Center who have helped me for all these years: Prof. Janyce Wiebe, Wendy
Bergstein, Patsy Guzzi, Eric Fussenegger, Jo-Anne Krevy, De Ivanhoe and many others on
the second and fifth floors.
Finally, and most importantly, without my family, I most definitely would not have been
able to finish this dissertation. My dearest parents and foster parents have been always
so supportive while I worked toward my Ph.D. They never complained that I could not be
around with them for all these years. They have always been there to support me, love me,
and worry about me. I want to also say thank you to my sister, who has taken full responsibly
of caring for my parents and foster parents for all these years. My parent-in-laws, Edie and
Tim, who have always there for me and support me unconditionally. Kate, who has always
been so much fun to talk to. She is so encouraging and sends me all kinds of funny things
to make my dissertation life easier. Finally, I am dedicating this dissertation and the effort
to my husband, Collin, who got no more sleep than I did for all these months and has been
xviii

my best friends for years. Meeting and marrying him is one of the most wonderful things
that has happened in my life.

xix

1.0

INTRODUCTION

Human one-on-one tutoring is one of the most effective educational interventions. Tutored
students often perform significantly better than students in classroom settings [Bloom, 1984,
Cohen et al., 1982]. Computer learning environments that mimic aspects of human tutors
have also been highly successful. Intelligent Tutoring Systems (ITSs) have been shown to
be highly effective in improving students’ learning in real classrooms [Anderson et al., 1995,
Koedinger et al., 1997, VanLehn, 2006].
The development of ITSs has enabled schools and universities to reach out and educate
students who otherwise would be unable to take advantage of one-on-one tutoring due to
cost and time constraints [Koedinger et al., 1997]. Despite the high payoffs provided by
ITSs, significant barriers remain. High development costs and the challenges of knowledge
engineering have prevented widespread deployment.
In order to design an effective ITS, developers must form the basic core of the system, determine what is taught, and how. Moreover, in order to increase ITSs’ deployments, individual instructors should have the ability to alter the ITSs to fit their preferred teaching style and fill in with their preferred domain contents. Authoring tools
[Murray et al., 2003, Aleven et al., 2006, Aleven et al., 2005, Ainsworth and Fleming, 2005]
that provide support for the software-engineering aspects of development, and thus enable
non-developers to implement a system, are one promising approach to this problem. These
software tools allow each individual to build customized ITSs to meet his or her own needs.
Generally speaking, users of authoring tools face challenges not only in developing the content to be taught, but also in determining how to interact with the students.
Most authoring tools are built with a predefined pedagogical strategy and allow domain
experts to configure parameters such as the amount of help the tutor will provide. One
1

potential problem is that this approach assumes all students learn best using the same set
of teaching strategies. However, there are no well-established domain-general pedagogical
strategies in the learning and cognitive literature, and thus, the effectiveness of these predefined pedagogical strategies is often not clear. Additionally, instructors are domain experts
not learning scientists. Therefore, determining how to interact with students is a challenging
task for them, because they do not necessarily have a good understanding as to how these
parameters will impact student performance or subsequent behaviors [Chi et al., 2004]. In
order to improve their effectiveness, the authoring tools should provide more, and more
effective, methods, to help instructors decide how to interact with students.
On the other hand, it is still an open question as to whether the decisions on how to
interact with students would impact learning. For any form of tutoring, the tutor’s behavior can be viewed as a sequential decision process wherein, at each discrete step, the
tutor is responsible for selecting the next action to take. That is, the tutor’s main task
can be seen as deciding what action to take at each turn. Each of these tutorial decisions affects successive actions. One preferred assumption as to the effectiveness of human one-on-one tutoring has been that the human tutors are good at making such types
of tutorial decisions; moreover, these decisions are responsible for students’ learning gains
[Chi et al., 2001, Collins and Stevens, 1982, McArthur et al., 1982, Merrill et al., 1992]. In
the learning literature, the skills used to making such tutorial decisions are often referred to
as pedagogical skills. More formally, Chi, Siler, and Jeong [Chi et al., 2004] define these pedagogical skills as those that “involve skillful execution of tactics, such as giving explanations
and feedback, or selecting the appropriate problems or questions to ask the students.”
However, little evidence has shown that either human tutors have effective pedagogical skills, or pedagogical skills were the reason the students learned. In fact, many previous studies indicated that human tutors rarely employ any pedagogical skills when tutoring [Cade et al., 2008, Chi et al., 2004, Cho et al., 2000, Core et al., 2003, Katz et al., 2007,
Evens and Michael, 2006, Merrill et al., 1995, Merrill et al., 1992, VanLehn, 1999], see also
[VanLehn et al., 2003]. Additionally, skillful execution of these pedagogical skills may require
that tutors adapt their actions to tutorial context, which includes each student’s current
knowledge level and general aptitude, the subject matter under discussion, the institutional
2

context in which the tutoring takes place, and so on. But little evidence has been found either
that human tutors are able to monitor students’ understanding accurately [Chi et al., 2004],
or that tutors really adapt their decisions based on the tutorial context [Putnam, 1987]. For
instance, Chi, Siler, and Jeong [Chi et al., 2004] found that human tutors do not seem to
process an accurate model of students’ knowledge levels during the tutoring. In fact, Putnam [Putnam, 1987] found that experienced tutors did not attempt to form highly-detailed
models of their students’ knowledge before attempting remedial instruction; rather, each
teacher appeared to move through a curriculum script to teach the individual students.
If it was not superior pedagogical skills that enabled students to learn in these previous
studies [Bloom, 1984, Cohen et al., 1982], then what did cause students to learn? One indisputable explanation is instructional content and practice opportunities. For example, previous research repeatedly showed that students working with a tutor often learned significantly
more than those without one [Anderson et al., 1995, Chi et al., 2008b, Koedinger et al., 1997,
Lane and VanLehn, 2005, VanLehn et al., 2007a, VanLehn et al., 2005]. However, once content was controlled to be the same across all conditions, little evidence was found that there
was any difference among students under different learning treatments. Several techniques
have been employed to control for content. For example, in some previous studies the domain content was controlled by ensuring students worked on the same training problems with
the same human tutors or on a computer tutor that was scripted by the same human tutors [Evens and Michael, 2006, VanLehn et al., 2007a, Reif and Scott, 1999]. Additionally,
the content can be controlled to be equivalent by running a human tutoring condition first,
videotaping the tutoring sessions, and then having another group of students watch those
videotapes [Chi et al., 2008b]. Though it is often assumed that human tutors possess more
effective pedagogical skills than ITSs, previous research has shown that students who were
tutored under human expert tutors were no more effective than those who were tutored
under ITSs [Evens and Michael, 2006, VanLehn et al., 2007a, Reif and Scott, 1999]. Therefore, it seems the large benefit of tutoring over no-tutoring found in previous studies, may
be due to a difference in instructional content rather than a difference in pedagogical skills.
Until recently, there have been rising doubts in cognitive science regarding the impact of
pedagogical skills on students’ learning [Chi et al., 2008b, Chi et al., 2004, Chi et al., 2001,
3

VanLehn et al., 2007a].
However, absence of evidence is not evidence of absence. The lack of evidence supporting
the impact of pedagogical skills on learning does not mean these skills are irrelevant or not
important. In the studies underlying this thesis, I applied and evaluated a general data-driven
methodology to learn how to make these tutorial decisions from pre-existing interactivity
data rather than, as is presently common, implementing a priori pedagogical theories drawn
from experts.
In order to investigate the effect of pedagogical skills on learning, it was necessary to
separate tutorial decisions from instructional content, strictly controlling content so that it
is equivalent for all students. It is generally difficult to control tutoring content with human
tutors. Computer tutors, on the other hand, permits much greater control over, and tracking
of, the tutorial content than human tutors [Evens and Michael, 2006, VanLehn et al., 2007a,
Reif and Scott, 1999]. In this thesis, Cordillera, a Natural Language (NL) tutoring system
was implemented to teach college students introductory physics.
Tutoring in domains like math and science is often structured as a two-loop procedure.
An outer loop selects the problem or task the student should work on next, while the inner
loop governs step/level decisions during problem solving [VanLehn, 2006]. In this structure,
there are two main sources of content variation: selection of different problems, and guiding students along a different solution path. In order to minimize content variation, all
participants in this thesis solved the same problems and followed the same major problemsolving steps for each problem. In educational literature, the term “step” often refers to
the application of a major domain principle or equation such as Newton’s Third Law of
Thermodynamics during problem solving. However, in this thesis a step generally consists
of multiple micro-level steps. For example, one of the training problems, P4, is defined in
Figure 1.1.
In order to solve P4, students need to apply several domain principles, and some principles
need to be applied more than once. For instance, one of the domain principles the students
need to apply to solve for P4 is the definition of Kinetic Energy (KE =

1
mv 2 ).
2

More

specifically, they need to apply the definition of Kinetic Energy to the rock at T0. In order
to do so, the tutor takes the following four micro-level steps: selecting the principle to apply,
4

A 0.6kg rock in space has a velocity of magnitude 2.0m/s at point A and kinetic energy
of 7.50J at point B. What is the net work done on the rock as it moves from A to B? We
define:
T0 : the time point when the rock is at point A.
T1 : the time point when the rock is at point B.

Figure 1.1: A Training Problem: P4

writing the corresponding equation, solving the equation, and engaging in some qualitative
discussion about the principle. At each micro-level step, at least one tutorial decision needs
to be made as to how to carry out the step. As a result, this study focused on pedagogical
skills that govern low-level tutorial interaction decisions on a single micro-level step: for
example, whether the student is told what principle to apply, or if the system elicits it from
him or her with a prompt; and whether or not the student is asked to justify the answer,
once he or she has completed a step.
Figure 1.2 shows a sample dialog for how student-tutor interactions would look for carrying out the four micro-steps. In Figure 1.2, each numbered line represents a dialog turn.
The labels T and S designate tutor and student turns respectively. In the example shown
in Figure 1.2 the tutor and student complete a single principle application, the application
of the definition of Kinetic Energy to solve for KE0 (the rock’s kinetic energy at T0 ) and
begin to solve for v1 (the rock’s instantaneous magnitude of velocity at T1 ). The tutor and
student first selected a principle to apply (lines 2 and 3), wrote the corresponding equation
(4), then solved it (5), and finally, discussed the principle (6-8).
Some of the tutor turns in Figure 1.2 are labeled {ELICIT} or {TELL}. This label
designates a tutorial decision step wherein the tutor has to make a tutorial action decision
deciding whether to tell the student the information or to elicit the requisite information
with a question. For example, in line 2, the targeted principle is the definition of Kinetic
Energy, labeled as KC20 , and the tutor chooses to elicit the answer from the student by
asking the question, “Which principle will help you calculate the rock’s kinetic energy at
5

1. T: So let’s start with determining the value of KE0 .
2. T: Which principle will help you calculate the rock’s kinetic energy at T0? Please
provide the name of the principle, not an equation. {ELICIT}, KC20
3. S: Definition of kinetic energy
4. T: Yes, I agree. Now I will write the equation for applying the definition of kinetic
energy to the rock at T0: KE0 = 1/2*m*v0ˆ2 {TELL}, KC20
5. T: I calculate the value for KE0 = 0.5*0.6kg*2.0 m/sˆ2 = 1.2 J {TELL}, KC23
6. T: We do not know the direction of the rock’s velocity at T0 from the problem
statement. Would the direction of v0 affect the rock’s kinetic energy at T0?
{ELICIT}, KC20
7. S: No, it would not affect
8. T: Yes, I agree. This is because the kinetic energy only depends on the rock’s mass
and magnitude of velocity, not the direction of the rock’s velocity. {TELL}, KC20
9. T: We’ve calculated KE0, recall that our plan is to calculate v1 next.
Figure 1.2: An example Dialog with Cordillera

T0? Please provide the name of the principle, not an equation.” By contrast, if the tutor
elected to tell the students, then he or she would have stated, “To calculate the rock’s kinetic
energy at T0, let’s apply the definition of Kinetic Energy.” Both steps cover the same target
knowledge.
For the purposes of this thesis the term “pedagogical tutorial tactics” will be used to
refer to the policies for selecting the tutorial action at each micro-step level when there
are multiple actions available. The primary research question in this thesis is whether pedagogical tutorial tactics focused on individual interaction decisions impact students’ learning.

Existing ITSs typically employ hand-coded pedagogical rules that seek to implement
6

existing cognitive or instructional theories. These theories may or may not have been wellevaluated. For example, in both the CTAT [Anderson et al., 1995, Koedinger et al., 1997]
and Andes systems [VanLehn et al., 2005], help is provided upon request because it is assumed that students know when they need help and will only process help when they desire
it. Research on gaming, however, has raised some doubts about this, by showing that students sometimes exploit these mechanisms for shallow gains thus voiding the help value
[Baker et al., 2004b, Baker et al., 2004a]. It is often difficult to evaluate hand-coded rules in
a tutoring system as their performance depends upon a number of factors, such as the content
difficulty, the student’s incoming competence, the system’s usability, and so on. Previous
researchers have largely treated the specification of tutorial tactics as a design problem: several versions of a system are created, the only difference among them being the pedagogical
model employed. Data is then collected from human subjects interacting with each version
of the system, and the students’ performance is then statistically compared. Due to cost
limitations, typically, only a handful of alternative tutorial tactics are explored.
Recent work on ITSs has shifted focus from hand-coded tutoring designs to more datadriven methodologies. For example, ITSs researchers have used decision theory to guide the
tutoring system in lieu of hand–crafted rules [Murray and VanLehn, 2006]. In this thesis,
the approach adopted does not have to rely upon a priori belief about how the tutor should
teach. Instead, it proposes to “learn” how to make tutorial decisions from pre-existing
student-computer interactivity corpora. The machine-learning technique chosen for this
task is reinforcement learning (RL). The methodology reported in this thesis is heavily
motivated by previous research in non-tutoring dialog systems. In these previous studies
RL has been successfully applied to improve the effectiveness of non-tutoring dialog systems
[Williams et al., 2005, Walker, 2000, Singh et al., 2002]. The system employed in this thesis
is a NL tutoring system named Cordillera [Jordan et al., 2007, Jordan et al., 2006]. While
NL tutoring systems can be seen as complex dialogue systems, applying RL to NL tutoring
systems raises certain challenges in that the research is focused on a more complex task –
instruction – than most dialogue systems. Thus it is still an open question whether RLderived policies will prove effective in an educational context. In the following paragraphs,
I will describe a general methodology showing how RL was applied to derive tutorial tactics
7

from computer-student interactivity data. The secondary research question is: Will RL
provides a feasible method to induce pedagogical tutorial tactics?

1.1

1.1.1

RESEARCH QUESTIONS

Question 1: Do Micro-level Pedagogical Tutorial Decisions Affect Students’ Learning?

1.1.1.1

Background on Pedagogical Tutorial Tactics Many studies of one-on-one

tutoring show that tutors tend to dominate the tutoring sessions. For instance, they take
more initiative. The tutor’s primary task can be seen as deciding what action to take on each
turn [Chi et al., 2001, Graesser et al., 1995]. Much of this research takes an implicitly tutorcentric perspective. It assumes that the tutors’ actions are primarily responsible for tutoring
effectiveness based upon the way they craft and adapt their actions to the students’ needs
[Collins and Stevens, 1982]. Even though students can benefit from being tutored by novice
tutors [Cohen et al., 1982], expert human tutors seemingly produce better learning outcomes
[Lu et al., 2007, Eugenio et al., 2006]. Here both expert and novice tutors are domain experts who differ only in terms of their tutoring experience. Similar, but less significant, results
were found by Chae et al. [Chae et al., 2005] and Kim, Chae and Glass [Kim et al., 2005].
In their work students’ learning gains under expert tutors were larger than learning gains
under novice tutors; however, their results were only marginally significant. On the other
hand, it has also been shown that expert human tutors employ different tutorial tactics than
novice tutors [Hume et al., 1995, Kim et al., 2005, Lu et al., 2007]. In short, these results
suggest that expert tutors may be more effective than novice tutors because they make more
effective tutorial decisions.
On the other hand, the majority of previous research studies have shown that human
tutors may not be very effective when selecting tutorial actions and the tutors’ pedagogical skills may not determine students’ learning. For example, Clark, Snow, and Shavelson [Clark et al., 1976] found that human tutors’ educational effectiveness was not necessar8

ily correlated with their level of training or prior experience. They conducted a comparison
study between trained human tutors and rank novices in the domain of physics. Participants were tutored for five one-hour sessions that were completed in one week. Results
showed that the trained tutors were no more effective than the inexperienced tutors. Chi et
al. investigated three hypotheses regarding tutor effectiveness: a tutor-centered hypothesis
assuming that tutoring effectiveness arises from the tutors’ pedagogical skills; a studentcentered hypothesis assuming it arises from the students’ active generation; and an interactive hypothesis assuming that it arises from the joint effort of both the tutors and students
[Chi et al., 2001, Chi et al., 2004, Chi et al., 2008b]. They found evidence supporting the
latter two hypotheses, but not the tutor-centric hypothesis.
Research in computer learning environments has found a similar lack of evidence for the
tutor-centric view of tutoring effectiveness. Evens and Michael conducted a series of studies
comparing four learning treatments in cardiovascular physiology [Evens and Michael, 2006].
The no-tutoring condition studied a text that included examples of the correct reasoning
for solving a pacemaker problem. The CIRCSIM condition solved one training problem on
a tutoring system, CIRCSIM, which presented a short text passage for each incorrect step.
The CIRCSIM-tutor condition solved the same training problem on a sophisticated natural
language tutoring system, CIRCSIM-tutor, which replaced the text passages in CIRCSIM
with typed natural language dialogue. The human tutor condition also solved the same
training problem with expert human tutors. Results showed that the latter three conditions
out-performed the no-tutoring condition, but the three treatments, CIRCSIM, CIRCSIMtutor and expert human tutors, tied with each other.
While ITSs generally support students both in the selection of problems to work on and
the solving of those problems, computer-aided instructional (CAI) environments generally
support only the outer loop, the problem-selection loop. Previous studies have shown that
when students study the same materials and solve the same problems, a CAI will be as effective as an ITS [Sleeman et al., 1989]. More recently VanLehn et al. [VanLehn et al., 2007a]
compared students who studied the same material and then studied the same training problems under a variety of conditions, including expert human tutors and a variety of ITSs. All
students in the study showed learning gains but no significant difference was found among
9

the groups. In a subsequent review of studies of human tutors, VanLehn [VanLehn, 2009]
noted that human tutors were seldom more effective than moderately interactive forms of
tutoring, such as step-based tutoring systems [VanLehn et al., 2007a].
In sum, previous research has suggested that tutorial content is indisputably an important
source that contributes to the effectiveness of one-on-one tutoring. The effectiveness of the
pedagogical tutorial tactics, however, is still an open question. In order to investigate whether
pedagogical tutorial tactics alone will make a difference in learning, it is necessary to control
such factors as the tutoring content.
In this thesis, all students studied the same subject matter, the same training problems
using the same tutorial scripts, and interacted with the computer tutors using the same user
interface. For each training problem, all students experienced the identical information for
all of the non-tutorial decision steps, and the variance among the students was on tutorial
decision steps. For any given tutorial decision step, once tutorial action was taken, the same
domain content would be carried out for all students. The following example will illustrate
this.
The example used here is P4 (shown in Figure 1.1), one of the seven training problems
used in this dissertation. For each training problem all participants followed a two-phase
strategy which consists of collaborative solution wherein the student and tutor solve the
problem together (phase 1 ), followed by post-problem discussion where the student reflects
upon the solution (phase 2 ).
During phase 1, the student and the tutor solve the problem together. One important
characteristic of this phase is that the tutor guides the student by applying one principle at
a time. For example, solving training problem P4 (shown in Figure 1.1) involves applying
three major domain principles with some principles needing to be applied twice. The three
domain principles are: the definition of Kinetic Energy (KE: KE = 12 mv 2 ), the definition
of Total Mechanical Energy (TME: T M E = KE + GP E + SP E), and the Change of Total
Mechanical Energy for Non-isolated Systems (N etW = T M E2 − T M E1 ). The solution path
for P4 students followed in this dissertation was: 1) applying the definition of Kinetic Energy
to solve for the rock’s kinetic energy at T0 , 2) applying the definition of Kinetic Energy to
the rock’s magnitude of velocity at T1 , 3) applying the definition of Total Mechanical Energy
10

to solve for rock-system’s Total Mechanical Energy at T0 , 4) applying the definition of Total
Mechanical Energy to solve for rock-system’s Total Mechanical Energy at T1 , and 5) applying
the Change of Total Mechanical Energy for Non-isolated Systems to solve for the work done
on the rock-system from T0 to T1 . All of the students applied one domain principle at a time
and followed the same solution path in the same order of 1-5. For each domain principle
application, the tutor generally would make 3-5 micro-level tutorial decisions as shown in
Figure 1.2.
During phase 2 the tutor highlights the solution’s main steps, reviews any confusion that
students may have had during the solution, and considers how the solution varies when the
problem statement is varied in certain ways. For example, in the post-problem discussion for
training problem P4 (shown in Figure 1.1), the tutor would cover eight main topics, generally
one topic for each domain principle. These eight topics include the discussion about whether
there are any extra steps in the solution path during the problem solving, the definition
of potential energy, how changing the mass of the rock would affect the final result, and
so on. In this dissertation, all four groups of students went through all main topics in the
post-problem discussion in the same order (shown in Appendix I). Similar to the problem
solving, the difference is how these discussions were carried out. For example, in the domain
of work and energy, potential energy always involves two objects, such as potential energy
of block-earth pair or potential energy of block-spring pair. However, students often focus
on only one object such as the block. One of the eight topics in the post-problem discussion
for P4 is regarding potential energy and there were two versions of discussion: elicit version
vs. tell version. An example of the elicit version of post-problem discussion in P4 looks like:
Tutor: In this problem, we have selected the rock as the system. Is it possible to define
potential energy for the rock system?
Student: No, it is not possible.

The tell version of the same point is:
Tutor: In this problem, we have selected the rock as the system and we *cannot* define a
potential energy for the rock system.

This project employed four types of tutorial tactics: Exploratory, Dichotic Gain (Dich11

Gain), Normalized Gain (NormGain), and Inverse Normalized Gain (InvNormGain). All
four groups of participants covered the same material using the same procedure and were
trained on four versions of the same tutoring system. The only difference among the different
versions employed by the four groups was the policy that each tutoring system followed to
make tutorial decisions.
The primary research question is: “Will pedagogical tutorial tactics focused on individual interaction decisions impact students’ learning?” For the purposes of this thesis, two
types of interaction decisions, Elicit/Tell and Justify/Skip-Justify, have been made the focus
of this research. For each Elicit/Tell decision step, some students would be told the information, while others would be asked to provide the information. For each Justify/Skip-Justify
decision step, the tutor sometimes would execute the justification for some students and
at other times the tutor would skip the justification. Each type of interaction decision is
described in detail below.

1.1.1.2

Elicit/Tell During the course of one-on-one tutoring, the tutor often faces a

simple decision, should he or she elicit the next step information from the student, or should
he or she tell the student the next step directly? We refer to such tutorial decisions as
elicit/tell (ET) decisions. While a lecture can be viewed as a monologue consisting of an
unbroken series of tells, individual tutoring is characterized by a great deal of give and take
and is thus a mixture of elicits and tells. Generally speaking, eliciting more information from
the student during tutoring will result in a more interactive tutorial dialogue.

Figure 1.3 presents a pair of sample dialogues comparing elicit and tell versions of a single
tutorial dialogue extracted from a log file collected in this project. Both dialogues begin and
end with the same tutor turns (lines 1 and 6 in (a) and 1 and 4 in (b)). In dialogue (a) the
tutor chooses to elicit twice (lines 2-3 and 4-5 respectively). In contrast, dialogue (b) covers
the same domain content with two tell actions (lines 2 and 3). As a consequence, tutorial
dialogue (a) is more interactive than (b).
For the purposes of this dissertation interactivity is quantified in terms of Interactivity
ratio (I-ratio), which is defined as the number of elicit decisions a student received divided
12

(a) Elicit Version
1. T: So let’s start with determining the value of v1.
2. T: Which principle will help you calculate the rock’s instantaneous magnitude of
velocity at T1? {ELICIT}, KC20
3. S: definition of kinetic energy
4. T: Please write the equation for how the definition of kinetic energy applies to this
problem at T1 {ELICIT}, KC20
5. S: ke1 =0.5*m*v1ˆ2
6. T: From KE1 = 0.5*m*v1ˆ2, we get v1ˆ2=KE1/(0.5*m). We substitute KE1 with 7.5
J and m with 0.6 kg. The equation now becomes: v1ˆ2=7.5 J/(0.5*0.6 kg). The
magnitude of velocity is always positive, therefore, v1= 5.0 m/s. {TELL}, KC31

(b) Tell Version
1. T: So let’s start with determining the value of v1.
2. T: To calculate the rockś instantaneous magnitude of velocity at T1, we will apply the
definition of kinetic energy again. {TELL}, KC20
3. T: Let me just write the equation for you: KE1 = 0.5*m*v1ˆ2. {TELL}, KC20
4. T: From KE1 = 0.5*m*v1ˆ2, we get v1ˆ2=KE1/(0.5*m). We substitute KE1 with 7.5
J and m with 0.6 kg. The equation now becomes: v1ˆ2=7.5 J/(0.5*0.6 kg). The
magnitude of velocity is always positive, therefore, v1= 5.0 m/s. {TELL}, KC31
Figure 1.3: Elicit vs. Tell

by the total number of ET decisions received in a given dialogue and can be expressed in:

I − ratio =

NElicit
NElicit + NT ell
13

(1.1)

The higher this value, the more interactive the dialogue. If I − ratio = 0.5 means that
students were given elicitation prompts as often as they were simply told the information
while I − ratio > 0.5 means that they were more likely to be prompted for information, and
I − ratio < 0.5 means that the conversation was more didactic.
A key characteristic of one-on-one tutoring, whether from human tutors or computer support, is high interactivity. A common assumption, often referred as the monotonic interaction
hypothesis [VanLehn et al., 2007a] is that greater interactivity leads to greater learning. But
Chi et al. [Chi et al., 2001, Chi et al., 2008b] and Rose [Rose et al., 2001] found no difference
in learning between students tutored on an interactive tutor and those tutored on a more
didactic one. A detailed review of the literature [VanLehn et al., 2005, VanLehn, 2009] (submitted) distinguished between the widely-accepted, monotonic interactivity hypotheses and
the better supported interaction plateau hypothesis. The former states that an increase in
interactivity causes consistent increases in learning gains, while the latter states that beyond
a given threshold point, increasing interactivity will yield diminishing educational returns.
In this dissertation, it will be argued that it may not be the absolute volume of interactivity
that is at issue, but rather how the interactivity is guided.
Some existing theories of learning suggest that when deciding whether to elicit or tell,
a tutor should take into account several factors including the students’ current knowledge
model. Vygotsky [Vygotsky, 1971] coined the term “zone of proximal development” (ZPD)
to describe the space between abilities that a student may display independently and those
that they may display with support. He hypothesized that the most learning occurs when
students are assigned tasks within their ZPD. In other words, the task should neither be
so simple that students can achieve it independently or trivially, nor so difficult that they
simply cannot make progress even with assistance. We expect, based upon this theory, that
if students are somewhat competent at a given step, the tutor should elicit, and provide help
only if the students fail, so that they can practice their knowledge. If students are completely
unfamiliar with the step, however, then the tutor should tell them directly. Collins, Brown
and Newman [Collins et al., 1989] describe a progression from tells to elicits following their
“model, scaffold & fade” rubric. Koedinger and Aleven [Koedinger and Aleven, 2007] by
contrast defined an “assistance dimension”, which includes elicits and tells. The level of
14

assistance a tutor should provide may be resolved differently for different students and should
be adapted to: the learning environment, the domain materials used, the students’ knowledge
level, their affect state, and so on.

1.1.1.3

Justify/Skip-Justify The second tutorial decision investigated was to execute

or to skip a justification step. During the tutoring process, human tutors sometimes ask
students to justify a step they have taken or an entry they have made. We refer to such
tutorial decisions as justify/skip-justify (JS) decisions. Their apparent goal appears to be
to help students understand domain knowledge in a deeper way. The open question is
when should tutors conduct an elaborate discussion of a problem solving step when this
discussion is not necessary for the solution? Some authors including [Chi et al., 1994],
[Conati and VanLehn, 2000], [Aleven et al., 2004] and others have found that asking students to justify their solution steps improves learning. However, eliciting such a discussion
may not always be desirable if, for example, the student is well aware of the rationale. If
so, typing in a justification can be slow, frustrating, and distracting. Katz, O’Donnell, and
Kay [Katz et al., 2000] found that in some cases it may be better to delay discussion of the
justifications until after the problem has been solved, especially if the justification is abstract,
plan-based, or lengthy.
After a JS decision is made and the tutor has decided to execute a justification step, the
tutor sometimes needs to make an ET decision immediately. Thus, there are three possible
decisions for these decision steps. Figure 1.4 presents three dialogue examples. Among them,
parts in (a) and (b) justification is employed to guide the student. More specifically, in parts
(a) and (b), the tutor first made a JS decision and decided to execute the justification step.
Then the tutor needed to make another decision about whether to elicit the information from
the students or to tell the students the justification directly. Examples (a) and (b) shows
what a tutorial dialogue may look like if the tutor decides to execute an elicit justification
and a tell justification respectively. In part (c), the justification is skipped. As before, all
three examples start and end with the same tutor turns (lines 1 and 5 in part (a) and lines
1 and 4 in part (b) and (c)). In part (a) the tutor asked the student to justify his or her
answer (lines 3-4); in part (b) the tutor told the student the justification directly (line 3),
15

while in part (c) the tutor skipped the justification (line 3).
Similar to I-ratio, justification can be quantified in terms of Justification ratio (J-ratio),
which is defined as the number of times the tutor does not skip a justification step divided
by the total number of JS decisions the tutor made and can be expressed in:
J − ratio =

NJustif y
NJustif y + NSkipJustif y

(1.2)

The higher this value, the deeper and more elaborate the dialogue might be. In addition
to J-ratio, justification can also be quantified by the number of justification steps students
receive. The more justification steps in a student’s tutorial dialogue, the more deep learning
the student may gain.
Neither the ET (elicit/tell) nor JS (justify/skip-justify) steps are well-understood, and
there is no widespread consensus on how or when they should be used. This is why one
research objective is to derive tutorial tactics for them from empirical data. Given that
expert human tutors do not always make optimal, or effective, tutorial decisions, handcrafting rules based upon human data may be a suboptimal strategy. Inducing data directly
from interaction data may be the most suitable method.

16

(a) Justify Version & Elicit
1. T:Can we infer the direction of the velocity of the rock at T1 from the rock’s kinetic
energy at T1? {ELICIT}, KC20
2. S:nope.
3. T:Excellent! Please explain why. {JUSTIFY, ELICIT}, KC20
4. S:Only the magnitude of the velocity and not the direction of it is part of the definition
of kinetic energy.
5. T:Excellent! Now that we know v1, we will concentrate on the sought quantity for this
problem, that is, the net work done on the rock system from A to B.
(b) Justify Version & TELL
1. T:Can we infer the direction of the velocity of the rock at T1 from the rock’s kinetic
energy at T1? {ELICIT}, KC20
2. S:nope.
3. T:Excellent! This is because the kinetic energy only depends on mass and the magnitude
of velocity, not the direction of velocity.{JUSTIFY, TELL}, KC20
4. T:Now that we know v1, we will concentrate on the sought quantity for this problem,
that is, the net work done on the rock system from A to B.
(c) Skip-justify Version
1. T:Can we infer the direction of the velocity of the rock at T1 from the rock’s kinetic
energy at T1? {ELICIT}, KC20
2. S:no, we cannot
3. T:Excellent! {Skip-JUSTIFY}, KC20
4. T:Now that we know v1, we will concentrate on the sought quantity for this problem,
that is, the net work done on the rock system from A to B.

Figure 1.4: Justify vs. Skip-justify

17

1.1.2

Question 2: Is Reinforcement Learning a Feasible Method to Induce Tutorial Tactics?

1.1.2.1

Previous research about applying RL in ITSs RL has been applied to

conventional ITSs and used successfully to improve system behavior. In [Beck et al., 2000]
the authors applied RL to develop a tutorial policy that would minimize the time students
take to complete a problem. In Beck et al.’s study, the resulting policy caused the students
to spend significantly less time per problem than their peers who did not follow the policy.
However, the authors used simulated data for the training datasets, making it possible to
accurately model time on task. As a consequence, they faced no problems of data sparsity.
In this dissertation, given that the cause of student learning is still an open question, it would
be difficult to accurately simulate students’ responses to the tutor and simulate how students
would learn. Therefore, we used a training corpus collected from real human subjects and,
due to the high cost of collecting educational data, the training corpus is comparatively
small.
Barnes and Stamper [Barnes and Stamper, 2008, Stamper et al., 2007] have applied RL
to automatically construct problem solutions for an ITS called Proofs Tutorial, which teaches
college-level discrete mathematics. In their work, each student’s solution is defined as a
diagraph with a series of states connected by actions. A state is represented by the list
of premises generated in the solution so far and the actions are axiom, principle, or rules
applications taken at each step. The authors collected and merged all of the student solutions
into a single super-graph by taking the union of all possible states and transitions. Once the
super-graph was constructed, it included all previously examined paths taken by students in
solving the problem. The authors then applied MDP to induce an optimal solution using the
super-graph as the search space. More specifically, they assigned scores of 100 to the goal
state and -10 to each incorrect state, and a cost of 1 for each action taken. They then used
value iteration to calculate the value for each state in this single graph, which was then used
to generate hints for new students. They found that the extracted MDPs and the proposed
hint-generating functions were able to provide hints over 80% of the time.
The work described in [Tetreault et al., 2007, Tetreault and Litman, 2006a] used RL to
18

learn tutorial tactics governing whether or not the system should provide feedback and what
type of questions it should ask. They used a previously collected corpus of physics tutorial
dialogues. Their state representation consisted of five feature vectors: Certainty, Correctness,
Percent Correct, Concept Repetition, and Frustration; and they defined four possible tutor
actions: ask a simple answer question, a complex answer question, a combination of the
two, or none at all. As with the present work, they used students’ final normalized learning
gains as reward. Their work is close to that of this thesis because both seek to apply RL
to derive effective pedagogical tutorial tactics. However, their dataset was not collected
with the goal of exploring the full range of tutorial tactics in that the tutor often executed
only one type of action in many dialogue states. Additionally, manually annotated features
such as Certainty and Frustration were used in their work while only features that could
be computed automatically or evaluated objectively, such as gender, were included in this
thesis and their feature space is also substantially smaller than the one explored in this
thesis. Moreover, the learned policies in their work were not tested on real students and thus
their predicted success was not verified empirically. In this dissertation, the research is based
on both an exploratory corpus designed to test the range of tutor actions and conducting
empirical tests of the resulting KC-based strategies using real human subjects.
In this dissertation, RL was applied to induce pedagogical tutorial tactics from studentcomputer interactivity data. The studies tackled two challenges: the high cost of collecting
a training corpus and the lack of prior knowledge as to what information to include in the
state representations. Previous research on applying RL in ITSs focused on some, but not
both, issues addressed here. While there have been other methods for deriving effective
pedagogical tutorial tactics, they have typically involved other machine-learning methods.
Murray and VanLehn [Murray and VanLehn, 2006], for instance, applied decision theory
to determine the type of hints and feedbacks the tutor should give. In their work, a Dynamic
Decision Network was applied in an ITS that would decide the best actions. Their approach
showed that human tutors would agree with the ITS’s actions more frequently than a random
tutor. However, a Dynamic Decision Network requires knowing the utility function for each
state and deriving it is not trivial. The goal of this thesis research is to improve students’
learning gains and thus the utility function is only available for the last state. A Dynamic
19

Decision Network would be required to perform look-ahead search, like a chess program,
all the way to the end before it could select a single move. Consequently, it would not be
straightforward to apply Dynamic Decision Networks to the current research.
As mentioned before, this research is chiefly motivated by the previous work on applying
RL to non-tutoring dialogue systems. However, Natural Language (NL) tutoring systems
differ from the non-tutoring dialogue system and thus it is not clear that RL is a feasible
approach here.

1.1.2.2

Applying RL to Dialogue Systems vs. Natural Language Tutoring Sys-

tems Dialogue Systems is a field of Computer Science that focuses on the construction of
computer systems that interact with human users via natural-language dialogues. Much of
the work in this area is focused on systems that obtain information or search databases such
as querying bus schedules [Raux et al., 2005], booking airline tickets [Rudnicky et al., 1999],
and accessing train schedules [Swerts et al., 2000]. NL tutoring systems can be seen as systems that use natural dialogue for instructional purposes such as helping students to learn a
subject by engaging in a natural language conversation. Auto-tutor [Graesser et al., 2001],
WHY-Atlas [VanLehn et al., 2002], and ITSPOKE [Litman and Silliman, 2004] for example, are all NL tutoring systems that teach students conceptual physics. For both general
dialogue systems and NL tutoring systems the central component is the dialogue manager.
At each point in the dialogue, it decides which action to take. In recent years, work on the
design of dialogue systems has involved an increasing number of data-driven methodologies.
Among these, Reinforcement Learning (RL) has been widely applied [Williams et al., 2005,
Walker, 2000, Singh et al., 2002].
RL is a machine learning method that centers on the maximization of expected rewards.
RL has many features well-suited to the problem of designing the dialogue manager such
as unobservable states, delayed rewards, and so on. Its primary advantage is its ability
to compute an optimal policy within a much larger search space, using a relatively small
training corpus. It is data-efficient because it evaluates actions as a function of states.
Much of the previous research on the use of RL to improve dialogue systems such as
[Levin and Pieraccini, 1997, Singh et al., 1999] has used Markov Decision Processes (MDPs)
20

[Sutton and Barto, 1998] to model the dialogue data and then optimize the policies from
the training corpus. An MDP formally corresponds to a 4-tuple (S, A, T, R), in which:
S = {S1 , . . . , Sn } is a state space; A = {A1 , . . . , Am } is an action space represented by a
set of action variables; T is a set of transition probabilities between states that describe the
dynamics of the modeled system; and R = r(si , sj , ak ) denotes a reward model that assigns
rewards to state transitions and models payoffs associated with such transitions. The goal
of RL is to find an optimal policy π ∗ that maps each state to the proper actions that would
generate the maximum rewards. The dialogue management problem can be naturally cast
into the MDP formalism: the states {S1 , . . . , Sn } in the MDP correspond to the dialogue
states (or an abstraction thereof); the actions {A1 , . . . , Am } correspond to the particular
actions the dialogue manager might take; and the rewards r(si , sj , ak ) are defined to reflect
a dialog performance metric, such as learning gains. Once the MDP structure has been
defined, the transition probabilities between states T are estimated from a training corpus of
dialogues, and, based on them, the policy which maximizes the expected cumulative reward
is computed.
An MDP describes a stochastic control process and the state transitions possess the
Markov property, which assumes that only the present state gives any information about
the future behavior of a process and knowledge of the history of a process does not add
any new information. However, in many real-world applications, including this thesis, the
Markov property does not always hold. For example, in order to construct the MDP model,
one needs to simplify the sample dialogue data, reducing it to a set of computable features.
In doing so, we cannot avoid losing information some of which may be relevant. Given
that these defined features do not represent the whole state, the state representations in
this research do not have the Markov property. However, previous studies have shown
some theories which apply to cases exhibiting the Markov property, can also be applied to
many problems that do not, such as the task domain of this thesis [Williams et al., 2005,
Walker, 2000, Singh et al., 1999, Singh et al., 2002]. This is because the induced policies
may not be optimal, but they can still improve the effectiveness of the system and in most
of these previous studies, the baseline policy is making random decisions.
While most previous work on using MDPs to train dialogue systems has been successful
21

[Walker, 2000, Henderson et al., 2005], whether it can be used to improve the effectiveness
of NL tutoring systems is still an open question. One major source of uncertainty comes
from the fact that the rewards used in RL are much more delayed in NL tutoring systems
than those in non-tutoring dialogue systems. Non-tutoring dialogue systems often use user
satisfaction or task completion as the rewards, while the most preferable rewards for NL
tutoring systems are students’ learning gains. More immediate rewards are more effective
than more delayed rewards for RL induction. This is because the issue of assigning credit
for a decision, attributing responsibility to the relevant decision is substantially easier in the
former case. The more we delay success measures from individual decisions, the more difficult
it becomes to identify the decision(s) responsible for our success or failure. Even though the
rewards in both types of systems will not be available until the conversation is over, NL
tutoring systems are more complex than the database-access dialogue systems described
above. In dialogue systems like the train scheduler, the interaction time is much shorter,
often less than 20 minutes, and the number of interactions within user-dialogue systems is
generally less than 20 turns [Singh et al., 1999, Singh et al., 2002]. In NL tutoring systems,
on the other hand, the preparatory training materials and testing typically exceed these
timeframes significantly. In the studies reported here, it took students roughly 4-9 hours
using the tutoring system itself, with around 280 interactions between a human subject and
the NL tutoring system during the entire training sequence.
Additionally, compared with non-tutoring dialogue systems, there are two major challenges in applying RL to NL tutoring systems. Each of these will be discussed in the following
section.

1.1.2.3

Whether RL Is Able To Induce Effective Tutorial Tactics Is Still An

Open Question. The first main challenge is that it is difficult to determine which features of the learning environment are relevant, and thus, should be included in the state
representation. Ideally the state should include all of the relevant dialogue history necessary
to determine which action is taken next. One obvious but impractical choice is to use a complete record of the dialogue to the present point; however, in practice we need to compress
the dialogue history to make the space tractable. The challenge lies in identifying the useful
22

features. Increasing the size of the state space may make the learning problem intractable,
while the alternative may make the available data a much sparser sampling of the domain.
While most of the work on the use of MDPs to improve dialogues has focused on developing
the best policy given a set of features [Walker, 2000, Henderson et al., 2005], there has been
relatively little work done on feature selection.
Early work on RL and MDP in non-tutoring dialogue systems focused largely on relatively
simple task domains and used slot-based state representations. NJFun, for example, is a
real-time spoken dialogue system that provides users with information about things to do in
New Jersey. In applying RL to improve NJFun, Singh et al [Singh et al., 1999] used seven
features for the state representation, such as whether the system has greeted the user, how
many times a given piece of information has been asked for, and so on. However, as RL
and MDP have been applied to more complex domains [Frampton and Lemon, 2006], the
state space representations have increased in size and complexity, which creates a danger of
making the learning problem intractable or the decision space too large to sample effectively.
Some of the previous studies in this area have focused on domain-specific features
that should be included in the state-space. Singh et al. [Singh et al., 1999] showed that
dialogue length was useful, while Frampton and colleagues [Frampton and Lemon, 2005,
Frampton and Lemon, 2006] showed that incrementally adding high-level contextual information (such as the user’s last dialogue act and the last system move) into a state model,
was also beneficial for building a better dialogue manager.
Previous research on applying RL in non-tutoring dialogue systems also investigated an
effective feature selection procedure. Paek and Chickering’s work, for example, showed how a
state-space can be reduced by only selecting features that are parents of the local immediate
reward performs just as well as a more complicated model with other variables that are
not parents [Paek and Chickering, 2005]. Recently, [Rieser and Lemon, 2006] used logistic
regression to select the best state features for a multi-modal dialogue system and showed
marked improvement over the baseline and some supervised learning methods. Most recently,
Tetreault, et al [Tetreault and Litman, 2008] tackled the feature selection issue by exploring
three evaluation metrics for assessing the utility of adding a particular state feature to a
model of user state. The feature selection procedure employed in this dissertation is based
23

upon work by them. [Tetreault and Litman, 2008]. However, this thesis is fundamentally
different than Tetrault et al’s work because they explored three evaluation metrics and
used a relatively simple feature selection procedure. This thesis explored several different
feature selection procedures, but used only one evaluation metric, the Expected Cumulative
Reward (ECR). Specifically, Study 2 explored four categories of features suggested by the
previous learning literature and a “greedy-like” feature selection method. Study 3 explored
six categories of features and eleven more domain-general feature selection approaches.
The second main challenge is obtaining a training corpus. In order to use RL to induce
an effective policy, it is necessary to collect an exploratory dataset that explores the relevant
space of possible decision sequences. A common problem in RL is finding a balance between
exploration (attempting to discover more about the world) and exploitation (using what we
already know about the world to get the best results we can). A tutor in the real world must
often choose between maximizing its expected utility according to its current knowledge
about the world and trying to learn more about the world, since the latter may improve its
future gains. This problem is known as the trade-off between exploitation and exploration.
Balancing exploration and exploitation is particularly important in educational contexts
as data collection is generally very expensive. On one hand, without exploration, the tutor
might not find an effective policy at all. On the other hand, if the tutor explores too
much, it cannot stick to a path; in fact, it is not really learning as it cannot exploit its
knowledge, and so acts as though it knows nothing. Thus, it is important to find a good
balance between the two, to ensure that the tutor is really learning to take effective actions.
It is often unclear how much exploration should be done in order to induce an effective
policy. Ideally, of course, the training dataset should be as large as possible. One way to
speed the process would be to use simulated student data [Beck, 2001, Beck et al., 2000,
Ai and Litman, 2009]. Accurate simulations, however are difficult because the requirements
for and causes of students’ learning are still open questions. An alternative approach is to use
pre-existing data that was collected for other purposes. This route, however, is complicated
by the fact that pre-existing systems often explore a small space and number of the actions
and thus may yield biased or limited information.
In this dissertation, a different approach was taken. Instead of collecting a large ex24

ploratory training corpus at once, the corpus was accumulated over several stages. An
initial exploratory dataset was collected that was large enough to apply RL to derive some
tutorial policies. Once derived, those policies were used to train a new group of students and
collect a new dataset. The new dataset was added to the original exploratory data to derive
additional policies since the new group of students experienced the identical procedure as
the original exploratory group. Such a process can be repeated until the learned policies
either become stable or reach the desired results. It will be argued that this incremental
improvement is the only practical method for continued improvement of an ITS. One would
not want to continue using a poor quality tutoring system semester after semester when a
better one could be available at the end of each semester. In this dissertation the process
was repeated twice to determine if it resulted in an improved ITS. All data was collected
before using the RL to adjust the tutoring system. In all, this dissertation includes three
studies.
In Study 1, an initial NL dialogue system, called Cordillera, was built, in which the
tutorial decisions on ET (elicit/tell) and JS (justify/skip-justify) were randomly made. This
was used to collect an exploratory corpus by training a set of real students using the system.
In Study 2, RL was used on the exploratory corpus to derive tutorial tactics, incorporate
them back into Cordillera, train another group of students on the new version of system,
and collect a new corpus. In Study 3, RL was applied to both the exploratory corpus from
Study 1 and the new corpus from Study 2, individually and again on a single merged dataset
combining students’ corpus from both studies.

1.2

GENERAL APPROACH

As described above, there are two primary research questions in this thesis:
Question 1: Given content controlled among conditions, will micro-level pedagogical tutorial decisions affect student learning?
Question 2: Is RL a feasible method to induce tutorial tactics?

25

Each question represents a potential contribution to a field of research. Question 1 is
relevant to the fields of learning and cognitive science; Question 2 is relevant to the fields of
Intelligent Tutoring Systems, AI in Education, and Educational Data mining.
In order to investigate these two questions, Cordillera, a NL tutoring system was built
which teaches students introduction to physics. Since Fall, 2007, three studies have been
run [Jordan et al., 2007]. All three studies followed the same procedure: completing a background survey, reading a textbook, taking a pre-test, training on Cordillera, and finally,
taking a post-test. All three studies used the same training problems and instructional materials but on different versions of Cordillera. The versions differed only in terms of the
pedagogical tutorial tactics employed for micro-step level interactive decisions.
In Study 1, the Cordillera made interactive decisions randomly. This allowed us to
collect an exploratory corpus that examined the consequences of each tutorial decision with
real students. The student group for this study is referred to as the Exploratory Group. In
order to differentiate this version of Cordillera from the ones used in subsequent studies, this
version is referred to as Random-Cordillera.
In Study 2, RL was applied to the Exploratory corpus to induce a set of tutorial tactics named Dichotic Gain (DichGain) tutorial tactics. This version of Cordillera was named
DichGain-Cordillera. DichGain-Cordillera employed the new policies to guide its interactive
decisions. As before, this version of the system was used to train students in a complete
study. The resulting corpus was named the DichGain corpus and the student group was
named the the Dichotic Gain (DichGain) group. A preliminary analysis of these tactics,
presented in Chapter 5, showed that they were no more effective than simple random decisions. In Study 3, RL was applied to induce tutorial tactics from both the Exploratory and
DichGain corpora, both individually and again as a merged set. Two sets of tutoring tactics
were derived from the three corpora, Normalized Gain (NormGain) and Inverse Normalized Gain (InvNormGain). The NormGain set was derived with the goal of enhancing the
tutorial decisions that contribute to the students’ learning; while the InvNormGain set was
derived with the goal of enhancing those decisions that contribute less or even none to the
students’ learning. We then ran a comparison study using the same educational materials
as those in Studies 1 and 2. In Study 3 students were randomly assigned to one of two con26

ditions. One condition, the NormGain condition, was assigned to use a version of Cordillera
which implemented the NormGain policies, named NormGain-Cordillera while another condition, the InvNormGain condition, was assigned to a another version of Cordillera with the
InvNormGain policies, named InvNormGain-Cordillera.
Our primary hypothesis is:
The Normalized Gain (NormGain) group will out-perform
the Inverse Normalized Gain (InvNormGain) group.
The following thesis chapters will expand upon processes and outcomes from the studies
summarized in Chapter 1. Chapter 2 provides a more detailed description of Cordillera,
the Natural Language Tutoring System used in this thesis. Chapter 3 presents the detailed
methodology for using the Reinforcement Learning toolkit to induce the dialogue management policies in this thesis. Chapters 4 through 7 present the three empirical studies. Chapter 4 focuses on collecting the Exploratory corpus. Study 2, collecting the DichGain corpus,
is described in Chapter 5. Chapter 6 discusses the process for deriving the NormGain and
the InvNormGain tutorial tactics for Study 3. Chapter 7 presents an experimental comparison of the induced NormGain and InvNormGain tutorial tactics in Study 3. Chapter 8
presents a general comparison across three studies and summarizes the conclusions. Chapter
9 discusses contributions to the fields of Cognitive and Learning Science, and to the fields
of Artificial Intelligence and Education, Intelligent Tutoring Systems, and Educational Data
Mining. Finally, this chapter considers future research initiatives that may evolve from this
work.

27

2.0

CORDILLERA

This dissertation made use of the Cordillera system [VanLehn et al., 2007b]. Cordillera
is a Natural Language (NL) based Tutoring System for introductory physics. The word
“cordillera” is defined as an extensive range of mountains along a coastline, often consisting
of a number of parallel chains. The Andes mountain range in South America is an example
which includes the Cordillera Oriental and the Cordillera Occidental. As noted in Chapter
1, four different versions of the system were constructed, each of which differed only in terms
of the tutoring tactics employed. Random-Cordillera, used in Study 1, made elicit/tell
(ET) and justify/skip-justify (JS) decisions randomly; Dichotic Gain (DichGain) Cordillera,
used in Study 2, followed DichGain policies induced from the exploratory corpus; while
the Normalized Gain (NormGain) and Inverse Normalized Gain (InvNormGain) Cordillera
systems, used in Study 3, followed the NormGain and InvNormGain policies induced from the
Exploratory and DichGain corpora, individually or combined. The remaining components
of the system, including the GUI interface, were identical for all participants.
Cordillera is based upon the TuTalk NL tutorial dialogue toolkit [Jordan et al., 2006,
Jordan et al., 2007]. TuTalk is an authoring tool which enables domain experts to construct
natural language tutoring systems without programming. Instead, the domain experts focus
on defining the tutoring content through scripts, which are then used for automating interaction. TuTalk supports dialogues in which the tutor tries to elicit a line of reasoning from a
student via a series of questions. This style of dialogue was inspired by the CIRCSIM-Tutor’s
directed lines of reasoning [Evens and Michael, 2006]. In addition, TuTalk is modular, so
that core modules, such as NL understanding, can be replaced or supplemented as needed.
To reduce confounds due to imperfect NL understanding in our experiments, the NL understanding module was replaced with a human interpreter called the language understanding
28

Figure 2.1: Student Interface

wizard [Bernsen and Dybkjaer, 1997]. In this format, Cordillera works as a communications
framework that connects a student interface to a wizard interface. The student interface is
used by students to read the tutor’s tutorial instructions, to answer his or her questions, and
to respond to them by means of natural language entries. The wizard interface is used to
match students’ answers to a list of potential responses. These two interfaces are discussed
in detail below.

2.1

STUDENT INTERFACE

Figure 2.1 shows a screen shot of the student interface. The Message Window, located in the
bottom-left corner is where the dialogue interaction takes place. The remaining four panes
29

are the Dialogue History Pane (upper-left), Problem Statement pane (upper-right), Variable
Pane (lower-right) and the Equation Pane (not shown). The tabs included on three of the
panels allow the student to select which four panels are visible and how where they will be
displayed. Brief descriptions of each pane follow.
The Message Window is the focus of interaction between the student and tutor. All
tutor messages appear here. Messages are displayed in this window, and students are then
able to enter a response below. In some cases the response is merely an acknowledgement,
i.e. clicking the [OK] button to proceed to the next action. In other cases, such as when the
tutor asks a question, the student can submit an answer by typing in a text field.
The Dialogue History Pane shows a record of the student-tutor dialogue thus far.
The Problem Statement Pane shows the problem statement and any accompanying figures if present.
The Variable Description Pane shows all the variables defined during problem solving.
The variables can be defined either by the student using a form interface or provided by the
tutor (elicit vs. tell).
Finally, the Equation Pane displays the equations that have been input either by students
or by tutors up to that point in the problem solving. An equation is presented as a twocolumn table where each row consists of a formula and its description. A description consists
of the name of the principle and its arguments.

2.2

WIZARD INTERFACE

The Wizard Interface, shown in Figure 2.2, mirrors the student interface in all respects
with the exception of the Message Window. This is replaced by the Student Response
Classification Window, which displays the student’s most recent response along with a set of
check-boxes for classifying the response. In the example below, the student’s response was
classified as the third choice. Once the student’s response has classified, the system would
follow control scripts to decide what to do next, and the dialogue manager would decide how
to do it. If none of the choices match the student’s entry, then the wizard makes no selection
30

Figure 2.2: Students’ Response Classification Window in Wizard Interface

and simply clicks the OK button.

2.3

AN EXAMPLE SCRIPT

Cordillera dialogues are governed by control scripts authored by domain experts. These
scripts control the dialogue messages as well as the content of each information panel. An
example script is shown in Table 2.1. In the example script, the highest level step is “definesystem” in Line 1. By name, it is about defining a system. This step consists of three
micro-steps listed in the order of Lines 2, 3, and 4.

31

Table 2.1: A Sample Cordillera Script

1. g define-system
2.

do choose-system SEM ELICIT/TELL

3.

do system-justification SEM JUSTIFY; ELICIT/TELL

4.

do isolated-system SEM ELICIT/TELL

5.

g choose-system SEM ELICIT

6.

say “What would be your choice of the system for this problem?”

7.

if “truck-Earth.” true

8.

if “truck and the Earth.” true

9.

otherwise do bottom out choose-system

10.

g bottom out choose-system

11.

say “What are the object(s) in the problem?”

12.

if “truck and the Earth” true

13.

otherwise say “[cont]There are two objects here, the truck and the Earth.”

14.

say “[cont]The best choice of the system here is to select both.”

15.
16.

g choose-system SEM TELL
say “[cont]There are two objects in the problem, the truck and the Earth. The
best choice of the system for this problem is to choose both. “

Line 2 shows that the tutor needs to make an Elicit/Tell (ET) decision for the microstep “choose-system” since it is labeled with “SEM ELICIT/TELL.” Here “SEM” stands
for the term “semantic”. This is a feature of Tutalk, which was meant to be used to mark
semantically similar turns and to allow the student model to make decisions relative to that
semantically similar content. Line 3 shows that the tutor needs to make two decisions on the
32

micro-step “system-justification” since it is labelled with “SEM JUSTIFY; ELICIT/TELL”.
When a tutorial decision step involves both ET and JS decisions, the system always makes
the JS decision first. If it decides to skip the justification step, then the system does not
need to make the ET decision and goes to next micro-step “isolated-system” in Line 4. On
the other hand, if it decides not to skip the “system-justification”, the system would then
make the ET decision. Line 4 shows that the tutor needs to make the ET decision on the
micro-step “isolated-system”. The next paragraph describes how the elicit and tell versions
of the “choose-system ” are executed in Cordillera.

Executing the Elicit Version of “choose-system”: If the system decides to elicit on
the micro-step “choose-system ” in Line 2, then the scripts from Lines 5-14 will be executed,
because they are the elicit version of “choose-system. ” Lines beginning with the command
“say”, that is, lines 6, 11, 14, and 16, initiate a tutor message. For each “say” line, if the
content immediately following it is a normal sentence, as in lines 6 and 11, then the tutor will send the text to the student as a question, requiring the student to respond. For
example, when TuTalk sends Line 6 to Cordillera, it will then display the question “What
would be your choice of the system for this problem?” in the Message Window on the student’s interface along with a text field for the student’s answer. The question will also be
shown in the Student Response Classification Window on the Wizard Interface. Once the
student inputs an answer and clicks OK, that answer will be added to the Student Response
Classification Window in the Wizard Interface along with the set of possible choices as a
checklist. These choices are taken from the lines prefixed with if below the “say.” In the case
shown in Figure 2.1 the answer choices for the “say” in Line 6 are “truck-earth” in Line 7 or
“truck and the earth” in Line 8. The human wizard chooses from this list the answer that
is closest to the student’s answer and submits it. Unmatched answers are left blank. Both
the student’s answer and the wizard’s match are sent to the system to guide the next decision.

Executing the Tell Version of “choose-system”: On the other hand, if the system
decided to tell on “choose-system ”, then the tell version of “choose-system” in Lines 15-16
will be executed. If the contents after “say” is a sentence starting with [cont], which rep33

resents “continue,” as in lines 14 and 16, then the tutor will tell the sentence following the
[cont]. For example, when line 16 is sent to Cordillera by TuTalk, the Message Window on
the student’s Interface will show the tutor’s message “There are two objects in the problem,
the truck and the Earth. The best choice of the system for this problem is to choose both.”
followed by an [OK] button. The student acknowledges the dialogue by clicking the OK
button at which point the dialogue will move on to the next topic.

Executing Justify/Skip-justify: For Justify/Skip-justify (JS) decisions, the scripts are
much simpler. An example of this is listed in Line 3. The label “JUSTIFY” in 3 indicates
that this is a JS tutorial decision step. If the dialogue manager decides to execute the microstep system-justification in Line3, the system will make the next ET decision on the step;
otherwise, the dialogue manager skips this micro-step and goes directly to execute the next
micro-step “isolated-system” in Line 4.
To summarize, the design of Cordillera allows domain experts to manage a naturallanguage tutorial dialogue including GUI components such as variable listings and an equation display. The scripts allow variables and equations to be added or removed as needed as
well as for selected portions of the display to be highlighted. Alternative dialogue actions
such as elicits and tells, justify and skip-justify, are encoded in the dialogue for selection by
the dialogue manager. The script authors determine the flow of the dialogue within these
alternatives and the content of each question, including alternative choices. The NL tutoring system or the wizards in the thesis study match these alternatives to student input at
runtime.
At present, the dialogue scripts can be written in XML or other more readable formats,
which are then compiled by the TuTalk system into executable script form or other textual
formats. TuTalk will be able to translate these human readable files into TuTalk- and
Cordillera-readable ones. Moreover, the reverse is also possible. As a result, the domain
experts can build an NL tutoring system by simply focusing only on the subject matter.
This chapter illustrated the Cordillera system. Chapter 3 will present a detailed description of the research methodology focusing on how to apply reinforcement learning to induce
the tutorial policies in this thesis.

34

3.0

REINFORCEMENT LEARNING PROCEDURE

Chapter 3 further develops the previous discussion of RL and describes the general procedure
by which tutorial dialogue policies were induced from student interactivity data. The chapter
begins with a description of how a problem of inducing pedagogical tutorial tactics can be
fit into the general RL and MDP framework. In this thesis it is assumed that inducing
tutorial tactics specific to each Knowledge Component (KC) will be more effective than
inducing an overall KC-general policy. This chapter provides an overview of KCs (a more
detailed description of the identified KCs may be found in Chapter 4) and the approach
used to generate KC-based MDPs from the training corpus. It also describes the induction
toolkit employed and the assessment metrics used. Finally, the chapter discusses the issues
confronted when the induced policies were implemented back into Cordillera.

In this dissertation, a toolkit is used to calculate an optimal dialogue policy given a
suitable Markov Decision Process (MDP) model. The major challenge faced therefore was
the production of the MDP model, especially the KC-based MDPs. There were five distinct
issues that needed to be addressed. The list included collecting and/or selecting training
corpora from which the tutorial tactics will be derived, determining on which KCs the
tutorial tactics should be induced for, the reward function, the state representation, and how
to handle conflicting policies. These issues are discussed below. Additional details about
how these issues are addressed in Studies 2 and 3 and the resulting models are presented in
Chapters 5 and 6.
35

3.1

REINFORCEMENT LEARNING FOR TUTORIAL TACTICS

Previous research on using RL to improve dialogue systems (e.g. [Levin and Pieraccini, 1997,
Singh et al., 1999]) has typically used MDP’s [Sutton and Barto, 1998] to model dialogue
data. The central idea behind this approach is to transform the problem of inducing effective
dialogue policies into computing an optimal policy for choosing actions in an MDP. An MDP
formally corresponds to a 4-tuple (S, A, T, R), in which:
S = {S1 , · · · , Sn } is a state space.
A = {A1 , · · · , Am } is an action space represented by a set of action variables;
T : S × A × S → [0, 1] is a set of transition probabilities between states that describe the
dynamics of the modeled system; for example: P (Sj |Si , Ak ) is the probability that the
model would transition from state Si to state Sj by taking action Ak .
R : S × A × S → R denotes a reward model that assigns rewards to state transitions and
models payoffs associated with such transitions.
Additionally, π : S → A is defined as a policy or tutorial tactics.

Dialogue management can be easily represented using an MDP: the states are vector
representations composed of relevant student and dialogue characteristics; the transitions
are dialogue system acts; and the reward function is calculated from the dialogue system’s
success measures such as completion on task, and, in the present case, learning gains. More
formally, we can view each dialogue di as a trajectory in the chosen state space determined
by the system actions and user responses:
a1d ,rd1

s1di

i

i

−−−−→

s2di

nd

a2d ,rd2
i

nd

i
i
nd ad ,rd
· · · sdi i −−i−−−i→

i

−−−−→

ajd ,rdj

i
i
Here sjdi −−−
−→
sj+1
indicated that at the jth turn in the dialogue di , the system was
di

in state sjdi , executed action ajdi , received reward rdj i , and then transferred into state sj+1
di .
The number of turns in di is ndi . For a training corpus consisting of L dialogues with
n1 , n2 , · · · , nL turns in each dialogue respectively, the training corpus looks like:
s1d1

a1d ,rd1
1

1

−−−−→

s2d1

1

n

n

a2d ,rd2

1

−−−−→

· · · snd11

ad 1 ,rd 1
1

1
−−−−→

···
a1d ,rd1

n

a2d ,rd2

n

ad i ,rd i

i
i
i
i
i
s1di −−−
−→
s2di −−−
−→
· · · sndii −−i−−→

···
s1dL

a1d ,rd1
L

L

−−−−→

s2dL

a2d ,rd2
L

L

−−−−→

36

n

· · · sndLL

n

ad L ,rd L
L

L
−−−−−→

Once the MDP structure S, A, R has been defined, the model parameters T are estimated
,m
from the training corpus as: T = {p(Sj |Si , Ak )}k=1,···
i,j=1,··· ,n . More specifically, it is calculated

by taking the number of times that the system is in state Si , took step Ak , and arrived in
state Sj divided by the number of times the system was in Si and took Ak . The reliability
of these estimates clearly depends upon the size and structure of the training dataset. Once
a complete MDP is constructed, a dynamic programming approach can be used to learn the
optimal control policy π ∗ , i.e. the set of actions the model should take at each state, to
maximize its expected cumulative reward.

3.2

ISSUE 1: TRAINING CORPUS

One of the main characteristics that differentiate RL from other machine learning techniques
is exploration. In order to have confidence in the constructed MDP, the training corpus must
explore various possible actions from various possible states, and preferably, many times. In
other words, the training corpus must be exploratory with respect to the chosen states and
actions. If we never try an allowed action from a certain state, we cannot expect to know the
value of taking that action in that state. As a result, unexplored state transitions cannot be
estimated, and transitions that are explored infrequently will have poor or strongly biased
estimates. Even a large, but biased, corpus presents problems in that it may focus extensively
on one small subset of the domain. This is especially true with pre-existing tutoring corpora
where the data is gathered by using a system with a hand-tooled rule set. In that situation,
the existing policy and the subjects’ use of it may bias the dataset and prevent adequate
exploration. Some authors have proposed using simulated students to generate training data
[Levin and Pieraccini, 1997, Young, 1999]. It is still an open question, however, about what
causes students to learn and how they learn. As a result, constructing a valid simulation
that provides an accurate estimate of students’ responses and their learning is doubtful.
Therefore this thesis focused solely on real user data. Singh et al. suggest that authors avoid
using biased data by collecting “exploratory data,”, that is, data collected from a system
that makes tutorial decisions randomly, thus ensuring that the transitions are adequately
37

explored [Singh et al., 2002].
In this research two approaches were adopted. In Study 1 students made use of RandomCordillera which made the crucial Elicit/Tell and Justify/Skip-justify decisions randomly.
In Study 2 students made use of Dichotic Gain (DichGain) Cordillera which made decisions
based upon a policy induced from the training corpus collected in Study 1. The former
random route is consistent with the exploration literature, while the latter route is consistent
with the task of gradually improving induced policies over time. As will be described in
Chapter 6, in preparation for Study 3 both corpora were used for policy induction.

3.3

ISSUE 2: KNOWLEDGE COMPONENTS

In tutoring literature, it is commonly assumed that relevant knowledge in domains such as
math and science is structured as a set of independent but co-occurring Knowledge Components (KCs). A KC is “a generalization of everyday terms like concept, principle, fact,
or skill, and cognitive science terms like schema, production rule, misconception, or facet”
[VanLehn et al., 2007b]. For the purposes of tutoring systems, these are the atomic units of
knowledge. Problem solving in such domains typically involves complex problems consisting
of multiple steps, each of which involves a single or a combination of independent KCs. For
example, a simple algebraic equation, 2x + 5 = 21 can be solved via two steps: 1) subtract
the same term 5 from both sides of the equation; and 2) divide both sides by the non-zero
term 2. Here subtracting the same term from both sides of the equation is one KC and dividing both sides of the equation by the non-zero term is another KC. As problems grow more
complex, the number of KC’s involved, and their combinations, can increase exponentially.
In the tutoring literature it is commonly assumed that KC’s are learned independently
of one another. A number of standardized tests, for example, are constructed based on this
assumed independence among KCs. Techniques exist to re-engineer the definition of KCs
so that they are independently learnable [Cen et al., 2006, Cen et al., 2007], thus improving
the overall effectiveness of the resulting tutoring system. When dealing with a specific KC,
the expectation is that the tutor’s interactive decision on that KC, elicit or tell, would
38

be based upon the student’s mastery of the KC in question, its intrinsic difficulty, and
other relevant, but not necessarily known, factors specific to that KC. In other words, the
assumption is that an optimal policy for one KC might not be optimal for another. Therefore,
the assumption made in this dissertation is that inducing tutorial tactics specific to each
Knowledge Component (KC) would be more effective than inducing an overall KC-general
policy. The KCs identified by the domain experts for the domain are described below. In
order to derive KC-based tutorial tactics, KC-based MDPs needed to be generated from the
training corpus.

3.3.1

Identified KCs in the Selected Domain

In order to learn a policy for each KC, the KCs in a domain need to be identified. The
domain chosen for this dissertation covers the work-energy chapter in college-level physics
textbook. Two domain experts (not the author) who are also knowledge representation
experts identified 32 KCs in the domain. For example, KC20 and KC21 are the two KCs that
were involved in the majority of the tutorial decisions on elicit/tell (ET) and justify/skipjustify (JS) respectively.
Definition of Kinetic Energy (KE = 21 mv 2 ) —KC20 : If an object is moving, then its
kinetic energy at a time is 12 mv 2 , where m is the object’s mass and v is the magnitude
of the object’s instantaneous velocity.
Definition of Gravitational Potential Energy (GP E = mgh) —KC21 : If an object
and a planet are in a system (or equivalently, the gravitational force of the earth on the
object is an internal force), then their gravitational potential energy is mgh, where m
is the mass of the object, g is the gravitational acceleration of the planet, and h is the
object’s height above a zero point. The zero point is arbitrary, but is often chosen to be
the planet’s surface.
Note that a complicated domain like physics can often be broken into many KCs. Here the
32 identified KCs are believed to cover the most important knowledge in the domain. There
are some other KCs shown in the tutorial decision steps that are not among 32 identified
KCs. After identifying the KCs involved in the domain, we needed to decide which KCs are
39

needed in order to induce KC-based policies. Intuitively, it should depend on the relative
importance of these KCs and also on the frequency of their appearances in the training
corpus, and so on. To determine which KCs were required to induce the KC-based tutorial
tactics, the training tutorial dialogues were annotated with the tutorial action decisions
based on the KCs involved.

3.3.2

Tutorial Dialogue Annotation

A group of five individuals (including the author) annotated each of the tutoring dialogues
and action decisions with the relevant KCs. The KCs were drawn from the set of 32 KCs
described in Appendix A. Each tutorial dialogue consists of one human participant’s interaction with Cordillera to solve 7 problems. For each of seven problems, there were at least
two annotators. For each of 32 identified KCs, the final kappa was ≥ 0.77 which is fairly
high given the complexity of the task.
Table 3.1 shows an example. Here lines 1, 5, 6, 7, 9 and 11 are tutor turns with the
action decisions and their target KCs listed on the right. Next, various labels in Table 3.1
will be explained.
Line 1 was labelled “(ELICIT, KC22 )”. This indicates that this is an ET decision step
on KC22 and the tutor decided to elicit.
Line 2 was labelled “(+KC22 )”. It shows that the student answered the question correctly, so a +KC22 was added to the student’s turn. Or it is a correct entry generated by
the student. Alternatively, a −KC22 would represent that it is an incorrect entry input by
the student on KC22 .
Line 5 was labelled “(TELL, ∗KC22 )”. It indicates that the tutor made an ET decision
on KC22 and decided to tell the student directly. Here “∗KC22 ” indicates that this piece of
information is about KC22 , and it was generated by the tutor. By contrast, ones that were
generated by the students are labelled with “+” and “-” to represent correct and incorrect
entries respectively.
Line 6 was labelled with a simple “(TELL)”. It indicated that the tutor made an ET
decision, but this step does not involve any identified KCs.

40

Table 3.1: Example of Tutorial Dialogue with KC Labels

1. T: At T2 we also know the spring is uncompressed. So, what type of energy can we
easily find now? (ELICIT, KC22 )
2. S: spe (+KC22 )
3. T: Yes, I agree.
4. T: At this point we know the values for KE2 and GPE2. It is time to compute the value
for SPE2.
5. T: It is important to remember that we cannot find the SPE of an OBJECT. We need
to find the spring potential energy of a pair: object-spring. (TELL, ∗KC22 )
6. T: It is possible to compute the spring potential energy at T2 because we know the
spring is uncompressed. That is, d2=0 m.(TELL)
7. T: Using this information we can infer that the spring potential energy of the blockspring pair at T2 is SPE2=0 J. (TELL, {∗KC22 ∗KC23 )}
8. T: Now we have all we need to find the value for the system’s total mechanical energy
at T2.
9. T: TME of the system at T2 is the sum of the block‘s KE and the block-Earth pair‘s
GPE because the value for the spring potential energy is zero. In this case TME2=11.88
J+1.62 J =13.5 J. (TELL, {∗KC23 ∗KC24 )})
10. T: At this point we have found the value for TME2. We can now easily find the system‘s
TME at all other time points since the system is isolated.
11. T: What principle gives us the system‘s total mechanical energy at every time point,
knowing its value at T2 and also given that the system is isolated? (ELICIT, {KC27 )})
12. S: student conservation of energy (+KC27 )

Line 9 was labelled with “(TELL, {∗KC23 ∗KC24 )})”. It indicated that the tutor made
an ET decision and that this step involves two KCs: {KC23 KC24 )}). Because the tutor
told the information, both KCs are labeled with “*”.

41

The last tutor turn in the example is Line 11, labelled “(ELICIT, {KC27 )})”. This
indicates that the tutor made an ET decision on KC27 and decided to elicit the information
from the student.
Finally, Line 12 was labelled “(+KC27 )”. It indicated that it was a student turn and
the student input a correct entry for KC27 .
For a training dialogue di , one dialogue trajectory can be constructed for each KC. More
specifically, we use ndi ,KCk to represent the number of turns on KCk in the dialogue di and
we expect different ndi ,KCk for different KCk because the number of tutorial actions on each
KC varies in the di . Thus, we have:
s1di ,KC1

i ,KC1

,rd1

i ,KCk

,rd1

a1d

i ,KC1

−−−−−−−−−→

s2di ,KC1
···

s1di ,KCk

a1d

i ,KCk

−−−−−−−−−→

s2di ,KCk

i ,KCP

,rd1

i ,KCP

s1di ,KCP −−−−−−−−−−→ s2di ,KCP

ni,KC

ni,KC

ni,KC

ad ,KC k ,rd ,KC k
ni,KC
k
k
−−i−−→
· · · sdi ,KCkk −−i−−−

···
a1d

ni,KC

ad ,KC 1 ,rd ,KC 1
n
1
1
i
i
1
−
· · · sdii,KC
,KC1 −−−−−−−−→

ni,KC

ni,KC

P
P
ni,KCP adi ,KCP ,rdi ,KCP
· · · sdi ,KCP −−−−−−−−−−→

Here P is the number of the KCs used to induce policies, and they are represented as:
ajd

,KC

,rdj

,KC

k
k
−−i−−→
sj+1
KC1 , · · · , KCP . And sjdi ,KCk −−i−−−
di ,KCk indicated that in the jth turn in the

dialogue di for KCk , the system was in state sjdi ,KCk , executed action ajdi ,KCk , received
reward rdj i ,KCk , and then transferred into state sj+1
di ,KCk .
For each KCk , all of the dialogue trajectories on KCk from each tutorial dialogue were
combined in the training corpus and used to generate the training corpus for KCk . From that
training corpus, an M DPKCk could be constructed and a dynamic programming approach
∗
could be used to learn the optimal control policy πKC
. In the next section, the procedure
k

is described in detail.

42

3.4

KC-BASED MDPS

In order to induce KC-based tutorial tactics, an MDP model was constructed for each KC.
For example, for KC KCk , the corresponding MDPKCk is defined as:
SKCk = {SKCk ,1 , · · · , SKCk ,n } correspond to the dialogue states related with KCk
AKCk = {AKCk ,1 , · · · , AKCk ,m } correspond to the tutorial actions involving KCk only.
TKCk : SKCk × AKCk × SKCk → [0, 1] is a set of transition probabilities between KCk
related states and action AKCk on KCk . Once the M DPKCk structure has been
defined, they are estimated from the corresponding annotated training corpus on KCk .
RKCk : S × AKCk × SKCk → RKCk are defined to reflect dialog performance metric on KCk
only.
∗
Additionally, πKC
: SKCk → AKCk is defined as a KC-based policy or tutorial tactics for
k
KCk .

The general approach for defining KC-based state representation, action choice space,
and reward function is described in the next subsection of this dissertation.

3.4.1

Issue 3: State Representation

For RL, as with all machine learning tasks, success is dependent upon choosing an appropriate
set of features to represent dialogue states. An ideal state representation should include all
of the tutorial dialogue information that is relevant and necessary to determine what action
the system should take next. Ideally this would include a complete record of the tutoring
interaction thus far–both for the present problem and preceding problems–as well as derived
features such as gender, MSAT, detailed pre-test scores, and so on. However, the high cost
of obtaining human tutorial dialogues makes it crucial to limit the size of the state space.
Nevertheless, even a state based on a handful of features can yield an enormous state space.
In order to obtain an effective representation that both minimizes data sparsity while
retaining sufficient information, a small but carefully selected feature space is preferable.
Using a small state representation to approximate the true state reduces the amount of data
required. The disadvantage of doing so is that it increases the risk that educationally relevant
features will be missed, resulting in a non-representative state space. To this end this thesis
began with a large set of features to which a series of feature-selection methods were applied
43

to reduce them to a tractable subset. Because of this, the state representation issue can be
divided into four sub-issues for discussion purposes.

3.4.1.1

Sub-Issue 1: Feature Choices This sub-issue concerned what types of relevant

information can be included in the state space. For this dissertation only features that
could be computed automatically or evaluated objectively, such as gender, were included.
Hand-annotated dialogue features were omitted as the tutor would require the features to
be available in real time when the learned policies are employed. Moreover, in order to
induce KC-based tutorial tactics, the state representations were also KC-based. For example,
“pctCorrectKCPM” is a feature choice in Study 2. It is defined as the students’ performance
on the specific KC. In MDP(KC20 ) it refers to the students’ performance on KC20 , while in
MDP(KC21 ) it refers to the students’ performance on MDP(KC21 ).

3.4.1.2

Sub-Issue 2: Feature Discretization An MDP model generally requires all

the state features in the model to be discrete variables. Most of the features of interest
here, such as “pctCorrectKCPM”, are continuous. It is thus necessary to choose an effective
method for feature discretization.

3.4.1.3

Sub-Issue 3: Feature Selection One of the main challenges in this dissertation

was feature selection. For Study 2, a greedy-like search strategy for feature selection was
employed, while in Study 3 more extensive feature selection methods were employed. These
will be discussed in more detail in Chapters 5 and 6 respectively.

3.4.1.4

Sub-Issue 4: Maximum Number of Features The last major sub-issue re-

lated to state representation is the maximum number of features to be included in the state
space. The number should be small so that we have enough training data to cover each
state, yet large enough to would include enough features to represent states without losing
information necessary to make good tutorial decisions.
44

3.4.2

KC-based Action

In the present studies there are two types of choices in the action space: Elicit/Tell (ET)
and Justify/Skip-justify (JS). These are available to the tutor at different times for different
KCs in the tutoring process. In each MDP(KCk ), the tutorial action choices were those that
involved the specific KCk only.

3.4.3

Issue 4: KC-based Reward

Based on previous research by [Tetreault and Litman, 2008, Tetreault and Litman, 2006b,
Tetreault and Litman, 2006a] Normalized Learning Gain (NLG) was selected as a reward
function because it measures students’ gain irrespective of their incoming competence. In
addition to mapping the training problems to KCs, a domain expert also mapped the pre/post test problems to the sets of relevant KCs. This resulted in a KC-specific NLG score
for each student. The reward function is defined as follows:

N LGKCk =

posttestKCk − pretestKCk
1 − pretestKCk

(3.1)

Here post − testKCk and pre − testKCk refer to the KC-specific pre- and post-test scores on
KCk for each student.
In this dissertation, only terminal dialogue states have non-zero rewards because a student’s NLG will not be available until the entire of his/her tutorial dialogue is completed.
nd

,KC

i
k
Thus for a tutorial dialogue di , rd1i ,KCk · · · , rdi ,KC
k

reward

ndi ,KCk
rdi ,KC
k

−1

are all equal to 0 and only the final

equals to non-zero rewards. The final reward in this thesis is determined by

the student’s NLG on the corresponding KC. Here ndi ,KCk represents the number of turns
that tutorial dialogue di had made decisions on KCk .

3.5

INDUCE KC-GENERAL POLICIES

To this point, the dissertation has focused on inducing KC-based tutorial tactics. However,
certain tutorial decision steps do not involve any identified KCs. Line 6 in Table 3.1 is
45

such an example. Next we need to decide how the dialogue manager should perform in this
instance. In this dissertation, the issue was resolved by inducing a KC-general tutorial tactic.
In both Study 2 and Study 3, we induced one KC-general policy for ET and one for JS.
This was done by using the same general approach as inducing KC-based policies except that
the state representation, action, and choice are no longer based on any particular KC. For
KC-general policies, the final rewards are calculated based upon the student’s cumulative
KC-based NLGs. When a tutorial decision step does not involve any KCs, the dialogue
manager would follow the KC-general policies.
Once an MDP model has been completed, calculation of an optimal policy is straightforward. This dissertation work employed an RL toolkit developed by Tetreault and Litman
[Tetreault and Litman, 2008, Tetreault and Litman, 2006b, Tetreault and Litman, 2006a].

3.6

TETREAULT AND LITMAN’S RL TOOLKIT

Tetreault, & Litman’s toolkit [Tetreault and Litman, 2008, Tetreault and Litman, 2006b,
Tetreault and Litman, 2006a] uses a dynamic programming algorithm for policy iteration
[Sutton and Barto, 1998]. The code was originally built on the MDP toolkit written in Matlab [Chades et al., 2005]. The purpose of this algorithm is to handle the problem of reward
propagation. As noted above, rewards, in this case learning gains, are not assigned until the
end of the tutoring process, long after any action has occurred. The dynamic programming
algorithm propagates the rewards back to the internal states weighting the V-value of each
state, s via the following recursive equation:

V (s) = max
R(s, a) +
0

X

a

P (s0 |s, a)γV (s0 )

(3.2)

s0

Here P (s0 |s, a) is the estimated transition model from the training corpus, R(s, a) is
the estimated reward model, and 0 ≤ γ ≤ 1 is a discount factor. If γ is less than 1,
then it will discount rewards obtained later. For all the studies reported here, a discount
factor of 0.9 was used, which is common in other RL models [Tetreault and Litman, 2008,
Tetreault and Litman, 2006b, Tetreault and Litman, 2006a].
46

The V-values, as defined by Equation 3.2, can be estimated to within a desired threshold
using policy iteration [Sutton and Barto, 1998]. Here an estimated v-value and a best possible action to take for each state are recorded. These are then iteratively updated based on
the values of its neighboring states. This iteration stops when each update yields a difference
below some threshold . Once the policy iteration process is complete, the optimal dialogue
policy π∗ is obtained by selecting the action that produces the highest expected reward (or
V-value) for that state. At this time we also compute the Expected Cumulative Reward
(ECR) and a 95% confidence interval for the ECR (hereafter, 95%CI) for the optimal policy.

3.6.1

Expected Cumulative Reward (ECR)

The Expected Cumulative Reward (ECR) of a policy is derived from a side calculation in
the policy iteration algorithm: the V-values of each state, the expected reward of starting
from that state and finishing at one of the final states. More specifically, the ECR of a policy
π can by calculate as follows:

ECRπ =

m
X
i=1

Ni
× V (si )
N1 + · · · + Nm

(3.3)

Where s1 , · · · , sm is the set of all starting states and v(si ) is the V-values for state si ; Ni
is the number of times that si appears as a start state in the model and it is normalized by
dividing

Ni
.
N1 +···+Nm

In other words, the ECR of a policy π is calculated by summing over all

the initial start states in the model space and weighting them by the frequency each state
appears as a start state.
In Tetreault and Litman’s work [Tetreault and Litman, 2008], the authors used ECR
as a evaluation metric for feature selection. Additionally, ECR has been widely used as
the criteria for evaluating the policy in the area of inducing policy from simulated corpus
[Janarthanam and Lemon, 2009, Williams and Young, 2007b, Williams and Young, 2007a].
More specifically, given two MDP structures: M DP1 = {S1 , A, R} and M DP2 = {S2 , A, R},
which have the same action choices A and reward function R but different state representation, the transition probability T1 and T2 were estimated from the same training corpus.
Two different policies, π1 and π2 , were derived based on M DP1 and M DP2 respectively.
47

The higher the ECR value of a policy, the better the policy is supposed to perform.

3.6.2

Confidence Interval

Tetreault and Litman pointed out one limitation of using the ECR as an evaluation metric
for a policy: it assumes that there was sufficient collected data to derive a reliable policy
[Tetreault and Litman, 2008, Tetreault and Litman, 2006b, Tetreault and Litman, 2006a].
However, in practice researchers frequently have to deal with issues of data sparsity. They
proposed a novel approach by taking into account the reliability of the transition probability
estimates from the training data and constructing a confidence interval for the ECR for the
learned policy.
As described earlier, an estimate for the ECR was computed by using the transition probabilities derived from the training corpus. Note that these transition probabilities are simply
estimates which are more or less accurate, depending on how much data is available. As an
illustration, Tetreault and Litman used the following example [Tetreault and Litman, 2008]:
In an MDP model, we have S = {S1 , S2 , S3 }, A = {A1 , A2 }. From a training corpus, there
were ten cases that an action A1 was taken from state S1 . Out of these, three times the
system transitioned back to state S1 , two times it transitioned to state S2 , and five times to
state S3 . Thus we have

3
= 0.3
10
2
P (S2 |S1 , A1 ) =
= 0.2
10
5
P (S3 |S1 , A1 ) =
= 0.5
10

P (S1 |S1 , A1 ) =

(3.4)
(3.5)
(3.6)

From the same corpus, there were 1000 times that action A2 was taken from state S2 .
In 300 of those cases it transitioned to state S1 ; in 200 cases to state S2 ; and the remaining
48

500 times to state S3 . Thus,
300
= 0.3
1000
200
P (S2 |S2 , A2 ) =
= 0.2
1000
500
P (S3 |S2 , A2 ) =
= 0.5
1000
P (S1 |S2 , A2 ) =

(3.7)
(3.8)
(3.9)

While both sets of transition parameters have the same value, the second set is more
reliable. In order to take this lack of reliability into account, Tetreault and Litman proposed a CI estimate based upon the available data in [Tetreault and Litman, 2008], see also
[Tetreault and Litman, 2006b, Tetreault and Litman, 2006a]. It is done by taking transition
matrix T for slice and sample from each row using Dirichlet distribution for q times. As a
result, it generates a large number of new transition metrics T1 , T2 , · · · , Tq that are all very
similar to T . They then run MDP on all q transition matrices to get a range of ECR’s (in
this dissertation m=1000 was used, which is also used in [Tetreault and Litman, 2008]).
Their algorithm looks like this:
1. Compute transition probability matrix T from from the training data.
2. Use Policy iteration to compute an optimal policy π ∗ for S,A,T,R.
3. Sample q transition metrics T1 , T2 , · · · , Tq by sampling from the Dirichlet
distributions corresponding to the counts observed in the training data;
4. Compute the value of the optimal policy π ∗ in each of these m models.
5. Numerically build the 95% confidence interval for the policy π ∗ based on the
resulting value estimates: the bounds for the confidence interval are set at the lowest
and highest 2.5 percentile of the resulting distributions.

3.6.3

An Example to Illustrate ECR and CI

This section illustrates ECR and CI with an example. In [Tetreault et al., 2007] the authors
employed five feature choices to represent the state space: Certainty, Correctness, Percent
Correct, Concept Repetition, and Frustration. Their system employed four possible tutor
actions: ask a simple answer question; ask a complex answer question; ask a combination of
the two; or do nothing. They estimated the reward value based upon the students’ NLG.
For the purpose of strategy induction, they assigned a reward of +100 if the students’ NLG
49

π1 :
Features: Certainty, Correctness, and Concept Repetition.
ECR: 42.56
95%CI: [28.37, 59.29]
Interval Width: 23.52

π2 :
Features: Certainty, Correctness, and Percent Correctness
ECR: 28.50
95%CI: [−5.89, 57.82]
Interval Width: 63.71

Figure 3.1: ECR and CI Sample Learned policies

was above the median value. NLG scores below the median value were assigned a score of
−100. Two of the learned policies are summarized in Figure 3.1.
According to this assessment, π1 will be both more effective and more reliable than π2
because the former has a higher ECR, but a narrower CI than the latter. In Study 3, both
the ECR and 95%CI were employed as feature selection criteria. More specifically, the
upper and lower bounds of the CI were used, and are referred to as the lower-bound and
upper-bound of the policy.

3.7

ISSUE 5: CONFLICTING POLICIES

Once the tutorial tactics were induced, in order to test their effectiveness on the real subjects
the researchers needed to implement them back to Cordillera. In order to execute these
tutorial tactics, the dialogue manager needed to keep a record of the student’s current states
50

on each KC. Moreover, it should also retain a KC-based record on the tutorial decision
steps. So when a tutorial decision step occurred, the dialogue manager first looked up the
KC(s) involved in that step and then looked up the corresponding policies. When a tutorial
decision did not involve any specific KCs, the dialogue manager followed the KC-general
tutorial tactics. When it involved a specific KC, the dialogue manager followed the tutorial
tactics for that KC only. However, sometimes a tutorial decision involved multiple KCs,
which generated conflicting decisions. In this case, the researchers needed to decide how the
dialogue manager should deal with conflicting policies. This was the fifth and final issue
requiring a decision.

3.8

DISCUSSION

To summarize, the general procedure for RL applications in this thesis was to first select a
training corpus and then which KCs would be used to derive specific tutorial tactics for. For
each KCi , two types of tutorial decisions: < ET, JS > were derived and a KC-based reward
RKCi for each student’s tutorial dialogue is defined. The KC-based reward was defined based
on the student’s KC-specific NLG scores on KCi . Then the KC-based feature choices were
defined and each feature choice was discretized into discrete variables. Finally, the procedure
described in Figure 3.2 was executed.
The five main RL-related issues addressed in this methodology are 1) selection of a
training corpus, 2) choosing knowledge components to derive specific tutorial tactics on,
3) determining the state representation, 4) defining the reward function, and finally, 5)
dealing with conflicting policies on multi-KC steps. State representation was divided into
four sub-issues: a) defining feature choices, b) identifying feature discretization procedure,
c) determining the feature selection procedure, and d) determining maximum number of
features included in a policy.
In Chapters 5 and 6, the procedure for applying RL to derive KC-based tutorial tactics
will be described, including how the five issues and four sub-issues were addressed. The rest
of the procedure in both Studies 2 and 3 follows the general methodology described above.
51

1. Select representations for dialogue states SKCi , AKCi , and reward RKCi
0
2. FOR each subset of features SKC
selected from S by following a feature selection procei

dure, do:
0
• Use a training corpus to building an MDP’ model based on SKC
, AKCi and RKCi .
i
0
The transition probability TKC
of this MDP’ is approximated based on the collected
i

exploratory corpus.
• Compute a policy from the learned MDP’ by Tetreault, & Litman’s toolkit.
3. Select a policy from all of induced policies that has the highest ECR.

Figure 3.2: General RL Procedure For Inducing KC-based Tutorial Tactics

This series of studies was designed to investigate two primary research questions: (1) Do
pedagogical tutorial tactics on Elicit/Tell and Justify/Skip-justify impact students’ learning?
And (2) Is reinforcement learning a feasible method to induce tutorial tactics?
In Study 1, an exploratory corpus was collected by training a set of real students in a
version of Cordillera that made random ET and JS decisions. In Study 2, we defined a set of
18 pedagogically relevant features, applied a greedy-like feature selection method to narrow
the list down to four and applied RL to induce KC-based pedagogical tutorial tactics from
the Exploratory corpus. The induced policies were then incorporated back into Cordillera
and a second group of students was trained with this new version of Cordillera.
Finally, in Study 3, a set of 50 features was defined and a variety of feature selection
methods were used to winnow them down to a set of six. More specifically, two sets of pedagogical tutorial tactics were induced: Normalized Gain (NormGain) and Inverse Normalized
Gain (InvNormGain). The NormGain set was derived with the goal of enhancing the tutorial
decisions that contribute to the students’ learning; while the InvNormGain set was derived
with the goal of enhancing those decisions that contribute less or even none to the students’
learning. Both sets were then incorporated back into Cordillera, and students were trained
52

on the new versions of Cordillera with random assignments to conditions. The expectation
was that the NormGain group would out-perform the InvNormGain group.

53

4.0

STUDY 1: EXPLORATORY CORPUS

The goal in Study 1 was to collect an exploratory corpus. The main advantage of collecting
an exploratory training corpus is to potentially compute an effective policy within a large
state space using a relatively small amount of training data. It addresses the situation in
which collecting real-world experience is highly costly, but computation is relatively cheap.
For example, it has been shown that this approach is effective at automatically learning the
effective action to take in any state in various dialogue systems where collecting data is even
less expensive than ITSs [Williams et al., 2005, Walker, 2000, Singh et al., 2002].
Study 1 used the Random-Cordillera on which the dialogue manager made random decisions at each tutorial decision step. A set of real human participants interacted with
Random-Cordillera, from which an exploratory training corpus was collected for deriving
pedagogical tutorial tactics in Study 2 and Study 3. In the sections below this process will
be described in detail.

4.1

4.1.1

METHODS

Participants

Data was collected over four months during Fall 2007. Seventy college students were recruited. They were required to have a basic knowledge of high-school algebra, but not to
have taken college-level physics courses. All subjects were paid for their time, regardless of
completion. Subjects who completed the study took from two to three weeks to complete
the study. In all, 64 students completed the experiment.
54

4.1.2

Materials

As with the other studies in this thesis, Study 1 was done in the Physics work-energy domain,
a common component of introductory college physics courses.

4.1.2.1

32 Knowledge Components Two domain experts (not the author) who are

also knowledge representation experts, identified 32 KCs in the domain (see Appendix A).
They had experience identifying KCs for a series of previous studies involving college physics.
One example of their work is KC20 which is defined as:
Definition of Kinetic Energy (KE = 21 mv 2 ) —KC20 : If an object is moving, then its
kinetic energy at a time is 12 mv 2 , where m is the object’s mass and v is the magnitude
of the object’s instantaneous velocity.

4.1.2.2

Physics Textbook The physics textbook used in this study is web-based. It was

written by a domain expert who is also a native English speaker (not the author). It includes
all the physics concepts that were needed for the domain. For each physics concept and
domain principle, a general description was presented together with some worked examples
(see Appendix D). For example, the description of KC20 in the textbook begins with “One
type of energy, called kinetic energy (KE), is associated with individual objects. It depends
only on an object’s mass and on the magnitude of its velocity...”. More information can be
found at section D.0.2.13 in Appendix D. The textbook was 27 pages long. When reading
the textbook, students were free to move forward and back. Appendix D shows the textbook
content.

4.1.2.3

Pre- and Posttest The pre- and post-tests were identical in Study 1. Both

contained a total of 33 problems selected from the Physics literature (see Appendix E) by two
domain experts (not the author). The tests were given online and consisted of both multiplechoice and open-ended questions. The latter questions required the students to derive an
answer by writing or solving one or more equations. Once an answer was submitted, the
students automatically proceeded to the next question without receiving any feedback on the
55

Table 4.1: Major Principles of Work and Energy

KC

Descriptions of the principles

Expressions

KC1

Weight Law (w)

W = mg

KC14

Definition of Work (W)

W = F dcos(α)

KC20

Definition of Kinetic Energy (KE)

KE = 21 mv 2

KC21

Gravitational Potential Energy (GPE)

GP E = mgh

KC22

Spring Potential Energy (SPE)

SP E = 12 kd2

KC24

Total Mechanical Energy (TME)

T M E = KE + GP E + SP E

KC27

Conservation of Total Mechanical Energy

T M E1 = T M E2

(CTME)
KC28

Change of Total Mechanical Energy for Non-

N etW = T M E2 − T M E1

isolated Systems (TMENC)

correctness of a response. Students were not allowed to return to prior questions. Appendix
F listed the number of times each KC showed up in the tests. Except for KC7 , all the rest
of the KCs appeared in at least one test item. For example: the first test problem is an
open-ended question involving KC20 . It stated:
1. Enter the equation that defines the kinetic energy of an object (remember to use * for
multiplication and f̂or exponentiation, and be sure to include an = sign):

4.1.2.4

Domain Principles The eight major principles in the domain are shown in

Table 4.1. In Table 4.1, the first column lists its corresponding KC number. The second
column describes the name of the principle. The last column is the formula or mathematical
expression of the principle. To differentiate these KCs from the rest of 24 non-domain
principle KCs, the name the domain principle-related KCs were named as primary KCs. As
the table shows there are eight primary KCs in this domain. For example, the fourth row in
Table 4.1 is the definition of Kinetic Energy and its corresponding KC is KC20 .
56

4.1.2.5

Seven Training Problems Participants solved a series of seven training prob-

lems. The problem statements are listed in Appendix G. The problems were arranged in
order of increasing complexity. Table 4.2 contains a list of the problems in the order they
were presented to the students and identifies which of the eight main KCs were relevant to
each problem. For example, P4 is an example used earlier in this dissertation. It is defined
as follows:
A 0.6kg rock in space has a velocity of magnitude 2.0m/s at point A and kinetic energy
of 7.50J at point B. What is the net work done on the rock as it moves from A to B? We
define:
T0 : the time point when the rock is at point A.
T1 : the time point when the rock is at point B.

As mentioned in an earlier chapter, solving training problem P4 involved applying three
major domain principles, with some principles needing to be applied twice. The three domain
principles are: the definition of Kinetic Energy (KE: KE = 21 mv 2 ), the definition of Total
Mechanical Energy (TME: T M E = KE+GP E+SP E), and the Change of Total Mechanical
Energy for Non-isolated Systems (N etW = T M E2 − T M E1 ). These were represented as
KC20 , KC24 , and KC28 respectively. Therefore, the fifth row in Table 4.2 shows that the
relevant KCs for the training problem P4 are KC20 , KC24 , and KC28 .
Table 4.2: Seven Training Problems

Problems:
P1
P2
P3
P4
P5
P6
P7

4.1.2.6

KC1

KC14

KC20
X

Primary KCs
KC21 KC22

KC24

KC27

KC28

X
X

X

X
X
X
X

X
X
X

X

X
X
X
X

X
X
X
X

Training Scripts For each training problem, a tutorial script was written to

cover the relevant content. The content included how to solve the problem and the postproblem discussions. The tutorial scripts were written collaboratively by a group of five
57

authors (including this author). Four of the five were domain experts, while one was trained
in tutoring, specifically in the authoring of tutoring dialogues. Two of the domain experts
were native English speakers while the remainder were fluent in English. Appendix I provides
an example of the written tutorial script for training problem P4. Eventually each script
was checked by at least two out of five domain experts, who agreed upon the content of the
script.

4.1.2.7

Random-Cordillera The students in Study 1 trained on Random-Cordillera.

The dialogue manager on Random-Cordillera did not follow any tutorial tactics, but made
all the tutorial decisions randomly.

4.1.2.8

Human Wizards As described earlier, in order to reduce confounds due to

imperfect Natural Language understanding, human wizards were used. In Study 1, there
were a total of six human wizards. Their sole function was to map students’ entries to the
closest answer. They cannot control which tutorial actions the dialogue manager should take
next.

4.1.2.9

Some Clarification On The Number Of KCs Appearing In This Disser-

tation As mentioned before, for the Work and Energy domain, we have identified a total
of 32 KCs. Among them, 31 KCs were evaluated in the pre- and post-tests (KC7 was not).
Of these 31 KCs, 21 KCs were involved in the ET decisions while 10 KCs were involved in
the JS decisions. In Study 2, KC-specific tutorial tactics for all possible KCs were learned,
so 21 KC-specific ET tutorial tactics and 10 JS ones were induced. In Study 3, however, the
main focus was on the eight primary KCs that are also domain principles. Among the eight
primary KCs, KC1 did not show up in any JS tutorial decisions. Therefore, in Study 3, eight
KC-specific ET tutorial tactics and seven KC-specific JS tutorial tactics were induced.

4.1.3

Procedure

The study itself consisted of five standard phases: 1) background survey, 2) pre-training, 3)
pre-test, 4) training, and 5) post-test. In each phase, students were not restricted to any
58

time limits. This was also true for Studies 2 and 3. The background survey asked students
for demographic information such as gender, age, SAT scores, high school GPA, experience
with algebra, calculus, physics, and other information (see Appendix C). Following the background survey, students read the physics textbook during the pre-training (see Appendix D)
and took the pre-test (see Appendix F). The physics textbook was only available during
phase 2, pre-training. This was also true for Studies 2 and 3.
In phase 4, students were first trained to solve a demonstration problem, which did not
include physics content, on Cordillera. The sole purpose of this step was to familiarize them
with the GUI interface. They then solved the seven training problems on Random-Cordillera.
Finally, students took the post-test which was identical to the pre-test. Students were given
the same set of questions in the same order.

4.1.4

Grading

All of the tests were graded in a double-blind manner by a single domain expert who was an
experienced grader (not the author). In a double-blind manner, neither the students nor the
grader know who belongs to which group. In this case, the grader was not familiar with the
hypotheses being tested. Each test question was assigned two grades: overall and KC-based
grade. The overall grade was a score in the range [0, 1] describing the correctness of an
answer as a whole. Since there were 33 test questions in all tests across the three studies,
the maximum overall score for each test was 33.
Under the KC-based grading criteria, the grader first identified all of the relevant KCs
for a test question, and then assigned a score in the range [0, 1] for each KC application.
Each of these scores was called the KC-based score. Except for KC7 , all the remaining
31 KCs were present in at least one question of the tests. The maximum score for a test
question under the KC-based grading was the number of KCs involved in the question. A
KC cumulative score was calculated for each student by summing up all KC-based scores
across all of the test questions in the test. In this thesis, there were a total of 168 KCs in all
pre- and post-tests.
For each KC, e.g. KCk , a KC-based score was calculated by simply summing over the
59

KC-based scores on KCk across all 33 test questions divided by the number of test questions
problems that involves KCk in the test. The number of occurrences of each KC in the preand post-tests in this dissertation is shown in the last row in Appendix F. It shows that the
frequencies of 31 KCs in the test vary from one to up to 16.
The following example illustrates these three grading scores. Figure 4.1 presents a student’s answer to a test question 10. In this example, the question statement is listed first
and then the correct answer. The mapped KCs refer to the two KCs that needed to be
applied to solve the question: KC20 and KC23 . Their corresponding descriptions are also
listed. The student’s answer is partially correct in that the number is correct but without
unit. So the overall score for this answer is 0.75 (maximum is 1 for each test question) and
the cumulative KC score is 1 (the maximum is 2 which is the number of KCs involved in
the test question 10). Additionally, the student also received two KC-based scores for this
problem: 1 for KC20 and 0 for KC23 .

For comparison purposes all of the scores were normalized to fall in the range of [0,1].
Most of the analysis in the following sections is based upon the overall and cumulative KC
scores. The KC-based scores will be presented only for Study 1 and Study 2. This is because
the KC-based scores are used to describe the characteristics of the Exploratory and the
Dichotic Gain (DichGain) Corpora and the KC-based NLGs (calculated from the KC-based
pre- and post-test scores) were used to derive KC-based tutorial tactics in Studies 2 and 3.

4.1.5

Measures

The main purpose of Study 1 was to collect an Exploratory corpus. Therefore, the focus will
primarily be on two aspects: learning performance and the characteristics of the Exploratory
corpus. For the learning performance, students’ pre- and post-test scores were compared
under both grading criteria to determine whether the Exploratory group learned by training
on Random-Cordillera.
For the characteristics of the corpus, the average number of ET and JS decisions and
overall decisions across students’ tutorial logs were presented. The I-ratio and the J-ratio
60

Question 10: A toy cart moves with a kinetic energy of 30 J. If the magnitude of its velocity
is doubled, what will its kinetic energy be?
Correct Answer:
1. Kinetic energy = 30J
2. KE = 1/2 ∗ m ∗ v 2
3. newv = 2 ∗ v
4. newKE = 1/2 ∗ m ∗ (2v)2 = 4 ∗ (1/2 ∗ m ∗ v 2 ) = 4 ∗ KE = 4 ∗ 30J = 120J
Mapped KCs:
KC20 : definition of kinetic energy KE = 1/2 ∗ m ∗ v 2
KC23 : The unit for energy is the Joule (J)
Student Answer: newKE = 1/2 ∗ m ∗ (2v)2 = 4 ∗ (1/2 ∗ m ∗ v 2 ) = 120
Overall Score : 0.75 (maximum is 1 for this problem)
Mapped KC Score: {KC20 : 1}; {KC23 : 0}
Culmulative KC-based Score : 1 (maximum is 2 for this problem)

Figure 4.1: An Example of Three Grading Criteria

were also checked to determine whether the random decisions worked. Recall that previously
the I-ratio and J-ratio were defined as:
NElicit
NElicit + NT ell
NJustif y
J − ratio =
NJustif y + NSkipJustif y
I − ratio =

(4.1)
(4.2)

The higher the I-ratio is, the more interactive the dialogue might be. The higher the Jratio is, the more likely the students would be presented a justification step. Specifically, the
average number of justification steps each student received was also presented. Additionally,
because the Exploratory Corpus would be used to induce KC-specific tutorial tactics in
61

Studies 2 and 3, the analysis will include discussion of the characteristics of the Exploratory
Corpus and the KC-based learning performance.
Study 1 was conducted as a part of an NSF ITR (Information Technology Research)1
project, for which this author acted as a script author developing tutorial scripts for the
training problems. This author was primarily responsible for the scripts associated with
training problems P4 and P5. The scripts were checked by other members of the group.
This author was also involved in annotating training problems with relevant KCs and acting
as a human wizard during the collection of the Exploratory Corpus.

4.2

4.2.1

RESULTS

Time

Each student took between six and fourteen hours (3-7 sessions) to finish the study. Each
session typically lasted about two hours. In general the students spent roughly five hours
(ranging from as low as four to as high as nine hours) training with Random-Cordillera.
For analysis purposes, each student’s training dialogues were concatenated into a single
consecutive dialogue resulting in a single “super-dialogue” for each student. These superdialogues, together with pre- and post-test KC-based scores were used to derive the KC-based
policies for use in Studies 2 and 3.
On average, it took each student M = 280.38 mins (SD = 66.88) to finish the seven
training problems. For each training problem, the average time spent by the Exploratory
Group varied. In Figure 4.2, the y-axis shows the time on task in minutes. It shows the
Exploratory students spent less than 30 minutes on each of the first three simple problems,
while on the last problem P7, the averaged 67 minutes, the longest average time per problem.
As the training problems became more complicated, the more time it took the Exploratory
students to finish it.
1

Support for this research was provided by NSF grants #0325054

62

Figure 4.2: Average Time Spent Per Training Problem By the Exploratory Group

4.2.2

Learning Results

A one-way ANOVA was used to test for performance preference differences between the preand posttests. Regardless of grading criteria, participants made significant gains from pretest to post-test (Table 4.3), F (1, 126) = 18.76, p < .001, R 2 = .73 under the overall grading
criteria and F (1, 126) = 11.01, p = .001, R 2 = .69 under the cumulative KC-based grading
criteria. Table 4.3 also lists the overall NLG scores and the Cumulative KC-based NLG
scores. Since a student’s NLG is defined as his or her actual average learning gain divided
by his or her maximum possible actual gain, on average the Exploratory students made 29%
gains under the overall grading criteria and 25% of the possible gains under the cumulative
KC scoring rubric.
Figure 4.3 shows the Exploratory group’s pre- and post-test scores under the two grading
criteria. A double asterisk (**) shows that the difference is statistically significant (p < 0.05).
Table 4.3 summarizes the minimum, maximum, mean, and SD of each score.
63

Figure 4.3: Learning Performance on Exploratory Group

Table 4.3: Exploratory Students’ Learning Performance

Grading

Overall

Cumulative KC-based

Min

Max

Mean

σ

Pre-test

0.14

0.94

0.48

0.20

Post-test

0.21

1.00

0.62

0.18

NLG

-0.08

1.00

0.29

0.21

Pre-test

0.13

0.96

0.46

0.20

Post-test

0.11

1.00

0.58

0.20

NLG

-0.38

1.00

0.25

0.26

64

4.2.3

Exploratory Corpus

4.2.3.1

Overall Characteristics The total number of ET tutorial decisions, referred to

as ET decisions in this thesis, ranged from 250 to 332 (M = 273.89, SD = 12.46) and that
of JS tutorial decisions, referred to as JS decisions, ranged from 52 to 71 (M = 56.61, SD =
3.43). The total number of the tutorial decisions regardless of actions, referring to as overall
decisions, for each student ranged from 288 to 372 (M = 305.48, SD = 14.01) 2 .
In the Exploratory Corpus, since all tutorial decisions were randomly decided, the dialogues’ I-ratio was expected to be around 0.5. An analysis of the log files showed that
the I-ratios ranged from 0.44 to 0.56 (M = 0.50, SD = 0.03). Similarly, it was expected
that the J-ratio would be roughly 0.5 as well. The analysis of the log files showed that
this value ranged from 0.39 to 0.68 (M = 0.53, SD = 0.06). The larger justification range
is unsurprising as there were fewer possible justification steps in the script. More specifically, the number of justification steps in a student’s tutorial dialogue ranged from 21 to 40
(M = 30.17, SD = 3.83).
Table 4.4 summarizes the overall characteristics of the tutorial decisions in the Exploratory Corpus.

Table 4.4: Overall Characteristics On Tutorial Decisions in Exploratory Corpus

Value

Min

Max

Mean

ET Decisions

250

332

273.89 12.46

JS Decisions

52

71

56.61

Overall Decisions

288

372

305.48 14.01

I-ratio 0.44

0.56

0.50

0.03

J-ratio 0.39

0.68

0.53

0.06

40

30.17

3.83

Justify

21

2

σ

3.43

overall decisions < ET decisions+JS decisions because on certain tutorial decision steps, the tutor makes
both types of decisions: JS first and then ET. For instance: Line 3 in Figure 2.1. When we calculated the
overall decisions, such a step was counted as one decision step.

65

4.2.3.2

KC-based Characteristics As described above, not all KCs appeared on tu-

torial decision steps in the authored scripts. In order for the tactic induction process to be
effective, a KC must be involved in the tutorial dialogues, the decision steps and the preand post-tests. There were a total of 21 KCs that satisfied this requirement. All 21 KCs
appeared in at least one ET decision step. Only 10 out of 21 KCs appeared in at least one JS
decision step. The characteristics of the 21 KCs in the Exploratory Corpus will be discussed
next.
Table 4.5 presents KC-based scoring and breakdown for the KC-related information in the
corpus. More specifically, the second column lists the 21 KCs. The third and fourth columns
list the average number of ET and JS decisions for the corresponding KC respectively. The
last column presents the comparison of KC-based post-test and pre-test scores to show
whether students’ performance on the KC was improved after the seven training problems.
From columns 2, 3 and 4, we can see that the number of decision steps varies dramatically
across the KCs. Column 2 shows that the average number of ET decisions ranges from 1.5
for KC18 to 72.6 for KC20 : the definition of kinetic energy (KE = 12 mv 2 ); column 3 shows
that 10 KCs appeared in the JS decision steps and the average number of JS decisions ranges
from 2 for KC12 , KC14 , KC26 to 16.9 for KC21: definition of Gravitation Potential Energy
(GP E = mgh). The average number of overall decisions varies from 2 on KC18 to 81.5 for
KC20 .
In Table 4.5, the last column shows that students learned significantly on 13 out of 21
KCs (labeled with “**”) and on the remaining KCs, no significant difference was found
between their pre- and post-test scores.
The eight primary KCs are underlined. The Exploratory Group scored significantly
higher on the posttest than on the pre-test for six of the eight KCs. The two exceptions
are KC14 and KC28 . Among the eight primary KCs, KC1 never occurred in any of the JS
decisions. The remaining seven KCs appeared in both ET and JS decisions.

66

Table 4.5: KC-based Exploratory Corpus

KC

ET

JS

Overall Decisions

Compare Pre- and Posttests

4

4

t(126) = 3.280, p = 0.001

1

KC1 ∗ ∗

2

KC3

2.1

2.1

t(126) = 1.204, p = 0.231

3

KC5

6.9

6.9

t(126) = 1.257, p = 0.211

4

KC9 ∗ ∗

2

2.5

t(126) = 3.527, p = 0.001

5

KC12

2.1

3

t(126) = 1.392, p = 0.166

6

KC13

3

3

t(126) = 1.560, p = 0.121

7

KC14

8

9

t(126) = 1.076, p = 0.284

8

KC15 ∗ ∗

7.5

7.5

t(126) = 2.470, p = 0.015

9

KC17

3.8

4.4

t(126) = .880, p = 0.381

10 KC18

1.5

2

t(126) = 1.478, p = 0.142

11 KC20 ∗ ∗

72.6

15.4

81.5

t(126) = 5.379, p = 0.000

12 KC21 ∗ ∗

33.6

16.9

60.8

t(126) = 3.932, p = 0.000

13 KC22 ∗ ∗

30.7

3.3

32.1

t(126) = 2.389, p = 0.018

14 KC23 ∗ ∗

62.1

4

63

t(126) = 5.358, p = 0.000

15 KC24 ∗ ∗

52.7

15.5

60.5

t(126) = 3.924, p = 0.000

16 KC25 ∗ ∗

8.7

9.3

t(126) = 3.767, p = 0.000

17 KC26 ∗ ∗

4.2

2

6.1

t(126) = 2.063, p = 0.041

18 KC27 ∗ ∗

21.5

4.8

23.9

t(126) = 4.522, p = 0.000

19 KC28

14.2

4.1

16.6

t(126) = 1.911, p = 0.058

20 KC31 ∗ ∗

18.6

19.2

t(126) = 2.446, p = 0.016

21 KC32 ∗ ∗

14.4

14.4

t(126) = 3.888, p = 0.000

2

2

67

4.3

DISCUSSION

The goal of Study 1 was to collect an Exploratory Corpus. Ideally, the Exploratory Corpus
should be collected by exploring all possible states and testing all possible actions from
each possible state, preferably for many times. But given the high cost of collecting these
educational data, it is not possible to do so. Therefore, in this thesis the Exploratory Corpus
was collected by choosing actions randomly. It was expected that viable, effective tutorial
tactics would be collected from the“Exploratory” training corpus. Our analysis of the corpus
showed that random decisions seemingly balanced the number of elicits and tells students got
during the tutoring (The mean of the I-ratio was 0.50). It was less so for JS decisions, but the
mean was off only slightly. (The mean of the J-ratio is 0.53). It also demonstrated that the
ET decisions were four times more frequent than the JS decisions. Moreover, 21 KC-based
ET tutorial tactics and 10 KC-based JS ones could be induces from the Exploratory Corpus.
While randomness may not be the best guide, the students made significant learning
gains in Study 1 as evidenced by their pre- and post-test scores. The results seemed to
confirm the previous view that content exposure might cause students to learn even from
tutors with non-optimal pedagogical skills.

68

5.0

STUDY 2: DICHOTIC GAIN (DICHGAIN) GROUP

The main goal of Study 2 was to investigate whether the induced tutorial tactics from the
Exploratory Corpus would result in a more effective version of Cordillera. Ideally, in order to
investigate the effectiveness of the system, a full-scale comparison between the new system
and Random-Cordillera should be conducted by randomly assigning students to one of two
groups. However, given the cost of running a comparison, and the issues to be addressed
in order to apply the RL to induce tutorial tactics, Study 2 was treated as an engineering
project rather than a science project. A new group of subjects were tested on the new
system and the students’ results were compared with the Exploratory group. This type of
comparison is not rare and is used if the goal is to determine if a trend exists. For example,
in [Singh et al., 2002] the researchers tested the learned policy on a new group of users and
compared the new group’s results with the original training group.
If there was a trend showing the new system out-performing the initial Random-Cordillera,
a full- scale comparison would be conducted. However, a subsequent analysis suggested that
the learned policies may not be very effective. This led to the hypothesis that this trend
might be caused by the limited methodology used for applying RL to induce tutorial tactics.
For example, a greedy-like feature selection method was used to derive a set of pedagogical tutorial tactics from the Exploratory Corpus collected in Study 1 (described below).
Nevertheless, an important contribution of Study 2 is that it generated a new corpus that
is similar to many other preexisting corpora and datasets in that it follows some types of
tutorial tactics, whether effective or not. This new corpus can be further used to induce new
tutorial tactics. If a successful policy can be induced from this corpus, then it will show the
potential for applying RL repeatedly to improve ITSs from pre-existing data.
69

Based on the reward functions employed in Study 2, the induced tutorial tactics were
referred to as Dichotic Gain (DichGain) tutorial tactics and the new version of the system
was labelled DichGain-Cordillera. The new group of students who were trained on DichGainCordillera was named the DichGain group, and the new corpus was labelled the DichGain
Corpus. This chapter will describe Study 2 in detail.

5.1

APPLY RL TO INDUCE DICHGAIN POLICIES

Recall that the tutorial actions which are the central focus of this thesis were elicit/tell
(ET) and justify/skip-justify (JS). Chapter 3 provided a general overview of Reinforcement
Learning (RL) and described the toolkit used to derive tutorial tactics in this dissertation.
It was assumed that KC-based tutorial tactics would be more effective than KC-general
ones. Five issues needed to be addressed to induce KC-based tutorial tactics for each type of
action. The issues described were: 1) obtaining a training corpus; 2) selecting the target KCs
for which the tactics will be induced; 3) defining the reward function, 4) determining state
representation; and finally, 5) selecting a conflict-resolution policy for multi-KC decision
steps. In this section, it will be shown how those issues were addressed in Study 2.

5.1.1

Training Corpus

This study made use of the Exploratory Corpus collected in Study 1. That corpus consisted
of sixty-four student tutorial dialogues, one for each participant. Each dialogue covered the
entire interaction between the student and the Random-Cordillera system over the seven
training problems.

5.1.2

Knowledge Components

In order for the tactic induction process to be effective, a KC must be involved in the tutorial
dialogues, the decision steps and the pre- and post-tests. Table 4.5 in Chapter 4 lists the
21 KCs that meet these criteria. For this study, the decision was to induce one policy for
70

each relevant KC. Therefore, of the thirty-two KCs in the domain, a total of thirty-one KCbased tutorial tactics were induced: 21 KC-based tutorial tactics on ET decisions and ten
KC-based tactics on JS decisions. Additionally, two KC-general tactics, one for ET decisions
and another for JS decisions were induced. The KC-general tutorial tactics were used in the
decision steps that did not involve any of the identified KCs. In short, this resulted in a total
of thirty-three tutorial tactics: twenty-two ET tactics and eleven JS tactics. In the following
discussion, the study uses π(KCi , ET ) and π(KCi , JS) to refer to KC-based tutorial tactics
for KCi for the ET and JS decisions respectively. For KC-general tutorial tactics, the study
uses π(KC∗, ET ) and π(KC∗, JS).

5.1.3

KC-based Reward

As described in Chapter 3, for a tutorial dialogue di on KCk , there are a set of intermediate
nd

,kc

i
k
rewards for each state and KC: rd1i ,kck . . ., rdi ,kc
k

reward,

ndi ,kck
rdi ,kc
,
k

−1

all of which are equal to 0. Only the final

has a non-zero value. Here ndi ,kck represents the number of dialogue turns

di in which the system made decisions regarding KCk .
In Study 2, a similar approach to reward function was used as [Tetreault and Litman, 2008].
The student’s final reward for each KCk in his/her superdialogue di was based upon his/her
KC-based NLG for KCk . More specifically, for each KCk , the students were divided into
two groups, low learners and high learners, according to a median split of the students’
KC-based NLGs.
The high learners were assigned a final reward of +100, while the low learners were
assigned a final reward of −100. These final reward values will be propagated to the internal
states via a dynamic programming algorithm for policy iteration [Sutton and Barto, 1998].
For inducing KC-general tutorial tactics, the reward functions were based on the cumulative
KC-based NLG instead of KC-based NLG on a specific KC. The rest of the procedure
remained the same.
71

5.1.4

State Representation

As described in Chapter 3, the issue of the state representation can be divided into four subissues. They are: 1) defining the potential feature choices in state representation; 2) capping
the number of features included in each policy; 3) discretizing the features appropriately; and
4) determining feature selection procedures. The next section describes how these sub-issues
were addressed in Study 2.

5.1.4.1

Feature Choices

Moore et al. identified four types of features that are relevant for human tutors when
making tutorial decisions: autonomy, temporal situation, problem-solving state, and performance to model the dialogue and the student’s state [Moore et al., 2004]. For each category
three to seven features were defined. Note that in this dissertation only features that could
be both automatically computed and unambiguously evaluated were included. This was
because the tutor would require the features to be available in real time when the learned
policies were employed. In order to help readers to understand each feature better, at the
end of this subsection I will use a simplified example to illustrate how these features were
calculated in Study 2.
Autonomy — three features
Autonomy features are related to the amount of work performed by the student in the
dialog. All autonomy features end with an ‘A’ in their name and are numeric. They are
F1 tellsSinceElicitA: The number of tells the student has received since the last elicit
prompt, irrespective of the KC involved. For example, tellsSinceElicitA = 2 means that
two tell decisions have been made since the last elicit decision. This feature reflects
how active a student is currently, that is, how much work the student has performed
recently.
F2 pctElicitA: The percentage of elicit/tell decision points in which the tutor has opted
to elicit during the dialog, irrespective of KC. This feature describes how interactive
the overall tutorial dialogue is. If answering questions makes the student more active
and interactive than simply receiving information from the tutor, then the higher the
value of pctElicitA is the more active and interactive the tutorial dialogue is.
F3 pctTellsKCSessionA: The percentage of tells received in a session for a specific
KC, e.g.KC20 . This feature describes how interactive the tutorial dialogue is for this
session. This feature measures the autonomy characteristics of the student’s tutorial
dialogue. It uses a longer timeframe than tellsSinceElicitA, but a smaller one than
pctElicitA.

72

Of these features, pctTellsKCSessionA (F3) is KC-specific. The focus is on the value
specific to the current KC. For example, if a policy for KC21 is induced, this feature computes the KC performance in terms of the tutorial actions on all previous instances of KC21
solely. In order to differentiate from other feature choices, a label “KC” was added to the
name for all KC-specific features.

Temporal Situation — 3 features
Temporal situation features cover temporal information such as time spent on the training
thus far. All three temporal situation features end with a ‘T’ and were numeric. Three
features were defined:
F4 durationKCT: Time duration since the last tutorial decision was made on the current
KC. e.g KC20 . The feature reflects how active a student’s knowledge of the current KC
is. For example, if “durationKCT” is high, it means that the tutor has not mentioned
the KC recently, so the student’s knowledge on the current KC may be still.
F5 TimeInSessionT: The total time in the session so far. This feature can be used to
measure the student’s fatigue level.
F6 TimeBetweenSessionsT:The time elapsed between the end of the previous session
and the beginning of the current one. This feature reflects how likely it is that a student
has forgotten what was learned in previous sessions. The higher the value, the more
likely the student has forgotten what was previously learned. If TimeBetweenSessionsT
is high, then the tutor should probably remind the student of some domain knowledge
at the beginning of the session.

Among them, durationKCT is a KC-specific feature.

Problem Solving Context — 5 features
Problem solving features include state information, such as what phase the dialogue is in
(e.g. problem solving or post-problem discussion), the problem’s difficulty level, and so on.
All problem solving-related features end with “PS.” Five feature choices are defined below.
F7 EarlyTrainingPS: Problems P1, P2, P3 are categorized as early training problem
andthe rest four training problems are categorized as late ones For early training
problems, the tutor may ask students to practice certain entries to let them get used
to the tutor.
F8 ProblemComplexityPS: Problems P1, P2, P3 are simple; P4 and P5 are medium;
and P6 and P7 are complex. This feature reflects the increasing complexity of the
solutions for training problems. The feature is relevant because it is expected that

73

fewer students will be able to solve a training problem on his/her own as the complexity
of the training problem increases.
F9 DuringWalkThroughPS: For each training problem the tutorial dialogues follow
a two-phase procedure: first problem solving, followed by a post-problem discussion.
This feature describes whether a tutorial decision was made during the problem solving
or during the post-problem discussion. It is probably relevant to learning because
certain tutor actions would be better to happen during the problem solving but not
post-problem discussion and vice versa. For example, qualitative discussions may
sometimes distract students from problem solving, and thus it would be better to
reserve qualitative discussion for the post-problem discussion rather than during the
problem solving itself.
F10 nKCsPS: The number of times the current KC has occurred in the current tutorial
dialogue. This feature reflects the student’s overall familiarity with the current KC.
F11 nKCsSessionPS: The number of occurrences of the KC, e.g. KC 20 in this session.
This feature reflects how many times the student has accessed the current KC in this
session.

Two features are KC-specific: nKCsPS(F10) and nKCsSessionPS (F11).

Performance — seven features
Performance features describe factors such as the quality of the student’s previous answers and the student’s ability. All performance-related features end with “PM”. Seven have
been defined. These are described below:
correct
F12 pctCorrectPM: Defined as: correct+incorrect
on all KCs. The number of correct
and incorrect entries calculated in the students’ logs that were labelled with + and −
respectively. This feature measures the student’s overall competence when only elicits
are counted as learning opportunities.
correct
F13 pctOverallCorrectPM: Defined as correct+incorrect+tells
on all KCs. This feature is
probably relevant to learning in that it reflects the student’s overall competence when
both elicits and tells are counted as learning opportunities.
F14 nCorrectKCPM: The number of correct responses on the current KC, e.g. KC 20.
This feature reflects the student’s competence on the current KC.
correct
F15 pctCorrectKCPM: Defined as correct+incorrect
on the current KC. e.g. KC20 . This
feature is probably relevant to learning in that it reflects the student’s competence on
the current KC when only elicits are counted as learning opportunities.
correct
F16 pctOverallCorrectKCPM: Defined as correct+incorrect+tells
on the current KC. e.g.
KC20 . This feature reflects the student’s competence on the current KC when both
elicits and tells are counted as learning opportunities.
F17 nIncorrectKCPM: The number of incorrect responses on the current KC, e.g. KC
20. This feature reflects the student’s incompetence on the current KC.
correct
F18 pctCorrectKCSessionPM: Defined as correct+incorrect
on the current KC. e.g.
KC20 in this session. This feature reflects the student’s lack of competence on the
current KC in this session.

74

Five out of seven features are KC-specific.

They are: nCorrectKCPM(F14), pctOver-

allCorrectKCPM(F15), pctCorrectKCPM(F16), nIncorrectKCPM(F17), pctCorrectKCSessionPM(F18).

As mentioned above, a successful application of RL is heavily dependent upon choosing
an appropriate set of features to represent tutorial contextual states. In other words, the
state representation for RL should include all of the tutorial dialogue information that is
relevant and necessary to determine what action should be taken next. On the other hand,
user modeling focuses on developing cognitive models of human users, such as the modeling
of users’ skills, knowledge level, and so on. Therefore, the features included in the state representation for the RL should include, but not be limited to, the features that model human
users. In this project, some features, especially performance-related features as defined in
Study 2 and Study 3 (described in Chapter 6) can be seen as modeling students’ knowledge
levels. One example of such a feature is “pctCorrectKCPM” which represents the percentage
of times a student had the correct answer on a specific KC.
Earlier an explanation was provided for how KC-specific features were calculated for
inducing specific KC tutorial tactics. However, when inducing KC-general tutorial tactics
on either ET or JS decisions, all the KC-specific features become KC-general features and
take into count all of the previous instances regardless of KC. For example, nCorrectKCPM
becomes the number of correct responses on all the KCs instead of on a specific KC.
This section explains how the eighteen features were calculated from the log files. The
following is an sample of a tutorial dialogue that was extracted from log files of a student
solving the training problem P4. The entire dialogue between the student and RandomCordillera is contained in Appendix H. All the tutor turns and the student turns are labelled
to the corresponding KCs in Appendix H. The sample dialogue shown here covers one step
applying KC20 : the definition of Kinetic Energy, to solve the KE of the rock at T0 .
The sample dialogue covers five micro-steps. They represent the first principle application
in solving the training problem P4. So EarlyT rainingP S = 0 and P roblemComplexityP S =
medium. The dialogue occurred during problem solving as opposed to post-problem discussion so DuringW alkT hroughP S = 1. The sample dialogue happened in the student’s first
75

training session on Cordillera, so the time duration between his current session (start time of
the current session) and his last session (the end time of last session) TimeBetweenSessionsT
is 0. In fact, this is the student’s fourth training problem in the current session.
In Table 5.1, the first column refers to the relative order. For simplicity, it begins at
one. The second column lists the time the action happened. The third column lists the
dialogue between the tutor and the student. All the tutor turns start with “Tutor: ” while
the student’s turns start with “Student: ”. In the last column, each of the student’s and
tutor’s turns were mapped to the corresponding KCs. For example, in Line 2, the tutor
made an ET decision and decided to tell. The target KC is KC20 . Because it is a tell, this
turn is labelled with ∗KC20 which means this is tutor-generated information in the dialogue
on KC20 . Line 4 shows an example of an entry generated by the student, which is correct
and also targeted to KC20 . So the turn is labelled with +KC20 .
In Table 5.1, there are total of five micro-step decisions. They are labelled as “D+ line
number”, D2, D3, D5, D8, and D10 respectively. Among the five tutorial decisions, with
the exception of D5, all remaining tutorial decisions involved KC20 . Therefore, to induce
a KC-specific tutorial tactics on KC20 , only the tutorial decisions that involved KC20 were
taken into account.
What follows is a simplified illustration of how the sample tutorial dialogue was transformed into one of eighteen features on KC20 . In Table 5.1 the feature I picked to illustrate is
“tellsSinceElicitA”, one of three autonomy features. Initially the “tellsSinceElicitA”’s value
is two, which means that the tutor made two tell decisions since the last elicit decision before
this sample dialogue. For the decision D2, the tutor made another tell decision and now the
“tellsSinceElicitA” value is three. The dialogue continues to the next decision, D3, and this
time the tutor has made an elicit decision. So “tellsSinceElicitA”’s value goes back to 0.
The next micro-step decision involving KC20 is D8. Because the tutor did not make any
tell decisions between lines 3 and 8 the “tellsSinceElicitA” value is still 0. The tutor made
two decisions in D8: first on JS and then on ET. The tutor decided not to skip the justification step and also decided to elicit the step from the student. So “tellsSinceElicitA” value
remains 0 until the tutor made the decision in D10. At D10, the tutor made a decision to
tell and thus “tellsSinceElicitA”’s value became 1 until the tutor was ready to make another
76

decision.

Table 5.1: A Simplified Example of Part of Student Log on Training Problem P4

#

Time

String

Label

1

14:56:08 Tutor:So let‘s start with determining the value of
KE0.

2

14:56:11 Tutor:To calculate the rock‘s kinetic energy at T0, TELL *KC20 , D2
let‘s apply the definition of kinetic energy.

3

14:56:27 Tutor:Please write the equation for applying the

ELICIT KC20 , D3

definition of kinetic energy to the rock at T0.
4

14:56:31 Student:ke0 = 1/2 * M * V0ˆ2

+KC20

5

14:56:32 Tutor:Since all the variables in the equation are

ELICIT KC23 , D5

known except for KE0, we can calculate KE0 easily.
What is the value of KE0?
6

14:56:47 Student:1.2J

+KC23

7

14:56:48 Tutor:Okay.Before we go any further, let‘s discuss
this principle in a little more detail.

8

14:56:56 Tutor:Why does the rock have a non-zero kinetic
energy at T0?

9

JUSTIFY ELICIT
KC20 , D8

14:57:17 Student: because it has a nonzero mass and a +KC20
nonzero magnitude of velocity

10 14:57:18 Tutor: Yes. We do not know the direction of the
rock‘s velocity at T0 from the problem statement.
However, the direction of v0 does not affect the
rock‘s kinetic energy at T0.

77

TELL *KC20 , D10

Table 5.2 illustrates how the three autonomy features got updated as the sample dialogue
in Table 5.1 continues. In Table 5.2, the first row shows the corresponding values for the
three autonomy features when the tutor reaches D2 in Figure 5.1 and the last row shows
their values when the tutor reaches the decision D10 in Figure 5.1. The rest of the 18 features
were calculated in a similar manner. Appendix J show how the eighteen features varied as
the sample example dialogue in Table 5.1 goes on.

Table 5.2: Autonomy Features Updated

5.1.4.2

Decision

tellsSinceElicitA

pctElicitA

pctTellsKCSessionA

D2

2

0.47

0.64

D3

3

0.47

0.65

D8

0

0.48

0.64

D10

0

0.49

0.63

Maximum Number of Features Previously I discussed the problems of data

sparsity for RL. In an RL model, the size of the state space increases exponentially as the
number of involved features increases. In order to learn effective tutoring tactics, a corpus
should cover each of these states at least once, which means at least 218 in our case. However,
it is almost impossible to do so due to the high cost of collecting educational data. On the
other hand, the learned policy may become too subtle to be necessary. Based on the size
of the Exploratory Corpus collected in Study 1 and the number of categories of features
defined in this study is four, the state representation was capped four features. Moreover,
as discussed in subsection 5.1.4.1, the maximum number of features within each of the four
categories was limited to one. This was done because it was anticipated that this would
better represent the relevant information.

5.1.4.3

Feature Discretization In Study 2, except for EarlyTrainingPS(F7, binary),

ProblemComplexityPS (F8, three), and DuringWalkThroughPS (F9, binary) which are dis78

crete features, the remaining fifteen features are continuous features and need to be discretized. In Study 2, this was accomplished for each feature by turning each feature into a
binary feature via a median split. This was done in order to balance the number of cases
across clusters. Therefore, apart from ProblemComplexity, which has three categories, all
other features are binary.

5.1.4.4

Feature Selection In Study 2 a two-pass feature selection process was em-

ployed. For each KC and decision pair (e.g. KC1 , ET) 18 single-feature MDPs were generated. Each MDP used one and only one of the 18 features to represent the state, and used the
relevant tutorial decisions to represent the actions. All other features were ignored. For each
of these MDPs, the Tetreault and Litman’s toolkit was applied [Tetreault and Litman, 2008]
to induce a single-feature policy together with its corresponding ECR.
In the second pass the four best features were selected. Using Moore’s categories and
the ECR, the policy with highest ECR from each of the categories was selected. The
process involved choosing one from among features 1-3 for the single autonomy feature;
one from among features 4-6 for the temporal situation feature, and so on. The criteria
used for the selection ECR, specifically the single-policy feature who had the highest ECR
relative to its’ peers is selected. Note that the policy’s Confidence Interval was ignored
because ECR was more widely used in RL community for evaluating the derived policies
[Williams and Young, 2007b, Williams and Young, 2007a, Janarthanam and Lemon, 2009].
An MDP was then defined for the KC and action decisions by using the four lead features
for the state representation, and induced a new four-feature policy from it. From all eighteen single-feature policies and the four-feature policy, the policy with the highest ECR is
selected for each KC and action decision. Recall that the higher the ECR of a policy, the
more effective the policy is supposed to be.
However, a subsequent analysis of this feature selection method showed its limitations.
Notably, other feature selection methods were applied to the 18 features, which included
four RL-based feature selection methods (reviewed in Chapter 6) and a random feature
selection method, the induced policies had significantly higher ECR [Chi et al., 2008a]. For
example, Table 5.3 shows the ECR of the DichGain tutorial tactics and the tutorial tactics
79

induced by applying new feature selection approaches to the eight primary KCs for the
ET decisions. The new tutorial tactics had higher ECRs than the DichGain ones across
all eight KCs. For example, line 5 shows that ECR(πDichGain (KC22 , ET )) = 9.4 while
ECR(πnew (KC22 , ET )) = 44.29, four times higher than the former.

Table 5.3: Compare DichGain Tactics With Tutorial Tactics Under New Feature Selection
Methods On Eight Primary KCs

DichGain Tutorial Tactics

Tutorial Tactics With New Feature Selection

ECR

95% CI

Range

ECR

1 KC1

20.196

[5.19, 34.19]

29.01

51.79 [32.67, 63.71]

31.04

2 KC14

54.15

[47.9, 59.58]

11.69

59.48

[54.3, 63.21]

8.91

3 KC20

4.81

[0.75, 8.66]

7.9

8.08

[4.24, 11.9]

7.66

4 KC21

15.48

[7.85, 21.78]

13.94

26.94

[19.8, 29.28]

9.48

5 KC22

9.4

[-5.37, 20.69]

26.05

44.29 [23.49, 50.51]

27.02

6 KC24

7.23

[2.72, 11.31]

8.59

12.91

[7.22, 16.43]

9.21

7 KC27

16.78

[5.95, 24.9]

18.95

27.25 [13.87, 32.16]

18.29

8 KC28

15.29

[2.52, 26.05]

23.52

32.8

16.53

5.1.5

95% CI

Range

[22.08, 38.61]

Conflicting Policies

In some cases, a given tutorial step involves multiple KCs and thus, multiple policies. When
multiple policies are relevant, the policy with the highest ECR was followed.

5.1.6

Summary: Procedure of Inducing Tutorial Tactics in Study 2

In sum, Study 2 involved using the Exploratory Corpus collected in Study 1 as the training
corpus and the reward functions are defined as either +100 (high learner) or −100 (low
learner) based on corresponding KC-based NLGs. In Study 2 a total of thirty-three policies
80

were induced: twenty-two ET policies (21 KC-specific and one KC-general) and eleven JS
policies (10 KC-specific and one KC-general).
In order to induce the necessary policies a multi-pass approach was adopted. In the
multi-pass approach, a set of 18 MDPs were constructed for each KC and decision pair (e.g.
KC4, JS) with all features either discrete or discretized via a median split. Then a set of 18
single-feature policies was induced, one for each MDP. For each of Moore’s four categories, a
feature was selected whose corresponding single-feature policy had the highest ECR among
features in the same category. A four-feature MDP was then defined from which a more
complex policy was induced. All of the work here was done with Tetreault and Litman’s
toolkit [Tetreault and Litman, 2008]. Finally, the policy with highest ECR for the KC and
decision pair was selected from among the 19 derived policies: 18 single-feature policies and
one four-feature policy.
Appendix K lists the 22 ET tutorial tactics and the 11 JS policies that were induced
and applied in Study 2. One of the resulting policies is shown in Figure 5.1. This policy
involved four features: durationKCT, ProblemComplexityPS, tellsSinceElicitA, and pctCor-

1. Features: durationKCT, ProblemComplexityPS,
tellsSinceElicitA, pctCorrectKCSessionPM
2. Cutoff: durationKCA =’50.0’ tellsSinceElicitA =’0.0001’
pctCorrectKCSessionPM =’0.7179’
3. Policy:
a.

Elicit:

0:MED:1:0, 1:COMP:1:0, 0:COMP:1:1, 0:MED:0:0, 0:COMP:1:0,

0:MED:1:1, 0:COMP:0:1, 1:COMP:0:1
b. Tell: 1:MED:0:1, 1:MED:0:0, 1:MED:1:0, 1:MED:1:1, 0:MED:0:1
c. Else: 0:COMP:0:0, 1:COMP:0:0, 1:COMP:1:1

Figure 5.1: The Induced Policy πDichGain (KC21 , ET ): Gravitational Potential Energy

81

rectKCSessionPM. Three of these features were continuous. The median cutoff values used
to discretize them are shown in line 2 (“cutoff”). This policy contains a total of 24 = 16
rules. In eight cases, the tutor elicited (line a); in five cases the tutor elected to tell (line b);
and in the remaining three the tutor could choose to do either (line c). For example, “elicit:
[0:MED:1:0]” (shaded in line a) means:
IF the duration since the most recent decision made on KC21 is less than 50sec;
AND the ProblemComplexity is ’medium’;
AND the students have received at least one tell since the most recent elicit (tellsSinceElicit)
AND the student’s performance on this kc in today’s session is less than 71.79% correct;
THEN: the tutor should elicit the next step from the student.

The example in Figure 5.1 indicated that the induced tactics were a very specific set of
case decisions, and could easily be implemented back into Cordillera. Moreover, the tactics
were quite subtle.

Then the thirty-three induced policies were implemented back into Cordillera producing
a version of the system called DichGain-Cordillera. This version of the system used the
KC-specific policies when facing a relevant decision, resolved ties by selecting the policy
with the best ECR, or followed the KC-general strategies when no policy was relevant.
As described below, a set of students were trained on this system to collect the DichGain
Corpus. For Study 2, the author was responsible for application of the MDP toolkit to the
Exploratory Corpus and induced the 33 DichGain tutorial policies. Once the policies were
implemented back into Cordillera, the author acted as a human wizard during the collection
of the DichGain Corpus.

5.2

5.2.1

METHODS

Participants

Data was collected over a period of three months during Spring 2008. As in Study 1, a set
of forty-two college students were recruited and paid for their time regardless of completion.
82

The students were required to have a basic knowledge of high-school algebra and not to
have enrolled in college-level physics courses. All told, thirty-seven students completed the
experiment.

5.2.2

Materials & Procedures

The students followed the same procedure, used the same preparatory materials and problems, and involved the same group of human wizards as in Study 1. More specifically, the
DichGain group completed a background survey, read a textbook covering the target domain
knowledge, took a pre-test, solved the same seven training problems in the same order on
DichGain-Cordillera, and finally took a post-test. Only two salient differences exist between
Study 1 and Study 2:

1. Interaction decisions made by DichGain-Cordillera were guided by thirty-three derived
tutorial tactics; and
2. One test problem, Q20 , on the pre- and post-test was changed for Study 2 to, Q∗20 . Both
Q20 and Q∗20 are multiple-choice questions and cover the same KCs. But Q20 is a simple
question and had only two choices (true, false), so there is a good chance that students
could guess the answer. The new version of Q∗20 ,covered the same KCs but was more
difficult by providing five choices. So it is less likely that students could guess the answer.
The remaining 32 test items were identical in both studies. And as with Study 1, the
pre- and post-tests in Study 2 were identical.

5.2.3

Grading

All tests were graded by the same grader as in Study 1 (not the author). She applied the
same grading metrics and carried out the same grading process resulting in both the overall
and KC-based grades.
83

5.2.4

Measures

There were two research objectives in Study 2: first, to determine whether the DichGain
group learned by training on DichGain-Cordillera, and second, to examine the DichGain
group and the Exploratory group to see whether the induced DichGain tutorial tactics would
result in better learning performance than making random decisions. Note that given the
cost of the study, we did not run a strict control-experimental study but simply confirmed
whether the trend of the DichGain group over-performing the Exploratory group existed.
The two groups’ learning performances were compared, using both the students’ pre-test,
post-test, adjusted post-test scores and NLG, under both the overall grading criteria and the
cumulative KC-based grading criteria. The adjusted post-test can be measured as a linear
association between the real post-test score for each student and the difference between the
pre-test score for the students and the mean of pre-test scores. The formula for the adjusted
post-test score is:

posttest∗i = posttesti − β × (pretesti − pretest)

(5.1)

where i stands for the student, posttest∗i for the adjusted post-test score for student i,
posttesti for the true post-test score for the student i, β is the regression coefficient of
the post-test score upon the pre-test score, pretesti is the true pre-test score for the student
i, and pretest is the mean of the pre-test scores.
Results showed that there was no significant difference between the DichGain and Exploratory groups under either grading criteria. There are two potential reasons for this. One
is the lack of random assignments and two is that the RL approach may be limited. As a
result, Study 3 focused on a full-scale comparison by exploring a wider range of methods to
deal with the five RL issues. For example, three training corpora were explored in Study 3:
the Exploratory Corpus collected in Study 1, the new DichGain Corpus in this study, and
a combination of the two in a new corpus. Because of this decision, the second part of the
results section will focus on the characteristics of the DichGain corpus. These characteristics
will include the general number of decisions the tutor made, the number of ET decisions and
84

the I-ratio, and the number of JS decisions and the J-ratio. Additionally, because the DIchGain corpus will be used to induce KC-specific tutorial tactics in Study 3, the description
will include some KC-based learning performance and corpus characteristics as well.

5.3

RESULTS

In Study 2, it took each student from three to six sessions to complete the study. These
sessions were spaced over a period of one to three weeks. The sessions generally took less
than two hours to complete. The students spent roughly five hours, ranging from as few as
four hours to as many as nine hours, training on DichGain-Cordillera.

5.3.1

Compare Pre- and Post-test

Table 5.4: DichGain Students’ Pre- vs. Post-test Performance

Overall Grading

Min

Max

Mean

σ

Pretest

0.04

0.74

0.40

0.18

Posttest

0.18

0.96

.58

.19

NLG

-0.09

0.89

0.33

.21

0.04

0.77

.42

.17

Posttest

0.08

0.97

.54

.20

NLG

-0.33

0.86

0.25

0.23

Cumulative KC-based Grading Pretest

A one-way ANOVA was used to test for performance preference differences between the
pre- and posttests. Regardless of grading criteria, participants made significant gains from
pre-test to post-test (Table 5.4), F (1, 72) = 16.86, p = .000, R 2 = .69 under the overall
grading criteria and F (1, 72) = 8.55, p = .005, R 2 = .71 under the culmulative KC-based
grading criteria. The overall NLG scores ranged from −0.09 to 0.89 (M = 0.33, SD = .21).
85

The cumulative KC-based NLG scores ranged from −0.33 to 0.86 (M = 0.25, SD = .23).
Table 5.4 summarize the minimum, maximum, mean, and SD values for each test scores.
Figure 5.2 shows the DichGain group’s pre- and post-test scores under the two grading
criteria. A double asterisk (**) indicates that the difference is statistically significant (p <
0.05). To summarize, the DichGain group scored significantly higher in the post-test than
in the pre-test. .

5.3.2
5.3.2.1

Post-hoc Comparison: DichGain vs. Exploratory
Post-hoc Comparison: DichGain vs. Exploratory On Training Time In

a post-hoc comparison with a one-tailed paired t-test, there were no significant overall time on
task differences between the DichGain group (M = 294.33, SD = 87.50) and the Exploratory
group (M = 280.38, SD = 66.88) across the seven training problems : t(99) = .88, p = .38.

Figure 5.2: Learning Performance of Exploratory Group

86

Figure 5.3: Per Problem Time Comparison: DichGain vs. Exploratory Group

However, a significant difference was found between the two groups in the time they spent
on P1, P2 and P7 Figure 5.3 compared the average time students spent on each training
problem between the two groups. On P1 the DichGain group spent significantly less time
than the Exploratory group with a one-tailed paired t-test (t(98) = 3.15, p = .002) while on
P2, the DichGain group spent longer than the Exploratory group ((t(99) = 2.56, p = .012)).
Similarly on P7, the DichGain group spent significantly longer time than the Exploratory
group: (t(99) = 2.46, p = .016).

5.3.2.2

Post-hoc Comparison: DichGain vs. Exploratory On Learning Perfor-

mance Because of an administrative error, all of the background information for DichGain
group was not available for comparison. As mentioned above, one test problem Q20 was
changed from Study 1 to Q20∗ for Studies 2 and 3. So in order to compare the two groups,
Q20 and Q20∗ were excluded from the scores used here. As described in the previous chapter, the tests contained thirty-three test items which covered 168 KC occurrences. Removing
Q20 or Q20∗ reduced this total by one leaving thirty-two test items covering 166 KC occur87

Table 5.5: DichGain vs. Exploratory Scores: Pre vs. Post-Test (No Q20 )

Overall

Cumulative
KC-Based

PrePostAdj. PostNLG
PrePostAdj. PostNLG

Dich m(σ)a
.40 (0.18)
0.58 (0.19)
0.62 (0.10)
0.34 (0.20)
0.41 (0.17)
0.54 (0.20)
0.57 (0.12)
0.26 (0.23)

Exp m(σ)b
.47 (0.20)
0.61 (0.18)
0.59 (0.10)
.28 (0.21)
0.45 (0.20)
0.57 (0.21)
0.56 (0.12)
0.25 (0.26)

Statc
t(99) = 1.86, p = 0.066
t(99) = 0.78, p = 0.44
F (1, 98) = 1.99, p = 0.16
t = 1.36, p = 0.18
t(99) = 0.99, p = 0.32
t(99) = 0.66, p = 0.51
F (1, 98) = 0.09, p = 0.77
t(99) = 0.23, p = 0.82

d
−0.37
−0.16
0.3
0.29
-0.21
-0.15
0.08
0.04

1−β
0.39
0.54
0.46
0.47
0.5
0.59
0.78
0.82

a

The Mean and SD of DichGain Group.
The Mean and SD of Exploratory Group.
c
Except an ANCOVA using pre-test score as the covariate on Adj.Post-test scores, the two groups were
compared with one-tailed paired t-tests on the pre-test, post-test and NLG scores.
b

rences. In the subsections learning performance will be compared across both groups using
both the overall and cumulative KC-based scores. For the overall scores, the maximum raw
score was 32 points while for the cumulative KC-based score had a maximum of 166 points.
For comparison purposes both scores were normalized to 1.
A one-way ANOVA was used to test for performance preference differences between
the pre- and posttests across the two groups. Across 32 test questions, participants in the
first two studies made significant gains from pre-test to post-test, F (1, 200) = 35.88, p =
.000, R 2 = .70 under the overall grading criteria and F (1, 200) = 19.51, p = .000, R 2 = 0.69
under the cumulative KC-based grading criteria. In a post-hoc comparison, however, no
significant pre-test score differences were found between the two groups on pre-test scores,
post-test scores, adjusted post-test scores, and NLG under either the overall-grading rubric
or the cumulative KC-based scores (Table 5.5). The first column in Table 5.5 shows the
eight comparisons: pre-test scores, posttest scores, adjusted posttest scores, and NLG under
both the overall-grading rubric and the cumulative KC-based scores. The second column
in Table 5.5 lists the means (m) and SDs σ of two groups’ corresponding scores. The third
column lists the corresponding statistical comparisons. No significant difference was found
88

between the two groups across the eight comparisons. However, the DichGain students did
demonstrate marginally significant lower pre-test scores than the Exploratory group under
the overall grading criteria only.
The fourth column lists the effect size of the comparison. There are several accepted
ways to measure effect size, such as Cohen’s d effect sizes based on means, Hedges’ g and so
on. For this dissertation, Cohen’s d is selected and it is defined as the mean learning gain
of the experimental group minus the mean learning gain of the control group, divided by
the groups’ pooled standard deviation. The final column listed the statistical power of the
comparison, 1−β 1 . Generally speaking, it must be kept correspondingly high. Ideally, power
should be at least 0.80 to detect a reasonable departure from the null hypothesis. The reward
functions used for inducing DichGain tutorial tactics were based on the students’ cumulative
KC-based NLGs or KC-based NLG scores. However, the last row in Table 5.5 shows that the
Exploratory and DichGain groups were not significantly different on cumulative KC-based
NLGs and its power reached an acceptable level: 0.82 (often considered to be between .80
and .90).
Although no significant difference was found between two groups on learning performance
and overall time on training, the DichGain students did have a marginally significant lower
pre-test score than the Exploratory group under the overall grading criteria. One potential
reason for an absence of difference in learning between the two groups may be because the
lack of random assignment. However, there are other potential reasons for this. For example,
it might be because of the limitation of the RL approach used in Study 2. As discussed above,
the feature selection method in Study 2 is somewhat greedy-like. So in Study 3, significantly
more feature selection methods were explored to find ways to better use RL and a full-scale
comparison was run.
At this point in the research, two training corpora existed: the Exploratory Corpus in
which all decisions were randomly made, and the corpus that was collected by following the
induced DichGain tutorial tactics induced from the Exploratory Corpus. Although there
was no significant learning performance difference between the two groups, the two corpora
1

β represents Type II error: false negative. It refers to the error of failing to reject a null hypothesis when
it is in fact not true.

89

may differ in other aspects. In Study 3, both the Exploratory and the DichGain corpora
were used as training corpora. The characteristics of the Exploratory Corpus were discussed
previously in Chapter 4 and the characteristics of the DichGain corpus will be discussed
below.
Finally, the two corpora will be compared using the following measurements: the average
number of ET decisions, JS decisions, and overall decisions that the tutor made. Another
point of comparison is measured by the I-ratio, the J-ratio, and the number of justification
steps. In addition, because the DichGain corpus will be used to induce KC-specific tutorial
tactics in Study 3, its KC-based learning performance and corpus characteristics will also be
discussed.

5.3.3

Post-hoc Comparison: DichGain vs. Exploratory Tutorial Corpora

The DichGain corpus was used as one of the training corpora to derive KC-based tutorial
tactics in Study 3. The decision to choose one corpus over another means that it is valuable
to compare the characteristics of the two corpora. Similar to the Exploratory corpus, each
student’s individual problem dialogues were combined into a single super-dialogue listing all
tutor-student interactions in order of occurrence. Thus, one tutorial dialogue was combined
per participant.

5.3.3.1

Post-hoc Comparison: DichGain vs. Exploratory On Overall Tutorial

Decisions Table 5.6 compares the various tutorial decisions with a one-tailed paired t-test
across all KCs between the DichGain and Exploratory Corpora. Except for the total number
of overall decisions and the total number of ET decisions, the two corpora differed on all
the other seven aspects (labeled with “**”). Overall, the DichGain Corpus is significantly
less interactive in that the DichGain students received more tells and less elicits from the
tutor than the Exploratory Corpus. As a result, the I-ratio of the DichGain corpus was
significantly lower than that of the Exploratory corpus. Moreover, the DichGain-Cordillera
skipped more and executed less justification steps than the Exploratory-Cordillera.
90

Table 5.6: Overall Tutorial Decision Characteristics: DichGain vs. Exploratory Corpora

Decision
tell**

elicit**

Condition

Mean

σ

Stats

DichGain (37) 152.46

13.05

t(99) = 6.663, p = 0.000

Exploratory (64) 138.02

8.71

DichGain (37) 118.08
Exploratory (64) 135.88

ET decisions

DichGain (37) 270.54
Exploratory (64) 273.89

skip-Justify**

Justify**

JS decisions**

Overall Decisions

I-ratio**

J-ratio**

5.3.3.2

13.33 t(99) = −6.956, p = 0.000
11.82
10.00 t(99) = −1.396, p = 0.166
12.46

DichGain (37)

33.54

4.80

Exploratory (64)

26.44

4.24

DichGain (37)

24.89

3.59

Exploratory (64)

30.17

3.83

DichGain (37)

58.43

2.81

Exploratory (64)

56.61

3.43

DichGain (37) 307.57

12.45

Exploratory (64) 305.48

14.01

DichGain (37)

0.44

0.04

Exploratory (64)

0.50

0.03

DichGain (37)

0.43

0.07

Exploratory (64)

0.53

0.06

t(99) = 7.728, p = 0.000

t(99) = −6.826, p = 0.000

t(99) = 2.742, p = 0.007

t(99) = 0.749, p = 0.456

t(99) = −7.967, p = 0.000

t(99) = −7.894, p = 0.000

Post-hoc Comparison: DichGain vs. Exploratory On Individual KCs

Table 5.7 shows the number of tutorial decision steps for each KC and each type of tutorial
decision in DichGain Corpus. The third and fourth columns list the number of ET and JS
tutorial decisions for the KC. The last column lists the statistical results of comparing the
KC-based pre-test scores with post-test scores with one-tailed paired t-tests. If the DichGain
group had significantly higher post-test scores than its pre-test scores, the corresponding KC

91

were labelled with “**” next to their name in (column 2).
From Table 5.7, it can be seen that, as with the Exploratory Corpus, the DichGain
corpus had twenty-one KCs that occurred in at least one ET tutorial decision step and ten
KCs for JS decisions. Additionally, the number of occurrences of ET decisions varied from
one to seventy-two occurrences; for JS decisions it varied from two to sixteen occurrences.
Among the eight primary KCs, students learned significantly from six of them: KC1 , KC14 ,
KC20 , KC21 , KC22 , KC24 but not on KC27 and KC28 .

92

Table 5.7: Tutorial Decisions Per KC.
KC

ET

1

KC1 **

2

Total

Pre- and Posttests

4.16

4.16

t(72) = 2.80, p = 0.01

KC3

2.11

2.11

t(72) = 0.66, p = 0.51

3

KC5

7.05

7.05

t(72) = 1.64, p = 0.11

4

KC9 **

1.22

2

t(72) = 2.46, p = 0.02

5

KC12

1.46

3

t(72) = 1.27, p = 0.21

6

KC13

3

3

t(72) = 1.27, p = 0.21

7

KC14 **

8.38

9

t(72) = 2.03, p = 0.05

8

KC15

7.62

8.11

t(72) = 0.98, p = 0.33

9

KC17

4.43

5.43

t(72) = 1.68, p = 0.10

10

KC18

1.14

2

t(72) = 1.60, p = 0.11

11

KC20 ** 72.43 16.43

82.84

t(72) = 4.45, p = 0.00

12

KC21 **

16.86

59.05

t(72) = 4.30, p = 0.00

13

KC22 ** 31.32

3.97

32.78

t(72) = 2.41, p = 0.02

14

KC23 **

4

62.95

t(72) = 3.39, p = 0.00

15

KC24 ** 48.97 15.89

60.24

t(72) = 3.11, p = 0.00

16

KC25 **

9.43

9.78

t(72) = 2.65, p = 0.01

17

KC26

5.41

2.81

7.46

t(72) = 1.76, p = 0.08

18

KC27

21.92

4.62

23.73

t(72) = 1.99, p = 0.051

19

KC28

14.19

5.14

18.32

t(72) = 1.82, p = 0.07

20

KC31

18.32

19.16

t(72) = 1.82, p = 0.07

21

KC32 **

14.24

14.24

t(72) = 2.29, p = 0.03

49

61.11

JS

2

2

Overall, there was a significant difference between the two corpora on the I-ratio and Jratio. However, as this difference is analyzed, the variance becomes more complex. Table 5.8
shows the I-ratio difference between the DichGain and Exploratory corpora with one-tailed

93

paired t-tests. The third and fourth columns of the table list the mean of the I-ratio in the
DichGain Corpus and Exploratory Corpus respectively. The fifth column gives the direction
of the difference in which “DG > EX” represents that the DichGain Corpus was more
interactive than the Exploratory Corpus on corresponding KCs. Similarly, “DG < EX”
means the reverse is true. If the column is blank, it means that there were no significant
differences between the corpora on the I-ratio for the corresponding KC. The last column
shows the statistical results between the two corpora. If the difference is significant, the KC
name in column 2 is labeled with “**”.
As shown in Table 5.8 the corpora differed significantly in terms of I-ratio on all but
three KCs: KC9 , KC22 , and KC32 . The DichGain group was significantly less interactive
than the Exploratory group on six KCs (KC12 , KC18 , KC20 , KC23 , KC24 , and KC27 ), and
significantly more interactive than the Exploratory group on the remaining twelve KCs.

94

Table 5.8: I-Ratio Between DichGain vs. Exploratory on a per-KC basis.

KC

DichGain

Exploratory

Diff

Stats Comparison

1

KC1 **

0.90

0.49

DGa > EXP b

t(99) = 8.92, p = 0.0000

2

KC3 **

0.80

0.51

DG > EXP

t(99) = 4.05, p = 0.0000

3

KC5 **

0.56

0.46

DG > EXP

t(99) = 3.37, p = 0.0010

4

KC9

0.35

0.47

5

KC12 **

0.21

0.52

DG < EXP

t(99) = −3.81, p = 0.0000

6

KC13 **

0.90

0.45

DG > EXP

t(99) = 8.17, p = 0.0000

7

KC14 **

0.89

0.50

DG > EXP

t(99) = 12.00, p = 0.0000

8

KC15 **

0.60

0.47

DG > EXP

t(99) = 4.38, p = 0.0000

9

KC17 **

0.71

0.48

DG > EXP

t(98)4.66, p = 0.0000

10 KC18 **

0.04

0.43

DG < EXP

t(99) = −5.39, p = 0.0000

11 KC20 **

0.23

0.50

DG < EXP

t(99) = −17.26, p = 0.0000

12 KC21 **

0.65

0.50

DG > EXP

t(99) = 6.76, p = 0.0000

13 KC22

0.46

0.49

14 KC23 **

0.45

0.49

DG < EXP

t(99) = −3.36, p = 0.0010

15 KC24 **

0.28

0.50

DG < EXP

t(99) = −17.43, p = 0.0000

16 KC25 **

0.73

0.50

DG > EXP

t(99) = 7.70, p = 0.0000

17 KC26 **

0.58

0.43

DG > EXP

t(99) = 3.05, p = 0.0030

18 KC27 **

0.41

0.51

DG < EXP

t(99) = −3.78, p = 0.0000

19 KC28 **

0.64

0.48

DG > EXP

t(99) = 4.23, p = 0.0000

20 KC31 **

0.67

0.51

DG > EXP

t(99) = 7.42, p = 0.0000

21 KC32

0.52

0.50

a
b

t(99) = −1.35, p = 0.1810

t(99) = −1.53, p = 0.1300

t(99) = 0.55, p = 0.5830

DG = DithGain
EXP=Exploratory

Table 5.9 shows the J-ratio difference between the two corpora. Similarly, the third and
fourth columns of the table list the mean of the J-ratio in the DichGain Corpus and Ex-

95

Table 5.9: Justify Ratio Differences on a per-KC Basis.

KC

DichGain

Exploratory

Diff

Stats Comparison

1

KC12 **

0.23

0.54

DG < EXP

t(99) = −4.40, p = 0.0000

2

KC14 **

0.69

0.48

DG > EXP

t(99) = 3.44, p = 0.0009

3

KC20 **

0.42

0.51

DG < EXP

t(99) = −4.24, p = 0.0001

4

KC21 **

0.46

0.56

DG < EXP

t(99) = −4.27, p = 0.0000

5

KC22

0.63

0.61

6

KC23 **

0.54

0.78

DG < EXP

t(99) = −6.72, p = 0.0000

7

KC24 **

0.33

0.57

DG < EXP

t(99) = −8.58, p = 0.0000

8

KC26 **

0.84

0.60

DG > EXP

t(99) = 3.68, p = 0.0004

9

KC27 **

0.63

0.51

DG > EXP

t(99) = 2.49, p = 0.0143

10 KC28 **

0.36

0.54

DG < EXP

t(99) = −3.99, p = 0.0001

t(99) = 0.25, p = 0.8043

ploratory Corpus respectively. The fifth column gives the direction of the difference in which
“DG > EX” demonstrates that DichGain Corpus got justification steps more frequently
than the Exploratory Corpus on corresponding KC, while “DG < EX” means the reverse is
true. If this column is blank, it means that there were no significant differences between the
corpora on the justification ratio on the KC. The last column shows the statistical results
between the two corpora with one-tailed paired t-tests. If the difference is significant, the KC
name in column 2 was labeled with “**”. In Table 5.9. There was no significant difference
in terms of the number of justifications between the two corpora on KC22 . While on six
(KC12 , KC20 , KC21 , KC23 , KC24 , and KC28 ) the DichGain group were more likely to skip
a justification step than the Exploratory group. There were only three instances for two KCs
(row: KC14 , KC26 , and KC27 ) in which the DichGain group was more likely to receive tells
than the Exploratory group.
Thus, although no significant learning differences were found between the two groups
for time on task or learning performance, significant differences were found between the two
corpora. The DichGain corpus was significantly less interactive and included less justification
96

steps than the exploratory corpus.

5.4

DISCUSSION

The goals in Study 2 were to investigate how to apply RL to induce tutorial tactics from a
training corpus and then to test whether the induced tutorial tactics would result in more
effective learning performance than making random decisions. This was to be accomplished
without running a full-scale comparison. Results showed that following the DichGain tutorial
tactics generated significantly less elicits and included fewer justification steps than following
the random decisions in the Exploratory group. A more detailed analysis, however, showed
that this difference varied from KC to KC. While applying RL did induce tutorial tactics
from the Exploratory corpus and the induced tutorial tactics were subtle, they did not seems
to be more effective. Despite of the lack of random assignment, no significant difference was
found between the two groups on either the pre-test, post-test, adjusted post-test or the
NLG.
There were at least three potential reasons for lack of difference in learning performance
between the DichGain and Exploratory groups. First, a full comparison of the DichGain
and Exploratory groups was not run by assigning students randomly into the two groups.
Second, the hypothesis may simply be incorrect, that micro-level policies covering interactive decisions like ET and JS do not affect students’ learning. The decisions may be too
“fine-grained” to have a real impact on learning, no matter how optimal the policy. Initial analysis based on the comparison of the DichGain and Exploratory groups appears to
support previous research. That research suggests that given that content is controlled to
be same, pedagogical tutorial tactics may not result in different learning. Third, it is also
possible that lack of a difference in learning performance may be caused by limitations in
the RL approach.
In other words, applying RL to induce tutorial tactics may not be a simple task for
which we can plug a toolkit into the training corpus and induce effective tutorial tactics. As
demonstrated in Study 2, tutorial tactics depend on many factors, such as feature choices,
97

feature selection, feature discretization and so on. Their effectiveness might also depend on
how we implement them back into Cordillera. For example, how we deal with conflicting
policies. It can be argued that in Study 2, exploration of these factors was limited. For
example, only eighteen features were included in our search space, but no more than four
appeared in the final induced tutorial tactics. It is possible that the selected features were
insufficient to adequately represent the state space. Moreover our greedy-like feature selection process and the discretization procedure of using simple median splits may also have
limited our success.
Study 3 was designed to address these reasons in hopes of producing more effective
pedagogical tutorial tactics. In it the approach to RL-related issues was modified. For
example, the training dataset was expanded to include both the Exploratory Corpus and the
DichGain Corpus in the induction process. Also, more features were included in the feature
states. To address the more weighty issue of learning performance, one set of tutorial tactics,
like the policies in the present study, was derived with the goal of enhancing the tutorial
decisions that contribute to the students’ learning; while the other was derived with the goal
of enhancing those decisions that contribute less or even none to the students’ learning. To
summarize, in contrast to Study 2, Study 3 included multiple datasets, a larger feature set,
induction of policies based on multiple corpora, and random assignment of subjects to two
comparable groups. The methods and outcomes of Study 3 are discussed in the next two
chapters.

98

6.0

APPLYING RL TO INDUCE NORMALIZED GAIN (NORMGAIN)

AND INVERSE NORMALIZED GAIN (INVNORMGAIN) TUTORING
TACTICS

The conclusion of Chapter 5 identified three potential problems with Study 2 that might
explain the absence of a learning difference in the two groups’ performance. The earlier
study did not run a full comparison by randomly assigning students into the two groups.
The feature space selection may have been inadequate. The lack of a learning difference
may also suggest that decisions on the level of elicit/tell (ET) and justify/skip-justify (JS),
however well timed, cannot significantly affect the students’ performance. Many previous
studies showed that after solving the same training problems with the tutorial scripts written
by the same authors, no significant difference was found among students’ learning by means
of different learning treatments [VanLehn et al., 2007a]. In this study, the content was controlled to be equivalent even at a much lower level than in these previous studies. Therefore,
it is possible that these micro-decisions would not make a difference in students’ learning.
As shown in Studies 1 and 2, both the Exploratory and DichGain groups gained significantly. However, no signifciant difference was found between the two groups in a post-hoc
comparison.
On the other hand, even if there was an impact by tutorial decisions on learning, random
selection might have a good chance (50% chance given that both decisions were binary) to
guess the “proper” decisions, and thus might have made enough effective decisions. If so,
the impact of the tutorial decisions on learning would be canceled out. Therefore, in order
to investigate whether micro-step decisions would make a difference in learning, the contrast
between the two conditions in Study 3 was sharpened. Instead of choosing “random” as the
control condition, the InvNormGain Group was selected.
99

In short, in Study 3 two sets of tutorial policies were induced: the Normalized Gain
(NormGain) set induced by using the students’ NLG as rewards and the Inverse Normalized
Gain (InvNormGain) set was induced by specifically using students’ (1-NLG) as rewards.
In other words, the NormGain tutorial tactics were derived with the goal of enhancing the
tutorial decisions that contribute to the students’ learning, while the InvNormGain tutorial
tactics were derived with the goal of enhancing those decisions that contribute less, or not
at all, to the students’ learning. If RL did live up to its promise, then it is expected that
the NormGain students would out-perform their InvNormGain peers. This would occur if
the micro-level decisions on ET and JS do impact learning.
Apart from the reward functions, the tactics were induced using the same general procedure. In this chapter, the main focus is on describing how RL was applied to induce these
two sets of tutorial tactics. The experimental comparison of these two sets will be presented
in Chapter 7. While the previous two studies were implemented by the ITR research group,
Study 3 was designed, executed and evaluated by the author.
In order to induce tutorial tactics in Study 3, the same general learning procedure described in Chapter 3 and again in Chapter 5 was employed. As in the proceeding chapter the
five major RL issues are addressed in Study 3 and changes made from Study 2 are explained.

6.1

TRAINING CORPUS

In Study 2, the only corpus available was the Exploratory Corpus. At this stage, three training corpora were available: the Exploratory Corpus collected in Study 1, the DichGain corpus
from Study 2, and a combined corpus from both sets. The Exploratory Corpus consisted
of 64 students’ super-dialogues, the DichGain-Corpus consisted of 37 super-dialogues. The
combined set contained 101. Each super-dialogue covered one student’s entire interaction
with the Cordillera system including all seven training problems.
The choice of Training Corpus is a complex one. As explained previously, the Exploratory
Corpus was collected for RL and designed to explore the feature space evenly and without
bias. The DichGain Corpus, by contrast, is similar to many other pre-existing corpora.
100

Inducing a successful policy from it would show the potential for applying RL to induce
effective tutorial policies from most pre-existing data. The combined corpus, in theory, offers
the benefits of both as well as an increased dataset. In this study, rather than selecting one
corpus a priori, all three were used. More specifically, tutorial tactics were derived from each
corpus separately, and then the best policies from all the sets were selected by ECR.

6.2

KNOWLEDGE COMPONENTS

Study 2 opted to induce tutorial policies for as many KCs as possible, covering every KC that
was involved in at least one tutorial step. However, these KCs were not equally important.
For example, in a domain such as physics, the domain principles are more challenging and
important than other KCs. Consider, for example, KCs 23 and 20:
KC23 : The unit for energy is the Joule (J).
KC20 : If an object is moving, then its kinetic energy at a time is 12 mv 2 , where m is the
objectś mass and v is the magnitude of the object’s instantaneous velocity.
In domains such as physics, solving a problem requires producing an argument, proof
or derivation consisting of one or more inference steps; each step is the result of applying a
domain principle, operator or rule. Here KC20 is one of the major domain principles, i.e.
the definition of Kinetic Energy, while KC23 is not a major principle. Therefore, KC20 is
more important than the latter in that the student’s overall learning performance depends
more on learning a domain principle such as KC20 and less so on KC23 . Additionally, clearly
KC20 is a complex principle with a non-trivial cognitive load while KC23 is an atomic fact
and thus much simpler to convey and apply.
In Study 2 the ECRs of the KC-based tutorial tactics for KC23 are 42.45 on ET decisions
and 47.22 on JS decisions. Either ECR is much higher than the corresponding ECR of the
tutorial tactics on KC20 : 4.81 on ET decisions and 4.29 on JS decisions respectively. So
when KC23 and KC20 co-occurred in a tutorial decision step, the dialogue manager would
follow the policy for KC23 even though KC20 is a domain principle and learning it is more
101

important for students to learn the domain. Therefore, in Study 3, the decision was made
to focus only on the eight primary KCs: KC1 , KC14 , KC20 , KC21 , KC22 , KC24 , KC27 &
KC28 , each of which represent a major domain principle shown in Table 4.1.
Table 6.1 compares the frequency and ratio of various tutorial decisions on the eight main
KCs among the three corpora; the last row presents the comparison over all KCs across the
three corpora. Columns 3 and 4 list the average number of ET and JS decisions per KC in
each corpus. Column 5 shows the average number of tutorial overall decisions (regardless
of whether it is ET or JS). Columns 6 and 7 present the I-ratio and J-ratio respectively.
The last column presents a t-test comparison of the students’ KC-based pre- and post-test
scores. In the last column, if students’ KC-based post-test scores were significantly greater
than their corresponding pre-test scores, then the results of the t-test were listed in the last
column. There were no cases in which students’ post-test scores on a KC were significantly
lower than their corresponding pre-test scores.
From Table 6.1, it can seen that the average number of tutorial decisions (column 5)
varies significantly across KCs: from as few as four on KC1 to more than 80 on KC20 . The
average number of tutorial decisions on elicit/tell (ET) (column 3) and justify/skip-justifys
(JS) (column 4) also varies across KCs. There are only 4.05 ET decisions on KC1 and more
than 70 on KC20 . Similarly, there are only 3.34-3.97 JS decisions for KC22 on average and
more than 16 for KC21 . Overall, the ET tutorial decisions were much more frequent than
the JS ones.

6.3

KC-BASED REWARD

In Study 2, the student’s final reward was based upon his/her KC-based NLG. More specifically, for each KCk , the students were divided into two groups, low learners and high
learners, according to a median split of the students’ KC-based NLGs. The high learners
were assigned a final reward of +100 while the low learners were assigned a final reward of
−100.
However, there were at least two limitations from doing this. First, there was little to
102

Table 6.1: Compare Three Corpus on Eight Primary KCs

ET
KC1

KC14

KC20

KC21

KC22

KC24

KC27

KC28

Overall

JS

overall

I-ratio

J-ratio

pre-post

Exp

4.05

4.05

1.16

t(126) = 3.28, p = 0.0013

Dich

4.16

4.16

2.37

t(72) = 2.80, p = 0.0066

Comb

4.09

4.09

1.33

t(200) = 4.30, p = 0.0000

Exp

7.95

2.00

9

1.36

0.74

t(126) = 1.076, p = 0.284

Dich

8.38

2

9.00

7.44

0.95

t(72) = 2.03, p = 0.0462

Comb

8.11

2

9.00

3.49

0.80

t(200) = 2.04, p = 0.0422

Exp

72.59

15.36

81.53

1.04

1.21

t(126) = 5.38, p = 0.0000

Dich

72.43

16.43

82.84

0.33

0.78

t(72) = 4.45, p = 0.0000

Comb

72.53

15.75

82.01

0.78

1.05

t(200) = 6.94, p = 0.0000

Exp

33.63

16.92

60.75

1.01

1.45

t(124) = 3.93, p = 0.0001

Dich

49.00

16.86

59.05

1.90

0.99

t(72) = 4.30, p = 0.0001

Comb

39.26

16.9

60.13

1.77

1.28

t(198) = 5.73, p = 0.0000

Exp

30.7

3.34

32.06

1.02

1.34

t(126) = 2.39, p = 0.0184

Dich

31.32

3.97

32.78

0.97

1.73

t(72) = 2.41, p = 0.0185

Comb

30.93

3.57

32.33

1.00

1.50

t(200) = 3.30, p = 0.0011

Exp

52.7

15.48

60.45

1.05

1.52

t(124) = 3.92, p = 0.0001

Dich

48.97

15.89

60.24

0.40

0.58

t(72) = 3.11, p = 0.0027

Comb

51.34

15.63

60.38

0.81

1.18

t(198) = 4.99, p = 0.0000

Exp

21.45

4.83

23.89

1.10

1.32

t(126) = 4.52, p = 0.0000

Dich

21.92

4.62

23.73

0.89

1.75

t(72) = 1.99, p = 0.051

Comb

21.62

4.75

23.83

1.02

1.47

t(200) = 4.67, p = 0.0000

Exp

14.2

4.06

16.58

1.06

1.58

t(126) = 1.911, p = 0.058

Dich

14.19

5.14

18.32

3.58

0.69

t(72) = 1.82, p = 0.07

Comb

14.20

4.46

17.22

1.98

1.24

t(200) = 2.61, p = 0.0099

Exp

273.89

56.61

305.48

0.99

1.19

t(126) = 3.32, p = 0.0012

Dich

270.54

58.43

307.57

0.79

0.77

t(72) = 2.92, p = 0.0046

Comb 272.66

57.28

306.25

0.92

1.03

t(200) = 4.40, p = 0.0000

103

no differentiation between the students who learned much more than the median split and
those who were merely above the median and between those who were just below the median
and those who achieved a much lower score than the median. Additionally, the difference
between the high and low learners rewards were always 200 across all KCs, but the actually
NLG difference between the high learners and low learners varied across KCs: for example it
was 0.30 on KC20 but 0.49 on KC23 . It is actually difficult to compare the induced KC-based
tutorial tactics across KCs (when there were conflicting policies in multi-KC steps) in this
way, because it also depends how much difference existed between the high and low learners
on that KC. So in Study 3, instead of using a median split, the final rewards were made
directly proportional to the real NLG scores.

As described above, one primary goal in this study was to compare the NormGain tutoring tactics with the InvNormGain ones. For inducing NormGain policies the final reward
value was set for each di on KC k as: N LGKCk × 100. That is, the student’s KC-based normalized learning gain for the given KC multiplied by 100. For the KC-general policies, the
final reward for each super-dialogue di was cumulative KC-based N LG × 100 where N LG
was the students’ learning gain as calculated based on his/her cumulative KC scores. Because N LG ∈ (−∞, 1], the maximum final reward was +100 and the minimum was −∞ for
enhancing learning tutorial tactics. Therefore, the NormGain tutoring tactics were expected
to enhance students’ learning.

For inducing InvNormGain policies, the inverted final rewards were used. More specifically, for KC-specific policies the reward was set for each di as (1 − N LGKCk ) × 100. For the
KC-general tutorial tactics, the reward was set for each di was: (1 − N LG) × 100. Because
(1 − N LG) ∈ [0, +∞), the maximum final reward was +∞ and the minimum was 0. So
the induced InvNormGain tutoring tactics were expected to enhance the reward for tutorial
actions that contributed less or nothing to the students’ learning.
104

6.4

STATE REPRESENTATION

As described in Chapter 3, the issue of state representation can be divided into four subissues. They are 1) defining the potential feature choices in state representation; 2) capping
the number of features included in each policy; 3) discretizing the features appropriately;
and 4) determining feature selection procedures. Compared to Study 2, several changes were
made in state representation. The number of features was expanded, and the maximum
number of features that could be included in a policy were also increased. In addition, a
different method on feature discretization was adopted, and more general feature selection
approaches were explored. How these four sub-issues were addressed in Study 3 is discussed
below.

6.4.1

Sub-issues 1: Feature Choices

In Study 2, 18 features were defined in four categories. One of major concern in Study 2
was that the 18 feature choices might not represent the state well enough. For example,
all three autonomy features were based on the number of elicits or tells the tutor gave to
that point in the session. However, the number may also depend on how much a student
said so far rather than the number of times the student input. Two tutorial dialogues can
have the same number of elicit/tells, but a student who generated a lot of words in his/her
entry generally did more work than another student who only generated one or two words
per turn. Therefore, the first motivation in Study 3 was to include more features in each
category so that it would represent the dialogue states better.
The second motivation was to expand the number of categories. In addition to the four
categories proposed in [Moore et al., 2004], two other categories were included that had been
suggested by the previous literature. For example, previous research indicated that there
was the learning difference between genders [Coley, 2001, Gallagher, 2001, Quek et al., 2002].
Additionally, we have shown that other background information such as MathSat score can
predict a student’s learning in math and science [Chi and VanLehn, 2008]. Therefore, a new
category of features that included certain background information was added. The category
105

was named Background Features.
Additionally, previous analyses by Litman’s group have shown that simple linguistic
features computed from the students’ contributions to the tutorial dialogue are correlated
with learning. Forbes-Riley et al. [Forbes-Riley et al., 2007], for example, discovered that the
number of times a student mentioned a physics concept and the number of physics concepts
involved in a student’s dialogue were significantly correlated with learning. Additionally, in
[Purandare and Litman, 2008] the authors identified several additional features that can be
used to predict learning gains. These include the number of physics concepts mentioned in the
student’s turn, the concept-to-word ratio, the number of student turns with physics concepts,
and so on. Therefore, a new feature category was added that describes the characteristics of
dialogue generated by students. This category was named Student Dialogue Features.
In a word, feature choices were expanded from four categories and eighteen features in
Study 2 to six categories and fifty features in Study 3. The categories are: amount of the
work that the tutor has let the student perform (Autonomy); time-related tutorial contextual information (Temporal situation); contextual information about the solution process
( Problem Solving Contextual features); the student’s current performance (Performance);
background information about the student (Background ); and semantic information about
the students’ tutorial dialogues (Student Dialogue). All of these features are static information, or can be computed in real time as the student works. The individual features are
described below.

6.4.1.1

Autonomy — five features Autonomy Features relate to the amount of work

performed by the student in the dialogue. All five autonomy features end with an ‘A’ in their
name and are numeric. Three of the five were included in Study 2 while two of the features,
stuWordsToTuWordsA and stuWordsToTuWordsSessionA, are new. In the following, the
label “**” is used to represent that the feature is a new feature and was not included in the
state choice in Study 2.
1. tellsSinceElicitA: The number of tells the student has received since the last elicit
prompt, irrespective of the KC involved. This feature reflects how active a student is
right now.
106

2. pctElicitA:The percentage of elicit/tell decision points compared to what the tutor has
opted to elicit during the dialogue, irrespective of KC. This feature reflects how active a
student is overall.
3. stuWordsToTuWordsA** : The ratio of student-generated words to tutor-generated
words over the entire tutoring history, regardless of KCs. This feature also reflects how
active a student is overall, but it uses the words ratio. This is because when two students
receive the same percentage of elicits, a student with higher stuWordsToTuWordsA is
assumed to be more active than the one with lower stuWordsToTuWordsA.
4. stuWordsToTuWordsSessionA**: The ratio of student-generated words to tutorgenerated words in this session regardless of KCs. This feature also reflects how active a
student is in this session by using the words ratio between the student and the tutor.
5. pctTellsKCSessionA: The percentage of tells received this session for the given KC,
KCk . This feature reflects how active a student is on a specific KC in this session.

6.4.1.2

Temporal Situation — three features Temporal Situation Features encode

time-related information about the problem-solving process. All three temporal situation
features end with a ‘T’ and are numeric. All three were included in Study 2.
1. durationKCBetweenDecisionT: Time since the last tutorial decision was made on
the current KC. This feature reflects how active a student’s knowledge of the current KC
is. If “durationKCBetweenDecisionT” is high, it means that the tutor has not mentioned
the KC recently so the student’s knowledge on the current KC may be still.
2. TimeInSessionT: The total time spent in the current session. This feature reflects a
student’s fatigue level.
3. TimeBetweenSessionT: The time elapsed between the end of the previous session and
the beginning of the current one. This feature reflects how likely a student has forgotten
what they learned in previous sessions.

6.4.1.3

Problem Solving Contextual — fifteen features Problem Solving Contex-

tual features encode information about the current problem-solving context. All fifteen
problem solving-related features end with ‘PS.’ In Study 2, we included five features in this
107

category (The first five features listed below — without “** ” in their names). However,
there are certain features that are important to describe the context of the tutorial decisions
which were not previously included. Thus, this category primarily consists of 10 new feature
choices that the author believes will represent the tutorial context. The selection of these features was informed by prior research. For example, previous research suggests that whether
to tell or to elicit should depend on the student’s current competence and how difficult the
knowledge is. Therefore, a new feature was included, “conceptDifficultyPS”, to describe
the tutorial questions’ difficulty level. Moreover, [Purandare and Litman, 2008] found that
the number of concepts introduced by the tutor per-turn correlates with students’ learning,
so the features tutAverageConceptsPS** and tutAverageConceptsSessionPS** were in the
following list:
1. EarlyTrainingPS: For the first three problems, the value is 0 and for the later four
problems, the value is 1. This feature reflects how well a student might get used to the
tutoring system.
2. SimpleProblemPS: The first three problems are categorized as simple problems since
solving them involves only one domain principle; the next two are medium; and the final
two are complex. This feature reflects the complexity of the training problems’ problem
solutions.
3. DuringWalkThroughPS: For each training problem, the tutorial dialogues followed a
two-stage procedure: first problem solving followed by a post-problem discussion. This
feature describes whether a tutorial decision occurred during the problem solving or
post-problem discussion.
4. nKCsPS: The number of times the present KC has occurred in the current tutorial
dialogue. This feature reflects overall how familiar the student is with the current KC.
5. nKCsSessionPS: The occurrences of the current KC in the tutorial dialogue in this
session so far. This feature reflects how many times the student has accessed the current
KC in this session.
6. newLevelDifficultyPS**: If the current problem is more complex than the prior problem (i.e. we have crossed a boundary from simple to medium or medium to complex).
108

This value is 1 for P1, P4, and P6 and 0 for the rest. If a problem is much more difficult
than its predecessors, it might take a student long time to learn the problem.
7. conceptDifficultyPS**: The current question’s difficulty level. This feature is roughly
estimated from the combined training corpus of Exploratory and DichGain Corpus. For
each tutorial decision step, we count the number of times the tutor decided to elicit the
answer from the students from the combined corpus and represent it as #elicit. Then
among all these occurrences, we count the number of occurrences of correct answers in
the corpus and represent it as #correct; the number of occurrences of incorrect answers
as #incorrect; the number of occurrences that students’ simply did not answer the
tutor’s question by input “I do not know” as #unknown, and finally, the number of
occurrences of partially correct answers as #partial. A partial correct answer refers to a
correct but incomplete answer; for example, to calculate the value of the Kinetic Energy
of the rock at T0 in P4, sometimes a student’s answer was “1.200 instead of “1.2J 00 . The
conceptDifficultyPS was calculated by

conceptDif f iculty =

0.0 ∗ #correct + 1.0 ∗ (#incorrect + #unknown) + 0.5 ∗ #partial
#elicit
(6.1)

ConceptDifficulty is always in the range of [0, 1]. If conceptDifficulty =1, it means it is
a difficult question, whereas if it is close to 0, it means it is an easy question.
8. QuantitativeDegreePS**: This feature measures how quantitative the tutorial action
is. When the value is 1, it indicates the tutorial action is purely quantitative; when it
is 0, it is purely qualitative. When it is in the middle, then it is mixed. For example,
Line 4 in Figure 1.2 is a quantitative step since the tutor asks about the name of the
principle to apply to solve for KE0 , while Line 6 in Figure 1.2 is a qualitative step since
the tutor asks the student “Would the direction of v0 affect the rock’s kinetic energy at
T0 ?”. For some decision steps, both types of discussion are involved. For example, in
a post-problem discussion, the tutorial decision step was about what physics quantities
the Kinetic Energy depends on. A correct answer should look like From KE= 12 mv 2 . We
can infer that the Kinetic Energy of an object at time point T is influenced by the mass
of the object and its magnitude of velocity at T. This is both a quantitative step in that
109

students need to know the formula of kinetic energy and it is also a qualitative step in
that they need to know what the variables represent here, especially that v only refers to
the object’s magnitude of the velocity at time T, which does not include the direction.
So for this tutorial decision step, the QuantativeDegreePS is 0.5.
9. numPhysConceptsTutorDialogueSessionPS**: The number of tutor’s physics concepts

1

in this session so far. These physics concepts were identified and generated by

the two domain experts and knowledge representations (not the author); these are the
key words and physics concepts in the domain of work and energy.
10. tutAverageConceptsPS**: The average number of a tutor’s physics concepts in each
turn. This feature reflects how many physics concepts the tutor has mentioned so far
and how important the tutor’s turns might be. The more frequently physics concepts
showed in tutor’s turns, the more likely students might learn from these previous tutor’s
turns.
11. tutAverageConceptsSessionPS**: The average number of physics concepts in each
tutor’s turn in this session. This feature reflects how important the tutor’s turns might
be in this session.
12. tutConceptsToWordsPS**: The ratio of physics concepts to the words that have been
used in the tutor’s turn. This feature also reflects how often the tutor has mentioned
physics concepts overall.
13. tutConceptsToWordsSessionPS**: The ratio of physics concepts to the words that
have been mentioned in the tutor’s turn in this session. This feature also reflects how
often the tutor has mentioned physics concepts in this session.
14. tutAverageWordsPS**: The average number of words in the tutor’s turn. This feature reflects how verbose the tutor is overall.
15. tutAverageWordsSessionPS**: The average number of words in the tutor’s turn in
this session. This feature reflects how verbose the tutor is in the current session.
1

“Physics concepts:” A word is a physics concept if it is one of the following: ’scalar’, ’vector’, ’mass’,
’displacement’, ’velocity’, ’acceleration’, ’gravitation’, ’gravity’, ’force’, ’weight’, ’normal’, ’friction’, ’system’,
’isolated’, ’non-isolated’, ’kinetic’, ’energy’, ’potential’, ’total mechanical’, ’gravitational’, ’spring’, ’tme’,
’spe’, ’ke’, ’gpe’, ’conservation’, ’non-conservation’, ’work’, ’network’, ’net’, ’direction’, ’perpendicular’.

110

6.4.1.4

Performance — twelve features Performance Features describe information

about the student’s performance during the training. All twelve performance-related features
end with “PM.” In Study 2, seven features were included (listed as the first seven in below).
Five of the twelve features in this category are new for Study 3. Most of the original seven
features defined here described a student’s overall performance to that point. However, a
more accurate description of his/her performance should be based upon a student’s more
recent capability or performance. Therefore, five performance features have been added
which measure students’ more recent performance — in the session so far.
1. pctCorrectPM: We compute this by assessing all of the correct KCs in students’ entries
divided by the total number of KCs in the students’ entries. This feature reflects the
student’s overall competence when only elicits are counted as learning opportunities.
2. pctOverallCorrectPM: We compute this by assessing all of the correct KCs in student’s entries divided by the total number of KCs shown in both the tutor’s entries and
the student’s entries. This feature reflects the student’s overall competence, when both
elicits and tells are counted as learning opportunities.
3. nCorrectKCPM: The absolute number of correct responses on the current KC in the
student’s entries. This feature reflects the student’s overall competence on the current
KC by measuring how many times the student have given correct responses on the KC.
4. pctCorrectKCPM: We compute this by assessing all of the correct cases on the present
KC in the student’s entries divided by the total number of cases the present KC showed
in the student’s entries. This feature reflects the student’s overall competence on the
current KC when only elicits on the KC are counted as learning opportunities.
5. pctOverallCorrectKCPM: We compute this by assessing all of the correct cases on the
present KC in the student’s entries divided by the total number of cases on the present
KC in both the tutor’s and the student’s entries. This feature reflects the student’s
overall competence on the current KC when both elicits and tells that involve the KC
are counted as learning opportunities.
6. pctCorrectKCSessionPM: We compute this by assessing all of the correct cases on
the present KC in the student’s entries in this session divided by the total number of
cases on the present KC in the student’s entries in this session. This feature reflects the
111

student’s competence on the current KC in this session when only elicits on the KC in
this session are counted as learning opportunities.
7. nIncorrectKCPM: The number of incorrect student responses on the current KC since
the start point. This feature reflects the student’s overall incompetence on the current
KC.
8. nCorrectKCSessionPM**: The absolute number of correct responses on the current
KC in the student’s entries in this session. This feature reflects the student’s incompetence on the current KC in this session.
9. pctCorrectSessionPM**: We compute this by assessing all of the correct KCs in the
student’s entries in this session divided by the total number of KCs in the student’s
entries in this session. This feature reflects the student’s overall competence across all
KCs in this session when only elicits in this session are counted as learning opportunities.
10. pctOverallCorrectSessionPM**: We compute this by assessing all of the correct
KCs in the student’s entries in this session divided by the total number of KCs shown in
both the tutor’s entries and the student’s entries in this session. This feature reflects the
student’s overall competence across all KCs in this session, when both elicits and tells in
this session are counted as learning opportunities.
11. pctOverallCorrectKCSessionPM**: We compute this by assessing all of the correct
cases of the present KC in the student’s entries in this session divided by the total number
of cases of the present KC in both the tutor’s and the student’s entries in this session.
This feature reflects the student’s overall competence on the current KC in this session
when both elicits and tells that involve the current KC in this session, are counted as
learning opportunities.
12. nIncorrectKCSessionPM**: The number of incorrect student responses on the current KC in this session. This feature reflects the student’s overall incompetence on the
current KC in this session.

6.4.1.5

Background — five features As described above, previous research has shown

that certain background features describe general information about the student’s ability to
learn. Five Background Features, such as the student’s pre-test scores, have been included.
112

None of these features change during problem solving. All five background features end with
“BG.” All features in this category are new features which were not incorporated into Study
2. One important note was that for DichGain group, the following features, genderBG,
ageBG, MathSatBG, and VerbalSatBG, were not available because of the administrative
error.
1. genderBG**: The student’s gender. It may be the case that the effectiveness of tutorial
policies depends upon differences in gender. For example, male students might learn
better by answering the questions, while female ones might learn better by reading the
information.
2. ageBG**: The student’s age. This feature reflects how much school experience the
participant might have.
3. MathSatBG**: The student’s math SAT scores. This feature reflects the participant’s
math skill since the physics domain is a math-related domain.
4. VerbalSatBG**: The student’s verbal SAT scores. This feature reflects the participant’s reading skill since the selected domain also has a lot qualitative discussions.
5. pretestBG**: The student’s pre-test scores. This feature reflects the participant’s
competence in physics before he/she starts the training session.

6.4.1.6

Student Dialogue — ten features This is also a new category. It describes

the characteristics of the entries input by students. These are simple linguistic features that
are computed from the student’s part of the tutorial dialogues. These features were inspired
by previous work on tutoring. Forbes-Riley et al., for example, discovered that the number of
times a student mentioned a physics concept and the number of physics concepts involved in
the students’ dialogue were significantly correlated with learning [Forbes-Riley et al., 2007].
Additionally, Purandare and Litman, identified several additional features that can be used
to predict learning gains: the number of physics concepts mentioned in the students’ turn,
the concept-to-word ratio, the number of the student’s turns with physics concepts, and so
on [Purandare and Litman, 2008].
113

1. averagePhysConceptsStudentDialogueSD**: The average number of physics concepts mentioned per student turn since the training started. This feature reflects how
physics-like the student-generated answers have been since the beginning of the training.
2. numStudentConceptualDialogueSD**: The number of the student’s turns that includes at least one physics concept. This feature reflects how many times the studentgenerated answers included at least one physics concept.
3. stuConceptToWordRatioSD**: The ratio of physics concept words to total words
in the student’s turns. This feature also reflects how physics-like the student-generated
answers have been since the beginning of the training.
4. stuAverageWordsSD**: The average number of words per student turn. This feature
also reflects how verbose the student was overall. It might also reflect how active the
student was.
5. stuAverageConceptSD**: The average number of the student turns that involve at
least one physics concept. This feature reflects how often the student’s answers involved
at least one physics concepts since the start of the training.
6. averagePhysConceptsStudentDialogueSessionSD**: The average number of physics
concepts mentioned per student turn in this session. This feature reflects how physics-like
the student-generated answers are in this session.
7. numStudentConceptualDialogueSessonSD**: The number of the student turns
that mention physics concepts in this session. This feature reflects how many times the
student- generated answers included at least one physics concept in this session.
8. stuConceptToWordRatioSessionSD**: The physics concepts to words ratio per
student’s turn in this session. This feature also reflects how physics-like the studentgenerated answers are in this session.
9. stuAverageWordsSessionSD**: The average length of student turns in this session.
This feature reflects how verbose the student was in this session, and it might also reflect
how active the student was in this session so far.
10. stuAverageConceptSessionSD**: The average number of student turns which involve at least one physics concept over all the student turns in this session. This feature
114

reflects how often the student’s answers involved at least one physics concepts in this
session.

6.4.1.7

Simplified Example of Deriving Fifty Features from Log Files. Similar

to Chapter 5, the same sample tutorial dialogue in Table 5.1 were transformed into fifty
features for inducing tutorial tactics on KC20 . I attach how the 50 features were updated as
the sample dialogue in Table 5.1 goes on in the Appendix L.

6.4.2

Sub-issues 2: Maximum Number of Features

In Study 2, the maximum number of features was capped at four because of the four categories. The effect of this was that the maximum number of features involved in the induced
tutorial tactics was limited to only one. It is quite possible, however, that for some KCs
there was more than one feature from one category that should have been included in the
state representation. Therefore, in Study 3, no limit was set for the number of features that
each category could contain, nor was a requirement set that there needed to be a feature
from each category.
In order to determine the maximum number of features in the induced policy, it is
necessary to consider the amount of available data and available computational power. In
the worst case scenario, there were only 2 JS tutorial decision steps in the DichGain training
corpus for KC14 . In order to learn effective tutoring tactics, we should have a corpus that
covers each of these states at least once. Therefore, based on the minimum data available
from the three training corpus for KC14 , we capped the number of features in each policy at
six, which means that there are at least 26 = 64 states in the learned policy. Alternatively,
we could have used a flexible number for different KCs. However, given that six features
would already result in very subtle policies, as shown in Table 6.5, it is not the case that
learned tutorial tactics with six features were most effective. Instead the final induced policies
primarily have 3-5 features in their state representation, and only two of 34 final tactics have
six features. So it appears that six is a reasonable number for this study.
115

6.4.3

Sub-issues 3: Feature Discretization

Five of the fifty features, EarlyTrainingPS, SimpleProblemPS, DuringWalkThroughPS, genderBG, and newLevelDifficultyPS are discrete. The remaining forty-five features are numeric
and must be discretized before a suitable MDP can be constructed. Previously, in Study 2
a median split was implemented. For the present study, a more complicated procedure was
adopted. The discretization procedure in Study 3 used two clustering procedures, one based
upon bounding the number of clusters, and the other based upon identifying the optimal
cluster means.
For each of the continuous valued features, the ideal number of clusters was identified
using a TwoStep package embedded in SPSS. TwoStep clustering is a scalable cluster analysis
algorithm designed to handle very large datasets. It is capable of handling both continuous
and categorical variables and attributes. Its key advantage is that it can find the optimal
number of clusters when the ideal value is unknown. Once the proper number of clusters
has been determined, it is possible to apply more traditional K-mean methods to identify
the contents of each cluster. After the clusters and their mean values were identified, the
clusters were ranked by value, assigned the student values, and discretized according to
cluster membership.
Although the median split in study 2 may not have been optimal, it capped the number
of possible values for each state feature at two. By using the automatic procedure, it is highly
likely that each feature it would have been discretized into many clusters. Increasing the
number of possible values in a state would increase the number exponentially. For example,
the application of the TwoStep package to stuAverageConceptSD from the Exploratory
training corpus on KC20 resulted in seven clusters. If each feature has seven possible values,
for a six-feature policy, it would have 76 . In order to control the number of possible values,
the maximum number of discrete values for each feature was set at four. However, this
procedure is still risky in that it can generate small clusters containing at most a handful of
data points. This can then lead to problems of data sparsity during the RL phase. These
two problems were addressed by adding a reduction loop to the procedure. If any given
cluster within a set contained less than 20% of the total cases after K-means clustering took
116

place, then the number of clusters was reduced by one and the K-means algorithm was run
again. This process was repeated until all clusters exceeded the 20% threshold.
If this requirement could not be met, then the feature was discretized through a median
split. A Pseudo-code representation of the algorithm is shown below:
1. for each feature choice fi :
2.
Step 1: Count the clusters numCluster by applying two-step clustering approach
3.
Step 2: Via k-means clustering, discretize the feature fi into numCluster clusters.
4.
Step 3: if one cluster has less than 20% cases:
5.
Step 4: if numCluster =2:
6.
Step 5: median split
7.
Step 6: else:
8.
Step 7: numCluster = numCluster − 1
9.
Step 8: Go to Step 2.

For example, by running this procedure, the system performed a median split on “stuAverageConceptSD” so that values in the range of 0 to 0.228395 is 0 and values in the range
of 0.228395 to 1 is 1 (The feature choices were normalized in this dissertation). Finally, the
number of clusters for each feature ranged from, at minimum, two clusters to as many as
four.

6.4.4

Sub-issues 4: Feature Selection

In Study 2 a simple greedy-feature selection method was used. The procedure was repeated
later, using the same Exploratory Corpus, the same 18 features and the discretization procedure, rewards and so on. The only difference was that some new feature selection methods
were applied. Results showed that by simply changing the feature selection methods, the
induced policies had a much higher ECR than the ones used in Study 2. Recall that the
ECR was the criteria for picking the best policies [Chi et al., 2008a]. For example, the policy π(KC22 , ET ) used in study 2 previously had an ECR of 9.40. Under the new feature
selection methods, the policy π(KC22 , ET )∗ had an ECR of 44.29, almost four times higher.
Therefore, in Study 3 a more complex set of eleven feature-selection approaches were
explored. The domain general feature selection methods were the main ones explored, and by
doing so, the relationships among the features were neglected. For example, a domain-specific
feature selection approach could only select features that correlated with NLG. However, by
117

doing so, most of the problem-solving contextual features would never be considered such as
DuringWalkThroughPS, EarlyTrainingPS, and so on.
Almost every approach described below involved inducing single-feature policies first.
That is, for each of the fifty feature choices, the RL package was used to induce a singlefeature-policy. Because generating single-feature policies does not involve any feature selection procedure, such policies were labelled as “single”. In the following, the focus is on
using feature selection to select at least two features in a policy. To differentiate from the
single-feature policies, the policies induced through feature selection were labelled as nonsingle-feature policies. In short, Study 3 explored eleven feature-selection methods to induce
non-single-feature policies. Four of the approaches were based upon RL (Upper Bound,
Lower Bound, ECR, and Hedge) used in the previous studies [Chi et al., 2008a] ; one was
based on Principal Component Analysis (PCA) (PCA-only); four were combinations of PCA
and RL (PCA-Upper Bound, PCA-Lower Bound, PCA-ECR, and PCA-Hedge); while the final pair were based upon stochastic selection (Random and PCA-random).
In the following, π(KCi , Dj , N ormGain) and π(KCi , Dj , InvN ormGain), are used to
represent a NormGain and an InvNormGain KC-based policy on KCi for tutorial decisions
Dj respectively. Here Dj ∈ {ET, JS}, KC ∗ is used to represent KC-general policies, and
KCi ∈ {KC1 , KC14 , KC20 , KC21 , KC22 , KC24 , KC27 , KC28 , KC ∗ }.

6.4.4.1

RL-based Feature Selection As described in Chapter 3, after being given

a complete MDP structure, the Tetreault and Litman’s toolkit would calculate a policy
together with the policy’s ECR and 95% CI [Tetreault and Litman, 2008]. Lower-Bound
and Upper-Bound were used to refer to the 95% confidence bounds calculated for the
ECR. For example, a final tutorial tactic in Study 3 π(KC14 , ET, N ormGain) was based
on feature: durationBetweenDecisionT alone which is derived from the combined corpus.
π(KC14 , ET, N ormGain) states that “if the duration since last the tutorial decisions on
KC14 is less than 160.07 sec, then the tutor should elicit.”; π(KC14 , ET, N ormGain) has
ECR = 9.99 (range [−∞, 100]) with a 95% confidence interval= [9.85, 10.06], which means
there is a 95% chance that the ECR of the learned policy is between a lower-bound of
9.85 and an upper-bound of 10.06. Another π(KC14 , ET, N ormGain)∗ was based on the
118

feature conceptDifficultyPS and is derived from training the Exploratory Corpus; it states
that: “if the current tutorial decision step is easy (< 49.53%), then the tutor should elicit.”
π(KC14 , ET, N ormGain)∗ has ECR = 1.19 with a 95% confidence interval = [0.03, 3.39].
To this point ECR has always been used as the criteria for selecting the best policies. However, the policy’s Lower-Bound or Upper-Bound can also be used as the criteria. More specifically, the former evaluates the performance of policies in the worst case,
while the latter describes how well the policy can perform. As in the example above,
π(KC14 , ET, N ormGain) is more effective than policy π(KC14 , ET, N ormGain)∗ because
even its Lower-Bound is much higher than policy π(KC14 , ET, N ormGain)∗ ’s Upper-Bound.
Sometimes researchers encounter situations in which the ECR for Policy A is the same as
the ECR for Policy, B, but the confidence interval of A is much narrower than that of B.
In this case, a new criterion, Hedge, can be applied to compare the two policies. Hedge is
defined as a learned policy:

Hedge =

ECR
U pperBound − LowerBound

(6.2)

By applying Hedge, policy A is shown to be more effective than policy B. Any of these
criteria, ECR, Lower-Bound, Upper-Bound, or Hedge can be used to evaluate policies, and
thus they are used as four different criteria for feature selection. These feature-selection
methods are fairly straightforward and use the same general procedure, described below.
For each ranking metric in [ECR, Lower-Bound, Upper-Bound, Hedge]
1. For each of the 50 feature choices, use the RL package to induce a single-feature-policy.
2. Rank the policies in descending order based upon the ranking metric.
3. For i = 2 to 6
Pick the top i features from the ranked list and construct an MDP using them for the
state representation.
Induce a policy for that MDP and set it aside.

These feature-selection metrics based upon the sorting criteria used: ECR, Lower-Bound,
Upper-Bound, and Hedge respectively. These are RL-based methods as the feature selection
procedures all use the policy ECR to identify optimal feature choices. For each of the ranking
metrics, the above procedure resulted in five lists of policies and resulted in twenty policies
119

that involved at least two features in the state representation for each KC on each type of
tutorial action decision from each corpus.
Here is an example which applies the Upper-Bound feature selection for deriving policies on KC14 for the ET decision from the Exploratory Corpus. First, from the fifty features fifty single-feature policies were learned: π1 , · · · , π50 . Then the fifty single features
were sorted based on the upper-bound of its corresponding single-feature policy. Then the
first six features were selected for which the corresponding single-feature policies had the
highest upper-bounds. In this example, the six selected features are in the order of frequency: {durationBetweenDecisionT, numPhysConceptsTutorDialogueSessionPS, nKCsPS,
nKCsSessionPS, nIncorrectKCSessionPM, tutAverageWordsSessionPS}. For the first feature, “durationBetweenDecisionT” a single-feature policy has already been learned, so the
process begins with the second one. The process used is to learn a two-feature policy based
on the first two features: “durationBetweenDecisionT, numPhysConceptsTutorDialogueSessionPS”, and then a three-feature policy based on the first three features: “durationBetweenDecisionT, numPhysConceptsTutorDialogueSessionPS, nKCsPS”, and so on. The most complicated policy to be learned would be a six-feature policy which includes all six features in
the state. Therefore, for KC14 on ET tutorial decisions, five non-single-feature tutorial
tactics were induced by following the Upper-Bound feature from each training corpus.

6.4.4.2

PCA-based Feature Selection Given the data sparsity problems that most

machine learning techniques face, an ideal state representation should have as few features
as possible while still being rich enough to represent the domain accurately. Unfortunately, some of the features available in this study were highly correlated which reduced
their expressiveness when used together. For example, given nCorrectKCPM, the number
of correct responses on the current KC KCk , and nIncorrectKCPM, the number incorrect
responses on KCk generated by the student, pctCorrectKCPM, the percentage of correct
responses given by the student on KCk could easily be calculated as: pctCorrectKCP M =
nCorrectKCP M
.
nCorrectKCP M +nIncorrectKCP M

Therefore, it was necessary to apply an analysis procedure to

avoid redundant features. One such procedure explored in this thesis is Principal component
analysis (PCA) [Jolliffee, 2002].
120

PCA is a mathematical procedure that transforms a large number of, possibly correlated,
variables into a smaller number of uncorrelated variables called principal components. It is
a popular dimensionality-reduction technique as it is simple, non-parametric, unsupervised,
and has been applied successfully in a number of domains. More formally, given a list of
n − dimension variables, PCA extracts a reduced set of p (p < n) principal components or
factors that account for most of the variance present in the original set.
This is done by first extracting n principal components from the original n variables. Each
component is a linear combination of the variables. Formally, it results in a set of n linear
equations with n unknown variables. In other words, PCA repackages the original variables
into an equal number of uncorrelated principal components. The first of these components
accounts for the largest possible amount of variance. The second component, which attempts
to explain the variance remaining after the first component has been extracted, accounts for
the second largest amount of variance, and so on. As the components are extracted they are
restricted to be orthogonal. Geometrically, they may be viewed as defining an n-dimensional
space.
The variance in this correlation matrix is “repackaged” into a set of n eigenvalues, one
for each principal component. Each eigenvalue represents the amount of variance covered
by its associated component. Thus the first eigenvalue accounts for the largest share of
the variability with each one accounting recursively for the largest share of the remaining
variability once its predecessors are accounted for. Each of the n variables’ variance is
normalized to 1. Each component’s eigenvalue may be compared to this standard value to
determine how much more or less variance it accounts for than a single variable. With n
variables there is a total of n variance to distribute. The extracted components, like the set of
variables, account for all of this variance collectively. The proportion of variance accounted
for by one component equals its eigenvalue divided by n.
Typically, the goal was to derive a set of less than n components. When a set of p out
of the variables share a considerable amount of variance, then p < n components will have
relatively large eigenvalues, while the rest will have substantially smaller eigenvalues. In this
situation what needs to be determined is how many components will be retained and how
many will be discarded. One rule of thumb is to drop any component with an eigenvalue
121

of less than 1, that is, any component that accounts for less variance than a single variable.
Having done that, the n − dimensional space defined by the original variables has been
reduced to a p − dimensional component space that still covers the bulk of the variance.
In this study, initially, all fifty feature choices were normalized. PCA was then applied
to the normalized features to generate fifty principal components and their corresponding eigenvalues. These eigenvalues were arranged in descending order, and all components
whose eigenvalues were less than 1 were removed. For each eigenvalue, the feature that
was maximally correlated with the corresponding principal component was identified. The
resulting features were a subset of the original fifty feature choices that were designated the
PCA-feature subset. PCA-feature subset is an ordered list arranged by the eigenvalues of
its corresponding principal components. Results showed that the number of PCA-features
selected for each KC varied from eight to thirteen.
Once the PCA-feature subset was identified, the PCA-only feature selection procedure
was straightforward. It began with the first feature in PCA-feature subset and added one
feature at a time to the MDP and learned a new policy. This process was repeated five
times.
Here is an example. After running PCA on a list of 50 − dimension feature variables
exacted from the Exploratory Corpus for inducing KC-general tutorial tactics on elicit/tell,
a reduced set of thirteen principal components whose eigenvalues were bigger than 1 was extracted. They were ordered by their corresponding eigenvalues as shown in Table 6.2. In that
table, the second column shows the eigenvalues of the corresponding components arranged
in decreasing order. Specifically, eigenvalues decreased from 6.59 for the first component
to 1.11 for the 13th component. The third column represents the cumulative eigenvalues.
For example, the last row of the third column is 81.42%, which means these 13 principal
components have 81.42% of the total information provided by the original fifty variables.
For each principal component, one feature with the highest correlation with the component was extracted. In this instance, “pretestBG” (the students’ pre-test score) is selected for the first principal component, SimpleProblemPS (whether the current problem
students are working on is a simple problem or not) as the second principal component,
and so on. The corresponding correlation value is listed in the last column. So our PCA122

Table 6.2: An Example PCA Feature Set from the Exploratory Corpus Induced for A KCgeneral Tutorial Tactics

Order

EigenVals

EigenCumulative

EigenVar

EigenCorr

1

6.589

13.179

pretestBG

0.962

2

4.858

22.894

SimpleProblemPS

0.933

3

4.746

32.387

nKCsSessionPS

0.939

4

4.416

41.218

stuAverageWordsSD

0.912

5

4.304

49.827

tutAverageConceptsSessionPS

0.859

6

3.626

57.08

pctCorrectSessionPM

0.882

7

3.601

64.283

stuConceptToWordRatioSessionSD

0.866

8

2.555

69.393

MathSatBG

0.914

9

1.357

72.107

newLevelDifficultyPS

0.871

10

1.205

74.516

pctElicitA

0.618

11

1.202

76.919

QuantativeDegreePS

0.868

12

1.141

79.202

conceptDifficultyPS

0.578

13

1.11

81.421

durationBetweenDecisionT

0.763

feature subset is simply the collection of the thirteen features in column 4, sorted by the
corresponding eigenvalues, which is pretestBG, SimpleProblemPS, nKCsSessionPS, stuAverageWordsSD, tutAverageConceptsSessionPS, pctCorrectSessionPM, stuConceptToWordRatioSessionSD, MathSatBG, newLevelDifficultyPS, pctElicitA, QuantativeDegreePS, conceptDifficultyPS, durationBetweenDecisionT.
For PCA-only feature selection, the researcher started with a single feature policy,the
first Eigen Variable pretestBG here, then a two-feature policy by using the first two Eigen
Variables: pretestBG, SimpleProblemPS, and so on. The most complicated policy would be
a six-feature policy (because the maximum number of features in a policy was capped at
six) by using the first six Eigen Variables: pretestBG, SimpleProblemPS, nKCsSessionPS,
123

stuAverageWordsSD, tutAverageConceptsSessionPS, pctCorrectSessionPM, which included
one background feature “BG”, three problem state contextual features “PS”, one feature
about the student’s dialogue “SD”, and one feature about the student’s performance “PM”.
Therefore, for each KC for each type of tutorial action from each individual corpus, five
non-single-feature tutorial tactics were induced using the PCA-only feature selection.

6.4.4.3

PCA and RL-based Feature Selection Thus far, four RL-based feature se-

lection methods and a PCA-only feature selection method have been described. By simply
combining PCA-only feature selection with the four RL-based feature selection methods,
four new feature selection approaches are created. In this method, PCA is used to identify
the PCA-feature subset from the original fifty features, creating a set of available features
that have eigenvaluse greater than one. The four RL-based methods, PCA-Upper Bound,
PCA-Lower Bound, PCA-ECR, and PCA-Hedge are then applied as before. In effect these
combined feature selection methods are being used to winnow the set of available features,
not once, but multiple times.
Here is a summary of the procedure:
[Stage 1:] Select the PCA-feature Subset:
[Phase 1:] Apply PCA on fifty features.
[Phase 2:] Identify the set of principal components with its
eigenvalues greater than or equal to one.
[Phase 3:] For each component, identify the feature F that is most
correlated with the component.
The resulting features are the PCA-feature choices and are ranked in
the order of their eigenvalues.
[Stage 2:] Apply the RL-based approach:
For each ranking metric in [ECR, Lower-Bounds, Upper-Bounds, Hedge]
For each of PCA-feature choices, use the RL package to induce a
single-feature-policy.
Rank the policies in descending order based upon the ranking metric.
For i = 2 to 6:
124

Pick the top i features from the ranked list and construct
an MDP using them for the state representation.
Induce a policy for that MDP and set it aside.
Based on the sort criteria in phase 2, four feature selection methods were named PCAECR, PCA-Lower Bound, PCA-Upper Bound, and PCA-Hedge respectively. Similar to previous approaches, for each KC on each type of tutorial action from each individual corpus,
five non-single-feature tutorial tactics were induced by combining the PCA and RL-based
feature selection methods.

6.4.4.4

Random Feature Selections Thus far nine feature selection methods have

been introduced. In order to evaluate their relative effectiveness, a random feature selection
method was also introduced. The expectation was that the nine feature selection approaches
described above would be more effective than a random feature selection. In other words,
it was anticipated that the final tutorial tactics would be induced by the feature selection
methods introduced above rather than the random feature selection. Two random feature
selection procedures were employed: Random and PCA-Random. In the former case features
were randomly selected from all fifty feature choices. In the latter case PCA-based feature
reduction was applied to reduce the set of variables to those with high variance correlations,
and then features were randomly selected from the reduced set. Here is the summary of this
procedure:

Random-selection :
For j= 1 to 2
For i = 2 to 6
Randomly select i features from fifty features.
Induce a policy for that MDP and set it aside.

125

PCA-Random:
[Stage 1:] Select the PCA-feature Subset:
[Phase 1:] Apply PCA on fifty features.
[Phase 2:] Identify the set of principal components with
eigenvalues greater than or equal than one.
[Phase 3:] For each component, identify the feature F
that is most correlated with the component.
The resulting features are the PCA-feature choices and
are ranked in the order of their eigenvalues.
[Stage 2:] Random Selection:
For j= 1 to 2
For i = 2 to 6
Randomly Select i features from the PCA-feature Subset.
Induce a policy for that MDP and set it aside.
For each KC for each type of tutorial action from each individual corpus, 10 non-singlefeature tutorial tactics were induced by following either random or PCA-random feature
selection.
To summarize, for each KCi and decision (ET or JS) < KCi , Dj >, three training
corpora, a space of fifty features, and eleven feature selection methods were explored. For
each KCi and decision pair one set of policy choices was collected for each training corpus.
For each corpus, there were fifty single-feature policies. Applying eleven feature selection
methods to them yielded 5 × 9 + 10 × 2 = 65 non-single-feature policies (the random and
PCA-random feature selection yielded 10 non-single-feature policies each and the remaining
nine methods yielded five non-single-feature ones each). A total of 115 potential tutorial
tactics were generated for a single KC and decision pair per training corpus. Taken together,
all three corppora resulted in a total of 115 × 3 = 445 policies for each pair < KCi , Dj >.
The best policy for < KCi , Dj > was selected from this pool by ECR. For the purposes of
this study, the highest ECR irrespective of the confidence bounds or hedging was selected.
This is similar to Study 2.
126

6.5

CONFLICTING POLICIES

In Study 2, a total of 21 KCs for elicit/tell decisions and 10 KCs for justify/skip-justify
decisions were considered. When faced with conflicting tutorial policies, the policy with
the highest ECR was chosen. However, in the selected domain, certain KCs are necessary
precursors for other KCs. KC24 , for example, is the definition of Total Mechanical Energy
: T M E = KE + GP E + SP E. In order to apply KC24 effectively, students need to know
the definition of Kinetic Energy, which is KC20 KE = 21 mv 2 . Thus KC20 is a necessary
precursor to KC24 . In Study 3, we have the ECR of π(KC20 , ET ) = 14.25 while the ECR
of π(KC24 , ET ) = 13.51. In some of the ET tutorial decisions that involve both KC24 and
KC20 , KC24 , what would the major topic of discussion be, given this precursor relationship?
By following the old procedure on conflicting policies, the system would always choose the
policy with the highest ECR. In this case, the system would follow the π(KC20 , ET ) because
its ECR is higher than π(KC24 , ET )’s ECR, even though the KC24 is the target topic.
Therefore, in Study 3 the focus was narrowed eight main KCs and a more complex conflictresolution approach was adopted. The KCs were first grouped according to three levels of
“priority” based upon the domain knowledge with “Top” being the highest and “Low” being
the lowest:
Top Level: KC27 , KC28 .
Medium Level: KC24 , KC14 .
Low Level: KC20 , KC21 , KC22 , KC1 .

This heuristic was built in such way that lower-level KCs are necessary precursors for the
higher level KCs. KC27 for example is conservation of total mechanical energy : T M E1 =
T M E0. In order to apply KC27 effectively, students need to know about the definition of
Total Mechanical Energy, which is KC24 T M E = KE + GP E + SP E. Thus KC24 is a
necessary precursor for KC27 and KC27 is not a necessary precursor for any other KCs in
the domain. Therefore we put KC27 in the highest level: Top Level and KC24 in the second
level: Medium Level. By always choosing the highest level KCs in a multiple-KC decision
step, we can pick the policy on the target KCs.
127

When a tutorial decision involved multiple KCs, the system first collected the set of
policies at the highest level. If a tutorial decision step did not involve any of the eight major
KCs, then the system will follow the KC-general tactics. If, however, a decision step did
involve some of the eight primary KCs, then the system would poll the tutorial tactics for
KCs at the maximum rank, and will follow the policy with the highest ECR.

6.6

SUMMARY: INDUCTION OF TUTORIAL TACTICS IN STUDY 3

For Study 3 two sets of tutorial tactics were induced. The first is Normalized Gain or the
NormGain set. The second is Inverse Normalized Gain or the InvNormGain set. Both
sets were induced using the same RL procedures on the same corpora and differed only in
the reward function applied to the training corpora. The NormGain set used a positive
NLG-based reward and the InvNormGain set used an inverse of that same function.
Both the final NormGain and InvNormGain sets contained 17 policies. Thus, we have
a total of 34 policies. In each set, two out of the 17 are KC-general policies, one is an ET
policy and the other is a JS policy. The remaining 15 policies are KC-based and consist of
seven pairs of policies for seven of the eight main KCs and one policy for KC1 . Each of the
seven pairs is associated with a single KC and contains one ET policy and one JS policy.
KC 1 does not arise in any JS decisions and thus only an ET policy was induced for it.
In order to examine a range of possible tactics for each KCi on either ET or JS tutorial
decisions < KCi , Dj >, three training corpora, a space of fifty features, and eleven feature
selection methods were used to yield 445 policies. As discussed above, the corpora were:
the Exploratory Corpus collected in Studies 1, the DichGain Corpus collected in Studies
2, and a combined corpus that merged both datasets. The fifty feature choices could be
divided into six categories as described in detail above, and eleven feature selection methods
could be applied to them. The best policy for each pair < KCi , Dj > was selected from
445 policies by ECR. For the purposes of this study only the policy with the highest ECR
irrespective of the confidence bounds or hedging was selected. This selection process was
repeated for each of the 34 policies with the KC-general policies being chosen from models
128

that ignored the involved KCs. The full list of NormGain policies used in Study 3 are
shown in Appendix N and the full list of InvNormGain policies are shown in Appendix O.
They detail the policies themselves, the corpus from which they were drawn, the features
involved, their discretization ranges, the feature selection method used, the policy’s ECR
and its confidence bounds.
The resulting NormGain and InvNormGain policies were implemented back into Cordillera
yielding two new versions of the system, named NormGain-Cordillera and InvNormGainCordillera respectively. Both systems applied the policies to guide tutorial decisions. KCbased decisions were guided using the KC-based policies. Conflicts were resolved using the
ranking among the primary KCs discussed in Section 6.5. For steps that did not involve one
of the eight primary KCs, the system used the KC-general tutorial tactics.
Table 6.3 summarizes the major differences between the RL procedure used in Study 2
and that in Study 3. The RL procedures differed on all the major issues. In the next section
the discussion will focus on some general characteristics of the induced tutorial tactics.
Specifically, it will focus on the source corpus that each of 34 tutorial tactics was derived
from, the features that were involved, and which feature selection method yielded the most
tutorial tactics, and so on.

129

Table 6.3: Issue-by-Issue Comparison of Studies 2 and 3

Study 2
Aspects
Training Corpora
KCs:
Features:

Discretization:
Feature Selection

DichGain

NormGain

InvNormGain

Exploratory

Exploratory, DichGain, & Combined

31 KCs

8 main KCs

18 features

50 features

4 categories

6 categories

Medium Split

TwoStep first and then k-means

Category-based

11 including Random; ECR; Hedge; & PCA

4

6

Max features /policy:
Reward:

Study 3

N LG × 100

(N LG > median)

(100 − N LG) × 100

→ +100;
(N LG ≤ median)
→ −100
Conflicting Policies:

Follow max ECR

6.7

Use KC ranking then ECR.

INDUCED POLICIES

In this section, the induced tutorial tactics will be described by identifying the training corpus
that each final tutorial tactic was derived from, which feature categories were most frequently
involved in the final tutorial tactics, and which feature selection method discovered the most
final tutorial tactics. The full list of policies used in Study 3 are shown in Appendix N
and Appendix O. The purpose of this section is to determine how RL-related decisions
described in the previous section had impacted the induced tutorial tactics. For example,
one decision was made to use all three training corpora, did the final induced policies come
from one corpus or from all three corpora? Moreover, which features appeared in the final
130

induced tutorial tactics? Which feature selection method(s) seemed to be more effective?
This section begins with a discussion of the training corpus involved in the final 34 tutorial
tactics.

6.7.1

Source Training Corpus

Table 6.4 shows which corpus was used to induce the corresponding tutorial tactics. The
second and third columns show the source training corpus used in deriving NormGain tutorial
tactics on ET and JS for corresponding KCs respectively. The fourth and fifth columns
show similar information for the InvNormGain tutorial tactics. The last three rows 10-12
summarize the number of tutorial tactics derived from each corresponding training corpus.
For example, Rows 10 and 11 show that the Exploratory Corpus and the DichGain Corpus
each generated sixteen final tutorial tactics. The Exploratory Corpus was used to generate
11 NormGain tutorial tactics (5 ET and 6 JS) and 5 InvNormGain ones (3 on ET and 2
on JS) while the DichGain Corpus was used to generate five NormGain tutorial tactics (3
ET and 2 JS) and eleven InvNormGain ones (6 on ET and 5 on JS). The combined corpus,
however, only generated one tutorial tactic each for NormGain and InvNormGain.
Table 6.4 also shows that both the Exploratory and DichGain Corpora were involved in
generating the final tutorial tactics. However, the majority of the NormGain tutorial tactics
were from the Exploratory Corpus, eleven out of seventeen, while most of the InvNormGain
tutorial tactics were from the DichGain Corpus, also eleven out of seventeen. This result
suggested that the choice of the training corpus is very important for deriving tutorial tactics.
However, it is not very intuitive to determine why most of the NormGain policies were from
Exploratory Corpus, while most InvNormGain ones were from DichGain Corpus. Future
work is needed to explore the characteristics of a training corpus and how to choose a
training corpus.

131

Table 6.4: The Source Training Corpus Of the Inducing 34 Tutorial Tactics

NormGain
ET

InvNormGain
JS

ET

JS

1

KC1

DichGain

2

KC14

Combined

3

KC20

Exploratory Exploratory Exploratory

4

KC21

Exploratory Exploratory Exploratory Exploratory

5

KC22

Exploratory Exploratory Exploratory

6

KC24

7

KC27

8

KC28

9

Overall

DichGain

DichGain
Exploratory

DichGain

Combined
DichGain

DichGain

Exploratory

DichGain

DichGain

Exploratory Exploratory

DichGain

DichGain

DichGain

DichGain

DichGain

Exploratory

Exploratory

DichGain

DichGain

DichGain

10 Exploratory

5

6

3

2

16

11

DichGain

3

2

6

5

16

12

Combined

1

0

0

1

2

6.7.2

Total

Number of Features

Table 6.5 shows the number of the features involved in the thirty-four final tutorial tactics.
The second and third columns show the training corpus used in deriving NormGain tutorial
tactics on ET and JS for corresponding KCs respectively. The fourth and fifth columns
show the same information for the InvNormGain tutorial tactics. Table 6.6 summarizes
distribution of sizes, that is, how many policies contained one feature, two, and so on. For
example, Row 12 in Table 6.6 shows that there were eight policies that involved three features,
four NormGain ones and four InvNormGain ones. To our surprise, only two tutorial tactics
involved six features and most of the policies involved three to five features.
132

Table 6.5: The Complexity of the 34 Induced Tutorial Tactics

NormGain

InvNormGain

#

KC

ET

JS

ET

JS

1

KC0

4

5

5

4

2

KC1

1

3

KC14

1

1

1

1

4

KC20

3

5

3

5

5

KC21

3

3

6

3

6

KC22

2

5

3

4

7

KC24

4

6

2

2

8

KC27

4

4

4

5

9

KC28

5

3

3

5

2

Table 6.6: Distribution of Policy Sizes.

NormGain
#

Size ET

InvNormGain

JS

ET

JS

Total

10

1

2

1

1

1

5

11

2

1

0

2

1

4

12

3

2

2

3

1

8

13

4

3

1

1

2

7

14

5

1

3

1

3

8

15

6

0

1

1

0

2

133

6.7.3

Feature Choices

The total number of feature occurrences across all thirty-four tutorial tactics was 117. For
each induced tutorial tactic, the number of features involved were counted and then totaled.
If a feature occurred in several induced tutorial tactics, then each occurrence was counted
as one. More specifically, the total number of feature occurrences across the NormGain and
InvNormGain tutorial tactics was fifty-nine and fifty-eight respectively.

6.7.3.1

Autonomy Features Autonomy Features relate to the amount of work done

by the student in the dialogue. Five Autonomy features were defined: tellsSinceElicitA,
pctElicitA, stuWordsToTuWordsA**, stuWordsToTuWordsSessionA**, and pctTellsKCSessionA. As mentioned earlier, features with “**” were new ones added in Study 3. The five
autonomy features occurred thirteen times. Among the five features, with the exception of
tellsSinceElicitA, the remaining four features occurred only once in one final induced policy.
Feature tellsSinceElicit occurred in nine out of the thirty-four final tutorial tactics included
in the state representation: five for NormGain and four for InvNormGain. Table 6.7 summarizes the occurrences of each feature in the induced NormGain and InvNormGain tutorial
tactics. The number in the parenthesis refers to the number of occurrences. For example,
NormGain(8) means there were eight occurrences of autonomy features in NormGain tutorial tactics and pctElicitA (1) means that “pctElicitA” occurred once in the final tutorial
tactics. The two new features labelled with “**” occurred only twice.
Table 6.7: Occurrence of Autonomy Features in The Final Tutorial Tactics

NormGain (8) InvNormGain(5)
1 tellsSinceElicitA (9)

5

4

2 pctElicitA (1)

1

0

3 stuWordsToTuWordsA** (1)

1

0

4 stuWordsToTuWordsSessionA** (1)

1

0

5 pctTellsKCSessionA (1)

0

1

134

6.7.3.2

Temporal Situation Features Temporal Situation Features encode the time-

related information about the problem-solving process. Three features are defined: durationKCBetweenDecisionT, TimeInSessionT, and TimeBetweenSessionT. Table 6.8 summarize the number of occurrences of each feature in the induced NormGain and InvNormGain
tutorial tactics. The three features occurred a total of fourteen times in the final thirty-four
policies. In Table 6.8, Row 1 shows that durationBetweenDecisionT showed up eight times,
more frequently than the other two features: four times in NormGain tutorial policies and
four times in InvNormGain ones.

Table 6.8: Occurrence of Temporal Situation Features in The Final Tutorial Tactics

NormGain (7)

InvNormGain(7)

1 durationBetweenDecisionT (8)

4

4

2 TimeBetweenSessionT (2)

1

1

3 TimeInSessionT (4)

2

2

135

Table 6.9: Occurrence of Problem Solving Contextual Features in The Final Tutorial Tactics

NormGain (30)

InvNormGain (28)

1

EarlyTrainingPS (2)

1

1

2

SimpleProblemPS (2)

1

1

3

DuringWalkThroughPS (6)

2

4

4

nKCsPS (4)

3

1

5

nKCsSessionPS (3)

2

1

6

newLevelDifficultyPS** (4)

2

2

7

conceptDifficultyPS** (12)

7

5

8

QuantativeDegreePS** (5)

0

5

9

numPhysConceptsTutorDialogueSessionPS**(1)

1

0

10 tutConceptsToWordsPS** (8)

5

3

11 tutConceptsToWordsSessionPS** (4)

3

1

12 tutAverageWordsPS** (5)

2

3

13 tutAverageWordsSessionPS** (2)

1

1

6.7.3.3

Problem Solving Contextual Features Problem Solving Contextual features

encode information about the current problem-solving context. There are fifteen features
defined in this category. Table 6.9 summarizes the occurrences of each feature in the induced
NormGain and InvNormGain tutorial tactics. This category seems to be the most active.
There were fifty-eight occurrences in the final thirty-four tutorial tactics, which represents
approximately half of all of feature occurrences. In some of the tutorial tactics, more than
one feature from this category was involved.
Among the fifteen Problem Solving Contextual features, conceptDifficultyPS** (Row 7)
is the most frequently occurrences, occurring in twelve induced tutorial tactics: seven for
NormGain and five for InvNormGain. The next most frequently occurring feature is in Row
136

ten: tutConceptsToWordsPS**, which describes the ratio of the tutor’s physics concepts to
their words. Two features (tutAverageConceptsPS** and tutAverageConceptsSessionPS**,
which represent the average number of tutor’s physics concepts in each turn overall and in
this session specifically, did not appear in any of the final induced tutorial tactics. Among the
fifty-eight occurrences, new features added for Study 3 occurred forty-one times: twenty-one
for NormGain and twenty on InvNormGain ones.

6.7.3.4

Performance Features Performance Features describe information about the

student’s performance during problem solving. Twelve feature choices were defined in this
category. Table 6.10 summarizes the occurrences of each feature in the induced NormGain
and InvNormGain tutorial tactics. The features in this category occurred seventeen times.
Row eight shows that “nIncorrectKCPM” (the number of incorrect response in the student’s
dialogue so far) is the most frequently occurring feature in that it appeared in five final
tutorial tactics: two for NormGain and three for InvNormGain. A feature such as pctOverallCorrectPM did not appear in any of the final tutorial tactics, probably because the closelyrelated feature, pctOverallCorrectSessionPM, better represents the state in that it measures
the student’s more recent performance. PctOverallCorrectSessionPM (Row two) occurred
three times: one for NormGain and two for InvNormGain. The new features occurred a total
of five times: “nCorrectKCSessionPM** (2)” in Row four, “pctCorrectKCSessionPM** (1)”
in Row seven, and “nIncorrectKCSessionPM** (2)” in Row nine.

6.7.3.5

Background Features Much to the author’s surprise, only one background

feature occurred in one final tutorial tactic: “ageBG**” (the age of the subject). The policy
involved “ageBG**” is on KC24 and Justify/Skip-Justify. The remaining four background
features were not involved in any policy.

6.7.3.6

Student Dialogue Features Student Dialogue Features are simple linguistic

features that are computed from the student’s entries in the tutorial dialogue. Ten features
were defined in this category. Table 6.11 summarizes the occurrences of each feature in
the induced NormGain and InvNormGain tutorial tactics. The features in this category
137

Table 6.10: Occurrence of Performance Features in The Final Tutorial Tactics
NormGain (5)

InvNormGain (12)

1 pctCorrectPM (1)

0

1

2 pctOverallCorrectSessionPM (3)

1

2

3 nCorrectKCPM (1)

0

1

4 nCorrectKCSessionPM** (2)

1

1

5 pctOverallCorrectKCPM (1)

0

1

6 pctCorrectKCPM (1)

0

1

7 pctCorrectKCSessionPM** (1)

0

1

8 nIncorrectKCPM (5)

2

3

9 nIncorrectKCSessionPM** (2)

1

1

occurred fourteen times. Among them, “stuAverageWordsSD**” (the average number of
words per student turn) occurred four times, while stuConceptToWordRatioSD** (the ratio of physics concept words to total words in the student’s turns) occurred three times.
Three features: averagePhysConceptsStudentDialogueSD**, stuAverageConceptSD**, averagePhysConceptsStudentDialogueSessionSD** did not appear in any of the final tutorial
tactics.

To summarize, Problem Solving Contextual Features occurred most frequently, fifty-eight
times, in the final thirty-four induced tutorial tactics. Background Features occurred the
fewest number of times. The newly added features were involved in the final in a total of
2 + 41 + 5 + 1 + 14 = 63 times, so it could be concluded that expanding feature choices to
include new features such as conceptDifficultyPS**, was a good decision given that it had
the most occurrences in the final tutorial tactics.
138

Table 6.11: Occurrence of Student Dialogue Features in The Final Tutorial Tactics

NormGain (8)

InvNormGain (6)

1 numStudentConceptualDialogueSD** (1)

1

0

2 stuConceptToWordRatioSD**(3)

1

2

3 stuAverageWordsSD** (4)

2

2

4 numStudentConceptualDialogueSessonSD** (1)

1

0

5 stuConceptToWordRatioSessionSD** (2)

1

1

6 stuAverageWordsSessionSD** (1)

0

1

7 stuAverageConceptSessionSD** (2)

2

0

6.7.4

Feature Selection

In this study, I applied 11 feature selection methods. It would be interesting to see which of
them found the most final tutorial tactics. Table 6.12 lists all the feature selection methods
that were followed to get the final tutorial tactics for the corresponding KCs on the two types
of tutorial tactics: the NormGain and InvNormGain ones. Additionally, “single” means it
is a single feature policy.
It can be concluded that the three feature selection approaches: PCA-only, PCA-ECR,
and PCA-Upper Bound did not elicit any of the final tutorial tactics. All other eight approaches resulted in at least one. Among them, the two RL-based feature selection methods
appeared to be most effective. The ECR-based method discovered four NormGain tutorial
tactics and six InvNormGain tutorial tactics. The Upper Bound method found five NormGain tutorial tactics and four InvNormGain tutorial tactics. The feature selection may still
need to be improved because one of the final induced policies is from the random feature
selection — π(KC20 , JS, InvN ormGain).

139

Table 6.12: Applying 11 Feature Selection Methods to Induce 34 Tutorial Tactics

NormGain

InvNormGain

ET

JS

ET

JS

KC1

single

KC14

single

single

single

single

KC20

ECR

PCA-Hedge

PCA-Lower Bound

Random

KC21

Upper Bound

PCA-Hedge

Hedge

ECR

KC22

Hedge

Upper Bound

ECR

ECR

KC24

ECR

Upper Bound

ECR

Upper Bound

KC27

PCA-Random

ECR

Lower Bound

Hedge

KC28

Upper Bound

Upper Bound

ECR

Upper Bound

Overall

Lower Bound

ECR

Upper Bound

ECR

6.8

Upper Bound

SUMMARY: RL IN STUDY 3

To summarize, compared with the RL approach in Study 2, a series of changes were made in
Study 3 to improve the effectiveness of the induced tutorial policies. However, there are many
ways this can be explored in future work. For example, except for following the heuristic
among the KCs to resolve the conflicting policies, we could choose the action that has the
most votes. For instance, if three KCs, KC20 , KC21 and KC24 , were involved in an ET
tutorial decision step. The ET policies on KC20 and KC24 selected to tell, but the KC21 ’s
policy selected to elicit, then the system would choose the one with the majority of the votes–
tell in this case. Similarly, instead of using the Exploratory Corpus and DichGainDichGain
Corpus individually or combined, a subset of the student’s dialogue from each corpus could
be selected to make a new training Corpus, and so on. Other research opportunities exist
which will be discussed in Chapter 9.
140

The RL approach in Study 3 showed that it appears that the Problem Solving Contextual
features are most involved in the final induced tutorial tactics and that most of the NormGain
tutorial tactics were derived from the Exploratory Corpus while most of the InvNormGain
tutorial tactics were derived from the DichGain Corpus. Among the 11 feature selection
approaches, it seemed that the two RL-based tutorial tactics: Upper-Bound and ECR, were
most effective. However, in order to investigate why these are the case, we need more
exploration and it is beyond the content of this dissertation. Additionally, one of the future
works I would like to investigate is how different choices of Training Corpus or feature
selection methods are correlated with learning gains.
Next, the induced tutorial tactics were evaluated on real human subjects to see whether
the students who followed the NormGain tutorial tactics would out-perform those who are
under the InvNormGain ones.

141

7.0

STUDY 3: NORMALIZED GAIN (NORMGAIN) VS. INVERSE
NORMALIZED GAIN (INVNORMGAIN)

In this chapter, I will present an experimental comparison of the induced Normalized
Gain (NormGain) and Inverse Normalized Gain (InvNormGain) tutorial tactics.

7.1

7.1.1

METHODS

Participants

Data was collected over a period of two months during the summer of 2009. Participants
were 64 college students who received payment for their participation. They were required
to have a basic understanding of high-school algebra. However, they could not have taken
any college-level physics courses. Students were randomly assigned to the two conditions.
Each took from one to two weeks to complete the study over multiple sessions. In total, 57
students completed the study (29 in the NormGain condition and 28 in the InvNormGain
condition).

7.1.2

NormGain-Cordillera and InvNormGain-Cordillera

NormGain-Cordillera and InvNormGain-Cordillera were used in Study 3. The only differences between the two systems were that interaction decisions made by NormGain-Cordillera
were guided by the 17 NormGain tutorial tactics and those made by InvNormGain-Cordillera
142

were guided by the 17 InvNormGain tutorial tactics. Only one human wizard (the author)
was involved in Study 3.

7.1.3

Materials & Procedures

Participants in both conditions experienced a background survey; read a textbook covering
the target domain knowledge; took a pre-test; solved the same seven training problems in the
same order on the NormGain-Cordillera for the NormGain condition and the InvNormGainCordillera for InvNormGain condition; and finally took a post-test that was identical to
the pre-test. The exams, introductory materials and training problems used were identical
to those used in Study 2 as was their order of presentation. As discussed in Chapter 5
subsection 5.2.2, in Study 3, everything but the exams were identical to those used in Study
1. The exams differed in only a single test question.

7.1.4

Grading

In study 3, the students’ test answers were graded by a different grader, the author, than the
grader from Studies 1 and 2. All of the tests in study 3 were graded in a double-blind manner
and followed the same procedure as used by the grader in Studies 1 and 2. Following the
same grading rubrics used in the previous studies, each question was assigned two grades:
overall and KC-based grade.
A grader agreement study was conducted in order to establish the validity of this grading.
The inter-grader agreement study, discussed in Appendix B, showed that the grading rubrics
were comparable to those used in the prior studies.

7.1.5

Measures

The main purpose of Study 3 was to investigate whether micro-level tutorial decisions would
make a difference in learning. The hypothesis was that the NormGain group would outperform the InvNormGain group. Therefore, Study 3 mainly focused on the two groups’
learning performances, which is measured by their pre-test and post-test scores. Moreover,
143

the students’ overall learning performance will be compared under both grading criteria.

7.2

RESULTS

Random assignment appears to have balanced the incoming student competence across conditions. A post-evaluation analysis showed that there were no statistically significant differences in the pre-test scores between the two conditions. Additionally, there were no
significant differences between two groups on the mathSAT scores with a one-tailed paired
t-test: t(39) = 0.536p = 0.595 (M = 633.48, SD = 140.14 for the NormGain group and
M = 654.55, SD = 108.92 for the InvNormGain group) or age: t(55) = 0.175p = 0.862
(M = 23.41, SD = 4.39 for the NormGain group and M = 23.64, SD = 5.47 for the InvNormGain group).

7.2.1

Time

No significant difference was found between the two groups in terms of the total training
time spent on Cordillera with a one-tailed paired t-test: t(55) = −.272, p = .787. The
NormGain group spent (M = 259.98, SD = 59.22) and the InvNormGain group spent (M =
264.57, SD = 67.60). By using a one-tailed paired t-test, a more detailed analysis of the time
spent on a per-problem basis revealed no significant difference between the two groups for
all but P6. Figure 7.1 compares the average time students spent on each training problem
between the two groups. On P6, the InvNormGain group spent an average of 57.72 min on
the problem. This was significantly more than the NormGain group’s 47.09 min (t(55) =
3.28, p = 0.002).

7.2.2
7.2.2.1

Learning Performance
Compare NormGain vs. InvNormGain conditions: Overall Learning

Performance A one-way ANOVA was used to test for learning performance differences
between the pre- and posttests. Participants in Study 3 made significant gains from pre-test
144

Figure 7.1: Compare Time Between NormGain vs InvNormGain Groups On Training Problem

to post-test: with F (1, 112) = 36.22, p = .000, R 2 = .70 under the overall grading criteria
and F (1, 112) = 27.58, p = .000, R 2 = 0.71 under the cumulative KC-based grading criteria.
Table 7.1 compares the pre-test, post-test, adjusted-post-test, and NLG scores between
the two conditions under two grading criteria. In Table 7.1, the Adj.Post-test scores were
compared between the two conditions by running an ANCOVA using the corresponding pretest score as the covariate. The rest of the three scores, the pre-test, post-test and NLG
scores, were compared with one-tailed paired t-tests. The first column in Table 7.1 shows
that there were two grading criteria and the second column shows the comparisons were
conducted on four types of test scores: pre-test scores, post-test scores, adjusted post-test
scores, and NLG under each grading criteria. The third and fourth columns in Table 7.1
list the means and SDs σ of the NormGain and InvNormGain groups’ corresponding scores.
The fifth column lists the corresponding statistical comparison and the sixth column lists
the effect size of the comparison. Similar to Study 2, in study 3 Cohen’s d, was used. This is
defined as the mean learning gain of the experimental group minus the mean learning gain
145

of the control group, divided by the groups’ pooled standard deviation. The final column
lists the statistical power of the comparison, 1 − β.
Table 7.1 shows that there was no significant difference between the two groups on pretest scores under either grading criteria. However, there was a significant difference between
the two groups on the post-test, adjusted-post-test, and NLG scores under both grading
criteria. For example, cumulative KC-based NLG scores were used as reward functions for
inducing NormGain and cumulative KC-based (1-NLG) scores were used as reward functions
for inducing InvNormGain. The last row in Table 7.1 shows that with a one-tailed paired
t-test the former group out-performed the latter group: t(55) = 3.058, p = 0.003 and the
effect size was 0.81. Across all measurements, the NormGain group performed significantly
better than the InvNormGain group and the effect size was large by Cohen’s d criteria.

7.2.2.2

Compare NormGain vs. InvNormGain Conditions: KC-based Learning

Performance Above students’ overall learning performance was compared in the previous
section. Since KC-based tutorial tactics were induced, it would be interesting to compare
the two groups’ performance on a KC basis. With first step was to investigate whether
students learned on the eight primary KCs. A one-way ANOVA was used to test for learning
performance differences between the pre- and post-tests on each KC. Table 7.2 shows that
the participants in study 3 gained significantly from pre-test to post-test on all eight primary
KCs.
Next, the two groups’ KC-based scores were compared. Table 7.3 compares the two
groups’ performance on the pre-test, post-test, adjusted-post-test, and NLG scores on the
eight primary KCs. If there was a difference between the two groups, the Cohen d value
is labeled with “**” and it is labeled with “*” if it is marginal significant (p < 0.1). No
significant difference was found between the two groups on the KC-based pretest scores
across all eight KCs. Only on KC27 , did the NormGain group score marginally higher than
the InvNormGain group.
On four out of eight primary KCs (KC1 , KC20 , KC21 , and KC27 ) the NormGain group
significantly out-performed the InvNormGain group on the post-test, adjusted post-test and
NLG scores. On KC22 and KC24 , the NormGain group significantly out-performed the In146

147
0.65 (0.15)
.63 (.095)
0.41 (0.19)

Posttest
Adjusted Posttest
NLG

0.25 (0.21)

.55 (.095)

0.54 (0.20)

0.39 (0.23)

0.31(0.19)

0.62 (0.078)

0.61 (0.18)

0.44 (0.22)

InvNormGain

0.53
0.49
0.48

0.65 ∗ ∗
F (1, 54) = 10.689, p = .002 0.86 ∗ ∗
0.81 ∗ ∗
t(55) = 3.058, p = 0.003

t(55) = 2.32, p = 0.024

0.55

0.15

t(55) = 0.71, p = .484

t(55) = 3.764, p = 0.000

0.59

0.81

F (1, 54) = 18.33, p = 0.000 1.17 ∗ ∗
0.99 ∗ ∗

0.51

t(55) = 2.817, p = 0.007

0.75 ∗ ∗

1−β
0.59

cohen da
0.20

t(55) = 0.719, p = 0.475

Stat

In this dissertation, I used Cohen’s d, which is defined as the mean learning gain of the experimental group minus the mean learning gain of the
control group, divided by the groups’ pooled standard deviation.
b
Std. Deviation

a

0.42 (0.16)

0.48 (0.16)

0.71(0.078)

Adjusted Posttest
NLG

0.72 (0.11)

Posttest

b

0.48 (0.18)

Pretest

Cumulative KC-based Pretest

Overall

NormGain

Table 7.1: NormGain vs. InvNormGain on Pre- and Post-Test

vNormGain group on post-test and adjusted post-test scores, but not on the NLG scores.
On KC14 , the NormGain group significantly out-performed the InvNormGain group on adjusted post-test score, but only marginally significantly on the post-test and NLG scores.
On KC28 , however, no significant difference was found between the two groups on post-test,
adjusted post-test or NLG scores. One of the potential explanations for the lack of difference
on learning between the two conditions across measurements may be that there were too few
times KC28 appeared during the training. For example, Table 4.2 showed that KC28 only
appeared during the problem solving for training problem P4 while the other KC that has
the equivalent complexity is KC27 (KC27 and KC28 are the only two top level KCs.), which
showed up in three training problems: P5, P6 and P7.

Table 7.2: KC-based Pre- and Post-Test Test Scores
Pretest

Posttest

Stat

KC1

0.40 (0.19) 0.59 (0.19) F (1, 112) = 26.67, p < 0.0000, R 2 = 0.582

KC14

0.44 (0.23) 0.59 (0.25) F (1, 112) = 10.85, p = 0.001, R 2 = 0.46

KC20

0.37 (0.20) 0.62 (0.16) F (1, 112) = 54.27, p < 0.0000, R 2 = 0.48

KC21

0.44 (0.22) 0.70 (0.18) F (1, 112) = 46.37, p < 0.0000, R 2 = 0.42

KC22

0.40 (0.25) 0.57 (0.24) F (1, 112) = 12.78, p = 0.001, R 2 = 0.44

KC24

0.43 (0.19) 0.61 (0.18) F (1, 112) = 25.68, p < 0.0000, R 2 = 0.58

KC27

0.48 (0.23) 0.68 (0.26) F (1, 112) = 19.98, p < 0.0000, R 2 = 0.52

KC28

0.36 (0.23) 0.50 (0.23) F (1, 55) = 10.35, p = 0.002, R 2 = 0.48

148

Table 7.3: Between-Group Comparison by KC-based
Pre- and Post-Test Scores
KC
KC1

KC14

KC20

KC21

KC22

KC24

NormGain

InvNormGain

Stat

Pre

0.42 (0.15)

0.39 (0.22)

t(55) = 0.66, p = 0.5095

0.16

0.58

Post

0.65 (0.16)

0.53 (0.21)

t(55) = 2.51, p = 0.0151

0.66**1

0.48

Adj.

0.64 (0.12)

0.54 (0.12)

F (1, 54) = 9.80, p = 0.0028

0.85**

0.52

NLG

0.41 (0.24)

0.24 (0.27)

t(55) = 2.59, p = 0.0122

0.68**

0.48

Pre

0.43 (0.23)

0.44 (0.25)

t(55) = −0.17, p = 0.8638

-0.04

0.87

Post

0.64 (0.22)

0.53 (0.26)

t(55) = 1.71, p = 0.0937

0.47*2

0.52

Adj.

0.65 (0.17)

0.53 (0.17)

F (1, 54) = 6.47, p = 0.0139

0.72**

0.56

NLG

0.38 (0.34)

0.18 (0.39)

t(53) = 2.00, p = 0.0502

0.56*

0.54

Pre

0.38 (0.17)

0.37 (0.22)

t(55) = 0.31, p = 0.7613

0.05

0.77

Post

0.68 (0.13)

0.57 (0.18)

t(55) = 2.48, p = 0.0163

0.72**

0.58

Adj.

0.67 (0.11)

0.58 (0.11)

F (1, 54) = 10.30, p = 0.0022 0.83**

0.47

NLG

0.47 (0.22)

0.32 (0.19)

t(55) = 2.75, p = 0.0080

0.74**

0.51

Pre

0.45 (0.20)

0.43 (0.24)

t(55) = 0.35, p = 0.7256

0.09

0.74

Post

0.75 (0.12)

0.65 (0.21)

t(55) = 2.32, p = 0.0238

0.6**

0.47

Adj.

0.75 (0.13)

0.65 (0.13)

F (1, 54) = 7.62, p = 0.0079

0.78**

0.57

NLG

0.56 (0.22)

0.36 (0.32)

t(55) = 2.73, p = 0.0086

0.74**

0.53

Pre

0.42 (0.25)

0.39 (0.26)

t(55) = 0.41, p = 0.6828

0.12

0.71

Post

0.64 (0.19)

0.50 (0.27)

t(55) = 2.34, p = 0.0228

0.61**

0.48

Adj.

0.63 (0.17)

0.51 (0.17)

F (1, 54) = 7.77, p = 0.0073

0.72**

0.47

NLG

0.32 (0.43)

0.22 (0.33)

t(54) = 0.97, p = 0.3380

0.26

0.54

Pre

0.46 (0.15)

0.41 (0.23)

t(55) = 0.89, p = 0.3782

0.26

0.57

Post

0.65 (0.14)

0.56 (0.20)

t(55) = 2.03, p = 0.0468

0.53**

0.49

Adj.

0.64 (0.11)

0.58 (0.11)

F (1, 54) = 4.22, p = 0.0448

0.56**

0.51

Continued on Next Page. . .
1
2

“**” means significant.
“*” means marginal significant.

149

d

1−β

Test

Table 7.3: Between-Group Comparison by KC-based
Pre- and Post-Test Scores
KC

KC27

KC28

7.2.2.3

d

1−β

Test

NormGain

InvNormGain

Stat

NLG

0.37 (0.22)

0.27 (0.27)

t(55) = 1.67, p = 0.1012

0.41

0.46

Pre

0.53 (0.21)

0.42 (0.24)

t(55) = 1.74, p = 0.0879

0.5*

0.55

Post

0.78 (0.21)

0.58 (0.28)

t(55) = 3.01, p = 0.0040

0.82**

0.54

Adj.

0.74 (0.18)

0.63 (0.18)

F (1, 54) = 5.88, p = 0.0187

0.62**

0.47

NLG

0.60 (0.34)

0.28 (0.37)

t(53) = 3.33, p = 0.0016

0.92**

0.55

Pre

0.37 (0.20)

0.36 (0.26)

t(55) = 0.13, p = 0.8997

0.04

0.9

Post

0.53 (0.22)

0.47 (0.24)

t(55) = 1.01, p = 0.3179

0.27

0.52

Adj.

0.53 (0.17)

0.47 (0.17)

F (1, 54) = 1.61, p = 0.2101

0.36

0.54

NLG

0.27 (0.37)

0.16 (0.29)

t(54) = 1.26, p = 0.2119

0.34

0.51

Summary of Learning Results showed that both groups of participants had

significant learning gains after training on NormGain-Cordillera and InvNormGain-Cordillera
respectively. More importantly, although no significant difference was found in time on task
and in the pre-test scores under both grading criteria, the NormGain group out-performed
the InvNormGain group on the post-test, adjusted post-test, and NLG scores regardless of
the grading criteria. Therefore overall, the results show that the micro-level tutorial decisions
on micro-steps made a significant difference in the students’ learning.
On a KC by KC basis, the difference between the two groups was not significant on all
of the eight primary KCs. Especially, on KC28 , no significant difference was found between
the two groups on post-test, adjusted post-test, and NLG scores. There are many potential
explanations for the lack of the difference on KC28 . For example, KC28 is the only KC for
which both ET and JS tutorial tactics were derived from the DichGain Corpus. One of the
hypothesis generated from this study was that Exploratory Corpus seemingly works more
effectively than the DichGain Corpus (this will be discussed in the next chapter).
150

To summarize, the overall test scores seemingly support the primary research hypothesis.
The NormGain condition indeed out-performed the InvNormGain condition. In order to
investigate why the NormGain tutorial tactics were more effective than the InvNormGain
one, it will be necessary to dig into the logs to have a detailed comparison of the differences
between the two sets of tutorial tactics. For example, the induced NormGain tutorial tactics
might simply elicit more answers from the students or execute more justification steps during
the tutoring. Therefore, the following section will investigate whether the NormGain and
InvNormGain tutorial tactics resulted in a different number of tutorial actions. The number
of overall decisions the tutor made, the number of ET decisions, the I-ratio, and the number
of JS decisions and the J-ratio between the two groups will all be compared. The goal is to
see whether the NormGain tutorial tactics resulted in different tutorial behaviors from the
InvNormGain policies when viewed from this shallow aspect.

7.2.3

Log Analysis

Table 7.4: Overall Characteristics of Tutorial Decisions in Exploratory Corpus

NormGain (29)

InvNormGain (28)

1 Tell

63.759 (19.528)

63.250 (4.656)

t(55) = 0.134, p = 0.894

2 Elicit

198.586 (17.463)

204.000 (7.679)

t(55) = −1.506, p = 0.138

3 ET Decisions

262.345 (6.149)

267.250 (6.775)

t(55) = −2.864, p = 0.006

4 Skip-Justify

9.345 (3.829)

11.000 (1.700)

t(55) = −2.096, p = 0.041

5 Justify

42.517 (3.786)

40.321 (1.442)

t(55) = 2.874, p = 0.006

6 JS Decisions

51.862 (0.833)

51.321 (1.156)

t(55) = 2.030, p = 0.047

7 Overall Decisions

280.103 (4.126)

285.464 (6.995)

t(55) = −3.539, p = 0.001

8 I-ratio

0.758 (0.073)

0.763 (0.018)

t(55) = −0.395, p = 0.694

9 J-ratio

0.820 (0.073)

0.786 (0.030)

t(55) = 2.273, p = 0.027

151

Stats

7.2.3.1

Overall Tutorial decision Steps Table 7.4 summarizes and compares the av-

erage number of various tutorial decisions, the I-ratio and J-ratio between the NormGain
and InvNormGain tutorial dialogues. Row 1 to row 7 shows the average numbers of various
tutorial decisions the students got during the tutoring. These include the average number
of tell decisions (row 1), elicit decisions (row 2), ET decisions (row 3), skip-justify decisions
(row 4), justify decisions (row 5), JS decisions (row 6), and overall decisions (row 7). Table 7.4, shows that except for the total number of tells and elicits, the two groups differed
in all other five numbers.
On average, the InvNormGain-Cordillera made more tutorial decisions during the tutoring than the NormGain-Cordillera. This is probably because the InvNormGain students
got more remediations in their dialogues. Row 3 shows that the InvNormGain students had
more ET decisions in their tutorial dialogue than the NormGain students. For JS decisions,
the NormGain students got more justification steps than the InvNormGain ones: on average
two more justification decisions (row 5).
Rows 8 and 9 compared the I-ratio and J-ratio between the two conditions. There were
no significant differences between the two groups on the I-ratio. However, on J-ratio, the
NormGain students were higher than the InvNormGain group. So by following the induced
tutorial tactics, the NormGain tutorial dialogues seemingly were no more interactive than
the InvNormGain ones but the NormGain students were more likely to get justification steps.

7.2.3.2

Comparing I-ratio Across Primary KCs Although no significant difference

was found between the two groups on the I-ratio overall, once the dialogue was broken into
a KC by KC basis there were significant differences between the two groups on each of the
eight primary KCs (see Table 7.5). In Table 7.5, row 2 shows that on KC14 the NormGain
group got all elicits while the InvNormGain group got all tells. Among the rest of seven
primary KCs, the NormGain condition was more likely to get elicits than the InvNormGain
condition on KC20 , KC21 , and KC22 ; and the InvNormGain condition was more likely to
get elicits than the NormGain condition on KC1 , KC24 , KC27 , and KC28 .
152

Table 7.5: Compare NormGain vs. InvNormGain on I-ratio Across Eight Primary KCs

7.2.3.3

NormGain(29)

InvNormGain (28)

Stats

1 KC1

0.500 (0.000)

0.696 (0.157)

t(55) = −6.72, p = 0.000

2 KC14

1.000 (0.000)

0.000 (0.000)

3 KC20

0.897 (0.024)

0.696 (0.030)

t(55) = 27.87, p = 0.000

4 KC21

0.923 (0.030)

0.863 (0.045)

t(55) = 5.95, p = 0.000

5 KC22

0.888 (0.099)

0.543 (0.089)

t(55) = 13.88, p = 0.000

6 KC24

0.866 (0.028)

0.920 (0.029)

t(55) = −7.21, p = 0.000

7 KC27

0.484 (0.137)

0.651 (0.112)

t(55) = −5.03, p = 0.000

8 KC28

0.000 (0.000)

0.525 (0.108)

t(55) = −26.08, p = 0.000

Comparing J-ratio Across Primary KCs Similarly, the J-ratio was broken

into a KC by KC basis. Only seven primary KCs (KC1 was not involved in JS decisions)
were involved in JS decisions (see Table 7.6). Surprisingly, on two KCs, KC22 (row 4) and
KC28 (row 7), both NormGain and InvNormGain tutorial tactics achieved the same results,
executing all justification steps. There are at least two potential explanations. One possible
explanation is that the JS decisions on these KCs may not matter to the students’ learning.
The other possible explanation is that the source training corpora used to induce these
two KC-specific policies might not be exploratory enough. On KC14 , however, following
the NormGain tutorial tactics resulted in skipping all justification steps, but following the
InvNormGain tutorial tactics resulted in executing all justification steps. For the remaining
four KCs, no significant difference was found between the two conditions on KC20 (row 2)
and KC24 (row 5). Only marginally significant difference was found between the two groups
on KC27 . On KC21 , however, the NormGain group was significantly more likely to get
justification steps than the InvNormGain group.

153

Table 7.6: Compare NormGain vs. InvNormGain on J-ratio across Eight Primary KCs

7.2.3.4

NormGain(29)

InvNormGain (28)

Stats

1 KC14

0.000 (0.000)

1.000 (0.000)

2 KC20

1.000 (0.000)

0.994 (0.022)

t(55) = 1.467, p = 0.148

3 KC21

0.815 (0.216)

0.573 (0.096)

t(55) = 5.445, p = 0.000

4 KC22

1.000 (0.000)

1.000 (0.000)

5 KC24

0.876 (0.024)

0.871 (0.005)

t(55) = 1.071, p = 0.289

6 KC27

0.046 (0.140)

0.000 (0.000)

t(55) = 1.736, p = 0.088

7 KC28

1.000 (0.000)

1.000 (0.000)

Summary of Log Analysis Following the NormGain tutorial tactics did not

generate more interactive tutorial tactics than following the InvNormGain ones. But once
broken into a KC by KC basis, the NormGain tutorial tactics resulted in different I-ratio
for each of the primary KCs. On the other hand, following the NormGain tutorial tactics
seemed more likely to execute a justification step but once broken it into KC by KC bases,
the NormGain and InvNormGain tutorial tactics’ J-ratio were only significantly different on
KC21 and KC14 (The NormGain tutorial tactics skipped all of them while the InvNormGain
executed all of them). To summarize, future work is needed to explore why the NormGain
tutorial policies resulted in better learning performance than the InvNormGain ones.

7.3

DISCUSSION

To summarize, the findings confirmed the primary hypotheses in this thesis. First, and
foremost, the pedagogical tutorial tactics applied at the interactive decision level affected
students’ learning. Secondly, the use of RL to derive tutorial tactics from existing data proved
to be feasible and successful. On the other hand, the results also suggested that content
154

exposure with the Cordillera system, irrespective of the tactics employed, was, indisputably,
an important factor in governing students’ learning even the InvNormGain students learned
significantly in Study 3. Despite this importance, however the results showed that the
pedagogical tutorial tactics also made a significant impact.
However, it is not clear as to what it was about the induced NormGain tutorial tactics
that caused the NormGain students to learn more effectively than the InvNormGain group.
By simply analyzing the log file in a relatively shallow way, it seems that it was not that the
NormGain tutorial tactics were simply more interactive or generated more justification steps
that caused the NormGain students to learn more than the InvNormGain students. Overall,
the preliminary results supported the conjecture that interactivity is not, necessarily, the
most important determiner of students learning. For example, no significant difference was
found between the two conditions in terms of the number of elicitation prompts and tells they
received and the I-ratio. However, NormGain students learned significantly more than the
InvNormGain students. Additionally, once broken into a KC by KC basis, the InvNormGain
students had significantly higher I-ratio than the NormGain group on KC1 , KC24 , KC27 ,
and KC28 , but the former did not learn more than the NormGain group.
For JS decisions, following the induced NormGain tutorial tactics indeed resulted in more
justification steps in students’ tutorial dialogues. However, once the tutorial decisions were
broken into a KC by KC basis, the two groups differed significantly only on KC21 and KC14 .
Therefore, future work is needed to investigate the induced tutorial tactics and find out what
actually caused these learning differences.
The NormGain and InvNormGain tutorial tactics in Study 3 were derived from the
Exploratory and DichGain Corpora in Studies 1 and 2. Therefore, it is possible to draw some
hypothesis from observations by running a post-hoc comparison among the four groups. A
cross-study analysis comparing the three studies will be presented in Chapter 8.

155

8.0

GENERAL DISCUSSION AND CONCLUSIONS

This chapter contains a general discussion of the results including a post-hoc comparison
across the study groups. I will then revisit the central research questions examining the data
relevant to them and draw conclusions.

8.1

8.1.1

POST-HOC COMPARISON

STUDY VARIATIONS

A total of 158 participants used four versions of Cordillera as part of the three studies:
The Exploratory Group contained 64 students who used Random-Cordillera (Study 1); the
Dichotic Gain (DichGain) Group was comprised of a total of 37 students who used DichGainCordillera (Study 2); The Normalized Gain (NormGain) group included 29 students who used
NormGain-Cordillera (Study 3); and the Inverse Normalized Gain (InvNormGain) group included 28 students who used InvNormGain-Cordillera (Study 3). All of the participants
followed the same procedure; used the same preparatory materials and problems; and interacted with Cordillera with the identical GUI. They all completed a background survey; read
a textbook covering the target domain knowledge; took a pre-test; solved the same seven
training problems in the same order on Cordillera; and finally took a post-test. Only four
salient differences existed across the three studies:
1. Although all of the participants were recruited in the same way, they were recruited in
different years. In Study 3 the students were randomly assigned into the NormGain and
InvNormGain groups (2009). On the other hand, in the first two studies participants
156

were not randomly assigned to the Exploratory (2007) and DichGain groups (2008).
2. Interaction decisions that were made by NormGain-Cordillera, InvNormGain-Cordillera,
DichGain-Cordillera, and Random-Cordillera were guided by different tutorial tactics.
Random-Cordillera made random decisions on tutorial decision steps. The other three
versions of Cordillera followed corresponding induced tutorial tactics to decide which
action to take.
3. Apart from a single question variation on Studies 2 and 3, all three studies used identical
exams containing a total of 33 test questions. The one variation occurred as the result
of the replacement of a single question, Q20, which had been used in Study 1. It was
judged to be too easy and was replaced with a more difficult question, Q20∗ that covered
the same KCs for Studies 2 and 3. The remaining 32 test items were identical across all
three studies.
4. A group of six human wizards (including the author) were involved in Studies 1 and 2;
but only one wizard (the author) was in Study 3.
Despite these differences, because the NormGain and InvNormGain groups trained in
Study 3 were guided using tutoring tactics derived from the Exploratory and DichGain corpora, a post-hoc comparison among the four groups will allow us to observe the characteristics
of the induced tutorial tactics from a wider point of view.
In Study 3, the exams were graded by a different grader, the author, than the grader in
Studies 1 and 2. An inter-grader agreement study, discussed in Appendix B, showed that
the grading rubrics were comparable to those used in the prior studies. Moreover, the high
level of correlation supports the conclusion that the grades assigned by the two graders were
equivalent and thus may be reliably compared. This chapter contains a post-hoc comparison
across the four student groups. This analysis will make use of the new grades exclusively.
In order to establish test equivalence, Q20 and Q20∗ were excluded from the scores used
here. As described in the previous chapter, the tests contained 33 test items which covered
168 KC occurrences. Removing Q20 reduced this total by 1 leaving 32 test items covering 166
KC occurrences. The subsections below compare learning on both the overall and cumulative
KC-based scores. For the overall scores, the maximum raw score is 32 points while for the
cumulative KC-based score it is 166. For comparison purposes both scores were normalized
157

to 1.
Based on the procedure of induced tutorial tactics, it was expected that N ormGain >
DichGain > Exploratory > InvN ormGain. However, in Chapter 5 , a post-hoc comparison showed no significant difference between the DichGain and Exploratory. These will be
compared again using the new grading criteria.
A one-way ANOVA showed that there were no significant differences among the four
groups on overall training time: (F (3, 147) = 1.531, p = .209). More specifically, the
average total training time across the seven training problems, was M = 278.73, SD =
67.38 for Exploratory group, M = 294.33, SD = 87.51 for DichGain group, M = 259.99,
SD = 59.22 for NormGain group, and M = 264.57, SD = 67.60 for InvNormGain group.
Additionally, no significant difference was found among the Exploratory, the NormGain, and
the InvNormGain groups on the MathSat scores: (F (2, 83) = .520, p = .596).

8.1.2

LEARNING PERFORMANCE

A one-way ANOVA was used to test for performance preference differences between the
pre- and post-tests. Participants across four groups made significant gains from pre-test to
post-test: F (1, 314) = 67.36, p = .000, R 2 = .68 under the overall grading criteria and
F (1, 314) = 41.82, p = .000, R 2 = 0.69 under the cumulative KC-based grading criteria.

Figure 8.1: Compare Four Groups Learning Performance under Overall Grading

158

Figure 8.1 compares the four groups on the pre-test, post-test, adjusted-post-test, and
NLG scores under the overall grading. A one-way ANOVA was used for learning performance differences among the four groups. No significant pre-test score differences were
found between the groups under the overall-grading rubric (F (3, 154) = 1.16, p = 0.32).
However, there were significant differences among the four groups on the remaining three
scores: F (3, 154) = 5.052, p = .002 on posttest-scores, F (3, 153) = 9.938, p = .000 for
adjusted post-test scores, and F (3, 154) = 8.33, p = 0.000 on the NLG scores.
Moreover, pairwise comparisons among the four groups1 showed that there was a significant difference between the NormGain and either of the three groups on the post-test scores,
adjusted post-test scores, and NLG scores. There were no significant differences among the
DichGain, Exploratory, InvNormGain on all four test scores (see Table 8.1). More specifically, Table 8.1 shows the pairwise comparisons among the four groups on pre-test, post-test,
adjusted post-test scores, and NLG scores. The first column lists the two groups in comparison and their corresponding mean and SD scores. The second column lists the statistical
result of the t-test comparison. The last two columns list the effect size and power of the
comparison. For effect size, Cohen’s d was still used. Results in Table 8.1 suggest that under
the overall grading rubric, on the pretest there were no significant differences among the
four groups but N ormGain > DichGain = Exploratory = InvN ormGain across post-test,
adjusted post-test, and NLG scores. Although this formulation holds that the latter three
groups were equally effective, in some tests the power was less than 0.80. So more subjects
would be needed to determine which pairs of groups were in fact equivalent.

1

A Bonferroni correction was not performed as the hypotheses being tested are independent and the
corpora were collected seperately.

159

Table 8.1: Compare Four Groups Under the Overall
Grading Criteria
Group Name µ(σ)

Stat

Cohen’s d

1−β

0.2

0.63

0.44

0.47

0.1

0.77

0.2

0.49

-0.1

0.69

-0.31

0.38

0.77

0.51

1.04

0.42

0.74

0.27

0.27

0.5

0.05

0.84

Pre-test
NormGain

0.48(0.18) t(55) = 0.63, p = 0.53

InvNormGain

0.44(0.22)

NormGain

0.48(0.18) t(64) = 1.75, p = 0.09

DichGain

0.40(0.19)

NormGain

0.48(0.18) t(91) = 0.31, p = 0.75

Exploratory

0.46(0.20)

InvNormGain

0.44(0.22) t(63) = 0.91, p = 0.37

DichGain

0.40(0.19)

InvNormGain

0.44(0.22) t(90) = −0.43, p = 0.67

Exploratory

0.46(0.20)

DichGain

0.40(0.19) t(99) = −1.65, p = 0.102

Exploratory

0.46(0.20)
Posttest

NormGain

0.73(0.11) t(55) = 2.85, p = 0.006

InvNormGain

0.61(0.18)

NormGain

0.73(0.11) t(64) = 4.12, p < 0.001

DichGain

0.57(0.19)

NormGain

0.73(0.11) t(91) = 3.28, p = 0.001

Exploratory

0.61(0.18)

InvNormGain

0.61(0.18) t(63) = 1.05, p = 0.30

DichGain

0.57(0.19)

InvNormGain

0.61(0.18) t(90) = 0.208, p = 0.836

Exploratory

0.61(0.18)
Continued on Next Page. . .

160

Table 8.1: Compare Four Groups Under the Overall
Grading Criteria
Group Name µ(σ)

Stat

DichGain

0.57(0.19) t(99) = −1.04, p = 0.301

Exploratory

0.61(0.18)

Cohen’s d

1−β

-0.22

0.48

1.14

0.52

1.18

0.42

1.21

0.2

0.15

0.61

0.24

0.47

0.09

0.68

1.06

0.52

1.09

0.43

1.06

0.23

0.05

0.85

Adjusted Posttest
NormGain

0.70(0.07) t(55) = 4.21, p < 0.0001

InvNormGain

0.62(0.09)

NormGain

0.70(0.07) t(64) = 4.70, p = 0.00001

DichGain

0.60(0.10)

NormGain

0.70(0.07) t(91) = 5.33, p = 0.00001

Exploratory

0.59(0.10)

InvNormGain

0.62(0.09) t(63) = 0.60, p = 0.551

DichGain

0.60(0.10)

InvNormGain

0.62(0.09) t(90) = 1.06, p = 0.293

Exploratory

0.59(0.10)

DichGain

0.60(0.10) t(99) = 0.45, p < 0.66

Exploratory

0.59(0.10)
NLG

NormGain

0.49(0.16) t(55) = 3.95, p < 0.001

InvNormGain

0.31(0.18)

NormGain

0.49(0.16) t(64) = 4.35, p = 0.000

DichGain

0.30(0.18)

NormGain

0.49(0.16) t(91) = 4.67, p = 0.0000

Exploratory

0.28(0.22)

InvNormGain

0.31(0.18) t(64) = 0.199, p = 0.84

DichGain

0.30(0.18)
Continued on Next Page. . .

161

Table 8.1: Compare Four Groups Under the Overall
Grading Criteria
Group Name µ(σ)

Stat

InvNormGain

0.31(0.18) t(90) = 0.733, p = 0.466

Exploratory

0.28(0.22)

DichGain

0.30(0.18) t(99) = −0.60, p < 0.552

Exploratory

0.28(0.22)

Cohen’s d

1−β

0.17

0.55

0.12

0.61

Under the Cumulative KC-based grading criteria, similar results were found. No significant pre-test score difference was found among the four groups under the Cumulative KCbased grading rubric (F (3, 154) = 0.38, p = 0.77). However, there were significant differences
among the four groups on the remaining three scores: F (3, 154) = 3.41, p = .02 on post-test
scores, F (3, 153) = 8.09, p = .000 for adjusted post-test scores, and F (3, 154) = 5.30, , p =
0.002 on the NLG scores (see Table 8.2). Similar to overall grading criteria, t-test comparisons showed that there was a significant difference between the NormGain group and either
of the three remaining groups on the post-test scores, adjusted post-test scores, and NLG
scores and there were no significant differences among the DichGain, Exploratory, InvNormGain on all four test scores (see in Table 8.2). Therefore, the cumulative KC-based rubric
results suggest that: N ormGain > DichGain = Exploratory = InvN ormGain across all
three different performance metrics: post-test, adjusted post-test, and NLG scores. Again,
note that Table 8.2 shows that although this formulation holds that the latter three conditions were equally effective, in some tests the power was less than 0.80. So more participants
would be needed to determine which pairs of groups were equally effective.

162

Table 8.2: Compare Four Groups Under the Cumulative
KC-based Grading Criteria
Group Name µ(σ)

Stat

Cohen’s d

1−β

0.16

0.58

0.25

0.49

0.05

0.8

0.05

0.87

-0.1

0.64

-0.16

0.52

0.64

0.53

0.82

0.46

0.63

0.35

0.2

0.55

0.05

0.82

Pretest
NormGain

0.42(0.15) t(55) = 0.66, p = 0.507

InvNormGain

0.39(0.23)

NormGain

0.42(0.15) t(64) = 1.05, p = 0.299

DichGain

0.38(0.17)

NormGain

0.42(0.15) t(91) = 0.29, p = 0.792

Exploratory

0.41(0.20)

InvNormGain

0.39(0.23) t(63) = 0.16, p = 0.871

DichGain

0.38(0.17)

InvNormGain

0.39(0.23) t(90) = −0.5, p = 0.618

Exploratory

0.41(0.20)

DichGain

0.38(0.17) t(99) = −0.81, p = 0.418

Exploratory

0.41(0.20)
Posttest

NormGain

0.65(0.15) t(55) = 2.32, p = 0.024

InvNormGain

0.54(0.20)

NormGain

0.65(0.15) t(64) = 3.28, p = 0.0017

DichGain

0.50(0.21)

NormGain

0.65(0.15) t(91) = 3.17, p = 0.0069

Exploratory

0.53(0.21)

InvNormGain

0.54(0.20) t(63) = 0.78, p = 0.439

DichGain

0.50(0.21)

InvNormGain

0.54(0.20) t(90) = 0.23, p = 0.820

Exploratory

0.53(0.21)
Continued on Next Page. . .

163

Table 8.2: Compare Four Groups Under the Cumulative
KC-based Grading Criteria
Group Name µ(σ)

Stat

DichGain

0.50(0.21) t(99) = −0.68, p = 0.498

Exploratory

0.53(0.21)

Cohen’s d

1−β

-0.14

0.57

0.89

0.59

1.18

0.38

1.12

0.3

0.28

0.45

0.28

0.43

0

0.95

0.87

0.54

0.95

0.18

0.84

0.33

0.14

0.63

Adjusted Posttest
NormGain

0.63(0.07) t(55) = 3.16, p = 0.003

InvNormGain

0.55(0.11)

NormGain

0.63(0.07) t(64) = 3.68, p = 0.0000

DichGain

0.52(0.11)

NormGain

0.63(0.07) t(91) = 4.77, p = 0.0000

Exploratory

0.52(0.11)

InvNormGain

0.55(0.11) t(63) = 1.20, p = 0.236

DichGain

0.52(0.11)

InvNormGain

0.55(0.11) t(90) = 1.24, p = 0.217

Exploratory

0.52(0.11)

DichGain

0.52(0.11) t(99) = −0.07, p = 0.945

Exploratory

0.52(0.11)
NLG

NormGain

0.42(0.19) t(55) = 3.15, p = 0.0026

InvNormGain

0.25(0.21)

NormGain

0.42(0.19) t(64) = 4.626, p = 0.000

DichGain

0.22(0.23)

NormGain

0.42(0.19) t(91) = 3.61, p = 0.0005

Exploratory

0.22(0.26)

InvNormGain

0.25(0.21) t(63) = 0.546, p = 0.587

DichGain

0.22(0.23)
Continued on Next Page. . .

164

Table 8.2: Compare Four Groups Under the Cumulative
KC-based Grading Criteria
Group Name µ(σ)

8.1.3

Stat

InvNormGain

0.25(0.21) t(91) = 0.506, p = 0.61

Exploratory

0.22(0.26)

DichGain

0.22(0.23) t(55) = −0.045, p = 0.964

Exploratory

0.22(0.26)

Cohen’s d

1−β

0.12

0.65

0

0.96

LEARNING PERFORMANCE ACROSS THE FOUR GROUPS

To summarize, a post-hoc comparison of learning performance across three studies shows that
the NormGain group significantly outperformed all other three groups while no difference
was found between the remaining three groups. These results were consistent both for the
adjusted post-test scores and the normalized learning gains. These results support the prior
analysis of Study 3 which showed that the NormGain tutorial tactics significantly improved
students’ learning compared with the InvNormGain ones.
However, the lack of a significant difference between the InvNormGain, DichGain, and
Exploratory groups seemingly contradicts the initial predictions. The InvNormGain strategies were specifically induced to enhance those decisions that contribute less or even none to
the students’ learning. Therefore, a lower performance on the students’ part there than in
at least the DichGain group, which sought to enhance the tutorial decisions that contribute
to the students’ learning, was expected. One possible explanation for the lack of difference
is that the tutorial tactics employed by the DichGain- and Random-Cordillera systems were
ineffective and thus presented a minimum bar. By ’ineffective’ it does not mean that they
prevented the students from learning but rather that they were not able to make a positive
impact on their learning above and beyond the baseline provided by Cordillera itself. Here
the basic practices and problems, domain exposure, and interactivity of Cordillera set a minimum bar of students’ learning that the tactics, however poor, cannot prevent. This is only a
165

post-hoc explanation not a tested hypothesis, however it merits further study. On the other
hand, note that under both grading criteria in some tests, the power of the comparisons
among the three groups was less than 0.80. Therefore, more participants would be needed
to determine which pairs of the three groups were truly equivalent in further studies.

8.1.4

LOG ANALYSIS

Having compared the individual groups’ learning performance, this subsection will compare
the log file variations across the four groups. Two types of tutorial actions are of principal
interest in this dissertation: elicit/tell (ET) and justify/skip-justify (JS). Therefore, the
following section will focus on two aspects of the ET and JS actions. The first aspect of
interest is the I-ratio of the tutorial dialogue. That is, how often were the students given
elicitation prompts in the course of the dialogue? The second is the J-ratio. This is the
number of times the dialogue manager did not skip a justification step. Both values range
from 0 (no elicits or justifies) to 1 (all elicitation or justification).

8.1.4.1

I-Ratio Table 8.3 summarizes t-test comparisons on the I-ratio among the four

tutorial corpora. In Table 8.3, the first two columns list the two groups in comparison and
their corresponding mean and SD scores. The last column lists the statistical results of the
t-test comparisons. From the Table 8.3, the I-ratios for the four student groups were: 0.76
(NormGain), 0.76 (InvNormGain), 0.44 (DichGain), and 0.50 (Exploratory) respectively.
Except for no significant difference between the NormGain and InvNormGain on the Iratio, both groups were significantly more interactive than either the DichGain group or
Exploratory group. Altogether, the result is N ormGain = InvN ormGain > Exploratory >
DichGain on the I-ratio.
Although high interactivity is a key characteristic of one-on-one human tutoring, the
more successful tutorial tactics were not necessarily more interactive than the less successful
tactics. Comparisons between the NormGain and InvNormGain groups suggest that it is
not the absolute level of interactivity that determines the students’ success. The NormGain
group was more successful than the others despite there being no significant difference in
166

interactivity ratios between it and the InvNormGain group. Conversely, the InvNormGain
group was no more successful than the Exploratory and DichGain groups despite being more
interactive than either.

8.1.4.2

Justify Ratio Table 8.4 summarizes t-test comparisons on J-ratio among the

four tutorial corpora. In Table 8.4, the first two columns list the two groups in comparison
and their corresponding mean and SD scores. The last column lists the statitical results of
the t-test comparisons. Table 8.4, shows that the mean of J-ratios for the four student groups
were: 0.82 (NormGain), 0.79 (InvNormGain), 0.43 (DichGain), and 0.53 (Exploratory). The
difference was statistically significant: F (3, 154) = 322.88, p = 0.000. Table 8.4 presents
the pair wise t-test comparisons. It shows that on J-ratio, the result is: N ormGain >
InvN ormGain > Exploratory > DichGain.
Table 8.5 summarizes and compares the average number of justification decisions students
experienced during the tutoring among the four groups. The average number of steps for each
student was: 42.52 for the NormGain group, 40.32 for the InvNormGain group, 24.89 for the
DichGain group and 30.17 for the Exploratory group. The pariwise comparisons among the
four groups are listed in Table 8.5. It shows that the number of justification steps students
received were in the same order as the justification ratio: N ormGain > InvN ormGain >

Table 8.3: Pairwise Comparison Among Four Groups On I-ratio

Group 1

Group 2

Group 1 vs. Group 2

NormGain

0.76 (0.07) InvNormGain

0.76 (0.02) t(55) = .395, p = .694

NormGain

0.76 (0.07) Exploratory

0.50 (0.03) t(91) = 24.72, p = 0.000

NormGain

0.76 (0.07) DichGain

0.44 (0.04) t(64) = 22.08, p = 0.000

InvNormGain

0.76 (0.02) Exploratory

0.50 (0.03) t(90) = 43.998, p = .000

InvNormGain

0.76 (0.02) DichGain

0.44 (0.04) t(63) = 36.34, p = .000

Exploratory

0.50 (0.03) DichGain

0.44 (0.04) t(99) = 7.967, p = .000

167

Table 8.4: Pairwise Comparison Among Four Groups On J-ratio

Group 1

Group 2

Group 1 vs. Group 2

NormGain

0.82 (0.07) InvNormGain

0.79 (0.03) t(55) = 2.27, p = .027

NormGain

0.82 (0.07) Exploratory

0.53 (0.06) t(91) = 18.95, p = 0.000

NormGain

0.82 (0.07) DichGain

0.43 (0.07) t(64) = 22.85, p = .000

InvNormGain

0.79 (0.03) Exploratory

0.53 (0.06) t(90) = 43.998, p = .000

InvNormGain

0.79 (0.03) DichGain

0.43 (0.07) t(63) = 26.65, p = .000

Exploratory

0.53 (0.06) DichGain

0.43 (0.07) t(99) = 7.894, p = .000

Exploratory > DichGain.
Table 8.5: Pairwise Comparison Among Four Groups On Number of Justification Steps

Group 1
NormGain

InvNormGain

Group 2

42.52 (3.79) InvNormGain

40.32 (1.44) t(55) = 2.87, p = 0.006

Exploratory

30.17 (3.83) t(91) = 19.33, p = 0.000

DichGain

24.89 (3.59) t(64) = 22.85, p = .000

40.32 (1.44) Exploratory
DichGain

Exploratory

Group 1 vs. Group 2

30.17 (3.83) t(90) = 13.57, p = .000
24.89 (3.59) t(63) = 21.45, p = .000

30.17 (3.83) DichGain

24.89 (3.59) t(99) = 6.83, p = .000

To summarize, applying RL to induce tutorial tactics designed to enhance students’
learning resulted in a set of tutorial tactics that involved substantially more justifications
than tutorial tactics designed with the goal of enhancing those decisions that contribute
less or even none to the students’ learning. However, although the NormGain group had
a higher ratio of justification prompts than the InvNormGain, Exploratory, or DichGain
groups it is not the case that the absolute justification ratio guarantees learning. As with
the interactivity ratio, the InvNormGain group received a higher justification ratio than the
168

Exploratory or DichGain groups despite having been induced to enhance those decisions that
contribute less or even none to the students learning, and despite the absence of a significant
difference in adjusted post-test scores or NLG between the groups. This is supported by the
absolute number of justification steps as the InvNormGain group received more than ten
more justification prompts on average than the Exploratory or DichGain groups while the
NormGain group had only two more justification steps than the InvNormGain group. Given
that the tutorial decisions in this dissertation were very fine-grained size level, an argument
can be made something other than more justification steps causes more learning.

8.2

8.2.1

REVISITING THE TWO RESEARCH QUESTIONS

Question 1: Micro-level Pedagogical Tutorial Decisions Affect Students’
Learning.

A comparison between the NormGain and InvNormGain groups from Study 3, discussed
in Chapter 7, shows that tutorial tactics covering micro-level interaction decisions do affect
students’ learning. More specifically, the results support the hypothesis that interactive
tutorial decisions such as the elicit/tell and justify/skip-justify decisions affect students’
learning. In Study 3, the students were randomly assigned to balanced conditions and
received identical training materials and procedures apart from the tutoring tactics employed.
After spending the same amount of time on training, the NormGain group outperformed
the InvNormGain group in terms of posttest scores, the adjusted post-test scores and the
normalized learning gain regardless of the grading criteria.
In order to investigate why the NormGain tutorial tactics were more effective, some preliminary analyses of the students’ log files was performed. It showed that the induced tutorial
tactics did not produce more or less interactive tutorial dialogues for the NormGain group
relative to the InvNormGain group. However the interactivity ratio (I-ratio) varied across
the eight primary KCs. On some KCs, the NormGain group was more likely to be elicited
for information rather than told it; while on other KCs, the NormGain group received more
169

didactic instruction. A wider comparison across the NormGain, InvNormGain, DichGain,
and Exploratory groups suggests that it might not be increased interactivity that caused the
NormGain students to learn more than the remaining three groups. Similarly, for justification decisions, although the NormGain group was given significantly more justification steps
than the InvNormGain group overall, once the totals are broken down by KC, the two groups
only differed on one KC, KC21 . Additionally, a wider comparison among the NormGain,
InvNormGain, DichGain, and Exploratory groups suggests that increased learning might not
be due to receiving a higher number of justification steps. The InvNormGain students had
significantly more justification steps than the DichGain, and Exploratory groups. However,
the former did not learn more than the latter two groups. Therefore, further analysis is
needed to understand what caused the NormGain tutorial tactics to be more effective.
The analysis in Chapters: 4; 5; and 7 show that all four groups learned significantly
by training on Cordillera. This result indicates that the content exposure and practice
opportunities can cause students to learn even from tutors with poor pedagogical tutorial
tactics. However, it also indicates that, with effective tutorial tactics, students can learn
more and more effectively than without.

8.2.2

Question 2: Reinforcement Learning is a Feasible Method to Induce Tutorial Tactics.

The results so far suggest that NormGain-Cordillera outperformed InvNormGain-Cordillera
system and further that it seems to be superior to the DichGain-Cordillera and RandomCordillera as well. This success supports the hypothesis that RL-induced rules are effective
and that the approach taken in Study 3 was a feasible one. However, inducing effective
tutorial tactics is not trivial. In Study 2, the DichGain tutorial tactics did not seem to be
more effective than the random decisions in Random-Cordillera. A number of factors were
changed between Study 2 and Study 3. These included the choice of training corpora, the
selections of knowledge components, the reward function, the feature choices, the maximum
number of the features in a policy, the discretization procedure, the feature selection methods,
and how the dialogue manager should respond when there were conflicting policies. All of
170

these factors might change the effectiveness of the resulting tactics. However, it is still
not clear which factor or factors caused a change in effectiveness. Despite that, the results
demonstrate the feasibility of RL to induce tutorial tactics. Or more accurately, the results
show that: applying RL with a suitable training corpus; defining reward functions; and using
the proper state representations on properly selected knowledge components; will produce
effective tutorial tactics with a reasonable policy conflict heuristic.
Moreover, the RL-induced tutorial tactics induced in this dissertation seemingly to be
highly adaptive both to the problem solving context and to other features. This fits in with
the conjecture proposed by both learning and cognitive scientists that pedagogical skills and
tutorial interactions should be adaptive to the tutorial context and students’ needs. However
more work remains to be done in this area to investigate whether it was the adaptiveness
that caused the NormGain students to learn more effectively. In inducing the tutorial tactics
the Expected Cumulative Reward (ECR) was used as the estimate of success. However
both the InvNormGain and DichGain systems contained tactics whose ECR estimates far
exceeded their actual performance with the students. More studies are necessary to identify
good performance metrics that will allow researchers to evaluate candidate tutorial tactics
without running expensive empirical studies.

171

9.0

CONTRIBUTIONS AND FUTURE WORK

In this dissertation Reinforcement Learning was applied to induce several sets of pedagogical
tutorial tactics from existing tutoring corpora and then integrated them into a system for
evaluation with human subjects. This study was mainly designed to: 1) examine the pedagogical importance of low-level interactive decisions in tutoring; and 2) test the viability
of using reinforcement learning to induce pedagogical tutorial tactics. As such this is an
interdisciplinary study that contributes to several fields.
In the field of cognitive science, this dissertation demonstrates that pedagogical skills
governing low-level interactive tutorial decisions can impact students’ learning. In the field
of the learning science, it informs the ongoing discussion of interactive vs. didactic tutoring
by suggesting that a tutor’s success is not governed by how often they give interactive
prompts or ask the students questions but how well. This dissertation also demonstrates that
RL may be fruitfully applied to derive adaptive pedagogical tutorial tactics from studentcomputer interactivity data, thus informing the general field of AI and Education. Further
this work demonstrates that existing a-priori theories about the importance of given features
to tutoring may be assessed by means of induction and feature selection which seek to
distinguish profitable feature choices from unprofitable ones thus informing the nascent field
of Educational Data Mining.

9.1

CONTRIBUTION TO COGNITIVE & LEARNING SCIENCE

Examinations of pedagogical skills have long been a focus of attention for the instructional and learning sciences. For example, one preferred explanation for the effectiveness
172

of human one-on-one tutoring is that human tutors possess effective pedagogical skills that
are responsible for the students’ learning gains [Chi et al., 2001, Collins and Stevens, 1982,
McArthur et al., 1982, Merrill et al., 1992]. Pedagogical skills generally involve the tutor’s
skillful execution of tutoring tactics such as eliciting student knowledge with a scaffolding
question. While it is generally assumed that the tutor’s interactive decisions are responsible
for tutoring effectiveness [Collins and Stevens, 1982], little evidence has been presented to
date demonstrating that either the human tutor has effective pedagogical skills or that pedagogical skills cause students to learn. In order to execute the pedagogical skills effectively,
it is assumed that tutors should adapt their behaviors to the students’ needs based upon
their current knowledge level, general aptitude, emotional state and other salient features.
Previous research, however has cast doubt on the tutor’s ability to monitor the student’s
state accurately [Chi et al., 2004] and on whether they really adapt their tutorial decisions
based on the present context. Chi, Siler, and Jeong for example, found that human tutors
do not seem to maintain an accurate model of student’s knowledge level during the tutoring
process. Similarly, [Putnam, 1987] found that experienced tutors did not attempt to form
detailed models of the students’ knowledge before attempting remedial instruction. Rather,
each teacher appeared to move through a general curricular script irrespective of the student’s state. In recent years, some cognitive scientists have begun to doubt the effect of
pedagogical skills on student’s learning [Chi et al., 2001, Chi et al., 2004, VanLehn, 2006].
This dissertation investigated on pedagogical skills at a micro-step level. i.e. pedagogical
tutorial tactics. These tactics do not govern the domain solution path selected for presentation or the problems presented. They only govern low-level tutorial interactions, e.g. whether
the student is told what principle to apply or if the system elicits it from them with a prompt,
and whether a student, once he/she has made a step, is asked to justify his/her answer or not.
If fine-grained pedagogical skills of this type turn out to be effective, then more complex or
content-oriented tactics, such as problem or sub-problem selection may be similarly effective.
Both the Normalized Gain (NormGain) and the Inverse Normalized Gain (InvNormGain)
groups in Study 3 solved the same training problems in the same training order, following
the same solution path in the process by using the same tutorial scripts. The only difference between the groups was the tutorial tactics employed. Additionally, the Exploratory
173

and DichGain groups, though run at different times from the NormGain and InvNormGain
groups, followed the same tutoring cycle save for the pedagogical tutorial tactics employed.
And, like the NormGain and InvNormGain groups, the Exploratory and DichGain groups
showed no salient difference in incoming competence. As reported in Chapters 6 and 8, the
NormGain group outperformed the other three groups under either grading rubrics. Thus,
a first, and primary, contribution of this work has been to show that pedagogical tutorial
tactics can impact student’s learning. However, inducing such tutorial tactics is not trivial.
As demonstrated in Studies 2 and 3, the induced DichGain tutorial tactics did not notably
improve the effectiveness of Cordillera compared with the Random-Cordillera. Given the
potentially important role tutorial tactics might play for the effectiveness of the tutoring,
more research needs to be done to investigate this issue.
On the other hand, high interactivity is a key characteristic of one-on-one tutoring.
A classroom lecture can be viewed as a monologue consisting of a long sequence of tutor
instructions or “tell” acts. Individual tutoring, by contrast, features a great deal of give
and take and can be viewed as a mixture of tutor questions or elicitation acts, student
responses, and tutor instructions. One common assumption, often referred as the monotonic
interaction hypothesis [VanLehn et al., 2007a], is that greater interactivity causes greater
learning. Previous studies, however, have suggested that when the instructional content is
strictly equivalent between conditions, highly interactive tutoring methods (such as human
tutoring) are no more effective than moderately interactive methods (such as step-based
NL tutoring systems) [VanLehn et al., 2007a, VanLehn, 2009]. Results such as found in this
thesis suggest that more interactivity does not necessarily guarantee more learning.
Previous researchers in the cognitive and learning sciences have identified a number
dialogue, domain, and student features that may be relevant when making pedagogical decisions. For the present studies, six categories of features were defined based upon prior
research. These categories are discussed in Chapter 6. The final sets of tutorial tactics (used
in the NormGain- and InvNormGain-Cordillera) employed more of the problem-solving context features than any other categories. The features in this category occurred 58 out of 117
times. ConceptDifficulty, which describes the difficulty level of the present tutorial decision,
was employed by 12 of the 34 tutoring policies used in Study 3 and was the most common
174

single feature.

9.2

CONTRIBUTIONS TO AI&ED, ITS & EDM

The development of Intelligent Tutoring Systems (ITSs) is typically viewed as a standard
system design problem. Every aspect of the system, including any pedagogical tutorial tactics
employed, are specified a-priori by system developers under the guidance of or in response to
domain experts. The tutoring behavior is similarly viewed as a sequential decision process
where, at each discrete step, the tutor is responsible for selecting the next action to take. As
with other features of the system these decisions are guided by a-priori policies. Tutoring
systems face a number of limitations to their deployment, notably, the cost and pace of
development make it difficult to quickly deploy systems or to adapt them to changing needs.
Recently, researchers have turned to the development of authoring tools to address these
problems [Aleven et al., 2006, Ainsworth and Fleming, 2005, Murray et al., 2003]. These
tools provide content support and tool libraries that permit domain experts to design and
deploy systems for their needs without extensive software development. Most authoring
tools, however, focus on supporting the adaptation of new content and, as such, use static
predefined pedagogical strategies that allow only small amounts of tailoring by the author,
such as specifying how many hints the system will provide. Doing so means assuming that
all students in all domains covered by the tutor learn best using the same strategy and that
domain experts know a-priori how best to teach. However, both cognitive and learning scientists have suggested that tutors should adapt to student’s needs and, as discussed above,
suggested that domain experts do not always track a student’s knowledge level accurately
[Chi et al., 2004] or make a habit of adapting their tutorial decisions to the present context
[Putnam, 1987].
This dissertation described and evaluated a general methodology suitable for ITS designers to derive effective pedagogical tutorial tactics from pre-existing interactivity data
rather than, as is presently common, implementing a-priori pedagogical theories drawn from
experts. This approach does not require ITS designer to have an a-priori belief about how
175

he/she thinks the tutor should teach. Instead, this is a data-driven architecture. Having
an ITS that makes effective decisions more automatically is important for its acceptance in
a broader community. Systems that may be adapted to an instructor’s preferred pedagogy
may be more easily acceptable to pedagogical experts. And, by applying RL to induce tutorial tactics, systems may be developed that are neither equipped with nor restricted to
potentially faulty a-priori tutorial tactics.
The results herein showed that it is possible and practical to develop a strategy-neutral
tutoring system (Random-Cordillera) and to use it to explore the relevant features of a
tutoring space. The results also show that it is possible to use existing pedagogical knowledge
(e.g. lists of relevant features) to structure the RL process and to use the results of that
process to assess the domain theories.

9.3

FUTURE WORK

Given the diverse contributions of this dissertation, the work here is a starting point for
additional analyses focusing on what makes this approach successful and whether it can be
transferred to other domains, other machine learning methods, and other applications. The
first branch of research will focus on the tutorial corpora. In the work for this dissertation
policies were induced from three training corpora: Exploratory, DichGain and Combined.
The final tactics used were drawn from all three corpora as discussed in Chapter 6. Among
them, 11 out of 17 NormGain tutorial tactics were from the Exploratory Corpus while 11 out
of 17 InvNormGain ones were from the DichGain Corpus. Given the effectiveness of NormGain relative to the Exploratory and DichGain and the null effect among the InvNormGain,
Exploratory and DichGain groups, the initial analysis suggests that the Exploratory Corpus
was a more effective source of tutorial tactics than the DichGain or Combined Corpora.
However, it should be noted that the DichGain tutorial tactics were also derived from the
Exploratory Corpus. Therefore, it can be argued that whether a training corpus is effective
or not also depends on many other factors such as feature selection approach, the feature
used to represent the state and so on. However this selection was based upon the estimated
176

success of each set of tutorial tactics and not on a full study comparison. This has important implications for the potential success of these policy induction methods with existing
datasets. In future work more direct comparisons between the tutorial tactics derived from
random corpora and those from preexisting corpora are planned.
A second line of proposed research will be to focus on the divergence between KC-general
and KC-specific tutorial tactics. Here it was assumed that KC-specific tutorial tactics would
be more effective to improve learning than KC-general ones. However, annotating everything
with corresponding KCs is very time-consuming. A question exists as to whether a KCgeneral policy would be just as effective. For example, in Study 3, the NormGain-Cordillera
and InvNormGain-Cordillera followed KC-specific tutorial tactics on eight main KCs and
KC-general ones for the remainder of the 23 KCs. Later results on KC-based learning
gains indeed showed that the NormGain students also learned significantly more than the
InvNormGain ones on some of these 23 KCs. So it suggests that KC-general policies may
be as or more effective than KC-specific tactics. Further comparisons are needed to analyze
this.
In Studies 2 and 3 the state representation had been composed from features that had
been suggested in the cognitive and learning science literatures and which could be both
automatically computed and unambiguously evaluated. The use of manual features is impractical in the current study due to the emphasis on online training. However, there are a
number of potentially relevant features that do not meet these criteria such as motivation
[Hume et al., 1995, Noe, 1986]. Future work may investigate manual features or features
that do not allow for unambiguous classification so much as estimation. For feature selection
procedures, although there was some success here, one of the best final induced policies was
from Random feature selection. So further explorations on feature selections are needed.
In addition to examining alternate feature selection policies the feature selection criteria
should be studied. In this study alternate candidate features were selected based upon
their Expected Cumulative Reward (ECR). In theory ECR values, as estimated from the
underlying models, are a reliable indicator of future performance. However this contention
has not been empirically verified in the present domain. The tactics employed in both the
DichGain- and the InvNormGain-Cordillera systems, for example, have comparatively high
177

ECR values. However they did not seem to either outperform or fall below the Exploratory
group’s performance. Future work may explore the quality of ECR and investigate ways
to evaluate new instructional policies. While the gold standard is to collect a new dataset
by executing the new policy, it may be possible to demonstrate that cheaper evaluation
methods provide sufficient accuracy to guide development. For example, one alternate policy
estimation method is to test them on simulated students. Alternatively, cross-validation on
ECR, e.g. train on n-1 students and test predicted vs. actual reward for the other one, can
be run.
Additionally, the induction model used in this research was based exclusively on Markov
Decision Processes (MDPs). While this framework seems effective, there are alternative
methods such as Partially Observable Markov Decision Process (POMDP) [Hauskrecht, 1997,
Aström, 1965], which may be more suitable. POMDPs allow for realistic modeling of the
student’s knowledge levels, the student’s intentions, and other hidden state components by
incorporating them into the state space. POMDPs explicitly represent two sources of uncertainty: non-determinism in the control process and partial observability of the students’
knowledge levels. In the former case, outcomes of the tutorial actions or student’s knowledge level are not deterministic. In the latter, the underlying student’s knowledge levels are
observed indirectly via incomplete or imperfect observations.
Another future study would be to investigate why the NormGain students learned more
effectively than the other three groups. One approach would be to investigate whether, by
using the number of tutors’ decisions on each KC, combined with the KC’s I-ratio and Jratio, its cognitive difficulty, and student’s pre-test scores, one is able to predict the student’s
learning gains. An additional study would be to investigate when and how different RL
techniques can provide increased leverage when compared to other learning techniques or be
combined with other techniques (such as learning decomposition [Beck and Mostow, 2008]).
Finally, comparing these induced policies with existing learning and cognitive theories
or with real human tutors in a more formal way such as evaluating and comparing them
in-vivo studies could be a fruitful area of study. Previous research in non-tutoring dialogue
systems has shown that the induced policy can sometimes beat human generated policies
[Lemon et al., 2006]. Given the computing power of the computers, it is expected that the
178

induced tutorial tactics might be more effective than human tutors’.

179

APPENDIX A

KNOWLEDGE COMPONENTS

180

In this appendix I list all 32 domain KC’s grouped by topic.

Table A1: Individual (not net) forces

ID

Knowledge Component (KC)

KC1 If an object is near a planet, and then the planet exerts a gravitational
force on the object. The force is straight down.
KC2 The magnitude of the gravitational force is m ∗ g, where m is the mass of
the object and g is the gravitational acceleration of the planet.
KC3 If a spring pushes on an object, then it exerts a force on the object. The
force’s direction is from the spring toward the object.
KC4 If a surface pushes on an object, it exerts a normal force on the object.
The force is perpendicular to the surface and away from it.
KC5 If an object contacts a surface while moving along it, and the surface is
not known to be frictionless, then the surface exerts a dynamic friction
force on the object. The force is parallel to the surface and in the opposite
direction of the object’s motion relative to the surface.
KC6 If object A pushes or pulls on object B, and the force is not one of the
types listed above, then there is an applied force on B due to A. If it is a
push, then it is directed from A toward B. If it is a pull, then it is directed
from B toward A.

181

Table A2: Choosing a system for COE

ID

Knowledge Component (KC)

KC7 When an object A exerts a force on an object in the system, and the
force is not a spring force nor a gravitational force, then object A must be
outside the system.
KC8

When an object in the system is moving, and a spring or gravitational
force acts on it and is perpendicular to its motion, then put the object
exerting the force outside the system.

KC9

When an object in the system is moving, and a spring or gravitational
force acting on it is not perpendicular to the moving object, then put the
object exerting the force inside the system.

182

Table A3: Individual (not net) work

ID

Knowledge Component (KC)

KC10 If an object moves while a force interacts with it, then the force does work
on the object, although the work may be zero.
KC11 If an object does not move while a force interacts with it, then the force
does no work on it. (Not in textbook, but easily inferred.)
KC12 If a force does work on an moving object, and the force on an object
is always perpendicular to its motion, then the work is zero. (This was
generalized from the textbook version for the sake of the roller coaster
training problem, 81).
KC13 If a constant force does work on an object which is moving in a straight
line, and the object moves anti-parallel to the force, then the work is
−F ∗ d, where F is the magnitude of the force and d is the displacement.
KC14 If a constant force does work on an object which is moving in a straight
line, and the object moves parallel to the force, then the work is F ∗ d,
where F is the magnitude of the force and d is the displacement.
KC15 The unit for work is the Joule (J) – used only when answer is work in
joules.

183

Table A4: Net work
ID

Knowledge Component (KC)

KC16 Suppose two objects interact via a force. If one object is inside the system
and one is outside, then the force is an external force. If they are both
inside the system, then the force is an internal force.
KC17 If there is only one external force during a time interval, then the work
done by it is also the net work done on the system during the time interval.
KC18 If there are multiple external forces acting on objects in a system, then
the net work on the system is the sum of the work done by these forces
on objects in the system.

184

Table A5: Individual (not net) mechanical energies

ID

Knowledge Component (KC)

KC19 If an object is not moving, then it has no kinetic energy.
KC20 If an object is moving, then its kinetic energy at a time is 0.5∗m∗v 2 , where
m is the object’s mass and v is the magnitude of the object’s instantaneous
velocity.
KC21 If an object and a planet are in a system (or equivalently, the gravitational
force of the earth on the object is an internal force), then their gravitational potential energy is m ∗ g ∗ h, where m is the mass of the object, g
is the gravitational acceleration of the planet, and h is the object’s height
above a zero point. The zero point is arbitrary, but is often chosen to be
the planet’s surface.
KC22 If an object and a spring are in a system (or equivalently, the force the
spring exerts on the object is an internal force), then their spring potential
energy is 0.5 ∗ k ∗ d2 , where k is the spring’s spring constant and d is
the displacement of the object relative to the equilibrium position of the
spring.
KC23 The unit for energy is the Joule (J) – used only when answer is in Joules.

185

Table A6: COE, TME and isolated/non-isolated

ID

Knowledge Component (KC)

KC24 The total mechanical energy of a system is the sum of the kinetic energies of each object in the system plus the sum of the potential energies
(gravitational and spring) of each pair of objects in the system that have
potential energies. Typically there is only one moving object and at most
one pair for each type of potential energy, so T M E = KE + GP E + SP E.
KC25 Given a system composed of one or more objects, it is isolated if there are
no physical interactions between an object inside the system and an object
outside the system where forces are one kind of physical interaction.
KC26 Otherwise, the system is non-isolated.
KC27 If a system isolated during a time interval, then its total mechanical energy
at the beginning of the time interval equals its total mechanical energy at
the end.
KC28 If a system is not isolated during a time interval, and forces are the only
physical interactions between objects inside the system and objects outside
the system, then its total mechanical energy at the beginning of the time
interval equals its total mechanical energy at the end plus the net work
done on the system during the time interval.

186

Table A7: Kinematics
ID

Knowledge Component (KC)

KC29 When an object is in projectile motion (only gravity acts on it), at the
apex of its trajectory, the vertical component of its velocity is zero and
the magnitude of its velocity is minimal. Also, it slows down as it climbs
and speeds up as it falls.
KC30 When an object slows to a stop and reverses direction, then its velocity is
momentarily zero.
KC31 The unit for velocity is

m
.
s

Used only when the answer is a numerical

velocity.
KC32 The value of g is 9.8

m
.
s2

187

APPENDIX B

GRADING

As described earlier, in studies 1 and 2, all tests were graded by a single experienced
grader whom I will refer to as the original grader. I will likewise refer to the grades she
assigned as the original grades. The original grades were used to calculate the KC-based
NLGs and cumulative KC-based NLGs for the Exploratory and Dichotic Gain (DichGain)
Corpora later used to derive the tutorial tactics for studies 2 and 3. The results I reported in
chapters 4 and 5 were all based on the original scores. For study 3, however, it was necessary
to engage a new grader, in this case me. I have expertise in the domain and have graded
exams in this domain for a previous experiment. Since the original scores were used as the
basis for our reward function it is important for us to use an equivalent rubric when grading
the final exams. In order to ensure that these rubrics can be adequately aligned, I re-graded
all of the original exams from studies 1 and 2 so as to conduct a full grader agreement study.

B.1

GRADING PROCEDURE

Across three studies in this dissertation, we have total 158 participants and the pre- and
post-tests were identical. In order to aid grading, I first developed a simple interface to
input a correct answer for each test question. As in the original grading process, each test
answer was assigned two grades: overall and KC-based grading. The overall grade was a
188

score in the range [0, 1] describing the correctness of the answer as a whole. The KC-based
grading assigned a score in the range [0, 1] indicating whether the student applied each
relevant KC in their answer and, if so, how correct their application of the KC was. Over
the course of the study we collected 64 × 2 = 128 students’ answers for Q22 in study 1 and
(37+29+28)×2 = 188 answers for Q22∗ in studies 2 and 3. For the remaining test questions
we obtained a total of 158 × 2 = 316 answers.
Two grader interfaces were developed for overall grading and KC-based grading respectively. During each grading process, all identifying information, such as the students’ ID,
group ID, the test (whether pre- or post-test), were concealed from the new grader and the
original grades and the original graders’ correct answers for each question were not presented.
The overall grading interface displays the test question statement, the correct answer
input by the new grader, a student’s solution and allows for the entry of an overall score for
the solution. All answers for a given question were graded serially before the grader moved
on to the next test question.
While overall grading did not borrow anything from the original grading, KC-based
grading borrowed the list of relevant KCs for each test question from the original grading.
As part of the initial grading process, the original grader identified the list of KCs that
were relevant to each test question. During the new grading process, I made use of that
list rather than re-mapping each test question to 31 KCs. As I stated previously, the lists
identify a total of 168 KC occurrences distributed over the 33 test questions. The KC-based
grading interface displays the students’ answers next to the correct answer and the list of
relevant KCs. The grader then entered a score for each KC separately. Again the grades
were assigned serially for all student answers before moving on to the next question.
In order to avoid errors, the test answers in study 3 were graded twice by the same procedure with the answers being checked for agreement. The other test questions were graded
twice, once by each grader with the original graders’ grades having already been checked for
errors. In order to adequately assess the new grades’ reliability, we investigated the intergrader agreement between the original grades and the new grades across the Exploratory
and DichGain groups.
189

B.2

INTER-GRADER AGREEMENT

Both graders agreed on the correct answers for all test questions. For the present study, intergrader agreement was calculated using Pearson’s product moment correlation which yields
a correlation statistic in the range [−1, 1]. This comparison was made both for the overall
and KC-based scores across the pre- and post-tests taken from studies 1 and 2. The level of
agreement on these grades was quite high on both the pre-test scores, r = .988, p = 0.000,
and post-test scores r = .988, p = 0.000 across the student groups. On the Exploratory
group alone, the correlation statistics were (r = .987, p = 0.000) on the pre-test and r =
.977, p = 0.000) on the post-test. For the DichGain group exam scores, the agreement was
(r = .996, p = 0.000) on the pre-test and (r = .993, p = 0.000) on the post-test.
Similar results were found for the cumulative KC-based scores across the two groups in
the two studies: r = .969, p = 0.000 (pre-test) and (r = .977, p = 0.000 (post-test). For the
Exploratory group within study 1 the agreement was similarly high: (r = .963, p = 0.000)
on the pre-test and (r = .969, p = 0.000) on the post-test. For the DichGain group in study
2, the results are (r = .983, p = 0.000) on the pre-test and (r = .984, p = 0.000) on the
post-test. Similarly high correlations were found on a per-KC basis.
The high level of correlation supports the conclusion that the grades assigned by the two
graders are equivalent and may thus be reliably compared. This conclusion is bolstered by
the fact that our previous findings for studies 1 and 2, reported in chapters 4 and 5, still
hold with the new grades.

190

APPENDIX C

BACKGROUND SURVEY

C.0.1

Instructions:

1. If you do not know or remember an item, enter your best guess.
2. For any item is not applicable to you, please enter N/A.
3. If you have any questions, please feel free to ask the experimenter.

C.0.2

Questions:

Which of the following courses did you complete in high school? To the best of your
recollection, please enter the grade you received for the highest level of each course you
completed. (For example, if you took both Algebra I and Algebra II, please indicate your
grade for Algebra II.)

Did you take an Advanced Placement(AP) high school course in either of the following
subjects? Please answer “Yes” or “No” for each course.

I have read and signed the consent form:
191

Table C1: Questions

Date:
User ID:

2009-11-02 13:15:48
777777

Age:
Gender:

Male

Native language:

English

High School GPA:
SAT scores:

Female
Other

out of a maximum value of: 4.0
Math:

Reading:

Writing:

(If you took the old SAT, please enter your Verbal score as Reading)

Name of college:
Year in college:

(1st = freshman, 2nd = sophomore, etc.)

College major(s):
College GPA:

out of a maximum value of:

Table C2: High School

High-school Course Grade Year(e.g 1999, 2006)
Algebra
Trigonometry
Calculus
Physics

192

Table C3: Advanced Placement
Course Yes

No

Calculus:
Physics:

Table C4: College-level Math
Please name any college-level math courses you have
taken, along with grades earned and years taken:
1
2
3
4
5
6

Table C5: College-level physics
Please name any college-level physics courses you
have taken, along with grades earned and years taken:
1
2
3
4
5
6

193

APPENDIX D

TEXTBOOK

194

D.0.2.1

Introduction

This brief textbook was designed to provide you with sufficient background knowledge
of physics to be able to learn something from this study. It assumes no prior college-level
physics knowledge. The next major section will introduce you to various physical quantities
covered in this study, and the last major section will cover topics specific to work and energy,
the topic domain for this study.
A Note About Notation: For consistency’s sake, this textbook shows equations and numbers that use superscripts and subscripts the same way you would type them in this study.
For example, to enter an equation with exponents, you would indicatethe superscripted
exponents using the carat character ˆ (shift-6):
a2 + b2 = c2 —>aˆ2 + bˆ2 = cˆ2
Similarly, to enter an equation with subscripts, you would indicate the subscripts using
adjacent [lower-case] letters:
vf = vi + a*t—>vf = vi + a*t
To enter an expression involving a square root, use sqrt:
c=

√

v0 =

(a2 + b2 )—>c = sqrt(aˆ2 + bˆ2)

√

[KE0 / ( 12 *m)]—>v0 = sqrt[KE0 /(0.5*m)]

195

D.0.2.2

Physical Quantities

In this study you will learn about and use different physical quantities. Quantities are
measurable physical features or properties, such as acceleration, length, or time, that can
be expressed as variables in equations. There are two different types of quantities in physics:
scalars and vectors.
A scalar quantity has only magnitude and is completely specified by a single numeric value
with units. Some examples are length (the field is 120 yds long and 50 yds wide) and time
(1 minute equals 60 s). Other examples are volume (a bottle has a volume of 1.5 liters)
and frequency (electrical current has a frequency of 60 Hz). Scalar quantities of the same
type can be added together using ordinary arithmetic (e.g., 60 s + 60 s = 120 s= 2 min).
A vector quantity has both magnitude and direction. Some examples are acceleration (32
ft/sˆ2 downwards) and velocity (60 mph due east). You must be careful to account for
direction when adding vector quantities. For example, when you add the velocity vectors
60 mph due east and 45 mph due west (i.e., in the opposite direction), the resulting vector
is 15 mph due east –not 60+45=105 mph.
In this textbook, we will represent vector variables in bold face (F, v, a) and scalar quantities in standard font (m, k). Variable names will usually begin with a letter denoting
its type, followed by letters or numbers distinguishing it from other variables of the same
type. For example, an object’s velocity at time point T1 may be represented by the variablev1, while the magnitude of its velocity at time T2 is v2. The mass of object A may be
represented as ma, and that of object B as mb.
Whenever you are asked to enter a numerical directional orientation for a vector in this
study, you should enter it in degrees relative to the right horizontal (counter-clockwise,
starting from 0), according to the following convention:

90
|
180 -+- 0
|
270
The next pages in this section cover some basic physical quantities that you will encounter
throughout the study.

196

D.0.2.3

Mass

Mass is a scalar measure of the amount of matter that makes up an object. Its standard
unit of measure is the kilogram (kg) and is often represented by a variable name beginning
with m.
Mass is the property of an object that causes it to have weight in a gravitational field. An
object of mass 1 kg will have 10 times as much mass as an object of mass 100 g, and it will
weigh 10 times as much. However, mass is not the same as weight, which is gravitational
force acting on an object. For example, an astronaut of mass 90 kg will have a different
weight on the Earth than he does on the moon. We will return to this distinction later.

197

D.0.2.4

Displacement

Displacement is a vector quantity representing the change in position of an object. Its
standard unit of measure is the meter (m) and is often represented by a variable name
beginning with d.
It is not the same as the scalar measure of distance traveled by a moving object. For
example, a swimmer who swims in a straight line from one end of a 100-m pool to the
other end and back swims a total distance of 200 m, but her displacement during that
same time interval is 0 m because she returns to her original position.
If a football player runs straight down the sideline from one end zone to the other (see
red arrow at the bottom of the figure below), he runs a total distance of 100 yds, and his
displacement is 100 yds in the direction of the far end zone. However, if he starts and ends
at the same two points on the sideline as before, but runs in a zig-zag or haphazard pattern
all over the field instead of a straight line (see yellow arrow below), his displacement is still
100 yds downfield but he will have run a distance greater than 100 yds.

Thus, the red arrow at the bottom of the figure (which represents the first running path)
also represents the player’s displacement after both running paths.

198

D.0.2.5

Velocity

Velocity is a vector quantity representing the rate of change in position of an object per
unit time. Its standard unit of measure is meters per second (m/s) and is often represented
by a variable name beginning with v.
The average velocity of a moving object is its displacement during a time interval divided by the duration of the time interval. However, velocity in this study will refer to
theinstantaneous velocity of an object, which is its displacement per unit time at a given
point in time (or, over an infinitesimal time interval).
For example, let’s say you throw a ball straight up in the air with an initial velocity of
5 m/s upwards. A short time later, its velocity will be less than 5 m/s upwards because
gravity will slow it down. At the point in time where the ball ceases to move upwards but
before it begins to fall back down, it has a velocity of 0 m/s. It will eventually land back
in your hands with a downward velocity of some magnitude.

199

D.0.2.6

Acceleration

Acceleration is a vector quantity representing the rate of change in velocity of an object
per unit time. Its standard unit of measure is meters per second per second, or meters per
second squared (m/sˆ2) and is often represented by a variable name beginning with a.
The average acceleration of a moving object is its change in velocity during a time interval
divided by the duration of the time interval. However, acceleration in this study will refer
to the instantaneous acceleration of an object, which is its change in velocity per unit time
at a given point in time (or, over an infinitesimal time interval).
As with other vector variables, be careful to account for direction when dealing with acceleration. For example, suppose the driver of a car moving east at 60 mph applies the
brakes. The car’s velocity vector (with magnitude v below) will continue to be eastward
until the car stops, but the braking acceleration vector (with magnitude a below) will be
to the west, in the direction opposite the car’s displacement.

Although some vectors change in magnitude or direction as an object moves (such as the
velocity on this and the previous page), most of the accelerations you will encounter in this
study will be constant.

200

D.0.2.7

Gravitational Acceleration

One such constant acceleration used in this study is the acceleration due to gravity of
a moving object near the surface of the Earth. It can be shown experimentally that all
objects near the Earth’s surface have exactly the same downward acceleration, whenever
the effects of air resistance can be eliminated or otherwise ignored.
Unlike many other accelerations, this special constant isnot represented by a variable name
beginning witha. Its magnitude is represented by the scalar variable g, and its direction is
always straight down (toward the center of the Earth):
g = 9.8 m/sˆ2

201

D.0.2.8

Force

A force is a push or a pull exerted on an object.
It is a vector quantity and its standard unit of measure is newtons (N), which is equivalent
to kilogram meters per second squared (kg*m/sˆ2). It is often represented by a variable
name beginning with F.
If a force is exerted by something in direct contact with the object, it is a contact force.
For example, if you pull as pring-loaded pinball plunger back toward you and then let it
go, it exerts a contact force on the pinball touching it, in the direction away from you. If
you tie a rope around a box and start pulling it across the floor, the rope exerts a tension
force on the box in the direction of your motion.
Other forces on an object that do not result from direct contact are called field forces.
A common example is gravitation. All objects near the Earth’s surface are subject to a
downward force due to the Earth’s gravitational field.

202

D.0.2.9

Weight Force

As discussed earlier, weight is different from mass. Weight is a force exerted on an object
within a gravitational field. It is, therefore, a field force. As with all forces, the weight force
is a vector quantity and its standard unit of measure is newtons(N). It is often represented
by the variable F.
The magnitude of the weight force on an object (F) equals the mass of the object multiplied
by the acceleration due to gravity (which is g if the object is near the Earth’s surface). As
with gravitational acceleration, the direction of the weight force vector is always straight
down.
F = m*g
As discussed earlier, a 90-kg astronaut will have a different weight on the Earth than he
does on the Moon. On the Earth he will weigh (90 kg)*(9.8 m/sˆ2) = 882 N, but on the
Moon he will weigh only about 1/6 as much (145.8 N), because the acceleration due to
gravity near the surface of the Moon is only 1.62 m/sˆ2.

203

D.0.2.10

Normal Force

When an object is pressed against a surface, the object experiences an opposing contact
force that is perpendicular to the surface. This force is called a normal force, because
“normal” is a mathematical term meaning perpendicular. As with all forces, a normal
force is a vector quantity and its standard unit of measure is newtons (N). It is often
represented by the variable F.
Suppose a rectangular object is resting on a table in your kitchen. You already know the
object experiences a downward weight force, proportional to its mass. It also experiences an
upward normal force from the table, which in this case is equal in magnitude and opposite
in direction to the weight force on the object.

The normal force from a surface is always perpendicular to the point of contact. Therefore,
if an object is moving along a curved surface (e.g., a rollercoaster car on a curved track),
the direction of the normal force will change as the object moves along the curved path:

204

D.0.2.11

Friction Force

For simplicity’s sake, many of the problem scenarios you encounter in this study will involve
frictionless surfaces, or will state that friction is negligible. However, some problems will
mention frictional force acting on a moving object, usually an object sliding across a surface.
As with all forces, a friction force is a vector quantity andits standard unit of measure is
newtons (N). A friction force (F) is a force along a surface that opposes the sliding of an
adjacent object across the surface. The friction force is parallel to the surface and opposite
in direction to the object’s motion.

205

D.0.2.12

Introduction to energy Energy can take many forms, but when properly

defined and measured, it turns out that the total energy of an isolated system does not
change over time. This law of nature is called conservation of energy. Notice that it uses
the terms “energy” and “isolated system,” so we will need to define those carefully.
For “isolated system”, we will first define a “system” to be a set of one or more objects
and then define “isolated system” as one where none of the objects inside the system interact
with objects outside the system. Because some systems are not isolated, we will also show
how to analyze a certain kind of non-isolated system that is particularly common.
For “energy,” we will define two forms of energy quite carefully and lump all other forms
of energy into a third, “catch-all” category. The three categories of energy are
• The kinetic energy of a moving object. We will define this carefully.
• The potential energy of a pair of interacting objects. This is energy that can be easily
converted into kinetic energy, which is why it is called “potential” energy. We will define
this carefully.
• The internal energy of an object. This category covers many kinds of energy, such as
temperature, chemical energy, or biological energy, that are not so easily converted to
kinetic energy. We will not define this category carefully because it encompasses so many
forms of energy.
Kinetic and potential energies are mechanical energies, while internal energy is not a
mechanical energy. From the general conservation of energy law we stated above, it follows
that, assuming the internal energy of the system does not change, then:
The total mechanical energy of an isolated system does not change.
This is a simple version of Conservation of Mechanical Energy. There is also a more
general version that works for certain kinds of non-isolated systems. We will first introduce
the simple version, then build up to the final, more complex version.
Because we want to focus on Conservation of Mechanical Energy, we will make the blanket
assumption throughout this experiment that the internal energies of objects do not change.
That is, we will assume that none of the objects in the examples, problems, or illustrations
ever change their temperature, their chemical energy, etc.

206

D.0.2.13

Kinetic energy

One type of energy, called kinetic energy (KE), is associated with individual objects. It
depends only on an object’s mass and on the magnitude of its velocity. In particular, it is
defined via the formula:
KE = 0.5 * m * vˆ2
where m is the mass of the object and v is the magnitude of the velocity of the object
relative to a stationary point of reference, such as the Earth. Although the Earth has mass
and moves through space, it can be considered a stationary point of reference for other
objects near it, with zero velocity and, hence, zero kinetic energy.
The standard unit of energy is (kg*mˆ2)/sˆ2 which is also denoted as J (Joule).
Example: a box of mass 2 kg travels in a circle with a velocity of 3 m/s. What is its
kinetic energy at any point in the circle?
Solution:
KE = 0.5 * m * vˆ2
= 0.5 * (2 kg) * (3 m/s)ˆ2
= 9 kg*mˆ2/sˆ2 = 9 J

207

D.0.2.14

Potential energy Suppose you toss a golf ball straight up so that it reaches

a peak of 10 meters before it turns around and falls to the ground. When it is on its way up,
say at 2 meters off the ground, it has a non-zero velocity so 0.5*m*vˆ2 is non-zero and thus
it has a non-zero kinetic energy. The golf ball slows down as it climbs, so its kinetic energy
decreases. At the very instant it reaches its peak, its velocity is zero for just that instant, so
0.5*m*vˆ2 is zero and thus its kinetic energy is zero at that instant. As the golf ball falls
down, it picks up speed, so it has a non-zero, increasing velocity, so 0.5*m*vˆ2 is non-zero
and increasing, and thus its kinetic energy is non-zero and increasing. In other words, while
the golf ball is in flight, its kinetic energy decreases to zero then increases again.
Conservation of Mechanical Energy suggests that the golf ball’s kinetic energy didn’t just
disappear as it rose, but instead was converted to another form of energy. As the golf ball
fell, its kinetic energy was returned to it, according to Conservation of Mechanical Energy.
So what is this “other form of energy” that borrows energy from kinetic energy and then
returns it?
When a form of energy is easily converted from kinetic energy and back, that form of
energy is called potential energy. Potential energy is associated with relative positions oftwo
objects: in the example above, different positions of the ball with respect to the Earth
account for the different values of the potential energy of the ball-Earth system.
We will not give a precise definition of “easily converted” in general nor of “potential
energy” in general. However, we will define precisely two common forms of potential energy:
gravitational potential energy and spring potential energy.

208

D.0.2.15

Gravitational potential energy Gravitational potential energy exists when

two objects are pulled together by a gravitational force. Although gravitational forces exist
between any pair of objects, as long as they both have mass, the force is too small to bother
with unless one of the objects is very massive (like a planet) and the other object is fairly
close to it. In that case, gravitational potential energy is defined as:
GPE = m*g*h
where h is the height of the object above the level of origin, g is the constant gravitational
acceleration near the surface of the planet (9.8 m/sˆ2 for Earth), and m is the object’s mass.
Since we will only be interested in the change of GPE as an object moves with respect
to the Earth, we can choose the level of origin to be anywhere, as long as it is fixed with
respect to the Earth. For any object at the level of origin, its height h = 0, and therefore the
potential energy of the object-Earth pair GPE =0. GPE will be positive when the object is
above the level of origin, and it will be negative when the object is below it.
The unit of energy is kg*mˆ2/sˆ2, also denoted as J (Joule).
Example: Suppose a 0.1 kg ball is tossed straight up so that it peaks at 10 meters off
the surface of the Earth. Suppose we define the origin to be 2 meters off the surface of the
Earth. Thus, h =(10-2) = 8 m when the ball is at its peak, so the gravitational potential
energy at its peak is:
GPE = m*g*h
= (0.1 kg) * (9.8 m/sˆ2) * (8 m)
= 7.84 kg*mˆ2/sˆ2 = 7.84J

If the ball reaches the ground, the GPE of the ball-Earth pair at that point is:
GPE = m*g*h
= (0.1 kg) * (9.8 m/sˆ2) * (-2m)
= -1.96 J

209

D.0.2.16

Spring potential energy A spring potential energy exists when one end of

a spring is anchored so that it does not move, and the other end exerts a force on an object
(e.g., see the Compressed spring in Figure 2).The value of the spring potential energy of an
object-spring pair is defined as:
SP E = 0.5 ∗ k ∗ d2
where k is a spring constant, and d is the extension (or compression) of the spring from
its equilibrium position. The spring constant k is a positive number that measures the
stiffness of a spring. The stiffer the spring, the larger the spring constant. The unit for a
spring constant is N/m (which is equivalent to kg/sˆ2). The equilibrium position of a spring
is the position of the unanchored end when the spring is not being compressed or extended.
The variable d measures how far the spring is compressed or extended, and it is a positive
scalar number in either case.
For the sake of simplicity, assume that all springs you encounter in this experiment are
ideal springs that have no mass of their own and do not degrade or deform, over time or due
to extended compression or extension.
The unit of energy is kg ∗ (m2 )/(s2 ), also denoted as J (Joule).
Example: Figure 1 shows a spring attached to a wall at one end and a mass m at the
other end. Suppose its spring constant is 0.2 N/m. It is at its equilibrium position, so the
spring potential energy of the spring-mass pair is:
SP E = 0.5 ∗ k ∗ d2 = 0.5 ∗ (0.2N/m) ∗ (0m)2 = 0J

Figure 1

Figure 2 shows the spring compressed by 0.3 m. The spring potential energy of the
spring-mass pair is thus:
SP E = 0.5 ∗ k ∗ d2 = 0.5 ∗ (0.2N/m) ∗ (0.3m)2 = 0.009kg ∗ (m2 )/(s2 ) = 0.009J

210

Figure 2

Figure 3 shows the spring stretched by 0.6 m. The spring potential energy of the springmass pair is thus:
SP E = 0.5 ∗ k ∗ d2 = 0.5 ∗ (0.2N/m) ∗ (0.6m)2 = 0.036kg ∗ m2 /s2 = 0.036J

Figure 3

211

D.0.2.17

Total mechanical energy A system, as mentioned before, is just a set of

one or more objects. Intuitively, the total mechanical energy of a system should be just the
sum of the energies of all the objects in the system. However, potential energies are defined
only forpairs of objects. We include a potential energy in the sum only if both objects are
included in the system. Thus:
The total mechanical energy of a system is the sum of the kinetic energies of each object
in the system plus the sum of the potential energies of each pair of objects in the system
that have potential energies.

For example, consider an object X that is being pushed by two springs, Sa and Sb. The
two springs are attached to a platform that is floating in deep space (i.e., nowhere near a
planet). Let us consider all the possible contributions to total mechanical energy:

• Suppose that X is moving with velocity v and has mass m, so ith as kinetic energy
1
2

∗ m ∗ v2.

• Suppose that springs Sa and Sb have no mass, so the kinetic energy of each spring is 0.
• Suppose that the spring constant of Sa is ka and it is compressed by da. Thus, the
potential energy of the pair X and Sais

1
2

∗ ka ∗ da2 .

• Suppose that the spring constant of Sb is kb and it is compressed by db. Thus, the
potential energy of the pair X and Sb is

1
2

∗ kb ∗ db2 .

212

Now we can demonstrate that the total mechanical energy (i.e., the sum of each object’s
kinetic energy plus any potential energies of object pairs) depends on which set of objects
comprise the system:
• The total mechanical energy of {X, Sa, Sb} is 21 ∗ m ∗ v 2 + 0 + 0 + 12 ∗ ka ∗ da2 + 12 ∗ kb ∗ db2
• The total mechanical energy of {X, Sa} is

1
2

∗ m ∗ v 2 + 0 + 12 ∗ ka ∗ da2

• The total mechanical energy of {X, Sb} is

1
2

∗ m ∗ v 2 + 0 + 12 ∗ kb ∗ db2

• The total mechanical energy of {X} is

1
2

∗ m ∗ v2

Similarly, gravitational potential energy is defined between a pair of objects, one of
which is usually a planet. For instance, if a rock of mass m is near the Earth and moving
with velocityv (relative to the Earth, which is to be considered stationary), then the total
mechanical energy of {rock, Earth} is

1
2

∗ m ∗ v 2 + m ∗ g ∗ h, where h is the height of the

rock above a reference point. The total mechanical energy of {rock} is
the Earth is not in the system.

213

1
2

∗ m ∗ v 2 , because

D.0.2.18

Isolated and non-isolated systems A system is called isolated whenever

all of the objects in it have no physical interaction with objects outside it. If at least one
object in a system has a physical interaction with some object outside the system, then the
system is called non-isolated.
There are many kinds of physical interactions. One kind is a force that exists between a
pair of objects. For instance, if object A pushes object B, then A exerts a force on B. If A
is in the system and B is not, then the system is non-isolated. In particular, if object P is a
planet that exerts a non-negligible force on object O, then a system that has O but not P is
non-isolated.
For example, consider an object falling toward the Earth in a vacuum. Because it is in
a vacuum, nothing is touching it, so the only force on it is the gravitational force of the
Earth. If we define the system to be {object, Earth}, then it is isolated because nothing
is interacting with the objects in it. If we define the system to be just {object}, then the
system is non-isolated, because the Earth is exerting a force on the object.
There are also physical interactions between objects that are not due to forces. For
instance, if one object radiates heat and warms up another object, then there is no force
between them, but there is a physical interaction.

214

D.0.2.19

Conservation of Mechanical Energy for isolated systems Having de-

fined the key terms “total mechanical energy” and “isolated system”, we can now return to
the simpler version of the law of Conservation of Mechanical Energy, which applies only to
isolated systems:
The total mechanical energy of an isolated system does not change.
Example: Suppose at time 1, a 0.1 kg ball is launched vertically upward from the surface
of the Earth with a velocity of 5 m/s. At time 2, it reaches its peak of 1.28 meters above
the surface of the Earth and its velocity is momentarily zero.
To analyze this situation with Conservation of Mechanical Energy, let us consider the
ball and the Earth to be a system. In order to keep the example simple, let us ignore air,
wind, ambient sound, sunlight, and all the other physical interactions that actually exist,
and treat this two-object system as isolated.
The law says that the total mechanical energy of {ball, Earth}at time 1 should equal the
total mechanical energy of {ball, Earth}at time 2. Let’s check: At time 1,
ME1 = KE1 + GPE1
where ME1 is the total mechanical energy at time1, KE1 is the total kinetic energy at
time 1, and GPE1 is the total gravitational potential energy at time 1. There is no spring
potential energy because there are no springs in the example. At time 2,
ME2 = KE2 + GPE2
where these variables refer to the total mechanical energy, kinetic energy, and gravitational potential energy at time 2. Now let’s consider each type of energy. For KE1, the
Earth is stationary but the ball starts with a velocity of 5 m/s. Thus, KE1= 0 + 0.5*m*vˆ2
= 0.5 * (0.1 kg) * (5 m/s)ˆ2 = 1.25 J. For KE2, the Earth is stationary and the ball has
zero velocity as well. Thus,KE2 = 0+0 = 0.
For gravitational potential energy, we need to choose an origin so we can calculate a height
above it. For simplicity’s sake let us choose the surface of the Earth as the origin. Thus, for
GPE1, m*g*h1 is 0 because the height (h1) is zero; therefore, GPE1=0. ForGPE2=m*g*h2,
we have h2=1.28 m, so GPE2 = (0.1 kg) * (9.8 m/sˆ2) *(1.28 m) = 1.25 J.
Now we can check Conservation of Mechanical Energy; that is, does ME1=ME2? We
simply plug in the values we have calculated:
215

ME1 = KE1 + GPE1
= 1.25 J + 0
ME2 = KE2 + GPE2
= 0 + 1.25 J
This shows that ME1=ME2. Essentially, the kinetic energy at time1 has been transformed into gravitational potential energy at time2.

216

D.0.2.20

Internal forces vs. external forces A system is non-isolated if there exists

at least one physical interaction between objects in the system and objects outside the
system. We can extend the law of Conservation of Mechanical Energy to handle a special
class of non-isolated system, where all the physical interactions are due to forces. In order
to do that, we first need to define several new terms: internal/external forces, work, and net
work. This page defines internal and external forces.
Recall that a force is defined only for a pair of objects: the object that the force acts on,
and the object that is exerting the force. For instance, when the Earth exerts a gravitational
force on a ball, there are two objects involved: the Earth and the ball. When a frictional
force slows down a sliding block, the surface (one object) is exerting a force on the sliding
block (the second object).
If there is a force involving a pair of objects, and we define the system to include only
one of them, then the force is crossing the system boundary and the system is non-isolated.
We call such a force an external force.
On the other hand, if both objects of the pair are inside the system, then we call the
force an internal force.

217

D.0.2.21

Work The previous page defined internal and external forces, but we still need

to define work in order to handle the afore mentioned special class of non-isolated system
where all the physical interactions are due to forces. A completely general definition of work
can be stated using vector calculus, but we will not do so here. Instead, we will define work
only for a few rather common configurations.
Work is defined for a particular object and a particular force over a particular time
interval. We will assume that through the time period, (1) the force is constant and has
magnitude F, and (2)the object moves in a straight line and its displacement has magnitude
d. Given these assumptions, the work W done on the object by the force depends on the
relative direction of the force and the displacement:
• If the object is moving in the same direction as the force, then W=F*d (positive).
• If the object is moving in the opposite direction to the force, then W= -F*d (negative).
• If the object is moving perpendicular to the force, then W=0.
Differences in how we handle the work done by internal versus external forces will be discussed on later pages (e.g., regarding different choices of systems).

218

D.0.2.22

Net work We have defined work for a single object, but Conservation of

Mechanical Energy applies to a system, which is a set of one or more objects. Thus, we need
to define the net work done on a system as the sum of the work done by the external forces
on objects inside the system. Notice that we only sum over theexternal forces and do not
include the internal ones.
For example, consider a crane that is lifting a block against gravity. If we define the
system to be {Earth, block} so that the crane is outside the system, then the force exerted
by the crane on the block is external whereas the gravitational force is internal. The net
work done on the system is the work done by the crane on the block. If the crane exerts a
force of magnitude F and lifts the block a displacement of magnitude d, then the work done
by the crane on the block (W) is F*d, so the net work on the system (Wnet)is also F*d.
On the other hand, if we define the system to be {block}, then both the crane and the
Earth are outside the system, so there are two external forces. As before, the work on the
block due to the crane is F*d. If we use Fw for the magnitude of the weight force due to
gravity, then the work done by the Earth is -Fw*d. (Note the minus sign, because the weight
force is downward but the block’s displacement is upward.) Thus, the net work done on the
system is Wnet=F*d-Fw*d.

219

D.0.2.23

Conservation of Mechanical Energy for systems whose non-isolation

is due to forces Now we can extend the law of Conservation of Mechanical Energy to
systems which are not isolated, but whose non-isolation is due to forces and where no other
kind of physical interaction (e.g., light, sound, heat) exists. In this case, the law is
ME1 + Wnet = ME2
where ME1 is the total mechanical energy at time 1, ME2 is the total mechanical energy
at time 2, and Wnet is the net work done on the system during that time interval.
Example: Suppose a little girl on ice skates is given a slow push by her father, who is
standing still. The magnitude of the force exerted by the father is 0.2 N, and the distance
traveled by the little girl while she is being pushed is 0.5 m. What kinetic energy does the
little girl attain, starting from rest?
We can solve this problem by using a non-isolated system comprised of the little girl and
the Earth. We will treat the ice and the father as being outside the system. First, let us
classify all the forces in this situation as either internal or external, and compute the work
done by the external ones.
• The father exerts a force on the girl. It is external, and so the work done is the force’s
magnitude (0.2 N) times the magnitude of the girl’s displacement (0.5 m). Thus, the
work done is 0.10J.
• The ice exerts a normal force on the girl. It is external. The force is straight up, and
the girl moves horizontally, so the force and the displacement are perpendicular. Thus,
the work done by the normal force on the girl is 0.
• The ice exerts a tiny friction force on the girl. Although it is external, it is so small that
we can ignore the work done by it.
• The Earth exerts a gravitational force on the girl. This force is internal, because we are
including the Earth in the system.
Summing up the net work done during the time interval on the{girl, Earth} system, we
have Wnet=0.10 J.
Initially, the system’s total mechanical energy, ME1, is KE1 +GPE1. After the push
ends, the system’s energy, ME2, is KE2 + GPE2. Substituting these into conservation of
220

energy, we have
KE1 + GPE1 + Wnet = KE2 + GPE2
The girl has the same height as she did initially, so GPE2=GPE1.Thus, we can simplify
the conservation of mechanical energy equation to
KE1 + Wnet = KE2
We know that KE1=0 because neither the girl nor the Earth are moving initially. We
know that KE2 is just the kinetic energy of the girl, because the Earth is still not moving.
After substituting these relationships for KE1 and KE2 into KE1 + Wnet = KE2, we obtain
Wnet = the girl’s kinetic energy, so her kinetic energy is0.10 J.

221

D.0.2.24

Different choices of system On the preceding page, we chose {girl, Earth}

as the system, making the Earth’s gravitational force internal to the system. Suppose we
instead chose {girl} as the system. Now the Earth is outside the system, so that its gravitational force becomes an external force.
Because we have added an external force, the net work done on the {girl} could be different than the net work done on the {girl, Earth}. However, as it turns out, the gravitational
force is perpendicular to the girl’s displacement, so the work done by it on the girl is zero.
Thus, the net work done on {girl} is the same as the net work done on {girl, Earth}.
There is a second change in the analysis. With {girl, Earth} as the system, the total
mechanical energies, ME1 and ME2, include the girl/Earth gravitational potential energies,
GPE1 and GPE2. With{girl} as the system, the gravitational potential energies are excluded
because potential energies exist only between pairsof objects in a system. Thus, applying
the law to {girl, Earth}, we got KE1 + GPE1 + Wnet = KE2 + GPE2, whereas applying
the law to{girl}, we now get KE1 + Wnet = KE2. However, GPE1 = GPE2 with{girl,
Earth}, so the solutions converge and we get exactly the same answer for both choices of
system.
What happens if we choose {father, girl, Earth} as the system? This would make the
father’s push into an internal force (it was external). Hence, the net work on the 3-object
system would be zero(it was F*d, the work done by the father on the girl). With a little more
math, we can show that the girl’s kinetic energy would be zero, which is wrong! Thus, we
do not have total freedom when choosing a system. Only some choices give correct answers.
In order to know which choice of system to make, we must first discuss the differences
between conservative and non-conservative forces.

222

D.0.2.25

Conservative and non-conservative forces A force is called conservative

if and only if we can define a potential energy for it. So far, we have defined potential
energies for gravitational forces and for spring forces, so those are the only conservative
forces introduced so far. All the other forces introduced so far (normal force, frictional force,
pulls by a crane, pushes by fathers, etc.) are non-conservativeforces.
Recall that the potential energy of a pair of objects depends only on their relative position. Thus, if one object of the pair moves around during a time interval but comes back to
exactly the same position, then the potential energy of the pair has not changed during the
time interval. For the non-conservative forces, there is no definition of potential energy that
has this property, and that is why they are non-conservative.
For instance, suppose we tried to define a potential energy for the father’s force on his
little girl. If there was such a “father’s force potential energy,” then we would include it
when summing up total mechanical energy. Now suppose at time 1 the girl is not moving.
Next the father pushes her away and pulls her back a hundred times. At time 2, she is
back at the original position and not moving. Now her kinetic energy at both times is
zero, her gravitational potential energy has not changed, and moreover, her “father’s force
potential energy” has not changed either, because she is back where she started. Thus,
her total mechanical energy, which includes the “father’s force potential energy,” has not
changed. Yet we know he will have expended some biological energy (i.e., burned some
calories), and thus his internal energy has changed. Thus, we have a change in internal
energy without a compensating change in total mechanical energy. This violates the general
law of Conservation of Energy. Thus, it is impossible to define a potential energy for the
father’s push that will allow the conservation laws to hold.
For this experiment, you can assume that the only conservative forces are gravitational
and spring forces. Although a few other conservative forces are known in physics, they will
not appear in any of the examples or problems used in this experiment.

223

D.0.2.26

Internal forces must be conservative When choosing a system, all the

internal forces must be conservative. As demonstrated earlier, when a non-conservative force
such as the father’s push is included as an internal force, the analysis is incorrect.
When we gather up all the various caveats that have been discussed, a final version of
the law of Conservation of Mechanical Energy can now be stated:
If a system can be chosen such that (a) all the internal forces are conservative, (b) external
forces are the only physical interactions between objects inside the system and objects
outside the system, and (c) no objects inside the system change their internal energy, then
ME1 + Wnet = ME2, where ME1 is the total mechanical energy at time 1, Wnet is the
net work done on the system by external forces between times 1 and 2, and ME2 is the
total mechanical energy of the system at time 2.

When the system is isolated, then there are no external forces, so the net work is zero
and so ME1 = ME2. Thus, the first version of Conservation of Mechanical Energy we
introduced, which applied only to isolated systems, is clearly just a special case of this more
general version.

224

D.0.2.27

Summary

Below is a summary table of all the major work and energy principles covered in this
textbook. Please review it now before continuing the next phase of the experiment.
PRINCIPLE
EQUATION
Kinetic energy
KE = 0.5 * m * vˆ2
Gravitational potential energy
GPE = m*g*h
Spring potential energy
SPE = 0.5 * k * dˆ2
Total mechanical energy
ME = KE + GPE + SPE
Work
W = F*d
W = F*d
if same direction
W = -F*d
if opposite direction
W = 0 if
perpendicular
Net work
Wnet = Σ W = sigma(W) ( = sum of all Ws)
Conservation of mechanical energy
ME0 + Wnet = ME1
(Wnet=0 if isolated system)

225

APPENDIX E

PRE- AND POSTTEST QUESTIONS.

Please enter the answer to the following question in the space below. (Value: 2/100)

1. Ente r the equation that defines the kinetic energy of an object (remember to use * for
multiplication and f̂or exponentiation, and be sure to include an = sign):

Please enter the answer to the following question in the space below. (Value: 2/100)

2. Enter the equation that defines the gravitational potential energy of an object (remember
to use * for ultiplication and f̂or exponentiation, and be sure to include an = sign):

Please enter the answer to the following question in the space below. (Value: 2/100)

3. Enter the equation that defines the spring potential energy of an object (remember to use
* for multiplication and f̂or exponentiation, and be sure to include an = sign):

Please enter the answer to the following question in the space below. (Value: 2/100)

226

4. During a time interval, an object moves in a straight line, a constant force acts on it, and
the force is in the same direction as the object’s motion. Enter the equation that defines
the work done on an object by the force (remember to use * for multiplication and f̂or
exponentiation, and be sure to include an = sign):

Please enter the answer to the following question in the space below. (Value: 2/100)

5. Enter the equation that defines the total mechanical energy of a system (remember to
use * for multiplication and f̂or exponentiation, and be sure to include an = sign):

Please select ALL of the possible answers to the following question from the list. (Value:
2/100)
6. Suppose an object is near Earth, moving, attached to a spring and acted on by an applied
force F. What does the kinetic energy of the object depend on directly? That is, what is
mentioned in the definition of kinetic energy? Check all that apply:
1. The mass of the object.
2. The magnitude of the object’s acceleration.
3. The direction of the object’s acceleration.
4. The magnitude of the object’s velocity.
5. The direction of the object’s velocity.
6. The magnitude of the object’s displacement.
7. The direction of the object’s displacement.
8. The magnitude of the force F.
9. The direction of the force F.
10. The spring constant of the spring.
11. The distance that the spring is compressed or extended.
12. The gravitational constant of the Earth.
13. The height of the object above the surface of the Earth.
227

14. The absence of any other physical interactions between the object and its environment
other than the ones mentioned above (the spring, the Earth and the applied force F).

Please select ALL of the possible answers to the following question from the list. (Value:
2/100)

7. Suppose an object is near Earth, moving, attached to a spring and acted on by an applied
force F. What does the gravitational potential energy of the object depend on directly? That
is, what is mentioned in the definition of gravitational energy? Check all that apply:
1. The mass of the object.
2. The magnitude of the object’s acceleration.
3. The direction of the object’s acceleration.
4. The magnitude of the object’s velocity.
5. The direction of the object’s velocity
6. The magnitude of the object’s displacement.
7. The direction of the object’s displacement.
8. The magnitude of the force F.
9. The direction of the force F.
10. The spring constant of the spring.
11. The distance that the spring is compressed or extended.
12. The ravitational constant of the Earth.
13. The height of the object above the surface of the Earth.
14. The absence of any other physical interactions between the object and its environment
other than the ones mentioned above (the spring, the Earth and the applied force F).

Please select ALL of the possible answers to the following question from the list. (Value:
2/100)

228

8. Suppose an object is near Earth, moving, attached to a spring and acted on by an applied
force F. What does the spring potential energy of the object depend on directly? That is,
what is mentioned in the definition of spring potential energy? Check all that apply:
1. The mass of the object.
2. The magnitude of the object’s acceleration.
3. The direction of the object’s acceleration.
4. The magnitude of the object’s velocity.
5. The direction of the object’s velocity.
6. The magnitude of the object’s displacement.
7. The direction of the object’s displacement.
8. The magnitude of the force F.
9. The direction of the force F.
10. The spring constant of the spring.
11. The distance that the spring is compressed or extended.
12. The gravitational constant of the Earth.
13. The height of the object above the surface of the Earth.
14. The absence of any other physical interactions between the object and its environment
other than the ones mentioned above (the spring, the Earth and the applied force F).

9. Suppose an object is near Earth, moving, attached to a spring and acted on by an applied
force F. What does the work done by F on the object depend on directly? That is, what is
mentioned in the definition of work done on an object? Check all that apply:
1. The mass of the object.
2. The magnitude of the object’s acceleration.
3. The direction of the object’s acceleration.
4. The magnitude of the object’s velocity.
5. The direction of the object’s velocity.
6. The magnitude of the object’s displacement.
229

7. The direction of the object’s displacement.
8. The magnitude of the force F.
9. The direction of the force F.
10. The spring constant of the spring.
11. The distance that the spring is compressed or extended.
12. The gravitational constant of the Earth.
13. The height of the object above the surface of the Earth.
14. The absence of any other physical interactions between the object and its environment
other than the ones mentioned above (the spring, the Earth and the applied force F).

Please enter the answer to the following question in the space below. (Value: 4/100)

10. A toy cart moves with a kinetic energy of 30 J. If the magnitude of its velocity is doubled,
what will its kinetic energy be?

Please enter the answer to the following question in the space below. (Value: 4/100)

11. A force of SON is exerted on a given object. How far can the object move if 5000 J of
work are available to move it?

230

Please enter the answer to the following question in the space below. (Value: 4/100)

12. A baseball with mass 0.145 kg is thrown straight up with an initial velocity of magnitude
20.0 mls. What is the work done by gravity on the baseball when it reaches a height of 18.0
m above the pitcher’s hand? (As you derive your answer, please show your work by typing
equations, explanations, etc. in the box below.)

Please enter the answer to the following question in the space below. (Value: 6/100)

13. An object of mass M is dropped from 10.0 m above the water surface. Use conservation
of mechanical energy to find the magnitude of its velocity 5.00 m above the water surface.
Neglect air resistance and assume it starts from rest. (As you derive your answer, please
show your work by typing equations, explanations, etc. in the box below.)

Please select one of the answers to the following question.(Value: 2/100)

14. A baseball with mass 0.145 kg is thrown straight up with an initial velocity of magnitude
25.0 mls. If we ignore the air friction, when the baseball is 20.0 m above the ground, the
magnitude of its velocity
1. does not depend on whether the baseball is moving upward or downward¡correct¿
2. depend on whether the baseball is moving upward or downward¡correct¿

Please enter the answer to the following question in the space below. (Value: 8/100)

15. A person pulls an 16.5 kg box across a floor by pulling on a rope with a constant force
of 47.5 N. The rope is horizontal. The frictional force on the box is 42.0 N. Determine the
magnitude of the velocity of the box after it has been pulled 6.50 m starting from an initial
231

velocity of 1.18 mls. Use work energy concepts. Solutions using other methods will not be
recognized. You may take the floor to be the zero level of gravitational potential energy.
(As you derive your answer, please show your work by typing equations, explanations, etc.
in the box below.)

Please select one of the answers to the following question.(Value: 2/100)

16. A block is slowly pushed against a spring, compressing it at a constant speed. During
this time interval, the spring is exerting a force on the block. The work done by the spring
on the block is:
1. positive
2. zero
3. negative

Please select one of the answers to the following question.(Value: 2/100)

17. Two men, Tom and Jerry, push against a wall with the same force. Jerry stops after 10
min, while Tom is able to push for 5 min longer. Compare the work against the wall they
each do.
1. Tom does 50% more work than Jerry.
2. Jerry does 50% more work than Torn
3. Tom does 75% more work than Jerry.
4. Neither of them does any work.

232

Please select one of the answers to the following question.(Value: 2/100)

18. A person pulls a box along the rough ground with a constant magnitude of velocity. If
we consider Earth and the box as our system, the work done by the person on the system is:
1. zero
2. nonzero

Please select one of the answers to the following question.(Value: 2/100)

19. When a student lifts a heavy box, the work done on the box by the Earth:
1. postive
2. negative
3. zero
4. need more information in order to answer.

Please select one of the answers to the following question.(Value: 2/100)

Q20*. An object can never have a negative kinetic energy. Used in study 1
1. True
2. False

Q20. For isolated systems, an increase in potential energy is always equal to: Used in
study 2 and 3
1. an increase in kinetic energy
2. a decrease in kinetic energy
3. an increase in mechanical energy
233

4. a decrease in mechanical energy
5. cannot tell without more information.

Please select one of the answers to the following question.(Value: 6/100)

21. Suppose a heavy box is suspended by a rope. A man holding the rope slowly lowers the
box onto a spring. The box is moving at constant velocity. Consider the system to consist
of the box alone. The system’s total mechanical energy:
1. increases.
2. decreases.
3. stays the same.
4. we cannot tell; more information is needed

Please select one of the answers to the following question.(Value: 6/100)

22. Suppose a heavy box is suspended by a rope. A man holding the rope slowly lowers the
box onto a spring. The box is moving at constant velocity. Define the system to be the box
and the spring, but no other objects. The system’s total mechanical energy:
1. increases.
2. decreases.
3. stays the same.
4. we cannot tell; more information is needed.

234

Please select one of the answers to the following question.(Value: 6/100)

23. Suppose a heavy box is suspended by a rope. A man holding the rope slowly lowers the
box onto a spring. The box is moving at constant velocity. Define the system to be the box
and the earth but no other objects. The system’s total mechanical energy:
1. increases.
2. decreases.
3. stays the same.
4. we cannot tell; more information is needed.

Please select one of the answers to the following question.(Value: 2/100)

24. You lift a ball at a constant velocity from a height hi to a greater height hf. Considering
the ball ALONE to be the system, which of the following statements is true?
1. The potential energy of the system increases.
2. The kinetic energy of the system decreases.
3. The earth does negative work on the system.
4. You do negative work on the system.
5. The source energy of the ball increases.
6. Two of the above.
7. None of the above.

Please select one of the answers to the following question.(Value: 2/100)

25. You lift a ball at constant velocity from a height hi to a greater height hf. Considering
the ball and the earth TOGETHER as the system, which of the following statements is true?
1. The potential energy of the system increases.
235

2. The kinetic energy of the system decreases.
3. The earth does negative work on the system.
4. You do negative work on the system.
5. The source energy of the ball increases.
6. Two of the above.
7. None of the above.

26. Consider the diagram of the trajectory of a thrown tomato: J

1. At what point is the potential energy greatest?
2. At what point is the kinetic energy the least?
3. At what point is the kinetic energy greatest?
4. At what point is the kinetic energy decreasing and the potential energy increasing?
5. At what two points are the kinetic energies equal and the potential energies equal?

Please select one of the answers to the following question.(Value: 2/100)

27. Two marbles, one twice as massive as the other, are dropped to the ground from the
roof of a building. Ignore air resistance. Just before hitting the ground,the heavier marble
has:
1. as much kinetic energy as the lighter one
2. twice as much kinetic energy as the lighter
236

3. one half as much kinetic energy as the lighter
4. one four times as much kinetic energy as the lighter one
5. square root of 2 (i.e., 1.414) times as much kinetic energy as the ligher one.
6. impossible to determine

Please select one of the answers to the following question.(Value: 2/100)

28. While flying along, a jet releases an empty propellent tank which eventually crashes
to the ground. Consider three cases: a)the jet is flying horizontally, b) the jet is climbing
upwards at a 45 degree angle, or, c) the jet is diving downwards at a 45 degree angle. In
all 3 cases, it is flying at the same speed and it releases the propellant tank from the same
height. Ignoring air friction, in which case is the tank’s speed greatest as it hits the ground?
1. The horizontal jet
2. The climbing jet
3. The diving jet
4. It doesn’t matter. The speed is the same in all three cases.
5. More information is needed in order to answer.

Please select one of the answers to the following question.(Value: 2/100)

29. Consider a system that consists of only the earth and box. A battery-powered motor
not considered part of the system lifts the box from the ground to a certain height above
the earth’s surface. Which statement below is most accurate after the box has been lifted
and corne to a stop?
1. The kinetic energy of the box has increased.
2. The kinetic energy of the box has decreased.
3. The total mechanical energy of the system remains the same, because the box is not
moving.
237

4. The total mechanical energy of the system has decreased.
5. The total mechanical energy of the system has increased.

Please select one of the answers to the following question.(Value: 2/100)

30. You support an object and move it to the right with a constant velocity. You exert a
force F on it to oppose the gravitational attraction of the earth for the object. If you do not
raise the object or increase its velocity, and air friction is negligible, do you do work on the
object?
1. Yes
2. No

31. A steel ball is placed at position A on the curved, hard surface shown [insert diagram].
The surface is fixed to the table on which it sits so that it does not move. The ball is held
at rest at position A and then is released. It rolls smoothly along the surface. Which choice
below most nearly describes the greatest height attained by the ball on the other side of the
curve?

1. It will not get over the hill in the middle.
2. Significantly below position C
3. Almost to position C
4. Almost to position B
5. Slightly higher than position B
238

Please select one of the answers to the following question.(Value: 4/100)
32. Two pucks on level, frictionless ice are shown above, pressed back by equal amounts
against identical springs. The pucks are the same size and shape, but one has four times the
mass of the other. The pucks are released and the springs propel them to the finish line. At
the finish line the kinetic energy of the less massive puck compare to the kinetic energy of
the more massive puck?

1. the same as the kinetic energy of the more massive puck.
2. four times the kinetic energy of the more massive puck.
3. twice the kinetic energy of the more massive puck.
4. half the the kinetic energy of the more massive puck.
5. one-fourth the kinetic energy of the more massive puck.

33. A puck sitting on level ice is pushed back against a spring that is attached to a wall.
This partially compresses the spring. The puck is released, and the spring propels it. If you
have only this spring, but a variety of different pucks, how could another puck be given more
energy? (i) Use a puck with less mass. (ii) Use a puck with more mass. (iii) Compress the
spring more.
1. i only
2. ii only
3. iii only
4. i and iii
5. ii and iii

239

APPENDIX F

PRE- AND POST-TEST KCS

240

241

KC:
Problem
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
Tot:

1

1
1
1
4

1
1
1
1
1
1
1
1
1
1
14

1
1
2

1

1

4

1
1
1
1

3

2

1

2

1

1

5

5

1
1

1
1

1

6

0

7

1
1
4

1

1

8

1
1
1
8

1
1
1

1
1

9

1
1
1
1
1
7

1

1

10

1

1

11

1
1
1
1
6

1

1

1

6

1

1

1

1

1

13

12

8

1

1
1

1

1

1

1

1

14

2

1
1

15

1
1
1
1
1
8

1

1
1

16

1

1

17

1
1
1
1
5

1

18

1
4

1

1

1

19

1
1
16

1

1
1

1
1
1
1
1

1
1
1

1

1

1

20

10

1
1
1
1
1

1

1
1

1

1

21

1
1
5

1

1

1

22

2

1

1

23

1
1
1
1
1
1
1
1
16

1
1
1
1

1
1
1

1

24

Table F1: KC Occurrences by Pre- & Post-test problem.

5

1
1
1

1
1

25

1
1
1
1
1
6

1

26

6

1
1
1

1

1
1

27

1
1
1
1
1
6

1

28

2

1

1

29

1

1

30

2

1

1

31

3

1
1
1

32

1
1
1
1
1
1
1
1
3
2
2
6
10
11
15
1
1
1
1
2
2
3
3
5
5
8
7
7
11
11
14
14
15
168

Tot

APPENDIX G

TRAINING PROBLEMS

P1. At T0, a 2000 kg car is moving with a velocity of magnitude 22.35 m/s. Find the kinetic
energy of the car.
P2. A man pushes a 20.0 kg crate across a frictionless floor with a horizontal force of 24.0
N. What work is done by the man on the crate in displacing it by 5.0 m?

We define:
T0: the time point when the man starts pushing the crate.
T1: the time point when the man has finished pushing the crate 5.0 m
Please see diagram below.

P3. Jan’s mountain bike has a spring with a constant of 64 N/m in the front wheel suspension. At T0 the front wheel suspension gets compressed by 0.17m when she hits a bump.
How much energy does the wheel-spring pair store at that point? Please see diagram
242

below.

P4. A 0.6 kg rock in space has a velocity of magnitude 2.0 m/s at point A and kinetic energy
of 7.50 J at point B. What is the net work done on the rock as it moves from A to B?
We define
T0: the time point when the rock is at point A.
T1: the time point when the rock is at point B.
P5. Suppose a 2000kg truck is dropped from a certain height above the ground, starting
from rest. Find the height from which it is dropped if it strikes the ground at 22.35 m/s.
Neglect air resistance. Assume that the origin is at the ground. We define
T0: the time point when the truck is dropped, from rest.
T1: the time point when it strikes the ground.
P6. At time T0, a 225 kg frictionless roller-coaster car reaches the top of the first hill with
a velocity of magnitude 3.0 m/s. At time T1, the car is at the point P1 and it reaches a
velocity of magnitude 7.8 m/s. At time T2, the car is at the point P2, 4 meters lower in
height than P1, and it reaches a velocity of magnitude 11.8 m/s. What is the magnitude
of the car’s instantaneous velocity at time T3, at the point when it reaches the base?
Assume that the origin is at the base of the roller-coaster and the height of the first hill
is 25 m above the base. Please see diagram below.
243

P7. A vertical spring is compressed by 0.15 m. A 3.00 kg block is then placed on the spring
and released. At time T1, the block passes the spring’s equilibrium position. Then the
block flies into the air. At 0.2 s later, 0.404 m above the uncompressed spring, the
block has a velocity of 1.04 m/s. How high will the block go above the uncompressed
spring? What is the constant of the spring? Choose the origin to be at the top of the
uncompressed spring. We define
T0: the time point when the spring is compressed by 0.15 m with the block resting on
it.
T1: the time point when the block passes the spring’s equilibrium position.
T2: the time point when the block is 0.404 m above the spring’s equilibrium position.
T3: the time point when the block reaches its maximum height.
Please see diagram below.

244

APPENDIX H

AN EXAMPLE OF STUDENT-CORDILLERA LOG FILE

Here is a student who is under the Exploratory Group solved P4 on Random-Cordillera.
Table H1: An Example of Student Log on Training Problem P4
Time
1

speaker

14:51:41 Tutor:

string

Label

[continue]This problem is a little more complex than the problems that you have solved
so far, but I think you can handle it.

2

14:51:41 Tutor:

[ent txt]What does the problem statement

ELICIT

ask you to find?
3

14:52:09 Student:

Work done on the rock

4

14:52:26 Tutor:

[continue]It asks you to find net work, more
specifically, net work done on the rock as it
moves from A to B.

5

14:52:36 Tutor:

[ent txt]What are the units for net work?

ELICIT
KC15

6

14:52:46 Student:

J

”+KC15”
Continued on Next Page. . .

245

Table H1: An Example of Student Log on Training Problem P4
Time

speaker

string

Label

7

14:52:47 Tutor:

Very good!

8

14:52:47 Tutor:

[continue]Net work always refers to two time
points.

9

14:52:54 Tutor:

[continue]In this problem, they are T0 and
T1. T0 is defined as the time when the rock
is at point A and T1 is defined as the time
when it is at point B.

10

14:53:04 Tutor:

[continue]As always, let‘s first define variables and give values to some of them.

11

14:53:09 Tutor:

[continue]I will define m to be the mass of
the rock.

12

14:53:14 Tutor:

[continue]There are two variables that we can
define at both T0 and T1, the rock‘s magnitude of velocity and the rock‘s kinetic energy.
I will define them for you. Look at the variable window after the next step.

13

14:53:24 Tutor:

[ent txt]Which variable has a value 0.60 kg?

14

14:53:46 Student:

M

15

14:53:47 Tutor:

Yes.

16

14:53:53 Tutor:

[ent txt]Which variable has a value of 2.0 ELICIT
m/s?

17

14:54:16 Student:

V0

18

14:54:17 Tutor:

Okay.
Continued on Next Page. . .

246

ELICIT

Table H1: An Example of Student Log on Training Problem P4
Time
19

speaker

14:54:18 Tutor:

string

Label

[main menu ‘num units‘] What is the value

ELICIT

of the kinetic energy of the rock at T1?

KC23
”-KC23”

20

14:54:46 Student:

1.2m/s

21

14:54:47 Tutor:

I disagree with you.

22

14:54:47 Tutor:

[continue]At T1,the rock is at point B, and

”*KC23”

the problem says the rock‘s kinetic energy at
point B is 7.50 J.
23

14:54:52 Tutor:

[continue] Next let‘s identify the sought variable.

24

14:54:55 Tutor:

[continue]The problem asks for the net work TELL
done on the rock from A to B. I will define
it as Wnet01 in the variable window.

25

14:55:00 Tutor:

[continue] Our main goal is to calculate

”*KC18

Wnet01. There are two principles that are

*KC28”

involved in net work. One is the definition
of net work and the other is conservation of
mechanical energy for non-isolated systems.
Let‘s examine whether one or both of these
principles are applicable to this problem.
Continued on Next Page. . .

247

Table H1: An Example of Student Log on Training Problem P4
Time
26

speaker

14:55:08 Tutor:

string

Label

[continue]According to the definition of net ”*KC18”
work, the net work done on the rock is the
sum of the work done by the external forces
on the rock. From the problem statement,
there is no information given about the forces
acting on the rock(e.g. gravitational force).
So we cannot apply the first principle directly
to this problem.

27

14:55:18 Tutor:

[continue]Now let‘s examine the remaining
principle involving net work, namely conservation of mechanical energy for non-isolated
systems. It should be applicable since we
don‘t have any other principles to try!

28

14:55:28 Tutor:

[continue]Conservation of mechanical energy ”*KC28”
for non-isolated systems in its generic form
says that when there are external forces acting on a system, the net work done by these
forces on the system equals the change in the
total mechanical energy of the system.

29

14:55:37 Tutor:

[continue]The first thing we need to do is to ”*KC28”
define a system because conservation of mechanical energy for non-isolated systems is
applied to a system, not an object.
Continued on Next Page. . .

248

Table H1: An Example of Student Log on Training Problem P4
Time
30

speaker

14:55:45 Tutor:

string

Label

[continue]In this problem, let‘s simply select TELL
the rock as the system.

31

14:55:48 Tutor:

justify choose-system

SKIPJUSTIFY

32

14:55:48 Tutor:

[continue]Next, we need to define variables
for the total mechanical energy for the rock
system at T0 and at T1. I will define them as
TME0 and TME1 respectively. Please look
at the variable window on the next step.

33

14:55:56 Tutor:

[continue]There is more than one way to solve
this problem. In the following discussion, I
may lead you on a bit of a detour at times
simply to help you practice applying some of
the principles.

34

14:56:03 Tutor:

[continue]We will solve this problem in three
steps. First, we will calculate KE0, then we
will calculate v1, and finally we will calculate
Wnet01.

35

14:56:08 Tutor:

[continue]So let‘s start with determining the
value of KE0.

36

14:56:11 Tutor:

[continue]To calculate the rock‘s kinetic en-

TELL

ergy at T0, let‘s apply the definition of ki-

*KC20

netic energy.
Continued on Next Page. . .

249

Table H1: An Example of Student Log on Training Problem P4
Time
37

speaker

14:56:27 Tutor:

string

Label

[main menu ‘eqn‘]Please write the equation

ELICIT

for applying the definition of kinetic energy KC20
to the rock at T0.
38

14:56:31 Student:

ke0 = 1/2 * M * V02̂

”+KC20”

39

14:56:32 Tutor:

[main menu ‘num units‘]Since all the vari-

ELICIT

ables in the equation are known except for KC23
KE0, we can calculate KE0 easily. What is
the value of KE0?
40

14:56:47 Student:

1.2J

”+KC23”

41

14:56:48 Tutor:

Okay.

42

14:56:49 Tutor:

[continue]Before we go any further, let‘s discuss this principle in a little more detail.

43

14:56:56 Tutor:

[ent txt]Why does the rock have a non-zero Justify
kinetic energy at T0?

ELICIT
KC20

44

14:57:17 Student:

because it has a nonzero mass and a nonzero ”+KC20”
magnitude of velocity

45

14:57:18 Tutor:

Yes.

46

14:57:19 Tutor:

[continue]We do not know the direction of

TELL

the rock‘s velocity at T0 from the problem *KC20
statement. However, the direction of v0 does
not affect the rock‘s kinetic energy at T0.
Continued on Next Page. . .

250

Table H1: An Example of Student Log on Training Problem P4
Time
47

speaker

14:57:23 Tutor:

string

Label

58-ke0 discuss qualitative 2

SkipJUSTIFY
KC20

48

14:57:23 Tutor:

[continue]We‘ve calculated KE0, recall that
our plan is to calculate v1 next.

49

14:57:26 Tutor:

[continue]To calculate the rock‘s instanta-

TELL

neous magnitude of velocity at T1, we will

*KC20

apply the definition of kinetic energy again.
50

14:57:31 Tutor:

[continue]Let me just write the equation for TELL
you: KE1=1/2*m*v12̂.

51

14:57:34 Tutor:

*KC20

[continue]From KE1=1/2*m*v12̂, we get TELL
v12̂=KE1/(0.5*m). We substitute KE1 with

*KC31

7.50 J and m with 0.60 kg. The equation
now becomes: v12̂= 7.50 J/(0.5* 0.60 kg).
The magnitude of velocity is always positive,
therefore, v1= 5.0 m/s.
52

14:57:41 Tutor:

58-v1 discuss qualitative

SKIPJustify
KC20

53

14:57:42 Tutor:

[continue]Now that we know v1, we will concentrate on the sought quantity for this problem, that is, the net work done on the rock
system from A to B.
Continued on Next Page. . .

251

Table H1: An Example of Student Log on Training Problem P4
Time
54

speaker

14:57:48 Tutor:

string

Label

[continue]Our system consists of the rock and

”*KC28”

we have planned to apply the conservation of
mechanical energy for non-isolated systems
from T0 to T1 to find the net work done on
the rock from point A to point B.
55

14:57:54 Tutor:

[continue]More specifically, we will first find
the system‘s total mechanical energy at T0,
then we will find it at T1, and finally we will
find the net work done on the rock from A to
B by examining the change in total mechanical energy from T0 to T1.

56

57

14:58:00 Tutor:

14:58:04 Tutor:

[continue]Let‘s use the definition of total me-

TELL

chanical energy to find TME0.

*KC24

[continue]The generic equation for the defi-

TELL

nition of total mechanical energy is T M E = *KC24
KE + GP E + SP E.
58

14:58:08 Tutor:

[continue]In this problem, the system only ”*KC21
has one object, the rock. Therefore, there are

*KC22”

no potential energies involved, neither gravitational potential energy nor spring potential
energy.
59

14:58:09 Tutor:

58-tme0 discuss truth

SKIPJUSTIFY
KC21

Continued on Next Page. . .

252

Table H1: An Example of Student Log on Training Problem P4
Time
60

speaker

14:58:10 Tutor:

string

Label

[main menu ‘eqn‘] Please write the equation

ELICIT

for the definition of total mechanical energy KC24
for the rock system at T0.
61
62

14:58:33 Student:
Tutor:

T M E0 = KE0

”+KC24”

[main menu ‘num units‘]It is easy to infer the

ELICIT

value of TME0. Please calculate the value of

KC23

TME0.
63

14:58:52 Student:

1.2J

”+KC23”

64

14:58:53 Tutor:

[cust menu ‘Yes, it is true—No, it is not ELICIT
true—I do not know—Not enough informa-

KC24

tion‘]Equation T M E0 = KE0 tells us that
the rock system‘s total mechanical energy is
equal to the rock‘s kinetic energy at T0. Is
it true *for any given time*?
65

14:59:29 Student:

No, it is not true

66

14:59:30 Tutor:

I disagree with you.

67

14:59:31 Tutor:

[continue]In fact ....it is true for any time

”-KC24”

”*KC24”

point in this problem.
68

14:59:32 Tutor:

[continue]This is because as long as the rock TELL
is the only object in the system, there are

*KC24

no potential energy involved, neither gravi-

*KC21

tational potential energy nor spring potential

*KC22

energy. Thus T M E = KE at any given time
point.
Continued on Next Page. . .

253

Table H1: An Example of Student Log on Training Problem P4
Time
69

speaker

14:59:40 Tutor:

string

Label

58-tme0 discuss relevance

SKIPJUSTIFY
KC28

70

14:59:41 Tutor:

[continue]We have computed TME0, let‘s
calculate TME1 next.

71

14:59:43 Tutor:

[continue]Since the total mechanical energy TELL
is equal to kinetic energy at any given time

*KC24

as long as the rock is the only object in a system, let‘s use the definition of total mechanical energy again to get the value for TME1.
72

14:59:50 Tutor:

[main menu ‘eqn‘] Please write the appropri-

ELICIT

ate equation for the definition of total me-

KC24

chanical energy of the system at T1.
73

14:59:58 Student:

T M E1 = KE1

74

14:59:59 Tutor:

[main menu ‘num units‘]What is the value of ELICIT
TME1?

”+KC24”

*KC23

75

15:00:16 Student:

7.5J

76

15:00:17 Tutor:

58-tme1 discuss qualitative

SKIPJUSTIFY
KC24

77

15:00:17 Tutor:

58-tme1 discuss truth

SKIPJUSTIFY
KC24

Continued on Next Page. . .

254

Table H1: An Example of Student Log on Training Problem P4
Time
78

speaker

15:00:17 Tutor:

string

Label

[continue]We just calculated TME1 and
TME0, let‘s calculate the net work done on
the rock from A to B.

79

15:00:22 Tutor:

[continue]As we have mentioned before, there

TELL

were two principles from the textbook which

*KC28

apply to net work: one is the definition of

*KC18

net work and the other is conservation of mechanical energy for non-isolated systems.
80

15:00:30 Tutor:

[continue]The definition of the net work cannot be applied to this problem because there
is no information given in the problem statement about the forces on the rock. Therefore, we will apply the remaining principle,
namely, conservation of mechanical energy
for non-isolated systems. We will start with
the generic equation.

81

15:00:38 Tutor:

[main menu ‘form‘] What is the generic equa-

ELICIT

tion for conservation of mechanical energy for KC28q
non-isolated systems?
82

15:01:06 Student:

T M E0 = T M E1

83

15:01:07 Tutor:

That doesn‘t sound right to me.

84

15:01:08 Tutor:

[continue]The generic equation for conservation of mechanical energy for non-isolated
systems: W net01 = T M E1 − T M E0
Continued on Next Page. . .

255

”-KC28q”

”*KC28q”

Table H1: An Example of Student Log on Training Problem P4
Time
85

speaker

15:01:15 Tutor:

string

Label

[ent txt]What time points and bodies should

ELICIT

we use when applying the principle of conser-

KC28

vation of mechanical energy for non-isolated
systems?
86

15:01:57 Student:

We should use the time points where the to-

”-KC28”

tal mechanical energy changes.
87

15:01:59 Tutor:

[ent txt]What objects should you apply the
conservation of mechanical energy for nonisolated systems to?

88

15:02:29 Student:

the objects that are in motion

”-KC28”

89

15:02:31 Tutor:

[continue]We have already defined the system

”*KC28”

to be the rock. Therefore, we should apply
conservation of mechanical energy to the rock
system.
90

15:02:37 Tutor:

[continue]To apply conservation of mechanical energy for non-isolated systems, we need
two time points.

91

15:02:47 Tutor:

[ent txt]What time points should we use
when applying conservation of mechanical
energy for non-isolated systems?

92

15:03:30 Student:

T0 and T1

93

15:03:32 Tutor:

Okay.

”+KC28”

Continued on Next Page. . .

256

Table H1: An Example of Student Log on Training Problem P4
Time
94

speaker

15:03:32 Tutor:

string

Label

[continue]Let me just write the equation for TELL
conservation of mechanical energy for the

*KC28

rock system between T0 and T1 for you:
T M E0 + W net01 = T M E1
95

15:03:39 Tutor:

[continue]Since we already know the values

ELICIT

for TME0 and TME1, we can calculate the

KC15

value of Wnet01 easily.
96

15:03:46 Tutor:

[main menu ‘num units‘]Please calculate the
value of Wnet01.

97

15:03:56 Student:

6.3J

”+KC15”

98

15:03:58 Tutor:

[continue]We have finished solving the whole
problem. Next let‘s do a little discussion.

99

15:04:00 Tutor:

[cust menu ‘KE0—v1—TME1—TME0—Wnet01—All
ELICIT
are necessary—I do not know.‘]While solving
the problem, we calculated the variables in
the order of KE0, v1, TME0, TME1, and
Wnet01. Are any of these variable(s) not
necessary to solve the problem?

100 15:04:54 Student:

v1

101 15:04:55 Tutor:

Yes.

102 15:04:57 Tutor:

[continue]The reason that we calculate v1 in
this problem is to practice the principle of
the definition of kinetic energy.
Continued on Next Page. . .

257

Table H1: An Example of Student Log on Training Problem P4
Time

speaker

103 15:05:02 Tutor:

string

Label

[cust menu ‘Yes, it is possible—No, it is not ELICIT
possible—I do not know—Not enough infor-

KC21

mation‘]In this problem, we have selected the
rock as the system. Is it possible to define
potential energy for the rock system?
104 15:05:22 Student:

No, it is not possible

105 15:05:23 Tutor:

Yes, I agree.

106 15:05:23 Tutor:

58-pe concept justification 2

”+KC21”

SKIPJustify
KC21

107 15:05:23 Tutor:

[continue]Next, we will change the place
where the rock is and discuss whether the
values of some variables would also change.

108 15:05:31 Tutor:

[cust menu ‘increase—decrease—same—not ELICIT
enough information—I do not know‘]Suppose

KC20

the 0.6 kg rock is put on the Earth and the
rock‘s kinetic energy at point B is still 7.5 J,
what will happen to the rock‘s magnitude of
velocity at the point B?
109 15:06:09 Student:

same

”+KC20”

110 15:06:10 Tutor:

Very good!

111 15:06:10 Tutor:

58-v1-ppd-discussion-2

SKIPJUSTIFY
KC20

Continued on Next Page. . .

258

Table H1: An Example of Student Log on Training Problem P4
Time

speaker

112 15:06:10 Tutor:

string

Label

[cust menu ‘increase—decrease—same—not ELICIT
enough

information—I

do

not KC28

know.‘]Suppose the rock is on the Earth and

KC24

everything in the problem statement is kept
the same. What will happen to net work
done on the rock if we still choose the rock
alone as the system?
113 15:06:36 Student:

same

”+KC28
+KC24”

114 15:06:37 Tutor:

Sounds good.

115 15:06:37 Tutor:

58-wnet01-ppd-discussion-2

SKIPJUSTIFY
KC28
KC24

116 15:06:37 Tutor:

[main menu ‘num units‘]Let‘s go back to the
situation where rock is in space. Look at the
variable window, tell me what is the magnitude of v0?

117 15:06:55 Student:

2m/s

”+KC31”

118 15:06:56 Tutor:

Yes, I agree.

119 15:06:57 Tutor:

[main menu ‘num units‘]What is the magnitude of v1?

120 15:07:08 Student:

5m/s

121 15:07:09 Tutor:

Excellent!

”+KC31”

Continued on Next Page. . .

259

Table H1: An Example of Student Log on Training Problem P4
Time

speaker

122 15:07:11 Tutor:

string

Label

[continue]Although the magnitude of v1 (5.0 TELL
m/s) is bigger than the magnitude of v0 (2.0 *KC31
m/s), we *cannot* infer that the rock‘s mag-

*KC20

nitude of velocity is increasing during the
ENTIRE period from T0 to T1.
123 15:07:17 Tutor:

[main menu ‘num units‘]Look at the variable
window, tell me what is the value of the
rock‘s kinetic energy at T0?

124 15:07:31 Student:

1.2J

”+KC23”

125 15:07:32 Tutor:

Yes, I agree.

126 15:07:34 Tutor:

[main menu ‘num units‘]What is the value of
KE1?

127 15:07:47 Student:

7.5J

”+KC23”

128 15:07:48 Tutor:

Excellent!

129 15:07:50 Tutor:

[continue]Compared with point A, the ki-

TELL

netic energy of the rock at point B is in-

*KC20

creased.
130 15:07:52 Tutor:

[ent txt]Please try to explain why.

Justify
ELICIT
KC20

131 15:08:17 Student:

it is so because th evelocity of the rock has
increased from A to B

132 15:08:18 Tutor:

Very good!
Continued on Next Page. . .

260

”+KC20”

Table H1: An Example of Student Log on Training Problem P4
Time

speaker

133 15:08:19 Tutor:

string

Label

[continue]Suppose there is point C on the

TELL

path of the rock between A to B. We do *KC20
not have enough information to say anything
about the kinetic energy of the rock at point
C compared with point A.
134 15:08:27 Tutor:

[continue]This is because we do not know

Justify

whether the rock‘s magnitude of velocity at TELL
C is bigger than its magnitude of velocity at *KC20
A or not. It is possible that the rock continuously increases its magnitude of velocity from
A to C then to B. It is also possible its magnitude of velocity decreases from A to C and
then increases from C to B. It is even possible that it does not change its magnitude of
velocity from A to C and then increases its
magnitude of velocity from C to B.
135 15:08:39 Tutor:

[menu

’increase—decrease—same—not ELICIT

enough

information—I

do

not KC20

know.’]Compared with point B, What
can you say about the kinetic energy of the
rock at point C?
136 15:09:12 Student:

not enough information

137 15:09:13 Tutor:

Sounds good.
Continued on Next Page. . .

261

”+KC20”

Table H1: An Example of Student Log on Training Problem P4
Time

speaker

138 15:09:13 Tutor:

string

Label

[ent txt]Please explain why.

JUSTIFY
ELICIT
KC20

139 15:10:07 Student:

It is so beacause the KE depends on the ve-

”+KC20”

locity and we do not know the velocity of the
rock at point C.
140 15:10:08 Tutor:

[continue]Next, we will change the mass of
the rock and discuss whether the values of
some variables would also change.

141 15:10:10 Tutor:

[continue]Suppose that the rock‘s mass in-

TELL

creased from 0.6 kg to 1 kg and the rock‘s

*KC20

kinetic energy at point B was still 7.5 J, the
rock‘s magnitude of velocity at point B would
decrease.
142 15:10:16 Tutor:

[continue]It is because we are assuming that JUSTIFY
the rock‘s kinetic energy at point B stays

TELL

same, 7.50 J, and if the rock‘s mass increases,

*KC20

according to KE= 1/2*m*v2̂, then the magnitude of rock‘s magnitude of velocity has to
decrease.
Continued on Next Page. . .

262

Table H1: An Example of Student Log on Training Problem P4
Time

speaker

143 15:10:22 Tutor:

string

Label

[cust menu ‘increase—decrease—same—not ELICIT
enough

information—I

do

not KC20

know.‘]Compared with point B, What
can you say about the kinetic energy of the
rock at point C?
144 15:10:46 Student:

increase

”+KC20”

145 15:10:47 Tutor:

I agree.

146 15:10:49 Tutor:

[ent txt]Why?

JUSTIFY
ELICIT
KC20
KC23

147 15:11:09 Student:

It is so because the KE of a body depends on

”-KC20”

it‘s mass
148 15:11:10 Tutor:

I disagree with you.

149 15:11:12 Tutor:

[continue]Because the magnitude of rock *KC20
magnitude of velocity is still 2.0 m/s at point *KC20q
A and the mass of the rock increases, according to KE= 1/2*m*v2̂ the kinetic energy of
the rock at point A increases.
Continued on Next Page. . .

263

*KC31

Table H1: An Example of Student Log on Training Problem P4
Time

speaker

150 15:11:18 Tutor:

string

Label

[cust menu ‘increase—decrease—same—not ELICIT
enough information—I do not know.‘]Finally,

KC28

if the rock‘s mass were 1 kg instead of 0.6 kg KC24
and the rock‘s magnitufe of velocity at point KC20
A was still 2.0 m/s and the rock‘s kinetic
energy at point B was still 7.5 J, what will
happen to the net work done on the rock as
it moves from A to B?
151 15:11:59 Student:

decrease

”+KC28,
+KC24,
+KC20”

152 15:12:00 Tutor:

58-inc mass3 2

SKIPJUSTIFY
KC28
KC24
KC20

153 15:12:02 Tutor:

[cust menu ‘Yes, we can—No, we can-

ELICIT

not—not enough information—I do not KC14
know.‘]If we already know that the work done
on the rock by force F is 9 J and we also know
that the rock is moving in the same direction
as F for 3 m in a straight line, can you infer
the magnitude of the force F?
154 15:12:28 Student:

Yes, we can

”-KC14”

Continued on Next Page. . .

264

Table H1: An Example of Student Log on Training Problem P4
Time

speaker

string

Label

155 15:12:29 Tutor:

I disagree with you.

156 15:12:31 Tutor:

[continue]No, we cannot infer anything about
the magnitude of the force F.

157 15:12:39 Tutor:

58-work concept justification

SKIPJUSTIFY
KC14

158 15:12:39 Tutor:

[continue]Let‘s go back to the original situ-

TELL

ation. The problem does not give any hint *KC28
about the path taken by the rock. However,

*KC24

taking different paths would not affect our fi-

*KC20

nal answer of the net work done on the rock
from A to B.
159 15:12:49 Tutor:

58-compare path 2

SKIPJUSTIFY
KC28
KC24
KC20
KC15

160 15:12:49 Tutor:

[continue]We‘re finished with this problem.
I think I should give you a more complex
problem next time.
Continued on Next Page. . .

265

Table H1: An Example of Student Log on Training Problem P4
Time

speaker

161 15:12:59 Tutor:

string

Label

[ent txt]The next problem will take about 40
minutes to complete. If you start this problem and do not have enough time to finish,
then you will have to start it over from the
beginning when you return for your next session. Are you ready for the next problem?

162 15:13:30 Student:

Yes

266

APPENDIX I

AN EXAMPLE TUTORIAL SCRIPT

Example tutorial script for problem 58, listed as problem 4 in Appendix G.

g start
say ‘‘[start_problem 58]’’
do solving_problem
do discuss_problem
say ‘‘[cont]We’re finished with this problem. I think I should give
you a more complex problem next time.’’
do logout

g solving_problem
say ‘‘[cont]This problem is a little more complex than the problems
that you have solved so far, but I think you can handle
it.’’
do pre-discuss-problem
do time_points
say ‘‘[cont]As always, let’s first define variables and give values
to some of them.’’
267

do given_soughts
do application

g pre-discuss-problem sem elicit
say ‘‘What does the problem statement ask you to find?’’
if ‘‘net work.’’ true
if ‘‘net work done on the rock as it moves from A to B.’’ true
otherwise say ‘‘[cont]It asks you to find net work, more specifically,
net work done on the rock as it moves from A to B.’’
say ‘‘What are the units for net work?’’
if ‘‘Joules.’’ true kc ‘‘+KC15’’
otherwise say ‘‘[cont]The units of the net work are Joules.’’
kc ’’-KC15 *KC15’’

g pre-discuss-problem sem tell
say
on

’’[cont]The problem statement asks us to find the net work done
the rock when it moves from point A to point B.’’

say ’’[cont]The units for net work are Joules. ’’ kc ’’*KC15’’

g time_points
say ’’[cont]Net work always refers to two time points.’’
say

’’[cont]In this problem, they are T0 and T1. T0 is defined as the

time when the rock is at point A and T1 is defined as
the time when it is at point B.’’
say

’’[add ’T0’]’’

say

’’[add ’T1’]’’

say

’’[add ’rock’]’’

g given_soughts

268

do list_givens
do list_soughts

g list_givens
say ’’[cont]I will define m to be the mass of the rock.’’
say ’’[add ’m’ omit=’value’]’’
say ’’[cont]There are two variables that we can define at both
T0 and T1, the rock’s magnitude of velocity and the
rock’s kinetic energy.

I will define them for you.

Look at the variable window after the next step.’’
say ’’[add ’v0’ omit=’mag’]’’
say ’’[add ’KE0’ omit=’value’]’’
say ’’[add ’v1’ omit=’mag’]’’
say ’’[add ’KE1’ omit=’value’]’’
do list_mass
do list_v0
do list_ke1

g list_mass sem elicit
say ’’Which variable has a value {m.value}?’’
if ’’m’’ true
otherwise say ’’[cont]It is m.’’
say ’’[add ’m’]’’

g list_mass sem tell
say ’’[cont]From the problem statement, we know that m
has a value {m.value}.’’
say ’’[add ’m’]’’

269

g list_v0 sem elicit
say ’’Which variable has a value of 2.0 m/s?’’
if ’’v0’’ true
otherwise say ’’[cont]v0. ’’
say ’’[add ’v0’]’’

g list_v0 sem tell
say ’’[cont]We can also see from the problem statement that
v0 has a value of 2.0 m/s.’’ kc ’’*KC31’’
say ’’[add ’v0’]’’

g list_ke1 sem elicit
say ’’[ask ’num_units’] What is the value of the kinetic
energy of the rock at T1?’’
if ’’{KE1.value}’’ true kc ’’+KC23’’
if ’’correct value, wrong/no unit’’
say ’’[cont]You entered
the correct value, but not the correct units.
The correct units for kinetic energy are J. ’’
kc ’’-KC23 *KC23’’
if ’’{KE0.value}’’
say ’’[cont] You seem to have calculated the value for
the kinetic energy of the rock at T0. Now we
are still listing the given values. At T1,the
rock is at point B, and the problem says the
rock’s kinetic energy at point B is {KE1.value}.
’’ kc ’’+KC23’’
otherwise say ’’[cont]At T1,the rock is at point B, and the
problem says the rock’s kinetic energy at
point B is {KE1.value}.’’ kc ’’*KC23’’

270

say ’’[add ’KE1’]’’

g list_ke1 sem tell
say ’’[cont]At T1,the rock is at point B, and the problem
says the rock’s kinetic energy at point B
is {KE1.value}’’ kc ’’*KC23’’
say ’’[add ’KE1’]’’

g list_soughts
say ’’[cont] Next let’s identify the sought variable.’’
do list_h0

g list_h0 sem elicit
say ’’What is the sought quantity for this problem?’’
if ’’net work done on the rock from T0 to T1.’’ true
say ’’[cont]I defined it as Wnet01 in the
variable window.’’
otherwise say ’’[cont]The problem asks for the net work
done on the rock from A to B. I defined
it as Wnet01 in the variable window.’’
say ’’[add ’Wnet01’ omit=’value’]’’

g list_h0 sem tell
say ’’[cont]The problem asks for the net work done on the rock
from A to B. I will define it as Wnet01 in the
variable window.’’
say ’’[add ’Wnet01’ omit=’value’]’’

g application

271

do global_discuss
say ’’[cont]So let’s start with determining the value of KE0.’’
do KE0
say ’’[cont]We’ve calculated KE0, recall that our plan is to
calculate v1 next.’’
do v1
say ’’[cont]Now that we know v1, we will concentrate on the sought
quantity for this problem, that is, the net work done on
the rock system from A to B. ’’
say ’’[cont]Our system consists of the rock and we have planned to
apply the conservation of mechanical energy for
non-isolated
on the rock

systems from T0 to T1 to find the net work done
from point A to point B.’’ kc ’’*KC28’’

say ’’[cont]More specifically, we will first find the system’s total
mechanical energy at T0, then we will find it at T1, and
finally

we will find the net work done on the rock

from A to B by examining the change in total mechanical
energy from T0 to T1.’’
do TME0
say ’’[cont]We have computed TME0, let’s calculate TME1 next.’’
do TME1
say ’’[cont]We just calculated TME1 and TME0, let’s calculate the net
work done on the rock from A to B.’’
do CME01

g global_discuss
say ’’[cont] Our main goal is to calculate Wnet01. There are two
principles that are involved in net work. One is the definition of
net work and the other is conservation of mechanical energy for nonisolated systems.

Let’s examine whether one or both of these prin

272

ciples are applicable to this problem.’’ kc ’’*KC18 *KC28’’
say ’’[cont]According to the definition of net work, the net work
done on the rock is the sum of the work done by the external forces
on the rock. From the problem statement, there is no information
given about the forces acting on the rock(e.g. gravitational force).
So we cannot apply the first principle directly to this problem.
’’ kc ’’*KC18’’
say ’’[cont]Now let’s examine the remaining principle involving net
work, namely conservation of mechanical energy for non-isolated
systems. It should be applicable since we don’t have any other
principles to try!’’
say ’’[cont]Conservation of mechanical energy for non-isolated
systems in its generic form says that when there are external
forces acting on a system, the net work done by these forces on
the system equals the change in the total mechanical energy of
the system.’’ kc ’’*KC28’’
say ’’[cont]The first thing we need to do is to define a system
because conservation of mechanical energy for non-isolated
systems is

applied to a system, not an object.’’ kc ’’*KC28’’

do choose-system_1
say ’’[cont]Next, we need to define variables for the total mechanical
energy for the rock system at T0 and at T1.
as TME0 and TME1 respectively.

I will define them

Please look at the variable

window on the next step. ’’
say ’’[add ’TME0’ omit=’value’]’’
say ’’[add ’TME1’ omit=’value’]’’
say ’’[cont]There is more than one way to solve this problem. In the
following discussion, I may lead you on a bit of a detour at
times simply to help you practice applying some of the principles. ’’
say ’’[cont]We will solve this problem in three steps. First, we will

273

calculate KE0, then we will calculate v1, and finally we will
calculate Wnet01.’’

g choose-system_1
do choose-system
do justify_choose-system sem justify

g choose-system sem elicit
say ’’[add ’rock’]’’
say ’’What would be your choice of the system for this problem?’’
if ’’rock’’ true
otherwise do bottom_out_choose-system

g justify_choose-system sem elicit
say ’’Why?’’
if ’’In this problem there is only one object: the rock. It is the
target object.’’ true
otherwise say ’’[cont]In this problem the target object is the rock
. Since we have no information about the forces being applied to
it, we do not have any clue whether there exist any other objects
besides the rock. Therefore, we choose the rock alone to be the
system.’’

g justify_choose-system sem tell
say ’’[cont]In this problem the target object is the rock. Since we
have no information about the forces being applied to it, we do
not have any clue whether there exist any other objects besides
the rock. Therefore, we choose the rock alone to be the system.’’

274

g bottom_out_choose-system
say ’’[cont]Let’s choose the rock to be the system.’’

g choose-system sem tell
say ’’[add ’rock’]’’
say ’’[cont]In this problem, let’s simply select the rock as the
system. ’’

g KE0
do KE0_principle_selection
do KE0_write_eqn
do KE0_solve_equation
do KE0_discuss_truth rand sem justify
do KE0_discuss_qualitative

g KE0_principle_selection sem elicit
say ’’Which principle will help you calculate the rock’s kinetic
energy at T0? Please provide the name of the principle, not
an equation. ’’
if ’’Definition

of kinetic energy. ’’ true kc ’’+KC20’’

otherwise say ’’[cont]Let’s apply the definition of kinetic
energy.’’ kc ’’-KC20 *KC20’’
say ’’[ask ’form’] What is the generic equation for the definition
of kinetic energy? KE=’’
if ’’{form_KE}’’ true kc ’’+KC20q’’
otherwise say ’’[cont]The generic equation for the definition of

275

kinetic energy is: KE= {form_KE}’’ kc ’’-KC20q *KC20q’’

g KE0_principle_selection sem tell
say ’’[cont]To calculate the rock’s kinetic energy at T0, let’s
apply the definition of kinetic energy. ’’ kc ’’*KC20’’
say ’’[cont]The generic equation for the definition of kinetic
energy is KE={form_KE}.’’

kc ’’*KC20q’’

g KE0_write_eqn sem elicit
say ’’[ask ’eqn’]Please write the equation for applying the
definition of kinetic energy to the rock at T0.’’
if ’’{eqn_KE0}’’ true say ’’[add ’eqn_KE0’]’’ kc ’’+KC20q’’
if ’’equation with substituted values’’ do bo_KE0_sub_write_eqn
otherwise do bo_KE0_write_eqn kc ’’-KC20q’’

g KE0_write_eqn sem tell
say ’’[cont]Now I will write the equation for applying the
defin ition of kinetic energy to the rock at T0: {eqn_KE0}’’
kc ’’*KC20q’’
say ’’[add ’eqn_KE0’]’’

g bo_KE0_sub_write_eqn
say ’’[cont]You substituted problem values into the equation.
Please use the variable labels instead.’’
do KE0_write_eqn

g bo_KE0_write_eqn
say ’’[add ’eqn_KE0’]’’
say ’’[cont]Let me write the equation for the rock’s kinetic

276

energy at T0 for you: {eqn_KE0}. Please look at the equation
window.’’ kc ’’*KC20q’’

g KE0_solve_equation sem elicit
say ’’[ask ’num_units’]Since all the variables in the equation
are known except for KE0, we can calculate KE0 easily. What is
the value of KE0?’’
if ’’{KE0.value}’’ true

kc ’’+KC23’’

if ’’correct value, wrong/no unit’’

say ’’[cont]You entered the

correct value, but not the correct units. The correct units for
kinetic energy are J. ’’ kc ’’-KC23 *KC23’’
otherwise do bo_KE0_solve_equation
say ’’[add ’KE0’]’’

g bo_KE0_solve_equation
say ’’[cont]I calculate the value for KE0 = 0.5*{m.value}*{v0.ma
g}^2= {KE0.value}’’ kc ’’*KC23’’

g KE0_solve_equation sem tell
say ’’[cont]Since all the variables in the equation are known
except KE0, we can calculate KE0 easily. KE0=0.5*{m.value}*
{v0.mag}^2= {KE0.value}’’ kc ’’*KC23’’
say ’’[add ’KE0’]’’

g KE0_discuss_truth sem elicit
say ’’[cont]Before we go any further, let’s discuss this
principle in a little more detail.’’
say ’’Why does the rock have a non-zero kinetic energy at T0?’’
if ’’Because the rock has a non-zero velocity at T0.’’ true

277

kc ’’

+KC20’’
if ’’Because the rock is moving at T0’’ true

kc ’’+KC20’’

if ’’Because the rock’s magnitude of velocity at T0 is above 0’’
true kc ’’+KC20’’
otherwise say ’’[cont]Because the rock has a non-zero velocity
at T0 and the rock’s mass is not zero.’’ kc ’’-KC20 *KC20’’

g KE0_discuss_truth sem tell
say ’’[cont]Because the rock has a non-zero velocity at T0 and
the rock’s mass is not zero, the rock has a non-zero kinetic
energy at T0.’’ kc ’’*KC20’’

g KE0_discuss_qualitative
do KE0_discuss_qualitative_1
do KE0_discuss_qualitative_2 rand sem justify

g KE0_discuss_qualitative_1 sem elicit
say ’’[menu ’Yes, it would affect|No, it would not affect|I do
not know|Not enough information’]We do not know the direction
of the rock’s velocity at T0 from the problem statement. Would
the direction of v0 affect the rock’s kinetic energy at T0?’’
if ’’No, it would not affect’’ true

kc ’’+KC20’’

otherwise say ’’[cont]Actually ... the direction of the rock at
T0 would not affect the rock’s kinetic energy at T0.’’
kc ’’-KC20 *KC20’’

g KE0_discuss_qualitative_1 sem tell
say ’’[cont]We do not know the direction of the rock’s velocity
at T0 from the problem statement. However, the direction of v0

278

does not affect the rock’s kinetic energy at T0.’’ kc ’’*KC20’’

g KE0_discuss_qualitative_2 sem elicit
say ’’Why?’’
if ’’The rock’s kinetic energy only depends on the rock’s mass
and its magnitude of velocity, not the direction of the rock.’’
true kc ’’+KC20’’
if ’’The direction of the rock at T0 does not matter. Only
magnitude of velocity matters.’’ true kc ’’+KC20’’
otherwise say ’’[cont]The direction of the rock at T0 would not
affect the rock’s kinetic energy at T0 because the kinetic
energy depends on mass and the magnitude of velocity, not the
direction of the rock’s velocity.’’ kc ’’-KC20 *KC20’’

g KE0_discuss_qualitative_2 sem tell
say ’’[cont]This is because the kinetic energy only depends on
the rock’s mass and magnitude of velocity, not the direction of
the rock’s velocity.’’ kc ’’*KC20’’

g v1
do v1_principle_selection
do v1_write_eqn
do v1_solve_equation
do v1_discuss_qualitative rand
.

g v1_principle_selection sem elicit

279

say ’’Which principle will help you calculate the rock’s
instantaneous magnitude of velocity at T1?’’
if ’’Definition of kinetic energy.’’ true kc ’’+KC20’’
otherwise say ’’[cont]We should apply the definition of kinetic
energy once again.’’ kc ’’-KC20 *KC20’’

g v1_principle_selection sem tell
say ’’[cont]To calculate the rock’s instantaneous magnitude of
velocity at T1, we will apply the definition of kinetic energy
again. ’’ kc ’’*KC20’’

g v1_write_eqn sem elicit
say ’’[ask ’eqn’]Please write the equation for how the
definition of kinetic energy applies to this problem at T1.’’
if ’’{eqn_KE1}’’ true say ’’[add ’eqn_KE1’]’’ kc ’’+KC20q’’
if ’’equation with substituted values’’ do bo_v1_sub_write_eqn
otherwise do bo_v1_write_eqn kc ’’-KC20q’’

g bo_v1_sub_write_eqn
say ’’[cont]You substituted problem values into the equation.
Please use the variable labels instead.’’
do v1_write_eqn

g bo_v1_write_eqn
say ’’[add ’eqn_KE1’]’’
say ’’[cont]Let me just write the equation for you: {eqn_KE1.
value}.’’ kc ’’*KC20q’’

g v1_write_eqn sem tell
say ’’[add ’eqn_KE1’]’’

280

say ’’[cont]Let me just write the equation for you: {eqn_KE1.val
ue}.’’ kc ’’*KC20q’’

g v1_solve_equation sem elicit
say ’’[ask ’num_units’]Now it is easy to calculate the magnitude
of v1. What is the magnitude of v1?’’
if ’’{v1.mag}’’ true say ’’[add ’v1’]’’ kc ’’+KC31’’
if ’’correct value, wrong/no unit’’

say ’’[cont]You entered the

correct value, but not the correct units. The correct units for
velocity are m/s. ’’ kc ’’-KC31 *KC31’’
otherwise do bo_v1_solve_equation
say ’’[add ’v1’]’’

g bo_v1_solve_equation
say ’’[cont]From {eqn_KE1.value}, we get v1^2=KE1/(0.5*m). We
substitute KE1 with {KE1.value} and m with {m.value}. The
equation now becomes: v1^2={KE1.value}/(0.5*{m.value}). The
magnitude of velocity is always positive, therefore,
v1= {v1.mag}. ’’ kc ’’*KC31’’

g v1_solve_equation sem tell
say ’’[add ’v1’]’’
say ’’[cont]From {eqn_KE1.value}, we get v1^2=KE1/(0.5*m). We
substitute KE1 with {KE1.value} and m with {m.value}. The
equation now becomes: v1^2={KE1.value}/(0.5*{m.value}). The
magnitude of velocity is always positive, therefore, v1=
{v1.mag}.’’ kc ’’*KC31’’

281

g v1_discuss_qualitative
say ’’[cont]Before going on to the next step, let’s think about
the application of this equation.’’
do v1_discuss_qualitative1
do v1_discuss_qualitative2 sem justify

g v1_discuss_qualitative1 sem elicit
say ’’[menu ’Yes, we can|No, we cannot|I do not know’]Can we
infer the direction of the velocity of the rock at T1 from the
rock’s kinetic energy at T1?’’
if ’’No. we cannot.’’ true

kc ’’+KC20’’

otherwise say ’’[cont]No. We cannot get the direction of the
rock’s velocity.’’ kc ’’-KC20 *KC20’’

g v1_discuss_qualitative1 sem tell
say ’’[cont]We cannot infer anything about the direction of the
rock at T1 from the rock’s kinetic energy at T1. ’’ kc ’’*KC20’’

g v1_discuss_qualitative2 sem elicit
say ’’Please explain why.’’
if ’’Because the kinetic energy only depends on mass and the
magnitude of velocity, not the direction of velocity.’’
true kc ’’+KC20’’
otherwise

say ’’[cont]This is because the kinetic energy only

depends on mass and the magnitude of velocity, not the
direction of velocity. ’’ kc ’’-KC20 *KC20’’

g v1_discuss_qualitative2 sem tell
say ’’[cont]This is because the kinetic energy only depends

282

on mass and the magnitude of velocity, not the direction
of velocity. ’’ kc ’’*KC20’’

g TME0
do TME0_principle_selection
do TME0_write_eqn
do TME0_solve_equation
do TME0_discuss_qualitative
do TME0_discuss_relevance rand sem justify

g TME0_principle_selection
do TME0_principle_selection1
do TME0_principle_selection2
say ’’[cont]In this problem, the system only has one object, the
rock. Therefore, there are no potential energies involved,
neither gravitational potential energy nor spring potential
energy. ’’ kc ’’*KC21 *KC22’’
do TME0_discuss_truth rand

g TME0_principle_selection1 sem elicit
say ’’First, let’s find out TME0. Which principle will help you
find TME0? ’’
if ’’Definition of total mechanical energy.’’ true kc ’’+KC24’’
otherwise say ’’[cont]let’s use the definition of total
mechanical energy.’’ kc ’’-KC24 *KC24’’

283

g TME0_principle_selection1 sem tell
say ’’[cont]Let’s use the definition of total mechanical energy
to find TME0.’’ kc ’’*KC24’’

g TME0_principle_selection2 sem elicit
say ’’[ask ’form’] What is the generic equation for the
definition of total mechanical energy? TME=’’
if ’’{form_TME}’’ true kc ’’+KC24q’’
otherwise say ’’[cont]The generic equation for the definition of
total mechanical energy is TME={form_TME}’’ kc ’’-KC24q *KC24q’’

g TME0_principle_selection2 sem tell
say ’’[cont]The generic equation for the definition of total
mechanical energy is TME={form_TME}.’’ kc ’’*KC24q’’

g TME0_write_eqn
say ’’[ask ’eqn’] Please write the equation for the definition
of total mechanical energy for the rock system at T0.’’
if ’’{eqn_TME0}’’ true say ’’[add ’eqn_TME0’]’’ kc ’’+KC24q’’
if ’’equation with substituted values’’ do bo_TME0_sub_write_eqn
otherwise do bo_TME0_write_eqn kc ’’-KC24q’’

g bo_TME0_sub_write_eqn
say ’’[cont]You substituted problem values into the equation.
Please use the variable labels instead.’’
do TME0_write_eqn

g bo_TME0_write_eqn
say ’’[add ’eqn_TME0’]’’
say ’’[cont]I have written the system’s total mechanical energy

284

equation at T0 for you: {eqn_TME0}’’ kc ’’*KC24q’’

g TME0_solve_equation sem elicit
say ’’[ask ’num_units’]It is easy to infer the value of TME0. Pl
ease calculate the value of TME0.’’
if ’’{TME0.value}’’ true say ’’[add ’TME0’]’’ kc ’’+KC23’’
if ’’correct value, wrong/no unit’’

say ’’[cont]You entered the

correct value, but not the correct units. The correct units for
total mechanical energy are J. ’’ kc ’’-KC23 *KC23’’
otherwise do bo_TME0_solve_equation
say ’’[add ’TME0’]’’

g bo_TME0_solve_equation
say ’’[cont]It is easy. TME0 and KE0 have the same value: {TME0.
value}’’ kc ’’*KC23’’

g TME0_solve_equation sem tell
say ’’[add ’TME0’]’’
say ’’[cont]From {eqn_TME0}, it is easy to infer the value of
TME0. TME0 and KE0 have the same value: {TME0.value}’’
kc ’’*KC23’’

g TME0_discuss_qualitative
do TME0_discuss_qualitative_1
do TME0_discuss_qualitative_2 sem justify

g TME0_discuss_qualitative_1 sem elicit

285

say ’’[menu ’Yes, it is true|No, it is not true|I do not know|No
t enough information’]Equation {eqn_TME0.value} tells us that
the rock system’s total mechanical energy is equal to the rock
’s kinetic energy at T0. Is it true *for any given time*?’’
if ’’Yes, it is true.’’ true kc ’’+KC24’’
otherwise say ’’[cont]In fact ....it is true for any time point
in this problem. ’’ kc ’’-KC24

*KC24’’

g TME0_discuss_qualitative_1 sem tell
say ’’[cont]Equation {eqn_TME0.value} tells us that the system’s
total mechanical energy is equal to the rock’s kinetic energy
at T0.

In fact, this is true for any given time point in

this problem.’’ kc ’’*KC24’’

g TME0_discuss_qualitative_2 sem elicit
say ’’Why?’’
if ’’Because as long as the rock is the only object in the
system, there are no potential energies involved. Thus the total
mechanical energy equals the rock’s kinetic energy at any given
time point. ’’ true

kc ’’+KC24 +KC21 +KC22’’

otherwise say ’’[cont]This is because as long as the rock is the
only object in the system, there are no potential energies
involved, neither gravitational potential energy nor spring
potential energy. Thus the rock system’s total mechanical energy
equals the rock’s kinetic energy at any given time point. ’’ kc
’’-KC24 -KC21 -KC22 *KC24 *KC21 *KC22’’

g TME0_discuss_qualitative_2 sem tell
say ’’[cont]This is because as long as the rock is the only
object in the system, there are no potential energy involved,

286

neither gravitational potential energy nor spring potential energy
. Thus TME = KE at any given time point. ’’
kc ’’*KC24 *KC21 *KC22’’

g TME0_discuss_truth
do TME0_discuss_truth1 sem justify

g TME0_discuss_truth1 sem elicit
say ’’Why are there no potential energies involved in this
problem?’’ rand
if ’’Because the rock is the only object in the system, there
are no potential energies involved. ’’ true kc ’’+KC21’’
otherwise say ’’[cont]Recall that potential energy is associated
with the relative positions of two objects. Because the rock
is the only object in the system, there are no potential
energies involved. ’’ kc ’’-KC21 *KC21’’

g TME0_discuss_truth1 sem tell
say ’’[cont]Next, I want to explain why there are no potential
energies involved in this problem.’’
say ’’[cont]Recall that potential energy is associated with the
relative positions of two objects.
in

There is only one object

this problem, the rock, and thus potential energy is not

involved in this problem. ’’ kc ’’*KC21’’

g TME0_discuss_relevance sem elicit
say ’’[cont]Let’s go back to continue our problem solving.’’

287

say ’’Thinking from the perspective of the whole solution plan,
why do we need to calculate the system’s total mechanical
energy at T0?’’ rand
if ’’We can calculate the net work done on the rock from T0 to
T1. ’’ true kc ’’+KC28’’
otherwise say ’’[cont]Because if we already know TME0 and TME1,
we can calculate the net work on the rock from T0 to T1.’’ kc ’’
-KC28 *KC28’’

g TME0_discuss_relevance sem tell
say ’’[cont]Let’s go back to continue our problem solving.’’
say ’’[cont]Thinking from the perspective of the whole solution
plan, the reason that we need to calculate the system’s total
mechanical energy at T0 is so that we can calculate the net
work done on the rock from T0 to T1. ’’ kc ’’*KC28’’

g TME1
do TME1_principle_selection
do TME1_write_eqn
do TME1_solve_equation
do TME1_discuss_truth rand
do TME1_discuss_qualitative rand

g TME1_principle_selection sem elicit
say ’’[cont]Since the total mechanical energy is equal to
kinetic energy at any given time as long as the rock is the only
object in a system, it is simple to infer the value for TME1.’’
kc ’’*KC24’’
say ’’Which principle will help you find TME1? ’’

288

if ’’Definition of total mechanical energy. ’’ true

kc ’’+KC24’’

otherwise say ’’[cont]Let’s use the definition of total
mechanical energy again.’’ kc ’’-KC24 *KC24’’

g TME1_principle_selection sem tell
say ’’[cont]Since the total mechanical energy is equal to
kinetic energy at any given time as long as the rock is the only
object in a system, let’s use the definition of total mechanical
energy again to get the value for TME1.’’ kc ’’*KC24’’

g TME1_write_eqn
say ’’[ask ’eqn’] Please write the appropriate equation for the
definition of total mechanical energy of the system at T1.’’
if ’’{eqn_TME1}’’ true say ’’[add ’eqn_TME1’]’’ kc ’’+KC24q’’
if ’’equation with substituted values’’ do bo_TME1_sub_write_eqn
otherwise do bo_TME1_write_eqn kc ’’-KC24q’’

g bo_TME1_sub_write_eqn
say ’’[cont]You substituted problem values into the equation.
Please use the variable labels instead.’’
do TME1_write_eqn

g bo_TME1_write_eqn
say ’’[add ’eqn_TME1’]’’
say ’’[cont]Let me just write the system’s total mechanical
energy equation at T1 for you: {eqn_TME1.value}.’’ kc ’’*KC24q’’

g TME1_solve_equation sem elicit
say ’’[ask ’num_units’]What is the value of TME1?’’
if ’’{TME1.value}’’ true say ’’[add ’TME1’]’’ kc ’’+KC23’’

289

if ’’correct value, wrong/no unit’’

say ’’[cont]You entered the

correct value, but not the correct units. The correct units for
total mechanical energy are J. ’’ kc ’’-KC23 *KC23’’
otherwise do bo_TME1_solve_equation
say ’’[add ’TME1’]’’

g bo_TME1_solve_equation
say ’’[cont]It is easy. TME1 and KE1 have the same value: {TME1.
value}’’ kc ’’*KC23’’

g TME1_solve_equation sem tell
say ’’[add ’TME1’]’’
say ’’[cont]We can easily get the value of TME1. TME1 and KE1
have the same value: {TME1.value} ’’ kc ’’*KC23’’

g

TME1_discuss_truth
do TME1_discuss_truth1_1
do TME1_discuss_truth1_2 sem justify
do TME1_discuss_truth2 sem justify

g

TME1_discuss_truth1_1 sem elicit
say ’’[cont]Ok, before going on to the next step, let me ask you
a few questions about the definition of total mechanical energy.’’
say ’’[menu ’Yes, it can be negative|No, it cannot be negative|I
do not know|Not enough information’]In this problem, can the
system’s total mechanical energy at any given time be negative? ’’
if ’’No, it cannot be negative.’’ true
otherwise say ’’[cont]Actually ... the system’s total mechanical

290

energy at any given time cannot be negative.

g

’’

TME1_discuss_truth1_1 sem tell
say ’’[cont]Okay, before going on to the next step, let me tell
you a few more things about the definition of total mechanical
energy.’’
say ’’[cont]In this problem, the rock system’s total mechanical
energy at any given time cannot be negative. ’’ kc ’’*KC24’’

g

TME1_discuss_truth1_2 sem elicit
say ’’Please explain why.’’
if ’’Because TME = KE at any given time point, and the rock’s
kinetic energy cannot be negative.’’ true

kc ’’+KC24 +KC20’’

otherwise say ’’[cont]This is because the rock system’s total me
chanical energy equals the rock’s kinetic energy at any given
time point, and the rock’s kinetic energy cannot be negative.
’’ kc ’’-KC24 -KC20 *KC24 *KC20’’

g

TME1_discuss_truth1_2 sem tell
say ’’[cont]This is because the rock system’s total mechanical
energy equals the rock’s kinetic energy at any given time point
in this problem, therefore, the rock system’s total mechanical
energy at any given time cannot be negative. ’’ kc ’’*KC24 *KC20’’

g

TME1_discuss_truth2 sem elicit
say ’’Explain why the rock’s kinetic energy at any time point
cannot be negative?’’
if ’’Because KE={form_KE.value}, and neither the rock’s mass

291

nor the v^2 can be negative, then kinetic energy cannot be
negative. ’’ true kc ’’+KC20q +KC20’’
otherwise say ’’[cont]Because KE={form_KE.value}. Neither the ro
ck’s mass nor v^2 can be negative, then kinetic energy cannot
be negative. ’’ kc ’’-KC20 *KC20q *KC20’’

g

TME1_discuss_truth2 sem tell
say ’’[cont]The reason that the rock’s kinetic energy cannot be
negative is because KE={form_KE.value}. Neither the rock’s
mass nor v^2 can be negative.’’ kc ’’*KC20q *KC20’’

g TME1_discuss_qualitative
do TME1_discuss_qualitative_1
do TME1_discuss_qualitative_2 sem justify

g

TME1_discuss_qualitative_1 sem elicit

say ’’[menu ’Yes, we can|No, we cannot|I do not know|Not enough
information’]Given the system’s total mechanical energy is
equal to rock’s kinetic energy at any given time point
in this problem, can we say anything about the

ROCK’s

total mechanical energy, instead of the system’s total
mechanical energy?’’
if ’’No, we cannot.’’

true

kc ’’+KC24’’

otherwise say ’’[cont]Actually... No, we cannot.’’
kc ’’-KC24 *KC24’’

g

TME1_discuss_qualitative_1 sem tell

say ’’[cont]Although the system’s total mechanical energy is equal
to the rock’s kinetic energy at any given time point in this
problem, we cannot refer to the rock’s total mechanical energy.

292

’’kc ’’*KC24’’

g

TME1_discuss_qualitative_2 sem elicit

say ’’Why?’’
if ’’Because the TME always refers to a system. In the problem,
we only have one object (the rock) in the system. But it
still only refers to the system of the rock, not the
object rock.’’

true kc ’’+KC24’’

otherwise say ’’[cont]This is because the total mechanical energy
always refers to a system. In the problem, we only have one
object in the system. But total mechanical energy must
still only refers to the system of the rock, not the
object rock.’’ kc ’’-KC24 *KC24’’

g

TME1_discuss_qualitative_2 sem tell

say ’’[cont]This is because the total mechanical energy always
refers to a system. Here we only have one object in the
system, but total mechanical energy must still only
refers to the system of the rock, not the object
rock.’’ kc ’’*KC24’’

g CME01
do CME01_principle_selection
do CME01_argument_selection
do CME01_write_eqn
do CME01_solve_equation
say ’’[cont]We have finished solving the whole problem. Next let’s do
a little discussion.’’

293

g CME01_principle_selection
do CME01_principle_selection2
do CME01_principle_selection3

g CME01_principle_selection2 sem elicit
say ’’Which principle will help you calculate the work done on
the rock from T0 to T1? Please provide the name of the
principle, not an equation. ’’
if ’’conservation of mechanical energy for non-isolated systems’’
true kc ’’+KC28’’
otherwise do temp-CME01_principle_selection2 kc ’’-KC28’’

g temp-CME01_principle_selection2 sem tell
say ’’[cont]As we have mentioned before, there were two
principles from the textbook which apply to net
work: one is the definition of net work and
the other is conservation of mechanical energy
for non-isolated systems.’’ kc ’’*KC28 *KC18’’
say ’’[cont]The definition of the net work cannot be
applied to this problem because there is no
information given in the problem statement about
the forces on the rock. Therefore, we will apply the
remaining principle, namely, conservation of
mechanical energy for non-isolated systems.
We will start with the generic equation.’’
kc ’’*KC18 *KC28’’

g CME01_principle_selection2 sem tell

294

say ’’[cont]As we have mentioned before, there were two principl
es from the textbook which apply to net work: one is the
definition of net work and the other is conservation of mechanical
energy for non-isolated systems.’’ kc ’’*KC28 *KC18’’
say ’’[cont]The definition of the net work cannot be applied to
this problem because there is no information given in the
problem statement about the forces on the rock. Therefore, we will
apply the remaining principle, namely, conservation of
mechanical energy for non-isolated systems. We will start with the
generic equation.’’ kc ’’*KC18 *KC28’’

g CME01_principle_selection3 sem elicit
say ’’[ask ’form’] What is the generic equation for conservation
of mechanical energy for non-isolated systems?’’
if ’’{form_Wnet01}’’ true kc ’’+KC28q’’
otherwise say ’’[cont]The generic equation for conservation of
mechanical energy for non-isolated systems: {form_Wnet01}’’ kc ’’
-KC28q *KC28q’’

g CME01_principle_selection3 sem tell
say ’’[cont]The generic equation for conservation of mechanical
energy for non-isolated systems is: {form_Wnet01}’’ kc ’’*KC28q’’

g CME01_argument_selection sem elicit
say ’’What time points and bodies should we use when applying
the principle of conservation of mechanical energy for non-isola
ted systems?’’ answer ’’T0 and T1’’ answer ’’system’’
if ’’T0 and T1’’ true do-nomatch CME01_argument_miss_T kc ’’+KC28’’
if ’’system’’ true

do-nomatch CME01_argument_miss_object kc ’’+KC

28’’

295

if ’’rock’’ false do CME01_argument_rock_system
otherwise

do CME01_argument_miss_both kc ’’-KC28’’

g CME01_argument_miss_both_time
say ’’[cont]To apply conservation of mechanical energy for
non-isolated systems, we need two time points.’’
say ’’What time points should we use when applying
conservation of mechanical energy for non-isolated
systems?’’
if ’’T0 and T1’’ true

kc ’’+KC28’’

otherwise say ’’[cont]We should apply conservation of
mechanical energy for non-isolated systems at TWO
time points: T0 and T1.’’
kc ’’-KC28 *KC28’’

g CME01_argument_miss_T
say ’’[cont]We should apply conservation of mechanical
energy for non-isolated systems at TWO time points:
T0 and T1.’’ kc ’’*KC28’’

g CME01_argument_miss_object
say ’’What objects should you apply the conservation of
mechanical energy for non-isolated systems to?’’
opt sem already-rock-system
if ’’system’’ true kc ’’+KC28’’
if ’’rock’’ false do CME01_argument_rock_system
otherwise say ’’[cont]We have already defined the system
to be the rock. Therefore, we should apply
conservation of mechanical energy to the
rock system.’’ kc ’’-KC28 *KC28’’

296

g CME01_argument_rock_system
say ’’Does the rock refer to the object or the system?’’
sem already-rock-system
if

’’system ’’ true

otherwise

kc ’’+KC28’’

say ’’[cont]We always use the term the total

mechanical energy in relation to a system. In this
problem, the rock is the system. Therefore, we
should apply conservation of mechanical
energy to the rock system. ’’ kc ’’-KC28 *KC28’’

g CME01_argument_miss_both
do CME01_argument_miss_object
do CME01_argument_miss_both_time

g CME01_argument_selection

sem tell

say ’’[cont]We should apply conservation of mechanical energy
for non-isolated systems for the rock system on the time points:
T0 and T1.’’ kc ’’*KC28’’

g CME01_write_eqn sem elicit
say ’’[ask ’eqn’]Please write the equation for conservation of
mechanical energy for non-isolated systems for the rock system
on the time points: T0 and T1.’’
if ’’{eqn_CME01}’’ true say ’’[add ’eqn_CME01’]’’ kc ’’+KC28q’’
if ’’equation with substituted values’’ do bo_CME01_sub_write_eqn
otherwise do bo_CME01_write_eqn kc ’’-KC28q’’

g bo_CME01_sub_write_eqn

297

say ’’[cont]You substituted problem values into the equation.
Please use the variable labels instead.’’
do CME01_write_eqn

g bo_CME01_write_eqn
say ’’[add ’eqn_CME01’]’’
say ’’[cont]Let me just write the equation for conservation of
mechanical energy for the rock system between T0 and T1 for you:
{eqn_CME01}.’’ kc ’’*KC28q’’

g CME01_write_eqn sem tell
say ’’[add ’eqn_CME01’]’’
say ’’[cont]Let me just write the equation for conservation of
mechanical energy for the rock system between T0 and T1 for you:
{eqn_CME01}’’ kc ’’*KC28q’’

g CME01_solve_equation sem elicit
say ’’[cont]Since we already know the values for TME0 and TME1,
we can calculate the value of Wnet01 easily.’’
say ’’[ask ’num_units’]Please calculate the value of Wnet01.’’
if ’’{Wnet01.value}’’ true say ’’[add ’Wnet01’]’’ kc ’’+KC15’’
if ’’correct value, wrong/no unit’’

say ’’[cont]You entered the

correct value, but not the correct units. The correct units for
net works are J. ’’ kc ’’-KC15 *KC15’’
otherwise do bo_CME01_solve_equation
say ’’[add ’Wnet01’]’’

g bo_CME01_solve_equation
say ’’[cont]Based on: {eqn_CME01}, TME1={TME1.value}, and TME0= {
TME0.value}, we get: {TME0.value}+Wnet01={TME1.value}. That is:

298

Wnet01={TME1.value}-{TME0.value}. I calculate the value for
Wnet01 to be {Wnet01.value}. ’’

kc ’’*KC15’’

g CME01_solve_equation sem tell
say ’’[cont]Since we already know the values for TME0 and TME1,
we can calculate the value of Wnet01 easily.’’
say ’’[add ’Wnet01’]’’
say ’’[cont]Based on: {eqn_CME01}, TME1={TME1.value}, and TME0=
{TME0.value}, we get: {TME0.value}+Wnet01={TME1.value}. That
is: Wnet01={TME1.value}-{TME0.value}. I calculate the value for
Wnet01 to be {Wnet01.value}. ’’ kc ’’*KC15’’

g discuss_problem
do extra_step
say ’’[cont]The reason that we calculate v1 in this problem is to
practice the principle of the definition of kinetic energy. ’’
do PE_concept_justification
say ’’[cont]Next, we will change the place where the rock is and
discuss whether the values of some variables would also change.’’
do v1-ppd-discussion
do Wnet01-ppd-discussion
do inc_dec
say ’’[cont]Next, we will change the mass of the rock and discuss
whether the values of some variables would also change.’’
do inc_mass
do work_concept sem justify
do compare_path

g extra_step sem elicit

299

say ’’[menu ’KE0|v1|TME1|TME0|Wnet01|All are necessary|I do not know
’]While solving the problem, we calculated the variables in the
order of KE0, v1, TME0, TME1, and Wnet01. Are any of these
variable(s) not necessary to solve the problem?’’
if ’’v1’’ true
otherwise say ’’[cont]Actually ... v1 was not necessary for solving
the problem. ’’

g extra_step sem tell
say ’’[cont]While solving the problem, we calculated the variables
in the order of KE0, v1, TME0, TME1, and Wnet01. However, v1 was
not necessary for solving the problem. ’’

g PE_concept_justification
do PE_concept_justification_1
do PE_concept_justification_2 rand sem justify

g PE_concept_justification_1 sem elicit
say ’’[menu ’Yes, it is possible|No, it is not possible|I do not
know|Not enough information’]In this problem, we have selected the
rock as the system. Is it possible to define potential energy for
the rock system?’’
if ’’No, it is not possible’’ true kc ’’+KC21’’
otherwise say ’’[cont]Actually ... it is not possible to define
potential energy for the rock system.’’ kc ’’-KC21 *KC21’’

g PE_concept_justification_1 sem tell

300

say ’’[cont]In this problem, we have selected the rock as the system
and we *cannot* define a potential energy for the rock system.
’’ kc ’’*KC21’’

g PE_concept_justification_2 sem elicit
say ’’Why not?’’
if ’’Potential energy can be defined only for a system which contains
two or more objects but here we only have only one object as the
system (the rock).’’ true kc ’’+KC21’’
otherwise say ’’[cont]This is because potential energy is energy
associated with the configuration of two or more objects. Thus
potential energy can be defined only for a system which contains
two or more objects. Here we only have the rock to select as the
system and thus we cannot define a potential energy for the rock.’’
kc ’’-KC21 *KC21’’

g PE_concept_justification_2 sem tell
say ’’[cont]This is because potential energy is an energy associated
with the configuration of two or more objects. Thus potential
energy can be defined only for a system which contains two
or more

objects.’’ kc ’’*KC21’’

say ’’[cont]In this problem, we only have the rock to select as the
system. Therefore, we cannot define a potential energy for the
rock. ’’

g v1-ppd-discussion
do v1-ppd-discussion-1
do v1-ppd-discussion-2 rand sem justify

301

g v1-ppd-discussion-1 sem elicit
say ’’[menu ’increase|decrease|same|not enough information|I do not
know’]Suppose the 0.6 kg rock is put on the Earth and the rock’s
kinetic energy at point B is still 7.5 J, what will happen to
the rock’s magnitude of velocity at the point B?’’
if ’’same’’ true kc ’’+KC20’’
if ’’{v1.mag}’’ true kc ’’+KC20 +KC31’’
otherwise say ’’[cont]Actually ... the rock’s magnitude of velocity
should be same, still be {v1.mag}. ’’

kc ’’-KC20 *KC20 *KC31’’

g v1-ppd-discussion-1 sem tell
say ’’[cont]Suppose the rock is put on the Earth and the rock’s
kinetic energy at point B is still 7.5 J, then the rock’s magnitude
of velocity at the point B is still {v1.mag}.’’

kc ’’*KC20 *KC31’’

g v1-ppd-discussion-2 sem elicit
say ’’Why?’’
if ’’Because if the rock’s KE at point B and the rock’s mass stay
the same, the rock’s magnitude of velocity at the point B is the
same.’’ true kc ’’+KC20’’
otherwise say ’’[cont]Because if the rock’s kinetic energy at point
B and the rock’s mass stay the same, then according to KE={form_
KE} the rock’s magnitude of velocity at the point B is the same.’’
kc ’’-KC20 *KC20q *KC20’’

g v1-ppd-discussion-2 sem tell
say ’’[cont]Because the kinetic energy stays the same and the rock’
s mass stays the same, then according to KE={form_KE} the rock’s
magnitude of velocity at the point B is same.’’ kc ’’*KC20q *KC20’’

302

g Wnet01-ppd-discussion
do Wnet01-ppd-discussion-1
do Wnet01-ppd-discussion-2 rand sem justify

g Wnet01-ppd-discussion-1 sem elicit
say ’’[menu ’increase|decrease|same|not enough information|I do not
know.’]Suppose the rock is on the Earth and everything in the
problem statement is kept the same.

What will happen to net work

done on the rock if we still choose the rock alone as the system?’’

if ’’same’’ true kc ’’+KC28 +KC24’’
otherwise say ’’[cont]Actually ... It should be the same:{Wnet01.va
lue}.’’ kc ’’-KC28 -KC24 *KC28 *KC24 *KC15’’

g Wnet01-ppd-discussion-2 sem elicit
say ’’Why?’’ rand
if ’’Since the rock is the only object in the system. The TME = KE
any given time. The rock’s KE0 and KE1 are still the same as
those when the rock is in space. Therefore, the TME1 and TME2 are the
same as those when the rock is in space. Therefore, the Wnet01 i
s still the same as when the rock is in space. ’’ true

kc ’’+KC28

+KC24q’’
otherwise do tepm-Wnet01-ppd-discussion

kc ’’-KC28 -KC24’’

g tepm-Wnet01-ppd-discussion
say ’’[cont]Let me explain it to you step by step. ’’
say ’’[cont]The rock is still the only object in the system and thus
the systems’ total mechanical energy is still equal to its
kinetic energy at any given time. ’’ kc ’’*KC24’’

303

say ’’[cont]The rock’s kinetic energy at T0 and T1 are still t
he same as those when the rock is in space because m, v0, an
d KE1 is kept the same. Therefore, the system’s TME1 and TME
2 are the same as those when the rock is in space. ’’ kc ’’*KC20’’
say ’’[cont]As a result, the net work done on the rock from T0
to T1 is still the same as when the rock is in space. ’’
kc ’’*KC28’’

g Wnet01-ppd-discussion-1 sem tell
say ’’[cont]Suppose the rock is on the Earth and everything in the
problem statement is kept the same. If we still choose the rock a
lone as the system, then the net work done on the rock from T0 to
T1 is still {Wnet01.value}. ’’

kc ’’*KC28 *KC24 *KC15’’

g Wnet01-ppd-discussion-2 sem tell
say ’’[cont]Let me explain it to you step by step. ’’
say ’’[cont]Since the rock is still the only object in the system a
nd thus the systems’ total mechanical energy is still equal to
its kinetic energy at any given time. ’’ kc ’’*KC24’’
say ’’[cont]The rock’s kinetic energy at T0 and T1 are still the
same as those when the rock is in space because m, v0, and KE1 is
kept the same. Therefore, the system’s TME1 and TME2 are the same
as those when the rock is in space. ’’ kc ’’*KC20’’
say ’’[cont]As a result, the net work done on the rock from T0 to T1
is still the same as when the rock is in space. ’’ kc ’’*KC28’’

g inc_dec

304

say ’’[ask ’num_units’]Let’s go back to the situation where rock is
in space. Look at the variable window, tell me what is the
magnitude of v0?’’
if ’’{v0.mag}’’ true kc ’’+KC31’’
if ’’correct value, wrong/no unit’’

say ’’[cont]You entered the

correct value, but not the correct units. The correct units for
velocity are m/s. ’’

kc ’’-KC31 *KC31’’

otherwise say ’’[cont]We can also see from the variable window that
v0 has a value of 2.0 m/s.’’ kc ’’*KC31’’
say ’’[ask ’num_units’]What is the magnitude of v1?’’
if ’’{v1.mag}’’ true kc ’’+KC31’’
if ’’correct value, wrong/no unit’’

say ’’[cont]You entered the

correct value, but not the correct units. The correct units for
velocity are m/s. ’’

kc ’’-KC31 *KC31’’

otherwise say ’’[cont]We can also see from the variable window that
v1 has a value of 5.0 m/s.’’

kc ’’*KC31’’

do v1_discuss_qualitative3
say ’’[ask ’num_units’]Look at the variable window, tell me what is
the value of the rock’s kinetic energy at T0?’’
if ’’{KE0.value}’’ true kc ’’+KC23’’
if ’’correct value, wrong/no unit’’

say ’’[cont]You entered the

correct value, but not the correct units. The correct units for
kinetic energy are J. ’’ kc ’’-KC23 *KC23’’
otherwise say ’’[cont]We can also see from the variable window that
KE0 has a value of {KE0.value}.’’ kc ’’*KC23’’
say ’’[ask ’num_units’]What is the value of KE1?’’
if ’’{KE1.value}’’ true kc ’’+KC23’’
if ’’correct value, wrong/no unit’’

say ’’[cont]You entered the

correct value, but not the correct units. The correct units for
kinetic energy are J. ’’ kc ’’-KC23 *KC23’’

305

otherwise say ’’[cont]We can also see from the variable window that
KE1 has a value of {KE1.value}.’’ kc ’’*KC23’’
do compare_A_B
do compare_C_A
do compare_C_B

g compare_A_B
do compare_A_B_1
do compare_A_B_2 sem justify

g compare_A_B_1 sem elicit
say ’’[menu ’increase|decrease|same|not enough information|I do not
know.’]Compared with point A, what can you say about the kinetic
energy of the rock at point B?’’
if ’’Increases’’

true

kc ’’+KC20’’

otherwise say ’’[cont]Actually ... It is increased.’’ kc ’’-KC20 *KC2
0’’

g compare_A_B_1 sem tell
say ’’[cont]Compared with point A, the kinetic energy of the rock
at point B is increased.’’ kc ’’*KC20’’

g compare_A_B_2 sem elicit
say ’’Please try to explain why.’’
if ’’Because {KE1.value} is bigger than {KE0.value}, therefore, it
is increased. ’’ true kc ’’+KC20 +KC23’’
otherwise say ’’[cont]This is a bit of a trick question. The problem
doesn’t say why the kinetic energy increase, but instead just
gives initial conditions (v0=2 m/s) and final conditions (KE1= 7.5
J) that imply that the kinetic energy has increased because {KE1

306

.value} is bigger than {KE0.value}.’’ kc ’’*KC23 *KC31’’

g compare_A_B_2 sem tell
say ’’[cont]The problem doesn’t say why the kinetic energy increase
but we can infer it from the fact that KE1= {KE1.value} is
bigger than the value of KE0: {KE0.value}.’’ kc ’’*KC23’’

g v1_discuss_qualitative3 sem elicit
say ’’[menu ’increase|decrease|same|not enough information|I do
not know’]What can we infer about that the rock’s magnitude of
velocity during the ENTIRE period from T0 to T1 given that
the magnitude of v1 (5.0 m/s) is bigger than the magnitude of v0
(2.0 m/s)?’’ kc ’’*KC31’’
if ’’not enough information.’’ true kc ’’+KC20’’
otherwise say ’’[cont]We do not have enough information to
deduce how the rock’s magnitude of velocity changes during the
process from T0 to T1. ’’ kc ’’-KC20 *KC20’’

g v1_discuss_qualitative3 sem tell
say ’’[cont]Although the magnitude of v1 (5.0 m/s) is bigger
than the magnitude of v0 (2.0 m/s), we *cannot* infer that the
rock’s magnitude of velocity is increasing during the ENTIRE
period from T0 to T1. ’’ kc ’’*KC31 *KC20’’

g compare_C_A
do compare_C_A_1
do compare_C_A_2 sem justify

g compare_C_A_1 sem elicit
say ’’[menu ’increase|decrease|same|not enough information|I do not

307

know.’]Suppose there is point C on the path of the rock between
A to B. Compared with point A, what can you say about the kinetic
energy of the rock at point C?’’
if ’’Not enough information.’’ true kc ’’+KC20’’
otherwise say ’’[cont]Actually ... We do not have enough information
to say about the kinetic energy of the rock at point C. ’’ kc ’’KC20 *KC20’’

g compare_C_A_1 sem tell
say ’’[cont]Suppose there is point C on the path of the rock
between A to B.

We do not have enough information to say anything

about the kinetic energy of the rock at point C compared with point A
. ’’ kc ’’*KC20’’

g compare_C_A_2 sem elicit
say ’’Please explain why.’’
if ’’We do not know whether the rock’s magnitude of velocity at C i
s bigger than its magnitude of velocity at A or not.’’ true
kc ’’+KC20’’
otherwise say ’’[cont]We do not know whether the rock’s magnitude
of velocity at C is bigger than its magnitude of velocity at A or
not. It is possible that the rock continuously increases its
magnitude of velocity from A to C then to B. It is also possible
its

magnitude of velocity decreases from A to C and then

increases magnitude of velocity from C to B. It is even possible
that it does not change its magnitude of velocity from A to C
and then increase its magnitude of velocity from C to B.
Therefore, we do not have enough information to say about the

308

relationship between the kinetic energy of the rock at point
C and at point A.’’ kc ’’-KC20 *KC20’’

g compare_C_A_2 sem tell
say ’’[cont]This is because we do not know whether the rock’s
magnitude of velocity at C is bigger than its magnitude of
velocity at A or not. It is possible that the rock continuously
increases its magnitude of velocity from A to C then to B. It is
also possible its magnitude of velocity decreases from A to C and
then increases from C to B. It is even possible that it does not
change its magnitude of velocity from A to C and then increases
its magnitude of velocity from C to B.’’ kc ’’*KC20’’

g compare_C_B
do compare_C_B_1
do compare_C_B_2 sem justify

g compare_C_B_1 sem elicit
say ’’[menu ’increase|decrease|same|not enough information|I do not
know.’]Compared with point B, What can you say about the kinetic
energy of the rock at point C?’’
if ’’Not enough information.’’ true kc ’’+KC20’’
otherwise say ’’[cont]Actually ... We do not have enough information
to say about the kinetic energy of the rock at point C compared
with point B either. ’’ kc ’’-KC20 *KC20’’

309

g compare_C_B_1 sem tell
say ’’[cont]Similarly, we do not have enough information to say any
thing about the kinetic energy of the rock at point C compared
with point B either.’’ kc ’’*KC20’’

g compare_C_B_2 sem elicit
say ’’Please explain why.’’
if ’’We do not have enough information about the relationship
between the rock’s magnitude of velocity at point C and at B.’’
kc ’’+KC20’’
otherwise say ’’[cont]The answer is similar to reasons that we do
not have enough information about the relationship between the
rock’s magnitude of velocity at point A and point C, it is
because we do not know whether the magnitude of the rock’s
magnitude of velocity at C is bigger than its magnitude of
velocity at B or not.

That is, we do not have enough information

about the relationship between the rock’s magnitude of velocity a
t point C and at B.’’ kc ’’-KC20 *KC20’’

g compare_C_B_2 sem tell
say ’’[cont]Because we do not know whether the rock’s magnitude of
velocity at C is bigger than its magnitude of velocity at B or not.
That is, we do not have enough information about the relationship
between the rock’s magnitude of velocity at point C and at B. ’’
kc ’’*KC20’’

g work_concept

310

do work_concept_whether
do work_concept_justification rand sem justify

g work_concept_whether sem elicit
say ’’[menu ’Yes, we can|No, we cannot|not enough information|
I do not know.’]If we already know that the work done on the rock
by f orce F is 9 J and we also know that the rock is moving in the
same direction as F for 3 m in a straight line, can you infer the
magnitude of the force F?’’ kc ’’*KC15’’
if ’’No, we cannot.’’ true
otherwise say ’’[cont]No, we cannot infer anything about the
magnitude of the force F. ’’

g work_concept_whether sem tell
say ’’[cont]If we already know that the work done on the rock by
force F is 9 J and we also know that the rock is moving in the
same direction as F for 3 m in a straight line,
we *cannot* infer the magnitude of the force F.’’ kc ’’*KC14 *KC15’’

g work_concept_justification

sem elicit

say ’’Why not?’’
if ’’Because we do not know whether F is a constant force or not.’’
true kc ’’+KC14’’
otherwise say ’’[cont]It is because we do not know whether F is a
constant force or not. If F is a constant force, we can apply
W=F* d to get that F = W/d= (9 J)/(3 m) = 3 N;

however, in most

cases, F is not constant force. Therefore, we *cannot* apply
W=F*d to

get the magnitude of the F.’’

kc ’’-KC14 *KC14q *KC14 *KC15’’

311

g work_concept_justification

sem tell

say ’’[cont]It is because we do not know whether F is a constant
force or not. Only if F is a constant force, we can apply W=F*d
to get that F = W/d= (9 J)/(3 m) = 3 N; however, in most cases,
F is

not constant force. Therefore, we *cannot* apply

W=F*d to

get the magnitude of the F.’’ kc ’’*KC14 *KC14q *KC15’’

g inc_mass
do inc_mass1_1
do inc_mass1_2 rand sem justify
do inc_mass2_1
do inc_mass2_2 rand sem justify
do inc_mass3_1
do inc_mass3_2 rand sem justify

g inc_mass1_1 sem elicit
say ’’[menu ’increase|decrease|same|not enough information|
I do not

know.’]Suppose the rock’s mass increased from 0.6 kg to

1 kg and the rock’s kinetic energy at point B was still 7.5 J,
what would happen to the rock’s magnitude of velocity at point B?’’
kc ’’*KC23’’
if ’’decrease’’ true

kc ’’+KC20’’

otherwise say ’’[cont]Actually ... the rock’s magnitude of velocity
decreases at point B. ’’

kc ’’-KC20 *KC20’’

g inc_mass1_1 sem tell
say ’’[cont]Suppose that the rock’s mass increased from 0.6 kg to 1

312

kg and the rock’s kinetic energy at point B was still 7.5 J,
the rock’s magnitude of velocity at point B would decrease. ’’
kc ’’*K C20 *KC23’’

g inc_mass1_2 sem elicit
say ’’Why?’’
if ’’It decreases because if the rock’s KE at point B stays same,
{KE1.value} and the rock’s mass increased from 0.6 kg to 1 kg.
According to KE={form_KE.value},
have to decrease.’’ true

the magnitude of velocity would

kc ’’+KC20q +KC20 +KC23’’

otherwise say ’’[cont]It decreases because we are assuming that the
rock’s kinetic energy at point B stays same, {KE1.value} but the
rock’s mass increased from 0.6 kg to 1 kg. According to KE=
{form _KE.value}, the magnitude of velocity would have to
decrease.’’

k c ’’-KC20 *KC20q *KC20 *KC23’’

g inc_mass1_2 sem tell
say ’’[cont]It is because we are assuming that the rock’s kinetic
energy at point B stays same, {KE1.value}, and if the rock’s
mass increases, according to KE={form_KE}, then the magnitude
of rock’s magnitude of velocity has to decrease.’’
kc ’’*KC20 *KC20q *KC23’’

g inc_mass2_1 sem elicit
say ’’[menu ’increase|decrease|same|not enough information|
I do not

know.’]Suppose the rock’s mass were 1 kg instead

of 0.6 kg and y et the rock’s magnitude of velocity at point
A was still 2.0 m/s.

What will happen to the rock’s kinetic

energy at point A?’’ kc ’’*KC31’’

313

if ’’Increase’’ true kc ’’+KC20’’
otherwise say ’’[cont]Actually ... the rock’s kinetic energy at
point A increases.’’

kc ’’-KC20 *KC20’’

g inc_mass2_1 sem tell
say ’’[cont]Suppose the rock still has a velocity of magnitude of
2.0 m/s at point A but the rock’s mass increased from 0.6 kg
to 1 kg, then the rock’s kinetic energy at point A should
increase.’’ k c ’’*KC31 *KC20’’

g inc_mass2_2 sem elicit
say ’’Why?’’ rand
if ’’Because the magnitude of velocity stays the same and the
mass increases, therefore the kinetic energy at point A
increases.’’ true

kc ’’+KC20’’

otherwise say ’’[cont]Because the magnitude of rock magnitude
of velocity is still 2.0 m/s at point A and the mass of
the rock increases, according to KE={form_KE.value} the
kinetic energy of the rock at point A increases.’’
kc ’’-KC20 *KC20 *KC20q *KC31’’

g inc_mass2_2 sem tell
say ’’[cont]Because the rock’s magnitufe of velocity is still
2.0 m /s at point A and the mass of the rock increases,
according to KE ={form_KE.value} the kinetic energy of
the rock at point A increases.’’ kc ’’*KC20 *KC20q *KC31’’

g inc_mass3_1 sem elicit
say ’’[menu ’increase|decrease|same|not enough information|I do not
know.’]Finally, if the rock’s mass were 1 kg instead of 0.6 kg

314

and the rock’s

magnitufe of velocity

at point A was still

2.0 m/ s and the rock’s kinetic energy at point B was still
7.5 J,

what

will happen to the net work done on the rock

as it moves from A to B?’’ kc ’’*KC31 *KC23’’
if ’’Decrease’’ true

kc ’’+KC28 +KC24 +KC20’’

otherwise say ’’[cont]Actually ... the net work done on the rock as
it moves from A to B will *decrease*. ’’
kc ’’-KC28 -KC24 -KC20 *K C28 *KC24 *KC20’’

g inc_mass3_2 sem elicit
say ’’Why?’’
if ’’Because the rock’s kinetic energy

at point A increases

and it s kinetic energy at point B is same. Therefore,
the net work done on the rock from A to B has to decrease.’’
true kc ’’+KC28 +KC24 +KC20’’
otherwise do temp-inc_mass3 kc ’’-KC28 -KC24 -KC20’’

g temp-inc_mass3
say ’’[cont]This is a tricky question. So I will explain it to
you step by step.’’
say ’’[cont]Given that the rock still has a velocity of
magnitude 2.0 m/s at point A and a kinetic energy of 7.50
J at point B, the rock’s kinetic energy at point A
increases when the rock’s mass increases from 0.6 kg
to 1 kg. The new value is 0.5* 1 kg * ({v0.mag})^2 =
2 J while the old value is 0.5* 0.6kg * ({v0.mag})^2
= {KE0.value}’’ kc ’’*KC20 *KC20q *KC23 *KC31’’
say

’’[cont]In other words, the system’s new TME1 is the same
as the old TME1, which is {TME1.value} but the system’s

315

new TME0 is 2 J, which is bigger than the old TME0,
which is

{TME0.value}.’’

kc ’’*KC24 *KC23’’
say ’’[cont] According to {eqn_CME01},

we have Wnet01= TME1 -

TME0. When the rock’s mass increased from 0.6 kg to
1 kg, the TME0 increased but TME1 remained the same,
therefore, Wnet01 must decrease. That is, the net work
done on the rock as it moves from A to B *decreases*
as the rock’s mass increases from 0.6 kg to 1 kg.’’
kc ’’*KC28q *KC28’’

g inc_mass3_1 sem tell
say ’’[cont]Finally, if the rock’s mass were 1 kg instead of 0.6 kg
and the rock’s magnitude of velocity at point A was still
2.0 m/ s and the rock’s kinetic energy at point B was still
7.5 J, the net work done on the rock as it moves from A to B
would decrease.’’
kc ’’*KC28 *KC24 *KC20 *KC23 *KC31’’

g inc_mass3_2 sem tell
say ’’[cont]Next, I will explain it to you step by step. The rock’s
kinetic energy at point A increases when the rock’s mass increases
from 0.6 kg to 1 kg. The new value is 0.5* 1 kg * ({v0.mag})^2
while the old value is

= 2 J

0.5* 0.6 kg * ({v0.mag})^2 = {KE0. value}.

’’ kc ’’*KC20 *KC20q *KC23 *KC31’’
say ’’[cont]In other words, the system’s new TME1 is the same as
the old TME1, which is {TME1.value} but the system’s new TME0 is
2 J, which is bigger than the old TME0, which is
kc

{TME0.value}.’’

’’*KC24 *KC23’’

say ’’[cont] According to {eqn_CME01},

316

we have Wnet01= TME1 -TME0.

When the rock’s mass increased from 0.6 kg to 1 kg, the TME0
increased but TME1 remained the same, therefore, Wnet01 must
decrease. That is, the net work done on the rock as it moves
from A to B

*decreases* as the rock’s mass increases from 0.6

kg to 1 kg.’’

kc ’’*KC28q *KC28’’

g compare_path
do compare_path_1
do compare_path_2 rand sem justify

g compare_path_1 sem elicit
say ’’[menu ’Yes, it would affect|No, it would not affect|
I do not know|not enough information|I do not know.’]Let’s go
back to the original situation. The problem does not give any
hint about the path taken by the rock. Would taking different
paths affect our final answer of the net work done on the rock
from A to B? ’’
if ’’No, it would not affect.’’ true kc ’’+KC28 +KC24 +KC20’’
otherwise say ’’[cont]Actually ... taking different paths would not
affect our final answer of the net work done on the rock
from A to B. ’’ kc ’’-KC28 -KC24 -KC20 *KC28 *KC24 *KC20’’

g compare_path_1 sem tell
say ’’[cont]Let’s go back to the original situation. The problem does
not give any hint about the path taken by the rock. However,
taking different paths would not affect our final answer of the
net work done on the rock from A to B. ’’
kc ’’*KC28 *KC24 *KC20’’

317

g compare_path_2 sem elicit
say ’’Why? ’’
if ’’As long as the rock’s mass, the rock’s magnitude of v0 and v1 are
still the same, then Wnet01 is still {Wnet01.value}. ’’
true kc ’’+KC28 +KC24 +KC20 +KC15’’
otherwise say ’’[cont]This is because the path that the rock took
from point A to point B was not involved in the problem solving,
only the rock’s mass, the rock’s magnitude of velocity at T0 and
T1 were involved. Thus, as long as they are still the same,
Wnet01 is still {Wnet01.value}. ’’
kc ’’-KC28 -KC24 -KC20 *KC28 *KC24 *KC 20 *KC15’’

g compare_path_2 sem tell
say ‘‘[cont]This is because the path that the rock took from point
A to point B was not involved in the problem solving, only the
rock’s mass, the rock’s magnitude of velocity at T0 and T1 were
involved. Thus, as long as they are still the same, Wnet01 is
still {Wnet01.value}.

’’ kc ’’*KC28 *KC24 *KC20 *KC15’’

318

APPENDIX J

STUDY 2: TUTORIAL FEATURES

319

320

30
T
tell
nil
nil
nil
0
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil

29
S
nil
0.03
0.04
0
0
0
0.5
1
0.12
0.28
0
0.5
0.62
0.8
0.57
0.83
0.26
0.75
0.51
0.31
0.11

0
0.04
0
0
0
0.5
1
0.12
0.28
0.08
0.49
0.62
0.8
0.57
0.83
0.26
0.75
0.51
0.31
0.11

31
S
nil
nil
nil
nil
0
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil

32
T
elicit
0
0.04
0
0
0
0.5
1
0.15
0.3
0
0.51
0.64
0.8
0.56
0.84
0.28
0.77
0.49
0.3
0.11

33
S
nil
nil
nil
nil
0
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil

34
T
elicit skippable
0
0.04
0
0
0
0.5
1
0.15
0.3
0
0.52
0.63
0.8
0.56
0.84
0.3
0.78
0.51
0.31
0.11

35
S
nil

b

The number of the decision orders in the tutorial dialogue on KC20
“S:” refers to the current state when the tutor make the decision while “T:” means that it is tutor decision turn
c
The action the tutor decided to take

a

Ordera
Speakerb
TMOVEc
Features:
v5durationBetweenDecisionT
TimeInSessionT
TimeBetweenSessionT
Rewards
v12EarlyTrainingPS
v13SimpleProblemPS
v14DuringWalkThroughPS
v15nKCsPS
v16nKCsSessionPS
v27tellsSinceElicitA
v28pctElicitA
v31pctTellsKCSessionA
v32pctCorrectPM
v33pctOverallCorrectPM
v34pctCorrectSessionPM
v40nCorrectKCPM
v42pctOverallCorrectKCPM
v44pctCorrectKCPM
v45pctCorrectKCSessionPM
v46nIncorrectKCPM
nil
nil
nil
0
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil

36
T
tell
0
0.04
0
0
0
0.5
1
0.15
0.3
0.08
0.51
0.63
0.8
0.56
0.84
0.3
0.78
0.51
0.31
0.11

37
S
nil

Table J1: Representing Sample Dialogue in Table 5.2 Using the 18 Feature Variables From Study 2

APPENDIX K

STUDY 2: DICHGAIN TUTORIAL TACTICS

Policy: KC-general policy on Justify/Skip-Justify
Features: [18]
ECR: 0.572656
ECR Lower Bound: 0.19178
Mapping: [.3060]
States: [’0 skip’,’1 else’]

Policy: KC-general policy on Elicit/Tell
Features: [5,9,17,30]
ECR: 0.974232
ECR Lower Bound: 0.262842
Mapping: [11.0000,0.5,.5063,.6459]
States: [’1:1:1:1 tell’,’0:0:1:1 elicit’,’0:1:1:0 tell’,’0:0:0:0 elicit’,’1:1:1:0 tell’,’0:1:0:0
elicit’,’1:0:1:0 tell’,’1:1:0:0 tell’,’1:1:0:1 tell’,’0:1:0:1 tell’,’1:0:0:0 elicit’,’1:0:1:1
elicit’,’1:0:0:1 else’,’0:1:1:1 elicit’,’0:0:0:1 elicit’,’0:0:1:0 elicit’]

321

Policy: 1a etb
Features: [10,13,16,29]
ECR: 20.196
ECR Lower Bound: 5.18551
Mapping: [None,162727.8586,.4961,.7333]
States: [’COMP:0:0:0

elicit’,’MED:1:0:0

elicit’,’COMP:1:0:0

tell’,’MED:1:1:0

tell’,’COMP:0:0:1

tell’,’COMP:1:1:0

tell’,’COMP:1:0:1

tell’,’MED:0:0:0

elicit’,’MED:1:0:1

elicit’,’COMP:0:1:0

elicit’,’MED:0:0:1

elicit’,’COMP:0:1:1

elicit’,’MED:1:1:1

tell’,’COMP:1:1:1

elicit’,’MED:0::1

elicit’,’MED:0:1:1

elicit’,’MED:0:1:0 elicit’,’MED:0::0 elicit’]

a

KC number. In this case, it is a KC-specific policy on KC1. Same below
Type of tutorial decisions. “et” refers to Elicit/Tell decisions while “skip” refers to Justify/SkipJustify decisions
b

Policy: 3et
Features: [9,12,16,17]
ECR: -25.2687
ECR Lower Bound: -45.363
Mapping: [0.5,1575.5233,.4980,.5558]
States: [’0:0:1:1 elicit’,’0:1:1:0 elicit’,’0:0:0:0 elicit’,’0:1:0:0 elicit’,’0:0::1 tell’,’0:1:0:1
tell’,’0:1:1:1 elicit’,’0:0:0:1 elicit’,’0:0:1:0 tell’]

Policy: 5et
Features: [9,13,15,25]
ECR: -4.57221
ECR Lower Bound: -13.6603
Mapping: [0.5,80963.9211,1.0000,2.0000]
States: [’1:1:1:1 elicit’,’0:0:1:1 elicit’,’0:1:1:0 tell’,’0:0:0:0 elicit’,’1:1:1:0 elicit’,’0:1:0:0
tell’,’1:0:1:0 elicit’,’1:1:0:0 elicit’,’1:1:0:1 elicit’,’0:1:0:1 elicit’,’1:0:0:0 elicit’,’1:0:1:1
elicit’,’1:0:0:1 elicit’,’0:1:1:1 elicit’,’0:0:0:1 tell’,’0:0:1:0 elicit’]

322

Policy: 9et
Features: [12]
ECR: 63.9232
ECR Lower Bound: 58.7516
Mapping: [1056.5808]
States: [’0 tell’, ’1 else’]

Policy: 12skip
Features: [5,9,15,17]
ECR: 0
ECR Lower Bound: 0
Mapping: [65.0000,0.5,1.0000,.4422]
States: [’1:1:1:1

noskip’,’0:1:1:0

noskip’,’1:1:1:0

noskip’,’0:1:0:0

noskip’,’1:1:0:0

noskip’,’1:1:0:1 noskip’,’0:1:0:1 noskip’,’0:1:1:1 noskip’]

Policy: 12et
Features: [5,9,15,17]
ECR: 0
ECR Lower Bound: 0
Mapping: [65.0000,0.5,1.0000,.4422]
States: [’1:1:1:1 elicit’,’0:1:1:0 elicit’,’1:1:1:0 elicit’,’0:1:0:0 elicit’,’1:1:0:0 elicit’,’1:1:0:1
elicit’,’0:1:0:1 elicit’,’0:1:1:1 elicit’]

323

Policy: 13et
Features: [5,9,15,28]
ECR: 14.5194
ECR Lower Bound: 1.08511
Mapping: [166.0000,0.5,1.0000,5.0000]
States: [’1:1:1:1 elicit’,’0:0:1:1 elicit’,’0:1:1:0 elicit’,’0:0:0:0 elicit’,’1:1:1:0 elicit’,’0:1:0:0
elicit’,’1:0:1:0 tell’,’1:1:0:0 elicit’,’1:1:0:1 elicit’,’0:1:0:1 elicit’,’1:0:0:0 elicit’,’1:0:1:1
tell’,’1:0:0:1 tell’,’0:1:1:1 elicit’,’0:0:0:1 elicit’,’0:0:1:0 elicit’]

Policy: 14skip
Features: [5,9,15,18]
ECR: 17.6695
ECR Lower Bound: 14.503
Mapping: [94.0000,0.5,1.0000,.3517]
States: [’1:1:1:1 else’,’0:0:1:1 else’,’0:1:1:0 else’,’0:0:0:0 noskip’,’1:1:1:0 else’,’0:1:0:0
else’,’1:0:1:0 noskip’,’1:1:0:0 else’,’1:1:0:1 else’,’0:1:0:1 else’,’1:0:0:0 noskip’,’1:0:1:1
noskip’,’1:0:0:1 else’,’0:1:1:1 else’,’0:0:1:0 noskip’]

Policy: 14et
Features: [9]
ECR: 54.151
ECR Lower Bound: 47.8956
Mapping: [0.5]
States: [’0 tell’,’1 elicit’]

324

Policy: 15et
Features: [5,9,15,17]
ECR: 0
ECR Lower Bound: 0
Mapping: [344.0000,0.5,1.0000,.4565]
States: [’1:1:1:1 elicit’,’0:0:1:1 elicit’,’0:1:1:0 elicit’,’0:0:0:0 tell’,’1:1:1:0 elicit’,’0:1:0:0
elicit’,’1:0:1:0 elicit’,’1:1:0:0 elicit’,’1:1:0:1 elicit’,’0:1:0:1 elicit’,’1:0:0:0 elicit’,’1:0:1:1
elicit’,’1:0:0:1 elicit’,’0:1:1:1 elicit’,’0:0:0:1 elicit’,’0:0:1:0 elicit’]

Policy: 17et
Features: [5,9,15,17]
ECR: 3.27521
ECR Lower Bound: 0.574497
Mapping: [88.5000,0.5,1.0000,.5610]
States: [’0:0:1:1 else’,’0:0:0:0 elicit’,’1:0:1:0 elicit’,’1:0:0:0 else’,’1:0:1:1 elicit’,’1:0:0:1
else’,’0:0:0:1 elicit’,’0:0:1:0 else’]

Policy: 18et
Features: [5,9,15,17]
ECR: 0
ECR Lower Bound: 0
Mapping: [1.0000,0.5,1.0000,.4578]
States: [’1:1:1:1 elicit’,’0:1:1:0 elicit’,’1:1:1:0 elicit’,’1:1:0:0 elicit’,’1:1:0:1 elicit’,’0:1:1:1
elicit’]

325

Policy: 20skip
Features: [5,14,15,26]
ECR: 4.28925
ECR Lower Bound: 0.376654
Mapping: [52.0000,0.5,1.0000,.5405]
States: [’1:1:1:1 skip’,’0:0:1:1 else’,’0:1:1:0 else’,’0:0:0:0 noskip’,’1:1:1:0 noskip’,’0:1:0:0
else’,’1:0:1:0 else’,’1:1:0:0 else’,’1:1:0:1 skip’,’0:1:0:1 else’,’1:0:0:0 skip’,’1:0:1:1
else’,’1:0:0:1 skip’,’0:1:1:1 else’,’0:0:0:1 else’,’0:0:1:0 noskip’]

Policy: 20et
Features: [5,10,15,26]
ECR: 4.80774
ECR Lower Bound: 0.753813
Mapping: [52.0000,None,1.0000,.5405]
States: [’0:SIM:1:0
elicit’,’1:MED:0:0
elicit’,’1:COMP:0:0
elicit’,’0:COMP:1:1

tell’,’1:SIM:1:1
elicit’,’1:COMP:1:0
else’,’1:MED:1:1
elicit’,’1:SIM:1:0

tell’,’1:MED:0:1

tell’,’0:COMP:0:0

elicit’,’0:MED:1:0

elicit’,’1:MED:1:0

tell’,’0:SIM:0:0

tell’,’1:COMP:1:1

elicit’,’0:MED:0:0

elicit’,’0:COMP:1:0

else’,’1:SIM:0:0 tell’,’0:SIM:1:1 tell’,’0:MED:1:1 tell’,’0:SIM:0:1 tell’,’0:MED:0:1
tell’,’0:COMP:0:1 tell’,’1:SIM:0:1 tell’,’1:COMP:0:1 else’]

326

Policy: 21skip
Features: [5,10,15,18]
ECR: 17.958
ECR Lower Bound: 12.1813
Mapping: [50.0000,None,0.0001,.2990]
States: [’1:MED:0:1
else’,’0:MED:1:0
else’,’1:COMP:1:1

else’,’0:COMP:0:0
else’,’1:MED:1:0

else’,’1:MED:0:0

else’,’1:COMP:1:0

noskip’,’1:COMP:0:0

noskip’,’1:MED:1:1

skip’,’0:COMP:1:1

skip’,’0:MED:0:0

else’,’0:COMP:1:0

skip’,’0:MED:1:1 else’,’0:MED:0:1 else’,’0:COMP:0:1 skip’,’1:COMP:0:1 else’]

Policy: 21et
Features: [5,10,15,29]
ECR: 15.4776
ECR Lower Bound: 7.84628
Mapping: [50.0000,None,0.0001,.7179]
States: [’1:MED:0:1

tell’,’0:COMP:0:0

else’,’1:MED:0:0

elicit’,’0:MED:1:0

elicit’,’1:MED:1:0

tell’,’1:COMP:0:0

tell’,’1:COMP:1:1

else’,’0:COMP:1:1

elicit’,’0:MED:0:0

tell’,’1:COMP:1:0
else’,’1:MED:1:1
elicit’,’0:COMP:1:0

elicit’,’0:MED:1:1 elicit’,’0:MED:0:1 tell’,’0:COMP:0:1 elicit’,’1:COMP:0:1 elicit’]

Policy: 22skip
Features: [5,9,24,30]
ECR: 27.4539
ECR Lower Bound: 9.55924
Mapping: [56.0000,0.5,75.0000,.6452]
States: [’1:1:1:1 else’,’0:0:1:1 skip’,’0:1:1:0 else’,’0:0:0:0 noskip’,’1:1:1:0 else’,’0:1:0:0
else’,’1:0:1:0 noskip’,’1:1:0:0 else’,’1:1:0:1 else’,’0:1:0:1 else’,’1:0:0:0 else’,’1:0:1:1
else’,’1:0:0:1 else’,’0:1:1:1 else’,’0:0:0:1 noskip’,’0:0:1:0 noskip’]

327

Policy: 22et
Features: [5,14,15,17]
ECR: 9.39791
ECR Lower Bound: -5.36517
Mapping: [56.0000,0.5,1.0000,.5120]
States: [’1:1:1:1 tell’,’0:0:1:1 elicit’,’0:1:1:0 elicit’,’0:0:0:0 tell’,’1:1:1:0 tell’,’0:1:0:0
elicit’,’1:0:1:0 elicit’,’1:1:0:0 tell’,’1:1:0:1 elicit’,’0:1:0:1 elicit’,’1:0:0:0 tell’,’1:0:1:1
elicit’,’1:0:0:1 tell’,’0:1:1:1 elicit’,’0:0:0:1 tell’,’0:0:1:0 else’]

Policy: 23skip
Features: [5,14,16,28]
ECR: 47.2221
ECR Lower Bound: 30.2939
Mapping: [96.0000,0.5,.4979,2.0000]
States: [’1:1:1:1

else’,’0:0:1:1

noskip’,’0:1:0:0

noskip’,’1:0:1:0

noskip’,’1:0:0:0

else’,’1:0:1:1

noskip’,’0:1:1:0
else’,’1:1:0:0
else’,’1:0:0:1

skip’,’0:0:1:0 skip’]

Policy: 23et
Features: [5]
ECR: 42.4539
ECR Lower Bound: 22.8901
Mapping: [96.0000]
States: [’0 else’, ’1 tell’]

328

noskip’,’0:0:0:0
noskip’,’1:1:0:1
noskip’,’0:1:1:1

skip’,’1:1:1:0
noskip’,’0:1:0:1
noskip’,’0:0:0:1

Policy: 24skip
Features: [5,9,15,18]
ECR: 2.92158
ECR Lower Bound: -0.302659
Mapping: [61.0000,0.5,1.0000,.2981]
States: [’0:0:1:1

skip’,’0:0:0:0

noskip’,’1:0:1:0

noskip’,’1:0:0:0

noskip’,’1:0:1:1

else’,’1:0:0:1 else’,’0:0:0:1 noskip’,’0:0:1:0 noskip’]

Policy: 24et
Features: [13,14,15,29]
ECR: 7.23353
ECR Lower Bound: 2.71676
Mapping: [91143.4986,0.5,1.0000,.7353]
States: [’1:1:1:1 tell’,’0:0:1:1 tell’,’0:1:1:0 tell’,’0:0:0:0 tell’,’1:1:1:0 elicit’,’0:1:0:0
else’,’1:0:1:0 elicit’,’1:1:0:0 else’,’1:1:0:1 tell’,’0:1:0:1 tell’,’1:0:0:0 else’,’1:0:1:1
elicit’,’1:0:0:1 tell’,’0:1:1:1 else’,’0:0:0:1 tell’,’0:0:1:0 else’]

Policy: 25et
Features: [5,10,29,30]
ECR: 22.5096
ECR Lower Bound: 7.75064
Mapping: [400.5000,None,.7273,.6347]
States: [’1:MED:0:1

tell’,’0:COMP:0:0

tell’,’0:MED:1:0

else’,’1:MED:1:0

tell’,’1:COMP:1:1

elicit’,’0:COMP:1:1

else’,’1:MED:0:0

tell’,’1:COMP:1:0

tell’,’1:COMP:0:0

elicit’,’1:MED:1:1

tell’,’0:MED:0:0

elicit’,’0:COMP:1:0

elicit’,’0:MED:1:1 else’,’0:MED:0:1 elicit’,’0:COMP:0:1 else’,’1:COMP:0:1 tell’]

329

Policy: 26skip
Features: [9,13,15,26]
ECR: 19.2677
ECR Lower Bound: 8.82795
Mapping: [0.5,166692.6890,1.0000,.3333]
States: [’0:0:1:1 noskip’,’0:1:1:0 else’,’0:0:0:0 else’,’0:1:0:0 else’,’0:1:0:1 noskip’,’0:1:1:1
skip’,’0:0:0:1 noskip’,’0:0:1:0 else’]

Policy: 26et
Features: [5,9,15,26]
ECR: 18.7551
ECR Lower Bound: 10.592
Mapping: [70.0000,0.5,1.0000,.3333]
States: [’0:0:1:1 tell’,’0:0:0:0 tell’,’1:0:1:0 tell’,’1:0:0:0 elicit’,’1:0:1:1 elicit’,’1:0:0:1
else’,’0:0:0:1 else’,’0:0:1:0 elicit’]

Policy: 27skip
Features: [9,13,15,17]
ECR: 10.5732
ECR Lower Bound: 2.7791
Mapping: [0.5,164567.3945,1.0000,.5406]
States: [’0:0:1:1 else’,’0:1:1:0 noskip’,’0:0:0:0 skip’,’0:1:0:0 noskip’,’0:1:0:1 else’,’0:1:1:1
else’,’0:0:0:1 else’,’0:0:1:0 noskip’]

330

Policy: 27et
Features: [5,10,27,30]
ECR: 16.7804
ECR Lower Bound: 5.94975
Mapping: [74.0000,None,.3077,.6270]
States: [’1:MED:0:1

else’,’0:COMP:0:0

tell’,’0:MED:1:0

tell’,’1:MED:1:0

tell’,’1:COMP:1:1

tell’,’0:COMP:1:1

elicit’,’1:MED:0:0
tell’,’1:COMP:0:0
elicit’,’0:MED:0:0

elicit’,’1:COMP:1:0
else’,’1:MED:1:1
elicit’,’0:COMP:1:0

tell’,’0:MED:1:1 else’,’0:MED:0:1 elicit’,’0:COMP:0:1 elicit’,’1:COMP:0:1 else’]

Policy: 28skip
Features: [5,9,15,27]
ECR: 13.9743
ECR Lower Bound: 6.00922
Mapping: [59.0000,0.5,1.0000,.2083]
States: [’0:0:1:1 noskip’,’0:0:0:0 else’,’1:0:1:0 else’,’1:0:0:0 skip’,’1:0:1:1 else’,’1:0:0:1
skip’,’0:0:0:1 skip’,’0:0:1:0 else’]

Policy: 28et
Features: [9,13,16,27]
ECR: 15.2862
ECR Lower Bound: 2.52373
Mapping: [0.5,81595.8369,.4916,.2083]
States: [’0:0:1:1 else’,’0:1:1:0 tell’,’0:0:0:0 tell’,’0:1:0:0 tell’,’0:1:0:1 elicit’,’0:1:1:1
tell’,’0:0:0:1 elicit’,’0:0:1:0 elicit’]

331

Policy: 31et
Features: [5,9,15,17]
ECR: 1.29047
ECR Lower Bound: -8.00777
Mapping: [425.0000,0.5,0.0001,.5046]
States: [’1:1:1:1 elicit’,’0:0:1:1 elicit’,’0:1:1:0 tell’,’0:0:0:0 elicit’,’1:1:1:0 elicit’,’0:1:0:0
tell’,’1:0:1:0 else’,’1:1:0:0 tell’,’1:1:0:1 tell’,’0:1:0:1 elicit’,’1:0:0:0 elicit’,’1:0:1:1
tell’,’1:0:0:1 else’,’0:1:1:1 tell’,’0:0:0:1 elicit’,’0:0:1:0 else’]

Policy: 32et
Features: [9,12,26,30]
ECR: 14.5397
ECR Lower Bound: 0.175324
Mapping: [0.5,2387.6022,.4000,.6000]
States: [’0:0:1:1 tell’,’0:1:1:0 tell’,’0:0:0:0 elicit’,’0:1:0:0 tell’,’0:1:0:1 tell’,’0:1:1:1
tell’,’0:0:0:1 tell’,’0:0:1:0 elicit’]

332

APPENDIX L

STUDY 3: EXAMPLE LOG 50 FEATURES

Representating the Sample Dialogue in Table 5.2 By 50 Feature Variables

Table L1: Student Autonomy Features
Ordera :
Speakerb
TMOVEc
v27tellsSinceElicitA
v28pctElicitA
v29stuWordsToTuWordsA
v30stuWordsToTuWordsSessionA
v31pctTellsKCSessionA

29
S
nil
0
0.5
0.32
0.21
0.62

30
T
tell
nil
nil
nil
nil
nil

31
S
nil
0.08
0.49
0.32
0.21
0.62

a

32
T
elicit
nil
nil
nil
nil
nil

33
S
nil
0
0.51
0.3
0.2
0.64

34
T
elicit skippable
nil
nil
nil
nil
nil

35
S
nil
0
0.52
0.31
0.2
0.63

36
T
tell
nil
nil
nil
nil
nil

37
S
nil
0.08
0.51
0.31
0.2
0.63

The number of the decision orders in the tutorial dialogue on KC20
“S:” refers to the current state when the tutor make the decision while “T:” means that it is tutor
decision turn
c
The action the tutor decided to take
b

333

Table L2: Problem Solving Context
Order:
Speaker
TMOVE
v12EarlyTrainingPS
v13SimpleProblemPS
v14DuringWalkThroughPS
v15nKCsPS
v16nKCsSessionPS
v17newLevelDifficultyPS
v18conceptDifficultyPS
v19QuantativeDegreePS
v20numPhysConceptsTutorDialogueSessionPS
v21tutAverageConceptsPS
v22tutAverageConceptsSessionPS
v23tutConceptsToWordsPS
v24tutConceptsToWordsSessionPS
v25tutAverageWordsPS
v26tutAverageWordsSessionPS

29
S
nil
0
0.5
1
0.12
0.28
1
0.56
0.5
0.35
0.82
0.66
0.87
0.74
0.85
0.75

30
T
tell
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil

31
S
nil
0
0.5
1
0.12
0.28
1
0.5
1
0.35
0.82
0.66
0.87
0.74
0.85
0.75

32
T
elicit
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil

33
S
nil
0
0.5
1
0.15
0.3
1
0.15
0
0.39
0.84
0.68
0.88
0.75
0.86
0.76

34
T
elicit skippable
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil

35
S
nil
0
0.5
1
0.15
0.3
1
0.15
0
0.39
0.84
0.68
0.88
0.75
0.86
0.76

36
T
tell
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil

37
S
nil
0
0.5
1
0.15
0.3
1
0.55
0
0.39
0.84
0.68
0.88
0.75
0.86
0.76

Table L3: Background Features
Order:
Speaker
TMOVE
v48genderBG
v49ageBG
v50MathSatBG
v51VerbalSatBG
v52pretestBG
v53averagePhysConceptsStudentDialogueSD

29
S
nil
0
0.38
0.8
0.67
0.69
0.19

30
T
tell
nil
nil
nil
nil
nil
nil

31
S
nil
0
0.38
0.8
0.67
0.69
0.19

32
T
elicit
nil
nil
nil
nil
nil
nil

33
S
nil
0
0.38
0.8
0.67
0.69
0.18

34
T
elicit skippable
nil
nil
nil
nil
nil
nil

35
S
nil
0
0.38
0.8
0.67
0.69
0.19

36
T
tell
nil
nil
nil
nil
nil
nil

37
S
nil
0
0.38
0.8
0.67
0.69
0.19

Table L4: Student Dialogue Features

Order:
Speaker
TMOVE
v53averagePhysConceptsStudentDialogueSD
v54numStudentConceptualDialogueSD
v55stuConceptToWordRatioSD
v56stuAverageWordsSD
v57stuAverageConceptSD
v58averagePhysConceptsStudentDialogueSessionSD
v59numStudentConceptualDialogueSessonSD
v60stuConceptToWordRatioSessionSD
v61stuAverageWordsSessionSD
v62stuAverageConceptSessionSD

29
S
nil
0.19
0.23
0.1
0.41
0.31
0.19
0.44
0.1
0.14
0.31

30
T
tell
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil

334

31
S
nil
0.19
0.23
0.1
0.41
0.31
0.19
0.44
0.1
0.14
0.31

32
T
elicit
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil

33
S
nil
0.18
0.23
0.1
0.41
0.3
0.18
0.44
0.1
0.14
0.3

34
T
elicit skippable
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil

35
S
nil
0.19
0.25
0.1
0.42
0.31
0.19
0.47
0.1
0.14
0.31

36
T
tell
nil
nil
nil
nil
nil
nil
nil
nil
nil
nil

37
S
nil
0.19
0.25
0.1
0.42
0.31
0.19
0.47
0.1
0.14
0.31

335

Order:
Speaker
TMOVE
v5durationBetweenDecisionT
v9TimeInSessionT
v10TimeBetweenSessionT
Start
End

29
S
nil
0.03
0.04
0
1187039153
1187042098

30
T
tell
nil
nil
nil
nil
nil

31
S
nil
0
0.04
0
1187042098
1187042117

32
T
elicit
nil
nil
nil
nil
nil

33
S
nil
0
0.04
0
1187042117
1187042212

34
T
elicit skippable
nil
nil
nil
nil
nil

Table L5: Temporal Features
35
S
nil
0
0.04
0
1187042212
1187042252

36
T
tell
nil
nil
nil
nil
nil

37
S
nil
0
0.04
0
1187042252
1187042253

336

Order:
Speaker
TMOVE
v32pctCorrectPM
v33pctOverallCorrectPM
v34pctCorrectSessionPM
v35pctOverallCorrectSessionPM
KC
sem
goal name
Justify
v40nCorrectKCPM
v41nCorrectKCSessionPM
v42pctOverallCorrectKCPM
v43pctOverallCorrectKCSessionPM
v44pctCorrectKCPM
v45pctCorrectKCSessionPM
v46nIncorrectKCPM

29
S
nil
0.8
0.57
0.83
0.31
20
nil
nil
nil
0.26
0.3
0.75
0.83
0.51
0.31
0.11

30
T
tell
nil
nil
nil
nil
20
nil
58-ke0 principle selection
nil
nil
nil
nil
nil
nil
nil
nil

31
S
nil
0.8
0.57
0.83
0.31
20
nil
nil
nil
0.26
0.3
0.75
0.83
0.51
0.31
0.11

32
T
elicit
nil
nil
nil
nil
20
nil
58-ke0 write eqn
nil
nil
nil
nil
nil
nil
nil
nil

33
S
nil
0.8
0.56
0.84
0.3
20
nil
nil
nil
0.28
0.32
0.77
0.84
0.49
0.3
0.11

Table L6: Performance Features

34
T
elicit skippable
nil
nil
nil
nil
20
nil
58-ke0 discuss truth
nil
nil
nil
nil
nil
nil
nil
nil

35
S
nil
0.8
0.56
0.84
0.31
20
nil
nil
nil
0.3
0.32
0.78
0.84
0.51
0.31
0.11

36
T
tell
nil
nil
nil
nil
20
nil
58-ke0 discuss qualitative 1
nil
nil
nil
nil
nil
nil
nil
nil

37
S
nil
0.8
0.56
0.84
0.31
20
nil
nil
nil
0.3
0.32
0.78
0.84
0.51
0.31
0.11

APPENDIX M

STUDY 3: NORMGAIN AND INVNORMGAIN TUTORIAL TACTICS
(FEATURES)

337

Table M1: NormGain Tutorial Tactics:

KC
KC1
Cols:
KC14
Cols:
KC14
Cols:
KC20
Cols:
KC20
Cols:

action
ET

#features
1

Corpus
DichGain

Selection Method
0

ECR
36.33

Lower Bound
13.76

Upper Bound
44.11

0

9.97

9.85

10.06

Exp

0

10.89

10.89

10.89

Exp

ECR

14.25

10.04

18.12

v24tutConceptsToWordsSessionPS

ET

1

Comb

v5durationBetweenDecisionT

JS

1

v12EarlyTrainingPS

ET

3

v18conceptDifficultyPS v23tutConceptsToWordsPS v26tutAverageWordsSessionPS

JS

5

Exp

v9TimeInSessionT

PCA-Hedge

v15nKCsPS

20.23

v28pctElicitA

15.11

22.12

v56stuAverageWordsSD

v62stuAverageConceptSessionSD

KC21
Cols:
KC21
Cols:
KC22
Cols:

JS

3

Exp

PCA-Hedge

32.26

29.8

34.59

v13SimpleProblemPS v18conceptDifficultyPS v20numPhysConceptsTutorDialogueSessionPS

ET

3

Exp

Upper Bound

20.99

15.2

26.64

22.45

47.12

v15nKCsPS v18conceptDifficultyPS v30stuWordsToTuWordsSessionA

JS

5

Exp

v23tutConceptsToWordsPS

Upper Bound

37.02

v24tutConceptsToWordsSessionPS

v27tellsSinceElicitA

v46nIncorrectKCPM v47nIncorrectKCSessionPM

KC22
Cols:
KC24
Cols:

ET

2

Exp

Hedge

33.42

10.55

40.08

20.05

16.09

24.45

v23tutConceptsToWordsPS v27tellsSinceElicitA

JS

6

Exp

Upper Bound

v14DuringWalkThroughPS v18conceptDifficultyPS v49ageBG v59numStudentConceptualDialogueSessonSD
v60stuConceptToWordRatioSessionSD v62stuAverageConceptSessionSD

KC24
Cols:

ET

4

DichGain

v10TimeBetweenSessionT

ECR

13.51

v14DuringWalkThroughPS

8.41

16.35

v18conceptDifficultyPS

v55stuConceptToWordRatioSD

KC27
Cols:

ET

4

Exp

v5durationBetweenDecisionT

PCA-Random

48.51

v17newLevelDifficultyPS

38.9

57

v23tutConceptsToWordsPS

v35pctOverallCorrectSessionPM

KC27
Cols:
KC28
Cols:
KC28
Cols:

JS

4

Exp

ECR

55.68

30.89

44.05

v9TimeInSessionT v25tutAverageWordsPS v27tellsSinceElicitA v56stuAverageWordsSD

JS

3

DichGain

Upper Bound

30.11

15.82

41

v15nKCsPS v24tutConceptsToWordsSessionPS v54numStudentConceptualDialogueSD

ET

5

DichGain

Upper Bound

20.45

8.45

30.43

v5durationBetweenDecisionT v16nKCsSessionPS v18conceptDifficultyPS v29stuWordsToTuWordsA
v41nCorrectKCSessionPM

KC-general
Cols:
KC-general
Cols:

ET

4

Exp

Lower Bound

4.51

1.38

8.33

v5durationBetweenDecisionT v18conceptDifficultyPS v27tellsSinceElicitA v46nIncorrectKCPM

JS

5

DichGain

ECR

9.84

6.77

12.19

v16nKCsSessionPS v17newLevelDifficultyPS v23tutConceptsToWordsPS v25tutAverageWordsPS
v27tellsSinceElicitA

338

Table M2: InvNormGain Tutorial Tactics:

KC
KC0
Columns:
KC1
Cols:
KC14
Cols:
KC14
Cols:
KC20
Cols:
KC20
Cols:

action #features
Corpus
Selection Method
ECR Lower Bound Upper Bound
JS
4
DichGain
ECR
28.7
21.91
33.91
v17newLevelDifficultyPS
v23tutConceptsToWordsPS
v25tutAverageWordsPS
v27tellsSinceElicitA
ET
2
DichGain
Upper Bound
70.39
62.26
74.12
v19QuantativeDegreePS v41nCorrectKCSessionPM

ET

1

DichGain

0

87.63

87.63

87.63

Comb

0

86.67

86.67

86.67

Exp

PCA-Lower Bound

31.15

26.91

34.98

33.99

47.57

v10TimeBetweenSessionT

JS

1

v12EarlyTrainingPS

ET

3

v19QuantativeDegreePS v23tutConceptsToWordsPS v25tutAverageWordsPS

JS

5

DichGain

v13SimpleProblemPS

Random

v14DuringWalkThroughPS

41.43

v18conceptDifficultyPS

v44pctCorrectKCPM

v55stuConceptToWordRatioSD

KC21
Cols:

ET

6

Exp

v5durationBetweenDecisionT

Hedge

23.43

v14DuringWalkThroughPS

18.01

27.06

v19QuantativeDegreePS

v55stuConceptToWordRatioSD v56stuAverageWordsSD v61stuAverageWordsSessionSD

KC21
Cols:
KC22
Cols:
KC22
Cols:
KC24
Cols:
KC24
Cols:
KC27
Cols:
KC27
Cols:

JS

3

Exp

ECR

46.74

41.83

51.55

v18conceptDifficultyPS v35pctOverallCorrectSessionPM v45pctCorrectKCSessionPM

JS

4

DichGain

ECR

95.36

88.01

103.16

v5durationBetweenDecisionT v19QuantativeDegreePS v27tellsSinceElicitA v32pctCorrectPM

ET

3

Exp

ECR

79.29

58.21

102.28

v24tutConceptsToWordsSessionPS v26tutAverageWordsSessionPS v27tellsSinceElicitA

JS

2

DichGain

Upper Bound

49.19

33.14

61.55

36.36

30.77

42.19

37.7

28.76

45.02

v17newLevelDifficultyPS v47nIncorrectKCSessionPM

ET

2

DichGain

ECR

v14DuringWalkThroughPS v18conceptDifficultyPS

ET

4

DichGain

Lower Bound

v5durationBetweenDecisionT v9TimeInSessionT v19QuantativeDegreePS v27tellsSinceElicitA

JS

5

v9TimeInSessionT

DichGain

Hedge

v40nCorrectKCPM

60.04

v42pctOverallCorrectKCPM

40.64

50.87

v46nIncorrectKCPM

v56stuAverageWordsSD

KC28
Cols:
KC28
Cols:
KC-general
Cols:
KC-general
Cols:

ET

3

DichGain

ECR

67.97

51.86

81.84

35.71

58.29

v5durationBetweenDecisionT v18conceptDifficultyPS v46nIncorrectKCPM

JS

5

Exp

Upper Bound

v14DuringWalkThroughPS
v15nKCsPS
v60stuConceptToWordRatioSessionSD

ET

5

DichGain

Upper Bound

v16nKCsSessionPS v18conceptDifficultyPS
v35pctOverallCorrectSessionPM

ET

5

DichGain

9.28

v23tutConceptsToWordsPS

Upper Bound

v16nKCsSessionPS v18conceptDifficultyPS
v35pctOverallCorrectSessionPM

67.37

v25tutAverageWordsPS

9.28

v23tutConceptsToWordsPS

339

v46nIncorrectKCPM

5.94

12.54

v31pctTellsKCSessionA

5.94

12.54

v31pctTellsKCSessionA

APPENDIX N

STUDY 3: NORMAGAIN TUTORIAL TACTICS (POLICIES)

Policy: ’KC-general policy on Elicit/Tell’
Features: [5,18,27,46]
ECR: 4.505940
ECR Lower Bound: 1.381010
Mapping: [[22.00581],[0.29775,0.60095],[2.3034],[88.0896,143.231]]
States: [’0:0:0:0 elicit’,’0:0:0:1 tell’,’0:0:0:2 elicit’,’0:0:1:0 elicit’,’0:0:1:1 tell’,’0:0:1:2
tell’,’0:1:0:0 elicit’,’0:1:0:1 tell’,’0:1:0:2 tell’,’0:1:1:0 elicit’,’0:1:1:1 else’,’0:1:1:2
tell’,’0:2:0:0 elicit’,’0:2:0:1 tell’,’0:2:0:2 else’,’0:2:1:0 elicit’,’0:2:1:1 else’,’0:2:1:2
else’,’1:0:0:0

tell’,’1:0:0:1

tell’,’1:0:0:2

tell’,’1:0:1:0

elicit’,’1:0:1:1

tell’,’1:0:1:2

tell’,’1:1:0:0 elicit’,’1:1:0:1 tell’,’1:1:0:2 tell’,’1:1:1:0 elicit’,’1:1:1:1 else’,’1:1:1:2
tell’,’1:2:0:0 elicit’,’1:2:0:1 tell’,’1:2:0:2 elicit’,’1:2:1:0 elicit’,’1:2:1:1 tell’,’1:2:1:2 tell’]

340

Policy: ’KC-general policy on Justify/Skip-Justify’
Features: [16,17,23,25,27]
ECR: 9.844580
ECR Lower Bound: 6.774750
Mapping: [[92.26325,183.4505],[0.5],[0.075178],[22.759655,23.592848],[1.0]]
States: [’0:0:0:0:0 noskip’,’0:0:0:0:1 noskip’,’0:0:0:1:0 skip’,’0:0:0:1:1 noskip’,’0:0:0:2:0
noskip’,’0:0:0:2:1 skip’,’0:0:1:0:0 noskip’,’0:0:1:0:1 noskip’,’0:0:1:1:0 noskip’,’0:0:1:1:1
noskip’,’0:0:1:2:0 noskip’,’0:0:1:2:1 skip’,’0:1:0:0:0 skip’,’0:1:0:0:1 noskip’,’0:1:0:1:0
noskip’,’0:1:0:1:1 skip’,’0:1:0:2:0 noskip’,’0:1:0:2:1 noskip’,’0:1:1:0:0 skip’,’0:1:1:0:1
noskip’,’0:1:1:1:0

noskip’,’0:1:1:1:1

noskip’,’0:1:1:2:0

noskip’,’0:1:1:2:1

skip’,’1:0:0:0:0 noskip’,’1:0:0:0:1 noskip’,’1:0:0:1:0 noskip’,’1:0:0:1:1 noskip’,’1:0:0:2:0
skip’,’1:0:0:2:1 noskip’,’1:0:1:0:0 noskip’,’1:0:1:0:1 noskip’,’1:0:1:1:0 noskip’,’1:0:1:1:1
noskip’,’1:0:1:2:0 noskip’,’1:0:1:2:1 noskip’,’1:1:0:0:0 skip’,’1:1:0:0:1 skip’,’1:1:0:1:0
noskip’,’1:1:0:1:1 noskip’,’1:1:0:2:0 skip’,’1:1:0:2:1 noskip’,’1:1:1:0:0 noskip’,’1:1:1:0:1
noskip’,’1:1:1:1:0 noskip’,’1:1:1:1:1 skip’,’1:1:1:2:0 noskip’,’1:1:1:2:1 noskip’,’2:0:0:0:0
noskip’,’2:0:0:1:0 skip’,’2:0:0:1:1 noskip’,’2:0:0:2:0 noskip’,’2:0:0:2:1 noskip’,’2:0:1:1:0
noskip’,’2:0:1:1:1 noskip’,’2:0:1:2:0 noskip’,’2:0:1:2:1 skip’,’2:1:0:0:0 skip’,’2:1:0:1:0
skip’,’2:1:0:1:1

skip’,’2:1:0:2:0

noskip’,’2:1:0:2:1

skip’,’2:1:1:1:0

skip’,’2:1:1:1:1

noskip’,’2:1:1:2:0 noskip’,’2:1:1:2:1 skip’]

Policy: 1a etb ’
Features: [24]
ECR: 36.331200
ECR Lower Bound: 13.755700
Mapping: [[0.062402,0.071909]]
States: [’0 elicit’,’1 tell’,’2 elicit’]

a

KC number. In this case, means it is a KC-specific policy on KC1. Same below
Type of tutorial decisions. “et” refers to Elicit/Tell decisions while “skip” refers to Justify/SkipJustify decisions
b

341

Policy: 14et’
Features: [5]
ECR: 9.966550
ECR Lower Bound: 9.848810
Mapping: [[160.073973]]
States: [’0 elicit’,’1 else’]

Policy: 14skip’
Features: [12]
ECR: 10.887100
ECR Lower Bound: 10.887100
Mapping: [[0.5]]
States: [’0 skip’,’1 noskip’]

Policy: 20et’
Features: [18,23,26]
ECR: 14.254900
ECR Lower Bound: 10.036400
Mapping: [[0.3778],[0.073981],[22.577706]]
States: [’0:0:0 elicit’,’0:0:1 elicit’,’0:1:0 tell’,’0:1:1 else’,’1:0:0 else’,’1:0:1 elicit’,’1:1:0
elicit’,’1:1:1 elicit’]

342

Policy: 20skip’
Features: [9,15,28,56,62]
ECR: 20.227600
ECR Lower Bound: 15.113600
Mapping: [[3040.799131],[65.0135],[0.493592],[4.176873],[0.28604]]
States: [’0:0:0:0:0

noskip’,’0:0:0:0:1

skip’,’0:0:0:1:0

skip’,’0:0:0:1:1

skip’,’0:0:1:0:0

noskip’,’0:0:1:0:1 skip’,’0:0:1:1:0 noskip’,’0:0:1:1:1 skip’,’0:1:0:0:0 skip’,’0:1:0:0:1
noskip’,’0:1:0:1:0 skip’,’0:1:0:1:1 noskip’,’0:1:1:0:0 noskip’,’0:1:1:0:1 noskip’,’0:1:1:1:0
noskip’,’0:1:1:1:1 noskip’,’1:0:0:0:0 noskip’,’1:0:0:0:1 skip’,’1:0:0:1:0 noskip’,’1:0:0:1:1
skip’,’1:0:1:0:0 noskip’,’1:0:1:0:1 noskip’,’1:0:1:1:0 noskip’,’1:0:1:1:1 noskip’,’1:1:0:0:0
skip’,’1:1:0:0:1 noskip’,’1:1:0:1:0 skip’,’1:1:0:1:1 skip’,’1:1:1:0:0 noskip’,’1:1:1:0:1
noskip’,’1:1:1:1:0 noskip’,’1:1:1:1:1 noskip’]

Policy: 21et’
Features: [15,18,30]
ECR: 20.993900
ECR Lower Bound: 15.201400
Mapping: [[44.15375],[0.30125,0.6075],[0.08421]]
States: [’0:0:0 elicit’,’0:0:1 elicit’,’0:1:0 elicit’,’0:1:1 elicit’,’0:2:0 elicit’,’0:2:1 elicit’,’1:0:0
elicit’,’1:0:1 elicit’,’1:1:0 elicit’,’1:1:1 tell’,’1:2:0 else’,’1:2:1 else’]

Policy: 21skip’
Features: [13,18,20]
ECR: 32.255300
ECR Lower Bound: 29.801900
Mapping: [[0.75],[0.30125,0.6075],[235.1497,468.0707]]
States: [’0:0:0 noskip’,’0:0:1 noskip’,’0:0:2 skip’,’0:1:0 else’,’0:1:1 else’,’0:1:2 else’,’0:2:0
noskip’,’0:2:1

noskip’,’0:2:2

noskip’,’1:0:0

else’,’1:0:1

skip’,’1:0:2

noskip’,’1:1:1 noskip’,’1:1:2 noskip’,’1:2:0 skip’,’1:2:1 skip’,’1:2:2 skip’]

343

else’,’1:1:0

Policy: 22et’
Features: [23,27]
ECR: 33.419600
ECR Lower Bound: 10.554100
Mapping: [[0.067549,0.07119,0.074431],[2.2836]]
States: [’0:0 elicit’,’0:1 tell’,’1:0 tell’,’1:1 else’,’2:0 tell’,’2:1 else’,’3:0 tell’,’3:1 else’]

Policy: 22skip’
Features: [23,24,27,46,47]
ECR: 37.015500
ECR Lower Bound: 22.445200
Mapping: [[0.067549,0.07119,0.074431],[0.067915],[2.2836],[4.197,7.44825],[9.085]]
States: [’0:0:0:0:0

noskip’,’0:0:0:0:1

else’,’0:0:1:0:1

else’,’0:0:1:1:0

else’,’0:0:0:1:0

else’,’0:0:0:1:1

else’,’0:0:1:0:0

else’,’0:0:1:1:1

else’,’0:0:1:2:0

else’,’0:1:0:0:0

else’,’1:0:0:1:0

skip’,’1:0:0:1:1

noskip’,’1:0:1:0:1

else’,’1:0:1:1:0

else’,’0:1:1:0:0

else’,’1:0:0:0:0

noskip’,’1:0:0:0:1

else’,’1:0:0:2:0

skip’,’1:0:0:2:1

else’,’1:0:1:0:0

else’,’1:0:1:1:1

skip’,’1:0:1:2:0

noskip’,’1:0:1:2:1

else’,’1:1:0:0:0

else’,’1:1:0:0:1

else’,’1:1:0:1:0

skip’,’1:1:0:1:1

noskip’,’1:1:0:2:0

else’,’1:1:0:2:1

skip’,’1:1:1:0:0

else’,’1:1:1:0:1

else’,’1:1:1:1:0

else’,’1:1:1:1:1

noskip’,’1:1:1:2:1

else’,’2:0:0:0:0

skip’,’2:0:0:0:1

else’,’2:0:0:1:0

else’,’2:0:0:1:1

noskip’,’2:0:0:2:0

else’,’2:0:0:2:1

skip’,’2:0:1:0:0 else’,’2:0:1:1:0 noskip’,’2:0:1:1:1 else’,’2:0:1:2:0 noskip’,’2:0:1:2:1
noskip’,’2:1:0:0:0 skip’,’2:1:0:0:1 else’,’2:1:0:1:0 noskip’,’2:1:0:1:1 else’,’2:1:0:2:0
else’,’2:1:0:2:1

noskip’,’2:1:1:0:0

noskip’,’2:1:1:2:0

else’,’2:1:1:2:1

else’,’2:1:1:0:1

else’,’2:1:1:1:0

skip’,’2:1:1:1:1

else’,’3:0:0:0:0

else’,’3:0:0:1:0

else’,’3:0:0:2:0

else’,’3:0:0:2:1

else’,’3:0:1:0:0

else’,’3:0:1:2:0

else’,’3:0:1:2:1

else’,’3:1:0:0:0

else’,’3:1:0:0:1

skip’,’3:1:0:1:0

else’,’3:1:0:1:1

else’,’3:1:0:2:0

else’,’3:1:0:2:1

else’,’3:1:1:0:0

else’,’3:1:1:0:1

noskip’,’3:1:1:1:0

else’,’3:1:1:1:1

else’,’3:1:1:2:0

else’,’3:1:1:2:1 skip’]

344

Policy: 24et’
Features: [10,14,18,55]
ECR: 13.511000
ECR Lower Bound: 8.405400
Mapping: [[218512.016452],[0.5],[0.52575],[0.07082,0.086168,0.106655]]
States: [’0:0:0:0 else’,’0:0:0:1 elicit’,’0:0:0:2 elicit’,’0:0:0:3 elicit’,’0:0:1:0 else’,’0:0:1:1
tell’,’0:0:1:2

tell’,’0:0:1:3

tell’,’0:1:0:0

else’,’0:1:0:1

else’,’0:1:0:2

else’,’0:1:0:3

elicit’,’0:1:1:0 else’,’0:1:1:1 else’,’0:1:1:2 else’,’0:1:1:3 else’,’1:0:0:0 elicit’,’1:0:0:1
elicit’,’1:0:0:2 else’,’1:0:0:3 elicit’,’1:0:1:0 elicit’,’1:0:1:1 else’,’1:0:1:2 elicit’,’1:0:1:3
tell’,’1:1:0:0 elicit’,’1:1:0:1 tell’,’1:1:0:2 elicit’,’1:1:0:3 elicit’,’1:1:1:0 else’,’1:1:1:1
else’,’1:1:1:2 else’,’1:1:1:3 else’]

345

Policy: 24skip’
Features: [14,18,49,59,60,62]
ECR: 20.046000
ECR Lower Bound: 16.093400
Mapping: [[0.5],[0.341425],[20.999997],[14.0911],[0.1],[0.266733]]
States: [’0:0:0:0:0:0

noskip’,’0:0:0:0:1:0

skip’,’0:0:0:1:1:0

noskip’,’0:0:0:1:1:1

skip’,’0:0:1:1:0:0

skip’,’0:0:1:1:0:1

skip’,’0:0:0:1:0:0
skip’,’0:0:1:0:0:0

skip’,’0:0:0:1:0:1
noskip’,’0:0:1:0:1:0

skip’,’0:0:1:1:1:0

skip’,’0:0:1:1:1:1

noskip’,’0:1:0:0:1:0

skip’,’0:1:0:0:1:1

skip’,’0:1:0:0:0:0

skip’,’0:1:0:0:0:1

skip’,’0:1:0:1:0:0

noskip’,’0:1:0:1:0:1

noskip’,’0:1:0:1:1:0

noskip’,’0:1:0:1:1:1

skip’,’0:1:1:0:0:0

noskip’,’0:1:1:0:0:1

noskip’,’0:1:1:0:1:0

noskip’,’0:1:1:0:1:1

skip’,’0:1:1:1:0:0

skip’,’0:1:1:1:0:1

noskip’,’0:1:1:1:1:0

skip’,’0:1:1:1:1:1

skip’,’1:0:0:0:0:0 skip’,’1:0:0:0:0:1 skip’,’1:0:0:0:1:0 skip’,’1:0:0:0:1:1 skip’,’1:0:0:1:0:0
noskip’,’1:0:0:1:0:1

noskip’,’1:0:0:1:1:0

skip’,’1:0:1:0:0:1

skip’,’1:0:1:0:1:0

skip’,’1:0:1:1:0:1

noskip’,’1:0:1:1:1:0

skip’,’1:1:0:0:0:1
noskip’,’1:1:0:1:0:1
skip’,’1:1:1:0:0:1

skip’,’1:1:0:0:1:0
noskip’,’1:1:0:1:1:0
noskip’,’1:1:1:0:1:0

noskip’,’1:0:0:1:1:1
skip’,’1:0:1:0:1:1
skip’,’1:0:1:1:1:1
noskip’,’1:1:0:0:1:1
noskip’,’1:1:0:1:1:1
noskip’,’1:1:1:0:1:1

skip’,’1:0:1:0:0:0
noskip’,’1:0:1:1:0:0
skip’,’1:1:0:0:0:0
noskip’,’1:1:0:1:0:0
noskip’,’1:1:1:0:0:0
skip’,’1:1:1:1:0:0

noskip’,’1:1:1:1:0:1 noskip’,’1:1:1:1:1:0 skip’,’1:1:1:1:1:1 noskip’]

Policy: 27et’
Features: [5,17,23,35]
ECR: 48.509500
ECR Lower Bound: 38.899400
Mapping: [[78.989305],[0.5],[0.072768],[0.348344]]
States: [’0:0:0:0 else’,’0:0:0:1 else’,’0:0:1:0 else’,’0:0:1:1 else’,’0:1:0:0 else’,’0:1:0:1
tell’,’0:1:1:0

else’,’0:1:1:1

tell’,’1:0:0:0

tell’,’1:0:0:1

tell’,’1:1:0:0 tell’,’1:1:0:1 else’,’1:1:1:0 else’,’1:1:1:1 else’]

346

tell’,’1:0:1:0

elicit’,’1:0:1:1

Policy: 27skip’
Features: [9,25,27,56]
ECR: 55.684600
ECR Lower Bound: 30.887000
Mapping: [[3657.04962],[22.760564,23.335972],[0.999999],[4.203393]]
States: [’0:0:0:0 skip’,’0:0:0:1 skip’,’0:0:1:0 skip’,’0:0:1:1 noskip’,’0:1:0:0 noskip’,’0:1:0:1
noskip’,’0:1:1:0
skip’,’0:2:1:1

noskip’,’0:1:1:1
noskip’,’1:0:0:0

noskip’,’0:2:0:0
noskip’,’1:0:0:1

skip’,’0:2:0:1

skip’,’0:2:1:0

noskip’,’1:0:1:0

skip’,’1:0:1:1

skip’,’1:1:0:0 skip’,’1:1:0:1 noskip’,’1:1:1:0 skip’,’1:1:1:1 noskip’,’1:2:0:0 skip’,’1:2:0:1
noskip’,’1:2:1:0 noskip’,’1:2:1:1 noskip’]

Policy: 28et’
Features: [5,16,18,29,41]
ECR: 20.446500
ECR Lower Bound: 8.450380
Mapping: [[73.053098],[114.40575],[0.393393],[0.046351],[32.2232]]
States: [’0:0:0:0:0

tell’,’0:0:0:1:0

elicit’,’0:0:1:0:0

else’,’0:0:1:1:0

elicit’,’0:1:0:0:0

else’,’0:1:0:0:1

tell’,’0:1:0:1:0

else’,’0:1:0:1:1

else’,’0:1:1:0:0

else’,’0:1:1:0:1

tell’,’0:1:1:1:0

elicit’,’0:1:1:1:1

tell’,’1:0:0:0:0

elicit’,’1:0:0:1:0

tell’,’1:0:1:0:0

tell’,’1:1:0:0:0

else’,’1:1:0:0:1

tell’,’1:1:0:1:0

elicit’,’1:0:1:1:0

elicit’,’1:1:0:1:1

else’,’1:1:1:0:0 else’,’1:1:1:0:1 tell’,’1:1:1:1:0 tell’,’1:1:1:1:1 tell’]

Policy: 28skip’
Features: [15,24,54]
ECR: 30.114700
ECR Lower Bound: 15.824200
Mapping: [[20.42295],[0.075692],[21.0157]]
States: [’0:0:0 noskip’,’0:0:1 noskip’,’0:1:0 noskip’,’0:1:1 noskip’,’1:0:0 noskip’,’1:0:1
skip’,’1:1:0 noskip’,’1:1:1 skip’]

347

APPENDIX O

STUDY 3: INVNORMGAIN TUTORIAL TACTICS (POLICIES)

348

Policy: ’KC-general policy on Elicit/Tell’
Features: [16,18,23,31,35]
ECR: 9.282570
ECR Lower Bound: 5.942560
Mapping: [[113.0632],[0.2696,0.60035],[0.075261],[0.2993,0.61905,0.78495],[0.2504]]
States: [’0:0:0:0:0

elicit’,’0:0:0:0:1

tell’,’0:0:0:1:0

elicit’,’0:0:0:2:1

elicit’,’0:0:0:3:0

elicit’,’0:0:1:0:0

elicit’,’0:0:0:1:1
elicit’,’0:0:1:0:1

elicit’,’0:0:0:2:0
tell’,’0:0:1:1:0

elicit’,’0:0:1:1:1 elicit’,’0:0:1:2:0 elicit’,’0:0:1:2:1 elicit’,’0:0:1:3:0 elicit’,’0:1:0:0:0
elicit’,’0:1:0:0:1

tell’,’0:1:0:1:0

else’,’0:1:0:1:1

else’,’0:1:0:3:0

elicit’,’0:1:1:0:0

elicit’,’0:1:1:0:1

tell’,’0:1:1:2:0

elicit’,’0:1:1:2:1

tell’,’0:1:1:3:0

tell’,’0:1:0:2:0
tell’,’0:1:1:1:0
tell’,’0:2:0:0:0

elicit’,’0:1:0:2:1
tell’,’0:1:1:1:1
elicit’,’0:2:0:0:1

elicit’,’0:2:0:1:0 elicit’,’0:2:0:1:1 elicit’,’0:2:0:2:0 elicit’,’0:2:0:2:1 elicit’,’0:2:0:3:0
elicit’,’0:2:1:0:1

tell’,’0:2:1:1:0

elicit’,’0:2:1:1:1

elicit’,’0:2:1:2:0

elicit’,’0:2:1:2:1

elicit’,’0:2:1:3:0

else’,’1:0:0:1:0

elicit’,’1:0:0:1:1

elicit’,’1:0:0:2:0

elicit’,’1:0:0:2:1

elicit’,’1:0:0:3:0

elicit’,’1:0:1:1:0

elicit’,’1:0:1:1:1

tell’,’1:0:1:2:0

elicit’,’1:0:1:2:1

elicit’,’1:0:1:3:0

tell’,’1:1:0:1:0

elicit’,’1:1:0:1:1

tell’,’1:1:0:2:0

else’,’1:1:0:2:1

tell’,’1:1:0:3:0

elicit’,’1:1:1:1:0

elicit’,’1:1:1:1:1

tell’,’1:1:1:3:0

elicit’,’1:2:0:1:0

elicit’,’1:2:0:1:1

tell’,’1:2:0:3:0

elicit’,’1:2:1:1:0

elicit’,’1:2:1:1:1

tell’,’1:2:1:3:0 elicit’]

349

tell’,’1:1:1:2:0
tell’,’1:2:0:2:0
tell’,’1:2:1:2:0

elicit’,’1:1:1:2:1
else’,’1:2:0:2:1
elicit’,’1:2:1:2:1

Policy: ’KC-general policy on Justify/Skip-Justify’
Features: [17,23,25,27]
ECR: 28.701200
ECR Lower Bound: 21.911800
Mapping: [[0.5],[0.075178],[22.759655,23.592848],[1.0]]
States: [’0:0:0:0

noskip’,’0:0:0:1

noskip’,’0:0:1:0

noskip’,’0:0:1:1

noskip’,’0:0:2:0

noskip’,’0:0:2:1 noskip’,’0:1:0:0 noskip’,’0:1:0:1 noskip’,’0:1:1:0 noskip’,’0:1:1:1
noskip’,’0:1:2:0

noskip’,’0:1:2:1

skip’,’1:0:0:0

noskip’,’1:0:1:1

skip’,’1:0:2:0

noskip’,’1:0:2:1

skip’,’1:0:0:1
skip’,’1:1:0:0

noskip’,’1:0:1:0
skip’,’1:1:0:1

noskip’,’1:1:1:0 noskip’,’1:1:1:1 noskip’,’1:1:2:0 noskip’,’1:1:2:1 skip’]

Policy: ’1a etb ’
Features: [19,41]
ECR: 70.394200
ECR Lower Bound: 62.264200
Mapping: [[0.066964],[22.0]]
States: [’0:0 elicit’,’0:1 tell’,’1:0 tell’,’1:1 tell’]

a

KC number. In this case, it is a KC-specific policy on KC1. Same below
Type of tutorial decisions. “et” refers to Elicit/Tell decisions while “skip” refers to Justify/SkipJustify decisions
b

Policy: ’14et’
Features: [10]
ECR: 87.631600
ECR Lower Bound: 87.631600
Mapping: [[0.0]]
States: [’0 tell’,’1 tell’]

350

Policy: ’14skip’
Features: [12]
ECR: 86.666700
ECR Lower Bound: 86.666700
Mapping: [[0.5]]
States: [’0 noskip’,’1 noskip’]

Policy: ’20et’
Features: [19,23,25]
ECR: 31.148800
ECR Lower Bound: 26.913700
Mapping: [[0.22755,0.71735],[0.073981],[22.954695]]
States: [’0:0:0 elicit’,’0:0:1 elicit’,’0:1:0 elicit’,’0:1:1 elicit’,’1:0:0 else’,’1:0:1 else’,’1:1:0
elicit’,’1:1:1 elicit’,’2:0:0 tell’,’2:0:1 tell’,’2:1:0 elicit’,’2:1:1 elicit’]

Policy: ’20skip’
Features: [13,14,18,44,55]
ECR: 41.425700
ECR Lower Bound: 33.987700
Mapping: [[0.75],[0.5],[0.530107],[0.133398],[0.07486,0.10008]]
States: [’0:0:0:0:0 noskip’,’0:0:0:0:1 noskip’,’0:0:0:0:2 noskip’,’0:0:0:1:0 noskip’,’0:0:0:1:1
noskip’,’0:0:0:1:2 skip’,’0:0:1:0:0 skip’,’0:0:1:0:1 noskip’,’0:0:1:0:2 noskip’,’0:0:1:1:0
skip’,’0:0:1:1:1 noskip’,’0:0:1:1:2 skip’,’0:1:0:0:0 skip’,’0:1:0:0:1 noskip’,’0:1:0:0:2
skip’,’0:1:0:1:0

skip’,’0:1:0:1:1

skip’,’0:1:0:1:2

skip’,’0:1:1:0:0

noskip’,’0:1:1:0:1

noskip’,’0:1:1:0:2 skip’,’0:1:1:1:0 noskip’,’0:1:1:1:1 noskip’,’0:1:1:1:2 noskip’,’1:0:1:0:0
noskip’,’1:0:1:0:1 noskip’,’1:0:1:0:2 noskip’,’1:0:1:1:0 skip’,’1:0:1:1:1 skip’,’1:0:1:1:2
noskip’,’1:1:0:0:0 noskip’,’1:1:0:0:1 skip’,’1:1:0:0:2 skip’,’1:1:0:1:0 noskip’,’1:1:0:1:1
skip’,’1:1:0:1:2 noskip’]

351

Policy: 21et
Features: [5,14,19,55,56,61]
ECR: 23.429100
ECR Lower Bound: 18.009100
Mapping: [[50.00604],[0.5],[0.2361,0.7172],[0.079174,0.102734],[3.92965,5.668881],[5.8023]]
States: [’0:0:0:0:0:0 else’,’0:0:0:0:1:0 elicit’,’0:0:0:0:1:1 else’,’0:0:0:0:2:1 elicit’,
’0:0:0:1:0:0 elicit’,’0:0:0:1:1:0 elicit’,’0:0:0:1:1:1 else’,’0:0:0:1:2:0 elicit’,
’0:0:0:1:2:1 elicit’,’0:0:0:2:0:0 elicit’,’0:0:0:2:1:0 elicit’,’0:0:0:2:1:1 elicit’,
’0:0:0:2:2:0 elicit’,’0:0:0:2:2:1 else’,’0:0:1:0:0:0 else’,’0:0:1:0:1:0 else’,
’0:0:1:0:1:1 elicit’,’0:0:1:0:2:1 else’,’0:0:1:1:0:0 else’,’0:0:1:1:1:0 else’,
’0:0:1:1:1:1 elicit’,’0:0:1:1:2:0 elicit’,’0:0:1:1:2:1 elicit’,’0:0:1:2:0:0 else’,
’0:0:1:2:1:0 elicit’,’0:0:1:2:2:0 else’,’0:0:1:2:2:1 else’,’0:0:2:0:0:0 elicit’,
’0:0:2:0:1:0 elicit’,’0:0:2:0:1:1 elicit’,’0:0:2:0:2:1 elicit’,’0:0:2:1:0:0 elicit’,
’0:0:2:1:1:0 elicit’,’0:0:2:1:1:1 tell’,’0:0:2:1:2:0 elicit’,’0:0:2:1:2:1 tell’,
’0:0:2:2:0:0 elicit’,’0:0:2:2:1:0 tell’,’0:0:2:2:1:1 elicit’,’0:0:2:2:2:0 elicit’,
’0:1:0:0:0:0 else’,’0:1:0:0:1:0 tell’,’0:1:0:0:1:1 elicit’,’0:1:0:0:2:0 elicit’,
’0:1:0:0:2:1 elicit’,’0:1:0:1:0:0 elicit’,’0:1:0:1:0:1 tell’,’0:1:0:1:1:0 elicit’,
’0:1:0:1:1:1 tell’,’0:1:0:1:2:0 else’,’0:1:0:1:2:1 tell’,’0:1:0:2:0:0 elicit’,
’0:1:0:2:1:0 elicit’,’0:1:0:2:1:1 elicit’,’0:1:0:2:2:0 else’,’0:1:0:2:2:1 tell’,
’0:1:1:0:0:0 tell’,’0:1:1:0:1:0 elicit’,’0:1:1:0:1:1 else’,’0:1:1:0:2:0 elicit’,
’0:1:1:0:2:1 tell’,’0:1:1:1:0:0 tell’,’0:1:1:1:1:0 else’,’0:1:1:1:1:1 elicit’,
’0:1:1:1:2:0 tell’,’0:1:1:1:2:1 tell’,’0:1:1:2:0:0 tell’,’0:1:1:2:1:0 elicit’,
’0:1:1:2:1:1 else’,’0:1:1:2:2:0 else’,’0:1:1:2:2:1 elicit’,’0:1:2:0:0:0 tell’,
’0:1:2:0:0:1 tell’,’0:1:2:0:1:0 elicit’,’0:1:2:0:1:1 elicit’,’0:1:2:0:2:1 elicit’,
’0:1:2:1:0:0 tell’,’0:1:2:1:0:1 tell’,’0:1:2:1:1:0 elicit’,’0:1:2:1:1:1 tell’,
’0:1:2:1:2:0 tell’,’0:1:2:1:2:1 elicit’,’0:1:2:2:0:0 tell’,’0:1:2:2:1:0 elicit’,
’0:1:2:2:1:1 elicit’,’0:1:2:2:2:0 tell’,’0:1:2:2:2:1 elicit’,’1:0:0:0:0:0 else’,
’1:0:0:0:1:0 else’,’1:0:0:0:1:1 else’,’1:0:0:0:2:1 else’,’1:0:0:1:0:0 else’,
’1:0:0:1:1:0 elicit’,’1:0:0:1:1:1 tell’,’1:0:0:1:2:0 elicit’,’1:0:0:1:2:1 else’,
’1:0:0:2:0:0 else’,’1:0:0:2:1:0 elicit’,’1:0:0:2:1:1 elicit’,’1:0:0:2:2:0 tell’,
352

’1:0:0:2:2:1 elicit’,’1:0:1:0:0:0 elicit’,’1:0:1:0:1:0 tell’,’1:0:1:0:1:1 elicit’,
’1:0:1:0:2:1 elicit’,’1:0:1:1:0:0 tell’,’1:0:1:1:1:0 else’,’1:0:1:1:1:1 elicit’,
’1:0:1:1:2:1 tell’,’1:0:1:2:0:0 elicit’,’1:0:1:2:1:0 elicit’,’1:0:1:2:2:0 elicit’,
’1:0:1:2:2:1 tell’,’1:0:2:0:0:0 tell’,’1:0:2:0:1:0 tell’,’1:0:2:0:1:1 tell’,
’1:0:2:0:2:1 tell’,’1:0:2:1:0:0 tell’,’1:0:2:1:1:0 elicit’,’1:0:2:1:1:1 tell’,
’1:0:2:1:2:0 elicit’,’1:0:2:1:2:1 elicit’,’1:0:2:2:0:0 tell’,’1:0:2:2:1:0 tell’,
’1:0:2:2:1:1 elicit’,’1:0:2:2:2:0 elicit’,’1:0:2:2:2:1 elicit’,’1:1:0:0:0:0 tell’,
’1:1:0:0:0:1 else’,’1:1:0:0:1:0 tell’,’1:1:0:0:1:1 else’,’1:1:0:0:2:0 elicit’,
’1:1:0:0:2:1 elicit’,’1:1:0:1:0:0 tell’,’1:1:0:1:0:1 elicit’,’1:1:0:1:1:0 else’,
’1:1:0:1:1:1 tell’,’1:1:0:1:2:0 elicit’,’1:1:0:1:2:1 tell’,’1:1:0:2:0:0 elicit’,
’1:1:0:2:0:1 else’,’1:1:0:2:1:0 tell’,’1:1:0:2:1:1 else’,’1:1:0:2:2:0 tell’,
’1:1:0:2:2:1 elicit’,’1:1:1:0:0:0 tell’,’1:1:1:0:0:1 tell’,’1:1:1:0:1:0 tell’,
’1:1:1:0:1:1 elicit’, ’1:1:1:0:2:0 tell’, ’1:1:1:0:2:1 elicit’, ’1:1:1:1:0:0 elicit’,
’1:1:1:1:0:1 elicit’, ’1:1:1:1:1:0 else’, ’1:1:1:1:1:1 tell’, ’1:1:1:1:2:0 tell’,
’1:1:1:1:2:1 elicit’, ’1:1:1:2:0:0 tell’, ’1:1:1:2:0:1 elicit’, ’1:1:1:2:1:0 tell’,
’1:1:1:2:1:1 tell’, ’1:1:1:2:2:0 tell’, ’1:1:1:2:2:1 tell’, ’1:1:2:0:0:0 elicit’,
’1:1:2:0:1:0 elicit’, ’1:1:2:0:1:1 elicit’, ’1:1:2:0:2:1 tell’, ’1:1:2:1:0:0 tell’,
’1:1:2:1:0:1 elicit’, ’1:1:2:1:1:0 elicit’, ’1:1:2:1:1:1 tell’, ’1:1:2:1:2:0 elicit’,
’1:1:2:1:2:1 elicit’, ’1:1:2:2:0:0 tell’, ’1:1:2:2:1:0 tell’, ’1:1:2:2:1:1 tell’,
’1:1:2:2:2:0 elicit’]

Policy: ’21skip’
Features: [18, 35, 45]
ECR: 46.744200
ECR Lower Bound: 41.828300
Mapping: [[0.30125, 0.6075], [0.25015, 0.45635], [0.25015, 0.45635]]
States: [’0:0:0 else’, ’0:1:1 else’, ’0:2:2 else’, ’1:0:0 else’, ’1:1:1 noskip’, ’1:2:2 else’, ’2:0:0
skip’, ’2:1:1 noskip’, ’2:2:2 noskip’]

353

Policy: ’22et’
Features: [24, 26, 27]
ECR: 79.291800
ECR Lower Bound: 58.21120
Mapping: [[0.067915], [21.303669], [2.2836]]
States: [’0:0:0 else’, ’0:0:1 elicit’, ’0:1:0 else’, ’0:1:1 elicit’, ’1:0:0 else’, ’1:0:1 elicit’,
’1:1:0 else’, ’1:1:1 elicit’]

Policy: 22skip’
Features: [5, 19, 27, 32]
ECR: 95.363400
ECR Lower Bound: 88.012500
Mapping: [[631.9412], [0.25], [2.459, 6.635], [0.498302]]
States: [’0:0:0:0 noskip’, ’0:0:0:1 noskip’, ’0:0:1:0 noskip’, ’0:0:1:1 noskip’, ’0:0:2:0
noskip’, ’0:0:2:1 noskip’, ’1:0:0:0 noskip’, ’1:0:0:1 noskip’, ’1:0:1:0 noskip’, ’1:0:1:1
noskip’, ’1:0:2:0 noskip’, ’1:0:2:1 noskip’, ’1:1:0:0 noskip’, ’1:1:0:1 skip’, ’1:1:1:0
noskip’, ’1:1:1:1 noskip’]

Policy: ’24et’
Features: [14, 18]
ECR: 36.355700
ECR Lower Bound: 30.768000
Mapping: [[0.5], [0.52575]]
States: [’0:0 elicit’, ’0:1 tell’, ’1:0 elicit’, ’1:1 else’]

354

Policy: ’24skip’
Features: [17, 47]
ECR: 49.194000
ECR Lower Bound: 33.139500
Mapping: [[0.5], [13.31575]]
States: [’0:0 noskip’, ’0:1 noskip’, ’1:0 noskip’, ’1:1 skip’]

Policy: ’27et’
Features: [5, 9, 19, 27]
ECR: 37.696400
ECR Lower Bound: 28.755200
Mapping: [[76.037616], [3933.717329], [0.21775, 0.71775], [0.999999]]
States: [’0:0:0:0 else’, ’0:0:0:1 elicit’, ’0:0:1:0 else’, ’0:0:1:1 elicit’, ’0:0:2:0 elicit’, ’0:0:2:1
elicit’, ’0:1:0:0 elicit’, ’0:1:0:1 elicit’, ’0:1:1:0 else’, ’0:1:1:1 elicit’, ’0:1:2:0 tell’,
’0:1:2:1 elicit’, ’1:0:0:0 else’, ’1:0:0:1 elicit’, ’1:0:1:0 else’, ’1:0:1:1 elicit’, ’1:0:2:0 tell’,
’1:0:2:1 tell’, ’1:1:0:0 tell’, ’1:1:0:1 elicit’, ’1:1:1:0 else’, ’1:1:1:1 elicit’, ’1:1:2:0 tell’,
’1:1:2:1 tell’]

Policy: ’27skip’
Features: [9, 40, 42, 46, 56]
ECR: 60.041000
ECR Lower Bound: 40.637100
Mapping: [[4831.936247], [6.2033], [0.5205], [6.40425], [3.70513]]
States: [’0:0:0:0:0 skip’, ’0:0:0:0:1 skip’, ’0:0:0:1:0 noskip’, ’0:0:0:1:1 noskip’, ’0:0:1:0:0
skip’, ’0:0:1:0:1 skip’, ’0:1:0:1:0 noskip’, ’0:1:0:1:1 noskip’, ’0:1:1:0:0 noskip’,
’0:1:1:0:1 noskip’, ’0:1:1:1:1 noskip’, ’1:0:0:0:0 skip’, ’1:0:0:0:1 skip’, ’1:0:0:1:0
noskip’, ’1:0:0:1:1 skip’, ’1:0:1:0:0 noskip’, ’1:0:1:0:1 noskip’, ’1:1:0:1:0 noskip’,
’1:1:1:0:0 noskip’, ’1:1:1:0:1 noskip’, ’1:1:1:1:1 noskip’]

355

Policy: ’28et’
Features: [5, 18, 46]
ECR: 67.974200
ECR Lower Bound: 51.863700
Mapping: [[73.053098], [0.393393], [5.3352]]
States: [’0:0:0 else’, ’0:0:1 tell’, ’0:1:0 tell’, ’0:1:1 tell’, ’1:0:0 else’, ’1:0:1 tell’, ’1:1:0
elicit’, ’1:1:1 tell’]

Policy: ’28skip’
Features: [14, 15, 25, 46, 60]
ECR: 67.365600
ECR Lower Bound: 35.707600
Mapping: [[0.5], [15.10085], [22.754688, 23.282859], [5.191], [0.103777]]
States: [’0:0:0:0:0 noskip’, ’0:0:1:0:0 skip’, ’0:0:1:0:1 noskip’, ’0:1:0:0:0 noskip’,
’0:1:0:0:1 skip’, ’0:1:0:1:0 noskip’, ’0:1:0:1:1 noskip’, ’0:1:1:0:0 skip’, ’0:1:1:0:1
noskip’, ’0:1:1:1:0 skip’, ’0:1:1:1:1 noskip’, ’0:1:2:0:0 noskip’, ’0:1:2:0:1 skip’,
’0:1:2:1:0 noskip’, ’0:1:2:1:1 noskip’, ’1:0:0:0:0 skip’, ’1:0:0:0:1 noskip’, ’1:0:0:1:1
skip’, ’1:0:1:0:0 noskip’, ’1:0:1:0:1 noskip’, ’1:0:1:1:1 noskip’, ’1:0:2:0:0 skip’,
’1:0:2:0:1 noskip’, ’1:0:2:1:1 skip’]

356

BIBLIOGRAPHY

[Ai and Litman, 2009] Ai, H. and Litman, D. (2009). Setting up user action probabilities in
user simulations for dialog system development. In Proceedings of the 47th Annual Meeting
of the Association for Computational Linguistics: Human Language Technologies (ACL).
Suntec, Singapore.
[Ainsworth and Fleming, 2005] Ainsworth, S. and Fleming, P. (2005). Evaluating a mixedinitiative authoring environment: Is redeem for real? In [Looi et al., 2005], pages 9–16.
[Aleven et al., 2005] Aleven, V., McLaren, B. M., and Koedinger, K. R. (2005). Rapid
development of computer-based tutors with the cognitive tutor authoring tools (ctat). In
[Looi et al., 2005], page 990.
[Aleven et al., 2006] Aleven, V., McLaren, B. M., Sewall, J., and Koedinger, K. R. (2006).
The cognitive tutor authoring tools (ctat): Preliminary evaluation of efficiency gains. In
[Ikeda et al., 2006], pages 61–70.
[Aleven et al., 2004] Aleven, V., Ogan, A., Popescu, O., Torrey, C., and Koedinger, K. R.
(2004). Evaluating the effectiveness of a tutorial dialogue system for self-explanation. In
[Lester et al., 2004], pages 443–454.
[Anderson et al., 1995] Anderson, J. R., Corbett, A. T., Koedinger, K. R., and Pelletier,
R. (1995). Cognitive tutors: Lessons learned. The Journal of the Learning Sciences,
4(2):167–207.
[Aström, 1965] Aström, K. J. (1965). Optimal control of markov decision processes with the
incomplete state estimation. Journal of Computer and System Sciences, 10:174–205.
[Baker et al., 2004a] Baker, R. S., Corbett, A. T., and Koedinger, K. R. (2004a). Detecting
student misuse of intelligent tutoring systems. In [Lester et al., 2004], pages 531–540.
[Baker et al., 2004b] Baker, R. S., Corbett, A. T., Koedinger, K. R., and Wagner, A. Z.
(2004b). Off-task behavior in the cognitive tutor classroom: when students ”game the
system”. In Dykstra-Erickson, E. and Tscheligi, M., editors, CHI, pages 383–390. ACM.
357

[Barnes and Stamper, 2008] Barnes, T. and Stamper, J. C. (2008). Toward automatic hint
generation for logic proof tutoring using historical student data. In [Woolf et al., 2008],
pages 373–382.
[Beck et al., 2000] Beck, J., Woolf, B. P., and Beal, C. R. (2000). Advisor: A machine
learning architecture for intelligent tutor construction. In AAAI/IAAI, pages 552–557.
AAAI Press / The MIT Press.
[Beck, 2001] Beck, J. E. (2001). Advisor: A Machine-Learning Architecture for Intelligent
Tutor Construction. PhD thesis, Graduate School of the University of Massachusetts
Amherst.
[Beck and Mostow, 2008] Beck, J. E. and Mostow, J. (2008). How who should practice:
Using learning decomposition to evaluate the efficacy of different types of practice for
different types of students. In [Woolf et al., 2008], pages 353–362.
[Bernsen and Dybkjaer, 1997] Bernsen, N. O. and Dybkjaer, L. (1997). Designing Interactive Speech Systems: From First Ideas to User Testing. Springer-Verlag New York, Inc.,
Secaucus, NJ, USA.
[Bloom, 1984] Bloom, B. S. (1984). The 2 sigma problem: The search for methods of group
instruction as effective as one-to-one tutoring. Educational Researcher, 13:4–16.
[Cade et al., 2008] Cade, W. L., Copeland, J. L., Person, N. K., and D’Mello, S. K. (2008).
Dialogue modes in expert tutoring. In [Woolf et al., 2008], pages 470–479.
[Calzolari et al., 2006] Calzolari, N., Cardie, C., and Isabelle, P., editors (2006). ACL 2006,
21st International Conference on Computational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics, Proceedings of the Conference, Sydney,
Australia, 17-21 July 2006. The Association for Computer Linguistics.
[Cen et al., 2006] Cen, H., Koedinger, K. R., and Junker, B. (2006). Learning factors analysis
- a general method for cognitive model evaluation and improvement. In [Ikeda et al., 2006],
pages 164–175.
[Cen et al., 2007] Cen, H., Koedinger, K. R., and Junker, B. (2007). Is over practice necessary? - improving learning efficiency with the cognitive tutor through educational data
mining. In [Luckin et al., 2007], pages 511–518.
[Chades et al., 2005] Chades, M. C., Garcia, F., and Sabbadin, R. (2005). Mdp toolbox v2.0
for matlab.
[Chae et al., 2005] Chae, H. M., Kim, J. H., and Glass, M. (2005). Effective behaviors in a
comparison between novice and expert algebra tutors. In Proceedings of Sixteenth Midwest
AI and Cognitive Science Conference, Dayton, pages 25–30.
358

[Chi et al., 2008a] Chi, M., Jordan, P. W., VanLehn, K., and Hall, M. (2008a). Reinforcement learning-based feature seleciton for developing pedagogically effective tutorial dialogue tactics. In de Baker, R. S. J., Barnes, T., and Beck, J. E., editors, EDM, pages
258–265. www.educationaldatamining.org.
[Chi and VanLehn, 2008] Chi, M. and VanLehn, K. (2008). Eliminating the gap between the
high and low students through meta-cognitive strategy instruction. In [Woolf et al., 2008],
pages 603–613.
[Chi et al., 1994] Chi, M. T. H., de Leeuw, N., Chiu, M.-H., and LaVancher, C. (1994).
Eliciting self-explanations improves understanding. Cognitive Science, 18(3):439–477.
[Chi et al., 2008b] Chi, M. T. H., Roy, M., and Hausmann, R. G. M. (2008b). Observing tutorial dialogues collaboratively: Insights about human tutoring effectiveness from
vicarious learning. Cognitive Science, 32(2):301–342.
[Chi et al., 2004] Chi, M. T. H., Siler, S., and Jeong, H. (2004). Can tutors monitor students’
understanding accurately? Cognition and Instruction, 22(3):363–387.
[Chi et al., 2001] Chi, M. T. H., Siler, S., Jeong, H., Yamauchi, T., and Hausmann, R. G.
(2001). Learning from human tutoring. Cognitive Science, 25:471–533.
[Cho et al., 2000] Cho, B.-I., Michael, J. A., Rovick, A. A., and Evens, M. W. (2000). An
analysis of multiple tutoring protocols. In Gauthier, G., Frasson, C., and VanLehn, K.,
editors, Intelligent Tutoring Systems, volume 1839 of Lecture Notes in Computer Science,
pages 212–221. Springer.
[Clark et al., 1976] Clark, C. M., Snow, R. E., and Shavelson, R. J. (1976). Three experiments on learning to teach. Journal of Teacher Education, 27:174–180.
[Cohen et al., 1982] Cohen, P. A., Kulik, J. A., and Kulik, C.-L. C. (1982). Educational outcomes of tutoring: A meta-analysis of findings. American Educational Research Journal,
19(2):237–248.
[Coley, 2001] Coley, R. J. (2001). Differences in the gender gap: Comparisons across
racial/ethnic groups in education and work. policy information report. Technical report, Educational Testing Service, Rosedale Road, Princeton, NJ 08541-0001 609-7345694; (www.ets.org/research).
[Collins et al., 1989] Collins, A., Brown, J. S., and Newman, S. E. (1989). Cognitive apprenticeship: Teaching the craft of reading, writing and mathematics. In Resnick, L. B.,
editor, Knowing, learning and instruction: Essays in honor of Robert Glaser, chapter 14,
pages 453–494. Lawrence Erlbaum Associates: Hillsdale New Jersey.
[Collins and Stevens, 1982] Collins, A. and Stevens, A. (1982). Goals and strategies for
inquiry teachers. Advances in Instructional Psychology, 2:65–119.
359

[Conati and VanLehn, 2000] Conati, C. and VanLehn, K. (2000). Toward computer-based
support and cristina conati and kurt vanlehn. International Journal of Artificial Intelligence in Education, 11:398–415.
[Core et al., 2003] Core, M. G., Moore, J. D., and Zinn, C. (2003). The role of initiative in
tutorial dialogue. In EACL, pages 67–74.
[Eugenio et al., 2006] Eugenio, B. D., Kershaw, T. C., Lu, X., Corrigan-Halpern, A., and
Ohlsson, S. (2006). Toward a computational model of expert tutoring: A first report. In
Sutcliffe, G. and Goebel, R., editors, FLAIRS Conference, pages 503–508. AAAI Press.
[Evens and Michael, 2006] Evens, M. and Michael, J. (2006). One-on-one Tutoring By Humans and Machines. Mahwah, NJ: Erlbaum.
[Forbes-Riley et al., 2007] Forbes-Riley, K., Litman, D. J., Purandare, A., Rotaru, M., and
Tetreault, J. R. (2007). Comparing linguistic features for modeling learning in computer
tutoring. In [Luckin et al., 2007], pages 270–277.
[Frampton and Lemon, 2005] Frampton, M. and Lemon, O. (2005). Reinforcement learning
of dialogue strategies using the user’s last dialogue act. In Proceedings of the IJCAI
Workshop on K&R in Practical Dialogue Systems, pages 62–67.
[Frampton and Lemon, 2006] Frampton, M. and Lemon, O. (2006). Learning more effective
dialogue strategies using limited dialogue move features. In [Calzolari et al., 2006].
[Gallagher, 2001] Gallagher, T. (2001). Equal opportunities commission conference on boys
and girls in the 21st century: Gender differences in learning. Technical report, Equal
Opportunities Commission.
[Graesser et al., 1995] Graesser, A. C., Person, N., and Magliano, J. (1995). Collaborative
dialog patterns in naturalistic one-on-one tutoring. Applied Cognitive Psychology, 9:359–
387.
[Graesser et al., 2001] Graesser, A. C., VanLehn, K., Rosé, C. P., Jordan, P. W., and Harter, D. (2001). Intelligent tutoring systems with conversational dialogue. AI Magazine,
22(4):39–52.
[Hauskrecht, 1997] Hauskrecht, M. (1997). Planning and control in stochastic domains with
imperfect information. PhD thesis, MIT. Available as Technical Report: MIT-LCS-TR738, 1997.
[Henderson et al., 2005] Henderson, J., Lemon, O., and Georgila, K. (2005). Hybrid reinforcement/supervised learning for dialogue policies from communicator data. In IJCAI
Workshop on K&R in Practical Dialogue Systems, pages 68–75.
[Hume et al., 1995] Hume, G., Michael, J., Rovick, A., and Evens, M. (1995). Controlling
active learning: how tutors decide when to generate hints. In Proceedings of the 8th Florida
Artificial Intelligence Research Symposium., pages 157–161.
360

[Ikeda et al., 2006] Ikeda, M., Ashley, K. D., and Chan, T.-W., editors (2006). Intelligent
Tutoring Systems, 8th International Conference, ITS 2006, Jhongli, Taiwan, June 26-30,
2006, Proceedings, volume 4053 of Lecture Notes in Computer Science. Springer.
[Janarthanam and Lemon, 2009] Janarthanam, S. and Lemon, O. (2009). User simulations
for online adaptation and knowledge-alignment in troubleshooting dialogue systems. In
Proceedings of the 12th SEMdial Workshop on on the Semantics and Pragmatics of Dialogues.
[Jolliffee, 2002] Jolliffee, I. T. (2002). Principal Component Analysis, Series: Springer Series
in Statistics. Springer, New York, 2nd edition.
[Jordan et al., 2007] Jordan, P. W., Hall, B., Ringenberg, M. A., Cue, Y., and Rosé, C. P.
(2007). Tools for authoring a dialogue agent that participates in learning studies. In
[Luckin et al., 2007], pages 43–50.
[Jordan et al., 2006] Jordan, P. W., Ringenberg, M. A., and Hall, B. (2006). Tools for
authoring a dialogue agent that participates in learning studies. In Proceedings of ITS06
Workshop on Teaching with Robots, Agents, and NLP.
[Katz et al., 2007] Katz, S., Connelly, J., and Wilson, C. (2007). Out of the lab and into the
classroom: An evaluation of reflective dialogue in andes. In [Luckin et al., 2007], pages
425–432.
[Katz et al., 2000] Katz, S., O’Donnell, G., and Kay, H. (2000). An approach to analyzing
the role and structure of reflective dialogue. International Journal of Artificial Intelligence
and Education, 11:320–343.
[Kim et al., 2005] Kim, J. H., Chae, H. M., and Glass, M. (2005). Expert and novice algebra
tutor behaviors compared (poster abstract). In Proceedings of the 27th Annual Conference
of the Cognitive Science Society, COGSCI 2005, Stresa, Italy.
[Koedinger and Aleven, 2007] Koedinger, K. R. and Aleven, V. (2007). Exploring the assistance dilemma in experiments with cognitive tutors. Educational Psychology Review,
19(3):239–264.
[Koedinger et al., 1997] Koedinger, K. R., Anderson, J. R., Hadley, W. H., and Mark, M. A.
(1997). Intelligent tutoring goes to school in the big city. International Journal of Artificial
Intelligence in Education, 8(1):30–43.
[Lane and VanLehn, 2005] Lane, H. C. and VanLehn, K. (2005). Teaching the tacit knowledge of programming to novices with natural language tutoring. Computer Science Education, 15(3):183–201.
[Lemon et al., 2006] Lemon, O., Georgila, K., and Henderson, J. (2006). Evaluating effectiveness and portability of reinforcement learned dialogue strategies with real users: the
talk towninfo evaluation. In IEEE/ACL Spoken Language Technology.
361

[Lester et al., 2004] Lester, J. C., Vicari, R. M., and Paraguaçu, F., editors (2004). Intelligent Tutoring Systems, 7th International Conference, ITS 2004, Maceiò, Alagoas, Brazil,
August 30 - September 3, 2004, Proceedings, volume 3220 of Lecture Notes in Computer
Science. Springer.
[Levin and Pieraccini, 1997] Levin, E. and Pieraccini, R. (1997). A stochastic model of
computer-human interaction for learning dialogue strategies. In In EUROSPEECH 97,
pages 1883–1886.
[Litman and Silliman, 2004] Litman, D. J. and Silliman, S. (2004). Itspoke: an intelligent
tutoring spoken dialogue system. In HLT-NAACL ’04: Demonstration Papers at HLTNAACL 2004 on XX, pages 5–8, Morristown, NJ, USA. Association for Computational
Linguistics.
[Looi et al., 2005] Looi, C.-K., McCalla, G. I., Bredeweg, B., and Breuker, J., editors (2005).
Artificial Intelligence in Education - Supporting Learning through Intelligent and Socially
Informed Technology, Proceedings of the 12th International Conference on Artificial Intelligence in Education, AIED 2005, July 18-22, 2005, Amsterdam, The Netherlands, volume
125 of Frontiers in Artificial Intelligence and Applications. IOS Press.
[Lu et al., 2007] Lu, X., Eugenio, B. D., Kershaw, T. C., Ohlsson, S., and Corrigan-Halpern,
A. (2007). Expert vs. non-expert tutoring: Dialogue moves, interaction patterns and
multi-utterance turns. In Gelbukh, A. F., editor, CICLing, volume 4394 of Lecture Notes
in Computer Science, pages 456–467. Springer.
[Luckin et al., 2007] Luckin, R., Koedinger, K. R., and Greer, J. E., editors (2007). Artificial Intelligence in Education, Building Technology Rich Learning Contexts That Work,
Proceedings of the 13th International Conference on Artificial Intelligence in Education,
AIED 2007, July 9-13, 2007, Los Angeles, California, USA, volume 158 of Frontiers in
Artificial Intelligence and Applications. IOS Press.
[McArthur et al., 1982] McArthur, D., Stasz, C., and Zmuidzinas, M. (1982). Tutoring techniques in algebra. Cognition and Instruction, 7(3):197–244.
[Merrill et al., 1995] Merrill, D. C., Reiser, B. J., Merrill, S. K., and Landes, S. (1995).
Tutoring: Guided learning by doing. Cognition and Instruction, 13(3):315–372.
[Merrill et al., 1992] Merrill, D. C., Reiser, B. J., Ranney, M., and Trafton, J. G. (1992).
Effective tutoring techniques: A comparison of human tutors and intelligent tutoring
systems. The Journal of the Learning Sciences, 2(3):277–306.
[Moore et al., 2004] Moore, J. D., Porayska-Pomsta, K., Varges, S., and Zinn, C. (2004).
Generating tutorial feedback with affect. In Barr, V. and Markov, Z., editors, FLAIRS
Conference. AAAI Press.
362

[Murray and VanLehn, 2006] Murray, R. C. and VanLehn, K. (2006). A comparison of
decision-theoretic, fixed-policy and random tutorial action selection. In [Ikeda et al., 2006],
pages 114–123.
[Murray et al., 2003] Murray, T., Blessing, S., and Ainsworth, S. (2003). Authoring Tools for
Advanced Technology Learning Environments. Kluwer Academic/Springer Pub.: Netherlands.
[Noe, 1986] Noe, R. A. (1986). Trainee’s attributes and attitudes: neglected influences on
training effectiveness’. Academy of Management Review, 11:736–749.
[Paek and Chickering, 2005] Paek, T. and Chickering, D. (2005). The markov assumption
in spoken dialogue management. In 6th SIGDial Workshop on Discourse and Dialogue.
[Purandare and Litman, 2008] Purandare, A. and Litman, D. J. (2008). Content-learning
correlations in spoken tutoring dialogs at word, turn, and discourse levels. In Wilson, D.
and Lane, H. C., editors, FLAIRS Conference, pages 439–443. AAAI Press.
[Putnam, 1987] Putnam, R. T. (1987). Structuring and adjusting content for students: A
study of live and simulated tutoring of addition. American Educational Research Journal,
24(1):13–48.
[Quek et al., 2002] Quek, C. L., Wong, A. F., and Fraser, B. J. (2002). Gender differences
in the perceptions of chemistry laboratory classroom environments. Queensland Journal
of Educational Research, 18:164–182.
[Raux et al., 2005] Raux, A., , Langner, Bohus, D., and Eskenazi, M. (2005). Let’s go public!
taking a spoken dialog system to the real world. In Proceedings of Interspeech (Eurospeech).
[Reif and Scott, 1999] Reif, F. and Scott, L. A. (1999). Teaching scientific thinking skills:
Students and computers coaching each other. American Journal of Physics, 67(9):819–831.
[Rieser and Lemon, 2006] Rieser, V. and Lemon, O. (2006). Using machine learning to explore human multimodal clarification strategies. In [Calzolari et al., 2006].
[Rose et al., 2001] Rose, C. P., Moore, J. D., VanLehn, K., and Allbritton, D. (2001). A
comparative evaluation of socratic versus didactic tutoring. In Proceedings of Cognitive
Sciences Society, pages 869–874.
[Rudnicky et al., 1999] Rudnicky, A., Thayer, E., Constantinides, P., Tchou, C., Shern, R.,
Lenzo, K., Xu, W., and Oh, A. (1999). Creating natural dialogs in the carnegie mellon
communicator system. In Proceedings of Eurospeech., volume 4, pages 1531–1534.
[Singh et al., 1999] Singh, S. P., Kearns, M. J., Litman, D. J., and Walker, M. A. (1999).
Reinforcement learning for spoken dialogue systems. In Solla, S. A., Leen, T. K., and
Müller, K.-R., editors, NIPS, pages 956–962. The MIT Press.
363

[Singh et al., 2002] Singh, S. P., Litman, D. J., Kearns, M. J., and Walker, M. A. (2002).
Optimizing dialogue management with reinforcement learning: Experiments with the njfun
system. J. Artif. Intell. Res. (JAIR), 16:105–133.
[Sleeman et al., 1989] Sleeman, D. H., Kelly, A. E., Martinak, R., Ward, R. D., and Moore,
J. L. (1989). Studies of diagnosis and remediation with high school algebra students.
Cognitive Science, 13(4):551–568.
[Stamper et al., 2007] Stamper, J. C., Barnes, T., and Croy, M. J. (2007). Extracting student
models for intelligent tutoring systems. In AAAI, pages 1900–1901. AAAI Press.
[Sutton and Barto, 1998] Sutton, R. S. and Barto, A. G. (1998). Reinforcement Learning.
MIT Press Bradford Books.
[Swerts et al., 2000] Swerts, M., Litman, D., and Hirshberg, J. (2000). Corrections in spoken
dialogue systems. In Proceedings of the Sixth International Conference on Spoken Language
Processing (ICSLP 2000), volume 2, pages 615–618.
[Tetreault and Litman, 2006a] Tetreault, J. and Litman, D. (2006a). Using reinforcement
learning to build a better model of dialogue state. In Proceedings 11th Conference of
the European Chapter of the Association for Computational Linguistics (EACL), Trento,
Italy.
[Tetreault et al., 2007] Tetreault, J. R., Bohus, D., and Litman, D. J. (2007). Estimating
the reliability of mdp policies: a confidence interval approach. In Sidner, C. L., Schultz,
T., Stone, M., and Zhai, C., editors, HLT-NAACL, pages 276–283. The Association for
Computational Linguistics.
[Tetreault and Litman, 2006b] Tetreault, J. R. and Litman, D. J. (2006b). Comparing the
utility of state features in spoken dialogue using reinforcement learning. In Moore, R. C.,
Bilmes, J. A., Chu-Carroll, J., and Sanderson, M., editors, HLT-NAACL. The Association
for Computational Linguistics.
[Tetreault and Litman, 2008] Tetreault, J. R. and Litman, D. J. (2008). A reinforcement
learning approach to evaluating state representations in spoken dialogue systems. Speech
Communication, 50(8-9):683–696.
[VanLehn, 1999] VanLehn, K. (1999). Rule learning events in the acquisition of a complex
skill: An evaluation of cascade. Journal of the Learning Sciences, 8(1):71–125.
[VanLehn, 2006] VanLehn, K. (2006). The behavior of tutoring systems. International Journal Artificial Intelligence in Education, 16(3):227–265.
[VanLehn, 2009] VanLehn, K. (2009). The two-sigma effect revisited: A meta-analysis of
human tutoring and several types of computer tutoring. (submitted).
364

[VanLehn et al., 2007a] VanLehn, K., Graesser, A. C., Jackson, G. T., Jordan, P., Olney,
A., and Rose, C. P. (2007a). When are tutorial dialogues more effective than reading?
Cognitive Science, 31(1):3–62.
[VanLehn et al., 2007b] VanLehn, K., Jordan, P., and Litman, D. (2007b). Developing pedagogically effective tutorial dialogue tactics: Experiments and a testbed. In Proceedings
of SLaTE Workshop on Speech and Language Technology in Education ISCA Tutorial and
Research Workshop.
[VanLehn et al., 2002] VanLehn, K., Jordan, P. W., Rosé, C. P., Bhembe, D., Böttner, M.,
Gaydos, A., Makatchev, M., Pappuswamy, U., Ringenberg, M. A., Roque, A., Siler, S.,
and Srivastava, R. (2002). The architecture of why2-atlas: A coach for qualitative physics
essay writing. In Cerri, S. A., Gouardères, G., and Paraguaçu, F., editors, Intelligent
Tutoring Systems, volume 2363 of Lecture Notes in Computer Science, pages 158–167.
Springer.
[VanLehn et al., 2005] VanLehn, K., Lynch, C., Schulze, K., Shapiro, J. A., Shelby, R.,
Taylor, L., Treacy, D., Weinstein, A., and Wintersgill, M. (2005). The andes physics
tutoring system: Lessons learned. Int. J. Artif. Intell. Ed., 15(3):147–204.
[VanLehn et al., 2003] VanLehn, K., Siler, S., Murray, R. C., Yamauchi, T., and Baggett,
W. B. (2003). Why do only some events cause learning during human tutoring? Cognition
and Instruction, 21(3):209–249.
[Vygotsky, 1971] Vygotsky, L. (1971). Interaction between learning and development. In
Cole, T. M., editor, In Mind in Society., pages 79–91. Harvard University Press: Cambridge Massachusetts.
[Walker, 2000] Walker, M. A. (2000). An application of reinforcement learning to dialogue
strategy selection in a spoken dialogue system for email. Journal of Aritificial Intelligence
Research, 12:387–416.
[Williams et al., 2005] Williams, J., Poupart, P., and Young, S. (2005). Factored partially
observable markov decision processes for dialogue management. In Int. Joint Conf. on
Artificial Intelligence.
[Williams and Young, 2007a] Williams, J. and Young, S. (2007a). Partially observable
markov decision processes for spoken dialog systems. Computer Speech and Language,
21(2):231–422.
[Williams and Young, 2007b] Williams, J. and Young, S. (2007b). Scaling pomdps for spoken
dialog management. IEEE Trans. on Audio, Speech, and Language Processing.
[Woolf et al., 2008] Woolf, B. P., Aı̈meur, E., Nkambou, R., and Lajoie, S. P., editors
(2008). Intelligent Tutoring Systems, 9th International Conference, ITS 2008, Montreal,
Canada, June 23-27, 2008, Proceedings, volume 5091 of Lecture Notes in Computer Science. Springer.
365

[Young, 1999] Young, S. (1999). Probabilistic methods in spoken dialogue systems. Philosophical Transactions of the Royal Society (Series A, 358:1389–1402.

366

To Elicit Or To Tell: Does It Matter?
Min CHIa, Pamela JORDANa, Kurt VANLEHN b and Diane LITMANa
a
Learning Research Development Center, University of Pittsburgh
b
Computer Science & Engineering, Arizona State University
Abstract. While high interactivity has been one of the main characteristics of oneon-one human tutoring, a great deal of controversy surrounds the issue of whether
interactivity is indeed the key feature of tutorial dialogue that impacts students’
learning results. There are two commonly held hypotheses regarding the issue: a
widely-believed monotonic interactivity hypothesis and a better supported
interaction plateau hypothesis. The former hypothesis predicts increasing in
interactivity causes an increase in learning while the latter states that increasing
interactivity yields increasing learning until it hits a plateau, and further increases
in interactivity do not cause noticeably increase in learning. In this study, we
proposed the tactical interaction hypothesis which predicts beyond a certain level
of interactivity, further increases in interactivity do not cause increase in learning
unless they are guided by effective tutorial tactics. Overall our results support this
hypothesis. However, finding effective tactics is not easy. This paper sheds some
light on how to apply Reinforcement Learning to derive effective tutorial tactics.
Keywords. Reinforcement Learning, Intelligent Tutoring Systems, Natural
Language Tutoring Systems, Pedagogical Tutorial Tactics, Knowledge Component

Introduction
High interactivity is a key characteristic of one-on-one human tutoring. Whereas a
classroom lecture can be viewed as monologue consisting of a long sequence of tutor
instructions or “tell” acts, individual tutoring features much give and take and can be
viewed as a mixture of tutor questions or elicit acts, students’ responses, and tutor
instructions. A common assumption, often referred as the monotonic interaction
hypothesis [11][13], is that greater interactivity causes greater learning. Most Intelligent
Tutoring Systems (ITSs), especially Natural Language (NL) tutoring systems, are
designed to be highly interactive.
However, previous studies have shown that when the content of the instruction is
strictly controlled to be equivalent in all conditions, highly interactive tutoring (such as
human tutoring) is seldom more effective than moderately interactive instruction (such as
step-based NL tutoring systems), even though both are often more effective than low
interaction instruction (e.g. answer based instruction) [3][6][11]. A detailed review of the
literature [13] distinguished between the widely-believed monotonic interactivity
hypotheses and the better supported interaction plateau hypothesis. The former states that
increase in interactivity causes an increase in learning while the latter states that
increasing interactivity yields increasing learning until it hits a plateau, and further
increases in interactivity do not cause noticeably increase in learning. Thus the key
difference between the two hypotheses is whether increasing interaction would impact
students’ learning gain or not. The monotonic interactivity hypothesis predicts high >
moderate while the interaction plateau hypothesis predict high = moderate.
Most studies cited above did not focus on the role of tutoring tactics in guiding
interaction and its effects on tutor success. Chi et al and others have showed that human

tutors may not always select optimal tutorial actions [2][11]. Human tutors have to make
tutorial decisions from their episodic memory of tutoring sessions and then execute them
in real time with limited resources. Therefore, in this paper we propose a third hypothesis:
the tactical interaction hypothesis. It states that increasing interactivity does not yield
increasing learning unless they are guided by effective tutorial tactics. Tutorial tactics are
policies used to select the optimal tutorial action at any given time from the available set.
Thus, we hypothesize that: (high + effective tactics) > moderate and (high + no/ineffective
tactics) = moderate.
To investigate the three hypotheses, we focused on two tutorial actions: elicit and tell.
An elicit asks students a question about the problem at hand. Whereas a tell presents the
information directly to the students. Figure 1 presents an example comparing elicit and
tell versions of the same topics extracted from a log file we collected. Both tutorial
dialogues start and end with the same tutor turns (lines 1 and 5 in (a) and (b)). However,
the tutor chooses to elicit first then tell in (a) (lines 2-3 and line 4 respectively) and instead
to tell first and then elicit in part (b) (line 2 and lines 3-4 respectively). Note that (a) and
(b) cover the same corresponding content. Generally speaking, eliciting more information
from the students during tutoring results in a more interactive tutorial dialogue. In this
paper, we defined interactivity in terms of the elicit-tell ratio, which is defined as the
number of elicits a student received divided by the number of tells he/she received in a
given tutorial dialogue. The higher this value, the more interactive the tutorial dialogue.
1. Tutor: So let's start with determining the value of KE0.
2. Tutor: Which principle will help you calculate the rock's instantaneous magnitude of velocity
at T1? {elicit}
3. Student: definition of kinetic energy
4. Tutor: Let me just write the equation for you: ke1 = ½*m*v1^2. {tell}
5. Tutor: From ke1 = ½*m*v1^2, we get v1^2=ke1/(0.5*m). We substitute…
(a) Elicit-Tell Version
1. Tutor: So let's start with determining the value of KE0.
2. Tutor: To calculate the rock's instantaneous magnitude of velocity at T1, we will apply the
definition of kinetic energy again. {tell}
3. Tutor: Please write the equation for how the definition of kinetic energy applies to this problem
at T1 {elicit}
4. Student: ke1 = ½*m*v1^2
5. Tutor: From ke1 = ½*m*v1^2, we get v1^2=ke1/(0.5*m). We substitute…
(b) Tell-Elicit Version
Figure 1. Elicit vs. Tell

Unlike the other two hypotheses, validation of the tactical interaction hypothesis
relies on an important assumption that we have effective tutorial tactics. Most tutorial
tactics for ITSs are encoded as hand-coded rules that seek to implement cognitive and/or
pedagogical theories. The theories may or may not have been well-evaluated. Typically,
system designers and domain expert design tutorial tactics by hand and make many
nontrivial design decisions. It is often not easy to evaluate these decisions because the
performance of these tutorial tactics depends on many other factors, such as the difficulty
of domain content, the student’s competence, the usability of the system, how easily the
dialogues are understood, and so on. Previous research has primarily treated the
specification of tutorial tactics as a system design problem: several versions of a system
are created and the only difference among them is the tutorial tactics used. Data is
collected with human subjects interacting with these different versions of the system and
results from students' performance on different versions are statistically compared. Due to
the costs of experiments, only a handful of policies are typically explored. Yet, many such
other reasonable tutorial tactics are still possible.

In recent years, work on the design of dialogue systems has involved several datadriven methodologies. Among them, Markov decision processes (MDP) and
Reinforcement Learning (RL) have been most widely applied. In this study, we applied
RL to semi-automatically induce effective tutorial tactics. We say semi-automatically
because manual effort was necessary to identify the relevant feature states. We used a
complex task domain where it is common to view the yet to-be-learned knowledge as
comprised of several independently learned components, called Knowledge Components
(KCs) [1]. A KC is "a generalization of everyday terms like concept, principle, fact, or
skill, and cognitive science terms like schema, production rule, misconception, or
facet"[12]. For the purposes of our tutoring system these are the atomic units of
knowledge. Techniques exist to re-engineer the definition of KCs so that they are
independently learnable [1], and this improves the overall effectiveness of the resulting
tutoring system. It is commonly assumed that these KCs are learned independently. For
example, various standardized tests are built based on the assumed independence among
these KCs. Since KCs are assumed to be learned independently, we argue that tutorial
tactics specific to each KC should also be induced independently. That is, when the tutor
is about to mention a KC, whether to use an elicit or a tell should depend on the student’s
current mastery of that KC, its intrinsic difficulty, and the dialogue history. Thus, the best
elicit/tell policy for one KC might not be optimal for another. In this study, we have eight
primary KCs. We induced eight policies and conducted eight tests of three hypotheses,
once per KC.
Later results indicated that every student in this study received at least some elicit
prompts in each KCs, thus based on the standard of [13] the least interactive dialogues we
collected are still moderately interactive. We expect that on all KCs:
1.If the interactivity hypothesis is correct, the group with higher elicit-tell ratios would
learn more.
2.If the interaction plateau hypothesis is correct, students would learn equally well
regardless of interactivity difference.
3.If the tactical interaction hypothesis is correct and our RL-based tutorial tactics are
indeed effective, students trained with effective, more interactive tutorial instruction
would learn more than those with less effective, lower interactive ones.
First we will briefly describe how we apply RL to the NL tutoring system we used in
this study. Then we will describe our approach and finally present our results.

1. Applying RL to NL Tutoring Systems
RL is a machine learning method that centers on the maximization of expected rewards
and has commonly used Markov Decision Processes (MDP’s) [9] to model a dialogue. An
MDP describes a stochastic control process whose state transitions possess the Markov
property. An MDP formally corresponds to a 4-tuple (S, A, T, R), in which: S = {S1,…,
Sn}, is a state space. A={A1, …, Am} is an action space represented by a set of action
variables; T : S × A × S → [0, 1] is a set of transition probabilities between states that
P ak

describe the dynamics of the modeled system; for example: Si, Sj is the probability that the
model would transition from state Sj to state Si by taking action Ak. Finally, R : S × A × S
→ R denotes a reward model that assigns rewards to state transitions and models payoffs
associated with such transitions. While, π: S → A is defined as a policy.
In order to effectively derive tutorial tactics using RL, we followed the following
procedure (See [4] for details). In the first stage, we built a NL tutoring system named

Cordillera-explore, in which decisions on when to elicit or tell were made randomly. Then
a group of students, called the Exploratory group, were trained on it. More specifically,
they 1) took a background survey, 2) read a textbook covering the target domain
knowledge, 3) took a pretest, 4) trained on Cordillera-explore, and 5) took a posttest. Each
individual student’s interaction log together with his/her pre- and post-test scores is
treated as one whole dialogue. The Exploratory corpus was the collection of these
dialogues.
In the second stage, we used the Exploratory corpus to construct the MDP’s 4-tuple
<S, A, T, R>. Ideally, S should summarize the dialogue history compactly, employing the
fewest features possible, while retaining the relevant information about the dialogue
interaction. For RL, as with all machine learning tasks, success is dependent upon
choosing an appropriate feature set for representing states. We used a procedure that
began by defining a large set of features. In particular, we started by defining 18 features
based upon the four categories of features considered by [7] to be relevant for human
tutors when making their tutorial decisions: autonomy, temporal situation, problem
solving state, and performance. We then select a small subset from them. For this study
we only included features that could be computed automatically because hand coded
dialogue features would be infeasible given that the feature values need to be available in
real time when the learned policies are used to control the tutoring system.
For each dialogue in the Exploratory corpus, a scalar performance measure, the
reward, was needed. We defined reward based on students’ normalized learning gains
(NLG). More specifically, students were split into two groups by the median value of their
NLG. We gave the better-performing half of students' dialogues a positive reward of +100
and the remaining ones a negative reward of -100. The rewards were assigned in the final
dialogue state. Following [8], we view each student’s dialogue as a trajectory of chosen
tutorial actions determined by dialogue context and system actions:
S1 → (A1, R1) → S2 → (A2, R2)… Sn → (An, Rn)
Here Si → (Ai, Ri) → Si+1 means that at the ith turn, the current state (i.e, context of
dialogue, students performance) was in state Si, the NL tutor executed action Ai and
received reward Ri, and then the state changed to Si+1. The first state S1 reflects the
student’s performance on the pre-test. For each student, the reward is delayed, and thus
we have R1… Rn-1 all equal to 0 and only the final reward Rn equals to either 100 or 100 depending on the student’s NLG. The problem of deriving effective tutorial tactics
thus becomes calculating the optimal policy for certain action decisions in an MDP. To
calculate the best policy, we used Tetreault and Litman’s tool since it has proven to be
both reliable and successful [10]. In order to learn a policy for each KC, we annotated our
tutoring dialogues (final kappa ≥ 0.77 for each of the eight KCs) and action decisions
based on which KCs a tutor action or tutor-student pair of turns covered. Additionally, we
have mapped students' pre- and post-test scores to the relevant KCs for each test item.
While there are other KCs involved in the tutorial dialogue, they appeared significantly
less frequently and often co-occurred with these eight main KCs. Thus, they are not our
focus in this study.
The high cost of collecting human data precludes us from collecting a large
Exploratory corpus. Given the complexity of task at hand, we therefore need to conduct
effective feature selection. In this study, we followed a greedy-like search of the feature
space. More specifically, for each of the 18 features, we employed the MDP to induce a
single-feature policy. MDP generally requires discrete features, therefore we employed a
median split to convert all numerical features into binary variables. Thus, for each KC, we
have 18 single-feature-policies. For each of four categories of features, we selected the
one feature which produced the single-feature policy with the highest Expected

Cumulative Reward (ECR) in the category. ECR is calculated by normalizing the value of
each state by the number of times it occurs in a dialogue and then summing over all states.
The higher the ECR, the more effective the learned policy is expected to be. The four
features selected were then used to induce a more complicated four-category-feature
policy (see Figure 2 for an example). We then picked the one policy that has the highest
ECR from the 19 learned policies: 18 single-feature policies and one four-category-feature
policy. We call this resulting policy the Greedy-RL tutorial tactics. Figure 2 shows an
example of one such policy for KC21 (definition of gravitational potential Energy).
In Figure 2, line 1 (‘features’) indicates the four features involved in the policy for
KC21: duration, ProblemComplexity, tellsSinceElicit, and pctCorrectKCSession. Line 2
(‘cutoff’) lists the median values used to convert the three corresponding features from
real numbers to binary values. A total of 16 rules were learned: in 8 situations the tutor
should elicit (line 4), in 5 it would tell (line 5); in the remaining 3 the tutor can do either
(line 6). For example, “0:MED:1:0” (bolded and underlined in line 4) means when the
duration since the most recent decision made on KC21 is less than 50s, the
ProblemComplexity of the current problem is medium, the students has been told at least
once since the most recent elicit (tellsSinceElicit), and the student’s performance on
KC21in current session is less than 71.79% correct, then the tutor should elicit the next
step from the student. As can be seen, the derived four-category-feature tutorial tactics are
quite subtle.
1. 'features'=[duration,ProblemComplexity,tellsSinceElicit, pctOverallCorrectKC],
2. 'cutoff' =[ duration =’50.0' tellsSinceElicit ='0.0001' pctCorrectKCSession ='0. 7179' ],
3. 'policy':
4. 'elicit: [0:MED:1:0, 1:COMP:1:0, 0:COMP:1:1, 0:MED:0:0, 0:COMP:1:0, 0:MED:1:1,
0:COMP:0:1, 1:COMP:0:1],
5. 'tell: [1:MED:0:1, 1:MED:0:0, 1:MED:1:0, 1:MED:1:1, 0:MED:0:1]
6. 'else: [0:COMP:0:0, 1:COMP:0:0, 1:COMP:1:1]
Figure 2. A Greedy-RL Policy On KC21: Gravitational Potential Energy

In the final stage, we replaced the random policy with these Greedy-RL policies and
called the new system Cordillera-GreedyRL. We then trained a new group of students on
the new system. All students went through the same training procedure as in Stage 1.

2. Approach
2.1. Participants
All participants in the training were required to have basic knowledge of high-school
algebra, no experience with college-level physics, and were paid for their time. Each
student trained during and completed the study in a period of two to three weeks. Data
was collected in two stages. The first data collection with Cordillera-explore lasted over
four months during the fall of 2007 and 64 students completed the experiment. The
second data collection with Cordillera-GreedyRL lasted over three months during spring
2008 and 37 students completed the experiment. Therefore, we have a total of 101
students who finished the study: 64 were in the Exploratory group and 37 were in the
GreedyRL group.

2.2. Domains & Main Knowledge Components
The experiment used the Physics work-energy domain as covered in a first-year college
physics course. The eight primary KCs were: the weight law (KC1), definition of work

(KC14), Definition of Kinetic Energy (KC20), Gravitational Potential Energy (KC21),
Spring Potential Energy (KC22), Total Mechanical Energy (KC24), Conservation of Total
Mechanical Energy (KC27), and Change of Total Mechanical Energy (KC28).

2.3. Procedure
All students went through the same online training procedure: 1) background survey, 2)
textbook, 3) pre-test, 4) training on the tutoring system, and 5) post-test. For each
principle, the textbook provided a general description and reviewed some examples. The
textbook was not available to students during any other phase of the experiment. Both
tests were taken online, and once an answer was submitted, students automatically
proceeded to the next question without any feedback on the correctness of their answer.
Students were not allowed to return to earlier questions. The pre- and post-tests were
identical. There were 33 problems selected from the Physics literature on the tests. In
phase 4, students first walked through a demonstration problem with Cordillera. Then, all
students solved the same seven problems in the same order. The seven training problems
were ordered roughly by increasing complexity.

2.4. Grading Criteria
All tests were graded in a double-blind manner by a single experienced grader who was
not familiar with the hypotheses being tested. The maximum score for each problem was
1. Additionally, the grader identified all relevant KCs and gave a score for each KC
application. Each problem was assigned a difficulty weighting so that the total score
possible on the test was 100 points for 33 problems. We evaluated the student’s
competence on each KC separately weighted by the problem difficulty. That is, given a
problem containing KC21 with difficulty 6, the student would receive 6 points if they
completed the KC21 correctly in that problem irrespective of their work on the other KCs
in it. All KC-based scores were normalized by dividing with the corresponding total
maximum possible scores.

3. Results
Our data was collected at different times and thus students were not randomly assigned to
the groups. The Exploratory group had higher incoming competence than the Greedy-RL
group as measured by pre-test score: t(99) = 2.00, p < 0.05. This fact is important because
we have found that highly competent students often manage to learn regardless of
instructional methods [4]. Our results show that students scored significantly higher in the
posttest than pretest: F(1, 64) = 12.71, p=0.001 for the Exploratory group and F(1, 37)
=16.061, p=0.000 for the Greedy-RL group respectively. On a KC by KC basis, both
conditions learned significantly on all the main KCs save for KC14 and KC28. On these
KCs, no significant difference between pre- and post-test scores was found in the
Exploratory group: F(1, 64)=0.251, p=0.617 for KC14 and F(1, 64)=2.80, p=0.097 for
KC28; however, a significant difference was found in the Greedy-RL group: F(1,
37)=4.10, p=0.047 for KC14 and F(1, 37)=4.175, p=0.045 for KC28 respectively. Thus,
following the Greedy-RL tutorial tactics on KC14 and KC28, students performed
significantly better in the posttest than in the pretest but not when the decisions were
randomly made. This suggests that the derived tutorial tactics may be effective.
Given the unbalanced incoming competence between the two groups, we used an
ANCOVA to factor out pretest scores and compared the resulting adjusted posttest scores
for the two groups We found that the Greedy-RL group had significantly higher adjusted

post-test scores than the Exploratory group on just one KC: KC21, F(1)= 4.93, p<0.029.
However, no significant differences were found between the two groups on the adjusted
scores on the other seven KCs.
compare elicit-tell ratio
Greedy-RL

3.5

Exploratory

3
2.5
2
1.5
1
0.5
0
KC1**

KC14***

KC20***

KC21***

KC22

KC24***

KC27

KC28***

Figure 3. Compare the elicit-tell ratio across the two groups of students

We next investigated the interactive characteristics of the derived KC-based tutorial
tactics by comparing the tutorial dialogues’ elicit-tell ratios between the two groups. For
the Explore group, the ratios approached 1/1 for each KC, as Cordillera-Explore randomly
chose between elicit and tell. As Figure 3 shows, the elicit-tell ratio of the Greedy-RL
group varied depending on the policy, with some KCs getting more elicits than tells (KC1,
KC14, KC21 and KC28) and some getting more tells than elicits (KC20 and KC24). For
these KCs, the elicit-tell ratios were significantly different from the elicit-tell ratios of the
Exploratory group (all are at the level p≤.001). On KC22 and KC27, no significant
difference was found between the two groups in terms of their elicit-tell ratios. Thus, on 6
of the 8 main KCs, the Greedy-RL policies clearly resulted in significantly different
interactive patterns from the random selection.

4. Discussion
The monotonic interactivity hypothesis states that more interactivity lead to increased
learning. If this is true, we would expect the Exploratory group to have learned more than
the Greedy-RL group on KC20 and KC24. Similarly, on KC1, KC14, KC21, and KC28
the Greedy-RL group should learn more than the Exploratory. However, the only
significant difference between the two groups in terms of adjusted post-test scores was on
KC21. On the other 5 KCs, the groups did not differ. Thus, on 5 out of 6 KCs, the data
does not provide much support for the monotonic interactivity hypothesis.
The interaction plateau hypothesis states that more interactivity beyond given point
will not increase learning and thus students would learn equally well regardless of
interactivity levels. If this is true, then we expect the students should learn equally across
the board. However, on KC21 the more interactive group (Greedy-RL) learned more than
the less interactive group (Exploratory). On the other 7 KCs, the groups did not differ, as
reported earlier. Thus, neither of these two hypotheses is consistently supported across the
board.
Finally, the tactical interaction hypothesis states that more interactivity beyond given
point does not cause increases in learning unless it is governed by effective the tutorial
tactics. If this is true and all our derived RL-based policy were indeed effective, we expect
that on KC1, KC14, KC21, and KC28, Greedy-RL would learn more than the Exploratory
while KC20 and KC24, no difference should been found. This hypothesis was supported

on KC21, KC20 and KC24 but not on KC1, KC14, and KC28. One likely explanation is
that the tutorial tactics we derived for KC1, KC14, and KC28 were not effective enough.
This is likely given the fact that our state features were relatively restricted and our feature
selection procedure is quite greedy. Additionally, in subsequent work we found a
significant correlation between the predicted rewards for the derived policies used in this
study and the actual improvements made. Moreover, applying improved feature selection
methods has yielded policies with higher predicted rewards than the Greedy-RL policies
employed in this study (at least double see [4] for a detailed discussion).
Therefore, we concluded that the tactical interaction hypothesis is still supported by
our results but our application of RL in the study was not optimal. This is why we did not
see clear difference between the two groups on KC1, KC14, and KC28. In particular, our
features or our Greedy-RL method for selecting them may have been flawed (see [4] for a
detailed discussion). Overall, our results suggest that the tactical interaction hypothesis
may be right but deriving effective tutorial tactics is not easy. However, RL seems to be
an effective, useful tool to derive tutorial tactics.
Additionally, we have identified a number of questions for future exploration. For
example, we only included features that can be computed automatically; grader tagged
features have not been tested. In future studies, we will employ a larger feature set, better
policies, will randomly assign subjects to groups, and experiment with new policy
induction mechanisms.

References
[1] Cen, H., Koedinger, K. R., & Junker, B. (2006). Learning Factors Analysis: A general method for
cognitive model evaluation and improvement. In M. Ikeda, K. D. Ashley, T.-W. Chan (Eds.)
Proceedings of the 8th International Conference on ITS, 164-175. Berlin: Springer-Verlag.
[2] Chi, M.T.H., Siler, S.A. & Jeong, H. (2004). Can tutors monitor students’ understanding accurately?
Cognition and Instruction, 22(3): 363-387.
[3] Chi, M. T. H., Siler, S., Jeong, H., Yamauchi, T.,&Hausmann, R. G. (2001). Learning from human
tutoring. Cognitive Science, 25, 471–533.
[4] Chi, M & VanLehn, K. (2008). Eliminating the Gap between the High and Low Students through MetaCognitive Strategy Instruction. 9th International Conference, ITS2008, pp 603-613.
[5] Chi, M, Jordon, P, VanLehn, K, & Litman, D (in progress) Applying Reinforcement Learning To Induce
Effective Pedagogical Knowledge-Components Based Tutorial Tactics
[6] Katz, S., Connelly, J., & Allbritton, D. (2003). Going beyond the problem given: How human tutors use
post-solution discussions to support transfer. JAIED, 13, 79-116.
[7] Moore, J.D, Porayska-Pomsta, K, Varges, S, Zinn, C (2004): Generating Tutorial Feedback with Affect.
FLAIRS Conference
[8] Singh, S., Kearns, M. S., Litman, D. J., & Walker, M. A. (1999). Reinforcement learning for spoken
dialogue systems. In Proc. NIPS 99
[9] Sutton, R. S. and Barto, A. G. (1998). Reinforcement Learning: An Introduction.A Bradford Book. The
MIT Press.
[10] Tetreault, J., and Litman D (2008). A Reinforcement Learning Approach to Evaluating State
Representations in Spoken Dialogue Systems. Speech Communication Volume 50 , Issue 8-9 Pages
683-696
[11] VanLehn, K., Graesser, A. C., Jackson, G. T., Jordan, P., Olney, A., & Rose, C. P. (2007). When are
tutorial dialogues more effective than reading? Cognitive Science 31(1), 3-62.
[12] VanLehn, K., Jordan, P., Litma, D., (2007). Developing pedagogically effective tutorial dialogue tactics:
Experiments and a testbed. In proceedings of SLaTE Workshop.
[13] VanLehn, K. (submitted). The interaction plateau: Less interactive tutoring is often just as effective as
human tutoring.

Defining the Behavior of an Affective Learning
Companion in the Affective Meta-tutor Project
Sylvie Girard, Maria Elena Chavez-Echeagaray, Javier Gonzalez-Sanchez,
Yoalli Hidalgo-Pontet, Lishan Zhang, Winslow Burleson, and Kurt VanLehn
Arizona State University, Computing, Informatics, and Decision Systems Engineering,
Tempe, AZ, 85281, U.S.A.
{sylvie.girard,helenchavez,javiergs,lzhang90,yhidalgo,
winslow.burleson,kurt.vanlehn}@asu.edu

Abstract. Research in affective computing and educational technology has
shown the potential of affective interventions to increase student’s self-concept
and motivation while learning. Our project aims to investigate whether the use
of affective interventions in a meta-cognitive tutor can help students achieve
deeper modeling of dynamic systems by being persistent in their use of metacognitive strategies during and after tutoring. This article is an experience
report on how we designed and implemented the affective intervention. (The
meta-tutor is described in a separate paper.) We briefly describe the theories of
affect underlying the design and how the agent’s affective behavior is defined
and implemented. Finally, the evaluation of a detector-driven categorization
of student behavior, that guides the agent’s affective interventions, against a
categorization performed by human coders, is presented.
Keywords: affective computing, affective learning companion, intelligent tutoring
system, robust learning, meta-cognition.

1

Introduction

Research in AIED has taken interest in the potential of using interventions of affective
nature in intelligent tutoring systems to improve learning [2, 19, 23] and motivation
[8, 13, 20] and to reduce undesirable behaviors such as gaming [3-5] and undesirable
affective states such as disengagement [17]. The interventions have been designed to
either respond to student’ specific behavior [14, 19], or to elicit a certain emotional
state in the student [9], often by providing cognitive support and scaffolds within the
learning environment.
The hypothesis of our project [24] is that affective interventions in a metacognitive tutor can help students achieve robust learning by being persistent in their
use of meta-cognitive strategies during and after tutoring. In order to test this
hypothesis, an affective intervention was designed, using an affective learning companion to convey the affective message. This article describes the design of the affective intervention. In the first section, a three-dimensional design space of affective
interventions is outlined, along with our choice along each dimension. The second
K. Yacef et al. (Eds.): AIED 2013, LNAI 7926, pp. 21–30, 2013.
© Springer-Verlag Berlin Heidelberg 2013

22

S. Girard et al.

section describes the implementation of the design using categorization of student
behavior based on log data detectors. The last section describes an empirical evaluation of the classification accuracy.

2

Design of the Affective Intervention

2.1

Definition of the Affective Intervention

Over the past decade, numerous affective interventions have been designed and evaluated with respect to alternate techniques in the field of educational technology. In
order to define a design space of the affective intervention for the AMT project, a
review of current research was performed. The design space has three dimensions:
mechanism for delivery of the affective intervention, timing of the intervention, and
type of message delivered during the intervention. We briefly describe each dimension, then indicate where along it our design falls.
Mechanism: How Is the Intervention Message Conveyed?
There are various ways to intervene affectively in tutoring systems, ranging from the
presentation of an affective message via a user-interface component [2, 19], to the use
of bio-feedback and affect-sensitive tutors that respond to the user’s emotional state
[9]. Some results [2,8,12,23] have shown the potential of using pedagogical agents, or
Affective Learning Companion (ALC), to portray the affective message. These interventions involve design decisions concerning the different components of a pedagogical agent that can impact learning, such as the presence of facial expressions or
deictic gestures [2,14], vocal intonation [6], gender [2,8,16], or ethnicity and student’s
cultural background [12,19].
In this phase of our project affective messages in the form of pop-up text messages
are provided by a pedagogical agent, represented by an image with neutral facial expression. The agent is a humanoid comic-like gendered character, representing a
student of a similar age to our target population (16-21 yrs olds). This decision took
into account the results from [12] for the agent’s image type, and [2, 23] where pairing students’ gender to the agent’s gender was found beneficial for user’s self-concept
and learning.
Timing: When Is the Affective Intervention Happening in the Learning Process?
The affective intervention can happen before any tutoring takes place, between learning
tasks during the tutoring, and at different moments while a learner is performing a
specific task or learning a specific set of skills. In order to describe when the affective
intervention occurs, we first must describe the instruction.
The AMT software teaches students how to create and test a model of a dynamic
system. The instruction is divided into three phases: (1) an introduction phase where
students learn basic concepts of dynamic system model construction and how to
use the interface; (2) a training phase where students are guided by a tutor and a
meta-tutor to create several models; and (3) a transfer phase where all scaffolding is

Defining the Behavior of an Affective Learning Companion

23

removed from software and students are free to model as they wish. The tutor gives
feedback and corrections on domain mistakes. The meta-tutor requires students to
follow a goal-reduction problem solving strategy, using the Target Node Strategy
[24], which decomposes the overall modeling problem into a series of “atomic” modeling problems whose small scope encourages students to engage in deep modeling
rather than shallow guess-based modeling strategies. Using various measures of
deep and shallow learning [5], an experiment demonstrated that requiring students to
follow this strategy during training did indeed increase the frequency of deep modeling compared to students who were not required to follow the strategy. However, the
effect was not strong, and the amount of deep modeling could certainly be improved.
The goal of the ALC is to encourage students to do even more deep modeling.
The pedagogical agent conveying the affective message in AMT intervenes at three
different moments of software interaction:
•

•

•

At the beginning and the end of the introduction: These interventions aim to
introduce the agent and its role in the instruction, as well as building rapport
between the student and the ALC which has been shown in [7] to help keep
students motivated and on task.
Between each modeling task in the training phase: The main purpose of these
interventions is to invite the student to reflect on his/her actions and decisions during the task, as well as maintain the interest of the student. As performing a given task can require from 3 to 15 minutes, the ALC intervenes
after each task rather than intervening after a pre-defined number of tasks as
in [1,2,23].
At the end of the training phase: This intervention tries to convince the student to persevere in the use of the deep modeling strategy during the forthcoming transfer phase.

Type: What Type of Message Is Given/Transmitted During the Intervention?
Finally, the third dimension of the intervention represents its affective or motivational
content: what does the ALC say and what emotional tone does it use when saying it?
Our design is based on the following policies:
•

Baylor and Kim [6] showed that a combination of cognitive and affective
interventions (the “Mentor”) led to better student self-regulation and selfefficacy than the presence of either type of intervention alone. Our meta-tutor
and tutor already provide cognitive information without affect (like the “Expert”
of [6]). To avoid boring redundancy, the ALC presents as little cognitive and
meta-cognitive content as possible (just enough to maintain context) while
presenting motivational messages (described below) in a friendly, encouraging
manner.

The content of the intervention has been designed to help low-achievers and shallow
learners get back on track and avoid gaming [3-5, 9, 19], while not interrupting highachievers who might not benefit from an affective intervention [2, 19, 23]. It involves
the following theories:

24

S. Girard et al.

•

•
•

•

Dweck’s “the mind is a muscle” theory [10]: the more you exercise your
mind, the more competent you become. Before the introduction phase, all
students read a text introducing this theory. The between-task interventions
reinforce the message by mentioning passages of the reading and referring to
how different activities help to improve the brain’s function.
Attribution theory [21]: failures should be attributed to the difficulty of the
task or lack of preparation, whereas success should be attributed to the student’s effort.
Theory of reflection [15]: Students have been found to be more receptive after completing a problem rather than during problem solving [15]. Every
time a task is finished the ALC invites students to reflect on what they have
experienced. It encourages them to replicate the action if it was positive or
to change the action if it was negative.
Use of a meta-cognitive representation of student’s modeling depth [1, 22]:
Alongside the ALC is a bar showing the depth of the student’s modeling
while working on the current task. That is, it shows the proportion of student
actions that were classified as deep, based on the detectors described in [11].
ALC messages often refer to the modeling depth bar in combination with the
other theories listed above.

The following section illustrates how we defined the ALC behavior by using learners’
prior interactions with the system.

3

Implementing the ALC’s Behavior

While students learn, their motivation and attention to detail can fluctuate. In the context of a problem solving activity requiring modeling skills, the depth of the modeling
techniques used by students can also vary. The ALC should adapt to these fluctuations, presenting different affective messages depending on the student’s recent behavior. Simply mapping the student’s behavior onto competence would not suffice, so
we defined several behavioral classifications such as “engaged,” “gaming” and “lack
of planning.” We then defined log data detectors relevant to each behavioral classification. We also paired affective messages with each behavioral classification. In
the first subsection, the detectors that measure the user’s behavior are described. The
second sub-section then describes the behavioral classification, how they were created
and how they are mapped to the detectors’ output.
3.1

How to Detect Shallow Modeling Practices?

The detectors process a stream of user interface activity (log data) and output behavioral measures. The detectors require no human intervention and run in real time,
because they will eventually be used to regulate the system’s responses to the student.
Our detectors extend the gaming detectors of [4] by including measures relevant to
depth of modeling and other constructs.

Defining the Behavior of an Affective Learning Companion

25

Nine detectors were defined. The first six detectors were based on classifying and
counting segments in the log, where a segment corresponds roughly to a correct step
in the construction or debugging of a model. Each segment holds the value of the
detector that best represents the situation, for example a student showing both a single_answer and good_method behavior would be defined as following a
good_method behavior for this segment. The output per task for each detector is a
proportion: the number of segments meeting its criteria divided by the total number of
segments in the log for the task. Based on an extensive video analysis of student’s
past actions and HCI task modeling techniques [11], six segmental detectors were
defined:
•
•
•
•
•
•

GOOD_METHOD: The students followed a deep method in their modeling.
They used the help tools1 provided appropriately including the one for planning each part of the model.
VERIFY_INFO: Before checking their step for correctness, students looked
back at the problem description, the information provided by the instruction
slides, or the meta-tutor agent.
SINGLE_ANSWER: The student’s initial response for this step was correct, and the student did not change it.
SEVERAL_ANSWERS: The student made more than one attempt at completing the step. This includes guessing and gaming the system.
UNDO_GOOD_WORK: This action suggests a modeling misconception on
the students’ part. One example is when students try to run the model when
not all of the nodes are fully defined.
GIVEUP: The student gave up on finding the answer and clicked on the
“give up” button.

A limitation of the above detectors is the inability to distinguish between a student
trying hard to complete a step but making a lot of errors versus a student gaming or
guessing a lot. This led to the development of two additional detectors based on earlier work in detecting robust learning and gaming [5, 9, 18, 23]: (1) the time spent on
task and (2) the number of times the learner misused the “run model” button. While
the former is self-explanatory and commonly used in ITSs, the latter is specific to the
AMT software. As students construct a system dynamics model, they can reach a
point where all elements are sufficiently defined to “run the model” (the model is
correct in terms of syntax) and therefore test whether its semantics corresponds to the
system they were asked to model. Students clicking on this button before the model’s
syntax is correct, or clicking repetitively on the model without making changes once
it is correct in syntax but not in semantics, is considered shallow behavior that shows
a lack of planning, a lack of understanding of the task to perform, or a tendency to
guess/game the answer rather than think it through.

1

Two help systems are available to users: (1) referring back to the instructions always available for
viewing, and (2) looking at the problem situation where all details of the dynamic system to
model are described.

26

S. Girard et al.

The ninth and last detector is a function of the six segmental detectors. It is intended to measure the overall depth of the students’ modeling. Although it is used as
an outcome measure in the transfer phase, it helps drive the ALC during the training
phase. It is based on considering two measures (GOOD_ANSWER, VERIFY_INFO)
to indicate deep modeling, one measure (SINGLE_ANSWER) to be neutral, and three
measures (SEVERAL_ANSWERS, UNDO_GOOD_WORK, and GIVE_UP) to indicate shallow modeling.
In order to facilitate writing rules that defined the students’ behavioral category
(e.g., engaged, gaming, etc.) in terms of the detector outputs, we triaged the output of
each detector so it reports its output as either low, medium and high. The rules are
mostly driven by the values: low and high. To implement the triage, we collected
logs from 23 students. For each of the nine detectors, we determine the 33rd and 66th
percentile points and used them as thresholds. Thus, for each detector, roughly a
third of the 23 students were reported as low, as medium and as high. Because the
tasks vary in complexity, different thresholds were calculated for each task.
3.2

From Shallow Learning Detection to the ALC Intervention

A series of 6 types of ALC behavioral categories were defined using video analysis of
past user’s actions on software. Human coders reviewed screen-capture videos and
verbal protocols of a pool of 20 students using the meta-cognitive tutor. Following
their recommendations and a review of messages transmitted in affective interventions in the literature, the following set of ALC categories was defined:
• Good Modeling: The students think about their steps, do not hesitate to go
back to the introduction or the situation to look for answers, use the plan feature judiciously in their creation of nodes, and have a minimum of guessing and wrong actions
performed on task.
• Engaged: The students respond by thinking about the problem rather than
guessing, refer back to the instructions or problem situation when they find themselves stuck rather than trying all possible answers. The students take a medium to a
high amount of time to complete the task, favoring reflection to quick decisions.
• Lack of Planning: The students answer quickly, relying heavily on the feedback given in the interface to get the next steps. While the students sometimes refer to
instructions and the situation, they only use the features when they are stuck, not
when planning the modeling activity.
• Help Avoidance: The students attempt a lot of answers without referring back
to the instructions or the problem situation. They rarely make use of the information
filled in the plan tab and try to skip the meta-tutor instructions. Instead of using help
when they are confused, they spend a lot of time trying to get the interface green or
give up rather than thinking about the problem.
• Gaming: The students try multiple possible answers within the interface
without pausing long enough to think about the problem. They may give up when this
random guessing doesn't work. They rarely refer to the instructions or the problem
situation and pay little attention to the plan tab or the meta-tutor instructions.
• Shallow Modeling (default, not recognized as the above mentioned categories): The students tend to try several answers on the interface rather than pausing
and thinking about the problem. They sometimes refer back to the instructions and
problem situation, but not frequently.

Defining the Behavior of an Affective Learning Companion

27

Table 1. Examples of ALC intervention between-task
Behavior
Good Modeling
Engaged

Lack of Planning

Help Avoidance

Gaming

Shallow Modeling
(default)

Example
You’re a Green Master! What was your secret? I know… you make your reading
count and thus your brain is getting rewired.
Even though it might take a little bit longer, it is worth it to explore the available
resources. You are giving your brain a great workout. Look at that green bar! Keep
up the good work!
Going fast is good, but it doesn't always help you reach your potential… Why don't
you stop and think about what you want to model when you are confused. To
make more of the bar green, try re-reading the problem description and noting what
it asks you to do.
It might be worth rereading the problem description and paying more attention to
the suggestions presented by the pop-up messages. Our brain needs to engage the
material deeply so it can create good connections. That’s how we can get more of
the bar green!
Hmmm! It seems that you need to put quality time into your tasks. Maybe "trial
and error" is not always the best strategy. Although you might move quickly
through the problem, your brain doesn’t get a workout, and it shows in the length
of the green bar.
You are getting there! Look at that bar! But remember that to strengthen your
brain you have to engage the problem and all its details.

Once these six behaviors were defined, human coders applied them to a sample of
100 tasks and students. The outputs of the detectors on the sample were obtained,
and rules were defined to map their values to the behavioral categories.
Using the theories of affect defined in section 2, ALC messages were created for
each behavior in order to provide affective support to the learner. A stereotypical
message was first created, as illustrated in table 1, for each behavior. The research
group then created many synonymous versions of each message, so that the ALC
would not repeat itself and thus reduce the student’s perception of the ALC as an
artificial agent. A separate message was produced for the first and last task performed by the user in the training phase, in order to introduce and wrap-up the ALC
interventions.

4

Evaluation of the Behavior’s Accuracy

Before working with students, we first tested the detectors and behavioral categorizer
via test cases. We wrote scenarios of software use that typified each of the six
behavioral categories. A member of the research group enacted each scenario, and
we confirmed that the detector outputs fell in the anticipated range (low, typical or
high) and that the rules assigned the anticipated behavioral classification.
The second part of the validation of ALC behaviors involved pilot subjects and
human coders. Seven college students used the AMT system with the ALC turned on.
They were asked to speak aloud as they worked. Their voice and screen were recorded as videos. A sample video was made from screen recordings. It included 15
tasks. Three human coders watched each task, paying attention to the depth of modeling shown by the student’s actions. Independently of what the software chose, they

28

S. Girard et al.

chose the ALC intervention that they felt best matched the student’s modeling practices. A multi-rater and pairwise kappa was then performed, and showed a sufficient
level of inter-reliance with a level of .896.

5

Conclusion and Future Work

This article described the development of an affective intervention based on an affective learning companion (ALC) that works with a meta-tutor and a tutor. It described
the theories of affect underlying the interventions, and how we defined and implemented the ALC’s behavior. The ALC’s messages were based on deciding which of
six behavioral categories best represented the student’s work on the most recently
completed task. This categorization was driven by log data. When compared to
human coders working with screen captures and verbal reports of students, the detector-driven categorizations agreed with the human coding with a kappa of .896.
The next step in the research is to measure the benefits of this version of the ALC
in a two-condition experiment. One group of students will use the system with the
ALC turned on during the training phase, and the other will used it without the ALC
turned on. We hypothesize that this will cause measurable differences in the depth of
students’ modeling during the transfer phase.
The forthcoming evaluation will also have students wear physiological sensors
while they work so that we can collect calibration data that will be used to supplement
the detectors’ assessment of the students’ affective state. This extra information will
be used to help define affective interventions not only between tasks but also while
the learner performs on task.
Acknowledgements. This material is based upon work supported by the National
Science Foundation under Grant No. 0910221.

References
1. Arroyo, I., Ferguson, K., Johns, J., Dragon, T., Meheranian, H., Fisher, D., et al.: Repairing disengagement with non-invasive interventions. Frontiers in Artificial Intelligence and
Applications, vol. 158, p. 195 (2007)
2. Arroyo, I., Woolf, B.P., Cooper, D.G., Burleson, W., Muldner, K.: The Impact of Animated Pedagogical Agents on Girls’ and Boys’ Emotions, Attitudes, Behaviors and Learning. In: Proceedings of the 2011 IEEE 11th International Conference on Advanced Learning Technologies. Proceedings from ICALT 2011, Washington, DC, USA (2011)
3. Baker, R.S.J.d., et al.: Adapting to when students game an intelligent tutoring system. In:
Ikeda, M., Ashley, K.D., Chan, T.-W. (eds.) ITS 2006. LNCS, vol. 4053, pp. 392–401.
Springer, Heidelberg (2006)
4. Baker, R.S.J.d., Gowda, S.M., Corbett, A.T.: Towards predicting future transfer of learning. In: Biswas, G., Bull, S., Kay, J., Mitrovic, A. (eds.) AIED 2011. LNCS, vol. 6738, pp.
23–30. Springer, Heidelberg (2011)

Defining the Behavior of an Affective Learning Companion

29

5. Baker, R.S.J.d., Gowda, S.M., Corbett, A.T., Ocumpaugh, J.: Towards automatically detecting whether student learning is shallow. In: Cerri, S.A., Clancey, W.J., Papadourakis,
G., Panourgia, K. (eds.) ITS 2012. LNCS, vol. 7315, pp. 444–453. Springer, Heidelberg
(2012)
6. Baylor, A.L., Kim, Y.: Simulating instructional roles through pedagogical agents. International Journal of Artificial Intelligence in Education 15(2), 95–115 (2005)
7. Bickmore, T.W., Picard, R.W.: Establishing and maintaining long-term human-computer
relationships. ACM Transactions on Computer-Human Interaction (TOCHI) 12(2), 293–
327 (2005)
8. Burleson, W., Picard, R.W.: Gender-Specific Approaches to Developing Emotionally Intelligent Learning Companions. IEEE Intelligent Systems 22(4), 62–69 (2007),
doi:10.1109/MIS.2007.69
9. D’Mello, S.K., Lehman, B., Person, N.: Monitoring affect states during effortful problem
solving activities. International Journal of Artificial Intelligence in Education 20(4), 361–
389 (2010), doi:10.3233/JAI-2010-012
10. Dweck, C.: Self-Theories: Their role in motivation, personality and development. Psychology Press, Philadelphia (2000)
11. Girard, S., Zhang, L., Hidalgo-Pontet, Y., VanLehn, K., Burleson, W., Chavez-Echeagary,
M.E., Gonzalez-Sanchez, J.: Using HCI task modeling techniques to measure how deeply
students model. In: Chad Lane, H., Yacef, K., Mostow, J., Pavlik, P. (eds.) AIED 2013.
LNCS (LNAI), vol. 7926, pp. 766–769. Springer, Heidelberg (2013)
12. Gulz, A.: Benefits of Virtual Characters in Computer Based Learning Environments:
Claims and Evidences. International Journal of Artificial Intelligence in Education 14(3),
313–334 (2004)
13. Gulz, A., Haake, M., Silvervarg, A.: Extending a teachable agent with a social conversation module – effects on student experiences and learning. In: Biswas, G., Bull, S., Kay, J.,
Mitrovic, A. (eds.) AIED 2011. LNCS, vol. 6738, pp. 106–114. Springer, Heidelberg
(2011)
14. Hayashi, Y.: On pedagogical effects of learner-support agents in collaborative interaction.
In: Cerri, S.A., Clancey, W.J., Papadourakis, G., Panourgia, K. (eds.) ITS 2012. LNCS,
vol. 7315, pp. 22–32. Springer, Heidelberg (2012)
15. Katz, S., Connelly, J., Wilson, C.: Out of the lab and into the classroom: An evaluation of
reflective dialogue in Andes. In: Proceeding of the 2007 Conference on Artificial Intelligence in Education: Building Technology Rich Learning Contexts That Work, pp. 425–
432 (2007)
16. Kim, Y., Baylor, A., Shen, E.: Pedagogical agents as learning companions: the impact of
agent emotion and gender. Journal of Computer Assisted Learning 23(3), 220–234 (2007)
17. Lehman, B., D’Mello, S., Graesser, A.: Interventions to regulate confusion during Learning. In: Cerri, S.A., Clancey, W.J., Papadourakis, G., Panourgia, K. (eds.) ITS 2012.
LNCS, vol. 7315, pp. 576–578. Springer, Heidelberg (2012)
18. Muldner, K., Burleson, W., Van de Sande, B., VanLehn, K.: An analysis of students’ gaming behaviors in an intelligent tutoring system: predictors and impacts. User Modeling and
User-Adapted Interaction 21(1-2), 99m–135m (2011), doi:10.1007/s11257-010-9086-0
19. Rodrigo, M.M.T., Baker, R.S.J.d., Agapito, J., Nabo, J., Repalam, M.C., Reyes, S.S., San
Pedro, M.O.C.Z.: The Effects of an Interactive Software Agent on Student Affective Dynamics while Using an Intelligent Tutoring System. IEEE Transactions on Affective Computing 3, 224–236 (2012), doi:http://doi.ieeecomputersociety.org/10.
1109/T-AFFC.2011.41

30

S. Girard et al.

20. Wang, N., Johnson, W.L., Mayer, R.E., Rizzo, P., Shaw, E., Collins, H.: The politeness effect: Pedagogical agents and learning outcomes. International Journal of Human-Computer
Studies 66(2), 98–112 (2008), doi:10.1016/j.ijhcs.2007.09.003
21. Weiner, B.: An attributional theory of achievement motivation and emotion. Psychological
Review 92(4), 548 (1985)
22. Walonoski, J.A., Heffernan, N.T.: Prevention of off-task gaming behavior in intelligent tutoring systems. In: Ikeda, M., Ashley, K.D., Chan, T.-W. (eds.) ITS 2006. LNCS,
vol. 4053, pp. 722–724. Springer, Heidelberg (2006)
23. Woolf, B.P., Arroyo, I., Muldner, K., Burleson, W., Cooper, D.G., Dolan, R., Christopherson, R.M.: The Effect of Motivational Learning Companions on Low Achieving Students
and Students with Disabilities. In: Aleven, V., Kay, J., Mostow, J. (eds.) ITS 2010, Part I.
LNCS, vol. 6094, pp. 327–337. Springer, Heidelberg (2010)
24. Zhang, L., Burleson, W., Chavez-Echeagaray, M.E., Girard, S., Gonzalez-Sanchez, J., Hidalgo-Pontet, Y., VanLehn, K.: Evaluation of a meta-tutor for constructing models of dynamic systems. In: Chad Lane, H., Yacef, K., Mostow, J., Pavlik, P. (eds.) AIED 2013.
LNCS (LNAI), vol. 7926, pp. 666–669. Springer, Heidelberg (2013)

Muldner, K., Burleson, W., & VanLehn, K. (2010). "Yes!": Using tutor and sensor data to
predict moments of delight during instructional activities. In P. de Bra, A. Kobsa & D. Chin
(Eds.), User Modeling, Adaptation and Personalization: 18th International conference, UMAP
2010 (pp. 159-170). Heidelberg, Germany: Springer.

“Yes!”: Using Tutor and Sensor Data to Predict Moments
of Delight during Instructional Activities
Kasia Muldner, Winslow Burleson, and Kurt VanLehn
Arizona State University
{Katarzyna.Muldner,Winslow.Burleson,Kurt.VanLehn}@asu.edu

Abstract. A long standing challenge for intelligent tutoring system (ITS)
designers and educators alike is how to encourage students to take pleasure and
interest in learning activities. In this paper, we present findings from a user
study involving students interacting with an ITS, focusing on when students
express excitement, what we dub “yes!” moments. These findings include an
empirically-based user model that relies on both interaction and physiological
sensor features to predict “yes!” events; here we describe this model, its
validation, and initial indicators of its importance for understanding and
fostering student interest.
Keywords: interest, motivation, empirically-based model, sensing devices.

1 Introduction
In some cultures, the classic “yes!” gesture is to clench the fist of one’s dominant arm,
jerk the arm downward and exclaim “yes!” - everyone understands this as an
expression of triumphal victory. When we noticed this behavior among students using
our physics tutoring system, we began to wonder about it. For instance, what causes a
“yes!” during tutoring? Is the “yes!” behavior a desirable outcome in itself or is it also
associated with other desirable outcomes?
Because we are interested in building affective learning companions, we are also
interested in how a companion could use students’ “yes!” behavior for its own ends,
such as increased bonding with the student. This requires, however, that the
companion can detect “yes!” behaviors in real time. This paper reports our progress
on addressing these issues and questions, including:
1. Is the “yes!” behavior a desirable outcome for a tutoring system or associated
with one? We argue from the literature that it is both.
2. What causes “yes!” events and how can we increase their frequency? We
compare “yes!” episodes with ones where a “yes!” could have occurred but did
not. This descriptive analysis sets the stage for future work on what could cause
an increase in “yes!” events.
3. How can “yes!” events by used by tutors, learning companions or other agents?
We present a review of the literature that suggests some possibilities.
P. De Bra, A. Kobsa, and D. Chin (Eds.): UMAP 2010, LNCS 6075, pp. 159–170, 2010.
© Springer-Verlag Berlin Heidelberg 2010

160

K. Muldner, W. Burleson, and K. VanLehn

4. Can a “yes!” event be detected more accurately than a baseline approach? We
developed a regression model based on sensor and tutor log data analysis that
has high accuracy.
The rest of this introduction contains literature reviews that address points 1 and 3,
and a review of related work on affect detection (point 4).
1.1 The Likely Role of “yes!” in Learning and Interest
As we describe in Sect. 3, we view “yes!” as a class of brief expressions of (possibly
highly exuberant) positive affect. Positive affect has been linked to increased personal
interest [1, 2], which is in turn associated with a facilitative effect on cognitive
functioning [3], and improved performance on creative problem solving and other
tasks [4], persevering in the face of failure, investing time when it is needed and
engaging in mindful and creative processing (for a review see [5]). Although there is
work in the psychology community on how interest develops and is maintained (e.g.,
[6, 7]), to date there does not yet exist sufficient work on these topics to understand
the role of positive affect in general and of “yes!” events in particular, so calls for
additional research are common (e.g., [8]).
We should point out, however, that while positive affect could itself be considered
a desirable property during tutoring, it has not always shown strong correlations with
learning [9]. For instance, doing unchallenging problems may make students happy
but may not cause learning. However, the “yes!” expression of positive affect may
well be correlated with learning, because as we show later, “yes!” occurs only after
the student has been challenged, and challenge fosters learning [10].
1.2 How Can “yes!” Events Be Used during Tutoring and Learning?
In general, about 50% of human tutor interventions relate to student affect [11],
highlighting the importance of addressing affect in pedagogical interactions. As far as
addressing “yes!” events, work on the impact of tutorial feedback provides some
direction regarding how “yes!” detection can be valuable to a tutoring system for
generating subsequent responses. For instance, praise needs to be delivered at the
right moment, e.g., be perceived as representative of effort and sincere, to be effective
[12], and so a “yes!” event may be exactly the right time for an agent to give praise.
If “yes!” events do predict increased learning, interest and motivation, then they
can be used as proximal rewards for reinforcement learning of agent policies. For
instance, Min Chi et al. [13] found that a tutorial agent’s policies could be learned
given a distal reward, namely, a students’ learning gains at the end of six hours of
tutoring. It seems likely that even better policies could be learned if the rewards
occurred more frequently. That is, if a “yes!” event occurs, then perhaps the most
recent dialogue moves by the agent could be credited and reinforced.
1.3 Related Work on Detecting Brief Affective States
Affect recognition has been steadily gaining prominence in the user modeling
community, motivated by the key role of affect in various interactions. Like us,

“Yes!”: Using Tutor and Sensor Data to Predict Moments of Delight

161

some researchers have proposed models for identifying a single emotion. For
instance, Kappor et al. [14] rely on a sensor framework , incorporating a mouse,
posture chair, video camera and skin conductance bracelet, to recognize
frustration. McQuiggan and Lester [15] describe a data-driven architecture called
CARE for learning models of empathy from human social interactions. In contrast
to Kappor’s and our work, CARE only uses situational data as predictors for
empathy assessment. Others have focused on identifying a set of emotions. Cooper
et al.’s [16] four linear regression models each predict an emotion (frustration,
interest, excitement, confusion). Like our work, these models are built from a
combination of tutor and sensor log data, although only we explore the utility of
eye tracking and student reasoning data. D’Mello et al. [17] use dialog and posture
features to identify four affective states (boredom, flow, confusion, and
frustration). In Conati’s model, [18] a set of six emotions are assessed (joy/regret,
admiration/reproach, pride/shame) from tutor log data, subsequently refined to
include one sensor modality, namely an EEG [19].
While there is some work on modeling users with eye tracker information, most of
it has focused on how attention shifts predict focus (e.g., [20]), or how pupillary
response predicts cognitive load [21]. This latter work is inspired by findings in
psychology showing that pupillary response is increased by cognitive load [22];
likewise, affect also increases pupillary size [23]. However, results from experiments
less tightly controlled than traditional psychology ones have been mixed, with many
failing to find the anticipated link between pupillary response and state of interest
(e.g., [24]). In the past we investigated how only pupillary response distinguishes
different types of affect [25], and did not propose a model based on our results. In
contrast, here we present a model that relies on a broad range of features across both
interaction and sensor data to predict “yes!” moments. In doing so, we provide insight
into the utility of pupillary information for predicting “yes!” events.
In short, although others have investigated predicting positive affective states,
including joy [26], engagement [17] and excitement [16], our work distinguishes
itself in several ways. First, we identify a novel set of features unique to “yes!”,
including time on task, degree of reasoning and pupillary response. A more
important difference relates to our methodology. A fundamental challenge in
inferring affect from data is finding the appropriate gold standard against which to
compare a model’s predictions. A common approach is to elicit affect information by
explicitly querying users [16, 26]. This approach has the potential to be disruptive,
thus resulting in inaccurate affect labels; it can also miss salient moments of interest
(i.e., when affect is actually occurring). Another common approach relies on using
human coders to identify affect in users [17], a technique that also suffers from
limitations since human coder performance can be variable [17]. In contrast, we rely
on talk-aloud for obtaining affective labels. Doing so has the potential to avoid the
above pitfalls, because it is a form of naturally occurring data that has been shown to
not interfere with the task at hand [27]. Talk-aloud is also used in [28], although
there, only conversational cues are considered as affect predictors, while we use an
array of tutor and sensor features.

162

K. Muldner, W. Burleson, and K. VanLehn

2 Obtaining Data on “yes!” Moments
We obtained data on “yes!” moments from a previous user study we conducted
[25], which involved students interacting with an intelligent tutoring system (ITS)
for introductory Newtonian physics. This ITS, referred to as the Example Analogy
(EA)-Coach [29], provides support to students during problem solving in the
presence of worked-out examples. To solve problems with the EA-Coach, students
use the problem window (Fig. 1, left) to draw free body diagrams and type
equations; students are free to enter steps in any order and/or skip steps. For each
solution entry, the EA-Coach responds with immediate feedback for correctness,
realized by coloring entries red or green, indicating correct vs. incorrect entries.
Instead of providing hints, for instance on instructional material, the EA-Coach
makes examples available to students (accessed with the “GetExample” button);
these are displayed in the example window (Fig. 1, right). The system relies on a
decision-theoretic approach to tailor the choice of example to a student’s needs by
considering problem/example similarity, a student’s knowledge and reasoning
capabilities (see [29] for details).
The study involved 15 participants, all Arizona State University students, who
either were taking or had taken an introductory-level physics course. Each
participant solved two physics problems with the EA-Coach of the type shown in
Fig. 1; each problem solution involved about 15 steps (for further study details, see
[25]). We used a variety of data collection techniques. First, the EA-Coach logged
all interface actions. Second, we used talk aloud protocol [27]: we asked students
to verbalize their thoughts; all sessions were taped and subsequently transcribed.
Third, a sensor logger captured students’ physiological responses from four
sensing devices (see Fig. 2): (1) a posture chair pad measured position shifts (the
pad included three pressure points on the chair seat and three on the back); (2) a
skin-conductance (SC) bracelet captured skin conductance; (3) a pressure mouse
measured the pressure exerted on the mouse (via six “pressure points”); (4) an eye
tracker captured pupillary responses (the tracker was an integrated model that
appeared as a regular computer screen).

Fig. 1. EA-Coach problem and example windows

“Yes!”: Using Tutor and Sensor Data to Predict Moments of Delight

163

Fig. 2. Affective sensors (from left to right): posture chair, skin conductance (SC) bracelet,
pressure mouse, Tobii eye tracker

3 Data Pre-processing
As our “gold standard” for “yes!” moments during the study, we relied on the verbal
protocol data. Since a “yes moment” corresponds to excitement and/or positive affect,
the transcripts were coded by the first author to identify such instances. As a starting
point, we used data from an earlier affect coding [25], reanalyzing the codes to
identify “yes!”. We identified 68 “yes!” moments; all but one were directly associated
with subjects generating a correct solution step and were expressed directly after
doing so (recall that the EA-Coach provided immediate feedback for correctness so
students were aware of the status of their entries). The one “yes!” that was not
associated with a solution step occurred when a participant was reading the second
problem statement after having already successfully solved the first problem.
While some of the “yes!” events were expressed in a very effusive manner (“yes!
I’m smart!”, “oh yay!”), others were more subdued (“I got it right and that makes me
feel good”). In general, we found that when participants expressed a “yes!”, it varied
in terms of tone, expression, etc. Because we found it very difficult to disambiguate
between the various forms of positive affect related to a “yes!”, we decided to keep all
instances in the analysis without trying to further distinguish between them.
We also had data on how subjects were reasoning during the study, obtained from
an earlier coding [25] that included information on various types of reasoning, e.g.,
whether students were self-explaining by deriving physics principles, drawing
comparisons between the problem/example, and/or expressing some form of cognitive
processing (for examples, see [25]). For the purposes of this study, we collapsed the
various types of reasoning into a single “reasoning” code, because as a starting point
we were interested in how reasoning was related to “yes!” events.
Data Features. To analyze what events predict “yes!” moments, we identified a set of
features we believed could be relevant. Note that the list presented here is not meant
to be exhaustive, but rather to provide a starting point for understanding predictors of
excitement/positive affect in instructional situations. First, we identified interaction
data features we obtained from the EA-Coach logger corresponding to events in the
tutor’s interface, as follows:
- Time: The amount of time taken to generate a correct solution step (as described in
Sect. 4, we focus on correct solution entries);
- NumAttempts: The number of attempts required to generate a correct solution step;

164

K. Muldner, W. Burleson, and K. VanLehn

- NumReasoning: The number of “reasoning” utterances a student expressed in the
process of generating a solution step;
- Type of step: The type of solution step (e.g., a force, an axis, an equation).
Second, we identified sensor features that we obtained from the sensor logger:
- Pupillary response: The mean change in pupil dilation around a point of interest
(described in Sect. 4). For instance, if the point of interest is when a student
generates a solution step, then mean change = (mean pupil size over time span T
directly following the step) - (mean pupil size over time span T directly preceding
the step). We set the threshold T=2 seconds, since this comparable to that used in
other related work involving analysis of pupillary response (e.g., [30]).
- Skin Conductance (SC) response: The mean change in SC response around a point
of interest (calculated as for pupillary response). We set the threshold T=2 seconds,
based on the timeframe containing a SC response [14].
- Mouse response: The mean change in mouse pressure before and after an event of
interest, using the method in [16] (where the mean pressure was obtained by
summing over the pressure points, dividing by a constant and finding the mean).
We set the threshold T=10 seconds, because this sensor does not measure
instantaneous responses (like SC and pupillary response) but rather longer scale
transitions in behavior.
- Chair: The number of “sitForward” events, when a student leaned forward prior to
generating a solution step, calculated by obtaining the pressure on the seat back via
the formula in [16]. Here, we used a threshold T=10 seconds, as for the mouse.

4 Results
In order to understand predictors of “yes!” in instructional activities, we compared
“yes!” moments to other instances when students obtained a correct solution step but
did not generate a “yes!”. Since the “yes!” moments directly followed the generation
of a correct solution step, we felt this would be the most appropriate comparison; this
gave us 67 “yes!” instances1 and 218 other events. As a final pre-processing step, for
each logged correct step we extracted the above-described features, merging across
the different log files (transcript, EA-Coach, sensor) to produce a single file.
Our hypotheses were that students would only express a “yes!” if they invested
some effort into generating the solution step, and that there would be physiological
manifestations of “yes!” that differed from other correct entries. To analyze whether
these hypotheses were correct we carried out several types of analysis.
4.1 The Unique Nature of “yes!”
As a starting point, we wanted to determine if “yes!” moments differed from other
correct entries (referred to as other below) in terms of the features listed above. Thus,
we compared data on these two types of entries for our set of features through
1

There was one exception where a student expressed “yes!” when reading an example; given
our scheme, we did not consider this one data point in our analysis.

“Yes!”: Using Tutor and Sensor Data to Predict Moments of Delight

165

univariate ANOVA. As far as the interaction features are concerned, we found that
students took significantly longer to generate a correct solution step corresponding to
a “yes!” than other correct entries (on average, 206 sec. vs. 54 sec.; F(1,297) = 77.27;
p < 0.001). Students also generated significantly more attempts for “yes!” entries, as
compared to other correct entries (on average, 5.1 vs. 1.7; F(1,297) = 40.47, p <
0.001), and expressed significantly more reasoning episodes for “yes!” (on average,
1.34 vs. 0.58 F(1,283) = 11.614, p = 0.001). Our data was too sparse to analyze
whether type of step had an effect.
As far as the sensor features are concerned, students had a significantly larger
pupillary response for a correct solution step associated with a “yes!”, as compared to
other correct entries (on average, .043mm vs. -.037mm; F(1,271)=8.422, p=0.004).
Skin conductance response had a marginal effect on “yes!” as compared to other
entries (.000388µS vs. -.0000422µS, F(1,291)=3.257, p=0.07), suggesting a higher
level of arousal for “yes!”. Likewise, students had significantly fewer sitForward
events before a “yes!”, as compared to other entries (6.4 vs. 10.8; F(1,296)=4.63,
p=0.032). One possibility for why this was the case is that students were more
focused for “yes” entries and so were fidgeting less. We did not find “yes!” to have a
significant effect on mouse response.
4.2 An Empirically-Based Model for Predicting “yes!”
The above analysis showed that “yes!” moments are uniquely distinguishable. To
develop a user model, however, we need to understand how the various features
predict “yes!” events. Thus, we conducted regression analysis. Because we have a
nominal dependent variable (“yes!” vs. other), we used a logistic regression. A key
consideration behind our choice of modeling technique was our data set size: while
acceptable for modeling with logistic regression, where the rule of thumb is at least 20
data points per independent variable, it was not large enough for some other machine
learning techniques, e.g., support vector machine. Of the applicable techniques,
regression was chosen based on prior research showing its suitability for classifying
affect ([16, 31]); [31] found that regression yielded the highest affect classification
accuracy over other machine learning methods.
We begin by presenting the baseline model, one that always predicts the most
likely event (here, lack of a “yes!”). Given the base rates of the two decision options
and no other information, the best strategy is to predict that each step is not a “yes!”.
This model achieves 76% accuracy (# of correctly classified events / total # of
events), but obviously completely misses our goal of predicting when “yes!” occurs
(i.e., never predicts “yes!”, and so has a true positive value of 0%, see Table 1, top).
Using the Step method, we then added our features to the logistic regression
model2. The resulting model containing time, numReasoning, pupil response, SC
response and chair was significantly better from the baseline model (p<0.001, see
Table 1, top). Below, we will analyze the contribution of some of our features to the
model’s accuracy, but first we examine the full model accuracy.
2

Because increasing the number of predictors decreases experimental power, we omitted
numTries from this analysis, as it was redundant due to its high correlation with time; we also
omitted mouse response since it did not significantly distinguish “yes!” from other entries.

166

K. Muldner, W. Burleson, and K. VanLehn

Table 1. Logistic regression “yes!” models (TP=Sensitivity, TN=Specificity; Acc= TP + TN / N)
Overall Logistic Regression Equation
-2.206
-2.206+time*.008+ numReasoning*.309 +
pupilResponse*1.68 +SC*126.4+chair*-.019
Time*+numReasoning* -2.437 + time*.01 + numReasoning*.345
Time*+pupilResponse* -2.157+time*.009+pupilResponse*2.082
Time*+SC
-2.308 + time*0.01+ SC*128.97
Time*+Chair
-2.084 +time*0.01 + chair*-.017
Baseline model
Full model **

TP
0
60.3

TN
100
87.2

Acc.
76
81.4

55.2
54.2
57.6
56.7

89.2
85.0
89.9
88.3

81.6
78.4
82.6
81.2

** Significantly better than baseline model, p<0.05
* Each feature significantly improves model fit over previous model (i.e., model 1=baseline,
model 2=time, model 3= time+2nd feature), p<0.05

The output of a logistic regression equation is a probability that a given event
belongs to a particular class. In order to use the model for prediction, it is therefore
necessary to have a decision rule: if the probability of an event is greater or equal to
some threshold then we will predict that event will take place (and not take place
otherwise). To choose the optimal threshold, we built a Receiver Operating
Characteristic (ROC) curve (Fig. 3). The ROC curve is a standard technique used in
machine learning to evaluate the extent to which a classifier can successfully
distinguish between data points (episodes correctly classified as positive, or true
positives) and noise (episodes incorrectly classified as positive, or false positives),
given a choice of different thresholds. Figure 3 shows the ROC curve we obtained for
our “yes!” models, where each point on the curve represents a model with a different
threshold value. As is standard practice, we chose as our final threshold the point on
the curve that corresponds to a reasonable tradeoff between too many false positives
vs. too few true positives (P=0.26, labeled by a cross on the curve in Fig. 3).
When reporting classifier accuracy, it is standard to provide sensitivity (true
positives) and specificity (true negatives), since these are more informative then
overall accuracy (true positives + true negatives / total number of instances). Our
classifier is significantly better than the baseline model (p < 0.05) and obtained a
sensitivity of 60.3%, a specificity of 87.2% (and overall accuracy of 81.4% - see
Table 1, top). Thus, this classifier correctly identifies 60% of “yes” moments, without
incorrectly classifying other entries as “yes!” for 87% of the time.
Model Validation. To validate the above model, we conducted a leave-one-out cross
validation. Specifically, we trained the classifier using N-1 data points and tested on
the remaining data point, repeating this process N times (where N is equal to the
number of samples, 269 full samples, i.e., without any missing data points that were
the result of, for instance, the eye tracker failing to find a valid pupil reading). The
validation showed that our model accuracy does not degrade substantially (i.e.,
sensitivity=55.2%, specificity = 87.1%, accuracy = 79.5%).
Parsimonious Models. We wanted to explore what kind of model fit we could obtain
with a subset of our features, which helps to make an informed decision as to which
sensors to use if not all are available. Thus, we ran a series of regressions using time
as the tutor variable (as this variable was highly significant in our regression model)

“Yes!”: Using Tutor and Sensor Data to Predict Moments of Delight

167

Fig. 3. ROC curve for various decision rule thresholds

and one of the other features. As Table 1 illustrates, we obtained reasonable results in
terms of sensitivity and specificity with these reduced models, although only
reasoning and pupil response resulted in significantly better models over a model that
only included time (SC response and chair both improved the model fit, but this did
not reach significance, i.e., p=.151 for the SC response and p=.153 for the chair).

5 Discussion and Future Work
In this paper, we reported on our analysis of moments of excitement and positive
affect during instructional activities, which we refer to as “yes!” events. We found
that “yes!” always followed a correct solution step, but conversely, a correct step was
not always followed by a “yes!”. In particular, students were significantly more likely
to express a “yes!” after investing more time, generating more attempts, and
expressing more reasoning episodes, as compared to correct entries for which
corresponding enthusiasm was not expressed. Note that in addition to verbal
expression of “yes!”, another indication of arousal related to these events was
provided by the pupil dilation and skin conductance data. These findings imply that
students experience excitement and/or positive affect in tutoring situations when they
have invested effort into the process and that effort pays off (i.e., correct solution is
obtained). It is possible, however, that students express “yes!” not because they
invested thoughtful, deliberate processing but because they guessed and/or arrived at
the solution by luck. Our analysis does provide some indication that this is not the
case, as students engaged in significantly more reasoning (captured by the
“reasoning” code that included self-explanation, a form of deep processing) prior to
“yes!”. This does not guarantee every student behavior related a “yes!” is an instances
of “deep” reasoning – in the future, we plan to delve deeper into this issue of mindful
processing and “yes!”.
To the best our knowledge, ours is the first work to propose a model for affect
recognition incorporating pupillary response data. Although in contrast to the other
low-cost sensors we used, eye tracking technology is more expensive, it is becoming
more and more accessible, and so investigating its utility for user modeling is
important. In a prior study [25], we also found a significant difference in pupil size
between affective responses, but there are four key differences between that study and
the present. First, in [25] we analyzed how pupillary response differs between positive
and negative affect, without developing a model based on this data. Second, here we

168

K. Muldner, W. Burleson, and K. VanLehn

focus on “yes!” while in [25], we focused on differences between four affective states.
Third, in [25] we normalized the pupil data using Z scores – while this approach is
sometimes used (e.g., [19]) and increases experimental power, it requires subtracting
the overall signal mean from each data point. Since this mean can only be obtained
after a user finishes interacting with a system, the findings are difficult to apply for
real-time user models. In contrast, here we use the raw signal values, making our
findings more applicable to real-time modeling. Fourth, our feature set includes an
array of sensors and tutor features, while in [25], we analyzed only pupillary data.
Overall, the tutor and sensor features resulted in a model that predicted “yes!” with
60% sensitivity and 87% specificity, a significant improvement over the baseline
model. We also analyzed how using subsets of features impacts model fit: although
the model incorporating the full set of features allowed the best trade-off between
sensitivity and specificity, using a subset of features also resulted in models with
reasonable fit. For instance, a model that includes only information on time and
reasoning performs quite well – this may be useful if a system already has the tools to
capture reasoning style (e.g., as in [32]) but sensors are not available. As far as the
sensor features are concerned, when we explored parsimonious models, each sensor
improved model fit over the time-only model. However, this improvement was only
reliable, as reported by the p value, for the pupillary response feature. Compared to
the pupil-based model, the models incorporating the other sensors resulted in higher
specificity and/or specificity. These results, however, have to be interpreted with
caution, since they approached but did not reach significance. This may be due to our
modest sample size, and so more data is needed to confirm these sensors’ utility.
While there is room for improvement, our model is a first step in providing
information on “yes!” moments, which in turn can be used for tailoring pedagogical
scaffolding to foster interest. For this purpose, it is key that the classifier not
misclassify too many other entries as “yes!” (i.e., has high specificity), while still
identifying some “yes!” moments, as is the case for our classifier. Given our limited
sample size and particular instructional context, however, more work is needed to
validate and generalize our findings.
Returning to our original four questions, we summarize the progress made so far
and directions for future work.
1. Are “yes!” events desirable outcomes or associated with desirable outcomes?
We argue that it is both. We now know that “yes!” occurs after students appear to
have overcome a challenge related to generating a solution step, as indicated by
time spent and number of tries produced. Since challenge fosters interest, this
suggests that “yes!” events may be suitable as a predictor of increased learning,
interest and motivation, something we plan to explore in future studies.
2. What causes “yes!” events and how can we increase their frequency? We now
know that “yes!” events occur after a challenge is overcome with an example as
the only the aid from the tutor. This is consistent with Lepper’s advice of keeping
the student optimally challenged [10].
3. How can “yes!” events be useful to tutors, learning companions and other
agents? We offer some suggestions based on theory, but this remains to be
empirically explored.
4. Can “yes!” events be detected more accurately than a baseline approach? Yes!

“Yes!”: Using Tutor and Sensor Data to Predict Moments of Delight

169

Acknowledgements. The authors thank the anonymous reviewers for their helpful
suggestions and David Cooper, who contributed the sensor logging software. This
research was funded the National Science Foundation, including the following grants:
(1) IIS/HCC Affective Learning Companions: Modeling and supporting emotion
during learning (#0705883); (2) Deeper Modeling via Affective Meta-tutoring (DRL0910221) and (3) Pittsburgh Science of Learning Center (SBE-0836012).

References
1. Reeve, J.: The Interest-Enjoyment Distinction in Interest Motivation. Motivation and
Emotion 13, 83–103 (1989)
2. Isen, A., Reeve, J.: The Influence of Positive Affect on Intrinsic and Extrinsic Motivation:
Facilitating Enjoyment of Play, Responsible Work Behavior, and Self-Control. Motivation
and Emotion 29(4), 297–325 (2005)
3. Hidi, S.: Interest and its Contribution as a Mental Resource for Learning. Rev. of Ed.
Research 60(4), 549–571 (1990)
4. Isen, A., Daubman, K., Nowicki, G.: Positive Affect Facilitates Creative Problem Solving.
J. of Personality and Social Psychology 52, 1122–1131 (1987)
5. Lepper, M.: Motivational Considerations in the Study of Instruction. Cognition and
Instruction 5(4), 289–309 (1988)
6. Hidi, S., Renninger, A.: The Four-Phase Model of Interest Development. Educational
Psychologist 41(2), 559–575 (2006)
7. Deci, E., Koestner, R., Ryan, R.: Extrinsic Rewards and Intrinsic Motivation in Education:
Reconsidered Again. Rev. of Ed. Research 71, 1–27 (2001)
8. Baker, R., D’Mello, S., Rodrigo, M., Graesser, A.: Better to Be Frustrated Than Bored:
The Incidence, Persistence, and Impact of Learners’ Cognitive-Affective States During
Interactions with Three Different Computer-Based Learning Environments. Int. J. of
Human-Computer Studies (in press)
9. Boyer, K., Phillips, R., Wallis, M., Vouk, M., Lester, J.: Balancing Cognitive and
Motivational Scaffolding in Tutorial Dialogue. In: Woolf, B.P., Aïmeur, E., Nkambou, R.,
Lajoie, S. (eds.) ITS 2008. LNCS, vol. 5091, pp. 239–249. Springer, Heidelberg (2008)
10. Lepper, M., Malone, T.: Intrinsic Motivation and Instructional Effectiveness in ComputerBased Education. In: Snow, R., Farr, M. (eds.) Aptitude, Learning and Instruction, vol. 3,
pp. 255–296. Erlbaum, Hillsdale (1987)
11. Lepper, M., Woolverton, M., Mumme, D., Gurtner, J.: Motivational Techniques of Expert
Human Tutors: Lessons for the Design of Computer-Based Tutors. In: Lajoie, S., Derry, S.
(eds.) Computers as Cognitive Tools, pp. 75–105. Lawrence Erlbaum Associates,
Hillisdale (1993)
12. Henderlong, J., Lepper, M.: The Effects of Praise on Children’s Intrinsic Motivation: A
Synthesis and Review. Psychological Bulletin 128(5), 774–795 (2002)
13. Chi, M., VanLehn, K., Litman, D., Jordan, P.: Inducing Effective Pedagogical Strategies
Using Learning Context Features. In: Proc. of the 18th Int. Conference on User Modeling,
Adaptation and Personalization (in press)
14. Kapoor, A., Burleson, W., Picard, R.: Automatic Prediction of Frustration. Int. J. of
Human-Computer Studies 65(8), 724–736 (2007)
15. McQuiggan, S., Lester, J.: Diagnosing Self-Efficacy in Intelligent Tutoring Systems: An
Empirical Study. In: Ikeda, M., Ashley, K.D., Chan, T.-W. (eds.) ITS 2006. LNCS,
vol. 4053, pp. 565–574. Springer, Heidelberg (2006)

170

K. Muldner, W. Burleson, and K. VanLehn

16. Cooper, D., Arroyo, I., Woolf, B., Muldner, K., Burleson, W., Christopherson, R.: Sensors
Model Student Self Concept in the Classroom. In: Houben, G.-J., McCalla, G., Pianesi, F.,
Zancanaro, M. (eds.) UMAP 2009. LNCS, vol. 5535, pp. 30–41. Springer, Heidelberg (2009)
17. D’Mello, S., Graesser, A.: Mind and Body: Dialogue and Posture for Affect Detection in
Learning Environments. In: Luckin, R., Koedinger, K., Greer, J. (eds.) Proc. of the 13th
Int. Conference on Artificial Intelligence in Education, pp. 161–168. IOS Press,
Amsterdam (2007)
18. Conati, C., Zhou, X.: Modeling Students’ Emotions from Cognitive Appraisal in
Educational Games. In: Cerri, S.A., Gouardéres, G., Paraguaçu, F. (eds.) ITS 2002. LNCS,
vol. 2363, pp. 944–954. Springer, Heidelberg (2002)
19. Conati, C., Maclaren, H.: Modeling User Affect from Causes and Effects. In: Houben, G.-J.,
McCalla, G., Pianesi, F., Zancanaro, M. (eds.) UMAP 2009. LNCS, vol. 5535, pp. 4–15.
Springer, Heidelberg (2009)
20. Conati, C., Merten, C.: Eye-Tracking for User Modeling in Exploratory Learning
Environments: An Empirical Evaluation. Know. Based Systems 20(6), 557–574 (2007)
21. Iqbal, S., Zheng, X., Bailey, B.: Task-Evoked Pupillary Response to Mental Workload in
Human-Computer Interaction. In: Dykstra, E., Tscheligi, M. (eds.) Proc. of the ACM
Conference on Human Factors in Computing Systems, pp. 1477–1480. ACM, NY (2004)
22. Marshall, S.: Identifying Cognitive State from Eye Metrics. Aviation, Space and
Environmental Medicine 78, 165–175 (2007)
23. Vo, M., Jacobs, A., Kuchinke, L., Hofmann, M., Conrad, M., Schacht, A., Hutzler, F.: The
Coupling of Emotion and Cognition in the Eye: Introducing the Pupil Old/New Effect.
Psychophysiology 45(1), 130–140 (2008)
24. Schultheis, H., Jameson, A.: Assessing Cognitive Load in Adaptive Hypermedia Systems:
Physiological and Behavioral Methods. In: De Bra, P.M.E., Nejdl, W. (eds.) AH 2004.
LNCS, vol. 3137, pp. 225–234. Springer, Heidelberg (2004)
25. Muldner, K., Christopherson, R., Atkinson, R., Burleson, W.: Investigating the Utility of
Eye-Tracking Information on Affect and Reasoning for User Modeling. In: Houben, G.-J.,
McCalla, G., Pianesi, F., Zancanaro, M. (eds.) UMAP 2009. LNCS, vol. 5535, pp. 138–149.
Springer, Heidelberg (2009)
26. Conati, C., Maclaren, H.: Empirically Building and Evaluating a Probabilistic Model of
User Affect. User Modeling and User-Adapted Interaction (in press)
27. Ericsson, K., Simon, H.: Verbal Reports as Data. Psych. Rev. 87(3), 215–250 (1980)
28. D’Mello, S., Craig, S., Sullins, J., Graesser, A.: Predicting Affective States Expressed
through an Emote-Aloud Procedure from Autotutor’s Mixed-Initiative Dialogue. Int. J. of
Artificial Intelligence in Education 16(1), 3–28 (2006)
29. Muldner, K., Conati, C.: Evaluating a Decision-Theoretic Approach to Tailored Example
Selection. In: Veloso, M. (ed.) Proc. of 20th Int. Joint Conference on Artificial
Intelligence, pp. 483–489. AAAI Press, Menlo Park (2007)
30. Van Gerven, P., Paas, F., Van Merrienboer, J., Schmidt, H.: Memory Load and the
Cognitive Pupillary Response in Aging. Psychophysiology 41(2), 167–174 (2001)
31. D’Mello, S., Picard, R., Graesser, A.: Towards an Affect Sensitive Auto Tutor. IEEE
Intelligent Systems 22(4), 53–61 (2007)
32. Conati, C., VanLehn, K.: Toward Computer-Based Support of Meta-Cognitive Skills: A
Computational Framework to Coach Self-Explanation. Int. J. of Artificial Intelligence in
Education 11, 389–415 (2000)

When is Tutorial Dialogue More Effective Than
Step-based Tutoring?
Min Chi1 , Pamela Jordan2 , and Kurt VanLehn3
1

2

3

Computer Science Department, North Carolina State University,Raleigh NC
USA,mchi@ncsu.edu,
Learning Research and Development Center, University of Pittsburgh, Pittsburgh,
PA USA pjordan@pitt.edu
School of Computing, Informatics and Decision Science Engineering, Arizona State
University, AZ USA, Kurt.Vanlehn@asu.edu

Abstract. It is often assumed that one-on-one dialogue with a tutor,
which involves micro-steps, is more effective than conventional step-based
tutoring. Although earlier research often has not supported this hypothesis, it may be because tutors often are not good at making micro-step
decisions. In this paper, we compare a micro-step based NL-tutoring
system that employs induced pedagogical policies, Cordillera, to a wellevaluated step-based ITS, Andes. Our overall conclusion is that the pairing of effective policies with a micro-step based system does significantly
outperform a step-based system; however, there is no significant difference in the absence of effective policies. Moreover, while micro-step tutoring is more time-consuming, the findings still hold for five out of six
learning performance measures when time on task is factored out.
Keywords: Natural Language tutoring systems, Step-based tutoring
systems, Reinforcement Learning, Pedagogical Strategy

1

Introduction

In domains like math and science solving a problem requires producing an argument, proof or derivation consisting of one or more inference steps, and each
step is the result of applying a domain principle, operator or rule. For instance,
solving an algebraic equation, 2x+5=21 can be done via two steps: 1) subtract
the same term 5 from both sides of the equation; and 2) divide both sides by 2.
A typical Intelligent Tutoring System (ITS) is step-based [13] while human
tutors or Natural Language (NL) tutoring systems, by contrast, often scaffold
students via a series of micro-steps leading to the full step. A step corresponds to
(writing) actions that are ordinarily done even without the tutor, whereas microsteps correspond roughly to dialogue acts in tutorial dialogues. For example, in
order to solve a physics problem, the student needs to apply several domain
principles, some of which may need to be applied multiple times. Each principle
application can be seen as a step in the ITS. In a physics tutor, for example,
applying the definition of Kinetic Energy (KE = 21 mv 2 ) to solve for the kinetic

1. T: So let’s start with determining the value of KE0 .
2. T: Which principle will help you calculate the rock’s kinetic energy at T0?
Please provide the name of the principle, not an equation. {ELICIT}
3. S: Definition of kinetic energy
4. T: Yes, I agree. Now I will write the equation for applying the definition of
kinetic energy to the rock at T0: KE0 = 1/2*m*v0ˆ2 {TELL}
Fig. 1. Illustrations of Two Micro-Steps

energy of a falling rock at T0 is a step; once a student enters a step, then the tutor
gives feedback and/or hints. Human tutors, by contrast, often scaffold students
via a series of micro-steps leading to the full step. In the step mentioned above,
for instance, a human tutor can take the following micro-level steps: selecting
the principle to apply; writing the corresponding equation; solving the equation;
and engaging in some qualitative discussion about the principle.
Fig. 1 illustrates two micro-steps and each numbered line represents a dialogue turn. The labels T and S designate tutor and student turns respectively.
In this example, the tutor and the student first select a principle (lines 2 & 3)
and then write the corresponding equation (line 4). Some of the tutor turns in
Fig. 1 are labeled {ELICIT} or {TELL}. This label designates a tutorial
decision step wherein the tutor has to make a tutorial decision whether to ask
the student for the requisite information or to tell it to the student. For example,
in line 2, the tutor chooses to elicit the answer by asking, “Which principle will
help you calculate the rock’s kinetic energy at T0? Please provide the name of
the principle, not an equation.” If the tutor elects to tell, however, then he or
she would state, “To calculate the rock’s kinetic energy at T0, let’s apply the
definition of Kinetic Energy.”
One common hypothesis as to the effectiveness of human one-on-one tutoring
comes from the detailed management of “micro-steps” in tutorial dialogue[6, 7]
and thus suggests that micro-step based tutors are more effective than step-based
tutors. In several tests of this hypothesis, however, neither human tutors nor NL
tutors designed to mimic human tutors, outperformed step-based tutors once
content was controlled to be the same across all conditions [5, 12]. All three types
of tutors were more effective than no instruction (e.g., students reading material
and/or solving problems without feedback or hints). One possible conclusion is
that tutoring is effective, but the micro-steps of human tutors and NL tutoring
systems provide no additional value beyond conventional step-based tutors[13].
Alternatively, we argue that the lack of difference between micro-step and
step-based tutors is because neither the human tutors nor the NL tutoring systems involved in those studies were good at making micro-step decisions and
several studies provide some support for this claim[3, 11, 2]. Previously, we investigated the impact of pedagogical policies on student learning by comparing
different versions of a micro-step based NL tutoring system called Cordillera [2].

We applied a general data-driven methodology, Reinforcement Learning (RL),
to induce pedagogical policies directly from student interactivity logs and found
that Cordillera with effective pedagogical policies, RL-induced Cordillera significantly out-performed other versions of Cordillera. However, it is still unclear
whether the former is significantly better than a step-based ITS.
In this paper, we directly compare RL-induced Cordillera with a well-evaluated
step-based conventional ITS, Andes [14]. Our main research question is: Can a
NL tutoring system with machine-learned pedagogical policies be more effective
than a step-based ITS? Overall, we find that RL-induced Cordillera significantly
outperforms Andes. In order to investigate whether this result is indeed caused
by effective RL-induced policies, we also compare Andes to two other versions
of Cordillera: Hybrid-RL and Random. In the following, we will briefly describe
the two types of tutoring systems and the pedagogical policies employed in them
and then describe our study and finally present our results.

2

Two Types of ITSs

The Micro-step based Cordillera: NL Tutorial Dialogue System
The Cordillera tutorial dialogue system tutors students in both quantitative and
qualitative physics in the work-energy domain and was implemented using the
TuTalk tutorial dialogue system toolkit toolkit [8]. TuTalk supports dialogues in
which a tutor tries to elicit the main line of reasoning from a student by a series
of coherent questions. This style of dialogue was inspired by CIRCSIM-Tutor’s
directed lines of reasoning [5]. The Cordillera style of dialogue is system-initiative
in that the system always chooses the topics discussed.

Fig. 2. An example of the Cordillera interface

Figure 2 illustrates a sample student dialogue with Cordillera. The upper
top right pane of the figure shows the problem that the student is attempting to
solve. The top left pane shows a portion of the dialogue history, and illustrates
a few questions and student responses, as well as a number of system informs;
the pending tutor question is shown in the input pane at the bottom followed by
the response the student is entering. Finally, the variables in the bottom right
pane and the equations (hidden) were entered either by the student using a form
interface (not shown) or provided by the tutor. When the tutor asks the student
to compute the value for a variable, the student must transform the equation
to a solvable form with the known values substituted and then the tutor will
do the final calculation. In order to avoid confounds due to imperfect NL understanding, a human wizard replaced the NL understanding module. During
tutoring, the wizard matched students’ answers to one of the available responses
but made no tutorial decisions.
The step-based Andes Tutoring System
Andes provides a multi-paned screen that consists of a problem-statement window, a variable window, an equation window, and a dialogue window. An example of the Andes interface, as the student would see it, is shown in Figure 3. On
Andes, students construct and manipulate a solution. The interaction is openended, event-driven and student-initiated. Students can enter an equation that
is the algebraic combination of several principle applications and Andes provides
immediate feedback on each entry. Andes can also algebraically manipulate equations to calculate the value for a variable. It considers an entry correct if it is
true, regardless of whether it is useful for solving the problem. When an entry is
incorrect, students can either fix it independently, or ask for what’s-wrong help.
When they do not know what to do next, they can ask for next-step help. Both
next-step and what’s-wrong helps are provided via a sequence of hints that gradually increase in specificity. The last hint in the sequence, called the bottom-out
hint, tells the student exactly what to do.
Andes provides conceptual and procedural help that is designed to encourage
students to think on their own. Students can always enter any correct step and
Andes does not attempt to determine their problem-solving plans. If necessary
for giving a hint, it asks students what principle they are working on. If students
indicate a principle that is part of a solution to the problem, Andes hints an
uncompleted step from the principle application. If no acceptable principle is
chosen, Andes picks an unapplied principle from the solution that they are most
likely to be working on.

3

Decision Policies within Cordillera and Andes

In many tutoring systems, the system’s behaviors can be viewed as a sequential
decision process wherein, at each discrete step, the system is responsible for selecting the next action to take. Pedagogical strategies are defined as policies to
decide the next system action when multiple are available. Each of these sys-

Fig. 3. An example of the Andes interface

tem decisions affects the user’s successive actions and performance. Its impact
on student learning cannot often be observed immediately and the effectiveness of one decision also depends on the effectiveness of subsequent decisions.
Ideally, an effective tutor should craft and adapt its decisions to users’ needs
[1, 10]. However, there is no existing well-established theory on how to make
these system decisions effectively. In this work, different versions of micro-step
based Cordillera employed different pedagogical policies. The step-based Andes
employs hand-coded rules.
Three versions of Cordillera - Random, Hybrid-RL, and RL-induced - were involved. The only difference among the three is the policy used. Random Cordillera
made tutorial decisions randomly. Hybrid-RL Cordillera used expert-guided datadriven induced rules. These rules were induced by using 18 features and a greedylike procedure to prune the features to meet efficiency and training constraints[4].
Both the initial features and pruning procedure were suggested by human experts
and the final induced policies were also checked and approved by human experts.
But no significant difference was found on overall learning performance between
the Hybrid-RL and random policies. For RL-induced Cordillera, the data-driven
approach was greatly improved. More specifically, the RL approach involved a
much larger feature set (50 features), and more advanced domain-general feature
selection approaches. Human experts were not involved in directing the policy
generation. As reported earlier[2], these RL-induced policies indeed helped students learn more and in a deeper way than either Hybrid-RL or random policies.
Andes, on the other hand, like most existing ITSs employs hand-coded pedagogical policies. For example, help in Andes is provided upon request because
it is assumed that students know when they need help and will only process
help when they desire it. A student deciding to request help can be seen as a
human-like decision policy for whether to skip or not skip content.

4

Methods

Participants: A total of 163 participants used either Andes or one of the three
versions of Cordillera: the Andes group comprised 33 students; the Random
Cordillera Group comprised 64 students so that we could collect enough data
for RL policy induction; the Hybrid-RL Cordillera Group comprised 37 students;
and the RL-induced Cordillera group comprised 29 students. All participants
were recruited in the same way but in different years.
Domain & Procedure: The training covered the first-year college physics
work-energy domain. All participants experienced identical procedures: 1) a
background survey; 2) read a textbook covering the target domain knowledge;
3) took a pretest; 4) solved the same seven training problems in the same order on either Andes or Cordillera; and 5) finally took a posttest. The pretest
and posttest were identical and contained 16 quantitative items and 16 qualitative items. Both quantitative and qualitative items include multiple choice and
open-ended problems.
Students’ learning outcomes were measured by using three types of scores:
quantitative, qualitative and overall. All tests were graded in a double-blind
manner by experienced graders. In a double-blind manner, neither the students
nor the graders know who belongs to which group. For comparison purpose all
test scores were normalized to fall in the range of [0,1].
Except for following the policies (Random, Hybrid-RL, or RL-induced), the
remaining components of Cordillera, including the interface, the training problems, and the tutorial scripts, were identical for all students. However, there are
some noticeable differences for the Andes training compared to Cordillera.
Differences in the Training: The Cordillera dialogues guided students through
the training problems by hinting at the next problem solving step to be completed, or telling them what it is. Hints took the form of short answer questions.
In addition to guiding the student through problem solving, Cordillera also attempted to help the student increase his/her conceptual understanding of the
domain by asking for justifications for the most important problem solving steps.
The decision for when to ask for a justification was determined by a set of pedagogical policies. For an example of a justification requested during problem
solving, see the current tutor turn in the bottom left input pane in Figure 2.
There was also a post-problem discussion for each problem which sought to
increase the student’s conceptual qualitative understanding.
We implemented the same seven training problems in Andes and because
Cordillera provided drawings and pre-defined some variables for each problem,
we set-up Andes to provide the same. We added a post-problem discussion to
Andes by collecting all the post-problem discussion for Cordillera into a static
text document so that the content coverage for post-problem discussion was
about the same. The post-problem discussion was delivered in a series of web
pages after the experimenter verified that the student had completed the Andes
problem.

Note that we did not attempt to provide identical content for the problem
solving help since it reflects two different tutoring systems, but what is available
is similar. For example, while the Cordillera system’s micro-steps will always
present the content illustrated in Fig. 1, Andes will show the following series of
hints for this same step after the student makes four consecutive help requests:
1) Why don’t you continue with the solution by working on the definition of
kinetic energy. 2) What is the kinetic energy of the rock at T0? 3) The kinetic
energy of an object is defined as one half its mass times its velocity squared.
That is, 0.5 ∗ m ∗ v 2 . 4) Write the equation KE0 = 0.5 ∗ m ∗ v02 . So for this
illustration asking for all hints on the Andes step is equivalent to a decision to
tell for all the related micro-steps in Cordillera.
While the problem solving help content is similar, there is also some conceptual qualitative discussion during Cordillera’s problem solving that Andes does
not offer. It is up to the student to consider the concepts involved on their own.
However, as has been pointed out, novice students have a tendency to simply
manipulate equations to isolate the unknown and seldom consider the conceptual
knowledge involved during problem solving [9].

5

Results

Overall Training Time
A one-way ANOVA showed significant differences among the four groups on
overall training time: F (3, 154) = 53.90, p < 0.001. The Andes group spent
significantly less time4 than the other three groups but there were no significant differences in time on task among the three Cordillera groups. The average
training time (in minutes) across the seven training problems, was M = 115.94,
SD = 42.03 for Andes, M = 280.38, SD = 66.88 for Random, M = 294.33,
SD = 87.51 for Hybrid-RL, and M = 259.99, SD = 59.22 for RL-induced.
Learning Performance
Although students were recruited during different time periods, they appear
balanced on incoming competence across the conditions. A one-way ANOVA
showed that there were no significant differences in pretest scores among the
four groups on either quantitative: F (3, 159) = 1.18, p = .32, or qualitative:
F (3, 159) = 0.06, p = .98 , or overall questions F (3, 159) = 0.46, p = .71.
A repeated measures analysis using test (pretest vs. posttest) as a factor and
test score as the dependent measure showed that there was a main effect for
test. All four groups of students scored significantly higher on the posttest than
the pretest, F (1, 32) = 19.87, p < 0.001 for Andes, F (1, 63) = 78.37, p < 0.001
for Random, F (1, 36) = 48.36, p < 0.001 for Hybrid-RL, and F (1, 28) = 238.58,
p < 0.001 for RL-induced.
The same results were found from pretest to posttest on both quantitative
and qualitative questions as well. More specifically, on quantitative questions,
4

Some reading times for the last problem were lost so we used the minimum average
reading time for all other easier problems.

Table 1. RL-induced Cordillera vs. Andes on Various Test Scores
Test Item Test
Set
Score
quant
Pre
Post
Adj Post
NLG
qual
Pre
Post
Adj Post
NLG
Overall Pre
Post
Adj Post
NLG

RL-induced
Cordillera
0.35 (0.25)
0.64 (0.22)
0.61 (.18)
0.49 (0.28)
0.46(0.12)
0.65 (0.14)
0.65 (.14)
0.36 (0.24)
0.42 (0.15)
0.65 (0.15)
0.64 (.11)
0.42 (0.19)

Andes
0.28 (0.26)
0.41 (0.30)
0.44 (.17)
0.16 (0.38)
0.45(0.14)
0.54 (0.18)
0.54 (.14)
0.14 (0.34)
0.39 (0.16)
0.50 (0.21)
0.51 (.12)
0.17 (0.28)

Stat

cohen
d
t(60) = 1.01, p = .28
0.27
t(60) = 3.29, p = 0.002
0.87 ∗ ∗
F (1, 59) = 13.793, p < .0001 0.97 ∗ ∗
F (1, 59) = 14.442, p < 0.0001 0.99 ∗ ∗
t(60) = 0.40, p = .688
0.08
t(60) = 2.68, p = 0.010
0.68 ∗ ∗
F (1, 59) = 7.74, p = .007
0.79 ∗ ∗
F (1, 59) = 8.86, p = 0.004
0.75 ∗ ∗
t(60) = 0.87, p = .39
0.19
t(60) = 3.35, p = 0.001
0.82 ∗ ∗
F (1, 59) = 16.50, p < .0001 1.13 ∗ ∗
F (1, 59) = 15.97, p < 0.0001 1.04 ∗ ∗

F (1, 32) = 15.83, p < 0.001 for Andes, F (1, 63) = 33.55, p < 0.001 for Random,
F (1, 36) = 58.01, p < 0.001 for Hybrid-RL, and F (1, 28) = 95.79, p < 0.001
for RL-induced. On qualitative questions, F (1, 32) = 7.68, p = 0.009 for Andes, F (1, 63) = 40.62, p < 0.001 for Random, F (1, 36) = 17.20, p < 0.001 for
Hybrid-RL, and F (1, 28) = 89.56, p < 0.001 for RL-induced. Therefore all four
conditions made significant gains from pre-test to post-test across all three sets of
questions: quantitative, qualitative and overall questions. In order to investigate
whether micro-step based tutors can be more effective than step-based tutors,
we first investigated whether the most effective version of Cordillera would outperform Andes.
RL-induced Cordillera vs. Andes
Table 1 compares the pre-test, post-test, adjusted post-test, and NLG scores
between the RL-induced Cordillera and Andes conditions by question type. The
adjusted Post-test scores were compared between the two conditions via an ANCOVA with the corresponding pre-test score as a covariate. NLG measures students’ gain irrespective of their incoming competence: N LG = posttest−pretest
.
1−pretest
Here 1 is the maximum score. The third and fourth columns in Table 1 list the
means and SDs of the two groups’ corresponding scores. The fifth column lists
the statistical comparison and the last column lists the effect size of the comparison using Cohen’s d5 . Table 1 shows that there was no significant difference
between the two conditions on pre-test scores. However, there were significant
differences between them on the post-test, adjusted post-test, and NLG scores
for all three question types.
We then compared the two groups’ performance on six types of learning
measures: {Quantitative, Qualitative, Overall} × {Posttest, NLG} using both
5

Which is defined as the mean learning gain of the experimental group minus the
mean of the control group, divided by the groups’ pooled standard deviation.

pre-test and total training time as the covariates. On one measure, quantitative
posttest, there was no significant difference between the two groups: F (1, 58) =
2.34, p = 0.132. But on the remaining five measures, RL-induced Cordillera
significantly outperformed Andes: F (1, 58) = 7.27, p = 0.009 for qualitative
posttest, F (1, 58) = 5.94, p = 0.018 for overall posttest, F (1, 59) = 4.72, p =
0.034 for quantitative NLG, F (1, 59) = 7.34, p = 0.009 for qualitative NLG and
F (1, 58) = 9.71, p = 0.003 for overall NLG respectively.
In sum, our results showed that micro-step based tutors can indeed be more
effective than step-based tutors as RL-induced Cordillera significantly outperformed Andes on all types of test questions. Even when time on task is factored
out, the same results hold for five out of six learning measures. Next, we compared Random and Hybrid-RL Cordillera with Andes to investigate whether the
micro-step tutor would still be more effective than the step-based tutor without
effective pedagogical policies.
Random vs. Andes & Hybrid-RL Cordillera vs. Andes: There were no
significant differences between the Random-Cordillera and Andes groups on any
of the learning outcome measures. Since Andes students spent significantly less
time than Cordillera students, we compared the two conditions’ posttest scores
using both pre-test score and total training time as covariates and their NLG
scores using total training time as the covariate. To our surprise, we still found
no significant differences between the two groups. We had expected the efficiency
of the Andes group to have some impact.
Similar results were found when we compared Hybrid-RL Cordillera and Andes on all types of learning outcome measures either when time on task is factored
in or out. Since Hybrid-RL Cordillera employed human-influenced pedagogical
rules, these results again indicate that expert tutors’ pedagogical rules may not
always be effective. Again, this study suggests that fine-grained interactions at
micro-steps are a potential source of pedagogical power, but human tutors may
not be particularly skilled at choosing the right micro-steps.

6

Conclusions and Future Work

Although it is often believed that micro-step based NL tutoring systems should
be more effective than conventional step-based ITSs, little evidence was previously found to support this. Our hypothesis is that it is because the existing micro-step based NL tutoring systems do not employ effective pedagogical
strategies. Previous work applied a general data-driven RL approach to induce
effective pedagogical policies directly from student logs and found them to be
more effective than either random or Hybrid-RL policies. However, it was still
not clear whether these RL-induced policies would make micro-step based NL
tutoring systems more effective than step-based ITSs.
In this paper, we found that RL-induced Cordillera significantly outperforms
Andes while neither Hybrid-RL Cordillera nor Random Cordillera were significantly different from step-based Andes. Our overall conclusion is that a microstep based system with effective RL-induced policies can significantly outperform

a step-based ITS with hand-coded policies; however, there is no significant difference between micro-step based and step-based tutoring systems in the absence of
effective policies. Note that micro-step based Cordillera is more time-consuming
than Andes. However, even when time on task is factored out, the micro-step
based tutoring system with effective RL-induced policies is still significantly better than the step-based tutoring systems with hand-coded policies on five out of
six learning performance measures.
Future work that remains is to explore policy-induction for Andes and to
conduct a comparison of step-based tutoring to micro-step tutoring when both
have effective RL-induced pedagogical policies. This may improve our understanding of the grain-size (step vs. micro-step) issue.
Acknowledgments This work was supported by NSF Award #0325054.

References
1. Anderson, J.R., Corbett, A.T., Koedinger, K.R., Pelletier, R.: Cognitive tutors:
Lessons learned. The Journal of the Learning Sciences 4(2), 167–207 (1995)
2. Chi, M., VanLehn, K., Litman, D.J., Jordan, P.W.: Empirically evaluating the
application of reinforcement learning to the induction of effective and adaptive
pedagogical strategies. User Model. User-Adapt. Interact. 21(1-2), 137–180 (2011)
3. Chi, M.T.H., Siler, S., Jeong, H.: Can tutors monitor students’ understanding
accurately? Cognition and Instruction 22(3), 363–387 (2004)
4. Chi, M., Jordan, P.W., VanLehn, K., Litman, D.J.: To elicit or to tell: Does it
matter? In: Dimitrova, V., Mizoguchi, R., du Boulay, B., Graesser, A.C. (eds.)
AIED. pp. 197–204. IOS Press (2009)
5. Evens, M., Michael, J.: One-on-one Tutoring By Humans and Machines. Mahwah,
NJ: Erlbaum (2006)
6. Graesser, A.C., Person, N., Magliano, J.: Collaborative dialog patterns in naturalistic one-on-one tutoring. Applied Cognitive Psychology 9, 359–387 (1995)
7. Graesser, A.C., VanLehn, K., Rosé, C.P., Jordan, P.W., Harter, D.: Intelligent
tutoring systems with conversational dialogue. AI Magazine 22(4), 39–52 (2001)
8. Jordan, P.W., Hall, B., Ringenberg, M., Cui, Y., Rosé, C.: Tools for authoring a
dialogue agent that participates in learning studies. In: Proceedings of AIED 2007.
pp. 43–50 (2007)
9. Leonard, W., Dufresne, R., Mestre, J.: Using qualitative problem-solving strategies to highlight the role of conceptual knowledge in solving problems. American
Journal of Physics 64(12) (1996)
10. Phobun, P., Vicheanpanya, J.: Adaptive intelligent tutoring systems for e-learning
systems. Procedia - Social and Behavioral Sciences 2(2), 4064 – 4069 (2010), innovation and Creativity in Education
11. Putnam, R.T.: Structuring and adjusting content for students: A study of live and
simulated tutoring of addition. Amer. Edu. Res. Journal 24(1), 13–48 (1987)
12. VanLehn, K., Graesser, Jackson, Jordan, Olney, Rose: When are tutorial dialogues
more effective than reading? Cog. Sci. 31(1), 3–62 (2007)
13. VanLehn, K.: The relative effectiveness of human tutoring, intelligent tutoring systems, and other tutoring systems. Educational Psychologist 46(4), 197–221 (2011)
14. VanLehn, K., Lynch, C., Schulze, K., Shapiro, J.A., Shelby, R., Taylor, L., Treacy,
D., Weinstein, A., Wintersgill, M.: The andes physics tutoring system: Lessons
learned. IJAIED 15(3), 147–204 (2005)

AIED 2013 Workshops Proceedings
Volume 4

AIED Workshop on Simulated Learners
Workshop Co-Chairs:
Gord McCalla
Department of Computer Science, University of Saskatchewan
John Champaign
RELATE, Massachusetts Institute of Technology

https://sites.google.com/site/aiedwsl/

ii

Preface
This workshop is intended to bring together researchers who are interested in
simulated learners, whatever their role in the design, development, deployment, or
evaluation of learning systems. Its novel aspect is that it isn’t just a workshop about
pedagogical agents, but is also concerned about other roles for simulated learners in
helping system designers, teachers, instructional designers, etc.
As learning
environments become increasingly complex and are used by growing numbers of
learners (sometimes in the hundreds of thousands) and apply to a larger range of
domains, the need for simulated learners (and simulation more generally) is compelling,
not only to enhance these environments with artificial agents, but also to explore design
issues using simulation that would be otherwise too expensive, too time consuming, or
even impossible using human subjects. The workshop aims to be broadly integrative
across all possible roles for simulated learners.

July, 2013
Gord McCalla & John Champaign.

iii

Program Committee
Co-Chair: Gord McCalla, University of Saskatchewan (mccalla@cs.usask.ca)
Co-Chair: John Champaign, Massachusetts Institute of Technology (jchampai@mit.edu)

Esma Aimeur, Université de Montréal
Roger Azvedo, McGill University
Gautam Biswas, Vanderbilt University
Tak-Wai Chan, National Central University of Taiwan
Robin Cohen, University of Waterloo
Ricardo Conejo, Universidad de Málaga
Evandro Costa, Federal University of Alagoas
Michel C. Desmarais, Ecole Polytechnique de Montréal
Sylvie Girard, Arizona State University
Lewis Johnson, Alelo
Yang Hee Kim, Utah State University
James Lester, North Carolina State University
Noboru Matsuda, Carnegie Mellon University
Kurt VanLehn, Arizona State University
Beverly Park Woolf, University of Massachusetts, Amherst

iv

Table of Contents
Using Simulated Learners and Simulated Learning Environments
within a Special Education Context
Carrie Demmans Epp and Alexandra Makos

1

Simulated Students, Mastery Learning, and Improved Learning Curves
for Real-World Cognitive Tutors
Stephen Fancsali, Tristan Nixon, Annalies Vuong and Steven Ritter

11

Exploring through Simulation the Effects of Peer Impact on Learning
Stephanie Frost and Gordon McCalla

21

Using HCI Task Modeling Techniques to Measure How Deeply Students Model
Sylvie Girard, Lishan Zhang, Yoalli Hidalgo-Pontet, Kurt Vanlehn,
Winslow Burleson, Maria Elena Chavez-Echeagary
and Javier Gonzalez-Sanchez

31

Validating Item Response Theory Models in Simulated Environments
Manuel Hernando, Eduardo Guzmán and Ricardo Conejo

41

Toward a reflective SimStudent:
Using experience to avoid generalization errors
Christopher MacLellan, Noboru Matsuda and Kenneth Koedinger

51

Towards Moment of Learning Accuracy
Zachary Pardos and Michael Yudelson

61

Impact of Prior Knowledge and Teaching Strategies on Learning by Teaching
Ma. Mercedes T. Rodrigo, Aaron Ong, Rex P. Bringula, Roselle S. Basa,
Cecilio Dela Cruz and Noboru Matsuda

71

1

Using Simulated Learners and Simulated Learning
Environments within a Special Education Context
Carrie Demmans Epp1 and Alexandra Makos2
1

Technologies for Aging Gracefully Laboratory (TAGlab), Dept. of Computer Science
University of Toronto, Toronto, Canada
carrie@taglab.ca
2
Ontario Institute for Studies in Education (OISE)
University of Toronto, Toronto, Canada
alexandra.makos@mail.utoronto.ca

Abstract. The needs of special education populations require specific support
to scaffold learning. The design and use of intelligent tutoring systems (ITS)
has the potential to meet these needs. Difficulty in the development of these
systems lies in their validation due to the ethics associated in studying learners
from this population as well as the difficulty associated with accessing members
of this learner group. This paper explores the use of simulated learners as a potential avenue for validating ITS designed for a special education population.
The needs of special education learners are discussed. Potential avenues for
employing simulated learners and simulated learning environments to test ITS,
instructional materials, and instructional methods are presented. Lastly, the expansion of an educational game designed to develop emotion recognition skills
in children with autism spectrum disorder is used to illustrate how simulated
learning environments can be used to support the learning of these students.
Keywords: Special Education, Ethics, Simulated Learners, Simulated Learning
Environments

1

Introduction

Many intelligent learning environments have been shown to help learners who belong
to the general population, but few existing systems have been shown to meet the
needs of those who fall under the umbrella of special education [1]. Learners in this
category have highly differentiated needs that are specified in an individual education
plan (IEP) [2]. Their increased need for personalization and continuous reinforcement
makes the argument for augmenting their education with intelligent tutoring systems
(ITS) even stronger. However, this has not been done widely.
Several factors may contribute to the lack of ITS use within special education. The
lack of validation that has been performed on the systems for special education populations [1], the difficulty of integrating ITS into special education settings [3], and the
difficulty of designing activities that ensure deep understanding may contribute to the

2

lack of ITS that support this population. The variability of learner needs presents additional challenges for system designers with respect to content development [3].
Furthermore, challenges that relate to the motivation, attitude, and social vulnerability
of members of this population make it more difficult to design and validate systems.
Developing systems for the special education population as a whole is difficult [4].
In addition to the above challenges, it may be difficult for designers to obtain access to a sufficiently large sample of the population to ensure that their ITS is beneficial in special education contexts. This is where the use of simulated learners and
simulated learning environments can be advantageous since their use can mitigate the
challenges presented by limited access to this vulnerable population and reduce the
negative ethical implications of testing these systems on members of this population.
It is important to look at the research on situated learning in order to understand the
achievements in best practices and lessons from research on simulated learning. Critical to this research is the combination of immersion and well-designed guidance that
supports the situated understanding of learners whereby they not only have a deep
understanding of the particular concepts that are being targeted, but the learners are
able to then generalize and apply these learned concepts to other contexts [5]. Research shows that game-like learning through digital technologies is a viable tool
across disciplines [6] and suggests that elements of game-like learning scaffold and
guide learners towards a deep understanding of concepts. The on demand instruction
of information that is vital to progress in the game is also important [5] and can be
exploited to encourage learning. Simulations can include these elements and use
stimuli to which special education populations react positively. Some stimuli that
have been shown to increase student engagement include music, visual cues, and
social stories [7]. Not only do these “strategies…help teachers increase engagement
[but they] are vital for promoting positive outcomes for students” [7].
To support the argument for the use of simulated learners in this educational context, we first describe the characteristics and needs of this population as well as the
learning environments in which they can be found. Following this, we discuss the use
of ITS by special education students, which includes student interactions with agents.
After laying this groundwork, we discuss the ethical implications and potential benefits to using simulated learners for validating ITS for use by special education populations. We then describe the potential uses of simulated learners and learning environments. This includes the description of an educational game, called EYEdentify,
which was designed to develop emotion recognition skills in children with autism
spectrum disorder (ASD). A discussion of how gaming principles and simulated environments can be further employed to expand EYEdentify for the purposes of helping
scaffold learners’ social interactions is provided.

2

Special Education

An introduction to the learning environments that exist in schools and the needs of
learners who are classified as special education is presented. The use of agents and
other forms of intelligent tutoring, within special education contexts, is then provided.

3

2.1

Learners and Learning Environments

These learners are either segregated into dedicated special education classrooms or
integrated into classrooms whose majority population consists of learners from the
general student body. Research has explored the design and integration of ubiquitous
technology into special education classrooms [8], but few e-learning environments
have been created to specifically support these students.
The needs and abilities of this population are highly variable, which can make generalizability hard [9]. This variability can be used to argue for the importance of personalizing students’ learning materials, environments, and experiences, which is evidenced by the existence of IEP that detail the learner’s specific needs and the accommodations that can be used to help the learner succeed [2]. Some of these accommodations include providing learners with additional time in order to complete tasks [1]
or allowing learners to perform tasks using different modalities (e.g., oral responses
rather than written ones) [2]. While these accommodations are necessary to ensuring
the learner’s success, it can be difficult to provide the necessary support, especially in
integrated classrooms. The use of ITS that better support the individual needs of these
learners could help alleviate the teacher’s need to provide these supports.
2.2

Simulated Learner and Agent Use

While the use of agents within ITS used by special education populations has been
studied, it appears that researchers and system developers are not simulating learners
who have special needs. Nilsson and Pareto have instead used teachable agents within
a special education context to help learners improve their math skills [3]. However,
they experienced difficulty integrating the ITS into the classroom. Whereas, Woolf et
al. were able to integrate their ITS into a classroom that had a mixed demographic:
the class consisted of both low and high performing students, and of those who were
low-performing, one third had a learning disability [10]. In this case, students interacted with an agent who played the role of a learning companion in order to support
the learner’s affective needs. It was found that this approach was especially beneficial to the low-performing students in the study, which may indicate the potential that
this system holds for helping many of the learners who fall under the special education umbrella. Other work has also shown that interactions with agents within an ITS
can improve or maintain learner interest and motivation [1].

3

Ethics

Given the vulnerable nature of this population, it is important that we not increase the
risk that they are exposed to by introducing them to ITS or other learning techniques
that have not been properly vetted since these could threaten the emotional well-being
of learners or their learning success [11]. The use of simulated learners can help ensure that these systems are properly tested before we expose special education learners to them. Simulated learners can help teachers, instructional designers, and system
developers meet the ethical guidelines of professional bodies by providing evidence

4

of the limitations and appropriateness of the instructional methods used by systems or
of the system itself [12].

4

Potential for Simulated Learner Use

We foresee two potential uses for simulated learners within a special education context both of which have been explored within other contexts. The first is during the
development and testing of ITS [13, 14], and the second is for teacher training [13].
Using simulated learners in these ways provides developers and instructors with access to learners in this population and prevents any potential harm that could result
from experimenting with members of this population. However, it may create a false
sense of the validity and usefulness of different systems and instructional techniques,
especially when we lack a full understanding of the abilities and symptomology of
some members of this population (e.g., those with Phelan-McDermid Syndrome).
Generalizability is difficult to perform with this population [9], but some level of
generalizability is required if a system is to be used by many people. Unfortunately,
current design methods, such as participatory design, fail to address how the system's
use and design should change over time. Furthermore, most users are unable to predict how they will use a system until they have integrated that system into their environment [15]. Carrying these challenges into the special education domain increases
their severity because of the additional communication barriers that may exist between system designers and learners with special needs [4]. While observation is a
component of many design methods, the lack of access to this population when combined with the communication challenges that exist reduces the feasibility of employing many of the more traditional user-centered design techniques.
Using simulated learners could benefit system designers and developers by allowing them to evaluate a system with various members of the special education population. This could reduce demands on a vulnerable population while allowing for some
level of system validation to be performed. Furthermore, the use of simulated learners
would allow systems to be tested with a far greater variety of learner types in order to
identify where the system may or may not be beneficial. If the system were webbased, the simulated learners could be implemented using a Selenium test suite based
on behavioural models of the system's target learners.
To effectively use simulated learners in this context, it is important to create these
learners using different and competing theoretical models of their behaviours and
abilities. This also alleviates some of the concerns that have been expressed over the
use of simulated users when testing adaptive systems [16]. The source of these models can be teachers or special education experts since their mental models might inform good stereotype-based models of learners that capture general behaviours which
are grounded in the expert's classroom experience. For example, haptic feedback can
be used to reinforce certain behaviours (e.g., pressing a button) in children with ASD.
However, we would argue for also including models from other sources since the
above experts are in short supply and cannot provide sufficient diversity in the models
to ensure that systems are adequately tested for a general special education popula-

5

tion. Simulated learners can be created from the cognitive models that are currently
described in the educational psychology literature or through the application of educational data mining and learning analytics techniques to the logs of ITS usage where
low performing and special education students were included in the classroom intervention. An example from the educational psychology literature could consider models of attention deficit hyperactivity disorder (ADHD), which include the amount of
hyperactivity and inattention that a learner has, to create simulated students that behave in a way that is consistent with both the inattention that is known to affect individual outcomes and the hyperactivity that can affect the classroom environment for
all students. Thus, allowing teachers to explore strategies that minimize the impact of
both of the behaviours that characterize students with ADHD [17].
The diversity of models on which the simulated learners are based may help compensate for the inaccuracies that are inherent to modeling techniques, therefore, reducing the need for simulated learners to have high-fidelity cognitive models. Especially,
since there is an incomplete understanding of the cognitive processes of all those who
fall under the umbrella of special education, as is demonstrated by research in mathematics and learning disabilities [18].
That said, simulated learners that are based on these models could be used to validate the design of learning materials and to ensure their effectiveness or comprehension [13, 14]. Teachers could use simulated learners to test learning materials for their
ability to increase learner engagement across a variety of contexts [7] before trying
the materials on learners in their class. This would give teachers the opportunity to
refine their teaching materials and confirm their suitability for students in the class.
Simulated learners can also be used to help prepare teachers either during preservice training or before a new school year begins when the teacher is preparing for
his/her incoming students [13]. The use of agents who play different types of special
education learners reduces the need to worry about the possible negative consequences that mistakes would have on learners [19]. This use of simulated learners also holds
the potential to reduce teacher errors since teachers can try new techniques with the
simulated learners and learn from those experiences, which may reduce the risk of
their committing errors with live learners.

5

Potential for Simulated Learning Environment Use

While simulated learning environments can pose a threat to learning because of the
complexity of the learning experience [20], they still hold the potential to benefit
learners with special needs. Simulated environments allow learners to take risks in
order to develop a deeper understanding of the situations they encounter [5]. This can
increase learner awareness of potential situations that could be encountered when
interacting with others. Ideally, simulated learning environments would be used to
help the learner develop and transfer skills into the real world by gradually increasing
the external validity of the tasks being performed.
Simulations allow system designers to ensure that the problems or activities being
studied resemble those that learners experience outside of the simulation [1] and they

6

allow for the gradual increase in the complexity and ecological validity of tasks [21].
This means that learners can begin their learning activities in a simpler environment
that is safe and progress towards more realistic situations, enabling the use of van
Dam's spiral approach, where learners encounter a topic multiple times at increasing
levels of sophistication [22]. This can help learners transfer their developing skills
into the real world. Additionally, the use of simulations accessible on different technologies can shift learner dependence on experts to technology whereby learner use of
the technology can help learners gain a sense of independence and begin to develop
the skills required to expand and extend their interactions to the real world [23]. We
illustrate this trajectory through a discussion of a mobile game that was designed to
help children with autism spectrum disorder learn to recognize emotions.
5.1

EYEdentify: An Educational Game for Emotion Recognition

EYEdentify is a mobile application for the Android platform that is designed to develop the emotion recognition skills of children with ASD since these are lacking.
Previous technologies that have tried to teach this skill to children with ASD have
primarily focused on the use of videos to model emotions for the learner [24]. Current
research focuses on social skill development through the use of interventions that use
a video series to develop social skills by exploiting the relationship between facial
expressions and emotion [4, 25]. Emotion recognition research suggests the most
important features of the face necessary to correctly identify emotions are the eyes
and the mouth [26]. Considering research on social skill development and advancements in portable technology, a mobile application that can support anytimeanywhere support to children with this deficit is timely.
EYEdentify is a game that uses a basic learner model to provide a flexible intervention in the form of an engaging game. It has an open learner model that can show
the child's progress to parents, caregivers, teachers, and specialists. The first version
of this application incorporates four emotions (i.e., happy, sad, frustrated, and confused) into a matching game that progresses through different levels (Fig. 1). There
are three types of images that are used in this game to help scaffold the child’s learning: cartoon robot faces, real faces that are superimposed on robot faces, and photographs of actual faces. The cartoon robot faces are designed to emphasize the eyes
and mouth. The superimposed faces are designed to activate the child’s knowledge of
focusing on the eyes and mouth to correctly identify the displayed emotions while
maintaining the scaffold of the robot head. The photograph of an individual making a
particular expression is used to activate the knowledge from the previously superimposed images to correctly identify the emotions. Difficulty increases with respect to
the type of emotion that is incorporated into game play and the types of images that
are used. Positive feedback is provided to the child throughout the game to encourage
continuous play. The game also has a calming event that is triggered by the accelerometer when the mobile device is shaken aggressively. The calming event increases
the volume of the music that is being played and prompts the child to count to ten.
The child is then asked whether or not s/he wants to continue playing the game.

7

Fig. 1. The gameplay screen with the correct responses identified (surrounded in green).

The mobile application provides the ability to customize game play by incorporating personalized feedback and images. Users can customize feedback by typing a
comment and recording an audio message before adding this feedback to the schedule. Image customization uses the front camera of the device to capture individuals
parroting the facial expression represented on the robot prompt. As children progress
through the levels, they are rewarded with parts to assemble their own robot.
The current version focuses on developing emotion recognition skills for four of
the fifteen basic emotions identified by Golan et al. [25]. The addition of the remaining eleven emotions could be used to extend game play. Currently, the mobile application is functional; however, more emotions are being incorporated and iOS versions
are being developed before releasing EYEdentify on Google Play and the App Store.
5.2

Expanding EYEdentify to Include a Simulated Learning Environment

The expansion of EYEdentify to include a simulated learning environment draws on
Csikszentmihalyi's definition of flow and research on gaming. Flow is described as
the experience of being fully engaged in an activity where an individual is “so involved…that nothing else seems to matter” [27]. This is derived from activities where
a person’s skills are matched to the challenges encountered [27]. For learners, this
means that they will be in a mental state that keeps them motivated to stay involved in
a particular activity. Research in gaming and game design incorporates these psychological underpinnings whereby elements of a game seek to cultivate and support the
player’s active engagement and enhanced motivation [28]. In educational games,
these elements are employed to scaffold learning just-in-time and provide instructors
with the ability to adapt the system to the specific needs of the learner [29].
EYEdentify currently provides a matching game with rewards that are selfcontained within the mobile application. Preliminary trials indicate that it keeps learners involved in the activity of identifying emotions for long periods of time. These

8

trials parallel the findings of research that used a video intervention program known
as “The Transporters” to develop the social skills of children with ASD [30].
EYEdentify’s game play can be expanded into simulated learning environments to
move players beyond the acquisition of emotion recognition skills toward the development of social skills. In creating game-based simulations for learners to use, the
capacity to scaffold their learning within game play and support the development of
transferable skills to the real-world increases.
There are several ways to expand game play into a simulated learning environment. All possibilities would require the mastery of basic emotion recognition and
could involve levels of progressive difficulty that incorporates these emotions into
depictions of social situations. The front camera of the mobile device could be used to
scaffold the recognition of emotions by way of augmented reality, as could the recent
introduction of Google glass. Avatars that represent individuals from the learner’s
day-to-day life could be used by learners to practice particular social situations. Additionally, game play could incorporate depictions of situations that model different
social interactions. This could then be incorporated with a Sims-like environment
where learners would have to identify the emotion of the character that they are interacting with and demonstrate the appropriate behaviour or emotional response. Specific to keeping learners engaged, the addition of an emotion recognition system that can
detect the learner’s emotion from the front camera and keep track of their emotion
when playing the game to determine that learner’s level of engagement would be
useful. Through the development of these possibilities, EYEdentify has the potential
to enhance learners’ emotion recognition and social skill development in a way that
enables the learner to transfer these skills to their day-to-day encounters.

6

Conclusion

The use of simulated learners and learning environments within special education
contexts holds great potential for improving the quality and applicability of ITS use
by members of this population. Simulated learners can be used to test learning materials, learning methods, and ITS to ensure their appropriateness for the members of this
population, who have highly variable needs. The use of simulated learners and learning environments can be further exploited for teacher training. In addition to this use,
simulated learning environments can be used to help learners who have been classified as having special needs to transfer their knowledge and skills to their everyday
lives. The potential for members of this population to use simulated learning environments was illustrated through an example of an educational game, EYEdentify,
that is used to help children with autism spectrum disorder improve their ability to
recognize emotions. The described potential expansions of this game show how different approaches to simulated learning environments and the use of augmented reality can be used to help learners transition between the simulated world and the one
they encounter every day.

9

References
1.

2.
3.

4.

5.
6.

7.

8.

9.

10.

11.

12.
13.

14.

Bruno, A., Gonzalez, C., Moreno, L., Noda, M., Aguilar, R., Munoz, V.: Teaching mathematics in children with Down’s syndrome. In: Artificial Intelligence in Education
(AIED). Sydney, Australia (2003).
Government of Ontario: Individual Education Plans Standards for Development, Program
Planning, and Implementation. Ontario Ministry of Education (2000).
Nilsson, A., Pareto, L.: The complexity of integrating technology enhanced learning in
special math education - a case study. In: 5th European Conference on Technology Enhanced Learning on Sustaining TEL: from Innovation to Learning and Practice. pp. 638–
643. Springer-Verlag, Berlin, Heidelberg (2010).
Wainer, A.L., Ingersoll, B.R.: The use of innovative computer technology for teaching
social communication to individuals with autism spectrum disorder. Research in Autism
Spectrum Disorders. 5, 96–107 (2011).
Assessment, equity, and opportunity to learn. Cambridge University Press, Cambridge ;
New York (2008).
Jackson, L.A., Witt, E.A., Games, A.I., Fitzgerald, H.E., von Eye, A., Zhao, Y.: Information technology use and creativity: Findings from the Children and Technology Project.
Computers in Human Behavior. 28, 370–376 (2012).
Carnahan, C., Basham, J., Musti-Rao, S.: A Low-Technology Strategy for Increasing
Engagement of Students with Autism and Significant Learning Needs. Exceptionality. 17,
76–87 (2009).
Tentori, M., Hayes, G.: Designing for Interaction Immediacy to Enhance Social Skills of
Children with Autism. Ubiquitous Computing (Ubicomp). pp. 51–60. ACM, Copenhagen,
Denmark (2010).
Moffatt, K., Findlater, L., Allen, M.: Generalizability in Research with Cognitively Impaired Individuals. In: Workshop on Designing for People with Cognitive Impairments,
ACM Conference on Human Factors in Computing Systems (CHI). ACM, Montreal, Canada (2006).
Woolf, B.P., Arroyo, I., Muldner, K., Burleson, W., Cooper, D.G., Dolan, R., Christopherson, R.M.: The Effect of Motivational Learning Companions on Low Achieving
Students and Students with Disabilities. In: Aleven, V., Kay, J., and Mostow, J. (eds.) Intelligent Tutoring Systems (ITS). pp. 327–337. Springer Berlin Heidelberg, Berlin, Heidelberg (2010).
Cardon, T.A., Wilcox, M.J., Campbell, P.H.: Caregiver Perspectives About Assistive
Technology Use With Their Young Children With Autism Spectrum Disorders. Infants &
Young Children. 24, 153–173 (2011).
Code of Fair Testing Practices in Education. Washington, D.C.: Joint Committee on Testing Practices, American Psychological Association (1988).
VanLehn, K., Ohlsson, S., Nason, R.: Applications of Simulated Students: An Exploration. International Journal of Artificial Intelligence in Education (IJAIED). 5, 135–175
(1996).
Mertz, J.S.: Using A Simulated Student for Instructional Design. International Journal of
Artificial Intelligence in Education (IJAIED). 8, 116–141 (1997).

10

15. Dawe, M.: Design Methods to Engage Individuals with Cognitive Disabilities and their
Families. In: the Science of Design Workshop, ACM Conference on Human Factors in
Computing Systems (CHI) (2007).
16. Paramythis, A., Weibelzahl, S., Masthoff, J.: Layered Evaluation of Interactive Adaptive
Systems: Framework and Formative Methods. User Modeling and User-Adapted Interaction (UMUAI). 20, 383–453 (2010).
17. Rogers, M., Hwang, H., Toplak, M., Weiss, M., Tannock, R.: Inattention, working
memory, and academic achievement in adolescents referred for attention deficit/hyperactivity disorder (ADHD). Child Neuropsychology. 17, 444–458 (2011).
18. Geary, D.C.: Mathematics and Learning Disabilities. J Learn Disabil. 37, 4–15 (2004).
19. Ogan, A., Finkelstein, S., Mayfield, E., D’Adamo, C., Matsuda, N., Cassell, J.: “Oh dear
stacy!”: social interaction, elaboration, and learning with teachable agents. In: ACM Conference on Human Factors in Computing Systems (CHI). pp. 39–48. ACM, New York,
NY, USA (2012).
20. Moreno, R., Mayer, R., Lester, J.: Life-Like Pedagogical Agents in Constructivist Multimedia Environments: Cognitive Consequences of their Interaction. In: World Conference
on Educational Multimedia, Hypermedia and Telecommunications (EDMEDIA). pp. 776–
781 (2000).
21. Henderson-Summet, V., Clawson, J.: Usability at the Edges: Bringing the Lab into the
Real World and the Real World into the Lab. In: Workshop on Usability in the Wild, International Conference on Human-Computer Interaction (INTERACT) (2007).
22. Van Dam, A., Becker, S., Simpson, R.M.: Next-generation educational software: why we
need it & a research agenda for getting it. EDUCAUSE Review. 40, 26–43 (2007).
23. Stromer, R., Kimball, J.W., Kinney, E.M., Taylor, B.A.: Activity schedules, computer
technology, and teaching children with autism spectrum disorders. Focus on Autism and
Other Developmental Disabilities. 21, 14–24 (2006).
24. DiGennaro Reed, F.D., Hyman, S.R., Hirst, J.M.: Applications of technology to teach
social skills to children with autism. Research in Autism Spectrum Disorders. 5, 1003–
1010 (2011).
25. Golan, O., Ashwin, E., Granader, Y., McClintock, S., Day, K., Leggett, V., Baron-Cohen,
S.: Enhancing Emotion Recognition in Children with Autism Spectrum Conditions: An
Intervention Using Animated Vehicles with Real Emotional Faces. Journal of Autism and
Developmental Disorders. 40, 269–279 (2009).
26. Erickson, K., Schulkin, J.: Facial expressions of emotion: A cognitive neuroscience perspective. Brain and Cognition. 52, 52–60 (2003).
27. Csikszentmihalyi, M.: Flow: The Psychology of Optimal Experience. Harper Perennial
Modern Classics (2008).
28. Tom Chatfield: 7 ways games reward the brain | Video on TED.com. (2010).
29. Fernández López, Á., Rodríguez Fórtiz, M.J., Noguera García, M.: Designing and Supporting Cooperative and Ubiquitous Learning Systems for People with Special Needs. In:
Confederated International Workshops and Posters on the Move to Meaningful Internet
Systems: ADI, CAMS, EI2N, ISDE, IWSSA, MONET, OnToContent, ODIS, ORM,
OTM Academy, SWWS, SEMELS, Beyond SAWSDL, and COMBEK. pp. 423–432.
Springer-Verlag, Berlin, Heidelberg (2009).
30. The Transporters. Changing Media Development Ltd (2006).

11

Simulated Students, Mastery Learning, and Improved
Learning Curves for Real-World Cognitive Tutors
Stephen E. Fancsali, Tristan Nixon, Annalies Vuong, and Steven Ritter
Carnegie Learning, Inc.
Frick Building, Suite 918
437 Grant Street, Pittsburgh, PA 15219

{sfancsali, tnixon, avuong, sritter}
@carnegielearning.com

Abstract. We briefly describe three approaches to simulating students to develop and improve intelligent tutoring systems. We review recent work with simulated student data based on simple probabilistic models that provides important
insight into practical decisions made in the deployment of Cognitive Tutor
software, focusing specifically on aspects of mastery learning in Bayesian
Knowledge Tracing and learning curve analysis to improve cognitive (skill)
models. We provide a new simulation approach that builds on earlier efforts to
better visualize aggregate learning curves.
Keywords: Knowledge tracing, learning curves, student modeling, Cognitive
Tutor, simulation, simulated students, mastery learning

1

Introduction

There are at least three general approaches to simulating students for the purposes
of improving cognitive (skill) models and other features of intelligent tutoring systems (ITSs). One approach, generally connoted in discussions of “simulated” students or learners, employs aspects of cognitive theory to simulate students’ learning
and progression through ITS problems (e.g., via machine learning or computational
agents like SimStudent [2]). Another class of simulations makes use of relatively
simple probabilistic models to generate response data (i.e., Bayesian Knowledge
Tracing [BKT] [1]) intended to represent a (simulated) student’s evolving performance over many practice attempts. Third, there are data-driven approaches that do
not easily fit into either of the first two categories.
In this work, we explicate and provide examples of each approach and briefly describe Carnegie Learning’s Cognitive Tutors (CTs) [3]. We then focus on the second
approach and review recent work on simulations of student learning with simple
probabilistic models. These simulation studies provide novel insights into a variety of
features of CTs and their practical deployment.

12

CTs implement mastery learning; mathematics content is adaptively presented to
students based upon whether the tutor has judged that a student has mastered particular skills. Mastery is assessed according to whether the tutor judges that the probability that a student has mastered a particular skill exceeds a set threshold. We review a
simulation study that provides for best and worst-case analyses (when “ground truth”
characteristics of simulated learner populations are known) of tutor skill mastery
judgment and efficient student practice (i.e., adaptively providing students with opportunities to practice only those skills they have not mastered). This study not only
provides justification for the traditionally used 95% probability threshold, but it also
illuminates how the threshold for skill mastery can function as a “tunable” parameter,
demonstrating the practical import of such simulation studies.
Finally, learning curves provide a visual representation of student performance on
opportunities to practice purported skills in an ITS. These representations can be used
to analyze whether a domain has been appropriately atomized into skills. If opportunities correspond to practice for a single skill, we expect to see a gradual increase in
the proportion of correct responses as students get more practice opportunities. If, for
example, the proportion of students responding correctly to an opportunity drastically
decreases after three practice opportunities, it seems unlikely that the opportunities
genuinely correspond to one particular skill. Turning to the third, data-driven approach to simulating students, we provide a new method to visualize aggregate learning curves to better drive improvements in cognitive (skill) models used in CTs, This
approach extends recent work that explores several problems for utilizing learning
curves aggregated over many students to determine whether practice opportunities
correspond to a single skill.

2

Cognitive Tutors

CTs are ITSs for mathematics curricula used by hundreds of thousands of K-12
and undergraduate students every year. Based on cognitive models that decompose
problem solving into constituent knowledge components (KCs) or skills, CT implements BKT to track student skill knowledge. When the system’s estimate of a student’s knowledge of any particular skill exceeds a set threshold, the student is judged
to have mastered that skill. Based on the CT’s judgment of skill mastery, problems
that emphasize different skills are adaptively presented so that the student may focus
on those skills most in need of practice.

3

Three Approaches to Simulating Learners

There are at least three general simulation methods used to model student or learner performance. One simulation strategy, based on cognitive theories such as ACT-R
[4], explicitly models cognitive problem-solving processes to produce rich agentbased simulated students. The SimStudent project ([2], [5]), for example, has been
developed as a part of a suite of authoring tools to develop curricula for CTs, called
Cognitive Tutor Authoring Tools (CTAT) [6]. SimStudent learns production rules

13

from problem-solving demonstrations (e.g., an author providing simple demonstrations of problem solutions or via ITS log data). These human-interpretable production
rules correspond to KCs that comprise cognitive models vital to CTs. SimStudent
aims to simplify development of new CT material by automating the discovery of KC
models in new domains via a bottom-up search for skills that potentially explain the
demonstrations.
Second, there are numerous probabilistic methods that model task performance as a
function of practice, according to various task and learner-specific parameters. One
may instantiate numerous such models, with varying parameters, and sample from the
resulting probability distributions to obtain simulated performance data for an entire
hypothetical learner population.
One common example is a Hidden Markov Model (HMM) with two latent and two
observable states, that can serve as a generative BKT model, using parameters specified according to expert knowledge or inferred by a data-driven estimation procedure.
Two hidden nodes in the HMM represent “known” and “unknown” student
knowledge states. In practice, of course, student knowledge is latent. Simulated students are assigned to a knowledge state according to BKT’s parameter for the probability of initial knowledge, P(L0), and those in the “unknown” state transition to the
“known” state according to the BKT parameter for the probability of learning or
transfer, P(T). Simulated, observed responses are then sampled according to BKT
parameters that represent the probability of student guessing, P(G) (i.e., responding
correctly when in the unknown state) and slipping, P(S) (i.e., responding incorrectly
when in the known state), depending upon the state of student knowledge at each
practice opportunity.
Contrary to her real-world epistemological position, simulations generally allow an
investigator to access the student’s knowledge state at each simulated practice opportunity. This allows for comparisons between the “ground truth” of skill mastery and
any estimate derived from resulting simulated behavior. Clearly, richer cognitive
agents, such as SimStudent, provide a more complete picture of the student’s cognitive state at any point.
Simpler probabilistic models represent student knowledge of a skill with a single
state variable, so they correspondingly scale better to larger scale simulations of
whole populations. While a probabilistic model only requires a reasonable distribution
over initial parameters, richer cognitive models may require training on a great deal of
detailed, behavioral or demonstration data. Nevertheless, cognitive model-based
simulations allow us to investigate issues like timing (i.e., response latency), sensitivity to input characteristics, and error patterns in learner responses.
There are many cases in which a relatively simple probabilistic model may be of
utility, despite its impoverished nature. A simplistic representation of student
knowledge provides an ideal situation to test the performance and characteristics of
inference methods using data from a known generating process and parameters. One
might, for example, compare the point at which simulated students acquire knowledge
of a skill to the point at which the CT judges the student to have mastered the skill.
The approach thus allows for students of “best” and “worst” case scenarios with respect to the relationship between how the CT models students and the actual make up

14

of (simulated) student populations. We can better understand the dynamics of the
student sub-populations we inevitably face in practice by simulating data from diverse
sub-populations, the make up of which we can specify or randomize in various ways.
Furthermore, we can simulate student performance (sometimes augmenting available
empirical data) both with and without mastery learning (i.e., students being removed
from a population because they have mastered a skill) on learning curves constructed
from aggregate data.
Previous work [7] explored a third, data-driven simulation method that “replays”
empirical student performance data through CT in order to estimate the impact of a
change in BKT parameters in a more substantive way. For each KC that occurred in a
given problem, we sampled the next observed response on that KC from the sequence
actually observed from a real student. These responses would then drive updates to
CT’s cognitive model, knowledge tracing, and the problem-selection mechanism. If
more data were required than were observed for a given student, further observations
were sampled from a BKT model initialized to the state inferred from the student’s
actions thus far. By repeating this process for all students in the observed data set, we
could obtain estimates of the number of problems students would be expected to
complete if a change to the cognitive model were implemented.
This method has the advantage of preserving characteristics of real student data rather than resorting to a theoretical model of student performance. However, it does
make several assumptions about the reproducibility of that behavior under the hypothesized changes. Specifically, it assumes that the observed sequence of correct/incorrect responses would not change even given a different selection of problems, potentially emphasizing different KCs. This assumption may be justified if we
believe we have complete coverage of all KCs relevant to the task in question in the
cognitive model and that all KCs are truly independent of each other.
While simulation methods based on rich cognitive theory and data-driven re-play
of empirical data provide many opportunities for future research, we focus in this
paper on simple, probabilistic simulations in the context of the BKT framework.

4

Substantive Measures of Efficient Student Practice

Before we discuss how the BKT mastery threshold probability functions as a “tunable” parameter in an ITS like the CT, we provide “substantive” quantification of
goodness of fit of cognitive/skill models for CTs beyond mere RMSE of prediction
(i.e., beyond the extent to which models can predict whether students will respond
correctly to particular practice opportunities) [8-11]. New error or goodness of fit
measures are countenanced in terms of efficient student practice, based on the number
of practice opportunities (i.e., “over-practice” or “under-practice”) we might expect a
student to experience in a CT. Over-practice refers to the continued presentation of
new practice opportunities, despite the student’s mastery or knowledge of the relevant
KC.1 Student “under-practice” instances are those in which a student has yet to
1

One exception is an experimental study [11] that reports increased efficiency by deploying
parameters estimated using a data mining method called Learning Factors Analysis (LFA).

15

achieve knowledge of a KC, and yet the mastery learning system has judged the student as having mastered it, ending the presentation of further learning opportunities.
From estimates of expected under- and over-practice, one can calculate other meaningful measures of students gains and losses, such as time saved or wasted.
Some of this work [8, 9] uses empirical data to estimate the extent of underpractice and over-practice we might expect students to experience. Specifically, the
expected numbers of practice opportunities it takes a student to reach mastery when
parameters are individualized per student are compared to the expected practice when
a single (population) set of parameters is used to assess all students. One individualization scheme used to study under and over-practice estimates all four BKT parameters, per student, from response data over all relevant skills (i.e., each student receives
one individualized quadruple of BKT parameters for all KCs) [8]. Another approach
[9] only individualizes P(T) for each student based on both per-student and per-skill
components estimated from observed data [12]. Both individualization schemes provide for substantive gains (compared to using a set of population parameters to assess
all students’ progress to mastery) in the efficiency of practice (i.e., fewer expected
under and over-practice opportunities) as well as better prediction performance
judged, in the standard way, by a metric like RMSE.

5

Idealized Performance of Mastery Learning Assessment

Now we address how BKT performs with respect to efficiency of practice in idealized cases in which the composition of student (sub-) populations is known. Simulation studies can shed light on how BKT performs when mastery learning parameters
used by the CT run-time system exactly match those of the generating model (i.e., the
best case), and in worst cases in which student parameters either maximally differ
from mastery learning parameters or vary at random for each student.
Recent work addresses these issues by adopting a probabilistic simulation regime
[10]. Since we can track the point at which a simulated student acquires knowledge
of a skill, we are able to compare this to the opportunity at which the mastery learning
system first judges it to be acquired. Simulations were run for fourteen skills, a subset
of those found by [13] to be representative of a substantial portion of skills in deployed CT curricula, across thousands of virtual students.
Even in idealized, best case scenarios (i.e., when parameters used to assess skill
mastery perfectly match simulated student data-generating parameters), for most
skills and a large number of students, we expect there to be one to four “lagged” practice opportunities between the point at which simulated students transition to mastery
and the point at which the BKT run-time system judges mastery. That is, in general,
even when a student population is modeled “perfectly,” and given the traditional setting of the probability threshold for mastery at 95%, most students should be expected
to see at least a few opportunities beyond the point of skill acquisition. That some
“over-practice” may be inevitable provides a relevant context within which to considEfficiency is operationalized as decreased time required to work through material in the Geometry CT without decreasing overall learning.

16

er empirically driven results of [8, 9]. Although a certain amount of lag may be inherent in the nature of BKT, we seek to establish a range for the “acceptable” lag, and to
better appraise efficiency of practice [10].

6

Mastery Learning Threshold as a “Tunable” Parameter

In addition to lagged opportunities and over-practice, situations in which students
under-practice skills are important to consider. Given the possibly inevitable lag between skill acquisition and mastery judgment, simulations [10] have also been used to
explore how the mastery probability threshold might be “tuned” to influence the
trade-off of over-practice and under-practice experienced by students in mastery
learning systems like CTs.
Pre-mature mastery judgments can lead, for example, to students being moved
along by the CT to problems that emphasize new KCs without having mastered prerequisite KCs. Other things held equal, simulations in [10] provide that pre-mature
mastery judgment is more likely to occur in worst-case scenarios, when masterylearning parameters do not match parameters for sub-populations of simulated students.
Simulations in [10] also establish that the mastery-learning threshold can function
as a tuning parameter, partially governing the trade-off between the expected proportion of students pre-maturely judged to have reached skill mastery and the number of
over-practice opportunities they are likely to experience. As the threshold probability
is increased, the proportion of students assessed as having pre-maturely mastered
skills decreases while the proportion of those that are exposed to practice opportunities after skill acquisition increases (along with the number of lagged and overpractice opportunities, i.e., those beyond a calculated acceptable lag they experience).
The results of [10] show that the traditionally used 95% threshold seems to provide
for a “conservative” tutor that is more likely to present opportunities after skill acquisition rather than under-practice. Depending upon course design and practice regimes, the mastery-learning threshold might be manipulated to important, practical
effect. For example, pre-mature mastery judgments might be acceptable in larger
numbers when there is a mixed-practice regime that will allow students to practice
KCs later in the curriculum.

7

Using Simulations to Illuminate Learning in Learning Curves

Learning curves provide a visual representation of student performance over opportunities to practice skills. For each (purported) skill, we construct a learning curve
by plotting opportunities (i.e., 1st, opportunity, 2nd opportunity, and so on) on the xaxis and the proportion of students that provide correct responses at each opportunity
on the y-axis. Aggregated over real-world student practice opportunity data, such

17

curves provide means by which to visually2 inspect whether opportunities genuinely
correspond to practice of one particular skill. If opportunities correspond to one particular skill, we expect a gradual increase in the proportion of students that respond
correctly with increasing practice. Generally, for well-modeled skills (and a variety
of other cognitive tasks), it is thought that such a plot should correspond roughly to a
power law function (i.e., the power law of practice [14]), though this point is not
without controversy [15]. Recent research [16-17] demonstrates how some aggregate
learning curves can distort the picture of student learning. Aggregate learning curves
may, for example, appear to show no learning, when, in fact all students are learning
at different rates. Others may provide for a small rise in probability of correct response initially but then “drop,” as if students were forgetting, even when individual
students are consistently mastering their skills.
The learning curve of Fig. 1 illustrates aspects of both problems, with a relatively
flat portion, followed by a drop, after a small increase in probability correct from its
initial value. The red line, representing the size of the student population at each opportunity, illustrates that BKT is determining that students are mastering the skill
relatively quickly.

70

900 1000

80

1200

90

100

1400

Learning Curve
Skill: select form of one with numerator of one−1

●
●

●

●
●

800

% correct
50
60

●

●
●

●

●

●
●

●

●
●

●
●

●

●
●

●

●

700

●

●

600

●

500

●

10

200

300

20

400

30

40

●

# students

●
●
●

0

100

% correct = 100 − 45.2 * opportunity^−0.0421
R^2 = 0.0571

1

2

3

4

5

6

7

8

9 10

12

14

16

18

20

22

24

26

28

30

opportunities

Fig. 1. Empirical Learning Curve for Skill “Select form of one with numerator of one”; the blue
line represents empirical data plotted as percentage of correct responses, and the black line
represents a fitted power function. The red line provides the size of the student population.

Two ways to re-visualize problematic, aggregated learning curves have been suggested [16]. One is to provide multiple learning curves (on the same plot) for individual
2

Developers at Carnegie Learning also deploy several data-driven heuristics (that correspond to
various visual features of learning curves) to analyze our large portfolio of KCs (i.e., several
thousand KCs over several mathematics CT curricula) and observed student data to draw attention to those KCs that may require revision in our deployed cognitive models.

18

“segments” of students based upon how many opportunities students, in observed
data, take to reach the mastery learning threshold for a skill. Such segmented learning
curves are provided with the same x-axis and y-axis as standard learning curves (i.e.,
practice opportunity count on the x-axis and, e.g., percentage of student correct response on the y-axis).
The second approach suggested by [16] has the analyst plot “mastery-aligned”
learning curves. In such learning curves, students are also segmented according to the
number of opportunities required to reach mastery, but the end-point of the x-axis
corresponds to the opportunity at which students’ reach mastery (m) and moving left
along the x-axis corresponds to the opportunity before mastery (m-1), the second to
last opportunity before mastery (m-2), and so on.
Further work [17] provides a mathematical explanation, along with proof-ofconcept simulation studies based on HMMs, for the dynamics of aggregate learning
curves to explain how both mastery learning itself and differing student subpopulations, when aggregated, can contribute to learning curves that do not show
learning (or manifest other peculiar, possible deceptive, phenomena like “negative”
learning).
We illustrate an alternative to [16] by providing a method that relies on probabilistic simulation to construct aggregate learning curves that better represent learning in
empirical student data. Specifically, we “pad” empirical data for student skill opportunities with simulated data to mask the effects of attrition due to mastery learning
and possibly “reveal” student learning. Student opportunity data are generated with
the same parameters used to track student progress and the probability of student
knowledge estimated at the point at which the student crossed the mastery threshold.
Such simulations provide us data after a student no longer receives practice opportunities for a particular skill because they have been judged as having achieved mastery.
For the aggregate learning curve of Fig. 1, the “padded” learning curve is Fig. 2.
The fitted power-law slope parameter decreases from -0.042 to -0.363 (indicating
more learning), and the goodness-of-fit of the power law function (R2) increases from
0.0571 to 0.875. We apply the method to 166 skills identified3 by [16] as possibly
problematic in the Cognitive Tutor Algebra I (CTAI) curriculum. We find an improvement (i.e., power-fit parameter decreases from above -0.1 to below -0.1, a criterion deployed by [16]) for 98 skills (59%). While this method provides an improved
visualization and understanding of fewer skills than the disaggregation procedures
suggested by [16], this seems to provide evidence of the great extent to which mastery
learning attrition obfuscates evidence for student learning.
Importantly, our simulation method does not eliminate the early dip in the learning
curve at opportunity 3 when little attrition has yet to take place, but only masks the
effects of attrition due to mastery learning. Such an approach focuses largely on a
better representation or visualization of the “tail” of aggregate learning curves. This
3

These skills were chosen because the over-whelming majority of students are judged to
eventually master them (i.e., CT “thinks” the students are learning); they are not premastered (i.e., P(L0) < 0.95); they do not show learning in their aggregate learning curve
(i.e., power-law fit parameter > -0.1); aggregate learning curves for these skills do not have
multiple maxima; and we have data for at least 250 students for these skills [16].

19

allows us to focus on other features of the learning curve that may indicate illmodeled KCs in a cognitive model, software bugs, and other possible problems.

90

100

Simulation−Padded Learning Curve
Skill: select form of one with numerator of one−1

80

●

●

70

●
●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●
●

●

●

●

10

20

30

40

% correct
50
60

●

0

% correct = 100 − 67.5 * opportunity^−0.363
R^2 = 0.875

1

2

3

4

5

6

7

8

9

10

12

14

16

18

20

22

24

26

28

30

opportunities

Fig. 2. Simulation-Padded Learning Curve for Skill “Select form of one with numerator of one”

8

Summary

We briefly reviewed several methods for simulating learners. We focused on ways
in which simple probabilistic models, in contrast to methods that rely on rich cognitive theory, can be used to generate student performance data to help drive practical
decision-making about CT deployment, focusing first on the mastery threshold probability of BKT as a tunable parameter to determine aspects of efficient practice. Then
we introduced a new method for visualizing aggregate learning curves that relies on
both empirical and simulated data that helps to mask the bias introduced by mastery
learning attrition. Future work will further explore these methods, new simulation
regimes, and their practical import.

References
1.

2.

Corbett, A.T., Anderson, J.R.: Knowledge Tracing: Modeling the Acquisition of
Procedural Knowledge. User-Modeling and User-Adapted Interaction 4, 253–278
(1995)
Matsuda, N., Cohen, W.W., Sewall, J., Koedinger, K.R.: Applying Machine Learning
to Cognitive Modeling for Cognitive Tutors. Human-Computer Interaction Institute,
Carnegie Mellon University. Paper 248 (CMU-ML-06-105) (2006)

20

3.

4.
5.

6.

7.

8.

9.

10.

11.

12.

13.

14.

15.
16.

17.

Ritter, S., Anderson, J.R., Koedinger, K.R., Corbett, A.T.: Cognitive Tutors: Applied
Research in Mathematics Education. Psychonomic Bulletin & Review 14, 249–255
(2007)
Anderson, J.R.: Rules of the Mind. Erlbuam, Hillsdale, NJ (1993)
Matsuda, N., Cohen, W.W., Sewall, J., Lacerda, G., Koedinger, K.R.: Evaluating a
Simulated Student Using Real Students Data for Training and Testing. In: Proceedings
of the International Conference on User Modeling (LNAI 4511), pp. 107–116 (2007)
Aleven, V., McLaren, B.M., Sewall, J., Koedinger, K.R.: The Cognitive Tutor Authoring Tool (CTAT): Preliminary Evaluation of Efficiency Gains. In: Proceedings of the
8th International Conference on Intelligent Tutoring Systems, pp. 61-70 (2006)
Dickison, D., Ritter, S., Nixon, T., Harris, T., Towle, B., Murray, R.C., Hausmann,
R.G.M.: Predicting the Effects of Skill Model Changes on Student Progress. In: Proceedings of the 10th International Conference on Intelligent Tutoring Systems (Part II),
pp. 300-302 (2010)
Lee, J.I., Brunskill, E.: The Impact of Individualizing Student Models on Necessary
Practice Opportunities. In: Proceedings of the 5th International Conference on Educational Data Mining, pp. 118–125 (2012)
Yudelson, M.V., Koedinger, K.R.: Estimating the Benefits of Student Model Improvements on a Substantive Scale. In: Proceedings of the 6th International Conference on
Educational Data Mining (2013)
Fancsali, S., Nixon, T., Ritter, S.: Optimal and Worst-Case Performance of Mastery
Learning Assessment with Bayesian Knowledge Tracing. In: Proceedings of the 6th International Conference on Educational Data Mining (2013)
Cen, H., Koedinger, K., Junker, B.: Is Over-Practice Necessary? – Improving Learning
Efficiency with the Cognitive Tutor through Educational Data Mining. In: Proceedings
of the 13th International Conference on Artificial Intelligence in Education, pp. 511–518
(2007)
Yudelson, M.V., Koedinger, K.R., Gordon, G.J.: Individualized Bayesian Knowledge
Tracing Models. In: Proceedings of the 16th International Conference on Artificial Intelligence in Education (2013)
Ritter, S., Harris, T.K., Nixon, T., Dickison, D., Murray, R.C., Towle, B.: Reducing the
Knowledge Tracing Space. In: Proceedings of the 2nd International Conference on Educational Data Mining, pp. 151-160 (2009)
Newell, A., Rosenbloom, P.S.: Mechanisms of Skill Acquisition and the Law of Practice. In: Anderson, J.R. (ed.) Cognitive Skills and Their Acquisition, pp. 1-55. Erlbaum,
Hillsdale, NJ (1981)
Heathcote, A., Brown, S.: The Power Law Repealed: The Case for an Exponential Law
of Practice. Psychonomic Bulletin & Review 7, 185-207 (2000)
Murray, R.C., Ritter, S., Nixon, T., Schwiebert, R., Hausmann, R.G.M., Towle, B.,
Fancsali, S., Vuong, A.: Revealing the Learning in Learning Curves. In: Proceedings of
the 16th International Conference on Artificial Intelligence in Education, pp. 473-482
(2013)
Nixon, T., Fancsali, S., Ritter, S.: The Complex Dynamics of Aggregate Learning
Curves. In: Proceedings of the 6th International Conference on Educational Data Mining
(2013)

21

Exploring through Simulation the Effects of Peer
Impact on Learning
Stephanie Frost1 and Gord McCalla1
ARIES Lab, Dept. of Computer Science, U. of Saskatchewan, Saskatoon, Canada
stephanie.frost@usask.ca, mccalla@cs.usask.ca

Abstract. Simulation modelling helps designers to keep track of many
possible behaviours in a complex environment. Having a technique to
simulate the effect of peer impact on learning allows designers to test the
social effects of their educational software. We implement an agent-based
simulation model based on the ecological approach (EA) architecture [9].
The model considers learner attributes, learning object attributes and
two styles of peer impact to explore the effects when learners are either
positively or negatively impacted by high achieving peers. In this study,
we observe different patterns of behaviour based on the style of peer
impact and by limiting simulated learners’ access to information (the
EA metadata). Gaining understanding of these patterns will inform our
future work on recommending sequences of learning objects (LOs).
Keywords: simulated learning environments, simulated learners, ecological approach, instructional planning

1

Introduction

Before taking an action in a learning environment, it is important for an intelligent tutoring system (ITS) to have some way of estimating the likelihood that
the action will be successful, i.e. that it will benefit the learner(s) involved. To
compute such an estimate, there are many dimensions to consider such as: the
nature of the content being learned, the pedagogical style of the environment,
learning goals, individual learner characteristics, and social factors such as how a
learner’s own performance can be influenced by knowledge of peer performance.
Such complexity is often managed through the use of models.
Simulation modelling can be used by instructional developers for testing their
systems; this was identified by VanLehn, Ohlsson and Nason [11] in a survey of
possible uses of simulated students. One example is SimStudent by Matsuda et
al. [8] which can be used by designers to explore through simulation the effects of
various decisions on cognitive tutor design. Whether a model is used “internally”
(by an ITS to compute the next action) or “externally” (to evaluate a system design), a challenge remains: How does the model estimate the amount of learning
that occurs when a learner interacts with a Learning Object (LO)? In particular, we wanted to explore the impact on learning when learner performance is
influenced by the performance of peers. Some learners may become encouraged

22

when observing high peer achievement and perform even better than they would
have otherwise. Other learners might become discouraged in the same situation and perform even worse. Having a technique to simulate the effects of peer
performance would allow instructional developers to test social effects of their
designs. In this paper, we use simulation to explore the behaviours exhibited by
two different reactions to peer impact.
We describe our approach in Section 2, followed by the simulation study in
Section 3. It is possible to simulate many different kinds of educational software
in the ecological approach (EA) architecture [5], and then test the simulation
under various conditions to get insight into issues the designer is interested in.
Because our model is implemented in the EA architecture, our approach for modelling peer impact can be used across many different styles of learning systems.
The data to feed our simulation is synthetic, but could, itself, be modelled on
data extracted from actual learner behaviour [5]. We follow with a description
of ongoing research that uses simulation for testing and developing a method for
recommending sequences of LOs, and conclude with a discussion of our findings.

2

Model Structure

In another paper [5], we have argued that it is not necessary to model every
detail of the learning process, but that systems can be tested in a simulation
that captures only the most relevant characteristics for a given purpose. Therefore, we take an approach that lets an instructional developer choose different
dimensions – such as attributes of the learning objects, aspects of the pedagogical environment, attributes of the learner – and assign weights to each dimension
according to the priorities of the developer. This section describes the structure
of the simulation model so as to provide background for the experiment around
peer impact, described in Section 3.
The EA architecture [9] provides a way to record metadata about learner
interactions with LOs. As learners interact with LOs, any information that is
known about the learner at the time of the interaction can be saved as metadata
and associated with the LO. The EA assumes that each learner is represented by
a learner model that contains static attributes (characteristics) as well as other
data gathered as they interact with the LOs (episodic).
We developed an agent-based simulation model with very simple abstractions
of learners and LOs. Each learner agent has an attribute, aptitude-of-learner, a
number between (0,1), which we use to model the range of aptitudes (low to high)
different learners have for a given subject matter. In our model, this attribute is
assigned at the start of the simulation and does not change, but in future work we
plan to create more sophisticated simulations where this attribute is not static.
The simulated LOs have an attribute to represent difficulty level, which is also a
number between (0,1) where higher values represent more difficult material. The
simulated LOs are arranged into a random directed acyclic graph to represent
prerequisite relationships between the LOs.

23

The model execution revolves around an atomic action: the learner’s interaction with a LO. This action might occur hundreds or thousands of times during
a simulation run, thus creating a multitude of EA metadata from which measurements can be taken. In related work [5], we introduce the term evaluation
function to describe the function that computes the degree of success as result
of an interaction between a learner and a LO. We will use the term P[learned] to
describe the value that is generated by the evaluation function, i.e. the “probability that the learner learned the LO”, or the “system’s belief that the learner
knows the LO”. The P[learned] value is included as part of the EA metadata
that is associated with LOs after learners interact with them.
Our evaluation function is a weighted sum, where each term deals with a
dimension of learning to be considered. Each dimension of learning is calculated
with a mini function. For example, suppose LearnerA were a novice with aptitudeof-learner=0.1. Next, suppose LOX were a fairly easy LO, which implies a high
probability of success. We use a mini function, difficulty-of-LO, to translate the
LO difficulty attribute into a high probability value, giving difficulty-of-LO=0.8.
Suppose we also wish to take into account that the likelihood of the learner
learning the LO is higher if the learner has already viewed prerequisite LOs.
Prerequisite information is given in the LO attributes. Our simulation model
has a function for hasPrerequisites which searches through the EA metadata to
discover whether the learner has indeed viewed the prerequisites and returns
1.0 if the answer is yes and 0.0 otherwise. If we want these dimensions to have
approximately equal weights, then we can define the evaluation function below
and obtain P[learned] as follows:

(w)(aptitude-of-learner) + (w)(difficulty-of-LO) + (w)(hasPrerequisites)
= (0.33)(0.1) + (0.33)(0.8) + (0.34)(1.0) = 0.637

If, on the other hand, we wish to give the aptitude a higher weight, such
as 60%, then the new value could be (0.6)(0.1) + (0.2)(0.8) + (0.2)(1.0), or
0.42. As expected, giving greater weight to this learner’s low aptitude decreases
the P[learned] somewhat. More dimensions can be incorporated so long as the
weights sum to 1.0. The evaluation function, implemented as a weighted sum,
will provide an estimated likelihood the LO has been learned between (0,1),
making it easy to compare averages of such P[learned] values between simulation runs. However, we caution against comparing two simulation runs with
different evaluation functions (i.e. different weights or dimensions) because that
would be like comparing two numbers with different units of measure.
The independent variables in our experiment are the aptitude-of-learner values, the difficulty level values, the directed acyclic graph giving prerequisite relationships between LOs, as well as a dimension called peer-impact, which is
explained in the next section.

24

3

Experiment

Our experiment is intended to explore through simulation the effects of peer
impact on learning. We motivate the experiment by visiting literature around
how peers can impact each other’s scores.
Students are impacted by their peers even in their ordinary lives. A study
was performed by Hanushek et al. [6] to clarify the impacts of peer group characteristics on achievement in the context of family and school factors, race and
socio-economic status. Results suggested that students benefitted from higher
achieving schoolmates. In contrast, the American Academy of Pediatrics warned
that Facebook pages can make some children feel badly because they see themselves as being inferior to their peers [10]. This effect is due to the nature of
Facebook, where most users will censor their posts and only share the most positive information about themselves, skewing the view of reality. Along the same
lines, Daniel et al. [3] found in a study that learners will usually only participate
in online learning activities if they have trust in their peers or some degree of
self confidence.
Others have used simulations to study peer effects. Mao et al. [7] used a
simulation model to study the impact of social factors in a course where students
shared learning materials with each other. The output of Mao et al.’s model was
a comparison of the amount of sharing connected to status levels: gold, silver,
bronze, common. Populations fluctuated as users began at the common status
and gradually transitioned between levels. The paper concluded that simulation
models can be useful for developing and improving incentive mechanisms in
virtual communities. In a different study, Zhang et al. [12] studied the fluctuation
of a population of learners through various activities: registration, activation,
action and adaptation. The authors found that learners who participated the
most were also the ones most sensitive to changes in the community and had
the most fluctuations.
This research, and other research, shows that a learner’s score can be impacted by peer performance. We decided to explore this issue by creating a
notion of “peer impact”, where learners respond differently from one another
according to how well other learners are doing in mastering the LOs. This takes
the form of a new dimension in our evaluation function called peer-impact. Like
the other dimensions we discussed in Section 2 (aptitude-of-learner, difficulty-ofLO, hasPrerequisites), this is a function that produces a value between (0,1) to
represent a positive or negative impact on P[learned]. In our experiment, we use
the following Equation 1 to compute P[learned] each time a learner visits a LO.

.25(apt-of-learner) + .25(diff-of-LO) + .25(hasPrereq) + .25(peer-impact) (1)
We created two styles of peer impact called reinforcing and balancing which
refer to a comparison between an individual learner’s average P[learned] on
the LOs they have viewed so far, compared to the average P[learned] of all
learner agents, which we call “class average”. The information to compute these

25

P[learned] averages is obtained from the EA metadata. Each learner is given
one of these styles at the start of the simulation and it remains fixed. Future
work could explore more sophisticated learner agents where this attribute is not
static.
The reinforcing style means that the learner’s score is “attracted” to the class
average P[learned]. That is, when the class average is higher than their own, the
peer impact function for a reinforcing learner produces a value close to 1; thus
the learner will perform even better than they would have otherwise. This is a
positive feedback loop, because as the learner performs better so does the class
average thus further encouraging the learner to do better. If the class average is
lower than their own, then the peer-impact function gives a value close to zero;
thus the learner will do even worse than they would have otherwise.
Balancing is the opposite. In this case, a learner’s score is “repelled” from
the class average P[learned]. That is, when the class average is higher than the
individual’s average P[learned], then their score will be pulled down lower than
it would have been otherwise. This is a negative feedback loop because when
the class average is high, the learner’s average goes in the other direction. When
the class average is low, then the learner’s score will be boosted higher than it
would have otherwise. In Figure 1, we show the peer-impact function (the values
0.2 and 0.8 were chosen as thresholds to allow clear effects of the two types of
learner to emerge).
if currentLearner BALANCING
if class average is HIGHER than mine
set peerImpact == randomNumBetween(0.0,0.2)
if class average is LOWER than mine
set peerImpact == randomNumBetween(0.8,1.0)
if currentLearner REINFORCING
if class average is HIGHER than mine
set peerImpact == randomNumBetween(0.8,1.0)
if class average is LOWER than mine
set peerImpact == randomNumBetween(0.0,0.2)

Fig. 1. Function to generate peer-impact for a given learner at a given time in the
simulation

The dependent variable in our experiment is the P[learned] values generated
by the simulation; we gain insight into whether the peer impact has a positive or negative effect by observing the relative P[learned] values. We varied
this experiment under six conditions. We varied the proportions of balancing
and reinforcing styles: mostly balancing, mostly reinforcing, and fifty-fifty. For
instance, if the model is set to mostly balancing, when new learners are initialized, they have a high chance of being assigned the balancing personality and
a low chance of being assigned the reinforcing personality. These three propor-

26

tions were each run under two difficulty levels: one with mostly easy LOs and
high aptitude learners, and the other with mostly difficult LOs and low aptitude
learners. These six conditions were hand picked to be representative samples on
a curve of possible population mixes that should provide some insight about the
effect of these two kinds of personality on the learning environment. We ran each
of the six conditions 5 times because our model is stochastic; it produces slightly
different results each time even under the same starting conditions.
A typical result is shown in Figure 2 (fifty-fifty, high difficulty with low aptitude learners). Each line represents the average P[learned] of different portions
of the simulated learner population: the lightest thin line for all learners, black
thin line for the learners who were assigned the reinforcing personality, and the
dark grey thin line for the learners who were assigned the balancing personality.
Normally, our simulation model would be used to evaluate a particular instructional planning technique, but because this experiment is intended to illuminate
peer impact, the order in which LOs are consumed isn’t important. Therefore,
the simulated learners, of which there are 80, visited random LOs, of which there
are 100.

Fig. 2. Typical result

At the start of the simulations, the class average starts at zero. The balancing
simulated learners had higher scores in this state because this is the behaviour
defined in the evaluation function – that balancing learners do well when the class
average is lower than their individual average. The learning gradually increases
for both groups as the simulated learners visit more and more LOs. Although
the results seem low overall – P[learned] only reaching short of 0.3 – this is due
to the number of LOs (100) created in the simulation and the time it would
take for learners to visit them all. We ran the simulation again with only 30
LOs and observed the same patterns, but with a steeper slope; the average
P[learned] reached around 0.5. This raises interesting questions about whether
the amount of time required to learn a set of LOs should actually be represented
with a linear function. In reality, learners would get tired or lose interest or
change their learning goals. Future work could compare instructional plans with
learners having different levels of stamina.

27

The thick lines in Figures 2 and 3 represent subsets of the balancing and
reinforcing personalities whose behaviour we wish to discuss in this experiment.
Simulated learners do not have access to the actual class average, but compute the average based on what other simulated learners have allowed them to
perceive about their performance. Based on Daniel et al.’s [3] results that confident learners are more likely to share their success, simulated learners with high
P[learned] values shared their EA metadata, while those with lower P[learned]
values did not. This creates a suppression effect, where each simulated learner
has access to different information in the computation of how others are doing,
depending on which other learners have suppressed information at the time they
are computing the average.
The thick grey line shows only the balancing learners with low aptitudes while
the thick black line shows only the reinforcing learners with high aptitudes. At
the start of the simulation, the thick black line is below the thick grey line: it is
perhaps surprising that a group of simulated learners with high aptitudes would
have overall lower scores than a group of simulated learners with low aptitudes.
We highlight this because it shows that different parts of the evaluation function
– peer-impact, aptitude-of-learner etc. – can dominate at different times. In this
case, high aptitude can be dominated by peer impact for reinforcing personalities
when the class average is low.
In Figure 3, we observe another interesting phenomenon by injecting 80 more
simulated learners halfway though the experiment, a somewhat contrived situation, although one that might happen in the real world if, say, two classes merged
partway through a course, or if two study groups in an online course were mashed
together, or due to the openness of many online courses (e.g. MOOCs) when new
learners can join any time. Under most of the experimental conditions we tried,
such as the typical result in Figure 2, although the influx of new learners caused
the class average to drop (as expected, because each new learner starts with an
average P[learned] of zero), there was no apparent change in the relative ranking of the groups of learners being measured. That is, if the balancing learners
had the highest average before the influx, this continued afterward. However, in
about a third of the runs with low difficulty LOs and high aptitude learners, the
influx of learners caused a phase shift: now the thick black line jumps above the
thick grey line (see Figure 3). This makes sense: the balancing learners who tend
to do more poorly when the class average drops, do just that. The influx also
creates a situation where there are now learners with high averages intermingled with learners with zero averages; this creates a different environment than
the starting condition where everyone started at zero. Different environmental
conditions cause the model to exhibit different behaviour. With the suppression
effect deactivated, all learners have access to the same information. In this condition, we observed that the thick grey line overlapped with the thick black line
and there was no apparent phase shift (i.e. no lines crossing over).
Even though the observed patterns are merely a result of the evaluation
function implementation – that is, the model is simply doing what it was programmed to do – it helps system designers to keep track of the different possible

28

Fig. 3. Condition showing phase shift

behaviours as they try to design systems to support learning in all of these
conditions: low or high aptitude learners, easy or difficult material, peer effects,
prerequisites and many other possible dimensions, with each behaving differently
in different situations. Without simulation, it is unlikely we would have made
our observations about the phase shift as well as the observation about the high
aptitude reinforcing learners having lower scores than low aptitude balancing
learners. These observations reveal the specific circumstances that instructional
developers should address in order to maximize the expected learning. For example, the system could be programmed to intervene when it detects that the
current class average will push a learner’s expected outcome in an undesirable
direction. When the class average is higher than an individual’s average, the
scores of other learners should be displayed more prominently for the balancing
learners but not for the reinforcing learners.
Through this experiment, we have also shown that simulations can be used to
test unexpected situations. Future experiments could test for influxes of new LOs
instead of new learners. Other variations could look at adding or removing LOs to
impact the difficulty level of the course or the level of expertise of peer learners.
When we injected a herd of simulated learners, we observed some surprising
results. But, by examining the underlying dynamic behaviour as the simulation
proceeded, we could actually explain why these results happened, thus gaining
more intuition about learning that would help to better inform an experiment
that might be carried out with real learners.

4

Other Research Directions

In ongoing work, we are also developing a technique for recommending sequences
of LOs. Instructional planners have been built that explore different kinds of sequencing such as sequencing things of the same type, like “lessons” or even
sequencing several types of activities, like presentations and assessments [1].
Our method involves using the EA metadata to identify “trails” of LOs. We are
investigating the use of user-based and item-based approaches to generate recom-

29

mendations of these trails using Apache Mahout 1 . Using information captured
in the EA metadata, we create metrics for giving sequences a score to reflect the
quality of the sequence, for example does P[learned] increase or decrease over the
sequence. We are also exploring changes to the evaluation function to favour sequences that suggest coherence, such as trails that give learners a view of the big
picture before going into the details. Sequences with high scores are then used
as a basis for recommending sequences to other learners. Our study will examine
whether learners receiving sequence recommendations see any improvement over
learners receiving one LO recommendation at a time.
Other work in simulating recommender systems for learning systems has
been done by Drachsler et al. [4]; but the main difference is that this work did
not involve sequences, peer impact or the EA architecture. Champaign [2] uses
the ecological approach architecture to use the experiences of past learners to
suggest sequences of LOs for future learners while also studying the impact of
peer ratings, which are not the same as our peer impact because our peer impact
is linked to the evaluation function.
Even with the simplistic models of learners and LOs we have presented so
far, the peer impact experiment demonstrates the combinatorics of the various
features is already becoming too complex to rely on human intuition; this is one
of the main reasons for simulation modelling.

5

Conclusion

We created simulated learners whose overall learning was influenced by one
of two styles of peer impact. Our study demonstrated that different patterns
emerge when when simulated learners change their own behaviour based on the
behaviour of the group and when these learners have limited access to information due to others’ ability to suppress their EA metadata. In some conditions, a
phase shift occurred from the initial situation where the class average is zero to a
new situation with some learners having relatively high averages. The simulated
learners prior to the influx had higher averages because they had the opportunity to visit LOs before the arrival of the new simulated learners. One style of
peer impact is not universally better or worse than another, but each has advantages in different circumstances. It is important for instructional developers
to understand such patterns. In future work, the use of simulations with the EA
architecture will shed more light on peer impact and will allow us to also factor
in the effects of different kinds of sequence recommendations.
The EA metadata make it easy to look deeply into the underlying dynamics
and identify the conditions that create such behaviours. The EA metadata also
allow us to change the inputs of the simulation and take measurements, as we
did to compare the P[learned] averages between learners with different styles of
peer impact. By using the EA architecture for the simulation studies, the later
construction of a real learning system is made easier if the real system also uses
1

http://mahout.apache.org/

30

the EA architecture. That is, if the real system also stores information about a
learner’s interaction with a LO as metadata associated with the LO, then estimating the likelihood of success for a real learner follows the same methods used
by developers to estimate the success of simulated learners.
Acknowledgements
We would like to thank Dr. Julita Vassileva for discussions with the first author
about using simulations for instructional planning and for insights on applying
social concepts in the simulation model; Graham Erickson for his ideas about the
evaluation function; and Dr. Nathaniel Osgood for discussions about cause loop
diagrams and agent-based modelling in AnyLogic. We also wish to acknowledge
the Natural Sciences and Engineering Research Council of Canada for funding
some aspects of this research through a Discovery Grant to the last author.

References
[1] Brusilovsky, P. and Vassileva, J.: Course sequencing techniques for large-scale webbased education. International Journal of Continuing Engineering Education and
Lifelong Learning, 13, 75-94 (2003).
[2] Champaign, J.: Peer-based intelligent tutoring systems: a corpus-oriented approach.
Ph.D. Thesis, University of Waterloo, Waterloo, Canada (2012)
[3] Daniel, B., McCalla, G., Schwier, R.: Social Network Analysis techniques: implications for information and knowledge sharing in virtual learning communities. Int. J.
of Interactive Media in Education, 2(1), 20-34 (2008)
[4] Drachsler, H., Hummel, H., and Koper, R.: Using simulations to evaluate the effects
of recommender systems for learners in informal learning networks. In Learning, 3
CEUR Workshop Proc., 404-423 (2008).
[5] Erickson, G., Frost, S., Bateman, S., and McCalla, G.: Using the ecological approach
to create simulations of learning environments. To appear in Lane, H.C., Yacef, K.,
Graesser, A., Mostow, J. (eds.), Proc. 16th Int. Conf. on AIED, Memphis (2013)
[6] Hanushek, E., Kain, J., Markman, J., Rivkin, S.: Does peer ability affect student
achievement? Journal of Applied Economics, 18, 527-544 (2003)
[7] Mao, Y., Vassileva, J., and Grassmann, W.: A system dynamics approach to study
virtual communities. In Proc. 40th Annual Hawaii Int. Conf. on System Sciences,
HICSS ’07, pp.178a-, Washington, DC, USA, IEEE Computer Society (2007)
[8] Matsuda, N., Cohen, W.W., Sewall, J., Lacerda, G. and Koedinger, K.R.: Predicting
students performance with SimStudent that learns cognitive skills from observation.
In Luckin, R., Koedinger, K.R., and Greer, J. (eds.), Proc. 12th Int. Conf. on AIED,
Marina del Rey, 467-476 (2007)
[9] McCalla, G: The ecological approach to the design of e-learning environments:
purpose-based capture and use of information about learners. Journal of Interactive
Media in Education.
[10] Tanner, L: Docs warn about teens and ’Facebook depression’. Associated Press.
http://www.msnbc.msn.com/id/42298789/ns/health-mental health/t/docs-warnabout-teens-facebook-depression/ Accessed April 13, 2013.
[11] VanLehn, K., Ohlsson, S., and Nason, R.: Applications of simulated students: an
exploration. Int. J. Artificial Intelligence in Education, 5, 135-175 (1996)
[12] Zhang, Y., and Tanniru, M.: An agent-based approach to study virtual learning
communities. Hawaii International Conference on System Sciences, 1(11c) (2005)

31

Using HCI Task Modeling Techniques to Measure How
Deeply Students Model

Sylvie Girard, Lishan Zhang, Yoalli Hidalgo-Pontet, Kurt VanLehn,
Winslow Burleson, Maria Elena Chavez-Echeagary, Javier Gonzalez-Sanchez
Arizona State University, Computing, Informatics, and Decision Systems Engineering, Tempe,
AZ, 85281, U.S.A.

{sylvie.girard, lzhang90, yhidalgo, kurt.vanlehn, winslow.burleson, helenchavez, javiergs}@asu.edu

Abstract: User modeling in AIED has been extended in the past decades to
include affective and motivational aspects of learner’s interaction in intelligent
tutoring systems. An issue in such systems is researchers’ ability to understand
and detect students’ cognitive and meta-cognitive processes while they learn. In
order to study those factors, various detectors have been created that classify
episodes in log data as gaming, high/low effort on task, robust learning, etc.
When simulating students’ learning processes in an ITS, a question remains as
to how to create those detectors, and how reliable their simulation of the user’s
learning processes can be. In this article, we present our method for creating a
detector of shallow modeling practices within a meta-tutor instructional system.
The detector was defined using HCI (human-computer interaction) task modeling as well as a coding scheme defined by human coders from past users’
screen recordings of software use. The detector produced classifications of student behavior that were highly similar to classifications produced by human
coders with a kappa of .925.
Keywords: intelligent tutoring system, shallow learning, robust learning, human-computer interaction, task modeling

1

Introduction

Advances in student modeling in the past two decades enabled the detection of
various cognitive [3, 4, 8, 11, 13, 16, 17], meta-cognitive [1,6], and affective [2, 9]
processes during learning based on classification of episodes in log data. Steps have
been taken toward detecting when learning occurs [4] and to predict how much of the
acquired knowledge students can apply to other situations [5, 6]. However, an obstacle in such research is how to gain an understanding of the user’s cognitive or metacognitive processes while learning. While some of the indicators used in the literature

32

are common to any intelligent tutoring system, others are closely linked to the activities and pedagogical goals of a specific application. The adaptation of such indicators
to the design of a new system often necessitates a detailed analysis of the new domain
and how the tutoring system guides learners to acquire its skills and knowledge. In
particular, an issue within this process is the ability to reach common ground between
learner scientists that perform an analysis of learners (meta-)cognitive actions at a
high level - via video or log analysis of student’s past actions for example – and the
definition of the indicators by software engineers, related to how the system was implemented, that can be used to simulate such processes in agreement with the constraints and functionalities of software. We view the specificity of detectors as unavoidable, so the best solution is to develop good methods for analyzing the new tutoring system and designing the detectors. This short article describes our method
and its application to out project, AMT. In the AMT project, a choice was made to use
HCI (human computer interaction) task modeling - a method for formally representing human activity, and by extension, the behavior of an interactive system -, as well
as video coding schemes from human coders, to develop the detectors. The detectors
aim to evaluate student’s use of shallow and deep modeling practices with and without being guided by a meta-tutor, on the domain of dynamic systems modeling.
In Section 2, the AMT learning environment, for which the detectors were created,
is introduced. In a third section, the task model of the user’s activity in AMT is described. Next, the process of defining a coding scheme for the detector with human
coders is presented, followed by the definition of the different classifications that
define the value, the implementation and empirical evaluation of the detector. The
final section summarizes the uses of task modeling within this work, and how it could
be applied in future to other applications.

2

AMT software: a meta-tutor to teach deep modeling of
dynamic systems.

AMT software teaches students how to create and test a model of a dynamic system. In our modeling language, a model is a directed graph with one type of link, as
illustrated in Figure 1. Each node represents both a variable and the computation that
determines the variable’s value. There are three types of nodes.
• A fixed value node represents a constant value that is directly specified in the problem. A fixed value node has a diamond shape and never contains incoming links.
• An accumulator node accumulates the values of its inputs. That is, its current
value is the sum of its previous value plus or minus its inputs. An accumulator
node has a rectangular shape and always has at least one incoming link.
• A function node’s value is an algebraic function of its inputs. A function node has
a circular shape and at least one incoming link.

33

The students’ learning objective is to draw a model representing a situation that is
described in the form of a relatively short text. In the example of Figure 1, the description of the problem was “ Rust destroys steel and can spread quickly. Suppose
you take a large sheet of steel, such as one that might be used as the roof of the boxcar on a train, and you put it outside in the weather. Suppose it starts with a spot of
rust that is 10 square inches in area. However, each week the rust spot gets bigger, as
it grows by 30%. Therefore at the end of the first week, the rust spot is 13 square
inches in area.” and the objective of the problem was to “Graph the size of the rust
spot over 10 weeks.”

Fig. 1. The left image is the example of model, with gray callouts added to explain the
contents of nodes. The right image is the example of a node editor.

The student constructs the model node by node, by filling in all information within
each node in the form of four interactive tabs (description, plan, inputs, and calculations). During construction, students can use the Check button to evaluate the correctness of the current tab, or the Solve it for me button to ask the system to fill out the tab
automatically.
The instruction is divided into three phases: (1) an introduction phase where students learn basic concepts of dynamic system model construction and how to use the
interface; (2) a training phase where students are guided by a tutor and a meta-tutor to
create several models; and (3) a transfer phase where all scaffolding is removed from
soft-ware and students are free to model as they wish. The tutor gives feedback and
corrections on domain mistakes.
The meta-tutor requires students to follow a goal-reduction problem solving strategy, the Target Node Strategy [18]. The basic idea is to focus on one node at a time
(the target node) and completely define it before working on any other node. This
process decomposes the whole problem of modeling a system into a series of atomic
modeling problems, one per node. Like Pyrenees [2], it teaches students that if they
just master this one difficult but small skill, then the rest of the problem solving will
be straight-forward. In addition, the meta-tutor complains if students appear to be
guessing too much or giving up too early, just as the Help Tutor did [3].
While students learn, their motivation, attention to details, and modeling depth can
fluctuate. To assess students, the project needed detectors that detect shallow and
deep modeling practices both with and without the meta-tutor. The measure should be
usable in the transfer phase of the experiment as a dependent variable, because deep

34

modeling is the skill/knowledge that AMT teaches. The depth measure should also
apply to student’s behavior during the training phase so that we can check whether the
instructional manipulations done during that phase have their intended effects (i.e.,
the measure serves as a manipulation check). The detector should further operate in
real time (i.e., it doesn’t require to know future actions or states in order to interpret
the current action) so that it can be eventually be used by the system itself to condition its behavior.

3

Task Modeling: analysis of user’s actions on software

A task model is a formal representation of the user’s activity. It is represented by a
hierarchical task tree to express all sub-activity that enables the user to perform the
planned activity. The tasks need to be achieved in a specific order, defined in the task
tree by the ordering operators. In AMT, every modeling activity follows the same
procedure involving the same help features, task flow, and meta-tutor interventions.
With a single task model of a prototypical modeling task, it is therefore possible to
account for all of the user’s activity in software. Due to the complexity of the final
model, only one sub-activity will be described in this paper, illustrated in Figure 2.
Only part of the model is deployed in the figure, and some subtasks will not be detailed here. In this part of the model the sub-activity the learner wishes to perform is
to create a new node for the dynamic system s/he is currently modeling. We will first
describe the task tree, and then insert the iterations and conditions that enable a formal
verification of the flow of the task within the task model.
Figure 2: Sub-task “Creating a Node” in the AMT activity task model using K-MADe

35

Short description of the sub-task to model:
In order for a node to be created, the description tab of the node editor needs to be
completed by selecting a node description, which corresponds to a valid quantity in
the system to model. Each node is unique and cannot be created more than once. The
user can engage in the task only if at least one node still needs to be created for the
model to be complete.
Task tree and order of the tasks:
At the top level of the task tree “Creating a node”, the learner can either attempt to
create the node (task 1) or give up on the creation (task 2). The second task is represented in software by the user closing the node editor window, and can be done at any
time during the task. The task “Creating a node” is over when a good description has
been found and validated. The system can then try to initialize the selection and create
the node.
In the first level of the task “Attempting”, the learner first needs to select a node
description (task 1.1), i.e.: what quantity the node will represent. S/he is then allowed
to finish the creation of the node by validating the selection (task 1.2).
In order to select a node description, the user first needs to choose a node description (task 1.1.1) among the set of node descriptions offered by the system. This process involves the user choosing mentally one description (task 1.1.1.1), exploring the
help features offered by software (task 1.1.1.2) and exploring the set of node descriptions displayed (task 1.1.1.3). S/he can then select the node (task 1.1.2). This subtask
is not described in Figure 1 for a lack of space.
In order to validate the selection, the learner can choose to go back to the description of the problem to verify the correctness of his solution according to the problem
to be simulated (task 1.2.1), and then has to validate the selection (task 1.2.1.2). When
the user checks the validity of the selection, it can either be performed by checking
the solution against the set of nodes still remaining to be modeled (task 1.2.1.2.1) or
asking software to produce the solution (task 1.2.1.2.2). The user is allowed to ask for
the solution only when a description has been checked at least once.
Now that the different actions of the learner are defined, the iterations and conditions will help represent the flow of the activity on the subtask “Selecting a node description” (task 1.1).
Iterative and Optional tasks
• Task 1.1 is iterative: it is possible to make several selections before trying
to finish the description by validating.
• Task 1.1.1.2 is optional: The learner is not forced to explore the help features to choose a description, this is merely a choice on the learner’s part.
• The main task, “creating a node”, is iterative until the node is created or
the activity is abandoned. The later is represented in the task model by an
interruptible task: the learner can stop his/her creation of node activity any
time by choosing to close the node editor window.
Conditions on tasks:
• Main task 1 has a pre-condition attached to it: the software only allows the
user to engage in a creation of a new node if there is at least one node re-

36

lated to the modeling of the dynamic system that still remains to be created.
A first task model was created to represent learner’s activity on software without
the presence of the meta-tutor. This corresponds to the first version of software, which
was evaluated against the interface including the meta-tutor in [18]. This second software interface includes a text-based agent that intervenes as the students engage in
modeling to help them achieve deeper modeling behaviors, by applying constraints to
the user’s actions and giving meta-cognitive feedback. The meta-tutor was therefore
added to the task model under the type “system” and the model was completed to
include the constraints and interventions of the meta-tutor.
The final task model produced represented all possible actions of the learner on
software in order to model a dynamic system. Next, a study of these actions, which
led to the definition of the depth detectors, is detailed.

4

Detecting when students are modeling using shallow practices

The task model developed with K-MADe was used to define the episode structure.
The first step in creating a coding scheme is to define a unit of measurement for the
user’s modeling actions. The task model clearly highlighted the different subactivities the learner could engage in, referred to as goals. All goals are interruptible
tasks in favor to accessing the help features1 or abandoning the completion of the
current goal for a new one. After a brainstorming session where researchers studied
how students’ actions fell in line with those goals, the following unit of depth, called
“segment”, was defined. This established the unit of coding to be used in the next
phase.
Screen videos representing the learners’ use of the AMT software with and without
the meta-tutor were recorded during an experimental study described in [6]. These
videos were studied to determine how much shallow vs. deep modeling occurred and
the contexts, which tended to produce each type. A coding system was then created
for video recordings of the learners’ behavior. Three iterations of design for this coding scheme were performed, ending with a coding scheme that reached a multi-rater
pairwise kappa of .902. The final coding scheme mapped learners’ behavior to six
classifications, which were implemented as the following depth detectors[AIED short
paper]
• GOOD_METHOD: The students followed a deep method in their modeling. They used the help tools appropriately, including the one for planning
each part of the model.
• VERIFY_INFO: Before checking their step for correctness, students
looked back at the problem description, the information provided by the instruction slides, or the meta-tutor agent.
1

It is to be noted that two help systems are available to users: (1) referring back to the instructions always available for viewing, and (2) looking at the problem situation where all details
of the dynamic system to model are described.

37

• SINGLE_ANSWER: The student’s initial response for this step was correct, and the student did not change it.
• SEVERAL_ANSWERS: The student made more than one attempt at
completing the step. This includes guessing and gaming the system:
o The user guessed the answer, either by clicking on the correct answer by mistake or luck, or by entering a loop of click and guessing to find
the answer.
o The user “games the system” by using the immediate feedback
given to guess the answer: series of checks on wrong answers that help deduce the right answer.
• UNDO_GOOD_WORK: This action suggests a modeling misconception
on the students’ part. One example is when students try to run the model
when not all of the nodes are fully defined.
• GIVEUP: The student gave up on finding how to do a step and clicked on
the “give up” button.
Another detector was defined as a linear function of the six episode detectors. It
was intended to measure the overall depth of the students’ modeling, therefore providing an outcome measure in the transfer phase in future experimental studies. It considered two measures (GOOD_ANSWER, VERIFY_INFO) to indicate deep modeling, one measure (SINGLE_ANSWER) to be neutral, and three measures
(SEVERAL_ANSWERS, UNDO_GOOD_WORK, and GIVE_UP) to indicate shallow modeling.
Once the coding scheme reached a sufficient level of agreement between coders,
the task model was used to adapt the coding to students’ actions on the software. The
episodes that were coded for depth by human analysts in the sample video were analyzed by creating scenarios from the task model within K-MADe. The validation of
six detectors’ implementation involved three human coders, who watched a sample of
50 episodes, paying attention to the depth of modeling exhibited by the student’s actions, and chose the classification that best represented the depth of the learner modeling at the time of the detected value. A multi-rater and pairwise kappa was then performed, reaching a level of inter-reliance of .925.

5

The different uses of the Task Model

The task modeling language K-MAD and its task model creation and simulation
environment, K-MADe [7] were chosen for the following reasons: the environment
enables the creation and replay of scenarios of student’s actions, a set of functionalities not described here enable a formal verification of the model. Additionally the
associated simulation environment ProtoTask [14] allows non-specialists in task modeling to visualize the flow of the task model, via scenarios in a clear and simple manner.
The use of K-MAD helped in the creation of the detectors and are a first step in offering an alternative technique to simulated learners, by tackling the following problems:

38

Breaching the gap between learner scientists’ understanding of how the
learning process works and programmers’ definition of the application
flow, functionalities, and indicators.
• Enabling a formal validation of software flow, understandable by all.
• Using simulated learners scenarios to define the detectors.
A researcher in educational technology - expert in teaching modeling and part of the
AMT project - and an HCI practitioner, realized the task model. The former was an
expert on how AMT software was designed in terms of pedagogical content and task
flow. His expertise focused in particular on the actions the students were allowed/incited/forbidden to do within software at each moment of the modeling task.
The HCI practitioner was not familiar with intelligent tutoring systems or meta-tutors.
She was involved in the creation of the task model in a consulting capacity, in regards
to her expertise in task modeling of interactive systems.
The task model could be defined at the level of the user’s planning of actions and
system flow, with iterations and conditions alone. However, the objects in K-MADe
enable us to represent the constraints of the learner’s actions concretely and to apply a
formal verification of task flow. It was therefore possible to represent the set of descriptions as either valid or invalid, to detect when a node has been checked and the
result of that check, and to add constraints on the checking procedure such as to avoid
node duplication. This enabled a formal verification of software flow prior to validate
its fidelity to learner scientists’ ideas about possible actions on software and the underlying processes involved.
Once the model was constructed, the use of ProtoTask to visualize software flow
and follow learners’ possible sets of actions allowed by software enabled the ability to
simulate learners by creating scenarios of use that could be played and replayed at
will, focusing on the cognitive and meta-cognitive levels of learner’s experience on
software. In the process of creating our detectors, a video analysis of learner’s past
actions was performed. The model could be used to check the possible actions of
users with what the designer of the system wanted to offer as functionalities and software flow. During this analysis, the task model could be used once again to define
scenarios that simulated learner’s pertinent behaviors using ProtoTask. Once those
scenarios were formed, the task analyst came back to the original K-MAD modeling
language and studied the similarities and contrasts between scenarios to define the
rules that govern the detection of shallow and deep modeling practices within AMT.
Once the task model identified points of detection of such practices, it became easy
for programmers to go back to software and implement the rules.
•

6

Conclusion and Future Work

In this paper, a method to create a detector of deep modeling within a meta-tutor
using HCI task modeling and video coding schemes was described. The main outcome of this process was the creation of detectors inferring the depth of students’
modeling practices while they learn on a meta-tutoring system, reaching a multi-rater
and pairwise kappa score of .925. We believe the use of the task model to define shal-

39

low and deep modeling practices by helping to create the detectors to be of value for
any simulated learning environments, in particular for indicators that a common to all
learning tasks present in a tutoring system.
In interdisciplinary teams, the design of indicators can lead to communication issues due to misunderstandings and a lack of common ground between analysis made
at a high level of learners’ cognitive and meta-cognitive processes, and the representation of those behaviors within software. In particular, video-coding processes can
become costly when the coders’ understanding of the details of how the system works
differs from how the system actually works. Our experience using K-MADe and ProtoTask highlighted an ease in this project in gaining a better view of the tutoring system and the detection of deep modeling within the interface. In particular, the use of
ProtoTask by the non-specialists in task modeling helped clarify issues of task flow
and the definition of the set of user’s actions at each moment of interaction.
A limitation of the method is the applicability to different types of tutoring systems. In AMT, a single task model was able to represent the entirety of a users’ learning activity. In tutoring systems that teach a set of skills through different pedagogical
approaches for diverse types of learning tasks, the creation of such task models might
prove more costly and may not be completely adapted to the creation of detectors that
need to be adapted to each task specifically.

Acknowledgements
This material is based upon work supported by the National Science Foundation
under Grant No. 0910221. We would like to thank Sybille Caffiau for consulting in
the project and sharing her expertise in task modeling of interactive systems.

References
1. Aleven, V., McLaren, B.M., Roll, I., Koedinger, K.R.(2006): Toward meta-cognitive tutoring: A model of help seeking with a Cognitive Tutor. International Journal of Artificial Intelligence and Education 16, 101–128
2. Arroyo, I., and Woolf, B.P., 2005. Inferring learning and attitudes from a Bayesian Network of log file data. In Proceedings of the 2005 conference on Artificial Intelligence in
Education: Supporting Learning through Intelligent and Socially Informed Technology,
Chee-Kit Looi, Gord McCalla, Bert Bredeweg, and Joost Breuker (Eds.). IOS Press, Amsterdam, The Netherlands, The Netherlands, 33-40.
3. Baker, R. S. J. d., Corbett, A. T., Koedinger, K. R., Evenson, S., Roll, I., Wagner, A. Z., …
Beck, J. E. (2006). Adapting to when students game an intelligent tutoring system, Proceedings of the 8th international conference on Intelligent Tutoring Systems, Jhongli, Taiwan Berlin, Heidelberg.
4. Baker, R.S.J.d., Goldstein, A.B., Heffernan, N.T.: Detecting the Moment of Learning. In:
Aleven, V., Kay, J., Mostow, J. (eds.) ITS 2010. LNCS, vol. 6094, pp. 25–34. Springer,
Heidelberg (2010)
5. Baker, R. S. J. D., Gowda, S. M., & Corbett, A. T. (2011). Towards predicting future
transfer of learning, Proceedings of the 15th international conference on Artificial intelli-

40

6.

7.

8.

9.

10.

11.

12.

13.
14.

15.

16.

17.

18.

gence in education. Proceedings from AIED’11, Auckland, New Zealand Berlin, Heidelberg.
Baker, R. S. J. D., Gowda, S. M., Corbett, A. T., & Ocumpaugh, J. (2012). Towards automatically detecting whether student learning is shallow., Proceedings of the 11th international conference on Intelligent Tutoring Systems, Chania, Crete, Greece Berlin, Heidelberg.
Caffiau, S., Scapin, D., Girard, P., Baron, M., & Jambon, F. (2010). Increasing the expressive power of task analysis: Systematic comparison and empirical assessment of toolsupported
task
models.
Interacting
with
Computers,
22(6),
569–593.
doi:10.1016/j.intcom.2010.06.003
Corbett, A.T., MacLaren, B., Kauffman, L., Wagner, A., Jones, E.A.: Cognitive Tutor for
Genetics Problem Solving: Learning Gains and Student Modeling. Journal of Educational
Computing Research 42(2), 219–239 (2010)
D’Mello, S. K., Lehman, B., & Person, N. (2010). Monitoring affect states during effortful
problem solving activities. International Journal of Artificial Intelligence in Education,
20(4), 361–389., doi:10.3233/JAI-2010-012
Girard, S., Chavez-Echeagary, H., Gonzalez-Sanchez, J., Hildalgo-Pontet, Y., Zhang, L.,
Burleson, W., and VanLehn, K., (2013), Defining the behavior of an affective learning
companion in the affective meta-tutor project, in K. Yacef et al. (Eds.): Proceedings of the
16th international conference on Artificial Intelligence in EDucation (AIED’13), LNAI
7926, pp. 21--30. Springer-Verlag, Berlin, Heidelberg.
Gowda, S.M., Pardos, Z.A., and Baker, R. S. J. D. 2012. Content learning analysis using
the moment-by-moment learning detector. In Proceedings of the 11th international conference on Intelligent Tutoring Systems (ITS'12), Stefano A. Cerri, William J. Clancey, Giorgos Papadourakis, and Kitty Panourgia (Eds.). Springer-Verlag, Berlin, Heidelberg, 434443. DOI=10.1007/978-3-642-30950-2_56
Koedinger, K.R., Corbett, A.T., Perfetti, C. (2010): The Knowledge-Learning-Instruction
(KLI) Framework: Toward Bridging the Science-Practice Chasm to Enhance Robust Student Learning. Carnegie Mellon University Technical Report, June, 2010
Martin, J., VanLehn, K.: Student assessment using Bayesian nets. International Journal of
Human-Computer Studies 42, 575–591 (1995)
Lachaume, T., Girard, P., Guittet, L., & Fousse, A. (2012). ProtoTask, new task model
simulator. In M. Winckler, P. Forbrig, & R. Bernhaupt (Eds.), Human-Centered Software
Engineering (Vol. 7623, pp. 323– 330). Berlin, Heidelberg: Springer Berlin Heidelberg.
doi:10.1007/978-3-642-34347-6
Muldner, K., Burleson, W., Van, D. S., Brett, & Vanlehn, K. (2011). An analysis of students’ gaming behaviors in an intelligent tutoring system: predictors and impacts. User
Modeling and User-Adapted Interaction, April 2011, 21(1-2), 99–135m
doi:10.1007/s11257-010-9086-0
Shih, B., Koedinger, K.R., Scheines, R.: A response time model for bottom-out hints as
worked examples. In: Proceedings of the 1st International Conference on Educational Data
Mining, pp. 117–126 (2008)
Walonoski, J. A., & Heffernan, N. T. (2006). Prevention of off-task gaming behavior in intelligent tutoring systems Proceedings of the 8th international conference on Intelligent
Tutoring Systems. Jhongli, Taiwan Berlin, Heidelberg.
Zhang, L., Burleson, W., Chavez-Echeagary, H., Girard, S., Gonzalez-Sanchez, J., Hildalgo-Pontet, Y., and VanLehn, K., (2013), Evaluation of a meta-tutor for constructing
models of dynamic systems, in K. Yacef et al. (Eds.): AIED’13, LNAI 7926, pp. 666--669.
Springer-Verlag, Berlin, Heidelberg.

41

w  	
	
			
 	

	
¹

¹
  ¹ 
¹
 ¹  
!
"
#
$¹%
&' ¹  ¹¹
&¹(')¹'$*+,-
¹¹
¹

 	
 	


 	
	

È   ".#$'
'"./'¹'''$.
0
¹/' 

$'$
'/'$'#$'¹¹$

'&

&

$'0'$.
$
  ¹ ¹
$  ¹$¹ ".¹$ '' ' '$''  1$.  
  
¹
'
1..$.¹$
'$.¹$¹'$ 
$ ¹'.¹ $$
$2
1
#" ' 
$¹'¹¹$¹'$' ' $$.¹$¹
¹$
¹
 1.¹&$3
1¹$
¹$&'4
$.'¹$
¹$&''¹

1¹/ '¹ 0¹'/$' '#"".' '¹
.¹1¹/ $1
 .$' ¹
  /$'  ' ¹
  0 '' ¹$¹
$.¹
/$' '

¹$&$.¹
 .$'
'¹
¹$
 $.' 
1  ' ' ¹ &/ $ '' 
 ¹ ¹ 
&

$ '

'$ 
$ 5
1  & ' 
$ '&¹ 6 '¹$
 
&

$  
. '$ &/ 
1  '  #" ' ' 1$.¹'¹$1 ¹
'$ /

$'
¹'¹
 '&.1 .¹&'
$.

	)
&

&

$'
$ 
$ 
) ¹
2
1 '$¹$#$'
'"./






$ 
$ 

'&

&

$'¹
$¹
$''
$.6#7
  
'$¹
$¹'   
 8 9 :; ¹
  
$& "$' 8 "9 :; ¹ $.
$'$¹
 
¹¹.'
$.¹$¹$$  '¹¹'$
'$¹
$'¹''¹$ 
$ 
'
$. ¹
$.¹$    $. &¹$  
$$.' 
'$¹
$'¹
¹$ $ ¹¹$&
'$. ¹
 "
  ¹'$ 
$5
1
  $/ '$ 
$
$¹$
'$..¹$.
0¹ 2
1 "¹

 :; 1.. ' ¹'  
 ¹/'¹
  ' ¹
  '$¹$' $. ¹$/ $.¹$ ¹
'$ 
$.¹'¹
 ¹$¹
$. ¹
&
.'<.¹$
'
) ¹ 5
1     ¹' 
  / $. $.
0' '. ¹' $.#$
'
'"./8#"9:*;1..'
$.'$$¹
$'$¹$' ¹¹
$&5
1 ¹''''
$
$'$
'/'$'4'¹¹/
#"$
'&

&

$'$'¹

$
$1
'&

&

$¹
 

42

$'$
$.¹$'1¹5¹¹$.
$1
$.
$''&
¹
 
$.
$'$'$
1¹
 $/¹/#"$
 ¹5
1 #

$.''
'1 $.'$
¹.¹'¹ $ ¹.1.
 '¹
'$¹$'  $. '$
 ¹$.  $.  ¹
   ' ¹ $¹
'$
' $1
 '$¹$'
%'
$.¹$'
$¹$
¹.
  
 '$ ¹'¹0'$
¹
 ¹. 
¹'¹
$

$.0'$

4.¹
'¹'$ &¹
¹$¹$8'¹$¹$9 


$.'&
¹$.$.''$ 
$'=¹
1.'&
$
".'

''1 ¹ '¹'1$
$.#"

$'1..1
'  ¹
'
$. ¹5
1 >$.1¹'1¹
$$.'
 $  /
¹ $.¹$ ' $. '$
 ¹$. ¹
  $. 

  #" 

$'
.¹& $   ¹$  /
¹¹/ 1.
 
1 ' 1  ¹&¹¹ 6 
/ $.
¹.1
 ¹'''$ 
$¹$
''1.
¹'$ 
$$'
¹
 ¹$
 $.¹$ 
& 1¹' $  / ¹
$. '$ 
$ $ .¹' $  
   
 $.
¹.".'
1¹$
'.¹&$$¹5

$¹
$1.
$. ¹
5
1  ' ¹''''  $.¹$ ' $. ¹¹$
    $. #" .¹' $  

1.

1¹$
'¹
¹$ $$.¹.
".¹¹
/$1¹'#"¹'  '¹ 
$.1$./ ¹$
$.'$¹$ '$ 
$5
1 
$'$.'$ 
$=''
'? .$'¹
 
/$' '7.$' '
' ¹.'
'¹'$.$

$ 1.¹' /$'  ' 
'  ¹. '
' 
 & ¹/ ".'
$¹ $
¹#" ' 
$¹' ¹'$'3$  ¹
1$.$.'$/
¹¹$
''
 .$' '¹
$¹'
¹$&¹'1
 
$.'5
 
'¹
 $.1¹$
'1$.$$& 
8¹/¹$
'1 /
'$ 
$'9$/$'¹¹$
'¹/
 
  ¹
'1.$.
'$''¹$
''&/¹
#
 $3
1#" '$.¹$$$$1$..¹
'1.¹& 
& ¹'¹$

&

$
1..&$¹'$ 
$'81$.¹5
1
¹
¹ 5
1   $. ¹
9 '& &$¹ ' 8'¹$
 $. .¹&
¹ 
$$.5
1 9¹
 1.¹&¹ $.'$¹$ 5
1 
1$. $. ¹ 5
1  #
 $.' '
' 1 .¹& &  ¹ 
1 ¹/  #"
¹'  '¹ ß 	 
	1..¹.¹1¹/ .$'¹
 
/$' '
' 

$¹'''
''$¹''$$.



	
	
			

".#"'
$.'$'''¹
 1
  '$¹$'5
1 



$'$
'/'$':*;#"
'¹
  ''$ 
$¹
/¹
'
 ' ¹'$ 
$
' ¹  .¹¹$'$ &' $. ¹ '$.¹$ '$ 
$='
'$' 3¹
 /¹'$

'&¹¹$'8
'$¹
$.5
1
 &9
".¹¹$#" '¹' 
.1$. ' ¹$$.'$¹$ '$

$5
1 
$'.'<.'
'$./ :@;?

43

	 7.$'  '? ¹. '
' ' 
'   ¹' $. $  
$
A.
¹'$ 
$'$'¹
$

¹0'$
$'$.'<.'$¹$ 5
1 '
 ¹$ ¹ 
$1.$.$.$
'$ '$.$
$.¹$'$.
 ".'  ' 0 
/ ¹ .¹¹$'$ &  $ $.¹$ '
$' $.
¹$/$.¹$¹'$ 
$1$.¹$¹
5
1 &¹
'1'$$/".'
.¹¹$'$&'¹ $.¹¹$'$&8# 9
	 )/$' '?¹.'''
'.¹'$'1
.¹¹$'$&¹ 
¹$
 .¹¹$'$ & 84 9 :+; 1.. 3''' $. ¹$/ $.¹$ ¹
'$ 
$ '$ $.¹$ ¹
'1 :B; ". '$ 
$ '$¹$  5
1  '  ¹$  /
¹
'$.4 ¹$ $$.'$ '
'



8 .¹¹$'$&'¹
$
  .$'¹
 /$' '


)/$'  ' ¹  
¹$& $.¹
 .$' 
' '
 $./ $¹5

$¹
$¹.'''
'
 
 
$/
'$¹ 
' 
¹.¹
'1
¹'$
$
>  '.1' $. &'  ¹
 $  $. $. .$' ¹
  /$'
 '#"". .$' ''.1

>8¹9¹
 .¹'
/$.# 
1.. ' $. ¹$/  ¹
'1
 $/ $.' $ 8/¹3'9 &
 ¹ $¹

5
1  & 83¹3'9 ". /$'   ' '
$  
 > 89¹.
$..¹'$'1
.¹¹$'$&1..'$.¹$/.'
$.'
.8/¹3'9&
¹$¹
5
1 &83¹3'9
".'$¹'¹' 
$. .$'.¹¹$'$&'¹
$. '$  ' 1.. ' '$ 
$
' ".'  '    ¹'' 

' 
$.
¹¹$'$.¹$$.
$
.¹'6 
$$.'¹''
¹$
 $. ¹ * 5
 '  '$  '? )( )( ¹
  *)( 1$. 
 $1 ¹
 
$.¹¹$''$&/6
*)(# ¹
$

' 
 ¹'1'?


     	 
   




  

89

44

A.'¹¹¹$
$  $$$.&'¹$$.
¹&¹¹
$ '$. '
¹$
¹¹$'$. $/¹¹$
'$.''

¹¹$'$.$
¹
 
'$.5
1 &
".¹¹$/$' '#"#
151.¹&
'  $.
#"  '  / ".''
¹
  
$
 $. $':-; #
 $.'
  ' ' $. '&¹ ¹$' 8'$¹ .'9 $. ' ¹
$. 


'&¹¹
 ¹$
$¹$/¹ C
D8729$.¹$3'''$.¹
$/$.¹$¹'$ 
$ '
$5
1.1$¹
'1$.$¹.'&¹¹$/.¹'
¹$
$.¹$/72
  '
'$ 
$'1. 
$5
11'$¹

'&¹ ¹$/ ". ¹  ¹. '&¹ ¹$/ ' 3'  1 


'
$'$.'
'$$.$¹
 '$.¹$/'$ ?


    

  

    

	 



  


    

89



¹$/  ' $.

'&¹ ¹$/ 72 ¹
  ' $. $
  $.¹$ ¹$/

   
 ¹. '&¹ ¹$/  ". ¹¹$' 
$  /  $' $.
 ¹' 1¹' '
¹$
 $. ¹$'¹
  $.¹¹$' 
$  / 
$$.¹$&0
/$.'$
¹.¹$
¹$&

ß
 	

". ¹¹$
  #" $  '&
 
&

$' 0' ¹ /$'
  '
 ¹. '$ 
$¹$
 '.   $¹5
 
$¹
$E ¹ .$'  

/ 1   ¹ $ '$¹'.  ¹
 ¹$
 ' $  
$ 1& '
¹$
' .¹&$$& 
¹
 #"¹¹$
 
$¹'¹¹$¹$$'
3$ >$.¹$¹'
1.¹& & ¹
1¹/ '#"¹ 
0¹'/$' '#"1..¹
$..¹1¹/$1
 .$'¹
 
/$'
'
F¹'/$'  ' 
'  
$ ¹ .' ¹' 
 
 
$ $ 
/ $.'
$.¹$ .¹& 
. & 
 > 
'$¹
 $ ' 
'  ¹
 $ 1$.  .'
81.¹$''¹
'&

&

$1
 ¹'$ 
$¹$
'9G
$..¹&
'$ 
/ /'$ 
$'$./ 
$'
.&

$ ¹/$'¹¹$
#
'$¹  
$1
' $.'G.'
¹'¹'. 
$.
4 '8
.9$*8

 & ¹.'¹
 ¹
3$¹.$.¹$$.$.G9$.¹$.¹&¹$.
& 

.$ ¹
#"¹¹$

#
 $'$
 '/'$' $.    $' 1$. ¹ $  .' $ $ ' 
' 

1..$.
.'''
1.¹&$$¹5
$¹
$$.
$¹$
$1
$.'+
$'


45


86
$1$..'

ß


 	
	

#
 $&/0¹'/$' '1.¹& & ¹'¹$
1..
&$¹ '$ 
$'.¹& ¹¹ 89 5
1  ¹''
  ¹
  $./ .¹& $ '& '
&$¹'¹ 
$$.5
1 &4
$.'$ 
$''&$.
' '$.5
1 ''$¹$ /¹
'¹0¹'/$' 
#"¹
 $.
$.''$¹$
'¹¹ 1$.$.'$ 
$'=¹5
1 
ß

w 
 	


#
$.''¹$

&

$¹&$¹''
$ ¹'¹$

$'E¹.$'¹'$¹$'&
¹$.1$.¹$¹

''
$¹
'$
'$$.'$¹$'".'$¹
'$
'¹$..'$.$
A. '$ 
$' ¹ '&
 ' 
1 '$ 
$'= ¹$
'   ¹¹ 
 $.
'/'$ ¹
  $./ 
  $  
  
 $.   > $.¹$¹'
 &$¹ '
¹
$'$¹$
$$'$$./¹
.¹
 
'$ 
$'=
$¹$
'
6$ $. 


 ¹.  .¹' 
/ $.  ¹ '$
 ¹$. $.¹$ ' $.'

 ' $.¹$ ¹ ¹$  $.  ¹ '$
 ".' 
 ' ¹    / .$'
$'¹ 
$$.0¹$
6.¹¹$'$&$.# '
.$ 
$.' 5
   $' 
$ 
$' 1.  
$ ¹
'1 $/ $. $   1$. '
¹$/ ¹5 ¹ 
1 ¹$
 ¹$ $.' '$   '&
 ".
 $. '$
&8# 9'¹
. 
$$1&'.¹

$.
¹ .$'$
$ ¹ /$' 
 6 
 $ $.  3¹
¹$
 '$ 
$'= ¹$
'  
&5$.¹ $

1
 '$$.¹.



8ß6 $

1&'$¹
$

46

>*'.1'.1
1&'¹¹  $¹
$>'$/1.¹&$.'$
#  & 1.. 3''' $. ¹$/ $.¹$ ¹ '$ 
$ &
 .'<. 5
1 
&1¹
'1$.$
$/".''&¹5 ¹'
$. &

'¹
. 
$&'
¹
 1.
$.¹$
'
$ /&'¹  $
$. E
¹/&'¹
. ¹¹
¹
 $.
&'
¹
 ¹¹  $$.
$ ". $ 1¹' ¹$ $. 


 ¹ .$' $ 1$. $1 '' '

''E$.
¹
1¹$
1¹'
  
$¹
 ¹'¹
'0
$.1
&
1¹' 
&$ 
 $.$1 &' 6¹
 ¹
1¹$
 1¹' ¹  $$. ¹
 
$.$.¹
'$¹@.'$
ß

w 	

6 &$¹ '$ 
$ ' ¹
 
$$/ $.¹$ .¹' ¹ ¹ 89 5
1  ¹''¹$  ¹
  '
¹$'&'¹ 
$$". ¹'$.¹$ 
 

$'5
1 
&$.'$ 
$ 
$..$.¹./$
¹
¹$

$.>$.$./ ¹'.
1¹$
'1$.¹$¹
¹$/
6'$ 
$1'$¹
¹$
$.'$¹$¹ 
$.'<.5
1 &¹
 
$. .¹¹$'$ & '
 $.' &' ¹ ¹'$ 
$
'  $. 5
1
 &>@'.1'¹
3¹¹&$¹'$ 
$'$
¹.
¹@
.$".¹$'$ 
$1'$¹..1$.¹¹$/*GG
¹
 @'$&/A.
¹

$¹$
''$ $.1¹¹
$/ $.¹$ $. '$ 
$ 1 ¹'. ¹ 
1 ¹$
 ¹
  $. $.' & 1 
¹
. 
$$.$1


 ¹ 5
1   &$¹ '$ 
$' ' ¹''
   '¹$
 1 ¹

.' 1.¹$ ¹$
 '$$
 1 1¹
$ 
 ¹. 3
$ #$ ' ¹
 $¹
$
¹$$.'¹$
'
1¹
'$ /$.¹$ 
$5
 ¹$


 $&¹ ¹$
1 '
¹¹''
/
'$.


8$¹'$ 
$'$
¹.

47




	
	 

#
 $&/ 
¹'¹$ 
&

$1.¹&
 $ '
3
$' 1$. &$¹ '$ 
$' ¹
  &$¹ ' ". 3
$' .¹& 

¹'.  $ &/ $.   1$. 
$ ¹$
 '$$
' ¹' 1 ¹'

$ '#"¹
  
$
'



	
		


".3
$'¹'. 1$.'¹$1¹$¹''
 /0¹'/$' '#"
 
$'$¹$
'¹.3
$.¹'

 ¹'.  * $' 
   $   $. ¹$  ¹
¹' ¹$¹ ".

&$¹'$ 
$'
¹.3
$1¹'
#
 $$
 
'$'1.¹&
$ 
$.¹¹$
#".¹'

'$¹ $.¹$1.¹&'$¹$ 5
1 &1$.$.5
1
&'¹. 
)/$' ' 
$¹'¹¹$¹'3$ 
 
$.¹¹$

'$¹'
'$.' .¹&
$
.& 
$¹¹$ 
/ ". '$ 
$ '$¹$  5
1  & ' ¹¹$  '
 $. ¹  $.
0¹$
 * $. ¹$/  .¹&
 ¹ ' 5
1  & &
 $. '$' 
1 /$.'$ 
$'&
$.'¹¹$ $/
$.¹$/
'$
¹.'$&
$.5
1 &


   !

"! # ! $ 

 %$& 





8*9


#
3
$'&$¹'$ 
$'.¹&'& $1'1$.$'A
.¹&¹'. ¹
/$.3
$'&¹/
$.¹$
 '$$
$.
¹$/
¹$

1¹$
'¹
 $.$ $/'$&/#
3

$' 1 .¹& ¹  ¹¹/  5
1  & '$¹$
 $¹
  / ¹
0¹'/$' 1$.$.'$¹
 //$'¹
  .$' '
1$.$.'¹ ¹$¹
".¹¹/5
1 &'$¹$
1¹'¹¹$ '
$.
3$¹
1. 
 '$.¹ 5
1   '$. '$¹$  5
1  ¹
   $.
 
'$ 
$'?


 ! ' (  '  '

( +
,
- )*) 

.



8@9

A .¹& .'
  $/'  ¹$
 '$$
 $ ¹'. $. 3

$>'$¹
¹ '$$
¹$
1¹''$ E$.¹$/.¹&

¹5
1 & 1¹'
$

$. 5
1 ¹
'
$.¹$¹

1¹':**;E$. '$$
1¹'
$ 
".'
 ¹$
1¹'¹



1..$.¹$/.¹&
¹5
1 &'0¹/ '$$ ".
$.$1¹$
'1¹1&¹
 ¹..&¹$
1..1

¹ '$$
' 
$  
 '¹ ¹
 .. &¹''$&/ > + '.1' $.

48

5
1 & '$$
'' 
3
$'>$$.$$./¹$.

¹$.
$.1&¹
 $...& '$$
'$&/



8
$ 
$'=5
1 & '$$
'' 
$.3
$'

".¹$$.
¹$

1¹$
'1¹'¹''$  
3
$'
"$.'
 1.¹&
 $ 3
$'&¹/
$.
$¹¹ 
¹
1
¹$
 &
 $ &¹'  H +H H¹
  +H  $.¹
+H  ¹ 

1
¹$
' ¹ $¹&/¹
&'
$. 
>
¹/1.¹&¹'
'  $. $/$.$'A.¹&
 $ ¹

3
$ .¹

 $. &¹  $.' ¹¹$ ". $/ &¹ ' ¹¹$ 
¹ 
$¹
¹ '$$

$ 
¹$¹
&¹1..'$. $/
¹&¹A.¹& 
3
$'1$.&¹'¹
 '$&/



	
		

3
$'
 $ ''$$.¹$/$' ' #"¹
¹$/$.¹
 .$'
'".''$'
$''
'
/$' 
'¹
¹$&3
$'¹''.1$.¹$0¹'/$' '#"
¹ 
$ ¹' ¹¹$ ¹' /$' 
' $  ¹¹$ $.¹
 .$' 
'
' ' '$' $¹
  / 0¹'/$'  ' ¹ &/ '¹ $ $.' 
$¹
 //$'
'
"¹  '.1' '$' $¹
  1$. 
$ ¹$
 '$$
#
¹ ¹''
¹¹/ $¹
  / /$'  ' ¹ $$ $.¹
 $¹
  / .$'
¹
 0¹'/$' '".' 
'¹..
$.
¹ '$$

¹
 11$.¹1&¹$



	6¹/#" '.¹

$.¹$
 '$$


 


! 





"
 
@+*G
-,+-B*,
B-@*+
#$

*G@@**
BG@@+
+*,--+-
%&		
,,G
@GB-@
-*+G*
'&		
*-G@-*G
GB-BG
GG+-B

A¹
'
$.$¹$.¹$ 
'$1
'$'$¹
 /0¹'
/$' '¹
 /$'
'¹
$'
¹
$
4'
 3
$
 $ '$  .1¹$'$.¹$/¹ 
¹

1¹$
"$.'
 1.¹
 $.
$¹¹ 
¹
1¹$
H

49

$ +H "¹  '.1' $.' '$' 1.
 1 
¹' $. ¹$/  ¹ 
 ¹

1 ¹$
 $. ¹¹/  .$' ¹
  0¹'/$'  ' $' 1'
'
 $ $.¹¹/  $. /$'  ' $' $$ '
 $./ $¹
 
'
¹$



	6¹/#" '.¹

$.
$¹¹ 
¹
1¹$


(	 


! 





)(
*,*G
BB,GG,
BBB@B
(
*@G,,+B
BB,++-B
+G@+,-
)(
*B*@B@
B--GG
++,****
(
*B-@+GG
-B,G
+*-,@B

>
¹/1.¹&¹ '$'$¹
 /$.' '&¹/
$8'&

¹$.'$9 $/¹&¹".'$ 
$5
1 & ' 
3
$'

$.¹
 :* *; A .¹& 
'  ¹&¹ $ $/   ¹
   6

$1$. $/ 1¹
'1 $//¹'$ 
$1$.5
1 &
1$.¹¹$/+"¹*'.1'$.'$'$.'3
$$. .
$' '.¹&¹$$.¹&1.
$. $/'1.1&0¹'
/$' ¹
  /$'  ' $ $$ '$' 1.
 $. $/ 
¹''
)/$' ¹
  0¹'/$'  ' ¹¹$ $$ 1.
$. $ $/ '
.. '
 $¹1' $.   $ .¹&  &' $ '$¹$ $. '$ 
$ 5
1
 &


	ß6¹/#" '.¹

$.$ $/¹&¹

$$
	 ¶&
	 ¶)
	 ¶







*B@*
**G
++B-*+

! 


,
+@G-
,G




++G@@G
@@++G,
,,*



#
 $.' ¹ 1 .¹& &¹ ¹$  ¹ 
1 ¹¹. $.¹$ '' $. #$ '
'
"./¹1
  $./
¹/'  ¹¹$&5
1 '$¹$


$'$
 '/'$' $ 
  ¹ '5' 
  '&
 
&

$' " 
$.¹$1.¹& & ¹
1 #"$.0¹'/$' ".' 
' .¹1¹/ $1
 .$' ¹
  /$'  ' 
  
¹$&
$.¹
 .$' '¹
 
 
''¹
$ ¹$¹$.¹
/$'
'
".'&¹$
  $/¹'. 
¹¹
&

$'
1

 $5
1$.5
1 &'$ 
$'$¹'$.'$¹$
¹¹
/".'5
1 &.1&'¹¹$
$$¹$$.¹$'
$'&¹#
¹ $


50

1
  ¹
$ 
&

$1.$.'$ 
$'=¹
1¹'
$¹' 
/ 3$
¹ ¹$' > ¹ $.' ¹'
' 1 .¹& &  ¹ '¹$
 
&


$
%'
¹'¹$

&

$1¹
.'$.
¹$$.¹$


 $ '$ / .1 1 $.   ' 
 
$ '$¹$
' ¹
  1$. 
$
'$ 
$'='¹'A¹'¹
  $. $/$.'$'$.8
$'9?  ¹
/ ¹¹$
 $. '¹$ ' ¹ $   1.¹$ $' ¹ 
$¹
 1.¹$¹¹'
4$.¹ &¹
$¹'
¹'¹$

&

$'$.¹$1¹
¹$¹.3

$ 
   $   $. ¹$  ¹
¹' ¹$¹ >$. 1 ¹

'
$$.'$¹¹
'$ 
$'1..  $
¹¹

&

$
¹ 
$.0¹'/$' #"3
$''.1$.¹$$'¹
¹$
''
'&

&

$8' '¹
/5
  ¹$¹'5
¹
  
 ¹¹$& ¹
'  

 ¹¹$& 5
1 9 '¹/  1
15 1$.  
  ¹
' 
 1.. $. ¹
$  '' 
1 ¹$
' ' &/
¹F¹'/$' '#"'¹'$¹$
'¹'/$' 
'$
 '' ¹$¹' '0¹'/$' '#"¹
¹$&
$.¹
 .$'
''
$./$ ¹$¹$¹
 
$'
''

È*		
	?".'15'¹$776(4!$1..'
¹
 /
$. 6
 ¹'¹
 
¹ 
'$/  

 #

&¹$
 ¹
  
$' 8),"# 
++9

+

	$			

	 $&6¹$

¹1¹)?#
$
$$$'¹?".
'$¹
$¹' ¹
¹.##
$
$
/'$'8-9*G@+
	 6
 '
 I $$ 6" 2 
 2 )$ ? 
$& "$'? (''
'
¹
 ".!
¹$.(¹



'@8,,+9B--
*	 $'

'
)?#$'
'$./'/.'$'(¹1
¹
¹.1¹.89
@	 
 
! ¹ I()?6 ¹$&$'$
.¹.¹'$ 
$ 
'%' %'6 ¹$#
$¹$-8-9,+-
+	 7 76/¹¹I2.A? $ ¹ ¹$&$'$
1$./$'
$'6 )'/.¹¹'
$,8,,+9+
B	 
 
!?6 '$ 
$5
1  ¹
''$..¹ ¹$&$'$

 #
? #
 ) 
'  -$. #
$
¹$
¹ 

 #
$
$ "$
 
/'$'
#"
@¹

¹8@9
-	 ".''
7
$
(?6'
' $.$')'/.$5¹@,
8,G@9++,


51

Toward a reflective SimStudent: Using
experience to avoid generalization errors
Christopher J. MacLellan, Noboru Matsuda, and Kenneth R. Koedinger
Human-Computer Interaction Institute
Carnegie Mellon University
Pittsburgh PA 15213, USA
cmaclell@cs.cmu.edu, mazda@cs.cmu.edu, and koedinger@cmu.edu

Abstract. Simulated learner systems are used for many purposes ranging from computational models of learning to teachable agents. To support these varying applications, some simulated learner systems have
relied heavily on machine learning to achieve the necessary generality.
However, these efforts have resulted in simulated learners that sometimes
make generalization errors that the humans they model never make. In
this paper, we discuss an approach to reducing these kinds of generalization errors by having the simulated learner system reflect before acting.
During these reflections, the system uses background knowledge to recognize implausible actions as incorrect without having to receive external
feedback. The result of this metacognitive approach is a system that
avoids implausible errors and requires less instruction. We discuss this
approach in the context of SimStudent, a computational model of human
learning that acquires a production rule model from demonstrations.
Keywords: simulated learners, metacognition, cognitive modeling, representation learning, grammar induction, generalization error

1

Introduction

Simulated learning systems can be used for a wide range of tasks, such as modeling how humans learn, as teachable agents, and as a means to automate the
construction of models that can be used in cognitive tutors. In an effort to reduce the amount of developer effort needed to deploy simulated learners for these
tasks, researchers have been relying increasingly on the use of machine learning
algorithms. However, by increasing the generality of these systems through machine learning approaches, these systems become more susceptible to making
unrealistic generalization errors.
When using simulated learners to model human learning, we desire systems
that predict student’s errors as well as their correct behavior. Unrealistic generalization errors, in the context of these systems, are errors that the system predicts
humans will make, but that they never actually make. If a system is prone to
making these kinds of errors, then it becomes difficult to draw conclusions from
the predictions the simulated learners makes for novel tasks.

52

These generalization errors also complicate the use of simulated learners as
teachable agents because they result in a system that produces non-human behavior. When human students are teaching a simulated learner in a peer-tutoring
scenario and it makes errors that humans never make, then it decreases the authenticity of the experience. This inauthenticity might effect the social dynamics
of the learning-by-teaching scenario possibly making the teachable agent less effective.
Finally, generalization errors also have negative effects when using simulated
learners to automatically build cognitive tutors. For this purpose, simulated
learners have been used to author production rule models via interactive demonstrations of the solutions to the problems the system will tutor. This approach
may decrease the amount of work required to build a cognitive tutor and allow
subject-matter experts to author tutors directly, without an AI developer. In
this paradigm, SimStudent’s errors are useful to the extent that they correspond
with typical student errors; in these cases, the resulting production rules can
be added to the tutor’s bug library. However, if the errors are unrealistic, the
author must waste time identifying and deleting these nonsensical production
rules.
In this paper, we propose an approach that uses background knowledge to
mitigate unrealistic generalization errors with no changes to the underlying algorithms and which should increase the effectiveness of the underlying learning
mechanisms. Before presenting this approach in section 4, we first review SimStudent, the simulated learning system that provides the context for this work
(section 2) and introduce a motivating example of a nonsensical generalization
error SimStudent currently makes (section 3). After presenting this approach,
we present some initial results and discuss conclusions and future work.

2

The SimStudent Architecture

The simulated learner system that we focus on in this paper is SimStudent, a
system that induces production rule models from demonstration and problem
solving. The SimStudent system is used primarily for three tasks: to model and
predict human learning, to author cognitive tutors, and to function as a teachable
peer-agent.
In order to understand how SimStudent works and the situations in which
it makes unrealistic generalization errors, we will review the types of knowledge used by SimStudent, how this knowledge is represented, and the learning
mechanisms SimStudent uses to acquire this knowledge from experience.
2.1

Knowledge and Representation

There are three kinds of knowledge in SimStudent: primitive operator function
knowledge, conceptual knowledge, and procedural knowledge. The first kind of
knowledge is hand-constructed and consists of the low-level functions for manipulating data available to the system (i.e., adding two values, appending two

53

strings together, etc.). One example of a low-level function is SkillAdd, which
accepts two arguments, each of type arithmetic expression, and returns the sum
of these two expressions as a single arithmetic expression. These functions constitute SimStudent’s background knowledge. Depending on the task SimStudent
is being used for, different kinds of background knowledge may be appropriate.

Head
Expression
Expression
Variable
Minus
Number
Number

←
←
←
←
←
...
←

Body
Number Variable
Minus Variable
x
0

Prob
0.95
0.05
1.0
1.0
0.1

9

0.1

Fig. 1. A simple probabilistic context-free grammar and example parses of two expressions using this grammar.

The second kind of knowledge is conceptual, or representational, knowledge,
which is encoded as a probabilistic context-free grammar. It is automatically
acquired by SimStudent and is used to interpret the interface and information
in it. Figure 1 shows a simple example of the conceptual knowledge SimStudent
might possess about expressions for an algebra domain. This knowledge enables
SimStudent to automatically extract plausible “chunks” from the input, such as
the coefficient or term in an equation, which can subsequently be manipulated
by primitive operator functions or procedural rules. Furthermore, this knowledge
can be used to determine the likelihood that a given example was produced by
the grammar.

If (current-row ’output-cell ’row)
then (write-text ’output-cell
(cell-in-row ’row 1 ’left-side)
→ (append “divide” ’coefficient)).
(is-left-child-of ’left-side ’coefficient)
Fig. 2. An example production rule for division.

The final kind of knowledge is procedural knowledge, which represents the
skills that we desire students to learn. This knowledge is encoded as production
rules, which contain conditions under which the rules apply and what to do
under those conditions. Figure 2 shows an example of a production rule signifying
that when the left side of the equation’s parse tree has a left child (here called
coefficient), then enter “divide <the coefficient>” into the output cell.

54

2.2

Learning Mechanisms

Fig. 3. A diagram of the SimStudent learning mechanisms and how they interact.

Of the three kinds of knowledge manipulated by the SimStudent system, two
are learned automatically: the conceptual and procedural knowledge. To acquire
these two kinds of knowledge the system employs four learning mechanisms: what
learning, where learning, when learning, and how learning. The what learning
is used to acquire the conceptual knowledge whereas the where, when, and how
learning are used to acquire the procedural knowledge. Figure 3 shows how these
four learning mechanisms interact. Before SimStudent is used, the what learning
is run to acquire the conceptual knowledge. When SimStudent encounters a
situation where it does not know how to act, which is common initially, it requests
a demonstration from the author (the tutor developer or student tutor). This
demonstration is comprised of four parts:
• Focus of attention: the set of relevant interface elements (e.g., the left and
right hand sides of an equation);
• Selection: the interface element to manipulate (e.g., the output cell);
• Action: the action taken in the selection (e.g., update the text value); and,
• Input: the argument to the action (e.g, the text string used to update the
selection).
Every time the system sees a new demonstration or gets corrective feedback on
its performance, it learns or modifies a production rule. Production rule learning
is done in three parts: 1) how learning attempts to explain the demonstration
and produce the shortest sequence of primitive operator functions that replicates
the demonstrated steps and ones like it, 2) where learning identifies a generalized
path to relevant elements in the tutor interface that can be used as arguments
to the function sequence, and 3) when learning identifies the conditions under
which the learned production rule produces correct actions. We will now review
each of these learning mechanisms.

55

What This mechanism operates off-line to acquire a probabilistic context-free
grammar from only positive examples. This task can be defined as:
• Given: a set of examples of correct input;
• Find: a probabilistic context-free grammar with the maximal likelihood of
producing the examples.
This task is performed using a grammar induction approach outlined by Li et
al. [1], which uses a greedy approach to hypothesize the grammar structure and
Expectation Maximization to estimate the grammar parameters.
Whenever a demonstration is given to SimStudent, it augments the provided
information with the most likely parse trees of the content of each element in
the focus of attention. This additional information is used by SimStudent in
the subsequent learning mechanisms to extract deep feature knowledge from the
content (e.g., to recognize and extract the coefficient of a term in an equation).
The parse trees make this deep feature information directly accessible to SimStudent through the nodes in the parse tree (e.g., the left child of the parse tree
for “3x” in Figure 1 corresponds to the coefficient).
How This is the first of three mechanisms executed in response to a demonstration. The how learning task can be defined as:
• Given: a set of demonstrations consisting of the state of the relevant interface elements and the parse trees of the contents of these elements as well
as the resulting input for each state;
• Find: a sequence of primitive functions that when applied to each state
produces the corresponding input.
This task is performed by exhaustively applying the primitive operator functions
over all nodes in the focus of attention parse trees until the input is produced.
The iterative-deepening depth-first search strategy is used to find the shortest
sequence of functions that explains the data [1]. If no sequence exists, then a
special functions is created that takes the states and produces the corresponding
inputs.
Where This learning mechanism identifies the path to the relevant tutor interface elements. The tutor interface elements are specified by a hierarchical tree
structure (a table is comprised of rows which each contain cells). During interactive instruction, the relevant interface elements are specified by the author
teaching SimStudent. For each relevant element, SimStudent generates a parse
tree for the contents. The relevant portions of these parse trees are defined as
those that are utilized by the operator function sequence acquired through the
how learning. The task of learning a general path to this relevant information
can be defined as:
• Given: a hierarchical representations of the interface elements and their
parse trees, the function sequence from the how learner, and a set of elements
that have been identified as relevant;

56

• Find: a list of paths through the representation hierarchy to all of the relevant elements and the relevant portions of their parse trees.
The SimStudent approach to this task is to conduct specific-to-general learning
over the set of relevant interface elements and parse trees [1]. Returning to the
table examples, if the first cell in the first row of the table is always relevant,
then a path to that specific cell will be returned. However, if all of the elements
in the row are specified as relevant, then the entire row will be returned. After
the location to the relevant elements has been identified, the system utilizes
the function sequence to identify the relevant portions of the parse trees for
each element. This same specific-to-general learning is then conducted over these
relevant parse trees (within each element).
When This final mechanism identifies the conditions when the learned production rule is applicable. This task is defined as:
• Given: a set of positive and negative examples, each consisting of a set of
features and their associated label;
• Find: a set of conditions over the features that separate the positive and
negative examples.
As specified, this is a supervised learning task. The features used by SimStudent
to represent each example are predicates that are automatically generated from
the relevant portions of the parse trees. For example, there exists an “is-leftchild-of” predicate, which says that a particular argument is the left child of a
given node in one of the parse trees. This type of feature enables the retrieval
of equations, terms, coefficients, and variables. Given the feature descriptions of
each example, the positive and negative labels come from the user instructing
the SimStudent system. The first positive example is the initial demonstration.
Subsequent examples are generated when SimStudent tries to use the learned
rules to solve novel problems and receives yes/no feedback from the author.
To derive the set of conditions given the examples, SimStudent uses the FOIL
algorithm [2], which uses information theory to perform a general-to-specific
exploration of the space of hypothetical conditions.
These four learning mechanisms result in a simulated learning system that
accepts user demonstrations and feedback and automatically acquires probabilistic context-free grammar rules and production rules. The system requires little
background knowledge; for each task only the primitive functions need to be
defined by the developer. However, the cost of this generality is a system that
sometimes makes unrealistic generalization errors.

3

An example of an unrealistic generalization error

To explore the types of generalization errors that SimStudent makes, we turn
to the algebra domain. One of the skills that students learn in this domain is
how to proceed when given a problem of the form < Symbol >< V ariable >=<

57

Symbol > (e.g., 3x = 6). The skill that we desire the student to learn in this
situation is to specify that their next step is to divide both sides by the coefficient
of the term on the left side of the equation (the production rule from Figure 2).

Fig. 4. SimStudent requesting a demonstration in an algebra tutor interface after the
author has just entered “divide 3.”

When SimStudent is first presented with a problem of this form, such as
3x = 6, it will inform the author that it does not know how to proceed and
ask for a demonstration. The author might demonstrate to SimStudent that the
cells containing the left and right hand sides of the equation are relevant to the
problem (by double-clicking on these cells) and update the next step interface
element with “divide 3” (see Figure 4).
After receiving this demonstration, SimStudent parses the contents of the
focus of attention (The first parse tree in Figure 1 shows an example of what
the left hand of the equation might look like). Next, it employs the how learning
mechanism, which searches for a sequence of functions that when applied to
the nodes in the parse tree produce the input. In this example, it might learn
to append the left child of the parse tree (for the left side of the equation) to
the word “divide” and place it into the tutor interface (the then part of the
production rule in Figure 2). Using the locations of the relevant elements (the
left child of the parse tree), SimStudent then learns a general path through
the representation hierarchy to the relevant elements and the relevant portions
of the parse trees for these elements. Finally, SimStudent runs FOIL over the
relevant information to learn the conditions under which the learned behavior is
applicable. This results in the if portion of the production rule in Figure 2.
The learned production rule is more general than the single demonstration it
was learned from; it is applicable for many equations, such as 4x = 12 or 2x = 8.
However, when SimStudent is presented with a subtly different example that
utilizes the same skill, −x = 2, it results in the mistaken generation of the input
“divide -” (instead of “divide -1”). This is because in this situation the left child
of the parse tree on the left hand side of the equation is a minus sign instead of
the coefficient (see the second parse tree in Figure 1). In a review of problems
of the form −x =< Constant > in the ‘Self Explanation CWCTC Winter 2008
(CL)’ dataset accessed via DataShop [3], none of the human student made this
error– therefore it is an example of unrealistic generalization error.

58

4

Reflecting before Acting

One reason that humans do not make this error is that they have a “sense” for
what are reasonable output actions and they (subconsciously) reflect on actions
before taking them. When a student is faced with the problem −x = 2 they may
mentally produce the output “divide -,” but realize that a “-” by itself is not
mathematically grammatical because they have never seen an instance where
this has occurred. This might lead them to consider a different action or to ask
for help.
To reproduce this type of behavior, we modified SimStudent to utilize its
conceptual knowledge, the probabilistic context-free grammar trained on example inputs (described as “what” learning in section 2). The acquired grammar is
used to recognize when a potential output is not grammatical (when it cannot
be parsed) and automatically flag the situation as a negative example. In other
words, the system supervises itself and provides negative feedback (which the
when learner uses) to improve its learning.
Now, when SimStudent is presented with a problem and finds an applicable
rule, it simulates the execution of the rule and constructs a probabilistic parse
of the value generated by the rule. If the value cannot be parsed by the current
grammar (there is a 0% probability that the grammar produced the value),
then SimStudent flags the trace as a negative instance and re-runs the when
learning, which refines the conditions of the rule so that it no longer applies
in the erroneous situation. If SimStudent has no other applicable rules, then it
request a demonstration from the author, exactly like a human student.

5

Initial Results

To evaluate the effectiveness of this metacognitive loop, we have tested the probabilistic parser’s ability to separate correct from incorrect actions based on the
parse probability defined by the probabilistic context-free grammar. Table 1
shows five problems where SimStudent might make unrealistic errors. The first
three are problems where SimStudent might induce a rule for dividing by the
symbol before the variable instead of the coefficient. The last two problems correspond to inducing a rule retrieving the symbol after the variable and division
sign instead of the entire denominator. On all five problems, the probabilistic
grammar was capable of identifying the correct from the incorrect actions.
These results suggest that this approach is capable of identifying these kinds
of errors. In general, this approach will be effective at identifying errors that result in non-grammatical output, where grammatical is defined by the probabilistic context-free grammar. This is effective because the rules are learned specificto-general on a substantial amount of positive example inputs. By bringing this
previous experience to bare, SimStudent can avoid nonsensical generalization
errors and produce its own negative feedback, which enhances the effectiveness
of its other learning mechanisms (more self-labeled examples for the when learning). Furthermore, this requires no additional work from an author and should
reduce the amount of required author feedback.

59

Table 1. Five examples of problems where SimStudent might make the generalization
error of retrieving the character before the variable or after the variable and the division
sign, the corresponding correct and incorrect actions, the validity of these actions, and
the parse probability of the actions.
Example

Possible Action
divide −
−x = 2
divide −1
divide )
(−2)x = 6
divide (−2)
divide (
3(x + 1) = 6
divide 3
multiply (
x/(−3) = 3
multiply (−3)
multiply −
x/ − 5 = 1
multiply −5

Valid Parse Probability
No
0.00%
Yes
19.64%
No
0.00%
Yes
0.09%
No
0.00%
Yes
27.90%
No
0.00%
Yes
0.09%
No
0.00%
Yes
19.64%

This task of verifying the output could alternatively be viewed as applying constraints to SimStudent’s output and learning from constraint violations.
Viewed this way, our work is related to the work on constraint-based tutoring systems [4]. In our case, there is only one constraint, “the output must be
grammatical” where grammatical is defined as the probability of the output being produced by the grammar must be greater than 0%. We use a threshold of
greater than 0% to signify grammatical, but one could imagine using a different
threshold (e.g., greater than 0.05%). Thus, this constraint could be viewed as
a probabilistic constraint that is automatically acquired from positive training
examples.

6

Conclusion and Future work

In this paper, we outlined a novel approach to detecting and learning from unrealistic generalization errors that can be employed by simulated learner systems.
The implications of this approach are threefold: (1) its use will result in models of learning that more closely aligns with human data, (2) teachable agents
using this approach will be more realistic for the students using them, and (3)
developers can produce cognitive tutor models with less work.
While this approach shows promise, it clearly has some shortcomings that
should be remedied in future work. First, a more in-depth analysis of the alignment between SimStudent and human students is necessary. Previous work [5, 6]
has looked at the human errors that SimStudent is capable of predicting, but a
more detailed analysis of the unrealistic generalization errors, or errors that SimStudent makes that human students do not, would be useful. This would serve
as a baseline to evaluate the SimStudent model and to evaluate the effectiveness
of this approach.

60

A second direction for future work is to compare this approach to other approaches that might reduce these errors. We could imagine a system that has
additional condition knowledge for the operator functions so that it would not
generalize to situations where the function sequence would not be applicable
(such as trying to divide by a symbol instead of a number). It would also be
interesting to explore how reflection might facilitate the acquisition of this additional condition knowledge for the operator functions.
Finally, we are interested in applying this approach in other more complex
and open-ended domains such as in RumbleBlocks, an educational game that
teaches K-3 children about the relationships between the concepts of stability,
low center of mass, wide base, and symmetry. We have been exploring how probabilistic grammars can be used to learn conceptual knowledge in RumbleBlocks
[7] and we believe that this approach should scale up to this more complex
domain.

References
1. Li, N., Schreiber, A.J., Cohen, W.W., Koedinger, K.R.: Efficient Complex Skill
Acquisition Through Representation Learning. Advances in intelligent tutoring systems 2 (2012) 149–166
2. Quinlan, J.R.: Learning Logical Definitions from Relations. Machine Learning 5
(1990) 239–266
3. Koedinger, K.R., Baker, R.S.J.d., Cunningham, K., Skogsholm, A., Leber, B., Stamper, J.: A Data Repository for the EDM community: The PSLC DataShop. In
Romero, C., Ventura, S., Pechenizkiy, M., Baker, R.S.J.d., eds.: Handbook of Educational Data Mining. CRC Press (2010)
4. Mitrovic, A., Ohlsson, S.: Evaluation of a constraint-based tutor for a database
language. International journal of artificial intelligence in Education 10 (1999)
238–256
5. Lee, A., Cohen, W.W., Koedinger, K.R.: A Computational Model of How Learner
Errors Arise from Weak Prior Knowledge. In Taatgen, N., van Rijn, H., eds.: Proceedings of the Annual Conference of the Cognitive Science Society, Austin, TX
(2009) 1288–1293
6. Matsuda, N., Cohen, W., Sewall, J., Lacerda, G., Koedinger, K.R.: Evaluating a
Simulated Student using Real Students Data for Training and Testing. In Conati,
C., McCoy, K., Paliouras, G., eds.: Proceedings of the International Conference on
User Modeling. (2007) 107–116
7. Harpstead, E., MacLellan, C., Koedinger, K.R., Aleven, V., Dow, S.P., Myers, B.:
Investigating the Solution Space of an Open-Ended Educational Game Using Conceptual Feature Extraction. In: Proceedings of the International Conference on
Educational Data Mining. (2013)

61

Towards Moment of Learning Accuracy
Zachary A. Pardos† and Michael V. Yudelson‡
†Massachusetts Institute of Technology
77 Massachusetts Ave., Cambridge, MA 02139
‡Carnegie Learning, Inc.
437 Grant St., Pittsburgh, PA 15219, USA
zp@csail.mit.edu,yudelson@carnegielearning.com

1

Introduction

Models of student knowledge have occupied a significant portion of the literature in the area of Educational Data Mining1 . In the context of Intelligent
Tutoring Systems, these models are designed for the purpose of improving prediction of student knowledge and improving prediction of skill mastery. New
models or model modifications need to be justified by marked improvement
in evaluation results compared to prior-art. The standard evaluation has been
to forecast student responses with an N-fold student level cross-validation and
compare the results of prediction to the prior-art model using a chosen error
or accuracy metric. The hypothesis of this often employed methodology is that
improved performance prediction, given a chosen evaluation metric, translates
to improved knowledge and mastery prediction. Since knowledge is a latent, the
estimation of knowledge cannot be validated directly. If knowledge were directly
observable, would we find that models with better prediction of performance also
estimate knowledge more accurately? Which evaluation metrics of performance
would best correlate with improvements in knowledge estimation? In this paper
we investigate the relationship between performance prediction and knowledge
estimation with a series of simulation studies. The studies allow for observation
of the ground truth knowledge states of simulated students. With this information we correlate the accuracy of estimating the moment of learning (mastery)
with a host of error metrics calculated based on performance.

2

Bayesian Knowledge Tracing

Among the various models of knowledge, a model called Bayesian Knowledge
Tracing [2] has been a central focus among many investigators. The focus on
this model has been in part motivated by its use in practice in the Cognitive Tutors [4], used by over 600,000 students, and by its grounding in widely adopted
cognitive science frameworks for knowledge acquisition. For our experiments we
will be employing the most frequently used basic Bayesian Knowledge Tracing
1

A session during the main proceedings of EDM 2012 was dedicated to papers on
Knowledge Tracing, a frequently used approach to modeling student knowledge.

62

model for both simulation and evaluation; however, there are implications beyond BKT models. Knowledge Tracing is a simple Hidden Markov Model of
Knowledge defined by four parameters; two performance parameters and two
knowledge parameters. The performance parameters, guess and slip, are the
emission parameters in an HMM which respectively correspond to the probability that a student answers correct even if she is in the negative knowledge
state (guess) and the probability that she answers incorrectly even if she is in
the positive knowledge state (slip). The knowledge parameters, prior and learn
rate, are the probability that a student knows the skill before answering any
questions and the probability that, if the student is in the negative knowledge
state, she will transition to the positive state at any given opportunity.

3

Related Work

There has been a limited amount of prior work focusing on detecting the moment
of learning. We were able to track one relevant publication by Baker and colleagues [1]. They investigated detection of moment of learning in student data by
modifying BKT structure. Another relevant result was published by [5]. They
looked at scoring student model fits on simulated data and found a disparity
between rankings of two frequently used metrics: root mean squared error and
area under ROC curve. In this work we would like to address the question of
the quality of detecting the moment of learning and investigate the problem of
choosing a goodness-of-fit metric for that purpose.

4

Data

Our simulation dataset consisted of 1,000 simulated students and 100 skills with
30 questions per skill. Every student answered all 30 questions for each of the
100 skills. In the BKT simulation model we included no dependencies between
skills and also no student specific parameters; therefore, the data can be thought
of as either being produced by 1,000 students total or a new 1,000 students for
every skill. Programmatically, data for each skill is stored in a separate file. Each
row in each file represents one students data for that skill. The data stored from
the simulation contains the students ground truth binary state of knowledge
(mastered or not) at each of the 30 opportunities to answer (first 30 columns)
and also the students correctness of responses to the 30 questions (stored in the
second set of 30 columns).
In addition to the simulated data files containing student knowledge states
and observed responses, we had corresponding files containing inferences of
knowledge states and predictions of responses made with 16 different parameter sets resulting in 1,600 prediction files. Details of the parameter selection for
simulation and prediction are discussed in the next section.

63

5
5.1

Methodology
Simulation

We generated 1,000 students knowledge and performance for 100 skills. Skills are
defined by a set of four knowledge tracing parameters which the skill data is generated from. The 100 sets of four parameters were selected at random, uniformly
sampling from the following constrained ranges for the parameters; prior between
0.01-0.80, learn rate between 0.01-0.60, and guess and slip between 0.05-0.40. After the 100 sets of parameters were selected, simulated data was produced by
specifying a Dynamic Bayesian Network representation of Knowledge Tracing
with a time slice length of 30. This representation, defined in Kevin Murphys
Bayes Net Toolbox, with a particular parameter set fixed in the conditional
probability tables, was then sampled 1,000 times, representing each simulated
student. The sample Dynamic Belief Network function in BNT for simulation is
a simple one; a random number between 0 and 1 is generated, if the number is
equal to or lower than the prior parameter, the simulated student begins in the
negative (not learned) state at time slice 1. To generate the observed response at
this time slice, another random number is generated, if that number is greater
than the guess parameter, the observed response is incorrect. To determine if
the students knowledge state is positive (learned) at the next time slice; a random number is generated, if that number is less than or equal to the learning
rate, then the students state is positive. With a positive state, the new random
number needs to be greater than the slip parameter in order to produce a correct response. This is repeated for 30 times to simulate 30 knowledge states and
observed responses per student.
5.2

Prediction

Typically, to predict student data, a hold-out strategy is used whereby a fraction
of the students and their data is used to find a good fitting set of parameters.
That good fitting set is then used to predict the fraction of students not used in
training. The research question of this paper did not involve parameter fitting
but rather required us to evaluate various models and observe how the models
prediction of performance corresponded to its inference of knowledge. To do this
we needed variation in models which we accomplished by choosing 16 candidate
parameter sets with which to predict student data from each of the 100 skills.
Since no training was involved, all data served as the test set. The top five sets
of parameters used in the Cognitive Tutors was used, as well as 10 randomly
generated parameters sets using the the same parameter constraints as the simulation, and, lastly, the ground truth parameter set for the skill was used to
predict. The the same 15 parameter sets were used to predict the 100 skills, only
the ground truth parameter set changed.
The prediction procedure is the same one used in all papers that use Knowledge Tracing; the prior, guess and slip parameters dictate the probability of
correct on the first question. After the prediction is made, the correctness of

64

Table 1: Confusion Table
Actual
Correct
Incorrect
Correct True Topisitve(TP) False Positive (FP)
Predicted
Incorrect False Negative (FN) True Negative (TN)

the first question is revealed to the KT algorithm, which incorporates this observation using Bayes Theorem to infer the likelihood that the knowledge was
known at that time. A learning rate transition function is applied and the processes is repeated 30 times in total to create 30 predictions of knowledge and 30
predictions of correctness per student for a skill.

6

Metrics

The most common metrics used to evaluate prediction performance in the EDM
literature has been Area Under the Receiver Operator Curve (AUC) and Root
Mean Squared Error (RMSE). One of the goals of our experiment is to reveal
how indicative these measures are of the models accuracy in inferring knowledge.
While these are the most common metrics, many others have been used in machine learning to evaluate predictions. We utilize a suite of metrics to investigate
which metric is best at forecasting knowledge inference accuracy.

6.1

Model Performance

We selected a set of metrics in wide use today to score models when predicting
student performance and knowledge state. Below is a short description of them.

Confusion Table Metrics Confusion table (rf. Table 1) is a table widely used
in information retrieval and is a basis for a set of metrics capturing correctness
of a retrieval or classification algorithm. Rows and columns of the confusion
table denote the predicted and actual classes respectively and the cells in the
intersection contain the counts of cases. Refer to Table 1 for an illustration. Here
we illustrate a case for binary classification akin to the problem of binary classification of student performance (correct or incorrect) and state of knowledge
(known or unknown).
If prediction is not categorical, say a probability from [0, 1], it is customary
to round it: probabilities of 0.5 and greater become 1. For example, the cases
when prediction matches the reality are captured in True Positive cell and the
cases when the actually incorrect responses are marked as correct are captured
in False Positive cell. We will use the confusion table metrics below.

65

TP + TN
TP + FP + TN + FN
TP
precision =
TP + FP
TP
recall =
TP + FN
precisionṙecall
F − measure = 2
precision + recall
accuracy =

(1a)
(1b)
(1c)
(1d)

As opposed to the so-called point measures described above, there is also
a frequently used Area Under Receiver Operating Characteristic curve (AUROC), which is a curve measure. The curve is produced by varying the rounding
threshold (0.5 for point measures) from 0 to 1 and computing and plotting False
Positive Rate (FPR) vs. True Positive Rate (TPR) (see below).
TP
TP + FN
FP
FPR =
FP + FN
TPR =

(2a)
(2b)

An area under resulting curve is the sought metric. An area of 0.5 is equivalent
to random chance for a binary classifier. An area greater than 0.5 is, thus, better
than chance. An exact AUC calculation can also be derived by enumerating
all possible pairs of predictions. The percentage of the pairs in which the true
positive prediction is higher is the AUC. This is the ability of the predictor to
discriminate between true and false.
Pseudo R2 R2 or percent variance explained is often used as a goodness of
fit metric in linear regression analysis. For with binary classification, there exist
several versions of R2 called pseudo R2 . Applicable to our situation is Efrons
pseudo R2 (refer to Equation below).
PN
yi − yˆi
R2 = 1 − Pi=1
N
i=1 yi − ȳ

(3)

Where N is the number of data points, yi is the i-th component of the
observed variable, y¯i is the mean observed value, and yˆi the prediction of i-th
component of the observed variable.
Metrics Based on Log-Likelihood Likelihood functions are widely used in
machine learning and classification. Likelihood captures the probability of the
observing data given parameters of the model. In binary classification a natural log transformation of the likelihood function is often used (see below). Here

66

N is the total number of datapoints, yi is the i-th component of the dependent variable, yˆi is the predicted value of the i-th component of the dependent
variable.

loglikelihood =

N
X

yi ln(yˆi ) + (1 − yi ) ln(1 − yˆi )

(4)

i=1

In addition to log-likelihood itself, there are several metrics that use loglikelihood as kernel component. For example, Akaike Information Criterion (AIC),
Akaike Information Criterion with correction for finite sample size (AICc), Bayesian
Information Criterion (BIC), and several others. These metrics introduce various
forms of penalty for the size of the model (number of parameters) and number
of datapoints in the sample in order to put overfitting models at disadvantage
when performing model selection. Here k is the number of model parameters, N
is the number of datapoints.

AIC = −2loglikelihood + 2k
2k(k + 1)
AICc = AIC +
N −k−1
BIC = −2loglikelihood + k ln(N )

(5a)
(5b)
(5c)

Since we are comparing models that are only different in the parameter values
and are doing so on the same dataset, we will not see difference in ranks assigned
by log-likelihood, AIC, AICc, and BIC metrics.

Capped Binomial Deviance In addition to log-likelihood and log-likelihoodbased metrics, we include the Capped Binomial Deviance (CBD). Capped binomial deviance is a version of the log-likelihood where prediction values are
mandated to be at least away from 0 and 1 values and uses a logarithm with
base 10 instead of natural logarithm. The is usually set to a small value of 0.001.

6.2

Moment of Learning

To capture the quality of detecting the moment of learning we devised a metric
based on mean absolute deviation (MAD). Namely, moment of learning MAD is
the average absolute difference of number of skill application opportunities between the moment when the internal state of the generating skill model switched
to learned state and the moment when the probability of the skill being in a
learned state reaches 0.95 (a traditionally used threshold in the area of intelligent tutoring systems). A perfect model would have a moment of learning MAD
of 0. The larger the moment of learning MAD is the worse the model prediction
of model of learning is.

67

7
7.1

Experiments and Results
Experiment 1

Research question: Among accuracy metrics used for ranking various parameter
sets (models), which ones correlate best with accuracy of moment of learning
prediction?
7.2

Results

The Table 2 below contains the correlations of performance prediction value,
knowledge prediction value for all metrics, and moment of learning mean absolute error. Since prediction of performance is most widely adopted as a standard
approach and the fact that we are trying to contrast it to the moment of learning
mean absolute error, we sorted the rows corresponding to various statistical metrics by the respective column. The first column lists the metric used to evaluate
the goodness of performance and knowledge prediction. The second column is the
correlation between knowledge and performance prediction using the particular
metric on both (this is the column the table is sorted by). The third column is
the correlation between the particular metric used to evaluate performance and
Mean Absolute Deviation (MAD) of Moment of Learning prediction. This is the
column which tells us if the metrics used to evaluate performance are correlated
with error in mastery / Moment of Learning prediction. The fourth column gives
correlations of Moment of Learning MAD and metric values for predicting internal knowledge state. This correlation captures agreement between identifying the
moment student learned a skill (this happens once per student-skill tuple) and
the correctness of identifying the skills knowledge state for the student across
all skill attempts.
7.3

Experiment 2

Hypothetically, the ground truth parameter sets should be the best at both
making predictions of performance and estimating knowledge. A good metric
should favor the ground truth parameters, therefore we ask: How often is the
ground truth model the best at prediction performance according to the various
metrics?
7.4

Results

The correlations of the performance and knowledge state prediction metrics from
prior section targeted the 15 model parameter combinations that were different
from the generating ground truth model parameters. Now, let us look at how
the ground truth model compares to the other 15 we tested with respect to the
statistical metrics we chose. Table 3, for each metric, gives the number of times
a ground truth model parameter set is the best with respect to a given metric,
and an average rank of the ground model parameter set as compared to the

68

Table 2: Metric correlations
Metric

Correlation of per- Correlation of per- Correlation of knowlformance and knowl- formance metric and edge metric and
edge metric
Moment of Learning Moment of Learning
MAD
MAD
recall
0.878 ***
-0.954 ***
-0.819 ***
F-measure
0.561 ***
-0.839 ***
-0.792 ***
accuracy
0.522 ***
-0.802 ***
-0.822 ***
precision
0.334 ***
-0.797 ***
-0.628 ***
RMSE
0.470 ***
0.754 ***
0.828 ***
AIC
0.375 ***
0.751 ***
0.702 ***
AICc
0.375 ***
0.751 ***
0.702 ***
BIC
0.375 ***
0.751 ***
0.702 ***
CBD
0.409 ***
0.751 ***
0.762 ***
log-likelihood 0.375 ***
0.751 ***
0.702 ***
pseudo R2
0.592 ***
-0.236 *
-0.296 **
AU ROC
0.335 ***
-0.119
-0.652 ***
Note: with respect to correlations with moment of learning MAD, in some cases a
negative correlation is desirable (e.g., for accuracy), and for some cases a positive
correlation is desirable (e.g., for RMSE). This is due to the fact that the smaller the
moment of learning MAD the better, which is true for some metrics and the inverse
is true for others. The table is sorted while observing this phenomenon (effectively
sorting by the absolute value of the correlation coefficient).

Table 3: ground truth model rank vs. the other 15 models
Metric

Ground truth model Mean rank of ground
has rank of 1
truth model
AIC
88/100
1.82/16
AICc
88/100
1.82/16
BIC
88/100
1.82/16
CBD
88/100
1.82/16
log-likelihood 88/100
1.82/16
RMSE
88/100
1.82/16
pseudo R2
88/100
1.83/16
accuracy
33/100
2.52/16
F-measure
12/100
4.27/16
AU ROC
26/100
4.35/16
recall
0/100
6.65/16
precision
5/100
9.71/16

other 15 model. In each case we are aggregating across 100 different sets of 15
models plus one ground truth model. As we can see log-likelihood based models
and RMSE form a group of metrics that gives ground truth models a large edge
over the 15 reference models. Confusion table metrics, Area under ROC curve
and the pseudo R2 gibe a drastically smaller support for it.

69

7.5

Experiment 3

Ground truth parameters do not always predict the data the best, but often do
when using metrics like RMSE or log-likelihood. Do the parameter sets that are
not predicted well by ground truth share a common pattern? Does the relative
performance of ground truth correlate with high or low values of prior, learn,
guess or slip in the generating parameters?
7.6

Results

Seeing log-likelihood based and RMSE metrics score the ground truth model
at the same level of mean rank, we are wondering whether, across all 100 of
generating parameter sets, the data produced by the same sets of parameters is
equally hard to predict with ground truth model. For that we looked at whether
the BKT parameter values correlate with ranks ground truth model receives on
the moment of learning MAD metric.
First of all, moment of learning MAD metric ranked ground truth as best
only 33/100 times with an average rank of 2.53/16. Correlations of moment
of learning MAD ranks for ground truth models showed that theres a small
marginally significant effect of pInit probability on the moment of learning MAD
score (r = 0.18, p − val = 0.07). Guessing probability does not correlates with
moment of learning MAD (r = −.06, p − val = 0.55).
Probability of learning and slip probability, however, are very strongly related
to the moment of learning metric. The larger the learning rate of a simulated skill
is, the higher the rank of the ground truth model is (r = 0.68, p − val < 0.001).
Namely, the faster the skill is learned, the worse job ground truth model is doing.
In the case of pSlip, the relation is the opposite: the higher the guess rate is,
the higher rank moment of learning MAD assigns to the ground truth model
(r = −0.52, p − val < 0.001).
Both the pLearn and pSlip parameters are controlling the process of skills
transitioning into the learned state. Strong negative correlation of moment of
learning MAD and pSlip is quite logical. Higher pSlip results in more errors even
when the skill is mastered, as a result the transition to the learned state becomes
more blurred. In this situation the ground truth model has an edge over other
models. However, it is high to explain a high positive correlation of moment of
learning MAD and pLearn. Higher pLearn means more correct responses overall,
this should put ground truth model at an advantage. Additional investigation is
necessary to address this phenomenon.

8

Discussion

In our first experiment we found that three less commonly used accuracy metrics
showed the best correspondence to accuracy of moment of learning estimation.
These metrics were: recall, F-measure, and accuracy, with recall giving a very
high correlation of 0.954. Also noteworthy was the poor performance of AUC

70

with a correlation of -0.119. This was the worst correlation and suggests that
AUC should not be used to determine the relative goodness of models based
on prediction performance if the underlying goal is to rank models based on
knowledge estimation goodness. Metrics like recall and F-measure ought to be
adopted in place of AUC for these purposes.
We also found that ground truth model parameters did not always perform
the best and that RMSE and log-likelihood based metrics tended to predicted
ground truth being the best parameter set more than the others. AUC, recall,
F-measure, and precision, however, were among the worst. Therefore, if the underlying goal of an analysis is to recover ground truth parameters (such as with
inferring pedagogical efficacy), RMSE and log-likelihood measures should be
used and the aforementioned accuracy metrics should be avoided. The experiments 2 raised the question of why ground truth may not always predict the
best experiment 3 indicated that high learning rate and low slip in the generating
parameters can prove difficult for mastery prediction.
Overall detecting the moment of learning in the generated data by observing
a switch from a string of all 0s (unknown state) to the string of all 1s (known
state) is often not easy even when ground truth parameters are used. Especially
if guess and slip parameters are larger, several back-and-forths between known
and unknown state are common. In the area of ITS it is customary to wait till
three correct attempts in a row to be sure student has mastered the underlying
skill. In our case, when we assumed the moment of learning is the first time
when probability of knowing the skill crosses the 0.95 threshold. Following from
recent results on the lag with detecting the moment of learning that occurs in the
Bayesian Knowledge Tracing [3], in future, we will experiment with adjustments
to our computation of the moment of learning to compensate for this.

References
1. Baker, R.S.J.d., Goldstein, A.B., Heffernan, N.T. (2010) Detecting the Moment
of Learning. Proceedings of the 10th Annual Conference on Intelligent Tutoring
Systems, 25-34.
2. Corbett, A. T. and Anderson, J. R.: Knowledge tracing: Modeling the acquisition of
procedural knowledge. User Modeling and User-Adapted Interaction, 4(4), 253-278.
(1995)
3. Fancsali, S.E., Nixon, T., Ritter, S. (2013) Optimal and Worst-Case Performance of
Mastery Learning Assessment with Bayesian Knowledge Tracing. In: Proceedings
of the 6th International Conference on Educational Data Mining.
4. Koedinger, K. R., Anderson, J. R., Hadley, W. H., and Mark, M. A. (1997). Intelligent tutoring goes to school in the big city. International Journal of Artificial
Intelligence in Education, 8, 3043.
5. Pardos, Z. A., Wang, Q. Y., Trivedi, S. (2012) The real world significance of performance prediction. In Proceedings of the 5th International Conference on Educational Data Mining. Crete, Greece. pp 192-195.

71

Impact of Prior Knowledge and Teaching
Strategies on Learning by Teaching
Ma. Mercedes T. RODRIGO1, Aaron ONG1, Rex BRINGULA2, Roselle S.
BASA2, Cecilo DELA CRUZ2, Noboru MATSUDA3
1

Ateneo Laboratory for the Learning Sciences, Department of Information Systems
and Computer Science, Ateneo de Manila University, Loyola Heights, Quezon City,
Philippines
2
University of the East, Manila, Philippines
3
Human-Computer Interaction Institute, Carnegie Mellon University, Pittsburgh, PA,
USA
mrodrigo@ateneo.edu, icemanfresh@yahoo.com
rexbringula@gmail.com, roselle_basa@yahoo.com
noboru.matsuda@cs.cmu.edu

Abstract. We investigate cognitive factors that are predictive of
learning gains when students learn to solve equations by teaching a
synthetic peer, called SimStudent. Previous empirical studies
showed that prior knowledge is strongly predictive of post-test
scores. However, in a recent study in the Philippines that replicated
our previous study in the USA, there were students with low priorknowledge who tutored their SimStudent better than other equally
low prior students. In this paper, we analyze both process data (tutoring interactions) and outcome data (test scores) to understand
what makes learning by teaching more effective. The results imply a
presence of individual behavioral differences beyond the difference
in the prior knowledge that might have affected SimStudent’s learning, which in turn had non-trivial influence on tutor learning.
Keywords. Learning by teaching, teachable agent, SimStudent, Algebra equations, prior knowledge

1.

Introduction

Since the late 1990s, researchers have investigated intelligent tutoring
systems with intelligent pedagogical agents (often called teachable agents)
to study a promising type of learning where students learn by teaching [1-3].
These technologies allow researchers to conduct tightly controlled experiments and to collect detailed process data representing interactions between
students and teachable agents that together provide empirical evidence for
the benefit of learning by teaching [4].
Matsuda et al. (in print), for example, showed that students’ learning significantly correlated with the learning of teachable agents. Biswas et al. [5]

72

studied whether students could learn to self-regulate their teaching activities
and how the ability of self-regulation affects the tutor learning. It is therefore of intellectual interest to uncover how the tutoring interaction affects
students’ learning by teaching.
In the current study, we use SimStudent, which is a teachable agent that
helps students learn problem-solving skills by teaching [6]. It has been tested and redesigned several times, resulting in insights regarding the effects of
learning by teaching and related cognitive theories to explain when and how
students learn by teaching. Previous studies showed that pre-test score were
highly predictive of post-test scores when students learn equation solving by
teaching SimStudent [7]. In general, when students do not have sufficient
prior knowledge on the subject to teach, they are not able to teach correctly
and appropriately hence the benefit of learning by teaching would be arguably decreased.
Nonetheless, there are some students with low prior knowledge who
learned more than others by teaching SimStudent. Among equally low-prior
students, those who showed better performance on the post-test actually
tutored their SimStudent better as well. The difference in the learning gain
among students with comparable prior-knowledge indicates a presence of
effective interaction for learning by teaching that might bootstrap tutor
learning even with insufficient prior knowledge.
The goal of this paper is to investigate cognitive factors that affect tutor
learning. The central research question is why some students (even with low
prior knowledge) learned more than other students with comparable prior
knowledge. To address this research question, the current paper analyzes
data from two classroom (in-vivo) studies conducted in the USA and the
Philippines. The Philippines study was a replication of the USA study reported earlier [8].
In the rest of the paper, we first introduce a learning environment in
which students learn to solve linear equations by teaching SimStudent. We
will then introduce two classroom studies conducted in the USA and the
Philippines followed by the results and discussions.

2.

Online Learning Environment with SimStudent

This section provides a brief overview of SimStudent and the online
learning environment, Artificial Peer Learning environment using
SimStudent (APLUS), in which students learn to solve algebra equations by
interactively teach SimStudent. Technical details about SimStudent and
APLUS can be found elsewhere [7]
2.1.

SimStudent

SimStudent is a synthetic pedagogical agent that acts as a peer learner. It
learns procedural skills from examples. That is, a student gives SimStudent

73

a problem to solve. SimStudent then attempts to solve the problem one step
at a time, occasionally asking the student about the correctness of each step.
If SimStudent cannot perform a step correctly, it asks the student for a hint.
To respond to this request, the student has to demonstrate the step.
Students may not be able to provide the correct feedback and hints. As
SimStudent is unable to distinguish correct from incorrect feedback, it continues to try to generalize examples and generate production rules that represent the skills learned. SimStudent is also capable of making incorrect
inductions that would allow SimStudent to learn incorrect productions even
when students teach SimStudent correctly. SimStudent’s ability to model
students’ incorrect learning is one of the unique characteristics of
SimStudent as a teachable agent.
2.2.

APLUS: Artificial Peer Learning Environment using SimStudent

Figure 1 shows an example screen shot of APLUS. In APLUS, students
act as a tutor to teach SimStudent how to solve equations. SimStudent is
named Stacy and visualized at the lower left corner of APLUS. The tutoring
interface allows the student and Stacy to solve problems collaboratively. In
the figure, a student poses the problem 3x+6=15 for Stacy to solve. Stacy
enters “divide 3” and asks the student whether this is correct. The student
responds by clicking on the [Yes/No] button. If the student gets stuck, she
can consult the examples tabbed at the top of the screen.
The student has the option of gauging how much Stacy has learned with
the use of a quiz. The student chooses when and how often to administer

Fig 1. A screen shot of APLUS. SimStudent is visualzed with an avatar
image and names Stacy.

74

the quiz by clicking a button at the bottom of the interface. The quiz interface looks like the tutoring interface, however, when Stacy takes the quiz,
she does so independently, without any feedback or intervention from the
student. At the end of the quiz, the student is presented with a quiz result.
The quiz is divided into 4 sections, each with two equation problems.
The quiz items were created from the mix of one-step, two-step, and target
equations (i.e., the equations with variables on both sides).
Stacy cannot progress to a section until she passes the previous section.
The students were asked to tutor Stacy to be able to solve equations with
variables on both sides. In the classroom studies, the students were informed
that their goal was to help Stacy pass all four (4) sections of the quiz.

3.
3.1.

Methods
Participants

The USA study took place in one high school in Pittsburgh, PA, under
the supervision of the Pittsburgh Science of Learning Center [8]. There were
eight Algebra I classes with an average of 20 students per class. A total of
160 students with ages ranging from 14 to 15 participated in the study.
The Philippines study took place in one high school in Manila, Philippines, under the supervision of the co-authors from the University of the
East and the Ateneo de Manila University. We enlisted participation from
five first year high school sections with an average of 40 students per class.
There were 201 study participants in all with ages ranging from 11 to 15.
The average age of the participants was 12.5 years.
3.2.

Structure of the study

In both the USA and the Philippine studies, each participant was randomly assigned to one of two versions of SimStudent: an experimental condition
in which Stacy prompted the participants to self-explain their tutoring decisions and a control condition with no self-explanation prompts. The study
was designed this way to investigate a particular research question on the
effect of self-explanation for tutor learning [8], which is beyond the scope of
the current paper. For three consecutive days, participants used their assigned version of SimStudent for one classroom period per day (42 minutes
for the USA and 60 minutes for the Philippines study).
3.3.

Measures

Students took pre- and post-test before and after the intervention. The
students also took a delayed-test two weeks after the post-test was administered. Three versions of isomorphic tests were randomly used for pre-, post-,
and delayed-tests to counterbalance the test differences. Students had the
entire class period to finish the tests.

75

The tests are divided into five parts. Of these five parts, three parts are to
test procedural knowledge on how to solve equations (the Procedural Skill
Test, or PST), whereas other two parts are to test conceptual knowledge
about algebra equations (the Conceptual Knowledge Test, or CKT). 102 out
of 160 USA participants took all three tests, whereas in the Philippines 146
out of 201 participants took all three tests. In the following analyses, unless
otherwise indicated, only those students who took all three tests are included.
The system automatically logged all of the participants’ activities including problems tutored, feedback provided, steps performed, examples reviewed, hints requested, and quiz attempts. In the following analysis, we
use these factors as process data.

4.
4.1.

Results
Overall Test Scores

Table 1 shows mean test scores plus or minus SD for the pre, post, and
delayed Procedural Skill Tests from two studies. To see how students’ test
scores varied before and after teaching SimStudent, we conducted a twoway repeated-measures ANOVA with condition as a between-subjects variable and test-time (pre, post, and delayed) as a within-subjects variable. For
the USA study, the repeated measure analysis revealed a weak trend for the
main effect for test-time. A post-hoc analysis detected a difference from pretest to post-test [8]. In the Philippines study, the test-time was also the main
effect, and the post-hoc analysis detected that delayed-test was significantly
higher than pre-test; t(247.1) = 2.457, p < 0.05. This difference, however,
was likely due to the classroom instruction that students were taking during
the two-week interval between the intervention and the delayed test.
Both in the USA and the Philippine studies, condition was not the main
effect—the presence of self-explanation did not affect tutor learning with
the version of APLUS and SimStudent used in two studies.
Table 1: Mean test scores ± SD for pre, post, delayed procedural skill test for
each study.
Pre-test
Post-test
Delayed-test
Philippines (PH)
0.21±0.01
0.22±0.02
0.25±0.03
USA (US)
0.68±0.04
0.71±0.05
0.69±0.06

4.2.

Impact of prior knowledge

As shown in Table 1, there was a notable difference in the pre-test scores
suggesting that USA students had higher level prior knowledge than Philippine students; t(142.4) = -22.25, p < 0.001.

76

To see how prior knowledge affected learning and if the impact of prior
knowledge differ between two studies, we ran a regression analysis with
post-test score as a dependent variable and study (US vs. PH) as a fixed
factor using pre-test score as a covariate. The results showed that pre-test is
a strong predictor of post-test; t(244) = 2.80, p < 0.01. There was also a
strong interaction between pre-test and study; the regression coefficient
(slope) differed significantly between two studies; bPH = 0.32 vs. bUS = 0.76;
F(1,244) = 11.24, p < 0.001—suggesting that, in general, USA students
gained (from pre- to post-test) more than Philippine students. Figure 2
shows the scatter plot for pre-test (x-axis) and post-test (y-axis) scores.
USA students (red triangles) had steeper regression line than Philippine
students.
4.3.

Quiz Results

0.0

0.2

0.4

Post.

0.6

0.8

1.0

In the USA study, 36 out of 102(35%) students made their SimStudents
pass all four quiz sections. In the Philippines study, no students passed all
four sections. At the best, only 7 out of 146 (5%) of Philippine students had
their SimStudents pass quiz section 2.
In the Philippines study, there were 73 students who solved quiz item #1
correctly. Of those 73 students, 68 students solved quiz item #2 correctly
(hence by definition passing quiz section 1). Of those 68, only 11 students
passed quiz section 2 (i.e., solving the first four quiz items correctly).
One possible explanation for the Philippine students’ poor performance
on the quiz is that Philippine students have insufficient prior knowledge, as
indicated by the
factor(Country)
low pre-test scores
PH
and the weak reUS
gression slope. A
number of factors
may account for the
difference
prior
knowledge, including curricular and
age differences.
Still, some Philippine
students
managed to solve
the first four quiz
items (i.e., passing
the quiz section 2),
0.0
0.2
0.4
0.6
0.8
1.0
while others did
Pre
not. Why might
this be so? The Fig. 2: Scatter plot of pre-test (x-axis) and post-test
next section ad- (y-axis) scores. US students had larger regression
dresses this issue.
slope (0.76) than the PH students (0.32).

77

4.4.

What makes learning by teaching more effective?

To understand why some SimStudents performed better on the quiz than
others, we have analyzed the process data. In this analysis, we grouped students depending on the quiz sections their SimStudents passed. We call
students whose SimStudents passed and failed quiz section x the “passing
Sx” and “failing Sx” students, respectively. By definition, there were no
passing S3 students in the Philippines study.
Our focus in this particular analysis is to understand how some students
managed to pass quiz sections in the Philippines study. Therefore, we only
included Philippine students for this analysis unless otherwise noted.
4.4.1. Accuracy of tutoring
One cognitive factor that had a significant contribution to tutor learning
in the past studies is the accuracy of tutoring—i.e., the accuracy of recognizing correct and incorrect steps made by SimStudent as well as the accuracy
the steps demonstrated as hint.
We thus compared the mean accuracy of passing/failing S1 and S2 students. The result suggested that the accuracy of tutoring is a key for success
on the quiz in the Philippines study as well. For S1: MPassing = .70 (SD = .14)
vs. MFailing = .52 (SD = 0.16); t(119.3)=-6.89, p < 0.001. For S2: MPassing =
.75 (SD = 0.09) vs. MFailing = .59 (SD = 0.18); t(8.7)=-4.39, p < 0.01.
Students’ prior knowledge should have affected tutoring accuracy. There
was actually a strong correlation between the prior knowledge (measured as
the pre-test score on the Procedural Skill Test) and the accuracy of tutoring.
There was also a study difference—USA students tutored more accurately
than Philippine students. The centered polynomial regression with the centered pre-test score (i.e., the difference from the mean) as the covariate
(C.Pre) and the study (US vs. PH) as a fixed factor predicting the accuracy
of tutoring (AT) revealed the following regression coefficients: AT = 0.62 +
0.16*C.Pre + 0.18[if US]; r2=0.42, F(2, 235)=88.31, p<0.001; meaning that
Philippine students at the average procedural skill pre-test tutored with a
62% accuracy rate. USA students tutored 18% more accurately than Philippine students in general. There was no study difference for the regression
slope—suggesting that the prior knowledge affected the accuracy of tutoring
equally in two studies.
A further analysis that compared passing and failing S1 students revealed
that the prior knowledge was not the dominant factor that affected the accuracy of tutoring. In the Philippines study, the average pre-test score of the
Procedural Skill Test for passing S1 students (M=.21, SD=0.10) was not
higher than failing S1 students (M=.20, SD=0.09). However, the average
accuracy of tutoring was higher for passing S1 students (M=.70, SD=.14)
than failing S1 students (M=.52, SD=0.17).
As for the students’ learning, there was a weak trend on the average normalized gain from pre- to post- favorable to passing S1 students (M=.05,
SD=0.22) than failing S1 students (M=.01, SD=0.18); t(92.3)=-0.46, p=0.65.

78

This indicates that the passing S1 students in the Philippines study learned
more by teaching than the failing S1 students although where was no significant difference of the prior knowledge among them. There might have been
difference in the way passing and failing S1 students tutored SimStudent.
The next section shows the results on analyzing process data.
4.4.2. Tutoring strategies
Since quiz items were fixed, using quiz items for tutoring could be a
good strategy to help SimStudent pass the quiz. Actually, in the USA study,
passing S4 students showed a higher percentage of using quiz problems for
tutoring (MUS = .95, SD = .11) than failing S4 students (MPH = .59, SD =
.42); t(28) = -4.08, p < 0.001.
Thus, we first investigated whether passing S1 and S2 students in the
Philippines study used more quiz items for tutoring than failing S2 students.
We found that only 47% (1826 out of 3898) problems tutored in the Philippines study were the quiz items. Philippine students did not copy quiz items
for tutoring as often as the successful (i.e., passing S4) USA students.
If time on task were a crucial factor for learning by teaching, then students who tutored on more problems should have learn more than those who
tutored on fewer problems. To test this hypothesis, we first analyzed if passing S1 students simply tutored more problems than failing S1 students. The
average number of problems tutored was 28.9±14.6 for passing S1 students
and 20.9±12.2 for failing S1 students. The difference was not statistically
significant. There was no notable difference in the number of problems tutored between passing and failing S1 students.
4.4.3. Resource usage
Did passing S1 students self-learn the materials by using resources more
than failing S1 students? When counting the number of times students referred to worked-out examples, there was actually a notable difference. The
passing S1 students referred to worked-out examples more than failing S1
students; MPassing S1 (N=52) = 164±116 vs. MFailing S1 (N=79) = 106±94;
t(93.19) = -3.00, p < 0.01.
Furthermore, passing S1 students copied more example problems for tutoring than failing S1 students; MPassing S1 = 2.2 vs. MFailing S1 = 1.4; t(111.16)
= -3.62, p < 0.001. Even when students did not actually understand how to
solve equations, they could simply copy worked-out examples line by line to
tutor SimStudent, which should have certainly affected SimStudent’s ability
to pass the quiz.
There was also a significant correlation between the number of example
problems tutored and number of times example tab were clicked; r2=0.36,
t(133)=8.67, p < 0.001—suggesting that Philippine students were actually
switching between tutoring interface and example tabs frequently when they
were copying example problems and their solutions for tutoring.
4.4.4.

Predictor of learning

79

Since there were several factors that contributed SimStudent’s and students’ learning found in the data, we conducted a regression analysis to see
how certain factors contributed to the post-test score on the procedural skill
test. The following variables were entered in the regression model: pre-test
score on the Procedural Skill Test, total number of problems tutored, total
number of quiz items tutored, total number of examples viewed, total number of example problems tutored, accuracy of tutoring, and study.
The result showed that pre-test score, accuracy of tutoring (AT), and
study were significant predictors of post-test score (PTS) on the Procedural
Skill Test. When pre-test score was centered (C.Pre), the following regression coefficients were revealed: PST = 0.21 + 0.61*C.Pre + 0.23*AT +
0.14[if US]; r2 = 0.77, F(3, 234)=267.7, p < 0.001. Since pre-test and accuracy of tutoring are highly correlated, dropping accuracy of tutoring from
the model also showed an equally good fit: PST = 0.34 + 0.63*C.Pre +
0.34[if US]; r2 = 0.76, F(2, 245) = 399.3, p < 0.001.

5.

Discussions and Concluding Remarks

We found that the prior knowledge had a strong influence on tutor learning—if students do not have sufficient prior knowledge for tutoring, they
would not benefit from tutoring as much as students who have appropriate
prior knowledge. The regression model mentioned in the results section
shows that prior knowledge is the dominating predictor of post-test score for
the Procedural Skill Test.
Nonetheless, in the Philippines study, students who managed to have
their SimStudent pass the first quiz section (i.e., the first two quiz problems)
outperformed those who failed to do so on the post-test of the Procedural
Skill Test (albeit the small effect size) even when there was no pre-test difference between passing and failing students. Students who tutored
SimStudent better learned more. The same correlation between
SimStudent’s and students’ learning was observed in previous studies [7].
These results indicate that some students had actually learned how to tutor better SimStudent via the actual tutoring interaction. We found that, in
the Philippines study, students who managed their SimStudent to pass the
first two sections of the quiz copied worked-out examples more often than
those who failed to pass the quiz. Furthermore, those passing students reviewed the worked-out examples more often than failing students. Further
investigation would be necessary to understand how to better assist students
with low prior knowledge to learn by teaching.
Learning by teaching is a promising type of learning especially when
combined with an advanced agent technologies. Yet, there are many to understand when and how students learn by teaching and how to best facilitate
their learning with various individual differences.

80

6.

Acknowledgements

The authors thank the Ateneo Laboratory for the Learning Sciences,
Marc Lester Armenta, Regina Ira Antonette M. Geli, Victoria Keiser, Gabriel Jose G. Vitug, and Evelyn Yarzebinski. We thank the Department of Science and Technology Philippine Council for Industry, Energy, and Emerging Technology Research and Development (PCIEERD) for the grant entitled, "Development of Affect-Sensitive Interfaces" and the Engineering
Research and Development for Technology (ERDT) program for the grant
entitled, "Development of an Educational Data Mining Workbench."

7.

References

1. Chin, D., et al., Preparing students for future learning with Teachable
Agents. Educational Technology Research and Development, 2010.
58(6): p. 649-669.
2. Pareto, L., et al., A Teachable-Agent Arithmetic Game's Effects on
Mathematics Understanding, Attitude and Self-efficacy, in Proceedings
of the International Conference on Artificial Intelligence in Education,
G. Biswas, et al., Editors. 2011, Springer: Heidelberg, Berlin. p. 247255.
3. Uresti, J.A.R. and B. du Boulay, Expertise, Motivation and Teaching in
Learning Companion Systems. International Journal of Artificial
Intelligence in Education, 2004. 14(2): p. 193-231.
4. Roscoe, R.D. and M.T.H. Chi, Understanding tutor learning:
Knowledge-building and knowledge-telling in peer tutors' explanations
and questions. Review of Educational Research, 2007. 77(4): p. 534574.
5. Biswas, G., et al., Measuring Self-Regulated Learning Skills through
Social Interactions in a teachable Agent Environment. Research and
Practice in Technology Enhanced Learning, 2010: p. 123-152.
6. Matsuda, N., et al., Learning by Teaching SimStudent – An Initial
Classroom Baseline Study comparing with Cognitive Tutor, in
Proceedings of the International Conference on Artificial Intelligence in
Education, G. Biswas and S. Bull, Editors. 2011, Springer: Berlin,
Heidelberg. p. 213-221.
7. Matsuda, N., et al., Cognitive anatomy of tutor learning: Lessons
learned with SimStudent. Journal of Educational Psychology, in print.
8. Matsuda, N., et al., Studying the Effect of Tutor Learning using a
Teachable Agent that asks the Student Tutor for Explanations, in
Proceedings of the International Conference on Digital Game and
Intelligent Toy Enhanced Learning (DIGITEL 2012), M. Sugimoto, et
al., Editors. 2012, IEEE Computer Society: Los Alamitos, CA. p. 25-32.

The Affective Meta-Tutoring Project: Lessons Learned
Kurt VanLehn1, Winslow Burleson1, Sylvie Girard2,
Maria Elena Chavez-Echeagaray1, Javier Gonzalez-Sanchez1,
Yoalli Hidalgo-Pontet1, and Lishan Zhang1
1
Arizona State University, Tempe, AZ, USA
{kurt.vanlehn,winslow.burleson,mchaveze,javiergs,
yoalli.hidalgopontet,Lishan.zhang}@asu.edu
2
University of Birmingham, Birmingham, UK
s.a.girard@bham.ac.uk

Abstract. The Affective Meta-Tutoring system is comprised of (1) a tutor that
teaches system dynamics modeling, (2) a meta-tutor that teaches good strategies
for learning how to model from the tutor, and (3) an affective learning
companion that encourages students to use the learning strategy that the metatutor teaches. The affective learning companion’s messages are selected by
using physiological sensors and log data to determine the student’s affective
state. Evaluations compared the learning gains of three conditions: the tutor
alone, the tutor plus meta-tutor and the tutor, meta-tutor and affective learning
companion.
Keywords: Tutoring, meta-tutoring, learning strategies, affective learning
companion, and affective physiological sensors.

1

Introduction

A learning strategy is a method used by a student for studying a task domain and
doing exercises; a good learning strategy tends to increase the learning of students
who follow it, whereas a poor learning strategy tends to decrease learning. A learning
strategy is a kind of meta-strategy or meta-cognition. That is, it is knowledge about
knowledge acquisition. For example, when studying a worked example, a good
learning strategy is to self-explain every line of the example [1]. When working on a
tutoring system that gives hints, a good learning strategy is to ask for hints when and
only when one is unsure about what to do [2].
A perennial problem is that after students have mastered a learning strategy, they
may still choose not to use it [3]. The AMT (Affective Meta-Tutoring) project tested
whether an affective learning companion (ALC) could persuade students who were
taught a learning strategy to continue using it after instruction in the learning strategy
had ceased. The project built a system composed of four modules:
• An editor, which was used by students to take the steps needed to solve problems.
• A tutor, which taught students a problem-solving skill by giving hints and
feedback on each step as the problem is being solved.
S. Trausan-Matu et al. (Eds.): ITS 2014, LNCS 8474, pp. 84–93, 2014.
© Springer International Publishing Switzerland 2014

The Affective Meta-Tutoring Project

85

• A meta-tutor, which taught a learning strategy by giving hints and feedback about
it as the students’ used the tutor.
• An affective learning companion, having the goal of persuading students to use the
learning strategy even after the meta-tutor is turned off.
The evaluation of the system focused on students’ learning gains. We
hypothesized that when ranked by learning gains, the three conditions we studied
would exhibit this pattern:
tutor < meta-tutor + tutor < ALC + meta-tutor + tutor
We also tested whether students instructed with the affective pedagogical agent
persisted in using the learning strategy when the meta-tutoring ceased.
This paper summarizes the AMT system and its evaluation, and concludes by
discussing similar work. Many details are suppressed in order to keep the paper short,
but can be found in the project publication referenced herein.

2

The Task Domain: System Dynamics Modeling

Recent standards for K-12 science and math education have emphasized the
importance of teaching students to engage in modeling [4, 5]. Although “modeling”
can mean many different things [6], we are interested in teaching students to construct
models of systems that change over time (dynamic systems) where the model is
expressed in a graphical language that is equivalent to sets of ordinary temporal
differential equations.
Stella (www.iseesystems.com), Vensim (vensim.com),
Powersim (hwww.powersim.com) and similar graphical model editors are now widely
used in education as well as industry and science. Much is known about students’
difficulties with “systems thinking” and how it improves when students learn how to
construct models [6]. The practical importance and strong research base motivated
our choice of task domain.
However, even with kid-friendly editors [7], students still require a long time (tens
of hours) to acquire even minimal competence in the task. Most science and math
classes cannot afford to dedicate this amount of time to learning a modeling tool, so
this path to deeper understanding of systems, too often, remains closed. One of the
long-term practical goals of this work is to reduce the time-to-mastery from tens of
hours to just an hour or two.

3

The AMT System

This section introduces the main parts of the AMT system: the editor, tutor, metatutor and ALC.
3.1

The Model Editor

The model editor had two tabs. One presented the problem to be solved, such as:

86

K. VanLehn et al.

A bottle initially holds 100 bacteria. They grow quickly. On average, 40% of
the bacteria reproduce each hour, each creating one new bacterium. Graph
the number of bacteria in
n the bottle each hour.
The second tab was for co
onstructing the model, which was done by creating noodes
(see Figure 1). Each node represented a quantity. A node was defined in 3 steeps,
each achieved by filling ou
ut a form. The first step in defining a node was selectinng a
quantity from a large menu
u that included both relevant and irrelevant quantities. T
The
second step (which was acctually split into two forms) required qualitative plannning
and consisted of deciding whether the quantity was a numerical constant (e.g., the
bacteria birthrate is 0.4), a quantity that was a function of other quantities (e.g., the
number of bacteria births per hour is a function of the bacteria birthrate and the
current bacteria population)), or a quantity that changed by a specified amount per uunit
time (e.g., bacteria populattion increased each hour by the number of bacteria birrths
per hour). The third step was
w stating a specific formula for calculating the quanntity
represented by the node.

Fig. 1. Model for the
t bacteria problem, and graph of node “population”

When students had finisshed constructing a model, they could click on a buttonn to
execute it. This added a grraph to each node, which students could see by clickingg on
the node (See Figure 1). Even
E
if the tutoring system was turned off, students w
were
given feedback on the corrrectness of their graphs. They could see both their grraph
and the correct graph. Thee little “g” circle inside the node icons (see Figure 1) w
was
green if the students’ graph
h matched the correct graph and red otherwise. Studeents
could go on to the next prob
blem only when all the nodes’ graphs were correct.
3.2

The Tutor

d on, students could get minimal feedback and bottom out
When the tutor was turned
hints. When they were filliing out forms, the student could click on a button labellled
“Check.” This would colo
or the student’s entries either green for correct or red for
incorrect. The color codin
ng comprised minimal feedback on correctness. Studeents
could also click on a button
n labelled “Solve it for me.” It would finish filling in the
form, but would also color the entries yellow. This comprised a bottom-out hint. In
order to discourage overusee of the Solve-it-for-me button, when the student finished

The Affective Meta-Tutoring Project

87

editing a node, the status of their work was visible as colors on the little circles inside
the node icons (“i” means input; “c” means calculation).
When the tutor was turned off, its feedback and hints were disabled. In particular,
the Solve-it-for-me button was always disabled, and the Check button was disabled on
all forms except the first one. The Check button was enabled during the first step
because the system needed to know which node the student was trying to define so
that it could associate a correct graph with the node.
3.3

The Meta-Tutor

When the meta-tutor was turned off, students tended to first define nodes for all the
numbers in the problem statement, even if the numbers were irrelevant. Next, they
tried to guess definitions of more nodes using keywords such as “initially,” “increase”
or “altogether.” Sometimes they used methodical guessing. Indeed, some students
seldom looked at the tab containing the text of the problem. These represent a few of
the practices called “shallow modeling” in the literature [6]. The purpose of the metatutor is to prevent shallow modelling and encourage deep, thoughtful modeling.
Inspired by the success of the Pyrenees meta-tutor [8], our meta-tutor explicitly
taught students a general goal decomposition method. For the students’ benefit, we
called it the Target Node Strategy and described it as follows:
1. Pick the quantity that the problem asks you to graph, create a node for it, and call it
the target node.
2. Define the target node completely. If the node needs some inputs that haven’t been
defined yet, create those nodes but don’t bother filling them in yet. Return to
working on the target node, and don’t stop until it’s finished.
3. When the target node is finished, if there are nodes that have been created but not
defined, then pick any of them as the new target node, and go to step 2. If every
node has been defined, then the model is complete and you can execute it.
When the meta-tutor was on, it required the student to follow the Target Node
Strategy. It also complained if the students overused the Solve-it-for-me or Check
buttons, just as other meta-tutors do [9]. The meta-tutor also advised students on how
to debug models (e.g., if several nodes have incorrect graphs, examine first those
whose input nodes have correct graphs). We use the term “meta-strategy” to refer to
this whole collection of strategic advice about how to use the tutor and the editor.
3.4

The Affective Learning Companion (ALC)

The main job of the ALC was to persuade students that the meta-strategy was worth
their time and effort, and thus they should use it frequently not only when the metatutor was nagging them, but also when the meta-tutor and the ALC were turned off.
To achieve this persuasion, we used both affect-based and motivation-based designs
for the agent and its behavior. These designs are discussed in the order in which they
were encountered by the student.

88

K. VanLehn et al.

Following Dweck and others [10], all students began their training by reading a
brief text introducing the “mind is a muscle” paradigm: the more you exercise your
mind, the more competent you become. The ALC often referred to this concept,
whereas the non-ALC interventions never mentioned it again.
After reading the “mind is a muscle” passage, students in the ALC condition first
encountered the agent. The agent’s appearance and initial behavior were designed to
help establish rapport with the student. Following Gulz [11], the agent was a cartoon
of a human. Following Arroyo et al. [12], its gender matched the gender of the
student. Given the mixed results of D’Mello and Graesser [13], the agent display a
fixed neutral expression. Following Gulz et al. [14], the agent introduced him/herself,
and engaged the student in light conversation about the student’s interests. The
agent’s dialogue turns were text, and the student’s turns were selected from menus.
The student’s next activity was to study a series of PowerPoint slides interwoven
with simple exercises. These taught the basics of modeling and the user interface.
This activity was the same for both the ALC intervention and the non-ALC
intervention, and the agent was absent during it.
When the student had finished the introduction and was about to begin problem
solving with the tutor, the ALC appeared and expressed enthusiasm about the
upcoming challenges. It also reminded the student that the “mind is a muscle.”
Once the student began solving a problem, the ALC “spoke” via a pop-up text
approximately once a minute. If the student was practicing deep modeling frequently,
then the agent remained silent.
When the agent “spoke,” its message was selected based on log data and
physiological sensor data that were interpreted by machine-learned models. The
sensors were a facial expression camera and a posture-sensing chair. The sensor data
were cleaned, synched and input to a regression model that predicted the student’s
emotional state. The emotional state and the output of the log data detectors drove a
decision tree that selected one of the following 7 categories, whose message was then
presented to the student:
1. Good Modeling: Students exhibit frequent deep modeling behaviors and low
variation among affective states. ALC: “You really nailed it efficiently! It seems
like you are using the strategy and that all your efforts are helping you to make
strong connections in your brain. Nice work!”
2. Engaged: Students make few errors and show high level of excitement and
confidence. ALC: “That’s it! By spending time and effort verifying your answers
and planning ahead as you use the strategy, your brain is creating more connections
that will help you in your future modeling.”
3. New Understanding: Students show some shallow behaviors without making too
many errors, and some may show some frustration. ALC: “You’re getting good at
this. Planning ahead is the way to go. I can almost see the connections forming in
your brain.”
4. Inconsistent: Students make many shallow behaviors and show high level of
frustration. ALC: “Remember to stay focused and use the strategy and your plan.
Your actions seem to be inconsistent with the plan you picked earlier. If you

The Affective Meta-Tutoring Project

89

planned on having a <fixed value> node, then why are you trying to create a
<function>? It’s OK; sometimes it can be confusing; just remember to always try
to do your best…”
5. Guessing: Students enter several answers before getting one correct, perform many
shallow behaviors, and show low level of excitement: “Sometimes one must guess.
But even if you’ve been guessing recently, try to figure out why the response that
got green was correct. That way you can get there faster next time without
guessing.”
6. Fluttering, Confused, Lost: Students make many errors. While the student
sometimes refers to instructions and the problem, the student only uses these
features when stuck, not when planning the modeling activity. ALC: “You seem a
little lost. Sometimes these activities can be confusing. Do you think you can go
back to the strategy and use it to make a plan about the best way to spend your
effort? This will probably help you make progress.”
7. Boredom: Students make some errors and show consistently low level of interest.
ALC: “If this activity seems boring, why not turn it into a game to make it more
fun? For instance, do you think you can finish a node while getting green on your
first try at every tab?”
The ALC messages quoted above were the ones presented initially. If the same
message needed to be presented later, one of 10 short versions was presented instead.
When students finished a problem, a rectangular bar appeared alongside the agent
in order to reify the student’s meta-cognitive performance, following [15, 16]. The
bar was divided into three segments that displayed the proportion of student actions
that were deep (green), shallow (red) or neutral (yellow). The modeling depth bar
was intended to shift students’ motivational focus from correctness (the red/green
coding of the tutor) to effort (the red/green coding of the bar). After the ALC
explained what the bar meant, it presented a message based on a 6-way categorization
that took into account the student’s behavior throughout the solving of the problem
[17]. The student was then prompted to begin the next problem.
When the training phase was completed, the ALC appeared for the last time and
encouraged the student to continue to use deep modeling practice in the forthcoming
transfer phase.
The ALC’s messages turned out to be mostly motivational and meta-cognitive.
The messages were designed “bottom up” by experienced human coders who were
familiar with the affect and motivation literature. The messages were tailored to fit
the student’s state as the coders interpreted it rather than to cleave precisely to one
affect/motivation theory or another.
However, the ALC did choose which message to present on the basis of the
student’s affective state, as detected by the sensors. As advocated by [18], some
messages probably work best if they were delivered only in some affective states.
For instance, criticizing the students’ effort when they are frustrated may cause
disengagement, but the same message delivered to a bored student might have a better
chance at re-engaging them.

90

4

K. VanLehn et al.

Evaluation

This section reports the outcomes (main results) of our experiments evaluating the
meta-tutor (studies 3, 4 and 5) and the ALC (studies 6 and 7). Studies 1 and 2 were
pilot studies that involved only the editor and the tutor, and will not be discussed here.
4.1

Methods

Procedure: All five experiments used the same procedure. There were two phases: A
75 minute training phase and a 30 minute transfer phase. During the training phase,
all students studied PowerPoint slides which introduced them to system dynamics
modeling, the model editor and the Target Node Strategy. They also engaged in a
series of training problems of increasing complexity. The Check and Solve-it-for-me
buttons were available to give them feedback and demonstrations, respectively, on
each step in constructing a model. During the transfer phase, the tutor, meta-tutor and
ALC were all turned off. Thus, the transfer phase allowed students to display both
competence at system dynamics modeling and the Target Node Strategy.
Design: Students were randomly assigned to treatment groups. The treatment
manipulation occurred only during the training phase and only while the students
were solving problems. There were three treatment conditions: tutor alone; tutor +
meta-tutor and tutor + meta-tutor + ALC.
Measures: The studies used basically the same measures, although there were
improvements as the studies progressed. There were three types of measures, which
were all calculated from log data:
• Efficiency: How much modeling were students able to complete in a fixed period
of time?
• Error rate: How many mistakes did students make when defining a node? How
often did they get green (correct) the first time they clicked the Check button?
• Modeling depth: Did students use deep or shallow modeling practices?
─ How frequently did students guess or otherwise “game the system?”
─ How frequently were their actions consistent with the Target Node Strategy?
─ How frequently did students refer to the problem statement?
─ How frequently did students refer back to the introductory PowerPoint slides?
─ How many irrelevant nodes did students create?
─ How many episodes were classified as deep by the log data detectors?
Participants: Because we aimed at evaluating affective interventions, we conducted
the studies (except 6) in a classroom context, namely ASU summer schools for high
school students. ASU summer school classes always had between 40 and 50 students
each. Background questionnaires indicated that students varied in their mathematical
preparation from Algebra I to Calculus. We attempted to deal with the high incoming
variance using co-variants (studies 3, 4 and 5) and stratified sampling (studies 6 and 7).

The Affective Meta-Tutoring Project

91

Nonetheless, the high variance in incoming attributes and the limited number of
participants resulted in our studies being underpowered, which partly explains why
several tests presented below turned out to be statistically unreliable.
4.2

Results of Comparing Meta-Tutor + Tutor to Tutor Alone

Studies 3, 4 and 5, which are fully described in [19], evaluate the impact of metatutoring using two treatment groups. The experimental group had both the meta-tutor
and tutor turned on, whereas the control group had the meta-tutor turned off leaving
only the tutor active. Our three main hypotheses and their evaluations follow.
During the training phase, meta-tutoring should improve students’ efficiency, error
rate and depth of modeling. In all three studies, on almost all measures, the results
were in the expected direction, but the differences were statistically reliable only
about half the time. The results for efficiency were weakest, probably because
guessing often took less time than thinking hard. On the whole, we conclude that
meta-tutoring probably did improve training phase performance.
During the transfer phase, efficiency and error rate should be better for the metatutored group because they should have acquired more skill in modeling during the
training phase. Although there were weak trends in the expected direction, only one
of the depth measures showed a statistically reliable difference. We conclude that
meta-tutoring did not improve transfer phase performance enough to be detectable.
During the transfer phase, the meta-tutored group should not use deep model
practices more frequently than the control group because the meta-tutor merely nags;
it is the job of the ALC to persuade students to keep using deep modeling practices.
This hypothesis predicts a null result, which was observed with all measures in all
experiments, but the low power prevents drawing any conclusion from the null
results.
4.3

Results of Adding the ALC to the Meta-Tutor + Tutor

Study 6 evaluated a preliminary version of the ALC that only intervened between
modeling tasks and was not driven by the physiological sensors. None of the Study 6
measures showed benefits for this preliminary ALC compared to using the system
without the ALC. Unlike the other studies, this was a lab study with university
students intended mostly to collect data for calibrating the physiological sensors.
Study 7 compared the complete system to the same system with the ALC turned
off. Our findings were:
• During the training phase, the ALC group was better than the non-ALC group on
all measures, although the differences were reliable on only half the measures.
• During the transfer phase, the two groups tied on all error rate and efficiency
measures, suggesting that they both learned the same amount during training.
• Also during the transfer phase, the ALC group was not different from the non-ALC
group in this use of deep modeling practices.
Our interpretation of the results of Study 7 is that the ALC probably acted like an
improved meta-tutor. That is, during training, it caused students to use deeper

92

K. VanLehn et al.

modeling strategies, which increased their efficiency and decreased their error rates,
but did not apparently affect their learning very much, because their advantage over
the comparison group did not persist into the transfer phase. Although the AMT
project has made many contributions, this finding is perhaps the main result of the
project.

5

Discussion

While our studies were being conducted, other related studies were being done. There
are now 12 studies in the literature besides our own where an ALC acted somewhat
like ours [20], and only 4 had reliable main effects. Of them, 3 studies used
memorization tasks, and the fourth study confounded instructional information with
the affective intervention. On the other hand, all 8 studies with null effects used
complex tasks, as did our studies. It is tempting to hypothesize that ALCs work best
with simple, short tasks perhaps because there are more frequent opportunities for
interacting with the ALC between tasks.
Overall, the good news is that we have discovered improvements to meta-tutoring
that increase the frequency of deep modeling practices when the meta-tutoring is
operating. This is important because modeling is becoming a more central part of the
math and science standards, and students have strong tendencies to use shallow
modeling practices. Unfortunately, we have not yet found a way to get this improved
performance to persist when the meta-tutoring is turned off.
Another piece of good news is that students were able to achieve adequate
competence in constructing system dynamics models with only 75 minutes of
training. This is nearly an order of magnitude faster than earlier work with high
school students [6].
In one key respect, the ALC’s intervention could be improved. Our hypothesis
was that using the affect sensors and detectors would allow the ALC’s messages to be
presented at emotionally optimal times. However, we did not actually vary the time
of the messages enough. This would be a good topic for future work.
Acknowledgements. This material is based upon work supported by the National
Science Foundation under Grant No. 0910221.

References
1. Fonseca, B., Chi, M.T.H.: The self-explanation effect: A constructive learning activity. In:
Mayer, R.E., Alexander, P. (eds.) The Handbook of Research on Learning and Instruction,
pp. 296–321. Routledge, New York (2011)
2. Aleven, V., et al.: Help seeking and help design in interactive learning environments.
Review of Educational Research 73(2), 277–320 (2003)
3. Hattie, J., Biggs, J., Purdie, N.: Effects of learning skills interventions on student learning:
A meta-analysis of findings. Review of Educational Research 66, 99–136 (1996)
4. National, R.C.: A Framework for K-12 Science Education: Practices, Crosscutting
concepts, and Core Ideas. National Academies Press, Washington (2012)

The Affective Meta-Tutoring Project

93

5. CCSSO, The Common Core State Standards for Mathematics (2011),
http://www.corestandards.org (October 31, 2011)
6. VanLehn, K.: Model construction as a learning activity: A design space and review.
Interactive Learning Environments 21(4), 371–413 (2013)
7. Metcalf, S.J., Krajcik, J., Soloway, E.: Model-It: A design retrospective. In: Jacobson,
M.J., Kozma, R.B. (eds.) Innovations in Science and Mathematics Education: Advanced
Designs for Technologies of Learning, pp. 77–115 (2000)
8. Chi, M., VanLehn, K.: Meta-cognitive strategy instruction in intelligent tutoring systems:
How, when and why. Journal of Educational Technology and Society 13(1), 25–39 (2010)
9. Roll, I., et al.: Improving students’ help-seeking skills using metacognitive feedback in an
intelligent tutoring system. Learning and Instruction, 267–280 (2011)
10. Dweck, C.S., Leggett, E.L.: A social-cognitive approach to motivation and personality.
Psychological Review 95(2), 256–273 (1988)
11. Gulz, A.: Benefits of virtual characters in computer-based learning environments: Claims
and evidence. International Journal of Artificial Intelligence and Education 14(3), 313–334
(2004)
12. Arroyo, I., et al.: The impact of animated pedagogical agents on girls’ and boys’ emotions,
attitudes, behaviors and learning. In: International Conference on Advanced Learning
Technologies (ICALT 2011), Athens, Georgia (2011)
13. D’Mello, S., Lehman, B., Sullins, J., Daigle, R., Combs, R., Vogt, K., Perkins, L.,
Graesser, A.: A time for emoting: When affect-sensitivity is and isn’t effective at
promoting deep learning. In: Aleven, V., Kay, J., Mostow, J. (eds.) ITS 2010, Part I.
LNCS, vol. 6094, pp. 245–254. Springer, Heidelberg (2010)
14. Gulz, A., Haake, M., Silvervarg, A.: Extending a teachable agent with a social
conversation module – Effects on student experiences and learning. In: Biswas, G., Bull,
S., Kay, J., Mitrovic, A. (eds.) AIED 2011. LNCS, vol. 6738, pp. 106–114. Springer,
Heidelberg (2011)
15. Arroyo, I., et al.: Repairing disengagement with non-invasive interventions. In: Luckin, R.,
Koedinger, K.R., Greer, J. (eds.) Artificial Intelligence in Education, pp. 195–202. IOS
Press, Amsterdam (2007)
16. Walonoski, J.A., Heffernan, N.T.: Prevention of off-task gaming behavior in intelligent
tutoring systems. In: Ikeda, M., Ashley, K.D., Chan, T.-W. (eds.) ITS 2006. LNCS,
vol. 4053, pp. 722–724. Springer, Heidelberg (2006)
17. Girard, S., Chavez-Echeagaray, M.E., Gonzalez-Sanchez, J., Hidalgo-Pontet, Y., Zhang,
L., Burleson, W., VanLehn, K.: Defining the behavior of an affective learning companion
in the affective meta-tutor project. In: Lane, H.C., Yacef, K., Mostow, J., Pavlik, P. (eds.)
AIED 2013. LNCS, vol. 7926, pp. 21–30. Springer, Heidelberg (2013)
18. D’Mello, S.K., Graesser, A.C.: Dynamics of affective states during complex learning.
Learning and Instruction 22, 145–157 (2012)
19. Zhang, L., et al.: Evaluation of a meta-tutor for constructing models of dynamic systems.
Computers & Education (in press)
20. Girard, S., et al.: How can Affect be used to improve the Learning outcomes of Interactive
Instructional Systems? ( in prep.)

Computers & Education 75 (2014) 196–217

Contents lists available at ScienceDirect

Computers & Education
journal homepage: www.elsevier.com/locate/compedu

Evaluation of a meta-tutor for constructing models of
dynamic systems
Lishan Zhang*, Kurt VanLehn, Sylvie Girard, Winslow Burleson,
Maria Elena Chavez-Echeagaray, Javier Gonzalez-Sanchez, Yoalli Hidalgo-Pontet
Arizona State University, Computing, Informatics, and Decision Systems Engineering, Tempe, AZ 85281, USA

a r t i c l e i n f o

a b s t r a c t

Article history:
Received 5 September 2013
Received in revised form
17 February 2014
Accepted 25 February 2014
Available online 13 March 2014

Modelling is an important skill to acquire, but it is not an easy one for students to learn. Existing
instructional technology has had limited success in teaching modelling. We have applied a recently
developed technology, meta-tutoring, to address the important problem of teaching model construction.
More speciﬁcally, we have developed and evaluated a system that has two parts, a tutor and a meta-tutor.
The tutor is a simple step-based tutoring system that can give correct/incorrect feedback on student’s
steps and can demonstrate steps for students when asked. Because deep modelling requires difﬁcult
analyses of the quantitative relationships in a given system, we expected, and found, that students
tended to avoid deep modelling by abusing the tutor’s help. In order to increase the frequency of deep
modelling, we added a meta-tutor that coached students to follow a learning strategy that decomposed
the overall modelling problem into a series of “atomic” modelling problems. We conducted three experiments to test the effectiveness of the meta-tutor. The results indicate that students who studied with
meta-tutor did indeed engage in more deep modelling practices. However, when the meta-tutor and
tutor were turned off, students tended to revert to shallow modelling. Thus, the next stage of the
research is to add an affective agent that will try to persuade students to persist in using the taught
strategies even when the meta-tutoring and tutoring have ceased.
Ó 2014 Elsevier Ltd. All rights reserved.

Keywords:
Meta-tutor
Gaming the system
Intelligent tutoring systems
Modelling
Learning strategies

1. Introduction
This paper reports progress on two research problems: (1) teaching students how to construct mathematical models of dynamic systems,
and (2) teaching students to use effective learning strategies. Both research problems have long histories, which are covered in the next few
sections.
1.1. A brief history of educational uses of system dynamics modelling
There are two distinct reasons why students should learn model construction. First, modelling is an important cognitive skill in itself. The
Common Core State Standards for Mathematics (CCSSO, 2011) considers modelling to be one of 7 essential mathematical practices that
should be taught at all grade levels. The Next Gen standards for science instruction (National, 2012) also have 7 strands that are threaded
throughout the standards, and modelling is one of them.
Second, modelling is widely believed to be an important method for learning domain knowledge. For instance, modelling has been
claimed to help in achieving a deep understanding of scientiﬁc systems, economic systems and other systems (Chin et al., 2010; Metcalf,
Krajcik, & Soloway, 2000; Stratford, 1997), removing misconceptions and making of conceptual changes (Booth Sweeney & Sterman,
2000; Bredeweg & Forbus, 2003; Hestenes, 2007; Lee, Jonassen, & Teo, 2011; Mandinach & Cline, 1994b; Wilensky, 2003; Wilensky &
* Corresponding author.
E-mail addresses: lishan.zhang@asu.edu (L. Zhang), kurt.vanlehn@asu.edu (K. VanLehn), sylvie.girard@asu.edu (S. Girard), winslow.burleson@asu.edu (W. Burleson),
mchaveze@asu.edu (M.E. Chavez-Echeagaray), javiergs@asu.edu (J. Gonzalez-Sanchez), yhidalgo@asu.edu (Y. Hidalgo-Pontet).
http://dx.doi.org/10.1016/j.compedu.2014.02.015
0360-1315/Ó 2014 Elsevier Ltd. All rights reserved.

L. Zhang et al. / Computers & Education 75 (2014) 196–217

197

Reisman, 2006), understanding the epistemology of models in science (Treagust, Chittleborough, & Mamiala, 2002), and developing intuitions, predilections and skills at understanding complex phenomena in general (Hogan & Thomas, 2001; Mandinach & Cline, 1994a;
Schecker, 1993; Steed, 1992).
In short, modelling is both an important cognitive skill and a potentially powerful means of learning many topics. That is, it is both an end
goal and a means to other end goals.
The modelling activity addressed here is traditionally called system dynamics modelling (see Collins and Ferguson (1993) for a particularly comprehensive taxonomy of modelling). The model is comprised of real-valued variables that are constrained by temporal differential
equations. The variables denote quantities in the system whose values change over time. The job of the model is to predict those changing
values as accurately as parsimony allows.
There is a long history of using system dynamics model construction as in instructional activity. According to the oral history of the
System Dynamics Society (http://www.systemdynamics.org/oral-history/), system dynamics began to be used for university instruction
around 1957 with Jay Forrester’s formulation of system dynamics for teaching management. When a graphical language, Stella, became
available (Richmond, 1985), instructional usage dramatically increased and extended to high school. Many early Stella projects trained
teachers in modelling and let them invent activities (Mandinach & Cline, 1994a; Zaraza & Fisher, 1997).
After years of experience by hundreds of teachers, observers began to report that getting students to actually construct models took so
much class time that most teachers used Stella only for model exploration activities, wherein students were given a model and were asked
to observe how graphs of the variables values changed as the students manipulated parameters of the model (Alessi, 2000; Doerr, 1996;
Mandinach & Cline, 1994b; Stratford, 1997).
Laboratory studies conﬁrmed the observers’ reports about both the length of time required for model construction and the importance of
model construction. For example, Hashem and Mioduser (2011) found that students who constructed NetLogo models learned more about
emergence, self-organization and other complex system concepts than students who explored NetLogo models that were given to them. The
comparison took place during two 90-min lessons on complex systems, which were ﬂanked by a pre-test and a post-test. However, prior to
the pre-test, it took only 2 h to train the model exploration group whereas it took 48 h to train the model construction group. In a review of
the modelling literature, VanLehn (2013) found that the only experiments that produced reliable positive results for model construction also
devoted at least 5 h to training the students before the main lessons.
This history motivates the speciﬁc research problem addressed here: How can we speed up students’ acquisition of skill in constructing
system dynamics models?
Many methods for accelerating the acquisition of skill in model construction have been implemented, but only a few have been
compared to baseline versions of the model construction activity in order to test their effectiveness (see VanLehn, 2013, for a review). Of
those that have been evaluated, one form of scaffolding has shown considerable promise: The use of feedback and hints on student’s
steps in constructing the model. A whole model is usually composed of many parts (e.g., nodes, links, equations, labels, icons, numbers,
etc.) which the student enters one at a time. Entering such a part is called a “step”. Systems that give feedback and hints on steps are
called step-based tutoring systems (VanLehn, 2006). Step-based tutoring systems have been used for a wide variety of tasks besides
model construction, and appear to be almost as effective as human tutors (VanLehn et al., 2011). Thus, this project decided early on to
build a step-based tutoring system for system dynamics modelling in the hope that it would accelerate students’ acquisition of
modelling skill.
1.2. Learning strategies research
A learning strategy is a process, procedure or method that meets two criteria (Donker, de Boer, Kostons, Dignath van Ewijk, & van der
Werf, 2014): (1) Students can use the strategy when studying, but it is not required by the material that they are studying. (2) Using the
learning strategy is believed to affect the student’s learning. A good learning strategy is thought to improve students’ learning, while a poor
learning strategy is thought to harm the students’ learning. When used without modiﬁcation, “learning strategy” generally means a good
learning strategy. Some examples are:
 When memorizing facts, a good learning strategy is to construct a mental image and associate each fact with a part of the image.
 When studying an example, a good learning strategy (called self-explanation) is to explain each step in the example to yourself, asking
“Why is this true? Why did the author include this step?”
 When reading a text, a good learning strategy is to reﬂect afterwards on what you have learned.
 When reading a text, a poor learning strategy is to ignore words or passages that you don’t understand.
Learning strategies have been studied for decades, and comprehensive meta-analytic reviews exist (Donker et al., 2014; Hattie, Biggs, &
Purdie, 1996). Some of the main ﬁndings are:
A. Students often exhibit poor learning strategies.
B. Good learning strategies can be taught, often with little difﬁculty.
C. When students use the taught learning strategies, their domain learning often increases compared to students who are not taught to use
the learning strategies.
D. When instruction in the learning strategy includes meta-cognitive and motivational components, students can often be prevented from
reverting to poor learning strategies when the instruction ceases.
E. Speciﬁc learning strategies often have larger effect sizes than general purpose ones.
These ﬁndings mean that research on learning strategies is intimately linked to advances in instruction. Whenever a new instructional
method or subject matter is developed, there are likely to be poor learning strategies that are speciﬁc to it (ﬁnding A) as well as speciﬁc
good learning strategies that are likely to be effective (ﬁnding E) and easily taught (B). The key question is whether they are effective for

198

L. Zhang et al. / Computers & Education 75 (2014) 196–217

domain learning (C) and students can be persuaded to continue using them after being taught (D). We have developed a step-based
tutoring system for teaching students how to construct models, so it is likely that for this new form of instruction, students tend to
exhibit poor learning strategies, but that good strategies can be easily taught; the main questions are whether the good strategies really
are effective at increasing domain learning and whether students can be taught to persist in using them. These are the main research
questions addressed here.
The next introductory section focuses on reviewing projects that are similar to the one described here. However, their relationship to
learning strategies can be a bit hard to see, so a few prefatory remarks may be helpful.
Because this paper is concerned with teaching students how to construct models, and constructing a model is a kind of problem solving,
it is worth clarifying how learning strategies differ from problem solving strategies. When students are taught an effective strategy for
solving problems, the strategy qualiﬁes as either a learning strategy, a problem solving strategy or both depending on the instructional
objectives.
 If the objective is for students to solve problems well, then a strategy for solving problems counts as domain knowledge and is called a
problem solving strategy. In the jargon of students, it’s on the test.
 On the other hand, when problem solving is done only to give student practice in applying certain concepts and principles and the
problem solving itself is not an instructional objective, then the strategy counts as a learning strategy because it is optional and yet it
probably impacts the students’ learning of the concepts and principles. In the jargon of students, it is not on the test.
 As pointed out earlier, the process of constructing models is both an instructional objective (especially in math classes) and a means for
acquiring domain knowledge (especially in science classes). Thus, an effective strategy for constructing models counts as both a problem
solving strategy in some cases (it’s on the test) and a learning strategy in other cases (it’s not on the test).
For example, suppose the instructional objective is to learn which botanical features go with which plants, and the instruction involves
arranging cards labelled with plants and features. If this matching activity does not appear on the exams, then teaching students a
methodical method for arranging the cards counts as a learning strategy even though it is also a problem solving strategy. From the students’
point of view, what matters is whether the strategy is “on the test.” That is, if a problem solving strategy is an instructional objective and is
part of a test or other assessment, then students have a different attitude towards it than a strategy that is merely helpful for learning the
material that will be on the test. As we discuss various methods for teaching model construction, it is important to note whether students
believe a taught strategy is required or merely helpful.
Interactive instructional systems, such as the tutoring system described herein, have their own unique learning strategies. Here are some
examples:
1. Teachers often complain that their students would rather click than think. Actuating buttons, menus, etc. without thinking or even
reading the rest of the screen is a poor learning strategy.
2. When a tutoring system gives immediate feedback on the correctness of a student’s entry, then students often guess instead of think.
That is, they rapidly revise and resubmit an incorrect entry until the tutoring system says that it is correct.
3. If the tutoring system gives a sequence of hints that get gradually mores speciﬁc until they tell the student exactly what to do,
then students abuse the hint sequences by clicking rapidly on the Hint button until they get the ﬁnal hint that tells them what
to do.
4. A good learning strategy for tutoring systems is to ask for a hint only when you need one (Aleven, McLaren, Roll, & Koedinger, 2004).
5. After asking for hints from a tutoring system and ﬁnally making a correct entry, a good learning strategy is to reﬂect on why it is correct
(self-explanation) and whether the hint makes sense (Shih, Koedinger, & Scheines, 2008).
6. Examples 2, 3 and 4 are help-seeking strategies (Aleven, Stahl, Schworm, Fischer, & Wallace, 2003). Examples 2 and 3 are often referred
to as “gaming the system” (Baker, Corbett, Koedinger, & Wagner, 2004).
Feedback and hints are the signature methods of tutoring, but several projects have used feedback and hints for two distinct purposes, so
let us distinguish them as follows:
 Let domain knowledge refer to what students are supposed to learn. It is typically measured with a post-test and sometimes a pre-test.
 When the hints address the domain and the feedback indicates whether the domain knowledge has been correctly applied, the system
is said to be tutoring and the module responsible for it is called the tutor.
 A learning strategy is an optional method for using the system (i.e., it is not domain knowledge) that is believed to increase student’s
learning of domain knowledge.
 When the feedback and hints refer only to the learning strategy and whether it is being applied correctly, the system is said to be metatutoring and the module responsible for it is called the meta-tutor.
In this paper, meta-tutoring will only means a method for teaching students a learning strategy. However, in the tutoring literature,
“meta-tutoring” is used more broadly to mean using feedback and hints to teach anything other than domain knowledge. For instance,
meta-tutoring is sometimes used to increase motivation or change beliefs about self-efﬁcacy. Du Boulay, Avramides, Luckin, MartinezMiron, and Rebolledo-Mendez (2010) propose a framework that includes many types of meta-tutoring.
Now we can turn to reviewing prior work on using learning strategies to help students learn how to construct models.
1.3. Prior work on learning strategies for model construction
Betty’s Brain (Leelawong & Biswas, 2008) was a step-based tutoring system for constructing models that also taught a learning strategy. It
could give feedback and hints on the student’s model (tutoring) or it could give feedback and hints on the way that the student was using the

L. Zhang et al. / Computers & Education 75 (2014) 196–217

199

system to create the model (meta-tutoring).1 For instance, sometimes it would not permit the student to evaluate the model with an
instructor-provide test suite (called a “quiz”) until the student had ﬁrst examined speciﬁc predictions of their model (e.g., if air temperature
goes down, what does body temperature do?). The system included some multimedia resources on the task domain, so another part of the
learning strategy was encouraging students to read them. As these examples indicate, the learning strategy was speciﬁc to the particular
instructional features provided by system. This is consistent with the ﬁndings in the learning strategies literature, which suggest that such
speciﬁcity provides better results than general learning strategies.
Three evaluations of the effectiveness of Betty’s meta-tutor were conducted (Biswas, Leelawong, Schwartz, & Vye, 2005, study 2;
Leelawong & Biswas, 2008; Tan, Biswas, & Schwartz, 2006). Their methods will be described fully here, as the experiments to be reported
later used similar methods. The Betty’s Brain experiments all had two phases, called the training phase and the transfer phase here. During
the training phase, ﬁfth-grade students worked with Betty for approximately seven 45-min sessions on constructing a model of a river
system. One group of students used Betty’s Brain with the meta-tutor turned on, and another group used the same system with the metatutor turned off. Two months later the transfer phase occurred, where all the students used Betty’s Brain with the meta-tutor turned off to
create models for the nitrogen cycle. This transfer phase was used to assess their modelling skill.
The results were roughly the same in all three studies. Using the general results on learning strategy mentioned above as a framework,
the results from the Betty’s Brain studies were:
A. Students often exhibit poor learning strategies. True of the control conditions in all three studies.
B. Good learning strategies can be taught, often with little difﬁculty. In the training phase of all three studies, meta-tutored students’
behaviour was consistent with the learning strategies.
C. When students use the taught learning strategies, their domain learning increases compared to students who are not taught to use the
learning strategies. Unfortunately, using conventional science tests of ecology concepts, there were few signiﬁcant differences between
the meta-tutored students and the students who used Betty’s Brain without meta-tutoring.2 Using four measures of model quality,
meta-tutored students’ models were better than control students’ models on only one measure.
D. When instruction in the learning strategy includes meta-cognitive and motivational components, students can often be prevented from
reverting to poor learning strategies when the instruction ceases. Students who received meta-tutoring during the training phase tended to
continue using the learning strategy in the transfer phase when the meta-tutoring was turned off.
E. Speciﬁc learning strategies often have larger effect sizes than general purpose ones. Not tested.
In short, although the meta-tutor in Betty’s Brain was successful at teaching the learning strategy and persuading students to continue
using it, the learning strategy had little impact on domain learning. More recent work has focused on ﬁnding out what behaviours
distinguish good learners from poor learners (Segedy, Kinnebrew, & Biswas, 2012a, 2012b).
The Help Tutor (Aleven et al., 2004; Roll, Aleven, McLaren, & Koedinger, 2007a, 2007b; Roll, Aleven, McLaren, & Koedinger,
2011; Roll et al., 2006) was a meta-tutor that augmented an existing step-based tutoring system, the Cognitive Geometry Tutor
(www.carnegielearning.com). Although the tutoring system did not teach students to construct models, it is included in this review of
past work because it is an often-cited representative of using meta-tutoring to improve students’ learning from step-based tutoring.
The Geometry Cognitive Tutor allowed students to ask for a hint. The ﬁrst hint was rather general, but if the student kept asking for hints,
then the last hint told the student exactly what to do. This was called the “bottom-out” hint. Students sometimes clicked rapidly on the Help
button so that they could get to the bottom-out hint. As mentioned earlier, this abuse of the help system is a kind of “gaming the system”
(Baker, Corbett, Koedinger, et al., 2004). In order to reduce the frequency of gaming the system, the Help Tutor gave students feedback and
hints on their use of the help system.
Two studies evaluated the Help Tutor. As in the Betty’s Brain studies, the Help Tutor studies had both a training phase where the metatutor was used by half the students and a transfer phase where none of the students used the meta-tutor. Using the general ﬁndings
mentioned above as a framework, the results were:
A. Students often exhibit poor learning strategies. True of the control conditions in all both studies.
B. Good learning strategies can be taught, often with little difﬁculty. In the training phase of both studies, meta-tutored students’ behaviour
was consistent with the taught strategies.
C. When students use the taught learning strategies, their domain learning increases compared to students who are not taught to use the
learning strategies. Unfortunately, students taught the learning strategy did not differ from the control group in their acquisition of
geometry knowledge.
D. When instruction in the learning strategy includes meta-cognitive and motivational components, students can often be prevented from
reverting to poor learning strategies when the instruction ceases. Students who received meta-tutoring during the training phase tended to
continue using the learning strategy in the transfer phase when the meta-tutoring was turned off, albeit with less frequency.
E. Speciﬁc learning strategies often have larger effect sizes than general purpose ones. Not tested.
Betty’s Brain and the Help Tutor provided most of their instruction via feedback and hints. A somewhat more didactic approach is to
provide forms and phases that constrain students’ modelling behaviour. For instance, the ﬁnal version of Model-It (Metcalf et al., 2000) had

1
Those familiar with Betty’s Brain might be surprised to see it described as a tutoring system because the students see the system as two agents: Betty (a teachable agent)
and Mr. Davis (a mentor). Students were told that the model they were constructing comprised the knowledge of Betty, so editing the model comprised “teaching Betty.”
When they ask Betty take a quiz, Mr. Davis gives the student feedback on the correctness of the model’s predictions which would sometimes include unsolicited hints about
ﬁsh, algae, carbon dioxide and other domain entities. Mr. Davis personiﬁes the system’s tutoring. On the other hand, meta-tutoring was done by both agents. For instance,
Betty would sometimes refuse to take a quiz, and Mr. Davis would discourage students from using trial-and-error methods.
2
In a fourth study, the meta-tutored students produced slightly better river system models than the control students (Tan, Wagster, Wu, & Biswas, 2007; Wagster, Tan,
Biswas, & Schwartz, 2007; Wagster, Tan, Wu, Biswas, & Schwartz, 2007). However, results on the other measures were not reported.

200

L. Zhang et al. / Computers & Education 75 (2014) 196–217

4 phases, which were selected by clicking on one of four buttons labelled Plan, Build, Test and Evaluate. The Build mode was the actual model
editor. The other phases presented forms to be ﬁlled in by the student. Fig. 1 shows the form for the Test phase. Students were given feedback
and hints, which they could suppress if they desired, when they attempted to bypass a suggested activity. The Carnegie Learning’s Algebra
Cognitive Tutor (www.carnegielearning.com) and the Word Problem Solving Tutor (Wheeler & Regian, 1999) also scaffolded problem
solving strategies via lightweight constraints, implemented with phases and forms. None of these meta-tutors were evaluated separately
from the rest of the system.
On the other hand, Mulder, Lazonder, de Jong, Anjewierden, and Bollen (2011) did assess the effects of a phase-based learning strategy.
The experiments used Co-Lab, a system that taught system dynamics modelling. Co-Lab was not a step-based tutoring. Instead, students
received feedback only on the accuracy of the model’s predictions. The learning strategy was to encourage students to do their model
construction in three stages. In Stage 1, they deﬁned variables and drew undirected links between variables that directly affected each other
somehow. In Stage 2, they added qualitative labels to the links indicating whether an increase in one variable caused an increase or decrease
in the other variable. In Stage 3, they added equations to the model that further speciﬁed the relationships between variables. Three versions
of the learning strategy were compared to a control version of Co-Lab that lacked phases. The three versions differed in how strictly they
enforced the learning strategy. The restricted strategy required students to achieve a certain level of success in one stage before moving to
the next. The semi-restricted version of the strategy allowed students to move from one stage to the next at will, but they were not allowed
to move backwards. The unrestricted version allowed students to move at will between stages. Compared to the control version, all three
versions of the learning strategy increased the number of correct model elements generated by students as they used the system. The three
versions were not signiﬁcantly different in productivity from each other, but there was a trend for the semi-restricted version to be better
than the other two. However, this experiment included only a training phase and not a transfer phase, and it did not assess students’ domain
knowledge with pre- and post-testing. Thus, its consistency with the 5 general ﬁndings mentioned above (A through F) cannot be determined. Nonetheless, this work is interesting because the learning strategy was taught without using a meta-tutor, and the system was not a
step-based tutoring system.
The meta-tutoring of Betty’s Brain and the Help Tutor placed only weak constraints on students’ behaviour. The phases and forms of CoLab, Model-It, the Cognitive Tutors and the Algebra Word Problem tutor placed somewhat stronger constraints on students’ behaviour. At
the far end of this progression is procedural scaffolding, which places very strong constraints on student behaviour. The basic idea of procedural scaffolding is to require students to temporarily follow a speciﬁc procedure for constructing a model. Although the procedure is not
required by the task and there are many other ways to successfully construct models, the procedure is used as a temporary scaffolding to
guide students who might otherwise be quite lost.

Fig. 1. Scaffolding for the Test mode of Model-It (Metcalf, 1999).

L. Zhang et al. / Computers & Education 75 (2014) 196–217

201

Although procedural scaffolding was ﬁrst used by (Marshall, Barthuli, Brewer, & Rose, 1989) to scaffold arithmetic story problem solving,
its beneﬁts were not evaluated. Procedural scaffolding was ﬁrst evaluated with Pyrenees (Chi & VanLehn, 2010; Vanlehn & Chi, 2012), which
required students to construct models using a version of goal reduction, which is a well-known general purpose reasoning strategy used in
artiﬁcial intelligence applications (Russell & Norvig, 2009).
Pyrenees’ procedural scaffolding was evaluated in a two-phase experiment. In the training phase, students learned to construct model of
probabilistic systems. In the transfer phase, student learned to construct models of mechanical energy systems. Using the framework
mentioned earlier, the ﬁndings were:
A. Students often exhibit poor learning strategies. True of the control conditions in both phases.
B. Good learning strategies can be taught, often with little difﬁculty. This could not be determined, because Pyrenees required students in the
experimental condition to follow the learning strategy during the training phase.
C. When students use the taught learning strategies, their domain learning increases compared to students who are not taught to use the
learning strategies. In both the training phase and the transfer phase, the experimental group acquired more domain knowledge than
control group. The effect sizes were large (d z 1.0).
D. When instruction in the learning strategy includes meta-cognitive and motivational components, students can often be prevented from
reverting to poor learning strategies when the instruction ceases. The experimental group tended to use the learning strategy during the
transfer phase on difﬁcult problems, but not on simple problems. However, even on simple problems, they did not use poor learning
strategies.
E. Speciﬁc learning strategies often have larger effect sizes than general purpose ones. Not tested.
In summary, Betty’s Brain and the Help Tutor used the standard methods of hints and feedback and their meta-tutoring succeeded in
improving students’ behaviour, but there were only weak improvements at best in their learning of the domain. Co-Lab’s learning strategy
increased performance, but the effect on learning was not measured. Pyrenees used procedural scaffolding, and its meta-tutoring caused
large improvements in both behaviour and domain learning.
1.4. Our research questions
Our research questions are the same 4 questions that our predecessors have focused one:
A. Without instruction in good learning strategies, do students often exhibit poor learning strategies?
B. Can good learning strategies be easily taught?
C. When students use the taught learning strategies, does their domain learning increase compared to students who are not taught to use
the learning strategies?
D. When instruction in the learning strategy ceases, do students revert to poor learning strategies?
Although these 4 questions about learning strategies are the central focus of the research, we are also interested in seeing if the time
required to achieve adequate competence in model construction can be reduced from 5 or more hours to 2 for fewer hours.
Because Pyrenees was arguably more successful than the other methods, we chose to use procedural scaffolding as the basic method of
teaching learning strategies. However, there is a major difference between our research problem and the one addressed by Pyrenees.
Pyrenees assumed that students had already been taught all the relevant domain principles. Indeed, the students repeatedly selected
principles from a menu. In contrast, when students construct system dynamics models, they are seldom taught domain principles in
advance. They are instead asked to infer domain relationships from multimedia resources, common sense and/or experimentation. They do
this while constructing the model. This allows modelling to be used as part of enquiry-based instruction where students construct the
domain knowledge themselves. Having to infer domain relationships while learning to model may be one of the reasons why it takes
students so long to master system dynamics model construction. Nonetheless, inferring quantities and their relationships is exactly the skill
we want students to learn.
This suggests that if we require students to follow a procedure, as Pyrenees did, then they will learn that the key to modelling is a
single non-mechanical step: ﬁguring out the mathematical relationship among a set of directly related quantities. Once they have
mastered that step, we expect that they will be able to construct models well, even if they don’t follow the procedure. Thus, our research
question is whether applying the Pyrenees approach of heavy-handed procedural scaffolding to the cognitive skill of model construction
will cause deeper, more effective modelling skills to develop during the training phase. If the tutor does work in the training phase, our
next question is whether the beneﬁts will persist in the transfer phase. To answer these two questions, we developed an instructional
system, called AMT, and conducted experiments to evaluate it. The following sections described the system, how it was implemented, and
the evaluation at last.
The name “AMT” is an acronym for the overall project, which is called the Affective Meta-Tutoring project. The ﬁrst objective of the
project is to develop and test a meta-tutor that improves student’s learning of modelling by using procedural scaffolding as well as the
traditional feedback and hints. The results of that phase of the project are reported here. The second phase of the project will be to add an
affective learning companion to the AMT system; hence the term “affective” in the project name. The second phase will be described further
in the discussion section.
2. The AMT system’s design and behaviour
This section has three parts. The ﬁrst describes the student’s task, and in particular, the graphical language in which they write models.
The second section describes the tutoring system. The third section describes the meta-tutor and the strategy that it teaches students to use.

202

L. Zhang et al. / Computers & Education 75 (2014) 196–217

2.1. Constructing models in the AMT modelling language
In order to decrease the difﬁculty of learning how to construct system dynamics models, most prior work has used graphical modelling
languages where a model consists of several types of nodes and links. These graphical languages are easier to learn than text-based languages (Löhner, Van Joolingen, & Savelsbergh, 2003). The traditional “stock and ﬂow” language has two types of links, and one of them (the
ﬂow links) acts somewhat like nodes. In pilot studies, our high school students found this confusing, so we removed the confusing type of
link.
In our modelling language, a model is a directed graph with one type of link. Each node represents both a variable and the computation
that determines the variable’s value. The inputs of that computation, which are themselves variables, are indicated by incoming links. In
Fig. 2 for example, the computation for “births” requires the values of “growth rate” and “population.” The value of a variable is a real
number that can change over time, where time is represented discretely.
There are three types of nodes:
 A ﬁxed value node represents a constant value that is directly speciﬁed in the problem. A ﬁxed value node has a diamond shape. It never
has incoming links.
 An accumulator node accumulates the values of its inputs. That is, its current value is the sum of its previous value plus its inputs. An
accumulator node has a rectangular shape and always has at least one incoming link.
 A function node’s value is an algebraic function of its inputs. A function node has a circular shape and at least one incoming link.
The students’ task is to draw a model that represents a situation that is completely described by a relatively short text. For instance, Fig. 2
is a correct model for the following problem:
Rust destroys steel and can spread quickly. Suppose that you take a large sheet of steel, such as one that might be used as the roof of the boxcar
on a train, and you put it outside in the weather. Suppose it starts with a spot of rust that is 10 square inches in area. However, each week the
rust spot gets bigger, as it grows by 30%. Therefore at the end of the ﬁrst week, the rust spot is 13 square inches in area. Graph the size of the rust
spot over 10 weeks.
Such a text contains all the information that students need. However, it sometimes contains extra quantities (e.g., the rust spot at the end
of the ﬁrst week) that are not needed in the model. Students are asked to draw only the necessary nodes, so drawing a node for the rust spot
at the end of the ﬁrst week is an indication of shallow modelling.
2.2. The tutoring system
Many tutoring systems have a sequence of feedback and hints (VanLehn, 2006). When the student makes an entry, the tutoring system
ﬁrst just tells the student whether the entry is correct or incorrect. If the student asks for help again, the system gives a general hint.
Subsequent requests for help on the same entry generate increasingly speciﬁc hints. Eventually, the ﬁnal hint (called the “bottom-out hint”)
tells the student what the correct entry is. There is some evidence that the mid-level hints are not pedagogically useful (Muldner, Burleson,
van de Sande, & VanLehn, 2011; Timms, 2007), so our tutoring system does not use them. Instead, it generates just the ﬁrst and last members
of the typical hint sequence. More speciﬁcally, it has a Check and a Give-up button. Clicking on the Check button causes the tutoring system
to give minimal feedback: It colours entries red if they are incorrect and green if they are correct. Clicking on the Give-up button causes the
tutoring system to ﬁll in entries correctly, that is, to give a bottom-out hint. This section describes the AMT model editor, pointing out where
the Check and Give-up buttons appear.
The system presents itself to the student as a large tabbed window (Fig. 3). The Model tab (shown in the ﬁgure) is for drawing models.
The Situation tab shows a textual representation of the problem, and is illustrated by a static picture. The Instructions tab contains a slide
deck that teaches students the basics of the modelling language, the user interface, the modelling process and the learning strategy. Students can access the Introduction and Situation tab at any moment during the construction of the model (cf. Section 4.4).
The Model tab has three buttons: Create Node, Run Model and Done. The Done button is disabled (grey) until the student has completed a
problem successfully (i.e. created an accurate model of the system), at which time clicking on the Done button advances the system to the
next problem.

Fig. 2. A model. The grey bubbles have been added to this ﬁgure in order to show how the value of each variable is calculated.

L. Zhang et al. / Computers & Education 75 (2014) 196–217

203

Fig. 3. The tutoring system’s screen.

The Create Node button creates a new node symbol in the model area and opens the Node Editor on it. The Node Editor (Fig. 4) is divided
into ﬁve tabs: Description, Plan, Inputs, Calculations and Graph. Students can edit all tabs with the exception of the Graph tab, where a graph
showing the evolution of the node’s value over time is generated by the tutoring system once the model is run. To create a node, the student
ﬁlls out the four tabs in order, left to right. Students can also change the information within a node by double clicking on the node shape
within the model area, which opens the Node Editor, and then selecting the tab they wish to modify.
The Description tab was engineered to facilitate grounding, where “grounding” is the process of negotiating mutual understanding of the
meaning of a term between two participants in a conversation (Clark & Brennan, 1991). In system dynamics model editors that are not used
for tutoring, such as Stella, Vensim and Powersim, users merely type in the name of the node that they want. This allows them to use names
such as “x” that do not match anything in the problem. While this is unproblematic for such editors, it makes it difﬁcult or impossible for a

Fig. 4. The node editor, showing the Description tab.

204

L. Zhang et al. / Computers & Education 75 (2014) 196–217

tutoring system to determine what quantity the student’s node denotes, and hence the tutoring system cannot determine whether the node
is deﬁned correctly. Urging the students to choose adequately precise names is only partially successful. In one study, only 78% of the node
names could be identiﬁed by the tutoring system (Bravo, van Joolingen, & de Jong, 2009). This is a case of bad grounding: the tutoring system
did not understand the meaning of 22% of the students’ terms.
On the other hand, some tutoring systems for system dynamics modelling, including the ﬁrst version of this system, provide students
with nodes that already have names but are otherwise undeﬁned (VanLehn et al., 2011). Unfortunately, students often do not pay enough
attention to the names, which can be rather similar. Students sometimes create models that are correct except that the names of two nodes,
such as “rust growth factor” and “new rust area per week”, have been switched. This is also a case of bad grounding: the student did not
understand the meaning of the system’s terms.
The Description tab of AMT, illustrated in Fig. 4, is intended to prevent bad grounding. First, the student walks through the tree in the
Description tab located in the large box at the top of the window. Clicking on a leaf in the displayed tree selects both a description and a
name for the node. Each problem has a different tree, and the tree’s contents are engineered to make the student select among subtly
different descriptions. After selecting a leaf, the student clicks on the Check button. If the selected description denotes a quantity that the
system understands, and that quantity does not already have a node deﬁned for it, then clicking on the Check button turns some boxes
green, as shown in Fig. 4. Otherwise, the boxes turn red. Students may also click on the Give-up button that ﬁlls out the tab correctly but
colours the boxes yellow. When the student exits the node editor, the node’s name, which is shown beneath the node, is highlighted by the
colour of the Description tab’s boxes. Thus, a student who had given up on the Description tab would forever see yellow highlighting on the
node’s name. This feature is intended to discourage giving up.
When the Description tab is completed correctly and its boxes are either green or yellow, then students can go on to the Plan tab. The Plan
tab lets students choose among 7 different plans (Fig. 5). The Instruction tab describes the meaning of these plans and gives an example of
each plan. After a selection is made, students may click on the Check button and see their selection coloured green (correct) or red
(incorrect). Clicking on the Give-up button causes the correct plan to be selected and coloured yellow. Unlike the other tabs, ﬁlling out the
Plan tab correctly is optional. Students can go to the Inputs tab regardless of how their plan selection is coloured, and they can skip the Plan
tab entirely if they want.
The Inputs tab (Fig. 6) allows students to indicate whether the node’s value is a ﬁxed, given constant or if it is computed from other nodes’
values. In the latter case, they click on “Inputs:” and choose some of the existing nodes as inputs, which causes links are drawn between
those nodes and the current node. When students see that the input they want is not in the list because they have not yet created a node for
it, they can click on the convenient “Create a new node” button, deﬁne the desired node using a pop-up version of the Description tab, then

Fig. 5. The plan tab.

L. Zhang et al. / Computers & Education 75 (2014) 196–217

205

Fig. 6. The Inputs tab.

return to this Input tab by closing the pop-up. As always, the Check and Give-up buttons colour the student’s choices in either red (incorrect),
green (correct) or yellow (gave up). While red choices can be revised later on, green or yellow choices cannot be changed.
When the Inputs tab is ﬁlled out correctly, students can go to the Calculation tab (Fig. 7). If they had selected “Fixed Value” on the Inputs
tab, then “has a ﬁxed value” is checked here on the Calculation tab, and so all they need to do here is enter the number that is the value of the
Fixed Value node. On the other hand, if they had selected “Inputs:” on the Inputs tab, then they must click either the “accumulates the values
of its inputs” button or the “is a function of its inputs” button. This selection determines which form appears in the lower part of the tab.
Fig. 7 shows the tab to ﬁll for the Accumulator node type on the left, and the Function node type on the right. For both the Function and
Accumulator nodes, the inputs appear initially in the “Available inputs:” box, and then move rightward into the calculation box as they are
clicked. In this fashion, students enter a calculation.
As a model is being constructed, the state of the node’s deﬁnition is indicated by little circular indicators (“i” for input, “c” for calculation
and “g” for graph) and the node’s outline (see Fig. 8). A dotted border indicates that the type of the node hasn’t been deﬁned yet. A blue
border means that the node’s deﬁnition is complete.
When all the nodes have blue borders, the student can click on the Run Model button. If there are syntactic errors in the model that
prevent running it, a pop-up window describes them. Otherwise, Run Model colours the “g” indicators on every node either green or red,
depending on whether the node’s graph is correct or not. Opening the Graph tab of a node displays both the user’s model’s graph and the
expected graph (see Fig. 9). Students can use the difference in the graphs as a clue to where the bug in the model could be found.
A model is considered completely correct when all the graphs match, in which case all the “g” indicators are green. At this point, students
can click on the Done button and go to the next problem.
This is a step-based tutoring system (VanLehn, 2006) because it can give feedback on every step taken by the student. It only gives such
feedback when the student clicks on the Check button. The feedback is minimal: just correct vs. incorrect. Absent are the usual sequences of
hints that many step-based tutoring systems have. However, the “Give-up” button implements the bottom-out hint in that it gives away
exactly what step the student should do at this point.
The AMT system has a mode, called test mode, which is used to assess students’ skill at modelling. Although students can still debug their
model by running it and seeing which graphs are correct, they cannot use the Check and Give-up buttons on any of the tabs that they ﬁll out,
with one exception. The Check button is always enabled on the Description tab. This is because the system and the student must agree on the
meaning of nodes. If the system doesn’t know which quantity is denoted by the student’s node, then it can’t know which graph is correct and
can’t colour the “g” indicators. In test mode, the Check button is enabled only on the Description tab, whereas in training mode, it is enabled
everywhere. In test mode, the Give-up button is never enabled.

206

L. Zhang et al. / Computers & Education 75 (2014) 196–217

Fig. 7. The Calculations tab.

2.3. The meta-tutor
So far, only the tutoring system has been described. It is essentially just a model editor with Check and Give-up buttons. This section
described the meta-tutor, which is the remaining module of the AMT system. This section begins by describing and motivating the learning
strategy that the meta-tutor teaches, then describes how it teaches that strategy.
Like Pyrenees (Chi & VanLehn, 2010), the meta-tutor teaches students a simple, goal reduction procedure for constructing models called
the Target Node Strategy. The basic idea is to focus on one node at a time (the target node) and complete all the tabs for this node before
working on other nodes. As students complete the target node, they may create new nodes as a side effect, but the new nodes will only be
named and not fully deﬁned. These nodes are displayed with dotted borders. Thus, when students have ﬁnished the target node, they can
pick any node that has a dotted border as the next target node, and begin working on deﬁning it. When there are no more nodes with dotted
borders, the model is complete.
There are sound reasons to think that the Target Node Strategy might help students learn more effectively, but it will take several
paragraphs to describe them. These paragraphs also illustrate the distinction between deep and shallow modelling practices.
The key steps in the Target Node Strategy are ﬁlling out the Inputs tab and the Calculation tab. These steps correspond to the moment
where students must analyze the given system information and determine the quantitative relationships between the target node’s
quantity and other quantities in the problem. That is, given a particular target quantity, students must ﬁnd a set of quantities such that the

Fig. 8. An incomplete model.

L. Zhang et al. / Computers & Education 75 (2014) 196–217

207

Fig. 9. The Graphs tab.

target quantity’s value can be computed as a simple function of their values. The whole problem of constructing a model of a system can be
decomposed into making a series of decisions like this one. One might call these decisions “atomic modelling problems”, as they cannot be
easily decomposed further.
For example, consider the system described earlier:
“Rust destroys steel and can spread quickly. Suppose that you take a large sheet of steel, such as one that might be used as the roof of the boxcar
on a train, and you put it outside in the weather. Suppose it starts with a spot of rust that is 10 square inches in area. However, each week the
rust spot gets bigger, as it grows by 30%. Therefore at the end of the ﬁrst week, the rust spot is 13 square inches in area. Graph the size of the rust
spot over 10 weeks.”
Suppose students are following the Target Node Strategy and have decided that the ﬁrst target node is the size of the rust spot. They
create a node and select “rust spot area” as its description. Now they face an atomic modelling problem. How to determine the value of rust
spot area? What we want students to think is something like, “I know the rust spot area is changing, and the value increases every week”.
This reasoning sufﬁces for ﬁlling out the Plan tab correctly. Next the students need to ﬁll out the Input tab, so they should think, “The rust
spot area is increased by the rust produced during the week”. There are no other nodes at the moment, so the student must click on “Create a
new node.” The student browses the available descriptions, and chooses the one that comes closest to the student’s idea of “rust produced
during the week.” Clicking on this description deﬁnes the node new rust area per week. The student can then select this node as an input for
the node rust spot area. Next comes the Calculation tab, which requires for the student to turn his/her qualitative understanding of the
relationship into a formal, mathematical one: the student should decide that the next value of rust spot area is its current value plus new rust
per week. This illustrates how the user interface decomposes the key reasoning into three major steps, corresponding to the Plan, Input and
Calculation tabs. These correspond roughly to the phases of Co-Lab (Mulder et al., 2011) and Model-It (Metcalf et al., 2000). This paragraph
also illustrates how the key reasoning should be done: by thinking hard about the system, its quantities and their interrelationships, and
then abstracting the mathematical relationships from them. This is the “deep” way to solve an atomic modelling problem.
However, there are shallow ways to solve the atomic modelling problem as well. When students need to create nodes in order to ﬁll out
the Inputs tab, there are only a ﬁnite number of choices: the leaves in the tree of descriptions on the Description tab. Thus, they can work
through all combinations until they have found the appropriate inputs. When the Check button is available and the problem is simple, this
can be done rapidly. For instance, the rust spot area problem’s Description tab has 7 possible descriptions, of which only 4 are legal
quantities in the problem (1 is an extra node; 3 are necessary for the model). It will not take the student long to create all the legal nodes, and

208

L. Zhang et al. / Computers & Education 75 (2014) 196–217

then test each one to see if it turns the Inputs tab green. Such extensive guessing is one “shallow” method for solving atomic modelling
problems. The Give-up button is another. Other methods involve looking for keywords (e.g., “initially”) or patterns.
Many of our students’ shallow modelling methods involve abusing the Check button or the Run Model button, so their behaviour is a
form of gaming the system (Baker et al., 2006; Baker, Corbett, & Koedinger, 2004), which is what the Help Tutor (Roll et al., 2011), Scooter the
Tutor (Baker et al., 2006), and other tutoring systems have tried to address. However, studies of modelling that did not use tutoring systems
have also noted this propensity of students to do shallow modelling (Alessi, 2000; Booth Sweeney & Sterman, 2000; Doerr, 1996; Mandinach
& Cline, 1994b; Nathan, 1998; Zaraza & Fisher, 1999). Thus, we prefer to refer to the phenomenon as “shallow modelling” rather than
“gaming the system.”
Teaching the Target Node Strategy does not require that student do deep modelling, so one might wonder why we think it could help
students model better. However, it does decompose the whole problem of modelling a system into a series of atomic modelling problems,
and even decomposes an atomic modelling problem into three major steps. Like Pyrenees, it teaches students that if they just master this
one difﬁcult but small skill (deep modelling applied to atomic modelling problems), then the rest of the problem solving is purely
mechanical.
Having described and motivated the Target Node Strategy, it is ﬁnally time to describe how the meta-tutor teaches it. First, the Introduction slides describe the strategy brieﬂy. Even students who use AMT with the meta-tutor turned off see this presentation of the strategy.
Also when the meta-tutor is turned off, students can edit any node at any time. Moreover, they can ﬁll out some of a node’s tab, then quit
editing the node and come back to it later. This freedom is removed when the meta-tutor is turned on. Students are required to correctly ﬁll
out each tab before moving on to the next, and they must ﬁll out all tabs before closing the node editor. When they are on the Description tab
and choosing which quantity to create a node for, if they choose one that would not be selected by Target Node Strategy and yet it will
eventually be include in the model, then the selection turns blue and a pop-up window says, “That quantity is in the correct model, but it is
too early to deﬁne it now.” This message is typical of others that pop up when students stray from the Target Node Strategy. In short, the
main job of the meta-tutor is simply to keep students on the one of the paths that are consistent with the Target Node Strategy.
In addition to requiring students to follow the Target Node Strategy, the meta-tutor enacts a bit more scaffolding. For completeness, this
section describes the remaining forms of help.
When the meta-tutor is turned on, it complains if students appear to be guessing too much or giving up too early, just as the Help Tutor
did. When a student clicks on the Check button on the same tab twice within 3 s and gets red both times, the meta-tutor complains that the
student is guessing. If the student clicks on the Give-up button without ﬁlling out anything and checking it, the meta-tutor complains that
the student is giving up too early.
Some of the training problems let students practice debugging an incorrect model. They present a model that will run but generates
incorrect graphs. This kind of situation occurs frequently when the system is in test model (the tutor and meta-tutor are turned off). When
the model has been run, students see that some nodes have red “g” indicators and some have green ones. They face a decision of where to
start looking for an error in the model. The Introduction slides teach all students two heuristics:
 When possible, start by examining a node that has a red “g” indicator and no incoming links from nodes with red “g” indicators. Such a
node is guaranteed to have an error inside it.
 Avoid editing nodes that have green “g” indicators, because if the graph is correct, the node’s deﬁnition is probably correct.
When the meta-tutor is on and students are working on a debugging problem, it constrains them to obey these two heuristics.
In summary, the meta-tutor actually uses three types of scaffolding: (1) it requires students to follow the Target Variable Strategy; (2) it
complains when they abuse the Check or Give-up buttons; and (3) it teaches some heuristics for locating errors in buggy models. Although
(1) is procedural scaffolding, for convenience, we refer to this collection as “the meta-strategy.”

3. System architecture and implementation
The AMT system can be divided into two parts: the tutor and the meta-tutor. The tutor’s main functionalities included drawing a model,
checking correctness and running the model. It is an example-tracing tutor (VanLehn, 2006) in that an author must provide a correct model
with each problem; the students’ work is checked against that model. On the other hand, the meta-tutor is driven by algorithms (e.g., the
Target Node Strategy) that work for any problem. The student’s actions are check against the actions selected by the meta-strategy.
Although the meta-tutor and the tutor belong to the same Java project, they don’t share objects in the memory. This made it possible to
develop the tutor and meta-tutor programme independently. It also facilitates data analysis as will be explained later. More details about the
tutor’s architecture and implementation are presented in (Gonzalez-Sanchez, Chavez-Echeagaray, VanLehn, & Burleson, 2011). This section
describes the meta-tutor only.
Between the tutor and meta-tutor, there are three kinds of communication, each with its own channel as shown in Fig. 10:
(1) Every action made by student is sent to the meta-tutor via the activity channel.
(2) Before executing certain user actions, such as closing a node, the tutor sends a message to the meta-tutor via the block channel and
meta-tutor responds to the tutor, indicating whether the action should be blocked or not.
(3) Whenever the meta-tutor detects that the student needs an unsolicited hint, it sends a command to the tutor via the message channel to
tell it to initialize the tutorial dialogue. The tutor sends back the student’s response. Based on the response, the meta-tutor tells the tutor
either to display another dialogue box or to close the dialogue.
The meta-tutor is a production system, implemented in Java using Drools Expert (http://www.jboss.org/drools/drools-expert.html). The
production system implements two major functionalities: requiring the student to follow the Target Node Strategy and discouraging
gaming. The following sections describe the implementations of each function.

L. Zhang et al. / Computers & Education 75 (2014) 196–217

Tutor

Activity
channel

Student’s actions
Query at student’s actions

Message Block
channel channel

Response[block or not]

Activity
channel

209

Metatutor

Block Message
channel channel

Tutorial dialog
Student’s answer to dialog
Fig. 10. How the tutor and meta-tutor communicate with each other.

3.1. Constraining students to follow the Target Node Strategy
One main function of the meta-tutor is to constrain the student to follow the Target Node Strategy. In order to do so, it implements the
algorithm shown in Fig. 11. Aside from a little bookkeeping, the procedure’s actions are either “Prompt and constrain the student to do
<step>,” or “Constrain the student to do <step>”.
Prompting means that as soon as the problem state allows a step to be done, the meta-tutor gives the student an unsolicited hint about
the step. Here are examples of production rules that implement prompting:
 IF the activity channel reports that the student’s action is “closed a node”
AND there are at least two nodes that can become the next target node
THEN
send a command via the message channel to show a dialogue box that lists the nodes and ask which one the student wants to choose
as the target node.
 IF the context is that the node editor is open and the current tab is the plan tab,
AND the activity channel reports that the student’s action is “clicked on the Check”
AND the correct plan of this node is “a ﬁxed, given number”
AND the student has selected this plan
THEN
send a command via the message channel to show a message that says that the next step is to ﬁll in the Inputs tab by clicking on
“Value is ﬁxed, so no inputs.”
Constraining the student to do a step means that if the problem state is such that the student should do a certain action, and the student
tries to do another action instead, then the meta-tutor blocks the student’s action from occurring and pops up a message indicating the right
action to do. Some examples of production rules that implement constraining are:
 IF the context is that the student is in the model tab and the node editor is not open
AND the student’s action is clicking on a node in order to open it in the node editor
AND the node that student is trying to open is not the target node
THEN
send via the block channel the message “Please focus on your target node, <target node name>” and block opening the clicked-on
node.
 IF the context is that the node editor is open
AND the student’s action is closing the node
AND the node’s calculation tab is not ﬁnished yet
THEN
Send via the block channel the message “Please ﬁnish the target node, <target node name>, before leaving the editor” and block
closing of the node editor.
In order for the meta-tutor to track the student’s progress and respond appropriately, its rules need to have up-to-date information about
the state of the problem. Using the activity channel, the tutoring system sends all student actions to the meta-tutor, which converts some of
them (e.g., not node movements) into elements in the production system’s working memory. The working memory is also used to store the
identity of the target node, the contents of the sought set and other bookkeeping information that is kept updated via production rules.
3.2. Discouraging gaming
As mentioned earlier, the Give-up and Check buttons can be easily misused by students to construct models without thinking deeply or
learning, e.g., “gaming the system” (Baker et al., 2006; Baker, Corbett, Koedinger, et al., 2004). Although constraining students to follow the
Target Node Strategy may discourage gaming, the meta-tutor also looks for patterns of student actions that indicate gaming is occurring.
When it sees such a pattern, it pops up a warning message but does not block the student’s actions.
Detecting and responding to gaming of the Check button is implemented by the ﬁnite state machine shown in Fig. 12. The students are
always in one of 5 states. When they start working on a problem, they start in state S1 (the node editor is not open). Clicking on a node opens
the node editor and moves to state S2 (no check yet). Edits to the contents of the open tab can occur in any of the states except S1, and are not
shown in Fig. 12, as they merely add a loop from a state back to itself. The arc label “Wrong check” means that the student clicked on the
Check button and got red, indicating the tab was not ﬁlled out correctly. The arc label “Got the answer right” means the student either

210

L. Zhang et al. / Computers & Education 75 (2014) 196–217

1. Initialize the target node to the top level goal (i.e., the quantity the problem wants graphed).
Initialize the sought set to null.
2. Constrain the student to create a node and fill out its Description tab to correspond to the top level
goal quantity.
3. Constrain the student to fill out the Plan tab correctly.
4. Prompt and constrain the student to fill out the Input tab correctly. Add any newly created nodes
to the sought set.
5. Prompt and constrain the student to fill out the Calculation tab correctly.
6. Constrain the student to close the node editor.
7. If there are no nodes in the sought set, then prompt and constrain student to run the model.
If there is just one node in the sought set, then set it as the target node and tell the student.
If there are two or more nodes in the sought set, ask the student which one should be the target
node and set it to be the target node.
8. Remove the target node from the sought set.
9. Prompt and constrain the student to create a node and fill out its Description tab to correspond to
the target node.
10. Go to step 3 above.
Fig. 11. The Target Node Strategy implemented by the meta-tutor.

clicked on the Check button and got green, or clicked on the Give-up button. Whenever the student enters state S4 (gaming detected), the
meta-tutor sends a randomly chosen message such as, “Guessing so quickly wastes the opportunity to learn.”
In short, because the main job of the meta-tutor was to teach students to stay on a certain path deﬁned by the meta-strategy by blocking
off-path actions, and the meta-strategy was simple and easily deﬁned, the only signiﬁcant technical challenge was integrating the metatutor with the tutor. The tutor was implemented with a standard model-view-controller architecture, and the meta-tutor essentially was
given a chance to intervene after the user’s actions had actuated the control and before the model was updated.
At this point, the design and implementation of the AMT system have been deﬁned, so it is time to consider whether it works. That is,
does meta-tutoring improve students’ learning compared to tutoring without meta-tutoring?
4. Evaluating the meta-tutor
So far, we have conducted 5 studies of the AMT system. In the ﬁrst two studies, which were conducted in the summer of 2010, students
used the tutoring system without the meta-tutor. This led to signiﬁcant changes in the design of the tutoring system (VanLehn et al., 2011),
the materials and the experimental procedure (VanLehn et al., 2011). The two studies also produced data revealing students’ deep and
shallow modelling practices, and thus allowed us to design the meta-tutor. This article presents the results of the next three studies. There
were slight changes to the tutor and the experimental procedure in between the studies 3 and 4, which were conducted in summer 2011.
Analysis of the data from these two studies over the next year led to signiﬁcant changes to the AMT system, such as adding the Plan tab,
which led to study 5, which was conducted in summer 2012.
All the three studies had two phases, training and transfer, as did most of the studies reviewed earlier. During the training phase, a metatutor taught students both to use a good learning strategy (adapted from Pyrenee’s) and to avoid using poor learning strategies. During the
transfer phase, the meta-tutor and the tutor were turned off, thus allowing us to measure both domain learning and spontaneous use of the
learning strategy.
4.1. Research hypotheses
Recall that our framework, adopted from the literature on learning strategies, poses four research questions:
A. Without instruction in good learning strategies, do students often exhibit poor learning strategies?
B. Can good learning strategies be easily taught?
C. When students use the taught learning strategies, does their domain learning increase compared to students who are not taught to use
the learning strategies?
Close the node
/leave the tab

Close the node
/leave the tab

S1

Wrong check
& interval > 3s
Wrong check

Open a node
Get to the input tab

S2

S3

Got the answer Got the answer right
right

Interval
>3s

Wrong check
& interval < 3 sec

Close the node
/leave the tab

S5

Got the answer right

Close the node
/leave the tab

S4
Wrong check
& interval < 3s

S1: no opened node editor
S2: no check
S3: one wrong check
S4: gaming
S5: got right

Fig. 12. Finite state machine for gaming the Check button.

L. Zhang et al. / Computers & Education 75 (2014) 196–217

211

D. After instruction in the learning strategy ceases, do students revert to poor learning strategies?
Our observations during studies 1 and 2 answered question A by demonstrating that our students often use poor learning strategies
when they are not taught good ones. Moreover, because the meta-tutor requires students to follow the Target Node Strategy during the
training phase, question B cannot be addressed by our studies. This leaves us to focus on C and D.
Addressing question C requires that we clearly deﬁne the expected domain learning. As mentioned earlier, two entirely different
instructional objectives are associated with model construction: Using model construction to learn domain concepts and principles (usually
in a science classes) and learning how to construct models (usually in math classes). In these studies, we focus only on the second objective,
so “domain learning” in question C means learning how to do deep model construction. Moreover, question C can be asked of both the
training phase, where the learning strategy is taught, and the transfer phase, when the meta-tutor and tutor are turned off. Thus, the speciﬁc
hypotheses we tested in studies 3, 4 and 5 are:
1. In the transfer phase, do meta-tutored students display deep modelling more frequently than students who were not meta-tutored?
2. In the training phase, do meta-tutored students display deep modelling more frequently than students who do not receive metatutoring?
3. In the transfer phase, do meta-tutored students follow the Target Node Strategy more frequently than students who were not metatutored?
Although we predict positive answers for all three questions, there is a caveat concerning the third hypothesis. The instruction used in
studies 3, 4 and 5 did not provide much meta-cognitive and motivational encouragement to use the learning strategy, because we are
developing that part of the instruction for use in later studies. Thus, we have low expectations for hypothesis 3.
4.2. Method
Students were randomly assigned to one of two conditions: with and without meta-tutoring. The difference between the conditions
occurred only during a training phase where students learned how to solve model construction problems. The meta-tutor group solved
problems with the meta-tutor turned on, while the control group solved the same problems with the meta-tutor turned off. During the
training phase, all students could use the Check and Give-up buttons at any time.
In order to assess how much students learned, a transfer phase followed the training phase. During the transfer phase, all students solved
model construction problems with almost no help. That is, the meta-tutor and the Give-up button were turned off, and the Check button was
turned off everywhere except on the Description tab where it remained enabled in order to facilitate grounding, as mentioned earlier.
Because system dynamics is rarely taught in high school, the procedure did not include a pre-test in modelling dynamic systems.
In order to provide a motivation similar to that which occurs in school, we told students that prizes would be awarded to students who
solved the most problems during the transfer phase. In particular, we repeatedly told them to use the Check and Give-up buttons judiciously
during the training, trying always to learn as much as possible, so that they could rapidly solve problems during the transfer phase when the
Check and Give-up buttons would not be available.
4.3. Participants
There were 34 student participants in the ﬁrst experiment, 44 students participated in the second experiment, and 34 students in the
third experiment. No person participated in more than one experiment. All were high school students who were participating in summer
camps at our university.
4.4. Procedure
The three experiments followed the same procedure. Each lasted two and a half hours and had two main phases, training and transfer.
Sensors recorded students’ physiological states throughout both the training and transfer phases. The sensors were: wireless skin
conductance bracelets, facial expression cameras, posture-sensing chairs, and pressure sensitive mice. Periodically, a window would pop up
and ask students to report their affective state. These data are being used to develop algorithms for affect detection, and will not be discussed further here. Also in the ﬁrst two experiments, but not the third, students were asked to speak their thoughts aloud into a headset
microphone, and their verbal protocols were recorded by screen-capture software.
The summer camp students were available for only a ﬁxed period of time and all needed to be kept occupied productively during that
period. Thus, each phase of the experiment lasted a ﬁxed period of time, and students solved as many problems as they could during that time.
We obtained parental consent for minors prior to the experiment. During the experiment, student gave informed consent, ﬁlled out a
background questionnaire, donned sensors and started the training phase. The training phase lasted a total of 75 min. It consisted of ﬁrst
studying a sequence of PowerPoint slides that introduced them to the user interface, model construction and the Target Node Strategy, and
then solving a sequence of model construction problems. The introduction slides remained available while students were solving problems.
During pilot testing, we noticed that some students spent a long time studying the introduction then rarely referred back to it, whereas
other studied the introduction brieﬂy and referred back to it frequently as they solved problems. Thus, we let students decide how to allocate
their 75 min between the studying introduction and solving the training problems. The training phase was followed by a 15-min break
where students went to another room and had a snack. After the break, all the students began the transfer phase, where both conditions
were identical. The transfer phase lasted for 30 min, and a debrieﬁng of the students followed.
Students in both conditions solved the same problems in the same order. Except for a few debugging problems during the training phase,
where student were asked to correct a given faulty model, both training and transfer problems required students to construct a correct

212

L. Zhang et al. / Computers & Education 75 (2014) 196–217

model, which is deﬁned as a model whose graphs match graphs of correct values. When the model was correct, students were allowed to go
on to the next problem. This procedure was followed for both the training and transfer phases.
The only procedural difference between the training and transfer phases was the amount of scaffolding available. During the training
phase, the Check and Give-up buttons were enabled, and the meta-tutor was active for students in the meta-tutor condition. During the
transfer phase, the Check button was enabled only on the Description tab because it was necessary for grounding, as discussed earlier. None
of the other scaffolding was available during the transfer phase.
4.5. Measures
To test the hypotheses and answer the research questions, 7 measures were deﬁned by extracting information about student’s interaction with software from students’ log ﬁles. The measures are described in this section.
Hypothesis 1 is that the meta-tutored students will use deep modelling more frequently than the control students during the transfer
phase. Deep modelling is not easy to measure directly, so we used three indirect indicators:
 The number of the “Run model” button presses for completing one problem in the transfer phase indicates how much help the student needs
from the system. Deep modellers should be able to complete problems using the Run model button only a couple of times per model.
 Another measure was the number of extra nodes created during the transfer phase, where extra nodes are deﬁned as the nodes that can
be legally created for the problem but are not required for solving the problem. Deep modellers should realize that these quantities are
irrelevant and therefore avoid modelling them.
 The number of problems completed during the 30 min transfer period was considered to be an indicator of deep modelling with the
assumption that it would be faster for students to solve atomic modelling problems deeply than shallowly.
Hypothesis 2 is that meta-tutored students should use deep modelling more frequently than the control group students during the
training phase. The three dependent measures used to evaluate this hypothesis are described next.
 Help button usage: Ideally, a deep modeller would ﬁll out a tab, click on the Check button and the tab’s ﬁelds would turn green (correct).
A student who is trying to do deep modelling but hasn’t mastered the skill might have to click the Check button twice, thinking hard
before each attempt, in order to get them right. On the other hand, shallow modellers might click on the Check button repeatedly as they
guess or use the Give-up button. To measure usage of the help buttons, the following measure was calculated:

Help usage ¼ nwc þ 3ngu



nrn

where nwc is the number of Check button presses that yielded red, ngu is the number of Give-up buttons the student clicked, and nrn is the
number of nodes required by the problem. The “3” here is a weight to make ngu be roughly comparable to nwc. The denominator represents
the number of nodes required for a correct, minimal solution rather than the actual number of nodes the student completed. Should the
prediction for Hypothesis 2 hold true, meta-tutored students’ usage of the help button should be lower than the control students’ one.
 Correct on ﬁrst Check: This measure for Hypothesis 2 is a traditional one in tutoring system research. It represents how frequently
students succeed on their ﬁrst attempt at ﬁlling out a tab. That is, what proportion of the tabs turned green when students ﬁrst clicked
on the Check button? Deep modellers should get almost everything right the ﬁrst time, whereas a student who is confused or guessing
might rarely get elements right on the ﬁrst try. Unfortunately, this measure could only be applied to experiment 5, as insufﬁcient log
data was kept during the 2011 experiments.
 Training efﬁciency: This measure is based on the (now dubious) assumption that deep modelling is faster than shallow modelling. Speed
is often measured by counting the number of problems solved during a ﬁxed training period. However, our problems’ solutions varied in
their complexity. Some had many nodes and some had few nodes. Moreover, students could always use the Give-up button, which does
part of the problem solving for them. There was one Give-up button per tab, and completing a node requires ﬁlling out three tabs, so
clicking on three Give-up buttons per node would solve the problem. Thus, a better measure of the amount of problem solving
accomplished than “problems completed” is the number of tabs the student completed without using the Give-up button, which is
given by:

Training efficiency ¼ 3ncn  ngu
where ncn is the number of nodes that the student completed correctly (so 3ncn is the number of tabs), and ngu is the number of Give-up
buttons the student clicked. The prediction for Hypothesis 2 is that the training efﬁciency of meta-tutored students should be higher than
the training efﬁciency of control students.
Hypothesis 3 is that the experimental group, which was required to follow the Target Node Strategy during training, would continue to
use it during the transfer phase. To evaluate this hypothesis, we calculated the proportion of student steps consistent with the target node
strategy. The algorithm of Fig. 12 describes how it is calculated.
5. Results
Experiments 1 and 2 found, as expected, that students often exhibit shallow learning strategies (VanLehn et al., 2011a). They also led to
many revisions in the software and the experimental procedure (VanLehn et al., 2011b). However, both experiments had only one condition,
and it did not include the meta-tutor. This section reports on comparisons of the system with the meta-tutor turned on and turned off.

L. Zhang et al. / Computers & Education 75 (2014) 196–217

213

5.1. Experiment 3 results
Experiment 3 was conducted in June 2011. Of the 34 participants, some students’ data needed to be omitted. Probably because there were
too many introduction slides (101 in total), 11 out of 34 students were still in the introduction phase at the break, and thus had no time in the
training phase where the manipulation occurred. These 11 students were omitted from the analyses. Two students, one in each condition,
performed much better than others. Both students’ training measures and test measures were greater than three standard deviations from
the means. These two students were also excluded from the analyses. The ﬁnal number of students left for each group was 11 (control) and
12 (meta-tutor). All the analyses below were computed based on these 23 students.
5.1.1. Hypothesis 1 (transfer phase deep modelling)
Run Model button usage: Of the 23 students, there were only 9 Meta-tutored students and 4 control students that ﬁnished the ﬁrst
problem in the transfer phase. Among the 13 students, meta-tutored students used the Run Model button 4.88 times per problem, while
students in control group used it 6.67 times. Due to the limited number of subjects, it is not surprising that the difference is not signiﬁcant
(p ¼ 0.72, d ¼ 0.12). This T-test, and all other tests reported here, are two-tailed.
Extra nodes: The ﬁrst problem of the transfer phase allowed 2 extra nodes, thus allowing us to measure the number of extra nodes created
by students who completed that problem. Meta-tutor students deﬁned 0.44 (SD ¼ 0.8) extra nodes vs. 0.75 (SD ¼ 0.96) extra nodes for the
control students. The difference was not reliable (p ¼ 0.61, d ¼ 0.58) although it was in the expected direction.
Number of problems completed: The average number of problems completed during the transfer phase was 0.82 (SD ¼ 0.60) for the metatutor students vs. 0.35 (SD ¼ 0.52) for the control students. The difference was marginally signiﬁcant (p ¼ 0.057) even though the effect size
was large (d ¼ 0.88) and in the expected direction. These ﬁgures show that on average, students completed less than one problem. More
speciﬁcally, 9 meta-tutored and 4 control students ﬁnished one or more problems, which was a marginally reliable difference (c2 ¼ 3.486,
p ¼ 0.06).
In short, trends in the data support Hypothesis 1 but the differences were not reliable, probably due to the small sample size.
5.1.2. Hypothesis 2 (training phase deep modelling)
Help button usage: This measure is a weighted sum of help button uses divided by the total number of nodes completed. On this measure,
the meta-tutor students averaged 4.78 (SD ¼ 2.25) vs. 7.03 (SD ¼ 3.44) for the control students. The difference was marginally signiﬁcant
according with a large effect size (p ¼ 0.08, d ¼ 0.82).
Training efﬁciency: The average training efﬁciency measure the amount of correct work done by the student during the ﬁxed-length
training period. For the meta-tutor group, training efﬁciency was 13.00 (SD ¼ 6.94) vs. 11.45 (SD ¼ 6.77) for the control group. The difference was not signiﬁcant (p ¼ 0.30; d ¼ 0.23).
Thus, although there was a trend in the data supporting Hypothesis 2, the reliability was poor, perhaps due to the small sample size.
5.1.3. Hypothesis 3 (meta-strategy usage)
Hypothesis 3 was that meta-tutored students would voluntary the Target Node Strategy during the transfer phase more frequently than
the student who was not meta-tutored. During the transfer phase, 0.39 (SD ¼ 0.35) of the meta-tutor students’ steps matched the Target
Node Strategy’s steps, vs. 0.34 (SD ¼ 0.30) for the control students. This difference was quite small (p ¼ 0.70, d ¼ 0.16), suggesting that
hypothesis 3 may be false for this study.
5.1.4. Summary of results and next step forward
Although there were some trends in the expected directions, the students in both conditions performed quite poorly. This was probably
due to the large number of slides in the introduction phase as well as the difﬁculty of the tasks themselves.
In order to increase the number of training and transfer problems solved by students, thus allowing us to differentiate their performance
statistically, we reduced the number of slides from 101 to 64, and simpliﬁed some of the tasks. As the second experiment conﬁrmed, these
changes led to an improvement on the performance students in both phases.

5.2. Experiment 4 results
Experiment 4 took place in July 2011. Data from all 44 participants (22 in each condition) were used in the analyses here.
5.2.1. Hypothesis 1 (transfer phase deep modelling)
Run Model button usage: To complete the ﬁrst problem in the transfer phase, meta-tutored students used the Run model button 3.05
times on average. In comparison, students in the control group used the Run model button 5.13 times. However, the difference was not
signiﬁcant (p ¼ 0.31, d ¼ 0.32). Because the standard deviation was high (SD ¼ 6.77) due to extreme values, we divided all the students into
two types with the threshold being 2, which is the median. The students who used the run model button once or twice were considered
deep modellers. The rest of the students were considered shallow modellers. Chi-square test is then used to compare the number of deep
modellers in meta-tutored group to the number in control group. There was a trend in the expected direction, but the signiﬁcance was
marginal (c2 ¼ 3.36, p ¼ 0.067).
Extra nodes: Because most of the students ﬁnished at least two tasks in the transfer phase (only 3 students in the control group did not)
and the second task allowed up to two extra nodes, we used the second task to count extra nodes. As predicted by Hypothesis 1, metatutored students produced fewer extra nodes (0.27, SD ¼ 0.70) than control students (0.95, SD ¼ 1.03). The difference was signiﬁcant
with a large effect size (p ¼ 0.02, d ¼ 0.80).
Problems completed: The meta-tutor students solved 3.27 (SD ¼ 1.03) transfer problems vs. 3.23 (SD ¼ 1.57) for the control students. The
difference was small and not signiﬁcant (p ¼ 0.65, d ¼ 0.04).

214

L. Zhang et al. / Computers & Education 75 (2014) 196–217

5.2.2. Hypothesis 2 (training phase deep modelling)
Help button usage: Consistent with Hypothesis 2, meta-tutor students’ help button usage averaged 2.35 (SD ¼ 1.75) vs. 3.55 (SD ¼ 1.85)
for the control students. The difference was signiﬁcant (p ¼ 0.04, d ¼ 0.68).
Training efﬁciency: Contrary to Hypothesis 2, the control students had higher training efﬁciency 72.77 (SD ¼ 32.12) than the meta-tutor
students, 54.36 (20.17), and the difference was marginally signiﬁcant (p ¼ 0.05, d ¼ 0.70).
These results suggest that meta-tutor students did increase deep modelling in the training phase than the control students, but they also
moved slower than control students.
5.2.3. Hypothesis 3 (use of Target Node Strategy)
Missing data precluded testing this hypothesis in experiment 4.
5.2.4. Summary of results, revisions and the next iteration
Both hypothesis 1 (transfer) and hypothesis 2 (training) were supported, albeit by one measure each. We were not satisﬁed with the pace
of the students in the training phase, especially the meta-tutored students. Watching the screen-capture video suggested that meta-tutored
students spent a lot of time in reading and answering the tutorial pop-ups. Students also became “stuck” (Burleson & Picard, 2007) when
ﬁlling out the current Inputs tab and needing to create new nodes. In order to do so, students had to close the node editor and click on the
Create Node button on the canvas in the Model tab. However, many students could not realize that they had to close the node editor. They
complained that no button in the Inputs tab could help them get out of the stuck state. Thus, we spent several months reﬁning both the
tutoring system and the meta-tutor. We installed the Plan tab in order to reduce the time spent reading the meta-tutor’s pop-ups. We also
improved the interface, adding a “create a new node,” button on the Input tab with the goal of reducing students’ state of stuck in this stage.
The transfer phase proved to be extremely challenging for the students. This was evident in the relatively small number of problems
solved as well as direct observation of the students. The null result on our productivity measure, number of problems solved in the transfer
phase, might be due to students in both conditions becoming discouraged and ceasing to try hard. Thus, we modiﬁed the transfer phase so
that besides colouring the “g” indicator, the system coloured the “i” indicator and “c” indicator as well to show the correctness of the
corresponding tabs. This was intended to make it easier to locate errors while leaving unchanged the logic required for ﬁxing the errors, and
thus allowing students to make faster progress through the test problems while still allowing us to assess their skill.
5.3. Experiment 5 results
Experiment 5 was conducted in June 2012. Of the 34 participants, data from 33 were used in the analyses below (16 in the control group
and 17 in meta-tutor group). One student was excluded due to his extraordinary performance. He ﬁnished all 7 problems in the transfer
phase well before the end of the transfer phase, while the second fastest person only completed 4 problems.
5.3.1. Hypothesis 1 (transfer phase deep modelling)
Run Model button usage: On average, students in the control group used the Run model button 7.76 times, while meta-tutored students
used the Run model button 7.82 times. So they almost had the same performance (p ¼ 0.98, d ¼ 0.0093).
Extra nodes: Most of the students ﬁnished at least two tasks in the transfer phase (one in each group didn’t), so we again used the second
task to measure extra nodes. As predicted by Hypothesis 1, the meta-tutor students produced fewer extra nodes (0.88, SD ¼ 0.96) than the
control students (1.13, SD ¼ 0.99). However, the difference was not reliable (p ¼ 0.47, d ¼ 0.26).
Problems solved: Contrary to Hypothesis 1, the meta-tutor students solved 2.18 (SD ¼ 0.53) transfer problems vs. 2.56 (SD ¼ 0.78) for the
control students. Students in the control group outperformed meta-tutored student with marginal signiﬁcance (p ¼ 0.094, d ¼ 0.57).
Once again, the meta-tutor students tended to work more slowly than the control students. This time, there was only a trend to show that
they might be doing more deep modelling than the control students.
5.3.2. Hypothesis 2 (training phase deep modelling)
Help button usage: As expected, meta-tutored students’ help button usage averaged 3.92 (SD ¼ 2.19) vs. 6.13 (SD ¼ 2.73) for the control
students. The difference was signiﬁcant with a large effect size (p ¼ 0.016, d ¼ 0.89).
Correct on ﬁrst attempt: To provide an additional test of Hypothesis 2, we calculated the percentage of time that the ﬁrst Check on a tab
was correct. Meta-tutored students achieved a higher percentage (0.77, SD ¼ 0.068) than control students (0.68, SD ¼ 0.11), and the difference was signiﬁcant with a large effect size (p ¼ 0.015, d ¼ 0.98).
Training efﬁciency: Meta-tutor students scored 73.18 (SD ¼ 27.53), a little bit higher in training efﬁciency than control students, 68.88
(SD ¼ 17.16), but the difference was not reliable (p ¼ 0.59, d ¼ 0.19).
So students in both groups kept the same pace in the training session this time, and there was strong evidence that the meta-tutor
students were engaged in deeper modelling than the control students.
5.3.3. Hypothesis 3 (use of Target Node Strategy)
Contrary to hypothesis 3, the meta-tutored students had nearly the same level of Target Node Strategy usage (0.66, SD ¼ 0.23) as control
students (0.70, SD ¼ 0.19), and the difference is not reliable (p ¼ 0.59, d ¼ 0.19).
6. Discussion
Our results are summarized in Table 1. This section discusses our interpretation of them.
As mentioned in the introduction, whenever a new kind of instruction is developed, there are four classic questions to ask about learning
strategies that are speciﬁc to it:

L. Zhang et al. / Computers & Education 75 (2014) 196–217

215

Table 1
Summary of the results.
Measure (predicted dir.)

Experiment 3 (N ¼ 23)

Experiment 4 (N ¼ 44)

Experiment 5 (N ¼ 33)

Transfer phase (Hypothesis 1)
Run model button usage (E < C)
Extra nodes (E < C)
Probs completed (E > C)

Not available
E < C (p ¼ 0.61, d ¼ 0.58)
E > C (p ¼ 0.06, d ¼ 0.88)

E < C (p ¼ 0.31, d ¼ 0.32)
E < C (p [ 0.02, d [ 0.80)
E z C (p ¼ 0.65, d ¼ 0.04)

E z C (p ¼ 0.98, d ¼ 0.0093)
E < C (p ¼ 0.47, d ¼ 0.26)
E < C (p ¼ 0.09, d ¼ 0.57)

Training phase (Hypothesis 2)
Help button usage (E < C)
Correct on 1st Chk (E > C)
Efﬁciency (E > C)

E < C (p ¼ 0.08, d ¼ 0.82)
Missing data
E > C (p ¼ 0.30, d ¼ 0.23)

E < C (p [ 0.04, d [ 0.68)
Missing data
E < C (p ¼ 0.05, d ¼ 0.70)

E < C (p [ 0.02, d [ 0.89)
E > C (p [ 0.015, d [ 0.98)
E > C (p ¼ 0.59, d ¼ 0.19)

Missing data

E z C (p ¼ 0.59, d ¼ 0.19).

Transfer phase use of Target Node Strategy (Hypothesis 3)
Usage (E ¼ C)
E z C (p ¼ 0.70, d ¼ 0.16)

E stands for the meta-tutor group, and C stands for the control group. Reliable results are bold.

A. Without instruction in good learning strategies, do students often exhibit poor learning strategies?
B. Can good learning strategies be easily taught?
C. When students use the taught learning strategies, does their domain learning increase compared to students who are not taught to use
the learning strategies?
D. When instruction in the learning strategy ceases, do students revert to poor learning strategies?
After we developed a step-based tutoring system for model construction, experiments 1 and 2 answered question A by ﬁnding that
student did indeed exhibit poor learning strategies when using it. This led us to develop a meta-tutor that taught students a meta-strategy
that we hoped would increase their acquisition of skill in model construction.
Answering question B requires that even during the training phase, students have the freedom to choose between following or not
following the learning strategy. This freedom allows experimenters to measure the growth in compliance during the training phase. Of the
four systems reviewed earlier (Betty’s Brain, the Help Tutor, Co-Lab and Pyrenees; see Table 2), Betty’s Brain and the Help Tutor provide such
freedom, and thus were able to show that while students were being meta-tutored, their behaviours were more often consistent with the
learning strategy than the behaviours of students who were not being meta-tutored. This suggests that their meta-tutoring was effective at
getting students to use the taught learning strategy. AMT, Co-Lab and Pyrenees all required students to follow the taught learning strategy
during the training phase, so they could not answer question B.
Given a learning strategy that designers think is good, question C asks whether it really does increase domain learning. Of the four
systems reviewed earlier, only Pyrenees’ learning strategy increased domain learning. Thus, we adapted its learning strategy for use in AMT.
Because the goal of our system is teach students to construct models properly, most of the measures addressed the frequency of deep
modelling. During the training phase, on the measures “help button usage” and “correct on ﬁrst check” meta-tutored students scored
reliably higher than students who were not meta-tutored, except on experiment 3, which appears to have been underpowered. Moreover,
the effect sizes were large (d ¼ 0.89; d ¼ 0.98) or moderately larger (d ¼ 0.68). Thus, the AMT results for domain learning in the training
phase were nearly as good as those from Pyrenees, and substantially better than the other three meta-tutoring systems.
Unfortunately, neither the Target Node Strategy nor the domain learning advantages transferred. During the transfer phase, on the
measures “Run model button usage”, “extra nodes” and “Target Node Strategy usage”, the meta-tutored students were no better than the
students who had not been meta-tutored earlier, with one exception. On the measure “extra nodes” in experiment 4, meta-tutored students
outscored the control students. It is always difﬁcult to get learning to transfer from a supported context to an unsupported one, so this lack of
transfer should perhaps not be surprising. However, it does appear to be a bit weaker than the transfer obtained by Pyrenees, Betty’s Brain
and the Help Tutor.
In both the training and transfer phases, we attempted to measure deep modelling using efﬁciency: the amount of modelling done in a
ﬁxed period of time. This measurement made the tacit assumption that deep modelling is faster than shallow modelling. That is, thinking
hard to solve an atomic modelling problem should take less time than guessing, overusing the Give-up button, overusing the Run button,
scanning the problem statement for keywords and other shallow model construction tactics. However, the efﬁciency measures all produced
null results. If anything, there was trend for non-meta-tutored students to get more done than the meta-tutored students. This is consistent
with our informal analyses of the log data. It appears that guessing was actually quite a bit faster than thinking, especially in the training
phase when the Check button was enabled. Even when students could only use the Run Model button for feedback in the transfer phase,
guessing was fast because the models were so small for the ﬁrst few problems. After that, guessing became must less efﬁcient, but few
students got that far.
As the experience with Betty’s Brain and the Help Tutor shows, not every supposedly good learning strategy actually turns out to increase
domain learning. In the case of AMT, we have found a learning strategy that does increase domain learning, and thus deserves to be taught.
On the other hand, our method of teaching the learning strategy appears not to have had enough meta-cognitive or motivational impact
on students, because their gains while being meta-tutor did not persist when the meta-tutoring was turned off.
Thus, the next stage of the AMT project is to augment the instruction with an affective learning companion. Its job will be to persuade
students of the beneﬁts of using the Target Node Strategy and of not abusing the Check and Give-up buttons. The agent cannot use the
argument that the learning strategy and deep modelling will speed up the students’ work, because we have found that shallow modelling
strategies are actually faster at least on these simple problems. Thus, we plan on having the agent use Dweck’s well-known argument
(Dweck & Leggett, 1988) that the “mind is a muscle; the harder you exercise it, the stronger it becomes.” Our summer school students
presumably want stronger minds, so if they believe the agent, they should more often use both the learning strategy and deep modelling. In
order to make the agent easier to believe, we plan to use attribution shifting, empathy, rapport-building chit-chat, and other non-cognitive

216

L. Zhang et al. / Computers & Education 75 (2014) 196–217

Table 2
Comparison of four meta-tutors of model construction.
Meta-tutor

AMT
Pyrenees
Help Tutor
Betty’s Brain
Co-Lab

Training phase

Transfer phase

Knowledge/skill

Meta-strategy

Knowledge/skill

Meta-strategy

þ
þþ
NS
NS
NS

Required
Required
þ
þ
Required

þ
þþ
NS
þ

NS
NS
þ
þ

NS ¼ non-signiﬁcant difference, two-tailed. þ ¼ Signiﬁcant but weak. þþ ¼ Signiﬁcant and strong. Required ¼ meta-tutor required student to follow the learning strategy.

techniques. In order to optimize the timing and selection of these non-cognitive interventions, we plan to monitor the students’ affective
state using physiological sensors.
Lastly, by experiment 5, students seem to be acquiring decent amounts of competence in system dynamics modelling in only 1 h and
15 min total. This is a considerable reduction from the 5 or more hours required of Model-It students and others. Although we have no way
to actually compare our results to the early ones, because the systems, students, domains and almost everything else were quite different,
we nonetheless are quite encouraged. The combination of tutoring and meta-tutoring may be the key to getting model construction out into
the classrooms at last.
Acknowledgements
This material is based upon work supported by the National Science Foundation, United States (_100000001) under Grant No. 0910221.
References
Alessi, S. M.. (December 2000). The application of system dynamics modeling in elementary and secondary school curricula. In Paper presented at the RIBIE 2000 – The ﬁfth
Iberoamerican conference on informatics in education. Viña del Mar, Chile.
Aleven, V., McLaren, B., Roll, I., & Koedinger, K. (2004). Toward tutoring help seeking (applying cognitive modeling to help-seeking skills). In J. C. Lester, R. M. Vicari, &
F. Paraguacu (Eds.), Intelligent tutoring systems: Seventh international conference: ITS 2005 (pp. 227–239). Berlin: Springer.
Aleven, V., Stahl, E., Schworm, S., Fischer, F., & Wallace, R. M. (2003). Help seeking and help design in interactive learning environments. Review of Educational Research, 73(2),
277–320.
Baker, R. S. J. d., Corbett, A., & Koedinger, K. R. (2004). Detecting student misuse of intelligent tutoring systems. In Proceedings of the 7th international conference on intelligent
tutoring systems (pp. 531–540).
Baker, R. S., Corbett, A., Koedinger, K. R., Evenson, S., Roll, I., Wagner, A. Z., et al. (2006). Adapting to when students game an intelligent tutoring system. In Intelligent tutoring
systems (pp. 392–401). Berlin: Springer.
Baker, R. S. J. d., Corbett, A., Koedinger, K. R., & Wagner, A. Z. (2004). Off-task behavior in the cognitive tutor classroom: when students “game the system”. In E. DykstraErickson, & M. Tscheligi (Eds.), Proceedings of the SIGCHI conference on human factors in computing systems (pp. 383–390). New York, NY: ACM.
Biswas, G., Leelawong, K., Schwartz, D. L., & Vye, N. J. (2005). Learning by teaching: a new agent paradigm for educational software. Applied Artiﬁcial Intelligence, 19, 363–392.
Booth Sweeney, L., & Sterman, J. D. (2000). Bathtub dynamics: initial results of a systems thinking inventory. System Dynamics Review, 16(4), 249–286.
Bravo, C., van Joolingen, W. R., & de Jong, T. (2009). Using Co-Lab to build system dynamics models: students’ actions and on-line tutorial advice. Computer and Education, 53,
243–251.
Bredeweg, B., & Forbus, K. D. (2003). Qualitative modeling in education. AI Magazine, 24(4), 35–46.
Burleson, W., & Picard, R. W. (2007). Affective learning companions. Educational Technology, Special Issue on Pedagogical Agents, Saddle Brook, N.J., 47(1), 28–32.
CCSSO. (2011). The Common Core State Standards for mathematics. Downloaded from www.corestandards.org. on 31.10.11.
Chi, M., & VanLehn, K. (2010). Meta-cognitive strategy instruction in intelligent tutoring systems: how, when and why. Journal of Educational Technology and Society, 13(1), 25–
39.
Chin, D., Dohmen, I. I. M., Cheng, B. H., Oppezzo, M., Chase, C. C., & Schwartz, D. L. (2010). Preparing students for future learning with teachable agents. Educational Technology
Research and Development, 58, 649–669.
Clark, H. H., & Brennan, S. E. (1991). Grounding in communication. In L. B. Resnick, J. M. Levine, & S. D. Teasley (Eds.), Perspectives on socially shared cognition (pp. 127–149).
Washington, DC: American Psychological Association.
Collins, A., & Ferguson, W. (1993). Epistemic forms and epistemic games: structures and strategies to guide inquiry. Educational Psychologist, 28(1), 25–42.
Doerr, H. M. (1996). Stella ten-years later: a review of the literature. International Journal of Computers for Mathematical Learning, 1, 201–224.
Donker, A. S., de Boer, H., Kostons, D., Dignath van Ewijk, C. C., & van der Werf, M. P. C. (2014). Effectiveness of learning strategy instruction on academic performance: a metaanalysis. Educational Research Review, 11, 1–26.
Du Boulay, B., Avramides, K., Luckin, R., Martínez-Mirón, E., & Rebolledo-Méndez, G. (2010). Towards systems that care: a conceptual framework based on motivation,
metacognition and affect. International Journal of Artiﬁcial Intelligence in Education, 20(3), 197–229.
Dweck, C. S., & Leggett, E. L. (1988). A social-cognitive approach to motivation and personality. Psychological Review, 95(2), 256–273.
Gonzalez-Sanchez, J., Chavez-Echeagaray, M.-E., VanLehn, K., & Burleson, W. (2011). From behavioral description to a pattern-based model for intelligent tutoring systems. In
Paper presented at the Proceedings of the 18th international conference on pattern languages of programs (PLoP). Portland, OR.
Hashem, K., & Mioduser, D. (2011). The contribution of learning by modeling (LbM) to students’ understanding of complexity concepts. International Journal of e-Education, eBusiness, e-Management and e-Learning, 1(2), 151–157.
Hattie, J., Biggs, J., & Purdie, N. (1996). Effects of learning skills interventions on student learning: a meta-analysis of ﬁndings. Review of Educational Research, 66, 99–136.
Hestenes, D. (2007). Modeling theory for math and science education. In Paper presented at the ICTMA-13: The international community of teachers of mathematical modelling
and applications. Indiana, IL.
Hogan, K., & Thomas, D. (2001). Cognitive comparisons of students’ systems modeling in ecology. Journal of Science Education and Technology, 10(4), 319–345.
Lee, C. B., Jonassen, D., & Teo, T. (2011). The role of model building in problem solving and conceptual change. Interactive Learning Environments, 19(3), 247–265.
Leelawong, K., & Biswas, G. (2008). Designing learning by teaching agents: the Betty’s brain system. International Journal of Artiﬁcial Intelligence and Education, 18(3), 181–208.
Löhner, S., Van Joolingen, W. R., & Savelsbergh, E. R. (2003). The effect of external representation on constructing computer models of complex phenomena. Instructional
Science, 31, 395–418.
Mandinach, E. B., & Cline, H. F. (1994a). Classroom dynamics: Implementing a technology-based learning environment. Mahwah, NJ: Erlbaum.
Mandinach, E. B., & Cline, H. F. (1994b). Modeling and simulation in the secondary school curriculum: the impact on teachers. Interactive Learning Environments, 4(3), 271–289.
Marshall, S. P., Barthuli, K. E., Brewer, M. A., & Rose, F. E. (1989). Story problem solver: A schema-based system of instruction. San Diego, CA: Center for Research in Mathematics
and Science Education, San Diego State University.
Metcalf, S. J. (1999). The design of guided learner-adaptable scaffolding in interactive learning environment (Doctoral Dissertation). University of Michigan.
Metcalf, S. J., Krajcik, J., & Soloway, E. (2000). Model-It: a design retrospective. In M. J. Jacobson, & R. B. Kozma (Eds.), Innovations in science and mathematics education:
Advanced designs for technologies of learning (pp. 77–115).

L. Zhang et al. / Computers & Education 75 (2014) 196–217

217

Mulder, Y. G., Lazonder, A. W., de Jong, T., Anjewierden, A., & Bollen, L. (2011). Validating and optimizing the effects of model progression in simulation-based inquiry learning.
Journal of Science Education and Technology, 21, 722–729.
Muldner, K., Burleson, W., van de Sande, B., & VanLehn, K. (2011). An analysis of students’ gaming behaviors in an intelligent tutoring system: predictors and impacts. User
Modeling and User-Adapted Interaction, 21(1–2), 99–135.
Nathan, M. J. (1998). Knowledge and situational feedback in a learning environment for algebra story problem solving. Interactive Learning Environments, 5, 135–159.
National Research Council. (2012). A framework for K–12 science education: Practices, crosscutting concepts, and core ideas. Washington, DC: National Academies Press.
Richmond, B. M. (1985). STELLA: software for bringing system dynamics modeling to the other 98%. In Paper presented at the Proceedings of the 1985 international conference of
the System Dynamics Society: 1985 International system dynamics conference.
Roll, I., Aleven, V., McLaren, B., & Koedinger, K. R. (2007a). Can help seeking be tutored? Searching for the secret sauce of metacognitive tutoring. In Proceedings of the international conference on artiﬁcial intelligence in education (pp. 203–210). Amsterdam: IOS Press.
Roll, I., Aleven, V., McLaren, B., & Koedinger, K. R. (2007b). Designing for metacognition – applying cognitive tutor principles to the tutoring of help seeking. Metacognition and
Learning, 2(2).
Roll, I., Aleven, V., McLaren, B., & Koedinger, K. R. (2011). Improving students’ help-seeking skills using metacognitive feedback in an intelligent tutoring system. Learning and
Instruction, 267–280.
Roll, I., Aleven, V., McLaren, B., Ryu, E., Baker, R. S. J. d., & Koedinger, K. R. (2006). The Help Tutor: does metacognitive feedback improve student’s help-seeking actions, skills
and learning. In M. Ikeda, K. Ashley, & T.-W. Chan (Eds.), Intelligent tutoring systems: 8th International conference, its 2006 (pp. 360–369). Berlin: Springer.
Russell, S., & Norvig, P. (2009). Artiﬁcial intelligence: A modern approach (2nd ed.). Upper Saddle River, NJ: Prentice Hall.
Schecker, H. (1993). Learning physics by making models. Physics Education, 28, 102–106.
Segedy, J. R., Kinnebrew, J. S., & Biswas, G. (2012a). Relating student performance to action outcomes and context in a choice-rich learning environment. In S. A. Cerri,
W. J. Clancey, & G. Papadourakis (Eds.), Intelligent tutoring systems: 11th International conference its 2012 (pp. 505–510). Berlin: Springer-Verlag.
Segedy, J. R., Kinnebrew, J. S., & Biswas, G. (2012b). Supporting student learning using conversational agents in a teachable agent environment. In Paper presented at the
Proceedings of the 10th international conference of the learning sciences. Sydney, Australia.
Shih, B., Koedinger, K. R., & Scheines, R. (2008). A response time model for bottom-out hints as worked examples. In C. Romero, S. Ventura, M. Pechenizkiy, & R. S. J. d Baker
(Eds.), Handbook of educational data mining (pp. 201–211). Boca Raton, FL: Taylor & Francis.
Steed, M. (1992). Stella, a simulation construction kit: cognitive process and educational implications. Journal of Computers in Mathematics and Science Teaching, 11, 39–52.
Stratford, S. J. (1997). A review of computer-based model research in precollege science classroom. Journal of Computers in Mathematics and Science Teaching, 16(1), 3–23.
Tan, J., Biswas, G., & Schwartz, D. L. (2006). Feedback for metacognitive support in learning by teaching environments. In Proceedings of the twenty-eighth annual meeting of the
Cognitive Science Society. Mahwah, NJ: Erlbaum.
Tan, J., Wagster, J., Wu, Y., & Biswas, G. (2007). Effect of metacognitive support on student behaviors in learning by teaching environments. In R. Luckin, K. R. Koedinger, &
J. Greer (Eds.), Proceedings of the 13th international conference on artiﬁcial intelligence in education (pp. 650–652). Amsterdam: IOS Press.
Timms, M. J. (2007). Using item response theory (IRT) to select hints in an ITS. In R. Luckin, K. R. Koedinger, & J. Greer (Eds.), Artiﬁcial intelligence in education (pp. 213–221).
Amsterdam: IOS Press.
Treagust, D. F., Chittleborough, G., & Mamiala, T. (2002). Students’ understanding of the role of scientiﬁc models in learning science. International Journal of Science Education,
24(4), 357–368.
VanLehn, K. (2006). The behavior of tutoring systems. International Journal of Artiﬁcial Intelligence and Education, 16, 227–265.
VanLehn, K. (2013). Model construction as a learning activity: a design space and review. Interactive Learning Environments, 21(4), 371–413.
VanLehn, K., Burleson, W., Chavez-Echeagaray, M.-E., Christopherson, R., Gonzalez-Sanchez, J., Hastings, J., et al. (2011a). The level up procedure: how to measure learning
gains without pre- and post-testing. In T. Hirashima (Ed.), Proceedings of the 19th international conference on computers in education (pp. 96–100). Chiang-Mai, Thailand:
Asia-Paciﬁc Society for Computers in Education.
VanLehn, K., Burleson, W., Chavez-Echeagaray, M.-E., Christopherson, R., Gonzalez-Sanchez, J., Hastings, J., et al. (2011b). The affective meta-tutoring project: how to motivate
students to use effective meta-cognitive strategies. In Paper presented at the 19th International conference on computers in education. Chiang Mai, Thailand.
Vanlehn, K., & Chi, M. (2012). Adaptive expertise as acceleration of future learning: a case study. In P. J. Durlach, & A. Lesgold (Eds.), Adaptive technologies for training and
education. Cambridge: Cambridge University Press.
Wagster, J., Tan, J., Biswas, G., & Schwartz, D. L. (2007). How metacognitive feedback affects behavior in learning and transfer. In Paper presented at the 13th International
conference on artiﬁcial intelligence in education: Workshop on metacognition and self-regulated learning in ITSs. Marina del Rey, CA.
Wagster, J., Tan, J., Wu, Y., Biswas, G., & Schwartz, D. L. (2007). Do learning by teaching environments with metacognitive support help students develop better learning
behaviors?. In Proceedings of the twenty-sixth annual meeting of the Cognitive Science Society. Mahwah, NJ: Erlbaum.
Wheeler, J. L., & Regian, J. W. (1999). The use of a cognitive tutoring system in the improvement of the abstract reasoning component of word problem solving. Computers in
Human Behavior, 15, 243–254.
Wilensky, U. (2003). Statistical mechanics for secondary school: the GasLab multi-agent modeling toolkit. International Journal of Computers for Mathematical Learning, 8(1),
1–41.
Wilensky, U., & Reisman, K. (2006). Thinking like a wolf, a sheep, or a ﬁreﬂy: learning biology through constructing and testing computational theories – an embodied
modeling approach. Cognition and Instruction, 24(2), 171–209.
Zaraza, R., & Fisher, D. (1997). Introducing system dynamics into the traditional secondary curriculum: the CC-STADUS project’s search for leverage points. In Paper presented
at the 15th International system dynamics conference. Istanbul, Turkey.
Zaraza, R., & Fisher, D. (1999). Training system modelers: the NSF CC-STADUS and CC-SUSTAIN projects. In W. Feurzeig, & N. Roberts (Eds.), Modeling and simulation in science and
mathematics education (Vol. 1); (pp. 38–69). New York, NY: Springer.

Computers & Education 75 (2014) 196–217

Contents lists available at ScienceDirect

Computers & Education
journal homepage: www.elsevier.com/locate/compedu

Evaluation of a meta-tutor for constructing models of
dynamic systems
Lishan Zhang*, Kurt VanLehn, Sylvie Girard, Winslow Burleson,
Maria Elena Chavez-Echeagaray, Javier Gonzalez-Sanchez, Yoalli Hidalgo-Pontet
Arizona State University, Computing, Informatics, and Decision Systems Engineering, Tempe, AZ 85281, USA

a r t i c l e i n f o

a b s t r a c t

Article history:
Received 5 September 2013
Received in revised form
17 February 2014
Accepted 25 February 2014
Available online 13 March 2014

Modelling is an important skill to acquire, but it is not an easy one for students to learn. Existing
instructional technology has had limited success in teaching modelling. We have applied a recently
developed technology, meta-tutoring, to address the important problem of teaching model construction.
More speciﬁcally, we have developed and evaluated a system that has two parts, a tutor and a meta-tutor.
The tutor is a simple step-based tutoring system that can give correct/incorrect feedback on student’s
steps and can demonstrate steps for students when asked. Because deep modelling requires difﬁcult
analyses of the quantitative relationships in a given system, we expected, and found, that students
tended to avoid deep modelling by abusing the tutor’s help. In order to increase the frequency of deep
modelling, we added a meta-tutor that coached students to follow a learning strategy that decomposed
the overall modelling problem into a series of “atomic” modelling problems. We conducted three experiments to test the effectiveness of the meta-tutor. The results indicate that students who studied with
meta-tutor did indeed engage in more deep modelling practices. However, when the meta-tutor and
tutor were turned off, students tended to revert to shallow modelling. Thus, the next stage of the
research is to add an affective agent that will try to persuade students to persist in using the taught
strategies even when the meta-tutoring and tutoring have ceased.
Ó 2014 Elsevier Ltd. All rights reserved.

Keywords:
Meta-tutor
Gaming the system
Intelligent tutoring systems
Modelling
Learning strategies

1. Introduction
This paper reports progress on two research problems: (1) teaching students how to construct mathematical models of dynamic systems,
and (2) teaching students to use effective learning strategies. Both research problems have long histories, which are covered in the next few
sections.
1.1. A brief history of educational uses of system dynamics modelling
There are two distinct reasons why students should learn model construction. First, modelling is an important cognitive skill in itself. The
Common Core State Standards for Mathematics (CCSSO, 2011) considers modelling to be one of 7 essential mathematical practices that
should be taught at all grade levels. The Next Gen standards for science instruction (National, 2012) also have 7 strands that are threaded
throughout the standards, and modelling is one of them.
Second, modelling is widely believed to be an important method for learning domain knowledge. For instance, modelling has been
claimed to help in achieving a deep understanding of scientiﬁc systems, economic systems and other systems (Chin et al., 2010; Metcalf,
Krajcik, & Soloway, 2000; Stratford, 1997), removing misconceptions and making of conceptual changes (Booth Sweeney & Sterman,
2000; Bredeweg & Forbus, 2003; Hestenes, 2007; Lee, Jonassen, & Teo, 2011; Mandinach & Cline, 1994b; Wilensky, 2003; Wilensky &
* Corresponding author.
E-mail addresses: lishan.zhang@asu.edu (L. Zhang), kurt.vanlehn@asu.edu (K. VanLehn), sylvie.girard@asu.edu (S. Girard), winslow.burleson@asu.edu (W. Burleson),
mchaveze@asu.edu (M.E. Chavez-Echeagaray), javiergs@asu.edu (J. Gonzalez-Sanchez), yhidalgo@asu.edu (Y. Hidalgo-Pontet).
http://dx.doi.org/10.1016/j.compedu.2014.02.015
0360-1315/Ó 2014 Elsevier Ltd. All rights reserved.

L. Zhang et al. / Computers & Education 75 (2014) 196–217

197

Reisman, 2006), understanding the epistemology of models in science (Treagust, Chittleborough, & Mamiala, 2002), and developing intuitions, predilections and skills at understanding complex phenomena in general (Hogan & Thomas, 2001; Mandinach & Cline, 1994a;
Schecker, 1993; Steed, 1992).
In short, modelling is both an important cognitive skill and a potentially powerful means of learning many topics. That is, it is both an end
goal and a means to other end goals.
The modelling activity addressed here is traditionally called system dynamics modelling (see Collins and Ferguson (1993) for a particularly comprehensive taxonomy of modelling). The model is comprised of real-valued variables that are constrained by temporal differential
equations. The variables denote quantities in the system whose values change over time. The job of the model is to predict those changing
values as accurately as parsimony allows.
There is a long history of using system dynamics model construction as in instructional activity. According to the oral history of the
System Dynamics Society (http://www.systemdynamics.org/oral-history/), system dynamics began to be used for university instruction
around 1957 with Jay Forrester’s formulation of system dynamics for teaching management. When a graphical language, Stella, became
available (Richmond, 1985), instructional usage dramatically increased and extended to high school. Many early Stella projects trained
teachers in modelling and let them invent activities (Mandinach & Cline, 1994a; Zaraza & Fisher, 1997).
After years of experience by hundreds of teachers, observers began to report that getting students to actually construct models took so
much class time that most teachers used Stella only for model exploration activities, wherein students were given a model and were asked
to observe how graphs of the variables values changed as the students manipulated parameters of the model (Alessi, 2000; Doerr, 1996;
Mandinach & Cline, 1994b; Stratford, 1997).
Laboratory studies conﬁrmed the observers’ reports about both the length of time required for model construction and the importance of
model construction. For example, Hashem and Mioduser (2011) found that students who constructed NetLogo models learned more about
emergence, self-organization and other complex system concepts than students who explored NetLogo models that were given to them. The
comparison took place during two 90-min lessons on complex systems, which were ﬂanked by a pre-test and a post-test. However, prior to
the pre-test, it took only 2 h to train the model exploration group whereas it took 48 h to train the model construction group. In a review of
the modelling literature, VanLehn (2013) found that the only experiments that produced reliable positive results for model construction also
devoted at least 5 h to training the students before the main lessons.
This history motivates the speciﬁc research problem addressed here: How can we speed up students’ acquisition of skill in constructing
system dynamics models?
Many methods for accelerating the acquisition of skill in model construction have been implemented, but only a few have been
compared to baseline versions of the model construction activity in order to test their effectiveness (see VanLehn, 2013, for a review). Of
those that have been evaluated, one form of scaffolding has shown considerable promise: The use of feedback and hints on student’s
steps in constructing the model. A whole model is usually composed of many parts (e.g., nodes, links, equations, labels, icons, numbers,
etc.) which the student enters one at a time. Entering such a part is called a “step”. Systems that give feedback and hints on steps are
called step-based tutoring systems (VanLehn, 2006). Step-based tutoring systems have been used for a wide variety of tasks besides
model construction, and appear to be almost as effective as human tutors (VanLehn et al., 2011). Thus, this project decided early on to
build a step-based tutoring system for system dynamics modelling in the hope that it would accelerate students’ acquisition of
modelling skill.
1.2. Learning strategies research
A learning strategy is a process, procedure or method that meets two criteria (Donker, de Boer, Kostons, Dignath van Ewijk, & van der
Werf, 2014): (1) Students can use the strategy when studying, but it is not required by the material that they are studying. (2) Using the
learning strategy is believed to affect the student’s learning. A good learning strategy is thought to improve students’ learning, while a poor
learning strategy is thought to harm the students’ learning. When used without modiﬁcation, “learning strategy” generally means a good
learning strategy. Some examples are:
 When memorizing facts, a good learning strategy is to construct a mental image and associate each fact with a part of the image.
 When studying an example, a good learning strategy (called self-explanation) is to explain each step in the example to yourself, asking
“Why is this true? Why did the author include this step?”
 When reading a text, a good learning strategy is to reﬂect afterwards on what you have learned.
 When reading a text, a poor learning strategy is to ignore words or passages that you don’t understand.
Learning strategies have been studied for decades, and comprehensive meta-analytic reviews exist (Donker et al., 2014; Hattie, Biggs, &
Purdie, 1996). Some of the main ﬁndings are:
A. Students often exhibit poor learning strategies.
B. Good learning strategies can be taught, often with little difﬁculty.
C. When students use the taught learning strategies, their domain learning often increases compared to students who are not taught to use
the learning strategies.
D. When instruction in the learning strategy includes meta-cognitive and motivational components, students can often be prevented from
reverting to poor learning strategies when the instruction ceases.
E. Speciﬁc learning strategies often have larger effect sizes than general purpose ones.
These ﬁndings mean that research on learning strategies is intimately linked to advances in instruction. Whenever a new instructional
method or subject matter is developed, there are likely to be poor learning strategies that are speciﬁc to it (ﬁnding A) as well as speciﬁc
good learning strategies that are likely to be effective (ﬁnding E) and easily taught (B). The key question is whether they are effective for

198

L. Zhang et al. / Computers & Education 75 (2014) 196–217

domain learning (C) and students can be persuaded to continue using them after being taught (D). We have developed a step-based
tutoring system for teaching students how to construct models, so it is likely that for this new form of instruction, students tend to
exhibit poor learning strategies, but that good strategies can be easily taught; the main questions are whether the good strategies really
are effective at increasing domain learning and whether students can be taught to persist in using them. These are the main research
questions addressed here.
The next introductory section focuses on reviewing projects that are similar to the one described here. However, their relationship to
learning strategies can be a bit hard to see, so a few prefatory remarks may be helpful.
Because this paper is concerned with teaching students how to construct models, and constructing a model is a kind of problem solving,
it is worth clarifying how learning strategies differ from problem solving strategies. When students are taught an effective strategy for
solving problems, the strategy qualiﬁes as either a learning strategy, a problem solving strategy or both depending on the instructional
objectives.
 If the objective is for students to solve problems well, then a strategy for solving problems counts as domain knowledge and is called a
problem solving strategy. In the jargon of students, it’s on the test.
 On the other hand, when problem solving is done only to give student practice in applying certain concepts and principles and the
problem solving itself is not an instructional objective, then the strategy counts as a learning strategy because it is optional and yet it
probably impacts the students’ learning of the concepts and principles. In the jargon of students, it is not on the test.
 As pointed out earlier, the process of constructing models is both an instructional objective (especially in math classes) and a means for
acquiring domain knowledge (especially in science classes). Thus, an effective strategy for constructing models counts as both a problem
solving strategy in some cases (it’s on the test) and a learning strategy in other cases (it’s not on the test).
For example, suppose the instructional objective is to learn which botanical features go with which plants, and the instruction involves
arranging cards labelled with plants and features. If this matching activity does not appear on the exams, then teaching students a
methodical method for arranging the cards counts as a learning strategy even though it is also a problem solving strategy. From the students’
point of view, what matters is whether the strategy is “on the test.” That is, if a problem solving strategy is an instructional objective and is
part of a test or other assessment, then students have a different attitude towards it than a strategy that is merely helpful for learning the
material that will be on the test. As we discuss various methods for teaching model construction, it is important to note whether students
believe a taught strategy is required or merely helpful.
Interactive instructional systems, such as the tutoring system described herein, have their own unique learning strategies. Here are some
examples:
1. Teachers often complain that their students would rather click than think. Actuating buttons, menus, etc. without thinking or even
reading the rest of the screen is a poor learning strategy.
2. When a tutoring system gives immediate feedback on the correctness of a student’s entry, then students often guess instead of think.
That is, they rapidly revise and resubmit an incorrect entry until the tutoring system says that it is correct.
3. If the tutoring system gives a sequence of hints that get gradually mores speciﬁc until they tell the student exactly what to do,
then students abuse the hint sequences by clicking rapidly on the Hint button until they get the ﬁnal hint that tells them what
to do.
4. A good learning strategy for tutoring systems is to ask for a hint only when you need one (Aleven, McLaren, Roll, & Koedinger, 2004).
5. After asking for hints from a tutoring system and ﬁnally making a correct entry, a good learning strategy is to reﬂect on why it is correct
(self-explanation) and whether the hint makes sense (Shih, Koedinger, & Scheines, 2008).
6. Examples 2, 3 and 4 are help-seeking strategies (Aleven, Stahl, Schworm, Fischer, & Wallace, 2003). Examples 2 and 3 are often referred
to as “gaming the system” (Baker, Corbett, Koedinger, & Wagner, 2004).
Feedback and hints are the signature methods of tutoring, but several projects have used feedback and hints for two distinct purposes, so
let us distinguish them as follows:
 Let domain knowledge refer to what students are supposed to learn. It is typically measured with a post-test and sometimes a pre-test.
 When the hints address the domain and the feedback indicates whether the domain knowledge has been correctly applied, the system
is said to be tutoring and the module responsible for it is called the tutor.
 A learning strategy is an optional method for using the system (i.e., it is not domain knowledge) that is believed to increase student’s
learning of domain knowledge.
 When the feedback and hints refer only to the learning strategy and whether it is being applied correctly, the system is said to be metatutoring and the module responsible for it is called the meta-tutor.
In this paper, meta-tutoring will only means a method for teaching students a learning strategy. However, in the tutoring literature,
“meta-tutoring” is used more broadly to mean using feedback and hints to teach anything other than domain knowledge. For instance,
meta-tutoring is sometimes used to increase motivation or change beliefs about self-efﬁcacy. Du Boulay, Avramides, Luckin, MartinezMiron, and Rebolledo-Mendez (2010) propose a framework that includes many types of meta-tutoring.
Now we can turn to reviewing prior work on using learning strategies to help students learn how to construct models.
1.3. Prior work on learning strategies for model construction
Betty’s Brain (Leelawong & Biswas, 2008) was a step-based tutoring system for constructing models that also taught a learning strategy. It
could give feedback and hints on the student’s model (tutoring) or it could give feedback and hints on the way that the student was using the

L. Zhang et al. / Computers & Education 75 (2014) 196–217

199

system to create the model (meta-tutoring).1 For instance, sometimes it would not permit the student to evaluate the model with an
instructor-provide test suite (called a “quiz”) until the student had ﬁrst examined speciﬁc predictions of their model (e.g., if air temperature
goes down, what does body temperature do?). The system included some multimedia resources on the task domain, so another part of the
learning strategy was encouraging students to read them. As these examples indicate, the learning strategy was speciﬁc to the particular
instructional features provided by system. This is consistent with the ﬁndings in the learning strategies literature, which suggest that such
speciﬁcity provides better results than general learning strategies.
Three evaluations of the effectiveness of Betty’s meta-tutor were conducted (Biswas, Leelawong, Schwartz, & Vye, 2005, study 2;
Leelawong & Biswas, 2008; Tan, Biswas, & Schwartz, 2006). Their methods will be described fully here, as the experiments to be reported
later used similar methods. The Betty’s Brain experiments all had two phases, called the training phase and the transfer phase here. During
the training phase, ﬁfth-grade students worked with Betty for approximately seven 45-min sessions on constructing a model of a river
system. One group of students used Betty’s Brain with the meta-tutor turned on, and another group used the same system with the metatutor turned off. Two months later the transfer phase occurred, where all the students used Betty’s Brain with the meta-tutor turned off to
create models for the nitrogen cycle. This transfer phase was used to assess their modelling skill.
The results were roughly the same in all three studies. Using the general results on learning strategy mentioned above as a framework,
the results from the Betty’s Brain studies were:
A. Students often exhibit poor learning strategies. True of the control conditions in all three studies.
B. Good learning strategies can be taught, often with little difﬁculty. In the training phase of all three studies, meta-tutored students’
behaviour was consistent with the learning strategies.
C. When students use the taught learning strategies, their domain learning increases compared to students who are not taught to use the
learning strategies. Unfortunately, using conventional science tests of ecology concepts, there were few signiﬁcant differences between
the meta-tutored students and the students who used Betty’s Brain without meta-tutoring.2 Using four measures of model quality,
meta-tutored students’ models were better than control students’ models on only one measure.
D. When instruction in the learning strategy includes meta-cognitive and motivational components, students can often be prevented from
reverting to poor learning strategies when the instruction ceases. Students who received meta-tutoring during the training phase tended to
continue using the learning strategy in the transfer phase when the meta-tutoring was turned off.
E. Speciﬁc learning strategies often have larger effect sizes than general purpose ones. Not tested.
In short, although the meta-tutor in Betty’s Brain was successful at teaching the learning strategy and persuading students to continue
using it, the learning strategy had little impact on domain learning. More recent work has focused on ﬁnding out what behaviours
distinguish good learners from poor learners (Segedy, Kinnebrew, & Biswas, 2012a, 2012b).
The Help Tutor (Aleven et al., 2004; Roll, Aleven, McLaren, & Koedinger, 2007a, 2007b; Roll, Aleven, McLaren, & Koedinger,
2011; Roll et al., 2006) was a meta-tutor that augmented an existing step-based tutoring system, the Cognitive Geometry Tutor
(www.carnegielearning.com). Although the tutoring system did not teach students to construct models, it is included in this review of
past work because it is an often-cited representative of using meta-tutoring to improve students’ learning from step-based tutoring.
The Geometry Cognitive Tutor allowed students to ask for a hint. The ﬁrst hint was rather general, but if the student kept asking for hints,
then the last hint told the student exactly what to do. This was called the “bottom-out” hint. Students sometimes clicked rapidly on the Help
button so that they could get to the bottom-out hint. As mentioned earlier, this abuse of the help system is a kind of “gaming the system”
(Baker, Corbett, Koedinger, et al., 2004). In order to reduce the frequency of gaming the system, the Help Tutor gave students feedback and
hints on their use of the help system.
Two studies evaluated the Help Tutor. As in the Betty’s Brain studies, the Help Tutor studies had both a training phase where the metatutor was used by half the students and a transfer phase where none of the students used the meta-tutor. Using the general ﬁndings
mentioned above as a framework, the results were:
A. Students often exhibit poor learning strategies. True of the control conditions in all both studies.
B. Good learning strategies can be taught, often with little difﬁculty. In the training phase of both studies, meta-tutored students’ behaviour
was consistent with the taught strategies.
C. When students use the taught learning strategies, their domain learning increases compared to students who are not taught to use the
learning strategies. Unfortunately, students taught the learning strategy did not differ from the control group in their acquisition of
geometry knowledge.
D. When instruction in the learning strategy includes meta-cognitive and motivational components, students can often be prevented from
reverting to poor learning strategies when the instruction ceases. Students who received meta-tutoring during the training phase tended to
continue using the learning strategy in the transfer phase when the meta-tutoring was turned off, albeit with less frequency.
E. Speciﬁc learning strategies often have larger effect sizes than general purpose ones. Not tested.
Betty’s Brain and the Help Tutor provided most of their instruction via feedback and hints. A somewhat more didactic approach is to
provide forms and phases that constrain students’ modelling behaviour. For instance, the ﬁnal version of Model-It (Metcalf et al., 2000) had

1
Those familiar with Betty’s Brain might be surprised to see it described as a tutoring system because the students see the system as two agents: Betty (a teachable agent)
and Mr. Davis (a mentor). Students were told that the model they were constructing comprised the knowledge of Betty, so editing the model comprised “teaching Betty.”
When they ask Betty take a quiz, Mr. Davis gives the student feedback on the correctness of the model’s predictions which would sometimes include unsolicited hints about
ﬁsh, algae, carbon dioxide and other domain entities. Mr. Davis personiﬁes the system’s tutoring. On the other hand, meta-tutoring was done by both agents. For instance,
Betty would sometimes refuse to take a quiz, and Mr. Davis would discourage students from using trial-and-error methods.
2
In a fourth study, the meta-tutored students produced slightly better river system models than the control students (Tan, Wagster, Wu, & Biswas, 2007; Wagster, Tan,
Biswas, & Schwartz, 2007; Wagster, Tan, Wu, Biswas, & Schwartz, 2007). However, results on the other measures were not reported.

200

L. Zhang et al. / Computers & Education 75 (2014) 196–217

4 phases, which were selected by clicking on one of four buttons labelled Plan, Build, Test and Evaluate. The Build mode was the actual model
editor. The other phases presented forms to be ﬁlled in by the student. Fig. 1 shows the form for the Test phase. Students were given feedback
and hints, which they could suppress if they desired, when they attempted to bypass a suggested activity. The Carnegie Learning’s Algebra
Cognitive Tutor (www.carnegielearning.com) and the Word Problem Solving Tutor (Wheeler & Regian, 1999) also scaffolded problem
solving strategies via lightweight constraints, implemented with phases and forms. None of these meta-tutors were evaluated separately
from the rest of the system.
On the other hand, Mulder, Lazonder, de Jong, Anjewierden, and Bollen (2011) did assess the effects of a phase-based learning strategy.
The experiments used Co-Lab, a system that taught system dynamics modelling. Co-Lab was not a step-based tutoring. Instead, students
received feedback only on the accuracy of the model’s predictions. The learning strategy was to encourage students to do their model
construction in three stages. In Stage 1, they deﬁned variables and drew undirected links between variables that directly affected each other
somehow. In Stage 2, they added qualitative labels to the links indicating whether an increase in one variable caused an increase or decrease
in the other variable. In Stage 3, they added equations to the model that further speciﬁed the relationships between variables. Three versions
of the learning strategy were compared to a control version of Co-Lab that lacked phases. The three versions differed in how strictly they
enforced the learning strategy. The restricted strategy required students to achieve a certain level of success in one stage before moving to
the next. The semi-restricted version of the strategy allowed students to move from one stage to the next at will, but they were not allowed
to move backwards. The unrestricted version allowed students to move at will between stages. Compared to the control version, all three
versions of the learning strategy increased the number of correct model elements generated by students as they used the system. The three
versions were not signiﬁcantly different in productivity from each other, but there was a trend for the semi-restricted version to be better
than the other two. However, this experiment included only a training phase and not a transfer phase, and it did not assess students’ domain
knowledge with pre- and post-testing. Thus, its consistency with the 5 general ﬁndings mentioned above (A through F) cannot be determined. Nonetheless, this work is interesting because the learning strategy was taught without using a meta-tutor, and the system was not a
step-based tutoring system.
The meta-tutoring of Betty’s Brain and the Help Tutor placed only weak constraints on students’ behaviour. The phases and forms of CoLab, Model-It, the Cognitive Tutors and the Algebra Word Problem tutor placed somewhat stronger constraints on students’ behaviour. At
the far end of this progression is procedural scaffolding, which places very strong constraints on student behaviour. The basic idea of procedural scaffolding is to require students to temporarily follow a speciﬁc procedure for constructing a model. Although the procedure is not
required by the task and there are many other ways to successfully construct models, the procedure is used as a temporary scaffolding to
guide students who might otherwise be quite lost.

Fig. 1. Scaffolding for the Test mode of Model-It (Metcalf, 1999).

L. Zhang et al. / Computers & Education 75 (2014) 196–217

201

Although procedural scaffolding was ﬁrst used by (Marshall, Barthuli, Brewer, & Rose, 1989) to scaffold arithmetic story problem solving,
its beneﬁts were not evaluated. Procedural scaffolding was ﬁrst evaluated with Pyrenees (Chi & VanLehn, 2010; Vanlehn & Chi, 2012), which
required students to construct models using a version of goal reduction, which is a well-known general purpose reasoning strategy used in
artiﬁcial intelligence applications (Russell & Norvig, 2009).
Pyrenees’ procedural scaffolding was evaluated in a two-phase experiment. In the training phase, students learned to construct model of
probabilistic systems. In the transfer phase, student learned to construct models of mechanical energy systems. Using the framework
mentioned earlier, the ﬁndings were:
A. Students often exhibit poor learning strategies. True of the control conditions in both phases.
B. Good learning strategies can be taught, often with little difﬁculty. This could not be determined, because Pyrenees required students in the
experimental condition to follow the learning strategy during the training phase.
C. When students use the taught learning strategies, their domain learning increases compared to students who are not taught to use the
learning strategies. In both the training phase and the transfer phase, the experimental group acquired more domain knowledge than
control group. The effect sizes were large (d z 1.0).
D. When instruction in the learning strategy includes meta-cognitive and motivational components, students can often be prevented from
reverting to poor learning strategies when the instruction ceases. The experimental group tended to use the learning strategy during the
transfer phase on difﬁcult problems, but not on simple problems. However, even on simple problems, they did not use poor learning
strategies.
E. Speciﬁc learning strategies often have larger effect sizes than general purpose ones. Not tested.
In summary, Betty’s Brain and the Help Tutor used the standard methods of hints and feedback and their meta-tutoring succeeded in
improving students’ behaviour, but there were only weak improvements at best in their learning of the domain. Co-Lab’s learning strategy
increased performance, but the effect on learning was not measured. Pyrenees used procedural scaffolding, and its meta-tutoring caused
large improvements in both behaviour and domain learning.
1.4. Our research questions
Our research questions are the same 4 questions that our predecessors have focused one:
A. Without instruction in good learning strategies, do students often exhibit poor learning strategies?
B. Can good learning strategies be easily taught?
C. When students use the taught learning strategies, does their domain learning increase compared to students who are not taught to use
the learning strategies?
D. When instruction in the learning strategy ceases, do students revert to poor learning strategies?
Although these 4 questions about learning strategies are the central focus of the research, we are also interested in seeing if the time
required to achieve adequate competence in model construction can be reduced from 5 or more hours to 2 for fewer hours.
Because Pyrenees was arguably more successful than the other methods, we chose to use procedural scaffolding as the basic method of
teaching learning strategies. However, there is a major difference between our research problem and the one addressed by Pyrenees.
Pyrenees assumed that students had already been taught all the relevant domain principles. Indeed, the students repeatedly selected
principles from a menu. In contrast, when students construct system dynamics models, they are seldom taught domain principles in
advance. They are instead asked to infer domain relationships from multimedia resources, common sense and/or experimentation. They do
this while constructing the model. This allows modelling to be used as part of enquiry-based instruction where students construct the
domain knowledge themselves. Having to infer domain relationships while learning to model may be one of the reasons why it takes
students so long to master system dynamics model construction. Nonetheless, inferring quantities and their relationships is exactly the skill
we want students to learn.
This suggests that if we require students to follow a procedure, as Pyrenees did, then they will learn that the key to modelling is a
single non-mechanical step: ﬁguring out the mathematical relationship among a set of directly related quantities. Once they have
mastered that step, we expect that they will be able to construct models well, even if they don’t follow the procedure. Thus, our research
question is whether applying the Pyrenees approach of heavy-handed procedural scaffolding to the cognitive skill of model construction
will cause deeper, more effective modelling skills to develop during the training phase. If the tutor does work in the training phase, our
next question is whether the beneﬁts will persist in the transfer phase. To answer these two questions, we developed an instructional
system, called AMT, and conducted experiments to evaluate it. The following sections described the system, how it was implemented, and
the evaluation at last.
The name “AMT” is an acronym for the overall project, which is called the Affective Meta-Tutoring project. The ﬁrst objective of the
project is to develop and test a meta-tutor that improves student’s learning of modelling by using procedural scaffolding as well as the
traditional feedback and hints. The results of that phase of the project are reported here. The second phase of the project will be to add an
affective learning companion to the AMT system; hence the term “affective” in the project name. The second phase will be described further
in the discussion section.
2. The AMT system’s design and behaviour
This section has three parts. The ﬁrst describes the student’s task, and in particular, the graphical language in which they write models.
The second section describes the tutoring system. The third section describes the meta-tutor and the strategy that it teaches students to use.

202

L. Zhang et al. / Computers & Education 75 (2014) 196–217

2.1. Constructing models in the AMT modelling language
In order to decrease the difﬁculty of learning how to construct system dynamics models, most prior work has used graphical modelling
languages where a model consists of several types of nodes and links. These graphical languages are easier to learn than text-based languages (Löhner, Van Joolingen, & Savelsbergh, 2003). The traditional “stock and ﬂow” language has two types of links, and one of them (the
ﬂow links) acts somewhat like nodes. In pilot studies, our high school students found this confusing, so we removed the confusing type of
link.
In our modelling language, a model is a directed graph with one type of link. Each node represents both a variable and the computation
that determines the variable’s value. The inputs of that computation, which are themselves variables, are indicated by incoming links. In
Fig. 2 for example, the computation for “births” requires the values of “growth rate” and “population.” The value of a variable is a real
number that can change over time, where time is represented discretely.
There are three types of nodes:
 A ﬁxed value node represents a constant value that is directly speciﬁed in the problem. A ﬁxed value node has a diamond shape. It never
has incoming links.
 An accumulator node accumulates the values of its inputs. That is, its current value is the sum of its previous value plus its inputs. An
accumulator node has a rectangular shape and always has at least one incoming link.
 A function node’s value is an algebraic function of its inputs. A function node has a circular shape and at least one incoming link.
The students’ task is to draw a model that represents a situation that is completely described by a relatively short text. For instance, Fig. 2
is a correct model for the following problem:
Rust destroys steel and can spread quickly. Suppose that you take a large sheet of steel, such as one that might be used as the roof of the boxcar
on a train, and you put it outside in the weather. Suppose it starts with a spot of rust that is 10 square inches in area. However, each week the
rust spot gets bigger, as it grows by 30%. Therefore at the end of the ﬁrst week, the rust spot is 13 square inches in area. Graph the size of the rust
spot over 10 weeks.
Such a text contains all the information that students need. However, it sometimes contains extra quantities (e.g., the rust spot at the end
of the ﬁrst week) that are not needed in the model. Students are asked to draw only the necessary nodes, so drawing a node for the rust spot
at the end of the ﬁrst week is an indication of shallow modelling.
2.2. The tutoring system
Many tutoring systems have a sequence of feedback and hints (VanLehn, 2006). When the student makes an entry, the tutoring system
ﬁrst just tells the student whether the entry is correct or incorrect. If the student asks for help again, the system gives a general hint.
Subsequent requests for help on the same entry generate increasingly speciﬁc hints. Eventually, the ﬁnal hint (called the “bottom-out hint”)
tells the student what the correct entry is. There is some evidence that the mid-level hints are not pedagogically useful (Muldner, Burleson,
van de Sande, & VanLehn, 2011; Timms, 2007), so our tutoring system does not use them. Instead, it generates just the ﬁrst and last members
of the typical hint sequence. More speciﬁcally, it has a Check and a Give-up button. Clicking on the Check button causes the tutoring system
to give minimal feedback: It colours entries red if they are incorrect and green if they are correct. Clicking on the Give-up button causes the
tutoring system to ﬁll in entries correctly, that is, to give a bottom-out hint. This section describes the AMT model editor, pointing out where
the Check and Give-up buttons appear.
The system presents itself to the student as a large tabbed window (Fig. 3). The Model tab (shown in the ﬁgure) is for drawing models.
The Situation tab shows a textual representation of the problem, and is illustrated by a static picture. The Instructions tab contains a slide
deck that teaches students the basics of the modelling language, the user interface, the modelling process and the learning strategy. Students can access the Introduction and Situation tab at any moment during the construction of the model (cf. Section 4.4).
The Model tab has three buttons: Create Node, Run Model and Done. The Done button is disabled (grey) until the student has completed a
problem successfully (i.e. created an accurate model of the system), at which time clicking on the Done button advances the system to the
next problem.

Fig. 2. A model. The grey bubbles have been added to this ﬁgure in order to show how the value of each variable is calculated.

L. Zhang et al. / Computers & Education 75 (2014) 196–217

203

Fig. 3. The tutoring system’s screen.

The Create Node button creates a new node symbol in the model area and opens the Node Editor on it. The Node Editor (Fig. 4) is divided
into ﬁve tabs: Description, Plan, Inputs, Calculations and Graph. Students can edit all tabs with the exception of the Graph tab, where a graph
showing the evolution of the node’s value over time is generated by the tutoring system once the model is run. To create a node, the student
ﬁlls out the four tabs in order, left to right. Students can also change the information within a node by double clicking on the node shape
within the model area, which opens the Node Editor, and then selecting the tab they wish to modify.
The Description tab was engineered to facilitate grounding, where “grounding” is the process of negotiating mutual understanding of the
meaning of a term between two participants in a conversation (Clark & Brennan, 1991). In system dynamics model editors that are not used
for tutoring, such as Stella, Vensim and Powersim, users merely type in the name of the node that they want. This allows them to use names
such as “x” that do not match anything in the problem. While this is unproblematic for such editors, it makes it difﬁcult or impossible for a

Fig. 4. The node editor, showing the Description tab.

204

L. Zhang et al. / Computers & Education 75 (2014) 196–217

tutoring system to determine what quantity the student’s node denotes, and hence the tutoring system cannot determine whether the node
is deﬁned correctly. Urging the students to choose adequately precise names is only partially successful. In one study, only 78% of the node
names could be identiﬁed by the tutoring system (Bravo, van Joolingen, & de Jong, 2009). This is a case of bad grounding: the tutoring system
did not understand the meaning of 22% of the students’ terms.
On the other hand, some tutoring systems for system dynamics modelling, including the ﬁrst version of this system, provide students
with nodes that already have names but are otherwise undeﬁned (VanLehn et al., 2011). Unfortunately, students often do not pay enough
attention to the names, which can be rather similar. Students sometimes create models that are correct except that the names of two nodes,
such as “rust growth factor” and “new rust area per week”, have been switched. This is also a case of bad grounding: the student did not
understand the meaning of the system’s terms.
The Description tab of AMT, illustrated in Fig. 4, is intended to prevent bad grounding. First, the student walks through the tree in the
Description tab located in the large box at the top of the window. Clicking on a leaf in the displayed tree selects both a description and a
name for the node. Each problem has a different tree, and the tree’s contents are engineered to make the student select among subtly
different descriptions. After selecting a leaf, the student clicks on the Check button. If the selected description denotes a quantity that the
system understands, and that quantity does not already have a node deﬁned for it, then clicking on the Check button turns some boxes
green, as shown in Fig. 4. Otherwise, the boxes turn red. Students may also click on the Give-up button that ﬁlls out the tab correctly but
colours the boxes yellow. When the student exits the node editor, the node’s name, which is shown beneath the node, is highlighted by the
colour of the Description tab’s boxes. Thus, a student who had given up on the Description tab would forever see yellow highlighting on the
node’s name. This feature is intended to discourage giving up.
When the Description tab is completed correctly and its boxes are either green or yellow, then students can go on to the Plan tab. The Plan
tab lets students choose among 7 different plans (Fig. 5). The Instruction tab describes the meaning of these plans and gives an example of
each plan. After a selection is made, students may click on the Check button and see their selection coloured green (correct) or red
(incorrect). Clicking on the Give-up button causes the correct plan to be selected and coloured yellow. Unlike the other tabs, ﬁlling out the
Plan tab correctly is optional. Students can go to the Inputs tab regardless of how their plan selection is coloured, and they can skip the Plan
tab entirely if they want.
The Inputs tab (Fig. 6) allows students to indicate whether the node’s value is a ﬁxed, given constant or if it is computed from other nodes’
values. In the latter case, they click on “Inputs:” and choose some of the existing nodes as inputs, which causes links are drawn between
those nodes and the current node. When students see that the input they want is not in the list because they have not yet created a node for
it, they can click on the convenient “Create a new node” button, deﬁne the desired node using a pop-up version of the Description tab, then

Fig. 5. The plan tab.

L. Zhang et al. / Computers & Education 75 (2014) 196–217

205

Fig. 6. The Inputs tab.

return to this Input tab by closing the pop-up. As always, the Check and Give-up buttons colour the student’s choices in either red (incorrect),
green (correct) or yellow (gave up). While red choices can be revised later on, green or yellow choices cannot be changed.
When the Inputs tab is ﬁlled out correctly, students can go to the Calculation tab (Fig. 7). If they had selected “Fixed Value” on the Inputs
tab, then “has a ﬁxed value” is checked here on the Calculation tab, and so all they need to do here is enter the number that is the value of the
Fixed Value node. On the other hand, if they had selected “Inputs:” on the Inputs tab, then they must click either the “accumulates the values
of its inputs” button or the “is a function of its inputs” button. This selection determines which form appears in the lower part of the tab.
Fig. 7 shows the tab to ﬁll for the Accumulator node type on the left, and the Function node type on the right. For both the Function and
Accumulator nodes, the inputs appear initially in the “Available inputs:” box, and then move rightward into the calculation box as they are
clicked. In this fashion, students enter a calculation.
As a model is being constructed, the state of the node’s deﬁnition is indicated by little circular indicators (“i” for input, “c” for calculation
and “g” for graph) and the node’s outline (see Fig. 8). A dotted border indicates that the type of the node hasn’t been deﬁned yet. A blue
border means that the node’s deﬁnition is complete.
When all the nodes have blue borders, the student can click on the Run Model button. If there are syntactic errors in the model that
prevent running it, a pop-up window describes them. Otherwise, Run Model colours the “g” indicators on every node either green or red,
depending on whether the node’s graph is correct or not. Opening the Graph tab of a node displays both the user’s model’s graph and the
expected graph (see Fig. 9). Students can use the difference in the graphs as a clue to where the bug in the model could be found.
A model is considered completely correct when all the graphs match, in which case all the “g” indicators are green. At this point, students
can click on the Done button and go to the next problem.
This is a step-based tutoring system (VanLehn, 2006) because it can give feedback on every step taken by the student. It only gives such
feedback when the student clicks on the Check button. The feedback is minimal: just correct vs. incorrect. Absent are the usual sequences of
hints that many step-based tutoring systems have. However, the “Give-up” button implements the bottom-out hint in that it gives away
exactly what step the student should do at this point.
The AMT system has a mode, called test mode, which is used to assess students’ skill at modelling. Although students can still debug their
model by running it and seeing which graphs are correct, they cannot use the Check and Give-up buttons on any of the tabs that they ﬁll out,
with one exception. The Check button is always enabled on the Description tab. This is because the system and the student must agree on the
meaning of nodes. If the system doesn’t know which quantity is denoted by the student’s node, then it can’t know which graph is correct and
can’t colour the “g” indicators. In test mode, the Check button is enabled only on the Description tab, whereas in training mode, it is enabled
everywhere. In test mode, the Give-up button is never enabled.

206

L. Zhang et al. / Computers & Education 75 (2014) 196–217

Fig. 7. The Calculations tab.

2.3. The meta-tutor
So far, only the tutoring system has been described. It is essentially just a model editor with Check and Give-up buttons. This section
described the meta-tutor, which is the remaining module of the AMT system. This section begins by describing and motivating the learning
strategy that the meta-tutor teaches, then describes how it teaches that strategy.
Like Pyrenees (Chi & VanLehn, 2010), the meta-tutor teaches students a simple, goal reduction procedure for constructing models called
the Target Node Strategy. The basic idea is to focus on one node at a time (the target node) and complete all the tabs for this node before
working on other nodes. As students complete the target node, they may create new nodes as a side effect, but the new nodes will only be
named and not fully deﬁned. These nodes are displayed with dotted borders. Thus, when students have ﬁnished the target node, they can
pick any node that has a dotted border as the next target node, and begin working on deﬁning it. When there are no more nodes with dotted
borders, the model is complete.
There are sound reasons to think that the Target Node Strategy might help students learn more effectively, but it will take several
paragraphs to describe them. These paragraphs also illustrate the distinction between deep and shallow modelling practices.
The key steps in the Target Node Strategy are ﬁlling out the Inputs tab and the Calculation tab. These steps correspond to the moment
where students must analyze the given system information and determine the quantitative relationships between the target node’s
quantity and other quantities in the problem. That is, given a particular target quantity, students must ﬁnd a set of quantities such that the

Fig. 8. An incomplete model.

L. Zhang et al. / Computers & Education 75 (2014) 196–217

207

Fig. 9. The Graphs tab.

target quantity’s value can be computed as a simple function of their values. The whole problem of constructing a model of a system can be
decomposed into making a series of decisions like this one. One might call these decisions “atomic modelling problems”, as they cannot be
easily decomposed further.
For example, consider the system described earlier:
“Rust destroys steel and can spread quickly. Suppose that you take a large sheet of steel, such as one that might be used as the roof of the boxcar
on a train, and you put it outside in the weather. Suppose it starts with a spot of rust that is 10 square inches in area. However, each week the
rust spot gets bigger, as it grows by 30%. Therefore at the end of the ﬁrst week, the rust spot is 13 square inches in area. Graph the size of the rust
spot over 10 weeks.”
Suppose students are following the Target Node Strategy and have decided that the ﬁrst target node is the size of the rust spot. They
create a node and select “rust spot area” as its description. Now they face an atomic modelling problem. How to determine the value of rust
spot area? What we want students to think is something like, “I know the rust spot area is changing, and the value increases every week”.
This reasoning sufﬁces for ﬁlling out the Plan tab correctly. Next the students need to ﬁll out the Input tab, so they should think, “The rust
spot area is increased by the rust produced during the week”. There are no other nodes at the moment, so the student must click on “Create a
new node.” The student browses the available descriptions, and chooses the one that comes closest to the student’s idea of “rust produced
during the week.” Clicking on this description deﬁnes the node new rust area per week. The student can then select this node as an input for
the node rust spot area. Next comes the Calculation tab, which requires for the student to turn his/her qualitative understanding of the
relationship into a formal, mathematical one: the student should decide that the next value of rust spot area is its current value plus new rust
per week. This illustrates how the user interface decomposes the key reasoning into three major steps, corresponding to the Plan, Input and
Calculation tabs. These correspond roughly to the phases of Co-Lab (Mulder et al., 2011) and Model-It (Metcalf et al., 2000). This paragraph
also illustrates how the key reasoning should be done: by thinking hard about the system, its quantities and their interrelationships, and
then abstracting the mathematical relationships from them. This is the “deep” way to solve an atomic modelling problem.
However, there are shallow ways to solve the atomic modelling problem as well. When students need to create nodes in order to ﬁll out
the Inputs tab, there are only a ﬁnite number of choices: the leaves in the tree of descriptions on the Description tab. Thus, they can work
through all combinations until they have found the appropriate inputs. When the Check button is available and the problem is simple, this
can be done rapidly. For instance, the rust spot area problem’s Description tab has 7 possible descriptions, of which only 4 are legal
quantities in the problem (1 is an extra node; 3 are necessary for the model). It will not take the student long to create all the legal nodes, and

208

L. Zhang et al. / Computers & Education 75 (2014) 196–217

then test each one to see if it turns the Inputs tab green. Such extensive guessing is one “shallow” method for solving atomic modelling
problems. The Give-up button is another. Other methods involve looking for keywords (e.g., “initially”) or patterns.
Many of our students’ shallow modelling methods involve abusing the Check button or the Run Model button, so their behaviour is a
form of gaming the system (Baker et al., 2006; Baker, Corbett, & Koedinger, 2004), which is what the Help Tutor (Roll et al., 2011), Scooter the
Tutor (Baker et al., 2006), and other tutoring systems have tried to address. However, studies of modelling that did not use tutoring systems
have also noted this propensity of students to do shallow modelling (Alessi, 2000; Booth Sweeney & Sterman, 2000; Doerr, 1996; Mandinach
& Cline, 1994b; Nathan, 1998; Zaraza & Fisher, 1999). Thus, we prefer to refer to the phenomenon as “shallow modelling” rather than
“gaming the system.”
Teaching the Target Node Strategy does not require that student do deep modelling, so one might wonder why we think it could help
students model better. However, it does decompose the whole problem of modelling a system into a series of atomic modelling problems,
and even decomposes an atomic modelling problem into three major steps. Like Pyrenees, it teaches students that if they just master this
one difﬁcult but small skill (deep modelling applied to atomic modelling problems), then the rest of the problem solving is purely
mechanical.
Having described and motivated the Target Node Strategy, it is ﬁnally time to describe how the meta-tutor teaches it. First, the Introduction slides describe the strategy brieﬂy. Even students who use AMT with the meta-tutor turned off see this presentation of the strategy.
Also when the meta-tutor is turned off, students can edit any node at any time. Moreover, they can ﬁll out some of a node’s tab, then quit
editing the node and come back to it later. This freedom is removed when the meta-tutor is turned on. Students are required to correctly ﬁll
out each tab before moving on to the next, and they must ﬁll out all tabs before closing the node editor. When they are on the Description tab
and choosing which quantity to create a node for, if they choose one that would not be selected by Target Node Strategy and yet it will
eventually be include in the model, then the selection turns blue and a pop-up window says, “That quantity is in the correct model, but it is
too early to deﬁne it now.” This message is typical of others that pop up when students stray from the Target Node Strategy. In short, the
main job of the meta-tutor is simply to keep students on the one of the paths that are consistent with the Target Node Strategy.
In addition to requiring students to follow the Target Node Strategy, the meta-tutor enacts a bit more scaffolding. For completeness, this
section describes the remaining forms of help.
When the meta-tutor is turned on, it complains if students appear to be guessing too much or giving up too early, just as the Help Tutor
did. When a student clicks on the Check button on the same tab twice within 3 s and gets red both times, the meta-tutor complains that the
student is guessing. If the student clicks on the Give-up button without ﬁlling out anything and checking it, the meta-tutor complains that
the student is giving up too early.
Some of the training problems let students practice debugging an incorrect model. They present a model that will run but generates
incorrect graphs. This kind of situation occurs frequently when the system is in test model (the tutor and meta-tutor are turned off). When
the model has been run, students see that some nodes have red “g” indicators and some have green ones. They face a decision of where to
start looking for an error in the model. The Introduction slides teach all students two heuristics:
 When possible, start by examining a node that has a red “g” indicator and no incoming links from nodes with red “g” indicators. Such a
node is guaranteed to have an error inside it.
 Avoid editing nodes that have green “g” indicators, because if the graph is correct, the node’s deﬁnition is probably correct.
When the meta-tutor is on and students are working on a debugging problem, it constrains them to obey these two heuristics.
In summary, the meta-tutor actually uses three types of scaffolding: (1) it requires students to follow the Target Variable Strategy; (2) it
complains when they abuse the Check or Give-up buttons; and (3) it teaches some heuristics for locating errors in buggy models. Although
(1) is procedural scaffolding, for convenience, we refer to this collection as “the meta-strategy.”

3. System architecture and implementation
The AMT system can be divided into two parts: the tutor and the meta-tutor. The tutor’s main functionalities included drawing a model,
checking correctness and running the model. It is an example-tracing tutor (VanLehn, 2006) in that an author must provide a correct model
with each problem; the students’ work is checked against that model. On the other hand, the meta-tutor is driven by algorithms (e.g., the
Target Node Strategy) that work for any problem. The student’s actions are check against the actions selected by the meta-strategy.
Although the meta-tutor and the tutor belong to the same Java project, they don’t share objects in the memory. This made it possible to
develop the tutor and meta-tutor programme independently. It also facilitates data analysis as will be explained later. More details about the
tutor’s architecture and implementation are presented in (Gonzalez-Sanchez, Chavez-Echeagaray, VanLehn, & Burleson, 2011). This section
describes the meta-tutor only.
Between the tutor and meta-tutor, there are three kinds of communication, each with its own channel as shown in Fig. 10:
(1) Every action made by student is sent to the meta-tutor via the activity channel.
(2) Before executing certain user actions, such as closing a node, the tutor sends a message to the meta-tutor via the block channel and
meta-tutor responds to the tutor, indicating whether the action should be blocked or not.
(3) Whenever the meta-tutor detects that the student needs an unsolicited hint, it sends a command to the tutor via the message channel to
tell it to initialize the tutorial dialogue. The tutor sends back the student’s response. Based on the response, the meta-tutor tells the tutor
either to display another dialogue box or to close the dialogue.
The meta-tutor is a production system, implemented in Java using Drools Expert (http://www.jboss.org/drools/drools-expert.html). The
production system implements two major functionalities: requiring the student to follow the Target Node Strategy and discouraging
gaming. The following sections describe the implementations of each function.

L. Zhang et al. / Computers & Education 75 (2014) 196–217

Tutor

Activity
channel

Student’s actions
Query at student’s actions

Message Block
channel channel

Response[block or not]

Activity
channel

209

Metatutor

Block Message
channel channel

Tutorial dialog
Student’s answer to dialog
Fig. 10. How the tutor and meta-tutor communicate with each other.

3.1. Constraining students to follow the Target Node Strategy
One main function of the meta-tutor is to constrain the student to follow the Target Node Strategy. In order to do so, it implements the
algorithm shown in Fig. 11. Aside from a little bookkeeping, the procedure’s actions are either “Prompt and constrain the student to do
<step>,” or “Constrain the student to do <step>”.
Prompting means that as soon as the problem state allows a step to be done, the meta-tutor gives the student an unsolicited hint about
the step. Here are examples of production rules that implement prompting:
 IF the activity channel reports that the student’s action is “closed a node”
AND there are at least two nodes that can become the next target node
THEN
send a command via the message channel to show a dialogue box that lists the nodes and ask which one the student wants to choose
as the target node.
 IF the context is that the node editor is open and the current tab is the plan tab,
AND the activity channel reports that the student’s action is “clicked on the Check”
AND the correct plan of this node is “a ﬁxed, given number”
AND the student has selected this plan
THEN
send a command via the message channel to show a message that says that the next step is to ﬁll in the Inputs tab by clicking on
“Value is ﬁxed, so no inputs.”
Constraining the student to do a step means that if the problem state is such that the student should do a certain action, and the student
tries to do another action instead, then the meta-tutor blocks the student’s action from occurring and pops up a message indicating the right
action to do. Some examples of production rules that implement constraining are:
 IF the context is that the student is in the model tab and the node editor is not open
AND the student’s action is clicking on a node in order to open it in the node editor
AND the node that student is trying to open is not the target node
THEN
send via the block channel the message “Please focus on your target node, <target node name>” and block opening the clicked-on
node.
 IF the context is that the node editor is open
AND the student’s action is closing the node
AND the node’s calculation tab is not ﬁnished yet
THEN
Send via the block channel the message “Please ﬁnish the target node, <target node name>, before leaving the editor” and block
closing of the node editor.
In order for the meta-tutor to track the student’s progress and respond appropriately, its rules need to have up-to-date information about
the state of the problem. Using the activity channel, the tutoring system sends all student actions to the meta-tutor, which converts some of
them (e.g., not node movements) into elements in the production system’s working memory. The working memory is also used to store the
identity of the target node, the contents of the sought set and other bookkeeping information that is kept updated via production rules.
3.2. Discouraging gaming
As mentioned earlier, the Give-up and Check buttons can be easily misused by students to construct models without thinking deeply or
learning, e.g., “gaming the system” (Baker et al., 2006; Baker, Corbett, Koedinger, et al., 2004). Although constraining students to follow the
Target Node Strategy may discourage gaming, the meta-tutor also looks for patterns of student actions that indicate gaming is occurring.
When it sees such a pattern, it pops up a warning message but does not block the student’s actions.
Detecting and responding to gaming of the Check button is implemented by the ﬁnite state machine shown in Fig. 12. The students are
always in one of 5 states. When they start working on a problem, they start in state S1 (the node editor is not open). Clicking on a node opens
the node editor and moves to state S2 (no check yet). Edits to the contents of the open tab can occur in any of the states except S1, and are not
shown in Fig. 12, as they merely add a loop from a state back to itself. The arc label “Wrong check” means that the student clicked on the
Check button and got red, indicating the tab was not ﬁlled out correctly. The arc label “Got the answer right” means the student either

210

L. Zhang et al. / Computers & Education 75 (2014) 196–217

1. Initialize the target node to the top level goal (i.e., the quantity the problem wants graphed).
Initialize the sought set to null.
2. Constrain the student to create a node and fill out its Description tab to correspond to the top level
goal quantity.
3. Constrain the student to fill out the Plan tab correctly.
4. Prompt and constrain the student to fill out the Input tab correctly. Add any newly created nodes
to the sought set.
5. Prompt and constrain the student to fill out the Calculation tab correctly.
6. Constrain the student to close the node editor.
7. If there are no nodes in the sought set, then prompt and constrain student to run the model.
If there is just one node in the sought set, then set it as the target node and tell the student.
If there are two or more nodes in the sought set, ask the student which one should be the target
node and set it to be the target node.
8. Remove the target node from the sought set.
9. Prompt and constrain the student to create a node and fill out its Description tab to correspond to
the target node.
10. Go to step 3 above.
Fig. 11. The Target Node Strategy implemented by the meta-tutor.

clicked on the Check button and got green, or clicked on the Give-up button. Whenever the student enters state S4 (gaming detected), the
meta-tutor sends a randomly chosen message such as, “Guessing so quickly wastes the opportunity to learn.”
In short, because the main job of the meta-tutor was to teach students to stay on a certain path deﬁned by the meta-strategy by blocking
off-path actions, and the meta-strategy was simple and easily deﬁned, the only signiﬁcant technical challenge was integrating the metatutor with the tutor. The tutor was implemented with a standard model-view-controller architecture, and the meta-tutor essentially was
given a chance to intervene after the user’s actions had actuated the control and before the model was updated.
At this point, the design and implementation of the AMT system have been deﬁned, so it is time to consider whether it works. That is,
does meta-tutoring improve students’ learning compared to tutoring without meta-tutoring?
4. Evaluating the meta-tutor
So far, we have conducted 5 studies of the AMT system. In the ﬁrst two studies, which were conducted in the summer of 2010, students
used the tutoring system without the meta-tutor. This led to signiﬁcant changes in the design of the tutoring system (VanLehn et al., 2011),
the materials and the experimental procedure (VanLehn et al., 2011). The two studies also produced data revealing students’ deep and
shallow modelling practices, and thus allowed us to design the meta-tutor. This article presents the results of the next three studies. There
were slight changes to the tutor and the experimental procedure in between the studies 3 and 4, which were conducted in summer 2011.
Analysis of the data from these two studies over the next year led to signiﬁcant changes to the AMT system, such as adding the Plan tab,
which led to study 5, which was conducted in summer 2012.
All the three studies had two phases, training and transfer, as did most of the studies reviewed earlier. During the training phase, a metatutor taught students both to use a good learning strategy (adapted from Pyrenee’s) and to avoid using poor learning strategies. During the
transfer phase, the meta-tutor and the tutor were turned off, thus allowing us to measure both domain learning and spontaneous use of the
learning strategy.
4.1. Research hypotheses
Recall that our framework, adopted from the literature on learning strategies, poses four research questions:
A. Without instruction in good learning strategies, do students often exhibit poor learning strategies?
B. Can good learning strategies be easily taught?
C. When students use the taught learning strategies, does their domain learning increase compared to students who are not taught to use
the learning strategies?
Close the node
/leave the tab

Close the node
/leave the tab

S1

Wrong check
& interval > 3s
Wrong check

Open a node
Get to the input tab

S2

S3

Got the answer Got the answer right
right

Interval
>3s

Wrong check
& interval < 3 sec

Close the node
/leave the tab

S5

Got the answer right

Close the node
/leave the tab

S4
Wrong check
& interval < 3s

S1: no opened node editor
S2: no check
S3: one wrong check
S4: gaming
S5: got right

Fig. 12. Finite state machine for gaming the Check button.

L. Zhang et al. / Computers & Education 75 (2014) 196–217

211

D. After instruction in the learning strategy ceases, do students revert to poor learning strategies?
Our observations during studies 1 and 2 answered question A by demonstrating that our students often use poor learning strategies
when they are not taught good ones. Moreover, because the meta-tutor requires students to follow the Target Node Strategy during the
training phase, question B cannot be addressed by our studies. This leaves us to focus on C and D.
Addressing question C requires that we clearly deﬁne the expected domain learning. As mentioned earlier, two entirely different
instructional objectives are associated with model construction: Using model construction to learn domain concepts and principles (usually
in a science classes) and learning how to construct models (usually in math classes). In these studies, we focus only on the second objective,
so “domain learning” in question C means learning how to do deep model construction. Moreover, question C can be asked of both the
training phase, where the learning strategy is taught, and the transfer phase, when the meta-tutor and tutor are turned off. Thus, the speciﬁc
hypotheses we tested in studies 3, 4 and 5 are:
1. In the transfer phase, do meta-tutored students display deep modelling more frequently than students who were not meta-tutored?
2. In the training phase, do meta-tutored students display deep modelling more frequently than students who do not receive metatutoring?
3. In the transfer phase, do meta-tutored students follow the Target Node Strategy more frequently than students who were not metatutored?
Although we predict positive answers for all three questions, there is a caveat concerning the third hypothesis. The instruction used in
studies 3, 4 and 5 did not provide much meta-cognitive and motivational encouragement to use the learning strategy, because we are
developing that part of the instruction for use in later studies. Thus, we have low expectations for hypothesis 3.
4.2. Method
Students were randomly assigned to one of two conditions: with and without meta-tutoring. The difference between the conditions
occurred only during a training phase where students learned how to solve model construction problems. The meta-tutor group solved
problems with the meta-tutor turned on, while the control group solved the same problems with the meta-tutor turned off. During the
training phase, all students could use the Check and Give-up buttons at any time.
In order to assess how much students learned, a transfer phase followed the training phase. During the transfer phase, all students solved
model construction problems with almost no help. That is, the meta-tutor and the Give-up button were turned off, and the Check button was
turned off everywhere except on the Description tab where it remained enabled in order to facilitate grounding, as mentioned earlier.
Because system dynamics is rarely taught in high school, the procedure did not include a pre-test in modelling dynamic systems.
In order to provide a motivation similar to that which occurs in school, we told students that prizes would be awarded to students who
solved the most problems during the transfer phase. In particular, we repeatedly told them to use the Check and Give-up buttons judiciously
during the training, trying always to learn as much as possible, so that they could rapidly solve problems during the transfer phase when the
Check and Give-up buttons would not be available.
4.3. Participants
There were 34 student participants in the ﬁrst experiment, 44 students participated in the second experiment, and 34 students in the
third experiment. No person participated in more than one experiment. All were high school students who were participating in summer
camps at our university.
4.4. Procedure
The three experiments followed the same procedure. Each lasted two and a half hours and had two main phases, training and transfer.
Sensors recorded students’ physiological states throughout both the training and transfer phases. The sensors were: wireless skin
conductance bracelets, facial expression cameras, posture-sensing chairs, and pressure sensitive mice. Periodically, a window would pop up
and ask students to report their affective state. These data are being used to develop algorithms for affect detection, and will not be discussed further here. Also in the ﬁrst two experiments, but not the third, students were asked to speak their thoughts aloud into a headset
microphone, and their verbal protocols were recorded by screen-capture software.
The summer camp students were available for only a ﬁxed period of time and all needed to be kept occupied productively during that
period. Thus, each phase of the experiment lasted a ﬁxed period of time, and students solved as many problems as they could during that time.
We obtained parental consent for minors prior to the experiment. During the experiment, student gave informed consent, ﬁlled out a
background questionnaire, donned sensors and started the training phase. The training phase lasted a total of 75 min. It consisted of ﬁrst
studying a sequence of PowerPoint slides that introduced them to the user interface, model construction and the Target Node Strategy, and
then solving a sequence of model construction problems. The introduction slides remained available while students were solving problems.
During pilot testing, we noticed that some students spent a long time studying the introduction then rarely referred back to it, whereas
other studied the introduction brieﬂy and referred back to it frequently as they solved problems. Thus, we let students decide how to allocate
their 75 min between the studying introduction and solving the training problems. The training phase was followed by a 15-min break
where students went to another room and had a snack. After the break, all the students began the transfer phase, where both conditions
were identical. The transfer phase lasted for 30 min, and a debrieﬁng of the students followed.
Students in both conditions solved the same problems in the same order. Except for a few debugging problems during the training phase,
where student were asked to correct a given faulty model, both training and transfer problems required students to construct a correct

212

L. Zhang et al. / Computers & Education 75 (2014) 196–217

model, which is deﬁned as a model whose graphs match graphs of correct values. When the model was correct, students were allowed to go
on to the next problem. This procedure was followed for both the training and transfer phases.
The only procedural difference between the training and transfer phases was the amount of scaffolding available. During the training
phase, the Check and Give-up buttons were enabled, and the meta-tutor was active for students in the meta-tutor condition. During the
transfer phase, the Check button was enabled only on the Description tab because it was necessary for grounding, as discussed earlier. None
of the other scaffolding was available during the transfer phase.
4.5. Measures
To test the hypotheses and answer the research questions, 7 measures were deﬁned by extracting information about student’s interaction with software from students’ log ﬁles. The measures are described in this section.
Hypothesis 1 is that the meta-tutored students will use deep modelling more frequently than the control students during the transfer
phase. Deep modelling is not easy to measure directly, so we used three indirect indicators:
 The number of the “Run model” button presses for completing one problem in the transfer phase indicates how much help the student needs
from the system. Deep modellers should be able to complete problems using the Run model button only a couple of times per model.
 Another measure was the number of extra nodes created during the transfer phase, where extra nodes are deﬁned as the nodes that can
be legally created for the problem but are not required for solving the problem. Deep modellers should realize that these quantities are
irrelevant and therefore avoid modelling them.
 The number of problems completed during the 30 min transfer period was considered to be an indicator of deep modelling with the
assumption that it would be faster for students to solve atomic modelling problems deeply than shallowly.
Hypothesis 2 is that meta-tutored students should use deep modelling more frequently than the control group students during the
training phase. The three dependent measures used to evaluate this hypothesis are described next.
 Help button usage: Ideally, a deep modeller would ﬁll out a tab, click on the Check button and the tab’s ﬁelds would turn green (correct).
A student who is trying to do deep modelling but hasn’t mastered the skill might have to click the Check button twice, thinking hard
before each attempt, in order to get them right. On the other hand, shallow modellers might click on the Check button repeatedly as they
guess or use the Give-up button. To measure usage of the help buttons, the following measure was calculated:

Help usage ¼ nwc þ 3ngu



nrn

where nwc is the number of Check button presses that yielded red, ngu is the number of Give-up buttons the student clicked, and nrn is the
number of nodes required by the problem. The “3” here is a weight to make ngu be roughly comparable to nwc. The denominator represents
the number of nodes required for a correct, minimal solution rather than the actual number of nodes the student completed. Should the
prediction for Hypothesis 2 hold true, meta-tutored students’ usage of the help button should be lower than the control students’ one.
 Correct on ﬁrst Check: This measure for Hypothesis 2 is a traditional one in tutoring system research. It represents how frequently
students succeed on their ﬁrst attempt at ﬁlling out a tab. That is, what proportion of the tabs turned green when students ﬁrst clicked
on the Check button? Deep modellers should get almost everything right the ﬁrst time, whereas a student who is confused or guessing
might rarely get elements right on the ﬁrst try. Unfortunately, this measure could only be applied to experiment 5, as insufﬁcient log
data was kept during the 2011 experiments.
 Training efﬁciency: This measure is based on the (now dubious) assumption that deep modelling is faster than shallow modelling. Speed
is often measured by counting the number of problems solved during a ﬁxed training period. However, our problems’ solutions varied in
their complexity. Some had many nodes and some had few nodes. Moreover, students could always use the Give-up button, which does
part of the problem solving for them. There was one Give-up button per tab, and completing a node requires ﬁlling out three tabs, so
clicking on three Give-up buttons per node would solve the problem. Thus, a better measure of the amount of problem solving
accomplished than “problems completed” is the number of tabs the student completed without using the Give-up button, which is
given by:

Training efficiency ¼ 3ncn  ngu
where ncn is the number of nodes that the student completed correctly (so 3ncn is the number of tabs), and ngu is the number of Give-up
buttons the student clicked. The prediction for Hypothesis 2 is that the training efﬁciency of meta-tutored students should be higher than
the training efﬁciency of control students.
Hypothesis 3 is that the experimental group, which was required to follow the Target Node Strategy during training, would continue to
use it during the transfer phase. To evaluate this hypothesis, we calculated the proportion of student steps consistent with the target node
strategy. The algorithm of Fig. 12 describes how it is calculated.
5. Results
Experiments 1 and 2 found, as expected, that students often exhibit shallow learning strategies (VanLehn et al., 2011a). They also led to
many revisions in the software and the experimental procedure (VanLehn et al., 2011b). However, both experiments had only one condition,
and it did not include the meta-tutor. This section reports on comparisons of the system with the meta-tutor turned on and turned off.

L. Zhang et al. / Computers & Education 75 (2014) 196–217

213

5.1. Experiment 3 results
Experiment 3 was conducted in June 2011. Of the 34 participants, some students’ data needed to be omitted. Probably because there were
too many introduction slides (101 in total), 11 out of 34 students were still in the introduction phase at the break, and thus had no time in the
training phase where the manipulation occurred. These 11 students were omitted from the analyses. Two students, one in each condition,
performed much better than others. Both students’ training measures and test measures were greater than three standard deviations from
the means. These two students were also excluded from the analyses. The ﬁnal number of students left for each group was 11 (control) and
12 (meta-tutor). All the analyses below were computed based on these 23 students.
5.1.1. Hypothesis 1 (transfer phase deep modelling)
Run Model button usage: Of the 23 students, there were only 9 Meta-tutored students and 4 control students that ﬁnished the ﬁrst
problem in the transfer phase. Among the 13 students, meta-tutored students used the Run Model button 4.88 times per problem, while
students in control group used it 6.67 times. Due to the limited number of subjects, it is not surprising that the difference is not signiﬁcant
(p ¼ 0.72, d ¼ 0.12). This T-test, and all other tests reported here, are two-tailed.
Extra nodes: The ﬁrst problem of the transfer phase allowed 2 extra nodes, thus allowing us to measure the number of extra nodes created
by students who completed that problem. Meta-tutor students deﬁned 0.44 (SD ¼ 0.8) extra nodes vs. 0.75 (SD ¼ 0.96) extra nodes for the
control students. The difference was not reliable (p ¼ 0.61, d ¼ 0.58) although it was in the expected direction.
Number of problems completed: The average number of problems completed during the transfer phase was 0.82 (SD ¼ 0.60) for the metatutor students vs. 0.35 (SD ¼ 0.52) for the control students. The difference was marginally signiﬁcant (p ¼ 0.057) even though the effect size
was large (d ¼ 0.88) and in the expected direction. These ﬁgures show that on average, students completed less than one problem. More
speciﬁcally, 9 meta-tutored and 4 control students ﬁnished one or more problems, which was a marginally reliable difference (c2 ¼ 3.486,
p ¼ 0.06).
In short, trends in the data support Hypothesis 1 but the differences were not reliable, probably due to the small sample size.
5.1.2. Hypothesis 2 (training phase deep modelling)
Help button usage: This measure is a weighted sum of help button uses divided by the total number of nodes completed. On this measure,
the meta-tutor students averaged 4.78 (SD ¼ 2.25) vs. 7.03 (SD ¼ 3.44) for the control students. The difference was marginally signiﬁcant
according with a large effect size (p ¼ 0.08, d ¼ 0.82).
Training efﬁciency: The average training efﬁciency measure the amount of correct work done by the student during the ﬁxed-length
training period. For the meta-tutor group, training efﬁciency was 13.00 (SD ¼ 6.94) vs. 11.45 (SD ¼ 6.77) for the control group. The difference was not signiﬁcant (p ¼ 0.30; d ¼ 0.23).
Thus, although there was a trend in the data supporting Hypothesis 2, the reliability was poor, perhaps due to the small sample size.
5.1.3. Hypothesis 3 (meta-strategy usage)
Hypothesis 3 was that meta-tutored students would voluntary the Target Node Strategy during the transfer phase more frequently than
the student who was not meta-tutored. During the transfer phase, 0.39 (SD ¼ 0.35) of the meta-tutor students’ steps matched the Target
Node Strategy’s steps, vs. 0.34 (SD ¼ 0.30) for the control students. This difference was quite small (p ¼ 0.70, d ¼ 0.16), suggesting that
hypothesis 3 may be false for this study.
5.1.4. Summary of results and next step forward
Although there were some trends in the expected directions, the students in both conditions performed quite poorly. This was probably
due to the large number of slides in the introduction phase as well as the difﬁculty of the tasks themselves.
In order to increase the number of training and transfer problems solved by students, thus allowing us to differentiate their performance
statistically, we reduced the number of slides from 101 to 64, and simpliﬁed some of the tasks. As the second experiment conﬁrmed, these
changes led to an improvement on the performance students in both phases.

5.2. Experiment 4 results
Experiment 4 took place in July 2011. Data from all 44 participants (22 in each condition) were used in the analyses here.
5.2.1. Hypothesis 1 (transfer phase deep modelling)
Run Model button usage: To complete the ﬁrst problem in the transfer phase, meta-tutored students used the Run model button 3.05
times on average. In comparison, students in the control group used the Run model button 5.13 times. However, the difference was not
signiﬁcant (p ¼ 0.31, d ¼ 0.32). Because the standard deviation was high (SD ¼ 6.77) due to extreme values, we divided all the students into
two types with the threshold being 2, which is the median. The students who used the run model button once or twice were considered
deep modellers. The rest of the students were considered shallow modellers. Chi-square test is then used to compare the number of deep
modellers in meta-tutored group to the number in control group. There was a trend in the expected direction, but the signiﬁcance was
marginal (c2 ¼ 3.36, p ¼ 0.067).
Extra nodes: Because most of the students ﬁnished at least two tasks in the transfer phase (only 3 students in the control group did not)
and the second task allowed up to two extra nodes, we used the second task to count extra nodes. As predicted by Hypothesis 1, metatutored students produced fewer extra nodes (0.27, SD ¼ 0.70) than control students (0.95, SD ¼ 1.03). The difference was signiﬁcant
with a large effect size (p ¼ 0.02, d ¼ 0.80).
Problems completed: The meta-tutor students solved 3.27 (SD ¼ 1.03) transfer problems vs. 3.23 (SD ¼ 1.57) for the control students. The
difference was small and not signiﬁcant (p ¼ 0.65, d ¼ 0.04).

214

L. Zhang et al. / Computers & Education 75 (2014) 196–217

5.2.2. Hypothesis 2 (training phase deep modelling)
Help button usage: Consistent with Hypothesis 2, meta-tutor students’ help button usage averaged 2.35 (SD ¼ 1.75) vs. 3.55 (SD ¼ 1.85)
for the control students. The difference was signiﬁcant (p ¼ 0.04, d ¼ 0.68).
Training efﬁciency: Contrary to Hypothesis 2, the control students had higher training efﬁciency 72.77 (SD ¼ 32.12) than the meta-tutor
students, 54.36 (20.17), and the difference was marginally signiﬁcant (p ¼ 0.05, d ¼ 0.70).
These results suggest that meta-tutor students did increase deep modelling in the training phase than the control students, but they also
moved slower than control students.
5.2.3. Hypothesis 3 (use of Target Node Strategy)
Missing data precluded testing this hypothesis in experiment 4.
5.2.4. Summary of results, revisions and the next iteration
Both hypothesis 1 (transfer) and hypothesis 2 (training) were supported, albeit by one measure each. We were not satisﬁed with the pace
of the students in the training phase, especially the meta-tutored students. Watching the screen-capture video suggested that meta-tutored
students spent a lot of time in reading and answering the tutorial pop-ups. Students also became “stuck” (Burleson & Picard, 2007) when
ﬁlling out the current Inputs tab and needing to create new nodes. In order to do so, students had to close the node editor and click on the
Create Node button on the canvas in the Model tab. However, many students could not realize that they had to close the node editor. They
complained that no button in the Inputs tab could help them get out of the stuck state. Thus, we spent several months reﬁning both the
tutoring system and the meta-tutor. We installed the Plan tab in order to reduce the time spent reading the meta-tutor’s pop-ups. We also
improved the interface, adding a “create a new node,” button on the Input tab with the goal of reducing students’ state of stuck in this stage.
The transfer phase proved to be extremely challenging for the students. This was evident in the relatively small number of problems
solved as well as direct observation of the students. The null result on our productivity measure, number of problems solved in the transfer
phase, might be due to students in both conditions becoming discouraged and ceasing to try hard. Thus, we modiﬁed the transfer phase so
that besides colouring the “g” indicator, the system coloured the “i” indicator and “c” indicator as well to show the correctness of the
corresponding tabs. This was intended to make it easier to locate errors while leaving unchanged the logic required for ﬁxing the errors, and
thus allowing students to make faster progress through the test problems while still allowing us to assess their skill.
5.3. Experiment 5 results
Experiment 5 was conducted in June 2012. Of the 34 participants, data from 33 were used in the analyses below (16 in the control group
and 17 in meta-tutor group). One student was excluded due to his extraordinary performance. He ﬁnished all 7 problems in the transfer
phase well before the end of the transfer phase, while the second fastest person only completed 4 problems.
5.3.1. Hypothesis 1 (transfer phase deep modelling)
Run Model button usage: On average, students in the control group used the Run model button 7.76 times, while meta-tutored students
used the Run model button 7.82 times. So they almost had the same performance (p ¼ 0.98, d ¼ 0.0093).
Extra nodes: Most of the students ﬁnished at least two tasks in the transfer phase (one in each group didn’t), so we again used the second
task to measure extra nodes. As predicted by Hypothesis 1, the meta-tutor students produced fewer extra nodes (0.88, SD ¼ 0.96) than the
control students (1.13, SD ¼ 0.99). However, the difference was not reliable (p ¼ 0.47, d ¼ 0.26).
Problems solved: Contrary to Hypothesis 1, the meta-tutor students solved 2.18 (SD ¼ 0.53) transfer problems vs. 2.56 (SD ¼ 0.78) for the
control students. Students in the control group outperformed meta-tutored student with marginal signiﬁcance (p ¼ 0.094, d ¼ 0.57).
Once again, the meta-tutor students tended to work more slowly than the control students. This time, there was only a trend to show that
they might be doing more deep modelling than the control students.
5.3.2. Hypothesis 2 (training phase deep modelling)
Help button usage: As expected, meta-tutored students’ help button usage averaged 3.92 (SD ¼ 2.19) vs. 6.13 (SD ¼ 2.73) for the control
students. The difference was signiﬁcant with a large effect size (p ¼ 0.016, d ¼ 0.89).
Correct on ﬁrst attempt: To provide an additional test of Hypothesis 2, we calculated the percentage of time that the ﬁrst Check on a tab
was correct. Meta-tutored students achieved a higher percentage (0.77, SD ¼ 0.068) than control students (0.68, SD ¼ 0.11), and the difference was signiﬁcant with a large effect size (p ¼ 0.015, d ¼ 0.98).
Training efﬁciency: Meta-tutor students scored 73.18 (SD ¼ 27.53), a little bit higher in training efﬁciency than control students, 68.88
(SD ¼ 17.16), but the difference was not reliable (p ¼ 0.59, d ¼ 0.19).
So students in both groups kept the same pace in the training session this time, and there was strong evidence that the meta-tutor
students were engaged in deeper modelling than the control students.
5.3.3. Hypothesis 3 (use of Target Node Strategy)
Contrary to hypothesis 3, the meta-tutored students had nearly the same level of Target Node Strategy usage (0.66, SD ¼ 0.23) as control
students (0.70, SD ¼ 0.19), and the difference is not reliable (p ¼ 0.59, d ¼ 0.19).
6. Discussion
Our results are summarized in Table 1. This section discusses our interpretation of them.
As mentioned in the introduction, whenever a new kind of instruction is developed, there are four classic questions to ask about learning
strategies that are speciﬁc to it:

L. Zhang et al. / Computers & Education 75 (2014) 196–217

215

Table 1
Summary of the results.
Measure (predicted dir.)

Experiment 3 (N ¼ 23)

Experiment 4 (N ¼ 44)

Experiment 5 (N ¼ 33)

Transfer phase (Hypothesis 1)
Run model button usage (E < C)
Extra nodes (E < C)
Probs completed (E > C)

Not available
E < C (p ¼ 0.61, d ¼ 0.58)
E > C (p ¼ 0.06, d ¼ 0.88)

E < C (p ¼ 0.31, d ¼ 0.32)
E < C (p [ 0.02, d [ 0.80)
E z C (p ¼ 0.65, d ¼ 0.04)

E z C (p ¼ 0.98, d ¼ 0.0093)
E < C (p ¼ 0.47, d ¼ 0.26)
E < C (p ¼ 0.09, d ¼ 0.57)

Training phase (Hypothesis 2)
Help button usage (E < C)
Correct on 1st Chk (E > C)
Efﬁciency (E > C)

E < C (p ¼ 0.08, d ¼ 0.82)
Missing data
E > C (p ¼ 0.30, d ¼ 0.23)

E < C (p [ 0.04, d [ 0.68)
Missing data
E < C (p ¼ 0.05, d ¼ 0.70)

E < C (p [ 0.02, d [ 0.89)
E > C (p [ 0.015, d [ 0.98)
E > C (p ¼ 0.59, d ¼ 0.19)

Missing data

E z C (p ¼ 0.59, d ¼ 0.19).

Transfer phase use of Target Node Strategy (Hypothesis 3)
Usage (E ¼ C)
E z C (p ¼ 0.70, d ¼ 0.16)

E stands for the meta-tutor group, and C stands for the control group. Reliable results are bold.

A. Without instruction in good learning strategies, do students often exhibit poor learning strategies?
B. Can good learning strategies be easily taught?
C. When students use the taught learning strategies, does their domain learning increase compared to students who are not taught to use
the learning strategies?
D. When instruction in the learning strategy ceases, do students revert to poor learning strategies?
After we developed a step-based tutoring system for model construction, experiments 1 and 2 answered question A by ﬁnding that
student did indeed exhibit poor learning strategies when using it. This led us to develop a meta-tutor that taught students a meta-strategy
that we hoped would increase their acquisition of skill in model construction.
Answering question B requires that even during the training phase, students have the freedom to choose between following or not
following the learning strategy. This freedom allows experimenters to measure the growth in compliance during the training phase. Of the
four systems reviewed earlier (Betty’s Brain, the Help Tutor, Co-Lab and Pyrenees; see Table 2), Betty’s Brain and the Help Tutor provide such
freedom, and thus were able to show that while students were being meta-tutored, their behaviours were more often consistent with the
learning strategy than the behaviours of students who were not being meta-tutored. This suggests that their meta-tutoring was effective at
getting students to use the taught learning strategy. AMT, Co-Lab and Pyrenees all required students to follow the taught learning strategy
during the training phase, so they could not answer question B.
Given a learning strategy that designers think is good, question C asks whether it really does increase domain learning. Of the four
systems reviewed earlier, only Pyrenees’ learning strategy increased domain learning. Thus, we adapted its learning strategy for use in AMT.
Because the goal of our system is teach students to construct models properly, most of the measures addressed the frequency of deep
modelling. During the training phase, on the measures “help button usage” and “correct on ﬁrst check” meta-tutored students scored
reliably higher than students who were not meta-tutored, except on experiment 3, which appears to have been underpowered. Moreover,
the effect sizes were large (d ¼ 0.89; d ¼ 0.98) or moderately larger (d ¼ 0.68). Thus, the AMT results for domain learning in the training
phase were nearly as good as those from Pyrenees, and substantially better than the other three meta-tutoring systems.
Unfortunately, neither the Target Node Strategy nor the domain learning advantages transferred. During the transfer phase, on the
measures “Run model button usage”, “extra nodes” and “Target Node Strategy usage”, the meta-tutored students were no better than the
students who had not been meta-tutored earlier, with one exception. On the measure “extra nodes” in experiment 4, meta-tutored students
outscored the control students. It is always difﬁcult to get learning to transfer from a supported context to an unsupported one, so this lack of
transfer should perhaps not be surprising. However, it does appear to be a bit weaker than the transfer obtained by Pyrenees, Betty’s Brain
and the Help Tutor.
In both the training and transfer phases, we attempted to measure deep modelling using efﬁciency: the amount of modelling done in a
ﬁxed period of time. This measurement made the tacit assumption that deep modelling is faster than shallow modelling. That is, thinking
hard to solve an atomic modelling problem should take less time than guessing, overusing the Give-up button, overusing the Run button,
scanning the problem statement for keywords and other shallow model construction tactics. However, the efﬁciency measures all produced
null results. If anything, there was trend for non-meta-tutored students to get more done than the meta-tutored students. This is consistent
with our informal analyses of the log data. It appears that guessing was actually quite a bit faster than thinking, especially in the training
phase when the Check button was enabled. Even when students could only use the Run Model button for feedback in the transfer phase,
guessing was fast because the models were so small for the ﬁrst few problems. After that, guessing became must less efﬁcient, but few
students got that far.
As the experience with Betty’s Brain and the Help Tutor shows, not every supposedly good learning strategy actually turns out to increase
domain learning. In the case of AMT, we have found a learning strategy that does increase domain learning, and thus deserves to be taught.
On the other hand, our method of teaching the learning strategy appears not to have had enough meta-cognitive or motivational impact
on students, because their gains while being meta-tutor did not persist when the meta-tutoring was turned off.
Thus, the next stage of the AMT project is to augment the instruction with an affective learning companion. Its job will be to persuade
students of the beneﬁts of using the Target Node Strategy and of not abusing the Check and Give-up buttons. The agent cannot use the
argument that the learning strategy and deep modelling will speed up the students’ work, because we have found that shallow modelling
strategies are actually faster at least on these simple problems. Thus, we plan on having the agent use Dweck’s well-known argument
(Dweck & Leggett, 1988) that the “mind is a muscle; the harder you exercise it, the stronger it becomes.” Our summer school students
presumably want stronger minds, so if they believe the agent, they should more often use both the learning strategy and deep modelling. In
order to make the agent easier to believe, we plan to use attribution shifting, empathy, rapport-building chit-chat, and other non-cognitive

216

L. Zhang et al. / Computers & Education 75 (2014) 196–217

Table 2
Comparison of four meta-tutors of model construction.
Meta-tutor

AMT
Pyrenees
Help Tutor
Betty’s Brain
Co-Lab

Training phase

Transfer phase

Knowledge/skill

Meta-strategy

Knowledge/skill

Meta-strategy

þ
þþ
NS
NS
NS

Required
Required
þ
þ
Required

þ
þþ
NS
þ

NS
NS
þ
þ

NS ¼ non-signiﬁcant difference, two-tailed. þ ¼ Signiﬁcant but weak. þþ ¼ Signiﬁcant and strong. Required ¼ meta-tutor required student to follow the learning strategy.

techniques. In order to optimize the timing and selection of these non-cognitive interventions, we plan to monitor the students’ affective
state using physiological sensors.
Lastly, by experiment 5, students seem to be acquiring decent amounts of competence in system dynamics modelling in only 1 h and
15 min total. This is a considerable reduction from the 5 or more hours required of Model-It students and others. Although we have no way
to actually compare our results to the early ones, because the systems, students, domains and almost everything else were quite different,
we nonetheless are quite encouraged. The combination of tutoring and meta-tutoring may be the key to getting model construction out into
the classrooms at last.
Acknowledgements
This material is based upon work supported by the National Science Foundation, United States (_100000001) under Grant No. 0910221.
References
Alessi, S. M.. (December 2000). The application of system dynamics modeling in elementary and secondary school curricula. In Paper presented at the RIBIE 2000 – The ﬁfth
Iberoamerican conference on informatics in education. Viña del Mar, Chile.
Aleven, V., McLaren, B., Roll, I., & Koedinger, K. (2004). Toward tutoring help seeking (applying cognitive modeling to help-seeking skills). In J. C. Lester, R. M. Vicari, &
F. Paraguacu (Eds.), Intelligent tutoring systems: Seventh international conference: ITS 2005 (pp. 227–239). Berlin: Springer.
Aleven, V., Stahl, E., Schworm, S., Fischer, F., & Wallace, R. M. (2003). Help seeking and help design in interactive learning environments. Review of Educational Research, 73(2),
277–320.
Baker, R. S. J. d., Corbett, A., & Koedinger, K. R. (2004). Detecting student misuse of intelligent tutoring systems. In Proceedings of the 7th international conference on intelligent
tutoring systems (pp. 531–540).
Baker, R. S., Corbett, A., Koedinger, K. R., Evenson, S., Roll, I., Wagner, A. Z., et al. (2006). Adapting to when students game an intelligent tutoring system. In Intelligent tutoring
systems (pp. 392–401). Berlin: Springer.
Baker, R. S. J. d., Corbett, A., Koedinger, K. R., & Wagner, A. Z. (2004). Off-task behavior in the cognitive tutor classroom: when students “game the system”. In E. DykstraErickson, & M. Tscheligi (Eds.), Proceedings of the SIGCHI conference on human factors in computing systems (pp. 383–390). New York, NY: ACM.
Biswas, G., Leelawong, K., Schwartz, D. L., & Vye, N. J. (2005). Learning by teaching: a new agent paradigm for educational software. Applied Artiﬁcial Intelligence, 19, 363–392.
Booth Sweeney, L., & Sterman, J. D. (2000). Bathtub dynamics: initial results of a systems thinking inventory. System Dynamics Review, 16(4), 249–286.
Bravo, C., van Joolingen, W. R., & de Jong, T. (2009). Using Co-Lab to build system dynamics models: students’ actions and on-line tutorial advice. Computer and Education, 53,
243–251.
Bredeweg, B., & Forbus, K. D. (2003). Qualitative modeling in education. AI Magazine, 24(4), 35–46.
Burleson, W., & Picard, R. W. (2007). Affective learning companions. Educational Technology, Special Issue on Pedagogical Agents, Saddle Brook, N.J., 47(1), 28–32.
CCSSO. (2011). The Common Core State Standards for mathematics. Downloaded from www.corestandards.org. on 31.10.11.
Chi, M., & VanLehn, K. (2010). Meta-cognitive strategy instruction in intelligent tutoring systems: how, when and why. Journal of Educational Technology and Society, 13(1), 25–
39.
Chin, D., Dohmen, I. I. M., Cheng, B. H., Oppezzo, M., Chase, C. C., & Schwartz, D. L. (2010). Preparing students for future learning with teachable agents. Educational Technology
Research and Development, 58, 649–669.
Clark, H. H., & Brennan, S. E. (1991). Grounding in communication. In L. B. Resnick, J. M. Levine, & S. D. Teasley (Eds.), Perspectives on socially shared cognition (pp. 127–149).
Washington, DC: American Psychological Association.
Collins, A., & Ferguson, W. (1993). Epistemic forms and epistemic games: structures and strategies to guide inquiry. Educational Psychologist, 28(1), 25–42.
Doerr, H. M. (1996). Stella ten-years later: a review of the literature. International Journal of Computers for Mathematical Learning, 1, 201–224.
Donker, A. S., de Boer, H., Kostons, D., Dignath van Ewijk, C. C., & van der Werf, M. P. C. (2014). Effectiveness of learning strategy instruction on academic performance: a metaanalysis. Educational Research Review, 11, 1–26.
Du Boulay, B., Avramides, K., Luckin, R., Martínez-Mirón, E., & Rebolledo-Méndez, G. (2010). Towards systems that care: a conceptual framework based on motivation,
metacognition and affect. International Journal of Artiﬁcial Intelligence in Education, 20(3), 197–229.
Dweck, C. S., & Leggett, E. L. (1988). A social-cognitive approach to motivation and personality. Psychological Review, 95(2), 256–273.
Gonzalez-Sanchez, J., Chavez-Echeagaray, M.-E., VanLehn, K., & Burleson, W. (2011). From behavioral description to a pattern-based model for intelligent tutoring systems. In
Paper presented at the Proceedings of the 18th international conference on pattern languages of programs (PLoP). Portland, OR.
Hashem, K., & Mioduser, D. (2011). The contribution of learning by modeling (LbM) to students’ understanding of complexity concepts. International Journal of e-Education, eBusiness, e-Management and e-Learning, 1(2), 151–157.
Hattie, J., Biggs, J., & Purdie, N. (1996). Effects of learning skills interventions on student learning: a meta-analysis of ﬁndings. Review of Educational Research, 66, 99–136.
Hestenes, D. (2007). Modeling theory for math and science education. In Paper presented at the ICTMA-13: The international community of teachers of mathematical modelling
and applications. Indiana, IL.
Hogan, K., & Thomas, D. (2001). Cognitive comparisons of students’ systems modeling in ecology. Journal of Science Education and Technology, 10(4), 319–345.
Lee, C. B., Jonassen, D., & Teo, T. (2011). The role of model building in problem solving and conceptual change. Interactive Learning Environments, 19(3), 247–265.
Leelawong, K., & Biswas, G. (2008). Designing learning by teaching agents: the Betty’s brain system. International Journal of Artiﬁcial Intelligence and Education, 18(3), 181–208.
Löhner, S., Van Joolingen, W. R., & Savelsbergh, E. R. (2003). The effect of external representation on constructing computer models of complex phenomena. Instructional
Science, 31, 395–418.
Mandinach, E. B., & Cline, H. F. (1994a). Classroom dynamics: Implementing a technology-based learning environment. Mahwah, NJ: Erlbaum.
Mandinach, E. B., & Cline, H. F. (1994b). Modeling and simulation in the secondary school curriculum: the impact on teachers. Interactive Learning Environments, 4(3), 271–289.
Marshall, S. P., Barthuli, K. E., Brewer, M. A., & Rose, F. E. (1989). Story problem solver: A schema-based system of instruction. San Diego, CA: Center for Research in Mathematics
and Science Education, San Diego State University.
Metcalf, S. J. (1999). The design of guided learner-adaptable scaffolding in interactive learning environment (Doctoral Dissertation). University of Michigan.
Metcalf, S. J., Krajcik, J., & Soloway, E. (2000). Model-It: a design retrospective. In M. J. Jacobson, & R. B. Kozma (Eds.), Innovations in science and mathematics education:
Advanced designs for technologies of learning (pp. 77–115).

L. Zhang et al. / Computers & Education 75 (2014) 196–217

217

Mulder, Y. G., Lazonder, A. W., de Jong, T., Anjewierden, A., & Bollen, L. (2011). Validating and optimizing the effects of model progression in simulation-based inquiry learning.
Journal of Science Education and Technology, 21, 722–729.
Muldner, K., Burleson, W., van de Sande, B., & VanLehn, K. (2011). An analysis of students’ gaming behaviors in an intelligent tutoring system: predictors and impacts. User
Modeling and User-Adapted Interaction, 21(1–2), 99–135.
Nathan, M. J. (1998). Knowledge and situational feedback in a learning environment for algebra story problem solving. Interactive Learning Environments, 5, 135–159.
National Research Council. (2012). A framework for K–12 science education: Practices, crosscutting concepts, and core ideas. Washington, DC: National Academies Press.
Richmond, B. M. (1985). STELLA: software for bringing system dynamics modeling to the other 98%. In Paper presented at the Proceedings of the 1985 international conference of
the System Dynamics Society: 1985 International system dynamics conference.
Roll, I., Aleven, V., McLaren, B., & Koedinger, K. R. (2007a). Can help seeking be tutored? Searching for the secret sauce of metacognitive tutoring. In Proceedings of the international conference on artiﬁcial intelligence in education (pp. 203–210). Amsterdam: IOS Press.
Roll, I., Aleven, V., McLaren, B., & Koedinger, K. R. (2007b). Designing for metacognition – applying cognitive tutor principles to the tutoring of help seeking. Metacognition and
Learning, 2(2).
Roll, I., Aleven, V., McLaren, B., & Koedinger, K. R. (2011). Improving students’ help-seeking skills using metacognitive feedback in an intelligent tutoring system. Learning and
Instruction, 267–280.
Roll, I., Aleven, V., McLaren, B., Ryu, E., Baker, R. S. J. d., & Koedinger, K. R. (2006). The Help Tutor: does metacognitive feedback improve student’s help-seeking actions, skills
and learning. In M. Ikeda, K. Ashley, & T.-W. Chan (Eds.), Intelligent tutoring systems: 8th International conference, its 2006 (pp. 360–369). Berlin: Springer.
Russell, S., & Norvig, P. (2009). Artiﬁcial intelligence: A modern approach (2nd ed.). Upper Saddle River, NJ: Prentice Hall.
Schecker, H. (1993). Learning physics by making models. Physics Education, 28, 102–106.
Segedy, J. R., Kinnebrew, J. S., & Biswas, G. (2012a). Relating student performance to action outcomes and context in a choice-rich learning environment. In S. A. Cerri,
W. J. Clancey, & G. Papadourakis (Eds.), Intelligent tutoring systems: 11th International conference its 2012 (pp. 505–510). Berlin: Springer-Verlag.
Segedy, J. R., Kinnebrew, J. S., & Biswas, G. (2012b). Supporting student learning using conversational agents in a teachable agent environment. In Paper presented at the
Proceedings of the 10th international conference of the learning sciences. Sydney, Australia.
Shih, B., Koedinger, K. R., & Scheines, R. (2008). A response time model for bottom-out hints as worked examples. In C. Romero, S. Ventura, M. Pechenizkiy, & R. S. J. d Baker
(Eds.), Handbook of educational data mining (pp. 201–211). Boca Raton, FL: Taylor & Francis.
Steed, M. (1992). Stella, a simulation construction kit: cognitive process and educational implications. Journal of Computers in Mathematics and Science Teaching, 11, 39–52.
Stratford, S. J. (1997). A review of computer-based model research in precollege science classroom. Journal of Computers in Mathematics and Science Teaching, 16(1), 3–23.
Tan, J., Biswas, G., & Schwartz, D. L. (2006). Feedback for metacognitive support in learning by teaching environments. In Proceedings of the twenty-eighth annual meeting of the
Cognitive Science Society. Mahwah, NJ: Erlbaum.
Tan, J., Wagster, J., Wu, Y., & Biswas, G. (2007). Effect of metacognitive support on student behaviors in learning by teaching environments. In R. Luckin, K. R. Koedinger, &
J. Greer (Eds.), Proceedings of the 13th international conference on artiﬁcial intelligence in education (pp. 650–652). Amsterdam: IOS Press.
Timms, M. J. (2007). Using item response theory (IRT) to select hints in an ITS. In R. Luckin, K. R. Koedinger, & J. Greer (Eds.), Artiﬁcial intelligence in education (pp. 213–221).
Amsterdam: IOS Press.
Treagust, D. F., Chittleborough, G., & Mamiala, T. (2002). Students’ understanding of the role of scientiﬁc models in learning science. International Journal of Science Education,
24(4), 357–368.
VanLehn, K. (2006). The behavior of tutoring systems. International Journal of Artiﬁcial Intelligence and Education, 16, 227–265.
VanLehn, K. (2013). Model construction as a learning activity: a design space and review. Interactive Learning Environments, 21(4), 371–413.
VanLehn, K., Burleson, W., Chavez-Echeagaray, M.-E., Christopherson, R., Gonzalez-Sanchez, J., Hastings, J., et al. (2011a). The level up procedure: how to measure learning
gains without pre- and post-testing. In T. Hirashima (Ed.), Proceedings of the 19th international conference on computers in education (pp. 96–100). Chiang-Mai, Thailand:
Asia-Paciﬁc Society for Computers in Education.
VanLehn, K., Burleson, W., Chavez-Echeagaray, M.-E., Christopherson, R., Gonzalez-Sanchez, J., Hastings, J., et al. (2011b). The affective meta-tutoring project: how to motivate
students to use effective meta-cognitive strategies. In Paper presented at the 19th International conference on computers in education. Chiang Mai, Thailand.
Vanlehn, K., & Chi, M. (2012). Adaptive expertise as acceleration of future learning: a case study. In P. J. Durlach, & A. Lesgold (Eds.), Adaptive technologies for training and
education. Cambridge: Cambridge University Press.
Wagster, J., Tan, J., Biswas, G., & Schwartz, D. L. (2007). How metacognitive feedback affects behavior in learning and transfer. In Paper presented at the 13th International
conference on artiﬁcial intelligence in education: Workshop on metacognition and self-regulated learning in ITSs. Marina del Rey, CA.
Wagster, J., Tan, J., Wu, Y., Biswas, G., & Schwartz, D. L. (2007). Do learning by teaching environments with metacognitive support help students develop better learning
behaviors?. In Proceedings of the twenty-sixth annual meeting of the Cognitive Science Society. Mahwah, NJ: Erlbaum.
Wheeler, J. L., & Regian, J. W. (1999). The use of a cognitive tutoring system in the improvement of the abstract reasoning component of word problem solving. Computers in
Human Behavior, 15, 243–254.
Wilensky, U. (2003). Statistical mechanics for secondary school: the GasLab multi-agent modeling toolkit. International Journal of Computers for Mathematical Learning, 8(1),
1–41.
Wilensky, U., & Reisman, K. (2006). Thinking like a wolf, a sheep, or a ﬁreﬂy: learning biology through constructing and testing computational theories – an embodied
modeling approach. Cognition and Instruction, 24(2), 171–209.
Zaraza, R., & Fisher, D. (1997). Introducing system dynamics into the traditional secondary curriculum: the CC-STADUS project’s search for leverage points. In Paper presented
at the 15th International system dynamics conference. Istanbul, Turkey.
Zaraza, R., & Fisher, D. (1999). Training system modelers: the NSF CC-STADUS and CC-SUSTAIN projects. In W. Feurzeig, & N. Roberts (Eds.), Modeling and simulation in science and
mathematics education (Vol. 1); (pp. 38–69). New York, NY: Springer.

Learning, Interactional, and Motivational Outcomes in
One-to-One Synchronous Computer-mediated versus
Face-to-Face Tutoring
Stephanie Ann Siler, Psychology Department, Carnegie Mellon University, Pittsburgh, PA
15213, USA
siler@andrew.cmu.edu
Kurt VanLehn, Department of Computer Science, University of Pittsburgh, Pittsburgh, PA
15260, USA
vanlehn@cs.pitt.edu
Abstract. Face-to-face (FTF) human-human tutoring has ranked among the most effective forms of instruction.
However, because computer-mediated (CM) tutoring is becoming increasingly common, it is instructive to
evaluate its effectiveness relative to face-to-face tutoring. Does the lack of spoken, face-to-face interaction
affect learning gains and motivation? In this study, pairs of undergraduate students and tutors worked on physics
problems either face-to-face or via a typed chat window. Although face-to-face tutoring took less time, students
learned equal amounts in the two conditions. In both conditions, short tutor turns were associated with increased
student learning. In both conditions, students who were more active had higher learning gains. Students in the
CM condition who gained more produced more words per conversational turn. The same relationship was found
in the FTF context only after back-channel feedback was taken out. A more direct measure of student activity,
the relative proportion of student-initiated actions in problem-solving, was more strongly associated with student
learning in the FTF context, but only for students with higher verbal SAT scores. Of the motivational variables
we investigated, only students’ ability goals (i.e. wanting to demonstrate one’s ability to others) were influenced
somewhat differently by the two contexts. These results suggest that although the difference in communication
medium changes superficial characteristics of the tutoring such as its duration, most of the important pedagogical
characteristics – learning gains, tutorial interaction, the activity measures associated with learning gains, and
student motivation – were not affected.
Keywords. Tutoring, computer-mediated, face-to-face, motivation

INTRODUCTION
Face-to-face human-human tutoring is one of the most effective learning contexts (Bloom, 1984;
Cohen et al., 1982). Though face-to-face (FTF) tutoring is still the most common tutoring context,
computer-mediated (CM) tutoring is becoming increasingly common. This growth is in part due to
the recent growth in online learning (Goodyear et al., 2001), including online college courses offered
(Conole et al., 2002) and online colleges and universities, which offer online courses exclusively.
Additionally, online tutoring services are becoming more widely used. In both online courses and
online tutoring, tutors and students may communicate via chat programs. In this paper, we focus
specifically on this component of online instruction, one-to-one text-based synchronous CM tutoring.

Both because of its growing availability and because of its potential applications to text-based
computer tutoring, the learning and motivational outcomes of one-to-one synchronous CM tutoring are
important to study. During CM tutoring, the tutorial dialogue is typed. Participants cannot see each
other nor hear the prosodic information of speech, and thus they get less information about each other.
Most importantly, they probably get less information about each other’s affect. The lack of
information might reduce the learning and motivational gains of tutoring. This hypothesis has a great
deal of intuitive plausibility but it has not yet been adequately tested.
There have been many recent efforts at adding prosodic speech understanding and other affect
sensing to computer tutors (Aist et al., 2002; D’Mello et al., 2005; Forbes-Riley et al., 2006; Johnson
et al., 2004; Heylen et al., 2005; Moore et al., 2004; Moreno et al., 2001). These efforts are based on
the assumption that such extra information would allow tutoring to be more effective. If it turns out
that CM human tutoring is just as effective as FTF human tutoring, then adding prosodic speech and
other affect sensing to computer tutoring that does not go beyond what human tutors do naturally may
not improve the quality of CM tutoring.
Though there is a large volume of literature investigating CM communication, research on
learning from CM compared to FTF tutoring is more limited. The most direct antecedent of the
present study is experiment 1 of Litman et al. (2006), which compared one-to-one, synchronous
human tutoring of conceptual physics in a CM text-only environment with a spoken, but not face-toface, environment. The tutor in their study was a former physics professor and experienced tutor.
Turn-taking in the CM condition was enforced; that is, one person had to wait until the other had sent
their message before they could compose and send theirs. Although the spoken environment produced
larger learning gains than the CM environment, the difference was not statistically reliable.1
Most other prior work comparing FTF to CM communication has not investigated learning from
synchronous one-to-one CM tutoring, but rather:
• Learning from one-to-one tutorial support in asynchronous CM environments (e.g. email)
embedded within a traditional (FTF) course (e.g. Schpilberg & Hubschman, 2003),
• Collaborative learning (rather than tutoring) in synchronous CM environments (e.g. Cummings et
al., 1996; van der Meijden & Veenman, 2005; Pilkington & Walker, 2003; Tutty & Klein, 2008),
• Learning from both tutoring and collaborative groups in both synchronous and asynchronous
environments (e.g. Dennis, 2003),
• Collaborative learning in asynchronous CM environments (e.g. Ocker & Yaverbaum, 1999;
Marttunen & Laurinen, 2001; Cheung & Hew, 2004; Bordia, 1997),
• Learning in online courses (which typically involve both one-to-one and group learning) that
include: both synchronous and asynchronous communication (Warren & Holloman, 2005), or
asynchronous communication only (e.g. Neuhauser, 2002; Summers et al., 2005), and
• Productivity or process (not learning) differences from interactions in synchronous or
asynchronous CM environments (e.g. Coleman et al., 1999; Condon & Cech, 1996; Jones et al.,
2006; Heckman & Annabi, 2005; Lebie et al., 1995).
A large portion of studies comparing CM to FTF communication have investigated collaborative
group learning rather than one-to-one tutoring interactions. Generally, comparisons of collaborative
1

In an ANOVA with condition by test phase factorial design, Litman et al. (2006) found no reliable interaction.
In an ANCOVA, there was a strong trend toward a reliable difference between conditions, F(1,31) = 4.04, p
=.053, d = 0.74.

group learning in asynchronous environments have found no differences, provided that students were
first given time to become familiar with the technology (Bordia, 1997). For example, in Ocker and
Yaverbaum’s (1999) experiment, groups of graduate students discussed and then produced a report on
business case studies over two weeks while interacting either face-to-face or through asynchronous
computer conferencing. After this period, their understandings of the case study were individually
assessed. There was no difference between the face-to-face and computer-mediated groups on this
measure of learning. Similarly, comparisons of synchronous CM groups to face-to-face groups
generally report no differences in learning outcomes (e.g. Dennis, 2003; Pilkington & Walker, 2003),
though there are some exceptions. For example, Tutty and Klein (2008) found that FTF students
learned basic computer skills better than their CM counterparts.
Consistent with the majority of findings for groups, in one study of individual learning from
tutoring in an asynchronous environment (Schpilberg & Hubschman, 2003), there was no difference in
high school students’ math grades after they were given either face-to-face help (during teachers’
office hours) or help via email (non-restricted hours). Finally, several studies comparing online
courses to traditional FTF courses have found no significant differences in learning outcomes
(Neuhauser, 2002; Summers et al., 2005; Warren & Holloman, 2005).
CM groups may have some advantages in what they produce during group interactions, including
collaborative decision-making, group work, and writing (Luppicini, 2007; Benbunan-Fich & Hiltz,
1999). For example, Cummings et al. (1996) found that CM groups produced essays with “higher
integrative complexity” than FTF groups. However, this advantage may not hold for younger
students. For example, van der Meijden and Veenman (2005) found that dyads of sixth-grade students
in a FTF condition had higher group performance scores on math problems than those in a
synchronous CM condition. It is important to note, however, that greater group performance does not
necessarily translate into greater individual learning. For example, Tutty and Klein (2008) found that,
though group project performance was better for students in synchronous CM dyads, FTF students
demonstrated greater individual learning.
In the majority of reported studies, no learning outcome differences were found2. However, given
the many differences between asynchronous and synchronous CM communication, between group
learning and one-to-one tutoring, and between tasks, it is difficult to conclude from these results
whether or not learning differences would exist for one-to-one synchronous CM and FTF tutoring.
This suggests considering theoretical arguments for differences between CM and FTF tutoring. That
is, how would we expect learning outcomes from synchronous CM tutoring to compare to FTF
tutoring? The next three subsections review predictions stemming from differences in, respectively,
the media (speech vs. text), interactivity, and motivational differences between CM and FTF tutoring.

Potential differences in learning due to differences in media
In FTF tutoring, tutors and students communicate by speaking and listening to each other, whereas in
CM tutoring, they type and read each other’s messages. This section considers learning differences
due to listening vs. reading and speaking vs. typing.

2

In all of these studies except for Tutty and Klein (2008), sample sizes did not exceed 30 per group. For a
medium effect size, about 50 per group is necessary for an 80% power level at an alpha of .05. Thus, it is
possible that significant results were not found due to sample size. However, the consistency of null results
makes this less likely.

Does listening or reading lead to better comprehension? The answer appears to depend on many
factors including age. For young children, some studies have shown an advantage of listening over
reading. For example, Fletcher and Pumfrey (1988) found an advantage of listening over reading
silently for 7 and 8-year-old children. For older children and adults, however, an advantage of reading
over listening is a more common finding (e.g. Neville & Pugh, 1974; Rubin et al., 2000; Diakidoy et
al., 2004). Thus, for older children and adults, including college-aged students who typically
participate in studies comparing CM to FTF learning, including the present one, CM tutoring may give
students a learning advantage on the comprehension end of communicating.
One potential contributing factor to an advantage of reading is that a permanent record exists that
may be re-read (Hara et al., 2000). Re-reading may help students develop a better understanding and
resolve previous misunderstandings due to initial reading errors. The ability to re-read messages may
be particularly beneficial for students who are less skilled readers and thus less likely to understand a
message initially. Re-reading may also help to consolidate information into long-term memory by
repeated exposure to the same information. Rubin, Hafer, and Arata (2000) proposed the possibility
that reading may have an advantage over listening because it requires more mental effort to process. If
this additional mental effort translates into deeper processing of the reading material itself, then this
may also contribute to an advantage of reading over listening.
Another difference between the two modes of communication is that generally, less time is
acceptable between turns in FTF than in CM communication (Clark & Brennan, 1991). Thus, both
tutor and student have less time to think about and produce a response in a FTF context. This
decreased time may also lead to simultaneous encoding of the statement and production of a response
in FTF communication (Condon & Cech, 2001), which would result in a heavier load on working
memory. This may negatively impact both the students’ comprehension of the message and the
quality of the tutor’s response in FTF communication, especially for students and tutors with poorer
verbal processing skills. Longer turn lengths may be especially problematic for these students. On the
other hand, more time per turn in the CM context may allow students the opportunity to re-read
messages before responding, which may help students with lower reading comprehension in
particular. This time difference suggests that FTF tutoring may be less effective than CM tutoring.
One advantage of FTF over CM tutoring is, because people generally speak faster than they can
type (e.g. Olaniran, 1996), more information can be conveyed in the same period of time in FTF than
CM communication (CMC). For this reason, in the FTF context, students may learn more in the same
period of time, so FTF tutoring should be more time efficient. This was found in the Litman et al.
(2006) experiment, where students in the spoken condition completed training in about half the time as
students in the CM condition. Similarly, as reported by Bordia (1997) in a review of group learning,
“a number of studies reported longer times taken by CMC groups to complete the allotted task”
(p. 102).
Thus, from a media perspective alone, it appears that CM tutoring should be more effective than
FTF tutoring, albeit considerably slower.

Possible differences in learning due to differences in interactivity
Even a cursory examination of transcripts shows that FTF dialogues are more interactive than CM
dialogues, with more conversational turns for discussions of the same topics (e.g. Litman et al., 2006).
In FTF dialogues, participants may speak simultaneously, which allows them to “back-channel” their
understanding (Clark, 1996), to interrupt, or to complete an utterance started by the other participant.

In CM dialogues, including those of Litman et al. (2006) and the present study, the text typed by a
participant is only sent to and seen by the other participant after the Enter key is pressed. Thus, CM
participants can neither interrupt, back-channel, nor complete each other’s utterances within a given
turn.
Tutorial dialogues may vary between a highly didactic, lecturing style to a highly interactive,
collaborative style. This difference can be measured with both surface features of the dialogues (e.g.
word counts) and deeper codings. With respect to surface features, several correlations with learning
have been found in studies of CM human-to-human tutoring:
•

•

The number of student words per turn positively correlates with gains. That is, tutorial
dialogues with longer student turns result in larger gains (Litman et al., 2006; Rosé et al.,
2001; see also Core et al., 2003, for a related measure).
The number of tutor words per turn positively correlates with gains. Longer tutor turns
resulted in larger gains (Litman et al., 2006).

Similar correlations were not found with spoken tutoring (Litman et al., 2006). However, if we
take the correlations at face value and note that both student and tutor turns are longer in CM than
spoken tutoring (Litman et al., 2006), then the correlations predict an advantage for CM over FTF
tutoring.
Deeper coding of tutorial dialogues suggests a resolution of this apparent contradiction, where
both tutor and student words per turn are positively related to learning. Chi, Siler Jeong, Yamauchi,
and Hausmann (2001) coded FTF tutorial dialogues using deeper categories for tutor turns
(explanation, feedback, reading aloud, meta-cognitive, answering questions, asking content questions,
scaffolding, and asking comprehension questions) and for student turns (reading aloud, selfexplaining, asking questions, answering questions, responding to the tutor’s scaffolding prompts, and
reflecting). Using stepwise regression, Chi et al. found that only the number of student turns coded as
“reflection”, which were comprehension-monitoring statements, positively correlated with deep
learning gains. For shallow learning gains, both the number of student turns coded as “responses to
tutor scaffolding” and the number of tutor turns coded as “explanation” positively correlated with
gains. Their interpretation is that the tutor giving frequent, long explanations produced only shallow
learning; this would explain the second surface feature finding listed above, that the length of tutor
turns sometimes correlates with learning gains. For deep learning, long, frequent student reflection
turns produce learning gains, which is consistent with the first surface feature, that longer student turns
often correlated positively with learning gains. This is clearly the more important correlation, as it
appears in all analyses of tutorial dialogue and it relates to deep learning.
Thus, we hypothesize that it is primarily student activity or engagement that causes learning,
regardless of tutoring context. In the present study, we looked at measures of student activity such as
the number of student words, percent of words given by the student, and student turn length.
Whichever context allows for higher levels of productive student engagement should produce higher
learning gains.

Possible motivational effects of CM vs. FTF tutoring
In addition to favorable learning gains compared to other types of instruction, face-to-face tutoring has
also been found to improve motivation-related factors such as students’ interest in the tutored subject

(e.g. Cohen et al., 1982). Motivation is a large, amorphous variable composed of many social and
cognitive processes. Some relevant aspects are:
•

•

•

Students’ confidence in their ability to successfully perform a learning task, or their efficacy
beliefs, which predicts their persistence in the task (Vollmeyer & Rheinberg, 2000) and their
performances in math and science (Wigfield & Guthrie, 1997; Randhawa et al., 1993;
Zimmerman et al., 1992).
Students’ motivations may be intrinsic and/or extrinsic to the task. Intrinsic motivations for
performing a task include finding the task interesting or enjoyable and being curious about
the task. Extrinsic motivations include objective rewards (e.g. grades; pay), the approval of
teachers and peers, etc.
Achievement goals also define purposes students have for engaging in achievement-related
behavior; two types of achievement goals commonly studied are learning goals and
performance goals (e.g. Dweck & Leggett, 1988). Students with learning goals seek
challenges in an effort to gain competence in a domain, whereas students with performance
goals want to demonstrate their competence in some way. Learning goals are often
associated with deeper learning, whereas performance goals, including those that are
normative or competitive (where students wish to do better than others) or outcome
performance goals, which are “simply focused on obtaining positive outcomes” are generally
associated with positive outcomes, albeit more shallow learning (Grant & Dweck, 2003).
However, some types of performance goals (e.g. ability goals, in which students wish to be
seen by others as competent) have been found to be detrimental to either type of learning
when a set-back is encountered (Grant & Dweck, 2003).

It is plausible that tutors can influence motivation. For instance, tutors may influence students’
beliefs about their efficacy (Lepper et al., 1993), they may embody extrinsic motivation, or they may
encourage students to learn rather than perform.
The FTF vs. CM manipulation may differentially affect students’ motivation. In FTF
communication more channels of information are available to participants that are not available in CM
communication, including visual information such as each other’s facial expressions, gestures, general
appearance and auditory information, such as a speaker’s tone of voice, phrase inflections, and
emphasis on particular words or phrases. Perhaps for these reasons, people view FTF communication
as more personal than CM communication (Lebie et al., 1995; Coleman et al., 1999) and generally say
they prefer FTF over CM communication (e.g. Adrianson, 2001). Furthermore, tutors have more
potential ways to influence their students’ affect in FTF than CM tutoring (Tu, 2000). For example,
tutors can adopt a softer tone of voice or a sympathetic facial expression. Therefore, we expect interpersonal motivational goals in particular to be influenced more in the FTF context than in the CM
context. However, again, little is known about whether and how tutoring contexts differ in their
influence on students’ confidence, intrinsic motivation, and achievement goals.
In the present study, we compared student learning and changes in motivational beliefs in one-toone FTF and synchronous CM tutoring (Microsoft NetMeeting’s chat window and whiteboard were
used) during physics problem-solving and investigated features correlated with learning in both

contexts3. Because novice or peer tutors (rather than highly experienced, professional teachers) often
serve as tutors in FTF educational settings (Cohen et al., 1982), and adult novice tutors (e.g. graduate
research assistants or advanced undergraduates) may serve as online tutors (e.g. Anderson, 2002),
novice tutors were used in this study. We expected that, regardless of context, measures of student
productivity would be correlated with learning outcomes. We did not expect to find any differences in
learning, and therefore hypothesized that the two contexts would allow for equal levels of productive
student engagement. However, we did expect learning in the FTF context to be more efficient than in
the CM context. Finally, we expected that inter-personal motivational goals in particular would be
influenced more in the FTF context than in the CM context. These findings will have important
implications for FTF and synchronous CM human tutoring as well as computer tutoring systems that
incorporate text-based natural language.

METHODS
Participants
Students: Undergraduates who had taken an introductory physics class in high school and/or college
served as students (or tutees). The FTF condition consisted of 20 students (9 men and 11 women).
The CM condition consisted of 20 students (14 men and 6 women). Students who were recruited
through the psychology subject pool received course credit and those who were not received payment
of $7/hour for their participation.
Tutors: Advanced undergraduate or graduate students in physics or engineering who
demonstrated knowledge of the task domain and at least some tutoring experience, including informal
tutoring experience (e.g. tutoring a friend), served as tutors. The FTF condition consisted of 20 tutors
(16 men and 4 women), and the CM condition consisted of 20 tutors (also 16 men and 4 women).
Tutors received $10 per hour for their participation.

Materials
Materials for students included the following:
1. Background questionnaire: Assessed factors that may influence student learning, including
SAT math and verbal scores, physics classes taken and grades received in those classes.
2. Physics pre-test: Assessed students’ knowledge of the physics concepts relevant in the
tutoring questions. The physics pre-test included:
a. Definitions of terms that assessed students’ understanding of the individual physics
concepts (e.g. distance, displacement, speed, velocity, acceleration),
b. Short-answer questions that assessed their knowledge of the relationships between
these concepts (i.e. kinematics equations, Newton’s second law).
3. Pre-tutoring motivation questionnaire: The motivation questionnaire given to students was the
Motivation for Reading Questionnaire (Wigfield & Guthrie, 1997), adapted to college students

3

The FTF condition may be considered the “control” acting as a comparison for the CM condition. A (nointervention) control condition was not necessary in this study because its pre/post design allows for the
assessment of learning across the intervention.

4.
5.

6.
7.

and to the tutoring domain, physics (excluding questions that could not be sensibly adapted to
the college-aged population and subject-domain of this study, for example, “My parents often
tell me what a good job I am doing in reading”; “I make pictures in my mind when I read”; “I
like mysteries”). This questionnaire included items that targeted various aspects of students’
motivation, including their:
a. Self-confidence or efficacy in physics,
b. Desire for challenge in physics (consequence of holding a learning goal),
c. Curiosity about physics (intrinsic motivation),
d. Perceptions of the importance of being good in physics (learning goal),
e. Need for validation or recognition of ability from others (type of performance goal:
ability goal),
f. Competitiveness in physics (a normative performance goal),
g. Beliefs about grades in physics (type of performance goal),
h. Compliance, or willingness to work hard in physics (or inversely, to avoid work).
We added the item “I find physics generally interesting”, another aspect of intrinsic
motivation (interest).
Because the motivation questionnaire was modified for a different age group (college
students rather than elementary school children) and a different domain (physics rather than
reading), an exploratory factor analysis was performed on students’ responses to the pretutoring motivation questionnaire to determine the different underlying dimensions of student
motivation assessed by the questionnaire for this subject population. Students indicated their
agreement with items on the motivation questionnaires on a 7-point Likert scale (1 – strongly
disagree; 4 – no opinion; to 7 – strongly agree).
Mid-tutoring motivation questionnaire: Identical to the pre-tutoring motivation questionnaire
but with fewer statements (to minimize interruption to the tutoring session).
Mid-tutoring physics test: Included short-answer questions that assessed students’ knowledge
of the concepts and equations that were relevant in the solution to the question discussed in the
final tutoring segment. Questions targeting concepts that were not addressed in the final
tutoring segment were also included to prevent priming of only relevant concepts for the final
question. To minimize the disruption to tutoring, definition questions were not included on
the physics mid-test.
Post-tutoring motivation questionnaire: Identical to the pre-tutoring motivation questionnaire.
Physics post-test, which included the following:
a. Terms assessing students’ knowledge of the individual concepts relevant in the
tutoring questions (same as physics pre-test),
b. Short-answer questions requiring the application of various kinematics equations and
Newton’s second law,
c. One near-transfer question that was isomorphic to the final question discussed in
tutoring (different surface features but same deep structure).

Materials for tutors included the following:
1. Background questionnaire: Tutors completed a background questionnaire that assessed factors
that may influence tutoring effectiveness, including their prior tutoring experience and tutor
training.
2. Physics knowledge assessment: To ensure that tutors had adequate knowledge of the tutoring
topics, tutors completed a physics knowledge assessment that consisted of questions assessing

3.
4.

5.

6.

the concepts discussed in the questions for the tutoring session. Those who did not pass this
assessment did not serve as tutors.
Instructions for tutoring: Instructions about the tutoring session procedure.
Questions for tutoring, by segment of the tutoring session:
• Segment 1: Two questions on trigonometric relations,
• Segment 2: Two questions: (Q1) kinematics relation; (Q2) Newton’s second law,
definition of average acceleration,
• Segment 3: One question: two-dimensional kinematics, Newton’s second law,
• Segment 4: One question: trigonometric relation, two-dimensional kinematics
equations, Newton’s second law, and definition of average acceleration.
This was the most complex question covered in tutoring.
To control for the specific content that tutors covered in the tutoring session, they were
given the “ideal” solutions to each of the questions, which included a breakdown of all of the
solution steps that were considered relevant in the solution to that question.
Tutor mid-tutoring motivation questionnaire: This questionnaire was identical to the midtutoring motivation questionnaire given to the students, but with brief instructions for tutors.
The purpose of this questionnaire was to assess the accuracy of tutors’ assessments of their
students’ overall motivation, confidence, and different dimensions of motivation. Tutors were
instructed to predict their students’ responses on the 7-point Likert scale for all items on the
mid-tutoring motivation questionnaire.
Tutor mid-test (this will not be discussed here4).

Design
This study incorporated a 2 x 2 between-subjects design. The two independent variables were prior
experience (Same or Different), and tutoring context (FTF and CM). In the Same conditions, each
tutor was paired with a student and worked with that student in all four segments of the tutoring
session, whereas in the Different condition, the students rotated among tutors so that tutors worked
with a different student in each of the four tutoring segments. In this paper we compare only the FTF
Same and CM Same conditions, which are more typical of real-world tutoring, where tutors and
students work with each other for the duration of a tutoring session. We will refer to these simply as
the FTF and CM conditions.

Procedure
About one week before the tutoring session, all students first completed the background questionnaire,
then the physics pre-test. The experimental procedure is given in Table 1. Students were assigned to
conditions so that pre-test scores were counter-balanced. All tutors completed the background
questionnaire, then the physics knowledge assessment. Afterwards, tutors were given the instructions
for tutoring, the questions for tutoring and the corresponding ideal answers and figures for each

4

These materials are relevant for the discussion of the effect of tutoring context on the accuracy of tutors’
assessments of their students, discussed in Siler and VanLehn (2003). These results will not be discussed here.
Please refer to that paper for more information.

question. They were instructed to become familiar with the questions and answers before returning
for the tutoring session.
Table 1
Experimental procedure
Timeline

Student activity

Tutor activity

Before the
tutoring session

(1) Background questionnaire
(2) Physics pre-test

(1)
(2)
(3)
(4)

Immediately before
the tutoring session

(3) Pre-tutoring motivation
questionnaire

(no corresponding tutor activity)

During the
tutoring session

Background questionnaire
Physics knowledge assessment
Given instructions for tutoring
Given questions for tutoring
session

Segment 1 (15-minute time limit)
Segment 2 (30-minute time limit)
Segment 3 (15-minute time limit)
(4) Mid-tutoring motivation
questionnaire
(5) Physics mid-test

(5) Tutor mid-tutoring motivation
questionnaire
(6) Tutor mid-test (Likert ranking of
students’ general competence)

Segment 4 (no time limit)
After the
tutoring session

(6) Post-tutoring motivation
questionnaire
(7) Physics post-test

(tutor is finished)

Approximately one week after their respective first sessions, students and tutors returned for the
tutoring session. Students first completed the pre-tutoring motivation questionnaire. All tutors were
given hardcopies of the questions that were to be discussed in the tutoring session and the solutions to
each question, which they were permitted to refer to during the tutoring session. Tutors in the FTF
condition were told to not allow their students to see the problem solutions. After students completed
the pre-tutoring motivation questionnaire, each tutor-student pair was introduced face-to-face.
In the FTF condition, the tutor and student sat together at a desk in one room. All tutor-student
pairs were given pens and blank paper and told that they could draw. In the CM condition, each tutor
sat at a computer in one room and the student sat at another computer in a different room. The
questions and corresponding figures were saved on the Desktops of all tutors’ computers. Tutors and
students in the CM condition were separately instructed on the use of Microsoft NetMeeting. They
communicated with each other using this program’s chat window and whiteboard. Turn-taking in the
chat windows was not enforced; that is, tutors and students were able to send text messages to each
other at any time. A message was viewed by the receiver only after the composer sent it by hitting the
Enter key. All text that had been sent within a tutoring segment could be viewed by either the tutor or
student. They were told that they could draw in the whiteboard during the tutoring session. Tutors
and students informed the experimenter when they were comfortable enough using NetMeeting to
begin the first segment.

In the FTF condition, the starting time of each of the four tutoring segments was when the tutor
began reading the student the first question for that segment. In the CM condition, tutors pasted the
figures for each question into the drawing window, and then pasted the questions into the chat
window. The start time for the CM segments was when the tutors sent the first question for that
segment to the student. The experimenter was in the hallway outside the rooms to answer any
questions or address any problems that arose during the tutoring segments. Each of the first three
tutoring segments had a time limit (15 minutes for the first segment, 30 minutes for the second, and 15
minutes for the third). There was no time limit for the fourth and final tutoring segment. For the first
three tutoring segments, after the time limit had expired, tutoring was stopped if the tutor and student
had not finished discussing the questions for that segment. In the CM condition, the chat and drawing
windows were saved and closed, and new windows were opened. In the FTF condition, any drawings
made were taken by the experimenter. All FTF tutoring segments were audio-taped and transcribed.
After the third tutoring segment, tutors and students were moved to separate rooms to complete
the mid-tutoring motivation questionnaire and physics mid-test. All students first completed the midtutoring motivation questionnaire and then completed the physics mid-test. When both the tutor and
the student finished the mid-tutoring motivation questionnaire and physics mid-test, they began the
final tutoring segment.
When the tutor-student pair indicated that they were finished with the final problem, the student
completed the post-tutoring motivation questionnaire and finally the physics post-test. Students who
were not recruited from the psychology subject pool were paid and all were debriefed. All tutors were
paid and debriefed.

RESULTS
There were no significant differences between FTF and CM conditions on student subject variables
that may have influenced learning from tutoring, including physics pre-test scores, math SAT scores,
verbal SAT scores, or total SAT scores. There were also no significant differences between tutoring
conditions for tutor variables, including tutor pre-test, math SAT, and verbal SAT.

Student Learning by Context
Total post-test score: The total physics post-test score consisted of definitions, short-answer problems,
and a problem isomorphic to the final question discussed in the tutoring session. There was no
significant difference in post-test scores between the FTF and CM conditions (Table 2), F(1, 38) =
0.47, p = .50.
Table 2
Mean unadjusted and adjusted total post-test scores (standard deviation) by condition
Total post-test score:
Condition
FTF
CM

Unadjusted
42.88(17.57)
38.65 (21.30)

Adjusted for pre-test
40.02 (3.67)
41.51 (3.67)

Adjusted for mid-test
41.46 (2.67)
40.07 (2.67)

When student pre-test was factored out in an ANCOVA, there was still no difference between
conditions in total post-test score, F(1, 37) = 0.08, p = .785. There were no significant differences
between conditions for any of the subparts of the post-test (i.e. term definitions, short-answer
questions, or the isomorphic question). Thus, knowledge gains were similar for the two conditions.
To see if there were any learning differences across the final tutoring segment (which had no time
limit), mid-test scores were factored out of total post-test in an ANCOVA. There were again no
differences, F(1, 37) = 0.14, p = .72. For just the definitional scores, although there was a significant
pre-to-post-test increase, F(1, 38) = 24.20, p < .001, there was no time by condition interaction, F(1,
38) = 0.49, p = .49. Thus, students in the FTF and CM context gained similar amounts on this
measure of conceptual learning. There was also a significant gain on the short-answer problems, F(1,
38) = 43.55, p < .001. But again, there was no difference between conditions in the amount of gain,
F(1, 38) = 0.78, p = .38. Thus, on both of these measures of learning, although there were gains across
the tutoring session, there were no context differences.

Differences in tutoring efficiency
Next, we looked at tutoring efficiency. Because the final segment was the only one without an
imposed time limit, we chose to assess efficiency across this segment6. Time efficiency was defined
as the ratio of total post-test score adjusted for mid-test score to time on task.
Table 3
Efficiency measures for the final tutoring segment: means (and standard deviations)
Condition

Segment 4 time (minutes)

Efficiency a

29.35 (14.80)
49.04 (15.55)

18.42 (12.00)
9.17 (5.49)

FTF
CM
a

Efficiency values were multiplied by 1,000.

As in earlier studies (e.g. Litman et al., 2006), times were longer in the CM condition (about 1.7
times longer), a highly significant difference (Table 3), F(1, 38) = 16.81, p < .001. Correspondingly,
time efficiency was greater in the FTF condition, F(1, 38) = 9.83, p = .0037.
How would this compare to time efficiency differences if participants in the CM context had used
speech-to-text technology instead of typing messages? In this study, only the times that messages were
sent were recorded, and not the time typing of the message began. Thus, we cannot get an exact
measure of typing times. However, we can get an approximation of the time spent typing. Given the
total number of words exchanged and using an average typing rate of 40 wpm, we first approximated
segment 4 times if tutors and students had spoken to each other instead of typing. An important note
5

There was no aptitude-treatment interaction: the student pre-test by condition interaction was not significant,
F(1, 36) = 0.87, p = .36.
6
According to Bordia (1997, p. 112): “Providing a limited amount of time puts pressure on subjects to somehow
complete the task. This may distort the experimental manipulation and may introduce unintended effects.” For
these reasons, we chose to target the final tutoring segment.
7
The variance in time efficiency was significantly lower in the CM than in the FTF context. However, this is
likely just a mathematical consequence of dividing adjusted post-test scores by a larger number (segment 4
time).

is that this assumes that tutors and students did not type simultaneously, though they almost certainly
did at times. Therefore, almost certainly, more time was removed from the CM condition than their
actual typing time, and this can be considered an absolute best-case scenario. After converting typing
to speaking rate8, there was no longer a time difference, F(1, 38) = 0.17, p = .68. Efficiencies were
calculated as the same total post-test scores with mid-test scores factored out, divided by the new
estimates of time. There was no longer a difference in efficiency (Table 4), F(1, 38) = 0.29, p = .59.
However, the assumption that CM students would have learned the same amount in this reduced time
is unlikely. This analysis does not show that there would be no time or efficiency differences had the
participants in the CM context spoken rather than typed; rather, it merely does not rule out the
possibility that times and efficiencies in a CM context could approach those in a FTF context.
Table 4
Efficiency measures for the final tutoring segment: typed converted to spoken:
means (and standard deviations)
Condition

Segment 4 time (minutes)

Efficiency a

FTF

29.35 (14.80)

18.42 (12.00)

31.18 (13.40)

16.38 (11.91)

CM (converted)
a

Efficiency values were multiplied by 1,000.

Motivational Outcomes
The different dimensions of student motivation that were addressed by the motivation questionnaire
were found using exploratory principal components factor analysis with the Varimax rotation method.
This was performed on all 80 students’ (i.e. students in both contexts and experience conditions)
responses to the 26 statements on the pre-tutoring motivation questionnaire. All 80 students were used
to maximize the power of the factor analysis. Factorability was acceptable (KMO = .78). Seven
component factors with eigenvalues greater than one emerged from this analysis. These seven factors
accounted for over 73% of the total variance. The first component factor, which comprised 33.75% of
the total variance, tapped strongly into a factor related to confidence in one’s physics ability. This
factor loaded most highly (above the criterion .60) on the following statements:
•
•
•
•
•
•

I know I will do well in the tutoring session and on the physics post-test today
I am generally good in physics
I am generally better at solving physics problems than most people I know
I like hard, challenging physics problems
If I find it interesting, I can understand difficult physics concepts
I am usually able to learn difficult physics concepts

The second component factor, which comprised 11.34% of the total variance, loaded most highly
on four statements related to competition with other students in physics, a normative performance
goal:

8

An average speaking rate of 150 wpm (Wald & Bain, 2008) and an average typing rate of 40 wpm (Karat et al.,
1999) were used in the conversion: Time (if spoken) = Time (CM actual) – [total words*(1/40 – 1/150)].

•
•
•
•

I try (or tried) to do better on physics exams than my friends
I would like to get the highest scores on physics tests9
I would like to finish physics problems before other students in my class
I would like being the only one who knew an answer to a physics question

The third component factor, which comprised 6.63% of the total variance, loaded most highly on
four statements related to physics curiosity, a measure of intrinsic motivation:
•
•
•
•

If the physics topic is interesting, I might investigate it more later
I read to learn about physics topics that interest me
I like to read about physics in my free time
I only read about physics when I have to (e.g. for a class) (negative loading)

The fourth component factor, which comprised 6.50% of the total variance, loaded most highly
(above .60) on two statements related to desire for challenge in physics, a consequence of holding a
learning goal:
•
•

I like when physics questions make me think
Even if the physics topic is difficult, if I find it interesting I will try hard to understand it

The fifth component factor, 5.82% of the total variance, loaded most highly on two statements
that assessed students’ need for validation of their physics ability from others, another type of
performance goal. As stated previously, this goal has been called an ability performance goal in the
motivation literature (e.g. Grant & Dweck, 2003). We will refer to this factor as “ability goal”
henceforth. Because it taps into a wish for others to consider them competent, this can also be
considered a type of inter-personal goal:
•
•

I would like my physics instructor to think I am good in physics
I like (or would like) to get compliments for my physics ability

The sixth component factor, 5.14% of the total variance, loaded most highly on three statements
that can be considered related to students’ desire to get good grades in their physics class, an outcome
performance goal:
•
•
•

In comparison to other subjects, it is (or was) very important for me to do well in physics
I study (or studied) mainly to improve my grade in the class
I do (or did) as little work in my physics class as possible (negative loading)

No statements loaded onto the seventh and final component factor above .60. There were five
statements (including “I find physics generally interesting”) that did not load on any factor above the
criterion .60.
Changes in motivational beliefs by context
When considering students’ overall average motivation scores (the sum of Likert scores across items
divided by the number of items), there was no main effect of time, nor was there an overall context by

9

The underlined questions were not included on the mid-tutoring motivation questionnaire.

time (pre/post-tutoring) interaction (Table 5). Thus, overall average motivation scores were stable
across time for students in both contexts.
Table 5
Motivational measure means by time and context
Measure
Overall average
score (26 items)
F1: Confidence
F2: Competition
F3: Curiosity
F4: Challenge
F5: Ability goal
F6: Grades
Interest in
physics

Context
FTF
CM

Pre-tutoring
4.31 (0.85)
4.08 (0.85)

FTF
CM
FTF
CM
FTF
CM
FTF
CM
FTF
CM
FTF
CM
FTF
CM

4.17 (1.28)
4.13 (1.11)
4.73 (1.21)
4.50 (1.26)
3.11 (1.49)
3.04 (1.08)
5.15 (1.28)
4.75 (1.22)
4.60 (1.43)
3.83 (1.17)
4.68 (1.11)
4.53 (1.46)
4.85 (1.42)
4.70 (1.56)

Time
Post-tutoring
4.33 (0.75)
4.03 (0.90)
4.22 (1.07)
4.07 (1.27)
4.75 (0.96)
4.55 (1.20)
2.83 (1.57)
2.90 (1.22)
4.83 (1.42)
4.45 (1.20)
4.65 (1.17)
4.40 (1.26)
4.88 (1.02)
4.38 (1.55)
4.60 (1.54)
4.10 (1.52)

Time

Interaction

(n.s.)

(n.s.)

(n.s.)

(n.s.)

(n.s.)

(n.s.)

p = .07

(n.s.)

p = .07

(n.s.)

p = .03

p = .07

(n.s.)

(n.s.)

p = .02

(n.s.)

However, when looking at the individual factors10, there were some significant changes.
Students’ curiosity scores (Factor 3) showed a strong trend of declining from pre- to post-tutoring,
F(1, 38) = 3.37, p = .07, and this decline was similar across tutoring contexts (Table 5). Similarly,
students’ challenge scores also showed a strong trend of declining, F(1, 38) = 3.59, p = .07, again with
similar decreases across contexts. The only motivational factor that showed a strong trend of a time
by context interaction was ability goal (Factor 5), F(1, 38) = 3.54, p = .07. Again, these two items
were “I would like my physics instructor to think I am good in physics” and “I like (or would like) to
get compliments for my physics ability”. FTF students’ scores were stable across time whereas CM
students’ scores increased from pre- to post-tutoring. Finally, students’ agreement with the statement
“I find physics generally interesting” decreased significantly from the beginning of the tutoring
session, F(1, 38) = 6.45, p = .02. Though decreases were larger for CM students than for FTF
students, this difference was not significant (p = .30).
In summary, though there were pre-to-post tutoring changes for most of the motivational
measures, there was only one time by context interaction that showed a trend approaching
significance, for the ability goal factor (validation), where CM students’ scores increased with time
while FTF students’ scores were stable. As stated earlier, ability performance goals are generally
found to be negatively associated with performance. If this relationship between ability goal and
performance is causal, then this result favors learning in the FTF context.
The only factor for which there was a context difference in the accuracy of tutors’ predictions of
their students’ mid-tutoring responses on the motivation questionnaire was the challenge factor. That

10

Individual factor scores were averages across student responses to all items comprising that factor.

is, this was the only factor for which there was a significant context by tutor prediction interaction,
F(2, 37) = 3.77, p = .03. In the FTF context, there was a significant positive relationship between
tutors’ predictions of students’ challenge scores (r = +.65, p = .002). However, in the CM context, this
correlation was not significant (r = +.09, p = .70). Because only one of the six factors differed, these
results suggest that there is only a slight advantage in the FTF context in obtaining affective
information, which parallels the results for student motivational gains just discussed.

How did students learn?
There were no learning differences between the FTF and CM conditions, though the FTF condition
was more time-efficient. But were the features associated with learning similar across the two
contexts, as predicted? We will first look at context differences for student activity, tutor activity, and
tutor-student interactivity. Then, to address this question, we will look at the relationship between
each of these types of activity and student learning in each context.
Student and tutor activity by context
Consistent with prior findings (e.g. Litman et al., 2006), there were large differences between the
spoken and typed communication modalities. In the final tutoring segment (Table 6), compared to the
CM context, the FTF context had more student words, more tutor words, more turns and less time per
turn. These differences are expected due to the difference between speaking vs. typing.
Table 6
Student and tutor activity in the final tutoring segment: means (and standard deviations)
Measure
Student words
Tutor words
Turns
Time (sec) per turn
Student % words
Mean student words per turn
Mean tutor words per turn

FTF
616.20 (407.64)
1965.80 (1275.91)
224.67 (119.35)
10.19 (10.61)
24.62 (11.61)
5.82 (3.89)
23.11 (25.02)

CM
177 (116.49)
796.70 (299.49)
80.80 (46.18)
47.27 (29.01)
18.01 (9.53)
4.48 (1.71)
23.68 (11.11)

F(1,33), p
21.11, p < .001
15.78, p < .001
24.40, p < .001
22.15, p < .001
3.42, p = .07
1.90, p = .18
0.01, p = .93

In terms of relative word production, students in the FTF context showed a strong trend of being
more active than students in the CM context. That is, students in the FTF condition produced a higher
percentage of total words in the final tutoring session than students in the CM condition (Table 6).
Thus, students in the FTF context were more active (at least overtly) than students in the CM context.
However, there was no difference between conditions in mean student or tutor turn length, which may
give the impression that the FTF and CM contexts were similarly interactive. However, FTF
discussions generated almost three times as many turns, showing that the FTF context was actually
more interactive.
One would expect tutoring to be more adaptive in the FTF condition than the CM condition due
to the increased information contained in speech, gesture, etc., of FTF tutoring. Consistent with this
hypothesis, the variance in number of student and tutor words was significantly greater in the FTF
condition (F = 6.46, p = .02; F = 12.81, p = .001, respectively), as was the variance in number of turns

(F = 14.17, p = .001)11. Because a greater variance in tutor words, per se, is not evidence of more
adaptive tutoring in the FTF context, additional evidence is needed to support this hypothesis. If tutor
words were adaptive in the FTF context, one may expect the number of tutor words to be related to
student mid-test score only in the FTF context; specifically, one may expect a negative relationship if
tutors tend to say more to students who are less competent. In fact, there was a significant context by
student mid-test score interaction, F(1, 31) = 5.25, p = .03. In the FTF context, there was a significant
negative relationship between student mid-test score and tutor words, r = -.54, p = .04. However, in
the CM context there was no significant relationship, r = -.29, p = .22. Thus, these results are
consistent with the hypothesis that the greater variances in the FTF context may be attributable to
more adaptive tutoring12. The variance in time per turn was greater in the CM condition (F = 9.23, p =
.005), which could be a consequence of a greater variance in typing than in speaking speed.
Activity and student learning
Let us turn now to the relationship between different types of activity in the final tutoring segment and
student learning in that segment (total post-test score, with mid-test score factored out using
ANCOVA), starting with the CM condition (Table 7, last column). There were significant positive
pair-wise relationships with student words and mean student turn length – students who typed more
words learned more. There was a significant negative relationship between tutor turn length and
student learning – when tutors typed fewer words, students learned more. Given these two
relationships, one would expect percent student words to be strongly positively correlated with
learning, and it was. However, the two relationships had opposite slopes, so when student and tutor
turns were aggregated, words per turn only showed a strong trend of being correlated with learning.
Table 7
Summary of pair-wise correlations with student learning over the final tutoring segment
Measure
Student words
Mean student words per turn
Tutor words
Mean tutor words per turn
Student % words
Mean words per turn

FTF
n.s.
n.s.
n.s
r = -.54, p = .04
n.s.
r = -.55, p = .03

CM
r = +.52, p = .02
r = +.58, p = .008
n.s.
r = -.52, p = .02
r = +.71, p < .001
r = -.43, p = .06

To see which of these CM variables (some of which were inter-correlated) were most closely
associated with student learning, we included student words, percent student words, mean student turn
length, and mean tutor turn length in a stepwise regression analysis. Mean student turn length
remained positively related to student learning, r(partial) = +.66, p = .002, and mean tutor turn length
remained negatively related, r(partial) = -.62, p = .005. This suggests that student and tutor turn length
are independent contributors to student learning.

11

The difference in variance could also be due to a floor effect in the CM condition.
Similar trends (though with lower p-values) were found for student words and turns. However, this is not
surprising since student words, tutor words, and turns were all significantly correlated with each other.
12

To illustrate how mean tutor and student turn length related to learning in the CM context,
excerpts are given from the final tutoring segment for the CM students with the lowest and highest
gains, respectively, across this final tutoring segment (Figures 1 and 2). Note that in the low-learning
CM excerpt, the tutor dominated the discussion and completed every step of the problem solution with
the exception of calculating the numerical answer. In contrast, in the high-learning excerpt, the
student tended to talk more and produced the equation on his own, while the tutor hung back and let
the student do as much as he could.
______________________________________________________________________________________
T: this problem uses most of the stuff we already did
first we want to find the initial vertical velocity
S: ok
T: we will use the equation we used to find the Vi of the man jumping
S: ok
T: 2*a*(Xfinal-Xinitial)=Vf^2-Vi^2
we know the max height is 5m so we will use that
S: is 5m the Xfinal?
T: a= gravity, Xf=5m, Xi=0,Vf=0: because the object is at its max height and has essentially stopped, and
Vi is what we are looking for
S: ok
T: remember that this is only the vertical component of the velocity, and solve the equation
S: the answer is 10
______________________________________________________________________________________
Fig.1. Excerpt from the CM session with the lowest learning gain.
______________________________________________________________________________________
S: OK, vertical v is going to relate to max height (5M) and force of gravity (10 m/sec/sec)
T: Yep
S: am I allowed to pull the formulas out of my wallet?
…
S: ending velocity will be zero, correct?
T: We can't do this without the formulas. I say go ahead.
S: [writes equation] that's out of memory -- how wrong is it?
T: That' right.
S: No kidding? Cool
T: Yes.
S: Now what?
______________________________________________________________________________________
Fig.2. Excerpt from the CM session with the highest learning gain.

Now let us turn to the FTF condition (Table 7, middle column). The mean tutor turn length was
negatively related to student learning, as was the aggregated words per turn – dialogues with fewer
words per turn produced more learning. These results suggest that more interactive tutoring produced
higher learning gains in the FTF context. However, because student words per turn was not
significantly related to their learning, the aggregate words per turn relationship to student learning is
due to only the tutors’ words per turn.
Figures 3 and 4 are excerpts from the FTF tutoring sessions that resulted in the lowest and the
highest learning gains, respectively. In the low-learning excerpt, the tutor not only produced the
relevant equation, but also used the equation to calculate the target variable’s value. After the tutor
had derived the value without interacting with the student, the student showed confusion by asking the

tutor vague clarification questions. This example demonstrates how lack of interaction in the form of
a lengthy tutor turn may impair student learning in the FTF context. In contrast, for the highestlearning excerpt, the tutor did not give the student the equation, even after the student indicated in her
response that she needed some equations to move forward in the problem. Rather, the tutor
“scaffolded” the student, giving her only minimal support in producing the equation.
______________________________________________________________________________________
T: The first thing we want to find is this Vi, this initial velocity. That would be the easiest thing to do.
Let’s see… maximum height is 5 meters. So we have that Vi it’s broken into Vi cos 45, and Vi sin 45.
And then we have… so you know that the maximum height is 5 meters. So we have delta y equals 5
meters. And then we know a in the y-direction equals 10 meters per second squared. Let’s see… And
what you need is… let’s start with the equation 2 a delta x, which would be final minus v initial
squared… So we know what, V? If we just go from… so that would be in the y-direction. So your v,
that’s Viy, you know the final y velocity, right here, is zero, when it hits the ground. So you have
twenty equals Viy squared… times five… so you have Viy equal to ten. So, that’s believable.
[pause]
S … Viy squared?
T: Yeah, that’s Viy squared.
S: Square root?
T: Yeah, the square root of that’s ten.
S … and then the Vi?
T: This V final is zero. So you got Viy squared, what is that minus? It would be minus. We can go…
square root of a negative number. We have to take the absolute value. Should be… it’s just… it is ten.
The negative number, it’s in there, but it’s not important. You know that velocity…
S: No I mean… the square root.
T: Yeah, that’s the square root of 100 is ten. So, it’d give you viy.
S: viy…
T: Ok, this is… This isn’t Vi times y. This is a sub, like a subscript of y. That’s just what I mean with
those two. That would be Vi in the y-direction.
S: Ok
T: That’s the way it gets in the end. That’s where physics gets in the way, you get too many subscripts to
keep track of. And you see how, you understand by trigonometry you get that to equal ten?
S: Ok
______________________________________________________________________________________
Fig. 3. Excerpt from the FTF session with the lowest learning gain.

Thus far, our hypothesis that student activity should predict greater learning gains in both
contexts was satisfied only in the CM context. Student activity, as measured by words per
conversational turn, was only associated with student learning in the CM context. But this may not be
a good estimate of student activity in problem solving, especially in the FTF context, where tutors’
back-channel responses (responses used to indicate that one is listening such as “mm-hmm” or “uhhuh”) may break up longer student turns, hiding a possible relationship with learning (refer to the
middle portion of the dialog in Figure 4 for examples of tutor back-channel responses). For example,
in the FTF context, a lengthy student explanation may have been broken into several turns as a result
of tutor back-channel feedback, whereas in the CM context this explanation would be coded as one
turn. Thus, “removing” back-channel feedback could potentially have the effect of allowing similar
relationships between features of dialog and learning outcomes to be uncovered. For this reason, we

calculated FTF student words per turn ignoring tutor back-channel responses13. Again, there was no
pair-wise relationship between the two, r = -.03, p = .92, or partial relationship when tutor turn length
was also included in the regression, r(partial) = -.15, p = .59. However, when students’ back-channel
responses were removed in addition to tutors’, the relationships between measures of student and tutor
activity and student learning now look very similar to those in the CM context (Table 8)14. Now,
measures of student activity including words spoken, words per turn, and percentage of words spoken
were positively related to their learning. When a backward step-wise regression was run, only student
words per turn remained significantly related to learning.
______________________________________________________________________________________
T: and, uh, we know that this height is gonna be what? This is the 5 meters. OK? So, um... do you... do
you think that you have enough information to find the vertical velocity in the vert-- this Vy?
S: Uh, I don't know. I'm not really sure [what I'd do.]
T: Do you need some equations?
S: I guess, yeah. Cause I can't do anything without them.
T: Do know what kind of equation you'd like-- what quantities it should relate?
S: velocity
T: mm-hmm.
S: distance, I guess
T: mm-hmm.
S: uh, acceleration
T: mm-hmm, OK. Do you remember the equation that we-- from the other problems? The one with the
acceleration and distance?
S: the 2 a...
T: mm-hmm.
S: times...
T: distance. delta x.
S: OK, yeah. Is that it?
T: yep, yep. good, good.
______________________________________________________________________________________
Fig.4. Excerpt from the FTF session with the highest learning gain.
Table 8
Summary of pair-wise correlations with student learning over the final tutoring segment
(tutor and student back-channel feedback (BCFB) removed in FTF context)

13

Measure

FTF (no BCFB)

CM

Student words
Mean student words per turn
Tutor words
Mean tutor words per turn
Student % words
Mean words per turn

r = +.52, p = .048
r = +.60, p = .02
n.s
r = -.51, p = .05
r = +.49, p = .06
r = -.48, p = .07

r = +.52, p = .02
r = +.58, p = .008
n.s.
r = -.52, p = .02
r = +.71, p < .001
r = -.43, p = .06

That is, the number of words in consecutive student turns immediately before and after tutor back-channel
responses (e.g. “yeah”, “mm-hmm”) were combined and considered to be one turn.
14
One student was an outlier in terms of student words, words per turn, and percent words spoken (2.9, 3.2, and
2.3 SDs above the mean, respectively). This student was excluded from these analyses.

To supplement the prior analyses and more directly test our hypothesis that student productivity is
associated with learning regardless of context, we looked at a deeper, more direct measure, the
percentage of problem-solving steps in the first topic of the final problem that were student-initiated.
This included who initiated equation choice, who initiated finding the values for each of the terms in
the equation, and who initiated solving the equation. An example of a student initiating equation
choice is shown below.
______________________________________________________________________________________
S: Mm-hmm, mm-hmm, well you don’t know anything about time but you do know about
height? So you want to use this [equation]? Do you want to use that [equation]?
______________________________________________________________________________________
Fig. 5. Example of student initiating equation choice (FTF context).

The proportion of steps that were student-initiated was positively correlated with student words
per turn in the FTF context (r = +.64, p = .008). This suggests that student words per turn reflects
productive student activity. When student-initiated steps were included in a backward regression
analysis with student and tutor words per turn (excluding back-channel feedback), and math and
verbal SAT scores, only tutor words per turn and student-initiated problem steps remained
significantly related to student learning, r = -.71, p = .01; r = +.63, p = .03, respectively. Thus, like
student words per turn, this deeper measure of student activity may be a good predictor of learning,
and possibly a better predictor than student words per turn. To investigate interactions between
student activity and other measures, an ANCOVA was run. Because tutor turn length was negatively
related to student learning in both contexts, this measure was included in the model. Additionally,
because students’ ability to understand tutor turns (particularly longer ones) may have contributed to
the negative relationship between tutor words per turn and student learning (especially in the FTF
context), students’ SAT verbal scores were included in the model. If longer tutor turns were
negatively related to student learning because students were having more difficulty processing them in
the relatively short allowable time between turns, we would expect that students with lower verbal
ability would have more difficulty with longer tutor turns than students with higher verbal ability. In
other words, we would expect a stronger negative relationship between tutor words per turn and
student learning for students with lower verbal SAT scores than for students with higher scores. And
in fact, there was a tutor words per turn by student verbal SAT score interaction that approached
significance, F(1, 8) = 4.88, p = .058.
Additionally, there was an interaction between student verbal SAT scores and the percentage of
initial topic solution steps that were student initiated that approached significance, F(1, 8) = 4.86, p =
.05915. To investigate these interactions, students were classified as “high” or “low” verbal ability (i.e.
those with verbal SAT scores above and below the mean of 572, respectively). For high-verbal
students, a multiple regression model with both student initiation and tutor words per turn as
independent variables showed a significant positive relationship between the percentage of studentinitiated problem steps and learning across the final tutoring segment16, r(partial) = +.97, p = .006.
However, there was no significant relationship between student learning and tutor turn length,

15

When the same analysis was run using math scores rather than verbal scores, there were no interactions
between math scores and percentage of student-initiated problem steps or math scores and tutor words per turn
that were significant or approached significance.
16
As measured by the total post-test score with mid-test scores factored out.

r(partial) = +.22, p = .72. Unlike the higher-verbal students, for lower verbal students, there was a
non-significant negative relationship between student initiation and learning, r(partial) = -.25, p = .59.
Thus, lower-verbal students did not appear to benefit from their problem-step initiations during
tutoring. However, there was a significant negative relationship between tutor words per turn and
student learning, r = -.85, p = .02. Thus, consistent with our prediction, long tutor turns were
negatively related to students learning only for lower-verbal students. Additionally, the hypothesis that
greater student initiating activity is associated with greater student learning in the FTF context was
supported only for students with higher verbal scores.
It does not appear that this negative relationship was due to tutors simply taking over actions that
would have been more beneficial for the lower-verbal student to do themselves: there was no
significant relationship between student-initiated steps and tutor words per turn, r = -.34, p = .40. A
more plausible interpretation is that lower-verbal students had difficulty initially understanding or
remembering the content of longer tutor statements. This interpretation is supported by the dialog
shown in Figure 3, which is an excerpt from a tutoring session where the tutor’s turns are relatively
long and the student was classified as lower-verbal; this student had the lowest learning gain across
the final tutoring segment. As evidenced by the subsequent dialog, this student did not understand the
tutor’s initial lengthy explanation of how to find the initial vertical velocity.
In the CM context, as in the FTF context, the proportion of steps that were student-initiated was
again positively correlated with student words per turn in the FTF context (r = +.60, p = .01), which is
consistent with the assumption that student words per turn reflects productive student activity. When
student-initiated steps were included in a backward regression analysis with student and tutor words
per turn, and math and verbal SAT scores, only tutor words per turn and student words per turn
remained significantly related to student learning, r = -.65, p = .006; r = +.70, p = .002. Thus, in the
CM context, this deeper measure of student activity does not appear to be a better predictor of overall
learning than overall student words per turn. To investigate interactions between student-initiated
activity, tutor words per turn, and verbal SAT scores, an ANCOVA was run. There were no 2- or 3way interactions that were significant or approached significance. Only student initiating activity and
tutor turn length were related to student learning. With these two variables entered in a regression
model, student initiating activity was positively associated with learning, r(partial) = +.53, p = .0317.
Additionally, tutor turn length was negatively associated with student learning, r(partial) = -.55, p =
.02. But in the CM context, tutor turn length did not differentially impact higher and lower-verbal
students’ learning like it did in the FTF context. One possible explanation for this context difference
is that lower-verbal students in the CM context re-read and/or used more time to process tutor
messages, reducing the advantage the better readers had. The explanation that longer tutor turns were
indicative of tutors taking over actions that would have been more beneficial for students to have done
themselves is not supported because both variables independently contributed to learning gains.
Moreover, the correlation between student-initiated activity and tutor words per turn was not
significant, r = -.11, p = .66.
Because they are related to student learning in both contexts, student-initiated problem-solving
steps appear to be a good measure of “productive student activity”. Because there were no significant
differences in learning between contexts, we would expect there to be no differences between contexts
17

There was a higher correlation between student turn length and student learning than between the measure of
student activity in this analysis (i.e. percent of student-initiated action in the first topic of the final question) and
learning, r(partial) = +.66, p = .002. Thus, student turn length is a better predictor of learning.

in the proportion of actions that were student-initiated. And in fact, there was no difference between
conditions on this measure. In the FTF context, students initiated on average 27.19% of these steps,
whereas in the CM context, students initiated on average 34.07%, a non-significant difference, F(1,
32) = 0.39, p = .54.

DISCUSSION
The primary question addressed in the present study was how do student learning and motivation
outcomes compare in face-to-face and synchronous CM tutoring contexts. There was no difference
between the CM and FTF contexts in students’ overall learning, as measured by their total post-test
scores, nor were there any differences on any of the subsections of the post-test. This is consistent
with the Litman et al. (2006) study of human tutoring, which did not find a significant difference in
learning gains between a CM condition and a condition where students spoke with (but could not see)
their tutors. The lack of a difference is also consistent with some studies comparing synchronous or
asynchronous on-line and face-to-face learning (e.g. Ocker & Yaverbaum, 1999; Schpilberg &
Hubschman, 2003; Dennis, 2003; Pilkington & Walker, 2003). How might these results translate into
group tutoring results? Gallupe et al. (1992) found that as group size increased, per-person
productivity on a brainstorming activity decreased in FTF groups, but did not in asynchronous CM
groups. This result may be due to factors such as production blocking and social loafing in FTF
groups. This raises the possibility that if group size had increased, FTF group performance would
decrease relatively more than CM group performance, allowing the possibility that CM groups would
out-perform FTF groups on this task. However, task differences make this analogy questionable, and
additional studies are necessary to answer this question directly.
Although there was no difference in gains, the time required to achieve those gains was
significantly less in the FTF context. That is, tutoring in the FTF context was more time efficient.
This finding is consistent with the Litman et al. (2006) study and with general findings for group
interactions (Bordia, 1997). In CM tutoring, the student must wait while the tutor types a response, so
any time the student is not involved in productive activity would be wasted.
Although the tutors dominated the discussion in both the FTF and CM contexts, the percentage of
words produced by the students showed a strong trend of being higher in the FTF context than in the
CM context. In addition, there was a non-significant trend of FTF students producing more words per
turn. The Litman et al. (2006) study found the opposite trends. That is, the Litman et al. CM students
generated a significantly higher percentage of words than the spoken-tutoring students, and had a
significantly higher number of words per turn as well. The greater student activity in the Litman et al.
study compared to the present one may be due to a difference in the way typed communication was
handled. In the Litman et al. CM context, turn taking was enforced. In particular, the tutor could not
send a message until the student had finished and sent his or her message. In our study, an impatient
tutor could send a message while the student was still typing18. Doing so may have reduced the
overall number of student words and words per turn in the CM context of our study.
Face-to-face tutoring has been found to increase students’ interest in the tutoring domain (e.g.
Cohen et al., 1982). However, in the present study, over the course of the tutoring session, students’

18

It was not possible to tell when and how often this occurred from the existing data.

reported interest in tutoring actually decreased in both tutoring contexts19. But there was no difference
between contexts in the magnitude of this drop. Students’ change in confidence did not differ across
tutoring contexts, nor did confidence change across time. Thus, there was no evidence that tutoring
context had different effects on students’ domain interest or self-confidence.
The only motivational factor for which there was a strong trend towards a time by context
interaction was ability goal (validation); this was the only factor that targeted inter-personal
motivational beliefs. Surprisingly, FTF students’ scores were stable across time whereas CM
students’ scores increased from pre- to post-tutoring. This result is surprising because CM
communication is usually considered to be less personal than FTF communication because fewer
social cues are available such as facial expression and voice intonation. Thus, we did not expect that
CM students would be sensitive to these items. One possible reason why CM students’ ability goal
increased is that students’ need for validation from the tutor was not satisfied due to the limited social
cues in the CM context that can convey tutors’ approval of student ability (e.g. vocal intonations).
Because stronger ability goals have been linked to weaker performance (Grant & Dweck, 2003), this
result favors the FTF context. However, because this was not significant at the .05 level and was the
only context difference for our measures of student confidence, intrinsic motivations, and different
achievement goals, our findings are cause for optimism for the effect of CM tutoring on motivational
measures compared to FTF tutoring. These results may not be good news for those developing affect
sensing to computer tutoring that does not go beyond what human tutors do naturally. However, more
studies that address this issue are important to support these results and to assess other motivational
measures, especially those that are known to be related to student learning. Additionally, the only
motivational measure that FTF tutors demonstrated accurate assessment of (which CM tutors did not)
was the challenge factor.
The remaining findings concern correlations between learning gains and various measures of
student and tutor activity. In order to make sense of these findings, we will assume that two
independent factors are primarily responsible for determining the size of the student’s learning gain.
1.
2.

A student factor that is not measured by the pre-test – how productive they are during the
tutoring session. We previously hypothesized that, regardless of tutoring context, more
productive student engagement should cause higher learning gains.
A factor that we will call “tutorial skill”. More skillful tutors cause higher student gains.

Let us assume that random sampling caused equivalent distribution of student engagements and
tutoring skill in the FTF and CM conditions, and that neither student engagement nor tutoring skill
changed during the course of tutoring. Since by hypotheses, these two factors are the major ones
determining learning gains, these assumptions explain why the two conditions displayed the same
learning gains. Now the stage is set for explaining the within-condition findings as well.
In the CM condition, student turn length was positively correlated with learning, which was also
found in the Litman et al. (2006) and Rosé et al. (2001) studies. For typed communication, student
turns with more words were associated with more learning. Litman et al. (2006) suggested that this
effect may be due to tutors receiving more information about the students’ beliefs, which would allow
the tutor to more effectively adapt the tutoring to the student’s needs. However, if this were true, we
19

One possible reason that students’ reported interest decreased is that, unlike tutoring in real-world settings, in
this study, tutors and students could not choose which problems they discussed, which may be of more interest to
them.

would expect longer tutor turn lengths, which presumably contain more feedback for the student, to be
positively related to student learning, when, in fact, we found the opposite. Thus, we believe that
student engagement is the more likely explanation for the positive relationship. A positive
relationship between student words per turn and learning is a robust finding across studies. If this
relationship can be established as causal, then tutors in a CM context may be advised to try to
encourage more student elaboration and activity in their turns.
In the FTF condition, student turn length was not correlated with learning. This is consistent with
Litman et al. (2006), who found no correlation in the FTF condition between student turn length and
learning gains. Our hypothesis that student engagement is causing both increased learning and
increased turn length should apply equally well to FTF as it did to CM. Why do the two contexts
differ? A major difference is that FTF tutors and students may provide “back-channeling,” (Clark,
1996) to each other within a conversational turn. Figure 4 provides illustrations. When transcribed,
these interjections break a spoken utterance up into several smaller turns. Thus, back-channeling may
hide associations between student activity and learning in the FTF context. Since back-channeling
within a turn cannot occur in the CM context, it displays the expected association: longer student turns
are correlated with larger learning gains.
When we accounted for (verbal) tutor and student back-channel responses by factoring them out
of activity measures, we found significant positive correlations between student activity (i.e. words,
words per turn, and percent words) and learning. In fact, the pair-wise relationships between tutor and
student activity and student learning in the FTF context were strikingly similar to those in the CM
context. So it appears that student activity measures such as turn length, which is a good predictor of
learning in the CM context, may also be a good predictor of student learning in the FTF context after
back-channeling is removed.
In addition, we found that another, deeper, measure of engagement, student-initiated problemsolving steps, was predictive of student learning in the FTF context. However, this was true only for
higher-verbal students. Why did lower verbal FTF students fail to benefit from their initiations? One
possibility is that the additional memory load they experienced during tutoring precluded them from
retaining what they had learned, regardless of how engaged or active they were.
For both CM and FTF conditions, the number of tutor words per turn was negatively correlated
with student learning. This makes sense if we assume that long tutor turns were confusing. This
should be especially true in this task domain, quantitative physics, where the tutor turns often
contained mathematical expressions and variable terms in those equations that can be difficult to
convey. These may be particularly difficult to convey verbally, as in the FTF context. However, the
similar negative correlations in both contexts do not suggest that the mathematical nature of the task
was more problematic in the FTF context. This may be because in this study, in the FTF context,
tutors and students could write anything they wanted, including equations, which could supplement
these verbal explanations and resolve potential confusions that arise if presented only verbally. Thus,
the quantitative nature of the task does not appear to explain why the number of tutor words per turn
was negatively correlated with student learning.
Another possible explanation for the negative relationship between tutor words per turn and
student learning is that longer turns were related to tutors performing actions that students would have
benefited from doing themselves. If this were true, we would expect there to be negative correlations
between student-initiated problem-solving steps and tutor words per turn. However, there were no
significant correlations. So there is no evidence that this caused the negative relationship between turn
length and student learning.

Perhaps longer tutor turns were negatively associated with learning because longer turns are
simply more difficult to parse and understand. To assess this possibility, we looked at the relationship
between tutor turn length and learning as a function of student verbal SAT scores. In the CM
condition, the negative relationship between tutor words per turn and student learning was similar for
higher and lower-verbal students. Thus, in the CM context, the possibly confusing longer tutor
statements affected higher and lower-verbal students similarly. This could be because students with
lower verbal ability were able to re-read anything they did not initially understand, thus negating any
comprehension differences due to reading ability. But why was the overall relationship between tutor
words per turn and student learning negative in the CM context? In the Litman et al. (2006) study,
tutor turn length was positively correlated with learning in the CM condition. The Litman tutor was
not only highly skilled, but he had more practice tutoring students on the problems, so it is plausible
that his long turns may have been better crafted and easier to understand than the long turns of our
tutors. More specifically, his explanations may have been generally more coherent than those on-thefly explanations generated by the tutors in this study, requiring fewer inferences to understand. And,
because back-channel feedback is not possible within a turn in CM tutoring, less coherent explanations
may have a larger negative impact than in FTF tutoring. Additionally, the nature of the tutoring task
differed in the present and Litman et al. (2006) studies. In Litman et al., students first gave typed
essay responses to qualitative physics questions before the tutor began providing feedback on their
responses, whereas in the present study, students were never given an opportunity to solve the problem
on their own first. If tutors’ statements were more often in response to students’ work in the Litman et
al. study than in the present one, then this may have made them more understandable to the Litman
students. These final two explanations may account for both the positive relationship between tutor
words per turn and learning in the Litman et al. study and the negative results in the CM context of the
present study.
In the FTF context, tutor turn length was negatively correlated with student learning only for
students with lower verbal SAT scores. There was no significant relationship between tutor words per
turn and learning for students with higher verbal SAT scores. We would expect this pattern if tutors
were not trying to make their longer turns more coherent, thus requiring more inferences to
understand, which may be more difficult for less verbal students. In the Litman et al. study, there was
no significant relationship between tutor words per turn and student learning in their spoken
communication condition. If it is true that the Litman tutor gave better, more coherent explanations
than the tutors in the present study, then this could explain why in their study, wordier tutor turns were
not related to student learning in general, but in the present study, wordier tutor turns were negatively
related to learning for students with lower, but not higher verbal ability. However, further research is
necessary to determine reasons for different relationships between tutor words per turn and student
learning and establish causal patterns. Only then can tutors in a CM setting be advised of how much
explanation or elaboration to give their students.
In summary, our major finding is that FTF tutoring was not more effective than CM tutoring for
learning or for motivational gains. This finding may be surprising to many researchers, especially
those that are working hard to build spoken-language interfaces to tutoring systems. On the bright
side, FTF tutoring was more efficient than CM tutoring, so a spoken language tutoring system may
have considerable practical importance because it takes less time to achieve the same gains as a typedlanguage tutoring system. When typing time was converted to speaking time in the CM context, the
difference in time efficiency between the CM and FTF contexts was no longer significant. However,
because it is possible that a reduction in overall time may result in reduced student learning, this

analysis did not show that spoken language CM tutoring would be as efficient as FTF tutoring; it
merely did not rule out this possibility. This question can only be answered by directly comparing
these two conditions. Lastly, Chi et al. (2001) compared three hypotheses for why students learn so
much from human tutors. The tutor-centered hypothesis is that tutoring effectiveness arises from the
tutor’s pedagogical skills. The student-centered hypothesis is that it arises from the student’s active
generation during the learning task, and the interactive coordination hypothesis is that it arises from
the joint effort of both the tutor and student. The student-centered hypothesis seems to be most clearly
supported in both contexts by our finding of a positive correlation between learning gains and student
activity measures (words per turn and student-initiated problem steps); it is less clear which of these
hypotheses is supported by our finding of an overall negative relationship between tutor turn length
and student learning. However, for future studies that investigate these hypotheses, our results suggest
that they could be done in the somewhat more tractable CM context.

ACKNOWLEDGEMENTS
This research was supported by the Office of Naval Research, Cognitive and Neural Sciences Division
MURI grant N00014-00-1-0600 and NSF Grant 9720359 to Circle, a center for research on intelligent
tutoring. We also wish to thank Chris Schunn, Jonathan Schooler, and Susan Fussell, for their helpful
guidance in the earlier stages of this research. And thanks to David Klahr, Ken Koedinger, Kevin
Willows, and Jodi Davenport for their helpful comments on drafts of this paper.

REFERENCES
Adrianson, L. (2001). Gender and computer-mediated communication: Group processes in problem solving.
Computers in Human Behavior, 17, 71-94.
Aist, G., Kort, B., Reilly, R., Mostow, J., & Picard, R. (2002). Experimentally augmenting an intelligent tutoring
system with human-supplied capabilities: Adding human-provided emotional scaffolding to an automated
reading tutor that listens. In Proceedings of the ITS 2002 Workshop on Empirical Methods for Tutorial
Dialogue Systems (pp. 16–28), online. San Sebastian, Spain.
Anderson, D. (2002). Interfacing email tutoring: Shaping an emergent literate practice. Computers and
Composition, 19, 71-87.
Benbunan-Fich, R., & Hiltz, S. (1999). Impacts of asynchronous learning networks on individual and group
problem solving: A field experiment. Group Decision and Negotiation, 8, 409-426.
Bloom, B. S. (1984). The 2 sigma problem: The search for methods of group instruction as effective as one-toone tutoring. Educational Researcher, 13(6), 4 -16.
Bordia, P. (1997). Face-to-face versus computer-mediated communication: A synthesis of the experimental
literature. Journal of Business Communication, 34, 99-120.
Cheung, W. S., & Hew, K. F. (2004). Evaluating the extent of ill-structured problem solving processes among
pre-service teachers in an asynchronous online discussion and reflection log learning environment. Journal
of Educational Computing Research, 30(3), 197-227.
Chi, M. T. H., Siler, S. A., Jeong, H., Yamauchi, T., & Hausmann, R. G. (2001). Learning from human tutoring.
Cognitive Science, 25, 471-533.
Clark, H. H. (1996). Using language. Cambridge, UK: Cambridge University Press.
Clark, H. H., & Brennan, S. A. (1991). Grounding in communication. In L. B. Resnick, J. M. Levine, & S. D.
Teasley (Eds.) Perspectives on Socially Shared Cognition (127-149). Washington, DC: APA.

Cohen, P. A., Kulik, J. A., & Kulik, C. C. (1982). Educational outcomes of tutoring: A meta-analysis of findings.
American Educational Research Journal, 19(2), 237-248.
Coleman, L. H., Paternite, C. E., & Sherman, R. C. (1999). A reexamination of deindividuation in synchronous
computer-mediated communication. Computers in Human Behavior, 15, 51-65.
Condon, S. L., & Cech, C. G. (1996). Functional comparison of face-to-face and computer-mediated decisionmaking interactions. In Herring, S. (Ed.) Computer-mediated communication: Linguistic, social, and crosscultural perspectives (pp. 65-80). Philadelphia: John Benjamins.
Condon, S. L., & Cech, C. G. (2001). Profiling turns in interaction: Discourse structure and function.
Proceedings of the 34th Hawaii International Conference on System Sciences. Maui, Hawaii.
Conole, G., Hall, M., & Smith, S. (2002). An evaluation of an online course for medical practitioners.
Educational Technology & Society, 5(3), 66-75.
Core, M. G., Moore, J. D., & Zinn, C. (2003). The Role of Initiative in Tutorial Dialogue. Presented at the 10th
Conference of the European Chapter of the Association for Computational Linguistics, Budapest, Hungary,
April, 2003.
Cummings, A., Schlosser, A., & Arrow, H. (1996). Developing complex group products: Idea combination in
computer-mediated and face-to-face groups. Computer Supported Cooperative Work, 4, 229-251.
Dennis, J. K. (2003). Problem-based learning in online vs. face-to-face environments. Education for Health, 16,
198-209.
Diakidoy, I., Stylianou, P., Karefillidou, C., & Papageorgiou, P. (2004). The relationship between listening and
reading comprehension of different types of text at increasing grade levels. Reading Psychology an
International Quarterly, 26(1), 55-80.
D’Mello, S. K., Craig, S. D., Gholson, B., Franklin, S., Picard, R. W., & Graesser, A. C. (2005). Integrating
Affect Sensors in an Intelligent Tutoring System. Affective Interactions: The Computer in the Affective
Loop Workshop at 2005 International Conference on Intelligent User Interfaces, 7-13.
Dweck, C. S., & Leggett, E. L. (1988). A social-cognitive approach to motivation and personality. Psychological
Review, 95(2), 256-273.
Fletcher, J., & Pumfrey, P. D. (1988). Differences in text comprehension amongst 7-8-year-old children. School
Psychology International, 9(2), 133-145.
Forbes-Riley, K., Litman, D., Silliman, S., & Tetreault, J. (2006). Comparing Synthesized versus Pre-recorded
Tutor Speech in an Intelligent Tutoring Spoken Dialogue System. Proceeding 19th International FLAIRS
(Florida Artificial Intelligence Research Society) Conference, Melbourne Beach, FL.
Gallupe, R. B., Dennis, A. R., Cooper, W. H., Valacich, J. S., Bastianutti, C. M., & Nunamaker, J. F., Jr. (1992).
Electronic brainstorming and group size. Academy of Management Journal, 35, 350-369.
Goodyear, P., Salmon, G., Spector, J. M., Steeples, C., & Tickner, S. (2001). Competences for online teaching:
A special report. Educational Technology Research and Development, 49(1), 65-72.
Grant, H., & Dweck, C.S. (2003). Clarifying achievement goals and their impact. Journal of Personality and
Social Psychology, 85, 541-553.
Hara, N., Bonk, C. J., & Angeli, C. (2000). Content analysis of online discussion in an applied educational
psychology course. Instructional Science, 28, 115-152.
Heckman, R., & Annabi, H. (2005). A content analytic comparison of learning processes in online and face-toface case study discussions. Journal of Computer-Mediated Communication, 10(2), article 7.
Heylen, D., Nijholt, A., & op den Akker, R. (2005). Affect in tutorial dialogues. Applied Artificial Intelligence,
19, 287-311.
Johnson, W. L., Rizzo, P., Bosma, W., Kole, S., Ghijsen, M., & van Welbergen, H. (2004). Generating socially
appropriate tutorial dialog. In E. André et al. (Eds.) Proceedings of the ISCA Workshop on Affective
Dialogue Systems (ADS) (pp. 254–264). Berlin:Springer-Verlag.
Jones, R. H., Garralda, A., Li, D. C. S., & Lock, G. (2006). Interactional Dynamics in On-Line and Face-to-Face
Peer-Tutoring Sessions for Second Language Writers. Journal of Second Language Writing. 15(1), 1-23.
Karat, C.M., Halverson, C., Horn, D., & Karat, J. (1999). Patterns of entry and correction in large vocabulary
continuous speech recognition systems, CHI 99 Conference Proceedings (pp. 568–575).

Lebie, L., Rhoades, J. A., & McGrath, J. E. (1995). Interaction process in computer-mediated and face-to-face
groups. Computer Supported Cooperative Work, 4(2-3), 127-152.
Lepper, M. R., Woolverton, M., Mumme, D. L., & Gurtner, J.-L. (1993). Motivational techniques of expert
human tutors: Lessons for the design of computer-based tutors. In S. P. Lajoie & S. J. Derry (Eds.)
Computers as Cognitive Tools (pp. 75-105). Hillsdale, NJ: Erlbaum.
Litman, D. J., Rosé, C. P., Forbes-Riley, K., VanLehn, K., Bhembe, D., & Silliman, S. (2006). Spoken versus
typed human and computer dialogue tutoring. International Journal of Artificial Intelligence in Education,
16(2), 145-170
Luppicini, R. (2007). Review of computer mediated communication research for education. Instructional
Science, 35, 141-185.
Marttunen, M., & Laurinen, L. (2001). Learning of argumentation skills in networked and face-to-face
environments. Instructional Science, 29, 127-153.
Moore, J. D., Porayska-Pomsta, K., Varges, S., & Zinn, C. (2004). Generating tutorial feedback with affect.
Proceedings of the 17th International Florida Artificial Intelligence Research Society Conference (pp. 923928). Miami Beach, Florida.
Moreno, R., Mayer, R. E., Spires, H. A., & Lester, J. C. (2001). The case for social agency in computer-based
teaching: Do students learn more deeply when they interact with animated agents? Cognition and
Instruction, 19(2), 177-213.
Neuhauser, C. (2002). Learning style and effectiveness of online and face-to-face instruction. American Journal
of Distance Education. 16(2), 99-113.
Neville, M. H., & Pugh, A. K. (1974). Context in reading and listening: A comparison of children’s errors in
cloze tests. British Journal of Educational Psychology, 44(3), 224-232.
Ocker, R. J., & Yaverbaum, G. J. (1999). Asynchronous computer-mediated communication versus face-to-face
collaboration: Results of student learning, quality, and satisfaction. Group Decision and Negotiation, 8,
427-440.
Olaniran, B. A. (1996). A model of group satisfaction in computer-mediated communication and face-to-face
meetings. Behavior and Information Technology, 15(1), 24-36
Pilkington, R. M., & Walker, S. A. (2003). Facilitating debate in networked learning: Reflecting on online
synchronous discussion in higher education. Instructional Science, 31, 41-63.
Randhawa, B. S., Beamer, J. E., & Lundberg, I. (1993). Role of mathematics self-efficacy in the structural
model of mathematics achievement. Journal of Educational Psychology, 85 (1), 41-8.
Rosé, C. P., Moore, J. D., VanLehn, K., & Allbritton, D. (2001). A comparative evaluation of Socratic versus
didactic tutoring. In J. D. Moore & K. Stenning (Eds.) Proceedings of the Twenty-Third Annual Conference
of the Cognitive Science Society (pp. 897-902). Mahwah, NJ: Erlbaum.
Rubin, D. L., Hafer, T., & Arata, K. (2000). Reading and listening to oral-based versus literate-based discourse.
Communication Education, 49(2), 121-133.
Schpilberg, B., & Hubschman, B. (2003). Face-to-face and computer mediated tutoring: A comparative
exploration on high school students’ math achievement. Presented at the American Educational Research
Association conference, Chicago, IL.
Siler, S. A., & VanLehn, K. (2003). Accuracy of tutors’ assessments of their students by tutoring context. In R.
Alterman & D. Hirsch, (Eds.) Proceedings of the 25th Annual Meeting of the Cognitive Science Society.
Hillsdale, NJ: Erlbaum.
Summers, J. J., Waigandt, A., & Whittaker, T. A. (2005). A comparison of student achievement and satisfaction
in an online versus a traditional face-to-face statistics class. Innovative Higher Education, 29(3), 233-250.
Tu, H. (2000). On-line learning migration: from social learning theory to social presence theory in a CMC
environment. Journal of Network and Computer Application, 23, 27-37.
Tutty, J. I., & Klein, J. D. (2008). Computer-mediated instruction: A comparison of online and face-to-face
collaboration. Educational Technology Research and Development, 56(2), 101-124.
van der Meijden, H., & Veenman, S. (2005). Face-to-face versus computer-mediated communication in a
primary school setting. Computers in Human Behavior, 21(5), 831-859.

Vollmeyer, R., & Rheinberg, F. (2000). Does motivation affect performance via persistence? Learning and
Instruction, 10(4), 293-309.
Wald, M., & Bain, K. (2008). Universal access to communication and learning: The role of automatic speech
recognition. Universal Access in the Information Society, 6(4), 435-447.
Warren, L. L., & Holloman, H. L. Jr. (2005). On-line Instruction: Are the outcomes the same? Journal of
Instructional Psychology, 32(2), 148-151.
Wigfield, A., & Guthrie, J. T. (1997). Relations of children’s motivation for reading to the amount and breadth
of their reading. Journal of Educational Psychology, 89, 420-433.
Zimmerman, B. J., Bandura, A., & Martinez-Pons, M. (1992). Self-motivation for academic attainment: The role
of self-efficacy beliefs and personal goal setting. American Educational Research Journal, 29(3), 663-676.

From Behavioral Description to
A Pattern-Based Model for Intelligent Tutoring Systems
Javier Gonzalez-Sanchez, Maria Elena Chavez-Echeagaray,
Kurt VanLehn, Winslow Burleson
School of Computing, Informatics, and Decision Systems Engineering
Arizona State University Tempe, Arizona, US
University Drive and Mill Avenue, Tempe, AZ 85287
+1 (480) 965-9253
{javiergs, helenchavez, kurt.vanlehn, winslow.burleson}@asu.edu

Abstract. Intelligent Tutoring Systems are capable of becoming an alternative to
expert human tutors, able to provide direct customized instruction and feedback to
students. Although Intelligent Tutoring Systems could differ widely in their attached
knowledge bases and user interfaces (including interaction mechanisms), their
behaviors are quite similar; thus, it must be possible to establish a common software
model for them. A common software model is a step forward to move these systems
from proof-of-concepts and academic research tools to widely available tools in
schools and homes. The work reported here addresses: (1) the use of Design
Patterns to create an object-oriented software model for Intelligent Tutoring
Systems; (2) the application of the model into a two-year development project; (3)
the qualities achieved and trade-offs made. Besides that, this paper describes our
experience using patterns, and the impact in facts such as creating a common
language among stakeholders, supporting an incremental development and
adjustment to a highly shifting developing team.
Keywords: Design Patterns, Intelligent Tutoring Systems, Behavioral Description,
Object Oriented, Model.

1

Introduction

The use of Intelligent Tutoring Systems (ITS) is becoming more common and there is a
lot of work about their pedagogical and instructional design [4][16][2][21] but not about
their technological implementation. This paper describes our approach to address the
technological implementation of ITS within a context driven by three key elements:
a)

Incremental requirements. This was a two-year project with incremental software
requirements to implement the ITS in a components-oriented approach to support the
integration of meta-tutoring and affective learning companions.

b) Changing requirements. It was required to test several and diverse research
approaches, as part of the project implementation, creating for each approach a solid
system able to be delivered in students’ computers.

c)

Changing Team. The programming team composed of undergraduate students
shifted constantly (every 4 to 6 months).

In this context we made our choice to use Design Patterns to standardize a model for
ITS functionality that drives the way in which software was developed. Design Patterns
provided us with a common vocabulary, and help us to reduce system complexity by
naming and defining abstractions for building reusable components from which more
complex components were built [9].
We took as specification for ITS functionality the behavioral description given in [17],
that claims that few pedagogical features have been invented, and that the different
Intelligent Tutoring Systems, developed before now, offer different combinations of those
features. The behavior of ITS described in [17] was produced from the analysis of
different Intelligent Tutoring Systems currently available.
We mapped the functional description of ITS behavior described in [17] into a software
model using the “Gang of Four” (GoF) Design Patterns [8], and we proposed this model
as a software model for new Intelligent Tutoring Systems implementation. Using GoF
Design Patterns we addressed the creation of the model and look forward to incorporate
non-functional elements (i.e. software quality factors) particularly reusability,
extensibility, and flexibility [11]. These qualities help us to address the contextual
elements mentioned above: incremental requirements, changing requirements and
changing team.
With this approach we are attempting to make our contribution to move ITS
construction from software development as a one-of-a-kind endeavor to software
development as a system of components that are widely used and highly adaptable [12].
The work described here is part of the development of an ITS, named Affective Meta
Tutor (AMT) [1]. AMT project, funded by the National Science Foundation, is about
including in an ITS, meta-tutoring strategies and affective learning companions
technology. AMT project looks to improve ITS not only by adding new elements, but by
taking advantage of previous experiences of Intelligent Tutoring Systems implementations
from the ITS community. Most of our knowledge of those previous experiences is taken
from the analysis and comparison made of existing Intelligent Tutoring Systems described
in [17].
This paper is organized as follows: section 2 provides some terminology and
background about ITS and patterns; section 3 explores ITS functional specification and
the design process using patterns to model ITS software components; section 4 describes
our experience using our pattern-based model into the AMT project and evaluates pros
and cons; finally, section 5 concludes the paper and describes ongoing work.

2

Background

This section provides background about ITS structure and clarifies some related
terminology used within this paper. It also provides background information about Design
Patterns and the definition of the software qualities expected for the proposed model.
2.1

ITS Structure

ITS refers to a computer system that acts as a tutor showing an intelligent way to provide
feedback and hints to support student achievement. ITS structure can be represented as a
three-tier model, as shown in Figure 1, that decouples from the ITS Core, the Knowledge
Base and User Interface.
a)

Knowledge Base (KB) includes data structures and databases responsible for putting
into the computer system the information instructed by the ITS. The process of
putting data in KB is called “authoring”. Authoring involves a human expert
interacting with an authoring tool to provide this knowledge. Occasionally machinelearning algorithms has been used to create this expertise. Authoring and Knowledge
representation are topics outside of this paper.

b) User Interfaces (UI) includes graphical interfaces (windows, buttons, text and so on)
and interaction mechanisms (from simple keyboard events to more complex
interfaces, such as motion capture, voice recognition, brain-computer interface and so
on).
c)

Core implements ITS behavior. While Knowledge Bases and User Interfaces are
highly different from one ITS to others, the behavior of all of them is quite similar
and the next components can be identified: (1) Task Selector provides a Task
(problem or activity) the student must solve; (2) a Tool or Environment presents the
information that the student must know to complete the activity; (3) Step Analyzer
methodically examines and measures the performance of the student and provides
that information to the Assessor and the Pedagogical module; (4) Pedagogical
Module provides support (hints and feedback) to make the student successfully
complete the task; (5) Assessor learns from the student (how many hints he needed,
how skilled was in the topic, how much time he used to go from one step to another
in order to solve the task, etc.) and then stores this information in what is called a
Learner Model.

As examples of Intelligent Tutoring Systems we have Algebra Cognitive Tutor (an ITS
for Algebra in High School) [2], Andes (a tutor for Physics in College) [18], AutoTutor
(also an ITS for physics in College) [10], Sherlock (a simulator of avionic electronic
equipment) [13], and SQL-Tutor (an ITS to teach SQL language) [15]. They were
analyzed and compared in [17] where it is stated that their behaviors (and in consequence
their Cores) are similar but they differ widely in their software implementation.

From a software engineering perspective, this shows a lack of the use of software
engineering techniques and methodologies in the development of this kind of systems,
because the same specifications are creating different products. Subsequently, it will be
valuable to establish a model to go from the ITS behavior description to the system
implementation. An optimal model should be capable of satisfying the requirements of
these Intelligent Tutoring Systems and providing desired software qualities. The rest of
this paper is related to modeling the ITS Core, the layer that implements the behavioral
response of the ITS. Modeling Knowledge Bases and User Interfaces is out of the scope
of this paper.

Fig. 1. ITS structure: User interface (and interaction mechanisms), Functionality (Core) and Data
(Knowledge base) are decoupled.

2.2

ITS Terminology

For terms related with the ITS structure, mentioned before and used in the rest of this
paper, the following list states their meaning:
a)

Task refers to a multi-minute activity assigned to the student by the ITS. Tasks can
be skipped or interchanged with other tasks.

b) Step is each of the actions taken to achieve a Task. Each Task consists of multiple
Steps and each Step involves events with the user interface (either through a tool or
an environment).
c)

Knowledge Components are fragment of persistent, domain-specific information
that should be used to accomplish a Task. Knowledge Components are contained in
the Knowledge Base.

d) Outer Loop is the generic name given in [17] to the ITS process made by the Task
Selector. Task Selector creates and chooses Tasks to be accomplished by the Student
to become skilled in a particular Knowledge Component.
e)

Inner Loop refers to the name given in [17] to the ITS process made by the Step
Analyzer (which deals with the Steps of the chosen Task). This involves the
Pedagogical Model, which provides Help (hints and feedback), and the Assessor that
assesses the student performance and creates the Learner Model.

This terminology refers to a behavioral description, so the word “loop” must not be
interpreted as a programming loop. Outer Loop and Inner Loop are names for two
important processes accomplished by the ITS.
2.3

Why Patterns?

Software Design Patterns are used as a general reusable solution to a commonly occurring
problem in software design, to show relationships and interactions between components
and provide a skeleton for the implementation [8]. Even though, the concept of patterns
has received little attention so far from researchers in the field of ITS, in [7] they mention
that many Intelligent Tutoring Systems designers and developers use their own solutions
when faced with design problems that are common to different systems, models, and
paradigms; even when a closer look into that solutions and their comparison often shows
that different solutions and the contexts in which they are applied have much in common.
In that context, our choice about using Design Patterns into this project was driven by our
interest in:
a)

Communication. Patterns provide us with the description of the topology of the
system and the structural hierarchy of the subsystems and their interfaces and
connections. Patterns are more abstract than just a technical model, but more
technical than a conceptual model.

b) Collaboration. Patterns support the sharing of constructions between developers or
either use other’s constructions to enhance our own. No matter what is been built or
what others built, it is always known which are going to be the relations
(connections) among different constructions.
c)

Creativity. Patterns help to create components, and components support the creation
of families of products and/or several versions of the same product to prototype and
test new options of functionality.

d) Abstraction. Using patterns it is possible to provide a “controlled” freedom to the
programmers. They can develop functionality in their own creative way, but they
follow and preserve the guidelines of a defined design.
These benefits of using patterns (communication, collaboration, creativity and abstraction)
help us to overcome the challenging contextual elements of the project (incremental
requirements, changing requirements and a changing team).
2.4

ITS Qualities

One additional reason to use Design Patterns is related to Quality. Software quality
criteria are specified as non-functional requirements. Patterns let us take advantage of
previous experiences to implement non-functional requirements and to avoid accidental
complexity. Modeling ITS behavior is also about accomplish important non-functional
considerations that drive its design. Non-functional requirements addressed in this paper
are:
a)

Reusability refers to the degree to which a software module or other work product
can be used in more than one computer program or software system [11]. ITS
components must be able to be used again with slight or no modification, for the
implementation of other products or versions of the same project.

b) Extensibility is the degree to which a system or component can be easily modified to
increase its storage or functional capacity [11]. ITS components in the model must be
able to incorporate new functionalities or modify existing functionalities. By example
assessment strategies, task-creation strategies, learning algorithms to mining learner
model, etc.
c)

Flexibility or adaptability. The ease with which a system or component can be
modified for use in applications or environments other than those for which it was
specifically designed [11].

d) Robustness is the degree to which a system or component can function correctly in the
presence of invalid inputs or stressful environmental conditions [11]. Students expect
to get effective and efficient support from the ITS, as if it was a human tutor;
interruptions in the teaching-learning process due to software failures are highly
undesirable.
e)

Performance refers to the degree to which a system or component accomplishes its
designated functions within given constraints, such as speed, accuracy, or memory
usage [11]. The ITS must emulate real-time responses from a human tutor; delays
must be avoided and latency reduced.

The use of patterns becomes the keystone to satisfy the first three qualities enumerated
above. Satisfaction of both performance and robustness requirements are related to the
implementation of the model and not with the model per se. However, in our experience

communication, collaboration, creativity and abstraction impact performance and
robustness.

3

Modeling the ITS Behavior

This section uses the ITS behavior described in [17] to create a conceptual model for ITS
Core layer. The ITS behavior stated in [17] is summarized with a list of statements which
identifies the involved components, responsibilities for each component, and relationships
between components. In the list, components’ names were marked in bold and
relationships between components were explained. Complex components were split into
simple ones, identifying specific responsibilities and assigning them to new components.
The list of statements is as follows:
a)

Tool is the component that recreates an environment for the Student to work. Tool
handles the events fired by User Interfaces.

b) The ITS behaviors start in the Outer Loop.
c)

During the Outer Loop Task Selector place a Task into the Tool in order to be
solved by the Student.

d) Task Selector main responsibility is selecting the next Task that the student must
solve. The selection is done in an “intelligent way”; four basic methods to do
“selection” are described in [17].
e)

Task Selector needs to have access to a source of Tasks. We defined Task Factory
as that source of Tasks.

f)

Task Factory creates Tasks. Creating a Task means reading Tasks stored in a
repository (read previously human-authored Tasks) or creating Tasks in real-time.

g) Task Selector relies on Learner Model to choose a Task.
h) Learner Model is maintained by the Assessor component.
i)

Inner Loop is nested inside the Outer Loop. The Inner Loop works with the Steps
that conforms the Task. In the Inner Loop participate the Step Analyzer, the
Assessor and/or the Pedagogical Model.

j)

Step Analyzer assesses the Student performance while collecting and processing
data about the student's learning process.

k) Assessor looks at the information generated by Step Analyzer and store it in the
Learner Model. The accuracy of the diagnostic algorithms of this component is a
key factor for the adaptation process.

l)

Pedagogical Module provides Help using different strategies such as providing
immediate or delayed help, or providing requested or unsolicited help.

m) Steps include Assessment and Help.
n) Help could be Hints before completing the Step, or Feedback after completing the
Step.
o) Task is a set of Steps.
p) Task is related to a set of Knowledge Components.
q) Learner Model is a set composed of Tasks, measures of the time spent to complete
the Task and the status of the Task. For each Step in the Task a counter of the Hints
requested and Feedback (errors made) is kept, and for each Knowledge Component
a mastering measure is also kept for each Student.
r)

Knowledge Components are the information and skills being taught.

s)

Knowledge Base is the set of Knowledge Components in the ITS.

Figure 2 extends ITS structure showed in Figure 1 in order to identify components
(functional and data objects) and their relationships. UML notation is used, inside ITS
Core block, to create a first attempt of the object-oriented model. Figure 2 shows:
a)

Each component is represented as a box: white boxes are functional components and
gray boxes represent data components.

b) Association relationships are shown using arrows; the arrows go from the component
that requests a functionality to the component that provides that functionality. Some
examples are: Tool sends information about events in the User Interface to Step
Analyzer; Task Selector uses Tool to present a new Task; Task selector uses Task
Factory to obtain Tasks.
c)

Dependency relationships are represented using arrows with dashed lines; in the
model we are showing the dependency between functional components and the data
component they require to access them.

d) In data components (gray boxes) inheritance relationships (arrows with a triangular
shape in the arrow point) show a specialization hierarchy; in our case this relationship
shows that Help could be either a Hint or Feedback.
e)

Finally, composition relationships are shown with arrows starting with a diamond
shape. Specifically, they are used to represent Task as a composition of Steps, and
Knowledge Base as a composition of Knowledge Components.

The next section explains how this conceptual model became an object-oriented model,
using a pattern-based approach.

Fig. 2. ITS Conceptual model. White boxes show functional components and gray boxes show data
components. Relationships of association, dependency, composition and inheritance are showed
using UML notation.

3.1

Pattern-Based Modeling

The previous model showed in Figure 2 and extracted from the behavior described in
section 3 represents a conceptual description of who is doing what, and corresponds to an
abstraction of the expected functionality of each internal component in ITS Core. Even
though the conceptual model can be a starting point to implement ITS functionality, it is
still too abstract to be a software design and therefore there are a lot of diverse options to
implement it. The next step in our process was to evolve this model by defining more
specific relationships between components, using Design Patterns; that provided us with a
template for the software design and thus for the implementation.

Finding the appropriate pattern to be applied for each component and relationship was
a process based on experience and literature research [6][8]. There is not a rule about how
to choose a pattern, it is required to know the existent patterns (the problems they solve)
and then use them to describe in an effective way what is happening in the system. Our
approach consists of using the pattern that most closely matches the semantic description
of the requirement or group of requirements. From the “GoF” Design Patterns
documented in [6] and [8] we took the keywords: observer, abstract factory, builder,
singleton, chain of responsibilities, strategy, communicator, facade, composite and
singleton; each pattern are fairly close to implement what their name means and what our
components are supposed to do. For example, Task Factory naturally means to be an
ABSTRACT FACTORY of Tasks. Table 1 shows the relationship between components
previously defined matched to a pattern name with a description of the meaning of the
relationship
Table 1. Relationships between ITS components and Design Patterns

Components

Pattern

Description

Tool

FACADE

Tool is a high-level interface for a set of
subsystems.

TaskSelector

STRATEGY

TaskSelector component is implemented
using STRATEGY pattern to lead with the fact
that selecting the next Task for the Student is
done
with
different
algorithms
(methodologies), described in [11].

TaskSelector

OBSERVER

The relationship between TaskSelector and
Assessor can be described by OBSERVER
pattern. TaskSelector needs information
about changes in the Learner Model
(performance of the student) maintained by
Assessor component, in order to adjust the
level of the next Task.

TaskFactory

ABSTRACT FACTORY

TaskFactory creates Task objects. The
relationship between TaskFactory and Task
corresponds to the relationship between a
factory and a product in ABSTRACT
FACTORY pattern.

TaskFactory

STRATEGY

TaskFactory implements STRATEGY to
create Tasks, due to the fact that ITS could
implement either particular algorithms to
create Tasks in real-time, or create Tasks
recovering them from a data repository.

and
Assessor

CHAIN OF
RESPONSIBILITIES

CHAIN OF RESPONSIBILITIES is a design

Assessor

STRATEGY

Assessor implements STRATEGY to maintain
the Learner Model. Diverse strategies could
be tried to store and recover the Learner
Model information.

Pedagogical
Module

STRATEGY

Pedagogical Module implements STRATEGY
to provide support to the student in solving
the current Step. Options to provide Help go
from pressing a button asking for a Hint, to
the implementation of intelligent algorithms
that provide support to maintain the student
in the "zone of proximal development" [20]
where tasks are neither boringly easy nor
frustratingly difficult, but instead afford
maximal learning and motivating challenges.

Step

COMPOSITE

Step uses COMPOSITE pattern to compose
Steps into tree structures to represent partwhole hierarchies. COMPOSITE lets us treat
individual Steps and hierarchies of Steps
(and sub-Steps) uniformly.

Knowledge

SINGLETON

Making KnowledgeBase a SINGLETON
ensures only one instance of it and provides
a global access point to it.

OBSERVER

The relationship between Assessor and
StepAnalyzer can be described by
OBSERVER pattern. Assessor needs
information about student performance in
each Step. That information is obtained from
StepAnalyzer.

StepAnalyzer

Base
Assessor
and
StepAnalyzer

pattern that avoids coupling the sender of a
request to its receiver, by giving more than
one object a chance to handle the request.
StepAnalyzer chains the receiving objects
and passes the request along the chain until
one handles it. Since Step Analyzer works
with Steps, and Steps are close related with
user events, modeling Step Analyzer as a
chain gives us the opportunity to add and
remove behavior associated with specific
events quickly.

Pedagogical
Module

OBSERVER

and
StepAnalyzer

3.2

The relationship between Pedagogical
Module and Step Analyzer can be described
by OBSERVER pattern. Pedagogical Module
needs information about student
performance in each Step. That information
is obtained from StepAnalyzer.

Putting All Patterns Together

With the relationships expressed as pattern equivalences, as listed in the previous
section, creating a software design is fairly straightforward. Each pattern has a unique
equivalence in UML (as a class diagram). Converting the UML class diagram into code
files could even be an automatic process done by a development tool. Then, our
programming team would be able to focus on the detailed implementation of the desired
functionality, filling in specific places inside specific files, methods and attributes [5].
The UML class diagram for the ITS Core layer is shown in Figure 3. It is important to
note the following relationships in the diagram:
1.

TaskSelector, TaskFactory, Assessor, and PedagogicalModule are implementing the
STRATEGY pattern that permit us to define algorithms, encapsulate them, and make
them interchangeable. STRATEGY pattern lets the algorithm vary independent of the
classes that use it. Implementing a new way to select a Task, create a Task, manage
the Learner Model or provide Help to the student can be done for one developer who
needs no knowledge about the project at all; the developer just needs to follow the
pattern to: (1) create a new class that implements the corresponding interface; (2)
implement at least the algorithmInterface() method; and (3) create as many additional
methods and/or attributes as needed.
At AMT project: for TaskSelector only a SequencialTaskSelection strategy was
defined; for TaskFactory, a TaskFromRepository strategy was implemented. No
strategy was implemented for Assessor (this module is still an ongoing part of the
project); for PedagogicalModule a ConditionalPedagogicalStrategy strategy was
implemented in which conditionally the presence of certain events or actions from the
Student launches pre-established responses of PedagogicalModule.
In addition to this, a first version of a MetaTutor System was implemented and linked
with a MetaTutorPedagogicalStrategy class. The MetaTutorPedagogicalStrategy class
acts as an ADVOCATE sending and receiving information from and to the external
system (MetaTutor System). Those implementations are not shown in the diagram
because of space limitations.

2.

TaskSelector obtains information from Assessor, which as well as
PedagogicalModule obtains information from StepAnalyzer. The concept of
“observing” describes the relationship and clearly identifies how the structure of

communication must be implemented (methods and attributes). It is easy to notice
which component needs information from which other component.
3.

The implementation of Step as a COMPOSITION is highly useful, it provides the
capability of managing Steps as one or as a hierarchy of several hierarchized Steps.

Fig. 3. UML class diagram showing the pattern-based model for the ITS Core layer.

For AMT project Tool component is an environment in which Student is able to learn
about systems dynamic modeling, using graphical representation. Each model is a directed
graph formed by nodes and edges. The edges indicate flow of numeric information
between nodes and the nodes represent variables. A node encapsulates a variable’s value
as an algebraic combination of the numbers coming into or going out of it via edges.
Students read text describing the system, and then define nodes and edges, enter value or
equations in each node, run the model and compare its predictions to given facts. If any of
the model’s predictions are false (represented with red colors as feedback), students must
debug the model. Students also can ask for feedback by checking their model at each step
before running the model. [19].
Figure 4 shows the current implementation of AMT Tool component. Tool component
consists on a Canvas in which a Graph is drawn. A Graph is composed by Nodes (Vertex)
and Links (Edges) that connect the Nodes. Each Vertex maintains a register of all vertexes
going out and in. Each Edge maintains data of the Vertex in which it starts and ends.

Fig. 4. UML class diagram showing the Tool implemented in AMT project, encapsulated in the
model as a Tool by FACADE pattern.

Vertex, Edges and Graph can be selected from the Canvas and manipulated (drag and
drop, deleted, and so on). The Tool in execution is showed in Figure 5. The figure shows
the solution for a problem about Merchant marine that states:
After World War II, the United States had the largest merchant marine of any nation. The merchant marine
are the ships that transport goods and people over the oceans, not counting navy vessels. Unfortunately, the US
merchant marine has been getting smaller and smaller each year, while the merchant marine of other countries

has grown. Just for illustration, suppose that in 1950 the US merchant marine was 5,000 ships of 10,000 tons or
larger, where Panama's merchant marine was only 1,000 ships of 10,000 tons or larger. Suppose that the US
Merchant marine shrank by 5% each year, mostly because ships were sold to other countries. Suppose the
Panama's merchant marine grew by 100 ships a year, mostly because it was cheap to own a ship
registered in Panama. Graph the difference in size between the two fleets over 50 years. That is, the difference
starts out with the US having 4000 more ships than the Panama in 1950. What happens on the way to 2000?

The complete model built by a student that shows the difference in size between the
two fleets (Panama's and US's) described in the text above. This difference is calculated as
a function of the number of ships on each fleet, which also depend on the number or ships
added or removed annually. It is possible to observe that the student obtained all the
names (labels) for the nodes from the tool (observe the yellow color), however he
correctly defined the type for each node, almost all their inputs (i), as well as almost all
the equations or values (c) for each node (see green indicators). Observe that the student
did not check if the equation defined for the node that represents the difference between
the fleets was correct or not, this can be told due to the fact the (c) indicator is white.
When the student checked (run) his model, he obtained the feedback for the graphics for
each node (g), as it is shown the student have a problem in the node that represents the
difference on the fleets (red indicator).

(b)
Fig. 5. Look and feel of the AMT Tool. GraphCanvas is a UI component where the model is
displayed and could be manipulated. Model is composed by Vertex (geometrical shapes) and Edges
(arrows). The colors represent the Feedback from the student about the status of each Step.

4

Experience Report and Evaluation

Including structure and functionality, the current system, developed in Java, is formed by:
10 packages; 62 classes; 746 methods; 738 attributes; 22,434 lines of code; 1,150
revisions maintained in a revision control system (SVN) created between July 2009 and
July 2011 for a shifting team of nine programmers (maintaining a team of two

programmers at a time, with an average of six months of permanency) and two resident
software engineers; 8 versions released to clients; and 140 users working with the system,
who have been high school students and undergraduate students participating in four
summer camp courses and two university courses at Arizona State University.
Our experience using Design Patterns to create an ITS model and implement it to
create AMT software project can be summarized as follows. Stakeholders in general
mentioned as points in favor:
a)

Incremental development fully supported. Our goal to build the AMT project in an
incremental way, during two years has allowed us to: (1) provide in a window time of
two or three weeks a new product or a new version of the product; (2) achieve time
reduction to deployment with more programmers, in specific moments of the project,
when time is related with the implementation of new functionality.

b) Changing programming team almost twice a year. (1) Programmers, even without
knowledge of patterns, are able to focus their attention in the requirements assigned
to them; we used Subversion [3] to maintain a common repository of the project, each
programmer works in completing a specific module or set of components (defined as
a pattern section); (2) relationships between components are almost fully defined by
patterns connections, so the work done by each programmer is delimited and merging
the work from different programmers is a straightforward process.
Vocabulary. The use of pattern names such as FACTORY and STRATEGY has been
adopted as an abstract way to refer functionalities between stakeholders. The names
hide complexity from non-developers. Non-developers assume an easy thing must be
done, and programmers have a better idea about the boundaries of changes, bugs and
new requirements.

c)

However some dispute did emerge regarding:
a)

Size. It is arguable, but some people point to the increment of the size of the code
while using patterns. Yes! It could be true, using patterns generates more code, but it
is not only due to patterns (interfaces and abstract classes declarations), but also
because we decide to maintain the cyclomatic complexity (McCabe number) [14] for
every method under 10, which means an applied “divide and conquer” strategy, and
that generates more methods in the system.

b) Time delays. Unlikely others approaches our first step, even before showing a
prototype to the research group, was focused on the architecture definition (patterns).
Thus, software prototypes delayed its appearance into the scene; but once the first
prototype was presented, new prototypes emerged quicker that in previous projects.
4.1

Impacts of Design Patterns

It is important to specifically highlight how the use of Design Patterns impacted the
project:

a)

Communication. Since diverse stakeholders such as researchers in education
technology, computer scientist, developers and instructional designers were involved,
Design Patterns helped us to agree in the structure of the system and communicate it
to the programmers for each individual component in the project.

b) Collaboration. Sharing constructions between developers was a key element to
counterbalance the effect of a constant developers shifting.
c)

Creativity. Creativity was encouraged, enhanced and achieved by allowing the
creation of several versions of the project to prototype and test new options of
functionality and in consequence, in our project, new pedagogical approaches.

d) Abstraction. Providing a “controlled” freedom to the programmers using patterns as
the guidelines of a defined design was highly relevant to handle changing and
incremental requirements.

5

Conclusions and Ongoing Work

Many authors claim that their ITS follow and accomplish a software architecture because
they can identify components and relationships among those components inside their
systems. However, this does not mean that standard and good practices, such as Design
Patterns, have been followed.
We take advantage of the growing experience in the field of software Design Patterns
to both design and implement an ITS model in a pattern-based approach. Applying Design
Patterns was useful to create a high-quality software solution that is easy to maintain and
extend.
Designing with quality attributes as drivers, has resulted in a design that has proven to
be more modifiable, reusable and reliable. Using Design Patterns impacts our
communication, collaboration, and creativity. Design Patterns facilitate the adjustment of
a highly shifting programming team and thus the development of the system, where the
creation of new versions or variants of the software was relatively easy in terms of time
and effort. Adding Design Pattern in the development of ITS allowed us to create a
common vocabulary among stakeholders making the process more accurate and effective
design-wise.
We applied our model to build several variants of AMT system in two years of work,
with a high rate of changes in requirements for the product and a changing programming
team. In other words, we have been able to create a family of AMTs around the same
design.
Future research will focus on two additions: (1) the first one will be the inclusion of a
model for companions to provide support for the student, these are learning companions,

affective companions and teachable agents; (2) the second one will be the inclusion of
Meta-Tutoring components.

Acknowledgments
We are grateful to Hironori Washizaki for his support during the writing process of this
paper.
This research was funded by the National Science Foundation, including the following
grants: (1) IIS/HCC Affective Learning Companions: Modeling and supporting emotion
during learning (#0705883); (2) Deeper Modeling via Affective Meta-tutoring (DRL0910221) and (3) Pittsburgh Science of Learning Center (SBE-0836012).

References
[1] Affective Meta Tutor – Arizona State University. http://amt.asu.edu.
[2] Anderson, J. R., Corbett, A. T., Koedinger, K. R., & Pelletier, R. 1995. Cognitive

Tutors: Lessons Learned. Journal of the Learning Sciences, 4(2), 167-207.
[3] Collins-Sussman, B., Fitzpatrick, B. W., and Pilato, C. M. 2004 Version control with

subversion. O’Reilly Media, Inc.
[4] Baker, R. S. J. D., de Carvalho, A., Raspat, J., Aleven, V., Corbett, A. T., and

Koedinger, K. R. 2009. Educational software features that encourages and
discourage “gaming the system”. Proceedings of the International Conference on
Artificial Intelligence in Education. IOS Press.
[5] Booch, G., Maksimchuk, R., Engle, M., Young, B., Conallen, J., and Houston, K.

2007. Object-Oriented Analysis and Design with Applications, Third Edition.
Addison-Wesley Professional.
[6] Buschmann, F., Meunier, R., Rohnert, H., Sommerlad, P., and Stal, M. 1996. A

system of patterns: Pattern-oriented software architecture. Wiley.
[7] Devedzic, V. and Harrer, A. 2005. Software Patterns in ITS Architectures.

International Journal of Artificial Intelligence in Education, 15, 2 (April 2005), 6394.
[8] Gamma, E., Helm, R., Johnson, R., and Vlissides, J. 1995. Design Patterns: Elements

of Reusable Object-Oriented Software. Addison-Wesley Longman Publishing Co.,
Inc., Boston, MA, USA.
[9] Gamma, E., Helm, R., Johnson, R., and Vlissides, J. 2002. Design Patterns:

abstraction and reuse of object-oriented design. In Software pioneers. Manfred Broy
and Ernst Denert (Eds.). Springer-Verlag New York, Inc., New York, NY, USA 701717.
[10] Graesser, A. C., Lu, S., Jackson, G. T., Mitchell, H. H., Ventura, M., Olney, A., et al.

2004. AutoTutor: A tutor with dialogue in natural language. Behavioral Research
Methods, Instruments and Computers, 36, 180-193.

[11] IEEE. 1999. Standard Glossary of Software Engineering Terminology. 610.12-1990,

Vol.1. IEEE Press.
[12] Jacobson, I.: 1997. Software Reuse: Architecture, Process and Organization for

Business Success. Addison-Wesley Professional
[13] Katz, S., Connelly, J., & Allbritton, D. (2003). Going beyond the problem given:

How human tutors use post- solution discussions to support transfer. International
Journal of Artificial Intelligence in Education, 13,79-116.
[14] McCabe, T. 1976. A complexity measure. IEEE Trans. Software Engineering, 5, 45–

50.
[15] Mitrovic, A. 2003. An intelligent SQL tutor on the web. International Journal of

Artificial Intelligence in Education, 13(2-4), 197-243.
[16] Nelson, B. C. 2007. Exploring the use of individualized, reflective guidance in an

educational multi-user virtual environment. In Journal of Science Education and
Technology, 16(1), 83-97.
[17] VanLehn, K. 2006. The Behavior of Tutoring Systems. International Journal of

Artificial Intelligence in Education. Volume 16, Issue 3, Pages 227-265. IOS Press.
[18] VanLehn, K., Lynch, C., Schultz, K., Shapiro, J. A., Shelby, R. H., Taylor, L., et al.

2005. The Andes physics tutoring system: Lessons learned. International Journal of
Artificial Intelligence in Education, 15(3), 147-204.
[19] Van Lehn, K. et al. 2011. The Affective Meta-Tutoring Project: How to motivate

students to use effective meta-cognitive strategies. T. Hirashima et al. (Eds.)
Proceedings of the 19th International Conference on Computers in Education.
Chiang Mai, Thailand: Asia-Pacific Society for Computers in Education.
[20] Vygotsky, L. S. 1978. Mind in Society: The Development of Higher Psychological

Processes. Cambridge, MA: Harvard University Press.
[21] Wilson, B. G. 1997. Reflections on constructivism and instructional design.

Instructional development paradigms. C. R. Dills and A. A. Romiszowski (Eds.),
Englewood Cliffs NJ: Educational Technology Publications. 63-80.

Empirically Evaluating the Application of Reinforcement
Learning to the Induction of Effective and Adaptive
Pedagogical Strategies
Min Chi (minchi@cs.cmu.edu)
Machine Learning Department, Carnegie Mellon University, PA USA

Kurt VanLehn (kurt.vanlehn@asu.edu)
School of Computing, Informatics and Decision Science Engineering, Arizona
State University, AZ USA

Diane Litman (litman@cs.pitt.edu)
Department of Computer Science & Intelligent Systems Program & Learning
Research and Development Center, University of Pittsburgh, PA USA

Pamela Jordan (pjordan@pitt.edu)
Learning Research and Development Center, University of Pittsburgh, Pittsburgh,
PA USA
November, 2010
Abstract. For many forms of e-learning environments, the system’s behavior can be
viewed as a sequential decision process wherein, at each discrete step, the system is
responsible for selecting the next action to take. Pedagogical strategies are policies
to decide the next system action when there are multiple ones available. In this
project we present a Reinforcement Learning (RL) approach for inducing effective
pedagogical strategies and empirical evaluations of the induced strategies. This paper
addresses the technical challenges in applying RL to Cordillera, a Natural Language
Tutoring System teaching students introductory college physics. The algorithm chosen for this project is a model-based RL approach, Policy Iteration, and the training
corpus for the RL approach is an exploratory corpus, which was collected by letting
the system make random decisions when interacting with real students. Overall, our
results show that by using a rather small training corpus, the RL-induced strategies
indeed measurably improved the effectiveness of Cordillera in that the RL-induced
policies improved students’ learning gains significantly.
Keywords: reinforcement learning, pedagogical strategy, machine learning, human
learning

1. Introduction
For many forms of e-learning environments, the system’s behaviors can
be viewed as a sequential decision process wherein, at each discrete
step, the system is responsible for selecting the next action to take.
Pedagogical strategies are defined as policies to decide the next system
action when there are multiple ones available. Each of these system
c 2011 Kluwer Academic Publishers. Printed in the Netherlands.


USER658.tex; 26/01/2011; 15:41; p.1

2

Min Chi, Kurt VanLehn, Diane Litman, and Pamela Jordan

decisions affects the user’s successive actions and performance. Its impact on student learning cannot often be observed immediately and
the effectiveness of one decision also depends on the effectiveness of
subsequent decisions. Ideally, an effective learning environment should
craft and adapt its decisions to users’ needs [4, 52]. However, there is no
existing well-established theory on how to make these system decisions
effectively.
In this paper, we focus on one form of highly interactive e-learning
environment, Intelligent Tutoring systems (ITSs). Most existing ITSs
either employ fixed pedagogical policies providing little adaptability or
employ hand-coded pedagogical rules that seek to implement existing
cognitive or instructional theories [4, 41, 66]. These theories may or may
not have been well-evaluated. For example, in both the CTAT [4, 41]
and Andes systems [69], help is provided upon request because it is
assumed that students know when they need help and will only process
help when they desire it. Research on gaming the system, however,
has raised some doubts about this. It showed that students sometimes
exploit these mechanisms for shallow gains thus voiding the help value
[6, 5].
Previous researchers have largely treated the specification of pedagogical policies as a design problem: several versions of a system are
created, the only difference among them being the pedagogical policies
employed [47, 20, 56]. Data is then collected from human subjects interacting with each version of the system, and the students’ performance
is then statistically compared. Due to cost limitations, typically, only
a handful of alternative policies are explored. Yet, many such other
reasonable ones are still possible.
In recent years, work on the design of ITSs and Natural Language
(NL) dialogue systems has involved an increasing number of datadriven methodologies. Among these, Reinforcement Learning (RL) has
been widely applied [8, 30, 31, 32, 59, 46, 62, 72, 71]. In the project
reported here, we applied a form of RL, Policy Iteration, to improve the
effectiveness of an ITS by inducing pedagogical policies directly from
an exploratory corpus. The exploratory corpus was collected by letting
the ITS make random decisions when interacting with real students.
Compared with previous studies on applying RL to induce pedagogical policies on ITSs, this project makes at least three major contributions. First, we show that using a relatively small exploratory
training corpus is a feasible approach to induce effective pedagogical
policies. Second, we empirically show that the RL induced policies indeed improved students’ learning gains significantly. Third, while much
of the previous research on applying RL to ITSs and non-tutoring NL
dialogue systems used pre-defined state representation, our approach

USER658.tex; 26/01/2011; 15:41; p.2

Applying Reinforcement Learning To Pedagogical Strategies

3

in this project is to begin with a large set of features to which a series
of feature-selection methods were applied to reduce them to a tractable
subset. We will shed some light on the relative effectiveness of different
feature-selection methods and which features among the ones defined
were most involved in the final induced policies.
Overall, our results provide empirical evidence that, when properly
applied, RL can be applied to reach rather complicated and important
instructional goals such as learning and quantitatively and substantially improve the performance of a tutoring system. In this paper,
we will describe our detailed methodology for using RL to optimize the
pedagogical policies based on limited interactions with human students
and then present empirical results from validating the induced policies
on real students.
In the following, section 2 gives a brief overview of prior related
research on applying RL to either ITS or NL dialogue systems. Section
3 explains how RL can be used to optimize pedagogical policies with a
set of given features, section 4 explains our procedure on how a series
of feature-selection methods were applied to a large set of features in
this project in order to induce the “best” policy. Section 5 describes our
general approach and section 6 describes the general methods including
an introduction of the Cordillera system and procedures, while section
7 describes how Cordillera optimizes its decisions from experimentally
obtained dialogue data. Section 8 reports empirical results evaluating the performance of Cordillera’s induced pedagogical strategies and
demonstrates that the effectiveness of Cordillera was improved.

2. Background
Generally speaking, a RL model consists of three elements: 1. S =
{S1 , . . . , Sn } is a state space; 2. A = {A1 , . . . , Am } is an action space
represented by a set of action variables; 3. R = r(si , sj , ak ) denotes
a reward model that assigns rewards to state transitions. The goal of
RL is to find an optimal policy π ∗ that maps each state to the proper
action that would generate the maximum rewards.
The empirical procedure of applying RL to either ITS or non-tutoring
NL dialogue systems can be divided into two phases: a training phase
and a test phase. The training phase mainly involves defining an appropriate state representation S, a reasonable action space A, and an
appropriate reward function R, collecting a training corpus Γ, and
applying some RL algorithms to induce policies. In order to apply RL
to induce an effective policy, it is important for the system to explore
the relevant S of possible decision action sequences during the training

USER658.tex; 26/01/2011; 15:41; p.3

4

Min Chi, Kurt VanLehn, Diane Litman, and Pamela Jordan

phase. A common problem in RL is finding a balance between exploration (attempting to discover more about the world) and exploitation
(using what we already know about the world to get the best results
we can). On one hand, if the system does not explore enough, the RL
approach might not find an effective policy at all. On the other hand, if
the system explores too much, it cannot stick to a path; in fact, it is not
really learning as it cannot exploit its knowledge, and so acts as though
it knows nothing. It is often unclear how much exploration should be
done in order to induce an effective policy. The test phase generally
involves some sorts of evaluations to find out whether the RL-induced
policies indeed fulfill their promises.
There are two major categories of RL, model-free algorithms and
model-based algorithms [38]. Model-free algorithms learn a value function or policy directly from the experience while interacting with the
agent; at the end of learning, the agent knows how to act, but does
not explicitly know anything about the environment. Whereas modelbased algorithms first construct a model of the state transition and
outcome structure of the environment, and then evaluate actions by
searching this model. In other words, Model-based methods do explicitly learn state transitions. Generally speaking, model-free methods
are appropriate for domains where collecting data is not a big issue;
their corresponding algorithms include Monte Carlo methods and temporal difference methods. Model-based methods, on the other hand,
are appropriate for domains where collecting data is expensive; their
corresponding algorithms include dynamic programming such as Policy
Iteration and Value Iteration.
Previously both model-free and model-based RL algorithms were
applied to improve conventional ITSs. Next, we will briefly overview
related previous research.
2.1. Previous Research On Applying RL to ITSs
Beck et al. [8] investigated applying RL to induce pedagogical policies
that would minimize the time students take to complete each problem on AnimalWatch, an ITS that teaches arithmetic to grade school
students. In the training phase of their study, they used simulated
students. Given that the cost of collecting data with simulated students is relatively low, a model-free RL method, Temporal Difference
learning, was applied. In the test phase, the induced policies were added
to AnimalWatch and the new system was empirically compared with
the original version of AnimalWatch. Results showed that the policy
group spent significantly less time per problem than their no-policy
peers. Note that their pedagogical goal was to reduce the amount of

USER658.tex; 26/01/2011; 15:41; p.4

Applying Reinforcement Learning To Pedagogical Strategies

5

time per problem, however faster learning does not always result in
better learning performance. Nonetheless, their results showed that RL
can be successfully applied to induce pedagogical policies for ITSs.
Iglesias and her colleagues [30, 31, 32], on the other hand, focused on
applying RL to improve the effectiveness of an Intelligent Educational
System that teaches students DataBase Design. They applied another
model-free RL algorithm, Q-learning. The goal for the induced policy
was to provide students with direct navigation support through the
system’s content with the expectation that this would help students
learn more efficiently. In the training phase, similar to Beck et al.’s
approach, Iglesias and her colleagues used simulated students. In the
test phase, the induced policy was also empirically evaluated on real
students. Results showed that while the policy led to more effective
system usage behaviors from students, the policy students did not outperform the no-policy peers in terms of learning outcomes.
Martin and Arroyo [46] applied a model-based RL method, Policy Iteration, to induce pedagogical policies that would increase the
efficiency of hint sequencing on the Wayang Outpost web-based ITS.
During the training phase, the authors used a student model to generate
the training data for inducing the policies. Here the student model was
similar to the simulated students used in Beck et al and Iglesias et
al. In the test phase, the induced policies were tested on the student
model again and results showed that the induced policies increased its
predicted learning. However, since the policies were not tested with
human students, their effectiveness with real students is still an open
question.
Tetreault et al [62, 64] used an Intelligent Spoken Tutoring System,
ITSPOKE, which teaches students college physics [44]. In their work,
they used a previously collected corpus of physics tutorial dialogues as
a training corpus and investigated applying Policy Iteration to induce
pedagogical policies from it. The focus of their work was introducing a
novel method for evaluating state representations and thus the learning
gains of human students using the induced policy system were not
measured and compared to the prior system. Additionally, note that
because the training corpus used in this work was not collected with
the goal of exploring the full range of tutorial decisions, the tutor often
executed only one type of action in many dialogue states.
Although there have been other studies on application of RL to ITSs,
they mostly involved inducing domain models rather than pedagogical
policies. For example, Barnes and Stamper [7, 60] have applied RL to
construct problem solutions from existing students’ solutions for an ITS
called Proofs Tutorial, which teaches college-level discrete mathematics.
They used a form of the model-based RL method Value Iteration and

USER658.tex; 26/01/2011; 15:41; p.5

6

Min Chi, Kurt VanLehn, Diane Litman, and Pamela Jordan

the resulting problem solutions were then used to generate hints for new
students. They found that the extracted solutions and the proposed
hint-generating functions were able to provide hints over 80% of the
time for the new students.
In short, both model-free and model-based RL algorithms have been
applied to induce pedagogical policies in ITSs but only model-free RL
induced policies were empirically evaluated on real students [8, 30, 31,
32]. As far as we know, none of the previous research has empirically
shown that an RL induced policy significantly improved students’ learning performance. Given that improving learning is a primary goal for
any ITS, a better RL approach together with empirical evaluations is
needed and is attempted in this project.
Note that previous research on applying RL to induce pedagogical
policies either used pre-existing data that was collected for other purposes [62, 64] or simulated students or models [8, 30, 31, 32, 46, 1]
during the training phase. We argue that neither approach is optimal.
Using existing datasets is complicated by the fact that pre-existing
systems often explore a small part of the state-action space and thus
may yield biased or limited information. When using simulated students
or models, however, the effectiveness of the induced policies would
largely depend on the accuracy of the simulated students. Building
accurate simulated students is not easy and it is especially challenging
because human learning is a rather complex, poorly understood process.
For instance, one possible explanation for the difference between the
policy and no-policy groups that was found in [8] but not in Iglesias
et al’s work [30, 31, 32] may be that while it is possible to accurately
model time on task in [8], modeling how students would respond to
the system and simulating how students would learn is much more
challenging in [30, 31, 32].
On the other hand, previous research on applying RL to improve
dialogue systems employed an alternative approach that involved collecting an exploratory corpus from human subjects in the training
phase. Next, we will briefly describe how previous research in dialogue
systems tackled the issue.
2.2. Collecting An Exploratory Corpus
Dialogue Systems are a field of Computer Science that focuses on the
construction of computer systems that interact with human users via
natural-language dialogues. Much of the work in this area is focused on
systems that obtain information or search databases such as querying
bus schedules [54] and booking airline tickets [57]. In recent years, RL
has been widely applied to the design of dialogue systems [72, 71, 59].

USER658.tex; 26/01/2011; 15:41; p.6

Applying Reinforcement Learning To Pedagogical Strategies

7

In [59], Singh et al. explored avoiding biased training data by collecting an exploratory corpus. In their study, they used a dialogue system
called NJFun, a real-time spoken dialogue system that provides users
with information about things to do in New Jersey. In order to let
the system explore the relevant space of possible decision sequences,
they collected an exploratory corpus from a system that makes random system decisions as it spoke with human users, thus ensuring that
the transitions are adequately explored [59]. Then a model-based RL
method, Value Iteration [58], was applied to induce an effective dialogue policy from the collected exploratory training corpus. In the test
phase, they empirically evaluated the induced policy on real users and
showed that the dialogue policies significantly improve the likelihood
of task completion on NJFun. Similar approaches have been shown to
be successful in other studies [72, 71].
Therefore, in this project we use neither pre-existing system-user
interaction data nor simulated students and instead followed the approach of Singh et al [59]. More specifically, in the training phase, we
collected an exploratory corpus by training human students on an ITS
that makes random decisions and then applied RL to induce pedagogical policies from the corpus; and in the test phase, we re-implemented
the ITS using the learned pedagogical policies and measured the learning gains of the induced policies with a new group of students. The
ITS involved in this project is called Cordillera, a NL tutoring system
which teaches students introductory physics [68].
Although NL tutoring systems can be seen as a type of dialogue
system, it is an open question whether using an exploratory corpus
as training data would induce policies that improve learning gains as
opposed to task completion or user satisfaction.
One major source of uncertainty comes from the fact that the rewards used in RL are much more delayed in NL tutoring systems
than in non-tutoring dialogue systems. The most preferable rewards
for NL tutoring systems are student learning gains and the rewards
for non-tutoring dialogue systems are often user satisfaction or task
completion. Even though the rewards in both types of systems will not
be available until the entire system-user interaction is over, NL tutoring dialogues are longer and more complex than the database-access
dialogues described above.
In dialogue systems like a train scheduler or NJFun, the interaction
time is often less than 20 minutes and the number of user-system
interactions is generally less than 20 turns [58, 59]. In NL tutoring
systems, on the other hand, the preparatory training materials and
tests typically exceed these timeframes significantly. In this project, it

USER658.tex; 26/01/2011; 15:41; p.7

8

Min Chi, Kurt VanLehn, Diane Litman, and Pamela Jordan

took students roughly 4-9 hours to complete the training on Cordillera
and the number of Cordillera-student interactions was more than 280.
More immediate rewards are more effective than more delayed rewards for RL-based policy induction. This is because the issue of assigning credit for a decision or attributing responsibility to the relevant
decision is substantially easier with more immediate rewards. The more
we delay success measures from a series of sequential decisions, the more
difficult it becomes to identify which of the decision(s) in the sequence
are responsible for our final success or failure.
Another major source of uncertainty for using an exploratory corpus
to induce effective pedagogical policies comes from the fact that the
amount of training data is severely limited by the need for student-ITS
interactions. Compared with non-tutoring dialogue systems, collecting
data on NL tutoring systems is even more expensive. In this study, we
only included 64 students’ interaction logs in the exploratory corpus
which is quite small compared with 311 dialogues used in NJFun [59]
whose domain task is much simpler than ours.
To summarize, given the much delayed reward functions and the
high cost of collecting data, it is still an open question whether applying Policy Iteration to induce pedagogical policies directly from a
relatively small exploratory training corpus would indeed be effective
in this project. Moreover, while previous research on applying RL to
ITSs and dialogue systems gave us some guidance on the type of RL
algorithms we should apply and how to collect the training corpus, we
still face one primary challenge: how to define the state representation.
2.3. Prior Work on State Representation
For RL, as with all machine learning tasks, success depends upon an
effective state representation S. Ideally S should include all of the
relevant dialogue history necessary to determine which action should
be taken next. One obvious but impractical choice is to use a complete
record of the dialogue to the present point; however, in practice we
need to compress the dialogue history to make the space tractable.
In other words, an effective representation S should be an accurate
and compact model of the learning context. The challenge thus lies in
identifying useful state features.
Early work on applying RL on non-tutoring dialogue systems focused largely on relatively simple task domains and used expert defined
slot-based state representations. In applying RL to improve NJFun,
for example, seven features were included in their representation S
which includes information such as the confidence of ASR (automatic
speech recognition), whether the system has greeted the user, which

USER658.tex; 26/01/2011; 15:41; p.8

Applying Reinforcement Learning To Pedagogical Strategies

9

information is being worked on (time, actitivity, location), and how
many times a given piece of information has been asked for [58]. Additionally, some previous studies focused on the type of features that
should be included in the state representation. For instance, Frampton
and colleagues [24, 25] showed that incrementally adding high-level
contextual information (such as the user’s last dialogue act and the
last system move) into a state representation was beneficial.
As RL and MDP are applied to more complex domains, S may
increase rapidly in size and complexity. For example, when a student
is trained on an ITS, there are many factors that might determine
whether the student learns well from the ITS. Compared with nontutoring NL dialogue systems, where success is primarily a function
of communication efficiency, communication efficiency is only one of
the factors determining whether a student learns well from an ITS.
Moreover, many other factors are not well understood, so to be conservative, states need to contain features for anything that is likely to
affect learning.
To make the RL problem tractable, much prior work on applying
RL to ITSs also used pre-defined state representation. More specifically,
the representation S often consists of features that were suggested by
the learning literature. In the work of Iglesias and her colleagues’ [30,
31, 32], the state representation involves the student’s knowledge level
on various knowledge items. By doing so, they assumed that the system
should adapt its behavior based on student knowledge levels. However,
much other useful information about the learning environment, such
as the difficulty of the problem, student motivation and affect, was
largely ignored. By contrast, Tetreault et al. included a broader range
of features in their state representation: Certainty, Correctness, Percent
Correct, Concept Repetition, and Frustration [62, 64]. Note that some
of their features were manually annotated. Beck et al. [8] included 15
state features from 4 main categories that are suggested by the learning
literature. The four main categories included the student’s level of prior
proficiency, level of cognitive development, and background, and the
difficulty of the current problem [8].
While existing learning literatures and theories give helpful guidance on state representation S, we argue that such guidance is often
considerably more general than the specific state features chosen. The
following, for example, is a list of six features describing a student’s
knowledge level from different perspectives.

1. [Percentage Correct:] Defined as the number of the correct student entries divided by the total number of the student entries.

USER658.tex; 26/01/2011; 15:41; p.9

10

Min Chi, Kurt VanLehn, Diane Litman, and Pamela Jordan

2. [Number of Correct:] Defined as the number of the correct
student entries.
3. [Percent Correct in This Session:] Defined as the number of the
correct student entries in this session divided by the total number
of student entries in this session.
4. [Number of Correct in This Session:] Defined as the absolute
number of the correct student entries in this session.
5. [Number of Incorrect:] Defined as the number of the incorrect
student entries.
6. [Number of Incorrect in This Session:] Defined as the number
of the incorrect student entries in this session.
When making specific decisions about including a feature on student
knowledge level in S, for example, it is often not clear which one of
these six features should be included. Therefore a more general state
representation approach is needed. To this end this project began with a
large set of features to which a series of feature-selection methods were
applied to reduce them to a tractable subset. While much previous
research on the use of RL to improve ITSs and dialogue systems has
focused on developing the best policy for a set of given features [8, 30,
31, 32, 71, 29], only a little work has been done on feature selection.
Paek and Chickering’s work, for example, showed how a state-space
can be reduced by only selecting features that are parents of the local
immediate reward in that the former performs just as well as a more
complicated model with other non-parent variables [50]. In [55], Rieser
and Lemon used logistic regression to select the best state features
for a multi-modal dialogue system and showed marked improvement
over the baseline and some supervised learning methods. Most recently,
Tetreault, et al [65] tackled the feature selection issue by exploring
three evaluation metrics for assessing the utility of adding a particular
state feature to a model of user state. The feature selection procedure
employed in this project is motivated by their work [65]. However, our
approach is fundamentally different from Tetrault et al.’s work because
they explored three evaluation metrics and used a relatively simple
feature selection procedure. Here we explore a series of different feature
selection procedures, but use only one evaluation metric, the Expected
Cumulative Reward (ECR).
To summarize, while previous work on applying RL to induce pedagogical policies in ITSs indicated some potential benefits from RLinduced policies, little empirical evidence has shown that an RL induced
policy indeed improved students’ learning significantly. In this project,

USER658.tex; 26/01/2011; 15:41; p.10

Applying Reinforcement Learning To Pedagogical Strategies

11

to tackle this problem directly, our approach was: 1) to involve human
subjects in both training and test phases; 2) to begin with a large set
of features to which a series of feature-selection methods were applied
to reduce them to a tractable subset.
Previously, we reported our first round of RL results in [15]. Overall,
our results showed that even though the RL-induced policies generated
significantly different patterns of tutorial decisions than the random
policy, no significant difference was found between the two groups on
overall learning performance [15]. Only on one specific skill and one
particular post-test measurement did the induced policy students score
higher than the random policy students [15].
There are many possible explanations for the lack of improvement in
our first round of RL policy induction. We argue it was because our RL
approach was limited. More specifically, a greedy-like feature selection
approach selected four features out of a total of 18 feature choices.
In [14], we explored four RL-based feature selection methods on the 18
features. We found that simply improving the feature selection methods
resulted in policies with much higher ECR than those employed in [15].
The higher the ECR value of a policy, the better the policy is supposed
to perform. However, the effectiveness of induced policies from [14] were
not tested with human students.
We then executed a second round of policy induction. As reported
earlier [16], the empirical results showed that the second round induced
policies indeed caused students to learn more and deeper. To investigate
why this welcome benefit occurred, for this paper we analyzed computer
logs to characterize the induced policies. Given the page limitations in
the previous publications, a complete and detailed description of our
RL methodology has not been reported previously.
In this paper, we will present our RL methodology, empirical results
from validating the induced policies, log analysis on the characteristics
of the induced policies, findings on which features among the ones
defined were most involved in the final induced policies, and the relative
effectiveness of different feature-selection methods. More specifically, we
will mainly focus on the second round of RL policy induction because
it turned out to be more successful.

3. Applying RL to Induce Policies with a Set of Given State
Features
Previous research on using RL to improve dialogue systems (e.g. [43,
58]) has typically used MDPs (Markov decision process) [61] to model

USER658.tex; 26/01/2011; 15:41; p.11

12

Min Chi, Kurt VanLehn, Diane Litman, and Pamela Jordan

dialogue data. An MDP describes a stochastic control process and
formally corresponds to a 4-tuple (S, A, T, R), in which:
S = {S1 , · · · , Sn } is a state space.
A = {A1 , · · · , Am } is an action space represented by a set of action
variables;
T : S × A × S → [0, 1] is a set of transition probabilities between
states that describe the dynamics of the modeled system; for
example: P (Sj |Si , Ak ) is the probability that the model would
transition from state Si to state Sj by taking action Ak .
R : S × A × S → R denotes a reward model that assigns rewards
to state transitions and models payoffs associated with such
transitions.
Additionally, π : S → A is defined as a policy.
From the RL and MDP perspective, pedagogical strategies are simply a set of policies which are mappings from the set of states in the
state space S to a set of actions in A. Note that in order for RL to be
feasible, the number of states and actions should not be too large. On
the other hand, when an MDP is constructed for tutorial dialogues,
it can have millions of distinct dialogue states and tutor utterances.
Here both states and actions in an MDP are abstract. For instance, a
state in the MDP might be a set of features that represent thousands of
real dialogue states and the action “Elicit” in the MDP might denote
any information-seeking questions asked by the tutor. Thus, “state”
and “action” have different meanings in an MDP versus in a tutorial
dialogue. In order to induce a policy from the MDP perspective, there
must be deterministic functions for mapping from dialogue states to
MDP states and from a dialogue action to an MDP action. For instance, one type of tutorial decision we investigated is elicit/tell (ET)
decisions; a policy would determine, based on the features present in a
particular dialogue state, whether the tutor should elicit the next step
from students, or tell students the next step directly.
In this project, we applied Policy Iteration [61], which is a modelbased RL approach. One major difference between the model-based
and model-free RL methods is that the former need to learn transition
probabilities T explicitly. Generally T is estimated from a training
corpus Γ.
As each student solves a series of training problems on an ITS,
a system-student tutorial dialogue is generated. For each tutorial dialogue, a scalar performance measure called reward, R, is calculated. For
example, a common choice for R in ITSs is student learning gains. In
this project, the reward function R is based on a Normalized Learning

USER658.tex; 26/01/2011; 15:41; p.12

Applying Reinforcement Learning To Pedagogical Strategies

13

Gain (NLG). This is because NLG measures a student’s gain irrespective of his/her incoming competence and we have: N LG = posttest−pretest
.
1−pretest
Here posttest and pretest refer to the students’ test scores before and
after the training respectively; and 1 is the maximum score.
After training a group of students on the ITS, we get a training
corpus Γ, which is a collection of system-student tutorial dialogues.
Following Singh et al. (1999), we can view each system-student interaction log di as a trajectory in the chosen state space determined by
the system actions and student responses:

s1di
sjdi

a1d ,rd1
i

i

−−−−→

s2di

a2d ,rd2
i

i

−−−−→

nd

nd

i
i
nd adi ,rdi
−−−→
· · · sdi i −−−

ajd ,rdj

i
i
Here
−−−
−→
sj+1
indicates that at the jth turn in the tutorial
di
dialogue di , the system is in MDP state sjdi , executes MDP action ajdi ,
receives MDP reward rdj i , and then transfers into MDP state sj+1
di . The
number of turns in di is ndi . In this project, only terminal dialogue
states have non-zero rewards because a student’s learning gain will not
be available until the entire tutorial dialogue is completed.
Dialogue sequences obtained from the training corpus Γ then can
be used to empirically estimate the transition probabilities T as: T =
,m
{p(Sj |Si , Ak )}k=1,···
i,j=1,··· ,n . More specifically, p(Sj |Si , Ak ) is calculated by
taking the number of times that the dialogue is in MDP state Si , the
tutor took action Ak , and the dialogue was next in state Sj divided
by the number of times the dialogue was in Si and the tutor took Ak .
The reliability of these estimates depends upon the size and structure
of the training data.
Once an MDP model has been completed, calculation of an optimal policy is straightforward. This project employed an RL toolkit
developed by Tetreault and Litman [65].

3.1. Tetreault and Litman’s Rl Toolkit
Tetreault, & Litman’s toolkit [65, 63, 64] uses a dynamic programming
algorithm for Policy Iteration [61]. The code was originally built on
the MDP toolkit written in Matlab [11]. The purpose of this algorithm
is to handle the problem of reward propagation. As noted above, rewards, in this case learning gains, are not assigned until the end of the
tutoring process, long after most actions have occurred. The dynamic
programming algorithm propagates the rewards back to the internal
states weighting the V-value of each state, s, via the following recursive

USER658.tex; 26/01/2011; 15:41; p.13

14

Min Chi, Kurt VanLehn, Diane Litman, and Pamela Jordan

equation:
V (s) = max
R(s, a) +
0
a

X

P (s0 |s, a)γV (s0 )

(1)

s0

Here P (s0 |s, a) is the estimated transition model T , R(s, a) is the
estimated reward model, and 0 ≤ γ ≤ 1 is a discount factor. If γ is less
than 1, then it will discount rewards obtained later. For all the studies
reported here, a discount factor of 0.9 was used, which is common in
other RL models [65].
The V-values, as defined by Equation 1, can be estimated to within a
desired threshold using policy iteration [61]. Here an estimated V-value
and a best possible action to take for each state are recorded. These are
then iteratively updated based on the values of its neighboring states.
This iteration stops when each update yields a difference below some
threshold . Once the policy iteration process is complete, the optimal
dialogue policy π∗ is obtained by selecting the action that produces the
highest expected reward (or V-value) for each state.
Besides inducing an optimal policy, Tetreault, & Litman’s toolkit
also calculate the Expected Cumulative Reward (ECR) and a 95%
confidence interval for the ECR (hereafter, 95%CI) for the optimal
policy [65]. The Expected Cumulative Reward (ECR) of a policy is
derived from a side calculation in the policy iteration algorithm: the
V-values of each state, the expected reward of starting from that state
and finishing at one of the final states. More specifically, the ECR of a
policy π can be calculated as follows:
ECRπ =

n
X
i=1

Ni
× V (si )
N1 + · · · + Nn

(2)

Where s1 , · · · , sn is the set of all starting states and v(si ) is the
V-values for state si ; Ni is the number of times that si appears as a
Ni
. In
start state in the model and it is normalized by dividing N1 +···+N
n
other words, the ECR of a policy π is calculated by summing over all
the initial start states in the model space and weighting them by the
frequency with which each state appears as a start state. The higher
the ECR value of a policy, the better the policy is supposed to perform.
Tetreault and Litman pointed out one limitation of using the ECR
as an evaluation metric for a policy: it assumes that there is sufficient
collected data to derive a reliable policy [65]. However, in practice
researchers frequently have to deal with issues of data sparsity. They
proposed a novel approach of taking into account the reliability of the
transition probability T and constructing a confidence interval for the
ECR for the learned policy.

USER658.tex; 26/01/2011; 15:41; p.14

Applying Reinforcement Learning To Pedagogical Strategies

15

As described earlier, the transition probabilities T were derived from
the training corpus. Note that these transition probabilities T are simply estimates which are more or less accurate, depending on how much
data is available. As an illustration, Tetreault and Litman used the
following example [65]: in an MDP model, we have S = {S1 , S2 , S3 },
A = {A1 , A2 }. From a training corpus Γ, there were ten cases that an
action A1 was taken from state S1 . Out of these, three times the system
transitioned back to state S1 , two times it transitioned to state S2 , and
five times to state S3 . Thus we have
3
= 0.3
10
2
P (S2 |S1 , A1 ) =
= 0.2
10
5
P (S3 |S1 , A1 ) =
= 0.5
10
P (S1 |S1 , A1 ) =

(3)
(4)
(5)

From the same corpus, there were 1000 times that action A2 was taken
from state S2 . In 300 of those cases it transitioned to state S1 ; in 200
cases to state S2 ; and the remaining 500 times to state S3 . Thus,
300
= 0.3
1000
200
P (S2 |S2 , A2 ) =
= 0.2
1000
500
P (S3 |S2 , A2 ) =
= 0.5
1000

P (S1 |S2 , A2 ) =

(6)
(7)
(8)

While both sets of transition parameters have the same value, the
second set is more reliable. In order to take reliability into account,
Tetreault and Litman proposed a CI estimate based upon the available
data [65]. It is done by taking a transition matrix T for a slice and
sampling from each row using a Dirichlet distribution for q times (q =
1000 in this project). As a result, it generates a large number of new
transition metrics T1 , T2 , · · · , Tq that are all very similar to T . They
then run an MDP on all q transition matrices to get a range of ECR’s.
Both the ECR and the 95%CI are used for feature selection as described
in section 4.
To summarize, for each given < S, A, R > and selected training
data Γ, an optimal pedagogical policy π can be induced by applying
Tetreault, & Litman’s toolkit. In this case, our RL approach is quite
straightforward (see Algorithm 1).

USER658.tex; 26/01/2011; 15:41; p.15

16

Min Chi, Kurt VanLehn, Diane Litman, and Pamela Jordan

Algorithm 1 Apply RL To Induce Policies with < S, A, R > and
Given Γ
Represent the training corpus Γ with < S, A, R >.
Estimate the transition probabilities T.
Compute the optimal dialogue policy, π, based on < S, A, R, T >.
3.2. A Simple Example of Induced Policies
Figure 1 shows an example of an RL-induced policy. The policy is for
Elicit/Tell decisions so we have A = {Elicit, T ell} (shown in the second
line in Figure 1) and we used the exploratory corpus as the training
corpus ΓExploratory . The reward function R is students’ N LG × 100.
The first line in Figure 1 shows that the example policy involves
only one state feature: [StepSimplicityP S], which is estimated from
the training corpus based on the percentage of correct answers when
the tutor has done an Elicit (i.e., asked a question) in a specific dialogue
state. 1 Thus the higher the value of StepSimplicyPS, the easier the
dialogue state. By only including one feature in the state, we assume
that the decision to elicit or to tell should depend only on the simplicity
level of the dialogue “state”.

[S:] = {StepSimplicityP S}
[A:] = {Elicit, T ell}
[Policy:]
rule 1: [0] → Elicit
rule 2: [1] → Either Elicit or Tell
ECR: 8.09
95%CI: [4.37, 12.07]
Figure 1. Using StepSimplicityPS to Induce a Single Feature Policy on ET Decisions

The RL toolkit developed by Tetreault and Litman requires all state
features in the model to be discrete variables. [StepSimplicityP S],
however, is numeric and must be discretized before a suitable MDP
can be constructed. In this project, the discretization procedure used
1

Cordillera’s dialogue manager is a finite state network, so dialogue ”state” here
refers to a state in that network. Many students will traverse the same network
state, and some of those will leave the state via the tutor asking a question. This
percentage refers to those students’ answers.

USER658.tex; 26/01/2011; 15:41; p.16

Applying Reinforcement Learning To Pedagogical Strategies

17

two clustering procedures: the TwoStep procedure bounded the number
of clusters in SPSS and the K-means procedure used K-means clustering to locate the optimal cluster centers. But other discretization
procedures such as simple median split can also be applied.
After the discretization procedure, a continuous numeric variable
[StepSimplicityP S] is binned into two binary values: 0 and 1. We have:
“[StepSimplicityP S] : [0, 0.38) → 0; [0.38, 1] → 1”, which means if
StepSimplicityPS value is below 0.38, it is 0 (hard) otherwise, it is 1
(easy).
The number of MDP states in S determines the number of induced
pedagogical rules in π. Because we only have one binary feature in
the state representation, 2 pedagogical rules, rule 1 and rule 2, are
induced (shown under the [policy] in Figure 1). Rule 1 says that when
StepSimplicityPS is low (the content of this dialogue state is hard), the
tutor should elicit; while rule 2 says when StepSimplicityPS is high,
either elicit or tell will do. Figure 1 also shows that the example policy
has: ECR = 8.09 (range (−∞, 100]) with a 95% confidence interval
[4.37, 12.07], which means there is a 95% chance that the ECR of the
learned policy is between a lower-bound of 4.37 and an upper-bound
of 12.07.
So far, this section began with a description of how the problem
of inducing pedagogical policies fits into the general RL and MDP
framework. Then it described the induction toolkit employed and the
assessment metrics used. In short, the promise of the MDP and RL
approach is that once we build an estimated MDP that models the user
population and learning context accurately, the induced policy should
maximize the reward obtained from future students. With this approach, the problem of inducing effective pedagogical policies is thus reduced to computing the optimal policy for choosing actions in an MDP
– that is, the tutoring system should take actions so as to maximize
expected reward.
In this section, we described the abstract methodology by which
pedagogical policies are induced when < S, A, R > is defined and T
is estimated from a given training corpus Γ. While this approach is
theoretically appealing, the cost of obtaining human tutorial dialogues
makes it crucial to limit the size of the MDP state space in order to
minimize data sparsity problems, while retaining enough information in
the states to represent accurately the human population and learning
context. As mentioned above, our approach in this project is to begin
with a large set of features to which a series of feature-selection methods
were applied to reduce them to a tractable subset. More specifically,
we applied 12 feature selection methods. In the next section, we will

USER658.tex; 26/01/2011; 15:41; p.17

18

Min Chi, Kurt VanLehn, Diane Litman, and Pamela Jordan

describe our feature selection procedures in details. In section 7, we will
describe the instantiation of this methodology in Cordillera.

4. Applying Twelve Feature Selection Methods for Inducing
Pedagogical Policies
To differentiate from the specific state representation S used in the RL
and MDP formalism in section 2, we use Ω to represent a large set
of potential state features. In other words, for any induced policy in
this project, its corresponding state representation S is a subset of Ω.
In the following, we will describe for a defined < A, R > and a given
system-student interactivity training corpus Γ how to select a subset S
from a large feature choice set Ω that will generate the best policy.
In order to do feature selection, first we need to decide the maximum number of features, namely m̂, to be included in the final state
representation. In order to determine m̂, it is necessary to consider
the amount of available data and computational power. m̂ should be
small so that we have enough training data to cover each state, yet
be large enough to include enough features to represent states without
losing the information necessary to make good system decisions. In this
project, for example, based on the minimum data available from the
training corpora, we capped the number of features in each policy at
six (m̂ = 6), which means that there can be as many as 26 = 64 rules
in the learned policy.
Before getting into the details of our twelve feature selection methods, we first define an initially empty set Θ for accumulating the
induced policies for a defined < Ω, A, R > and a given training corpus
Γ. The final best policy π ∗ will be selected from Θ by ECR. This is
because ECR has been widely used as the criteria for evaluating induced
policies, for example in the field of applying RL to induce policies from
simulated corpora [34, 74, 73].
Almost every feature selection approach described below involves
inducing single-feature policies first. That is, for each state feature
choice in Ω, the RL toolkit was used to induce a single-feature policy
such as the one used earlier as an illustration (see Figure 1). Although
generating single-feature policies does not involve any feature selection
procedure, it is counted as a feature selection method labeled as “single”
for convenient reasons.
In the following, the focus is on using feature selection methods to
induce policies with at least two features, referred to as multi-feature
policies, and here we explored eleven feature-selection methods. Four
were based upon the RL toolkit and were previously described in [14];

USER658.tex; 26/01/2011; 15:41; p.18

Applying Reinforcement Learning To Pedagogical Strategies

19

one was based on Principal Component Analysis (PCA); four were
combinations of PCA and RL-based; and the final pair were based upon
stochastic selection. Next, we will describe each approach in detail.
4.1. Four RL-based Feature Selection Methods
Detailed descriptions of the four RL-based approaches can be found in
[14]. Here we will give a brief summary. As described above, Tetreault
and Litman’s toolkit calculates an optimal policy together with the
policy’s ECR and 95% CI [65]. Lower-Bounds and Upper-Bounds were
used to refer to the 95% confidence bounds calculated for the ECR. For
example, the example policy in Figure 1 has ECR = 8.09 with a 95%
confidence interval= [4.37, 12.07].
To this point ECR has always been used as the criteria for selecting
the best policies. However, a policy’s Lower-Bound or Upper-Bound
can also be used as the criteria. More specifically, the former evaluates the performance of policies in the worst case, while the latter
describes how well the policy can perform. Additionally, we defined a
ECR
. Any
new criterion named Hedge as: Hedge = U pperBound−LowerBound
of these criteria, ECR, Lower-Bound, Upper-Bound, or Hedge can be
used to evaluate policies. Thus they are used as four different criteria for
our RL-based feature selections. These feature-selection methods are
fairly straightforward and use the same general procedure, described in
Algorithm 2.
Algorithm 2 Four RL-based Feature Selection Procedure
for Ranking Metric in [ECR, Lower-Bound, Upper-Bound, Hedge]
do
Rank the features in Ω in descending order based upon the Ranking
Metric of their corresponding single-feature-policies.
for i = 2 to m̂ do
S = the top i features from the ranked Ω
Induce a pedagogical policy π with < S, A, R > and Γ
add π to Θ
end for
end for
Based upon the ranking metrics used, the four RL-based featureselection methods are named as ECR, Lower-Bound, Upper-Bound,
and Hedge respectively. If we set m̂ = 6, then each of the four methods
would add m̂ − 1 = 5 multi-feature policies to Θ.

USER658.tex; 26/01/2011; 15:41; p.19

20

Min Chi, Kurt VanLehn, Diane Litman, and Pamela Jordan

4.2. PCA-based Feature Selection Method
Sometimes, the state features in Ω were highly correlated which reduced their expressiveness when used together. For example, three
state features defined in this project are: the number of correct student
responses namely “nCorrectKCPM”, the number of incorrect student
responses namely “nIncorrectKCPM”, and the percentage of correct
student responses namely “pctCorrectKCPM”. The third variable can
be easily calculated from the first two in that we have:
pctCorrectKCP M =

nCorrectKCP M
nCorrectKCP M + nIncorrectKCP M

Therefore, it was necessary to apply an analysis procedure to avoid
redundant features. Here we used Principal Component Analysis (PCA)
[35]. More specifically, all state feature choices in Ω were first normalized; then PCA was applied to the normalized features to generate
principal components and their corresponding eigenvalues. These eigenvalues were arranged in descending order, and all components whose
eigenvalues were less than 1 were removed. For each eigenvalue, the
feature that was maximally correlated with the corresponding principal
component was identified.
The resulting features were a subset of Ω and thus we designated
them the PCA-feature subset, labeled as ΩP CA . ΩP CA is an ordered list
arranged by the eigenvalues of its corresponding principal components.
Once ΩP CA was identified, the PCA-only feature selection procedure
was straightforward. It began with the first feature in ΩP CA and added
one feature at a time to induce a new policy. This process was repeated
until the number of features in the state representation S reaches m̂.
Thus, for m̂ = 6, the PCA feature selection method would add (m̂−1 =
5) multi-feature policies to Θ.
4.3. Four PCA and RL-based Feature Selection Methods
Next four new feature selection approaches are created by simply combining PCA-only feature selection with the four RL-based feature selection methods. In these approaches, PCA is used to winnow Ω into ΩP CA
and then the four RL-based methods, ECR, Upper Bound, Lower Bound,
and Hedge are applied on ΩP CA only. The four feature selection methods were thus named PCA-ECR, PCA-LowerBound, PCA-UpperBound,
and PCA-Hedge respectively. Similar to previous approaches, if we set
m̂ = 6, then each of four PCA and RL-based combined feature selection
methods adds m̂ − 1 = 5 multi-feature policies to Θ.

USER658.tex; 26/01/2011; 15:41; p.20

Applying Reinforcement Learning To Pedagogical Strategies

21

4.4. Random Feature Selection Methods
So far nine relatively straightforward feature selection methods have
been introduced. In order to evaluate their relative effectiveness, two
random feature selection methods were employed. The expectation was
that the nine methods would be at least more effective than random
selection.
The two random feature selection methods were named Random
and PCA-Random respectively. This is because features were randomly
selected from Ω and ΩP CA respectively. Moreover, for a given m̂, both
random and PCA-random selections ran two rounds, generating m̂ − 1
policies in each round. In other words, 2×(m̂−1) multi-feature policies
were generated for either random method. If we set m̂ = 6, then the
two random selection methods would add in a total of 20 (4 × (m̂ − 1))
multi-feature policies to Θ.
4.5. Summary on the General Feature Selection
Procedure
To summarize, in this project we first defined a large set of features
that represent relevant information about the learning environment
and then applied twelve (including single) feature selection approaches
to compress the learning environment to a small but carefully selected
feature space. Our procedure can be summarized as:
1. Define < Ω, A, R > and choose Γ.
2. Decide m̂ to be included in the final policy.
3. Set Θ = ∅
a) Inducing all single-feature policies and add them to Θ.
b) Apply eleven feature selection methods to induce a total of 13×(m̂−1)
multi-feature policies 2 and add them to Θ.
4. Select the best policy from Θ by ECR.

4.6. An Example of the Best Policy
In this project we defined fifty features in Ω (details in section 7). After
running the twelve feature selection methods on Ω, a resulting “best”
policy is shown in Figure 2. The feature selection method involved in
inducing this policy is one of the four RL-based methods: ECR feature
selection. Note that here we used the same < A, R > and the same
2
Either the random or PCA-random selection would result in 2 × (m̂ − 1) multifeature policies each and each of the rest nine feature selection would result in (m̂−1)
policies.

USER658.tex; 26/01/2011; 15:41; p.21

22

Min Chi, Kurt VanLehn, Diane Litman, and Pamela Jordan

training corpus ΓExploratory as those used when inducing the singlefeature policy in Figure 1. Thus, we have A = {Elicit, T ell} (shown in
the second line in Figure. 2), the reward function is students’ N LG ×
100, and the training corpus is ΓExploratory .
Compared with the single-feature policy in Figure 1 which has only
StepSimplicityPS in the state representation, the policy in Figure 2 has
three state features. Their corresponding descriptions and discretization information are presented below:
[StepSimplicityPS [0, 0.38) → 0; [0.38, 1] → 1]: encodes a step’s simplicity level. Its value is estimated from the training corpus based on the
percentage of correct answers given on the dialogue state. The descretization
procedure binarized the feature into two values: 0 and 1. If less than 38% of
the answers given were correct, then this is considered to be ’hard’ content
and we set StepSimplicityPS =0; Otherwise, StepSimplicityPS = 1.
[TuConceptsToWordsPS [0, 0.074) → 0; [0.074, 1] → 1]: represents the
ratio of the physics concepts to words in the tutor’s utterances so far. The
higher this value, the greater the percentage of physics content included in
tutor turns. Dialogue states with less than 7.4% on this measure have TuConceptsToWordsPS=0 and 1 otherwise.
[TuAvgWordsSesPS [0, 22.58) → 0; [22.58, ∞) → 1]: encodes the average
number of words in tutor turns in this session. This feature reflects how
verbose the tutor is in the current session. The discretization procedure set
the threshold at 22.58, so dialogue states when the tutor had above 22.58
words per tutor turn in the current session were represented with 1 for TuAvgWordsSesPS and 0 otherwise.

Since each of the three features was discretized into 2 values, a threefeature state representation would result in a state space of 23 = 8.
Thus, 8 pedagogical rules are learned. Figure 2 shows that in 5 situations the tutor should elicit (rules 1-5), in one situation it should tell
(rule 6); in the remaining 2 cases either will do (rules 7-8).
For example, let’s explain rule 6 since it is the only situation in
which the tutor should tell. In rule 6, the state is [0 : 1 : 0], which represents the values of the three corresponding features: StepSimplicityPS,
TuConceptsToWordsPS and TuAvgWordsSesPS respectively. Rule 6
suggests that when the next dialogue content step is hard (as StepSimplicityPS is 0), the ratio of physics concepts to words in the tutor’s
entries is high (as TuConceptsToWordsPS is 1), and the tutor is not
very wordy in the current session (as TuAvgWordsSesPS is 0), then
the tutor should tell. As you can see, a three-feature policy is already
quite subtle and adaptive to the learning context. Moreover, it is not
like most of the tutorial policies derived from analyzing human tutorial
dialogues.

USER658.tex; 26/01/2011; 15:41; p.22

Applying Reinforcement Learning To Pedagogical Strategies

[S:]
=
{StepSimplicityP S
T uAvgW ordsSesP S}
[A:] = {Elicit, T ell}

×

T uConceptsT oW ordsP S

23
×

[Policy:]


rules 1-5:

rule 6:

rules 7-8:

ECR:
95%CI:


0:0:0


0:0:1


1:0:1


1:1:0


1:1:1
h

0:1:0

i

"

0:1:1
1:0:0

#

→ Elicit

→ Tell

→ Either Elicit or Tell

14.25
[10.04, 18.12]

Figure 2. An Example Of Selected “best” Policy on ET Decisions

Figure 2 also shows that the induced three-feature policy has: ECR =
14.25 with a 95% confidence interval [10.04, 18.12]. It shows that adding
TuConceptsToWordsPS and TuAvgWordsSesPS to the single state representation of StepSimplicityPS is effective. Compared with the single
feature policy StepSimplicityPS in Figure 1, the three-feature policy in
Figure 2 not only has higher ECR (14.25. vs. 8.09), but also has a higher
lower-bound (10.04 vs. 4.37) and upper-bound (18.12 vs. 12.07). Thus,
in theory it should be more effective than the single-feature policy.

5. General Approach
In the learning literature, it is commonly assumed that the relevant
knowledge in domains such as math and science is structured as a set of
independent but co-occurring Knowledge Components (KCs) and that
KC’s are learned independently. A KC is “a generalization of everyday
terms like concept, principle, fact, or skill, and cognitive science terms

USER658.tex; 26/01/2011; 15:41; p.23

24

Min Chi, Kurt VanLehn, Diane Litman, and Pamela Jordan

like schema, production rule, misconception, or facet” [68]. For the
purposes of ITSs, these are the atomic units of knowledge. It is assumed
that a tutorial dialogue about one KC (e.g., kinetic energy) will have
no impact on the student’s understanding of any other KC (e.g, of
gravity). This is an idealization, but it has served ITS developers well
for many decades, and is a fundamental assumption of many cognitive
models [3, 49].
When dealing with a specific KC, the expectation is that the tutor’s
best policy for teaching that KC (e.g., when to Elicit vs. when to Tell)
would be based upon the student’s mastery of the KC in question, its
intrinsic difficulty, and other relevant, but not necessarily known factors
specific to that KC. In other words, an optimal policy for one KC might
not be optimal for another. Therefore, one assumption made in this
project is that inducing pedagogical policies specific to each KC
would be more effective than inducing an overall KC-general
policy.
The domain chosen for this project is the Physics work-energy domain, a common component of introductory college physics courses.
Two domain experts, who are also knowledge representation experts
(not the authors), identified 32 KCs in the domain. They had experience
identifying KCs for a series of previous studies involving college physics.
Note that a complicated domain like physics can often be broken into
many KCs. Here the 32 identified KCs are believed to cover the most
important knowledge in the domain. In order to induce effective pedagogical policies, we investigated KC specific pedagogical policies in our
RL applications.
This was a three-year project that can be divided into three stages,
one stage per year since Fall 2007. In each stage, a group of students was
trained on Cordillera. All three groups followed the same procedure:
completing a background survey, reading a textbook, taking a pretest, training on Cordillera, and finally, taking a post-test. All three
groups used the training problems and instructional materials but on
different versions of Cordillera. The versions differed only in terms of
the pedagogical policies employed for interactive tutorial decisions.
In Stage 1, Cordillera made interactive decisions randomly and we
collected an exploratory corpus that examined the consequences of each
tutorial decision with real students. The student group is thus referred
to as the Exploratory Group. In order to differentiate this version of
Cordillera from the ones used in subsequent studies, this version is
referred to as Random-Cordillera.
In Stage 2, we tried our first round of policy induction on all 32
KCs. Then these RL-induced policies were empirically evaluated. More
specifically, Tetreault, & Litman’s toolkit was applied to the Exploratory

USER658.tex; 26/01/2011; 15:41; p.24

Applying Reinforcement Learning To Pedagogical Strategies

25

corpus to induce a set of pedagogical policies. Because we dichotomized
the students’ NLGs into +100 and −100 as reward functions, the induced policies were referred to as Dichotic Gain (DichGain) policies[15,
13]. The induced DichGain policies were added to Cordillera and this
version of Cordillera was named DichGain-Cordillera. Except for following different policies (random vs. DichGain), the remaining components
of Cordillera, including the GUI interface, the training problems, and
the tutorial scripts, were left untouched. DichGain-Cordillera’s effectiveness was tested by training a new group of 37 college students in
2008. Results showed that although the DichGain policies generated
significantly different patterns of tutorial decisions than the random
policy, no significant overall difference was found between the two
groups on the pretest, posttest, or the NLGs [15].
There are many possible explanations for the lack of difference in
learning outcomes between the DichGain and Exploratory groups. We
argue that the exploration of our RL approach in stage 2 was limited.
As described above, applying RL to induce effective tutorial policies
may not be a simple task for which we can plug a training corpus into
a toolkit. Rather it depends on many factors, such as state feature
choices, feature selection methods, and the definition of reward functions. In stage 2, only 18 features were included in our state feature
choices and no more than four appeared in the final induced tutorial
policies. Thus the defined 18 features may be insufficient to adequately
represent the state. Moreover our greedy-like feature selection process
may also have limited our success. More details on the procedure of
inducing DichGain policies and an example of DichGain policies can
be found in [15]. Therefore stage 3 was designed to address these
limitations in hopes of producing more effective pedagogical policies.
In stage 3, the approach to RL-related issues was greatly modified.
Instead of focusing on all 32 KCs, we only focused on the eight primary
KCs. We directly used students’ Normalized Learning Gain (NLG)×100
as the reward function instead of dichotomizing the NLG. The induced
set of tutorial policies is thus named Normalized Gain (NormGain)
tutorial policies and the version of Cordillera was named NormGainCordillera. We again ran a new group of students, named the NormGain
group, using the same educational materials as used by the previous
two groups. Next, we will describe the general methods involved in this
project.

USER658.tex; 26/01/2011; 15:41; p.25

26

Min Chi, Kurt VanLehn, Diane Litman, and Pamela Jordan

6. Method
In this section, we will first describe the NL tutoring system involved in
this project, Cordillera. Then we will describe the two types of tutorial
decisions, elicit/tell and justify/skip-justify, that the RL-induced pedagogical policies make. After that, we will describe the domain chosen
for this project and specifically focus on the eight major KCs on which
KC-specific NormGain policies were induced in Stage 3. Then we will
describe the experimental procedure which is identical for all three
studies in all three stages. Finally, we will describe our grading criteria
for this project.
6.1. Cordillera
Cordillera is an NL Tutoring System that teaches students introductory
college physics [68] and was developed using the TuTalk NL tutorial
dialogue toolkit [37, 36]. TuTalk is an authoring tool which enables
domain experts to construct natural language tutoring systems without programming. Instead, the domain experts focus on defining the
tutoring content by writing tutorial scripts, which are then used for
automating interactions. In other words, the script authors determine
the flow of the dialogue and the content of each tutor turn.
The student interface is used by students to read the tutor’s tutorial
instructions and to answer questions by means of natural language
entries. Figure 3 shows a screen shot of the student interface. The Message Window, located in the bottom-left corner is where the dialogue
interaction takes place. The remaining panes are the Dialogue History
Pane (upper-left), Problem Statement pane (upper-right), and Variable
Pane (lower-right).
To reduce potential confounds due to imperfect NL understanding,
the NL understanding module in Cordillera was replaced with a human
interpreter called the language understanding wizard [9]. The only task
performed by the human wizards is to match students’ answers to the
closest response from a list of potential responses and they cannot
make any tutorial decisions. In this format, Cordillera works as a communications framework that connects a student interface to a wizard
interface.
Across three stages of this project, three different versions of Cordillera
were constructed, each of which differed only in terms of the pedagogical policies employed. The remaining components of the system,
including the GUI interfaces and domain experts’ tutorial scripts, were
identical for all participants. In Cordillera the pedagogical policies are

USER658.tex; 26/01/2011; 15:41; p.26

Applying Reinforcement Learning To Pedagogical Strategies

27

Figure 3. Cordillera Student Interface

used to make two types of tutorial decisions. Next, we will briefly
describe them in detail.
6.2. Two types of Tutorial Decisions
Two types of tutorial decisions: Elicit/Tell (ET) and Justify/Skipjustify (JS) were the focus of our attention. For both ET and JS
decisions, there is no widespread consensus on how or when either of
these actions should be taken. This is why our research objective is
applying RL to learn policies on them.
6.2.1. Elicit/Tell
During the course of one-on-one tutoring, the tutor often faces a simple
decision, to elicit the next step from a student, or to tell a student the
next step directly. We refer to such tutorial decisions as elicit/tell (ET)
decisions. While a lecture can be viewed as a monologue consisting of
an unbroken series of tells, human one-on-one tutoring is characterized
by a mixture of elicits and tells. Some existing theories of learning
suggest that when making tutorial decisions, a tutor should adapt its
actions to the students’ needs based upon their current knowledge level,
affective state, and other salient features [53, 51, 22, 21, 70, 18, 40].

USER658.tex; 26/01/2011; 15:41; p.27

28

Min Chi, Kurt VanLehn, Diane Litman, and Pamela Jordan

Typically, these theories are considerably more general than the specific
interaction decisions that system designers must make. This makes it
difficult to instantiate these theories as specific pedagogical policies in
ITSs. Therefore when facing a decision whether to elicit or to tell a
new step, most existing tutoring systems always decide to elicit [4, 41,
69, 27, 67, 44]. For example, existing NL tutoring systems often mimic
a pervasive five-step dialogue pattern found in human tutoring that
starts with the tutor posing a question or problem [26, 67].
Figure 4 presents a pair of sample dialogues comparing elicit and
tell versions of a single tutorial dialogue extracted from the log files
collected during this project. Both dialogues begin and end with the
same tutor turns (lines 1 and 6 in (a) and 1 and 4 in (b)). However, in
dialogue (a) the tutor chooses to elicit twice (lines 2-3 and 4-5 respectively) while in dialogue (b) the tutor decides to tell twice (lines 2 and
3). Note that the two dialogues cover the same domain content.

6.2.2. Justify/Skip-justify
The second tutorial decision was whether to execute a justification step,
also referred to as self-explanation in much of the learning literature.
During the tutoring process, human tutors sometimes ask students to
justify a step they have taken or an entry they have made. Their apparent goal appears to be to help students understand domain knowledge
in a deeper way. The open question is whether or not the tutor should
conduct an elaborate discussion of a problem solving step given that
this discussion is not necessary for the solution. We refer to such tutorial
decisions as justify/skip-justify (JS) decisions.
Much previous research including [12], [19], and [2] found that asking students to justify their solution steps improves student learning.
However, eliciting such a discussion may not always be desirable if,
for example, the student is well aware of the rationale. If so, typing
in a justification can be slow, frustrating, and distracting. Indeed, in
domains like second language acquisition, Wylie et al. found that tutors
asking students to justify did not lead to better learning outcomes but
did significantly increase student training time when compared to a
control group that was not asked to enter justifications [75]. Additionally, Katz, O’Donnell, and Kay [39] found that in some cases it may
be better to delay the justifications until the problem has been solved,
especially if the justification is abstract, plan-based, or lengthy.
Figure 5 presents a pair of sample dialogues comparing justify and
skip-justify versions of a single tutorial dialogue extracted from project
log files. In part (a), a justification is employed to guide the student
(line 3-4); while in part (b), the justification is skipped.

USER658.tex; 26/01/2011; 15:41; p.28

Applying Reinforcement Learning To Pedagogical Strategies

29

(a) Elicit Version
1. T: So let’s start with determining the value of v1.
2. T: Which principle will help you calculate the rock’s
instantaneous magnitude of velocity at T1? {ELICIT}
3. S: definition of kinetic energy
4. T: Please write the equation for how the definition of kinetic
energy applies to this problem at T1 {ELICIT}
5. S: ke1 =0.5*m*v1ˆ2
6. T: From KE1 = 0.5*m*v1ˆ2, · · ·
(b) Tell Version
1. T: So let’s start with determining the value of v1.
2. T: To calculate the rockś instantaneous magnitude of velocity at
T1, we will apply the definition of kinetic energy again. {TELL}
3. T: Let me just write the equation for you: KE1 = 0.5*m*v1ˆ2.
{TELL}
4. T: From KE1 = 0.5*m*v1ˆ2, · · ·
Figure 4. Elicit vs. Tell

6.3. The Domain and Eight Primary Knowledge
Components
The domain chosen for this project, Physics work-energy problem solving, is a common component of introductory college physics courses.
As mentioned before, two domain experts identified 32 KCs for this
domain. However, in a domain like physics, solving a problem requires
producing an argument, proof or derivation consisting of one or more
inference steps; each step is the result of applying a domain principle,
operator or rule. Thus the major domain principles are more challenging and important than other KCs since the student’s overall learning
performance depends more on learning domain principles. Therefore,
when inducing NormGain policies, the decision was made to focus only

USER658.tex; 26/01/2011; 15:41; p.29

30

Min Chi, Kurt VanLehn, Diane Litman, and Pamela Jordan

(a) Justify Version
1. T:Can we infer the direction of the velocity of the rock at T1 from
the rock’s kinetic energy at T1? {ELICIT}
2. S:nope.
3. T:Excellent! Please explain why. {JUSTIFY, ELICIT}
4. S:Only the magnitude of the velocity and not the direction of it is
part of the definition of kinetic energy.
5. T:Excellent! Now that we know v1, · · · .
(b) Skip-justify Version
1. T:Can we infer the direction of the velocity of the rock at T1 from
the rock’s kinetic energy at T1? {ELICIT}
2. S:nope.
3. T:Excellent! {Skip-JUSTIFY}
4. T:Now that we know v1, · · · .

Figure 5. Justify vs. Skip-justify

on the eight primary KCs corresponding to the eight major domain
principles.
The eight major principles in the domain are shown in Table I.
In Table I, the first column lists its corresponding KC number. The
second column describes the name of the principle. The last column is
the formula or mathematical expression of the principle.
6.4. Procedure
All participants in this project experienced the same five standard
phases: 1) background survey, 2) pre-training, 3) pre-test, 4) training,
and 5) post-test. Unless specified explicitly in the following, the procedure, reading contents, training materials, GUI, and test items were
identical across all groups and in each phase there were no time limits.

USER658.tex; 26/01/2011; 15:41; p.30

Applying Reinforcement Learning To Pedagogical Strategies

31

Table I. Major Principles of Work and Energy
KC

Principle description

Expressions

KC1
KC14
KC20
KC21
KC22
KC24
KC27

Weight Law (w)
Definition of Work (W)
Definition of Kinetic Energy (KE)
Gravitational Potential Energy (GPE)
Spring Potential Energy (SPE)
Total Mechanical Energy (TME)
Conservation of Total Mechanical Energy
(CTME)
Change of Total Mechanical Energy for
Non-isolated Systems (TMENC)

W = mg
W = F dcos(α)
KE = 21 mv 2
GP E = mgh
SP E = 12 kd2
T M E = KE + GP E + SP E
T M E1 = T M E2

KC28

N etW = T M E2 − T M E1

The background survey asked students for demographic information
such as gender, age, SAT scores, high school GPA, experience with
algebra, calculus, physics, and other information.
Following the background survey, students read the physics textbook
during the pre-training and took the pre-test. The physics textbook was
only available during phase 2, pre-training.
In phase 4, students were first trained to solve a demonstration
problem, which did not include physics content, on Cordillera. The
sole purpose of this step was to familiarize them with the GUI interface.
They then solved the same seven training problems in the same order
on corresponding versions of Cordillera.
Finally, students took the post-test. The pre- and post-tests were
identical in this project. Both contained a total of 33 problems selected
from the Physics literature by two domain experts (not the authors).
The 33 problems covered 168 KC applications. The tests were given
online and consisted of both multiple-choice and open-ended questions.
Open-ended questions required the students to derive an answer by
writing or solving one or more equations. Once an answer was submitted, students automatically proceeded to the next question without
receiving any feedback on the correctness of a response. Students were
not allowed to return to prior questions.
As mentioned above, the reward function for the RL algorithm is
the NLG. Therefore we used identical pre- and post-tests to avoid the
need to factor out test differences. Students were not informed that
the tests would be identical at any point; they received no feedback on
their test answers or test scores; and the minimum time between the

USER658.tex; 26/01/2011; 15:41; p.31

32

Min Chi, Kurt VanLehn, Diane Litman, and Pamela Jordan

pre- and posttest was one week.
Overall, only three salient differences exist between the three groups:
1. The Exploratory group with a population of 64 was recruited in
2007; the DichGain group with a population of 37 was recruited
in 2008; and the NormGain group with a population of 29 was
recruited in 2009.
2. Random-Cordillera made random decisions and the DichGain-Cordillera
and NormGain-Cordillera followed the induced DichGain and NormGain policies respectively.
3. A group of six human wizards were used by the Exploratory and
DichGain groups; but only one of the six wizards was available for
the NormGain group.
6.5. Grading
All tests were graded in a double-blind manner by a single experienced
grader by mixing all students’ test answers together. In a double-blind
manner, the grader does not know the group to which each answer
belongs nor the test, pre-test or post-test to which each answer belongs.
For all identified relevant KCs in a test question, a KC-specific score for
each KC application was given. We evaluated the student’s competence
in the following sections based on the sum of these KC-specific scores,
named cumulative KC-specific scores. This is because the KC-specific
pre- and post-test scores were used to define the reward functions
when applying RL to induce KC-specific policies. Later analysis showed
that the same findings held for other scoring rubrics. For comparison
purposes all test scores were normalized to fall in the range of [0,1].

7. Inducing NormGain Pedagogical Policies
When inducing NormGain policies we mainly focused on the eight
primary KCs, one for each major domain principle. Thus, the overall problem of inducing a policy for ET decisions and a policy for JS
decisions is decomposed into 8 sub-problems of each kind, one per KC.
Among the eight KCs, KC 1 does not arise in any JS decisions and thus
only an ET policy was induced for it. For each of the remaining seven
KCs, a pairs of policies, one ET policy and one JS policy, were induced.
So we induced 15 KC-specific NormGain policies. During the tutoring

USER658.tex; 26/01/2011; 15:41; p.32

Applying Reinforcement Learning To Pedagogical Strategies

33

process, there were some decision steps that did not involve any of the
eight primary KCs. For them, two KC-general policies, an ET policy
and a JS policy, were induced. To sum, a total of 17 NormGain policies
were induced in stage 3.
Both inducing KC-general and KC-specific policies shared the same
common procedure. First, we need to define < Ω, A, R >, choose a
training corpus Γ, and determine the maximum number of features,
m̂ included in the induced policy. Once these factors are decided, the
general procedure for the 12 feature selection methods described in
section 4 are followed. In this project, we used the same state feature
choices in Ω, Γ, and m̂ for inducing each of the 17 NormGain policies.
7.1. STATE REPRESENTATION SET Ω
As described above, an effective state representation should minimize
state size while retaining sufficient relevant information about the learning context. In this project, Ω consists of only features that could be
computed automatically or evaluated objectively, such as gender. Handannotated dialogue features were omitted as the tutor would require
the features to be available in real time when the induced policies are
employed. Next, we will describe the 50 feature choices in our Ω.
The 50 feature choices were defined based upon six categories of
features considered by previous research [48, 8, 23] to be relevant for
making tutorial decisions.
Autonomy (A) Features relate to the amount of work performed by
the student in the dialogue. We defined five numeric features, which
end with an ‘A’ in their names. For example, [tellsSinceElicitA] is one
of the five features in this category. It refers to the number of tells
the student has received since the last elicit prompt, irrespective of
the KC involved. For example, tellsSinceElicitA = 2 means that two
tell decisions have been made since the last elicit decision. This feature
reflects how active a student may be currently, that is, how much work
the student has performed recently.
Background (BG) features describe general background information
about the student. The five Background Features include gender, age,
Math SAT, Verbal SAT, and pre-test scores. None of these features
change during training on Cordillera. All five background features end
with “BG”. One important note was that for DichGain group, the
following features, gender, age, Math SAT, and Verbal SAT, were not
available because of an administrative error.
Problem Solving Contextual (PS) features encode information about
the current problem-solving context. All fifteen problem solving-related
features end with ‘PS.’ For example; one feature defined in this category

USER658.tex; 26/01/2011; 15:41; p.33

34

Min Chi, Kurt VanLehn, Diane Litman, and Pamela Jordan

is StepSimplicityPS. It is used when inducing the single-feature policy
in Figure 1 and the three-feature policy in Figure. 2). StepSimplicityPS
is always in the range of [0, 1]. “StepSimplicityPS” = 1 means it is a
easy step, whereas if it is close to 0, it means it is a difficult question.
Performance (PM) features describe information about student performance during the training. All twelve performance-related features
end with “PM.” For example, pctCorrectKCPM refers to the percentage of the student’s correct entries on a KC. It is calculated by assessing
all of the correct cases on the present KC in the student’s entries divided
by the total number of cases the present KC appeared in the student’s
entries. This feature reflects the student’s overall competence on the
current KC.
Student Dialogue (SD) features characterize students language. All
Student Dialogue related features end with “SD.” They are simple
linguistic features that are computed from the student’s dialogue turns.
For example, one feature in this category is stuAverageWordsSD, which
refers to the average number of words per student turn. This feature
reflects how verbose the student was overall.
Temporal Situation (T) features encode time-related information
about the problem-solving process. All three temporal situation features are numeric and end with a ‘T’ in their names. For example, one
feature in this category is durationKCBetweenDecisionT. It refers to
the time since the last tutorial decision was made on the current KC.
This feature reflects how active a student’s knowledge of the current
KC is. If it is high, it means that the tutor has not mentioned the KC
recently so the student’s knowledge about the current KC may be less
activated.
7.2. Three Training Corpora
The choice of training corpus Γ is a complex one. In stage 3, we have
three training corpora available: the Exploratory corpus Γexploratory
consisted of 64 complete tutorial dialogues; the DichGain corpus ΓDichGain
contained 37; and the combined corpus Γcombined comprised a total of
101 dialogues. Γexploratory was collected for RL and designed to explore
the feature space evenly and without bias. ΓDichGain , by contrast, is
similar to many other pre-existing corpora by following a set of specific pedagogical strategies. Inducing a successful policy from ΓDichGain
would show the potential for applying RL to induce effective tutorial
policies from most pre-existing data. Γcombined , in theory, offers the
benefits of both as well as an increased dataset.
Across three corpora, the total number of ET decisions in a tutorial
dialogue ranged from 250 to 332 and we have: M = 273.89, SD = 12.46

USER658.tex; 26/01/2011; 15:41; p.34

Applying Reinforcement Learning To Pedagogical Strategies

35

in Γexploratory and M = 270.54, SD = 10.00 in ΓDichGain ; the number of
JS tutorial decisions ranged from 52 to 71, we have M = 56.61, SD =
3.43 in Γexploratory and M = 58.43, SD = 2.82 in ΓDichGain ; the total
number of the tutorial decisions regardless of decision types for each
system-student interaction dialogue ranged from 288 to 3723 , we have
M = 305.48, SD = 14.01 in Γexploratory and M = 307.57, SD = 12.45
in ΓDichGain .
On a KC by KC basis, however, the average number of tutorial
decisions varies significantly across KCs: from as few as four on KC1
to more than 80 on KC20 . The average number of tutorial decisions on
elicit/tell (ET) and justify/skip-justify (JS) also varies across the eight
primary KCs. There are only 4 ET decisions on KC1 and more than
70 ET decisions on KC20 . Similarly, there are only 2 JS decisions for
KC14 on average and more than 16 for KC21 . Overall, the ET tutorial
decisions were much more frequent than the JS ones.
In this project, when inducing NormGain policies, rather than selecting one corpus a priori, all three were used. More specifically, a set
of tutorial policies were derived from each training corpus separately
and then the best policy from all sets were selected by ECR.
7.3. Maximum Number of Features m̂
As mentioned above, in order to determine m̂, it is necessary to consider
the amount of available data and available computational power. In the
worst case scenario, there were only 2 JS tutorial decision steps in the
DichGain training corpus for KC14 . Therefore, based on the minimum
data available from the three training corpora, we capped the number
of features in each policy at six, m̂ = 6, which means that there are
at least 26 = 64 states in the learned policy. Alternatively, we could
have used a flexible number for different KCs. However, it is not the
case that learned tutorial policies with six features were most effective
and instead the final 17 induced NormGain policies primarily have 3-5
features in their state representation. Only one of 17 final policies has
six features. As shown earlier, the three-feature policy in Figure 2 is
already quite subtle so it appears that six is a reasonable number.
Once < Ω, A, R >, Γ, and m̂ are determined we just need to follow
the twelve feature selection procedures described in section 4 to induce
the 17 NormGain policies. Inducing KC-specific policies can be seen as
a special case of inducing KC-general policies. Therefore, we will start
with our policy induction procedure on two KC-general policies first.
3
overall decisions < ET decisions + JS decisions because on certain tutorial decision steps, the tutor makes both types of decisions: JS first and then ET. When
we calculated the overall decisions, such a step was counted as one decision step.

USER658.tex; 26/01/2011; 15:41; p.35

36

Min Chi, Kurt VanLehn, Diane Litman, and Pamela Jordan

7.4. Inducing Two KC-general Policies
When inducing KC-general pedagogical policies on ET or JS decisions,
we have A = {Elicit, T ell} or A = {Justif y, Skip − Justif y} respectively. The reward function R is the same for inducing either action
A, which is calculated based upon students’ NLGs in that we have:
R = N LG × 100. Moreover, the KC-specific features defined in Ω
become KC-general features by taking into account all of the previous
instances regardless of KC. For example, nCorrectKCP M becomes
the number of correct responses on all the KCs instead of on a specific
KC.
Once < Ω, A, R >, Γ, and m̂ are all defined, we can apply our twelve
feature selection methods to get the best policy for corresponding action
decision A. The overall procedure is described in Algorithm 3.
Algorithm 3 Induce Two KC-General Pedagogical Policy on ET and
JS
for Action in [ET, JS] do
if Action = ET then
A = {Elicit, T ell}
else
if Action = JS then
A = {Justif y, Skip − Justif y}
end if
end if
ΘA = ∅
for Γ in [ΓExploratory , ΓDichGain , ΓCombined ] do
Calculate reward function R = N LG × 100
ΘA,Γ = Apply twelve feature selection on < Ω, A, R >, Γ, and
m̂ (see Section 3)
ΘA = ΘA ∪ ΘA,Γ
end for
∗ from Θ by ECR
Select πA
A
∗
RETURN πA
end for
As described in section 4, the general feature selection procedure
would result in: one single-feature-policy for each feature choice in Ω
and 13 × (m̂ − 1) multi-feature policies for a given Γ. In this project,
we defined fifty features in Ω and m̂ = 6. For ET decisions, a total of
50 + 13 ∗ (6 − 1) = 115 KC-general ET policies were induced from each
training corpus. Taken together, all three corpora resulted in a total
of 115 × 3 = 445 KC-general ET policies. The final best KC-general

USER658.tex; 26/01/2011; 15:41; p.36

Applying Reinforcement Learning To Pedagogical Strategies

37

ET policy was selected from the pool by ECR. For the purposes of
this project, the highest ECR irrespective of the confidence bounds
or hedging was selected. The same procedure was also followed for
inducing the KC-general JS NormGain policy.
Inducing 15 KC-specifc NormGain policies followed the same general
procedure as inducing the two KC-general NormGain policies except
that < Ω, A, R > are KC-specific. Next, we will briefly describe the
procedure.
7.5. Inducing 15 KC-specific Pedagogical Policies
In order to learn KC-specific policies, KC-specific < Ω, A, R > are
needed. For example, to induce policies on the ET decisions involving
KC20 , we consider the ET decisions involving KC20 only, the state
representation for KC20 only, and use the learning gains on KC20 only.
Therefore, we annotated our tutoring dialogues with the KCs covered
by the content and action decisions with the KCs covered by each
action.
A group of five individuals (including the first author) annotated
each of the tutoring dialogues and action decisions with the relevant
KCs. The KCs were drawn from the set of 32 identified KCs. For each
of the seven training problems, there were at least two annotators.
For each of 32 identified KCs, the final kappa was ≥ 0.77 which is
fairly high given the complexity of the task. After each utterance in
the system-student interaction dialogue was annotated with the corresponding KCs, a KC-specific Ω can be defined and a KC-specific A can
be identified.
Additionally, a domain expert (not the authors) also mapped the
pre-/post test problems to the sets of relevant KCs. So we can calculate
students’ KC-specific reward functions R defined as: N LGKCi × 100.
Once KC-specific < ΩKC , AKC , RKC > are defined, we applied a
similar procedure as for inducing KC-general NormGain in Algorithm 3.
To summarize, to induce each of 17 NormGain policies, three training corpora, a space of fifty features, and twelve feature selection methods were explored and a total of 115 × 3 = 445 potential policies were
generated. Each final NormGain policy was selected from its corresponding pool by ECR. For example, the three-feature example policy
shown in Figure 2 is one of the final 17 NormGain policies. It is a
KC-specific ET policy on KC20 that is induced from the Exploratory
Corpus Γexploratory .
The resulting 17 NormGain policies were added to Cordillera yielding a new version of the system, named NormGain-Cordillera. In order

USER658.tex; 26/01/2011; 15:41; p.37

38

Min Chi, Kurt VanLehn, Diane Litman, and Pamela Jordan

to execute these tutorial policies, the dialogue manager needed to keep
a record of the student’s current states on each KC. Moreover, it also
retained a KC-specific record for the tutorial decision steps. So when
a tutorial decision step occurred, the dialogue manager first looked up
the KC(s) involved in that step and then looked up the corresponding
policies. When a tutorial decision did not involve any specific KCs,
the dialogue manager followed the KC-general tutorial policies. Next,
the induced tutorial policies were evaluated on real human subjects
to see whether the students whose interactions were guided by the
NormGain tutorial policies would out-perform those guided by random
or DichGain ones.
8. Experimentally Evaluating the NormGain Policies
The goal of the evaluation reported below is twofold: first, to test
whether our improved RL methodology and software produced more
effective pedagogical strategies than either random policies or the DichGain policies; and second, to determine the characteristics of the NormGain policies.
8.1. Overall Learning Results
A one-way ANOVA showed that there were no significant differences
among the three groups on overall training time: F (2, 122) = 1.831,
p = .17. More specifically, the average total training time (in minutes)
across the seven training problems, was M = 278.73 min, SD = 67.38
for Exploratory group, M = 294.33 min, SD = 87.51 for DichGain
group, and M = 259.99 min, SD = 59.22 for NormGain group.
After solving seven training problems on Cordillera, all three groups
scored significantly higher in the posttest than pretest: F (1, 126) =
10.40, p = 0.002 for the Exploratory group, F (1, 72) = 7.20, p = 0.009
for the DichGain group, and F (1, 56) = 32.62, p = 0.000 for the NormGain group respectively. The results suggested that the basic practices
and problems, domain exposure, and interactivity of Cordillera might
help students learn even from tutors with non-optimal pedagogical
strategies.
A one-way ANOVA was used for comparing the learning performance differences among the three groups. While no significant pre-test
score differences were found: F (2, 127) = 0.53, p = 0.59, there were
significant differences among the three groups on both post-test scores
and NLG scores: F (2, 127) = 5.16, p = .007 and F (2, 127) = 7.57, p =
0.001 respectively. Figure 6 compares the three groups on the pretest, post-test, and NLG scores. Moreover, a t-test comparison showed

USER658.tex; 26/01/2011; 15:41; p.38

39

Applying Reinforcement Learning To Pedagogical Strategies

that the NormGain group out-performed the DichGain group on both
post-test scores and NLG scores: t(64) = 3.28, p = .002, d 4 = 0.82
and t(64) = 3.68, p = 0.000, d = 0.95 respectively. Similar results
were found between the NormGain and Exploratory groups: t(91) =
2.76, p = .007, d = 0.63 on post-test, and t(91) = 3.61, p = 0.000, d =
0.84 on NLG scores respectively.

Maximum Score is 1

NormGain

0.65

0.70
0.60
0.50

DichGain

0.53
0.50

0.42
0.38

Exploratory

0.42

0.41

0.40
0.22

0.30

0.22

0.20
0.10
0.00
‐0.10

Pretest

Posttest

NLG

Figure 6. Compare Three Groups Learning Performance under Overall Grading

To summarize, the comparison among the three groups shows that
the NormGain group significantly outperformed both the Exploratory
and DichGain groups. These results were consistent both for the posttest scores and the NLGs and the effect sizes were large by Cohen’s d
criteria.
8.2. LOG ANALYSIS
Having compared the individual groups’ learning performance, this
subsection will compare the log file variations across the three groups.
4
Cohen’s d is defined as the mean learning gain of the experimental group minus
the mean learning gain of the control group, divided by the groups’ pooled standard
deviation.

USER658.tex; 26/01/2011; 15:41; p.39

40

Min Chi, Kurt VanLehn, Diane Litman, and Pamela Jordan

In order to quantify the amount of different decisions, we define an
Interactivity ratio (I-ratio) and a Justification ratio (J-ratio).
I-ratio is defined as the number of elicit decisions a student received
divided by the total number of ET decisions received in the entire tutorial dialogue. The higher this value, the more interactive the tutorial
dialogue. Similarly, J-ratio is defined as the number of times the tutor
executes a justification step divided by the total number of JS decisions
the tutor made in a tutorial dialogue. The higher this value, the deeper
and more elaborate the dialogue may be. Both values range from 0 (no
elicits or justifies) to 1 (all elicitation or justification).
8.2.1. I-Ratio
Table II summarizes t-test comparisons on the I-ratio among the three
tutorial corpora. In Table II, the first two columns list the two groups in
comparison and their corresponding mean and SD scores. The last column lists the statistical results of the t-test comparisons. From Table II,
the I-ratios for the three student groups were: 0.76 (NormGain), 0.44
(DichGain), and 0.50 (Exploratory) respectively and the differences
among them were significant: we have N ormGain > Exploratory >
DichGain. Therefore, the NormGain policies seemingly resulted in
more interactive tutorial dialogue than either the random or the DichGain policies.
Table II. Pairwise Comparison Among Three Groups On I-ratio
Group 1
NormGain
NormGain
Exploratory

0.76 (0.07)
0.76 (0.07)
0.50 (0.03)

Group 2
Exploratory
DichGain
DichGain

0.50 (0.03)
0.44 (0.04)
0.44 (0.04)

Group 1 vs. Group 2
t(91) = 24.72, p = 0.000
t(64) = 22.08, p = 0.000
t(99) = 7.967, p = .000

8.2.2. Justify Ratio
Similarly, Table III summarizes t-test comparisons on J-ratio among
the three tutorial corpora. In Table III, the first two columns list
the two groups in comparison and their corresponding mean and SD
scores. The last column lists the statistical results of the t-test comparisons. Table III, shows that the mean of J-ratios for the three student groups were: 0.82 (NormGain), 0.43 (DichGain), and 0.53 (Exploratory) and the pair wise t-test comparisons show that on J-ratio,
we have: N ormGain > Exploratory > DichGain.

USER658.tex; 26/01/2011; 15:41; p.40

Applying Reinforcement Learning To Pedagogical Strategies

41

To summarize, the NormGain policies produced substantially more
elicit and more justifications than the random or DichGain ones. In the
next section the discussion will focus on some general characteristics
of the induced tutorial policies. The 17 induced NormGain tutorial
policies will be described by identifying the training corpus that each
final tutorial tactic was derived from, which feature categories were
most frequently involved in the final tutorial policies, and which feature
selection method discovered the most final tutorial policies.
8.3. The Impact of Induction Decisions on the 17
NormGain Policies
The purpose of this section is to determine how the RL-related induction decisions described in the previous sections impacted the induced
tutorial policies. For example, one decision was made to use all three
training corpora; did the final induced NormGain policies come from
one corpus or from all three corpora? Moreover, which features appeared in the final induced NormGain policies? Which feature selection
method(s) seemed to be more effective? This section begins with a
discussion of the training corpus involved in the final 17 NormGain
tutorial policies.
8.3.1. Source Training Corpus, Γ
Table IV shows the source training corpus used to induce each of
the 17 NormGain tutorial policies. As described above, among the 17
NormGain policies, two were KC-general policies, an ET policy and a
JS policy. All of the remaining 15 NormGain policies are KC-specific
policies; eight were on ET decisions, one for each of eight major domain
principles, and seven were on JS decisions, one for each major domain
principle except for KC 1 . KC 1 does not arise in any JS decisions.
In Table IV, the first column lists the row number. The second
column lists the KC on which the induced NormGain policy is specific.
The third and fourth columns show the source training corpus used in
Table III. Pairwise Comparison Among Three Groups On J-ratio
Group 1
NormGain
NormGain
Exploratory

0.82 (0.07)
0.82 (0.07)
0.53 (0.06)

Group 2
Exploratory
DichGain
DichGain

0.53 (0.06)
0.43 (0.07)
0.43 (0.07)

Group 1 vs. Group 2
t(91) = 18.95, p = 0.000
t(64) = 22.85, p = .000
t(99) = 7.894, p = .000

USER658.tex; 26/01/2011; 15:41; p.41

42

Min Chi, Kurt VanLehn, Diane Litman, and Pamela Jordan

Table IV. The Source Training Corpus Of the
Induced 17 NormGain Tutorial Policies

1
2
3
4
5
6
7
8
9

KC1
KC14
KC20
KC21
KC22
KC24
KC27
KC28
KC-general

ET

JS

ΓDichGain
ΓCombine
ΓExploratory
ΓExploratory
ΓExploratory
ΓDichGain
ΓExploratory
ΓDichGain
ΓExploratory

−
ΓExploratory
ΓExploratory
ΓExploratory
ΓExploratory
ΓExploratory
ΓExploratory
ΓDichGain
ΓDichGain

deriving NormGain tutorial policies on ET and JS for corresponding
KCs respectively. For example, the cell “ΓDichGain ” in the first row
shows that the final NormGain ET policy on KC1 is induced by using
ΓDichGain . While the last cell in the first row is empty because KC 1
does not arise in any JS decisions and thus no corresponding policy
was induced.
Table IV shows that all three Corpora were involved in generating
the final tutorial policies. However, the majority of the NormGain
tutorial policies were induced by using ΓExploratory , 11 (5 ET and 6
JS) out of 17; ΓDichGain was used to generate five (3 ET and 2 JS); and
ΓCombine only generated one NormGain policy. It is not very intuitive to
determine why most of the NormGain policies were from ΓExploratory .
Future work is needed to explore the characteristics of a training corpus
and how to choose a training corpus. However, this result reinforces the
importance of collecting an Exploratory Corpus when applying RL to
induce effective pedagogical policies.
8.3.2. Feature Selection
To induce each of the 17 NormGain policies, we applied 12 feature
selection methods on fifty feature choices. It would be interesting to
see which feature selection method(s) found the most final NormGain
tutorial policies. Table V lists all the feature selection methods that
were followed to get the final 17 NormGain tutorial policies. “single”
means it is a single feature policy.
In Table V, the first column lists the row number. For example, the
ninth row shows that the final KC-general NormGain policy on ET

USER658.tex; 26/01/2011; 15:41; p.42

Applying Reinforcement Learning To Pedagogical Strategies

43

decisions is induced through applying “Lower-Bound” feature selection while the KC-general NormGain policy on JS decisions is induced
through applying “ECR” feature selection.
From Table V, it can be concluded that the five feature selection approaches: PCA-only, PCA-ECR, PCA-UpperBound, PCA-LowerBound,
and random did not elicit any of the final tutorial policies. All other
six approaches resulted in at least one. Among them, the two RLbased feature selection methods appeared to be most effective. The
Upper Bound method found five NormGain tutorial policies and the
ECR based method discovered four NormGain tutorial policies.
Previously, we explored four RL-based feature selection methods
on 18 features in [14]. That work also showed that the Upper-Bound
selection seemed to be the best among the four. When inducing NormGain policies, we explored the four RL-based feature selection methods
together with eight other methods on 50 features. Our results here
seems to be consistent with our previous results in [14] in that the
Upper Bound method seems to be an effective feature selection method
in applying RL to induce pedagogical policies.
One possible explanation may be due to our pedagogical goal. In
this project, we were mainly interested in improving student learning.
Generally speaking, learning occurs with low frequency but also with
low risk. In other words, while learning is difficult to achieve, the risk
of not learning is also low (no learning gains). However when learning
does occur, the reward is always positive. Therefore, it is reasonable
for Upper Bound to be a more effective feature selection method since
it can take risks toward selecting features that can cause learning to
occur. In short, this result indicates that in education, features that
may result in higher learning gains should always be considered by the
tutor when making decisions. This is likely due to the fact that in the
worst case a student will simply not learn rather than lose information
so the cost of considering superfluous features is low.
Overall, the results show the relative effectiveness of our twelve feature selection methods in that they at least beat the random feature
selection. However, our feature selection may still need to be improved
because one of the final induced policies is from the PCA-random
feature selection method — the ET NormGain policy on KC27 .

8.3.3. Feature Choices
Table VI summarizes features appearing in each of 17 NormGain policies. The first column lists the row number. The second and third
columns show the corresponding KC and action decisions. The fourth
column lists the number of features involved in the corresponding Nor-

USER658.tex; 26/01/2011; 15:41; p.43

44

Min Chi, Kurt VanLehn, Diane Litman, and Pamela Jordan

Table V. Applying 11 Feature Selection Methods to
Induce 34 Tutorial Policies

1
2
3
4
5
6
7
8
9

KC1
KC14
KC20
KC21
KC22
KC24
KC27
KC28
KC-general

ET

JS

single
single
ECR
Upper Bound
Hedge
ECR
PCA-Random
Upper Bound
Lower Bound

−
single
PCA-Hedge
PCA-Hedge
Upper Bound
Upper Bound
ECR
Upper Bound
ECR

mGain policies and the last column lists all the feature names in the
corresponding policy’s state representation.
For illustration purpose, we used simplified variable names here that
mainly indicate the category of the features. Recall that the six categories are: Autonomy (A) features, background (BG) related feature,
Performance (PM) related features, Problem Solving (PS) Contextual
features, Student Dialogue (SD) features, and Temporal Situation (T)
features.
For example, row 10 is about the JS NormGain policy on KC24 .
Row 10 shows that there are six features, “v14PS v18PS v49BG v59SD
v60SD v62SD”, in the state representation for this policy. Among the
six features, “v14PS” and “v18PS” are Problem Solving Contextual
features since their name ends with “PS”; “v49BG” is a background
feature; and the remaining three features are all Student Dialogue (SD)
features.
From Table VI, we can see that certain state features were involved
in multiple NormGain policies. For example, “v18PS” represents the
state feature StepSimplicityPS. Table VI shows that StepSimplicityPS
appears in 7 NormGain policies as “v18PS” is listed in row 4, 6, 7,
10, 11, 15, and 16. On the other hand, not every feature choice in
Ω occurred in the final 17 induced NormGain policies. For example,
“v44PM” represents the feature pctCorrectKCPM (percentage of correct student entries involving the current KC), which was not involved
in any NormGain Policy. In fact, only 30 out of 50 features occur in at
least one induced NormGain policy.

USER658.tex; 26/01/2011; 15:41; p.44

Applying Reinforcement Learning To Pedagogical Strategies

45

Table VI. Features Involved in 17 NormGain Tutorial Policies

KC
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17

KC1
KC14
KC14
KC20
KC20
KC21
KC21
KC22
KC22
KC24
KC24
KC27
KC27
KC28
KC28
KC − general
KC − general

action

#features

ET
ET
JS
ET
JS
JS
ET
JS
ET
JS
ET
ET
JS
JS
ET
ET
JS

1
1
1
3
5
3
3
5
2
6
4
4
4
3
5
4
5

Features
v24PS
v5T
v12PS
v18PS v23PS v26PS
v9T v15PS v28A v56SD v62SD
v13PS v18PS v20PS
v15PS v18PS v30A
v23PS v24PS v27A v46PM v47PM
v23PS v27A
v14PS v18PS v49BG v59SD v60SD v62SD
v10T v14PS v18PS v55SD
v5T v17PS v23PS v35PM
v9T v25PS v27A v56SD
v15PS v24PS v54SD
v5T v16PS v18PS v29A v41PM
v5T v18PS v27A v46PM
v16PS v17PS v23PS v25PS v27A

By summing column 4 in Table VI, we see that the total number of
feature occurrences across 17 tutorial policies was 59. For illustration
reasons, Table VII lists the number of features defined in each of the six
categories and the feature occurrences in the final 17 NormGain policies. For example, the third and fourth columns in row 4 in Table VII
show that there are fifteen PS features defined and they account for
thirty out of 59 feature occurrences in the final 17 tutorial policies.
In other words, more than half of all feature occurrences in the 17
NormGain policies were from PS.
Table VII shows that both the number of features defined for each
category and the feature occurrences in the six categories is not even.
However, we argue that the uneven distribution of the feature occurrences among the six categories was not caused by the uneven number
of features defined in each category.
Across the fifty features, the most frequent feature appears seven
times. Four features appear in more than three induced policies and
they are:

USER658.tex; 26/01/2011; 15:41; p.45

46

Min Chi, Kurt VanLehn, Diane Litman, and Pamela Jordan

Table VII. Occurrence of Six Category Features in The Final NormGain Tutorial
Policies
Defined Features

Feature Occurrences

1
2
3
4
5
6

Autonomy(A)
Background(BG)
Performance(PM)
Problem Solving Contextual(PS)
Student Dialogue(SD)
Temporal Situation(T)

5
5
12
15
10
3

8
1
5
30
8
7

7

Total

50

59

StepSimplicityPS (7 Occurrences): labeled as “v18PS” in VI. It is a
Problem Solving Contextual feature which encodes a step’s simplicity
level and its value is roughly estimated from the Combined Corpus based
on the percentage of answers that were correct for a step.
TuConceptsToWordsPS (5 Occurrences): labeled as “v23PS” in VI. It
is a Problem Solving Contextual feature which represents the ratio of
physics concepts to words in the tutor’s dialogue.
tellsSinceElicitA (5 Occurrences): labeled as “v27A” in VI. It is an Autonomy feature which represents the number of tells the student has
received since the last elicit.
durationKCBetweenDecisionT (4 Occurrences): labeled as “v5T” in VI.
It is a Temporal Situation feature which represents the time since the
last tutorial decision was made on the current KC.

While StepSimplicityPS can be seen as a domain-oriented feature,
the remaining three features are primarily system-behavior related features. The high occurrence of StepSimplicityPS in the NormGain policies is not very surprising because it is widely believed that difficulty
level is an important factor in a system behaving adaptively and effectively. The frequent involvement of system-behavior related features
in the induced policy may be because these features could reflect a
student’s general aptitude and the degree to which their knowledge on
a specific KC is activated. For example, tellsSinceElicitA reflects how
interactive a student has been recently and durationKCBetweenDecisionT reflects how active a student’s knowledge on the current KC is.
When durationKCBetweenDecisionT is high, it means that the tutor
has not mentioned the KC recently so the student’s knowledge on the
current KC may be less activated.

USER658.tex; 26/01/2011; 15:41; p.46

Applying Reinforcement Learning To Pedagogical Strategies

47

Much to our surprise, the features related to the students’ overall
or recent performance and background (e.g., MSAT, VSAT, gender,
pretest score) appeared the least in the NormGain policies. Row 4 in
Table VII shows that the twelve PM feature choices account for only
five occurrences of this category in the final 17 NormGain policies. They
seem to be less involved even than the Temporal Situation (T) related
features. With only three features in Temporal Situation category, they
account for 7 feature occurrences.
Among the twelve PM features, nIncorrectKCPM (the number of
incorrect responses in the student’s dialogue so far) is the most frequently occurring feature in that it appeared in two final NormGain
tutorial policies. A feature such as pctCorrectKCPM (percentage of
correct student’s entries involving the current KC) did not appear in
any of the final tutorial policies. Additionally, only one out of five
background features occurred in one final tutorial tactic: ageBG** (the
age of the student). The remaining four background features, such as
MSAT, VSAT, gender, pretest score, were not involved in the final 17
NormGain policies.
To summarize, Problem Solving Contextual Features occurred most
frequently, thirty times, in the final 17 induced tutorial policies. The
features related to the students’ overall or recent performance and background appeared the least in the NormGain policies. Although space
does not permit a detailed discussion of the prevalence of features, it
appears to be a mixture of easily anticipated dependencies (e.g., step
simplicity) and a few surprises (e.g., students’ overall and immediate
performances directly reflect their level of knowledge. So why don’t
these factors matter?).

9. Discussion
To summarize, we described a practical methodology for using RL to
improve the effectiveness of an ITS over time. In a nutshell, our RL
methodology is to:
1. Choose an appropriate reward measure for the instructional goals,
an appropriate list of features for the state representations, and
identify a set of reasonable system decisions for which a policy is
needed.
2. Build an initial training system that collects an exploratory dataset
(one that tries many times from each state each of the actions
between which the policy will decide). Despite being exploratory,
this system should still provide the desired basic functionality.

USER658.tex; 26/01/2011; 15:41; p.47

48

Min Chi, Kurt VanLehn, Diane Litman, and Pamela Jordan

3. Apply feature selection methods when necessary, based on the size
of the exploratory corpus, to select a subset of features that capture
the most effective factors in the learning environment. Then we use
the exploratory corpus to build an empirical MDP model for the
subset of state features. The transitions of this MDP model the user
population’s reactions and rewards for the various system action
sequences.
4. Compute the optimal dialogue policy according to this learned
MDP.
5. Add the learned policy to the system and evaluate the policy on a
new group of users.
In this project, we investigated pedagogical skills at a micro-step
level. i.e. pedagogical tutorial tactics. These tactics do not govern the
domain solution path selected for presentation or the problems presented. They only govern low-level tutorial interactions, e.g. whether
the student is told what principle to apply or whether the system elicits
it with a prompt, and whether a student, once he/she has made a step,
is asked to justify his/her answer. If fine-grained pedagogical skills
of this type turn out to be effective, then more complex or contentoriented tactics, such as problem or sub-problem selection may be
similarly effective.
Compared with previous studies on applying RL to induce pedagogical policies for ITSs, this project makes at least three major
contributions. First, we bypassed the need for building simulated students by collecting an exploratory corpus; moreover, we showed that
a relatively small exploratory corpus is enough for inducing effective
policies. Second, we empirically showed that the RL induced policies
indeed help students learn better. Third, while much of the previous
research on applying RL to ITSs used pre-defined state representations,
we defined a large set of features Ω to which several feature-selection
methods were applied to reduce them to a tractable subset.
Moreover, we believe that this project also contributes to applying
RL to induce dialogue policies for dialogue systems. We showed that
using a relatively small exploratory corpus, model-based RL methods
such as Policy Iteration can still induce effective policies even when the
reward function is much delayed and the task domain is rather complex.
We argue that this is done by a rather comprehensive definition of
the state feature choices and exploration of various feature selection
methods.
More specifically, we showed that RL is able to effectively search
a very large continuous space of dialogue policies (after being dis-

USER658.tex; 26/01/2011; 15:41; p.48

Applying Reinforcement Learning To Pedagogical Strategies

49

cretized, the space is ≥ 250 in size) using a relatively small amount
of training dialogue data (64 subjects in the Exploratory group and
37 in the DichGain group). A post-hoc comparison showed that our
learned policy outperformed both sets of training policies in terms of
learning performance. Our results demonstrate that the application of
RL allows one to optimize pedagogical strategies by searching through
a much larger search space than can be explored with more traditional
methods. This success supports the hypothesis that RL-induced rules
are effective and that the approach taken in this project was a feasible
one.
However, inducing effective tutorial policies was not trivial. The
DichGain tutorial policies did not seem to be more effective than the
random decisions in Random-Cordillera. A number of factors were
changed in deriving NormGain policies compared to inducing DichGain policies. These changes included the feature choices, the choice
of training corpora, feature selection methods and the definition of
reward functions. So it is still not clear which factor or factors caused
the change in effectiveness.
We also note that our learned polices made dialogue decisions based
on Problem Solving Contextual features in conjunction with other features such as Autonomy features, and also made very subtle decisions
compared with existing learning theories. As such, our RL-induced
policies are not the standard pedagogical strategies investigated in
the learning literature. For instance, it is widely believed that effective
tutors adapt their behavior to the individual student knowledge level
and incoming competence [70, 17, 40]. Indeed, individualized tutoring
is considered a Grand Challenge by the National Academy of Engineering. However, such features appeared to play little role in the effective
tutorial policies induced from our data. Rather it appears that the
Problem Solving Contextual features are most involved in the final
induced NormGain tutorial policies. Overall it appears that the learning
context features that make the most difference for determining when
to tell vs. elicit and when to Justify vs. Skip-Justify are not always
the ones that first come to mind given current theories of learning and
tutoring. Overall, our results suggest that when building an accurate
learning context model, adding domain-oriented and system behavior
related features would be beneficial.
Finally, our approach has begun to address some of the challenges
of the prevailing theory and application of RL (e.g., balancing the
competing concerns of random exploration with user experience in the
training corpus; keeping the state space as small as possible in order
to make learning data-efficient while retaining enough information for
decision-making; and providing a general methodology for reducing the

USER658.tex; 26/01/2011; 15:41; p.49

50

Min Chi, Kurt VanLehn, Diane Litman, and Pamela Jordan

state space to a manageable size). Our RL approach showed that most
of the NormGain tutorial policies were derived from the Exploratory
Corpus and among the twelve feature selection methods tried in this
project, the two RL-based feature selection methods, Upper-Bound and
ECR, were most effective. However, in order to investigate why these
are the case, we need more exploration. Additionally, one of the issues
we would like to investigate is how different choices of training corpora
or feature selection methods are correlated with learning gains. Also
note that we keep the training material and experimental procedure
identical for all three studies across all three stages, therefore it is not
clear whether the induced policies would still be valid if some aspect of
the training material or experimental procedure changes.
In this project, the induced NormGain policies is at best an approximation, we may be introducing the problem of hidden state or partial
observability into the problem of choosing optimal tutorial actions in
each state. For situations with hidden state a richer Partially observable
Markov decision process (POMDP) model is often more appropriate
[28]. In future work, we wish to explore POMDP as POMDPs allow
for realistic modeling of the student’s knowledge levels, the student’s
intentions, and other hidden state components by incorporating them
into the state space.

Acknowledgements
NSF (#0325054) supported this work. We also thank the Learning
Research and Development Center at the University of Pittsburgh for
providing all the facilities used in this work.

10. Author Biographies
1. Dr. Min Chi
Machine Learning Department, Carnegie Mellon University, 5000
Forbes Ave. Pittsburgh PA, 15213, USA
Dr. Chi is a Post-Doctoral Fellow in the Machine Learning Department and Pittsburgh Science of Learning Center at Carnegie
Mellon University. Dr. Chi received her B.A. degree in Information
Science and Technology from Xi’an Jiaotong University and in 2009
received her Ph.D. degree in Intelligent Systems from the University
of Pittsburgh. Her research focuses on the applications of machine

USER658.tex; 26/01/2011; 15:41; p.50

Applying Reinforcement Learning To Pedagogical Strategies

51

learning techniques to interactive learning environments. In particular, she has focused on applying machine learning techniques to induce pedagogical strategies that can automatically adapt effectively
to differences in students’s performances, the subject matter, and
instructional goals. She is also interested in comparing machinelearned pedagogical strategies to some of the existing learning theories.
2. Dr. Kurt VanLehn
School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, USA
Dr. VanLehn is a Professor of Computer Science at Arizona State
University. He completed his Ph.D. at MIT, did post-doctoral research at Xerox PARC, joined the faculty of Carnegie-Mellon University in 1985, moved to the University of Pittsburgh in 1990 and
joined ASU in 2008. His work involves applications of artificial
intelligence to education, including intelligent tutoring systems,
cognitive modeling and educational data mining.
3. Dr. Diane Litman
Department of Computer Science, 5105 Sennott Square, 210 South
Bouquet Street, University of Pittsburgh Pittsburgh PA, 15260,
USA
Learning Research and Development Center, Room 741, University
of Pittsburgh, 3939 O’Hara St, Pittsburgh PA, 15260, USA
Dr. Diane Litman is presently Professor of Computer Science, Senior Scientist with the Learning Research and Development Center,
and faculty in Intelligent Systems, all at the University of Pittsburgh. Previously she was a member of the Artificial Intelligence
Principles Research Department, AT&T Labs - Research (formerly
Bell Laboratories), and an Assistant Professor of Computer Science
at Columbia University. Dr. Litman received her B.A. degree in
Mathematics and Computer Science from the College of William
and Mary, and her M.S. and Ph.D. degrees in Computer Science
from the University of Rochester. Dr. Litman’s current research
focuses on enhancing the effectiveness of intelligent tutoring systems through spoken language processing, affective computing, and
machine learning.
4. Dr. Pamela W. Jordan
Learning Research and Development Center, University of Pittsburgh, 3939 O’Hara St, Pittsburgh PA, 15260, USA

USER658.tex; 26/01/2011; 15:41; p.51

52

Min Chi, Kurt VanLehn, Diane Litman, and Pamela Jordan

Dr. Jordan is a Research Associate at the University of Pittsburgh,
working in the area of natural language tutorial dialogue. She received her B.S. in Computer Science from the University of Virginia,
M.S. in Computer Science from George Mason University, M.S. in
Computational Linguistics from Carnegie Mellon University and
Ph.D. in Intelligent Systems from University of Pittsburgh. She
completed her Ph.D. under the supervision of Richmond Thomason, in the area of natural language discourse and dialogue. The
joint research described in this volume reflects her interest in building tutorial dialogue systems and understanding how educational
dialogues impact human learning.

References
1.

2.

3.
4.

5.

6.

7.

8.

9.

10.

Hua Ai and Diane J. Litman. Knowledge consistent user simulations for dialog systems. In Proceedings of Interspeech-2007, pages 2697–2700, Antwerp,
Belgium, 2007.
Vincent Aleven, Amy Ogan, Octav Popescu, Cristen Torrey, and Kenneth R.
Koedinger. Evaluating the effectiveness of a tutorial dialogue system for selfexplanation. In Lester et al. [42], pages 443–454.
John R. Anderson. The architecture of cognition. Cambridge, Mass. : Harvard
University Press, 1983.
John R. Anderson, Albert T. Corbett, Kenneth R. Koedinger, and Ray Pelletier. Cognitive tutors: Lessons learned. The Journal of the Learning Sciences,
4(2):167–207, 1995.
Ryan Shaun Baker, Albert T. Corbett, and Kenneth R. Koedinger. Detecting
student misuse of intelligent tutoring systems. In Lester et al. [42], pages
531–540.
Ryan Shaun Baker, Albert T. Corbett, Kenneth R. Koedinger, and Angela Z.
Wagner. Off-task behavior in the cognitive tutor classroom: when students
“game the system”. In Elizabeth Dykstra-Erickson and Manfred Tscheligi,
editors, CHI, pages 383–390. ACM, 2004.
Tiffany Barnes and John C. Stamper. Toward automatic hint generation for
logic proof tutoring using historical student data. In Beverly Park Woolf, Esma
Aı̈meur, Roger Nkambou, and Susanne P. Lajoie, editors, Intelligent Tutoring
Systems, volume 5091 of Lecture Notes in Computer Science, pages 373–382.
Springer, 2008.
Joseph Beck, Beverly Park Woolf, and Carole R. Beal. Advisor: A machine
learning architecture for intelligent tutor construction. In AAAI/IAAI, pages
552–557. AAAI Press / The MIT Press, 2000.
Nielsole Ole Bernsen and Laila Dybkjaer. Designing Interactive Speech Systems: From First Ideas to User Testing. Springer-Verlag New York, Inc.,
Secaucus, NJ, USA, 1997.
Nicoletta Calzolari, Claire Cardie, and Pierre Isabelle, editors. ACL 2006,
21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, Proceedings

USER658.tex; 26/01/2011; 15:41; p.52

Applying Reinforcement Learning To Pedagogical Strategies

11.

12.

13.

14.

15.

16.

17.
18.

19.

20.

21.

22.

23.

24.

53

of the Conference, Sydney, Australia, 17-21 July 2006. The Association for
Computational Linguistics.
Iadine Chadés, Marie-José Cros, Frédérick Garcia, and Régis Sabbadin.
Markov decision process (MDP) toolbox v2.0 for MATLAB.
http://www.inra.fr/internet/Departements/MIA/T/MDPtoolbox, 2005.
Michelene T. H. Chi, Nicholas de Leeuw, Mei-Hung Chiu, and Christian
LaVancher. Eliciting self-explanations improves understanding. Cognitive
Science, 18(3):439–477, 1994.
Min Chi. Do Micro-Level Tutorial Decisions Matter: Applying Reinforcement Learning To Induce Pedagogical Tutorial Tactics. PhD thesis, Intelligent
Systems Program, University of Pittsburgh, Dec. 2009.
Min Chi, Pamela W. Jordan, Kurt VanLehn, and Moses Hall. Reinforcement learning-based feature selection for developing pedagogically effective
tutorial dialogue tactics. In Ryan Shaun Joazeiro de Baker, Tiffany Barnes,
and Joseph E. Beck, editors, The 1st International Conference on Educational Data Mining (EDM), pages 258–265, Montreal, Québec, Canada, 2008.
www.educationaldatamining.org.
Min Chi, Pamela W. Jordan, Kurt VanLehn, and Diane J. Litman. To elicit
or to tell: Does it matter? In Vania Dimitrova, Riichiro Mizoguchi, Benedict
du Boulay, and Arthur C. Graesser, editors, AIED, pages 197–204. IOS Press,
2009.
Min Chi, Kurt VanLehn, Diane J. Litman, and Pamela W. Jordan. Inducing
effective pedagogical strategies using learning context features. In Paul De
Bra, Alfred Kobsa, and David N. Chin, editors, UMAP, volume 6075 of Lecture
Notes in Computer Science, pages 147–158. Springer, 2010.
A. Collins and A. Stevens. Goals and strategies for inquiry teachers. Advances
in Instructional Psychology, 2:65–119, 1982.
Allan Collins, John Seely Brown, and Susan E. Newman. Cognitive apprenticeship: Teaching the craft of reading, writing and mathematics. In L. B.
Resnick, editor, Knowing, learning and instruction: Essays in honor of Robert
Glaser, chapter 14, pages 453–494. Lawrence Erlbaum Associates: Hillsdale
New Jersey, 1989.
Christina Conati and Kurt VanLehn. Toward computer-based support of
meta-cognitive skills: a computational framework to coach self-explanation.
International Journal of Artificial Intelligence in Education, 11:398–415, 2000.
Albert T. Corbett and John R. Anderson. Locus of feedback control in
computer-based tutoring: impact on learning rate, achievement and attitudes.
In CHI, pages 245–252, 2001.
Sidney K. D’Mello, Scotty D. Craig, Amy M. Witherspoon, Bethany McDaniel,
and Arthur C. Graesser. Automatic detection of learner’s affect from conversational cues. User Modeling and User-Adapted Interaction, 18(1-2):45–80,
2008.
Sidney K. D’Mello and Arthur C. Graesser. Multimodal semi-automated affect
detection from conversational cues, gross body language, and facial features.
User Modeling and User-Adapted Interaction, 20(2):147–187, 2010.
Katherine Forbes-Riley, Diane J. Litman, Amruta Purandare, Mihai Rotaru,
and Joel R. Tetreault. Comparing linguistic features for modeling learning in
computer tutoring. In Luckin et al. [45], pages 270–277.
Matthew Frampton and Oliver Lemon. Reinforcement learning of dialogue
strategies using the user’s last dialogue act. In Proceedings of the IJCAI
Workshop on K&R in Practical Dialogue Systems, pages 62–67, 2005.

USER658.tex; 26/01/2011; 15:41; p.53

54
25.

26.

27.

28.

29.

30.

31.

32.

33.

34.

35.
36.

37.

38.

39.

Min Chi, Kurt VanLehn, Diane Litman, and Pamela Jordan

Matthew Frampton and Oliver Lemon. Learning more effective dialogue
strategies using limited dialogue move features. In Calzolari et al. [10], pages
185–192.
Arthur C. Graesser, Natalie K. Person, and Joseph P. Magliano. Collaborative dialog patterns in naturalistic one-on-one tutoring. Applied Cognitive
Psychology, 9(6):495–422, December 1995.
Arthur C. Graesser, Kurt VanLehn, Carolyn Penstein Rosé, Pamela W. Jordan,
and Derek Harter. Intelligent tutoring systems with conversational dialogue.
AI Magazine, 22(4):39–52, 2001.
Milos Hauskrecht. Planning and control in stochastic domains with imperfect
information. PhD thesis, MIT, 1997. Available as Technical Report: MITLCS-TR-738, 1997.
James Henderson, Oliver Lemon, and Kallirroi Georgila. Hybrid reinforcement/supervised learning for dialogue policies from communicator data. In
IJCAI Workshop on K&R in Practical Dialogue Systems, pages 68–75, 2005.
Ana Iglesias, Paloma Martı́nez, Ricardo Aler, and Fernando Fernández.
Learning teaching strategies in an adaptive and intelligent educational system through reinforcement learning. Applied Intelligence, 31:89–106, 2009.
10.1007/s10489-008-0115-1.
Ana Iglesias, Paloma Martı́nez, Ricardo Aler, and Fernando Fernández. Reinforcement learning of pedagogical policies in adaptive and intelligent educational systems. Knowledge-Based Systems, 22(4):266–270, 2009. Artificial
Intelligence (AI) in Blended Learning - (AI) in Blended Learning.
Ana Iglesias, Paloma Martı́nez, and Fernando Fernández. An experience applying reinforcement learning in a web-based adaptive and intelligent educational
system. Informatics in Education, 2(2):223–240, 2003.
Mitsuru Ikeda, Kevin D. Ashley, and Tak-Wai Chan, editors. Intelligent Tutoring Systems, 8th International Conference, ITS 2006, Jhongli, Taiwan, June
26-30, 2006, Proceedings, volume 4053 of Lecture Notes in Computer Science.
Springer, 2006.
Srinivasan Janarthanam and Oliver Lemon. User simulations for online
adaptation and knowledge-alignment in troubleshooting dialogue systems. In
Proceedings of LonDial the 12th SEMdial Workshop on on the Semantics and
Pragmatics of Dialogues., pages 51–58, Stockholm, 2008.
I. T. Jolliffee. Principal Component Analysis. Springer Series in Statistics.
Springer, New York, 2nd edition, 2002.
Pamela W. Jordan, Brian Hall, Michael Ringenberg, Yui Cue, and Carolyn
Rosé. Tools for authoring a dialogue agent that participates in learning studies.
In Luckin et al. [45], pages 43–50.
Pamela W. Jordan, Michael A. Ringenberg, and Brian Hall.
Rapidly
developing dialogue systems that support learning studies.
In ITS06
Workshop on Teaching with Robots, Agents and NLP, available
http://facweb.cs.depaul.edu/elulis/ITS2006RobotsAgentsWorkshop.html,
pages 29–36, 2006.
Leslie Pack Kaelbling, Michael L. Littman, and Andrew W. Moore. Reinforcement learning: A survey. Journal of Artificial Intelligence Research, 4:237–285,
1996.
Sandra Katz, Gabriel O’Donnell, and Heater Kay. An approach to analyzing
the role and structure of reflective dialogue. International Journal of Artificial
Intelligence and Education, 11(3):320–343, 2000.

USER658.tex; 26/01/2011; 15:41; p.54

Applying Reinforcement Learning To Pedagogical Strategies

40.

41.

42.

43.

44.

45.

46.

47.
48.

49.
50.

51.
52.

53.

54.

55.
56.

55

Kenneth R. Koedinger and Vincent Aleven.
Exploring the assistance
dilemma in experiments with cognitive tutors. Educational Psychology Review,
19(3):239–264, 2007.
Kenneth R. Koedinger, John R. Anderson, William H. Hadley, and Mary A.
Mark. Intelligent tutoring goes to school in the big city. International Journal
of Artificial Intelligence in Education, 8(1):30–43, 1997.
James C. Lester, Rosa Maria Vicari, and Fábio Paraguaçu, editors. Intelligent
Tutoring Systems, 7th International Conference, ITS 2004, volume 3220 of Lecture Notes in Computer Science, Maceiò, Alagoas, Brazil, August 30-September
3 2004. Springer.
Esther Levin and Roberto Pieraccini. A stochastic model of computer-human
interaction for learning dialogue strategies. In In EUROSPEECH 97, pages
1883–1886, 1997.
Diane J. Litman and Scott Silliman. Itspoke: an intelligent tutoring spoken
dialogue system. In Demonstration Papers at HLT-NAACL 2004, pages 5–8,
Morristown, NJ, USA, 2004. Association for Computational Linguistics.
Rosemary Luckin, Kenneth R. Koedinger, and Jim E. Greer, editors. Artificial Intelligence in Education, Building Technology Rich Learning Contexts
That Work, Proceedings of the 13th International Conference on Artificial
Intelligence in Education, AIED 2007, volume 158 of Frontiers in Artificial
Intelligence and Applications, Los Angeles, California, USA, July 9-13 2007.
IOS Press.
Kimberly N. Martin and Ivon Arroyo. Agentx: Using reinforcement learning to
improve the effectiveness of intelligent tutoring systems. In Lester et al. [42],
pages 564–572.
Jean McKendree. Effective feedback content for tutoring complex skills.
Human-Computer Interaction, 5(4):381–413, December 1990.
Johanna D. Moore, Kaska Porayska-Pomsta, Sebastian Varges, and Claus Zinn.
Generating tutorial feedback with affect. In Valerie Barr and Zdravko Markov,
editors, FLAIRS Conference, pages 923–928. AAAI Press, 2004.
Allen Newell. Unified Theories of Cognition. Harvard University Press; Reprint
edition, 1994.
Tim Paek and David Chickering. The Markov assumption in spoken dialogue
management. In 6th SIGDial Workshop on Discourse and Dialogue, pages
35–44, 2005.
Helen Pain and Kaska Porayska-Pomsta. Affect in one-to-one tutoring. In
Ikeda et al. [33], pages 817–817.
Pipatsarun Phobun and Jiracha Vicheanpanya. Adaptive intelligent tutoring
systems for e-learning systems. Procedia - Social and Behavioral Sciences,
2(2):4064 – 4069, 2010. Innovation and Creativity in Education.
Kaska Porayska-Pomsta, Manolis Mavrikis, and Helen Pain. Diagnosing and
acting on student affect: the tutor’s perspective. User Modeling and UserAdapted Interaction, 18(1-2):125–173, 2008.
Antoine Raux, Brian Langner, Dan Bohus, Alan W. Black, and Maxine Eskenazi. Let’s go public! taking a spoken dialog system to the real world. In
Proceedings of Interspeech (Eurospeech), pages 885–888, Lisbon Portugal, 2005.
Verena Rieser and Oliver Lemon. Using machine learning to explore human
multimodal clarification strategies. In Calzolari et al. [10], pages 659–666.
Michael A. Ringenberg and Kurt VanLehn. Scaffolding problem solving with
annotated, worked-out examples to promote deep learning. In Ikeda et al. [33],
pages 625–634.

USER658.tex; 26/01/2011; 15:41; p.55

56
57.

58.

59.

60.

61.
62.

63.

64.

65.

66.
67.

68.

69.

70.

Min Chi, Kurt VanLehn, Diane Litman, and Pamela Jordan

A. Rudnicky, E. Thayer, P. Constantinides, C. Tchou, R. Shern, K. Lenzo,
W. Xu, and A. Oh. Creating natural dialogs in the Carnegie Mellon communicator system. In Proceedings of Eurospeech, volume 4, pages 1531–1534,
1999.
Satinder P. Singh, Michael J. Kearns, Diane J. Litman, and Marilyn A. Walker.
Reinforcement learning for spoken dialogue systems. In Sara A. Solla, Todd K.
Leen, and Klaus-Robert Müller, editors, NIPS, pages 956–962. The MIT Press,
1999.
Satinder P. Singh, Diane J. Litman, Michael J. Kearns, and Marilyn A. Walker.
Optimizing dialogue management with reinforcement learning: Experiments
with the njfun system. Journal of Aritificial Intelligence Research (JAIR),
16:105–133, 2002.
John C. Stamper, Tiffany Barnes, and Marvin J. Croy. Extracting student
models for intelligent tutoring systems. In AAAI, pages 1900–1901, Vancouver,
British Columbia, Canada, July 22-26 2007. AAAI Press.
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning. MIT Press
Bradford Books, 1998.
Joel R. Tetreault, Dan Bohus, and Diane J. Litman. Estimating the reliability
of mdp policies: a confidence interval approach. In Candace L. Sidner, Tanja
Schultz, Matthew Stone, and ChengXiang Zhai, editors, HLT-NAACL, pages
276–283. The Association for Computational Linguistics, 2007.
Joel R. Tetreault and Diane J. Litman. Comparing the utility of state features
in spoken dialogue using reinforcement learning. In Robert C. Moore, Jeff A.
Bilmes, Jennifer Chu-Carroll, and Mark Sanderson, editors, Proceedings of
the Human Language Technology Conference of the NAACL, Main Conference, pages 272–279, New York, NY, USA, June 2006. The Association for
Computational Linguistics.
Joel R. Tetreault and Diane J. Litman. Using reinforcement learning to build a
better model of dialogue state. In Proceedings 11th Conference of the European
Chapter of the Association for Computational Linguistics (EACL), pages 289–
296, Trento, Italy, 2006.
Joel R. Tetreault and Diane J. Litman. A reinforcement learning approach to evaluating state representations in spoken dialogue systems. Speech
Communication, 50(8-9):683–696, 2008.
Kurt VanLehn. The behavior of tutoring systems. International Journal
Artificial Intelligence in Education, 16(3):227–265, 2006.
Kurt VanLehn, Arthur C. Graesser, G. Tanner Jackson, Pamela W. Jordan,
Andrew Olney, and Carolyn Penstein Rosé. When are tutorial dialogues more
effective than reading? Cognitive Science, 31(1):3–62, 2007.
Kurt VanLehn, Pamela Jordan, and Diane Litman. Developing pedagogically
effective tutorial dialogue tactics: Experiments and a testbed. In Proceedings
of SLaTE Workshop on Speech and Language Technology in Education ISCA
Tutorial and Research Workshop, pages 17–20, 2007.
Kurt VanLehn, Collin Lynch, Kay Schulze, Joel A. Shapiro, Robert Shelby,
Linwood Taylor, Don Treacy, Anders Weinstein, and Mary Wintersgill. The
andes physics tutoring system: Lessons learned. International Journal Artificial
Intelligence in Education, 15(3):147–204, 2005.
Lev S. Vygotsky. Interaction between learning and development. In Mind
and Society, pages 79–91. Harvard University Press, Cambridge Massachusetts,
1978.

USER658.tex; 26/01/2011; 15:41; p.56

Applying Reinforcement Learning To Pedagogical Strategies

71.

72.

73.

74.

75.

57

Marilyn A. Walker. An application of reinforcement learning to dialogue strategy selection in a spoken dialogue system for email. Journal of Aritificial
Intelligence Research, 12:387–416, 2000.
Jason D. Williams, Pascal Poupart, and Steve J. Young. Factored partially observable markov decision processes for dialogue management. In 4th Workshop
on Knowledge and Reasoning in Practical Dialog Systems, International Joint
Conference on Artifiical Intelligence (IJCAI), pages 76–82, Edinburgh, 2005.
Jason D. Williams and Steve Young. Partially observable markov decision processes for spoken dialog systems. Computer Speech and Language,
21(2):231–422, April 2007.
Jason D. Williams and Steve Young. Scaling POMDPs for spoken dialog
management. IEEE Transactions on Audio, Speech, and Language Processing.,
15(7):2116–2129, September 2007.
Ruth Wylie, Kenneth Koedinger, and Teruko Mitamura. Is self-explanation
always better? the effects of adding self-explanation prompts to an english
grammar tutor. In Proceedings of the 31st Annual Conference of the Cognitive Science Society, COGSCI 2009, pages 1300–1305, Amsterdam, The
Netherlands, 2009.

USER658.tex; 26/01/2011; 15:41; p.57

USER658.tex; 26/01/2011; 15:41; p.58

Interactive Learning Environments, 2013
Vol. 21, No. 4, 371–413, http://dx.doi.org/10.1080/10494820.2013.803125

Model construction as a learning activity: a design space and review

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

Kurt VanLehn*
Computing, Informatics and Decision Science Engineering, Arizona State University, Tempe,
AZ 85284, USA
(Received 20 February 2013; ﬁnal version received 3 May 2013)
Modeling is becoming increasingly important both as a way to learn science and
mathematics, and as a useful cognitive skill. Although many learning activities
qualify as “modeling”, this article focuses on activities where (1) students construct a
model rather than explore a given model, (2) the model is expressed in a formal
language rather than drawings, physical objects or natural language texts and (3) the
model’s predictions are generated by executing it on a computer. Most research on
such learning activities has focused on getting students to successfully construct
models, which they ﬁnd very difﬁcult to do. In the hope that new research can ﬁnd
ways to remove this bottleneck, this article attempts to list all the major ideas that
have appeared in the literature and might be useful to those developing new learning
activities involving model construction. The ideas are organized into a design space
with ﬁve dimensions: (1) modeling language types, (2) ways for describing the
systems that students should model, (3) instructional objectives and their
corresponding assessments, (4) common student difﬁculties and (5) types of scaffolding.
Keywords: modeling; model construction; scaffolding; interactive
environments; interactive learning activities; constructive learning

learning

1. Introduction
The literature on educational uses of modeling includes several recent reviews (Clariana &
Strobel, 2007; Clark, Nelson, Sengupta, & D’Angelo, 2009; Doerr, 1996; Hopper & Stave,
2008; Jacobson & Wilensky, 2006; Jonassen & Strobel, 2006; de Jong & van Joolingen,
1998; Löhner, 2005; Penner, 2001; Stratford, 1997). Although this article is a review in
that it presents no new work, it is not intended to be an exhaustive list of relevant
studies. Instead, it is intended to be an exhaustive list of the major ideas that have appeared
in the literature and might be useful to new research. The ideas are organized into a ﬁvedimensional design space. That is, the discussion separately addresses ﬁve questions that
must be addressed in designing and evaluating instruction that uses model construction:
(1)
(2)
(3)
(4)

What type of model should I have students construct?
How can students ﬁnd out about the systems they will model?
How can students’ learning be assessed?
What difﬁculties in learning can be anticipated?

*Email: kurt.vanlehn@asu.edu
© 2013 Taylor & Francis

372

K. VanLehn
(5) What scaffolding can be used to help students learn while modeling?

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

This organization was chosen simply to encourage more studies of model construction.
As argued in the next section, model construction is becoming increasingly prominent as an
instructional objective in science, mathematics and engineering, but we have still not found
efﬁcient, effective methods for teaching model construction. As this review demonstrates,
the design space of model construction activities is vast and relatively unexplored, so
further experimentation may ﬁnd instructional methods that work better than those that
have been studied so far.
2. Model construction is important
Model construction has been important and ubiquitous in recent US standards for K-12
science, mathematics and engineering. Standards for science instruction (National Research
Council, 2012) contain over 350 occurrences of “model” and “modeling”. The report has
only seven strands that are threaded throughout the standards, and model construction is
one of them. There are sections in the report devoted exclusively to deﬁning model construction and describing standards for proﬁciency in modeling. In many of the report’s
exemplary learning progressions, modeling activities are mentioned explicitly.
In the Common Core State Mathematics Standards (CCSSO, 2011), modeling is one of
only seven mathematical practices. Unlike the content topics (e.g. fraction addition and the
Pythagorean theorem), modeling is not allocated a standard of its own but is instead indicated by placing stars on content topics taught at the high-school level where modeling
should be applied. For example, there is a star on “Create equations and inequalities in
one variable and use them to solve problems”, whereas there is no star on “Solve
systems of linear equations exactly and approximately (e.g. with graphs)”. Of the 117
high school topics, 45% involved modeling.
Although there currently are no standards for K-12 engineering, a National Academy of
Engineering committee studying K-12 engineering education highlighted the importance of
modeling in such instruction when they said, “Mathematical analysis and modeling are
essential to engineering design…” (Katehi, Pearson, & Feder, 2008, p. 25).
Several theoretical frameworks suggest that modeling is important for achieving deep
understanding. For instance, Penner (2001) argues that constructivism strongly implies
that model construction will be effective if not essential to understanding science.
Studies of professional and/or highly educated decision-makers suggest that they need
to learn to model systems. In particular, their informed, experienced judgments were not as
accurate as simple models (Booth Sweeney & Sterman, 2000; Kainz & Ossimitz, 2002;
Moxnes, 2000).
3. Model construction vs. other modeling activities
In the science, mathematics and engineering community, the term “modeling” usually
means constructing an expression in a formal language that denotes properties of a given
system. Often the formal language is mathematical equations, a computer programming
language or a mixture of the two languages. The process of model construction involves
not only constructing an initial model, but also testing its predictions against the thing
being modeled, which will be called “the system”. If the predictions are inaccurate, the
model must be revised, which is often called “debugging” the model. For the class of
models considered here (which will be deﬁned more precisely later), generating the

Interactive Learning Environments

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

Figure 1.

373

Modeling is the process of constructing and debugging a model (CCSSO, 2011).

model’s predictions involves some kind of numeric or symbolic computation that can be
done by a computer. Thus, the overall process is a cycle of several subprocesses, as
shown in Figure 1, which is borrowed from the Common Core Mathematics Standards
(CCSSO, 2011). In short, in the professions, “modeling” usually means “model
construction”.
In the science education community, the term “modeling” is often used more broadly to
encompass any educational activity that involves a model. A particularly common activity
is to give students a model of a system and ask them to understand it. Students control the
model by manipulating sliders or other controls, and they observe its behavior via gauges,
graphs, animations or other displays. Often this activity is called model exploration.
The activities of model construction and model exploration are very different (Alessi,
2000b). Model construction requires knowing the modeling language and executing the
processes shown in Figure 1. Model exploration requires neither knowing the
modeling language nor executing the problem solving process of Figure 1. Consequently,
model-construction activities have a steeper entry cost than model-exploration activities,
because they require learning the modeling language and the skills involved in the cycle
of Figure 1.
One would hope that that this extra cost is paid back with larger learning gains, and
there is some evidence of this. Hashem and Mioduser (2010, 2011) compared four instructional activities involving NetLogo models: one was model construction and the other three
were different types of model exploration. The model-exploration groups had 2 hours of
training, whereas the model construction group required 48 hours of training in order to
gain sufﬁcient skill in NetLogo programming. As expected, the model-construction
group scored higher on the post-test as well on measures of post-intervention interviews.
Although NetLogo is much more complicated than most other modeling languages and
thus took a long time to learn, the experiment demonstrates some extra beneﬁt for the
extra cost.
The rest of this article describes a design space for learning activities based on model
construction. The intention is to list major ideas that have appeared in the literature and
might be usefully combined or reworked to form new instructional activities. The ideas
are organized into ﬁve more-or-less independent dimensions, each discussed in its own
section (Table 1).
4. Modeling languages
Because model construction is so different from model exploration, this review covers only
model-construction activities. Such activities can be classiﬁed along two dimensions: (1)
the type of model being constructed by the students and (2) the way the system is presented
to them. This section considers model types; the following section discusses methods for
presenting systems.

374

K. VanLehn

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

Table 1. A design space for learning activities involving model construction.
1. Selected modeling languages
a. Constraints on system states
b. System dynamics
c. Agent-based
2. Methods for presenting systems
a. Succinct text
b. Resources
c. Simulation
d. Virtual labs and virtual ﬁeld studies
e. Real labs and real ﬁeld studies
3. Instructional objectives and assessments
a. Improved model-construction skill
b. Improved domain knowledge
c. Improved understanding of a particular system
d. Concept inventories and mental modeling
e. Understanding the role of models
f. Less easily assessed objectives
4. Common student difﬁculties
a. Poor problem solving strategies
b. Difﬁculties understanding the modeling language
c. Difﬁculties understanding the presentation
5. Types of Scaffolding
a. Tutoring
i. Feedback/hints on the model
ii. Feedback/hints on the student’s process (meta-tutoring)
iii. Concrete articulation strategy
iv. Decomposition into subsystems
v. Reﬂective debrieﬁngs
vi. Answering student questions
b. Clarifying the modeling language
i. Notation for models
ii. Grounding the symbols
iii. Comparing predictions to system’s behavior
iv. Students explaining their model
c. Gradually increasing complexity
i. Hiding model features
ii. Qualitative model construction as scaffolding
iii. Model progressions
d. Other scaffolding
i. Teachable agents and reciprocal teaching
ii. Mental execution of models
iii. Test suites
iv. Generic schemas
v. Gamiﬁcation

Several frameworks for deﬁning and classifying types of models have been proposed
(Clariana & Strobel, 2007; Harrison & Treagust, 2000; Hestenes, 2007; Stratford, 1997).
Collins and Ferguson (1993) developed a particularly exhaustive taxonomy of “epistemic
forms”. An epistemic form is a language or syntax for writing models, and it is not a
model itself or a theory. Although “epistemic form” is an admirably precise term, it did
not catch on. Thus, “modeling language” will be used here in place of “epistemic form”.
Only some of the modeling languages are used in school science classes (Harrison & Treagust, 2000).

Interactive Learning Environments

375

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

Table 2. A taxonomy of modeling languages from Collins and Ferguson (1993).
Structural analyses: Decompose the system into parts and describe relationships among parts
• Spatial decompositions, e.g. anatomical diagrams or circuit diagrams.
• Temporal decompositions or stage models, e.g. a series of states or phases.
• Compare and contrast listings, e.g. of the solar system versus the Bohr atom
• Cost-beneﬁt analysis. Comparing and contrasting two economic alternatives.
• Primitive-elements game, e.g. how quarks etc. combine to make protons, electrons, etc.
• Cross-product or table game, e.g. the periodic table of elements.
• Tree-structure or hierarchy game, e.g. biological taxonomies
• Axiom systems, e.g. Euclid’s geometry or Peano’s arithmetic.
Functional analyses: Determine the causal or functional relationships among elements of a system
• Critical-event analysis, e.g. of an airplane crash or the invention of the printing press.
• Cause-and-effect analysis. A sequence of events, each causing the ones after it.
• Problem-centered analysis. A problem’s solution has side effects which pose problems that…
• Multicausal analysis or AND/OR graphs.
• Form-and-function analysis.
Behavior analyses: Describe the dynamic behavior or process of the system.
♥ Constraint systems. They determine the space of possible states of the system.
♥ System-dynamics models. They determine how the system changes over time.
♥ Aggregate-behavior models, e.g. of slime molds. Often called emergent or agent-based.
• Situation-action models, e.g. production systems, Markov models, ﬁnite state transition nets
• Trend and cyclical analyses, e.g. ﬁnding leading indicators for picking stocks

Table 2 shows the Collins and Ferguson taxonomy of modeling languages. Collins and
Ferguson divide modeling languages into those for analyzing the structure, function and
behavior of systems. The structure of a system is like its anatomy; structure models describe
the parts of a system and their relationships. The behavior of a system is how it changes
over time. The function of a system refers to the purpose of the system and how that
purpose is achieved. Although there are other ways to divide up the analyses of systems
– for example, de Kleer and Brown (1981) use just structure and function, and Weld
(1983) uses role, function, structure and mechanism – this three-way division of structure,
behavior and function is often found in the literature (Erden et al., 2008; Goel, Rugaber, &
Vattam, 2009; Hmelo-Silver & Pfeffer, 2004).
This review will address only a small number of the languages shown, namely three of
the modeling languages used for analyzing the behavior of systems. These languages are
marked in two with a ♥ bullet. Moreover, only executable languages will be reviewed.
That is, after the user has constructed a model in the language, the user presses a “run”
button and the modeling software calculates predictions about the behavior of the
system. Because these languages are all similar to a degree, there is a chance that this
review can ﬁnd commonalities in their instructional objectives, assessments, obstacles
and scaffolding. Now let us consider in more detail each of the three modeling language
types to be reviewed.

4.1

Constraint systems

Constraint systems predict the possible states of the behavior of a system. Thus, if one quantity of the model is perturbed, a constraint system predicts how the rest of the system must
change in order to remain in a legal state. The model consists of a set of variables and a set
of constraints on the values of those variables. The constraints are usually mathematical
equations or a set of qualitative equations. If there are N variables, then the equations determine which points in the N-dimensional space correspond to possible system states.

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

376

K. VanLehn

If one of the variables represents time and the equations are temporal differential or
difference equations, then the model is classiﬁed as a system-dynamics model rather
than a constraint system. System-dynamics models are described later in this section.
For quantitative constraint systems, several instructional systems have been built for
algebraic model construction (McArthur et al., 1989) and arithmetic model construction
(Marshall, Barthuli, Brewer, & Rose, 1989). They usually have students create graphs and
tables, then draft equations which are then solved by the system or by the student. When a
tutoring system is involved, it monitors the student’s progress in developing the equations,
offers feedback and hints when asked, and offers help when it detects that the student is ﬂoundering. When no tutoring system is involved, the students typically enter equations into a
spreadsheet or a computer algebra system such as MatLab or Mathematica. The program
solves the equations, allowing the student to focus on constructing the appropriate equations.
For qualitative constraint systems, node-link diagrams are used to represent models in
most languages, including these major ones:
.
.

Betty’s Brain (Leelawong & Biswas, 2008),
Garp and DynaLearn (Beek, Bredeweg, & Lautour, 2011; Bredeweg, Liem, Beek,
Salles, & Linnebank, 2010; Bredeweg, Linnebank, Bouwer, & Liem, 2009),

Figure 2. A model and query from Betty’s Brain. Reprinted from Leelawong and Biswas (2008),
with permission of IOS Press.

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

Interactive Learning Environments

Figure 3.
.
.

377

A concept map. Reprinted from Novak and Canas (2008), with permission of the authors.

IQON and VISQ (Miller et al., 1993) and
Vmodel (Forbus, Carney, Sherin, & Ureel Il, 2005).

Figure 2 shows a qualitative diagram drawn by a user of Betty’s Brain (Schwartz et al.
2009). The ++ label indicates that the two variables are directly related (as one increases,
so does the other), while a – – label indicates that they are inversely related (as one
increases, the other decreases). In Figure 2, the user is asking what happens to one variable
when another variables’ value is changed. This is one way to access the predictions of the
model. Another is to have the software report all the variables whose values are increased or
decreased when a certain variable’s value is modiﬁed. Some qualitative diagram languages
have other labels than + and −, and can do other types of predictions as well.
Qualitative diagrams should not be confused with concept maps even though they look
similar. A typical concept map language allows the user to enter anything on the links, as
shown in Figure 3. A concept map is essentially a collection of propositions such as
“Seasons are determined by amount of sunlight”. A concept map is a model, but it is not
an executable model. That is, there is no way for the concept map itself to answer questions
such as “what would happen if the tilt of the earth’s axis was zero?” Activities where student
construct a concept map are excluded from this review.
4.2 System-dynamics modeling languages
A system-dynamics model predicts the behavior of a system over time. A set of temporal
differential equations can be used as a system-dynamics model. However, for students who

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

378

K. VanLehn

Figure 4. A system-dynamics diagram. Working counterclockwise from the top, the equations inside
each rectangle and circle are as follows: Charget = Charget-1+It; Vct = Charget/C; Vft = S-Vct; It = Vft/
R; R = 1/(1/R1 + 1/R2). Reprinted from Mulder et al. (2010), with permission of Taylor & Francis Ltd.

are not familiar with calculus, a common system-dynamics modeling language is a quantitative diagram. A quantitative diagram summarizes a system of equations as a node-link
diagram. Typically, each node represents both a variable and a function for computing its
value. The inputs to the function are represented as incoming links. To avoid cluttering
the diagram, the functions can be seen only by clicking on the node. Figure 4 shows an
example from Co-Lab (Mulder, Lazonder, & de Jong, 2010). It represents time discretely
rather than continuously as differential equations would because that makes the mathematics accessible to students who have not taken calculus. There are several general purpose,
industrial strength tools for drawing such quantitative diagrams, including Stella (http://
www.iseesystems.com/), Powersim (http://www.powersim.com/), Dynasys (http://www.
hupfeld-software.de/pmwiki/pmwiki.php) and Vensim (http://www.vensim.com/). Tools
intended primarily for education include:
.
.
.

Modellingspace (Avouris, Margaritis, Komis, Saez, & Melendez, 2003),
CoLab (van Joolingen, De Jong, Lazonder, Savelsbergh, & Manlove, 2005) and
Model-It (Metcalf, Krajcik, & Soloway, 2000).

Although diagrams work well for small models, it can be difﬁcult to follow links when the
diagrams become more complex. However, system-dynamics models can be written as text,
and this may make them easier to understand when there are a large number of variables.
The caption of Figure 4 shows how its diagram can be expressed as a set of equations. Modellus is modeling tool based on equations as the model representation (Teodoro & Neves,
2011). Qualitative diagrams can also be expressed as a set of qualitative constraints
expressed as text.
4.3 Agent-based modeling languages
An agent-based model is for modeling the emergent behavior of systems. The model consists of a large number of agents. Each agent is simultaneously executing a simple program.

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

Interactive Learning Environments

Figure 5.

379

A NetLogo model of slime mold growth.

The program is expressed in a computer programming language, such as Logo or Java.
Some modeling languages are:
.
.
.
.
.

NetLogo (http://ccl.northwestern.edu/netlogo/)
AgentSheets (Repenning, Ioannidou, & Zola, 2000)
Worldmaker (Boohan, 1995).
OOTLS (Neumann, Feurzeig, & Garik, 1999)
CTiM (Basu et al., 2012).

Different types of agents run different programs. For instance, Figure 5 shows a
NetLogo model of slime molds. The left panel shows the interface, which has some
buttons and sliders for controlling values inside the program and an animation of the
agents. The right panel shows the code for the agents’ programs. Some agents represent
slime molds, and they run one program. Other agents represent patches that the slime
molds move across and deposit chemicals on; the ground agents run a different program
from the slime mold agents. At this writing, NetLogo is the most widely used agentbased modeling language for education.
Agent-based modeling overlaps with system-dynamics modeling in that both languages
can be used to model dynamic systems. For instance, a predator-prey ecosystem can be
easily modeled in both NetLogo and Stella. One study (Thompson & Reimann, 2010)
has compared these two modeling paradigms, but the small sample size and relatively
short period of the instruction prevented drawing any conclusions about their relative
effectiveness.

5.

Methods for presenting systems

Although the modeling language has a major impact on the students’ thinking and learning,
the method of presenting the system to be modeled is also important. Here are ﬁve major
ways to present systems to the students:
.

Succinct text: The system is described in text that succinctly mentions most of the
relevant entities and relationships, and seldom mentions irrelevant ones. For instance,

380

K. VanLehn

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

.

.

.

.

one might say, “In 2010, the population of Phoenix was 1,445,632 and was increasing
at 9.4% per year. Construct a model that predicts the population from 2010 to 2040.”
Resources: The student is given a task, such as “Predict the population of Phoenix in
from 2012 to 2040”, and a set of resources comprising text, tables, charts, pictures,
videos, diagrams, etc. The resources contain the relevant information about the
system along with irrelevant information. Examples of this mode of presentation
are the studies of Betty’s Brain (Biswas, Jeong, Kinnebrew, Sulcer, & Roscoe,
2010; Leelawong & Biswas, 2008; Schwartz, Blair, Biswas, Leelawong, & Davis,
2008; Schwartz et al., 2009).
Simulation: The student is given a simulation without being able to view the computation that drives it. In most cases, the student can control the simulation. For
instance, the student could be given only the interface to the slime mold simulation,
shown on the left of Figure 5 which can be controlled with the buttons and sliders.
Examples of this method of presentation are studies using CoLab (Basu et al.,
2012; Bravo, van Joolingen, & de Jong, 2009; van Joolingen et al., 2005; Löhner,
Van Joolingen, & Savelsbergh, 2003; Löhner, Van Joolingen, Savelsbergh, & Van
Hout-Wolters, 2005).
Virtual labs and virtual ﬁeld studies: The student is given a simulation of a laboratory
with appropriate apparatus and supplies or is allowed to explore a virtual world using
appropriate instruments. The students’ task is to set up experiments and run them. For
instance, to study slime mold growth, the student would have to manipulate simulated
Petri dishes, pipettes and photographic measurement tools. Although such virtual laboratories exists (Yaron, Karabinos, Lange, Greeno, & Leinhardt, 2010), it seems that
they have not yet been used as a presentation method for model-construction
activities.
Real labs and real ﬁeld studies: Although the model is constructed on a computer, the
system is presented in reality, such as visiting a stream and taking water samples, then
measuring pH with the litmus paper. Examples of this method of presentation are
(Metcalf et al., 2000).

For every choice of presentation type and every choice of modeling language, there is a
feasible model-construction activity. For instance, if we choose “simulation” as the method
for presenting the system and “system dynamics” as the modeling language, then we have a
learning activity where students can manipulate the simulation in order to understand how it
works, then construct a system-dynamics diagram. The student’s goal is to create a model
whose predictions match the behavior of the simulation. This is the activity most frequently
done with Co-Lab (van Joolingen et al., 2005).
Mathematical “word problems” correspond to activities where the presentation of the
system is succinct text and the modeling language is systems of equations. However,
these would be word problems done with a computer system where students enter the
equations into a computer (including calculators) and the computer solves them. Many students ﬁnd numerical answers for word problems using informal strategies that do not
involve writing equations (Koedinger, Alibali, & Nathan, 2008; Koedinger & Nathan,
2004), so they are not doing model construction.
6. Instructional objectives and assessments
Many claims have been made about the instructional beneﬁts of model construction, and
several types of assessment have been used for testing these claims. The subsections that

Interactive Learning Environments

381

follow each discuss a common instructional objective and its assessment. A ﬁnal section
brieﬂy mentions other beneﬁts of model construction that are not easily tested with
assessments.

Improved model-construction skill

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

6.1

Model-construction skill is operational knowledge about the modeling language, the system
presentation and the method for getting from the presentation to the model. This is a kind
of “sense making” (Quintana et al., 2004) or “scientiﬁc reasoning”. For instance, if the presentation method involves experimentation, then the control of variable strategy (“vary one
thing at a time”) is part of model-construction skill. If the experimental observations have a
chance of error, then model-construction skill includes knowing that one should do repeated
tests or samples. If the modeling language is systems of equations, then model-construction
skill includes knowing that when the number of equations is the same as the number variables,
then the solution may be a single point. If a qualitative diagram has a multi-link path between
two variables, then model-construction skill should include knowing how to determine from
the + and – labels on the links whether the variables are directly or inversely related.
The most common procedure for assessing model-construction skill uses the same
system presentation method, the same modeling language and different domain knowledge
and different systems from the ones used during instruction. For instance, several studies of
Betty’s Brain (Biswas, Leelawong, Schwartz, & Vye, 2005; Biswas et al., 2010; Leelawong
& Biswas, 2008) trained students by having them create models of the oxygen cycle in an
water-based ecosystem and then assessed them by having them create models of the nitrogen cycle in a land-based ecosystem. In both activities, the systems were presented with
resources and the modeling language was the qualitative diagram language shown in
Figure 2.
A less common assessment of model-construction skill involves varying the modeling
language. For instance, van Borkulo, van Joolingen, Savelsbergh, and de Jong (2012)
trained students in using the quantitative modeling language shown in Figure 4, then
tested them using a qualitative modeling language similar to the one shown in Figure 2.
This could be a considered a far transfer test of model-construction skill.
Two dependent measures are commonly used:
.
.

Product: One measures the quality of the models that the students’ construct, typically by scoring the number of elements of the model that are correct or acceptable.
Process: One measures the quality of the students’ behavior when constructing a
model, typically by coding videos or log data for events that are clearly unacceptable,
such as starting to construct a model before paying any attention to the presentation of
the system, or running a deterministic modeling language twice without changing the
model between runs.

6.2 Improved domain knowledge
Domain knowledge refers to general concepts and principles that apply more widely than
just the system being addressed by the model-construction activity. Model-construction
activities can impact this knowledge in three ways: by adding new domain knowledge,
by making existing knowledge more operational and robust, and by removing misconceptions. Let us consider each instructional objective in turn.

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

382

K. VanLehn

One goal of inquiry learning is that students develop hypotheses about a particular
system and that the hypotheses turn out to be general domain principles. For instance,
the Thinker Tools sequence of model-exploration activities led some sixth graders to discover general principles governing velocity, acceleration, mass and force (White, 1993).
Such an instructional objective can be measured with a conventional assessment, such as
asking the students to state their newly won beliefs about the domain or asking them questions about them.
Another instructional objective is for students to convert their inert, fragile prior domain
knowledge into operational, robust domain knowledge. That is, instead of discovering new
hypotheses, students are told the relevant scientiﬁc knowledge before or during the modelconstruction activities. For instance, the system can be presented in the context of a set of
resources that present the target domain knowledge, as in the Betty’s Brain studies (Chin
et al., 2010). The appropriate assessments are to see if students can apply the domain concepts and principles in near and far transfer situations. Retention tests may also be used to
measure robustness.
Students often have incorrect beliefs about the physical world, which are often called
misconceptions or alternative conceptions. Misconceptions often can be detected during
interviews with students or using multiple-choice tests whose foils offer responses consistent with the false beliefs. Using both methods, Lee, Jonassen, and Teo (2011) showed that
having students construct models in Model-It reduced their misconceptions compared to a
control group of students who received a more standard inquiry instruction over the same
period. Their task domain was the water cycle, and observed misconceptions include
(p. 54): “When the plant gives out carbon dioxide or oxygen, they would condense on
the transparent plastic bag and water droplets formed.”
6.3 Improved understanding of a particular system
Some systems are so important that it is worth the student’s time to come to a deep understanding of them. For instance, the effect of greenhouse gases on global warming may be
worth understanding in detail.
Whereas model-construction skill and domain knowledge are assessed by using different systems during the assessment from those used during the training, if the goal is to
deeply understand a particular system, then that system must be used in both training
and assessment phases. Because the system is the same in both training and assessment,
it would be wise to use a different task for assessment than for training. If the assessment
had students construct a model of the target system (e.g. global warming), and they had
already constructed such a model during training, then the assessment task might tap
only shallow, episodic memories rather than a deep understanding of the target system.
Thus, this instructional objective might be best assessed using tasks that are not model construction but nonetheless tap the same deep understanding of the system as model construction. Table 3 lists candidate assessment tasks culled from a classic edited volume on mental
models (Gentner & Stevens, 1983) and from a review of assessments used in the systemdynamics literature (Hopper & Stave, 2008).
Developing scoring methods for such assessments requires considering the use-speciﬁcity of transfer (Singley & Anderson, 1989). The basic idea is that if an assessment task
requires, say, 100 knowledge components and the model-construction task involves only
60 of them, then the expected best score on the assessment task would be only 60%, assuming that the score is linearly related to the number of knowledge components that have been
mastered. Determining the number of shared knowledge components often requires a

Interactive Learning Environments

383

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

Table 3. Methods for assessing students’ understanding of a system.
Assessments involving prediction.
° Qualitative relationships. Ask the student questions of the form, “If X increases, then will Y increase,
decrease or be unaffected?”
° Sketching graphs. Ask the student to sketch the relationship between two variables, or the value of a
variable over time.
° Classifying behavior. Ask the student whether the system, or a variable of the system, exhibits
oscillation, damped oscillation, asymptotic growth, unbounded growth, emergence, or some other
class of behavior that has a name known to the student.
Assessments involving faults.
° Response to a fault. When a fault is inserted into the system, how will that affect its behavior?
√ Recognizing faulty behavior. When given a system behavior or given a system whose behavior can
be tested, can the student determine if the system is operating normally or whether it has a fault?
√ Troubleshooting. When given a system whose behavior is faulty, can students ﬁnd and correct the
fault?
Assessments involving redesign.
√ Extensions. Can the student modify the system to add a new capability or behavior?
° Optimization. Can the student modify the system to exhibit the same behavior while reducing the
cost, error rate, complexity, latency or some other valued measure?
Assessments involving exposition.
° Explanation. How well can the student explain or describe the system’s behavior?
° Teaching. How well can the student teach the system’s behavior to another student?
° Analogies. How well can the student construct or judge analogies to the system?
Assessments involving operating a device.
° Plan a sequence of operations to minimize time or risk of error.
° Choose which of several methods for operating the device is best for a particular situation.
° Control the output of a device despite ﬂuctuations in its inputs.
° Design or optimize a general procedure for operating the device.
° Estimate the time or number of operations required to operate the device.
° Non-standard operation. When the normal operating procedures are blocked or fail, invent a
sequence of operations that will work.
Retention.
° In addition to performing one of the assessments above immediately after the students’ have engaged
in a model construction activity, the assessment is conducted several days or weeks later.

separate study. For instance, consider a study of the transfer between tasks (Kessler, 1988):
(1) constructing a computer program, (2) debugging a given computer program and (3) calculating the output of a given program with a given input by mentally executing it. These
three activities are rather similar to constructing a model, debugging a model and making
predictions using a model. Kessler thoroughly trained students on one of the three tasks, and
then tested them on all three tasks. He found large amounts of transfer between constructing
and debugging a program, but little transfer between mentally executing a program and the
other two tasks. This makes sense given that constructing and debugging programs seem
intuitively to share many steps, whereas mentally simulating the execution of a program
hardly shares any steps with the other two. If the analogy between program construction
and model construction holds, then use-speciﬁcity of transfer predicts little transfer from
model construction to mental prediction of system behaviors, which is the ﬁrst set of
tasks in Table 3.
Although predicting transfer accurately requires a detailed cognitive task analyses and
studies of both the model-construction activity and a given assessment task, Table 3 has
“√” bullets next to the assessments that seem most likely to overlap with model construction. These may be the most appropriate assessments if the goal is to determine which
instructional treatment causes the best understanding of the system.

384
6.4

K. VanLehn
Concept inventories and mental modeling

Concept inventories are non-quantitative assessments that are often used in quantitative
science and mathematics courses (Pfundt & Duit, 1998). They often have questions that
require students to construct a mental model, execute it, and thus predict the behavior of
a system. For instance, here is a question from the famous Force Concept Inventory
(FCI) (Hestenes, Wells, & Swackhamer, 1992):

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

Two metal balls are the same size, but one weighs twice as much as the other. The balls are
dropped from the top of a two story building at the same instant of time. The time it takes
the balls to reach the ground below will be:

(A)
(B)
(C)
(D)
(E)

About half as long for the heavier ball.
About half as long for the lighter ball.
About the same time for both balls.
Considerably less for the heaver ball, but not necessarily half as long.
Considerably less for the lighter ball, but not necessarily half as long.

To answer such questions, students should construct a mentally held model of the system and
then execute it in order to predict its behavior (Ploetzner & VanLehn, 1997). As another
example, Figure 6 shows a question from the concept inventory of Booth Sweeney and
Sterman (2000). It also requires students to construct a mentally held model of the system
and execute it in order to predict the system’s behavior. Like many concept inventory
tasks, students sketch their answer rather than selecting it from a set of choices.
Because correctly answering a concept inventory question requires both constructing a
mental model and mentally executing it, one must be careful in interpreting scores. When
students score well on a concept inventory, then they probably have mastered both skills.
However, lack of mastery of just one skill sufﬁces for generating low scores. In particular,
even if the instruction is completely successful at teaching model-construction skills and yet
provides no practice at mentally executing models, then scores on the concept inventories
will probably be low.
To make this issue more concrete, let us consider a speciﬁc analysis of transfer from
instruction based on model construction to assessment based on the FCI. Ploetzer and
VanLehn (1997) constructed rule-based cognitive models of both quantitative and qualitative physics problem solving. The rules for quantitative problem solving were validated
against verbal protocols of students studying examples and solving ordinary quantitative
physics homework exercises; they represented what students could reasonably learn from
such instruction. The rules for qualitative problem solving were constructed to both
answer the FCI questions and to be as similar as possible to the rules for quantitative
problem solving. If correctly answering FCI questions required only skill at constructing
models, then one would expect 100% of the FCI rules to correspond to quantitative
problem solving rules, which include rules for constructing equation-based (constraint
system) models. In fact, only 36% of the FCI rules corresponded to quantitative problem
solving rules. Of the FCI rules that did not match quantitative problem solving rules,
most involved deriving predictions from models, which is the mental equivalent of executing the model. This analysis suggests that success on the FCI requires both skills (mental
construction and execution of models) but that only one of these skills is taught in conventional physics instruction.
Some educators consider mental modeling, and the correct answering of concept inventory questions in particular, to be an essential instructional objective, perhaps even more
important than formal model construction. In such classes, the instructors should probably

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

Interactive Learning Environments

385

Figure 6. A question from a system-dynamics concept inventory. Reprinted from Booth Sweeney
and Sterman (2000), with permission of John Wiley and Sons.

give the students considerable practice in mentally constructing and executing models using
problems similar to those on the concept inventories. VanLehn and van de Sande (2009)
sketch such a curriculum for physics. Löhner (2005, chapter 5) presents preliminary evidence about the effects of adding model execution exercises to a model-construction learning activity.

6.5 Understanding the role of models
Model-construction activities allow students to learn about the role of models in science,
business, government and other domains. For instance, students can learn:

386

K. VanLehn
.
.
.
.

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

.
.
.

.

to distinguish the model’s structure from its predictions about the system’s behavior;
that validating a model means testing that its predictions match known, well-understood system behavior;
that models can explain poorly understood systems by matching the observed behavior and offering predictions about unobserved behavior;
that models can make predictions about the behavior of systems that have not yet
been observed or even built;
that models can help decision-makers by allowing them to compare the costs and
beneﬁts of alternatives by modeling each;
that models can be valid with respect to known observations but nonetheless make
incorrect predictions about future observations;
that models can be modular so that even the large, complex models used for, e.g.
weather, economics and nuclear explosions can be understood in terms of submodels
which are more easily understood and validated and
about parsimony, sensitivity, calibration, non-identiﬁability and many other epistemological concepts.

Such epistemological knowledge is declarative knowledge, so it can be assessed with
questionnaires, interviews and journals (Crawford & Cullin, 2004; Schwarz & White,
2005; Treagust, Chittleborough, & Mamiala, 2002). Schwarz et al. (2009) deﬁne two learning progressions for modeling epistemology and show how students’ drawings can be used
to assess their location along each progression.

6.6 Other less easily assessed instructional objectives
There have been many claims about the beneﬁts of model construction, including some that
would be rather difﬁcult to assess. This section lists a few.
.

.
.

.
.
.

When students construct an executable model, they have constructed a machine that
thinks. Moreover, they have told it exactly how it should think. The mere fact that
they can do this is a powerful idea (Papert, 1980).
Model construction may lead students to take ownership of their ideas and knowledge
(Penner, 2001).
The models can become an object of discussion and communication among students
and thus help them practice evidence-based discussion skills (Schwarz & White,
2005; White, 1993).
Model construction may help students see the “big picture” and overarching patterns
(Mandinach & Cline, 1994).
Students may more clearly distinguish observed phenomena from underlying causal
processes and make stronger connections between them (Doerr, 1996).
Students may develop intuitions, predilections and skills at understanding complex
phenomena in general (Hogan & Thomas, 2001; Mandinach & Cline, 1994;
Schecker, 1993; Steed, 1992).

7. Common student difﬁculties
Students have many difﬁculties with model construction, and this section lists some of the
most commonly mentioned ones. They are grouped, somewhat arbitrarily, into difﬁculties

Interactive Learning Environments

387

with the overall problem solving method, difﬁculties with the modeling language and difﬁculties understanding the presentation of the system.
Poor problem solving strategies

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

7.1

The cognitive process that students should ideally do is diagrammed in Figure 1. It is
included in the Common Core State Standards in part because there is wide agreement
that this build-test cycle is the ideal way to construct a model.
More speciﬁcally, students should build an initial version of the model while being sure
to include the quantities or behaviors that they want to explain. They should then test the
model by executing it and comparing its predictions to the behavior of the system, in so
far as they know what that behavior is. If the model’s predictions do match the behavior
of the system, then students should test different cases or starting values in order to
insure that a match is obtained over a variety of test cases. If they ﬁnd a mismatch
between the models’ predictions and the system’s behavior, then they need to debug the
model by locating and ﬁxing the part of the model responsible for the incorrect prediction.
They should then resume evaluating the model.
When a system is complicated enough, students should decompose the system into parts
and perform the build-test cycle described above on one part after another. Before beginning, students should identify their goal such as modeling a particular behavior, quantity
or relationship. This goal should drive their decomposition of the system. As they iteratively develop the model, they should reﬂect often on their goal in order to prevent building
unnecessary parts of the model.
Unfortunately, many students do not enact the cognitive processes described above.
This section lists common ways that student’s fail.
The goal of model construction is to construct a parsimonious model in the given modeling language such that the model’s predictions match the behavior of the given system.
However, students often appear to have a different goal in mind, such as:
.

.

.

to produce a single, numerical answer (Booth, 1988; Hogan & Thomas, 2001; Miller
et al., 1993). This misconception may have been encouraged by early experiences
with algebra story problems;
to construct a model that displays everything they believe to be true about the system,
without any regard to what predictions the model makes (Hogan & Thomas, 2001). In
fact, they may never execute the model. This misconception may be encouraged by
early experience with concepts maps and
to get the predictions of the model to match the behavior of the system without considering the domain or plausibility. As Löhner et al. (2005, p. 456) put it, “they spend
their effort trying to match their model output to the system simulation output, rather
than trying to explain the underlying mechanism.”

Even when students understand that the goal of model construction is to create a model
that explains the behavior of a system, they often fail to adequately test and debug their
model. Some noted examples of poor testing and debugging are:
.

.

When students execute their model, they sometimes do not compare its predictions to
the system’s behavior (Metcalf et al., 2000). Thus, they do not notice when the model
is making incorrect predictions.
Students sometimes test their models on only one or two cases (Alessi, 2000a).

388

K. VanLehn
.

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

.

To ﬁx incorrect predictions, students sometimes add “fudge factors”, which are formulas, constants or logical conditions designed to artiﬁcially ﬁx the problem, and not
to model the system (Alessi, 2000a).
Students sometimes make unmotivated changes to the model in the vague hope that
the results will come out right (Alessi, 2000a; Löhner et al., 2005).

As mentioned earlier, when a system is complicated enough, students should use a
decompositional method. That is, in order to create a model of a whole system, students
should divide the system into loosely coupled parts, build and test a model of each of
the parts, then integrate the parts’ models into a whole. Unfortunately, students often
create a large model before testing it (Metcalf et al., 2000). They then have trouble
ﬁnding out why it makes poor predictions.
Students often copy models from the instructional materials, perhaps thinking that it
will be easier to modify an existing model (which, unfortunately, they may not fully understand) rather than start from scratch (Alessi, 2000a). Students often try to incorporate the
formulas of previous science and mathematics classes (which they often do not fully understand) instead of doing true system analysis (Alessi, 2000a).
7.2

Difﬁculties understanding the modeling language

This section lists a few of the observed difﬁculties students have in understanding the
language in which models are constructed, and hence the models that they have constructed.
Although the language may be formal mathematical expressions or formal diagrams, it is
still a “foreign” language and quite non-trivial for students to learn (Koedinger et al., 2008).
Students often lose track of what the symbols in their models denote. For instance,
Paige and Simon (1966) found that algebra students would report a negative number for
L, where L should denote the length of a board. If students had kept track of the denotation
of L, they would have realized immediately that a negative length is impossible. When
building a model and especially when testing a model, it is essential that students keep
track of what the model’s symbols denote.
When students are constructing a model that is intended to be understood by someone
else, there is an additional problem, which is called “grounding”. The term comes from the
literature on analyses of natural language dialogues, and it denotes the psycholinguistic
process where two people come to a mutual understanding of what a phrase or term
means (Clark & Brennan, 1991). Often this process proceeds smoothly and invisibly, but
it occasionally breaks down. For instance, the term “next week” often denotes different
weeks for different participants in a conversation. Similarly, a variable labeled “birth
rate” in a model can denote different quantities to the student and the teacher. The
problem is exacerbated when the student is trying to communicate with a computer tutor,
which is handicapped by lack of linguistic skill. If help is offered by a teacher, computer
tutor or peer, and the participants have failed to ground all the formal terms in the
model, then the help can be extremely confusing.
Students often get confused when modeling languages use symbols for both objects and
quantitative properties of objects. When students are given a list of terms such as “water”,
“tanks”, “water pressure” and “water level in a tank”, they have trouble identifying which
terms are variables (Kurtz dos Santos & Ogborn, 1994).
Students are often confused when the modeling language allows a variable’s value to be
speciﬁed using a differential. Most system dynamic models have two kinds of relationships
between variables. One can be expressed by an equation of the form x = f(V1…Vn), where

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

Interactive Learning Environments

389

V1…Vn are variables not including x, and f is a mathematical function not including derivatives or integrals. The other kind of relationship is expressed by an equation of the form dy/
dt = Σ ± v_i which says that the change in y over time is a weighted sum of variables, where
the weights are either +1 or −1. In graphical languages for system dynamic models, the
variable y is called a “stock” and the variables v_i are called “ﬂows”. Students often
have difﬁculties understanding stocks and ﬂows (Booth Sweeney & Sterman, 2000;
Cronin, Gonzalez, & Sterman, 2009; Hopper & Stave, 2008; Kainz & Ossimitz, 2002;
Metcalf et al., 2000; Sterman & Booth Sweeney, 2002). Kurtz dos Santos and Ogborn
(1994) suggest that ﬂows that have constant values are much less confusing than ﬂows
whose values vary over time.
Students who are learning algebra sometimes have difﬁculty in understanding the
formal language. As a vivid illustration, Booth (1988) listed several misconceptions
about algebra notation found during interviewing students in the 8th to 10th grades:
.
.

.
.

Thinking that implicit multiplication is concatenation, so when y is 4, 5y is 54.
Thinking that coefﬁcient-variable terms, such as 5 m, act like number-unit terms.
Thus, 5 m means “5 meters,” so “5 y” would denote 5 yogurts or 5 yams or a set
of 5 other objects or units whose ﬁrst letter is “y”.
Thinking that the equals sign is a command to produce an answer.
Thinking that different variables must stand for different numbers. Thus, a + b = a + c
can never be true because b and c must stand for different numbers.

It is likely that other formal modeling languages, even the graphical ones, also present
difﬁculties and afford misconceptions. For instance, after a 3-hour pilot experiment in the
author’s lab where students used a stock-and-ﬂow language similar to the one of Figure 4,
students reported during focus groups that they did not understand the difference between
the single-line arrow and the double-line arrow. Löhner et al. (2003) found that students had
trouble entering equations that they knew about into a graphical modeling language. Heffernan and Koedinger (1997) found that students could solve parts of algebra word problems but could not compose the results algebraically, suggesting a lack of ﬂuency in
the language of algebra.
7.3

Difﬁculties understanding the presentation

As mentioned earlier, there are a variety of methods that instructors can employ for presenting a system to the student. This section just considers the two most widely used methods:
experimentation with simulations and reading text.
When the presentation of the system requires experimentation in order to uncover its
behavior, there are a variety of obstacles associated with inquiry. For instance, students’
experiments may be either motivated or unmotivated. A motivated experiment is done in
order to test a speciﬁc hypothesis, whereas an unmotivated experiment is sometimes characterized as guesswork. For instance, in one model-construction study, 39% of the students’
experiments were designed with no hypothesis in mind (Löhner et al., 2005). Low domain
knowledge is often associated with frequent unmotivated experiments (Lazonder, Hagemans, & de Jong, 2010; Mulder et al., 2010). This is just one example of the many difﬁculties that students have when attempting to understand a system by experimenting with it.
When the presentation of a system is text, then superﬁcial reading is a common
problem. For instance, keywords such as “per” or “altogether” can often be used to help
translate a word problem into an equation (Paige & Simon, 1966). By the time, students

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

390

K. VanLehn

have reached high school and are solving algebra story problems, the use of such superﬁcial
reading strategies may have diminished. Koedinger et al. (2008) found that accuracy for
story problems that used operator words (e.g. “subtracted” and “multiplied”) was not
higher than for story problems that used other words instead. Heffernan and Koedinger
(1997) found that simplifying the language made little difference on the error rate of
algebra word problems.
Nonetheless, there are certain phrases in English that invite superﬁcial reading. For
instance, Clement (1982) found that university engineering students usually translated
“there are six times as many students as professors in this university” into 6S = P. This misconception was dislodged neither by hints nor by using diagrams.
System-dynamics problems often involve quantities that are ratios and that vary over
time, and the English phrases that express such concepts are often awkward and difﬁcult
to understand. For instance, a variable named “rabbit birth rate” might be deﬁned as
either “the number of rabbits born per year per rabbit in the population” or “the ratio of
baby rabbits born in a year to the number of rabbits in the population that year.” Faced
with such convoluted language, many students may opt for a superﬁcial reading assuming
that it will be easier to debug the resulting model than to try to untangle the language.
The remarks made so far apply to all text, including presentations comprising a short
paragraph of text that presents all and only the information needed for constructing the
model. Another method of presentation is to give students a set of resources (e.g. multimedia; a list of web pages) with the relevant information about the system embedded in considerable irrelevant information. Ideally, students would alternate between constructing the
model, discovering that they needed to know something about the system, searching the
resources for the desired information, and resuming construction of the model. Although
fast learners using Betty’s Brain did exhibit that pattern of reading, slow learners spent
large amounts of time reading the resources in an unfocused fashion (Segedy, Kinnebrew,
& Biswas, 2012).

8.

Types of scaffolding

As the preceding section illustrated, model construction is difﬁcult, so considerable
research has investigated methods for accelerating students’ learning. These methods for
helping learners along are often called “scaffolding”, because they can be removed when
the learner has reached mastery. That is, scaffolding is not an intrinsic part of the modelconstruction activity, but is something added to the activity.
Because many creative forms of scaffolding have been invented, it is difﬁcult to
organize this section. The list of scaffolding types has been grouped into (1) those that
require tutoring or similar interactivity from the system, (2) those that involve clariﬁcation
of the modeling language, (3) those that involve gradually increasing the complexity of the
model-construction activities, and (4) all the rest.

8.1 Tutoring
A typical computer tutor knows the solution but gives the students only prompts, hints and
feedback about it. This kind of scaffolding is covered in the ﬁrst two subsections below. The
remaining subsections cover other interactive methods of scaffolding, such as answering
questions asked by students. The scaffolding methods described in this section often
require artiﬁcial intelligence or other sophisticated computation.

Interactive Learning Environments

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

8.1.1

391

Feedback and hints on the student’s model

Several systems give feedback and hints to students on the model. Bravo et al. (2009) extended
CoLab to do such tutoring. When the student clicked on a “Check” button, the system compared the student’s model to a reference model, noted the differences between them, selected
one, and told the student about it. A formative evaluation of the system was done, but the
system was not compared to the standard version of CoLab, which lacks such tutoring.
In an early version of Betty’s Brain, students could ask Mr Davis (an agent portraying a
tutor) for help after Betty took a quiz, and he would give hints (Biswas et al., 2005). If students asked for enough hints, Mr Davis would eventually reveal exactly how to change the
model. Unfortunately, students tended to ask for hints rapidly, ignoring them until Mr Davis
indicated how to ﬁx the model. This form of help abuse, sometimes called gaming the
system, is quite common in other tutoring systems as well (Baker, Corbett, Koedinger, &
Wagner, 2004; Muldner, Burleson, van de Sande, & VanLehn, 2011). This led Biswas
et al. to modify Mr Davis so that he gave hints on the process of model construction
rather than the product, i.e. the student’s model. That form of scaffolding is covered next.

8.1.2

Feedback and hints on the student’s process (meta-tutoring)

Whereas the preceding section discussed feedback and hints on the students’ model as they
are constructing it, this section discusses feedback and hints on the students’ behavior.
Feedback on the model often has to mention domain knowledge. For instance, if students
have constructed a model that says that all rainfall becomes runoff, then the system might
say, “Does any water soak into the ground? Is runoff really equal to rainfall?” On the other
hand, feedback and hints about the student’s behavior seldom need to mention domain
knowledge. For instance, if students continue to add to their model without testing it,
then a possible hint is, “It’s a good idea to run your model and check its predictions
even before it’s ﬁnished.” Because domain knowledge is not mentioned, feedback and
hints on process are sometimes called “meta-tutoring” because “tutoring” is generally
reserved for domain-speciﬁc feedback and hints.
Betty’s Brain was equipped to do both tutoring and meta-tutoring (Leelawong &
Biswas, 2008). It could comment on the student’s model or it could comment on the student’s behavior. Perhaps, the most powerful example of the latter was that Betty would
refuse to take a quiz until the student had tested her ﬁrst. That is, students had to ask speciﬁc
questions of their model (e.g. if air temperature goes down, what does body temperature
do?) before the model could be given a test suite of instructor-generated questions. In a
series of studies (Leelawong & Biswas, 2008; Tan, Biswas, & Schwartz, 2006; Tan,
Wagster, Wu, & Biswas, 2007; Wagster, Tan, Biswas, & Schwartz, 2007; Wagster, Tan,
Wu, Biswas, & Schwartz, 2007), meta-tutoring was found to be signiﬁcantly more effective
than the same system with meta-tutoring turned off.
Although Betty’s Brain taught students a strategy for model construction, it did so in a
piecemeal fashion as hints from Mr Davis or comments from Betty. Other model-construction systems offered more explicit scaffolding. For instance, the ﬁnal version of Model-It
(also called TheoryBuilder (Metcalf, 1999)) had several features intended to guide students’
process of constructing a model (Metcalf et al., 2000). The model editor had four modes,
which were selected by clicking on one of four buttons labeled Plan, Build, Test and Evaluate. The Build mode was the actual model editor. The other modes presented forms to be
ﬁlled in by the student. Figure 7 shows the form for the Test mode. Students were given
feedback and hints, which they could suppress if they desired, when they attempted to

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

392

K. VanLehn

Figure 7. Scaffolding for the test mode of Model-It. Reprinted from Metcalf (1999), with permission
of the author.

bypass a suggested activity. The Carnegie Learning’s Algebra Cognitive Tutor (www.
carnegielearning.com) and the Word Problem Solving Tutor (Wheeler & Regian, 1999)
also scaffolded problem solving strategies via lightweight constraints, implemented with
a small amount of didactic instruction followed by feedback and hints.
The meta-tutoring of Betty’s Brain placed only weak constraints on students’ behavior.
The four phases of Model-It placed somewhat stronger constraints on students’ behavior. At
the far end of this progression is procedural scaffolding, which places very strong constraints on student behavior. The basic idea of procedural scaffolding is to teach students
to temporarily follow a speciﬁc procedure for constructing a model. Although the procedure
is not required by the task and there are many other ways to successfully problems, the procedure is used as a temporary scaffolding to guide students who might otherwise be quite
lost. A physics tutoring system, Pyrenees (Chi & VanLehn, 2008, 2010) had students start
by identifying the quantity, such as the ﬁnal velocity of a falling object, that the physics

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

Interactive Learning Environments

393

problems asked them to calculate. They were then asked to identify a physics principle,
such as the deﬁnition of kinetic energy that contains it. Then they wrote the equation
down. Next, for each unknown quantity in the equation, they repeated this process, treating
the unknown as the sought quantity. In this fashion, they constructed a set of equations that
modeled the given system. Procedural scaffolding signiﬁcantly improved students’ model
construction in the task domain where it was required. Moreover, when procedural scaffolding students transferred to a new task domain where procedural scaffolding was not
required, they again learned signiﬁcantly faster than the control students.
Procedural scaffolding was used by Marshall et al. (1989) to scaffold arithmetic story
problem solving, and by AMT (VanLehn et al., 2011) to scaffold construction of systemdynamics models. In all three systems, students were ﬁrst given feedback and hints that
kept them following the procedure, and later the feedback and hints were turned off thus
allowing them to try solving problems without following the procedure. That is, the procedure was “faded out”.
Whereas giving students feedback and hints on their models appears to have problems,
giving them feedback and hints on the process of constructing a model has been shown to
be effective in a several studies. Such “meta-tutoring” is one of a small number of scaffolding method that seems reliably useful.
8.1.3 Concrete articulation strategy
One method for helping students write equations with variables is to ﬁrst ask them to write
several versions of the equation with speciﬁc numerical values for the quantities that will
eventually be variables. For instance, suppose students are asked to write an equation to
model the following system: “Cathy took a m mile bike ride. She rode at a speed of s
miles per hour. She stopped for a b hour break. Write an expression for how long the
trip took.” Before asking students to write an abstract algebraic expression, this scaffolding
method asks students to solve several concrete, arithmetic expressions e.g. “Suppose Cathy
is going at 20 miles per hour, rides 100 miles and takes a 2 hour break. Write an arithmetic
expression for how long the trip took.”
This scaffolding method has been used in several algebra tutoring systems (Heffernan,
Koedinger, & Razzaq, 2008; Koedinger & Anderson, 1998; McArthur et al., 1989). Heffernan, who coined the name “concrete articulation strategy”, found that it increased learning
compared to the same tutoring system without the method. The method is used in the
algebra tutors of Carnegie Learning (www.carnegielearning.com), and could probably be
applied to many types of model construction as well.
8.1.4 Decomposition into subsystems
One form of scaffolding is to suggest a decomposition of the system so that one can focus
on one subsystem while temporarily ignoring the rest of the system (Ramachandran &
Stottler, 2003). For instance, consider modeling this system:
Alan, Stan and Doug are picking cherries. Doug picks 7 gallons more cherries than Stan, and
Stan picks half as many cherries at Alan. If Alan picks A cherries, how many does Doug pick?

If the student has trouble writing an equation for this, the tutoring system can suggest, “Well,
suppose that Stan picks S cherries. What expression would you write using S for the number
of gallons of cherries that Doug picks?” The tutoring system has decomposed the system into
a Stan-Doug part and a Stan-Alan part, and asked the student to focus on just one part.

394

K. VanLehn

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

This scaffolding is used by several algebra word problem tutoring systems (Heffernan,
2001; Heffernan et al., 2008; Ramachandran & Stottler, 2003), including commercial ones
(e.g. http://www.pearsonhighered.com/educator/mylabmastering/products/index.page). As
typically deployed, students ﬁrst attempt to provide an answer to the given problem. If
they then ask for help, the tutoring system decomposes the problem into a series of subproblems which it poses to the student.

8.1.5 Reﬂective debrieﬁngs
One method of scaffolding is to wait until students have ﬁnished a model-construction
activity, then ask them reﬂection questions such as, “What did you learn?” or “What
parts of the task were confusing?” (Buckley et al., 2004). One can also ask qualitative questions such as, “What would be different if the mass were increased substantially?”
Such reﬂective debrieﬁngs were used in a series of studies with a physics tutoring
system (Connelly & Katz, 2009; Katz, Allbritton, & Connelly, 2003; Katz, Connelly, &
Wilson, 2007). Using both human tutors and computer tutors, it was found that the tutoring
system’s effectiveness was increased by adding reﬂective debrieﬁngs. This seems likely to
work with any model-construction instructional system.

8.1.6

Answering student questions during model construction

A simple scaffolding method for human instructors to implement is to answer questions
raised by students as they are constructing models. For example, Corbett and his colleagues
augmented a model-construction system to answer questions typed by students in a chat
window (Anthony, Corbett, Wagner, Stevens, & Koedinger, 2004; Corbett et al., 2005).
If a student had not asked questions recently, the system would prompt with, “Do you
want to ask a question?” Students can ask Betty’s Brain to explain why a given variable’s
value increases, and it will present the chain of logical inferences that led to this conclusion
(Leelawong & Biswas, 2008). DynaLearn both explains its reasoning and answers other
questions as well (Beek et al., 2011).
Unfortunately, students tend not to ask substantive questions. Betty Brain constantly
nagged students to ask for explanations because students tended not to do so (Leelawong
& Biswas, 2008; Segedy et al., 2012). In one study of Corbett’s system, students asked
about 14 questions per hour. Most questions were about the user interface and about the
correctness of answers. Students seldom asked about mathematical processes, knowledge
or principles. Of the 431 questions asked during the whole study, only one was about mathematics principles. Corbett et al. concluded that a major problem with this form of scaffolding is that students do not ask many deep questions.
This is typical of students. Even when working with a human tutor (not necessarily on a
modeling activity), students rarely ask substantive questions (Core, Moore, & Zinn, 2003;
Graesser, Person, & Magliano, 1995). Thus, the empirical record suggests that questionanswering fails as scaffolding simply because students do not ask enough questions
while they are learning.
8.2 Clarifying the modeling language
As mentioned earlier, students have many difﬁculties understanding the meaning of their
models. They often just push the formal symbols (e.g. nodes and links) around without

Interactive Learning Environments

395

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

any regard to what their manipulations mean. This section describes several methods for
combating this kind behavior, which Löhner et al. (2005) call “jigsaw puzzling”.

8.2.1 Notation for models
As mentioned earlier, students often ignore the meaning of modeling language elements
(e.g. variables, equations) as they try many possible ways of combining them to make
the model’s predictions match the system’s behavior (Löhner et al., 2005). One way to scaffold model construction is to design the notation for the model so that the elements display
their semantics more clearly.
Several model-construction systems supplement or replace equations with other notations. For describing a functional relationship, Model-It (Metcalf et al., 2000) had students
construct a sentence (see Figure 8, top pane) or ﬁll in a table (Figure 8, middle pane). The
system would draw a graph representing the functional relationship as described by the
student. For describing simple differential equations (i.e. x′ = y), students constructed a sentence (Figure 8, lower pane) without graphical representation of the relationship. Notice that
students had space for entering unconstrained text for explaining the relationship they had
selected. Before writing an equation, Kid’s World (McArthur et al., 1989) had students ﬁrst
express relationships between quantities by drawing an x-y graph or by ﬁlling in a table. CoLab (van Joolingen et al., 2005) allows students to express relationships in three forms: text,
graphs and mathematics (Figure 9).
Although constraint-based models all provide notation for variables and for constraints
among variables, some also provide explicit representations of objects and processes. Formally speaking, an “object” is merely a set of variables and a “process” is merely a set of
constraints. They serve only to organize the model and play no role in the underlying mathematical structure, which is a system of variables and constraints. Nonetheless, objects and
processes may add conceptual clarity to a model. For instance, Model-It lets the user deﬁne
“stream” as an object and deﬁne properties of stream such as turbidity and nitrate_concentration. The properties are variables. Prometheus (Bridewell, Sanchez, Langley, & Billman,
2006), VModel (Forbus et al., 2005) and Homer (Bredeweg & Forbus, 2003) let the user
deﬁne processes, such as “exponential growth” or “heat ﬂow”, and indicate a set of constraints that deﬁne it. Figure 10 shows a model in VModel, where the rectangular nodes
represent both objects (e.g. wall, inside, outside) and processes (e.g. heat ﬂow out), and
the oval nodes represent quantities.
Because many students ﬁnd it easy to construct concept graphs, some constraint-based
languages provide a similar notation for expressing extra relationships among quantities,
objects and processes. For instance, Betty’s Brain allows students to indicate that carbon
dioxide is a type of greenhouse gas (Figure 2). Constraints among quantities can also be
given names, such as “releases” or “burns”, which describe the relationship but play no
role in the mathematics of the constraint.
Although many innovative designs for notation have been implemented, only one study
has compared notations for model construction. Löhner et al. (2003, 2005) compared a
graphical notation and a traditional equation-based notation. Although many differences
in students’ behavior were apparently caused by the difference in notation, the authors
did not draw a conclusion about which notation was better overall.
So far, the discussion has focused exclusively on notation for constraint-based and
system-dynamics languages. There has been comparatively little experimentation with notations for agent-based languages. The major agent-based language, NetLogo, uses a dialect of

K. VanLehn

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

396

Figure 8. Methods of expressing relationships in Model-It. Reprinted from Metcalf (1999), with permission of the author.

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

Interactive Learning Environments

397

Figure 9. Methods for expressing relationships in CoLab. Reprinted from van Joolingen et al. (2005,
Figure 4), with permission of Elsevier Limited.

Figure 10. Objects, processes and quantities are all displayed in Vmodel. Reprinted from Bredeweg
and Forbus (2003, p.38), with permission of the American Association for Artiﬁcial Intelligence.

the Logo programming language. CTiM provides a graphical programming language instead
(Basu et al., 2012). The two notations have not yet been compared empirically.

8.2.2

Grounding the symbols

Models often have symbols that denote objects, properties or quantities in the system.
“Grounding a symbol” is the process of getting the student and another agent (e.g. the
teacher, another student or a tutoring system) to agree on what the symbol denotes. For
instance, if the student chooses “birth rate” as a variable name in a system-dynamics
model, it could denote either the number of births per unit time or the ratio of births to population. When the student is using a tutoring system for model construction that gives feedback on the students’ model, then the student and the tutoring system need to ground
symbols. That is, they need to mutually agree on what every symbol in the model stands
for. There are several common methods for doing this:

398

K. VanLehn
.

.

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

.

The software provides names for symbols and the students select those they wish to
use in the model. The software designers try to choose names that are so clear that the
students have no trouble understanding what they mean. This assumes that the students read the symbols’ names carefully, which is not often the case, especially
when the names become long.
The students name the symbols. The software has to match the students’ name to
names it expects and knows the meanings of. In one study, the software’s matches
were correct 78% of the time (Bravo et al., 2009).
The presentation of the system has elements that can be selected and used as names.
For instance, if the presentation is text, then phrases in the text can be selected as
names for variables (Marshall et al., 1989).

Model-it (Metcalf et al., 2000), ModelingSpace (Avouris et al., 2003) and Vmodel
(Forbus et al., 2005) have students ﬁrst deﬁne objects and then deﬁne variables as quantitative properties of the objects. Thus, students would ﬁrst deﬁne “stream” as an object then
deﬁne “phosphate concentration” as a quantitative property of it. Both objects and quantities were represented with student-selected icons. However, students often chose the
same icons for different quantities.
In order to help ground variables, Model-It had students describe the range of a variable
qualitatively (Metcalf et al., 2000). For instance, after typing in “% of tree cover” as the name,
one student described the upper and lower ends of its range as “dense” and “sparse” (Metcalf,
1999). This qualitative labeling probably was more helpful than “100%” and “0%” for
keeping in mind what the variable meant. ModelingSpace had a similar feature, plus the
ability to display the qualitative value of a variable as an image (Avouris et al., 2003).
Some instructors feel strongly that grounding can be improved by requiring students to
use units on the dimensional numbers that appear in equations and variable assignments,
while other instructors believe that such attention to detail can be confusing, particularly
when the dimensional number appears inside an equation (e.g. substitution converts F =
ma to either F = 5a or F = 5kga or F = (5 kg)a or F = 5 kg a). Modeling languages
also vary on their policies for use of units.
None of these scaffolding methods have been evaluated empirically. Moreover, similar
methods have not been applied to agent-based models.
8.2.3 Comparing the model’s predictions to the system’s behavior
It is common for students to see the predictions of the model as being consistent with the
behavior of the system when in fact the two behaviors are different. This section presents
several methods for encouraging diligent comparison of predictions to behaviors.
For constraint-based models, one method involves asking students to manipulate the
values of variables with sliders and observe the effects in real time on predictions of the
system. For instance, Figure 11 shows a model in Q-Mod (Figure 25 of Miller et al.,
1993). The vertical sliders inside the nodes function both as displays of the values of variables and as controls that can be dragged by students in order to modify the values of variables. The text inside the node displays the node’s current value. This kind of user interface
is widely used for model-exploration exercises, where students are given a model and asked
to understand it by manipulating sliders and observing gauges.
For system-dynamics models, the same idea can be applied but it becomes more complicated. Such models predict the values of variables over time. Thus, the display of a variable’s
predicted behavior cannot show just a single value, but must instead show a graph of the

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

Interactive Learning Environments

399

Figure 11. Sliders allow both controlling and displaying quantities. Reprinted from Miller et al.
(1993, Figure 25), with permission of Elsevier Limited.

variables’ value over time. When students manipulate a slider that controls a variable’s value, the
graphs of the other variables change in real time. This feature is available in many professional
system-dynamics model-construction system, as well as Model-It (Metcalf et al., 2000).
For agent-based models, a standard practice is to display the predictions of the model as
both graphs (e.g. the number of slime molds) and as an animation, as shown in Figure 5.
This practice may help students interpret the graphs and compare the predictions of the
model against the system’s known behavior.
The basic idea of animating the model’s predictions can be applied to certain constraintbased models. For example, Animate is a model-construction system where students are
presented with succinct text and asked to construct equations in a graphical language
(Nathan, 1998; Nathan, Kintsch, & Young, 1992). However, it also has an animation that
is driven by the model. For instance, if the problem states that a helicopter leaves 2
hours after a train and overtakes it, and the student’s model uses t_train and t_copter for
the duration of the train’s and the copter’s trips, but the student states that t_train =
t_copter – 2, then the animation shows the helicopter leaving before the train. This use
of multiple representations was more effective than a no-feedback control (Nathan et al.,
1992). More interestingly, it was also more effective than a traditional hint-sequence feedback (Nathan, 1998). While both forms of feedback got students to correct their sign errors
and other expression errors, the animation feedback was better at getting students to understand the correction and thus they did better on post-test problems, which were similar to the
training problems. Of all the methods for helping students compare the model predictions to
system behavior, this is the only one to be evaluated empirically.
8.2.4 Students explain the model in natural language
Students sometimes have the basic idea for a model but have trouble writing it in the modeling language. For instance, suppose students are given a problem such as “Alan, Stan and

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

400

K. VanLehn

Doug are picking cherries. Doug picks 7 gallons more cherries than Stan, and Stan picks
half as many cherries at Alan. If Alan picks A cherries, how many does Doug pick?”
Some students can express their idea for solving the problem as a procedure, such as
“Take half of A and add 7 to it”. However, they do not know how to write that basic
idea as an algebraic expression.
The tutoring system Ms Lindquist (Heffernan et al., 2008) sometimes asked students for
their basic approach and gave them a sequence of menus to phrase their answers. For
instance, the student might pick “Divide A by 2” from the ﬁrst menu and “then add 7 to
it” from the second menu.
Model-It collected natural language explanations in several places (Metcalf, 1999;
Metcalf et al., 2000). For instance, the system prompted students to type in explanations of
.
.
.
.

Their goal for the model and their ideas for building it;
What predictions they expected the model to generate;
What a quantity they deﬁned represented
What a constraint they deﬁned represented

Although Model-It allowed students to turn off the prompting for such explanations,
teachers often insisted students write good quality explanations.
Although this scaffolding has not been evaluated in isolation, a tutoring system that
used it along with other scaffolding was effective when compared to classroom and
other controls (Razzaq, Mendicino, & Heffernan, 2008).

8.3

Gradually increasing the complexity

There is no doubt that model-construction tasks are complicated. In addition to the modeling skills and domain knowledge that are the instructional objectives, students must learn
the syntax of the modeling language, its semantics, the user interface for the editor, the
user interface for executing the model and displaying its predictions, and many other
things as well. Thus, one form of scaffolding is to introduce these details slowly. That is,
the model-construction activities start simple and gradually become more complex.
Several variants of this basic idea have been ﬁelded so far.

8.3.1

Hiding model features

Modeling tools often have advanced features that can confuse novice users. To prevent such
confusions, the initial user interface can hide the advanced features. As users become more
skilled, they can access a property sheet that allows them to turn the features on. For
example, although Model-It (Metcalf et al., 2000) lets students enter the graphical equivalent of a differential equation (see Figure 8, bottom pane), this capability is initially hidden.
This form of scaffolding has not yet been evaluated.

8.3.2 Qualitative model construction as scaffolding
Modeling tools that use quantitative diagrams (e.g. Stella, CoLab and Model-It) often
provide concept maps as well. DynaLearn (Bredeweg et al., 2010) allows students to
start by creating a concept map of the system then enhance it as they move through a progression of ﬁve executable modeling languages.

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

Interactive Learning Environments

401

Although instructors often recommend that students start model construction by
forming a concept map, students seldom do. In one study, only 3 of 60 subjects ever
used this feature even though they had been trained in its usage (Mulder et al., 2010).
This suggests that students should be required to do concept mapping before doing
quantitative model construction. Mulder, Lazonder, de Jong, Anjewierden, and Bollen
(2011) created a progression where the modeling language evolved from simple to
complex, but the system stayed the same. Unfortunately, none of the measures of learning
showed reliable differences, probably because the model-construction tasks were too challenging and few students made much progress through the sequences. Thus, the virtues of
this form of scaffolding remain unknown.
Instead of using non-executable concept mapping, students could use an executable
qualitative modeling tool to prepare for learning quantitative model construction. To
explore this kind of scaffolding, Kurtz dos Santos and Ogborn (1994) had half their students
work with a qualitative modeling tool, IQON, and half drawing similar diagrams on paper.
Both groups then transferred to using a quantitative model-construction tool, Stella. The
authors observed qualitative differences in the behaviors of the two groups, but stopped
short of recommending one form of scaffolding over another.
8.3.3 Model progressions
The progressions discussed so far vary the complexity of the modeling language and
tools. Another form of model progression varies the domain content and difﬁculty. For
instance, several projects have developed a sequence of simulation-based problem
solving activities and games that gradually increased in domain content and complexity
(de Jong et al., 1999; Quinn & Alessi, 1994; Swaak, van Joolingen, & de Jong, 1998;
White, 1984, 1993; White & Frederiksen, 1990). When these model progressions are
compared to unordered sequences, the results were mixed. Sometimes the progression
helped learning and sometimes it did not. However, all these model progression studies
had students do model exploration. That is, they were not asked to construct models in
a formal language.
8.4 Other scaffolding
This section simply lists types of scaffolding that do not easily ﬁt in the preceding three
categories.
8.4.1 Teachable agents and reciprocal teaching
A widespread belief is that teaching is also a good way to learn. Thus, several projects have
human students teach software that acts like a student (Biswas et al., 2005; Chase, Chin,
Oppenzzo, & Schwartz, 2009; Chin et al., 2010; Gulz, Haake, & Silvervarg, 2011; Leelawong & Biswas, 2008; Matsuda et al., 2011; Obayashi, Shimoda, & Yoshikawa, 2000;
Pareto, Arvemo, Dahl, Haake, & Gulz, 2011; Tan & Biswas, 2006; Tan, Wagster, et al.,
2007; Wagster, Tan, Biswas, et al., 2007; Wagster, Tan, Wu, et al., 2007). The software
that acts like a student is called a teachable agent or simulated student. Two modes of
interaction are employed. One mode allows the human student to directly edit the knowledge of the teachable agent; the other mode requires the human student to tutor the
teachable agent using natural language, examples and other communication methods
that would be appropriate between human students. In the latter case, the teachable

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

402

K. VanLehn

agent’s knowledge is not editable by the human student and is typically not displayed to
the student either.
In some studies (Chan & Chou, 1997; Reif & Scott, 1999), the role of tutor vs. tutee
switched back and forth between human and computer agent. This instructional method
is called reciprocal teaching (Palinscar & Brown, 1984).
If the knowledge representation language used for the teachable agent is a modeling
language, and the human student can edit the agent’s knowledge, then teaching the agent
is a kind of model-construction activity. Perhaps, the best known example of this class
of system is Betty’s Brain (Biswas et al., 2005; Biswas et al., 2010; Chase et al., 2009;
Leelawong & Biswas, 2008; Schwartz et al., 2008; Schwartz et al., 2009; Tan & Biswas,
2006; Tan et al., 2006; Tan, Skirvin, Biswas, & Catley, 2007; Tan, Wagster, et al., 2007;
Wagster, Tan, Biswas, et al., 2007; Wagster, Tan, Wu, et al., 2007). The users of Betty’s
Brain edit the qualitative diagrams shown in Figure 2. They are asked to pretend that the
diagram is Betty’s knowledge. Instead of executing the model and checking that its predictions match the behavior of a given system, students can either ask Betty a question
(as shown in Figure 2) or have her take a quiz composed of a small number of such
questions. Either way, the qualitative diagram is executed and its results are shown to
the student. In the case of the quiz, the results are also marked as correct vs. incorrect
depending on whether they match or do not match the behavior of the system. The
system presented to the student as a set of resources describing, e.g. the oxygen cycle
in streams and lakes.
Several studies have shown that the teachable agent cover story improves learning compared to a version of the system without the teachable agent cover story (Biswas et al.,
2005; Chase et al., 2009; Pareto et al., 2011). In a particularly well-controlled experiment
(Chase et al., 2009), students used Betty’s Brain but were told that the model represented
their own knowledge and not Betty’s. The Betty agent was turned off but Mr Davis the
tutor was still present. Students who thought they were writing models learned signiﬁcantly
less than students who thought they were teaching Betty. Chase et al. call this the Protégé
effect. The Protégé effect can be enhanced by having the teachable agent make small talk
with the human students during breaks in the instruction (Gulz et al., 2011). These studies
suggest that teachable agents and reciprocal teaching can be quite effective at increasing
learning during model construction.
8.4.2 Having students execute models mentally
One way to have students examine their model more closely is to have them execute it
mentally. This can be done by asking qualitative questions such as, “If reﬂectance of the
earth’s atmosphere decreases, what happens to the earth’s temperature?” Students can
only reason for short distances (Kurtz dos Santos & Ogborn, 1994; Löhner, 2005).
Nonetheless, this does raise their scores when quizzed on the structure of the model
(Löhner, 2005, chapter 5), and could potentially raise scores on concept inventory questions, as discussed earlier.
8.4.3 Test suites
When there are multiple ways to test a model, students often do a poor job of selecting tests.
They often focus on just a few tests out of the many possible ones (Alessi, 2000a). This
suggests giving them test suites, which are composed by an expert to more thoroughly
sample the space of all possible tests.

Interactive Learning Environments

403

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

However, in one evaluation of Betty’s Brain, test suites proved to be harmful. Students
who had to generate their own tests performed better than students who were given test
suites in the guise of a quiz (Biswas et al., 2005). Perhaps students could utilize test
suites better if either they were given explicit instruction on how to use them, or the
test suites were covertly designed to highlight just one bug in the model and students
were prompted to ﬁnd it.
8.4.4 Generic schemas
An analysis of algebra and arithmetic word problems indicates that a surprisingly small set
of schemas sufﬁces to characterize most of them (Marshall et al., 1989; Mayer, 1981).
These schemas are not as abstract as mathematical relationships, yet they are general
enough to cover multiple word problems. For example, Marshal et al. (1989) found that
most single-step arithmetic problem could be characterized using only 5 schemas:
Change, Group, Compare, Restate and Vary. Similarly, Mayer (1981) found that 90
schemas sufﬁced to cover a large corpus of high school algebra problems. For instance,
there were 13 schemas for motion of an object including simple distance-rate-time, one
object overtaking another, objects moving in opposite directions, an object making a
round trip, and 9 others. There were nine schemas for work done per unit time: simple
work problems, two processes working together, one process joining another to ﬁnish a
job and six others. There were 8 schemas involving interest on investments, 5 schemas
involving mixtures, 10 schemas involving travelling on a river and so on.
While analyzing videos of competent physics students working problems in pairs at the
whiteboard, Sherin (2006) concluded that they use a type of schema he called symbolic
forms to generate parts of models. For instance, if the students viewed the system as
having balancing forces, then that would trigger their Balancing symbolic form, which
suggested an equation of the form □ = □. On the other hand, if they viewed the system
as one force canceling out another, that would trigger the cancelation symbolic form,
which suggested an equation of the form 0 = □−□.
Schema libraries were provided by some model-construction environments (Bredeweg
& Forbus, 2003; Bridewell et al., 2006; Marshall et al., 1989). A simple problem could be
solved by copying a generic schema from the library and editing it to make it applicable to
the given system. For more complex problems, students may combine multiple schemas to
form the model. Some of these systems coached students on how to combine schemas in
syntactically legal ways. Students were also able to add schemas to the library.
Unfortunately, schema-based scaffolding has not yet been evaluated. Such an evaluation could be done by comparing two versions of the model-construction activity: with
and without the schema library.
8.4.5 Gamiﬁcation
Gamiﬁcation is adding game-like features to an educational activity. There are many ways
that gamiﬁcation can be done. For example, one mode of Betty’s Brain put models in the
role of players in a game show (Schwartz et al., 2009). The host of the game show would
ask a player/model questions such as, “If the amount of ﬂuoride in the water goes up, what
happens to tooth decay?” Models that answered the question correctly would score points.
The game show typically had four models that competed against each other. Although
observation in the classroom leaves no doubt about the motivational beneﬁts of the game
show, its impact on learning has not been evaluated.

404

K. VanLehn
Summary: which forms of scaffolding have demonstrated beneﬁts?

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

8.5

Of the forms of scaffolding reviewed here, meta-tutoring has the most empirical evidence
for its beneﬁts. Although giving hints and feedback on the models that students construct
appears to be problematic, giving hints and feedback on the model-construction process
(i.e. “meta-tutoring”) has been shown in several studies to produce larger learning gains
than instruction that uses the same modeling tool without the meta-tutoring. Three other
techniques from the tutoring system literature – the concrete articulation strategy, decomposition and reﬂective debrieﬁngs – have a few evaluations behind them, but have not been
extensively tested. Finally, students ask too few questions to warrant tutoring based on
answering student questions. On the whole, the evidence suggests that reliable beneﬁts
can be obtained with the basic approach of guiding students via prompting, hinting and
feedback through the process of model construction including decomposition, reﬂection
and perhaps concrete articulation.
Another form of scaffolding with good empirical support is the use of teachable agents
and reciprocal teaching. These have mostly been used with younger students in classroom
contexts, and the leading explanations for their beneﬁts are motivational rather than cognitive. It is not clear whether they will continue to provide added value when they are used by
older students or by students working alone.
As Sections 7.2 and 7.3 indicated, many notations have been tried for modeling
languages, and many methods for helping students over the complexities and semantic
issues of the languages have also been tried. Unfortunately, little experimental evidence
exists about the effectiveness of notation-based scaffolding. It seems plausible that there
are large beneﬁts, but the experiments still need to be done.
Little evidence exists concerning the beneﬁts of the remaining forms of scaffolding,
which are covered in sections 7.4.2 onwards. Of these, generic schemas seem most plausibly helpful, but again, experimental evidence is lacking.

9.

Conclusions

The study of model construction as a learning activity is fairly new, and the ﬁeld has spent
most of its energy just trying to ﬁnd ways to get students to engage in model construction
successfully. Many of the studies have been formative evaluations whose results are
intended only to help redesign the instruction. Many papers allude to pilot studies but do
not report their ﬁndings in detail. In only a few cases has the model-construction instruction
reached a state of maturity that the developers thought it would be useful to do a summative
evaluation. Although all these evaluations were mentioned earlier, they will be summarized
here followed by suggestions for future work.

9.1 Summative evaluations of model construction compared to other instruction
Eight studies have compared instruction based on model construction to instruction that
does not involve model construction. In four of these, concept mapping was the comparison
instruction:
.

Chin et al. (2010, study 1) found that Betty’s Brain was signiﬁcantly more effective
than using commercial concept-mapping software. Sixth-grade students studied
global warming for 11 classes over a 3-week period.

Interactive Learning Environments
.

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

.

.

405

Biswas et al. (2005, study 1) found that Betty’s Brain was signiﬁcantly more effective
than concept mapping using Betty’s modeling language but with execution turned
off.1 Fifth-grade students learned about river ecosystems during three one-hour sessions with the software. Students engaged in additional independent study between
sessions in order to prepare themselves to use the software.
Kurtz dos Santos and Ogborn (1994) found that IQON caused different behaviors
than having students do concept mapping using its notation on paper. Qualitative
observation of both learning and transfer behavior was presented, but no numerical
measures of learning.
Löhner (2005, chapter 5) compared students using CoLab to students doing concept
mapping using CoLab’s modeling language with execution turned off. Students in
grades 11 and 12 constructed either executable models or concept maps of the greenhouse effect over 4 50-minute lessons that include some guided model exploration as
well as model construction. For factual knowledge, the concept-mapping instruction
was signiﬁcantly more effective. For predicting the system’s behavior, the model construction instruction was more effective. For explanation questions, there was a ﬂoor
effect – both conditions answered poorly.

These four studies suggest that concept mapping is less effective than instruction based
on model construction. However, this might be due to a difference in the availability of
feedback. If students in the concept-mapping condition received no feedback on the correctness of their maps, that might explain their disadvantage. Although Chin et al. (2010) carefully controlled for feedback, it is not clear that the other studies did.
Three of the studies compared model construction to having students write summaries
and explanations:
.

.

.

van Borkulo et al. (2012) found that learning from CoLab was superior to writing
summaries guided by a worksheet. Students in the 11th grade ﬁrst learned about
model construction for 2.5 hours then learned about global warming using either
CoLab or writing. Perhaps due to the short duration of the manipulation, statistically
reliable differences were observed for only two of eight assessments types.
Lee et al. (2011) found that Model-It was signiﬁcantly more effective than having students write while doing literature research. Fifth-grade students learned about the
water cycle during a 10-week unit, whose second half involved either using
Model-It or continuing with their directed study of the water cycle literature.
Schwartz et al. (2008, study 1) compared Betty’s Brain to having students write short
explanations to questions. The small number of students (16 college students) and the
short duration (40 minutes) prevented quantitative measures of results, but qualitative
observations suggest the Betty’s instruction was superior.

In all these studies, students were presented information about the system via access to
multimedia resources, and they were asked to answer the same set of questions about the
system. The experimental groups formulated their answers as models, and the control
group formulated their answers as writing. It is not clear how much feedback on the
writing was given to students in the control groups.
The last study compared model construction to three different types of model exploration (Hashem & Mioduser, 2011). This study had science majors in college work with two
different NetLogo models. Prior to pre-testing, the model-construction group spent 48
hours learning how to program NetLogo, and the model-exploration groups attended a

406

K. VanLehn

two-hour introduction to NetLogo. The instructional manipulation occurred over two 90minute sessions. It was preceded by a pre-test, and followed by a post-test and interviews.
Model construction proved to be more effective than model exploration by a moderately
large effect size: d = 0.48 (Hashem & Mioduser, 2011, Table 2).
As usual when reviewing studies of learning, the results are diverse and incomplete.
Nonetheless, the results strongly suggest that model construction can be more effective
than concept mapping and perhaps writing and concept exploration as well.

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

9.2

Future work

Perhaps, the major problem with using model construction for instruction is that it takes a
long time for students to become skilled enough that they can actually start learning about
the system and the domain. For instance, most of the studies of Betty’s Brain take about 10
lessons spread over 2 or 3 weeks. It takes that long for the beneﬁts of modeling to show up.
Figure 12, from (Chin et al., 2010), illustrates this vividly. The x-axis divides show student
performance on a pre-test, two mid-tests and a post-test. That is, about every 4 hours of
instruction, there was a test. The students’ scores on three different types of test questions
are shown: those requiring short, medium and long chains of reasoning. The model-construction group begins to pull away from the concept-mapping group (who were using
the commercial software Inspiration) only after 8 hours of instruction, and only on
medium-length chains of reasoning. It is not until 11 lessons have been conducted that
the beneﬁts of model construction appear on all question types.
Of the experiments reviewed here, most of those that showed statistically signiﬁcant
differences used long time periods for the instruction. The experiments that produced
null effects typically used short time periods (e.g. 2 hours) for the instruction. Clearly,
experimenters should all be using multi-hour instruction in model construction despite
the difﬁculty of running such experiments.

Figure 12. It takes many hours of instruction before model construction shows beneﬁts over concept
mapping. Reprinted from Chin et al. (2010, Figure 4), with the kind permission of Springer Science
and Business Media.

Interactive Learning Environments

407

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

Unless some method for dramatically speeding up students’ learning of model construction is found, educators must face the prospect that meeting the new standards in modeling
is going to require a substantial commitment of class time. On the other hand, once students
have acquired the appropriate model-construction skills, it appears that they can use them to
more easily acquire a deep understanding of systems and domains. Thus, a primary goal for
future work should be to reduce the time required for students to become ﬂuent in model
construction.

Acknowledgements
This research was supported by the National Science Foundation under grants DRL-0910221, IIS1123823 and DUE-1140901 and by the Ofﬁce of Naval Research under contract N00014-13-C-0029.

Note
1.

Although Biswas et al. (2005) do not present a statistical analysis of this comparison, the means
and error bars on Figure 5(b) suggest that it would be reliable and large. However, these measures
were taken over the models and concept maps constructed by students as they used the software,
and thus do not comprise a typical post-test.

Notes on contributor
Kurt VanLehn is the Diane and Gary Tooker Chair for Effective Education in Science, Technology,
Engineering and Math in the Ira A. Fulton Schools of Engineering at Arizona State University. He
received a Ph.D. from MIT in 1983 in Computer Science, was a post-doc at BBN and Xerox
PARC, joined the faculty of Carnegie-Mellon University in 1985, moved to the University of Pittsburgh in 1990 and joined ASU in 2008. He founded and co-directed two large NSF research
centers (Circle; the Pittsburgh Science of Learning Center). He has published over 125 peer-reviewed
publications, is a fellow in the Cognitive Science Society, and is on the editorial boards of Cognition
and Instruction and the International Journal of Artiﬁcial Intelligence in Education. Dr VanLehn’s
research focuses on intelligent tutoring systems and other intelligent interactive instructional
technology.

References
Alessi, S. M. (2000a, December). The application of system dynamics modeling in elementary and
secondary school curricula. Paper presented at the RIBIE 2000 – The Fifth Iberoamerican
Conference on Informatics in Education, Viña del Mar, Chile.
Alessi, S. M. (2000b). Building versus using simulations. In J. M. Spector & T. M. Anderson (Eds.),
Integrated and holistic perspectives on learning, instruction and technology (pp. 175–196).
Dordrecht, The Netherlands: Kluwer.
Anthony, L., Corbett, A. T., Wagner, A. Z., Stevens, S. M., & Koedinger, K. R. (2004). Student question-asking patterns in an intelligent algebra tutor. In J. C. Lester, R. M. Vicari, & F. Praguacu
(Eds.), Intelligent tutoring systems: 7th international conference, ITS 2004 (pp. 455–467).
Berlin: Springer-Verlag.
Avouris, N., Margaritis, M., Komis, V., Saez, A., & Melendez, R. (2003). ModellingSpace:
Interaction design and architecture of a collaborative modelling environment. Paper presented
at the Sixth International Conference on Computer Based Learning in Sciences (CBLIS),
Nicosia, Cyprus.
Baker, R. S. J. d., Corbett, A. T., Koedinger, K. R., & Wagner, A. Z. (2004). Off-task behavior in the
cognitive tutor classroom: When students “Game the System”. In E. Dykstra-Erickson & M.
Tscheligi (Eds.), Proceedings of the SIGCHI conference on human factors in computing
systems (pp. 383–390). New York, NY: ACM.

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

408

K. VanLehn

Basu, S., Kinnebrew, J. S., Dickes, A., Farris, A. V., Sengupta, P., Winger, J., & Biswas, G. (2012). A
science learning environment using a computational thinking approach. Paper presented at the
Proceedings of the 20th International Conference on Computers in Education, Singapore.
Beek, W., Bredeweg, B., & Lautour, S. (2011). Context-dependent help for the DynaLearn modelling
and simulation workbench. In G. Biswas (Ed.), Artiﬁcial intelligence in education
(pp. 4200–4422). Berlin: Springer-Verlag.
Biswas, G., Jeong, H., Kinnebrew, J. S., Sulcer, B., & Roscoe, R. D. (2010). Measuring self-regulated
learning skills through social interactions in a teachable agent environment. Research and
Practice in Technology Enhanced Learning, 5(2), 123–152.
Biswas, G., Leelawong, K., Schwartz, D. L., & Vye, N. J. (2005). Learning by teaching: A new agent
paradigm for educational software. Applied Artiﬁcial Intelligence, 19, 263–392.
Boohan, R. (1995). Children and computer modelling: Making worlds with WorldMaker. In J. D.
Tinsley & T. J. van Weert (Eds.), Proceedings of the sixth world conference on computers in education (pp. 975–985). London: Chapman and Hall.
Booth, L. R. (1988). Children’s difﬁculties in beginning algebra. In F. Coxford (Ed.), The ideas of
algebra, K-12 (1988 Yearbook) (pp. 20–32). Reston, VA: NCTM.
Booth Sweeney, L., & Sterman, J. D. (2000). Bathtub dynamics: Initial results of a systems thinking
inventory. System Dynamics Review, 16(4), 249–286.
van Borkulo, S. P., van Joolingen, W. R., Savelsbergh, E. R., & de Jong, T. (2012). What can be
learned from computer modeling? Comparing expository and modeling approaches to teaching
dynamic systems behavior. Journal of Science Education and Technology, 21, 267–275.
Bravo, C., van Joolingen, W. R., & de Jong, T. (2009). Using Co-Lab to build system dynamics
models: Students’ actions and on-line tutorial advice. Computer and Education, 53, 243–251.
Bredeweg, B., & Forbus, K. D. (2003). Qualitative modeling in education. AI Magazine, 24(4), 35–46.
Bredeweg, B., Liem, J., Beek, W., Salles, P., & Linnebank, F. (2010). Learning spaces as representational scaffolds for learning conceptual knowledge of system behavior. In M. Wolpers (Ed.),
EC-TEL (pp. 46–61). Berlin: Springer-Verlag.
Bredeweg, B., Linnebank, F., Bouwer, A., & Liem, J. (2009). Garp3 – Workbench for qualitative
modelling and simulation. Ecological Informatics, 4, 263–281.
Bridewell, W., Sanchez, J. N., Langley, P., & Billman, D. (2006). An interactive environment for the
modeling and discovery of scientiﬁc knowledge. International Journal of Human-Computer
Studies, 64, 1099–1114.
Buckley, B. C., Gobert, J. D., Kindﬁeld, A. C. H., Horwitz, P., Tinker, R. F., Gerlits, B., … Willett, J.
(2004). Model-based teaching and learning with BioLogica: What do they learn? How do they
learn? How do we know? Journal of Science Education and Technology, 13(1), 23–41.
Chan, T.-W., & Chou, C.-Y. (1997). Exploring the design of computer supports for reciprocal tutoring.
International Journal of Artiﬁcial Intelligence and Education, 8, 1–29.
Chase, C. C., Chin, D. B., Oppenzzo, M., & Schwartz, D. L. (2009). Teachable agents and the
Protégé effect: Increasing the effort towards learning. Journal of Science Education and
Technology, 18(4), 334–352.
Chi, M., & VanLehn, K. (2008). Eliminating the gap between the high and low students through metacognitive strategy instruction. In B. P. Woolf, E. Aimeur, R. Nkambou, & S. P. Lajoie (Eds.),
Intelligent tutoring systems: 9th international conference: ITS2008 (pp. 603–613). Berlin:
Springer.
Chi, M., & VanLehn, K. (2010). Meta-cognitive strategy instruction in intelligent tutoring systems:
How, when and why. Journal of Educational Technology and Society, 13(1), 25–39.
Chin, D., Dohmen, I. M., Cheng, B. H., Oppezzo, M., Chase, C. C., & Schwartz, D. L. (2010).
Preparing students for future learning with teachable agents. Educational Technology Research
and Development, 58, 649–669.
Clariana, R. B., & Strobel, J. (2007). Modeling technologies. In J. M. Spector (Ed.), Handbook of
research on educational communications and technology (pp. 329–344). New York: Taylor &
Francis.
Clark, D. B., Nelson, B. C., Sengupta, P., & D’Angelo, C. (2009). Rethinking science learning
through digital games and simulations: Genres, examples and evidence. An NAS commissioned
paper.
Clark, H. H., & Brennan, S. E. (1991). Grounding in communication. In L. B. Resnick, J. M. Levine,
& S. D. Teasley (Eds.), Perspectives on socially shared cognition (pp. 127–149). Washington,
DC: American Psychological Association.

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

Interactive Learning Environments

409

Clement, J. (1982). Algebra word problem solutions: Thought processes underlying a common misconception. Journal of Research in Mathematics Education, 13(1), 16–30.
Collins, A., & Ferguson, W. (1993). Epistemic forms and epistemic games: Structures and strategies
to guide inquiry. Educational Psychologist, 28(1), 25–42.
CCSSO. (2011). The common core state standards for mathematics. Retrieved October 31, 2011, from
www.corestandards.org
Connelly, J., & Katz, S. (2009). Toward more robust learning of physics via reﬂective dialogue extensions. In G. Siemens & C. Fulford (Eds.), Proceedings of the world conference on educational multimedia, hypermedia and telecommunications 2009 (pp. 1946–1951). Chesapeake, VA: AACE.
Corbett, A., Wagner, A. Z., Chao, C.-y., Lesgold, S., Stevens, S. M., & Ulrich, H. (2005). Student
questions in a classroom evaluation of the ALPS learning environment. In C.-K. Looi & G.
McCalla (Eds.), Artiﬁcial intelligence in education (pp. 780–782). Amsterdam: IOS Press.
Core, M. G., Moore, J. D., & Zinn, C. (2003). The role of initiative in tutorial dialogue. In P. Paroubek
(Ed.), Proceedings of the 11th conference of the European chapter of the association for computational linguistics (EACL) (pp. 67–74). Morristown, NJ: Association of Computational
Linguistics.
Crawford, B. A., & Cullin, M. (2004). Supporting prospective teachers’ conceptions of modelling in
science. International Journal of Science Education, 26(11), 1370–1401.
Cronin, M., Gonzalez, C., & Sterman, J. D. (2009). Why don’t well-educated adults understand
accumulation? A challenge to researchers, educators and citizens. Organizational Behavior
and Human Decision Processes, 108, 116–130.
Doerr, H. M. (1996). Stella ten-years later: A review of the literature. International Journal of
Computers for Mathematical Learning, 1, 201–224.
Erden, M., Komoto, H., van Beek, T. J., D’Amelio, V., Echavarria, E., & Tomiyama, T. (2008). A
review of function modeling: Approaches and applications. Artiﬁcial Intelligence for
Engineering Design, Analysis and Manufacturing, 22(2), 147–169.
Forbus, K. D., Carney, K., Sherin, B. L., & Ureel Il, L. C. (2005). VModel: A visual qualitative modeling environment for middle-school students. AI Magazine, 26(3), 63–72.
Gentner, D., & Stevens, A. L. (1983). Mental models. Mahah, NJ: Erlbaum.
Goel, A. K., Rugaber, S., & Vattam, S. (2009). Structure, behavior, and function of complex systems:
The structure, behavior, and function modeling language. Artiﬁcial Intelligence for Engineering
Design, Analysis and Manufacturing, 23, 23–35.
Graesser, A. C., Person, N., & Magliano, J. (1995). Collaborative dialog patterns in naturalistic oneon-one tutoring. Applied Cognitive Psychology, 9, 359–387.
Gulz, A., Haake, M., & Silvervarg, A. (2011). Extending a teachable agent with a social conversation
module – Effects on student experiences and learning. In G. Biswas, S. Bull, J. Kay, & A.
Mitrovic (Eds.), Artiﬁcial intelligence in education (pp. 106–114). Berlin: Springer.
Harrison, A. G., & Treagust, D. F. (2000). A typology of school science models. International Journal
of Science Education, 22(9), 1011–1026.
Hashem, K., & Mioduser, D. (2010). Learning by modeling (LbM) – The contribution of computer
modeling to students’ evolving understanding of complexity. Paper presented at the Second
International Conference on Educational Technology and Computer (ICETC), Shanghai, China.
Hashem, K., & Mioduser, D. (2011). The contribution of learning by modeling (LbM) to students’
understanding of complexity concepts. International Journal of e-Education, e-Business,
e-Management and e-Learning, 1(2), 151–157.
Heffernan, N. T. (2001). Intelligent tutoring systems have forgotten the tutor: Adding a cognitive
model of human tutors (PhD dissertation). Carnegie Mellon University, Pittsburgh, PA.
Retrieved from http://gs260.sp.cs.cmu.edu/diss/diss.pdf
Heffernan, N. T., & Koedinger, K. R. (1997). The composition effect in symbolizing: The role of
symbol production vs. text comprehension. In M. G. Shafto & P. Langley (Eds.), Proceedings
of the nineteenth annual meeting of the cognitive science society (pp. 307–312). Mahwah, NJ:
Erlbaum.
Heffernan, N. T., Koedinger, K. R., & Razzaq, L. (2008). Expanding the model-tracing architecture: A
3rd generation intelligent tutor for algebra symbolization. International Journal of Artiﬁcial
Intelligence in Education, 18, 153–178.
Hestenes, D. (2007). Modeling theory for math and science education. Paper presented at the ICTMA13: The International Community of Teachers of Mathematical Modelling and Applications,
Indiana, IL.

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

410

K. VanLehn

Hestenes, D., Wells, M., & Swackhamer, G. (1992). Force concept inventory. The Physics Teacher,
30, 141–158.
Hmelo-Silver, C., & Pfeffer, M. G. (2004). Comparing expert and novice understanding of a complex
system from the perspective of structures, behaviors and functions. Cognitive Science, 28, 127–
138.
Hogan, K., & Thomas, D. (2001). Cognitive comparisons of students’ systems modeling in ecology.
Journal of Science Education and Technology, 10(4), 319–345.
Hopper, M., & Stave, K. (2008). Assessing the effectiveness of systems thinking interventions in the
classroom. Paper presented at the International Conference of the System Dynamics Society,
Athens, Greece. Retrieved from http://www.systemdynamics.org/conferences/2008/proceed/
index.htm
Jacobson, M. J., & Wilensky, U. (2006). Complex systems in education: Scientiﬁc and educational
importance and implications for the learning sciences. Journal of the Learning Sciences, 15(1),
11–34.
Jonassen, D., & Strobel, J. (2006). Modeling for meaningful learning. In D. Hung & M. S. Khine
(Eds.), Engaged learning with emerging technologies (pp. 1–27). Amsterdam: Springer.
de Jong, T., Martin, E., Zamarro, J.-M., Esquembre, F., Swaak, J., & van Joolingen, W. R. (1999). The
integration of computer simulation and learning support: An example from the physics domain of
collisions. Journal of Research in Science Teaching, 36(5), 597–615.
de Jong, T., & van Joolingen, W. R. (1998). Scientiﬁc discovery learning with computer simulations
of conceptual domains. Review of Educational Research, 68(2), 179–201.
van Joolingen, W. R., De Jong, T., Lazonder, A., Savelsbergh, E. R., & Manlove, S. (2005). Co-Lab:
Research and development of an online learning environment for collaborative scientiﬁc discovery learning. Computers in Human Behavior, 21, 671–688.
Kainz, D., & Ossimitz, G. (2002). Can students learn stock-ﬂow-thinking? An empirical investigation.
Paper presented at the System Dynamics Conference, Palermo, Italy.
Katehi, L., Pearson, G., & Feder, M. (2008). Engineering in K-12 Education: Understanding the status
and improving the prospects. Retrieved October 31, 2011, from http://www.nap.edu/catalog.php?
record_id=12635
Katz, S., Allbritton, D., & Connelly, J. (2003). Going beyond the problem given: How human tutors
use post-solution discussions to support transfer. International Journal of Artiﬁcial Intelligence in
Education, 13, 79–116.
Katz, S., Connelly, J., & Wilson, C. (2007). Out of the lab and into the classroom: An evaluation of
reﬂective dialogue in Andes. In R. Luckin & K. R. Koedinger (Eds.), Proceedings of AI in education, 2007 (pp. 425–432). Amsterdam, The Netherlands: IOS Press.
Kessler, C. (1988). Transfer of programming skills in novice Lisp learners (PhD dissertation).
Carnegie Mellon University, Pittsburgh, PA.
de Kleer, J., & Brown, J. S. (1981). Mental models of physical mechanisms and their acquisition.
In J. R. Anderson (Ed.), Cognitive skills and their acquisition (pp. 285–310). Mahwah, NJ:
Erlbaum.
Koedinger, K. R., Alibali, M. W., & Nathan, M. J. (2008). Trade-offs between grounded and abstract
representations: Evidence from algebra problem solving. Cognitive Science, 32, 366–397.
Koedinger, K. R., & Anderson, J. R. (1998). Illustrating principled design: The early evolution of a
cognitive tutor for algebra symbolization. Interactive Learning Environments, 5, 161–180.
Koedinger, K. R., & Nathan, M. J. (2004). The real story behind story problems: Effects of representations on quantitative reasoning. Journal of the Learning Sciences, 13(2), 129–164.
Kurtz dos Santos, A. d. C., & Ogborn, J. (1994). Sixth form students’ ability to engage in computational modelling. Journal of Computer Assisted Learning, 10(3), 182–200.
Lazonder, A., Hagemans, M. G., & de Jong, T. (2010). Offering and discovering domain information
in simulation-based inquiry learning. Learning and Instruction, 20, 511–520.
Lee, C. B., Jonassen, D., & Teo, T. (2011). The role of model building in problem solving and conceptual change. Interactive Learning Environments, 19(3), 247–265.
Leelawong, K., & Biswas, G. (2008). Designing learning by teaching agents: The Betty’s Brain
system. International Journal of Artiﬁcial Intelligence and Education, 18(3), 181–208.
Löhner, S. (2005). Computer based modeling tasks: The role of external representation (PhD dissertation). University of Amsterdam, Amsterdam, The Netherlands.
Löhner, S., Van Joolingen, W. R., & Savelsbergh, E. R. (2003). The effect of external representation
on constructing computer models of complex phenomena. Instructional Science, 31, 395–418.

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

Interactive Learning Environments

411

Löhner, S., Van Joolingen, W. R., Savelsbergh, E. R., & Van Hout-Wolters, B. (2005). Students’
reasoning during modeling in an inquiry learning environment. Computers in Human
Behavior, 21, 441–461.
Mandinach, E. B., & Cline, H. F. (1994). Classroom dynamics: Implementing a technology-based
learning environment. Mahwah, NJ: Erlbaum.
Marshall, S. P., Barthuli, K. E., Brewer, M. A., & Rose, F. E. (1989). Story problem solver: A schemabased system of instruction. San Diego, CA: Center for Research in Mathematics and Science
Education, San Diego State University.
Matsuda, N., Yarzebinski, E., Keiser, V., Raizada, R., Stylianides, G. J., Cohen, W. W., & Koedinger,
K. R. (2011). Learning by teaching SimStudent – An initial classroom baseline study comparing
with Cognitive Tutor. In G. Biswas & S. Bull (Eds.), Proceedings of the international conference
on artiﬁcial intelligence in education (pp. 213–221). Berlin: Springer.
Mayer, R. E. (1981). Frequency norms and structural analysis of algebra story problems into families,
categories and templates. Instructional Science, 10(2), 135–175.
McArthur, D., Lewis, M., Ormseth, T., Robyn, A., Stasz, C., & Voreck, D. (1989). Algebraic thinking
tools: Support for modeling situations and solving problems in Kids’ World. Santa Monica, CA:
RAND Corporation, p. 22.
Metcalf, S. J. (1999). The design of guided learning-adaptable scaffolding in interactive learning
environments (PhD dissertation). University of Michigan, Ann Arbor, MI.
Metcalf, S. J., Krajcik, J., & Soloway, E. (2000). Model-It: A design retrospective. In M. J. Jacobson
& R. B. Kozma (Eds.), Innovations in science and mathematics education: Advanced designs for
technologies of learning (pp. 77–115). Mahwah, NJ: Lawrence Erlbaum Associates.
Miller, R., Ogborn, J., Briggs, J., Borough, D., Bliss, J., Boohan, R., … Sakonidis, B. (1993).
Educational tools for computational modelling. Computers and Education, 21(3), 205–261.
Moxnes, E. (2000). Not only the tragedy of the commons: Misperceptions of feedback and policies for
sustainable development. System Dynamics Review, 16(4), 325–348.
Mulder, Y. G., Lazonder, A., & de Jong, T. (2010). Finding out how they ﬁnd it out: An
empirical analysis of inquiry learners’ need for support. International Journal of Science
Learning, 32(15), 2033–2053.
Mulder, Y. G., Lazonder, A. W., de Jong, T., Anjewierden, A., & Bollen, L. (2011). Validating and
optimzing the effects of model progression in simulation-based inquiry learning. Journal of
Science Education and Technology, 21, 722–729.
Muldner, K., Burleson, W., van de Sande, B., & VanLehn, K. (2011). An analysis of students’ gaming
behaviors in an intelligent tutoring system: Predictors and impacts. User Modeling and UserAdapted Interaction, 21(1–2), 99–135.
Nathan, M. J. (1998). Knowledge and situational feedback in a learning environment for algebra story
problem solving. Interactive Learning Environments, 5, 135–159.
Nathan, M. J., Kintsch, W., & Young, E. (1992). A theory of algebra-word-problem comprehension
and its implications for the design of learning environments. Cognition and Instruction, 9(4),
329–389.
National Research Council. (2012). A framework for K-12 science education: Practices, crosscutting
concepts, and core ideas. Washington, DC: National Academies Press.
Neumann, E. K., Feurzeig, W., & Garik, P. (1999). An object-based modelling tool for science inquiry.
In W. Feurzeig & N. Roberts (Eds.), Modelling and simulation in science and mathematics education (pp. 138–148). New York: Springer.
Novak, J. D., & Canas, A. J. (2008). The theory underlying concept maps and how to construct and
use them. Pensacola, FL: Florida Institute for Human and Machine Cognition.
Obayashi, F., Shimoda, H., & Yoshikawa, H. (2000). Construction and evaluation of a CAI system
based on “Learning by teaching” to Virtual Student. Transactions of Information Processing
Society of Japan, 41(12), 3386–3393.
Paige, G., & Simon, H. A. (1966). Cognitive processes in solving algebra word problems. In B.
Kleinmuntz (Ed.), Problem solving: Research, method and theory (pp. 51–119). New York: Wiley.
Palinscar, A. S., & Brown, A. L. (1984). Reciprocal teaching of comprehension-fostering and comprehension-monitoring activities. Cognition and Instruction, 1, 117–175.
Papert, S. (1980). Mindstorms. New York: Basic Books.
Pareto, L., Arvemo, T., Dahl, Y., Haake, M., & Gulz, A. (2011). A teachable-agent arithmetic game’s
effects on mathematics understanding, attitude and self-efﬁcacy. In G. Biswas & S. Bull (Eds.),
Proceedings of artiﬁcial intelligence in education (pp. 247–255). Berlin: Springer.

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

412

K. VanLehn

Penner, D. E. (2001). Cognition, computers and synthetic science: Building knowledge and meaning
through modelling. Review of Research in Education, 25, 1–37.
Pfundt, H., & Duit, R. (1998). Bibliography: Students’ alternative frameworks and science education
(5th ed.). Kiel, Germany: Institute for Science Education.
Ploetzner, R., & VanLehn, K. (1997). The acquisition of informal physics knowledge during formal
physics training. Cognition and Instruction, 15(2), 169–206.
Quinn, J., & Alessi, S. M. (1994). The effects of simulation complexity and hypothesis-generation
strategy on learning. Journal of Research in Computing in Education, 27(1), 75–92.
Quintana, C., Reiser, B. J., Davis, E. A., Krajcik, J., Fretz, E., Duncan, R. G., … Soloway, E. (2004).
A scaffolding design framework for software to support scientiﬁc inquiry. Journal of the Learning
Sciences, 13(3), 337–386.
Ramachandran, S., & Stottler, R. (2003). A meta-cognitive computer-based tutor for high-school
algebra. In D. Lassner & C. McNaught (Eds.), Proceedings of world conference on educational
multimedia, hypermedia and telecommunications 2003 (pp. 911–914). Chesapeake, VA: AACE.
Razzaq, L., Mendicino, M., & Heffernan, N. T. (2008). Comparing classroom problem-solving with
no feedback to Web-based homework assistance. In B. P. Woolf, E. Aimeur, R. Nkambou, & S. P.
Lajoie (Eds.), Intelligent tutoring systems: 9th international conference, ITS2008 (pp. 426–437).
Berlin: Springer.
Reif, F., & Scott, L. A. (1999). Teaching scientiﬁc thinking skills: Students and computers coaching
each other. American Journal of Physics, 67(9), 819–831.
Repenning, A., Ioannidou, A., & Zola, J. (2000). AgentSheets: End-user programmable simulations.
Journal of Artiﬁcial Societies and Social Simulations, 3(3). Retrieved from http://jasss.soc.surrey.
ac.uk/3/3/forum/1.html
Schecker, H. (1993). Learning physics by making models. Physics Education, 28, 102–106.
Schwartz, D. L., Blair, K. P., Biswas, G., Leelawong, K., & Davis, J. (2008). Animations of thought:
Interactivity in the teachable agent paradigm. In R. Lowe & W. Schnotz (Eds.), Learning with
animations: Research and implications for design (pp. 114–141). Cambridge: Cambridge
University Press.
Schwartz, D. L., Chase, C., Chin, D. B., Oppezzo, M., Kwong, H., Okita, S. Y., … Wagster, J. (2009).
Interactive metacognition: Monitoring and regulating a teachable agent. In D. J. Hacker, J.
Dunlosky, & A. C. Graesser (Eds.), Handbook of metacognition in education (pp. 340–358).
New York: Taylor & Francis.
Schwarz, C. V., Reiser, B. J., Davis, E. A., Kenyon, L., Acher, A., Fortus, D., … Krajcik, J. (2009).
Developing a learning progression for scientiﬁc modeling: Making scientiﬁc modeling accessible
and meaningful for learners. Journal of Research in Science Teaching, 46(6), 632–654.
Schwarz, C. V., & White, B. Y. (2005). Metamodeling knowledge: Developing students’ understanding of scientiﬁc modeling. Cognition and Instruction, 23(2), 165–205.
Segedy, J. R., Kinnebrew, J. S., & Biswas, G. (2012). Supporting student learning using converstational agents in a teachable agent environment. Paper presented at the Proceedings of the 10th
International Conference of the Learning Sciences, Sydney, Australia.
Sherin, B. L. (2006). Common sense clariﬁed: The role of intuitive knowledge in physics problem
solving. Journal of Research in Science Teaching, 43(6), 535–555.
Singley, M. K., & Anderson, J. R. (1989). The transfer of cognitive skill. Cambridge, MA: Harvard
University Press.
Steed, M. (1992). Stella, a simulation construction kit: Cognitive process and educational implications. Journal of Computers in Mathematics and Science Teaching, 11, 39–52.
Sterman, J. D., & Booth Sweeney, L. (2002). Cloudy skies: Assessing public understanding of global
warming. System Dynamics Review, 18(2), 207–240.
Stratford, S. J. (1997). A review of computer-based model research in precollege science classroom.
Journal of Computers in Mathematics and Science Teaching, 16(1), 3–23.
Swaak, J., van Joolingen, W. R., & de Jong, T. (1998). Supporting simulation-based learning: The
effects of model progression and assignments on deﬁnition and intuitive knowledge. Learning
and Instruction, 8(3), 235–252.
Tan, J., & Biswas, G. (2006). The role of feedback in preparation for future learning: A case study in
learning by teaching environments. In M. Ikeda, K. Ashley, & T.-W. Chan (Eds.), Intelligent
tutoring systems: 8th international conference, ITS 2006 (pp. 370–381). Berlin: Springer-Verlag.

Downloaded by [Arizona State University], [Mr Kurt VanLehn] at 13:35 11 July 2014

Interactive Learning Environments

413

Tan, J., Biswas, G., & Schwartz, D. L. (2006). Feedback for metacognitive support in learning by
teaching environments. In R. Sun & N. Miyake (Eds.), Proceedings of the 28th annual
meeting of the cognitive science society (pp. 828–833). Mahwah, NJ: Erlbaum.
Tan, J., Skirvin, N., Biswas, G., & Catley, K. (2007). Providing guidance and opportunities for selfassessment and transfer in a simulation environment for discovery learning. In D. S. McNamara &
J. G. Trafton (Eds.), Proceedings of the 29th annual meeting of the cognitive science society (pp.
1539–1544). Austin, TX: Cognitive Science Society.
Tan, J., Wagster, J., Wu, Y., & Biswas, G. (2007). Effect of metacognitive support on student behaviors in learning by teaching environments. In R. Luckin, K. R. Koedinger, & J. Greer (Eds.),
Proceedings of the 13th international conference on artiﬁcial intelligence in education
(pp. 650–652). Amsterdam: IOS Press.
Teodoro, V. D., & Neves, R. G. (2011). Mathematical modeling in science and mathematics education. Computer Physics Communications, 182, 8–10.
Thompson, K., & Reimann, P. (2010). Patterns of use of an agent-based model and a system dynamics
model: The application of patterns of use and the impacts on learning outcomes. Computers and
Education, 54, 392–403.
Treagust, D. F., Chittleborough, G., & Mamiala, T. (2002). Students’ understanding of the role
of scientiﬁc models in learning science. International Journal of Science Education, 24(4),
357–368.
VanLehn, K., Burleson, W., Chavez Echeagaray, M.-E., Christopherson, R., Gonzalez Sanchez, J.,
Hastings, J., … Zhang, L. (2011). The affective meta-tutoring project: How to motivate students
to use effective meta-cognitive strategies. Paper presented at the 19th International Conference on
Computers in Education, Chiang Mai, Thailand.
VanLehn, K., & van de Sande, B. (2009). Acquiring conceptual expertise from modeling: The case of
elementary physics. In K. A. Ericsson (Ed.), Development of professional expertise:
Toward measurement of expert performance and design of optimal learning environments
(pp. 356–378). Cambridge: Cambridge University Press.
Wagster, J., Tan, J., Biswas, G., & Schwartz, D. L. (2007). How metacognitive feedback affects behavior in learning and transfer. Paper presented at the 13th International conference on
Artiﬁcial Intelligence in Education: Workshop on Metacognition and Self-Regulated Learning
in ITSs, Marina del Rey, CA.
Wagster, J., Tan, J., Wu, Y., Biswas, G., & Schwartz, D. L. (2007). Do learning by teaching environments with metacognitive support help students develop better learning behaviors? In D. S.
McNamara & J. G. Trafton (Eds.), Proceedings of the 29th annual meeting of the cognitive
science society (pp. 695–700). Austin, TX: Cognitive Science Society.
Weld, D. S. (1983). Explaining complex engineered devices. Cambridge, MA: Bolt, Beranek and
Newman, p. 50.
Wheeler, J. L., & Regian, J. W. (1999). The use of a cognitive tutoring system in the improvement of
the abstract reasoning component of word problem solving. Computers in Human Behavior, 15,
243–254.
White, B. Y. (1984). Designing computer games to help physics students understand Newton’s Laws
of Motion. Cognition and Instruction, 1(1), 69–108.
White, B. Y. (1993). ThinkerTools: Causal models, conceptual change and science education.
Cognition and Instruction, 10(1), 1–100.
White, B. Y., & Frederiksen, J. R. (1990). Causal model progressions as a foundation for intelligent
learning environments. Artiﬁcial Intelligence, 42, 99–157.
Yaron, D., Karabinos, M., Lange, D., Greeno, J. G., & Leinhardt, G. (2010). The chemcollective–
virtual labs for introductory chemistry courses. Science, 328(5978), 584–585.

Collaborative Dialog While Studying
Worked-out Examples
Robert G.M. HAUSMANN1, Timothy J. NOKES1,
Kurt VANLEHN2, and Brett VAN DE SANDE2
1
Learning Research and Development Center, Pittsburgh Science of Learning Center
University of Pittsburgh, 3939 O’Hara Street, Pittsburgh, PA, 15260
2
School of Computing and Informatics, Ira A. Fulton School of Engineering
Arizona State University, P.O. Box 878809, Tempe, AZ 85287 - 8809

Abstract. Self-explaining is a beneficial learning strategy for studying worked-out
examples because it either supplies missing information through the generation of
inferences or because it provides a mechanism for repairing flawed mental models.
Although self-explanation is generated with the purpose of helping the individual,
is it also helpful to produce explanations in a collaborative setting? Can
individuals help each other infer missing information or repair their flawed mental
models collaboratively? To find out, we coded the dialog from dyads
collaboratively studying examples and contrasted it with individuals studying
examples alone. The results suggest that dyads were more likely to attempt to
reconcile the examples with their attempted solutions, and avoid shallow
processing of examples through paraphrasing.
Keywords. Self-explanation, peer collaboration, prior knowledge, physics

Introduction
Although studying solutions to problems can facilitate learning, there is always the
danger that students will study these examples shallowly and not learn much. One way
to increase the effectiveness of learning from examples is to prompt students to selfexplain each step [1]. Another way is to alternate examples with problem solving [2]. A
third possibility is to have students work together in dyads while studying examples.
We hypothesized that the participants working in dyads would have an opportunity to
engage in a wider range of constructive cognitive processes than they would when
explaining examples alone. Consistent with other research on collaboration, we found
that the dyads (at the group level) learned more from studying examples than solos as
measured by problem-solving performance [3]. The purpose of the current work is to
investigate and unpack the cognitive processes underlying this advantage.
In the experiment, our baseline condition was solos who were prompted to selfexplain examples that alternated with problem solving on the Andes tutoring system.
Our experimental group was dyads who did the same activities as solos. Because the
main (positive) result has already been published [3], this paper focuses on contrastive
analyses of the protocols aimed at finding out why the dyads learned more than the
solos.

1. Experimental Context and Coding the Verbal Protocols
Undergraduates enrolled in a second semester physics course were randomly assigned
to either the Solo (n = 11) or Dyad (n = 14) condition. During the experiment, the
students were asked to alternate between solving electrodynamics problems in Andes
and studying video-based, isomorphic examples. The data for example studying
consisted of coding the transcribed monolog or dialog generated by the participants.
To summarize the relevant findings from the experiment, the solos solved fewer
problems during the fixed, two-hour training period; the solos asked for more hints and
bottom-out hints when solving problems; and they made more deep errors than the
dyads [3]. This suggests that the dyads were more effective in solving the problems. To
better understand the problem-solving results, the verbal protocols collected during the
example-studying phase of the experiment were analyzed by categorizing the
verbalizations. Each of the four categories are defined in Table 1.
Table 1. Definitions for the coarse-grained coding scheme
Code
Explain

Definition
Typically, an explanation is an answer to the question why? In addition, explanations
identify the applicability conditions of a rule.

Prior

Relates an example step to any of the following: work on prior problems in the
experiment (either noting similarities or differences), or background knowledge from
physics course.

Meta-cognitive

A positive or negative assessment regarding one’s understanding of the material.

Paraphrase

A restatement of the given information. The restatement can be a verbatim repetition
of the information, the information stated in slightly different words, or a summary.

Because the dyads naturally generated more dialog than the solos (F(1, 20) = 6.18,
p < .05, d = 1.11), all of the subsequent analyses statistically controlled for the overall
number of coded statements using an analysis of covariance (ANCOVA). The results
for all four categories of coded dialog are reported in
Figure 1.
While controlling for the total amount of talk, there was no difference between the
two conditions in terms of the number of explanations produced, F < 1. However, there
was a marginal, negative correlation between the number of explanation statements and
the error rate for the dyad condition, r(10) = -.57, p = .06. The same negative
correlation, however, was not true for the solo condition. This suggests that the
explanations generated by the dyads helped them to avoid errors on later problems,
whereas the explanations of the solos were remarkably unhelpful.
For the other categories, there were reliable differences between the two groups in
terms of the amount of paraphrasing and use of prior knowledge. The solo condition (M
= 27.00, SE = 4.71) produced more paraphrases than the dyad condition (M = 7.74, SE
= 4.25), F(1, 19) = 8.14, p < .05, d = -.63. This result suggests that the solos were more
likely than the dyads to use paraphrasing as a method for studying the examples.
Because paraphrasing tends to produce less learning than self-explanation [4], the main
results (i.e., the solos learned less than dyads) may be in part due to the solos choosing
to do more shallow processing (i.e., paraphrasing) of the examples than the dyads.
The pattern of means was the opposite for the use of prior knowledge. The dyad
condition (M = 33.65, SE = 4.23) demonstrated a greater number of prior knowledge
episodes than the solo condition (M = 18.92, SE = 4.69). The difference in the dyads’

explicit use of prior knowledge was both statistically significant, with a large effect
size, F(1, 19) = 4.81, p < .05, d = 1.60. This may represent another contribution to the
dyads’ superior learning, as more reference to prior knowledge is often associated with
larger learning gains [5].

Average Frequency 

100.0 
80.0 
60.0 
40.0 
20.0 
0.0 
Total Talk  

Explana3on 
Solos 

Meta‐cog 

Paraphrase  

Prior  

Dyads 

Figure 1. The estimated marginal means (± standard error bars) for the frequency of each type of statement.

2. Discussion
The current “state of the art” in studying examples suggests that one should have
students alternate between studying an example and solving a problem [2]. When
studying the example, students should be prompted to self-explain [1]. We have shown
that further benefits can be obtained by having students do these activities in pairs
rather than alone [3]. This paper presented evidence from protocol analyses that point
toward a potential source of this extra benefit. It appears that the solos are spending
more time on paraphrasing the example, a type of shallow processing associated with
reduced learning [4]. In contrast, the dyads more frequently compared the example’s
solution with their solutions during the preceding problem solving [6]. This led to
better problem-solving performance with an intelligent tutoring system.

References
[1]
[2]
[3]
[4]
[5]
[6]

Chi, M.T.H., DeLeeuw, N., Chiu, M.-H., LaVancher, C.: Eliciting self-explanations improves
understanding. Cognitive Science 18 (1994) 439-477
Trafton, J.G., Reiser, B.J.: The contributions of studying examples and solving problems to skill
acquisition. Proceedings of the Fifteenth Annual Conference of the Cognitive Science Society. Erlbaum,
Hillsdale, NJ (1993) 1017-1022
Hausmann, R.G.M., van de Sande, B., VanLehn, K.: Shall we explain? Augmenting learning from
intelligent tutoring systems and peer collaboration. In: Woolf, B.P., Aimeur, E., Nkambou, R., Lajoie, S.
(eds.): Intelligent Tutoring Systems. Springer-Verlag, Berlin (2008) 636–645
Chi, M.T.H., Bassok, M., Lewis, M.W., Reimann, P., Glaser, R.: Self-explanations: How students study
and use examples in learning to solve problems. Cognitive Science 13 (1989) 145-182
Bransford, J.D., Brown, A.L., Cocking, R.R. (eds.): How people learn: Brain, mind, experience, and
school. National Academy Press, Washington, D.C. (2000)
Nokes, T.J., VanLehn, K.: Bridging principles and examples through analogy and explanation. In:
Kirschner, P.A., Prins, F., Jonker, V., Kanselaar, G. (eds.): Proceedings of the Eighth International
Conference for the Learning Sciences -- ICLS 2008, Vol. 3. ISLS, The Netherlands (2008) 100-102

International Journal of Artificial Intelligence in Education 21 (2011) 83–113
DOI 10.3233/JAI-2011-014
IOS Press

83

An Evaluation of Pedagogical Tutorial Tactics for a Natural
Language Tutoring System: A Reinforcement Learning
Approach
Min Chi, Human-Sciences and Technologies Advanced Research Institute, Stanford
University, CA, USA
minchi@stanford.edu
Kurt VanLehn, School of Computing, Informatics and Decision Science Engineering,
Arizona State University, AZ, USA
Kurt.Vanlehn@asu.edu
Diane Litman, Department of Computer Science and Intelligent Systems Program and
Learning Research and Development Center, University of Pittsburgh, Pittsburgh, PA, USA
litman@cs.pitt.edu
Pamela Jordan, Learning Research and Development Center, University of Pittsburgh,
Pittsburgh, PA, USA
pjordan@pitt.edu
Abstract. Pedagogical strategies are policies for a tutor to decide the next action when there are multiple actions
available. When the content is controlled to be the same across experimental conditions, there has been little evidence
that tutorial decisions have an impact on students’ learning. In this paper, we applied Reinforcement Learning (RL)
to induce two sets of pedagogical policies from pre-existing human interaction data. The NormGain set was derived
with the goal of enhancing tutorial decisions that contribute to learning while the InvNormGain set was derived with
the goal of enhancing those decisions that contribute less or even nothing to learning. The two sets were then tested
with human students. Our results show that when the content was controlled to be the same, different pedagogical
policies did make a difference in learning and more specifically, the NormGain students outperformed their peers.
Overall our results suggest that content exposure and practice opportunities can help students to learn even when
tutors have poor pedagogical tutorial tactics. However, with effective tutorial tactics, students can learn even more.
Keywords. Reinforcement learning, human learning, intelligent tutoring systems, pedagogical strategy

INTRODUCTION
Human one-on-one tutoring is one of the most effective educational interventions in that tutored students
often perform significantly better than students in classroom settings (Bloom, 1984). Computer learning
environments that mimic aspects of human tutors have also been highly successful. Intelligent Tutoring
Systems (ITSs) have been shown to be highly effective in improving students’ learning in the classroom.
Classroom instruction with an ITS produces measurably larger learning gains than the same classroom
instruction without an ITS (Anderson, Corbett, Koedinger, & Pelletier, 1995; Koedinger, Anderson, Hadley,
& Mark, 1997; VanLehn, Lynch, & et al., 2005). One hypothesis as to the effectiveness of human or computer
one-on-one tutoring is that it comes from the detailed management of “micro-steps” in natural language
1560-4292/11/$27.50 © 2011 – IOS Press and the authors. All rights reserved

84

M. Chi et al. / An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System

[Problem Statement:]
A 0.6kg rock in space has a velocity of magnitude 2.0m /s at point A and kinetic energy of 7.50J at
point B. What is the net work done on the rock as it moves from A to B? We define:
T0 : the time point when the rock is at point A.
T1 : the time point when the rock is at point B.
[Solution:]
Step 1: Applying the Definition of Kinetic Energy to solve for the rock’s kinetic energy at T0 : KE0 =

1
2
2 mv0

Step 2: Applying the Definition of Total Mechanical Energy to solve for the rock-system’s Total Mechanical
Energy at T0 , T ME0 = KE0 + 0 + 0
Step 3: Applying the Definition of Total Mechanical Energy to solve for the rock-system’s Total Mechanical
Energy at T1 , T ME1 = KE1 + 0 + 0
Step 4: Applying the Change of Total Mechanical Energy for Non-isolated Systems to solve for the work done
on the rock-system from T0 to T1 : NetW = T ME1 − T ME0

Fig. 1. A Four-step solution for training problem: P4 .

tutorial dialogue (Graesser, Person, & Magliano, 1995; Graesser, VanLehn, Rosé, Jordan, & Harter, 2001).
A typical ITS, however, is step-based (VanLehn, 2006).
In domains like math and physics, solving a problem requires producing an argument, proof or derivation
consisting of one or more inference steps; each step is the result of applying a domain principle, operator or
rule. For example, to solve a physics problem, generally several physics principles need to be applied and
some need to be applied more than once. Each principle application can be seen as a step in ITSs. Once a
student enters a step, then the ITS gives feedback and/or hints.
For example, on an ITS such as Andes the solution for a quantitative physics problem P4 involves four
main steps (shown in Fig. 1) along with some minor steps that are omitted here for simplicity. Among the
four steps, the principle “Definition of Total Mechanical Energy (TME = KE + GPE + SPE)” is applied
twice, steps 2 and 3 respectively; the “Definition of Kinetic Energy (KE = 21 mv2 )” and the “Change of
Total Mechanical Energy for Non-isolated Systems (NetW = TME2 − TME1 ). ” are applied once each.
Human tutors, by contrast, often scaffold students via a series of micro-steps that lead to a full step. In
[Step 1] of the solution for P4 above, for instance, a human tutor may take four micro-level steps (as shown
in Fig. 2). In Fig. 2, each numbered line represents a dialogue turn. The labels T and S designate tutor and
student turns respectively. In this example, the four micro-steps are selecting the principle to apply (lines
2 & 3), writing the corresponding equation (line 4), solving the equation (Line 5), and engaging in some
qualitative discussion about the principle, definition of Kinetic Energy (line 6–8) respectively.
It is often hypothesized that students will learn more when solving problems by progressing through
a series of micro-steps than through steps. When students are trained on step-based ITSs such as Andes,
they may mainly focus on learning quantitative forms of the domain principles and ignore the qualitative
perspective of physics concepts or domain principles. For instance, students may remember to use an
equation like KE = 21 mv2 without knowing that they are applying the definition of Kinetic Energy or
that the kinetic energy of an object is not affected by the direction of the object’s motion. As a result,
they may fail to transfer their quantitative problem solving skills to qualitative problems such as: “ While
flying along, a jet releases an empty propellant tank which eventually crashes to the ground. Consider three
cases: a) the jet is flying horizontally, b) the jet is climbing upwards at a 45 degree angle or, c) the jet is
diving downwards at a 45 degree angle. In all 3 cases, the jet is flying at the same speed and it releases the
propellant tank from the same height. Ignoring air friction, in which case is the tank’s speed greatest as it
hits the ground? (Correct Answer is: It doesn’t matter. The speed is the same in all three cases.)”

M. Chi et al. / An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System

85

1. T: So let’s start with determining the value of KE0 .
2. T: Which principle will help you calculate the rock’s kinetic energy at T0? Please provide the name of the
principle, not an equation. {ELICIT}
3. S: Definition of kinetic energy
4. T: Yes, I agree. Now I will write the equation for applying the definition of kinetic energy to the rock at
T0: KE0 = 1/2*m*v0^2 {TELL}
5. T: I calculate the value for KE0 = 0.5*0.6kg*2.0 m/s^2 = 1.2 J {TELL}, KC23
6. T: We do not know the direction of the rock’s velocity at T0 from the problem statement. Would the
direction of v0 affect the rock’s kinetic energy at T0? {ELICIT}, KC20
7. S: No, it would not affect
8. T: Yes, I agree. This is because the kinetic energy only depends on the rock’s mass and magnitude of
velocity, not the direction of the rock’s velocity. {TELL}, KC20
9. T: We’ve calculated KE0, recall that our plan is to calculate v1 next.

Fig. 2. An example dialogue.

Indeed, Halloun & Hestenes (1985) showed that even university physics students with high grades often
score poorly on tests of qualitative physics. Additionally, in a review of research on problem solving,
Maloney (1994) noted that there is surprisingly very little evidence on the role of problem solving in
learning physics concepts. On the other hand, a large body of research on comparing experts and novices
on physics problem solving indicates the importance of qualitative reasoning in physics instruction (Chi,
Feltovich, & Glaser, 1981; Larkin, McDermott, Simon, & Simon, 1980). For example, experts tend to be
fast at qualitatively understanding the problem situation and at representing the problem in terms of basic
physics concepts before translating it into mathematical equations while novices often rush into applying
equations with little or no qualitative description of the problem. Here we believe that by breaking a step
into a series of micro-steps as in Fig. 2, students are exposed to more qualitative discussion and reasoning
as shown in lines 6-8 and thus are more likely to learn domain principles in a deeper manner.
If the effectiveness of human one-on-one tutoring lies in tutors’ ability to scaffold a series of micro-steps
leading to a step entry, then we would expect human tutors to be more effective than step-based tutors as
both require students to enter the same major steps. There have been several tests of this hypothesis.

Prior research on the impact of micro-steps on learning
Evens & Michael (2006) conducted a series of studies comparing four learning treatments in cardiovascular
physiology. The no-tutoring group studied a text that included correct worked examples along with the
reasoning for solving a pacemaker problem. The CIRCSIM group solved one training problem on a tutoring
system, CIRCSIM, which presented a short text passage for each incorrect step. The CIRCSIM-tutor group
solved the same training problem on a sophisticated natural language tutoring system, CIRCSIM-Tutor,
which replaced the text passages in CIRCSIM with typed natural language dialogue. The human tutor
group also solved the same training problem with expert human tutors. Results showed that the latter three
groups out-performed the no-tutoring group, but the three treatments, CIRCSIM, CIRCSIM-Tutor and
expert human tutors, tied with each other.
Reif & Scott (1999a) compared three groups in their study. The no-tutoring group did their homework
and received no feedback until their homework was returned. The step-tutoring group did their homework
on a step-based tutoring system; and the human tutor group did their physics homework problems with the

86

M. Chi et al. / An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System

aid of a human tutor. In their experiment all groups were in the same physics class; the experiment varied
only in the way that the students did their homework. The results again showed that the human tutor and
the step-based tutoring system groups achieved learning gains but were not reliably different, and yet both
were reliably larger than the gains of the no-tutoring group.
Finally, VanLehn, Graesser, & et al. (2007) conducted seven experiments in conceptual physics. In their
experiments, all groups of students first studied a short textbook and then worked on several tasks that
involved answering conceptual physics questions. For each question, the students wrote a short essay as
their initial answer, were tutored on missing or incorrect steps, and then read a correct well-written essay.
The treatments differed only in how students were tutored when the essay lacked a step or had an incorrect
or incomplete step. There were five treatments. The human tutor group communicated via a text-based
interface with an expert human tutor. The Why2-Atlas and Why2-AutoTutor groups used two Natural
Language (NL) Dialogue-based Tutoring systems named Why2-Atlas and Why2-AutoTutor respectively.
The canned text group read the same tutorial content as Why2-Atlas; in fact this group can be seen as
training on a step-based tutoring system that uses text instead of dialogue for getting students to enter a step
correctly. Finally, the no-tutoring group read from a textbook without answering conceptual questions.
The results across the seven experiments showed that the learning gains of the three tutoring groups
and the canned text group were not reliably different, and were all higher than the read-only no-tutoring
group except in one experiment, Experiment 4 (VanLehn, Graesser, & et al., 2007). Experiment 4, found that
human tutoring was more effective than reading the canned text. However, upon reviewing transcripts of the
tutoring, the authors concluded that the Experiment 4 materials were too far over the students’ current level
of competence, so reading the canned text’s remedial text probably didn’t suffice for comprehension, and
yet a human tutor was able to help explain the content in novice terms (VanLehn, Graesser, & et al., 2007).
To summarize, once content was controlled to be the same across all groups, neither human tutors nor
Natural Language (NL) tutoring systems designed to mimic human tutors, reliably outperformed step-based
systems (Evens & Michael, 2006; VanLehn, Graesser, & et al., 2007). All three types of tutors, however,
were more effective than no instruction (e.g., students reading material and/or solving problems without
feedback or hints). Several techniques can be employed to control for content. For example, in some of the
studies described here the domain content was controlled by ensuring students worked on the same training
problems with the same human tutors or on a computer tutor that was scripted by the same human tutors
(Evens & Michael, 2006; VanLehn, Graesser, & et al., 2007; Reif & Scott, 1999b). Additionally, content can
be controlled to be equivalent by running a human tutoring group first, videotaping the tutoring sessions,
and then having collaborating pairs of students watch those videotapes (Chi, Roy, & Hausmann, 2008).
One possible conclusion is that tutoring is effective, but that the micro-steps of human tutors and NL
tutoring systems provide no additional value beyond conventional step-based tutors (VanLehn, 2008). On
the other hand, such a conclusion may be premature. It could simply be that neither human tutors nor their
computer mimics are good at making micro-step decisions. That is, the use of micro-steps could potentially
be better, but human tutors (and their mimics) may lack the pedagogical skills to make appropriate decisions
about which micro-steps to use when.

Do human tutors make effective tutorial decisions?
For any form of one-on-one tutoring, the tutor’s behavior can be viewed as a sequential decision process
where in, at each discrete step, the tutor is responsible for selecting the next action to take. For instance, some
of the tutor turns in Fig. 2 are labeled {ELICIT} or {TELL}. This label designates a tutorial decision step
wherein the tutor has to decide whether to elicit the requisite information with a question or to tell the student
the information. In line 2 in Fig. 2, the tutor chooses to elicit the answer from the student by asking the
question, “Which principle will help you calculate the rock’s kinetic energy at T0? Please provide the name

M. Chi et al. / An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System

87

of the principle, not an equation.” If the tutor elected to tell the students, however, then he or she would
have stated, “To calculate the rock’s kinetic energy at T0, let’s apply the definition of Kinetic Energy.” Both
actions cover the same target knowledge content.
Human tutors must make many rapid decisions in order to keep the tutorial dialogue flowing smoothly.
Each of these tutorial decisions affects a student’s successive actions and performance. It is often unclear
how to make each decision most effectively, because its impact on learning may not be immediate or
observed immediately, and more importantly decisions may not be independent of one another; in other
words, the effectiveness of one decision also depends on the effectiveness of subsequence decisions.
Pedagogical strategies are defined as policies for deciding the next tutorial action when multiple options
are available. It is commonly assumed that the effectiveness of human expert tutors is because they have
effective pedagogical strategies (Graesser, Person, & Magliano, 1995). In order to exhibit effective pedagogical strategies, a tutor should adapt his or her behavior to students’ needs including students’ current
knowledge levels, general aptitudes, emotional states and other salient features. However, previous research
indicates that human tutors may not make such adaptations (Cade, Copeland, Person, & D’Mello, 2008; Chi,
Siler, & Jeong, 2004; Katz, Connelly, & Wilson, 2007; Evens & Michael, 2006; Merrill, Reiser, Ranney, &
Trafton, 1992; VanLehn, Siler, Murray, Yamauchi, & Baggett, 2003). For example, Chi, Siler, and Jeong
(2004) found that human tutors do not seem to possess an accurate model of students’ knowledge levels
during the tutoring. Similarly, Putnam (1987) found that experienced tutors do not attempt to form detailed
models of students’ knowledge before attempting remedial instruction. Rather, each teacher appeared to
move through a general curricular script irrespective of a student’s state.
To summarize, although it is commonly assumed that human expert tutors have effective pedagogical
strategies that lead them to make appropriate tutorial decisions, little evidence has been presented to date
demonstrating this. Therefore, one explanation for the lack of difference among human tutors, Natural
Language (NL) tutoring systems, and step-based tutoring systems in previous studies is that neither human
tutors nor their computer mimics are always good at making micro-step decisions. In order to test this
hypothesis, we investigated whether micro-step decision making matters to learning.

Primary research question
We focused on pedagogical strategies that govern tutorial interactions at the level of micro-steps. We use
the term “pedagogical tutorial tactics” to refer to the pedagogical policies for selecting the tutorial action
at each micro-step when there are multiple action options available. Our primary goal is to investigate
whether pedagogical tutorial tactics impact students’ learning.
In order to investigate the effect of pedagogical tutorial tactics on learning, it was necessary to separate
tutorial decisions from instructional content, strictly controlling content so that it is equivalent for all
students. It is generally difficult to control tutoring content with human tutors. Computer tutors, on the
other hand, permit much greater control over, and tracking of, the tutorial content than do human tutors
(Evens & Michael, 2006; VanLehn, Graesser, & et al., 2007; Reif & Scott, 1999b). In this project a Natural
Language (NL) tutoring system, named Cordillera, was implemented to teach college students introductory
physics.
Tutoring in domains like math and science is often structured as a two-loop procedure. An outer loop
selects the problem or task the student should work on next, while the inner loop governs step level decisions
during problem solving (VanLehn, 2006). In order to minimize content variation, all participants in this
project solved the same training problems in the same order and followed the same major problem-solving
steps for each problem. Moreover, the same micro-step content was presented to all students regardless of
condition even though some students experienced the tell version and some experienced the elicit version.

88

M. Chi et al. / An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System

In short, our primary research question is whether pedagogical tutorial tactics impact students’ learning
if the instructional content is controlled so that it is equivalent for all students. Next, we will briefly describe
our general approach.

GENERAL APPROACH
Our approach was to apply a general data-driven methodology, Reinforcement Learning (RL), to induce
pedagogical tutorial tactics or pedagogical policies directly from pre-existing training corpora. In the
context of Reinforcement Learning, it is more appropriate to use the term “pedagogical policies”; while in
the context of tutoring and learning it is more appropriate to talk about “pedagogical tutorial tactics”. In
this paper, the pedagogical policies induced by RL are the same as “pedagogical tutorial tactics” employed
on tutoring systems. In the following we use both terms where it is proper.
One corpus used in this project was the Exploratory corpus. It was collected in 2007. 64 college students,
the Exploratory group, were trained on a version of Cordillera, called random-Cordillera, where certain
tutorial decisions were made randomly.
Previously, we investigated whether the RL-induced pedagogical tutorial tactics would improve students’ learning (Chi, Jordan, VanLehn, & Litman, 2009). In that study a set of policies was induced
from the Exploratory corpus. Those policies were named DichGain because when applying RL, we used
dichotomized learning gains as the reward function so that there were only two levels of reward. The induced
DichGain policies replaced the random policy in Cordillera and the new version was named DichGainCordillera. Apart from following different policies (random vs. DichGain), the remaining components of
Cordillera, including the GUI interface, the training problems, and the tutorial scripts, were left untouched.
DichGain-Cordillera’s effectiveness was tested by training a new group of 37 college students in 2008. It
was shown that no significant overall difference was found between the two groups on the pretest, posttest,
or the Normalized Learning Gains (NLGs)1 (Chi, Jordan, VanLehn, & Litman, 2009; Chi, 2009).
There were at least two potential reasons for this lack of difference. First, it might be caused by limitations
in our RL approach; for example, in order to induce the DichGain policies, we defined only 18 features and
used a greedy procedure to select a small subset of these for the state representation (Chi, Jordan, VanLehn,
& Litman, 2009). Second, rather than randomly assigning students into the two groups, the Exploratory
data were collected in 2007 while the DichGain data was collected in 2008.
Therefore, in this study we included multiple training datasets, a larger feature set and more feature
selection approaches in our RL approach and ran a full comparison by randomly assigning students to
two comparable groups. More specifically, we induced two sets of tutorial tactics: the Normalized Gain
(NormGain) tactics were derived with the goal of enhancing tutorial decisions that contribute to learning
while the Inverse Normalized Gain (InvNormGain) tactics were derived with the goal of enhancing those
decisions that contribute less or even nothing to learning. The two sets of policies were then compared
by having all students study the same materials and use versions of Cordillera with identical subject
matter, training problems, tutorial scripts and user interface. Because all students studied the same content,
we expected all students to learn, even those in the InvNormGain group. If our application of RL to
induce pedagogical tutorial tactics is effective, then we expect that the NormGain students will learn
more than their InvNormGain peers. This would occur if the micro-level decisions on ET and JS impact
learning.
NLG = posttest − pretest/1 − pretest. Here posttest and pretest refer to the students’ test scores before and
after the training respectively; and 1 is the maximum score.
1

M. Chi et al. / An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System

89

In the sections that follow, the first describes the two types of tutorial decisions, the second explains
how we applied RL to induce the pedagogical tutorial tactics in this study, the third describes our methods
and includes an introduction to the Cordillera system, procedures, and so on, while the fourth reports our
empirical results and some related log analysis. Finally, in the last section we present a post-hoc comparison
of all four policy groups.

TWO TYPES OF TUTORIAL DECISIONS
We focused on two types of tutorial decisions: Elicit/Tell (ET) and Justify/Skip-justify (JS). We choose
these two micro-step types because there is no widespread consensus on how or when these actions should
be taken, as the literature review in the rest of this section shows.

Elicit/tell
During the course of one-on-one tutoring, the tutor often faces a simple decision, to elicit the next step
information from a student, or to tell a student the next step directly. We refer to such tutorial decisions as
elicit/tell (ET) decisions.
While a lecture can be viewed as a monologue consisting of an unbroken series of tells, human one-on-one
tutoring is characterized by a mixture of tutor elicits and tells. Some existing theories of learning suggest
that when making tutorial decisions, a tutor should adapt his or her actions to the students’ needs based
upon their current knowledge level, affective state, and other salient features (Vygotsky, 1971; Collins,
Brown, & Newman, 1989; Koedinger & Aleven, 2007). Typically, these theories are considerably more
general than the specific interaction decisions that system designers must make. This makes it difficult
to instantiate these theories into specific pedagogical policies in ITSs. Therefore, when facing a decision
whether to elicit or to tell a new step, most existing ITSs always decide to elicit first (Anderson, Corbett,
Koedinger, & Pelletier, 1995; Koedinger, Anderson, Hadley, & Mark, 1997; VanLehn, Lynch, & et al.,
2005).
Figure 3 presents a pair of sample dialogues comparing elicit and tell versions of a single tutorial dialogue
extracted from Cordillera log files collected during this project. Both dialogues begin and end with the same
tutor turns (lines 1 and 6 in (a) and 1 and 4 in (b)). However, in dialogue (a) the tutor chooses to elicit twice
(lines 2-3 and 4-5 respectively) while in dialogue (b) the tutor decides to tell twice (lines 2 and 3). Note
that the two dialogues cover the same domain content.

Justify/skip-justify
The second tutorial decision was whether to execute a justification step. During the tutoring process, human
tutors sometimes ask students to justify a step they have taken or an entry they have made. It appears that
the tutors’ goal is to help students understand domain knowledge in a deeper way. They are prompting the
student to provide a certain kind of self-explanation (Chi & VanLehn, 1991). The open question is whether
or not the tutor should conduct an elaborate discussion of a problem solving step given that this discussion
is not necessary for the solution. We refer to such tutorial decisions as justify/skip-justify (JS) decisions.
Much previous research including (Chi, de Leeuw, Chiu, & LaVancher, 1994; Conati & Van- Lehn, 2000;
Aleven, Ogan, Popescu, Torrey, & Koedinger, 2004) found that asking students to justify their solution
steps improves student learning. However, eliciting such a discussion may not always be desirable if, for
example, the student is well aware of the rationale. If so, typing in a justification can be slow, frustrating,
and distracting. Indeed, in domains like second language acquisition, Wylie et al found that tutors asking

90

M. Chi et al. / An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System

(a) Elicit Version
1. T: So let’s start with determining the value of v1.
2. T: Which principle will help you calculate the rock’s instantaneous magnitude of velocity at T1?
{ELICIT}
3. S: definition of kinetic energy
4. T: Please write the equation for how the definition of kinetic energy applies to this problem at T1
{ELICIT}
5. S: ke1 = 0.5*m*v1^2
6. T: From KE1 = 0.5*m*v1^2, · · ·

(b) Tell Version
1. T: So let’s start with determining the value of v1.
2. T: To calculate the rockś instantaneous magnitude of velocity at T1, we will apply the definition of kinetic
energy again. {TELL}
3. T: Let me just write the equation for you: KE1 = 0.5*m*v1^2. {TELL}
4. T: From KE1 = 0.5*m*v1^2, · · ·

Fig. 3. Elicit vs. Tell.

students to justify did not lead to better learning outcomes and significantly increased student training time
when compared to a control group that was not asked to enter justifications (Wylie, Koedinger, & Mitamura,
2010). Additionally, Katz, O’Donnell, and Kay (Katz, O’Donnell, & Kay, 2000) found that in some cases
it may be better to delay the justifications until the problem has been solved, especially if the justification
is abstract, plan-based, or lengthy.
Figure 4 presents a pair of sample dialogues comparing justify and skip-justify versions of a single
tutorial dialogue extracted from Cordillera’s log files. In part (a), a justification is requested (lines 3-4);
while in part (b), the justification request is skipped.
In summary, neither the ET (elicit/tell) nor JS (justify/skip-justify) steps are well-understood, and there is
no widespread consensus on how or when they should be used. This is why one of our research objectives is
to derive tutorial tactics for them from empirical data. If expert human tutors do not always make optimal, or
effective, tutorial decisions, then hand-crafting rules based upon human data may be a suboptimal strategy.
Inducing data directly from interaction data may be a more suitable method. In the next sections, we will
briefly describe how we applied RL to induce the pedagogical tutorial tactics and then describe our study
and finally present our results.

APPLYING RL TO INDUCE NORMGAIN AND INVNORMGAIN PEDAGOGICAL
TUTORIAL TACTICS
RL and a Markov Decision Process (MDP)
Previous research on using RL to improve dialogue systems (e.g. (Levin & Pieraccini, 1997; Singh, Kearns,
Litman, & Walker, 1999)) has typically used MDPs (Sutton & Barto, 1998) to model dialogue data. An
MDP describes a stochastic control process and formally corresponds to a 4-tuple (S, A, T, R), in which:

M. Chi et al. / An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System

91

(a) Justify Version
1. T: Can we infer the direction of the velocity of the rock at T1 from the rock’s kinetic energy at T1?
{ELICIT}
2. S: nope.
3. T: Excellent! Please explain why.{JUSTIFY, ELICIT}
4. S: Only the magnitude of the velocity and not the direction of it is part of the definition of kinetic energy.
5. T: Excellent! Now that we know v1,· · · .

(b) Skip-justify Version
1. T: Can we infer the direction of the velocity of the rock at T1 from the rock’s kinetic energy at T1?
{ELICIT}
2. S: nope.
3. T: Excellent! {Skip-JUSTIFY}
4. T: Now that we know v1,· · · .

Fig. 4. Justify vs. Skip-justify.

S = {S1 , . . . , Sn } is a state space.
A = {A1 , . . . , Am } is an action space represented by a set of action variables.
T : S × A × S → [0, 1] is a set of transition probabilities between states that describe the dynamics of the
modeled system; for example: P(Sj |Si , Ak ) is the probability that the model would transition from
state Si to state Sj by taking action Ak .
R : S × A × S → R denotes a reward model that assigns rewards to state transitions and models payoffs
associated with such transitions.
Additionally, π : S → A is defined as a policy.
The central idea behind this approach is to transform the problem of inducing effective pedagogical
tactics into one of computing an optimal policy for choosing actions in an MDP. Note that in order for RL
to be feasible, the number of states and actions should not be too large. On the other hand, when an MDP
is constructed for tutorial dialogues, it can have millions of distinct dialogue states and tutor utterances.
Here both states and actions in an MDP are abstract.
For instance, if we use only two features to represent the learning context: whether or not a student is
engaged and whether or not the student already knows how to do the next step. All possible combinations
of these two features would result in four states in the MDP. They are {NK, DK, NU, DU} where N =
“engaged”, D = “disengaged”, K = “next step probably known”, U = “next step probably not known”.
Similarly, there might be just two actions in the MDP: “Elicit” in the MDP might denote any informationseeking questions asked by the tutor, and “Tell” represents all other tutorial utterances. Thus, “state” and
“action” have different meanings in an MDP versus in a tutorial dialogue. In order to induce a policy from
the MDP perspective, there must be deterministic functions for mapping from dialogue states to MDP states
and from dialogue actions to MDP actions.
In this project we applied Policy Iteration (Sutton & Barto, 1998) to induce policies. In order to apply
Policy Iteration, we need to learn transition probabilities T from a training corpus , which is a collection
of system-student tutorial dialogues. A system-student tutorial dialogue is generated as each student solves

92

M. Chi et al. / An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System

a series of training problems on an ITS. For each tutorial dialogue, a scalar performance measure called
reward, R, is calculated.
For example, a common choice for R in ITSs is student learning gain. In this project the reward function
R is based on Normalized Learning Gain (NLG), which measures a student’s gain factoring out his/her
incoming competence. Following Singh et al. (1999), we can view each system-student interaction log di
as a trajectory in the MDP state space determined by the system actions and student responses as follows:
sd1i
j

j
sdi

ad1 ,rd1

−→
i

i

sd2i

ad2 ,rd2

−→
i

i

nd

nd

i
i
nd adi ,rdi
· · · sdi i −→

j

ad ,rd

j+1

j

Here
−→ sdi indicates that at the jth turn in the tutorial dialogue di , the system is in MDP state sdi ,
j
j
j+1
executes MDP action adi , receives MDP reward rdi , and then transitions to MDP state sdi . The number of
turns in di is ndi . In this project, only terminal dialogue states have non-zero rewards because a student’s
learning gain is measured after the entire tutorial dialogue is completed.
For instance, suppose the MDP state space = {NK, DK, NU, DU} as described above and the MDP
actions are E, T where E = Elicit and T = Tell. Then the dialogue fragment of Fig. 2 might be represented as:
i

i

E,0

T,0

T,0

E,0

T,0

DU −→ NU −→ NU −→ NK −→ NK −→
Dialogue sequences obtained from the training corpus  then can be used to empirically estimate the
transition probabilities T as: T = {p(Sj |Si , Ak )}k=1,···,m
i,j=1,···,n . More specifically, p(Sj |Si , Ak ) is calculated by
taking the number of times that the dialogue is in MDP state Si , the tutor took MDP action Ak , and the
dialogue was next in state Sj and dividing by the number of times the dialogue was in Si and the tutor took
Ak . The reliability of these estimates depends upon the size and structure of the training data.
Once an MDP model has been built, calculation of an optimal policy is straightforward. For this project
we employed an RL toolkit developed by Tetreault and Litman (Tetreault & Litman, 2008), which uses a
dynamic programming algorithm for policy iteration (Sutton & Barto, 1998). The code was originally built
on the MDP toolkit written in Matlab (Chades, Garcia, & Sabbadin, 2005). The purpose of this algorithm
is to handle the problem of reward propagation.
The RL toolkit developed by Tetreault and Litman requires all state features in the model to be discrete
variables. However, most of the features involved in this project are numeric and had to be discretized
before a suitable MDP could be constructed. Our discretization procedure used two clustering procedures:
the TwoStep procedure which bounded the number of clusters in SPSS and the K-means procedure which
used K-means clustering to locate optimal cluster centers. Other discretization procedures such as a simple
median split can also be applied.
So far we have described the abstract methodology by which pedagogical policies are induced when
< S, A, R > are defined and T is estimated from a given training corpus . While this approach is theoretically appealing, the cost of obtaining human tutorial dialogues makes it crucial to limit the size of
the MDP state space so that all possible transitions are observed a sufficient number of times to obtain a
reliable estimate T , while still retaining enough information in the states to represent accurately the human
population and learning context.
So the main problem addressed here is to choose S, the set of MDP states, because we are already
committed to using a reward function based on learning gains and our action space A is also clearly defined.
More specifically, we will use A = {Elicit, Tell} for inducing ET policies and using A = {Justify, Skip −
Justify} for inducing JS policies respectively. Once S is selected, it is a mechanical process to induce the
MDP from the data and then calculate optimal policies.

M. Chi et al. / An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System

93

In this project several approaches have been employed to select an appropriate MDP state, S. These
approaches include applying a series of feature-selection methods to select a state representation from a
large set of features, using multiple training corpora, and finding different MDPs and policies for different
knowledge components. The rest of this section presents a few of the critical details of the process, but
many others must be omitted to save space. Next, we will briefly describe our three approaches.

Applying feature selection to induce pedagogical tutorial tactics
For RL, as with all machine learning tasks, success depends upon an effective state representation S.
Ideally it should include an abstraction of the relevant dialogue history that will be necessary to determine
the effects of each action in each state. In particular, for each state in the MDP, every action taken at that
state should occur enough times in the training corpus so that the distribution of resulting states can be
reliably estimated. However, getting enough instances of each state-action pair isn’t the only challenge.
The state representation must be such that different actions from the same state tend to lead to different
states; otherwise, a policy cannot have any impact on the rewards. The policy decides which action the
tutor should take in a given state; if a different action choice doesn’t affect the subsequent state, then the
induced policy may not be effective. The challenge thus lies in identifying a set of features that allows an
effective policy to be induced.
While much previous research on the use of RL to improve ITSs and Dialogue Systems has focused on
developing the best policy for a given set of features (Beck, Woolf, & Beal, 2000; Iglesias, Martínez, Aler, &
Fernández, 2009a,b; Iglesias, Martínez, & Fernández, 2003; Walker, 2000; Henderson, Lemon, & Georgila,
2005), our approach in this project was to begin with a large set of features to which a series of featureselection methods was applied to reduce them to a tractable subset. More specifically 50 state features were
defined based upon six categories of features considered by previous research (Moore, Porayska-Pomsta,
Varges, & Zinn, 2004; Beck, Woolf, & Beal, 2000; Forbes-Riley, Litman, Purandare, Rotaru, & Tetreault,
2007) to be relevant for making tutorial decisions; and we applied twelve feature selection methods. More
details on the 50 features and twelve feature selection methods can be found in (Chi, 2009).
To distinguish from the specific state representation S used in the RL and MDP formalisms, we use 
to represent a large set of potential state representations. In other words, for any induced policy in this
project, its corresponding state space S is a subset of . More specifically our RL task becomes, for a given
< A, R > and , how to select a subset S from a large set of potential states  that would generate the
best policy. Moreover we assume that S and  are defined by Cartesian products of features so selecting
S means selecting a subset of the features that define .
In order to do feature selection, first we need to decide the maximum number of features to be used
in defining S. The number should be small so that we have enough training data to cover the state-action
pairs, yet large enough that the selected features encode enough information to make good decisions about
actions. In order to determine the maximum number of features, it is necessary to consider the amount
of available data and computational power. In this project, based on the minimum data available from the
training corpora, we decided to use at most 6 binary features (m̂ = 6), which means that there can be as
many as 26 = 64 states in S.
Given a particular selection of features and a fixed training corpus and action representation, the methods
mentioned earlier calculate the optimal policy. So for every S, there is exactly one optimal policy π. In order
to compare policies from different Ss, we use a measure called the expected cumulative reward (ECR).
Even though a policy is induced, different traversals of the state space may result in different paths and
hence different rewards. The ECR of a policy is the reward averaged over a very large number of traversals.
The higher the ECR value of a policy, the better the policy is supposed to perform. So our goal is to choose
a set of features that maximizes the ECR of the resulting policy. ECR has been widely used as a criteria

94

M. Chi et al. / An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System

for evaluating policies in the study of policy induction such as when applying RL to induce policies from
simulated corpora (Janarthanam & Lemon, 2009; Williams & Young, 2007a,b).
To summarize, once <A, R> was defined and the system-student interactivity training corpus  provided,
we applied twelve feature selection methods to select a subset of features from the large set defined in
. Recall that the number of features used to define S was limited to no more than 6. The goal is to find
features that maximize the ECR of the resulting policy.

Three training corpora
In order to improve the effectiveness of the RL-induced policies, we used three training corpora. The
Exploratory corpus exploratory consisted of 64 complete tutorial dialogues from students who used a version
of Cordillera that chose actions randomly; the DichGain corpus DichGain contained 37 tutorial dialogues
from students who used a version of Cordillera that chose actions according to a policy derived from the
first round of RL and featured a dichotomous learning gain reward function; and the combined corpus
combined comprised a total of 101 dialogues from the Exploratory and DichGain groups.
exploratory was collected for RL and designed to explore the feature space evenly and without bias.
DichGain , by contrast, is similar to many other pre-existing corpora by following a set of specific pedagogical
strategies. Inducing a successful policy from DichGain would show the potential for applying RL to induce
effective tutorial policies from most pre-existing data. combined , in theory, offers the benefits of both as
well as an increased dataset.
When inducing NormGain and InvNormGain policies, rather than selecting one training corpus  a
priori, all three were used. More specifically, a set of tutorial policies were derived from each training
corpus separately and then the best policy from all sets was selected by ECR.

Inducing Knowledge Component (KC)-speciﬁc Policies
Finally, in order to improve the effectiveness of the RL-induced policies, we induced KC-specific policies.
In the learning literature it is commonly assumed that relevant knowledge in domains such as math and
science is structured as a set of independent but co-occurring Knowledge Components (KCs) and that KCs
are learned independently. A KC is “a generalization of everyday terms like concept, principle, fact, or
skill, and cognitive science terms like schema, production rule, misconception, or facet” (VanLehn, Jordan,
& Litman, 2007). For ITSs these are atomic units of knowledge. It is assumed that a tutorial dialogue
about one KC (e.g., kinetic energy) will have no impact on the student’s understanding of any other KC
(e.g., gravity). This is an idealization, but it has served ITS developers well for many decades, and is a
fundamental assumption of many cognitive models (Anderson, 1983; Newell, 1994).
When dealing with a specific KC, the expectation is that the tutor’s best policy for teaching that KC
(e.g., when to Elicit vs., when to Tell) would be based upon the student’s mastery of the KC in question, its
intrinsic difficulty, and other relevant but not necessarily known factors specific to that KC. In other words
an optimal policy for one KC might not be optimal for another. Therefore, one assumption made in this
project is that inducing pedagogical policies speciﬁc to each KC would be more effective than inducing an
overall KC-general policy.
The domain chosen for this project is the physics work-energy domain, which is a common topic in
introductory college physics courses. Two domain experts, who are also knowledge representation experts
(not the authors), identified 32 KCs in the domain. They had experience identifying KCs for a series
of previous studies involving college physics. Note that a complicated domain like physics can often be
broken into many KCs. Here the 32 identified KCs are believed to cover the most important knowledge in
the domain.

M. Chi et al. / An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System

95

Generally speaking, in a domain like math and physics, solving a problem mainly involves applying
major domain principles. The major domain principles are more challenging and important than the other
KCs since the student’s overall learning performance depends more on learning domain principles. Among
the 32 KCs, eight are major domain principles.
When inducing NormGain or InvNormGain policies, the decision was made to focus only on the eight
primary KCs corresponding to the eight major domain principles. Therefore, the overall problem of inducing
a policy for ET decisions and a policy for JS decisions is decomposed into 8 sub-problems for each decision
type, one per KC. More specifically, to learn a policy for each KC, we annotated the tutorial dialogues
with the KCs covered during each dialogue and the tutorial action decisions with the KCs covered by each
action. For each KC the final kappa was ≥ 0.77, which is fairly high given the complexity of the task. A
domain expert also mapped the pre- and post-test problems to relevant KCs. By doing so a KC-specific
NLG score could be generated for each student.
Among the eight KCs, KC1 does not arise in any JS decisions and thus only an ET policy was induced
for it. For each of the remaining seven KCs a pairs of policies, one ET policy and one JS policy, were
induced. So we induced a total of 15 KC-specific policies. During tutoring there were some decision steps
that did not involve any of the eight primary KCs. For those steps, two KC-general policies, an ET policy
and a JS policy, were induced. Cordillera applies a KC-general policy only when no KC-specific policy is
relevant. Thus a total of 17 NormGain and 17 InvNormGain policies were induced.

Summary
In this project our primary research question was whether pedagogical tutorial policies for micro-steps
would impact students’ learning. In order to investigate our research question, we focused on two types of
tutorial decisions: ET and JS. We applied RL to induce two sets of tutorial policies: the Normalized Gain
(NormGain) policies and the Inverse Normalized Gain (InvNormGain) policies.
The induction of the NormGain and the InvNormGain policies shared the same general RL policy
induction procedure. More specifically, in our policy induction procedure, twelve feature selection methods
were used to select up to six features from a total of 50 features, three training corpora were investigated,
and 15 KC-specific policies and two KC-general policies were induced. The only difference between the
NormGain policies and the InvNormGain policies is the definition of the reward functions. Although the
reward functions for inducing both sets were based on Normalized Learning Gain (NLG), the NormGain
tutorial tactics were induced by using the student  s NLG × 100 as the final reward; while the InvNormGain
ones were induced by using (1 − the student  s NLG) × 100 as the final reward. To sum, a total of 17
NormGain and 17 InvNormGain policies were induced.

An example induced policy
In this project we defined 50 features for . After running the twelve feature selection methods, 17
NormGain and 17 InvNormGain policies were induced. One of the NormGain policies is shown in Fig. 5.
The policy in Fig. 5 has three state features:
[StepSimplicityPS [0, 0.38) → 0; [0.38, 1] → 1]: encodes a step’s simplicity level. Its value is
estimated from the training corpus based on the percentage of correct answers given for the dialogue
state. The discretization procedure binarized the feature into two values: 0 and 1. If less than 38% of the
answers given were correct, then this is considered to be ‘difficult’ content and we set StepSimplicityPS =
0; Otherwise, StepSimplicityPS = 1.
[TuConceptsToWordsPS [0, 0.074) → 0; [0.074, 1] → 1]: represents the ratio of physics concepts to
words in the tutor’s utterances so far. The higher this value, the greater the percentage of physics content

96

M. Chi et al. / An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System
[S:] = {StepSimplicityPS × TuConceptsToWordsPS × TuAvgWordsSesPS}
[A:] = {Elicit, Tell}
[Policy:]

rules 1-5:

0:0:0
0:0:1
1:0:1
1:1:0
1:1:1

Elicit

rule 6:

0:1:0

Tell

rules 7-8:

0:1:1
1:0:0

Either Elicit or Tell

Fig. 5. An Example Of a NormGain policy for an ET Decisions.

that may have been included in tutor turns. Dialogue states with less than 7.4% on this measure have
TuConceptsToWordsPS = 0 and 1 otherwise.
[TuAvgWordsSesPS [0, 22.58) → 0; [22.58, ∞) → 1]: encodes the average number of words in tutor
turns in this session. This feature reflects how verbose the tutor is in the current session. The discretization
procedure set the threshold at 22.58, so dialogue states where the tutor had an average of more than 22.58
words per tutor turn for the current session were represented with 1 for TuAvgWordsSesPS and 0 otherwise.
Since each of the three features was discretized into 2 values, a three-feature state representation resulted
in a state space of 23 = 8 states. Thus, 8 pedagogical rules are learned. Each rule reads as, "In <state>
choose <action(s)>." If a rule has two actions as its choice for the "best action to do," that means that the
ECRs tied during induction. Figure 5 shows that in 5 states the tutor should elicit (rules 1-5), in one state
it should tell (rule 6); in the remaining 2 states either will do (rules 7-8).
For example, let’s unpack rule 6 since it is the only situation in which the tutor should tell. For this
rule to apply the state must be [0 : 1 : 0], which represents the values of the three corresponding features:
StepSimplicityPS, TuConceptsToWordsPS and TuAvgWordsSesPS respectively. Rule 6 suggests that when
the next dialogue content step is difficult (StepSimplicityPS is 0), the ratio of physics concepts to words in
the tutor’s turns so far is high (TuConceptsToWordsPS is 1), and the tutor has not been very wordy during
the current session (TuAvgWordsSesPS is 0), then the tutor should tell. As you can see a three-feature
policy is quite subtle and adaptive to the learning context.
The resulting 17 NormGain and 17 InvNormGain policies were added to Cordillera to yield two new versions of the system, named NormGain-Cordillera and InvNormGain-Cordillera respectively. The induced
tutorial tactics were evaluated on human subjects to see whether the NormGain students would out-perform
their InvNormGain peers.

METHODS
Cordillera
Cordillera is an NL Tutoring System teaching students introductory college physics (VanLehn, Jordan, &
Litman, 2007), which is based upon the TuTalk NL tutorial dialogue toolkit (Jordan, Ringenberg, & Hall,
2006; Jordan, Hall, Ringenberg, Cue, & Rosé, 2007). TuTalk is an authoring tool which enables domain

M. Chi et al. / An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System

97

Fig. 6. Cordillera student interface.

experts to construct natural language tutoring systems without programming. Instead, domain experts
focus on defining the tutoring content through writing tutorial scripts, which are then used for automating
interactions. In other words, script authors determine the flow of the dialogue and the content of each tutor
turn.
The student interface is used by students to read the tutor’s tutorial instructions and to answer questions
by means of natural language entries. Figure 6 shows a screen shot of the student interface. The Message
Window, located in the bottom-left corner is where the dialogue interaction takes place. The remaining
panes are the Dialogue History Pane (upper-left), Problem Statement pane (upper-right), and Variable Pane
(lower-right). The Equations Pane, which lists equations entered by the student or the tutor so far, is not
shown in the figure, but can be exposed by clicking on the appropriate tab.
To reduce confounds due to imperfect NL understanding, the NL understanding module in Cordillera
was replaced with a human interpreter called the language understanding wizard (Bernsen & Dybkjaer,
1997). In this format, Cordillera works as a communications framework that connects a student interface
to a wizard interface. The only task performed by the human wizards is to match students’ answers to the
closest response from a list of potential responses; they cannot make the tutorial decisions. From the wizard
interface, information such as the student’s identification or condition is not presented. In this study only
one human wizard (the first author) was involved and in most wizard sessions 3-6 participants’ answers
were matched at the same time.
In this project, different versions of Cordillera were constructed, each of which differed only in terms of
the pedagogical policies employed. The remaining components of the system, including the GUI interfaces
and domain experts’ tutorial scripts, were identical for all participants. In Cordillera the pedagogical policies
are used to make two types of tutorial decisions.

98

M. Chi et al. / An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System

Participants
Data was collected over a period of two months during the summer of 2009. Participants were 64 college
students who received payment for their participation. They were required to have a basic understanding
of high-school algebra. However, they could not have taken any college-level physics courses. Students
were randomly assigned to the two conditions. Each took from one to two weeks to complete the study
over multiple sessions. In total, 57 students completed the study (29 in the NormGain group and 28 in the
InvNormGain group).

Domain and eight knowledge components
The domain chosen for this project, Physics work-energy domain, is a common topic of introductory
college physics courses. As mentioned before, two domain experts identified 32 KCs for this domain, and
we focused only on the KCs corresponding to the eight major domain principles shown in Table 1. In
Table 1 the first column lists its corresponding KC number. The second column describes the name of the
principle. The last column is the formula or mathematical expression of the principle.
The remaining 24 KCs primarily cover various definitions of major physics concepts and important
physics facts in the domain. For example, major physics concepts include the definition of gravitational
force (KC2 ), spring force( KC3 ) , normal force (KC4 ), isolated system (KC25 ), and so on. Important physics
facts include examples such as that the unit for work is the Joule (J)(KC15 ), that when an object slows to a
stop and reverses direction its velocity is momentarily zero (KC30 ), that the unit for velocity is m/s (KC31 ),
and so on.

Procedure
The participants in this study experienced the same procedure and materials as the participants in the
Exploratory and DichGain studies conducted earlier. More specifically, all participants in this project
experienced the same five standard phases: 1) background survey, 2) pre-training, 3) pre-test, 4) training,
and 5) post-test. Unless specified explicitly in the following, the procedure, reading contents, training
materials, GUI, test items, and so on were identical across all groups and in each phase there were no time
limits.
Table 1
Major principles of work and energy
KC

Principle description

Expressions

KC1
KC14
KC20
KC21
KC22
KC24
KC27
KC28

Weight Law (w)
Definition of Work (W)
Definition of Kinetic Energy (KE)
Gravitational Potential Energy (GPE)
Spring Potential Energy (SPE)
Total Mechanical Energy (TME)
Conservation of Total Mechanical Energy (CTME)
Change of Total Mechanical Energy for Non-isolated
Systems (TMENC)

W = mg
W = Fdcos(α)
KE = 21 mv2
GPE = mgh
SPE = 21 kd 2
TME = KE + GPE + SPE
TME1 = TME2
NetW = TME2 − TME1

M. Chi et al. / An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System

99

The background survey asked students for demographic information such as gender, age, SAT scores,
high school GPA, experience with algebra, calculus, physics, and other information.
Following the background survey, students read the physics textbook during the pre-training and took
the pre-test. The physics textbook was only available during phase 2, pre-training.
In the training phase, students were first trained to solve a demonstration problem, which did not include
physics content, on Cordillera. The sole purpose of this step was to familiarize them with the GUI interface.
Both the NormGain group and the InvNormGain group then solved the same seven training problems in
the same order on their versions of Cordillera. Except for the policies (NormGain vs. InvNormGain), all
components of Cordillera, including the GUI interface and the tutorial scripts, were identical for all students.
Finally, students took the post-test. The pre- and post-tests were identical. Both contained a total of 33
problems selected from the physics literature by two domain experts (not the authors). The 33 problems
covered 168 KC applications. The tests were given online and consisted of both multiple-choice and openended questions. Open-ended questions required the students to derive an answer by writing or solving one
or more equations. Once an answer was submitted, students automatically proceeded to the next question
without receiving any feedback on the correctness of a response. Students were not allowed to return to
prior questions.
As mentioned above, normalized learning gains (NLGs) were used as the reward functions when inducing
NormGain and InvNormGain tutorial policies. Therefore, we used identical pre- and post-tests in order
to avoid the need to rescale the test to make their scores compatible. Students were not informed that the
tests would be identical at any point; they received no feedback on their test answers or test scores; and the
minimum time between the pre- and posttest was one week.

Grading
All tests were mixed together and graded in a double-blind manner by a single experienced grader (the first
author). In a double-blind procedure, neither the students nor the grader know who belongs to which group.
To keep the grading consistent, students’ test answers were graded problem-based. More specifically, all
students’ answers on one test problem (both pre- and posttest) were graded together. The grader was not
informed who an answer belonged to nor which test (pretest or posttest) the answer belonged to.
For all identified relevant KCs in a test question, a KC-based score for each KC application was given.
In the following sections, when we need to compare the NormGain group to the InvNormGain group,
we use the sum of these KC-based scores. A later analysis (not presented here) showed that using other
scoring rubrics resulted in essentially the same pattern of results as this scoring rubric Chi (2009). The tests
contained 33 test items which covered 168 KC occurrences. For comparison purposes all test scores were
normalized to fall in the range of [0, 1].

RESULTS
Learning performance
Overall learning performance
Random assignment appears to have balanced the incoming student competence across conditions.
There were no statistically significant differences between the two conditions in the pretest scores
t(55) = 0.71, p = 0.484. Additionally, no significant differences were found between the two conditions

100 M. Chi et al. / An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System

on the math SAT scores and the total training time spent on Cordillera: t(39) = 0.536, p = 0.595 and
t(55) = −.272, p = 0.787 respectively.
A one-way ANOVA was used to test for learning performance differences between the pre- and posttests.
Both conditions made significant gains from pretest to posttest: F (1, 56) = 31.34, p = 0.000 for the NormGain condition and F (1, 54) = 6.62, p = 0.013 for the InvNormGain condition. Table 2 compares the
pretest, posttest, adjusted-posttest, and NLG scores between the two conditions. In Table 2, the adjusted
posttest scores for each condition were calculated by running an ANCOVA using the pretest score as the
covariate. The second and third columns in Table 2 list the means and standard deviations of the NormGain and InvNormGain groups’ corresponding scores. The fourth column lists the corresponding statistical
comparison. The fifth column lists the effect size (Cohen’s d), which is the mean of the experimental group
minus the mean of the control group, divided by the groups’ pooled standard deviation. Table 2 shows
that there was no significant difference between the two groups on pretest scores. However, there were
significant differences between the two groups on the posttest, adjusted-posttest, and NLG scores. Across
all measurements, the NormGain group performed significantly better than the InvNormGain peers. The
effect size was large.
Since KC-based tutorial tactics were induced, it would be interesting to compare the two groups’ performance on a KC by KC basis. So next we will investigate whether students learned on all eight primary
KCs.

KC-based learning performance
In the pretest on a KC by KC basis, no significant difference was found between the two conditions across
all eight primary KCs except that on KC27 , the NormGain group scored only marginally higher than the
InvNormGain group: t(55) = 1.74, p = 0.088 (see Table 3). In order to account for varying pretest scores,
adjusted posttest scores were calculated by running an ANCOVA using the corresponding pretest score as
the covariate.
On a KC by KC basis, Table 3 summarizes the comparisons on the pretest and adjusted posttest scores
between the two conditions. The third and fourth columns in Table 3 list the means and standard deviations
of the NormGain and InvNormGain groups’ pretest or adjusted posttest scores on the corresponding KC.
The fifth column lists the corresponding statistical comparison. The sixth column lists effect size (Cohen’s
d). In short, Table 3 shows that the NormGain condition out-performed the InvNormGain across all primary
KCs (in bold) except for KC28 , on which no significant difference was found between the two groups.

Summary of learning performance
As expected, both NormGain and InvNormGain groups had significant learning gains after training. More
importantly, although no significant difference was found in time on task, MSAT scores, and pretest scores,
the NormGain group out-performed the InvNormGain group on the posttest and NLG scores. On a KC by
Table 2
Normgain vs. InvNormGain on various test scores

Pretest
Posttest
Adjusted Posttest
NLG

NormGain

InvNormGain

0.42 (0.16)
0.65 (0.15)
0.63 (0.095)
0.41 (0.19)

0.39 (0.23)
0.54 (0.20)
0.55 (0.095)
0.25 (0.21)

Stat
t(55) = 0.71, p = 0.484
t(55) = 2.32, p = 0.024
F (1, 54) = 10.689, p = 0.002
t(55) = 3.058, p = 0.003

Cohen’s d
0.15
0.65
0.86
0.81

M. Chi et al. / An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System 101
Table 3
Between-group comparison on pretest and adjusted posttest scores across primary KCs
KC

TestScore

NormGain

InvNormGain

KC1

Pretest
Adjusted Posttest
Pretest
Adjusted Posttest
Pretest
Adjusted Posttest
Pretest
Adjusted Posttest
Pretest
Adjusted Posttest
Pretest
Adjusted Posttest
Pretest
Adjusted Posttest
Pretest
Adjusted Posttest

0.42 (0.15)
0.64 (0.12)
0.43 (0.23)
0.65 (0.17)
0.38 (0.17)
0.67 (0.11)
0.45 (0.20)
0.75 (0.13)
0.42 (0.25)
0.63 (0.17)
0.46 (0.15)
0.64 (0.11)
0.53 (0.21)
0.74 (0.18)
0.37 (0.20)
0.53 (0.17)

0.39 (0.22)
0.54 (0.12)
0.44 (0.25)
0.53 (0.17)
0.37 (0.22)
0.58 (0.11)
0.43 (0.24)
0.65 (0.13)
0.39 (0.26)
0.51 (0.17)
0.41 (0.23)
0.58 (0.11)
0.42 (0.24)
0.63 (0.18)
0.36 (0.26)
0.47 (0.17)

KC14
KC20
KC21
KC22
KC24
KC27
KC28

Stat
t(55) = 0.66, p = 0.51
F (1, 54) = 9.80, p = 0.0028
t(55) = −0.17, p = 0.86
F (1, 54) = 6.47, p = 0.014
t(55) = 0.31, p = 0.76
F (1, 54) = 10.30, p = 0.002
t(55) = 0.35, p = 0.72
F (1, 54) = 7.62, p = 0.008
t(55) = 0.41, p = 0.68
F (1, 54) = 7.77, p = 0.007
t(55) = 0.89, p = 0.38
F (1, 54) = 4.22, p = 0.045
t(55) = 1.74, p = 0.088
F (1, 54) = 5.88, p = 0.019
t(55) = 0.13, p = 0.90
F (1, 54) = 1.61, p = 0.21

d
0.16
0.85
–0.04
0.72
0.05
0.83
0.09
0.78
0.12
0.72
0.26
0.56
0.5
0.62
0.04
0.36

KC basis, the NormGain condition also out-performed the InvNormGain across all primary KCs (in bold)
except one. Overall, the results show that the tutorial decisions on micro-steps made a significant difference
in the students’ learning.
To summarize, the results are consistent with the primary research hypothesis. The NormGain condition
indeed out-performed the InvNormGain condition. In order to investigate why the NormGain tutorial tactics
were more effective than the InvNormGain one, it will be necessary to dig into the logs and make a detailed
comparison of the differences between the two sets of tutorial tactics. For example, the induced NormGain
tutorial tactics might simply elicit more answers from the students or execute more justification steps during
the tutoring. Therefore, the following section will investigate whether the NormGain and InvNormGain
tutorial tactics resulted in different patterns of tutorial actions.

Log analysis
Overall log analysis
As mentioned earlier, we focused on two types of tutorial actions: Elicit/Tell (ET) and Justify/Skip-Justify
(JS). To quantify the relative frequency of these decisions, an Interactivity ratio (I-ratio) and Justification
ratio (J-ratio) are defined as:
NElicit
NElicit + NTell

(1)

NJustify
NJustify + NSkipJustify

(2)

I − ratio =
J − ratio =

The higher the I-ratio is, the more interactive the dialogue is (one view of interactivity). The higher
the J-ratio is, the more likely the students would be presented a justification step. In order to characterize

102 M. Chi et al. / An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System
Table 4
Overall characteristics of tutorial decisions in exploratory corpus

1
2
3
4
5
6
7
8
9

Tell
Elicit
ET Decisions
Skip-Justify
Justify
JS Decisions
Overall Decisions
I-ratio
J-ratio

NormGain (29)

InvNormGain (28)

63.759 (19.528)
198.586 (17.463)
262.345 (6.149)
9.345 (3.829)
42.517 (3.786)
51.862 (0.833)
280.103 (4.126)
0.758 (0.073)
0.820 (0.073)

63.250 (4.656)
204.000 (7.679)
267.250 (6.775)
11.000 (1.700)
40.321 (1.442)
51.321 (1.156)
285.464 (6.995)
0.763 (0.018)
0.786 (0.030)

Stats
t(55) = 0.134, p = 0.894
t(55) = −1.506, p = 0.138
t(55) = −2.864, p = 0.006
t(55) = −2.096, p = 0.041
t(55) = 2.874, p = 0.006
t(55) = 2.030, p = 0.047
t(55) = −3.539, p = 0.001
t(55) = −0.395, p = 0.694
t(55) = 2.273, p = 0.027

the log data, the I-ratio, J-ratio and several other easily obtained measures were calculated. The goal is to
see whether the NormGain tutorial tactics resulted in different tutorial behaviors from the InvNormGain
policies when viewed from this shallow aspect.
Table 4 summarizes the shallow measures and compares them between the NormGain and InvNormGain
tutorial dialogues. It also includes the I-ratio and J-ratio measures, which will be discussed in the following
two sections. The other shallow measures include the average number of tell decisions (row 1), elicit
decisions (row 2), ET decisions (row 3), skip-justify decisions (row 4), justify decisions (row 5), JS decisions
(row 6), overall decisions (row 7), I-ratio (row 8) and J-ratio (row 9). Table 4, shows that except for the total
number of tells (row 1) and elicits (row 2), the two groups differed significantly on the remaining measures.
Although 5 of the measures were statistically different, the absolute differences between them were small.
This suggests that these characteristics are not revealing what causes the learning gain differences between
the two groups. Perhaps the I-ratio and J-ratio, which are discussed next, will shed light on how the
NormGain policy caused more learning than the InvNormGain policy.

Comparing I-ratio across primary KCs
Although no significant difference was found between the two groups on the I-ratio overall, once the
dialogue was broken into a KC by KC basis there were significant differences between the two groups on
each of the eight primary KCs (see Table 5). In Table 5, row 2 shows that on KC14 the NormGain group got
all elicits while the InvNormGain group got all tells. Among the rest of seven primary KCs, the NormGain
condition was more likely to get elicits than the InvNormGain condition on KC20 , KC21 , and KC22 ; and
the InvNormGain condition was more likely to get elicits than the NormGain condition on KC1 , KC24 ,
KC27 , and KC28 .
Recall that Table 3 shows that the NormGain condition out-performed the InvNormGain across all
primary KCs (in bold) except for KC28 , on which no significant difference was found between the two
groups. Overall, our results seemingly suggest that elicits work better on KC14 , KC20 , KC21 , and KC22 ,
while tells work better on KC1 , KC24 , KC27 . There are many possible explanations for such phenomena.
Based on the number of ET tutorial decisions made on each KC, we can classify the eight KCs into three
categories. Next, we will explain the phenomena on a category by category basis.
KC20 , KC21 , and KC24 are high occurrence KCs in that each of them are involved in more than 50
ET tutorial decisions. Among the three KCs, KC20 and KC21 both involve multiplying several physics
quantities while KC24 involves summing over physics quantities. Perhaps because summation is much

M. Chi et al. / An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System 103
Table 5
Compare normgain vs. invnormgain on I-ratio across eight primary KCs

1
2
3
4
5
6
7
8

KC1
KC14
KC20
KC21
KC22
KC24
KC27
KC28

NormGain(29)

InvNormGain (28)

0.500 (0.000)
1.000 (0.000)
0.897 (0.024)
0.923 (0.030)
0.888 (0.099)
0.866 (0.028)
0.484 (0.137)
0.000 (0.000)

0.696 (0.157)
0.000 (0.000)
0.696 (0.030)
0.863 (0.045)
0.543 (0.089)
0.920 (0.029)
0.651 (0.112)
0.525 (0.108)

Stats
t(55) = −6.72, p = 0.000
t(55) = 27.87, p = 0.000
t(55) = 5.95, p = 0.000
t(55) = 13.88, p = 0.000
t(55) = −7.21, p = 0.000
t(55) = −5.03, p = 0.000
t(55) = −26.08, p = 0.000

easier to understand than multiplication, this is why NormGain policies chose to elicit more frequently
on the former two KCs and tell more frequently on KC24 . The next category is the medium occurrence
KCs, which include KC22 , KC27 , and KC28 . Each of these three KCs occurred in the range of 17 to 28 ET
decisions. Among the three KCs, KC22 involves multiplying several physics quantities while KC27 and
KC28 involve equating or summing over physics quantities. As with the three high occurrence KCs, the
multiply-vs-summation difference may explain why elicits were more effective for KC22 but not KC28 , and
tells were more effective on KC27 . Finally, the remaining two KCs, KC1 and KC14 , were involved in less
than 10 ET decisions and thus they are the low occurrence KCs. Our results suggest that tells seem to work
better on KC1 while elicits seem to work better on KC14 . While both KC1 and KC14 involve multiplication
of physics quantities, the concept of gravitational force (KC1 ) may not be completely new to many of the
participants, even physics novices, while the definition of work would be more likely to be unfamiliar to
these physics novices. Overall, these are only hypothetical explanations and further research is needed for
the reasons behind such results.

Comparing J-ratio across primary KCs
Similarly, the J-ratio can be examined on a KC by KC basis. Only seven primary KCs (KC1 was not involved
in JS decisions) were involved (see Table 6). Surprisingly, on two KCs, KC22 (row 4) and KC28 (row 7),
both NormGain and InvNormGain tutorial tactics achieved the same results, executing all justification steps.
There are at least two potential explanations. One possible explanation is that the JS decisions on these
KCs may not matter to the students’ learning. The other possible explanation is that the source training
corpora used to induce these two KC-specific policies might not be exploratory enough.
On KC14 , however, following the NormGain tutorial tactics resulted in skipping all justification steps, but
following the InvNormGain tutorial tactics resulted in executing all justification steps. For the remaining
four KCs, no significant difference was found between the two conditions on KC20 (row 2) and KC24
(row 5). Only a marginally significant difference was found between the two groups on KC27 . On KC21 ,
however, the NormGain group was significantly more likely to get justification steps than the InvNormGain
group.

Summary of Log Analysis
Overall, following the NormGain tutorial policies did not generate more interactive tutorial tactics than
following the InvNormGain ones. But once broken into a KC by KC basis, the NormGain tutorial tactics
resulted in different I-ratio for every one of the primary KCs. On the other hand, following the NormGain

104 M. Chi et al. / An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System
Table 6
Compare NormGain vs. InvNormGain on J-ratio across eight primary KCs

1
2
3
4
5
6
7

KC14
KC20
KC21
KC22
KC24
KC27
KC28

NormGain (29)

InvNormGain (28)

0.000 (0.000)
1.000 (0.000)
0.815 (0.216)
1.000 (0.000)
0.876 (0.024)
0.046 (0.140)
1.000 (0.000)

1.000 (0.000)
0.994 (0.022)
0.573 (0.096)
1.000 (0.000)
0.871 (0.005)
0.000 (0.000)
1.000 (0.000)

Stats
t(55) = 1.467, p = 0.148
t(55) = 5.445, p = 0.000
t(55) = 1.071, p = 0.289
t(55) = 1.736, p = 0.088

tutorial tactics seemed more likely to execute a justification step but once broken into KC by KC basis, the
NormGain and InvNormGain tutorial tactics’ J-ratio were only significantly different on KC21 and KC14
(The NormGain tutorial tactics skipped all of them while the InvNormGain executed all of them).

Summary on NormGain vs. InvNormGain
To summarize, the findings confirmed our primary hypotheses in this project: the pedagogical tutorial
policies applied at the micro-step level affected students’ learning. Moreover, the use of RL to derive tutorial
tactics from existing data proved to be feasible and successful. On the other hand, the results also suggested
that content exposure with the Cordillera system, irrespective of the micro-step policies employed, was,
indisputably, an important factor in students’ learning because even the InvNormGain students learned
significantly. Nonetheless, micro-step pedagogical policies also made a significant impact.
However, it is not clear as to what it was about the NormGain tutorial tactics that caused the NormGain
students to learn more effectively than the InvNormGain group. By simply analyzing the log file in a
relatively shallow way, it seems that it was not that the NormGain tutorial tactics were simply more
interactive or generated more justification steps. This is consistent with the conjecture that interactivity is
not, necessarily, the most important determiner of students’ learning. For example, although no significant
difference was found between the two conditions in terms of the number of elicitation prompts and tells they
received or the I-ratio, the NormGain students nonetheless learned significantly more than the InvNormGain
students. Additionally, once broken into a KC by KC basis, the NormGain group had a significantly higher
I-ratio than the InvNormGain group on KC14 , KC20 , KC21 , KC22 and the learned significantly more than
the latter on all three KCs; however the InvNormGain students had a significantly higher I-ratio than the
NormGain group on KC1 , KC24 , KC27 , and KC28 , but the former did not learn more than the latter group.
For JS decisions, the induced NormGain tutorial tactics indeed resulted in more justification steps in
students’ tutorial dialogues. However, once the tutorial decisions were broken into a KC by KC basis, the
two groups differed significantly only on KC21 and KC14 . Therefore, future work is needed to investigate
the induced tutorial tactics and find out what actually caused these learning differences.
The NormGain and InvNormGain tutorial tactics in this study were derived from the Exploratory and
DichGain Corpora in previous studies. Therefore, it is possible to draw some hypotheses from observations
by running a post-hoc comparison among the four groups. A cross-study analysis comparing the three
studies will be presented in the next section.

M. Chi et al. / An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System 105

COMPARISONS ACROSS ALL FOUR GROUPS
The preceding section focuses on two groups, NormGain and InvNormGain, because they were selected by
random assignment from the same population and thus provide the most rigorous test of our hypotheses. In
this section, we fold in an analysis of the results from two other groups, DichGain and Exploratory, in the
hope that this wider view will shed some light on the main results. We begin by summarizing methodological
differences among the groups, then compare their learning gains, then compare their I-ratio and J-ratio
measures.

Study Variations
A total of 158 participants used four versions of Cordillera as part of the three studies: The Exploratory
Group contained 64 students who used Random-Cordillera (Study 1); the Dichotic Gain (DichGain) Group
was comprised of a total of 37 students who used DichGain-Cordillera (Study 2); The Normalized Gain
(NormGain) group included 29 students who used NormGain-Cordillera and the Inverse Normalized Gain
(InvNormGain) group included 28 students who used InvNormGain-Cordillera (Study 3). All of the participants followed the same procedure, used the same preparatory materials and problems, and interacted with
Cordillera. They all completed a background survey, read a textbook covering the target domain knowledge,
took a pre-test, solved the same seven training problems in the same order using Cordillera, and finally
took a post-test. Only four salient differences existed across the three studies:
1. Although all of the participants were recruited in the same way, they were recruited in different years.
In Study 3 the students were randomly assigned into the NormGain and InvNormGain groups (2009).
On the other hand, in the first two studies participants were not randomly assigned to the Exploratory
(2007) and DichGain groups (2008).
2. Interaction decisions were guided by different micro-step policies. Random-Cordillera made random
decisions on micro-steps. The other three versions of Cordillera followed corresponding induced
tutorial policies to decide which action to take.
3. Apart from a single question variation on Studies 2 and 3, all three studies used identical exams
containing a total of 33 test questions. The one variation occurred as the result of the replacement of a
single question, Q20 , which had been used in Study 1. It was judged to be too easy and was replaced
with a more difficult question, Q∗20 that covered the same KCs for Studies 2 and 3. The remaining 32
test items were identical across all three studies.
4. A group of six human wizards were involved in Studies 1 and 2; but only one of the six wizards (the
first author) was in Study 3.
Despite these differences, because the NormGain and InvNormGain groups trained in Study 3 were
guided using tutoring tactics derived from the Exploratory and DichGain corpora, a post-hoc comparison
among the four groups will allow us to observe the characteristics of the induced tutorial tactics from a
wider point of view.
In order to establish test equivalence, Q20 and Q∗20 were excluded from the scores used below. As
described in the previous chapter, the tests contained 33 test items which covered 168 KC occurrences.
Removing Q20 reduced this total by 1 leaving 32 test items covering 166 KC occurrences. For comparison
purposes both scores were normalized to 1.
Based on the procedure of induced tutorial tactics, it was expected that NormGain > DichGain >
Exploratory > InvNormGain.
A one-way ANOVA showed that there were no significant differences among the four groups on overall
training time: (F (3, 147) = 1.531, p = 0.209). More specifically, the average total training time in minutes

106 M. Chi et al. / An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System

across the seven training problems, was M = 278.73, SD = 67.38 for Exploratory group, M = 294.33,
SD = 87.51 for DichGain group, M = 259.99, SD = 59.22 for NormGain group, and M = 264.57, SD =
67.60 for InvNormGain group. Additionally, no significant difference was found among the Exploratory,
the NormGain, and the InvNormGain groups on the Math SAT scores2 : (F (2, 83) = 0.520, p = 0.596).

Learning performance
A one-way ANOVA was used to test for performance differences between the pre- and post-tests. Participants across four groups made significant gains from pre-test to post-test: F (1, 314) = 41.82, p = 0.000.
While no significant pre-test score difference was found among the four groups (F (3, 154) = 0.38, p =
0.77), there were significant differences among the four groups on the posttest and NLG scores: F (3, 154) =
3.41, p = 0.02 and F (3, 154) = 5.30, p = 0.002 respectively. Moreover, t-test comparisons showed that
there was a significant difference between the NormGain group and all of the three remaining groups
on the post-test scores and NLG scores (see Table 7). In Table 7, the first column lists the two groups
in comparison and their corresponding mean and standard deviation scores. The second column lists
the statistical result of the t-test comparison. The last two columns list the effect size and power of the
comparison. For effect size, Cohen’s d was still used. However, there were no significant differences
among DichGain, Exploratory, and InvNormGain on all three test scores. Overall, our results suggest
that NormGain > DichGain = Exploratory = InvNormGain across two different performance metrics:
post-test and NLG scores.

Log analysis
Having compared the individual groups’ learning performance, this subsection will compare the log file
variations across the four groups. Moreover, given the limited space, we will only present the comparison
among the four groups on the I-ratio and J-ratio.

I-Ratio
Table 8 summarizes t-test comparisons on the I-ratio among the four tutorial corpora. In Table 8, the first
two columns list the two groups in comparison and their corresponding mean and SD scores. The last
column lists the statistical results of the t-test comparisons. From Table 8, the I-ratios for the four student
groups were: 0.76 (NormGain), 0.76 (InvNormGain), 0.44 (DichGain), and 0.50 (Exploratory) respectively.
Except for no significant difference between the NormGain and InvNormGain on the I-ratio, both groups
were significantly more interactive than either the DichGain group or Exploratory group. Altogether, the
result is NormGain = InvNormGain > Exploratory > DichGain on the I-ratio.
High interactivity is a key characteristic of one-on-one human tutoring. It is commonly believed that more
interactive would result in more learning. Our post-hoc comparisons also shows that the NormGain group
was more successful than the Exploratory and DichGain groups and the former is also being more interactive
than the latter two. However, our other post-hoc comparisons suggest that the more successful tutorial tactics
were not necessarily more interactive than the less successful tactics. Comparisons between the NormGain
and InvNormGain groups suggest that it is not the absolute level of interactivity that determines the students’
success. The NormGain group was more successful than the others despite there being no significant
difference in interactivity ratios between it and the InvNormGain group. Conversely, the InvNormGain
group was no more successful than the Exploratory and DichGain groups despite being more interactive
than either.
2

This data was lost for the DichGain group due to a data management error.

M. Chi et al. / An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System 107
Table 7
Compare four groups on overall learning performance
Group Name ␮ (␴)
NormGain
InvNormGain
NormGain
DichGain
NormGain
Exploratory

0.42(0.15)
0.39(0.23)
0.42(0.15)
0.38(0.17)
0.42(0.15)
0.41(0.20)

NormGain
InvNormGain
NormGain
DichGain
NormGain
Exploratory

0.65(0.15)
0.54(0.20)
0.65(0.15)
0.50(0.21)
0.65(0.15)
0.53(0.21)

NormGain
InvNormGain
NormGain
DichGain
NormGain
Exploratory

0.42(0.19)
0.25(0.21)
0.42(0.19)
0.22(0.23)
0.42(0.19)
0.22(0.26)

Stat

Cohen’s d

1− ␤

Pretest
t(55) = 0.66, p = 0.507

0.16

0.58

t(64) = 1.05, p = 0.299

0.25

0.49

t(91) = 0.29, p = 0.792

0.05

0.8

Posttest
t(55) = 2.32, p = 0.024

0.64

0.53

t(64) = 3.28, p = 0.0017

0.82

0.46

t(91) = 3.17, p = 0.0069

0.63

0.35

NLG
t(55) = 3.15, p = 0.0026

0.87

0.54

t(64) = 4.626, p = 0.000

0.95

0.18

t(91) = 3.61, p = 0.0005

0.84

0.33

Table 8
Pairwise comparison among four groups on I-ratio
Group 1
NormGain
NormGain
NormGain
InvNormGain
InvNormGain
Exploratory

Group 2
0.76 (0.07)
0.76 (0.07)
0.76 (0.07)
0.76 (0.02)
0.76 (0.02)
0.50 (0.03)

InvNormGain
Exploratory
DichGain
Exploratory
DichGain
DichGain

Group 1 vs. Group 2
0.76 (0.02)
0.50 (0.03)
0.44 (0.04)
0.50 (0.03)
0.44 (0.04)
0.44 (0.04)

t(55) = 0.395, p = 0.694
t(91) = 24.72, p = 0.000
t(64) = 22.08, p = 0.000
t(90) = 43.998, p = 0.000
t(63) = 36.34, p = 0.000
t(99) = 7.967, p = 0.000

Justify ratio
Table 9 summarizes t-test comparisons on J-ratio among the four tutorial corpora. In Table 9, the first two
columns list the two groups in comparison and their corresponding mean and SD scores. The last column
lists the statistical results of the t-test comparisons. Table 9 shows that the mean of J-ratios for the four
student groups were: 0.82 (NormGain), 0.79 (InvNormGain), 0.43 (DichGain), and 0.53 (Exploratory).
The difference was statistically significant: F (3, 154) = 322.88, p = 0.000. Table 9 presents the pair wise

108 M. Chi et al. / An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System
Table 9
Pairwise comparison among four groups on J-ratio
Group 1
NormGain
NormGain
NormGain
InvNormGain
InvNormGain
Exploratory

Group 2
0.82 (0.07)
0.82 (0.07)
0.82 (0.07)
0.79 (0.03)
0.79 (0.03)
0.53 (0.06)

InvNormGain
Exploratory
DichGain
Exploratory
DichGain
DichGain

Group 1 vs. Group 2
0.79 (0.03)
0.53 (0.06)
0.43 (0.07)
0.53 (0.06)
0.43 (0.07)
0.43 (0.07)

t(55) = 2.27, p = 0.027
t(91) = 18.95, p = 0.000
t(64) = 22.85, p = 0.000
t(90) = 43.998, p = 0.000
t(63) = 26.65, p = 0.000
t(99) = 7.894, p = 0.000

t-test comparisons. It shows that on J-ratio, the result is: NormGain > InvNormGain > Exploratory >
DichGain.
To summarize, NormGain tutorial policies resulted in substantially more justifications than InvNormGain
tutorial policies. However, although the NormGain group had a higher ratio of justification prompts than
the InvNormGain, Exploratory, or DichGain groups it is not the case that the absolute justification ratio
guarantees learning. As with the interactivity ratio, the InvNormGain group received a higher justification
ratio than the Exploratory or DichGain groups despite having been induced to enhance those decisions that
contribute less or even none to the students’ learning, and despite the absence of a significant difference in
adjusted post-test scores or NLG between the groups.

Summary of post-hoc comparison
To summarize, a post-hoc comparison of learning performance across four groups shows that the NormGain
group significantly outperformed all other three groups while no significant learning difference was found
between the remaining three groups. These results were consistent both for the post-test scores and the
normalized learning gains. These results support the prior analysis which showed that the NormGain tutorial
tactics significantly improved students’ learning compared with the InvNormGain ones.
However, the lack of a significant difference between the InvNormGain, DichGain, and Exploratory
groups seemingly contradicts the initial predictions. The InvNormGain tactics were specifically induced
to enhance those decisions that contribute less or even none to the students’ learning. Therefore, a lower
performance on the students’ part there than in at least the DichGain group, which sought to enhance the
tutorial decisions that contribute to the students’ learning, was expected. One possible explanation for the
lack of difference is that when tutorial tactics are done well, they add value to students’ learning experience,
but when they are done poorly, students are unaffected by them no matter how poor they are.
In other words, one possible explanation for the lack of difference is that the tutorial tactics employed by
InvNormGain-, DichGain- and Random-Cordillera systems were ineffective and thus presented a minimum
bar. By ‘ineffective’ it does not mean that they prevented the students from learning but rather that they
were not able to make a positive impact on their learning above and beyond the baseline provided by
Cordillera itself. Here the basic practices and problems, domain exposure, and interactivity of Cordillera
set a minimum bar of students’ learning that the tactics, however poor, cannot prevent. This is only a
post-hoc explanation not a tested hypothesis, however it merits further study.

M. Chi et al. / An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System 109

CONCLUSION
Human tutors often scaffold students via a series of micro-steps while a typical ITS is step-based (VanLehn, 2006). One hypothesis as to the effectiveness of human one-on-one tutoring comes from the detailed
management of micro-steps (Graesser, Person, & Magliano, 1995; Graesser, VanLehn, Rosé, Jordan, &
Harter, 2001). However, once content was controlled to be the same across all conditions, neither human
tutors nor Natural Language (NL) tutoring systems designed to mimic human tutors reliably outperformed
step-based systems (Evens & Michael, 2006; VanLehn, Graesser, & et al., 2007).
On the other hand, for any form of one-on-one tutoring, the tutor’s behavior can be viewed as a sequential
decision process wherein, at each discrete step, the tutor is responsible for selecting the next action to
take. Although it is commonly assumed that human expert tutors have effective pedagogical skills, little
evidence has been presented to date demonstrating that. Therefore, our explanation for the lack of difference
among the human tutors, Natural Language (NL) tutoring systems, and step-based tutoring systems in
previous research is that neither human tutors nor their computer mimics are good at making micro-step
decisions.
In this project our primary research question is whether pedagogical tutorial tactics would impact students’
learning if the instructional content was controlled to be equivalent for all students. In order to control the
instructional content in this project, we used Cordillera.
To investigate our research question, we focused on two types of micro-step tutorial decisions Elicit vs.Tell
(ET) and Justify vs. Skip-Justify (JS) and applied RL to induce two set of tutorial tactics: the Normalized
Gain (NormGain) tactics were derived with the goal of making tutorial decisions that contribute to students’
learning, while the Inverse Normalized Gain (InvNormGain) tactics were induced with the goal of making
less beneficial, or possibly useless, decisions. The two sets were then empirically compared on real students.
The students were randomly assigned to balanced conditions and received identical training materials and
procedures apart from the tutoring tactics employed.
After spending the same amount of time on training, the NormGain group outperformed the InvNormGain
group in terms of posttest scores and the normalized learning gain regardless of the grading criteria. This
result suggests that the lack of a difference among the human tutors, Natural Language (NL) tutoring
systems, and step-based tutoring systems in previous studies (Reif & Scott, 1999a; Evens & Michael,
2006; VanLehn, Graesser, & et al., 2007) is that neither human tutors nor their computer mimics are good
at making micro-step decisions.
On the other hand, applying RL to induce pedagogical policies on ITSs is not new (Beck, Woolf, & Beal,
2000; Iglesias, Martínez, Aler, & Fernández, 2009a,b; Iglesias, Martínez, & Fernández, 2003; Martin &
Arroyo, 2004; Tetreault, Bohus, & Litman, 2007; Tetreault & Litman, 2006). However, only a few studies
evaluated the induced policy on real students to verify whether RL indeed fulfilled its promise (Beck,
Woolf, & Beal, 2000; Iglesias, Martínez, Aler, & Fernández, 2009a,b; Iglesias, Martínez, & Fernández,
2003). As far as we know, none of these studies has empirically shown that the RL induced policy indeed
made a difference in students’ learning performance even though improving student learning is a primary
goal for any ITS. Therefore, the better learning gains of the NormGain group over the InvNormGain group
in this study indicated that applying RL to derive effective tutorial policies that would improve students’
learning is feasible.
The analyses of the log data showed that the NormGain policies did not differ significantly from the
InvNormGain policies in their overall interactivity (the ratio of Elicits to Tells). This is consistent with
earlier studies showing that increasing the overall interactivity of natural language dialogue tutoring tends
not to increase its effectiveness (VanLehn, 2008; Evens & Michael, 2006; VanLehn, Graesser, & et al.,
2007; Reif & Scott, 1999b; Chi, Roy, & Hausmann, 2008; Rose, Moore, Allbritton, & Lehn, 2001; Johnson
& Johnson, 1992).

110 M. Chi et al. / An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System

However, there were significant and large differences in I-ratio on a KC by KC basis. For instance,
KC14 was always elicited by the NormGain policy and always told by the InvNormGain policy, whereas
KC28 was never elicited by the NormGain policy and elicited about half the time by the InvGainPolicy.
This suggests that KCs really are learned independently in that some KCs benefit from high amounts of
interaction and other KCs benefit from low amounts of interaction. For instance, perhaps some KCs are
just easier to learn or more familiar initially, so it is better for the tutor to Tell them, whereas other KCs are
difficult to learn or completely unfamiliar, so it is better for the tutor to Elicit them and students to learn by
generating answers on their own.
This hypothesis offers an explanation for the surprising result that human tutors are often no better than
step-based tutoring systems (Reif & Scott, 1999a; Evens & Michael, 2006; VanLehn, Graesser, & et al.,
2007). Studies of human tutors suggest that they do not monitor students’ competence at a fine-grained
level (Chi, Siler, & Jeong, 2004; Putnam, 1987). They can indicate the overall competence of a student in
a topic, but cannot reliably diagnose bugs and probably cannot indicate competence per KC. This suggests
that they may not regulate their tutoring actions at a per-KC level either. That is, they may not say to
themselves, "Now I need the student to apply the Change of TME for non-isolated systems (KC28 ), which
most students tend to mess up, and I’ve been asking a lot of hard questions recently, so I’m just going to
show how to apply it myself." This particular policy rule is quite simple compared to the NormGain ones
(see Fig. 5 ), but it illustrates what human tutors would have to consider if they were to be as effective
as the NormGain policies suggest they can be. Moreover, they would have to make this decision very
rapidly.
On the other hand, the J-ratio (relative number of Justify actions) was significantly higher for the
NormGain group than the InvNormGain group. This shows that prompting for self-explanations tends
to be effective for promoting learning, which is consistent with the self-explanation literature (Chi, de
Leeuw, Chiu, & LaVancher, 1994; Conati & VanLehn, 2000; Aleven, Ogan, Popescu, Torrey, & Koedinger,
2004). However, once the totals are broken down by KC, the two groups only differed on two KCs,
KC14 and KC21 . On KC21 the NormGain tutorial tactics skipped all of the justify actions while the
InvNormGain tactics covered all of them; while on KC21 the J-ratio was higher for the NormGain
group than the InvNormGain group. Moreover, a wider comparison among the NormGain, InvNormGain, DichGain, and Exploratory groups suggests that increased learning might not be due to receiving a
higher number of justification steps. The InvNormGain students had significantly more justification steps
than the DichGain, and Exploratory groups. However, the former did not learn more than the latter two
groups.
Although more research is needed to understand what caused the NormGain tutorial policies to be more
effective than the others, the take-home message is that effective tutorial policies seem to depend on details
including the particular KCs involved in the micro-step. Computer tutors such as Natural Language tutoring
systems can handle such details faster and better than human tutors, so it may not be long before they are
more effective than human tutors. The post-hoc analysis in the prior section shows that all four groups
learned significantly by training on Cordillera. This result indicates that the content exposure and practice
opportunities can cause students to learn even from tutors with poor pedagogical tutorial tactics. However,
it also indicates that with effective tutorial tactics students can learn even more.

ACKNOWLEDGEMENTS
NSF (#0325054) supported this work. We also thank the Learning Research Development Center at the
University of Pittsburgh for providing all the facilities used in this work.

M. Chi et al. / An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System 111

REFERENCES
Aleven, V., Ogan, A., Popescu, O., Torrey, C., & Koedinger, K. R. (2004). Evaluating the effectiveness of a tutorial
dialogue system for self-explanation. In Lester, Vicari, & Paraguaçu (2004), (pp. 443-454).
Anderson, J. R. (1983). The architecture of cognition. Cambridge, Mass. : Harvard University Press.
Anderson, J. R., Corbett, A. T., Koedinger, K. R., & Pelletier, R. (1995). Cognitive tutors: Lessons learned. The
Journal of the Learning Sciences, 4(2), 167-207.
Beck, J., Woolf, B. P., & Beal, C. R. (2000). Advisor: A machine learning architecture for intelligent tutor
construction. In AAAI/IAAI, (pp. 552-557). AAAI Press / The MIT Press.
Bernsen, N. O. & Dybkjaer, L. (1997). Designing Interactive Speech Systems: From First Ideas to User Testing.
Secaucus, NJ, USA: Springer-Verlag New York, Inc.
Bloom, B. S. (1984). The 2 sigma problem: The search for methods of group instruction as effective as one-to-one
tutoring. Educational Researcher, 13, 4-16.
Cade, W. L., Copeland, J. L., Person, N. K., & D’Mello, S. K. (2008). Dialogue modes in expert tutoring. In Woolf,
Aı̈meur, Nkambou, & Lajoie (2008), (pp. 470-479).
Chades, M. C., Garcia, F., & Sabbadin, R. (2005). Mdp toolbox v2.0 for matlab.
Chi, M. (2009). Do Micro-Level Tutorial Decisions Matter: Applying Reinforcement Learning To Induce Pedagogical Tutorial Tactics. Ph.D. thesis, School of Art & Science University of Pittsburgh.
Chi, M., Jordan, P. W., VanLehn, K., & Litman, D. J. (2009). To elicit or to tell: Does it matter? In V. Dimitrova, R.
Mizoguchi, B. du Boulay, & A. C. Graesser (Eds.) AIED, (pp. 197-204). IOS Press.
Chi, M. T. H., Feltovich, P., & Glaser, R. (1981). Categorization and representation of physics problems by experts
and novices. Cognitive Science, 5, 121-152.
Chi, M. T. H., de Leeuw, N., Chiu, M. H., & LaVancher, C. (1994). Eliciting self-explanations improves understanding. Cognitive Science, 18(3), 439-477.
Chi, M. T. H., Roy, M., & Hausmann, R. G. M. (2008). Observing tutorial dialogues collaboratively: Insights about
human tutoring effectiveness from vicarious learning. Cognitive Science, 32(2), 301-342.
Chi, M. T. H., Siler, S., & Jeong, H. (2004). Can tutors monitor students’ understanding accurately? Cognition and
Instruction, 22(3), 363-387.
Chi, M. T. H. & VanLehn, K. (1991). The content of physics self-explanations. The Journal of the Learning Sciences,
1, 69-105.
Collins, A., Brown, J. S., & Newman, S. E. (1989). Cognitive apprenticeship: Teaching the craft of reading,writing
and mathematics. In L. B. Resnick (Ed.) Knowing, learning and instruction: Essays in honor of Robert Glaser,
chapter 14, (pp. 453-494). Lawrence Erlbaum Associates: Hillsdale New Jersey.
Conati, C., & VanLehn, K. (2000). Toward computer-based support of meta-cognitive skills: A computational
framework to coach self-explanation. International Journal of Artiﬁcial Intelligence in Education, 11, 398415.
Evens, M., & Michael, J. (2006). One-on-one Tutoring By Humans and Machines. Mahwah, NJ: Erlbaum.
Forbes-Riley, K., Litman, D. J., Purandare, A., Rotaru, M., & Tetreault, J. R. (2007). Comparing linguistic features
for modeling learning in computer tutoring. In Luckin, Koedinger, & Greer (2007), (pp. 270-277).
Graesser, A. C., Person, N., & Magliano, J. (1995). Collaborative dialog patterns in naturalistic one-on-one tutoring.
Applied Cognitive Psychology, 9, 359-387.
Graesser, A. C., VanLehn, K., Rosé, C. P., Jordan, P. W., & Harter, D. (2001). Intelligent tutoring systems with
conversational dialogue. AI Magazine, 22(4), 39-52.
Halloun, I., & Hestenes, D. (1985). The initial knowledge state of the college physics students. Am. J. Phys., 53(11),
1043-1055.
Henderson, J., Lemon, O., & Georgila, K. (2005). Hybrid reinforcement/supervised learning for dialogue policies
from communicator data. In IJCAI Workshop on K&R in Practical Dialogue Systems, (pp. 68-75).
Iglesias, A., Martı́nez, P., & Fernández, F. (2003). An experience applying reinforcement learning in a web-based
adaptive and intelligent educational system. Informatics in Education, 2(2), 223-240.

112 M. Chi et al. / An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System
Iglesias, A., Martı́nez, P., Aler, R., & Fernández, F. (2009a). Learning teaching strategies in an adaptive and intelligent
educational system through reinforcement learning. Applied Intelligence, 31, 89-106. ISSN 0924-669X. URL
http://dx.doi.org/10.1007/s10489-008-0115-1, 10.1007/s10489-008-0115-1.
Iglesias, A., Martı́nez, P., Aler, R., & Fernández, F. (2009b). Reinforcement learning of pedagogical policies in
adaptive and intelligent educational systems. Knowledge-Based Systems, 22(4), 266-270. ISSN 0950-7051.doi:
10.1016/j.knosys.2009.01.007. Artificial Intelligence (AI) in Blended Learning - (AI) in Blended Learning.
Janarthanam, S., & Lemon, O. (2009). User simulations for online adaptation and knowledge-alignment in troubleshooting dialogue systems. In Proceedings of the 12th SEMdial Workshop on on the Semantics and
Pragmatics of Dialogues.
Johnson, H., & Johnson, P. (1992). Different explanatory dialogue styles and their effects on knowledge acquisition
by novices. In Proceedings of the Hawaii International Conference on System Sciences, (pp. 47-57).
Jordan, P. W., Hall, B., Ringenberg, M. A., Cue, Y., & Rosé, C. P. (2007). Tools for authoring a dialogue agent that
participates in learning studies. In Luckin, Koedinger, & Greer (2007), (pp. 43-50).
Jordan, P. W., Ringenberg, M. A., & Hall, B. (2006). Tools for authoring a dialogue agent that participates in learning
studies. In Proceedings of ITS06 Workshop on Teaching with Robots, Agents, and NLP.
Katz, S., Connelly, J., & Wilson, C. (2007). Out of the lab and into the classroom: An evaluation of reflective
dialogue in andes. In Luckin, Koedinger, & Greer (2007), (pp. 425-432).
Katz, S., O’Donnell, G., & Kay, H. (2000). An approach to analyzing the role and structure of reflective dialogue.
International Journal of Artiﬁcial Intelligence and Education, 11, 320-343.
Koedinger, K. R., & Aleven, V. (2007). Exploring the assistance dilemma in experiments with cognitive tutors.
Educational Psychology Review, 19(3), 239-264.
Koedinger, K. R., Anderson, J. R., Hadley, W. H., & Mark, M. A. (1997). Intelligent tutoring goes to school in the
big city. International Journal of Artiﬁcial Intelligence in Education, 8(1), 30-43.
Larkin, J. H., McDermott, L. C., Simon, D. P., & Simon, H. (1980). Expert and novice performance in solving
physics problems. Science, 208, 1335-1342.
Lester, J. C., Vicari, R. M., & Paraguaçu, F. (Eds.) (2004). Intelligent Tutoring Systems, 7th International Conference,ITS 2004, Maceiò, Alagoas, Brazil, August 30 - September 3, 2004, Proceedings, volume 3220 of Lecture
Notes in Computer Science. Springer.
Levin, E., & Pieraccini, R. (1997). A stochastic model of computer-human interaction for learning dialogue
strategies. In EUROSPEECH 97, (pp. 1883-1886).
Luckin, R., Koedinger, K. R., & Greer, J. E. (Eds.) (2007). Artiﬁcial Intelligence in Education, Building Technology
Rich Learning Contexts That Work, Proceedings of the 13th International Conference on Artiﬁcial Intelligence
in Education, AIED 2007, July 9-13, 2007, Los Angeles, California, USA, volume 158 of Frontiers in Artiﬁcial
Intelligence and Applications. IOS Press.
Martin, K. N., & Arroyo, I. (2004). Agentx: Using reinforcement learning to improve the effectiveness of intelligent
tutoring systems. In Lester, Vicari, & Paraguaçu (2004), (pp. 564-572).
Merrill, D. C., Reiser, B. J., Ranney, M., & Trafton, J. G. (1992). Effective tutoring techniques: A comparison of
human tutors and intelligent tutoring systems. The Journal of the Learning Sciences, 2(3), 277-306.
Moore, J. D., Porayska-Pomsta, K., Varges, S., & Zinn, C. (2004). Generating tutorial feedback with affect. In V.
Barr & Z. Markov (Eds.) FLAIRS Conference. AAAI Press.
Newell, A. (1994). Uniﬁed Theories of Cognition. Harvard University Press; Reprint edition.
Putnam, R. T. (1987). Structuring and adjusting content for students: A study of live and simulated tutoring of
addition. American Educational Research Journal, 24(1), 13-48.
Reif, F., & Scott, L. (1999a). Teaching scientific thinking skills: Students and computers coaching each other. Am.J.
Phys., 67(9), 819-831.
Reif, F., & Scott, L. A. (1999b). Teaching scientific thinking skills: Students and computers coaching each other.
American Journal of Physics, 67(9), 819-831.
Rose, C. P., Moore, J., Allbritton, D., & Lehn, K. V. (2001). A comparative evaluation of socratic versus didactic
tutoring. In Proceedings of the 23rd annual Meeting of the Cognitive Science Society, Edinburgh, UK. URL
http://www.cs.cmu.edu/cprose/pubweb/cogsci01.pdf.

M. Chi et al. / An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System 113
Singh, S. P., Kearns, M. J., Litman, D. J., & Walker, M. A. (1999). Reinforcement learning for spoken dialogue
systems. In S. A. Solla, T. K. Leen, & K.-R. Müller (Eds.) NIPS, (pp. 956-962). The MIT Press.
Sutton, R. S. & Barto, A. G. (1998). Reinforcement Learning. MIT Press Bradford Books.
Tetreault, J., & Litman, D. (2006). Using reinforcement learning to build a better model of dialogue state. In
Proceedings 11th Conference of the European Chapter of the Association for Computational Linguistics
(EACL), Trento, Italy.
Tetreault, J. R., Bohus, D., & Litman, D. J. (2007). Estimating the reliability of mdp policies: a confidence interval
approach. In C. L. Sidner, T. Schultz, M. Stone, & C. Zhai (Eds.) HLT-NAACL, (pp. 276-283). The Association
for Computational Linguistics.
Tetreault, J. R., & Litman, D. J. (2008). A reinforcement learning approach to evaluating state representations in
spoken dialogue systems. Speech Communication, 50(8-9), 683-696.
VanLehn, K. (2006). The behavior of tutoring systems. International Journal Artiﬁcial Intelligence in Education,
16(3), 227-265.
VanLehn, K. (2008). The interaction plateau: Answer-based tutoring < step-based tutoring = natural tutoring. In
Woolf, Aı̈meur, Nkambou, & Lajoie (2008), (p. 7).
VanLehn, K., Graesser, A. C., & et al. (2007). When are tutorial dialogues more effective than reading? Cognitive
Science, 31(1), 3-62.
VanLehn, K., Jordan, P., & Litman, D. (2007). Developing pedagogically effective tutorial dialogue tactics: Experiments and a testbed. In Proceedings of SLaTE Workshop on Speech and Language Technology in Education
ISCA Tutorial and Research Workshop.
VanLehn, K., Lynch, C., & et al. (2005). The andes physics tutoring system: Lessons learned. Int. J. Artif. Intell.
Ed., 15(3), 147-204. ISSN 1560-4292.
VanLehn, K., Siler, S., Murray, R. C., Yamauchi, T., & Baggett, W. B. (2003). Why do only some events cause
learning during human tutoring? Cognition and Instruction, 21(3), 209-249.
Vygotsky, L. (1971). Interaction between learning and development. In T. M. Cole (Ed.) In Mind in Society.,
(pp.79-91). Harvard University Press: Cambridge Massachusetts.
Walker, M. A. (2000). An application of reinforcement learning to dialogue strategy selection in a
spoken dialogue system for email. Journal of Aritiﬁcial Intelligence Research, 12, 387-416. URL
http://citeseerx.ist.psu.edu/viewdoc/summary? doi=10.1.1.43.1121.
Williams, J., & Young, S. (2007a). Partially observable markov decision processes for spoken dialog systems.
Computer Speech and Language, 21(2), 231-422.
Williams, J., & Young, S. (2007b). Scaling pomdps for spoken dialog management. IEEE Trans. on Audio, Speech,
and Language Processing.
Woolf, B. P., Aı̈meur, E., Nkambou, R., & Lajoie, S. P. (Eds.) (2008). Intelligent Tutoring Systems, 9th International
Conference, ITS 2008, Montreal, Canada, June 23-27, 2008, Proceedings, volume 5091 of Lecture Notes in
Computer Science. Springer.
Wylie, R., Koedinger, K., & Mitamura, T. (2010). Is self-explanation always better? the effects of adding selfexplanation prompts to an english grammar tutor. In Proceedings of the 31st Annual Conference of the Cognitive
Science Society, COGSCI 2010, Amsterdam, The Netherlands.

January 2010
Volume 13 Number 1

Educational Technology & Society
An International Journal
Aims and Scope
Educational Technology & Society is a quarterly journal published in January, April, July and October. Educational Technology & Society
seeks academic articles on the issues affecting the developers of educational systems and educators who implement and manage such systems. The
articles should discuss the perspectives of both communities and their relation to each other:
 Educators aim to use technology to enhance individual learning as well as to achieve widespread education and expect the technology to blend
with their individual approach to instruction. However, most educators are not fully aware of the benefits that may be obtained by proactively
harnessing the available technologies and how they might be able to influence further developments through systematic feedback and
suggestions.
 Educational system developers and artificial intelligence (AI) researchers are sometimes unaware of the needs and requirements of typical
teachers, with a possible exception of those in the computer science domain. In transferring the notion of a 'user' from the human-computer
interaction studies and assigning it to the 'student', the educator's role as the 'implementer/ manager/ user' of the technology has been forgotten.
The aim of the journal is to help them better understand each other's role in the overall process of education and how they may support each
other. The articles should be original, unpublished, and not in consideration for publication elsewhere at the time of submission to Educational
Technology & Society and three months thereafter.
The scope of the journal is broad. Following list of topics is considered to be within the scope of the journal:
Architectures for Educational Technology Systems, Computer-Mediated Communication, Cooperative/ Collaborative Learning and
Environments, Cultural Issues in Educational System development, Didactic/ Pedagogical Issues and Teaching/Learning Strategies, Distance
Education/Learning, Distance Learning Systems, Distributed Learning Environments, Educational Multimedia, Evaluation, Human-Computer
Interface (HCI) Issues, Hypermedia Systems/ Applications, Intelligent Learning/ Tutoring Environments, Interactive Learning Environments,
Learning by Doing, Methodologies for Development of Educational Technology Systems, Multimedia Systems/ Applications, Network-Based
Learning Environments, Online Education, Simulations for Learning, Web Based Instruction/ Training

Editors
Kinshuk, Athabasca University, Canada; Demetrios G Sampson, University of Piraeus & ITI-CERTH, Greece; Ashok Patel, CAL Research
& Software Engineering Centre, UK; Reinhard Oppermann, Fraunhofer Institut Angewandte Informationstechnik, Germany.

Editorial Assistant
Barbara Adamski, Athabasca University, Canada.

Associate editors
Nian-Shing Chen, National Sun Yat-sen University, Taiwan; Vladimir A Fomichov, K. E. Tsiolkovsky Russian State Tech Univ, Russia;
Olga S Fomichova, Studio "Culture, Ecology, and Foreign Languages", Russia; Piet Kommers, University of Twente, The Netherlands;
Chul-Hwan Lee, Inchon National University of Education, Korea; Brent Muirhead, University of Phoenix Online, USA; Erkki Sutinen,
University of Joensuu, Finland; Vladimir Uskov, Bradley University, USA.

Advisory board
Ignacio Aedo, Universidad Carlos III de Madrid, Spain; Mohamed Ally, Athabasca University, Canada; Luis Anido-Rifon, University of
Vigo, Spain; Gautam Biswas, Vanderbilt University, USA; Rosa Maria Bottino, Consiglio Nazionale delle Ricerche, Italy; Mark Bullen,
University of British Columbia, Canada; Tak-Wai Chan, National Central University, Taiwan; Yam San Chee, Nanyang Technological
University, Singapore; Sherry Chen, Brunel University, United Kingdom; Darina Dicheva, Winston-Salem State University, USA; Michael
Eisenberg, University of Colorado, Boulder, USA; Robert Farrell, IBM Research, USA; Brian Garner, Deakin University, Australia;
Tiong Goh, Victoria University of Wellington, New Zealand; Mark D. Gross, Carnegie Mellon University, USA; Roger Hartley, Leeds
University, UK; J R Isaac, National Institute of Information Technology, India; Mohamed Jemni, University of Tunis, Tunisia; Paul
Kirschner, Open University of the Netherlands, The Netherlands; William Klemm, Texas A&M University, USA; Rob Koper, Open
University of the Netherlands, The Netherlands; Ruddy Lelouche, Universite Laval, Canada; Tzu-Chien Liu, National Central University,
Taiwan; David McConnell, Lancaster University, UK; Rory McGreal, Athabasca University, Canada; David Merrill, Brigham Young
University - Hawaii, USA; Marcelo Milrad, Växjö University, Sweden; Riichiro Mizoguchi, Osaka University, Japan; Permanand Mohan,
The University of the West Indies, Trinidad and Tobago; Kiyoshi Nakabayashi, National Institute of Multimedia Education, Japan; Hiroaki
Ogata, Tokushima University, Japan; Toshio Okamoto, The University of Electro-Communications, Japan; Thomas C. Reeves, The
University of Georgia, USA; Norbert M. Seel, Albert-Ludwigs-University of Freiburg, Germany; Timothy K. Shih, Tamkang University,
Taiwan; Yoshiaki Shindo, Nippon Institute of Technology, Japan; Kevin Singley, IBM Research, USA; J. Michael Spector, Florida State
University, USA; Timothy Teo, Nanyang Technological University, Singapore; Chin-Chung Tsai, National Taiwan University of Science
and Technology, Taiwan; Stephen J.H. Yang, National Central University, Taiwan.

Assistant Editors
Sheng-Wen Hsieh, Far East University, Taiwan; Dorota Mularczyk, Independent Researcher & Web Designer; Ali Fawaz Shareef,
Maldives College of Higher Education, Maldives; Jarkko Suhonen, University of Joensuu, Finland.

Executive peer-reviewers
http://www.ifets.info/

Subscription Prices and Ordering Information
For subscription information, please contact the editors at kinshuk@ieee.org.

Advertisements
Educational Technology & Society accepts advertisement of products and services of direct interest and usefulness to the readers of the journal,
those involved in education and educational technology. Contact the editors at kinshuk@ieee.org.
ISSN 1436-4522 (online) and 1176-3647 (print). © International Forum of Educational Technology & Society (IFETS). The authors and the forum jointly retain the
copyright of the articles. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that copies bear the full citation on the first page. Copyrights for components of this work owned by
others
IFETS must
be honoured.Forum
Abstracting
with creditTechnology
is permitted.&To
copy otherwise,
to republish,
to post
servers,
or toretain
redistribute
to lists, of
requires
prior
ISSNthan
1436-4522.
© International
of Educational
Society
(IFETS). The
authors and
the on
forum
jointly
the copyright
the articles.
specific
permission
and/or
a fee.
Request
permissions
from
the editors
at personal
kinshuk@ieee.org.
Permission
to make
digital
or hard
copies
of part or all
of this
work for
or classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear the full citation on the first page. Copyrights for components of this work owned by others than IFETS must be
honoured. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from the editors at kinshuk@massey.ac.nz.

i

Abstracting and Indexing
Educational Technology & Society is abstracted/indexed in Social Science Citation Index, Current Contents/Social & Behavioral Sciences, ISI
Alerting Services, Social Scisearch, ACM Guide to Computing Literature, Australian DEST Register of Refereed Journals, Computing Reviews,
DBLP, Educational Administration Abstracts, Educational Research Abstracts, Educational Technology Abstracts, Elsevier Bibliographic
Databases, ERIC, Inspec, Technical Education & Training Abstracts, and VOCED.

Guidelines for authors
Submissions are invited in the following categories:

 Peer reviewed publications: Full length articles (4000 - 7000 words)
 Book reviews
 Software reviews
 Website reviews
All peer review publications will be refereed in double-blind review process by at least two international reviewers with expertise in the relevant
subject area. Book, Software and Website Reviews will not be reviewed, but the editors reserve the right to refuse or edit review.
For detailed information on how to format your submissions, please see:
http://www.ifets.info/guide.php

Submission procedure
Authors, submitting articles for a particular special issue, should send their submissions directly to the appropriate Guest Editor. Guest Editors
will advise the authors regarding submission procedure for the final version.
All submissions should be in electronic form. The editors will acknowledge the receipt of submission as soon as possible.
The preferred formats for submission are Word document and RTF, but editors will try their best for other formats too. For figures, GIF and
JPEG (JPG) are the preferred formats. Authors must supply separate figures in one of these formats besides embedding in text.
Please provide following details with each submission:  Author(s) full name(s) including title(s),  Name of corresponding author,  Job
title(s),  Organisation(s),  Full contact details of ALL authors including email address, postal address, telephone and fax numbers.
The submissions should be uploaded at http://www.ifets.info/ets_journal/upload.php. In case of difficulties, they can also be sent via email to
(Subject: Submission for Educational Technology & Society journal): kinshuk@ieee.org. In the email, please state clearly that the manuscript is
original material that has not been published, and is not being considered for publication elsewhere.

ISSN 1436-4522 (online) and 1176-3647 (print). © International Forum of Educational Technology & Society (IFETS). The authors and the forum jointly retain the
copyright of the articles. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that copies bear the full citation on the first page. Copyrights for components of this work owned by
others
IFETS must
be honoured.Forum
Abstracting
with creditTechnology
is permitted.&To
copy otherwise,
to republish,
to post
servers,
or toretain
redistribute
to lists, of
requires
prior
ISSNthan
1436-4522.
© International
of Educational
Society
(IFETS). The
authors and
the on
forum
jointly
the copyright
the articles.
specific
permission
and/or
a fee.
Request
permissions
from
the editors
at personal
kinshuk@ieee.org.
Permission
to make
digital
or hard
copies
of part or all
of this
work for
or classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear the full citation on the first page. Copyrights for components of this work owned by others than IFETS must be
honoured. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from the editors at kinshuk@massey.ac.nz.

ii

Journal of Educational Technology & Society
Volume 13 Number 1 2010

Table of contents
Special issue articles
Guest Editorial – Intelligent Tutoring Systems
Roger Nkambou
Automatic Hint Generation for Logic Proof Tutoring Using Historical Data
Tiffany Barnes and John Stamper

1-2
3-12

Exploiting Sequential Patterns Found in Users' Solutions and Virtual Tutor Behavior to Improve Assistance in ITS
Philippe Fournier-Viger, Usef Faghihi, Roger Nkambou and Engelbert Mephu Nguifo

13-24

Meta-Cognitive Strategy Instruction in Intelligent Tutoring Systems: How, When, and Why
Min Chi and Kurt VanLehn

25-39

Affective Transitions in Narrative-Centered Learning Environments
Scott W. McQuiggan, Jennifer L. Robison and James C. Lester

40-53

Job announcements
Faculty Positions: Fall 2010 - Department of Curriculum & Instruction, Ruth S. Ammon School of Education,
Adelphi University

54-54

Full length articles
Interaction Chain Patterns of Online Text Construction with Lexical Cohesion
Hui-Chin Yeh, Yu-Fen Yang and Wing-Kwong Wong

55-68

Education Technology and Hidden Ideological Contradictions
Alan Amory

69-79

The Development and Implementation of Scaffolding-Based Self-Regulated Learning System for e/m-Learning
Kuei-Ping Shih, Hung-Chang Chen, Chih-Yung Chang and Tai-Chien Kao

80-93

The Influence of an Educational Computer Game on Children's Cultural Identities
Hsiang-Ping Chen, Chi-Jui Lien, Len Annetta and Yu-Ling Lu

94-105

Negotiating Contested Discourses of Learning Technologies in Higher Education
John Hannon and Tracey Bretag

106-120

Effect of an Interactive Courseware in the Learning of Matrices
Teoh Sian Hoon, Toh Seong Chong and Nor Azilah Binti Ngah

121-132

Effect of Live Simulation on Middle School Students' Attitudes and Learning toward Science
Ching-Huei Chen and Bruce Howard

133-139

Analyzing Online Behaviors, Roles, and Learning Communities via Online Discussions
Yu-Chu Yeh

140-151

Theory of Planned Behavior and Teachers' Decisions Regarding Use of Educational Technology
Jung Lee, Frank A. Cerreto and Jihyun Lee

152-164

Teachers' Perceptions of Technology Integration in the United Arab Emirates School Classrooms
Abdurrahman Ghaleb Almekhlafi and Farouq Ahmad Almeqdadi

165-175

Internet Use and Child Development: Validation of the Ecological Techno-Subsystem
Genevieve Marie Johnson

176-185

Divergence of Digital World of Teachers
Huseyin Uzunboylu and Nazime Tuncay

186-194

ISSN 1436-4522 (online) and 1176-3647 (print). © International Forum of Educational Technology & Society (IFETS). The authors and the forum jointly retain the
copyright of the articles. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies
are
not1436-4522.
made or distributed
for profitForum
or commercial
advantage
and that copies
bear the
full citation
on the first
Copyrights
components
of this work
by
ISSN
© International
of Educational
Technology
& Society
(IFETS).
The authors
and page.
the forum
jointlyforretain
the copyright
of theowned
articles.
others
than to
IFETS
be or
honoured.
Abstracting
is permitted.
To copy
otherwise,
to post fee
on provided
servers, orthat
to copies
redistribute
to made
lists, requires
prior
Permission
makemust
digital
hard copies
of part orwith
all ofcredit
this work
for personal
or classroom
usetoisrepublish,
granted without
are not
or distributed
specific
and/or
a fee. Request
permissions
from
kinshuk@ieee.org.
for
profitpermission
or commercial
advantage
and that
copies bear
the the
fulleditors
citationaton
the first page. Copyrights for components of this work owned by others than IFETS must be
honoured. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from the editors at kinshuk@massey.ac.nz.

iii

Development and Evaluation of an Interactive Mobile Learning Environment with Shared Display Groupware
Jie Chi Yang and Yi Lung Lin

195-207

Athens 2004 Team Leaders' Attitudes toward the Educational Multimedia Application "Leonidas"
Nikolaos Vernadakis, Maria Giannousi, Vassiliki Derri, Iraklis Kellis and Efthimis Kioumourtzoglou

208-219

Extended Relation Metadata for SCORM-based Learning Content Management Systems
Eric Jui-Lin Lu, Gwoboa Horng, Chia-Ssu Yu and Ling-Ying Chou

220-235

A Study of the Efficacy of Project-based Learning Integrated with Computer-based Simulation - STELLA
Rogheyeh Eskrootchi and G. Reza Oskrochi

236-245

Marking Strategies in Metacognition-Evaluated Computer-Based Testing
Li-Ju Chen, Rong-Guey Ho and Yung-Chin Yen

246-259

Book review(s)
Handbook of Research on Educational Communications and Technology (Eds. J. M. Spector, M. D. Merrill, J. van
Merrienboer, & M. P Driscoll)
Reviewer: Gi-Zen Liu

ISSN 1436-4522 (online) and 1176-3647 (print). © International Forum of Educational Technology & Society (IFETS). The authors and the forum jointly retain the
copyright of the articles. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that copies bear the full citation on the first page. Copyrights for components of this work owned by
others
IFETS must
be honoured.Forum
Abstracting
with creditTechnology
is permitted.&To
copy otherwise,
to republish,
to post
servers,
or toretain
redistribute
to lists, of
requires
prior
ISSNthan
1436-4522.
© International
of Educational
Society
(IFETS). The
authors and
the on
forum
jointly
the copyright
the articles.
specific
permission
and/or
a fee.
Request
permissions
from
the editors
at personal
kinshuk@ieee.org.
Permission
to make
digital
or hard
copies
of part or all
of this
work for
or classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear the full citation on the first page. Copyrights for components of this work owned by others than IFETS must be
honoured. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from the editors at kinshuk@massey.ac.nz.

260-263

iv

Nkambou, R. (2010). Intelligent Tutoring Systems (Guest Editorial). Educational Technology & Society, 13 (1), 1–2.

Intelligent Tutoring Systems (Guest Editorial)
Roger Nkambou
Computer Science Department, University of Quebec at Montreal, QC, Canada // nkambou.roger@uqam.ca

Intelligent Tutoring Systems (ITS) are meant to provide useful tutoring services for assisting the student. These
services include coaching, assisting, guiding, helping, and tracking the student during problem-solving situations. To
offer high-quality tutoring services, an ITS must be able to establish the correct student profile, then understand and
diagnose the student cognitive as well as its affective state. This special issue of Educational Technology & Society
presents recent works dealing with those matters.

Extracting Procedural Models Using Educational Data Mining
The main goal of an intelligent tutoring system is to actively provide guidance to the student in problem-solving
situations. Relevant feedback should be founded on a thorough understanding and diagnosis of student responses.
Building such understanding and diagnosis model is a difficult issue that is also a time-intensive process involving
human experts. This issue becomes even more difficult in ill-defined domains where an explicit representation of the
training task is hard, if not impossible, to set up. Educational data-mining (EDM) brings some promising solutions to
this issue.
You will find in this special issue two EDM-based solutions proposed for coping with this problem. Each of these
solutions consists of a model that can constantly learn from new learner or user data and thus, guaranties that the
tutor provides an up-to-date feedback.
In one hand, Barnes and Stamper propose a novel application of Markov decision processes (MDPs) to automatically
generate hints for an intelligent tutor that learns. This approach eases the process of building the understanding and
diagnosis model of student actions. The authors extracted MDPs from four semesters of student solutions created in a
logic proof tutor, and calculated the probability of being able to generate hints for students at any point in a given
problem. The results indicate that extracted MDPs and their proposed hint-generating functions are able to provide
hints over 80% of the time. The results also indicate that they can provide valuable tradeoffs between hint specificity
and the amount of data used to create an MDP. 
In the other hand, Fournier-Viger et al. present a novel framework for adapting the behavior of intelligent agents
based on human experts’ data. The framework consists of an extended sequential pattern-mining algorithm that, in
combination with association rule discovery techniques, is used to extract temporal patterns and relationships from
the behavior of human learners of multiple profiles, executing a procedural task. The proposed framework has been
integrated within CanadarmTutor, an intelligent tutoring system aimed at helping students solve procedural problems
that involve moving a robotic arm in a complex virtual environment. CanadarmTutor acts in an ill-defined domain
where the problem space associated with a given task consists of an infinite number of paths. The framework was
used to improve the behavior of a cognitive agent that adapts its decision by learning from data gathered during past
cognitive cycles. The results of the experimentation demonstrate the benefits of the framework for tutoring systems
acting in ill-defined domains.

Filling the Gap Between Student Profiles Through Metacognitive Problem-Solving Strategy
One benefit of tutoring is of narrowing, even eliminating the gap between High and Low learners. Low learners are
those who are more sensitive to variations in learning environments. Effective ITS should narrow the gap as much as
possible without pulling the High learners down. In their paper, Chi and VanLehn present a study that investigates
this issue. The study involved two groups of college students who studied probability first and then physics. The
experimental group studied probability with Pyrenees, an ITS that explicitly taught and required them to employ a

ISSN 1436-4522 (online) and 1176-3647 (print). © International Forum of Educational Technology & Society (IFETS). The authors and the forum jointly retain the
copyright of the articles. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that copies bear the full citation on the first page. Copyrights for components of this work owned by
others than IFETS must be honoured. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior
specific permission and/or a fee. Request permissions from the editors at kinshuk@ieee.org.

1

general problem-solving strategy; the control group studied probability with Andes, an ITS that does not teach or
require any particular strategy. During subsequent physics instruction, both groups used Andes.
Results showed that an Intelligent Tutoring System teaching a domain-independent problem-solving strategy indeed
closed the gap between High and Low learners, not only in the domain where it was taught (probability) but also in a
second domain where the strategy had not been taught (physics). The strategy includes two main components: one is
solving problems via Backward-Chaining (BC) from goals to givens, named the BC-strategy, and the other is
drawing students' attention on the characteristics of each individual domain, named the principle-emphasis skill.
Evidence suggests that the Low experimental group transferred the principle-emphasis skill to physics while the
High experimental apparently already possessed it and thus mainly transferred the BC-strategy.

Coping with Affective Issues in Tutoring Systems
Considering learners’ affective responses during learning episodes is a key issue for more effective tutoring dialogue.
Hence, recent work has begun to investigate the emotions experienced during learning in a variety of environments.
McQuiggan et al. contribute to this effort by investigating the likelihood of affective transitions that occur
throughout narrative-centered learning experiences. The study was conducted with the Crystal Island, a learning
environment in which narrative is used as a mechanism to contextualize learning.
The results suggest two directions for future work. First, they call for investigation of what type of feedback
pedagogical agents should consider when empathy does not promote desirable affective states for learning. For
instance, reactive empathy was likely to encourage transitions to either flow or frustration. Second, analysis of
individual differences is necessary to determine the affective transitions common across a variety of demographics
such as gender, but also across learning attributes such as efficacy, goal orientation, interest, and abilities to selfregulate both learning and affect.

2

Barnes, T., & Stamper, J. (2010). Automatic Hint Generation for Logic Proof Tutoring Using Historical Data. Educational
Technology & Society, 13 (1), 3–12.

Automatic Hint Generation for Logic Proof Tutoring Using Historical Data
Tiffany Barnes and John Stamper
University of North Carolina at Charlotte, Computer Science Department, Charlotte, NC, USA //
Tiffany.Barnes@gmail.com // John@stamper.org
ABSTRACT
In building intelligent tutoring systems, it is critical to be able to understand and diagnose student responses in
interactive problem solving. However, building this understanding into a computer-based intelligent tutor is a
time-intensive process usually conducted by subject experts. Much of this time is spent in building production
rules that model all the ways a student might solve a problem. In our prior work, we proposed a novel application
of Markov decision processes (MDPs) to automatically generate hints for an intelligent tutor that learns. We
demonstrate the feasibility of this approach by extracting MDPs from four semesters of student solutions in a logic
proof tutor, and calculating the probability that we will be able to generate hints for students at any point in a
given problem. Our past results indicated that extracted MDPs and our proposed hint-generating functions will be
able to provide hints over 80% of the time. Our results also indicated that we can provide valuable tradeoffs
between hint specificity and the amount of data used to create an MDP.

Keywords
Educational data mining, Hint generation, Intelligent tutoring, Propositional logic proofs

Introduction
According to the Joint Task Force on Computing Curricula (2005), discrete mathematics is a core course in computer
science, and an important topic in this course is solving formal logic proofs. However, this topic is of particular
difficulty for students, who are unfamiliar with logic rules and manipulating symbols. To allow students extra
practice and help in writing logic proofs, we are building an intelligent tutoring system on top of our existing proofverifying program. Results from student surveys and our experience in teaching discrete math indicate that students
particularly need hints when they get stuck.
The problem of offering individualized help is not unique to logic proofs. Through adaptation to individual learners,
intelligent tutoring systems (ITS) can have significant effects on learning (Anderson & Gluck, 2001). However,
building one hour of adaptive instruction takes between 100 and 1000 hours of work for subject experts, instructional
designers, and programmers (Murray, 1999), and a large part of this time is used in developing production rules that
model student behavior and progress. A variety of approaches have been used to reduce the development time for
ITSs, including ITS authoring tools and constraint-based student models. ASSERT is an ITS authoring system that
uses theory refinement to learn student models from an existing knowledge base and student data (Baffes & Mooney,
1996). Constraint-based tutors, which look for violations of problem constraints, require less time to construct and
have been favorably compared to cognitive tutors, particularly for problems that may not be heavily procedural
(Mitrovic, Keodinger, & Martin, 2003). However, constraint-based tutors can only provide condition-violation
feedback, not goal-oriented feedback that has been shown to be more effective (Van Lehn, 2006).
Some systems use teacher-authored or demonstrated examples to develop ITS production rules. RIDES is a “Tutor in
a Box” system used to build training systems for military equipment usage, while DIAG was built as an expert
diagnostic system that generates context-specific feedback for students (Murray, 1999). These systems cannot be
easily generalized, however, to learn from student data. In example-based authoring tools, teachers work problems in
what they predict to be common correct and incorrect approaches, and then annotate the learned rules with
appropriate hints and feedback. The Cognitive Tutors Authoring Tool (CTAT) has been used to develop examplebased tutors for genetics, Java, and truth tables (Koedinger, Aleven, Heffernan, McLaren, & Hockenberry, 2004).
This system has also been used with data to build initial models for an ITS, in an approach called Bootstrapping
Novice Data (BND) (McLaren, Koedinger, Schneider, Harrer, & Bollen, 2004). However, in both of these
approaches, considerable time must still be spent in identifying student approaches and creating appropriate hints.
Machine learning has also been used to improve tutoring systems. In the ADVISOR tutor, machine learning was used
to build student models that could predict the time students took to solve arithmetic problems, and to adapt
ISSN 1436-4522 (online) and 1176-3647 (print). © International Forum of Educational Technology & Society (IFETS). The authors and the forum jointly retain the
copyright of the articles. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that copies bear the full citation on the first page. Copyrights for components of this work owned by
others than IFETS must be honoured. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior
specific permission and/or a fee. Request permissions from the editors at kinshuk@ieee.org.

3

instruction to minimize this time while meeting teacher-set instructional goals (Beck, et al., 2000). In the Logic-ITA
tutor, student data was mined to create hints that warned students when they were likely to make mistakes using their
current approach (Merceron & Yacef, 2005). Another logic tutor called the Carnegie Proof Lab uses an automated
proof generator to provide contextual hints (Sieg, 2007).
Similar to the goal of BND, we seek to use student data to directly create student models for an ITS. However,
instead of feeding student behavior data into CTAT to build a production rule system, our method generates Markov
decision processes (MDPs) that represent all student approaches to a particular problem, and uses these MDPs
directly to generate hints. This method of automatic hint generation using previous student data reduces the expert
knowledge needed to generate intelligent, context-dependent hints and feedback. The system is capable of continued
refinement as new data is provided. In this work, we demonstrate the feasibility of our hint generation approach
through simulation experiments on existing student data.

Background and Proofs Tutorial context
The Proofs Tutorial is a computer-aided learning tool on NovaNET (http://www.pearsondigital.com/novanet/). This
program has been used for practice and feedback in writing proofs in university discrete mathematics courses taught
by the first author and others at North Carolina University since 2002 and at UNC Charlotte since 2006. In the Proofs
Tutorial, students are assigned a set of 10 problems that range from simpler logical equivalence applications to more
complex inference proofs. In the tutorial, students type in consecutive lines of a proof, which consist of four parts:
the statement, reference lines, the axiom used, and the substitutions that allow the axiom to be applied. After the
student enters these four parts, the statement, reference lines, axiom, and substitutions are verified. If a mistake is
made, a warning message is shown, and the line is deleted (but saved for later analysis). In this work, we examine
student solutions to Proof 1. Table 1 lists an example of a student solution that includes three errors.
In Barnes (2006), the first author has applied educational data mining to analyze completed formal proof solutions
for automatic feedback generation. However, this work did not take into account student errors, and could only
provide general indications of student approaches, as opposed to feedback tailored to a student’s current progress. In
Stamper (2006), the second author performed a pilot study to extract MDPs for a simple proof from three semesters
of student data, and verified that the extracted rules conformed to expert-derived rules and generated buggy rules that
surprised experts. In Barnes & Stamper (2007), we used visualization tools to explore how to generate hints based on
MDPs extracted from student data. In Croy, Barnes, & Stamper (2007), we applied the technique to visualize student
proof approaches to allow teachers to identify problem areas for students. This was just one method used to identify
students at risk of falling behind in the course.
Table 1. Sample Proof 1 solution
Statement
1. a → b
2. c → d
3. ¬ (a → d)
¬avd
4. a ^ ¬ d
5. a
b
b
6. b
7. ¬ d
8. ¬c
9. b ^ ¬c

Line

3
3
4
4
1
1,5
4
2,7
6,8

Reason
Given
Given
Given
rule IM (error)
rule IM implication
rule S simplification
rule MP (error)
rule MP (error)
rule MP modus ponens
rule S simplification
rule MT modus tollens
rule CJ conjunction

Markov decision processes
A Markov decision process (MDP) is defined by its state set S, action set A, transition probabilities P, and a reward
function R (Sutton & Barto, 1998). On executing action a in state s, the probability of transitioning to state s' is
4

denoted P(s' | s, a) and the expected reward associated with that transition is denoted R(s' | s, a). For a particular
point in a student’s proof, our method takes the current statements and conclusion as the state, and the student’s input
as the action. Therefore, each proof attempt can be seen as a graph with a sequence of states (each describing the
solution up to the current point), connected by actions. Specifically, a state is represented by the list of statements
generated in the student attempt, and actions are the axioms used at each step.
We combine all student solution graphs into a single graph by taking the union of all states and actions and mapping
identical states to one another. Once this graph is constructed, it represents all of the paths students have taken in
working a proof. Typically at this step, value iteration is used to find an optimal solution to the MDP. For the
experiments in this study, we set a large reward for the goal state (100), penalties for incorrect states (10), and a cost
for taking each action (1). Setting a non-zero cost on actions causes the MDP to penalize longer solutions (but we set
this at 1/10 the cost of taking an incorrect step). These values may need to be adjusted for different sizes of MDPs.
We apply the value iteration technique using a Bellman backup to assign reward values to all states in the MDP
(Sutton & Barto, 1998). The equation for calculating values V(s) for each state s, where R(s) is the reward for the
state,  is the discount factor (set to 1), and Pa(s,s') is the probability that action a will take state s to state s':



V s  Rs    max Pa s, s  V s
a
 s


For value iteration, V is calculated for each state until there is little change in the value function over the entire state
space. Once this is complete, the optimal solution in the MDP corresponds to taking a greedy traversal approach in
the MDP (Barnes & Stamper, 2007). The reward values for each state then indicate how close to the goal a state is,
while probabilities of each transition reveal the frequency of taking a certain action in a certain state.

Generating individualized help
We propose to provide real-time, individualized hints to support on-going student proof construction efforts. As
described in (Barnes & Stamper, 2007), we generate an MDP for each problem and use it to generate hints for new
students who are solving proofs. Since our tutorials have been used as a computer-aided instructional tool for a
number of years, we have many semesters of data from which to create large MDPs for each proof problem. We first
use these MDPs to add intelligent hints to every problem. As a new student works a problem, we match each of their
states to those in the MDP. If their state is present in the problem’s MDP, we enable a hint button to give contextual
help.
In Barnes & Stamper (2007), we have proposed several reward functions that could be used in hint generation,
including expert, typical, and least error-prone. The reward function we have described herein reflects an expert
reward function, where the value for a state reflects the shortest path to the goal state. Given the current state, when
the hint button is pressed, we select a reward function for the current student based on his or her student profile. If
we have identified the student as an at-risk student, we may select the “least error-prone” reward function for
generating hints. On the other hand, high-performing students would likely benefit from expert hints, while students
between these two extremes may benefit from hints reflecting typical student behavior (Barnes & Stamper, 2007).
After we’ve selected a reward function, we select the next state with the highest reward value. We create four levels
of hints from this state, as follows:
(1) Indicate a goal statement to derive (goal-setting hint).
(2) Tell the student what rule to apply next (rule hint).
(3) Indicate the statements where the rule can be used (pointing hint).
(4) Tell the student both the rule and the statements to combine (bottom-out hint).
The hint sequence is constructed to provide goal-setting, pointing, and bottom-out hints. Hints that help students set
intermediate goals have been shown to be effective in (McKendree, 1990). Pointing hints help focus user attention,
while bottom-out hints essentially tell students the answer (Van Lehn, 2006).
We plan to limit the number of hints a student can use and still receive credit for working the problem. We believe
that four hints is a fair number, to be used on a single state in sequence as above or on separate states in the same
problem. This results in giving the student one full step of the proof, or allowing rule hints up to four times.
5

If a student’s state is not found in the MDP, the hint button will be disabled. The student can then get the tutor’s
built-in feedback that indicates the correctness of each step, but will not get strategic help. However, we can add the
student’s action and its correctness to our database and periodically run value iteration to update the reward function
values. Before an update is applied, we will test the update to be sure that the instructors agree with the generated
hints.

Method
This experiment uses data from the four fall semesters of 2003–2006, during which an average of 220 students took
the discrete math course at NC State University. Students in this course were typically engineering and computer
science students in their second or third year of college, but most had not been exposed to a course in logic. Students
attended several lectures on propositional logic and completed online homework in which students completed truth
tables and filled in the blanks in partially completed proofs. Students then used the Proofs Tutorial to solve 10
proofs, directly or indirectly. Sixty percent of students used direct proof when solving proof 1. We extracted 537 of
students’ first attempts at direct solutions to proof 1 from the Proofs Tutorial.
The data were validated by hand by extracting all statements generated by students and removing those that 1) were
false or unjustifiable, or 2) were of improper format. We also removed all student steps using the axioms
Conjunction, Double Negation, and Commutative, since students were allowed to skip these steps in the tutorial.
After cleaning the data, there were 523 attempts at proof 1. Of these, 381 (73%) were complete and 142 (27%) were
partial proofs, indicating that most students completed the proof. The average lengths, including errors, were 13 and
10 steps, respectively, for completed and partial proofs. When excluding errors and removed steps, the average
number of lines in each student proof was 6.3 steps.
We performed several experiments to explore the capability of our method to generate automated hints. In each
experiment, we isolated the data into training and test sets, where the training set was used to generate the Markov
Decision Process (MDP) as described above, and the test set was used to explore hint availability. The process for
comparing the test set to the MDP consisted of several steps. Because of the structure of the tutorial, we first
removed all error states from the MDP and from student attempts before comparison, since the tutorial provides error
messages and deletes the corresponding error from the student proof. Then, each attempt in the test set was mapped
onto a sequence of states. For each test state, there are two requirements for a hint to be available: 1) there must be a
“matching” state in the MDP, and 2) the “matching” state must not be a leaf in the MDP. The closer the match
between a test state and the corresponding MDP state, the more context-specific the hint based on that match is.

Leaves
Because we included partial solutions in our training datasets, there are leaves in the MDPs or, in other words,
statements with no subsequent actions taken. Therefore, we potentially had a higher percentage of matches than the
number of states where hints could be generated. To investigate this, we examined all leaf nodes, and found that the
overwhelming majority of leaves occurred in only one semester, for one student. This means that these states are
never matched, and therefore do not count toward being able to provide hints. There were a total of three leaf states
that occurred in multiple semesters. For two of these states, they occurred twice in one semester, and once in another.
For the remaining leaf, it occurred once in one semester and once in another. Therefore, we have over-counted the
number of times we can give hints to students by at most two matches in any semester. Since the minimum number
of state visits in any semester is 304, this represents an error of at most 2/304 (0.06%) in any reported statistic.

Matching functions
In our experiments, we considered four matching functions that would allow us to select a source statement for hint
generation: 1) ordered, 2) unordered, 3) ordered minus the latest statement, and 4) unordered minus the latest
statement. An ordered, or exact, state match means that another student has taken the same sequence of steps in
solving the proof. An unordered state match means that there is a state with exactly the same statements, but the
states were not necessarily reached in the same order. An “ordered-1” match looks for an exact match between the
6

student’s previous state and an MDP state. An “unordered-1” match looks for an unordered match between the
student’s previous state and an MDP state. Once a match is made, we generate a hint based on knowing the next
optimal (highest reward value) step from the matching state. The more specific the match, the more contextualized
the hint.
To determine hint availability, we calculated two numbers. The first was the “move matches,” the number of test set
states or “moves,” including duplicates, that had matches in the MDP, divided by the total number of test set states.
The second was the “unique matches,” where we determined all unique test states and calculated the percentage of
these that have matches in the MDP. Move matches gave us a measure of the percentage of the time a particular
student was able to attain a hint while working a proof. Unique matches gave us a measure of the percentage of
overlap in the states in the test set and the MDP.
We conducted two experiments to test the feasibility of automated hint generation. The first was something like a
cross-validation study, comparing the hints we could generate using various semesters of data for MDP creation. The
second was a simulation of creating MDPs incrementally as students worked proofs and calculating the probability
of being able to generate hints as new attempts were added to the MDP.

Experiment 1: Comparing classes
In this experiment, we explored the ability of our system to provide hints after one, two, three, or four semesters of
data were used to build MDPs. Table 2 shows that each semester was used as a test set (denoted by f and the
semester), while all the remaining semesters were used as training sets for MDPs. For example, when fall 2003 was
used as test set f3, it was compared with MDPs created from one semester of data each (e.g., M4 = fall 2004), two
semesters of data each (e.g., M45 = fall 2004 and 2005), and three semesters of data (e.g., M456 = fall 2004 to
2006).
Table 2. Experimental design for comparing classes
Test 1-sem. MDP
2-sem. MDP
3-sem. MDP
f3
M4, M5, M6 M45, M46, M56
M456
f4
M3, M5, M6 M35, M36, M56
M356
f5
M3, M4, M6 M34, M36, M46
M346
f6
M3, M4, M5 M34, M35, M45
M345

This experiment provides us insight into the number of semesters of data we might need to provide hints a reasonable
percentage of the time while students are solving proofs. Table 3 presents the data for each semester. We note that
semester fall 2005 was unusual: there was a small number of states, but a large number of moves, suggesting that
students solved these proofs in very similar ways.
Table 3. Semester data, including attempts, moves, and MDP states
Semester # Attempts MDP states # Moves
f3
172
206
711
f4
154
210
622
f5
123
94
500
f6
74
133
304

We hypothesized that we could provide hints a majority of the time, using just one semester as our MDP training
data. Table 4 shows the percentage of ordered matches between each semester and the remaining combinations of
training sets. We were very encouraged by these data, which suggest that our system would provide highly
contextualized hints over 66% of the time, in the worst case, after just one semester of training. In all cases, adding
more data increased the probability of providing hints, though we do see diminishing returns when comparing the
marginal increase between 1–2 (6.8%) and 2–3 (2.8%) semesters of data.
7

Table 4. Average % move matches using the ordered function
Test set 1-sem. MDPs 2-sem. MDPs 3-sem. MDPs
f3
68.73%
75.67%
78.62%
f4
69.77%
77.71%
81.03%
f5
86.33%
90.80%
92.00%
f6
66.34%
74.12%
77.63%
Average
72.79%
79.57%
82.32%

Tables 5–7 show the results of this experiment using the unordered, ordered-1, and unordered-1 matching
techniques. These results show consistent increases within each table, going from 1-semester MDPs up to 2-semester
MDPs, as expected. However, the increases between 2- and 3-semester MDPs are decreasing, suggesting consistent
diminishing returns for adding more data to the MDPs.
Table 5. Average % move matches using the unordered function
Test set 1-sem. MDPs 2-sem. MDPs 3-sem. MDPs
f3
76.62%
82.16%
84.37%
f4
75.35%
81.99%
84.41%
f5
91.93%
94.40%
95.40%
f6
74.56%
82.35%
84.87%
Average
79.62%
85.22%
87.26%
Table 6. Average % move matches using the ordered-1 function
Test set 1-sem. MDPs 2-sem. MDPs 3-sem. MDPs
f3
76.92%
85.14%
89.00%
f4
76.26%
85.69%
90.35%
f5
90.78%
96.19%
97.80%
f6
75.55%
84.32%
89.14%
Average
79.88%
87.84%
91.57%
Table 7. Average % matches using the unordered-1 function
Test set
f3
f4
f5
f6
Average

1-sem. MDPs
82.63%
81.73%
94.60%
81.03%
85.00%

2-sem. MDPs
89.19%
90.14%
97.00%
89.69%
91.50%

3-sem. MDPs
91.99%
93.41%
98.00%
92.43%
93.96%

Table 8 lists the average percentage of matches for each of our experiments using the four match functions. This
table gives an indication of the tradeoffs between using multiple semesters of data versus multiple techniques for
matching. Here, we see that, on average, for 72% of moves, we can provide highly contextualized (ordered) hints
using just one semester of data. With two semesters of data, we can provide these hints almost 80% of the time, but
this only increases to 82% for three semesters of data. If we wished to provide hints after collecting just one semester
of data, we could provide less contextualized hints for those who didn’t have ordered matches in the MDP. There is a
nearly identical benefit to providing hints using unordered versus ordered-1 searches, increasing the match rate to
almost 80%. We did not calculate the marginal benefit of providing one of these over the other. However, we can
provide hints to an additional 5% of students if we add the unordered-1 match function.
8

When analyzing these data, we observed a skew in all statistics because of the unusual distribution of states and
moves in f5. We therefore repeated all experiments excluding f5, and the results are given in Table 9. The
differences caused by skew in f5 had a smaller effect as you move from top left to bottom right, suggesting that more
data or less sensitive matching can mitigate the effect of unusual training data.
Table 8. Comparison of % move matches and matching techniques
Matching
1-sem. MDPs 2-sem. MDPs 3-sem. MDPs
72.79%
79.57%
82.32%
Ordered
79.62%
85.22%
87.26%
Unordered
79.88%
87.84%
91.57%
Ordered-1
85.00%
91.50%
93.96%
Unordered-1

Table 9. Comparison of % move matches, excluding f5
Test set
1-sem. MDPs 2-sem. MDPs
70.97%
78.05%
Ordered
78.69%
83.59%
Unordered
79.02%
87.99%
Ordered-1
85.77%
91.86%
Unordered-1

Table 10 shows the marginal increase of each matching technique, for each MDP size, to illustrate the tradeoffs
between additional data and matching technique. When considering matching functions, the easiest technical change
is from ordered to ordered-1, where one statement is removed from the test state before comparison with the MDP
states. In all cases, the benefit of providing these hints is higher than that of providing hints based on unordered
matches. This is probably because there is some inherent partial ordering in proofs, so only limited benefit is seen
from reordering statements. When an ordered hint cannot be matched, it is perhaps more likely that the student has
just performed a step that no one else has done before, rather than generating a new ordering of steps, so the benefit
of ordered-1 can exceed that of unordered. Providing the unordered search requires us to maintain two separate
MDPs (one ordered and one unordered) to make the search more efficient, so there are both time and space tradeoffs
to using unordered matching. However, adding unordered-1 after adding unordered provides a very large difference
in our capability to provide hints, with little investment in time.
Table 10. Marginal increases when comparing matching techniques to ordered
Technique
1-sem. ordered 2-sem. ordered 3-sem. ordered
6.83%
5.65%
4.94%
Unordered
7.09%
8.27%
9.25%
Ordered-1
12.21%
11.93%
11.64%
Unordered-1

As part of this study we also compared the unique states across semesters, as shown in Table 11. This gives us a
measure of the percent overlap between MDPs. Three semesters of data with ordered matching or one semester of
data with unordered-1 matching will give us over 50% matching of states across MDPs.
Table 11. Unique state % matches across semesters and techniques
Test set
1-sem. MDPs 2-sem. MDPs 3-sem. MDPs
34.55%
45.84%
51.93%
Ordered
43.62%
55.23%
59.90%
Unordered
48.25%
63.07%
71.39%
Ordered-1
57.28%
71.98%
77.87%
Unordered-1

9

Experiment 2: Exploring the “cold start” problem
One critique of using data to generate hints has been the expected time needed for the method to be applied to a new
problem, or in other words, the “cold start” issue. Our hypothesis was that a relatively low number of attempts would
be needed to build an MDP that could provide hints to a majority of students. One method for building our hint MDP
would be to incrementally add MDP states as students solve proofs. This experiment explores how quickly such an
MDP is able to provide hints to new students, or in other words, how long it takes to solve the cold start problem. For
one trial, the method is given below.
Let Test = {all 523 student attempts}
Randomly choose and remove the next attempt a from the Test set.
Add a’s states and recalculate the MDP.
Randomly choose and remove the next attempt b from the Test set.
Compute the number of matches between b and MDP.
If Test is non-empty, then let a = b and go to step 3. Otherwise, stop.
For this experiment, we used the ordered and unordered matching functions, and plotted the resulting average
matches over 100,000 trials, as shown in Figure 1. These graphs show a very quick rise in ability to provide hints to
students, which can be fit using power functions, whether the system uses ordered or unordered MDP states and
matching.

Figure 1. Hints available when the MDP is constructed using a given number of attempts, averaged over 100,000
random orderings of the attempts selected for the MDP.

Clearly, the availability to give hints ramps up very quickly. Table 12 lists the number of attempts needed in the
MDP versus target hint percentages. For the unordered matching function, the 50% threshold is reached at just 8
student attempts and the 75% threshold at 49 attempts. For ordered matching, 50% occurs on attempt 11 and 75% on
attempt 88. These data are encouraging, suggesting that instructors using our MDP hint generator could seed the data
to jump-start new problems. By allowing the instructor to enter as few as 8–11 example solutions to a problem, the
method might already be capable of automatically generating hints for 50% of student moves.
Table 12. Attempts needed to achieve threshold % hints levels
50% 55% 60% 65% 70% 75% 80% 85%
Un-Ordered
8
11
14
20
30
46
80
154
Ordered
11
15
22
33
55
85
162
362

90%
360
N/A
10

Pilot study on hint generation and availability
We have constructed a hint generator using the methods described herein to add hints to Deep Thought, a visual logic
tutor created by Marvin Croy (2000). When a student presses the new hint button, our hint generator searches for the
current state in the MDP and checks that a successor state exists. If it does, the successor state with the highest value
is used to generate a hint sequence as described above. In our pilot experiment as described in Barnes, Stamper,
Lehman, & Croy (2008), hints were added to four logic proof problems in a spring 2008 deductive logic course with
40 students enrolled. MDPs were created for these four problems with 16 to 26 training examples, and the percent
hint availability was calculated for all problem states. Based on the results in this study, we predicted that hints
(using ordered matching) would be available approximately 56–62% of the time. In the pilot study, if a student had
pressed the hint button after every move taken, a hint would have been available about 48% of the time. Although
this percentage seems low, we found that when students requested hints, they were available 91% of the time. This
suggests that hints are needed precisely where we have data in our MDPs from previous semesters. There are several
potential explanations for this: students may be avoiding using hints; hints may be most needed in only a few key
steps; or the students may have felt very confident in solving proofs before working these problems. We plan to
investigate the reasons for this surprising result in future experiments.

Conclusions and future work
We have proposed and explored the feasibility of an approach to mining Markov decision processes from student
work to automatically generate hints. This approach differs from prior work in authoring tutoring systems by mining
actual student data, rather than relying on teachers to add examples the system can learn from. Our tutor can already
classify many errors students make. Adding the MDP to this tutor enables it to provide hints. This MDP can
constantly learn from new student data. We note that on cold start for a new problem that has no student data, the
system will still act as a problem-solving environment, but after even one semester of data is collected, a significant
number of hints can be generated. As more data are added, more automated hints can be generated. We have
implemented this hint system in the Deep Thought logic tutor and are currently testing whether our hints affect
overall learning. In our future work, we will continue to explore ways to learn general rules to build intelligent
feedback and help with greater coverage and robustness. For instance, we plan to group students according to their
proofs behavior and class performance, and create tailored MDPs for each group of students. In an interesting
extension, we are investigating ways to determine the usefulness of a problem step based on its frequency in the
MDP. This work will allow us to detect and prevent hints that may guide students to perform steps that others have
taken, but that did not contribute to the problem solution. This work on measuring the utility of a problem step can
also be used to generate help in ill-defined domains.
We believe the feasibility studies presented in this paper provide an important methodology for predicting the
reliability of data-driven methods for deriving feedback and hints for students. These methods address teacher
concerns regarding the availability of sufficient and accurate help while addressing the need to support student
learning with low-cost, scalable methods.

References
Anderson, J., & Gluck, K. (2001). What role do cognitive architectures play in intelligent tutoring systems? In D. Klahr & S.
Carver (Eds.), Cognition & Instruction: 25 years of progress (pp. 227–262). Mahwah, NJ: Erlbaum.
Baffes, P., & Mooney, R. J. (1996). A novel application of theory refinement to student modeling. AAAI-96: Proceedings of the
13th National Conference on Artificial Intelligence (pp. 403–408), Menlo Park, CA: AAAI Press.
Barnes, T. (2006). Evaluation of the q-matrix method in understanding student logic proofs. In G. Sutcliffe & R. Goebel (Eds.),
Proceedings of the Nineteenth International Florida Artificial Intelligence Research Society Conference, Menlo Park, CA: AAAI
Press.
Barnes, T., & Stamper, J. (2007). Toward the extraction of production rules for solving logic proofs, In Proc. 13th Intl. Conf. on
Artificial Intelligence in Education, Educational Data Mining Workshop. Online proceedings retrieved March 25, 2009, from
http://aied.inf.ed.ac.uk/AIED2007/AIED-EDM_proceeding_full2.pdf.
11

Barnes, T., Stamper, J, Lehman, L., & Croy, M. (2008). A pilot study on logic proof tutoring using hints generated from historical
student data. Proceedings of the 1st Annual International Conference on Educational Data Mining. Montreal, CA, June 20–21,
2008, Retrieved March 25, 2009, from http://www.educationaldatamining.org/EDM2008/index.php?page=proceedings.
Beck, J., Woolf, B., & Beal, C. (2000). ADVISOR: A machine learning architecture for intelligent tutor construction. In AAAI-96:
Proceedings of the 7th National Conference on Artificial Intelligence pp. (552–557). Menlo Park, CA: AAAI Press.
Croy, M. J, (2000). Problem solving, working backwards, and graphic proof representation, Teaching Philosophy 23, 169–187.
Croy, M., Barnes, T., & Stamper, J. (2007). Towards an intelligent tutoring system for propositional proof construction. In P.
Brey, A. Briggle, & K. Waelbers (Eds.), Proceedings of the 2007 European Computing And Philosophy Conference, Amsterdam,
Netherlands: IOS Publishers.
Joint Task Force for Computing Curricula, ACM, AIS, & IEEE. (2005). Computing curricula 2005. Retrieved March 26, 2009,
from http://www.acm.org/education/education/curric_vols/CC2005-March06Final.pdf
Koedinger, K., Aleven, V., Heffernan, N., McLaren, B., & Hockenberry, M. (2004, August). Opening the door to nonprogrammers: Authoring intelligent tutor behavior by demonstration. Proceedings of the 7th Intl. Conf. ITS-2004: Intelligent
Tutoring Systems. Maceió, Brazil: Springer.
McKendree, J. (1990, December). Effective feedback content for tutoring complex skills. Human-Computer Interaction, 5(4), pp.
381–413.
McLaren, B., Koedinger, K., Schneider, M., Harrer, A., & Bollen, L. (2004, August). Bootstrapping Novice Data: Semiautomated tutor authoring using student log files, In Proc. Workshop on Analyzing Student-Tutor Interaction Logs to Improve
Educational Outcomes, Proceedings of the 7th Intl. Conf. ITS-2004: Intelligent Tutoring Systems. Maceió, Brazil: Springer.
Merceron, A., & Yacef, K. (2005). Educational data mining: A case study. In 12th Intl. Conf. Artificial Intelligence in Education.
Amsterdam, Netherlands: IOS Press.
Mitrovic, A., Koedinger, K. & Martin, B. (2003). A comparative analysis of cognitive tutoring and constraint-based modeling.
User Modeling 2003: Proceedings of the 9th International Conference, UM 2003, Johnstown, PA, USA, June 22–26, 2003,
Lecture Notes in Computer Science (pp. 313–322). Berlin: Springer.
Murray, T. (1999). Authoring intelligent tutoring systems: An analysis of the state of the art. Intl. J. Artificial Intelligence in
Education, 10: 98–129.
Sieg, W. (2007). The AProS Project: Strategic thinking & computational logic. Logic Journal of IGPL. Retrieved March 26, 2009,
from http://jigpal.oxfordjournals.org/cgi/content/abstract/jzm026v1
Stamper. J. (2006, September). Automating the generation of production rules for intelligent tutoring systems. Proc. Intl. Conf.
Computer-Aided Learning (ICL2006), Villach, Austria: Kassel University Press.
Sutton, R., and Barto, A. G. (1998). Reinforcement learning: An introduction. Cambridge, MA: MIT Press.
Van Lehn, K. (2006). The behavior of tutoring systems, International Journal of Artificial Intelligence in Education 16, 227–265.

12

Fournier-Viger, P., Faghihi, U., Nkambou, R., & Mephu Nguifo, E. (2010). Exploiting Sequential Patterns Found in Users'
Solutions and Virtual Tutor Behavior to Improve Assistance in ITS. Educational Technology & Society, 13 (1), 13–24.

Exploiting Sequential Patterns Found in Users’ Solutions and Virtual Tutor
Behavior to Improve Assistance in ITS
Philippe Fournier-Viger, Usef Faghihi, Roger Nkambou and Engelbert Mephu Nguifo*
Department of Computer Sciences, University of Quebec in Montreal, Montreal, Canada
fournier_viger.philippe@courrier.uqam.ca // faghihi.usef@courrier.uqam.ca // nkambou.roger@uqam.ca
*
Department of Computer Sciences, Université Blaise-Pascal, Clermont-Ferrand, France mephu@isima.fr
ABSTRACT
We propose to mine temporal patterns in Intelligent Tutoring Systems (ITSs) to uncover useful knowledge that
can enhance their ability to provide assistance. To discover patterns, we suggest using a custom, sequential
pattern-mining algorithm. Two ways of applying the algorithm to enhance an ITS’s capabilities are addressed.
The first is to extract patterns from user solutions to problem-solving exercises for automatically learning a task
model that can then be used to provide assistance. The second way is to extract temporal patterns from a tutoring
agent’s own behavior when interacting with learner(s). In this case, the tutoring agent reuses patterns that
brought states of “self-satisfaction.” Real applications are presented to illustrate the two proposals.

Keywords
Temporal patterns, Sequential pattern mining, Educational data mining, Intelligent tutoring systems

Introduction
Using knowledge discovery techniques to uncover useful knowledge hidden in a massive amount of educational data
has been the subject of much recent research (Baker, Barnes, & Beck, 2008). However, no research has considered
mining temporal patterns in Intelligent Tutoring Systems (ITSs) and employed this knowledge to improve their
ability to provide assistance. In this paper, we propose two ways of improving the behavior of ITSs by exploiting
temporal patterns. Those are to (1) automatically learn task models from recorded novice and expert users’ solutions
to provide assistance to learners, and (2) building tutoring agents that can adapt their behavior to learners and
situations by reusing previously successful patterns of tutoring actions. Our hypothesis is that temporal patterns
found in ITSs constitute useful knowledge that can be exploited to improve their ability to provide relevant and
adaptive assistance.
The paper is organized as follows. First, it introduces an algorithm for mining temporal patterns. Next, the paper
describes two proposals based on this algorithm and describes how they are integrated in an ITS. Finally, the last
section presents our conclusions and previews our future work.

Mining temporal patterns from sequences of events
According to Han & Kamber (2006), there are four main kinds of patterns that can be mined from temporal data.
These are trends, similar sequences, sequential patterns, and periodical patterns. In this work we chose to mine
sequential pattern (Agrawal & Srikant, 1995), as we are interested in finding relationships between occurrences of
events that are logged in ITSs. To mine sequential patterns, several algorithms have been proposed (Han & Kamber,
2006). While classical sequential pattern-mining algorithms have for their only goal to discover sequential patterns
that occur frequently in several transactions of a sequence database (Agrawal & Srikant, 1995), other algorithms
have proposed numerous extensions to the problem of mining sequential patterns (Han & Kamber, 2006). For this
work, we chose a sequential pattern-mining algorithm that we have developed (Fournier-Viger, Nkambou, & Mephu
Nguifo, 2008a), as it provides several more features than classical sequential pattern algorithms, such as accepting
symbols with numeric values, eliminating redundancy, and handling time constraints and contextual information. For
a technical description of the algorithm, the reader can refer to Fournier-Viger et al. (2008a). Moreover, a Java
implementation of the algorithm can be downloaded from http://www.philippe-fournier-viger.com/spmf/.
The algorithm takes as input a database D of sequences of events. An event X = (i1, i2,… in) contains a set of items
i1, i2,… in, that are considered simultaneous, and where each item can be annotated with an integer value. Formally, a
ISSN 1436-4522 (online) and 1176-3647 (print). © International Forum of Educational Technology & Society (IFETS). The authors and the forum jointly retain the
copyright of the articles. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that copies bear the full citation on the first page. Copyrights for components of this work owned by
others than IFETS must be honoured. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior
specific permission and/or a fee. Request permissions from the editors at kinshuk@ieee.org.

13

sequence is denoted s = <(t1, X1), (t2, X2), … (tn, Xn)> where each event Xk is associated to a timestamp tk indicating
the time of the event. For example, the sequence S1 of figure 1 (left) contains two events. It indicates that item a
appeared with a value of 2 at time 0 and was followed by items b and c with a value of 0 and 4, respectively, at time
1. An event sequence sa = <(ta1, A1), (ta2, A2), … (tan, An)> is said to be contained in another event sequence sb =
<(tb1, B1), (tb2, B2), … (tbm, Bm)>, if there exist integers 1 ≤ k1 < k2 < … < kn ≤ m such that A1 Bk1, A2 Bk2, …, An
Bkn, and that tbkj – tbk1 is equal to taj – ta1 for each j {1…m}. The relative support of a sequence sa in a database
D is defined as the percentage of sequences s D that contain sa and is denoted by supD(sa). The problem of
mining frequent sequences is to find all the sequences sa such that supD(sa) ≥ minsup for a sequence database D,
given a support threshold minsup, and optional time constraints. The optional time constraints are the minimum and
maximum time intervals required between the head and tail of a sequence and the minimum and maximum time
intervals required between two adjacent events of a sequence.
As an example, figure1 illustrates a database of six sequences (left) and the corresponding patterns found for a
minsup of 33% (right). Consider pattern M5. This pattern appears in sequences S4 and S5, respectively. It has a
support of 33% (two out of six sequences) and is frequent. Now consider patterns M1 and M2. Because item a
appears in sequences S1, S2, S3, and S4, with values 2, 2, 5, and 6, respectively, the algorithm separated these values
into two groups to create patterns M1 and M2 instead of creating a single pattern with a support of 66 %. For each of
these groups, the median (2 and 5) was kept as an indication of the values grouped. This clustering of similar values
only occurs when the support is higher or equal to 2 * minsup (see Fournier-Viger et al., 2008a, for details).

ID
S1
S2
S3
S4
S5
S6

Sequences
<(0,a{2}), (1,bc{4})>
<(0,a{2}), (1,c{5})>
<(0,a{5}), (1,c{6})>
< (0,f), (1, g),(2,a{6}e)>
<(0, f b{3}), (1,h),(2,ef) >
<(0,b{2}), (1,d)>

→

ID
M1
M2
M3
M4
M5
M6

Mined sequences
<(0,a{2})>
<(0,a{5})>
<(0,a{2}), (1, c{5})>
<(0,c{5})>
<(0,f), (2, e)>
…

Support
33 %
33 %
33 %
50 %
33 %
…

Figure 1. A database of six sequences (left) and mined sequences (right)

First proposal: Automatically learning task models from users’ solutions
Our first proposal is for the acquisition of domain knowledge in an ITS. Typically, domain experts have to provide
relevant knowledge to an ITS so that it can guide a learner during problem-solving activities. One common way of
acquiring such knowledge is to use the method of cognitive task analysis that aims to produce effective problem
spaces or task models by observing expert and novice users for capturing different ways of solving problems.
However, cognitive task analysis is a very time-consuming process (Aleven, McLaren, Sewall, & Koedinger, 2006),
and it is not always possible to define a satisfying complete or partial task model, particularly when a problem is illstructured. According to Simon (1978), an ill-structured problem is one that is complex, with indefinite starting
points, multiple and arguable solutions, or unclear strategies for finding solutions. A domain that includes such
problems and in which tutoring targets the development of problem-solving skills is said to be ill-defined (within the
meaning of Lynch, Ashley, Aleven, & Pinkwart, 2006). An alternative to cognitive task analysis is constraint-based
modeling (CBM) (Mitrovic, Mayo, Suraweera, & Martin, 2001), which consists of specifying sets of constraints on
what is a correct behavior, instead of providing a complete task description. Though this approach was shown to be
effective for some ill-defined domains, a domain expert has to design and select the constraints carefully, and it
cannot support tutoring services such as suggesting next steps to be performed by a learner. Contrarily to these
approaches, where domain experts have to provide the domain knowledge, a promising approach is to use knowledge
discovery techniques to automatically learn a partial problem space from logged user interactions in an ITS, and to
use this knowledge base to offer tutoring services. A few works have been done in this direction in the field of ITS
(for example, Riccuci, Carbonaro, & Casadei, 2007; Matsuda, Cohen, Sewall, Lacerda, & Koedinger, 2007; Barnes
& Stamper, 2008), but they either (1) require specifying a minimal set of background knowledge, (2) have been
applied in well-defined domains, (3) rely on strong assumption such that tasks can be modeled as production rules, or
(4) do not take into account learner profiles.

14

We propose a solution that is not constrained by those limitations. We illustrate this proposal in the context of
CanadarmTutor (Kabanza, Nkambou, & Belghith, 2005) (Figure 2) a virtual learning environment for learning how
to operate the Canadarm2 robotic arm on the international space station. The main learning activity in
CanadarmTutor is to move the arm from one configuration to another. This is a complex task, because the arm has
seven joints and the user must chose at any time the three best cameras for viewing the environment from around
twelve cameras on the space station and adjust their parameters. We have integrated a tutoring agent in
CanadarmTutor to provide assistance to learners during this task. However, there are a very large number of
possibilities for moving the arm from one position to another, and because one must also consider the safety of the
maneuvers, it is very difficult to define a task model for generating the moves that a human would execute (FournierViger, Nkambou, & Mayers, 2008b). For this reason, instead of providing domain knowledge to the agent, we
implemented a learning mechanism that allows the tutoring system to learn by recording the behavior of users
performing a task. The tutoring system then uses this knowledge to provide assistance to learners. We describe next
the three operation phases of the learning mechanism as they are implemented in CanadarmTutor, and an
experiment.

Figure 2. The CanadarmTutor interface
The observing phase
In the first phase, the tutoring system records the solutions of users that attempt an exercise. In CanadarmTutor, an
exercise is to move the arm from an initial configuration to a goal configuration. For each attempt, the tutoring
system logs a sequence of events. In this context, an event is a set of actions (items) that are considered unordered
temporally. We defined 112 primitive actions that can be recorded in CanadarmTutor, which are (1) selecting a
camera, (2) performing an increase or decrease of the pan/tilt/zoom of a camera, and (3) applying a rotation value to
an arm joint. Actions of types (2) and (3) are annotated with integer values that indicate, respectively, the number of
increments/decrements applied to a camera parameter and the number of degrees that a joint is rotated. An example
of a partial action sequence recorded for an user in CanadarmTutor is <(0,6{2}),(1,63),(2,53{4}),(3,111{2})> ,
which represents decreasing the rotation value of joint SP (action 6) by two units, selecting camera CP3 (action 63),
and increasing the pan of camera CP2 (action 53) by four units and then its zoom (action 111) by two units.
To annotate sequences with contextual information, we have extended the notion of sequence database with
dimensional information, as suggested by Pinto, H., Han, J., Pei, J., Wang, K., Chen, Q., & Dayal, U. (2001). A
sequence database having a set of dimensions D = D1, D2, … Dn is called an MD-Database. Each sequence of an
MD-Database (an MD-Sequence) possesses a symbolic value for each dimension or the value “*”, which means any
value. A set of dimension values is called an MD-Pattern and is denoted d1, d2, … dn. An MD-Pattern Px = {dx1,
15

dx2, … dxn} is said to be contained in another MD-Pattern Py = {dy1, dy2, … dym} if dx1 dy1, … dxn dym. The
relative support of a sequence (or MD-Pattern) in a sequence database D is defined as the percentage of sequences
(or MD-Pattern) that contains it. Table 1 shows an example of MD-Database containing six learner plans annotated
with five dimensions. The first dimension “Solution state” indicates if the learner plan is a succesful or buggy
solution. In the case of CanadarmTutor, values for this dimension are produced by the tutoring system. The four
other dimensions of Table 2 are examples of dimensions that can be added manually. Here, whereas the dimension
“Expertise” denotes the expertise level of the learner that performed a sequence, “Skill_1”, “Skill_2”, and “Skill_3”
indicate, respectively, if three specific skills were shown by the learner who performed the sequence. This example
includes only five dimensions of three main types (skills, expertise level, and solution state). However, our
framework can accept any kind of learner information or contextual information encoded as dimensions. In fact, in
CanadarmTutor, we used 10 skills and the “solution state” dimension to annotate sequences.

ID
S1
S2
S3
S4
S5
S6

Table 1. An example database containing 6 user solutions
Dimensions
Sequence
Solution state
Expertise
Skill_1 Skill_2 Skill_3
successful
expert
yes
yes
yes
<(0, a),(1, bc)>
successful
novice
no
yes
no
<(0, d)>
buggy
expert
yes
yes
yes
<(0, a),(1, bc)>
buggy
intermediate
no
yes
yes
<(0, a), (1, c), (2, d)>
successful
expert
no
no
yes
<(0, d), (1, c)>
successful
novice
no
no
yes
<(0, c), (1, d)>

The learning phase
In the learning phase, the virtual agent applies the algorithm to find all MD-Sequence with a support higher or equal
to minsup. For mining patterns, we set up the algorithm to mine only sequences of size two or greater, as shorter
sequences would not be useful in a tutoring context. Furthermore, we chose to mine sequences with a maximum time
interval between two adjacent events of two actions. The benefits of accepting a gap of two is that it eliminates some
“noisy” (non-frequent) learners’ actions, but at the same time it does not allow a larger gap size that could make it
less useful for tracking a learner’s actions. As an example, Table 2 shows some patterns that can be extracted from the
MD-Database of Table 1, with a minsup of two sequences (33%). Consider pattern P3. This pattern represents doing
action b one time unit (immediately) after action a. The pattern P3 appears in MD-sequences S1 and S3. It has thus a
support of 33% or two MD-sequences. Because this support is higher or equal to minsup, P3 is deemed frequent.
Moreover, the dimension values for P3 tell us that this pattern was performed by expert users that possess skills
“Skill_1”, “Skill_2”, and “Skill_3”, and that P3 was found in plan(s) that failed, as well as in plan(s) that succeeded.
Another important consideration is that when applying sequential pattern mining, there can be many redundant
frequent sequences found. For example, in Table 2, the pattern P1 is redundant because it is included in the pattern
P3 and has the same support. To eliminate this type of redundancy, we have adapted our algorithm based on Wang,
Han, & Li (2007) and Songram, Boonjing, & Intakosum (2006) to mine closed MD-sequences. Closed MDsequences are MD-sequences that are not contained in another sequence having the same support. Mining frequent
closed MD-sequences has the advantage of greatly reducing the size of patterns found without information loss
(Wang et al., 2007). Once patterns have been mined by our sequential pattern-mining algorithm, they form a partial
problem space that can be used directly to provide tutoring services. However, one can also edit the patterns or
annotate them with tutoring resources, such as textual hints.

ID
P1
P2
P3
P4
P5
P6

Table 2. Some frequent patterns extracted from the dataset of Table 2 for minsup = 33%
Dimensions
Sequence
Support
Solution state
Expertise Skill_1 Skill_2 Skill_3
*
expert
yes
yes
yes
<(0, a)>
33%
*
*
*
yes
yes
<(0, a)>
50%
*
expert
yes
yes
yes
<(0, a), (1, b)>
33%
successful
*
no
*
*
<(0, d)>
50%
successful
expert
*
*
yes
<(0, c)>
33%
successful
novice
no
*
no
<(0, d)>
33%
16

The application phase
In the third phase, the tutoring system provides assistance to the learner by using the knowledge learned in the
second phase. The basic operation that is used for providing assistance is to recognize a learner’s plan. In
CanadarmTutor, this is achieved by the plan recognition algorithm RecognizePlan, which is executed after each
student action. When RecognizePlan is called for the first time, it iterates on the whole set of patterns found during
the learning phase to note all the patterns that include the sequence of actions performed by the learner. If no pattern
is found, the algorithm ignores the last action performed by the learner and searches again. This is repeated until the
set of matching patterns is not empty or the size of the sequence of student actions is smaller than 2. In our test,
removing user actions has shown to improve the effectiveness of RecognizePlan significantly. The next time it is
called, it will be called with the set of matching patterns found by its last execution. This ensures that the algorithm
will not consider patterns that have been previously rejected.
After performing preliminary tests with RecognizePlan, we noticed that, in general, after more than six actions
performed by a learner, it becomes hard to tell which pattern the learner is doing. For that reason, we improved how
the CanadarmTutor applies the sequential pattern-mining algorithm to extract a knowledge base. Originally, it mined
frequent patterns for a whole problem-solving exercise. We modified our approach to add the notion of “problem
states.” In the context of CanadarmTutor, where an exercise consists of moving a robotic arm to attain a specific arm
configuration, the 3D space is divided into 3D cubes, and the problem state at a given moment is defined as the set of
3D cubes containing the arm joints. An exercise is then viewed as going from a problem state P1 to a problem state
Pf. For each attempt at solving the exercise, CanadarmTutor logs (1) the sequence of problem states visited by the
learner A= P1, P2, … Pn and (2) the list of actions performed by the learner to go from each problem state to the next
visited problem state (P1 to P2, P2 to P3, … P(n-1) to Pn). After many users perform the same exercise, CanadarmTutor
extracts sequential patterns from sequences of problem states visited and from sequences of actions performed for
going from a problem state to another. To take advantage of the added notion of problem states, we modified
RecognizePlan so that every time the problem state changes, RecognizePlan will be called with the set of patterns
associated to the new problem state. Moreover, at a coarse grain level, a tracking of the problem states visited by the
learners is also achieved by RecognizePlan. This allows connecting patterns for different problem states. We
describe next the main tutoring services that a tutoring agent can provide based on the plan-recognition algorithm.
First, a tutoring agent can assess the profile of the learner by looking at the patterns applied. If, for example, a learner
applies 80% of the time patterns with value “intermediate: for dimension “expertise,” then CanadarmTutor can assert
with confidence that the learner expertise level is “intermediate.” In the same way, CanadarmTutor can diagnose
mastered and missing/misunderstood skills for users who demonstrated a pattern by looking at the “skills”
dimensions of patterns applied, and can estimate other aspects of a learner’s profile. This results in rich information
that can be used in various ways by a tutoring system. An example is given by the next tutoring service.
Second, a tutoring agent can guide the learner. This tutoring service consists of determining the possible actions from
the current problem state and proposing one or more actions to the learner. In CanadarmTutor, this functionality is
triggered when the student selects “What should I do next?” in the interface menu. CanadarmTutor then identifies the
set of possible next actions according to the matching patterns found by RecognizePlan. The tutoring service then
selects the action among this set that is associated with the pattern that has the highest relative support and that is the
most appropriate for the estimated expertise level and skills of the learner. If the selected pattern contains skills that
are not considered mastered by the learner, CanadarmTutor can use tutoring resources to explain them. If no actions
can be identified, CanadarmTutor can rely on a special path planner to generate approximate solutions (see Kabanza
et al., 2005 for details). In this current version, CanadarmTutor interacts with the learner only upon request. But it
would be possible to program CanadarmTutor so that it can intervene if the learner is following an unsuccessful
pattern or a pattern that is not appropriate for its expertise level. Testing different tutorial strategies with learners is
part of our current work.
Finally, a tutoring service that has been implemented in CanadarmTutor is to let learners explore patterns to learn
about possible ways of solving problems. Currently, the learners can explore patterns with a very simple interface.
However, the learner could be assisted in this exploration by using an interactive dialog with the system that could
prompt them on their goals and help them go through the patterns to achieve these goals.

17

Experiment
We conducted a preliminary experiment in CanadarmTutor with two exercises to qualitatively evaluate the virtual
agent’s capability to provide assistance. The two exercises each consist of moving a load with the Canadarm2 robotic
arm to one of the two cubes (figure 3a). We asked 12 users to record plans for these exercises. The average length of
a plan was 20 actions. From this data, CanadarmTutor extracted a partial problem space. In a subsequent work
session, we asked users to evaluate the tutoring services provided by this version of CanadarmTutor. All users agreed
that the assistance provided was helpful. We also observed that CanadarmTutor correctly inferred the expertise level
of all the learners and thus provided hints that were adapted to the user profile. As an example of interaction with a
learner, Figure 3b shows a hint message given to a learner upon request during scenario 1. The guiding tutoring
service selects the pattern that has the highest support value, matches the last student actions, is marked “successful,”
and corresponds with the estimated expertise level of the learner. The given hint is to select camera CP4 on
Monitor3, decrease the rotation value of the joint WP, and increase the rotation value of joint WE. The values on the
right column indicate the values associated to the action. In this context, values 1 and 3 mean to rotate the joints 10º
and 30º, respectively (1 unit equals 10º). By default, three steps are showed to the learners in the hint window
depicted in figure 3b. However, the learner can click on the “more” button to ask for more steps or click on the
“another possibility” button to ask for an alternative.
It should be noted that, although we applied the sequential pattern algorithm only one time after recording the
learners plan, it would be possible to make CanadarmTutor apply it periodically to update its knowledge base, while
interacting with learners.

Figure 3a. The two scenarios; 3b. A hint generated by the virtual agent

Second proposal: A tutoring agent that learns from its own behavior
Our second proposal is to build tutoring agents that can learn from their own behavior by reusing previously
successful patterns of tutoring actions. We illustrate this proposal with a virtual agent named CTS (Dubois, Poirier,
& Nkambou, 2007) that we have also tested in CanadarmTutor to provide assistance to learners. The following
subsections describe CTS, the three operation phases of the new learning mechanism that was integrated in CTS, and
two experiments carried in CanadarmTutor to validate (1) the behavior of the new CTS and (2) the behavior of our
sequential pattern-mining algorithm with large data sets.

The CTS cognitive agent
The Conscious Tutoring System (CTS) is a generic cognitive agent whose architecture (fig. 4) is inspired by
neurobiology and neuropsychology theories of human brain function. It relies on the functional “consciousness”
(Franklin & Patterson, 2006) mechanism for much of its operations. It also bears some functional similarities to the
physiology of the nervous system. Its modules communicate with one another by contributing information to its
working memory (WM) through information codelets. Based on Hofstadter et al’s idea (Hofstadter & Mitchell,
1992), a codelet is a very simple agent, “a small piece of code that is specialized for some comparatively simple
task.” As in Baars’s theory (Baars, 1997), these simple processors do much of the processing in the CTS architecture.
18

Figure 4. A simplified view of the CTS architecture (see Faghihi et al., 2008 for more details)

CTS possesses two routes for processing external stimuli (cf. fig. 4). Whereas the “long route” is the default route,
the “short route” (which will not be described here) allows quick reactions when received information is deemed
important by the pseudo-amygdala, the module responsible for emotional reactions in CTS (Faghihi, Poirier, Dubois,
Gaha, & Nkambou., 2008). In both cases, the stimuli processing begins with percept codelets (Hofstadter & Mitchell,
1992) that perform collective interpretations of stimuli. The active nodes of the CTS’s perception network constitute
percepts. In the long route, these percepts enter WM as a single network of codelets, annotated with an activation
value. These codelets create or reinforce associations with other already present codelets and create a coalition of
information codelets. In parallel, the emotional codelets situated in the CTS’s pseudo-amygdala inspect the
previously mentioned coalition’s informational content, and if is deemed important, infuse it with a level of
activation proportional to its emotional valence. During every cognitive cycle, the coalition in the WM that has the
highest activation is selected from the WM by the “attention mechanism” and is broadcast to all the modules in the
CTS architecture. This selection process ensures that only the most important, urgent, or relevant information is
broadcast in the architecture. Following a broadcast, every subsystem (module or team of codelets) that recognizes
the information may react to the coalition by appending additional information to it. This process of internal
publications (as suggested by Baars, 1997) can continue for many cognitive cycles before an action is executed by
CTS. The module responsible for action planning, selection, and execution is the behavior network (BN) (Maes,
1989). When the BN receives a broadcast coalition, it selects the appropriate action to execute. In the current CTS
version, we have designed the BN using a graphical authoring tool. We have implemented in CTS, the second
proposal that we consider in this article. This learning mechanism is implemented in CTS by the three operation
phases, described next.

The observation phase
In the first phase, the observation phase, CTS records a sequence of events (as defined in the second section of this
article) for each of its executions. Each event X = (ti, Ai) represents one cognitive cycle. While the timestamp ti of an
event indicates the cognitive cycle number, the set of items Ai of an event contains (1) an item that represents the
coalition of information-codelets that was broadcast during the cognitive cycle and (2) four optional items with
numeric values indicating the four emotional valences (high threat, medium fear, low threat, compassion) associated
with the broadcast coalition. CTS actually incorporates four emotions inspired by the OCC model of emotions
(Ortony, Clore, & Collins, 1988). See Faghihi et al. (2008) for in-depth details about the emotional mechanism of
CTS. An example of partial sequence recorded during our experiment was <(1, c2), (2, c4), (3, c8 e2{-0.4})>. This
sequence shows that during cognitive cycle 1 the coalition c2 was broadcast, followed by the broadcast of c4 during
cognitive cycle 2. Furthermore, it indicates that coalition c8 was broadcast during the third cognitive cycle and that it
generated a negative emotional valence of −0.4 for emotion e2 (medium fear).

19

The learning phase
The second operation phase consists of mining frequent patterns from the sequences of events recorded for all
executions of CTS by applying our sequential pattern-mining algorithm. This process is executed at the end of each
CTS execution, from the moment where five sequences are available (five CTS executions). Currently, we have set
up the sequential pattern-mining algorithm to mine only closed sequences with more than three events and with a
support higher than 25%. Applying the algorithm results in a set of frequent sequential patterns.

The application phase
The third operation phase consists of improving CTS behavior by making CTS reuse relevant patterns that carry
positive emotions. This is done by intervening in the coalition selection phase of CTS. The idea is here to find,
during each cognitive cycle, the patterns that are similar to CTS’s current execution, then to select as the next
coalition to be broadcast the one most probable of generating positive emotions for CTS according to these patterns.
Influencing the coalitions that are broadcast will then directly influence the actions that will be taken by the CTS
behavior network. This adaption of CTS could be implemented in different ways. We used the SelectCoalition
algorithm (figure 4), which takes as parameters (1) the sequence of previous CTS broadcasts (Broadcasts), (2) the set
of frequent patterns (Patterns), and (3) the set of coalitions that are candidates to be broadcast during a given
cognitive cycle (CandidateCoalitions). This algorithm first sets to zero a variable min and a variable max for each
coalition in CandidateCoalitions. Then, the algorithm repeats the following steps for each pattern p of Patterns. First,
it computes the strength of p by multiplying the sum of the emotional valences associated with the broadcasts in p
with the support of p. Then, it finds all the coalition c CandidateCoalitions that appear in p after the sequence of
the last k broadcasts of Broadcasts for any k ≥ 2. For each such coalition c, if the strength of p is higher than c.max,
c.max is set to that new value. If that strength is lower than c.min, c.min is set to that new value. Finally, when the
algorithm finishes iterating over the set of patterns, the algorithm returns to CTS’s working memory the coalition c in
CandidateCoalitions having the highest positive value for the sum c.min + c.max and where c.max > 0. This coalition
will be the one that will be broadcast next by CTS’s attention mechanism. In the case of no coalition meeting these
criteria, the algorithm will return a randomly selected coalition from CandidateCoalitions to CTS’s working memory.

Figure 5. The SelectCoalition algorithm

The c.max > 0 criterion is included to ensure that the selected coalition appears in at least one pattern having a
positive sum of emotional valences. Moreover, we have added the c.min + c.max criterion to make sure that the
patterns with a negative sum of emotional valences will decrease the probability of selecting the coalitions that it
contains. In our experiments, this criterion has proved to be very important as it can cause CTS to quickly stop
selecting a coalition appearing in positive patterns if it becomes part of negative patterns. The reader should note that
algorithms relying on other criteria could be used for other applications.

20

Testing the new CTS in CanadarmTutor
To test CTS’s new learning mechanism, users were invited to perform arm manipulations using CanadarmTutor with
integrated CTS. These experiments aimed at validating CTS’s ability to adapt its behavior to learners. During these
experiments, we qualitatively observed that CTS adapted its behavior successfully to learners. Two experiments are
described here. The first describes in detail a situation that occurred with User 3 that illustrates well how CTS adapts
its behavior thanks to the new learning mechanism. The second experiment describes how our sequential patternmining algorithm behaves when the number of recorded sequences increases.
User 3 tended to make frequent mistakes when he was asked to guess the arm’s distance from a specific part of the
space station. Obviously, this situation caused collision risks between the arm and the space station and was thus a
very dangerous situation. This situation was implemented in the CTS’s Behavior Network. In this situation, CTS has
to make a decision between (1) giving a direct solution such as “You should move joint SP” (Scenario 1) or giving a
brief hint such as “This movement is dangerous. Do you know why?” (Scenario 2).
During the interaction with different users, the learning mechanism recorded several sequences of events for that
situation, each of them carrying emotional valences. The average length of the stored sequences was 26 events. For
example, one partial trace saved when CTS gave a hint (scenario 2) to User 2 was <(13, c11), (14, c14), (15, c15),
(16, c18), (17, c19 e4{0.8})>. In this trace, the positive valence 0.8 for emotion e4 (compassion) was recorded
because the learner answered an evaluation question correctly after receiving the hint. In another partial trace saved
by CTS <(16, c11), (17, c14), (18, c16), (19, c17), (20, c20 e2{−0.4})>, User 2 received a direct solution from CTS
(Scenario 1), but failed to answer correctly an evaluation question. This resulted in the valence −0.4 being associated
to emotion e2 (medium fear). After five executions, the learning mechanism extracted ten frequent sequences from
the recorded sequences, with a minimum support (minsup) higher than 0.25.
Now turning back to User 3, during the coalition selection phase of CTS, the learning mechanism evaluated all
mined patterns to detect similar patterns having ended by self-satisfaction. The learning mechanism chose the pattern
<(0, c11), (1, c14), (3, c18), (4, c19 e4{0.8})> because it contained the most positive emotional valence, had the
highest frequency, and the events (0, c11), (1, c14) matched with the latest events executed by CTS. Therefore, CTS
decided that it was better to give a hint (Scenario 2) than to give the answer (Scenario 1) to User 3. This was
achieved by broadcasting coalition c18 (Scenario 2) instead of coalition c16 (Scenario 1). If the emotional valence
had not been as positive as was the case for previous users, CTS might have chosen Scenario 1 rather than Scenario
2. It should be noted that because the set of patterns is regenerated after each CTS execution, some new patterns can
be created, while others can disappear, depending on the new sequences of events that are stored by CTS. This
ensures that CTS behavior can change over time if some scenarios become less positive or more negative, and more
generally that CTS can adapt its behavior to a dynamic environment. In this experiment, the learning mechanism has
shown to be beneficial by allowing CTS to adapt its actions to learners by choosing between different scenarios
based on its previous experience. This feature is very useful, as it allows the designers to include many alternative
behaviors but to let CTS learn by itself which ones are the most successful.
We performed a second experiment with the learning mechanism, but this time to observe how our sequential
pattern-mining algorithm behaves when the number of recorded sequences increases. The experiment was done on a
3.6 GHz Pentium 4 computer running Windows XP, and consisted of performing 160 CTS executions for a situation
similar to the previous one where CTS had to choose between scenario 1 and scenario 2. In this situation, CTS
conducts a dialogue with the student that includes from two to nine messages or questions (an average of six)
depending on what the learner answers and the choices CTS makes (similar to choosing between scenarios 1 and 2).
During each trial, we randomly answered the questions asked by CTS, and took various measures during CTS’s
learning phase. Each recorded sequence contained approximately 26 broadcasts.
Figure 6 presents the results of the experiment. The first graph shows the time required for mining frequent patterns
after each CTS execution. From this graph, we see that the time for mining frequent patterns was generally short
(less than 6 seconds) and increased linearly with the number of recorded sequences. In our context, this performance
is very satisfying. However, the performance of the data-algorithm could still be improved as we have not yet fully
optimized all of its processes and data structures. In particular, in future works we will consider modifying the
algorithm to perform incremental mining of sequential patterns as some other sequential pattern-mining algorithms
21

do. This would improve performance, as it would not be necessary to recompute from scratch the set of patterns for
each new added sequence.
The second graph shows the average size of patterns found for each execution. The size ranges from 9 to 16
broadcasts. The third graph depicts the number of patterns found. It remains low and stabilized at around 8.5 patterns
during the last executions. The reason why the number of patterns is small is that we mined only closed patterns (see
definition in the third section). If we had not mined only closed patterns, all the sub-sequences of each pattern would
have been included in the results. Mining closed patterns is also much faster because, during the search for patterns,
large parts of the search space that are guaranteed not to lead to close patterns are pruned. For example, for mining
non-closed patterns from the first four sequences only, it took more than one hour (we stopped the algorithm after
one hour), while mining closed patterns took only 0.558 seconds. The reason for this is that the four sequences share
more than 15 common broadcasts. Therefore, if the pruning of the search space is not done, the algorithm has to
consider all combinations of these broadcasts, which is computationally very expensive. This demonstrates that it is
beneficial to mine closed patterns. Finally, the average time for executing the SelectCoalition algorithm at each
execution was always less than 5 milliseconds. Thus, the costliest operation of the learning mechanism is the
learning phase.

Figure 6. Results from the second experiment

Conclusion
In this article, we presented the idea of exploiting temporal data found in intelligent tutoring system logs to improve
their capability to provide relevant and adaptive assistance. To demonstrate this idea, we described two proposals.
While the first one is designed to learn task models from recorded novice and expert solutions to provide assistance
to learners in problem-solving activities, the second one is aimed at building tutoring agents that can adapt their
behavior to learners and situations by reusing previously successful patterns of tutoring actions. The two proposals
should be reusable in other tutoring agents and domains, as the format for encoding behaviors is fairly generic.
In future work, we will perform further experiments to measure empirically how the different versions of
CanadarmTutor influence the learning of students. We will investigate different ways of improving the performance
of our sequential pattern-mining algorithm, including modifying it to perform an incremental mining of sequential
patterns. We also plan to integrate the two proposals into other tutoring systems.

22

Acknowledgements
The authors thank the Canadian Space Agency, Fonds Québécois de la Recherche sur la Nature et les Technologies,
and the Natural Sciences and Engineering Research Council for their logistic and financial support. The authors also
thank members of GDAC/PLANIART teams who have participated in the development of CanadarmTutor.

References
Agrawal, R., & Srikant, R. (1995). Mining Sequential Patterns. Proceedings of the International Conference on Data Engineering
(pp. 3–14), Los Alamitos, CA: IEEE Computer Society Press.
Aleven, V., McLaren, B. M., Sewall, J., & Koedinger, K. (2006). The Cognitive Tutor Authoring Tools (CTAT): Preliminary
evaluation of efficiency gains. Proceedings the 8th International Conference on Intelligent Tutoring Systems (pp. 61–70), Berlin:
Springer.
Baars, B. J. (1997). In the theater of consciousness. New York: Oxford University Press.
Baker, R., Barnes, T., & Beck, J. E. (2008). Proceedings of Educational Data Mining 2008: 1st International Conference on
Educational Data Mining. Montreal, Quebec, Canada. June 20-21, 2008.
Barnes, T. & Stamper, J. (2008). Toward automatic hint generation for logic proof tutoring using historical student data,
Proceedings of the 9th International Conference on Intelligent Tutoring Systems (pp. 373–382), Berlin: Springer.
Dubois, D., Poirier, P., & Nkambou, R. (2007). What does consciousness bring to CTS? Proceedings of the 2007 AAAI Fall
Symposium (pp. 55–60), Menlo Park, CA: AAAI Press.
Faghihi, U., Poirier, P., Dubois, D., Gaha, M., & Nkambou, R. (2008). How emotional mechanism learn and helps other types of
learning in a cognitive agent, Proceedings of the 2009 IEEE/WIC/ACM Conference on Intelligent Agent Technology, Los
Alamitos, CA: IEEE Computer Society Press.
Fournier-Viger, P., Nkambou, R., & Mephu Nguifo, E. (2008a). A knowledge discovery framework for learning task models from
user interactions in intelligent tutoring systems. Proceedings of the 7th Mexican Conference on Artificial Intelligence (pp. 765–
778), Berlin: Springer.
Fournier-Viger P., Nkambou, R., & Mayers, A. (2008b). Evaluating spatial representations and skills in a simulator-based tutoring
system. IEEE Transactions on Learning Technologies, 1(1), 63–74.
Franklin, S., & Patterson, F.G.J. (2006). The LIDA architecture: Adding new modes of learning to an intelligent, autonomous,
software agent. Proceedings of 9th World Conference on Integrated Design & Process Technology. San Diego: Society for
Design and Process Science.
Han, J., & Kamber, M. (2006). Data mining: Concepts and techniques (2nd ed.). San Francisco: Morgan Kaufmann.
Hofstadter, D. R., & Mitchell, M. (1992). The copycat project: A model of mental fluidity and analogy-making. In K. Holyoak, J.
& Barnden, J. A. (Eds.) Advances in connectionist and neural computation theory 2 (pp. 31–113). Norwood, NJ: Ablex.
Kabanza, F., Nkambou, R., & Belghith, K. (2005). Path-planning for autonomous training on robot manipulators in space.
Proceedings of the 19th International Joint Conference on Artificial Intelligence (pp. 1729–173), Denver, CO: Professional Book
Center.
Lynch, C., Ashley, K., Aleven, V. & Pinkwart, N. (2006). Defining ill-defined domains: A literature survey. Proceedings of the
Intelligent Tutoring Systems for Ill-Defined Domains Workshop at the 8th International Conference on Intelligent Tutoring
Systems (pp. 1–10), Jhongli, Taiwan, June 27, 2006.
Maes, P. (1989). How to do the right thing. Connection Science, (1), 291–323.
Matsuda, N., Cohen, W. Sewall, J., Lacerda, G., & Koedinger, K. (2007). Predicting students’ performance with SimStudent:
Learning cognitive skills from observation. Proceedings of the 13th International Conference on Artificial Intelligence in
Education (pp. 467–478), Amsterdam: IOS Press.
Mitrovic, A., Mayo, M., Suraweera, P., & Martin, B. (2001). Constraint-based tutors: A success story. Proceedings of the 14th
Industrial & Engineering Application of Artificial Intelligence & Expert Systems (pp. 931–940), Berlin: Springer.
Ortony, A., Clore, G., & Collins, A. (1988). The cognitive structure of emotions. Cambridge University Press.

23

Pinto, H., Han, J., Pei, J., Wang, K., Chen, Q., Dayal, U. (2001). Multi-Dimensional Sequential Pattern Mining, Proceedings of
the International Conference of Information and Knowledge Management (pp. 81–88), New York: ACM.
Riccuci, S., Carbonaro, A., & Casadei, G. (2007). Knowledge acquisition in intelligent tutoring system: A data mining approach.
Proceedings of the 6th Mexican Conference on Artificial Intelligence (pp. 1195–1205), Berlin: Springer.
Simon, H. A. (1978). Information-processing theory of human problem solving. In W. K. Estes (Ed.), Handbook of learning and
cognitive processes: Vol. 5. Human information, Hillsdale, NJ: Lawrence Erlbaum Associates.
Songram, P., Boonjing, V., Intakosum, S. (2006). Closed multidimensional sequential pattern mining. Proceedings of the 3rd
International Conference on Information Technology: New Generations (pp. 512–517), Los Alamitos, CA: IEEE Computer
Society Press.
Wang, J., Han, J., Li, C. (2007), Frequent closed sequence mining without candidate maintenance, IEEE Transactions on
Knowledge and Data Engineering, 19(8), 1042–1056.

24

Chi, M., & VanLehn, K. (2010). Meta-Cognitive Strategy Instruction in Intelligent Tutoring Systems: How, When, and Why.
Educational Technology & Society, 13 (1), 25–39.

Meta-Cognitive Strategy Instruction in Intelligent Tutoring Systems: How,
When, and Why
Min Chi and Kurt VanLehn*
Learning Research and Development Center & Intelligent System Program, University of Pittsburgh, PA, USA //
mic31@cs.pitt.edu
*
Department of Computer Science & Engineering, Arizona State University, AZ, USA // Kurt.Vanlehn@asu.edu
ABSTRACT
Certain learners are less sensitive to learning environments and can always learn, while others are more sensitive
to variations in learning environments and may fail to learn (Cronbach & Snow, 1977). We refer to the former
as high learners and the latter as low learners. One important goal of any learning environment is to bring
students up to the same level of mastery. We showed that an intelligent tutoring system (ITS) teaching a
domain-independent problem-solving strategy indeed closed the gap between high and low learners, not only in
the domain where it was taught (probability) but also in a second domain where it was not taught (physics). The
strategy includes two main components: one is solving problems via backward chaining (BC) from goals to
givens, called the BC strategy, and the other is drawing students’ attention to the characteristics of each
individual domain principle, called the principle-emphasis skill. Evidence suggests that the low learners
transferred the principle-emphasis skill to physics while the high learners seemingly already had such skill and
thus mainly transferred the other skill, the BC strategy. Surprisingly, the low learners learned just as effectively
as the high learners in physics. We concluded that the effective element of transfer seemed not to be the BC
strategy, but the principle-emphasis skill.

Keywords
Intelligent tutoring systems, Meta-cognitive skills, Domain-independent problem-solving strategies

Introduction
Certain learners are less sensitive to learning environments and can always learn; while others are more sensitive to
variations in learning environments and may fail to learn (Cronbach & Snow, 1977). We refer to the former as high
learners and the latter as low learners. Bloom (1984) argued that human tutors not only raised the mean of test
scores, but also decrease the standard deviation of scores. That is, students generally start with a wide distribution in
test scores but as they are tutored, the distribution becomes narrower: the students at the low end of the distribution
begin to catch up with those at the high end. Another way to measure the same phenomenon is to split students into
high and low groups based on their incoming competence then measure the learning gains of both groups. According
to Bloom, a good tutor should exhibit an aptitude-treatment interaction: both groups should learn, and yet the
learning gains of the low students should be so much greater than those of the high ones that their performance in the
post-test ties with that of the high ones. That is, one benefit of tutoring is to narrow or even eliminate the gap
between high and low. In order to fully honor the promises of learning environments, an effective system should
narrow the gap as much as possible without pulling the high learners down. Many preexisting systems can decrease
such differences but not eliminate them. This is due in part to the fact that we do not fully understand why such
differences exist.
One of many hypotheses is that low learners lack certain specific skills about how to think, including general
problem-solving strategies and meta-cognitive skills. If this hypothesis is true, we expect that teaching students an
effective problem-solving strategy would not only improve students’ learning gains but also decrease the gap
between the low and the high learners. Furthermore, if such problem-solving strategy is domain independent, we
expect that learners would learn how to apply the strategy and seek to transfer it to new learning environments. Past
research has indicated that these skills can be transferred across domains (Lehman, Lempert, & Nisbett, 1988;
Lehman & Nisbett, 1990). However, few studies have investigated transfer of problem-solving strategy across
domains.
In this paper, we investigate these questions in a special class of learning environments, intelligent tutoring systems
(ITSs) (VanLehn, 2006). We present a study in which two groups of college students studied probability first and
then physics. The experimental group studied probability with Pyrenees, an ITS that explicitly taught and required
students to employ a general problem-solving strategy (VanLehn et al. 2004); while the control group studied
ISSN 1436-4522 (online) and 1176-3647 (print). © International Forum of Educational Technology & Society (IFETS). The authors and the forum jointly retain the
copyright of the articles. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that copies bear the full citation on the first page. Copyrights for components of this work owned by
others than IFETS must be honoured. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior
specific permission and/or a fee. Request permissions from the editors at kinshuk@ieee.org.

25

probability with Andes, an ITS that did not teach or require any particular strategy (VanLehn et al. 2005). During
subsequent physics instruction, both groups used Andes, which also did not teach or require students to employ any
particular strategy.
As reported earlier (Chi & VanLehn, 2007), we found that the experimental group out-performed the control group
not only in probability, where the strategy was taught and forced upon the participants, but also in physics where it
was not forced upon the participants. Furthermore, the strategy seemed to have lived up to our expectations and
transferred from probability to physics. In this paper, we determine whether explicit strategy instruction exhibits an
aptitude-treatment interaction, that is, whether it narrows or even eliminates the gap between high and low and,
moreover, whether both high and low indeed transfer the strategy to the second domain.

Background
A task domain is deductive if solving a problem requires producing an argument, proof, or derivation consisting of
one or more inference steps, and each step is the result of applying a domain principle, operator, or rule. For instance,
solving algebraic equations is a deductive domain, and in particular, 2x + 5 = 21 can be done via two steps: 1)
subtract the same term 5 from both sides of the equation; and 2) divide both sides by the non-zero term 2. Proving a
geometry theorem is deductive, as is solving quantitative physics problems. Deductive task domains are common
parts of mathematical and scientific courses such as probability and physics. Two common problem-solving
strategies in deductive domains are forward chaining (FC) and backward chaining (BC) (Russell & Norvig, 2003).
In FC, the solver starts with the set of given propositions, applies a principle to some subset of them (which produces
a new proposition), adds the new proposition to the known propositions, and repeats this process until the problem’s
goal is met or no new proposition can be produced. BC is goal-directed in that the goal is progressively broken down
into sub-goals and sub-sub-goals, etc. This constructs a partial plan in the form of a goal stack, which the solver uses
to guide its forward application of principles. This is easier to explain with the aid of an example, so we will combine
that explanation with an introduction to one of the task domains, probability. A portion of the probability task domain
is described in first row in Table 1.
Table 1. A subset of the probability principle and an example that can be solved by these rules
R1: For any event E, P (E) + P(~E) = 1
Rules:
R2: If events A and B are independent, P(A  B) = P(A)*P(B).
R3: If events A and B are independent, then A and ~B, ~A and B, and ~A and ~B are all independent
events.
Problem: Events A and B are independent, and P(~A) = 0.9, P(~B) = 0.8. Compute P(A  B).

Figure 1. A solution graph using forward chaining
26

An example of solving a problem using forward chaining is shown in Figure 1 and the problem is listed in the last
row of Table 1. One reads this graph starting at the bottom and working upward. The lowest propositions are given.
Rules are applied to produce the propositions above. Although forward chaining stops once the goal of the problem,
F9, is found, a few more propositions (F10, F11, and F12) are also shown in the graph because it is possible that they
may be produced before F9.

Figure 2. A solution graph using backward chaining

Figure 2 presents solving the same problem via BC. It is read from the top down. The problem’s goal is decomposed
by R1 into a sub-goal P(~(A  B)), but doesn’t look promising because no other rules can be fired further. Thus, BC
tries decomposing via R2. That yields three sub-goals, which look promising, so it continues decomposing each of
them until they all eventuate in sub-goals that match given propositions. It then applies the rules in the forward
direction, guided by the goal and sub-goal links that it has saved, doing computations as it goes and eventually
calculating the problem’s answer.
FC is complete (Russell & Norvig, 2003) but can be inefficient because its inference process is not directed toward
solving the problem’s goal. In Figure 1, FC did a lot of irrelevant work. Of the nine facts inferred, only three were
needed for solving the problem. BC, on the other hand, is focused on achieving the problem’s goals but can meet
dead ends. For instance, P(~(A  B)) in Figure 2, is one such dead end because it triggers no more rules in Table 1.

Studies comparing strategy instruction with no-strategy instruction
Although FC and BC are widely used in computer science, they are seldom observed in a pure form in natural human
problem solving. In Newell and Simon’s (1972) seminal study of logic problem solving, none of the subjects used
FC or BC in a pure form. Early studies of expert and novice physics problem solvers suggested that novices used BC
and experts used FC (Larkin, McDermott, Simon, & Simon, 1980; Simon & Simon, 1978), but later studies showed
that both used a mixture and, in fact, used fairly similar mixtures (Priest & Lindsay, 1992). Eventually, work in this
area diminished, perhaps because it appeared that most human solvers used a mixture of strategies, analogies,
27

heuristics, and many other kinds of knowledge during their problem solving.
Although neither experts nor novices seem to use FC and BC in their pure form, the strategies’ success in guiding
computer problem solvers suggests that teaching students to use pure FC or BC might improve their problem
solving. Several studies were conducted to test this hypothesis. Next, we will give a brief review.
Sweller and his colleagues conducted a series of studies comparing goal-free problem solving to ordinary problem
solving (Owen & Sweller, 1985; Sweller, 1989; Tarmizi & Sweller, 1988). In the goal-free problem solving
condition, students were not told a specific goal of a problem. Instead, they were asked to derive everything they
could from the given facts. Students could only use FC on these goal-free problems, as BC requires a goal to start
from. In the ordinary problem-solving condition, students were given problems with a specific goal, so they could
use FC, BC, or a mixture. In all of these studies, the goal-free group learned more than the ordinary problem-solving
group, thus suggesting that “teaching” a single problem-solving strategy in the form of pure FC improves learning.
However, the number of inferences made to solve a goal-free problem is generally much larger than the number of
inferences made to solve a goal-specific problem. Thus, the goal-free students could have benefited simply from
having more practice in applying the domain principles. Indeed, when the experimenters modified the study so that
students in both conditions applied the same number of domain principles, the difference between conditions
disappeared (Owen & Sweller, 1985; Sweller, 1988). Thus, although these studies are consistent with the
hypothesized benefits of explicit strategy instruction, there are other explanations for the results as well.
Trafton and Reiser (1991) tested the benefits of explicit strategy instruction in a sub-domain of computer
programming, wherein students had to compose primitive functions in order to produce composite goal function.
Three forms of instruction were compared based on the way in which the goal function could be assembled: forwardonly, backward-only or freely. After completing 13 training problems in less than an hour, all three groups achieved
the same learning gains. Although it is always hard to interpret a null result, it could be that the task domain was too
simple to allow an explicit instruction on problem-solving strategies to demonstrate benefits.
Scheines and Sieg (1994) gave students over 100 training problems in sentential logic over a five-week period. One
group of students was taught to use FC; a second group was taught to use BC; while a third group, the unconstrained
group, was not taught any strategy and operated freely. After five weeks of instruction, no significant differences
were found among the three groups on the mid-term exam (post-test). When the FC and BC groups were aggregated
as a one-way strategy condition, there were still no significant differences between them and the unconstrained group
on post-test scores. However, contrary to our hypothesis, the unconstrained students gained more than the one-way
strategy students on difficult problems, where one would expect an explicit search strategy to be especially helpful.
The experiment suggested that constraining students to use just one strategy may actually harm their performance.
VanLehn et al. (2004) compared an explicitly taught version of backward chaining to unconstrained problem-solving.
Students who had not taken college physics were taught elementary mechanics over several multiple-hour training
sessions. On some post-test measures, the students who were explicitly taught a strategy scored higher than those
who were not taught a strategy and could solve problems in any order. However, on other measures, the two groups
did not differ. Overall, performance on the post-test was quite poor, suggesting a floor effect — the post-test was too
difficult for both groups.
In summary, most studies above were conducted in a single domain and contrasted students who were taught a
strategy and those who were not. In this paper, we investigate the impact of explicit strategy instruction on
eliminating the gap between high and low across two unrelated domains and two different ITSs. The problemsolving strategy chosen is the target variable strategy (TVS), a domain-independent BC strategy (VanLehn et al.,
2004), and the two selected domains were probability and physics. During probability instruction, students in the
experimental group were trained on an ITS, Pyrenees, that explicitly taught the TVS; while students in the control
group were trained on another ITS, Andes, without explicit strategy instruction. During subsequent physics
instruction, both groups were trained on the same ITS, which did not teach any strategy. On both probability and
physics post-tests, we expect the following: high-experimental = low-experimental = high-control > low-control.
That is, for both task domains, the low students should catch up with the high students, but only if they were taught
the TVS.

28

Methods
Participants
Participants were 44 college students who received payment for their participation. They were required to have a
basic understanding of high-school algebra, but not to have taken college-level statistics or physics courses. Students
were randomly assigned to the two conditions. Two students were eliminated: one for a perfect score on the
probability pre-test and one for deliberately wasting time.
Target variable strategy (TVS)
The TVS consists of three phases: 1) translating the problem statement, 2) applying principles and generating
equations, and 3) solving equations. The central part of TVS happens in the second phase: applying principles and
generating equations, in which students follow:
(1) Choose one of the sought (unknown) variables as the target variable
(2) Select a principle application that will generate an equation containing the target variable
(3) Define new variables as necessary to ensure that every quantity in the equation has a variable
(4) Write the equation in terms of the defined variables
(5) Mark the target variable known
(6) Mark the unknown variables in the equation sought
This procedure is repeated until there is no variable marked sought anymore, and then students go to the final phrase,
solving equations. More details on TVS are described in (VanLehn et al., 2004). To illustrate, we will compare two
example solutions for a probability problem. Table 2 contains a TVS solution by following the TVS while Table 3
contains a non-TVS solution. Note that P(A), P(A  B), P(~A  ~B), etc. are algebraic variables even though their
names make them look like functions.
Table 2. Solving a problem by following the TVS
Problem: Given P(A) = 1/3, P(B) = 1/4, P(A  B) = 1/6, find the probability: P(~A  ~B).
Step
Proposition
Justification
Phase 1: Translating the problem statement
1
P(A) = 1/3
Given
2
P(B) = 1/4
Given
3
Given
P(A  B) = 1/6
4
Sought
P(~A  ~B)
Phase 2: Applying principles and generating equations
To find P(~A  ~B), apply De Morgan’s theorem.
5
P(~A  ~B) = P(~(A  B))
Delete sought from P(~A  ~B)
and mark P(~(A  B)) as sought
To find P(~(A  B)), apply the complement theorem.
6
P(A  B) + P(~(A  B)) = 1
Delete sought from P(~(A  B))
Phase 3: Solving equations.
7
Solve 6.
P(~(A  B)) = 5/6
8
Solve 5
P(~A  ~B) = 5/6
Table 2 shows that in the first phase the student defines four variables, gives values to three of them, and marks P(~A
 ~B) the sought variable. Then the student moves to the second phase. During each cycle of the second phase, the
student must make two decisions. One is which sought variables to select as the target variable if there is more than
one variable marked sought. Since all sought variables must eventually be selected as target variables, the order in
which they are chosen does not affect the solvability of the problem. The other decision is which principle
application to use if there happen to be several that contain the target variable. If the student makes an unlucky
selection, the problem will be unsolvable. If so, the student must back up and make a different selection. In this
example, during the first cycle, only P(~A  ~B) was marked as sought, so it is selected as the target variable. Then
29

the student chooses De Morgan’s theorem as the principle to solve for P(~A  ~B). To do so, the student defines a
variable P(~(A  B)) and then enters the equation P(~A  ~B) = P(~(A  B)). Then the student removes the sought
mark from P(~A  ~B), and marks the other variable in the equation, P(~(A  B)), as sought. This ends the first
cycle. On the next cycle, again, only one variable is marked as sought, P(~(A  B)), so the student selects it as the
target variable, applies the complement theorem to it, and removes the sought mark from it. At this point, no
variables are marked sought, so the second phase ends. During the third and final phase, the student solves the
equations in reverse chronological order.

Step
1
2
3
4
5
6

Table 3. A non-TVS solution of the same problem
Proposition
Justification
Addition theorem for two events:
P(A  B) = P(A)+P(B)- P(A B)
A and B
P(~B)+p(B) = 1
Complement Theorem
De Morgan’s Law and
P(A  B)+ P(~A  ~B) = 1
Complement Theorem
Given
P(A  B) = 1/6
Solve 3
P(~(~A  ~B)) = 1/6
Solve 4
P(~A  ~B) = 5/6

Table 3 presents a solution to the same problem derived without the TVS. The solution is neither FC nor BC. It
includes an equation (Step 2) that is not necessary for solving the problem and another equation (Step 3: P(A  B) +
P(~A  ~B) = 1) that is a combination of two principle applications: P(~(A  B)) = P(~A  ~B) and P(A  B) +
P(~(A  B)) =1. This is typical of the solutions by students who were not taught the TVS.
Students can use the TVS to solve problems in probability, physics, and many other tasks domains (VanLehn et al.,
2004). In general, the TVS applies in task domains where solving a problem consists of generating a solvable set of
equations. The TVS can be proved complete in that it will generate a set of equations that solves the problem if such
a set exists. So far, we have described only the tedious procedural aspects of the TVS. There is only one key aspect
of the TVS that has not yet been described, and we will describe it next.
A key detail and its implications for learning
In the second phase of the TVS, applying principles and generating equations, students select a principle application
whose equation will contain the target variable. To do this, the tutoring system has students first pick a principle by
name from a list of the domain principles that have been taught. It then has students specify how to apply that
principle, again by making menu selections. For example, in step 6 of Table 2, after students pick P(~(A  B)) as the
target variable and select the complement theorem to apply, but before they input the correct equation, P(A  B) +
P(~(A  B)) = 1, the tutoring system would say:
You have chosen the complement theorem to apply for the target variable. To apply the principle, you
must have noticed that there is a set of events that are mutually exclusive and collectively exhaustive.
What are these events?
Students should input the two events, ~(A  B) and (A  B). In AI terms, the students are being asked to supply
values for the arguments of the principle, thus establishing which of many possible instances of the principle should
be applied. Only when an instantiation has been selected is the equation generated by the principle application
completely determined and the tutoring system able to ask the student to enter it.
Therefore, the TVS is not simply a BC strategy, like the one used by Bhaskar and Simon (1977), which has students
simply enter an equation. It instead has them specify both a principle and its arguments. Even if students know what
equation they want, they have to figure out which principle and which arguments will generate it. Thus, they have to
learn the principles deeply instead of simply learn a syntactic version of the available equations. As for the example
above, students following the TVS are more likely to learn that the complement theorem should only apply in the
events that are mutually exclusive and collectively exhaustive instead of in normal events. The TVS taught the
students to focus their attention on acquiring a deep understanding of the principles, as that was all they needed in
30

order to run the TVS. To summarize, the TVS includes two main components: one is to solve problems via BC from
goals to givens, called the BC-strategy, and the other is to focus attention to the domain principles, called the
principle-emphasis skill.

Problem Statement Window

Variable Window

Equation Window

Dialogue Window

Figure 3. Pyrenees’s interface

Three ITSs
The three ITSs involved in this study were Pyrenees, Andes probability, and Andes physics. Their corresponding
screen shots are shown in Figure 3, Figure 4, and Figure 5, respectively. The first two taught probability, whereas the
third taught physics. Apart from their domain knowledge, Andes probability and Andes physics were identical, so we
use “Andes” to refer to both. Pyrenees required students to follow the TVS, while Andes did not require students to
follow any problem-solving strategy. In this study, students in the experimental group learned probability in
Pyrenees, and then learned physics in Andes; while student in the Control group learned both probability and physics
in Andes. Next, we will compare Pyrenees and Andes from the perspectives of both the user interface and students’
behaviors.

Variable Window
Problem

Equation Window

Dialogue Window
Figure 4. Andes probability’s interface
31

Figure 5. Andes physics’ interface

User Interfaces Perspectives
Both Pyrenees and Andes provide a multi-paned screen that consists of a problem-statement window, a variable
window for listing defined variables, an equation window, and a dialog window (see Figures 3–5). However, the
computer-student interactions were quite different for each system. Pyrenees guided students in applying the TVS by
prompting them to take steps dictated by the TVS. For example, when the TVS determined that it was time to define
a variable, Pyrenees popped up a tool for that purpose (see Figure 3). Thus the interaction with Pyrenees was a turntaking dialogue, where the tutor’s turns always ended with a question to which the student must reply. All interaction
with Pyrenees took place in the dialogue window. In Andes, on the other hand, students used GUI tools to construct
and manipulate a solution. Thus the interaction with Andes was open-ended and event-driven. Students could edit or
interact with any of the four windows by drawing vectors in the top left window, writing or editing equations in any
row of the equation window, and so on. Once an entry or edit was made successfully, Andes provided no further
prompting for the next step. If students didn’t know what to do next, they could ask for a hint by clicking on the
next-step help button.

Interactive behaviors perspectives
Both Andes and Pyrenees provide immediate feedback. However, their standard of correctness differs. Andes
considers an entry correct if it is true, regardless of whether it is useful for solving the problem. On Pyrenees,
however, an entry is considered correct if it is true and strategically acceptable to the TVS. Moreover, students can
enter an equation that is the algebraic combination of several principle applications on Andes but not on Pyrenees
because the TVS requires students to apply one principle at a time.
Both systems provide hints when students ask. When an entry is incorrect, students can either fix it independently or
ask for what’s-wrong help. When they do not know what to do next, they can ask for next-step help. Both next-step
help and what’s-wrong help are provided via a sequence of hints that gradually increase in specificity. The last hint in
the sequence, called the bottom-out hint, tells the student exactly what to do. Pyrenees and Andes give the same
what’s-wrong help for any given entry, but their next-step help differs. Because Pyrenees requires students to follow
the TVS, it knows what step they should be doing next so it gives specific hints. In Andes, however, students can
always enter any correct step, so Andes does not attempt to determine their problem-solving plans. Instead, it asks
students what principle they are working on. If students indicate a principle that is part of a solution to the problem,
Andes provides as a hint an uncompleted step from the principle application. If no acceptable principle is chosen,
Andes picks an unapplied principle from the solution that they are most likely to be working on.
32

Two domains
Two deductive domains, probability and physics, were involved in this study as the initial and transfer domain,
respectively. Each domain contained ten major principles. Probability included the complement theorem, Bayes rule,
and so on; while physics included the definition of kinetic energy, conservation of total mechanical energy, and so
on.

Procedure
The procedure in this study had four main parts: background survey, probability instruction, Andes interface training, and
physics instruction (shown in the left column of Table 4). All materials were online. The background survey asked for high
school GPA, SAT scores, experience with algebra, and other information.

Table 4. Experiment procedure
Part
Survey
Probability instruction

Andes interface training

Physics instruction

Experimental

Control
Background survey
Pre-training
Pre-test
Training on Pyrenees
Training on Andes probability
Post-test
Solve a probability problem on Andes
probability
Pre-training
Pre-test
Training on Andes physics
Post-test

The probability and physics instruction each consisted of the same four phases: 1) pre-training, 2) pre-test, 3) training
on the tutoring system, and 4) post-test. We describe each phase in turn, pointing out relevant differences, if any,
between the two task domains.

Pre-training
During pre-training all students studied the domain principles. For each principle, they read a general description,
reviewed some examples, and solved a series of single-principle and multi-principle problems. After solving a
problem, the answer was marked correct or incorrect, and the correct solution was displayed. If the answer was
incorrect, the students were asked to solve another problem isomorphic to the one that they had just failed to solve;
this repeated until they either succeeded in solving a problem or failed three times. On multiple-principle problems,
students had only one chance to solve the problem and were not asked to solve an isomorphic problem if their
answer was incorrect.

Pre-tests
During the pre-tests, after an answer was submitted, students automatically proceeded to the next question without
any feedback on the correctness of the answer. Students were not allowed to go back to earlier questions. This was
the procedure for the post-tests as well. All students took the same pre- and post-tests. All test problems were openended and required students to derive an answer by writing and solving one or more equations.

Training on ITSs
In Phase 3, students first watched a video that demonstrated problem solving in the corresponding ITS. During
probability instruction, the strategy students also read a text description of the TVS. Then, all students solved the
33

same twelve probability problems or eight physics problems in the same order. More specifically, students in the
experimental group solved all twelve probability problems in Pyrenees and students in the control group solved them
in Andes probability. Both conditions solved the eight physics problems on Andes physics. Students could also
access the domain textbook at any time during training. During the probability training, students in the experimental
group were able to access a description of the TVS. Each main domain principle was applied at least twice in both
trainings.

Post-tests
Finally, all students took a post-test. Five problems on both post-tests were isomorphic to training problems in Phase
3. In addition, there were five non-isomorphic, novel, multiple-principle problems in the probability post-test and
eight in the physics post-test. Table 5 shows the distribution of single-principle and multiple-principle problems in
the experiment.
Table 5. Number of various problems during pre-training, pre-test, training, and post-test
Single-principle
Multiple-principle
Pre-training
14
5
Pre-test
10
4
Probability
Training
−
12
Post-test
10
10
Pre-training
11
3
Pre-test
9
5
Physics
Training
−
8
Post-test
5
13

Total
19
14
12
20
14
14
8
18

Only the students in the experimental group took the third part, Andes interface training. Its purpose was to
familiarize them with the Andes GUI without introducing any new domain knowledge. The problem used was one of
the 12 probability training problems that they had previously solved on Pyrenees. Pilot studies showed that one
problem was sufficient for most students to become familiar with Andes GUI.
To summarize, the procedural difference between the two conditions were: 1) during probability instruction, students
in the experimental group trained on Pyrenees while students in the control group trained on Andes probability; 2)
students in the experimental group learned how to use the Andes user interface before they received physics
instruction.

Grading criteria
We used two scoring rubrics: binary and partial credit. Under the binary rubric, a solution is worth 1 point if it is
completely correct or 0 if not. Under the partial credit rubric, each problem score is a proportion of correct principle
applications evident in the solution. If they correctly applied four of five possible principles they would get a score of
0.8. Solutions were scored by a single grader blind to conditions.

Results
In order to measure aptitude-treatment interaction, we needed to define high and low groups based on some measure
of incoming competence. We chose to use MSAT scores because probability and physics are both math-like domains.
Our split point was 640, which divide into high (n = 20) and low (n = 22). Except for the MSAT scores and highschool GPA, no significant difference was found between high and low on other background information such as age,
gender, VSAT scores, and so on. As expected, the high group out-performed the low group during the probability
pre-training and the probability pre-test under the binary scoring rubric: t(40) = 3.15, p = 0.003, d = 0.96, t(40) =
2.15, p = 0.038, d = 0.66, and t(40) = 2.27, p < 0.03, d = 0.70 on single-principle, multiple-principle problems during
probability pre-training, and overall in probability pre-test, respectively. The same pattern was found under partial
34

rubric in the probability pretest. Thus, the MSAT score successfully predicted the incoming competence of the
students, which justifies using it to define our high vs. low split.
Incoming competence combined with conditions partitioned the students into four groups: high-experimental (n =
10), low-experimental (n = 10), high-control (n = 10), and low-control (n = 12). Fortunately, random assignment
balanced the experimental vs. control conditions for ability, and this balance persisted even with the groups
subdivided into high and low via MSAT score. On every measure of incoming competence, no significant difference
was found between the experimental and control groups, the low-experimental and low-control ones, or the highexperimental and high-control ones. These measures were the background survey, the probability pre-test, probability
pre-training scores, the time spent reading the probability textbook, and the time spent solving the pre-training
problems. Averaged over all students, the total time for each training phase were 2.4 hrs and 2.7 hrs for probability
pre-training and training and 1.5 hrs and 3.0 hrs for physics pre-training and training, respectively. No significant
differences were found among the four groups on any of these times.

Test scores
Error! Reference source not found. shows that the test score results are consistent with our hypothesis. After
training on Pyrenees, the low-experimental students scored significantly higher than their low-control peers on all
three assessments: probability post-test, physics pre-test and physics post-tests: t(20) = 4.43, p < 0.0005, d = 1.90;
t(20) = 3.23, p < 0.005, d = 1.34; and t(20) = 4.15, p < 0.0005, d = 1.84, respectively. More importantly, the lowexperimental students even seemed to catch up with the high ones: no significant difference was found among the
high-experimental, low-experimental, and high-control on all three assessments, even though the two experimental
groups seemed to out-perform the high-control group in Figure 6.

Figure 6. Comparison of four groups on four tests (maximum score = 100)

Thus, the Pyrenees instruction in probability caused the low-experimental group to learn more effectively than those in the
low-control group during probability training, physics training, and even physics pre-training. They seemed to have caught
up to the high ones while the low-control ones did not. Moreover, while the high-experimental group didn’t benefit much
from the TVS, they were not harmed either.

Dynamic assessments
While test results are the most common assessment of learning performance, one can also compare students’
behaviors as they learn. Such comparisons are called dynamic assessments (Haywood & Tzuriel 2002). In
performing dynamic assessments, we can identify students who are effective learners even though their test scores
35

may be equal to or even lower than those of others. Here we investigated students’ interactive behaviors on Andes
during physics training, as all students received the identical procedure during that period.

Frequency of help requests
Andes physics logs every user’s interface action performed, including help requests, tool usage, and equation entries.
We first tried to characterize the overall difference in students’ solutions via the amount of help they requested. On
each of eight physics training problems, the low-experimental students made significantly fewer next-step help
requests than the low-control ones. No significant difference was found among the low-experimental, highexperimental and high-control groups. This suggests that the low-experimental students may have transferred the
TVS. However, there are other possible explanations, so we conducted several other analyses.

Triage of logs
Solution logs were grouped into three categories: smooth, help-abuse, and rocky. Smooth solutions included no help
requests, except on problems that required more than eight principle applications. Students were permitted up to two
what’s-wrong help requests. Help-abuse solutions were produced when every entry was derived from one or more
next-step helps. Otherwise, the solution was categorized as rocky because students appeared capable of solving part
of the problem on their own, but needed help on the rest.
Figure 7 shows a significant difference among the four groups on the distribution of the three types of solutions.
While no significant difference was found between the high-experimental and low-experimental groups, there was a
significant difference between the low-control and the high-control groups: 2(2) = 11.33, p(2) = 0.003.
Furthermore, a significant difference was found between the low-experimental and high-control groups: 2(2) =
15.322, p(2) < 0.001, and between the high-experimental and high-control groups: 2(2) = 11.585, p(2) < 0.005.
Qualitatively, the results appear to be as follows: high-experimental = low-experimental > high-control > lowcontrol.

Figure 7. Solution percentage by type

For a more quantitative measure, we used a smaller unit of analysis, individual equations. We coded each correct
equation entry in the solution logs with 3 features:
 Relevance: The equation was labeled relevant or irrelevant based on whether it contributed to the problem
solution.
 Help: The equation was labeled “Help” if it was entered after the student asked for help from Andes physics.
Otherwise, it was labeled “No-help.”
 Content: The equation’s content was coded as either “a correct equation with new physics content” or “others.”
We sought to find out how frequently students made progress toward solving a problem without asking for any help
from Andes. In terms of the three-feature coding mentioned above, such a “desirable” equation would be coded as
“relevant,” “no-help,” or “correct equation with new physics content.” We called these desirable equations desirable
36

steps and defined the desirable steps ratio (DSR):

DSR 

Desirable steps in the solution
All steps in the solution

Figure 8. DSR on overall solutions
As shown in Figure 8, the low-experimental students had significantly higher DSR than the low-control ones: t(169)
= 7.50, p < 0.0001. In fact, the former even made significantly more progress than the high-control group: t(150) =
3.84, p < 0.001. While there is a significant difference between the low-control and high-control groups: (t(171) =
2.83, p < 0.01), there is no such difference between the two experimental groups. In short, this dynamic assessment
showed that the following: high experimental = low experimental > high-control > low-control.
To summarize, both test scores and dynamic assessments show that the low students catch up with the high ones in
the experimental condition but not in the control condition. On some measures, the low-experimental students even
surpass the high-control ones. Next, we’ll investigate what was transferred from probability to physics that made the
low experimental students so successful.

Transferring the two cognitive skills of the TVS
As we described above, the TVS includes two main components: solving problems via backward-chaining (BC)
from goals to givens, called the BC-strategy, and drawing students’ attention to the characteristics of each individual
domain principle, called the principle-emphasis skill. In the following, we will investigate whether either or both
skills were transferred by the two experimental groups to physics. In order to determine the BC-strategy usage, we
analyzed students’ logs to see whether the order of equations in their solutions followed the BC strategy. For the
principle-emphasis skill, we used the single-principle problems as our litmus test. Students who had applied the BCstrategy would have no particular advantage because solving these single-principle problems need to apply only one
principle. On the other hand, students who had learned the idea of focusing on domain principles should be at an
advantage.
Transferring the BC strategy
If students engaged in the BC-strategy, we expect they would apply the BC strategy when they had difficulties, that
is, on rocky solutions. On smooth solutions, students don’t have any difficulties since they may solve problems
mainly based on existing schemas (Sweller, 1989). Thus, we subcategorized each desirable step in the logs as BC or
non-BC, where non-BC included FC, combined equations, and so on. We then defined BC% as the proportion of
desirable steps that were coded as BC. Figure 9 showed that on Rocky solutions the high-experimental group applied
BC significantly more frequently than the other three groups: t(40) = 2.25, p = 0.03 while the low-experimental
group used the BC as frequently as the two control groups. Thus, apparently it was the high-experimental group
alone who transferred the BC-Strategy to physics.
37

Figure 9. BC usage on rocky solutions
Transfer of the principle-emphasis skill
The low-experimental students scored just as high as the high-experimental ones even though they used BC no more
frequently than the students in the two control groups. Our hypothesis is that they transferred the principle-emphasis
skill. We divided both post-tests into single-principle and multiple-principle problems. Furthermore, we divided the
multiple-principle problems into those that were isomorphic to a training problem and those that were not. If students
in the low-experimental group applied the principle-emphasis skill, we expected them to out-perform students in the
low-control group on all three types of problems in both post-tests. This turned out to be the case (see Table 6). In
Table 6, the third and fourth columns list the means of test scores of the low-experimental and low-control groups.
The low experimental group had reliably higher means than the low-control group in both probability and physics
post-tests across three types of problems: simple-principle, isomorphic multiple-principle and non-isomorphic
multiple-principle. This suggests that a main effect of teaching the TVS to the low students was to get them to focus
on the domain principles. Further analysis showed no significant difference among the students in high-control, lowexperimental, and high-experimental groups on any types of problems, which indicates that high students may
already have such skill.
Table 6. Scores on three types of problems in both probability and physics post-tests
Mean (lowProblem type
Mean (low-control)
Statics
experimental)
Probability post-test
t(20) = 3.62, p =
Single
0.93
0.70
0.002, d = 1.58
t(20) = 3.71, p =
Multiple, isomorphic
0.48
0.23
0.001, d = 1.55
Multiple,
t(20) = 3.734, p =
0.44
0.17
non-isomorphic
0.013, d = 1.15
Physics post-test
t(20) = 4.33, p <
Single
0.93
0.8
0.001, d = 1.85
t(20) = 4.55, p <
Multiple, isomorphic
0.60
0.18
0.001, d = 1.93
Multiple,
t(20) = 3.734, p <
0.70
0.19
non-isomorphic
0.001, d = 2.10
Test

Conclusions
Overall, our instructional manipulation indeed exhibited an aptitude-treatment interaction: the gap between highexperimental and low-experimental students seemed to be eliminated in both probability and physics, whereas it
remained between the high-control and low-control groups. More detailed analyses of the training behavior and posttest results suggest that students in the low-experimental group transferred the principle-emphasis skill to physics
while those in the high-experimental group apparently already possessed it. On the other hand, students in the highexperimental group transferred the BC strategy.
38

These results suggest that it is not the BC strategy that is most important to teach low learners. Instead, one should
teach the meta-cognitive skill of focusing on individual principle applications. It could be that low and high learners
differed initially in that low students lacked this “how to learn” meta-cognitive knowledge for a principle-based
domain like probability or physics. Such results suggest building an ITS that does not teach the TVS explicitly, but
instead just teaches to focus on principle applications in deductive domains. Perhaps it would be just as effective as
Pyrenees. Indeed, because its students need not learn all the complicated bookkeeping of the BC strategy, which may
cause cognitive overload (Sweller, 1989), it might even be more effective than Pyrenees not only for an initial
domain where the ITS was used but also for subsequent domains where it is not used.

References
Bhaskar, R., & Simon, H. A. (1977). Problem solving in semantically rich domains: An example from engineering
thermodynamics. Cognitive Science, 1, 193–215.
Bloom, B. S. (1984). The 2 sigma problem: The search for methods of group instruction as effective as one-to-one tutoring.
Educational Researcher, 13, 4–16.
Chi, M. & VanLehn, K. (2007). Accelerated future learning via explicit instruction of a problem solving strategy. In R. Luckin, K.
R. Koedinger, & J. Greer (Eds.) The13th International Conference on Artificial Intelligence in Education (pp. 409–416).
Amsterdam, Netherlands: IOS Press.
Cronbach, L. J., & Snow, R. E. (1977). Aptitudes and instructional methods: A handbook for research on interactions. New York:
Irvington.
Haywood, H.C. & Tzuriel, D. (2002). Applications and challenges in dynamic assessment. Peabody Journal of Education, 77(2),
40–63.
Larkin, J., McDermott, J., Simon, D. P., & Simon, H. A. (1980). Expert and novice performance in solving physics problems.
Science, 208, 1335–1342.
Lehman, D. R., Lempert, R. O., & Nisbett, R. E. (1988). The effects of graduate training on reasoning: Formal discipline and
thinking about everyday-life events. American Psychologist, 43, 431–442.
Lehman, D. R., & Nisbett, R. E. (1990). A longitudinal study of the effects of undergraduate training on reasoning.
Developmental Psychology, 26, 431–442
Newell, A., & Simon, H. A. (1972). Human problem-solving. Engelwood Cliff, NJ: Prentice-Hall.
Owen, E. & Sweller, J., (1985) What do students learn while solving mathematics problems? Journal of Educational Psychology
77(3), June 1985, 272–284.
Priest, A. G., & Lindsay, R. O. (1992). New light on novice–expert differences in physics problem-solving. British Journal of
Psychology, 83, 389–405.
Russell, Stuart J. & Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (2nd Ed.), Upper Saddle River, NJ: Prentice
Hall.
Simon, D. P., & Simon, H. A. (1978). Individual differences in solving physics problems. In R. S. Siegler (Ed.), Children’s
thinking: What develops? Hillsdale, NJ: Lawrence Erlbaum Associates.
Sweller, J. (1988). Cognitive load during problem-solving: Effect on learning. Cognitive Science, 12, 257–285
Sweller, J. (1989). Cognitive technology: Some procedures for facilitating learning and problem-solving in mathematics and
science. Journal of Educational Psychology, 8 (4), 457–466.
Tarmizi R. A., & Sweller, J. (1988). Guidance during mathematical problem-solving, Journal of Educational Psychology 80(4),
424–436.
Trafton, J. G., & Reiser, B. J. (1991). Providing natural representations to facilitate novices’ understanding in a new domain:
Forward and backward reasoning in programming. Proceedings of the Thirteenth Annual Conference of the Cognitive Science
Society (pp. 923–927). Hillsdale, NJ: Lawrence Erlbaum Associates, Inc.
VanLehn, K. (2006) The behavior of tutoring systems. International Journal of Artificial Intelligence in Education. 16 (3) 227–
265.
VanLehn, K., Bhembe, D., Chi, M., Lynch, C., Schulze, K., Shelby, R., Taylor, L., Treacy, D., Weinstein, A., & Wintersgill, M.
(2004). Implicit versus explicit learning of strategies in a non-procedural cognitive skill. In J. C. Lester, R. M. Vicari, & F.
Paraguacu (Eds.), Intelligent Tutoring Systems: 7th International Conference (pp. 521-530). Berlin: Springer-Verlag.
VanLehn, K., Lynch, C., Schulze, K. Shapiro, J. A., Shelby, R., Taylor, L., Treacy, D., Weinstein, A., & Wintersgill, M. (2005).
The Andes physics tutoring system: Lessons learned. International Journal of Artificial Intelligence and Education, 15(3), 1–47.
39

McQuiggan, S. W., Robison. J. L., & Lester, J. C. (2010). Affective Transitions in Narrative-Centered Learning Environments.
Educational Technology & Society, 13 (1), 40–53.

Affective Transitions in Narrative-Centered Learning Environments
Scott W. McQuiggan1, Jennifer L. Robison2 and James C. Lester3
1

Education Practice, SAS Institute Inc. // scott.mcquiggan@sas.com
Department of Computer Science, North Carolina State University // jlrobiso@ncsu.edu
3
Department of Computer Science, North Carolina State University // lester@csc.ncsu.edu
2

ABSTRACT
Affect has been the subject of increasing attention in cognitive accounts of learning. Many intelligent tutoring
systems now seek to adapt pedagogy to student affective and motivational processes in an effort to increase the
effectiveness of tutorial interaction and improve learning outcomes. To this end, recent work has begun to
investigate the emotions experienced during learning in a variety of environments. In this paper we extend this
line of research by investigating the affective transitions that occur throughout narrative-centered learning
experiences. Further analysis differentiates the likelihood of affective transitions stemming from pedagogical
agent empathetic responses to student affect.

Keywords
Affective transitions, Narrative-centered learning environments, Empathetic pedagogical agents

Introduction
Affect has begun to play an increasingly important role in intelligent tutoring systems (ITS). The ITS community has
seen the emergence of work on affective student modeling (Conati & Mclaren, 2005), detecting frustration and stress
(Burleson, 2006; McQuiggan, Lee, & Lester, 2007; Prendinger & Ishizuka, 2005), modeling student uncertainty
(Forbes-Riley & Litman, 2007), modeling agents’ emotional states (André & Mueller, 2003; Gratch & Marsella,
2004; Lester, Towns, & FitzGerald, 1999), devising affectively informed models of social interaction (Johnson &
Rizzo, 2004; Paiva et al., 2005; Porayska-Pomsta & Pain, 2004; Wang et al., 2008), detecting student motivation (de
Vicente & Pain, 2002), and diagnosing and adapting to student self-efficacy (Beal & Lee, 2005; McQuiggan, Mott,
& Lester, 2008). All of this work seeks to increase the fidelity with which affective and motivational processes are
understood and utilized in intelligent tutoring systems in an effort to increase the effectiveness of tutorial interactions
and, ultimately, learning.
Recent work seeking to characterize the affective experience of learners interacting with intelligent learning
environments has considered student affective trajectories occurring during learning. D’Mello, Taylor, & Graesser
(2007) studied the likelihood of affective transitions among six affective states (boredom, flow, confusion,
frustration, delight, and surprise) that were found to be relevant to complex learning (Craig, Graesser, Sullins, &
Gholson, 2004). In general, learners are likely to persist in the same affective state (e.g., transitioning from a state of
boredom to boredom is likely, and in some cases, significantly more likely than transitioning to another affective
state). This analysis was conducted in the AutoTutor learning environment (Craig et al., 2004; D’Mello et al., 2007).
Baker, Corbett, Koedinger, & Wagner (2004) were able to replicate many of D’Mello et al.’s (2007) findings when
they calculated the likelihood of affective transitions in the Incredible Machine: Even More Contraptions, a
simulation-based learning environment (2007). Baker et al. (2004) extend their analyses to investigate how usage
choices affect emotion transitions. This work found that confused learners are likely to game the system. Further, it
was found that students who game the system are unlikely to transition into a confused state (Baker, Rodrigo, &
Xolocotzin, 2007).
In this article we investigate the likelihood of affective transitions in a narrative-centered learning environment,
CRYSTAL ISLAND. The CRYSTAL ISLAND environment uses narrative as a mechanism to contextualize learning,
making the experience meaningful. Contextualized learning experiences are known to encourage regulated learning
behavior (Perry, 1998) and influence student learning and motivation (Linnenbrink & Pintrich, 2001). Because
CRYSTAL ISLAND incorporates an engaging storyline into the learning experience, we supplement the known relevant
emotions to learning used by D’Mello et al. (2007) and Baker et al. (2007) with affective states that may be relevant
to the story (anger, anxiety, boredom, confusion, delight, excitement, fear, flow, frustration, and sadness). We extend
our analysis of affective transitions to evaluate the impact of character empathetic responses (parallel vs. reactive
empathy) to student affect and the relative impact on transitions. We further extend our analysis to investigate whether
ISSN 1436-4522 (online) and 1176-3647 (print). © International Forum of Educational Technology & Society (IFETS). The authors and the forum jointly retain the
copyright of the articles. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that copies bear the full citation on the first page. Copyrights for components of this work owned by
others than IFETS must be honoured. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior
specific permission and/or a fee. Request permissions from the editors at kinshuk@ieee.org.

40

additional factors may affect the frequency of transitions between affective states, turning our attention to characteristics of
the students. We have chosen four characteristics to examine based on their potential influence on learning and reaction to
the learning environment: gender, personality, goal orientation, and presence.

The article is organized as follows. First we describe CRYSTAL ISLAND, the narrative-centered learning environment
that has been developed in our lab for the domains of microbiology and genetics. Next, we present the experimental
method utilized to study student affective experiences. We then report findings on probable transitions in narrativecentered learning and present analyses of the impact of empathy on such transitions. We discuss results and note
limitations, then provide conclusions and an indication of future work.

CRYSTAL ISLAND
The CRYSTAL ISLAND environment (Figure 1 and Figure 2) is being created for the domains of microbiology and
genetics for middle-school students. It features a science mystery set on a recently discovered volcanic island, where
a research station has been established to study the unique flora and fauna. The user plays the protagonist, Alex, who
attempts to discover the genetic makeup of the chickens at the research station whose eggs carry an unidentified
infectious disease. The story opens by introducing the student to the island and the members of the research team for
which the student’s father serves as the lead scientist. As members of the research team fall ill, it is the student’s task
to discover the cause and the specific source of the outbreak. She is free to explore the world and interact with other
characters while forming questions, generating hypotheses, collecting data, and testing her hypotheses. Throughout
the mystery, she can walk around the island and visit various locations. She can pick up and manipulate objects and
talk with characters to gather clues about the source of the disease. In the course of her adventure she must gather
enough evidence to correctly choose which breeds of chickens need to be banned from the island. The virtual world
of CRYSTAL ISLAND, the semi-autonomous characters that inhabit it, and the user interface were implemented with
Valve Software’s Source engine, the 3D game platform for Half-Life 2. The Source engine also provides much of the
low-level (reactive) character behavior control. The character behaviors and artifacts in the storyworld are the subject
of continued work.

Figure 1. Overview of CRYSTAL ISLAND

Figure 2. The user, Alex, with Jin, the camp nurse on CRYSTAL ISLAND

41

The following scenario illustrates a student’s interactive narrative experience in CRYSTAL ISLAND. In the course of
having members of her research team become ill, she learns that an infectious disease is an illness that can be
transmitted from one organism to another. As she concludes her introduction to infectious diseases, she learns from
the camp nurse that the mystery illness seems to be coming from eggs laid by certain chickens and that the source of
the disease must be identified. The student discovers through a series of tests that the bad eggs seem to be coming
from chickens with white feathers. The student then learns that this is a co-dominant trait and determines that any
chicken containing the allele for white feathers must be banned from the island immediately to halt the spread of the
disease. The student reports her findings back to the camp nurse.

Method
After describing the participants, we introduce the experimental design. We then present the results and discuss the
affective transitions observed in narrative-centered learning experiences.

Participants
The subjects of the study were 35 graduate students ranging in age from 21 to 60 (M = 24.4, SD = 6.41) including 9
females and 26 males. Among these students, 60% were Asian (n = 21) and approximately 37% were Caucasian (n =
13). One participant chose not to respond.

Procedure
Participants entered the experiment room where they completed informed-consent documentation. They were
randomly assigned to either the control condition or the empathy condition and were seated in front of a laptop
computer. They were then given an overview of the experiment agenda, and they completed the pre-experiment
questionnaires including the demographics survey, the interpersonal reactivity index survey (Davis, 1994), the goal
orientation survey (Elliot & McGregor, 2001), and the personality questionnaire (McCrae & Costa, 2003).
Upon completing the pre-experiment questionnaires, participants were instructed to review CRYSTAL ISLAND
instruction materials. These materials consisted of the backstory and task description, character overviews, a map of
the island, a control sheet, and a definition sheet of the self-report emotions. Participants were then further briefed on
the controls via a presentation summarizing the task and explaining each control in detail. Participants maintained
access to the materials, including the definition sheet of the self-report emotions, throughout their interaction.
Participants were given 35 minutes to solve the mystery. Solving the mystery consisted of completing 15 goals
including learning about various diseases, compiling the symptoms of the sickened researchers, testing a variety of
possible sources, and reporting the solution (cause and source) back to the camp nurse.
Six CRYSTAL ISLAND characters (Audrey, Elise, Jin, Quentin, Robert, and Teresa), each play distinct roles in the
CRYSTAL ISLAND environment. When subjects decided to interact with these particular characters, they were greeted
with empathetic reactions to their expressed affective state, which they communicated through self-reports via an ingame dialog. The self-report dialog asked participants to select the affective state that best described their feelings at
that time from a set of 10 affective states (anger, anxiety, boredom, confusion, delight, excitement, fear, flow,
frustration, and sadness). This set of emotions was comprised of emotions identified with learning (Craig et al., 2004;
D’Mello et al., 2007; Kort et al., 2001), together with basic emotions (Ekman & Friesen, 1978) that may play a role
in students’ experience of the CRYSTAL ISLAND narrative.
Immediately after solving the science mystery of CRYSTAL ISLAND (or after 35 minutes of elapsed interaction time
for subjects who had not solved the mystery), subjects completed a post-experiment questionnaire. This researcherdesigned questionnaire assessed perceptions of individual CRYSTAL ISLAND characters. The results of this instrument
are outside the scope of this discussion. Additionally, participants’ presence experience was captured with the presence
questionnaire (PQ), which was developed and validated by Witmer and Singer (1998). The PQ contains several subscales,
including involvement/control, naturalism of experience, and quality of the interface scales. The PQ accounts for four
categories of contributing factors of presence: control, sensory, distraction, and realism.
42

Results
In this section, we first present findings regarding common affective transitions observed in CRYSTAL ISLAND. These
findings are followed by an analysis comparing and contrasting likely affective transitions stemming from parallel
and reactive empathetic reactions by CRYSTAL ISLAND characters.
To compute transition likelihoods, we adopt D’Mello et al.’s L (2007), which is based on Cohen’s Kappa (1960), and
has been used by Baker et al. for affective transition analysis in their simulation learning environment (2007). L
computes the probability that a transition between two affective states (Current → Next) will occur, where Current
refers to a reported emotion at time t, while Next refers to the next reported emotion at time t + 1. D’Mello et al.’s L
accounts for the base frequency of the Next affective state in assessing the likelihood of a particular transition (2007).
Formally,

Table 1. Likelihoods for all transitions Current → Next for the affective states:
Frustration, Flow, Confusion, Delight, Boredom, Anxiety, Excitement, Anger, Sadness, and Fear

Current
Fr
Fl
Co
De
Bo
Anx
Ex
Ang
Sa
Fe

Fr

Fl

Co

De

Next
Bo

Anx

Ex

Ang

Sa

Fe

0.28

−0.19

0.10

−0.05

−0.07

−0.15

−0.10

−0.02

−0.01

0.09

−0.04

0.19

0.04

0.02

−0.01

0.03

−0.07

0.01

0.00

0.00

0.04

0.04

0.16

−0.03

0.05

−0.04

0.10

−0.01

−0.01

−0.03

0.01

0.10

−0.13

0.21

−0.03

−0.05

−0.33

−0.02

0.00

0.00

0.13

−0.03

−0.03

−0.08

0.13

−0.04

−0.04

0.00

−0.03

0.04

−0.08

0.06

0.04

−0.07

−0.01

0.14

−0.19

0.09

0.00

0.00

−0.05

−0.11

0.06

−0.03

−0.03

0.03

0.24

−0.01

0.01

−0.02

0.00

−0.07

0.09

−0.39

0.00

0.23

0.01

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

0.00

L’s numerator is divided by 1−P(Next) to normalize scores between -∞ and 1 (2007). A result of L equal to 1
translates to emotion Next always following the Current emotion; an L value equal to 0 means the likelihood of
transitioning to emotion Next is equal to chance, that is, the probability of experiencing Next (the base rate)
regardless of the Current emotion. An L value less than 0 translates to the likelihood of transitioning to emotion Next
being less than chance (the probability of experiencing Next regardless of the Current emotion).
To characterize affective transitions we first compute L for each transition (Current → Next), for each student. We
then use mean L values across students to determine the likelihood of transitioning from each emotion Current to
each emotion Next. The results of ANOVAs determine whether the differences in likelihoods of transitioning to each
Next emotion are significantly different for particular Current emotions.

Affective transitions
Aggregating self-reported affective states across the 35 participants, we find flow to be the most frequently reported
state (42%), followed by excitement (14%), confusion (13%), delight (11%), anxiety (8%), frustration (6%),
boredom (3%), sadness (2%), anger (1%), and fear (1%).
ANOVAs indicated that six affective states had statistically significant differences among the likelihoods of
transitions. Affective transitions were statistically significantly different transitioning from frustration (F(9, 340) =
2.06, p = .03), flow (F(9, 340) = 18.3, p < .0001), confusion (F(9, 340) = 1.79, p = .06), delight (F(9, 340) = 5.22, p
< .0001), anxiety (F(9, 340) = 2.98, p = .002), and excitement (F(9, 340) = 2.62, p = .006).
43

Notably, frustrated learners are most likely to remain frustrated (Mean L = .28) followed by transitions to confusion
(0.10) and fear (0.09). The remaining transitions were below chance levels (i.e., flow (−0.19, t(34) = −4.24, p <
.0001) and excitement (−0.10)).
Table 2. Interesting likelihood for transition differences by empathetic response type (parallel or reactive)
Current
Transition state (Next)
Parallel empathy likelihood
Reactive empathy likelihood
Boredom
.35
−.04
Boredom
Confusion
0
−.41
Flow
−.13
.32
Frustration
−.08
.26
Anxiety
.33
.05
Anxiety
Frustration
−.20
.17
Frustration
.57
−.13
Frustration
Flow
.10
−.25
Confusion
−.17
.15
Flow
.11
−.05
Flow
Confusion
.04
.08
Delight
.21
.21
Delight
Flow
.07
.17
Learners in the state of flow were most likely to remain in flow (0.19) followed by confusion (.04, t(34) = −3.09, p =
.003), anxiety (0.03), and delight (0.02). Both frustration (−0.04, t(34) = −7.91, p < .0001), and excitement (−0.07)
were below chance levels. The remaining transitions did not occur or occurred at chance levels.
Confused students were likely to remain in a confused state (0.16) followed by excitement (0.10), boredom (0.05),
frustration (0.04), and flow (0.04). The likelihood of these and all remaining conditions is summarized in Table 1.

Affective transitions by empathy
Empathy is the expression of emotion based on another’s situation and not merely one’s own (Davis, 1994). Its
expression can demonstrate that the feelings of the target (the recipient of empathetic expression) are understood or
shared. In the case of parallel empathy, an individual exhibits an emotion similar to that of the target (Davis, 1994).
This is typically based on an understanding of the target’s situation and shows the empathizer’s ability to identify
with the target. Reactive empathy, in contrast, focuses on the target’s affective state, in addition to her situation
(Davis, 1994). Reactive empathizers will display emotions that are different from the target’s, often in order to alter
or enhance the target’s own affective state. This type of empathy is focused on the target, whereas parallel empathy
is more self-oriented. As such, reactive empathy can be viewed as a higher level of empathetic behavior.
Recent research with the characters of CRYSTAL ISLAND has investigated the merit of providing characters with
empathetic capabilities to effectively respond to unfolding student experiences (McQuiggan, Robison, Phillips, &
Lester, 2008; McQuiggan, Rowe, & Lester, 2008). In CRYSTAL ISLAND, empathetic responses are short, text-based
responses consisting of one or two sentences. Parallel responses consist of the character expressing the same emotion
as the user through text responses. On the other hand, reactive responses demonstrate advanced cognitive processing
on the character’s part by providing responses designed to be more motivating, thus revealing the character’s desire
for the user to be in a positive emotional state. The results below investigate the likelihood of affective transitions
based on empathetic expressions by CRYSTAL ISLAND characters in response to student Current emotions. The
findings suggest that in certain situations, parallel and reactive empathy have significant differences in the affective
transitions (Next emotions) that are likely to occur.
While the relatively low frequencies of some transitions prevent many of the visible differences from being
statistically significant, interesting patterns do emerge. Figure 3 and Figure 4 present the transitions from the state of
flow and frustration by empathetic reaction type (parallel or reactive). Analyzing the transitions from the state of
flow, we find that parallel empathy (0.11) is somewhat significantly more likely to support students’ remaining in the
state of flow than reactive empathy (−.05), t(12) = −2.08, p = .06. Similarly, we find that the likelihood of
44

transitioning to frustration from a frustrated state is significantly greater when characters’ empathetic reactions are
more parallel in nature (0.57) than reactive (−0.13), t(12) = −2.09, p = .059. Other patterns with visible differences
emerging from this analysis of affective transitions are summarized in Table 2. Although the transition frequencies
were not sufficiently high for the differences to be statistically significant, they merit discussion.

Figure 3. Transitions from frustration to FRustration, FLow, COnfusion, DElight, BOredom, ANXiety,
EXcitement, ANGer, SAdness, and FEar

Figure 4. Transitions from flow to FRustration, FLow, COnfusion, DElight, BOredom, ANXiety, EXcitement,
ANGer, SAdness, and FEar
Student characteristics results
In this section we analyze the individual differences with which affective states are reported. This examination
includes demographics, personality, goal orientation, and presence. These findings are followed by a summary of
individual differences in affective transitions.
45

Gender refers to an individual’s identification as male or female. Interestingly, significant differences have been
found in how male and female students approach learning tasks. For example, women are more likely than men to
perceive intelligence as an immutable entity that cannot be improved with increased focus on learning tasks (Lips,
2007). This belief may mean that women are more likely to experience negative emotions such as frustration and
confusion, and also experience vicious cycles (D’Mello et al., 2007). In this case, intervention would be necessary to
break students of this cycle and encourage a more dynamic approach to learning.
Personality is an individual’s disposition over a long duration of time, distinguishing itself from emotions or moods
that are more limited in their duration (Rusting, 1998). Using the Big 5 Personality Questionnaire (McCrae & Costa,
2003), personality is divided into five main categories: openness, conscientiousness, extraversion, agreeableness, and
neuroticism. Of particular interest among these are openness, conscientiousness, and neuroticism, as these
characteristics are likely to affect emotion and learning. Additionally, since information on affective states was
obtained through self-report, we expect to find that individuals who score high on openness will display genuine
emotions, while others may limit themselves to what they feel comfortable reporting.
Goal orientation reflects a student’s primary objective when engaged in learning activities. Students may either view
learning in relation to performance or mastery (Elliot & McGregor, 2001). A performance approach would result in a
student’s wishing to prove his or her competence and achieve better results than other students. Students with a
mastery approach, however, view learning as an attempt to gain a skill, regardless of how their ability compares to
others. In addition, students may have avoidance strategies in relation to their goals. For example, students with a
performance-avoidance approach would simply try to not overtly fail, rather than try to top their fellow students.
Presence relates to the level of student involvement within the system (Witmer & Singer, 1998). Students who
experience high levels of presence will be very engaged with the activity, focusing solely on the task while
neglecting their external environment. We expect that these students will experience more salient affective states and
have more intense reactions to events within the system. Additionally, significant differences in transitions between
students who are and are not present may be able to serve as an indicator of presence.

Figure 5. Transitions from frustration to FRustration, FLow, COnfusion, DElight, BOredom, ANXiety,
EXcitement, ANGer, SAdness, and FEar by level of agreeableness
There were significant differences in the frequencies with which male and female participants reported emotions of
boredom. Females (n = 9) did not report feeling bored while the males did, leading to a marginally significant
difference, t(34) = 1.87, p = .07. There were no other significant differences across gender. Student personalities also
affected the frequency with which certain affective states were reported, namely, anger, boredom, confusion, delight,
and flow. There was a significant difference in the frequency of reported states of flow along the extraversion
46

dimension. Students who were more extraverted reported affective states of flow less frequently than less extraverted
students, t(34) = 2.14, p = .04. Also along the extraversion dimension were differences in the frequencies of delight
and anger. Marginally significant was the frequency of which the more extraverted students reported delight than did
the less extraverted students, t(34) = 1.82, p = .07. The more extraverted students reported delight approximately five
times per interaction compared to just two times for the less extraverted students. Anger was reported more
frequently by the more extraverted students than by the less extraverted students, t(34) = 2.77, p = .009.
There were significant differences across the personality dimensions of agreeableness (Figure 5 and Figure 6),
conscientiousness, and neuroticism in reports of confusion. The less agreeable students reported confusion more
frequently (M = 6.06, SD = 1.5) than the more agreeable students (M = 2.36, SD = 1.4), t(34) = 1.77, p = .08.
Similarly, the less conscientious students reported confusion more frequently (M = 6.0, SD = 1.43) than the more
conscientious students (M = 2.0, SD = 1.47), t(34) = 1.94, p = .06. Students with greater emotional stability
(neuroticism dimension) reported confusion more frequently (M = 7.93, SD = 1.48) than the less emotionally stable
students (M = 1.47, SD = 1.2), t(34) = 3.37, p = .001. The final significant difference in emotion frequencies along
personality dimensions is reports of boredom across student agreeableness. The more agreeable students reported
being bored less frequently (M = 0.1 SD = 0.4) than the less agreeable students (M = 2.2 SD = 0.44), t(34) = 3.45, p =
.001.

Figure 6. Transitions from anxiety to FRustration, FLow, COnfusion, DElight, BOredom, ANXiety, EXcitement,
ANGer, SAdness, and FEar by level of agreeableness
Student goal orientation (Figure 7 and Figure 8) also affected the frequency of which students reported anger,
anxiety, and flow. Anger was reported more frequently by students scoring higher on the performance approach
subscale than students scoring below the performance approach population mean, t(34) = 2.28, p = .03. Marginally
significant was the increased frequency with which students who were dominantly performance oriented reported
feeling anxious (M = 3.62, SD = 0.89) than students who were dominantly mastery oriented (M = 1.2, SD = 1.1),
t(34) = 1.71, p = .09. Also significant was the frequency with which students scoring high on the performance
avoidance subscale reported feeling anxious (M = 4.05, SD = 0.87) compared to students scoring below the
performance avoidance population mean (M = 0.8, SD = 1.01), t(34) = 2.43, p = .02. Flow was more frequently
reported by students who were dominantly mastery oriented (M = 18.2, SD = 2.8) than students who were
dominantly performance oriented (M = 10.04, SD = 2.2), t(34) = 2.25, p = .03. The frequency of flow reports was
impacted by students’ performance orientations. Students scoring lower on the performance avoidance subscale
reported more feelings of flow than students scoring above the performance avoidance population mean, t(34) =
2.13, p = .04. Comparatively, students scoring lower on the performance approach subscale reported more feelings of
flow than students scoring above the population mean for performance approach, t(34) = 1.87, p = .07.
47

Figure 7. Transitions from confusion to FRustration, FLow, COnfusion, DElight, BOredom, ANXiety, EXcitement,
ANGer, SAdness, and FEar by dominant goal orientation

Figure 8. Transitions from boredom to FRustration, FLow, COnfusion, DElight, BOredom, ANXiety, EXcitement,
ANGer, SAdness, and FEar by dominant goal orientation

Lastly, there were differences in the frequency of reports of frustration and anxiety across students’ reported sense of
presence (Figure 9 and Figure 10). Students scoring below the population mean of the presence questionnaire
reported frustration with greater frequency than students reporting a greater sense of presence with marginal
significance, t(34) = 1.70, p = .09. Anxiety was reported more frequently by students scoring above the population
mean on the presence questionnaire than by students reporting lower levels of presence, t(34) = 2.23, p = .03.

48

There were few statistically significant differences in affective transitions across individual differences. This is likely
due to a small population size (n = 35) resulting in small split population sizes. However, there are noticeable trends
that may be concretely uncovered in a large-scale study. We report on several of these trend findings below.

Figure 9. Transitions from confusion to FRustration, FLow, COnfusion, DElight, BOredom, ANXiety, EXcitement,
ANGer, SAdness, and FEar by level of reported sense of involvement/control (presence subscale)

Figure 10. Transitions from confusion and boredom to FRustration, FLow, COnfusion, DElight, BOredom,
ANXiety, EXcitement, ANGer, SAdness, and FEar by level of reported sense of involvement/control (presence
subscale)
For example, there are interesting differences in affective transitions when we consider student dominant-goal
orientations. Mastery-oriented students are not likely to stay confused and are most likely to transition to a state of
flow, a finding that suggests that mastery-oriented students are engaged or motivated by the cognitive disequilibrium
associated with confusion. Being in a confused state is associated with a need to learn, and the CRYSTAL ISLAND
environment supports mastery-oriented students’ goal of acquiring knowledge. There is a chance that performanceoriented students may stay confused or transition to negative states such as frustration, boredom, or anxiety. Perhaps
49

this is indicative of the fact that CRYSTAL ISLAND is guiding performance-oriented students into situations where they
must master content to proceed, thus slowing progress and inadvertently decreasing perceived performance. Also, we
notice that bored mastery-oriented students are not likely to remain bored and are more likely to transition to a state
of flow or confusion. These emotional states are thought to be preferred for learning (Craig et al., 2004).
Lastly, there are interesting differences in likely transitions when we consider reported student presence as well. The
participant population was broken into two groups around the population mean for the involvement/control subscale:
low and high. Here we notice that students reporting high levels of involvement are not likely to stay in a state of
confusion and are most likely to transition to a state of flow. On the other hand, students reporting lower levels of
involvement in their experience were likely to stay confused or transition to other affective states, such as frustration
or boredom. We notice a similar trend in transitions from a state of boredom. Students reporting high levels of
involvement are not likely to stay bored and are more likely to become confused, excited, or enter a flow state.
Students reporting lower levels of involvement are somewhat likely to stay bored, but are surprisingly more likely to
transition to flow or delight. However, the occurrences of the vicious boredom cycles may in part be the cause for
lower levels of reported involvement and control due to student disengagement.

Discussion
The analysis of affective state transitions in CRYSTAL ISLAND replicate findings by D’Mello et al. (2007) and Baker
et al. (2007). For instance, the state of flow dominated self-reported affect. The dominance of the flow state has been
reported in a number of affective studies with intelligent learning environments (Baker et al., 2007; Craig et al.,
2004; D’Mello et al., 2007). Frustration and boredom were reported notably less frequently than in D’Mello et al.’s
study (2007) and was comparably reported to frequencies found in Baker et al. (2007). Perhaps surprisingly,
emotions found to be relevant to learning (boredom, confusion, delight, flow, and frustration) were more prevalent
than the narrative affective states (anger, excitement, fear, and sadness) hypothesized to be relevant affective
outcomes to experiencing the CRYSTAL ISLAND story.
Among the most likely transitions were transitions where Next = Current. This was true for the affective states of
frustration, flow, confusion, delight, boredom, anxiety, excitement, and anger. This result also replicates the findings
of D’Mello et al. (2007) and Baker et al. (2007). D’Mello termed these cycles vicious cycles for negative affective
states (similar to Burleson’s notion of “state of stuck” [2006]) and virtuous cycles when students are likely to stay in
positive states (i.e., flow) (2007).
When we consider affective transitions where Next occurs at time t + 1 after an empathetic response from a CRYSTAL
ISLAND character, we notice differences in the likely affective outcomes. For instance, if a student is in a frustrated
state, parallel empathy is likely to elicit a transition in which the student stays frustrated. In contrast, reactive
empathy is less likely than chance to prompt the same vicious cycle. Instead, reactive empathy tends to promote
transitions to a confused state, which is known to have better correlations with learning (Craig et al., 2004).
When we consider likely transitions from the state of flow, we find that parallel empathy is likely to encourage
students to enter a virtuous cycle and remain in the state of flow. Reactive empathy is less likely than chance to
produce the flow state and is likely to promote an affective state transition to confusion. Since a flow state is an
optimal state of experience (Csikszentmihalyi, 1990), it seems reasonable that reactive empathy cannot motivate
students to enter a more engaged state.
Analyzing transition patterns from the state of boredom, we find that parallel empathy is likely to encourage a
vicious cycle, whereas reactive empathy is less likely than chance to produce the same cycle. Instead, reactive
empathy is most likely to transition to flow, with frustration slightly less likely than flow. In the future, when we can
accurately predict when reactive empathy is likely to encourage flow as opposed to when it is likely to promote
frustration, this diagnostic information can inform pedagogical agents’ empathetic responses to alleviate student
boredom and promote a state of flow.
Among the differences between personality traits, those relating to extroversion and conscientiousness are perhaps
the most interesting. Highly extroverted individuals were more likely to report narrative-based emotions such as
anger and delight and less likely to focus on learning or flow. Perhaps these individuals were more focused on the
50

narrative aspects of the environment, such as interacting with characters, and consequently their attention was drawn
away from learning tasks. Additionally, individuals who reported high levels of conscientiousness were less likely to
report experiencing confusion. Generally, conscientious individuals are more likely to regulate their own behavior
and perhaps this led them to focus on finding solutions to resolve their confusion. This notion was also supported by
the increased likelihood of conscientious individuals to transition into flow and the very low likelihood that they
remained confused.
Overall, the trend among affective frequencies shows that increased levels of performance orientation leads to
reduced levels of flow and increased levels of anxiety. This is true when examining students’ dominant orientation as
well as their avoidance and approach subscales. This correlates well with understanding the approaches used by these
two categories. Individuals who are mastery oriented are focused strongly on learning and may therefore be more
likely to immerse themselves in learning-oriented activities in the environment. Similarly, as suggested by the rates
of affective transitions, they may return more quickly to flow after experiences of other affective states. Conversely,
performance-dominant students are focused on their measures of success. The higher level of anxiety reported by
these students may be a direct result of concerns of performance. Because there is no objective measure of
performance in the CRYSTAL ISLAND environment, performance-dominant students may become nervous over
supposed comparison to others and opinions of the researcher present.
Interestingly, differences were found based on individual reports of presence. Students who reported higher levels of
presence were more likely to have been anxious and less likely to have experienced frustration. Perhaps students who
became frustrated disengaged themselves from the environment, resulting in lower levels of presence. Also, students
who were highly engaged may have felt more salient responses to the narrative aspects of the environment. They
may have become more concerned over the wellbeing of the characters and anxious over the outcome of the events.
These differences are especially significant as they suggest that anxiety might be used to indicate measures of
presence. Similarly, it appears that given an objective of maintaining presence, it would be highly important to avoid
frustrating users.

Limitations
It seems likely that the results of this study are influenced by the virtual characters that interacted empathetically
with participants. It is possible that the gender, narrative role, and pedagogical role of the characters may affect the
likelihood of transitions in addition to the type of empathy. Another limitation is that affective states were solely
collected from student self-reports. In contrast, both D’Mello et al. (2007) and Baker et al. (2007) used judged
reports of affect in their transition analysis. In the study reported here, participants’ faces were videotaped during
interactions with the learning environment to permit future work that considers judged reports of affect with this
dataset. Finally, to determine how broadly the results hold, the transitions that were found to be likely with this
subject population need to be validated with other populations, such as the intended population of middle-school
student users.

Conclusion
Given the central of role of affect and motivation in cognitive processes, it is becoming increasingly more important
for intelligent tutoring systems to consider the affective experiences of students. The study reported here replicates
the findings of studies conducted with AutoTutor (D’Mello et al., 2007) and The Incredible Machine simulationbased learning environment (Baker et al., 2007), including a demonstration of the prominence of the state of flow
during learning. By extending our analysis to consider how affective transitions differ given empathetic character
responses, the findings can inform the design of heuristics for pedagogical agents to determine when the use of
empathy is likely to have desired outcomes and what type of empathy (parallel or reactive) would be best utilized.
Such analysis can also inform the utility-induced models of empathy (McQuiggan, Robison, et al., 2008).
The results suggest two directions for future work. First, they call for investigation of what type of feedback
pedagogical agents should consider when empathy does not promote desirable affective states for learning. For
instance, reactive empathy was likely to encourage transitions to either flow or frustration. In instances where
empathy promoted frustration, we should determine why empathy does not work and what type of system response
51

would be more appropriate. Second, analysis of individual differences is necessary to determine the affective
transitions common across a variety of demographics such as gender, but also across learning attributes such as
efficacy, goal orientation, interest, and abilities to self-regulate both learning and affect.

Acknowledgments
The authors wish to thank the members of IntelliMedia Center for Intelligent Systems, Omer Sturlovich, and Pavel
Turzo for use of their 3D model libraries, and Valve Software for access to the Source engine and SDK. This
research was supported by the National Science Foundation under Grants REC-0632450, IIS-0757535, DRL0822200, IIS-0812291, and CNS-0540523. Any opinions, findings, and conclusions or recommendations expressed
in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.

References
André, E., & Mueller, M. (2003) Learning affective behavior. In Proceedings of the 10th Intl. Conf. on Human-Computer
Interaction (pp. 512–516). Mahwah, NJ: Lawrence Erlbaum.
Baker, R., Corbett, A., Koedinger, K., & Wagner, A. (2004). Off-task behavior in the cognitive tutor classroom: When students
“game the system.” Proceedings of ACM SIGCHI Conference on Human Factors in Computing Systems, (pp. 383–390).
Baker, R., Rodrigo, M., & Xolocotzin, U. (2007). The dynamics of affective transitions in simulation problem-solving
environments, Proceedings of the 2nd International Conference on Affective Computing and Intelligent Interactions (pp. 666–
677). Lisbon, Portugal.
Beal, C. & Lee, H. (2006). Creating a pedagogical model that uses student self reports of motivation and mood to adapt ITS
instruction. In Workshop on Motivation and Affect in Educational Software, in conjunction with the 12th Intl. Conf. on Artificial
Intelligence in Education.
Burleson, W. (2006). Affective learning companions: Strategies for empathetic agents with real-time multimodal affective sensing
to foster meta-cognitive and meta-affective approaches to learning, motivation, and perseverance. Ph. D. thesis, Massachusetts
Institute of Technology.
Cohen, J. (1960). A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20, 37−46.
Conati, C., & Mclaren, H. (2005). Data-driven refinement of a probabilistic model of user affect. In Proceedings of the 10th Intl.
Conf. on User Modeling (pp. 40–49). New York: Springer-Verlag.
Craig, S. D., Graesser, A. C., Sullins, J., & Gholson, B. (2004). Affect and learning: An exploratory look into the role of affect in
learning with AutoTutor. Journal of Educational Media, 29(3), 241–250.
Csikszentmihalyi, M. (1990). Flow: The psychology of optimal experience. New York: Harper and Row.
D’Mello, S., Taylor, R. S., & Graesser, A. (2007). Monitoring affective trajectories during complex learning. In Proceedings of
the 29th Annual Meeting of the Cognitive Science Society (pp. 203−208). Austin, TX.
Davis, M. (1994). Empathy: A social psychological approach. Madison, WI: Brown & Benchmark Publishers.
de Vicente, A., & Pain, H. Informing the detection of the students’ motivational state: An empirical study. In Proceedings of the
6th International Conference on Intelligent Tutoring Systems (pp. 933−943). New York: Springer-Verlag.
Ekman, P., & Friesen, W. (1978). The facial action coding system: A technique for the measurement of facial movement. Palo
Alto, CA: Consulting Psychologists Press.
Elliot, A., & McGregor, H. (2001). A 2 x 2 achievement goal framework. Journal of Personality and Social Psychology, 80(3),
501–519.
Forbes-Riley, K., & Litman, D. (2007). Investigating human tutor responses to student uncertainty for adaptive system
development. In Proceedings of the 2nd International Conference on Affective Computing and Intelligent Interaction (pp.
678−689). Lisbon, Portugal: Springer-Verlag.
Gratch, J., & Marsella, S. (2004). A domain-independent framework for modeling emotion. Journal of Cognitive Systems
Research, 5(4), 269−306.
Johnson, L., & Rizzo, P. (2004). Politeness in tutoring dialogs: “Run the factory, that’s what I’d do.” In Proceedings of the 7th
Intl Conf. on Intelligent Tutoring Systems (pp 67–76). New York: Springer-Verlag.
52

Kort, B., Reilly, R., & Picard, R. (2001). An affective model of interplay between emotions and learning: Reengineering
educational pedagogy—building a learning companion. In Proceedings of IEEE Intl. Conf. on Advanced Learning Technology:
Issues, Achievements and Challenges (pp. 43−48). Madison, WI: IEEE Computer Society.
Lester, J., Towns, S., & FitzGerald, P. (1999). Achieving affective impact: Visual emotive communication in lifelike pedagogical
agents. The International Journal of Artificial Intelligence in Education, 10(3−4), 278−291.
Linnenbrink, E., & Pintrich, P. (2001). Multiple goals, multiple contexts: The dynamic interplay between personal goals and
contextual goal stresses. In S. Volet & S. Jarvela (Eds.), Motivation in learning contexts: Theoretical advances and
methodological implications (pp. 251–269). New York: Elsevier.
Lips, H. (2007). Sex and Gender: An Introduction, (6th ed.). McGraw-Hill.
McCrae, R., & Costa, P. (2003). Personality in adulthood: A five-factor theory perspective (2nd ed.). New York: Guilford Press.
McQuiggan, S., Lee, S., & Lester, J. (2007). Early prediction of student frustration. In Proceedings of the 2nd International
Conference on Affective Computing and Intelligent Interaction (pp. 698−709). Lisbon, Portugal: Springer-Verlag.
McQuiggan, S., Mott, B., & Lester, J. (2008). Modeling self-efficacy in intelligent tutoring systems: An inductive approach. User
Modeling and User-Adapted Interaction, 18(1–2), 81–123.
McQuiggan, S., Robison, J., Phillips, R., & Lester, J. (2008). Modeling parallel and reactive empathy in virtual agents: An
inductive approach. In Proceedings of the 7th International Joint Conference on Autonomous Agents and Multi-Agent Systems
(pp. 167–174). Estoril, Portugal: International Foundation for Autonomous Agents and Multiagent Systems.
McQuiggan, S., Rowe, J., & Lester, J. (2008). The effects of empathetic virtual characters on presence in narrative-centered
learning environments. In Proceedings of the 2008 SIGCHI Conference on Human Factors in Computing Systems (pp. 1511–
1520). Florence, Italy: ACM.
Paiva, A., Dias, J., Sobral, D., Aylett, R., Woods, S., Hall, L., & Zoll, C. (2005). Learning by feeling: Evoking empathy with
synthetic characters. Applied Artificial Intelligence, 19, 235−266.
Perry, N. (1998). Young children’s self-regulated learning and the contexts that support it. Journal of Educational Psychology, 90,
715−729.
Porayska-Pomsta, K. & Pain, H. (2004). Providing cognitive and affective scaffolding through teaching strategies. In Proceedings
of the 7th International Conference on Intelligent Tutoring Systems (pp. 77−86). New York: Springer-Verlag.
Prendinger, H., & Ishizuka, M. (2005). The empathic companion: A character-based interface that addresses users’ affective
states. Applied Artificial Intelligence, 19, 267−285.
Rusting, C. (1998). Personality, mood, and cognitive processing of emotional information: Three conceptual frameworks.
Psychological Bulletin, 124(2), 165−196.
Wang, N., Johnson, W. L., Mayer, R., Rizzo, P., Shaw, E., & Collins, H. (2008). The politeness effect: Pedagogical agents and
learning outcomes. International Journal of Human Computer Studies, 66, 98−112.
Witmer, B., & Singer, M. (1998). Measuring presence in virtual environments: A presence questionnaire. Presence:
Teleoperators and Virtual Environments, 7(3), 225–240.

53

ISSN 1436-4522 (online) and 1176-3647 (print). © International Forum of Educational Technology & Society (IFETS). The authors and the forum jointly retain the
copyright of the articles. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that copies bear the full citation on the first page. Copyrights for components of this work owned by
others than IFETS must be honoured. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior
specific permission and/or a fee. Request permissions from the editors at kinshuk@ieee.org.

54

Yeh, H.-C., Yang, Y.-F., & Wong, W.-K. (2010). Interaction Chain Patterns of Online Text Construction with Lexical Cohesion.
Educational Technology & Society, 13 (1), 55–68.

Interaction Chain Patterns of Online Text Construction with Lexical Cohesion
Hui-Chin Yeh1, Yu-Fen Yang1 and Wing-Kwong Wong2
1

Graduate school of applied foreign languages and 2Department of Electronic Engineering, National Yunlin
University of Science & Technology, Douliu, Yunlin, Taiwan, R. O. C. // hyeh@yuntech.edu.tw //
yangy@yuntech.edu.tw // wongwk@yntech.edu.tw

ABSTRACT
This study aims at arousing college students’ metacognition in detecting lexical cohesion during online text
construction as WordNet served as a lexical resource. A total of 83 students were requested to construct texts
through sequences of actions identified as interaction chains in this study. Interaction chains are grouped and
categorized as a meaningful entity in order to investigate the students’ thinking process and behavior in general
and to understand the interaction between the computer and the students in particular. From the interaction
chains, it was found that some students revised incorrect sentences to correct ones. In making correct revision,
they needed to assess incoming information, interpret and organize textual information, engage in thinking what
they know, monitor their own meaning construction process, and take remedial actions to reach comprehension.
The rate of correct sentence selection increased from 34.04% to 55.02% in three sequential text construction
tasks. The recognition of lexical cohesion was found to be a determining factor for successful construction of a
text.

Keywords
Interaction patterns; Lexical cohesion, Reading comprehension, Metacognition, Text construction

Introduction
Lexical cohesion has been widely defined as a property of text to connect sentences. According to Halliday and
Hasan (1976), cohesive devices present a necessary semantic continuity between sentences for the purpose of
interpreting and comprehending a text. The interpretation of each sentence in a text may depend on both the selection
of vocabulary and its cohesive relations with other sentences. Halliday and Hasan further address that “typically, in
any text, every sentence except the first exhibits some forms of cohesion with a preceding sentence, usually with the
one immediately preceding” (p. 293). They also point out that “cohesive ties between sentences stand out more
clearly because they are the only source of texture…..it is the inter-sentence cohesion that is significant, because that
represents the variable aspect of cohesion, distinguishing one text from another” (p. 9). As such, lexical cohesion is
derived from the selection of vocabulary items. Lexical cohesive ties, namely, pairs of cohesive words, provide a
context for identifying the connections between the concepts embedded in a text. Hasan (1984) and Hoey (1991)
suggested that 40 to 50 percent of cohesive ties in a text are composed of lexical cohesion. It is worth further
investigation to what extent lexical cohesion can help students construct meaning in a text.
Many studies have affirmed the significance of the role that lexical cohesive ties play in the comprehension of a text
(Bridge & Winograd, 1982; Chapman, 1982; Rogers, 1974; Staddord, 1991; Nunan, 1993, Nunan, 2004; McCarthy,
1991, Wang, 1998). Nunan (1993) stresses the fact that the ability to detect the cohesive relationships across
sentence boundaries is significant for students to comprehend a text. When the sequence of sentences are scrambled
or altered, the meaning of the text is surely distorted or even radically changed. According to the results of
Bensoussan and Laufer’s study (1984), the major reading difficulty that English as a Second Language (ESL) or
English as a Foreign Language (EFL) college students encountered was their failure in recognizing the connections
among the sentences in a text. Along this line of research, Chu, Swaffar, and Charney (2002) pointed out that most
Taiwanese EFL students were found to be less aware of cohesive devices when reading English texts, as they occur
less often in Chinese. In other words, Taiwanese EFL students rarely use cohesive devices for integrating textual
information (Chen, 2003; Sharp, 2003). Their difficulty in identifying cohesive ties and finding out the relationships
among these devices in a text lowers their English proficiency.
Some researchers suggest that learners should take an active role and obtain a metacognitive ability to manage their
learning for better effectiveness (El-Hindi, 1997; Yang, 2002). Particularly, computer-assisted language learning
(CALL) environment has been reported to have had a positive impact on learners’ learning process, because the ease
of text manipulation facilitates the revelation of cognitive processes (Dewitt, 1996; Forbes, 1996; Hirvela, 2004,
2005). Some studies also indicate that CALL environment can help explore and facilitate students’ thinking and
ISSN 1436-4522 (online) and 1176-3647 (print). © International Forum of Educational Technology & Society (IFETS). The authors and the forum jointly retain the
copyright of the articles. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that copies bear the full citation on the first page. Copyrights for components of this work owned by
others than IFETS must be honoured. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior
specific permission and/or a fee. Request permissions from the editors at kinshuk@ieee.org.

55

critical learning skills (Dreyer & Nel, 2003; Sinclair, Renshaw, & Taylor, 2004; Yeh & Lo, 2001; Yeh, 2003)
because the computer “is a good tool to expand human cognitive development and knowledge construction” (Yeh,
2003, p. 613). In the past, it was extremely challenging to study either a learner’s thinking process, except with a few
labor-intensive and time-consuming methods: naturalistic observations, interviews, or think-aloud protocols
(Schacter, Herl, Chung, Dennis, & O’Neil, 1999). When these methods were used, the learners’ thinking and
learning processes were often disrupted so that the data obtained might be distorted as well.
This study reports on our design of a text construction system, in which EFL college students actively construct and
reconstruct text meanings among sentences. The constructing and reconstructing process also rely much on the use of
metacognitive abilities perceived as formative assessment skills or the ability to “think about thinking” (Abromitis,
1994; Underwood, 1997; Kolić--Vehovec & Bajsanski, 2001). Metacognition involves active control over the
cognitive processes engaged in learning” (Livingston, 1997, p. 1), and "active monitoring and consequent regulation
and orchestration of cognitive process to achieve cognitive goals" (Flavell, 1976, p. 252). By exercising
metacogntive skills, learners are in control of their learning process in assessing the new information they read, and
retrieving the knowledge needed for understanding the written text (Abromitis, 1994).
For example, during reading, readers are monitoring their comprehension process that activates their prior
knowledge, their failures of comprehension, and the strategies that can help them understand the text (Kolić-Vehovec
& Bajsanski, 2001; Brown, 1985). They will in turn attempt to modify their reading process to match their purpose
of understanding the text. In other words, they have to activate their metacognition by firstly acquiring the concepts
from the text while adjusting the concepts in the light of subsequent information (Baker & Brown, 1984a, 1984b;
Yang, 2002).
Metacognition refers to the consciousness of people when they are aware of monitoring and regulating their
cognitive activities in the process of performing a cognitive task (Baker & Brown, 1984a, 1984b). They should be
“monitoring ongoing activities to determine whether comprehension is occurring” and “taking corrective action”
(Baker & Brown, 1984a, p. 354). When encountering problems, metacognitive learners can address the problems
consciously, and act remedially. As learners “recognize mistakes and inconsistencies in texts and understand that
they impair readers’ comprehension” (Ruffman, 1996, p. 33), and “take remedial action” (Yang, 2006, p. 67) to
resolve the inconsistencies, they are actually engaged in metacognition to improve their proficiency. The learner
should be continuously encouraged to detect inconsistency and to make revision in order to achieve full
comprehension.
Brown (1987) also specifically characterized four components in metacognition: planning, monitoring, evaluating,
and revising. Planning refers to the deliberate activities that organize the entire learning process, including setting the
goal, sequence, strategies, and expected time for learning. Monitoring involves the activities that moderate the
current progress of learning. Evaluating one’s own learning process involves an assessment of the current progress of
the activity. This evaluation can assist learners to develop the necessary skills and strategies. Revising one’s own
learning process refers to the modifications of previous strategies related to goals and other possible learning
approaches. These four components of metacognition all lead to the improvement of comprehension.
Based on our design of the computer-based text construction task, students’ intensive engagement of metacognition
can be expected. At first, students will use tutoring and trial practice to get familiar with both the lexical cohesion
and the computer-based learning environment. This is the process of planning in this system for students have to be
aware of the ultimate goal of the text construction task and of what they are expected to do next. On the way to insert
sentences to construct a text, they will be required to first identify the lexical cohesive items, select types of lexical
cohesion, and select inter-sentential relations since these are key elements to establish a paragraph. The selection of
lexical cohesion types and sentence relation is defined as the process of monitoring in this system for students do not
have to achieve the ultimate goal of inserting a correct sentence. Instead, students merely need to monitor if they are
on the right track to approach the ultimate goal. The process of evaluating occurs when students have to evaluate
four optional sentences and select a correct one to be related to the previous sentence in a text. This process differs
from that of monitoring since correct sentence insertion is students’ ultimate and only goal in text construction. After
the submission of a sentence, students are allowed to revise their sentences. This is the process of revising in this
system. These four components make it possible to investigate the students’ comprehension and learning process
while students are required to exercise their metacognitive ability to identify lexical cohesive ties and inter-sentential
relations and select the correct sentence to complete the text construction task. The system is designed to serve as a
56

cognitive tool for a student to assess incoming information, interpret and organize textual information, engage in
thinking what he knows, monitor his own meaning construction process, and take remedial action to reach
comprehension. When constructing a text and identifying lexical cohesion in the computer system, the student
undergoes a dynamic and recursive process to reestablish the consistency of the text, while simultaneously searching
the lexical cohesion between sentences to achieve comprehension. The system aims to enhance his metacognition
and make his thinking process visible to both himself and the teacher.
The learning system used in this study is also a computer-supported interaction environment. With the Recording
module, the system would trace and document each action a student takes in order to understand his thinking
process. The student’s metacognition could then be further analyzed as he makes revision while comprehending the
text. The ability to determine “what has been done right or wrong,” and “to take remedial action when
comprehension failures occur” is a self-regulatory behavior (Yang, 2006, p. 67). Based on this definition, the
sequences of actions students take in this study are identified as interaction chains. The interaction chains are
grouped and categorized as a meaningful entity in order to investigate a student’s thinking process and behavior in
general and to understand the interaction between the computer and the student in particular. Several patterns are
used to explain both the students’ monitoring process and what facilitates or hinders their comprehension. The
interaction chain patterns can be used by a teacher to examine the students’ comprehension process. They also
provide the details of how the students utilized the system’s scaffolding to improve their metacognition.

Method
Participants
A total of 83 freshman students were recruited from two EFL classes in a technological university in central Taiwan.
These two classes were from two different departments, English and Engineering. The distribution of students is
shown in Table 1.

Department

Table 1. The distribution of students
Number of participants

Class A

English

41

Class B

Engineering

42

Gender
Male: 8
Female: 33
Male: 37
Female: 5

A total of 83 students participated in this study. Class A was comprised of eight males and thirty-three females and
Class B was comprised of thirty-seven males and five females.

Material
In order to provide appropriate English texts for the EFL college students to construct, the study adopted the
following criteria for the selection of the texts for the text construction task. First, the readability level of the English
texts should be controlled. The texts were all selected from College Reading Workshop (Malarcher, 2005). Second,
the texts should be similar in length. Three texts were chosen: The Best Travel Bargains (Text 1), with 168 words, 8
sentences and 6 lexical cohesive pairs; Traditional Markets VS. Modern Markets (Text 2), with 206 words, 11
sentences and 15 lexical cohesive pairs, and Fat to Store or Fat to Burn (Text 3), with 188 words, 10 sentences and
13 lexical cohesive pairs.

System architecture
The system built for this study includes three modules: user interface, recording module, and feedback module. The
relationships among these three modules are presented in Figure. 1. The teacher sets the objectives of the course,
selects the appropriate texts and enters the texts into a database through the teacher interface. The recording module
57

documents students’ constructive behavior and process. The lexical cohesion and relations of sentences that are
identified will be recorded in a database. The feedback module looks up WordNet, matches the lexical cohesive
items, and provides candidate words back to students when they have a hard time identifying the lexical cohesion.
These modules will be discussed in detail.

Figure 1. System architecture

User Interface
The user interface includes a teacher interface and a student interface. The teacher interface allows the teacher to
manage a course, provide the texts to be constructed, and analyze the students’ constructive process and behavior.
The student interface is where a student undertakes text construction tasks.

Figure 2. Student interface
As shown in Figure 2, the student interface is divided into five areas. The main text area (Ⅰ) presents the text that is
being constructed. An “insert” button tagged with a serial number represents a missing sentence that a student should
58

identify based on the first and the last sentences (in boldface) of a paragraph. A “Revise” button allows the student to
revise before final submission. Next, the area of multiple-choice items (Ⅱ) provides four sentences, tagged A, B, C,
and D for the student to select from. Three distracting sentences were randomly picked from the rest of the text
except the first and the last sentences. In the area of lexical cohesion (Ⅲ), students are asked to fill in two cohesive
items from the current sentence and its preceding sentence. They can get hints of cohesive words by clicking the
“Search” buttons or “Answer” buttons that are created based on the number of cohesive word-pairs embedded in the
two sentences.
In the area of cohesion types (Ⅳ), students have to fill in the relations between the two cohesive items from the text
fields A and B. The lexical cohesion types include: repetition, synonym, hypernym, hyponym, meronym. From the
menu of inter-sentential relations, students have to choose the relation between the two sentences. The relations
include: addition, clarification, comparison, contrast, example, location or spatial order, cause and effect, summary
and time (Sharp, 2003).

Recording Module
The system uses a recording module to trace students’ constructive process and behavior, which is presented as
interaction chains. The teachers can analyze the interaction chains and identify the difficulties students encounter and
the different performance among various proficiency groups. The analysis of interaction chains reveal whether the
students make good use of the scaffolding in identifying lexical cohesions during text construction. The records are
also helpful for the teacher to modify his instruction according to the demonstrated strengths and weaknesses of the
students. The module uses some predicates to record students’ behavior data (Table 2).
Table 2. The predicates for recording students’ data
Description
Search(W, S, t)
use WordNet search word W and type t at sentence S
Insert a sentence [n] (S)
insert a sentence s at sentence n
Identify cohesive words(w1, w2, t)
Identify cohesive words w1 and w2 and cohesive type t
Select relation(T)
Select inter-sentential relation T
Answer(S)
Provide a pair of lexical items as Answer for sentence S
Revise [n] (S, T)
Sentence n is revised from sentence S to sentence T
Predicates

An example of how the recording module traces students’ learning process of using WordNet to search for
hypernyms of cost, inserting sentence 1, identifying lexical cohesion and selecting sentence relation is shown in
Table 3.

Explanation
1.
1.Sentence:X

2.

2.Cohesion:0/2

3.
4.
5.

3. Relation:O

Table 3. Records of a student’s construction process
Sequence of interaction chain
Search (“cost”, “The best way to get a cheap airline ticket used to be by reserving a ticket
at least 21 days before you planned to travel.”, “Hypernym”)
Insert a sentence: [The best way to get a cheap airline ticket used to be by reserving a
ticket at least 21 days before you planned to travel.]
Identify cohesive words_1: [travel, travel, Repetition]
Identify cohesive words_2: [plan, plan, Repetition]
Select relation: [Addition]

Feedback Module
Among the above lexical cohesion, repetition, synonym, hypernym, hyponym, and meronym are found most
commonly in WordNet. WordNet 2.1 (Miller et al., 2005) is used in the current system to find these five types of
lexical cohesion. It also assists the student to identify the relationships between given lexical cohesive items.
WordNet is an online lexical database with a hierarchical structure, where a node is a synset (a set of synonyms) and
59

is linked to other nodes with some relationship. WordNet is thus adopted in the current computer system as the
knowledge source for the lexical semantic relationship utilized in matching cohesive words. In the past, WordNet has
been utilized as a corpus to analyze discourse automatically. It has been rarely used in a learning systems to assist
learners to comprehend a text.
In this study, a “SEARCH” button was designed to scaffold the student’s text construction. The candidate lexical
cohesive words generated from WordNet are shown in red color in both the main text and the multiple-choice items.
For example, when the student enters the word “Brazil” in text field A and chooses hypernym as the type of lexical
cohesion, the cohesive words “Brazil” and “country” found in WordNet will be highlighted in the text (Figures 3 and
4).

Figure 3. An example of using SEARCH button

Figure 4. Search results of “Brazil” (hypernym)

The student can click the “ANSWER” button to get a pair of lexical cohesive items, Brazil and country, when he
encounters difficulty in figuring out the cohesive items (Figure 5), but he still needs to decide which type of lexical
cohesion these two words form.

Figure 5. Example of using ANSWER button

After the student finishes constructing the text, the system will match the student’s inserted sentences, cohesion, and
sentential relation with the correct answers. The system takes a few steps to match the correct answers. First, when
60

the student inserts sentence S, the system would match the answer to the target text. Next, after the student identifies
lexical cohesive items (word 1 and word 2), the system would match the items against the lexical cohesion list.
Finally, after the student selects the sentence relation R, the system would match the selected relation with the target
answers.

Procedures of Data Collection
Eight three students were asked to construct three texts online at three different periods of time with one-month
intervals. Before the instructor introduces the strategy, identifying lexical cohesion and sentence relation in text
comprehension, the students undertook the text construction task for the first time. They were asked to complete the
second text after strategy instruction for a month and to finish the final text after instruction for two months. The
system recorded and traced the students’ construction process. The text construction tasks were to identify the target
sentence from the multiple choices, pairs of lexical devices and their cohesion type, and finally the sentential
relation. The online computer system graded the students’ text-construction by giving one score point to each correct
answer of (1) sentences of the constructed text, (2) pair of cohesive words, (3) type of lexical cohesive ties, and (4)
sentence relation.

Procedures of Data Analysis
The collected data were analyzed in terms of the students’ text construction product and process. Text construction
product refers to the students’ overall scores. Text construction process includes the students’ constructive process
and behavior traced by the system. Revision and no-revision behaviors were categorized into different interaction
chain patterns. The trace results revealed the students’ difficulty in the constructive process and how they improved
their understanding of the text.

Results
Product of Text Construction
The current study focused on understanding the students’ cognitive product and process in constructing online texts.
The data of 83 students’ performances on Text 1 (The Best Travel Bargains), Text 2 (Traditional Markets VS.
Modern Market), and Text 3 (Fat to Store or Fat to Burn) were analyzed. The overall reading performance is
presented in Table 4. The total score for sentence selection in Text 1 was 4 and that for lexical cohesive ties was 6.
The total score for sentence selection in text 2 was 7 and that for lexical cohesive ties was 17. The total score for
sentence selection in text 3 was 6 and that for lexical cohesive ties was 13.
Table 4. Results of the Correct Sentence Selection
Text 1
Text 2
Participant
Css*mean
SD
Css* mean
SD
1.36/4
3.80/7
All students
1.37
2.11
(34.00%)
(54.28%)
2.12/4
5.39/7
English major
1.5
2.01
(53.00%)
(77.00%)
0.77/4
2.77/7
Engineering major
1.24
2.21
(19.25%)
(39.57%)
*Css: Correct sentence selection

Text 3
Css* mean
3.30/6
(55.00%)
3.70/6
(61.67%)
2.61/6
(43.5%)

SD
1.69
1.45
1.93

As shown in Table 4, almost all the students made progress in three sequential text construction tasks as the
percentage of correct sentence selection increased from 34.04% to 55.02%. Specifically, the developmental progress
was evident for the engineering students. Their percentage of correct sentence selection increased from 19.25% to
43.5%.
61

Table 5. Results of the Correct Lexical Cohesive Ties
Text 1
Text 2
Participant
Clct** mean
SD
Clct** mean
SD
1.16/6
4.00/17
All students
1.19
2.30
(19.33%)
(23.53%)
1.64/6
5.52/17
English major
1.10
2.11
(27.33%)
(32.47%)
0.87/6
3.26/17
Engineering major
1.28
2.49
(14.50%)
(19.18%)
**Clct: Correct lexical cohesive ties

Text 3
Clct** mean
4.96/13
(38.15%)
5.55/13
(42.69%)
4.81/13
(37.00%)

SD
2.98
2.66
3.3

This was also true for their performance in lexical cohesive ties. The engineering students progressed from 14.5%
(Text1) to 37% (Text 3). The English-major students also made progress from 27.33% (Text1) to 42.69% (Text 3).

Process of Text Construction
According to the design of this system, the student must follow the steps to construct and reconstruct the text. One of
the major functions in the system is the “Revise” button, which encourages the students to activate their
metacognition. The steps of the students’ revision and no-revision processes are shown in Figure 6. If students
detected an inconsistency between the sentences, they would revise their thinking in order to reach new
understanding. Otherwise, they did not undertake revision. For the step of “search cohesive items,” students can
choose to use or not to use WordNet to search for cohesive items before completing the task.

Figure 6. Revision and no-revision processes

Table 6. Three revision patterns
Pattern
A

Incorrect→Correct

B

Incorrect→Incorrect

C

Correct→Incorrect

Description
An incorrect sentence was revised correctly with correct
identification of lexical items and their types of lexical cohesion.
An incorrect sentence was revised incorrectly with incorrect
identification of lexical items and their types of lexical cohesion.
A correct sentence was revised with incorrect identification of
lexical items and the types of lexical cohesion.
62

Patterns of Revision in the Text-construction Task
The students clicked on the “Revise” button to revise a sentence. The way a sentence could be revised can be
grouped into three different interaction chain patterns (Table 6). In a sample text given below, the first and the last
sentences of the paragraph were provided initially while sentence 1 and sentence 2 were inserted by a student.
Sentence 1 was used to illustrate the three major revision patterns.

Sample text
One reason that finding good prices for travel is so complicated is because airlines have complex formulas for
inventory management so they can maximize profits by filling plans. 1 When there are lots of reservations (during
peak seasons), these companies can charge higher prices and still be sure that somebody will need their services no
matter how much it costs. 2 On the other hand, during the off-peak season, demand is low, so companies cut their
prices to try and attract people who would normally not travel at that time. One good place in which to find these
last-minute bargains is on the Internet.
In Patten A, the students revised an incorrect sentence to the correct one by detecting the lexical items and the types
of lexical cohesion. Figures 8 and 9 shows an example for Pattern A taken from the record of a student.

Figure 7. Interaction chain of Pattern A

Figure 8. A graphical illustration of Figure 7
63

In Figure 7 the first three actions the student took for his text-construction activities were to insert sentence (action
13), identify lexical cohesion (actions 14, 15) and select sentence relation (action 16). These three actions constituted
a meaningful entity (circled in an oval shape with a dotted line in Figure 9). Before final submission, he reread the
sentence and searched for lexical items to confirm his ideas (action 18). Yet, he was aware of the inconsistency
between the sentences. In the construction process, the student asked for help in his text comprehension and
searching for the candidate lexical items. Thus, he revised both the sentence and the lexical cohesion correctly
(actions 19-21) after the search.
As shown in the graphical illustration of Figure 8, before the student took remedial actions 17 and 18 to obtain the
correct answers, he underwent a rereading process to reconsider his first answers. It is evident that the ability to
identify the key lexical cohesion contributed to the student’s comprehension of the text.

Figure 9. A graphical illustration of Pattern B
In Pattern B, the student revised the incorrect sentence to another incorrect one because he could not identify the
lexical items and the types of lexical cohesion correctly (Figure 9). The student did not use the “Search” function.
Though he detected the inconsistency between sentences, he did not get the correct sentence. He would have a better
chance getting the correct answer if he used the system to help detect the correct lexical cohesion.
With regard to Pattern C, the student replaced the correct sentence with an incorrect one because he could not
identify the lexical items and the types of lexical cohesion correctly no matter whether they revised or not (Figure
11). Though the student revised the sentence without asking for the assistance, the student did not fully understand
the text or the lexical cohesion.
D
E

Correct
Incorrect

Table 7. Two no-revision patterns
A student selects the correct sentence and fills in correct lexical cohesion
Student selects the incorrect sentence and fills in incorrect lexical cohesion

Among the students who did no revision, Pattern D characterizes those who selected the correct sentence and filled
in the correct cohesion while Pattern E characterizes those who selected the incorrect sentence and filled in the
lexical cohesion incorrectly (Table 7).
The student in Pattern D selected the sentence and lexical cohesion correctly. The student did not undergo the
complex reading process as those in the revision group. The student in Pattern E apparently did not detect the
inconsistency between sentences. He neither asked for feedback nor revised the sentence. Only when the student can
identify the inconsistency between sentences can they take remedial actions as reading strategies to fix the problem.
In summary, five different patterns emerged to classify how students behaved in their learning process and what
fostered or hindered their comprehension.
The relationship between metacognition and students’ learning outcomes
As Trainin and Swanson (2005) mentioned, the employment of metacognitive strategies is positively linked to
students’ learning outcomes in academic environments. In this study, all participants’ correctness rate in revision is
shown in Table 8. It could be seen that students learned to make more revisions from text 1 to text 3. As they made
64

more revisions, their correctness rate in selecting correct sentences of text construction also increased from 34.04 %
to 55.02% (see Table 9). The result of the Pearson product-moment correlation coefficient between revision and
correct sentence selection is shown in Table 10. The correctness rate of revision has a positive correlation with that
of sentence selection. That is, the more the participants revised their sentences, the higher scores they obtained in text
construction.

Number of Students
Percentage of correct revisions

Table 8. Students’ correctness rate in revision
Text 1
Text 2
83
83
26 %
31.5 %

Table 9. Students’ correctness rate in inserting sentences
Text 1
Text 2
Participant
Css*
Css*
mean
mean
1.36/4
All students
3.80/7 (54.28%)
(34.00%)
2.12/4
English major
5.39/7
(77.00%)
(53.00%)
0.77/4
2.77/7
Engineering major
(19.25%)
(39.57%)
*Css: Correct sentence selection

Table 10. The correlation between revision and correct sentence selection
Text 1
Text 2
N
PC
PC
83
.70
.76
*N: the number of participants
*PC: Pearson’s correlation

Text 3
83
42.6%

Text 3
Css*
mean
3.30/6
(55.00%)
3.70/6
(61.67%)
2.61/6
(43.50%)

Text 3
PC
.81

All these consciousness-raising requests designed in the system aim to arouse students’ metacognition so that they
can improve their cognitive process. That is, in each text construction and reconstruction, the previous process served
as a cognitive stimulus for the next process. The positive result might derive from whether students can actively take
on revision and search for the clues, such as lexical cohesion, in WordNet to facilitate their comprehension. Without
metacognition, students might remain in the original status of cognitive process for an extensive time without further
improvement.

Conclusion
In this study, the text construction system served as a medium for the students to monitor and regulate their
comprehension and for the teacher to understand the students’ learning behavior and process. The system is designed
to encourage the learners to use their metacognition by identifying the lexical cohesive ties across sentences and the
types of lexical cohesion. Some conclusions can be drawn from both the students’ text construction product and
process. The students made progress in three sequential text construction tasks as the percentage of correct sentence
selection increased from 34.04% to 55.02%. The improvement for their performance in lexical cohesive ties is also
evident as the percent of correct lexical cohesive ties increased from 19.28% to 38.18%. In the process of text
construction, five different interaction patterns emerged to characterize not only how the system supports the
student’s learning but also what processes the learners undergo while completing the required tasks. For the revision
group, three different patterns described how students revised from incorrectness to correctness, from incorrectness
to incorrectness, and from correctness to incorrectness. For the no-revision group, two different patterns

65

characterized: (1) how the student selected a correct sentence and filled in the correct lexical cohesion and (2) how
the student selected an incorrect sentence and filled in an incorrect lexical cohesion.
In the system, a Recording module traced and documented the student’s learning process and behavior in general and
their revision actions in particular. Interaction sequences presented in a chain manner illustrate the students’
metacognitive process. Different patterns generated from the data informed the teacher of how the students activated
their awareness to detect the inconsistency in sentences and how they took action to repair their understanding.
Through the construction tasks, the students had to carefully access incoming information by reading the preceding
sentence and the options from the multiple choices. The students also had to interpret and organize those incoming
information by reading between lines. When they were requested to identify the lexical cohesive items in text, they
were engaged in thinking what they knew and building the consistency between sentences. In this engagement, they
would try to monitor their constructive process and detect the inconsistency. From the findings, we can also learn
that the previous process of either taking on revision or asking for assistance from WordNet served as a cognitive
stimulus for the next process. There is a strong correlation between revision and correct sentence selection. In other
words, successful comprehension might result from whether students can actively take on revision and search for the
clues, such as lexical cohesion, in WordNet to facilitate their comprehension. Without metacognition, students might
remain lost in their cognitive process and would not be able to take any step to mend any comprehension breakdown.
Finally, if there were any comprehension failures, the students had the autonomy over taking remedial actions or not.
The students’ difficulties through the construction process in detecting lexical cohesion were examined and analyzed.
It was found that the recognition of lexical cohesion was a determining factor in successful construction of a text.
Whether or not the students could detect the lexical cohesion was a crucial factor in facilitating their learning process
and comprehension. It was also found that more proficient students tended to make good use of the feedback to
understand the connections in the text, and less proficient students seldom used the scaffolding provided by the
system. Future study would focus on the different interaction chains generated from students at different proficiency
levels in order to understand their text construction behavior and process, and explore the issues of how students at
different levels of proficiency benefit from the current system.
Based on the analysis and generalization from the trace results, the teacher could assist the students in overcoming
their difficulty not only in text construction per se but the lexical cohesion itself. Different interaction chain patterns
can possibly allow the teacher to analyze the students’ metacognitive process and understand what factors hinder
their successful text construction. The trace results provided by the system would serve as a guide for the teacher to
prepare for the instructional materials and effective teaching approaches. Their ability to identify correct lexical
cohesion as well as their metacognition were critical for successful text construction and comprehension. It is hoped
that this system can be used for a variety of course designs in additional to EFL courses because university courses
make extensive use of academic materials written in English (Carrell, 1998).
The teacher can select appropriate texts for the students to construct their meanings through the text construction
task. The students are encouraged to read across sentences instead of solely focusing on individual sentences in
detecting lexical cohesion. While completing the tasks, the students need to activate their metacognition and detect
the lexical cohesion across sentences. In future studies, we can generalize this approach to other types of cohesion
mechanisms, such as anaphora. When students encounter any difficulty or confusion, they are encouraged to ask for
feedback from the system to scaffold their comprehension. Then the scaffolding provided by the system can guide
the students to achieve full reading comprehension.

Acknowledgement
This study was supported by National Science Council in the Republic of China, Taiwan (NSC 97-2410-H-224-017MY2). The research grant made the continuation of this study possible.

References
Abromitis, B. (1994). The role of metacognition in reading comprehension: implications for instruction, Literacy Research and
Report Number 19, Dekalb, MI: Curriculum and Instruction Reading Clinic, Northern Illinois University.
66

Baker, L., & Brown, A. L. (1984a). Cognitive monitoring in reading comprehension. In J. Flood (Ed.), Understanding reading
comprehension, Newark, DE: International Reading Association, 21-44.
Baker, L., & Brown, A. L. (1984b). Metacognitive skills and reading. In P. D. Pearson (Ed.), Handbook of reading research, New
York: Longman, 353–394.
Bensoussan, M., & Laufer, B. (1984). Lexical guessing in context in EFL reading comprehension. Journal of Research in
Reading, 7, 15-32.
Bridge, C., & Winograd, P. (1982). Readers’ awareness of cohesive relationships during cloze comprehension. Journal of
Reading Behavior, 14, 299-312.
Brown, A. L. (1985). Metacognition: The development of selective attention strategies for learning form texts. In H. Singer & R.
B. Ruddell (Eds.), Theoretical models and processes of reading (3rd Ed.), Newark, DE: International Reading Assoc, 501-526.
Brown, A. L. (1987). Metacognition, executive control, self-regulation, and other more mysterious mechanisms. In F. E. Weinert
& R. H. Kluwe (Eds.), Metacognition, motivation, and understanding, Hillsdale, NJ: Lawrence Erlbaum, 65-116.
Carrell, P. L. (1998). Introduction: Interactive approaches to second language reading. In P. L. Carrell, J. Devine & D. Eskey
(Eds.), Interactive approaches to second language reading, Cambridge: CUP, 73-100.
Chapman, J. (1982). A Study in Reading Development: A Comparison of the Ability of 8, 10 and 13 Year Old Children to
Perceive Cohesion in Their School Texts. Paper presented at the 19th Annual Conference of the United Kingdom Reading
Association, July 19-23, Newcastle-upon-Tyne, England.
Chen, Y. J. (2003). On-line metacognitive enhancement program for essay writing. In Selected papers from the 12th International
Symposium on English Teaching, Taipei: The Crane Publishing, 338-347.
Chu, H. C. J., Swaffar, J., & Charney, D. H. (2002). Cultural representations of rhetorical conventions: The effects on reading
recall. TESOL Quarterly, 36 (4), 511-541.
Dewitt, S. L. (1996). The current nature of hypertext research in computers and composition studies: An historical perspective.
Computers and Composition, 13, 69-84.
Dreyer, C., & Nel, C. (2003). Teaching reading strategies and reading comprehension within a technology-enhanced learning
environment. System, 31 (3), 349-365.
El-Hindi, A. E. (1997). Connecting reading and writing: College learners’ metacognitive awareness. Journal of Developmental
Education, 21 (2), 10-17.
Flavell, J. H. (1976). Metacognition aspects of problem-solving. In L. B. Resnick (Ed.), The Nature of Intelligence, Hillsdale, NJ:
Erlbaum, 231-235.
Forbes, C. (1996). Cowriting, overwriting, and overriding in portfolio land online. Computers and Composition, 13, 195-205.
Halliday, M. A. K., & Hasan, R. (1976). Cohesion in English, London: Longman.
Hirvela, A. (2004). Connecting reading and writing in second language writing instruction, Ann Arbor, MI: University of
Michigan Press.
Hirvela, A. (2005). Computer-based reading and writing across the curriculum: Taiwan case studies of L2 writers. Computers and
Composition, 22 (3), 337-356.
Hoey, M. (1991). Patterns of lexis in text, Oxford: Oxford University Press.
Kolić-Vehovec, S., & Bajsanski, I. (2001). Children's metacognition as predictor of reading comprehension at different
developmental levels, retrieved May 1, 2009, from http://www.eric.ed.gov/ERICWebPortal/contentdelivery/servlet/ERICServlet?
accno=ED456422.
Livingston, J. A. (1996). Effects of metacognitive instruction on strategy use of college students, Unpublished manuscript,
Buffalo, NY: State University of New York.
Malarcher, S. (2005). Family Planning Success Stories in Sub-Saharan Africa Technical Brief. The INFO Project, Baltimore,
MD: Johns Hopkins Bloomberg School of Public Health Center for Communication Programs.
McCarthy, M. (1991). Discourse analysis for language teachers, Cambridge: Cambridge University Press.
Miller, G. A., Fellbaum, C., Tengi, R., Wakefield, P., Langone, H., & Haskell, B. R. (2005). WordNet, New Jersey: Princeton
University.
Nunan, D. (1993). Introducing discourse analysis, London: Penguin.
Nunan, D. (2004). Task-based language teaching, Cambridge: Cambridge University Press.
67

Rogers, D. (1974). Which connectives? Signals to enhance comprehension. Journal of Reading, 17, 462-466.
Ruffman, T. (1996). Higher order factors in comprehension disability: Processes and remediation. In C. Cornoldi & J. Oakhill
(Eds.), Reading Comprehension Difficulties, Mahwah, NJ: Erlbaum, 33-67.
Schacter, J., Herl, H. E., Chung, G. K. W. K., Dennis, R. A., & O’Neil, H. F. Jr., (1999). Computer-based performance
assessments: A solution to the narrow measurement and reporting of problem-solving. Computer in Human Behaviors, 15 (3-4),
403–418.
Sinclair, K. J., Renshaw, C. E., & Taylor, H. A. (2004). Improving computer-assisted instruction in teaching higher-order skills.
Computers and Education, 42 (2), 169-180.
Sharp, A. (2003). Reading comprehension and text organization, New York: Edwin Mellen.
Trainin, G., & Swanson, H. L. (2005). Cognition, metacognition, and achievement of college students with learning disabilities.
Learning Disability Quarterly, 28, 261-272.
Underwood, T. (1997). On knowing what you know: metacognition and the act of reading. Clearing Hourse, 71, 77-80.
Wang, Y. F. (1998). Facilitating EFL reading by teaching text cohesive ties. Proceedings of the 7th International Symposium on
English Teaching, Taipei: Crane, 855-866.
Yang, Y. F. (2002). Reassessing readers’ comprehension monitoring. Reading in a Foreign Language, 14 (1), 18-42.
Yang, Y. F. (2006). An investigation of undergraduates’ comprehension monitoring knowledge and strategy in EFL reading.
Journal of Science and Technology, 15 (1), 67-82.
Yeh, S. W. (2003). The effects of web page structure and metacognitive knowledge on EFL learners’ comprehension in
hypermedia CALL. Proceedings of the 2003 International Conference on English Teaching and Learning in the Republic of
China, Taipei: Crane, 613-624.
Yeh, S. W., & Lo, J. J. (2001). Metacognitive cues: A technique for enhancing EFL reading in hypermedia. Fu Jen Studies, 34,
28-40.

68

Amory, A. (2010). Education Technology and Hidden Ideological Contradictions. Educational Technology & Society, 13 (1), 69–
79.

Education Technology and Hidden Ideological Contradictions
Alan Amory
Department of Mathematics, Science, Technology and Computer Education, University of Johannesburg,
Johannesburg 2006, South Africa // aamory@uj.ac.za
ABSTRACT
This article examined, thought a Cultural Historical Activity Theory lens, how immersive- or pervasive
environments and pedagogical agents could more easily support social collaboration as foundation of
contemporary learning theory. It is argued that the fundamentalism-liberationism contradiction (learn from
versus learn with technology) is no longer justifiable as contemporary technology tools (pervasive/immersive
environments and agent technology), the understanding of social networks, and recent neuro-science discoveries
negate instructional design philosophies and innatist positions. The use of an activity lens allowed for
identification of a number of educational technology design principles including explication of ideological
positions, designs for contradictions, acceptance of a post-modern position, designs to overcome homophilic
associations, and use of complex real-world learning activities.

Keywords
Collaboration, Cultural historical activity theory, Mirror-neurons, Education technology, Ideological contradiction

Introduction
This article is premised on the notion that there are hidden ideological contradictions in education technology as a
field of practice and also of theory. These contradictions are embedded in the discourses of these fields and are
present in, for example, positions about what constitutes learning, what constitutes technology itself, what constitutes
theoretical positioning and also what constitutes the design for inquiries about these phenomena. It is important to
understand educational ideological positions so to facilitate the development of appropriate education technology
design and praxis.
Amory (2007) suggested that much of education technology replicates hegemonic practices that limit educational
transformation, have little to do with contemporary learning practices and much more to do with fundamental and
totalitarian ideologies of instruction. Similarly, Cohen (1987) argued that fundamentalists ideological beliefs
embedded in technological products are incongruent with educational transformation. In addition, Gerardi (2006)
suggested that advanced technologies are often tools of an authoritarian state leading to standardization of thought
and social conformity. However, not all educational technology is driven by fundamentalist approaches to maintain
the status quo. Referring to Cultural-Historical Activity Theory (CHAT), Stensenko (2005, p. 72) wrote:
“[P]eople not only constantly transform and create their environment: they also create and constantly
transform their very lives, consequently changing themselves in fundamental ways and, in the process,
gaining self-knowledge. Therefore, human activity – material, practical, and always, by necessity, social
collaborative processes aimed at transforming the world and human being themselves with the help of
collectively created tools – is the basic form of life of people .”
Individual ideologies therefore operate, as McAllister (2004) suggested, within societal dialectical struggles
reflecting the relationship between self and society and are a cultural artifact of mass-market post-modernism
production.
This paper explores immersive- or pervasive environments and pedagogical agents that could more easily support
social collaboration and individual transformation. In addition, the ideology-technology-learning triad is informed by
developing an understanding of social networks and by recent neuro-scientific discoveries. The main argument of
this paper is that personal and societal transformation can be cultivated through fostering social collaboration,
designing complex learning activities that include contradictions, and make use of education technology in which
embedded ideological positions are explicated. However, it is first necessary to position this exploration within an
appropriate theoretical framework, as discussed in the next section.

ISSN 1436-4522 (online) and 1176-3647 (print). © International Forum of Educational Technology & Society (IFETS). The authors and the forum jointly retain the
copyright of the articles. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that copies bear the full citation on the first page. Copyrights for components of this work owned by
others than IFETS must be honoured. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior
specific permission and/or a fee. Request permissions from the editors at kinshuk@ieee.org.

69

Tool

Actor

Object

Outcome

Community
Division of Labor
Rules
Figure 1. Activity system diagram (from Engeström, 1987)

Theoretical Framing
This paper made use of CHAT both as a way of understanding education technology (tool mediated construction)
and as an analytical frame to identify pertinent design principles. A recent contribution to the legacy of Lev
Vygotsky’s cultural historical theory of learning is Engeström’s (1987) broadening and amending of what Leontiev
(1978) began with regard to theory of activity. Engeström’s now often utilized model of an “activity system” is a
helpful tool for work in information technology that includes social mediated activity (Fig. 1). In an activity system
Outcomes result from Actors interrogating Objects by means of Tools (physical – pencils and technological artifacts;
or psychological – signs and symbols). Tools mediate interactions through the activity context that includes
Community, Division of Labour and associated Rules (Engeström, 2000, 2001; Barab, Evans & Baek, 2004; Roth &
Lee, 2007). Internal Contradictions create instability and drive the development of and change in the system
(Engeström, 2000, 2001). Contradictions are contextualized within dialectical approaches and change over time
(Roth & Lee, 2007). Objects, as cultural entities, are the prime unit of analysis within an activity system (Engeström,
2001), embody communal social practices that transform and further develop during human activity (Stetsenko,
2005), and, in conjunction with motive, give the system coherence (Engeström, 2000). Socially created Tools are
inseparable from the associated activity and are part of the purpose, relevance and value appropriated to them by the
Actor (Robbins, 2005), and may become Objects, or Outcomes, of activity (Roth & Lee, 2007). It is implicitly
understood that socially created Tools involved in cognitive mediation may not be ideologically neutral.
Education technological artifacts (Tools) contain specific ideological positions and often support fundamentalist
world-views (Amory, 2007). The design, development and deployment of technological artifacts (Objects) are part of
an activity system that includes complex communities (software engineers, interface designers, programmers, public
relation officers) who use other technological tools and language (both human and machine) to create new artifacts
(Tools). Therefore, the work of designers, developers and programmers is underpinned by specific ideologies, which
are therefore part of the design of the Objects, Outcomes and Tools.
The paper is written from my own ideological position within a liberationist world-view that supports social freedom
and equality. Knowledge is, therefore, viewed as a tool to support social reform (especially for the underprivileged
and those facing discrimination) in order to develop individual potential.
Education technology that could inherently support social collaboration is explored in the next section. First a brief
synopsis of the role in collaboration in learning is presented to frame the arguments. Thereafter Immersive and
Pervasive Environments as Social Spaces, Pedagogical Agents as Social Negotiators, Social Networks and Social
Neuro-scientific Discoveries are discussed and analyzed using CHAT.

70

Education Technology and Social Collaboration
Role of Collaboration in Learning
Collaboration, two or more people work together to realize a common objective, is an important component of
contemporary learning theory (Vygotsky, 1933/1978; Piaget, 1977; Duffy and Cunningham, 1996), human
development and transformation (Stetsenko, 2004), complex-games (Prensky, 2005), and learning environments that
include authentic games (Gee, 2003; Reeves, Herrington & Oliver, 2004; Shaffer, 2005). In Massively Multiplayer
Online (MMO) games collaboration played a greater role than did informational content, and such games should be
viewed as social practice (Steinkuehler, 2004). In addition, Thomas (2005) found that the use of collaboration,
reciprocal teaching, and sustained social and discursive practices in online role-playing environments supported
problem solving and learning. However, Puntambekar (2006) showed that in an online graduate course there was
little knowledge co-construction and collaboration that resulted in neither new ideas nor the inclusion of group ideas
into individual responses. Barab (2003, p. 197) contended that we are yet to understand the difference “between a
community of learners” (collaboration) and a “group of students learning collectively” (cooperation). While
collaboration is one of the cornerstones of contemporary educational practices, the mechanisms and processes of
collaboration in complex virtual worlds are not fully understood. Therefore, the relationships between learning and
collaboration in immersive and pervasive systems, roles of pedagogical agents in social environments, social
networks and neuro-scientific discoveries that support social interaction at the neurological level are explored to
better understand the role of collaboration in cyber environments.

Immersive and Pervasive Environments as Social Spaces
Immersive and pervasive environments are cyberspaces in which individuals need to work together to solve complex
problems that cannot be resolved individually. Here immersive and pervasive systems are discussed to discern the
role of collaboration in environments specifically designed to solve complex problems.
A network environment that includes collective and political actions is an example of an immersive environment
(McGonigal, 2003). In these spaces, which are different to pervasive ones (see below), McGonigal argued that
participants need to solve complex real-world problems that include both physical and cyber world interactions, and
require many different skills that include programming, translations, specific domain knowledge, and brute force.
Young, Schrader and Zheng (2006) argued that in such environments interaction between players, via their avatars, is
characterized by agent-environment, perception-action, and affordance-effectivity duals that led to active learning
rather than retrieval of information from memory.
Pervasive environments, on the other hand, “extend the gaming experience out into the real world – be it on the
streets, in the remote wilderness, or a living room. Players with mobile computing devices move through the world.
The game player … experiences a game that is interwoven with the real world and is potentially available at any
place and any time” (Benford, Magerkurth & Ljungstrand, 2005, p. 54). Pervasive systems consist of three core
technologies that include: content made available through mobile phones, hand-held computers and wearable
computers; wireless communication to support communication between participants; and sensing technology that
captures participants’ positioning. A pervasive learning environment includes a connected community of
autonomous players where learning takes place at places meaningful to the learners (Thomas, 2006). It is interesting
to note that the on-line community (in which the social interaction takes place) is more important in problem-solving
than knowing where participants are located in the real-world (Nova, Girardin, Molinari & Dillenbourg, 2006).
However, de Souza e Silva and Delacruz (2006) argued that the benefits of such environments relate to collaboration
and bridging players in separate spaces via different forms of technology. While the creation of pervasive
environments is not a trivial task (Lankoski, Heliö, Nummela, Lahti, Mäyrä & Ermi, 2004), such environments offer
the most interesting artifacts through which technology can support interaction and collaborative problem-solving.

Summary and CHAT Analyses
In immersive and pervasive environments activity centers around complex real-world problems (the Object) that are
situated in real and cyber space. Communication and work is supported by mobile and other “intelligent” devices
71

that function across dissimilar environments to connect wet (human) and digital (cyber) components to support
collaborative problem-solving (Tools). Immersive and pervasive systems include Contradictions related to Agent–
Environment, Perception–Action and Affordance–Effectivity. The wet-digital dichotomy allows multiple social
formations between other communities, device, people and situations (Communities) that allow various individual
identities (Division of Labour) to work in self-forming and regulated communities.
The next section investigates the role of software-based pedagogical agents in cyber communities to bring to the fore
important social interactions between synthetic characters (agents) and people working and playing in cyberspaces.

Pedagogical Agents as Social Negotiators
Wooldridge and Jennings (1995) argued that intelligent agents will be one of the most important computer systems in
the development of complex software - autonomous agents with social ability, can react to their environment, are
mobile within a network, benevolent, and rational. However, such a role of agents is not yet realized and this may be
due to a number of factors. Artificial Intelligent systems do not include the necessary procedures to support
appropriate interactions. In addition, the conceptualization of agents is limited to support current hegemonic
instructional approaches to learning. Pedagogical agents could also play a more social role in digital environments.
This is of particular interest to designers and developers of learning/instruction who wish to include suitable
characters within socially constructed cyber environments. Therefore this section deals with the role of pedagogical
agents in virtual environments and in learning.
Pedagogical agents play many different roles within virtual environments as: agents to support learning (mentors,
tutors, guides, coaches, learning companion, teacher support, or motivational mediators) (Lester, Converse, Stone,
Kahler & Barlow, 1997; Baylor, 2002; Payr, 2005); actors/coaches in interactive drama (Baylor, 2002; Chou, Chan
& Lin, 2003); role-play actors that perform and are part of knowledge-based systems (André & Rist, 2001); as
characters in content presentation, simulation, navigation, searching, management, and teacher support (Payr, 2005);
as empathic companions (Prendinger & Ishizuka, 2005); and as intermediaries to support collaboration (Payr, 2005).
The most fully described role of an agent in a learning environment is that of Chou et al (2003) who suggested that a
learning companion should be a computer-simulated character that has human characteristics, plays a nonauthoritative role, promotes social learning, and provides useful information. Hence, the role of pedagogical agents
in constructivist learning environments is of particular interest.
Problem solving skills by middle school students improved when advice was received from pedagogical agents with
visual and/or auditory modalities (Lester et al, 1997). Baylor (2002) found that the use of a constructivist agent
changed teacher perceptions that led to the production of more constructivist teaching plans. However, it was not
apparent if these changes were solely due to the presence of a pedagogical agent, the result of some other interaction
such as the learning environment, or the design of the research investigation. Nevertheless, there are a number of
research findings that point to the usefulness of pedagogical agents. Software agents that guided learners in
ActiveWorlds resulted in participants making better use of advice and the production of more and better quality
explanations (Holmes, 2007). However, this research was undertaken with small groups and might involve the
repetition of information provided by agents in the answers.

Summary and CHAT Analysis
When pedagogical synthetic characters are used in learning environments they function at many levels including, in
activity theory parlance, as Object (personalization of agent avatar); as Tool (acting as a knowledge agent, simulator,
navigator, searching device, manager); as part of the Community (socially adept, pro-active, mobile, benevolent,
veracious and rational); take on different roles – Division of Labour (mentor, tutor, guide, coach, companion,
motivator, role-player, and emphatic companion); and function within a set of Rules.
The previous section discussed issues related to collaboration in cyberspaces and with intelligent objects (agents)
without reference to specific theoretical constructs. The following section of the paper explores concepts related to
social networks.
72

Social Networks
Kossinets and Watts (2006) suggested that social networks are important as they form part of information processing,
social influence, and distributed search. The collective value of social networks is social capital forming part of the
building and maintenance of democracy supported through information flows, norms of reciprocity, collective
action, and broader identities and solidarity (Putnam, 1995). In this section, social capital, social network
interactions, and Social Network Analysis are discussed.
Nahapiet and Ghoshal (1998) suggested that social capital includes three dimensions: structural (patterns of social
interactions between actors), relational (the relationships between the actors), and cognitive (shared representations,
interpretations, and systems of meaning among actors). The structural dimension contributes to a shared
understanding and the cognitive dimension supports learning (Sorama, Katajamäki & Varamäki, 2004). Social
capital appeared to work when individuals perceive that they have an advantage due to their position within a social
structure (Burt, 2004). This is especially relevant to individuals who can act as brokers across groups to develop a
shared understanding of similarities and differences between groups, or to provide critical syntheses. Such brokers,
Burt argued, are more likely to propose ideas that are creative, openly discussed and are more likely to be accepted.
However, the design, flow of information in, and dynamics of social networks is complex.
Kossinets and Watts (2006) suggested that the evolution of a network arises from the topology of the network itself
and the organizational structure supporting the network. The movement of simple and complex knowledge within
networks is different: simple knowledge diffused equally between distant and close actors while complex knowledge
resisted diffusion (Sorenson, Rivkin & Fleming, 2006). The dynamics of social networks is bound by the focus,
friendships and homophily (love of the same). In many instances the cognitive needs of a group can be of less
importance when the foci of the interaction are highly valued (for example, the completion of a group task) (Feld,
1981). The degree to which two actors interact within a social network appeared to be directly related to the strength
of their friendship ties (Granovetter, 1973). McPherson, Smith-Cain and Cook (2001) agreed that actors who share
knowledge are more likely to interact. They showed that personal networks characterized by homophily and
ethnicity, based on race, created the greatest divides; sex, age, religion and education strongly influenced network
interactions and structures; and race and sex homophily appeared to be both a baseline and an inbreeding
phenomenon. Punishment-reward systems were not needed in networks of actors that are alike (Durrett & Levin,
2005). In addition, altruism, jealousy and fairness appeared to be the more important reward when playing a dictator
computer game (Andreoni & Miller, 2002). However, homophily attitudes can be less important in certain
circumstances. Yuan and Gay (2006) found that in computer mediated distributed networks, team members are more
sensitive to location than to gender or race homophily, and that social capital significantly influenced performance of
both actors from different or within groups. This would be particularly true in cyber communities as actor
representation through the use of avatars would allow the reconstruction of identities not based on reality. While the
building block of democracy is social capital, the relationships between actors and cognitive shared meaning making
dimensions that interact through altruism, or cooperation, are also important and may be more important in taskbased social networks.
Social Network Analysis allowed for the visualization of social networks to made explicit the social capital to an
actor (indegree – actors build on each other’s notes – collaborative writing; outdegree – the number of specific notes
a specific actor users; betweeness – whether an actor is a broker of information) (Sha & Aalst, 2003); allowed actors
to visualize their own status in a network (Lockerd & Selker, 1999); improved performance (Cho, Gay, Davidson &
Ingraffea, 2007); and permitted actors to serendipitously bump into each other thereby extending the social network
(Farnham, Kelly, Portnoy & Schwartz, 2004). Therefore, the use of SNA in co-operative learning environments
would support the development of social ties within learning communities.

Summary and CHAT Analysis
Social networks are used to develop social capital within democratic communities that are based on altruism and
social justice (Object). The social network Community is multicultural and consists of individuals who are friends,
part of a learning network and function as Actor and/or broker (Division of Labour). Social networks require Tools to
support collaboration and feedback, and to visualize through the use of SNA individual position within the
73

community (Tools). Rules of social networks are complex but support information flows, reciprocity, collective
actions, broader identities, solidarity, and homophily. Social networks include altruistic–co-operative Contradictions.
Recent neurological discoveries using non-invasive technologies to view brain functions offer the first clue to a
neurological social dance that appears to influence all aspects of our development and life. In the next part of the
paper some aspects of this research are presented to provide evidence to support the role of social interaction in
learning.

Social Neuro-scientific Discoveries
The sociological neuro-scientific advances that bind us socially together include the discovery of spindle cells and
mirror neurons. Spindle cells are the most plentiful in humans compared to other mammals and appear to guide snap
social decisions. Mirror neurons are involved in sensing the movement that others make to rapidly prepare us to
respond to such movements and are involved in language and culture development (Goleman, 2006). Neuroscientists are now, for the first time due to the development of advanced imaging technology, able to show the social
aspects of our brain functions. The following discussion includes an introduction to the phenomenon of mirror
neurons, and the role of mirror neurons in language development and collaboration.
Mirror neurons are scattered throughout key parts of the human brain – the promoter cortex and centers for language,
empathy and pain – and fire not only as we perform a certain action but also when we watch someone else perform
that action (Perrett, Rolls & Caan, 1982; Montgomery, Isenberg & Haxby, 2007). The mirror-neuron system appears
to be tightly coupled with action understanding and imitation learning argued to be the basis of human culture
(Rizzolatti & Craighero, 2004); involved in the evolution of language, music, art, tool-making and empathy (Azar,
2005; Oztop, Kawato, Arbib, 2006); and appeared to be an essential cognitive skill involved in social groups
(Erlhagen, Muskovsky & Bicho, 2006) and emotional awareness (Parr, Waller & Fugate, 2005). However, the link
between mirror neurons and language appears to be noteworthy.
Fogassi and Ferrari (2004) argued that gestural communication is the predecessor of human speech and Rizzolatti
and Craighero (2004) illustrated a direct link between hand gestures, mouth gestures and the oro-laryngeal
movement used in speech production. These works supported the proposal by Rizzolatti and Arbib (1998) that the
mirror-neuron system is the neuro-physiological mechanism of language evolution. Bickerton (2007) was critical of
these positions and argued that mirror neurons “could hardly be innately programmed to respond to action sequences
that nobody has yet produced” and the theory cannot “shed any light on how symbols originated or how syntax
originated” (p. 523). However, Arbib (2005) provided a detailed description of the role of mirror-neurons in the
development of human language that also included the development of protosign, protospeech and evolution of the
brain and body.
Goleman’s (2006) argument that “the major function of the social brain – interaction synchrony, the types of
empathy, social cognition, interactive skills, and concern for others – all suggest strands of social intelligence” (p.
329) is built on works such as that of Gallese, Keysers and Rizzolatti (2004) who discussed how mirror neurons
provide the first unifying theory of social cognition that afford us with insight into the minds of others. Rizzolatti and
Craighero (2004) suggested that mirror neuron development played a role in the development of altruism and
allowed individuals to understand the intensions, meanings and emotions of others. Gallese (2007) argued that the
premotor system (part of the mirror neuron system) is involved in the mastery of hierarchical structure of language
(one of the most important components involved in social cognition) and abstract inference. Therefore, the “circuitry
that controls how we move our bodies and enables our understanding of the action of others can, in principle, also
structure language and abstract thought” (Gallese, 2007, p. 666). Uddin, Iacoboni, Lange and Keenan (2007) argued
that social interactions are dependent on self- and other-representations and that the mirror neuron system supports
the physical other-to-self mapping. In addition, the discovery of a human auditory mirror system associated with the
left hemispheric temporo-parieto-premotor circuit may be involved in language evolution and was more strongly
activated in individuals that scored higher on an empathy scale (Gazzola, Aziz-Zadeh & Keysers, 2006) reinforced
the role of language in social networks.
Chernigovskaya (2007) wrote that the mirror neuron hypothesis is of importance “both for explaining the
organization of language functions … and for learning in general, as it allows links to be made between the agens
74

(who does the action), the patiens (the object of the action), and the instrument (the means or tool)” – or in
Vygotskian terms: the Actor, Object and Mediating Artifact triad. The neurological support for collaboration directly
challenges a number of positions including:
 The reductionist cognitivist’s agenda emphasizing value-free information processing in individual and isolated
minds (Barab, Evans & Baek, 2004; Stetsenko, 2005; Vianna & Stetsenko, 2006);
 Skinner’s operant conditioning model previously critiqued and rejected by Chomsky (1967);
 Instructional design, as proposed by Gagné and Merrill, based on behaviorism as frame through which concepts
are treated as distinct learning outcomes (objects of instruction) rather than cognitive tools for representing ideas
(Jonassen, 2006); and
 Chomsky’s innatist “universal grammar” of language development (Holden, 2004).
The acquisition metaphor to describe learning is therefore inappropriate. The associated reductionist approaches are
“directly affiliated with positivist, non-dialectical and ultimately conservative approaches in education” (Vianna &
Stetsenko, 2006, p. 82) and drive the constant re-invention of education practice that is not substantially different
from past practices. The dialectic struggle between perpetuating the past (fundamentalist) and the constant creation
and transformation of oneself and thereby the world (liberationist) is unsustainable: fundamentalism offers little
except that it drives the neo-liberal agenda that has little to do with learning and everything with maintenance of the
past.
Summary and CHAT Analysis
Neuro-scientific discoveries support the notion that learning is situated in the cognition of a social mind. The
development of language mediated action (Tools) in Actors is socially constructed thereby supporting the Actor,
Object and Mediating Artifact triad. In addition, Engeström’s (2001) notion that capitalism, and thus neo-liberalism,
is the primary contradiction within an activity system is part of an educational dialectical struggle to rid the world of
instructional design positions that are contrary to the social construction of knowledge.
The final section of the paper explores education technology in relation to CHAT both as argument and as tool of
analyses. Thereafter design elements pertinent to the development of education technology artifacts and to the
development of social interactions are presented.

Reflection, Documentation and Development of Design Principles
Amory (2007) argued that the design, development, integration and use of classroom technologies support current
hegemonic fundamentalist positions maintained through observation and control systems and include Reusable
Learning Objects (based on totalitarian ideologies of instruction), Learning Management Systems (which include
information redistribution, observation and monitoring), blended learning (the inclusion of technological tools into
existing courses with no pedagogical change perpetuating the past), and education games (ideologically suspect
simulations based on model-using rather than model-building approaches). However, learning systems and artifacts
built to support collaboration (immersive and pervasive environments, and pedagogical agents), as argued here, offer
opportunities to create tools to support transformative activity systems and foster liberationist approaches. Stetsenko
(2004) suggested that socially constructed tools can overcome the constraints of nature and the environment. This is
true when the tools originate as part of a social framework and are not the result of other ideologies. While
technological artifacts may include suspect ideological positions, such artifacts can foster social construction when
the embedded ideology is explicitly declared, or when the embedded ideological position is used to create a
contradiction that could be labeled the ‘fundamentalism-liberationism contradiction’. This contradiction is explored
in the next section.
Activity systems change and develop over historical time (Roth & Lee, 2007). Also, Vygotsky’s ideas are a direct
consequence of his historical context (theories were developed in opposition to Freud’s psycho-analysis and
behaviourist approaches) and are part of a social project that is both a product and an instrument (Stetsenko, 2004).
Similarly, educational tools are products and instruments that are part of our globalized neo-liberal world. Klein
(2007) shows that neo-liberal economic policies of privatization, free trade, decreased social spending and
privatization of government enterprises disenfranchise specific race groups, perpetuate gender exclusivity and
support fundamentalist religious belief systems. Education systems are not immune from neo-liberalisation. Neo75

managerialism – the use of a market-driven educational system – makes use of neo-conservative standards,
traditional hierarchies of class and race, accountability, national curriculum and national assessment policies
(Amory, 2008). Learning is therefore driven by and consumes neo-liberalization and learning technologies often
promote Fordist assembly line production of learning materials, and teaching and learning.
While, on the one hand, education technologies are often driven by neo-liberals to support a globalized economy that
protect fundamentalist hegemonies that perpetuate and support race, gender and religious homophily within social
networks. Technology, on the other hand, could support the liberalization of education practices and include
immersive and pervasive system, and pedagogical agents that are designed to support collaboration. Activity systems
that can lead to the production of Tools to support the “collaborative processes of material production of social life –
human object-related activity” (Stetsenko, 2005) need to include the collaboration of the future Actors in the
development process (Object activity) and make explicit the fundamentalism-liberationism contradiction. These
Actors subsequently use the Tool to transform both themselves and the world around them in ways that support
social reform to support the discriminated and underprivileged in order to develop fully individual potentials.
The final section of the essay uses the previously highlighted CHAT components to bring to the fore the
fundamentalism-liberationism contradiction and the software design concepts applicable to the designed
technological artifacts to support educational transformations. Design principles identified through CHAT analyses
are discussed to make concrete design for future educational technologies. Technological artifacts and social
interactions are discussed separately.
Analyses of design, development and use of immersive and pervasive social spaces and pedagogical agents as social
negotiators allowed for the identification of a number of concepts:
 Ideological positions of software designers and engineers are integral to any activity system and directly
influence the construction and development of the artifact, or Object;
 Production of technological artifacts minimize inherent Contradictions to improve usability and thereby
decreases learning opportunities; and
 Constructed artifacts function as either an Object (for example, the deconstruction of an artifact) or as a Tool.
With respect to social interactions numerous Actors involved in software design build and participate in social
networks. Activity systems of complex social networks are more complex than those associated with technological
artifacts, and include:
 Actors playing different roles, for example, to facilitate Object development or complex real-world problem
solving;
 Task-based Outcomes allowing Actors to overcome homophilic associations and thereby work together to reach
common goals;
 Inherent Contradictions supporting disruptions that challenging preconceived notions leading to new
understandings; and
 Access into activity systems using technological artifacts is normally via the Object or Tool component of the
system; the Actor space is the primary access point is the design of technological systems.
The theory and practical use of education technology needs to acknowledge the fundamentalism-liberationism
contradiction. Consequently, technological artifacts as Objects perpetuate past practices but when functioning as
Tool could mediate learning. Second, education technology design needs to include Contradictions that challenge
existing paradigms and allow for disruption, and therefore learning. Third, learning with technology needs to support
a post-modern view that there may be more than one correct solution to a given/existing problem. Lastly, designs for
learning technology need to allow for multiple voices to work together to solve complex real-world problems that
includes both the digital and wet worlds in order to overcome the powerful race-gender-belief homophily that often
dominates social network.

Conclusions
This paper argued that it is not longer tenable to consider the fundamentalist learn from technology position.
Contemporary technology tools, the functioning of social networks and the findings of neuro-science challenge
instructional design and innatist positions. The CHAT lens brings to the fore a number of principles that need to be
76

considered in the design, development and use of technology for teaching and learning and include: explication of
ideological positions, design for contradictions, acceptance of a post-modern position, power of social networks and
the use of complex-real world learning tasks to overcome homophilic attractions. The development of such systems
will be difficult and requires concerted efforts to topple the predominant use of techno-reductionist tools more
associated with content and user management than with the transforming people and the world through human
actions supported by socially constructed tools.

Acknowledgements
Special thanks to Elizabeth Henning, Duan van der Westhuizen, Geoffrey Lautenbach, Kathy Morgan, Gadija Petker
and Nadine Petersen for their critical comments and encouragement.

References
Amory, A. (2007). It’s not about the tool, it’s about the ideology. South African Journal of Higher Education, 22 (6), 655–671.
Amory, A. (2008). Academic administration as the puppet masters (Sicelankobe). The consequences. South African Journal of
Higher Education, 22 (3), 498-514.
André, E., & Rist, T. (2001). Presenting through performing: on the use of multiple life like characters in knowledge-based
presentation systems. Knowledge-Based Systems, 14 (1–2), 3–13.
Andreoni, J., & Miller, J. (2002). Giving according to Garp: An experimental test of the consistency of preferences for altruism.
Econometrica, 70 (2), 737–753.
Arbib, M. A. (2005). From monkey-like action recognition to human language: An evolutionary framework for neurolinguistics.
Behavioral and Brain Sciences, 28 (02), 105–124.
Azar, B. (2005). How mimicry begat culture: Researchers from varied disciplines look to mirror neurons to explain many aspects
of human evolution. Monitor on Psychology, 36 (9), 54.
Barab, S. A. (2003). An introduction to the special issue: Designing for virtual communities in the service of learning. The
Information Society, 19 (3), 197–201.
Barab, S. A., Evans, M. A., & Baek, E. O. (2004). Activity theory as a lens for characterizing the participatory unit. In D.H.
Jonassen (Ed.), Handbook of research on educational communities and technology, Washington, DC: AECT, 199–214.
Baylor, A. L. (2002). Expanding preservice teachers' metacognitive awareness of instructional planning through pedagogical
agents. Educational Technology Research and Development, 50 (2), 5–22.
Benford, S., Magerkurth, C., & Ljungstrand, P. (2005). Bridging the physical and digital in pervasive gaming. Communication of
the ACM, 48 (3), 54–57.
Bickerton, D. (2007). Language evolution: A brief guide for linguists. Lingua, 117 (3), 510–526.
Burt, R. S. (2004). Structural holes and good ideas. American Journal of Sociology, 110 (2), 349–99.
Chernigovskaya, T. V. (2007). The mirror brain, concepts, and language: The price of anthropogenesis. Neuroscience and
Behavioral Physiology, 37 (3), 293–302.
Chou, C., Chan, T., & Lin, C. (2003). Redefining the learning companion: the past, present, and the future of educational agents.
Computers & Education, 40, 255–269.
Cho, H., Gay, G., Davidson, B., & Ingraffea, A. (2007). Social networks, communication styles, and learning performance in a
CSCL community. Computers & Education, 49 (2), 309–329.
Chomsky, N. (1967). Review of Skinner's Verbal Behavior. In Leon A. Jakobovits &Murray S. Miron (Eds.), Readings in the
Psychology of Language, Prentice-Hall, 142-143.
Cohen, D. K. (1987). Educational technology, policy and practice. Educational Evaluation and Policy Analysis, 9 (2), 153–170.
Durrett, R., & Levin, S. A. (2005). Can stable social groups be maintained by homophilous imitation alone. Journal of Economic
Behavior and Organization, 57 (3), 267–286.
Duffy, T. M., & Cunningham, D. J. (1996). Constructivism: Implications for the design and delivery of instruction. In D.
Johansson (Ed.), Handbook of Research for Educational Communications and Technology, New York: Simon & Shuster
Macmillan, 170–198.
Engeström, Y. (1987). Learning by expanding: An activity-theoretical approach to developmental research, Helsinki: OrientaKonsultit.
77

Engeström, Y. (2000). Activity theory as a framework for analyzing and redesigning work. Ergonomics, 43 (7), 960–974.
Engeström, Y. (2001). Expansive learning at work: Toward an activity theoretical reconceptualization. Journal of Education and
Work, 14 (1), 133–156.
Erlhagen, W., Muskovskiy, A., & Bicho, E. (2006). A dynamic model for action understanding and goal-directed imitation. Brain
Research, 1083 (1), 174–188.
Farnham, S., Kelly, S. U., Portnoy, W., & Schwartz, J. L. K. (2004). Wallop: Designing social software for co-located social
networks. Proceedings of the 37th Annual Hawaii International Conference on System Sciences, Los Alamitos, CA: IEEE
Computer Society, 107-116.
Feld, S. L. (1981). The focused organization of social ties. American Journal of Sociology, 86 (5), 1015–1035.
Fogassi, L., & Ferrari, P. F. (2004). Mirror neurons, gestures and language evolution. Interaction Studies, 5 (3), 345–363.
Gallese, V. (2007). Before and below 'theory of mind': Embodied simulation and the neural correlates of social cognition.
Philosophical Transactions of the Royal Society B: Biological Sciences, 362 (1480), 659–669.
Gallese, V., Keysers, C., & Rizzolatti, G. (2004). A unifying view of the basis of social cognition. Trends in Cognitive Sciences, 8
(9), 396–403.
Gazzola, V., Aziz-Zadeh, L., & Keysers, C. (2006). Empathy and the somatotopic auditory mirror system in humans. Current
Biology, 16 (18), 1824–1829.
Gee, J. (2003). What video games have to teach us about learning and literacy, New York: Palgrave MacMillan.
Gerardi, S. (2006). Some implications of modern technology: Revisited. The Social Science Journal, 43, 293–295.
Goleman, D. (2006). Social intelligence: The new science of human relationships, London: Hutchinson.
Granovetter, M. S. (1973). The strength of weak ties. The American Journal of Sociology, 78 (6), 1360–1380.
Holden, C. (2004). Oldest beads suggest early symbolic behavior. Science, 304 (5669), 369.
Holmes, J. (2007). Designing agents to support learning by explaining. Computers & Education, 48 (4), 523–547.
Jonassen, D. H. (2006). On the role of concepts in learning and instructional design. Educational Technology Research and
Development, 54 (2), 177–196.
Klein, N. (2007). The shock doctrine: The rise of disaster capitalism, New York: Metropolitan.
Kossinets, G., & Watts, D. J. (2006). Empirical analysis of an evolving social network. Science, 311, 88–90.
Lankoski, P., Heliö, S., Nummela, J., Lahti, J., Mäyrä, F., & Ermi, L. (2004). A case study in pervasive game design: the songs of
north. Proceedings of the 3rd Nordic conference on Human-computer interaction, New York: ACM, 413–416.
Lester, J. C., Converse, S. A., Stone, B. A., Kahler, S. E., & Barlow, S. (1997). Animated pedagogical agents and problem-solving
effectiveness: A large-scale empirical evaluation. Proceedings of the SIGCHI Conference, New York: ACM, 359–366.
Leontiev, A. N. (1978). Activity, personality, and consciousness, Englewoods Cliffs: Prentice-Hall.
Lockerd, A., & Selker, T. (1999). DriftCatcher: Enhancing social networks through email. Proceedings of the 12th International
Sunbelt Social Network Conference, Retrieved May 1, 2009, from http://pubs.media.mit.edu/pubs/papers/sunbelt_paper.pdf.
McAllister, K. (2004). Game work. Language, power and computer game culture, Tuscaloosa: The University of Alabama.
McGonigal, J. (2003). 'This is not a game': Immersive aesthetics and collective play. Proceedings of the Digital Arts and Culture
Conference, Retrieved June 9, 2009, from http://hypertext.rmit.edu.au/dac/papers/McGonigal.pdf.
McPherson, M., Smith-Lovin, L., & Cook, J. M. (2001). Birds of a feather: Homophily in social networks. Annual Review of
Sociology, 27 (1), 415–444.
Montgomery, K. J., Isenberg, N., & Haxby, J. V. (2007). Communicative hand gestures and object-directed hand movements
activated the mirror neuron system. Social Cognitive and Affective Neuroscience, 2 (2), 114.
Nahapiet, J., & Ghoshal, S. (1998). Social capital, intellectual capital, and the organizational advantage. The Academy of
Management Review, 23 (2), 242-266.
Nova, N., Girardin, F., Molinari, G., & Dillenbourg, P. (2006). The underwhelming effects of location-awareness on collaboration
in a pervasive game. Paper presented at the International Conference on the Design of Cooperative Systems. May 9-12, Carry-leRouet, Provence, France.
Oztop, E., Kawato, M., & Arbib, M. (2006). Mirror neurons and imitation: A computationally guided review. Neural Networks,
19 (3), 254–271.
Parr, L. A., Waller, B. M., & Fugate, J. (2005). Emotional communication in primates: implications for neurobiology. Current
Opinion in Neurobiology, 15 (6), 716–720.
Payr, S. (2005). Not quite an editorial: Educational agents and (e-)learning. Applied Artificial Intelligence, 19 (3–4), 199–213.
78

Perrett, D. I., Rolls, E. T., & Caan, W. (1982). Visual neurones responsive to faces in the monkey temporal cortex. Experimental
Brain Research, 47 (3), 329–342.
Piaget, J. (1977). The development of thought: Equilibration of cognitive structures, New York: Viking.
Prendinger, H., & Ishizuka, M. (2005). The empathic companion: A character-based interface that addresses users' affective
states. Applied Artificial Intelligence, 19 (3–4), 267–285.
Prensky, M. (2005). In educational games, complexity matters - Mini-games are trivial - but “complex” games are not - An
important way for teachers, parents and others to look at educational computer and video games, Retrieved August 15, 2009,
from http://www.marcprensky.com/writing/Prensky-Complexity_Matters.pdf.
Putnam, R. D. (1995). Bowling alone: America's declining social capital. Journal of Democracy, 6, 65-65.
Puntambekar, S. (2006). Analyzing collaborative interactions: Divergence, shared understanding and construction of knowledge.
Computers & Education, 47 (3), 332–351.
Reeves, T. C., Herrington, J., & Oliver, R. (2004). A development research agenda for online collaborative learning. Educational
Technology Research and Development, 52 (4), 53–65.
Rizzolatti, G., & Arbib, M. A. (1998). Language within our grasp. Trends in Neurosciences, 21 (5), 188–194.
Rizzolatti, G., & Craighero, L. (2004). The mirror-neuron system. Annual Review of Neuroscience, 27, 169–192.
Robbins, J. (2005). Contexts, Collaboration, and cultural tools: A sociocultural perspective on researching children's thinking.
Contemporary Issues in Early Childhood, 6 (2), 140–149.
Roth, W. M., & Lee, Y. J. (2007). "Vygotsky's Neglected Legacy": Cultural-Historical Activity Theory. Review of Educational
Research, 77 (2), 186–232.
Sha, L., & van Aalst, J. (2003). An application of social network analysis to knowledge building. Paper presented at the Annual
Meeting of the American Educational Research Association, April 21-25, Chicago, IL.
Shaffer, D. W. (2005). Epistemic games. Journal of Online Education, 1 (6), Retrieved September 18, 2007, from
http://www.innovateonline.info/index.php?view=article&id=79.
Sorama, K., Katajamäki, A., & Varamäki, E. (2004). Cooperation between SMES: Social capital and learning perspective.
Proceedings of the 13th Nordic conference on small business research, Retrieved May 1, 2009, from http://web.bi.no/forskning/
ncsb2004.nsf/23e5e39594c064ee852564ae004fa010/a6cb7066ea59eda6c12567f30056ef4d/$FILE/Sorama&al.pdf.
Sorenson, O., Rivkin, J. W., & Fleming, L. (2006). Complexity, networks and knowledge flow. Research Policy, 35 (7), 994–
1017.
de Souza e Silva, A., & Delacruz, G. C. (2006). Hybrid reality games reframed. Potential uses in educational contexts. Games and
Culture, 1 (3), 231–251.
Steinkuehler, C. (2004). Learning in massively multiplayer online games. Proceedings of the 6th international conference on
learning science, International Society of the Learning Sciences, 521–528.
Stetsenko, A. (2004). Tool and sign in the development of the child. In R. W. Rieber (Ed.), The essential Vygotsky, New York:
Kluwer Academic/Plenum, 501–512.
Stetsenko, A. (2005). Activity as object-related: Resolving the dichotomy of individual and collective planes of activity. Mind,
Culture, and Activity, 12 (1), 70–88.
Thomas, A. (2005). Children online: Learning in a virtual community of practice. E–Learning, 2 (1), 27–38.
Thomas, S. (2006). Pervasive learning games: Explorations of hybrid educational gamescapes. Simulation and Gaming, 37 (1),
41–55.
Uddin, L. Q., Iacoboni, M., Lange, C., & Keenan, J. P. (2007). The self and social cognition: the role of cortical midline structures
and mirror neurons. Trends in Cognitive Sciences, 11 (4), 153–157.
Vianna, E., & Stetsenko, A. (2006). Embracing history through transforming it: Contrasting Piagetian versus Vygotskian
(Activity) theories of learning and development to expand constructivism within a dialectical view of history. Theory &
Psychology, 16 (1), 81–108.
Vygotsky, L. (1933/1978). Mind in society. The development of higher psychological processes, Cambridge, MA: Harvard
University Press.
Wooldridge, M., & Jennings, N. R. (1995). Intelligent agents: Theory and practice. Knowledge Engineering Review, 10 (2), 115–
152.
Young, M., Schrader, P., & Zheng, D. (2006). MMOGs as learning environments: An ecological journey into Quest Atlantis and
The Sims Online. Innovate, 2 (4), Retrieved May 1, 2009, from http://students.ou.edu/M/John.S.Madden-1/assets/pdf/mmogs.pdf.
Yuan, Y. C., & Gay, G. (2006). Homophily of network ties and bonding and bridging social capital in computer-mediated
distributed teams. Journal of Computer-Mediated Communication, 11 (4), 1062–1084.
79

Shih, K.-P., Chen, H.-C., Chang, C.-Y., & Kao, T.-C. (2010). The Development and Implementation of Scaffolding-Based SelfRegulated Learning System for e/m-Learning. Educational Technology & Society, 13 (1), 80–93.

The Development and Implementation of Scaffolding-Based Self-Regulated
Learning System for e/m-Learning
Kuei-Ping Shih1, Hung-Chang Chen2, Chih-Yung Chang1* and Tai-Chien Kao3
1

Department of Computer Science and Information Engineering, Tamkang University, Tamsui, Taipei County,
Taiwan // Tel: +886-2-26215656 Ext. 2748 // kpshih@mail.tku.edu.tw // *cychang@mail.tku.edu.tw
2
Department of Information Technology, Ching Kuo Institute of Management and Health, Keelung, Taiwan // Tel:
+886-2-24372093 Ext. 273 // gileschen@ems.cku.edu.tw
3
Institute of Education, National Dong Hwa University, Hualien, Taiwan // Tel: +886-3-8635571 Ext. 5571 //
mkao@mail.ndhu.edu.tw
ABSTRACT
This paper proposes a self-regulated learning (SRL) system with scaffolding support in order to develop
independent learning skills among students. The SRL system uses self-regulated learning and scaffolding
theories to appeal to both instructors and learners. On the part of the instructors, a Content Accessibility
Subsystem is provided to easily organize learning materials and to dynamically provide different levels of support
for their learners. As for the learners, many subsystems are proposed that provide a conducive mobile learning
environment for them. With the application of the scaffolding theory, the system can easily adjust to provide help
to the learners, facilitating SRL processes anytime and anywhere, and establishing the learners’ SRL patterns
gradually. The learners in the experiment deemed that that the proposed system could provide them selfregulatory attributes. The experiment results show that the average SRL score of learners increases, though the
improvement is not significant. However, the result also showed that the SR skills of students in the group of
Low SR significantly improved.

Keywords
Self-Regulated Learning, Self-Regulatory Learning Cycle, Scaffolding, Mobile Leaning, E-Learning, CAL systems

Introduction
The main goal of education is to develop the character of students and foster in them a spontaneous desire to learn.
To achieve this aim, self-regulated learning (SRL) is essential. However, while modern technologies have made
learning possible at any time and place, there still is the challenge to provide a conducive environment so that
learners can easily schedule their study plans and avail of learning materials outdoors. To address this, this paper
proposes an SRL system with wireless technologies.
Lately, people from both academic and government sectors have keenly promoted SRL because they recognize the
need to help learners take charge of their own education. However, SRL is not an easy task. Four factors are essential
in carrying out SRL: learning schedules, materials, scenarios, and quality (Zimmerman, Bonner, & Kovach, 1996).
Along with these factors, other difficulties in performing SRL may be pointed out:
 Learning schedules and materials: A suitable learning schedule makes a person’s own learning methodical.
However, if there is insufficient experience in the design of a learning schedule, a person may end up having
poor SRL performance. In addition, because the study materials are limited and varied, a learner may find it
difficult to organize the materials he needs.
 Learning scenarios and quality: The rapid development of modern technologies, such as broadband and wireless
communication engineering, makes learning materials easily available. However, because there is no tailormade learning environment for outdoor scenarios, learners may just give up learning due to the difficulty or the
complexity of accessing the learning materials. Moreover, because of many distractions, learners may be unable
to focus well.
Therefore, an SRL system that adopts the concept of a self-regulatory learning cycle (Zimmerman et al., 1996) is
proposed. Because having an ambitious and unrealistic aim may disappoint learners during the process of learning,
the proposed system firstly helps students set a reasonable goal in initiating their motive. Moreover, the system
adopts the scaffolding theory (Bruner, 1983), which gradually builds their learning patterns. Through this theory, the
system can provide students with information and materials they need. The success of the scaffolding depends on the
precise evaluation of the learning outcomes such that the learning scaffolding can be removed properly. Therefore, a
reliable evaluation system is provided so that learners can determine their progress. As a result, they can set
ISSN 1436-4522 (online) and 1176-3647 (print). © International Forum of Educational Technology & Society (IFETS). The authors and the forum jointly retain the
copyright of the articles. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that copies bear the full citation on the first page. Copyrights for components of this work owned by
others than IFETS must be honoured. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior
specific permission and/or a fee. Request permissions from the editors at kinshuk@ieee.org.

80

reasonable goals in the system and even develop their SRL skills as they go through each self-regulated learning
cycle.
Another aim of the system is to support a mobile learning environment for learners through modern wireless
technologies via mobile devices. The system enables learners to access learning materials easily and conveniently.
Moreover, the system automatically observes learners' behaviors to help them terminate unhelpful habits, e.g., using
instant messengers or surfing while learning. The mobile learning environment also enables them to share learning
materials, allowing them to adjust their strategies based on the data from their companions. Because of this, the
system would have the characteristics of a mobile learning environment: Urgency of learning need, Initiative of
knowledge acquisition, Mobility of learning setting, Situating of instructional activity, and Integration of
instructional content (Chen, Kao, & Sheu, 2003).
The rest of the paper is organized as follows: Section 2 introduces the design rationale of the proposed system as
well as previous works on SRL. Section 3 introduces the proposed SRL system. Section 4 describes the experimental
results, and Section 5 concludes the study.

Review of Related Literature and Design Rational
Self-Regulated Learning Theory
While there are various explanations and studies that focus on the definition of SRL (Butler & Winne, 1995; Pintrich,
2000), it can be simply described as a learning process with four attributes (Schunk & Zimmerman, 1994):
 Intrinsically or self-motivated: Self-regulated learners tend to maintain learning behavior with a very strong
motivation. Learners can raise this motivation through some practices, such as setting learning goals.
 Planned or automatized: Self-regulated learners are apt to use some strategies along with their learning
processes, including both cognitive and self-regulated strategies. Generally, learners improve their learning
performance when using self-regulated strategies rather than cognitive strategies. Self-regulated strategies
contain goal-setting, goal-planning, organization, transition, exercise, and so on. A self-regulated learner needs
to effectively use self-regulated strategies for his learning.
 Self-aware of performance outcomes: Throughout the learning process, self-regulated learners sharpen their selfawareness toward their learning behavior. To approach an ideal outcome, self-regulated learners should be
aware of their own learning qualities, and change the behavior or strategies correspondingly.
 Environmentally/socially sensitive and resourceful: The learning environment and resources can affect one's
learning pattern. Self-regulated learners have better skills in seeking learning resources or support. With such
ability, they should arrange the environmental conditions and search for other resources effectively.
Knowing how to possess the above attributes should be considered when designing an adequate system for selfregulated learners. Once such attributes are possessed, learners can then skillfully self-regulate their learning.

Figure 1. A cyclic model of self-regulatory learning (Zimmerman et al., 1996)
Zimmerman et al. (1996) proposed a self-regulatory learning cycle in order for learners to gain SR skills, as shown in
Figure 1. The cycle involves four interrelated processes which assist learners in evaluating their performance.
Generally, learners carry out their plans by themselves in these processes. Thus, such a model enables students to
arrange their own learning and voluntarily fulfill it at the same time.
81

SRL with the Support of Computer and Wireless Technologies
With the aid of modern technologies, students can learn efficiently and achieve remarkable performance. Unlike in
the traditional face-to-face set-up, today’s students can individually determine when and where to learn. Therefore,
many computer-based systems have been proposed to enhance a person’s performance when he or she learns
individually (Hadwin & Winne, 2001; Dabbagh & Kitsantas, 2004). However, Azevedo, Cromley, Thomas, Deibert,
& Tron (2003) indicated that, when receiving no assistance, students are less effective at regulating their learning in
their hypermedia environment. Because of this, some computer-assisted SRL tools have been proposed (Hadwin &
Winne, 2001; Dabbagh & Kitsantas, 2004).
Hadwin & Winne (2001) proposed a prototype electronic notebook, CoNets2, to support self-regulation through
explicit scaffolding. Its system can support monitoring and controlling engagement in the phases of SRL, but
unskillful self-regulated learners may shun the tool because CoNets2 lacks enough functions to motivate their
learning. Moreover, the tool is often limited to taking down notes, and does not develop important SRL skills such as
goal-setting, scheduling, and self-evaluation.
Dabbagh & Kitsantas (2004) classified Web-based pedagogical tools (WBPT) into four classes: (1) collaborative and
communication tools; (2) content-creation and delivery tools; (3) administrative tools; and (4) assessment tools.
These tools are also examined on their support of self-regulatory attributes. However, an integration of these tools
should be promoted to gradually make learners become skillful in SRL.
Zurita & Nussbaum (2004) proposed a constructivist learning environment, which allows students to build up their
own knowledge. Wireless interconnected handhelds are used in such an environment to achieve the creation of new
knowledge. By using wireless interconnected handhelds, students can able to modify their current knowledge
schemes which could integrate new information and acquire new knowledge. The process of knowledge construction
partially matches the processes of a self-regulatory learning cycle. However, the environment can only be used for
information sharing, and is unable to help students completely monitor their learning strategies and outcomes.
Looi et al., (2009) designed and implemented a software on a mobile device called the 3Rs (reduce, reuse, and
recycle) software. The mobile device used in the activity can lead students to carry out learning tasks in challengeexperiential cycles, including Challenge, Experience, Reflecting, Planning, and Applying. The activity also partially
matches the self-regulatory learning cycle. However, during the process of Experience, learners may not receive any
assistance, and their learning experience is not recorded and used for future leaning. The comparisons of the
proposed system and the aforementioned tools are summarized in Table 1.
Table 1. Comparisons of the proposed system to the related works
CoNets2
WBPT
Constructivist
3Rs software
(Hadwin &
(Dabbagh &
Learning Environment
(Looi et al.,
Winne,
Kitsantas,
(Zurita & Nussbaum,
2009)
2004)
2004)
2001)
Support
self-regulatory
Yes
Yes
Yes
Yes
attributes?
(Partial)
(Partial)
(Partial)
Support scaffolding?
Yes
No
No
No
E-learning?
Yes
Yes
Yes
Yes
M-learning?
No
No
Yes
Yes

The
proposed
system
Yes
Yes
Yes
Yes

The Proposed Learning System
Basic Concept
Shih, Chang, Chen, & Wang (2005) proposed the prototype of the system. According to learner feedback and
opinions from educational theorists, we will continually enhance the proposed system to efficiently improve the SRL
performance of learners. This section introduces the mapping of the proposed system and the cyclic model.
Moreover, the section also tackles the functions and the design considerations of the proposed system.
82

This paper proposes a state transition diagram that indicates the behavior of a learner by using the cyclic model of
SRL to become self-regulatory (as shown in Figure 2). The diagram consists of seven states, indicating the actions of
a learner in SRL. Among these seven, Activity Scheduling, Learning and Monitoring, Learning Evaluation, and
Analysis are the major states that map the four processes in the cyclic model of Zimmerman et al. (1996).

Figure 2. State transition diagram of the system
Through the proposed system, learners in the Activity Scheduling state can obtain information on what to learn and
can arrange suitable times according to the provided information, all of which leads to the Goal Setting and Strategic
Planning process. After setting their schedules, learners enter the Learning and Monitoring states. Because learners
can undertake scheduled activities by using various strategies while they are being observed, the state maps the
Strategic Implementation and Monitoring process. In the Learning Evaluation state, learners can assess their progress
through tests. Accordingly, the state also maps the process of Strategic Outcome Monitoring. In addition, learners
can evaluate their development by means of varied statistical charts, and can discern their SRL patterns in the
Analysis state before it leads to the Self-Evaluation and Monitoring processes. In obtaining a detailed understanding
of their learning characteristics, students can then go into the Activity Scheduling state again, and arrange more rigid
schedules for future learning.
In addition to the aforementioned states, the Synchronization, Help Seeking, and Schedule Reviewing states are also
involved in the system to help learners gradually develop their SRL skills.
Figure 3 illustrates the architecture of the proposed SRL System, which supports the state transition diagram in
Figure 2. The black arrows in Figure 2 are the data flows between subsystems. This means the learning template is
used to help learners in their schedule. The schedule is the learning schedule arranged by the learners. According to
the learning schedule, the proposed system can know when learners plan to learn and observe their behavior. The
result represents the information generated when learners use our system.
Generally, beginners or unskilled learners cannot arrange their learning well because of their lack of experience in
self-regulation. As such, instructors can use the instructor side system to help them in provide directions and scope,
or suitable scaffolds. On the other hand, the learner side system aims to form a pleasant SRL environment wherein
they would be able to practice their SRL skills. The system is planned to be installed in portable learning devices,
where they can schedule, perform, and evaluate their progress anytime and anywhere. In the following, the design of
the functions provided by the proposed system is presented according to the state transition in Figure 2 and the data
flows in Figure 3.
83

Figure 3. Architecture of the Self-Regulated Learning System
The Data Flow of the SRL System
However, in order to help learners efficiently arrange their learning activities, the information is controlled by a
Scaffold Support Module based on their SRL performance. The purpose of scaffolding is to provide novice learners
with limited complexities of learning context and to remove limits gradually until they become more skillful (Young,
1993). Therefore, the interface initially shows much information until they become more skillful at SRL, so the
learners can then control their learning gradually. Notice that learners are not forced to use given information, but
can decide to refer to the given information.

Figure 4. Learning schedule planning
Previous research have pointed out that intervention increases cognition and motivation, and leads to the
development of self-regulated capabilities (Hofer, Yu, & Pintrich, 1998). In our design, instructors are involved in
helping learners become skillful in SRL through the Content Accessibility Subsystem. Instructors can conveniently
design and give assignments and activities to learners. The subsystem also generates a learning schedule template
84

based on the assignments given by the instructors. Interfaces in Figures 5 and 6 enable instructors to arrange these
assignments and set up the access to learning materials. Through the interface in Figure 5, instructors can set up
detailed information, including degree, session, semester, and credit, in the template. The information can also be
shared by instructors to other instructors, making it easy for them to design templates.

Figure 5. Learning schedule template design
Once learning materials are available, instructors can arrange the details of an assignment through the interface in
Figure 6. Instructors can set up information on a learning unit, including its type (activity, self-examination, or
discussion), suggestion time, and materials. This way, the information can help learners in their schedules and can
also be used by the Scaffold Support Module to generate supplemental information and materials. Similarly,
instructors can query designed learning units, which can be directly imported to a schedule template. After the
instructor's arrangement, the subsystem then generates a schedule template for learners. Through the aid of wireless
technology, the system will automatically download the learning templates from a central database whenever
learners move into Wi-Fi hotspots.

Figure 6 Arrangement for the details of a learning template
85

After scheduling, the system automatically synchronizes to a central database and downloads learning materials
when available. Once learners enter the Schedule Reviewing state via the learning review tool of the Learning
Subsystem, a calendar-like interface, where the scheduled and learned activities are marked, is provided, as shown in
Figure 7. Each selected item on the interface represents a learning activity, and the interface can show basic statistics
(e.g., learning time and the number of interruptions) of a selected item. This information helps learners find their
preferred schedules (e.g., in the afternoon or in the morning).

Figure 7. Learning review tool
When in the Learning and Monitoring states, learners start by clicking the “Learning” button on the learning review
tool. The Learning Subsystem uses two tools, Hyperbook and Hyperpen, to enrich the learning experience.
Hyperbook is a hardcopy book with reference tags. In principle, reading a hardcopy book is comfortable for learners.
However, the content of a book is limited and fixed. In learning devices, staring at the monitor for a long time tire
students easily. Furthermore, they may shun the complicated operation of learning devices because they may need to
alternately use varied input equipment to obtain supplemental materials. Thus, to facilitate operations, students can
use a scanning device, termed Hyperpen, to scan reference tags for more supplemental materials, such as Flash
(Macromedia, 2005), audio, and video. Supplementary materials will then be shown in the learners’ mobile devices.
Hyperpen is embedded with a Bluetooth solution (Bluetooth, 2003) to avoid cables that may distract students as they
scan. Scanned keywords are then submitted to an Internet dictionary, such as Yahoo! Dictionary (Yahoo! Taiwan
Inc., 2007), or the database, such as Answers.com (Answers.com, 2005). Because wireless technologies are heavily
promoted, hotspots can be found all around, and almost all learning devices have WLAN capabilities. Therefore, the
function of searching supplemental materials on the Internet can be carried out everywhere. Figure 8 shows the
system interface where learners use Hyperbook and Hyperpen to avail of supplemental materials on the Internet.
Hyperbook is manufactured by HardSCORM Editor (Wang & Shih, 2006), an authoring tool that conforms to
SCORM 2.0 (ADL Technical Team, 2006). The Content Accessibility Subsystem is able to recognize the edited
courses and can split these into several learning activities based on their metadata.
Because the most important performance control process that distinguishes skillful from naive self-regulated learners
is self-monitoring (Zimmerman & Paulsen, 1995), an Event Monitoring Subsystem is needed to observe their
behavior. If learners can monitor their own progress, their academic performance, achievement, time on task,
classroom behavior, and problem-solving abilities can be improved (Lan, 1998). We therefore give much attention to
recording learner behavior.
86

Figure 8. Searching the database in Internet (Answers.com, 2005)
Monitoring items include the learner’s schedule and learning behavior. Schedule includes data on whether a learner
studies on time and how much time a learner spends on an activity. Learning behavior involves what students do
while learning, such as the time and reasons of interruptions, and the frequency, quantity, and their ways of seeking
help. Monitoring items can be also easily recorded when learners engage in learning activities. Some learning
interruptions may be caused by various situations, such as the presence of a TV or domestic errands. It is impossible
for a computer-based system to record events automatically. Because the proposed system is designed for students
who want to become self-regulated learners, our system provides an interface for them to manually input
interruptions easily. Monitoring items will be demonstrated in the form of charts after the behavior analysis so
learners can quickly understand their study habits.
Psychologists argue that regularity, referring to the need to observe behavior frequently instead of sporadically, and
proximity, referring to behavior that should be recorded close in time to its occurrence, are important characteristics
of effective self-monitoring (Bandura, 1986). The proposed system provides an easy self-monitoring environment
that has the characteristics of regularity and proximity. For regularity, the Event Monitor Subsystem continuously
observes learners’ behavior anywhere and anytime they experience SRL. As for proximity, learning behavior can be
monitored immediately. Therefore, it is expected that the system can help them understand their learning habits.
The learners need to evaluate their progress either during or after an activity because both the difficulty of the
examination and lack of preparation may incur disappointing performance (Ghatala, Levin, Foorman, & Pressley,
1989). The evaluation should be based on both objective and subjective criteria. The system has a Self-Evaluation
Subsystem that includes an Assessment Module and a Self-checking Module to obtain objective and subjective
cognitions, respectively. Learners are able to identify the gap between what they think about their learning and what
learning outlook they actually have by comparing the objective and subjective cognitions.
After finishing an activity, learners then proceed to the Analysis state, where their behavior is examined. Through the
aid of data mining, meaningful information is dug up from the data in the learners’ profile. The information includes
the differences between learning and the scheduling of an activity and the expected and real scores, among others.
This information is used to determine barriers and bottlenecks in the learning processes, and to find solutions to
certain problems. Students can also read others’ learning analysis, which may help them to either adopt strategies to
other learners or help them find their fulfillment (e.g., rank high among ones classmates.) According to the
monitoring items, many kinds of analysis charts for learning time, interruptions, used materials, and so on are
provided.
When in the Analysis state, learners are able to better understand their learning patterns when provided with the
aforementioned information. Learners therefore try to use various strategies to improve their performance. The
87

instructors can also receive the feedback information for the preparation of following learning templates through
information exchange. In addition, the system automatically downloads other learners’ profiles so that those who
learn the same activity can refer to the learning strategies of others. By entering the Activity Scheduling state again,
they better arrange their schedules based on the experience acquired from prior learning activities, allowing them to
perform more effectively. In the process of doing so, they can be self-regulated learners.

Evaluation of the proposed SRL system
An experiment was conducted in a high school to demonstrate the effectiveness of the proposed SRL system. The
experiment addresses the issues: “Can the system help learners possess the four self-regulatory attributes?” and
“How much does the system help learners to improve their SRL efficiency?”

Method
In the experiment, the target learners were secondary students because we assumed that, though they were eager to
learn, they still lacked SRL skills. The learning topic was English. The experiment consisted of two steps to evaluate
the effectiveness of the proposed system. The first step was to classify the students who learned English in a selfregulatory way. Students performed SRL without the help of the system in this step. Since the number of students in
this step is large enough, the classification indicates the SRL types of secondary school students in Taiwan. This
classification was used in Step 2 to identify the SRL types of students. Through this step, the following step
discusses the differences between SRL types of students before and after using our SRL system. A pre-test and a
post-test were conducted in the second step to evaluate the improvement of the students’ SRL skills after using the
proposed system. The SRL types of learners in Step 2 were classified into three types based on the classification built
in Step 1. The analysis of the experiment mainly focused on the learners in Step 2 because the proposed system was
involved in their learning. The impacts of the use of the proposed system on the learning performance of the different
types of SRL learners are observed in this step.
In Step 1, four grade ten classes (42 students in one section, and 43 students in the others) were chosen to selfregulate their English learning for 10 weeks in 2004. Students had the same English instructor and were asked to
study 10 lessons from the Studio Classroom magazines (StudioClassroom, 1962), a popular English instructional
magazine in Taiwan. Their instructor taught them some SRL skills and gave them hardcopy forms to record their
learning behavior and their reflections. After the ten weeks, every student was asked to fill out a MSLQ (Motivated
Strategies for Learning Questionnaire). The original MSLQ was designed by Pintrich, Garcia, & Mckeachie (1993)
to assess college students' motivational orientations and their use of learning strategies. Wu and Chan translated the
MSLQ into Chinese and modified the questionnaire for elementary school students (Wu, 1998). In the experiment,
the MSLQ was used for high school students taking up English. There are 91 items in the MSLQ, and some are listed
in Table 2.
The questionnaire used a seven-point Likert-type scale. The students would receive one to seven marks for each item
in the MSLQ. They were divided into three groups (high SR, medium SR, and low SR) according to normal
distribution. Students whose SRL scores were higher than 485 marks (25% students) formed the group with high SR,
students whose SRL scores were between 484 marks and 423 marks (50% students) formed the group with medium
SR, while the others formed the group with low SR (25% students).
Table 2. Partial items in the MSLQ used in the experiment
I believe I will receive an excellent grade in class.
I'm certain that I can understand the most difficult material in English Learning.
I am very interested in the content area of English learning.
In Step 2, 17 volunteers from one of the grade 11 classes (apart from the four classes in Step 1) were involved in a
three-week SRL. Each student was given a Hyperbook, a Hyperpen, and a tablet PC. The Hyperbook contained six
English lessons from the IVY magazines (Ivy League, 2006), which are also popular English learning materials in
Taiwan.
88

Before the experiment, a pre-test was employed for the 17 students. The MSLQ used in Step 1 was also used in the
pre-test, with the goal of determining the SRL patterns of the students before using our system. Three weeks later,
the students took a post-test. They were asked to fill out two questionnaires. One was an MSLQ, which was used to
discern the SRL patterns of the students after the experiment. The other was the Self-Regulated System Indication
Questionnaire (SRSIQ), which was used to evaluate the support of the self-regulatory attributes.
Generally, an SRL can be surveyed in different psychological dimensions of research by using following scientific
questions: why, how, what, and where (Schunk & Zimmerman, 1994). SRSIQ asked these scientific questions in a
questionnaire filled out by students, as listed in Appendix A. The questionnaire applied a five-point Likert-type scale
ranging from 1 (Strongly Disagree) to 5 (Strongly Agree). The results retrieved from this questionnaire were used to
determine the assistance of the system during the students' SRL.

Results
The experiment results consisted of two parts. The first focuses on the analysis of the four self-regulatory attributes
based on the SRSIQ results of the students. The second part looks into the students’ progress in acquiring the SRL
skills.

The Support of Self-regulatory Attributes
First, the students were asked if the system helped them acquire the four self-regulatory attributes. The reliability of
whether our system motivated students is 0.81, implying that the questionnaire items have a high reliability. The
result shown in Table 3 indicates that the system supports the attribute (Mean=3.329, SD=0.897). This subscale is
greatly influenced by the amount of motivation inspired by our system. The SRL system encourages students to use
supplementary multimedia materials (Q13, Q14, Q15, and Q16) and learning analysis (Q27). Most students thought
that supplementary multimedia materials were useful for their learning. According to the results of the behavior
observation, each student was willing to access the supplementary materials (74.14 materials per student) Therefore,
it can be shown that the supplementary materials and the methods used to access the Internet enriched their learning
experiences and made them eager to learn more. Therefore, the students had spontaneous pleasure in learning
English.

Q13
Q14
Q15
Q16
Q27

Table 3. Item statistics of attribute: Intrinsically or self-motivated
Mean
Std. Deviation
3.24
.970
3.47
.943
3.47
1.007
3.47
1.068
3.00
.707

N
17
17
17
17
17

The reliability of whether our system helps students become systematic is 0.758. This indicates that the questionnaire
items also have a high reliability. The results also show that all the functions enabled learners to use proper learning
strategies (Mean=3.34, SD=0.749). Among the questionnaire items listed in Table 4, prior scheduling experience
gave significant assistance to students in planning their schedules (Q9). Nevertheless, item Q10 indicates that the
interface of the learning review tool should be more user-friendly. Because the interface of the tool is divided into
two parts, students had to frequently switch from one part to the other in order to view their scheduled and learned
activities. Aside from this, other functions (e.g., synchronizing learning records and starting a learning activity) were
executed through the tool as well, so students thought that the operation of these functions was complicated.
Therefore, students deemed that the interface was not user-friendly enough, and that it should be simplified and
intuitive.
The response on whether the system helped students become self-aware of their performance outcomes also has a
good reliability (Cronbach's Alpha=0.91). Thirteen items tapped on subjective learning performance. The item
statistics, shown in Table 5, show that the information brought to learners gave them a sense of fulfillment
(Mean=3.357, SD=0.766). Through learning analysis, students thought that our system could precisely monitor their
89

behavior and easily record interruptions. Most students also agreed that the Self-Evaluation Subsystem helped them
to conveniently record the cognition to a learning activity, which was used to determine the gap between subjective
and objective learning achievements (Q18). Students also agreed that the learning analysis was helpful (Q25).
However, though learning monitoring was generally regarded as a useful function to understand one's own learning
status (Q19; Q20), the students felt that the function was a little bit hard to use (Q21). This may be attributed to the
fact that the behavior was observed when students used the functions of the proposed system. In the experiment, the
proposed system was installed in the tablet PC, which ordinarily had no input devices such as a keyboard and a
mouse. Unlike desktop PCs, the operation in the tablet PC was more difficult and unfamiliar to the students.
Additionally, the recognition ratio of the Hyperpen was about 89%. Students were not satisfied with the ratio. "It is
hard to scan the tags," one student said. Therefore, our future work should address this because higher recognition
rates can encourage the use of our system.

Mean
Q1
Q2
Q3
Q4
Q5
Q6
Q7
Q8
Q9
Q10
Q24
Q25
Q30

Q11
Q12
Q18
Q19
Q20
Q21
Q22
Q23
Q25
Q28
Q29
Q32
Q33

Table 4. Item statistics of attribute: Planned or automatized
Std. Deviation
N
3.12
.781
3.12
1.166
3.29
.849
3.29
.686
3.47
1.068
3.47
.717
3.47
.874
3.53
.874
3.59
.870
3.06
.966
3.41
.795
3.29
.686
3.29
.772
Table 5. Item statistics of attribute: Self-aware of performance outcomes
Mean
Std. Deviation
3.41
.939
3.35
.702
3.47
.800
3.29
.920
3.35
.702
3.00
.866
3.41
.795
3.41
1.004
3.47
.717
3.24
1.200
3.41
.870
3.47
.800
3.35
.931

Mean
17
17
17
17
17
17
17
17
17
17
17
17
17

N
17
17
17
17
17
17
17
17
17
17
17
17
17

On average, students were positive on the attribute Environmentally/socially sensitive and resourceful (Cronbach’s
Alpha=0.852). The result in Table 6 indicates that the function of the learning record synchronization (Q31), the
learning materials (videos (Q13), pronunciations (Q14), translations (Q15), and phrases (Q16)), as well as Internet
searching (Q17) could inspire students to obtain and seek useful learning resources, as shown in Table 6. Among
them, Searching on the Internet (Q17) was the least useful function. Because the topic was English, the students
thought that the resources provided by the system were enough and so they had less will to search for extra resources
from the Internet. In the future, the system should be modified to support different types of help seeking functions for
different kinds of learners. For example, the system should enable skillful learners to search for extra materials and
allow unskilled learners to become accustomed to using additional resources.
Genreally, the target students deemed that the proposed system could help them possess the four self-regulatory
attributes, albeit some functions have to be improved.
90

SRL Effectiveness
In this section, the SRL scores of the students are studied. These students are also classified into three groups,
according to the results in Step 1. Of the total number of students, 71% increased their SRL scores after using our
system. On average, the SRL scores of these students increased by 11.06 points.
For a precise analysis, a T-test was used to determine if the proposed system could efficiently improve the learners'
SRL scores. We find that the difference between the means is not statistically significant (t = -1.606, df = 16, p >
0.05, one tailed), so the system cannot help students to significantly improve their SRL performance. This may be
because SRL skills should be developed over a long period of time, and three weeks of SRL may not be long enough
for learners to improve their SRL skills.

Q31
Q13
Q14
Q15
Q16
Q17

Table 6. Item statistics of attribute: Environmentally/ socially sensitive and resourceful
Mean
Std. Deviation
N
3.29
.772
17
3.24
.970
17
3.47
.943
17
3.47
1.007
17
3.47
1.068
17
3.18
1.131
17

The SRL scores of all students in the group of low SR increased, except for one student. Those who lack learning
experience can easily follow different learning styles, such as computer-assisted learning or distance learning. On the
other hand, skillful learners have more difficulty in changing the learning patterns they have developed (Morgan,
Dingsdag, & Saenger, 1998). Therefore, whether or not the proposed system significantly improves the SRL scores
of the students in the group of low SR is analyzed. The difference between the means is significant at the 0.05 level
(t =-3.136, df = 9, one tailed). The result shows that the SR skills of students in the group of Low SR significantly
improved, which corresponds with the findings of Morgan et al.

Conclusions
This paper proposes an SRL system that involves the self-regulatory learning cyclic and scaffolding theories to
cultivate self-regulated learners. The system aims to construct a mobile, portable, and personalized learning
environment for SRL that can be used anywhere and anytime. To help learners gradually develop their SRL skills,
instructors are involved in the system. The learner side system enables students to start SRL anytime and anywhere,
obtain learning materials and assistance instantly, realize their learning patterns, cultivate their SRL behavior, and
sustain their interest in self-learning.
Generally, the experiment results show that the system has improved SRL skills, though the improvement is not
significant. However, the result showed that the SR skills of students in the Low SR group improved significantly.
Moreover, most of students deemed that the overall interfaces of the proposed system were user-friendly and could
give them valuable progress in SRL. The students also agreed that the system enabled them to acquire the four SRL
attributes.
Based on the suggestions of the students, we will make the interfaces of the SRL system more user-friendly and
improve the recognition ratio of the Hyperpen to facilitate its operations. We also aim to use other devices such as
PDAs or mobile phones as learning instruments because they are easy to carry and can facilitate quick learning. As a
result, learners would more be likely to use the proposed system. Experiments on a larger number of students over a
longer period of time shall also be conducted to improve the system.

Acknowledgement
The work was partially supported by the National Science Council of the Republic of China under Grants NSC 973114-E-119-001, NSC 97-2221-E-032-021, and NSC 98-2218-E-254 -001.
91

References
ADL Technical Team (2006). SCORM 2004 3rd Edition, retrieved Jun. 18, 2009 from http://www.adlnet.gov/scorm/index.cfm.
Answers.com (2005) Answers.com- Online Encyclopedia, Thesaurus, Dictionary definitions and more, retrieved Aug. 23, 2009
from http://www.answers.com/.
Azevedo, R., Cromley, J. G., Thomas, L., Deibert, D., & Tron, M. (2003). Online process scaffolding and students' self-regulated
learning with hypermedia. Annual Meeting of the American Educational Research Association, 31.
Bandura, A. (1986). Social Foundations of Thought and Action, Englewood Cliffs, NJ: Prentice-Hall.
Bluetooth® SIG, Inc. (2003). Bluetooth Core Specification Version 2.1 + EDR, retrieved Jun. 18, 2009 from
https://www.bluetooth.org/spec/.
Bruner, J. (1983). Child's talk. Learning to Use Language, New York: W. W. Norton.
Butler, D. L. & Winne, P. H. (1995). Feedback and self-regulated learning: a theoretical synthesis. Review of Educational
Research, 65(3), 245-281.
Chen, Y.-S, Kao, T.-C., & Sheu, J.-P. (2003). A Mobile Learning System for Scaffolding Bird Watching Learning. Journal of
Computer Assisted Learning, 19(3), 347-359.
Dabbagh, N., & Kitsantas, A. (2004). Supporting self-regulation in student-centered web-based learning environments.
International Journal on E-Learning, 3(1), 40-47.
Ghatala, E. S., Levin, J. R., Foorman, B. R., & Pressley, M. (1989). Improving children's regulation of their reading prep time.
Contemporary Educational Psychology, 14, 49-66.
Hadwin, A. F., & Winne, P. H. (2001). CoNoteS2: A software tool for promoting self-regulation, Educational Research &
Evaluation, 7, 313-334.
Hofer, B. K., Yu, S. L., & Pintrich, P. R. (1998). Teaching College Students to Be Self-Regulated Learners. In Schunk, D. H. &
Zimmerman B. J. (Eds.) Self-regulated learning- from teaching to self-reflective practice (pp. 57–85), New York: Guilford.
Ivy League (2006). Ivy League Analytical English, retrieved May 3, 2009 from http://www.ivy.com.tw/.
Lan, W. Y. (1998). Teaching self-monitoring skills in statistics. In Schunk, D. H. & Zimmerman B. J. (Eds.) Self-regulated
learning- from teaching to self-reflective practice (pp. 86–105), New York: Guilford.
Looi, C.-K., Seow, P., Zhang, B. H., So, H.-J., Chen, W., Wong, L.-H. (2009). Leveraging mobile technology for sustainable
seamless learning: a research agenda. British Journal of Educational Technology, 41(2), 154-169.
Pintrich, P. R. (2000). The role of goal orientation in self-regulated learning. In Boekaerts, M. & Pintrich, P. R. (Eds.) Handbook
of Self-Regulated (pp. 13-39), San Diego: Academic.
Pintrich, P. R., Garcia, T. & Mckeachie, W. J. (1993). Reliability and Predictive Validity of the Motivated Strategies for Learning
Questionnaire (MSLQ). Educational and Psychological Measurement, 53(3), 801-813.
Macromedia Inc. (2005). Macromedia Flash MX 2004, retrieved Sep. 25, 2009 from http://www.macromedia.com.
Morgan, C. J., Dingsdag, D. & Saenger, H. (1998). Learning strategies for distance learners: Do they help. International Journal
Distance Education, 19, 142-156.
Schunk, D. H. & Zimmerman B. J. (1994). Self-Regulation of learning and performance: Issues and educational applications,
Hillsdale: New Jersey: Lawrence Erlbaum.
Shih, K.-P., Chang C.-Y., Chen, H.-C., & Wang, S.-S. (2005). A Self-Regulated Learning System with Scaffolding Support for
Self-Regulated e/m-Learning. Proceedings of the 3rd IEEE International Conference on Information Technology: Research and
Education (ITRE 2005), 30-34.
StudioClassroom (1962). StudioClassroom.com-Your Friend for Life, retrieved Sep. 25, 2009 from http://studioclassroom.com/.
Wang, T.-H. & Shih, T. K. (2006). Integration of multimodal multimedia devices and hardcopy textbooks for supporting
pervasive e-learning. Paper presented at the First International Symposium on Pervasive Computing and Applications (SPCA06),
August 3-5, 2006, Urumchi, Xinjiang, China.
Wu, J.-J. (1998). Consortium of Research on Creativity and Innovation, retrieved Oct. 25, 2009 from
http://tim.nccu.edu.tw/croci/group/wujingji.htm.
Yahoo! Taiwan Inc. (2007). Yahoo! Dictionary, retrieved Sep. 12, 2009 from http://tw.dictionary.yahoo.com/.
Young, M.F. (1993). Instructional design for situated learning. Educational Technology Research & Development 41(1): 43-58.
Zimmerman, B. J., Bonner, S. & Kovach, R. (1996). Developing Self-Regulated Learners: Beyond Achievement to Self-Efficacy,
Washington, DC: APA.
Zimmerman, B. J. & Paulsen, A. S. (1995). Self-monitoring during collegiate studying: An invaluable tool for academic selfregulation. New directions for teaching and learning, 1995(63), 13-27.
Zurita, G. & Nussbaum, M. (2004). A constructivist mobile learning environment supported by a wireless handheld network.
Journal of Computer Assisted Learning, 20(4), 235-243.
92

Appendix A: Self-Regulated System Indication Questionnaire (SRSIQ)
Q1
Q2
Q3
Q4
Q5
Q6
Q7
Q8
Q9
Q10
Q11
Q12
Q13
Q14
Q15
Q16
Q17
Q18
Q19
Q20
Q21
Q22
Q23
Q24
Q25
Q26
Q27
Q28
Q29
Q30
Q31
Q32
Q33

Learning Scheduler Subsystem provides clear-cut lesson information to schedule learning.
With drag-and-drop, I plan my schedule smoothly.
Learning scheduler subsystem helps me determine the study plan.
Functions of Learning Scheduler Subsystem are varied and handy.
Learning Scheduler Subsystem is helpful in setting appropriate learning goals and plan setting.
Learning Review Tool helps me manage the learning activities.
Learning Review Tool makes me understand the time spent on prior learning activities.
I can see every learning activity by examining the information in the Learning Review Tool.
Tracking the previous learning schedules, I set better plan in the next step.
Learning Review Tool is designed with user-friendly interface.
The "pause" function helps me unhurriedly note down every interruption while learning.
Recording learning time helps me know my own learning progress.
Introductory videos provided by the system are useful for the beginning of my learning.
Recordings provided by the system are useful for my learning.
The function of text translation is useful for my learning.
The provided vocabularies and phrasal verbs in the Hyperbook are useful for my learning.
Hyperpen is convenient for searching resources on the Internet.
Self-evaluation helps me immediately note down my experience and feeling along the learning.
Items of learning recording and learning monitoring are listed comprehensively.
Learning and monitoring tools enable me to precisely manage my learning.
Learning and monitoring tools are designed with user-friendly interface.
Learning analysis outcome in diagrams and illustrations offers clear ideas.
Interruption analysis helps me to know where distractions come from.
In order to keep interruptions out of the process, I amend my learning strategies according to the reasons
of interruptions.
The statistics coming from the use of provided multimedia helps me know my own learning habits.
The analysis of learning outcome is a good reference for following a schedule design.
The analysis of all learners improves my learning desire.
Online analytical dictionary shows me unfamiliar words.
The analysis of online resource searching record shows me unfamiliar territory.
The learning analysis of all learners helps me amend my learning strategies.
The synchronization of learning records is designed with user-friendly interface.
The analytical tool assists me in understanding my own learning.
The analytical tool is designed with user-friendly interface.

93

Chen, H.-P., Lien, C.-J., Annetta, L., & Lu, Y.-L. (2010). The Influence of an Educational Computer Game on Children's Cultural
Identities. Educational Technology & Society, 13 (1), 94–105.

The Influence of an Educational Computer Game on Children’s Cultural
Identities
Hsiang-Ping Chen1, Chi-Jui Lien2, Len Annetta3 and Yu-Ling Lu4
3

1,2,4

Department of Science Education, National Taipei University of Education, Taiwan // College of Education,
North Carolina State University, USA // yllu@tea.ntue.edu.tw

ABSTRACT
This study develops an educational computer game, FORmosaHope (FH), to explore the influences that an
educational computer game might have on children’s cultural identities. FH is a role-playing game, in which
children can actively explore a mini-world to learn about science, technology, and society. One hundred and
thirty sixth-graders, about 11-12 years old, from four classes in a middle-sized elementary school in Taiwan,
participated in the study. A quasi-experimental design was used. The experimental group was two classes that
explored FH for a period of six weeks. The other two classes that served as the control group did not receive any
experimental treatment. Descriptive statistics, T-test and ANCOVA showed that the experimental group
significantly strengthened their cultural identities compared to the control group. This implies that educational
games can have an impact on children’s cultural identities through their educational contexts.

Keywords
Educational game, Computer game, Cultural identity, Effectiveness evaluation, Individual identity

Introduction
Culture recognition starts from a very early stage of life. Some studies have confirmed that different children hold
perceptions about culture differently (Nixon & Comber, 2006). These cultural perceptions not only shape and
determine a person's way of perceiving and reasoning (Hofmann, 2006), but also influence one’s learning (Nixon &
Comber, 2006). Moreover, these cultural identities contribute to group dynamics and the growth of institutes or
communities (Ledoux, 2005). Thus, many efforts from a local level, such as a community (Brooklyn Historical
Society, 1990), or from national level (Laitin, 1997; Van Gorp & Renes, 2007), have been made to increase the
knowledge and understanding of the culture around individuals. In many educational systems, e.g., California State
in US and Taiwan, explicitly or implicitly use cultural recognition and respecting different culture as the core of
cultural learning (California State Department of Education, 2000; Ministry of Education, 2006).
However, national or state formal curriculums have been doubted for their function on cultural learning. Many
studies have maintained that the incorporation of informal approaches into formal learning is a necessity (Kopong,
1995; Ninnes, 1995). It is reasonable to assume that teaching cultural identity may not be successful, if the formal
curriculum is the only approach. A value system needs to be built up holistically. Value learning and/or cultural
learning need some additional and perhaps more innovative approaches. We believe that giving students
opportunities with active involvement and spontaneously emotional attachment, such as those in video games, have
more potential to make cultural learning more meaningful than traditional schooling.

Purpose of the study
This study adopted an innovative approach by using a culturally enriched educational game, FORmosaHope (FH), to
ascertain the relative effectiveness of gameplay as a form of cultural learning. The research questions of this study
are the followings:
1. How effective is the educational game, FH, on Taiwanese student learning of cultural identity?
2. Do students’ gender or their family's societal status influence the growth of cultural identities in an educational
game environment?
3. How do students feel about the educational game, FH, after they have experienced it?

Theoretical Framework
An important declaration on cultural policies has been made in an UNESCO-based (United Nations Educational,
Scientific and Cultural Organization) conference held in Mexico City (World Conference on Cultural Policies,
ISSN 1436-4522 (online) and 1176-3647 (print). © International Forum of Educational Technology & Society (IFETS). The authors and the forum jointly retain the
copyright of the articles. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that copies bear the full citation on the first page. Copyrights for components of this work owned by
others than IFETS must be honoured. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior
specific permission and/or a fee. Request permissions from the editors at kinshuk@ieee.org.

94

1982). It asserted that “the cultural identity is a treasure that vitalizes mankind's possibilities of self-fulfillment by
moving every people and every group to seek nurture in its past, to welcome contributions from outside that are
compatible with its own characteristics, and so to continue the process of its own creation”. This declaration also
appealed that “the equality and dignity of all cultures must be recognized” and “the international community
considers it its duty to ensure that the cultural identity of each people is preserved and protected (World Conference
on Cultural Policies, 1982). Nowadays, developing one’s cultural identity has well recognized as human nature right
and protecting this right has also become universal value.
To preserve and to protect this important dimension of culture, an instrument to measure individual’s culture
identities was needed. However, after reviewing related literatures, this study found that there is no instrument has
been developed. Tracing back to the definition of culture, the Mexico declaration stated culture as, “the whole
complex of distinctive spiritual, material, intellectual and emotional features, that characterize a society or social
group (World Conference on Cultural Policies, 1982).” This highlighted rough components of “culture.” Another
framework that was helpful to establish the component of “cultural identity” was depicted in a booklet of guidelines
which was developed to help indigenous peoples to receive equitable culture resources (First Nations and Métis
Education Branch, 1995). This booklet examines three portrayals for maintaining qualities of transmitting “culture”
and “culture identity.” The three dimensions are “Portrayal of Cultural Interactions”, “Portrayal of Identity”, and
“Portrayal of Traditions and Institutions.” After reviewing the design intention and items in each dimension, some
characteristics were formulated. First, the “Cultural Interactions” in this booklet stressed contributions of my culture
to “other cultures.” Secondly, the “Portrayal of Identity” stresses “what we are”, but not “exchange and interact with
other cultures.” Third, the “Portrayal of Traditions and Institutions” was a mix. It examine both “what we are” and
“what interactions that we have with others.” Thus, two dimensions could be extracted from above three portrayals:
one focused on “my culture” and the other focused on “my culture vs. other cultures.” These viewpoints as well as
many studies in Taiwan, those defined cultural identified as spiritual, material, and societal identify (Lee, 1993;
Yang, 2001), have provided useful information for further development of instrument for measuring cultural
identities.
Cultural identity, in nature, is a matter of perspective-taking. Perspective-taking provides the opportunity to consider
others’ viewpoints and induce cognitive conflict. This type of growth and recognition of self does not happen in
isolation rather it occurs through the cognitive development of social interactions and/or moral experiences
challenging conflict between thought and behavior resulting in more sophisticated, consistent and comprehensive
perspective-taking behavior (Hall & Bishop, 2001; Selman, 1977). Through social games and social and moral
dilemmas, Selman and Byrne (1974) identified four developmental levels of social perspective-taking that are agerelated in a form similar to Piaget’s cognitive operations.
Perspective-taking levels are basic structures of social reasoning and are used in content areas such as interpersonal
relations, moral reasoning, social problem solving, communication skills, etc. This is in alignment with the five-step
sequence outlined in the practice-oriented Icelandic Project by Adalbjarnardottir (Selman, 2003) which lead class
discussions about social conflicts. This approach is designed along the lines of a three-step approach first suggested
by John Dewey in the early 1930s. Both approaches were employed by Selman (2003) as a pedagogical practice for
social conflicts. In brief social conflicts are approached by: defining the problem; considering the feelings of other’s
involved; brainstorming alternative ways to solve the dilemma; choosing a course of action; and evaluating the
probable outcome. This implies that intervention research should aim to stimulate perspective-taking through content
areas of social reasoning (Selman, 1977).
Gaming is a social construction that varies heavily according to culture, gender, social strata, and the various
representations (Brougere, 1999). Video games matter because they present players with simulated worlds; worlds
that, if well constructed, are not just about facts or isolated skills, but embody particular social practices. Games
bring together ways of knowing, ways of doing, ways of being, and ways of caring, making the player experts in the
situated environment. The skills and knowledge are distributed between the virtual actor and the player; hence the
values, skills and practices are distributed (Shaffer, Squire, Halverson, & Gee, 2005). In the book, Got Game, Beck
(2004) stated that Gamers are naturally global: Members of the game generation have been exposed to a multitude of
cultures through the gaming experience.

95

Methodology
This study incorporated a quasi-experimental design consisting of a diverse population and two questionnaires
designed to collect information on student understanding of cultural identity and their opinions toward the
educational game. This section will explain 1. Participants, 2. The Culture-Enriched Educational Game (FH), 3.
Questionnaires, including: The Questionnaire of Cultural Identities (QCI), and The Questionnaire of Students
Opinions toward the Educational Game (QSO), 4. Procedures, and 5. Data analysis. These are described in the
followings.

Participants
Four classes of sixth-grade students from a mid-size elementary school in Taipei City, Taiwan, participated in this
study. The experimental group consisted of 64 students (female, 29 and male, 35) in two classes using FH as a
supplemental activity; the control group consisted of 66 students (female, 30 and male, 36) in another two classes
proceeding with their normal instruction. Because the school’s policy is to try to maintain the equivalence of classes,
the class assigning process was based on students’ previous academic achievements. From the perspective of
students’ academic achievements, the four classes maintained equality to some degree, however, there is no evidence
to support that equality existed between the experimental and control group, in terms of cultural identities. Thus, a
statistical control of variability will be used. The community is located in a residentially and commercially mixed
area. Students, in both control and experimental groups, represented a variety of socioeconomic statuses and
backgrounds; which made this population viable for this study.

Figure 1. One of the design models used in developing this culture-enriched educational game, FH.
For the purpose of answering the second research question, students were stratified into three groups, the upper,
middle, and lower socioeconomic family groups. The criterion for grouping was adopted from a study (Lin, 2001),
which is a revised version from a wide-used Hollingshead Index of Social Position (HISP) (Hollingshead, 1965). The
Hollingshead grouping system used the education and occupation of the parent and guardian, as indicators. The
equation was: HISP score = (Education score X 4) + (Occupation score X 7). Lin’s system continued using the
equation, however, the seven levels of educations and seven levels of occupations were both degenerated to five
levels for simplifying the scoring process. Once, these two factors were assigned numbers from 1-5 (1 for lowest,
and 5 for highest), according to level of education and occupation. Then, the Lin’s Index of Social Position score was
96

calculated and used as indicator to classifying into group. Scores ranging between 41-55 was defined as upper; 30-40
as middle; and 11-29 as lower socioeconomic status group. After this grouping process, there were 20, 28, and 16
students in the upper, middle, and lower socioeconomic groups, respectively.

The Culture-Enriched Educational Game (FH)
This research group has developed an educational game, FORmosaHope (FH) (Lien, Lu, & Cheng, 2006), targeting
4th through 7th graders. The rationale used for organizing learning in the software is to integrate science, technology,
and society in the educational game. A design model used to organize the learning contents and activities in the
educational game is shown as Figure 1. In this figure, an example was used to show how the colorful culture,
traditional wisdom, and unique society of the Tao Tribe being integrated into a meaningful learning activity in the
education game, FH.
The FH consists mainly of two parts. The first one is a role exploration. A player enters the “Village” for a free
exploration and may trigger a series of learning events. Figure 2 is one of snapshots of “The Tao Tribe” event which
has illustrated as Figure 1. During the exploration, the player may interact with various kinds of people in a virtual
society. They also play and gain knowledge about science and technology around them as well as about the society
and culture.

Farmer Springer:
They (The Tao Tribe people, who live in a small island
in the south-east of Taiwan) will discard all flying
fishes that they still preserved after September, because
they believe that the excess catch will result in
misfortune.

Figure 2. Explore and learn in the “Village”
The second part of FH is “Touring Taiwan Island.” Once the player accomplishes some requirement in the village,
he or she will be able to visit more than one-hundred towns (stands) in Taiwan (Figure 3). Each will provide
opportunities allowing students to learn distinguishable features of that town. The educational content in the game is
organized with an inter-disciplinary approach. Students may experience different communities, cities, types of jobs,
folklore, and cultures. In the process, students will be able to know, to learn, to appreciate the society and culture,
and to grow.
Aunti Amei (in
the Township
Mei-loong):
(After the paper umbrella is well
assembled…) We put persimmon juice on
the surface. Then, skillful artists may
paint or write poems or inscriptions. After
that, wood oil is finally lacquered. By
doing these, a fine traditional paper
umbrella is born.

Figure 3. Screenshots of the Touring Taiwan
In addition, there are 11 sub-games to provide many types of learning through game play. These sub-games are
arranged randomly and will be triggered when students move to some place or stand. One of the sub-games is shown
in Figure 4.
97

For whom is this figure in the
spotlight most useful?

Writer

Musician

Philosopher

Scientist

Figure 4. Screenshot from one of the eleven sub-games
Questionnaires
Two types of questionnaire were developed to assess the effects of the FH: The Questionnaire of Cultural Identities
(QCI), and The Questionnaire of Students Opinions toward the Educational Game (QSO). The information collected
with QCI was used to answer the first and second research questions; the information collected with QSO was used
to answer the third research question.
The QCI stated with demographic questions related to student’s gender, parent/guardian's level of education and
occupation for further analyzing purposes. The construction of the QCI mainly borrowed ideas from the Mexico
Declaration (World Conference on Cultural Policies, 1982), Lee (1993), Yang (2001) and The First Nations and
Métis Education Branch, Saskatchewan (1995) and modified based on our research emphases. Lee’s and Yang’s
studies used spiritual, material and societal compartments to describe cultural identities. The Mexico Declaration
used spiritual, material, intellectual and emotional features to conceptualize the “culture”. In QCI, spiritual, and
material were adopted and namely, “Spiritual Cultural Identities (SpiCI)” and “Material Culture Identities (MtlCI)”.
However, considering that the educational game, FH, was heavily science-learning-oriented, the original intellectual
and emotional features were replaced to “Scientific Culture Identities (SciCI).” Furthermore, document of The First
Nations and Métis Education Branch, Saskatchewan highlighted the importance of societal tradition and institutions,
thus, the QCI followed Lee’s and Yang’s studies and use the name, “Societal Culture Identities (SocCI).” However,
there were ambiguities between the SocCI and SpiCI; this was resolved with the ideas derived from the same
manuscript. The distinction is that, SocCI in QCI focused on “my culture”, while the SpiCI focused on “my culture
vs. other cultures.”
After the four facets were decided, a total of 26 items were developed and tested. To confirm that the presentation of
all items would be suitable to assess the expected cultural identities, eight students represented different learning
achievements were assigned to the pilot test and to make sure the items were clear and easy to understand. Students’
comments were collected and used to revise the draft. Then, the revised draft was reviewed by experiencing science
teachers, social studies teachers and two professors in education to check whether all items matched the goals and to
help to revise the questionnaire to enhance the quality of the draft. Once the expert validity of the questionnaire was
established, 101 students answered the questionnaire for item analyses and reliability analyses. Three items of
twenty-six items did not pass the item analysis, thus were deleted. The final QCI has 23 items in total. Numbers of
items in each facet were 5, 7, 5, and 6 for SpiCI, MtlCI, SocCI, and SciCI, respectively. The internal consistency
reliability, Cronbach α, of this questionnaire was .84. The result showed that the QCI can be considered as a valid
instrument. A description for the four facets, with a sample item, was shown as follows:





In the SpiCI facet, measuring perception of the extent to which student’s confidence in his/her culture when
interacting with others cultures, for example, “If I can, I would like to let more people in different parts of the
world know more about Taiwan.”
In the MtlCI facet, measuring perception of the extent to which student prefers his/her environment and material
life in his/her society, for example, “I feel that imported clothes are better than locally made.”
In the SocCI facet, measuring perception of the extent to which student values his/her society and culture, for
example, “I believe that cultures of different ethnic groups in Taiwan are equally fine.”
In the SciCI facet, measuring perception of the extent to which student’s confidence in science and technology
of his/her society and his/her interest in science, for example, “Science was invented in the western world, and
then, passed to us.”
98

The items of the questionnaire were adapted to the Likert format. The numbers from 1 to 5 stands for strongly
disagree, disagree, neutral, agree, and strongly agree, respectively. For negative items, scores were converted, so that
higher scores always represent student’s higher cultural identity on his/her society. The scores of items in each facet
are used to indicate the strength of identities in each respective facet; a total score represents the strength of student’s
cultural identities on his/her society.
The other questionnaire is the QSO. It was designed to understand students’ opinions about the educational game,
FH. The QSO adapted a two-stage format. In each item, a student needs to make his/her decision either he/she likes
or dislikes the educational game, FH. Secondly, the student needs to choose reasons for like or dislike from what
researchers have provided in the questionnaire or write down the reasons those are not in the list.
Procedures
This study followed a pre-post test, quasi-experimental design with an additional post-test measure. In terms of
cultural identifies, the QCI was used as the pre-, post-test measure, and QSO was used as an additional post-test
measure for the experimental group to collect students’ overall opinions about the experience of using FH.
Before the intervention, the QCI was administered to all the participants in both groups as a pre-test. During the 6week treatment period, the experimental and control groups had the same regular curriculum which was based on
National Curriculum Guideline (Ministry of Education, 2006). The curriculum included seven major subjects and 33
study hours per week. However, for the experimental group, in addition to their routine classes, students played FH
one hour per week for 6 weeks. In the culture-enriched educational game, students traveled in the virtual world to
experience the local culture of the real world which was incorporated with science, technology, and society. In the
presence of each local cultural and related science event, students learned and responded to specially designed
questions or situations. With these, student acted as a tourist, or participant, who was capable of enjoying, learning,
appreciating, and even improving the virtual society. After the experimental treatment, the QCI was administered to
all the participants in both groups as a post-test to measure participants’ cultural identities. Finally, the QSO was
used to evaluate this culturally-enriched educational game learning for the experimental group.
Data analysis
The data collected with the QCI and QSO were analyzed with several sets of statistical analyses. For answering the
first research question, in addition to descriptive statistics by four facets, one-way analysis of covariance (ANCOVA)
was used to compare scores of whole and four facets in terms of cultural identities of the experimental and control
groups. Paired t-tests were also used to observe difference of each questionnaire item in QCI, before and after FH
intervention.
The second analysis examined the improvement of cultural identities of participants in the experimental group, in
terms of different genders and different socioeconomic classes. For this purpose, Two-way analysis of covariance
was used. Post-hoc comparisons were used to distinguish differences among socioeconomic family groups. A
dependent pre, post t-test was also used to observe the improvement in cultural identity, if needed.
For answering the third research question, descriptive statistics were used. The ratios of different opinions were
calculated to be used as the indicator of students’ likeness toward the educational game, FH.

Results and Discussion
The means and standard deviations in pre-tests and post-tests of the cultural identity evaluation for both groups are
presented in Table 1. The results revealed that mean scores indicated by cultural identities of the experimental group
increased in every facet and the total, while for the control group, the total score and scores in each facet did not
show significant gains from pre-test to post-test. The pre-post independent t-tests for both control and experimental
groups have also demonstrated that the treatment in the experimental group significantly improved students’ level of
cultural identities in QCI as a whole (t = 6.956, df = 63, p = .000), while a significant level of improvement was not
found in the control group. Based on results from previous studies, these findings not only demonstrate the stability
99

of cultural identities and their resistance to change in a traditional educational system based on evidence reported in
Table 1, but these results also reveal that a culture-enriched educational game, such as FH, is capable of improving
students’ understanding of cultural identities.
Table 1. Students’ pre- and post-test scores in terms of cultural identities in the experimental and control groups
Measures
Group
Pre-: Mean.
S.D
Post-: Mean
S.D.
Experimental
26.36
4.00
28.38
3.81
Material (MtlCI)
Control
24.68
3.88
24.76
4.34
Experimental
19.53
3.32
21.77
2.70
Societal (SocCI)
Control
19.50
3.16
19.50
3.30
Experimental
20.08
3.02
22.14
2.64
Spiritual (SpiCI)
Control
20.27
2.90
19.64
3.23
Experimental
18.73
4.04
21.56
4.28
Scientific (SciCI)
Control
18.97
3.29
19.23
3.55
Cultural Identities
Experimental
84.70
11.06
93.84
10.53
(total scores, QCI)
Control
83.42
9.29
83.12
10.28
Note. Students in the experimental group = 64, the control group = 66.
After confirming that there was no significant difference between slopes of the experimental group and the control
group, this study proceeded with ANCOVA analyses. The results, reported in Table 2, shows that there were
significant differences between the control and experimental group in all the facets: MtlCI, F = 18.684, p = .000;
SocCI, F = 22.641, p = .000; SpiCI, F = 28.096, p = .000; and SciCI, F = 14.625, p = .000. The significant difference
of the total score of QCI further demonstrated the difference of improvement between the experimental group and
control group, in terms of the growth in cultural identities, F = 43.647, p = .000. In this table, regression effects were
all significant (Pre-test, p = .000). The adjusted mean scores of the experimental and control groups were, 93.47 and
83.39, respectively, which revealed that when students were given opportunities of experiencing this culturalenriched educational game, a significant growth in cultural identities were observed when compared with that of
students not exposed to FH.
Table 2. One-way ANCOVA on cultural identities between the experimental and control groups
Measures
Source
df
Sum of Squares
F
p
1
625.489
52.551
.000*
Pre-test（covariate）
Between groups
1
222.389
18.684
.000*
Material (MtlCI)
Error
127
11.903
Total
130
1
243.398
33.433
.000*
Pre-test（covariate）
Between groups
1
164.831
22.641
.000*
Societal (SocCI)
Error
127
7.280
Total
130
1
146.576
19.143
.000*
Pre-test（covariate）
Between groups
1
215.133
28.096
.000*
Spiritual (SpiCI)
Error
127
7.657
Total
130
1
302.096
22.984
.000*
Pre-test（covariate）
Between groups
1
192.225
14.625
.000*
Scientific (SciCI)
Error
127
13.144
Total
130
1
4479.117
60.642
.000*
Pre-test（covariate）
Cultural Identities
Between groups
1
3223.825
43.647
.000*
(total scores, QCI)
Error
127
73.861
Total
130
Note. * for p < .05

100

Table 3 shows results that students changed before and after the intervention of the educational game, FH, paired ttests for each item were used.
Table 3. Paired t-tests of mean scores before and after educational game intervention
Means
Items
PrePost-

T

p

(MtlCI)
3.5
4.0
4.42 .00*
1. I prefer Taiwanese traditional food to western fast food.
2. Comparing with imported computer games, I lack confidence in games that
3.5
3.9
2.20 .03*
are designed in Taiwan. (-n )
(-n )
3. I feel that imported clothes are better than locally made clothes.
3.0
3.5
2.81 .01*
4. I think, there is no need to keep those historical relics located in valuable
4.4
4.5
0.82 .42
city land. (-n )
(-n )
5. I feel that Taiwan is not as beautiful as many other countries.
4.4
4.4
0.61 .54
6. We may find and import cheaper goods from abroad, thus, there is no need
4.6
4.6
0.36 .72
to develop our traditional industries. (-n )
7. I feel that Taiwan has convenient transportation.
3.1
3.4
2.07 .04*
(SocCI)
1. I feel that we need to preserve and protect different cultures of different
4.3
4.6
2.30 .03*
people in Taiwan.
2. I am interested in reading and knowing information concerning local
3.1
4.1
6.63 .00*
customs and practices.
3. I hope I can taste local specialties all over Taiwan.
4.3
4.5
1.93 .06
4. I feel there is no need to preserve traditional cultures in my society for they
4.4
4.4
0.34 .74
are outdated. (-n )
5. I believe that cultures of different ethnic groups in Taiwan are equally fine.
3.5
4.1
4.47 .00*
(SpiCI)
1. If I could, I would like to let more people in different parts of the world
4.1
4.5
3.47 .00*
know more about Taiwan.
2. Respect cultures of different ethical groups in my country will make us
4.3
4.7
3.81 .00*
more united.
(-n )
3. To worship our ancestor is a superior traditional virtue.
4.0
4.4
3.19 .00*
4. All cultures of different societies need to be preserved.
4.4
4.6
2.47 .02*
5. Invading of foreign cultures through media will eventually extinguish our
3.3
4.0
3.17 .00*
local culture.
(SciCI)
3.1
3.7
3.58 .00*
1. I am interested in doing science activities.
2. Learning science can help me to solve my future problems.
3.6
4.2
5.50 .00*
3. I consider that science in my country is not well developed as other
2.6
3.3
3.37 .00*
countries. (-n )
4. Technology products’ quality of my country is no lower than those of other
3.7
4.1
2.58 .01*
countries.
5. I hope to be a scientist when I grow up.
2.6
2.8
1.12 .27
6. Science was invented in the western world, and then, passed to us. (-n )
3.2
3.4
1.15 .26
Note. 1. (-n ) represented this item is a negative item. Score was then converted, so that the higher score represented
student’s higher cultural identity on his/her society; 2. N of experimental group = 64; 3. * for p < .05
The above t-tests results showed that means of post-tests for all items were enhanced. Moreover, there were 16 out of
23 showed significant differences. With the findings that cultural identities in the experimental group were superior
to that in the control group and the significant improvement demonstrated in each item after the educational game
intervention, both strongly supported the position of this study: an informal approach (i.e., through a computer
game), other than traditional education is a potential way of improving students’ cultural learning.
For answering the second research question, “Do students’ gender or their family's societal status influence the
growth of cultural identities in an educational game environment”, the descriptive statistics of different genders and
socioeconomic groups were calculated, and then a two-way ANCOVA tests were implemented. Table 4 presents the
101

means and standard deviations of the cultural identities pre-tests and post-tests for different genders and
socioeconomic statuses in experimental group. Table 5 presents the two-way analysis of covariance results for
experimental group by gender and socioeconomic status. In the two way ANCOVA, tested for possible differences in
the cultural identities by gender and socioeconomic status, the post-test scores were entered as the independent
variable while the pre-test score served as a covariate. The result of the analysis revealed significant main effects for
gender (F = 4.245, p = .044) and socioeconomic status (F = 5.158, p = .009).

Table 4. Student’s pre- and post-test scores of cultural identities by different gender and socioeconomic status in the
experimental groups
Pre-test
Post-test
Source
Group
M
SD
M
SD
Male
35
85.71
12.21
96.09
11.14
Gender
Female
29
83.48
9.56
91.14
9.21
Upper
20
83.85
12.34
97.55
11.13
Socioeconomic Status
Middle
28
85.39
11.25
90.93
10.79
Lower
16
84.56
9.55
94.31
8.07
Table 5. Two-way ANCOVA on cultural identities in the experimental group: Gender and socioeconomic status
Source
Type III Sum of Squares
df
Mean of Sum of Squares
F
p
1877.895
1
1877.895
26.419 .000*
Pre-test（covariate）
Gender
301.760
1
301.760
4.245 .044*
Socioeconomic Status
733.308
2
366.654
5.158 .009*
Gender * Socioeconomic Status
35.718
2
17.859
0.251
.779
Error
4051.584
57
71.080
Total
570614.000
64
Note. * for p < .05
In the gender aspect, the adjusted post-test scores revealed that male students grew significantly greater cultural
identities than female students. This result of finding differences between different genders is consistent with many
studies. It is believed that this could be attributed to three factors. The first, would this be caused by their attitude of
using computer (Cherney & London, 2006; Li & Kirkup, 2007; Ogletree & Drake, 2007)? A large-scale survey study
of 10,000 public school students in Texas, U.S. have stated, that, by Grades 4 and 5, girls are more positive in their
enjoyment, however, starting about Grade 6, girls' likeness of computers begins to become less positive than boys,
and by Grade 8 becomes significantly lower than boys (Christensen, Knezek, & Overall, 2005). Are Taiwanese
students in grade 6 in our study already demonstrating the same gender effect? Second, or, this could be caused by
the format and design of the game, as previous studies stated that boys prefer role playing games and they play
mainly because of curiosity, challenge, competition, and popularity in or of the game. On the other side, girls prefer
puzzle games, and they play mainly because of imagination, and relaxation in the game (Chuang, 2002; Hartmann &
Klimmt, 2006). We further postulated that to adopt more imaginary and relaxation factors in future educational
games as well as to give more attention and encouragement to female students may have more positive effects on
their learning in the educational game. Third, there are still possibilities that the gender difference in cultural
learning, which has been found, is substantially due to the true effect that female students are more resistant in
changing cultural perception, in terms of cultural identities. However, identifying which factors are important and
how do they influence learning of different gender students need further studies. It is worth noting that the dependent
pre-, post t-test of female students shows significant improvement (t = 3.869, df = 28, p = .001). Based on this
finding and the observed improvement in the female student group, when compare with the control group, it is
contented that the educational game is still reasonably effective in the female student group.
In the socioeconomic status aspect, the significant differences of means across the upper, middle, and lower groups,
requested a post hoc comparison. Although, the differences of means of student from the upper, middle, and lower
class families could be easily observed, no significant differences among groups was found except between the upper
and middle class groups (p = .002). One notable aspect of these results is that the students from lower societal
economics families, who were usually left behind in the learning or in most of innovations or transitions, did as well
as the other students from high and middle classes families (p = .133; p = .190). This reveals the potential of an
102

educational game in improving student learning from disadvantaged families or disadvantaged communities.
Sociologists affirmed that “stereotype threat effects” occur when members of a stigmatized group perform poorly on
a task for they fear confirming a negative stereotype that is associated with their ingroup (Spencer & Castano, 2007).
Students from lower societal economics families would more frequently feel threat from traditional learning and
evaluation. In the learning context of the educational game, FH, students feels they were playing while they were
learning. This may have reduced the “stereotype threat effects” and helped their learning.
For answering the third research question, “how do students feel about the educational game, after they have
experienced it”, the data were collected with the QSO. Students’ opinions about FH are shown in Table 6. This
analysis revealed that the majority of participants in the experimental group liked this educational game (88%). They
felt this cultural-enriched educational game was novel and interesting (88%). By playing the game, they felt they
may learn science (86%), while experiencing the wonder of culture (80%), in this cultural-enriched educational
game, FH.
In addition, students also provided some other opinions about the effectiveness of FH. This included such statements
regarding: characters in the game are cute; it is easy to learn while playing; the game is challenging; it gives me a
sense of accomplishment; the game makes me enjoy my playing, the game is creative, etc. However, some others
opinions include: too much talking, need to have more characters, some parts of the game are difficult.

Reasons

Reasons

Table 6. Students’ opinions about their computer game learning experiences in FH
Preference and Reasons
N
Like
56
I feel the FH educational game is novel and interesting.
49
By playing the FH, I recognize more about the wonder of our cultures.
48
By playing the FH, I learn more about the science.
45
Dislike
8
I do not think that the FH computer game is novel and interesting.
5
By playing the FH, I do not think it help me to recognize the wonder of our cultures.
3
By playing the FH, I do not think it help me to learn more about science.
4

Percentage
88％
88％
86％
80％
12％
63％
38％
50％

From the above quantitative and qualitative data, the computer game is a potential way to learn which is different
from tradition education and the culture-enriched educational game, FH, is effective in promoting students’ learning
in terms of cultural identities.

Limitations of Study
Culture is truly a complex whole (Tylor, 1920). The complexities have made the definition nearly impossible.
Moreover, identities involve sophisticated psychological factors. It’s influenced by one’s previous experiences,
personal characteristics, and societal interactions, etc. Thus, to design an instrument for measuring this quantity with
high variability is difficult in nature. To some extent, we expected that some uncertainty is inherent in the
measurement of cultural identities in this study. Authors of this study believe that the measurement of cultural
identities can be very sensitive to different culture and different ways of seeing what cultural identities are, thus,
suggest that QCI needs to be further modified for their special context and purposes.

Conclusions
Ladson-Billings (1995) defines culturally relevant pedagogy from the student perspective. Three propositions are
stated: students experience academic success; students develop/maintain cultural awareness; and students develop a
critical consciousness to question the status quo. The culture-rich educational game, FH, attempted impact the three
aforementioned propositions.

103

A simulation “approximates a real world setting” and provides a complex virtual learning environment (Reigeluth &
Schwartz, 1989; Winn, 2002). Simulations can be utilized to enhance student perspective-taking experiences by
providing the opportunity to consider others’ viewpoints and induce cognitive conflict. This type of growth and
recognition of self does not happen in isolation rather it occurs through the cognitive development of social
interactions and/or moral experiences challenging conflict between thought and behavior resulting in more
sophisticated, consistent and comprehensive perspective-taking behavior (Hall & Bishop, 2001; Selman, 1977).
Through social games and social and moral dilemmas Selman and Byrne (1974) identified four developmental levels
of social perspective-taking. Perspective-taking levels are basic structures of social reasoning and are used in content
areas such as interpersonal relations, moral reasoning, social problem solving, communication skills, etc. Selman
(1977) implies that intervention research should aim to stimulate perspective-taking through content areas of social
reasoning. Consistent with Piaget and Kohlberg, Selman (2003) believed individuals form their ways of thinking
through social experiences which help to influence our thinking about morality, justice, and fairness. It is a move
from how to understand oneself to how one actually relates to others. As Norman (1998) suggested, individuals
encounter cognitive conflict when experiencing different cultural. By their nature, video games provide conflict
which we posit creates even greater cognitive dissonance when learning about different cultures.
In this study, we demonstrated the feasibility of adopting cultural learning in an educational game platform. Major
results of this study also provides evidence that FH has significant effect on students’ cultural learning and is helpful
in some degree to students from lower socioeconomic families as compared to students from middle and upper
socioeconomic families. Thus, this study concludes that learning in affection domains, such as attitudes, value
judgment, or culture, etc. should not be counted on the formal educational system only, the informal education in
various formats as well as those may happen outside class needs to be given more weight. A well-developed
educational game, in addition to their potential for knowledge learning and entertainment, can promote the growth in
the affection domain. Through the equal learning effectiveness in different socioeconomic groups, educational games
are also helpful in attaining the principle of equal learning opportunity for all. The full potential of the educational
games is out there and just waiting to be discovered

Acknowledgements
The authors wish to thank specialists, students, Professor James A. Shymansky, Professor Chao-Ti Hsiung, and Ms.
Chien-Ju Li for their valuable efforts, opinions and help. This study was supported by grants from National Sciences
Council, Taiwan (Projects: NSC 93-3111-P-008-001-Y19, 92-2524-S-152-001, 152-003, 260-003, 94-2524-S-152002, 96-2511-S-152-004-MY3).

References
Beck, J. C., & Wade, M. (2004). Got game: How the gamer generation is reshaping business forever, Boston, MA: Harvard
Business School Press.
Brooklyn Historical Society. (1990). Many faces, many ways. Multi-cultural diversity of Brooklyn. A guide for teachers. New
York: Brooklyn Historical Society.
Brougere, G. (1999). Some elements relating to children's play and adult simulation/gaming. Simulation & Gaming, 30(2), 134146.
California State Department of Education. (2000). Updated history-social science framework, Sacramento, CA: California
Department of Education.
Cherney, I. D., & London, K. (2006). Gender-linked differences in the toys, television shows, computer games, and outdoor
activities of 5- to 13-year-old children. Sex Roles, 54(9), 717-726.
Christensen, R., Knezek, G., & Overall, T. (2005). Transition points for the gender gap in computer enjoyment. Journal of
Research on Technology in Education, 38(1), 23-37.
Chuang, Y.-Y. (2002). The relationship of children's computer game behavior, creativity, and loneliness of Taipei's upper
elementary school. Unpublished Master Thesis, Chinese Culture University, Taipei (in Chinese).
First Nations and Métis Education Branch, Saskatchewan (1995). Diverse Voices: Selecting Equitable Resources for Indian and
Métis Education. Retrieved May 31, 2009, from http://www.education.gov.sk.ca/adx/aspx/adxGetMedia.aspx?DocID=
244,234,140,107,81,1,Documents&MediaID=518&Filename=diversevoices.pdf.
104

Hall, A., & Bishop, R. (2001). Teacher ethics, professionalism and cultural diversity. New Zealand Journal of Educational
Studies, 36(2), 187-202.
Hartmann, T., & Klimmt, C. (2006). Gender and computer games: Exploring females' dislikes. Journal of Computer-Mediated
Communication, 11(4), 910-931.
Hofmann, S. G. (2006). The importance of culture in cognitive and behavioral practice. Cognitive and Behavioral Practice, 13(4),
243-245.
Hollingshead, A. (1965). Two-factor index of social position, New Haven, CT: Yale Station.
Kopong, E. (1995). Informal learning: a case study of local curriculum development in Indonesia. In Tedesco, J. C. (Ed.),
Prospects: quarterly review of comparative education, 25, 639-652.
Ladson-Billings, G. (1995). But that's just good teaching! The case for culturally relevant pedagogy. Theory Into Practice, 34(3),
159.
Laitin, D. D. (1997). The cultural identities of a European state. Politics & Society, 25(3), 277.
Ledoux, M. W. (2005). Institutional mission and identity: How do we carry the culture to the electronic forum? Educational
Technology & Society, 8, 191-197.
Lee, H.-C. (1993). Nationlism and cultural identities. In N. Polzer (Ed.), Nationlisium. Taipei: Li Ming Cultural Enterprise. (in
Chinese)
Li, N., & Kirkup, G. (2007). Gender and cultural differences in internet use: A study of China and the UK. Computers &
Education, 48(2), 301-317.
Lien, C. J., Lu, Y. L., & Cheng, S. H. (2006). An educational software game for learning science and society- "The Formosa
Hope". Elementary Education, 46(3), 9-15. (in Chinese)
Lin, S. C. (2001). Educational Sociology, Kaoshiung: Fu-Wen Publishing (in Chinese).
Ministry of Education, Taiwan. (2006). General guidelines of grades 1-9 curriculum for elementary and junior high school
education in Taiwan. Retrieved May 31, 2009, from http://english.moe.gov.tw/public/Attachment/66618445071.doc
Ninnes, P. M. ( 1995). Informal learning contexts in Solomon Islands and their implications for the cross-cultural classroom.
International journal of educational development, 15(1), 15-26.
Nixon, H., & Comber, B. (2006). Differential recognition of children's cultural practices in middle primary literacy classrooms.
Literacy, 40(3), 127-136.
Norman, A. J. (1998). Managing conflict building a multicultural collaborative. Cities, 15(3), 209-214.
Ogletree, S. M., & Drake, R. (2007). College students' video game participation and perceptions: Gender differences and
implications. Sex Roles, 56(7), 537-542.
Reigeluth, C., M., & Schwartz, E. (1989). An instructional theory for the design of computer-based simulations. Journal of
Computer-Based Instruction, 16(1), 1-10.
Selman, R. L. (1977). A structural-developmental model of social cognition: Implications for intervention research. Counseling
Psychologist, 6(4), 3-6.
Selman, R. L. (2003). The promotion of social awareness: Powerful lessons from the partnership of developmental theory and
classroom practice, New York: Russell Sage Foundation.
Selman, R. L., & Byrne, D. F. (1974). A structural-developmental analysis of levels of role taking in middle childhood. Child
Development, 45(3), 803-806.
Shaffer, D. W., Squire, K. R., Halverson, R., & Gee, J. P. (2005). Video games and the future of learning. Phi Delta Kappan,
87(2), 105-111.
Spencer, B., & Castano, E. (2007). Social class is dead. Long live social class! Stereotype threat among low socioeconomic status
individuals. Social Justice Research, 20(4), 418-432.
Tylor, E. (1920). Primitive culture, New York: J.P. Putnam's Sons.
Van Gorp, B., & Renes, H. (2007). A European cultural identity? Heritage and shared histories in the European Union. Tijdschrift
voor Economische en Sociale Geografie (Journal of Economic & Social Geography), 98(3), 407-415.
Winn, W. (2002). Current trends in educational technology research: The study of learning environments. Educational
Psychology Review, 14(3), 331-351.
World Conference on Cultural Policies, UNESCO. (1982). Mexico City Declaration on Cultural Policies. Retrieved May 15,
2008, from http://portal.unesco.org/culture/en/files/35197/11919410061mexico_en.pdf/mexico_en.pdf.
Yang, S. F. (2001). Multi-dimension national identities. Dr. Sun Yat-Sen Academic Journal, 22, 175-191 (in Chinese).

105

Hannon, J., & Bretag, T. (2010). Negotiating Contested Discourses of Learning Technologies in Higher Education. Educational
Technology & Society, 13 (1), 106–120.

Negotiating Contested Discourses of Learning Technologies in Higher
Education
John Hannon and Tracey Bretag*
Curriculum, Teaching and Learning Centre, La Trobe University, , Melbourne, Victoria 3086 // Tel: 61 3 9479 1533
// J.Hannon@latrobe.edu.au
*
School of Management, University of South Australia, Adelaide, South Australia 5001 // Tel: 61 8 8302 0224 //
tracey.bretag@unisa.edu.au
ABSTRACT
This paper explores the way that learning technologies frame teaching practice in higher education using both
autoethnography and discourse analysis (interpretative repertoires). The analysis juxtaposes our own experience
in the form of data from two interviews, with teaching and learning policy documents from the group of five
Australian Technology Network universities, as a means of investigating the centrality of these technologies in
the reconfiguring of teaching practice in higher education for the networked university. The data yielded three
distinct discourses: technology as a bridge to globalised opportunity; technology as delivery of learning; and
technology as communication and building relationships for learning. The first repertoire provides a utopian
vision which glosses over the complex practice of implementation. The second repertoire also omits details of
implementation, presenting learning technology unproblematically. The third repertoire, not present in the
policy documents, but central to the autoethnographic accounts, focusses on both the possibilities and challenges
of learning technologies in practice, and points to the potential for a complementary approach which
foregrounds the student-teacher relationship. How these discourses can be reconciled is a central issue for
academic teaching practice in higher education.

Keywords
Learning technologies, higher education, policy, practice, discourse

Introduction
The current climate in higher education
Soucek (1994) suggests that the function of tertiary institutions has changed since the 1980s “from guardianship of
knowledge and wisdom to ancillary production of knowledge for corporate capital” (p.54). During this time there has
been a redefinition of the role of the teacher, “from progressive educator and participant in educational politics to one
of competent performer of relatively neutral tasks related to efficient and profitable delivery of pre-specified
curriculum, and of being a responsible manager of learning contexts” (Seddon, 1998, p.5). There is widespread
concern at the effect that “supermarket” policies have had on teachers’ professional lives (Moloney, 2000, p.73). In
the current climate few academics have been openly critical of policies within their own institutions, although
according to Gaita (2000), far from betraying the institution, such open criticism is the mark of “the true champions
of a university” (p. 41). The rhetoric that accompanies online learning at the policy level does not necessarily match
actual practice (Conole, de Laat, Dillon, & Darby, 2008, p. 511). With networked technologies central to the
changing nature of universities (Lewis, Marginson & Snyder, 2005; Cornford & Pollock, 2003, p. 6), competing
agendas concerning online learning have emerged within institutions, with consequences for practice (Lewis et al.,
2005, p. 72). It is our contention that when educators are confronted with policies that potentially disregard both the
learning needs of diverse students and the recent research on teaching and learning, we have a responsibility to
engage with those policies, interrogate them, and make a space for constructive debate.
The contest over online learning
Teaching and learning in higher education across the globe is said to be undergoing profound change and
transformation by the technologies of learning (Castells et al., 1999; 1996; Kellner, 2003), and organisations have
invested in “big” solutions with mixed results. Others argue that online learning, or “e-learning” has heralded
“successive false dawns” (McMullin, 2005, p. 67) or “stalled” (Zemsky & Massey, 2004). Pollock & Cornford
(2002) discussed three failed online learning projects, and identified the issues contributing to this as not the
technology itself, nor any negative attitude of staff, rather “the underlying problem is the sheer volume and
complexity of the work required to configure people, machines, objects, texts, and money” (p.371).
ISSN 1436-4522 (online) and 1176-3647 (print). © International Forum of Educational Technology & Society (IFETS). The authors and the forum jointly retain the
copyright of the articles. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that copies bear the full citation on the first page. Copyrights for components of this work owned by
others than IFETS must be honoured. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior
specific permission and/or a fee. Request permissions from the editors at kinshuk@ieee.org.

106

This complexity does not seem to be acknowledged in the implementation of institution-wide implementations of
online learning (Barnett, 2000; Hannon, 2008), particularly in its demands on people, time and resources. While
online learning may be still in a developing phase, its technologies are integral to the reconfiguration of both
pedagogy and the organisation of learning. Learning technologies bring with them interests, internal and external
stakeholders and a politicised “covert curricula” (Roberts, 2004).
Teaching staff in universities are caught between two trajectories: on the one hand, the promise of the economies of
scale with “big” solutions in online learning, with heavy investment in proprietorial online learning, huge
commitment of resources and support, and on the other, teaching and learning practices opened up by innovative
uses of learning technologies. Lewis et al. (2005) described these two perspectives as inherent in the “networked
organisation” (p. 72), from which emerged contested discourses within institutions (p. 72). They noted how
networked technologies in organisations did not follow a single logic, but were “socially embedded and therefore
highly variable” (p. 71).
With the online technology administered and managed by an information technology unit, there is the risk that
“learning experience takes a back seat to the management functions” (Siemens, 2006). The focus of learning
management systems (LMS) on “content” and the management of students places pedagogy and engagement in the
background. Hotrum (2005) observes that LMS are “progressively being regarded as a hindrance to effective online
learning.” With the emergence of easy to use collaborative Web services, Hotrum suggests that “a new generation of
Web-based tools and approaches is evolving that are better suited to meet the need for dynamic online learning
content, interaction, collaboration, and networking.” The transformative potential of “social software” marks a clear
shift away from pedagogies based on managed “content” and repositories of learning objects towards student-centred
learning focussed on knowledge production (Gibbs & Gosper, 2006; McLoughlin & Lee, 2008).
The production of instructionally designed online content is one of the trajectories of learning technologies, in which
curriculum is relatively fixed, and produced for delivery by many instructors to learners en masse. The other
trajectory is the opportunities provided by networked communication to treat curriculum as always unfinished, as
“content for meddling with” (McWilliam, 2005). Each trajectory points to different structural arrangements for
teaching and learning in institutions, with different notions of learning, of expertise and of the division of
pedagogical labour. As the shape of online teaching and learning is still in flux, its contested nature is being played
out in a struggle of discourses, where online learning is being shaped by the terminology and the framing notions of
institutional, technological and pedagogical interests.
Two examples of the contested discourses of the “networked university” are described by Lewis et al., (2005), where
a concern with centralised, standardised control of teaching with technology, co-existed with an interest in the
collaborative and democratic use of technologies (pp.72-3). They noted these discourses, though “highly contested”,
were not necessarily opposed. A contentious recurring issue for academics concerns how academics negotiate
governance of their day to day work, caught between the opposing poles of collegiality and managerialism
(Marginson & Considine, 2001). This inquiry takes up an instance of this discussion.

Our research agenda
In this paper, our intention was to follow the mismatch suggested by Conole et al. (2007) between the rhetoric of
online learning and its practice, by selecting and contrasting discourses from both these realms. Our questions were:
how do rhetoric and practice connect or fail to connect? And what are the underlying issues or dilemmas in each
discourse? Both discourses concerned online learning in the changing environment of higher education: the realm of
rhetoric is represented by the institutional policies from the five Australian Technology Network (ATN) Universities
that reflect a similar orientation towards learning technologies; the realm of practice is represented by the interview
accounts of the two authors about their own experience with learning technologies. Our intention was to select a case
of authentic, situated practice that represents the typical enactment of those teaching and learning policies in higher
education practice, that of teaching in a mass learning environment enabled by networked technologies. Our decision
to deploy autoethnographic interview accounts by the authors, arose, in part, from experiences in our day-to-day
practice of being confronted with issues of large class teaching, approaches to tutoring online groups, reports of
intensification of work, all of which raised the ambiguous role of networked technologies and the way they occurred
in institutional policy texts and in practice. Our intention was to provide an “emic” or insider perspective on how we
107

negotiated our own pedagogical practice with institutional imperatives in this networked environment. We did not
intend to make universalising claims, but to enter this contested space, and track our personal judgements about
online pedagogies and their intersection with institutional policy, on the assumption of relevance: that other teaching
academics also encounter pedagogical dilemmas at the intersection of policy and practice in the new higher
education environment. While the setting and participants in this study were local, the learning technologies, the
policy context, and the teaching and learning practices are replicated globally in higher education. In this paper we
bring a innovative approach to analysing practice by deploying a type of discourse analysis which bridges local and
global contexts. This strategy problematises discourses of online learning, aims to expand the agency of teaching
academics, and thereby reclaim our practice.

Local context
Our current interest in online learning technologies occurs in a particular policy context. In 2004 the Vice Chancellor
at The University circulated a discussion paper to all staff entitled Towards an online strategy 2005-2015. We refer
to the institution in which we worked during the period of research as “The University” because we do not wish to
imply that the situation there was unique in terms of a general push towards online learning in Australia, or indeed
across the higher education sector internationally. The discussion paper in question documented a vision of how The
University would use new technologies in its service to staff, students, alumni, prospective students and partners. At
the heart of this vision was the desire to develop a sustainable, customer-focused, competitive, cost-effective,
standardised and disaggregated teaching and learning framework that made use of e-learning and online technology.
In particular, the online strategy advocated the use of “learning objects” – decontextualised online content that could
be shared within and outside The University – as a means of increasing efficiency and reducing costs. While the
paper reiterated a commitment to “high levels of customer service and well-developed customer relationship
management strategies” (Towards an online strategy 2004, p.3) there was no mention of the difficult-to-measure, but
nonetheless valuable, student-teacher relationship.

Data and methodology
A mixed methodology was designed to match the discourses of the public, institutional policies of teaching and
learning on the one hand with those of academic practice on the other. Following the researcher-practitioner
tradition, an “inside-out” approach (Mann, 2004, p. 206) was used, in which personal enquiry into practice combined
with critical reflection becomes “a valid source of knowing” (p. 207). Our approach juxtaposed this with an “outsidein” (p. 207) perspective which applied empirical approaches of coding and categorising to all the data, using a
discourse and content analytic approach to analyse the intersection of policy and practice, that is, draw analytical
comparisons between the public and personal discourses. This approach located the researchers in the enquiry setting
with a focus on practice settings, “both in terms of the discourse in which practices are described and understood and
in terms of socially and historically constructed actions and their consequences” (Kemmis & McTaggart, 2008, p.
292).
Two sources provided the data for analysis of these discourses: public documents on teaching and learning policy
from five ATN universities, and interview transcripts from two academics who had been working at one of the ATN
universities: Bretag (interviewed on 5 August 2005), a lecturer teaching communication courses in a business
faculty, and Hannon (interviewed on 26 April 2007), an academic developer based in the university teaching and
learning unit. This work is part of a larger project which began in 2005 when Hannon interviewed Bretag for his
doctoral research. In early 2007, after conducting research together on the use of computer mediated communication
to develop a community of learners (Bretag & Hannon 2008), we decided to revisit the initial interview with the idea
of reversing our roles. Using the same interview questions, Bretag interviewed Hannon (see Appendix 1). We then
contextualised our analysis of the interview data with a content analysis of five ATN university policies relating to
learning technologies. From both these sources, patterns and themes of discourses were analysed.
Teaching and learning strategy documents were from the five ATN universities (Curtin University, University of
South Australia, Royal Melbourne Institute of Technology, University of Technology Sydney and Queensland
University of Technology). These universities are grouped together under the ATN banner because of their shared
development history from colleges to universities in the 1990s, and because of their common focus on “undertaking
108

solution based research”, underpinned by a commitment to equity and access (Australian Technology Network of
Universities). The five ATN documents were:
 RMIT 2010: Designing the Future, (RMIT University, 2007)
 Curtin University of Technology, Teaching and Learning Enabling Plan, (Curtin University of Technology,
2006)
 Queensland University of Technology, Approach to and context of teaching and learning, (Queensland
University of Technology, 2007)
 University of Technology, Sydney, Setting the Pace 2006-2009: Strategic Directions for the Current Decade (of
Technology, Sydney, 2007)
 University of South Australia, Teaching and Learning Strategy 2006–8, (University of South Australia, 2007).
There were three stages of analysis. Both ATN documents and interview data were analysed using the qualitative
analysis software program N6. The first stage of analysis used a simple content analysis by weighting to indicate the
keywords most used relating to issues of higher education teaching and learning with technologies. The second stage
involved coding sentences and paragraphs to identify the concerns and preoccupations of authors and speakers, then
the coded units were organised into themes or categories. In the third stage, these categories of most concern were
examined for the regularities in the accounts as they reflected attempts to resolve particular dilemmas, or achieve
certain ends. These were identified as interpretative repertoires, for which an associated vocabulary and rhetorical
techniques were deployed.

Autoethnography
Rather than seeking to generate a “theory”, the aim in this research was to juxtapose policies relating to learning
technologies with our own experience, as a means of exploring and participating in this contested field of inquiry. As
practitioner researchers we wanted to situate our own and each other’s experience within the broader policy
environment, using autoethnography. Patton (2002) describes autoethnography as “self-awareness about and
reporting of one’s own experiences and introspections as a primary data source” (p. 86), and Ellis and Bochner
(2000) define it as “writing and research that displays multiple layers of consciousness, connecting the personal to
the cultural” (p. 739). Research which utilises autoethnography may include personal narratives, first-person
accounts, personal essays, opportunistic research, self-ethnography, and autobiography to mention just a few
approaches (as cited in Ellis & Bochner, 2000, p. 739). In keeping with the tenets of qualitative research (see
Cresswell, 1998), at times we strategically change to the first person in a conscious attempt to foreground our own
positions and experiences. While the personal revelation used in autoethnographic research has been critiqued as a
potential risk to the integrity of research, Bruni (2000) counters this view by offering a revisioning of research ethics
protocols based on the uniqueness of each research enterprise (p. 30). We would argue further that in foregrounding
our perspectives and context we make visible what is rarely apparent to the reader, our own agendas. We present
ourselves as insider researchers, and claim the benefit of prolonged and not disinterested, but critical engagement in
the research context. This engagement acts as an aid to validity and has also provided us with access to knowledge
which may be undiscoverable to outsiders (see Edwards, 2002).
We follow the example of Adams (2007), an ex-nurse, who interviewed a number of nurses to explore their everyday
work experience, and then used the same questionnaire to “provide a similar and comparable record of [her own]
experience” (p. 10). Adams then coded the autoethnographic interview using the same process used in the other
interviews. Like Adams, having foregrounded our own insider status, we interviewed each other using the same
interview structure, and then coded the interviews to reveal “interpretative repertoires”, as described in the following
section. In this research, autoethnographical material (as exemplifed in the coded interviews) was contrasted with the
coded policy documents of the five Australian ATN universities. By insisting on the legitimacy of our own
experience, and applying the same rigorous analysis to it, we have responded to Smyth and Hattam’s (1998) call for
academics to be “reflexively engaged” in policy issues which directly impact on our practice.

Discourse analysis using interpretative repertoires
A discourse analysis approach to spoken or textual accounts focuses on language use: rather than analysing social
representations or cognitive processes, it is concerned with how utterances and accounts are constructed, and the
109

social effects of what they may represent. Discourse can be viewed as a “social practice” (Wetherell & Potter, 1988,
p. 168), and as having an “action-orientation”, with “particular consequences” (p. 171). These effects range from
actions in local contexts, the “speech acts” of Austin (1962), where talk may be described as accusing, excusing,
justifying, and so on, to accounts which may be described as having broader effects such as arguing a particular
institutional position.
Much discourse analysis occurs in local settings, focused on the specifics of an encounter, and one of the criticisms
and challenges of such analysis is how to make connections beyond the situated context (Alvesson & Karreman,
2000, p. 1127). In our methodology we wished to bring an “outside-in” perspective to both sets of data, the author
interview transcripts and policy documents, and make the link between the local, conversational, “transient”
discourse to the extended, durable, “meanings ‘existing’ beyond” (p. 1130). We selected the particular discourse
analytical technique of “interpretative repertoires”, developed by Potter and Wetherell, (1987), to make this link
between the local setting and the social world of practice: in our study, to connect the discourses of the localised
interview transcripts and public policy documents. An interpretative repertoire was defined by Wetherell as: “a
culturally familiar and habitual line of argument comprised of recognisable themes, commonplaces and tropes”
(Wetherell, 1998, p. 400). Hence speakers and writers deploy rhetorical and lexical patterns to accomplish particular
tasks. An analysis of interpretative repertoires can explore the discourses individuals draw upon to construct their
social world through talk and texts in order to achieve particular ends. An example of an interpretative repertoire
may be the use of the term “flexibility”, and the effects this produces when used in powerful policy documents.
Interpretative repertoires tend to be deployed to resolve a dilemma in the social context for the speaker/writer (Potter
& Wetherell, 1987, p. 152). In the current study, repertoires were analysed to identify in the authors’ own talk or text
how they attempted to resolve contextual dilemmas in their practice and accomplish particular tasks, and their use of
specific rhetorical techniques to do this. We analysed the discourse of each of our interview transcripts, identified
repertoires in these accounts, and compared these with the repertoires expressed in the ATN university teaching and
learning strategy documents.

Findings
The two sets of data (ATN policy documents and authors’ interview transcripts) were coded using the qualitative
software N6. The findings below consist of content analysis for keywords, coding into categories, and interpretative
repertoires identified from both sets of data.

Keywords
The data was examined for recurring keywords relating to how institutions and academics engage with learning
technologies. Total occurrences of keywords, truncated for variations using an asterisk, are shown in Table 1:
Occurrences of key terms.
Table 1: Occurrences of key terms
ATN documents
(Total 25,696 words)
improv* (improve, improving, improvement)
135
communit* (community, communities)
57
engag* (engage, engagement)
43
respons* (response, responsible, responsibility)
41
techn* (technology, technological, technical)
37
flexib* (flexible, flexibility)
28
online
27
value
23
innovate* (innovate, innovation)
12
Key terms

Authors’ interview transcripts
(Total 19,510 words)
6
10
7
8
50
0
75
6
3

The key terms mentioned most frequently were:
110




For the five ATN institutional policy documents: improve/improving/improvement (135 occurrences), followed
by community/communities (57), then engage/engagement (43 occurrences), technology/technological/technical
(37), flexible/flexibility (28), online (27).
For the authors’ interview transcripts, two keywords were mentioned with much greater frequency than all
others: online (75), and technology/technological/technical (50).

Categories
The process of coding the texts from both data sources resulted in a total of 29 codes, with the coding unit being the
paragraph. The codes were grouped into four categories in response to the question: what issues of concern are
expressed around use of technologies for learning? These are shown with their coding frequencies in Table 2:
Categories and codes for each data source.

Coding

Table 2: Categories and codes for each data source
ATN documents

Academics
Attitude to new technologies
Changing role
Critique
Engagement with technology
Innovation
Interculturality
Leading
Multiple roles
Research
Sharing and mentoring
Teaching
Time constraints
Institutional context
Flexibility (of learning arrangements)
Institutional requirements
Institutional support / Workload
Policy vs practice
Principles & Values
Strategy
Strategic Actions
Online technologies
Communication issues
Complement to face-to-face
Embodiment
Limitations
Marginalisation
Technophobia
Types of e-learning
Students
Engagement with technology
Student needs
Student-teacher relationships

Authors’ interview
transcripts

0
0
0
0
0
0
0
0
0
0
0
0

15
13
15
11
13
1
9
15
7
10
25
15

35
0
16
0
50
111
25

6
19
4
27
0
0
0

0
0
0
0
0
0
6

17
24
17
16
17
4
14

0
0
0

11
21
39

The four categories were ordered to reflect the density of coding, shown in Table 3: Categories from data sources.

111

ATN documents
Institutional context
Online technologies
Academics
Students

Table 3: Categories from data sources
Coded units
Authors’ interview transcripts
237
Academics
6
Online technologies
0
Students
0
Institutional context

Coded units
149
109
71
56

The order of categories in Table 3 provides an indication of the relative emphasis each category or issue was given
by the authors of the ATN documents, compared to our interview transcripts: the institutional context was the
predominant concern of the ATN documents, comprising 97% of codes for that data source; and for the interview
transcripts, the main concerns were academics (39%) and online technologies (28%). A striking feature of the
comparison of codes in Table 2 was that the concerns expressed by us in our interview transcripts were shared with
only three codes with the ATN policy documents: flexibility, types of e-learning, and institutional support.
A strong disparity between policy documents and interview transcripts emerges from the coding. The two primary
codes for the ATN documents, strategy, and principles and values, comprised 66% of codes for that data source, and
were not coded at all in the Hannon and Bretag interviews. The coding for the two interviews encompassed a total of
26 codes, the main concerns indicated by five codes with over 20 coded units, which comprised 36% of codes for the
transcripts. These five codes were: teacher-student relationships (39 units), teaching (35), policy vs practice
(concerns with institutional pressure on teaching and learning practice) (27), complement to face-to-face (how
technologies complement face-to-face learning settings (24), and student needs (21).

Interpretative repertoires
Interpretative repertoires were identified first by focusing on those categories which reflected the greatest concern
expressed in the interviews and policy documents, then by locating specific uses of text or talk that attempted to
resolve uncertainties or dilemmas of practice. Three interpretative repertoires were identified, or ways of writing or
talking about the technologies of teaching and learning in higher education in the data sources. These were:
 technology as a bridge to globalised opportunity;
 technologies as delivery of learning; and
 technologies as communication and building relationships for learning.
The first repertoire was used exclusively by the ATN teaching and learning policy documents, and presents an
institutional perspective on technology in higher education, where the reader is offered, via the institution, access or
a bridge to a networked world. This repertoire reflects the concerns of the ATN documents coded as strategy,
principle and values, flexibility. The remaining two repertoires were both drawn on by Hannon and Bretag in their
interviews, but in contrasting ways: technologies as delivery of learning, and technologies as connecting people to
build relationships for learning. These correspond to the areas most frequently coded for the interview transcripts:
teacher-student issues, technologies and learning, and concerns with policy and practice. Each repertoire is discussed
below.

Analysis and discussion
Technologies as a bridge to globalised opportunity
The ATN teaching and learning strategy documents reflected the imperatives for universities to position themselves
in a global market, the need to respond to the demands of learners, and the fluid nature of the times and spaces of the
learning environment. Keywords used frequently were variations on “improve”, “community”, “engage”,
“response”, “technology”, “flexible” (see Table 1). These were associated with terms such as global passport, global
engagement, connected, strengthen, incorporate, provide a real-world education, best practice, appropriate
technology, define our global network. These keywords and terms located the university as an active agent in a
disaggregated, global field of endeavour.

112

Technology was offered to prospective students as a key which offers access to the world:
technological advances open up opportunities (QUT)
provision of resource-rich, technologically-mediated forms of delivery that enable access (UniSA)
Our flexible learning environment and effective use of technology in teaching and learning will underpin
the University’s reputation for excellence in the facilitation of learning (UTS)
In the teaching and learning strategy documents, technologies both underpinned and enabled institutional goals. The
authors of the ATN documents consistently framed a field of operations which reflected a volatile, globalised and
unpredictable world of shifting markets and demands, “major challenges” in global education, and competition with
other educational institutions. Having constructed such a challenging environment, a response was proffered by
institutions in the form of self-descriptions: they were adaptable, internationalised and technologically cutting edge,
and were able to offer their audience global reach, and access to opportunities in this unpredictable world. The
rhetoric of “community” was linked with work and business:
engage students and staff with the professions, industry, business and the community to maximise
opportunities (UTS)
a global university grounded in Melbourne and connected to communities, enterprises and industry across
the world (RMIT)
The use of metaphors of connection and community was associated with descriptions of a global, competitive,
dispersed and internationalised field. The response of the institutions was to embrace this dynamic world:
shaping the educational foundations of existing and emerging fields of practice (UTS)
reflect our global engagement with industries and communities. (RMIT)
research and teaching will be conducted in many countries through innovative use of e-learning and ebusiness (UniSA)
Institutions were positioned as locations of access to a complex global field, with access offered through
technologies and international networks. Potential dislocation of the times and spaces of learning was resolved with a
virtuous rhetoric associated with technology:
Technological advances open up opportunities for adding newer and more innovative methods to the
spoken lecture and the face-to-face seminar. (QUT)
The University will be a leader in global access to learning that is enabled by emerging technologies.
(UniSA)
The uncertainty of the “complex workplace and community of the 21st century” (UTS) is made palatable by the
keywords “community”, “engage”, “response”, “innovative”. Technologies, then, are a key to a dual network,
technical and social: one offers the reader (presumably a student) a network which provides “technologicallymediated forms of delivery”, the other offers social access and connection to this high-tech, utopian community.
The policy documents used the metaphor of access or the “bridging” repertoire to orient the reader to an imagined
high-tech world beyond the institution, and was not deployed at all in the interview transcripts. While the interviews
did, in fact, reflect institution concerns, in the codes policy and practice, and institutional requirements, these
concerns were oriented towards adapting teaching and learning practice to the institutional context; that is, they were
inward looking. The contrasting outward looking orientation of the policy documents constitutes one disjunction
between discourses of technology that impacts on practice.

Technologies as delivery of learning
In Bretag’s account of her teaching practice (Interview August 2005), she expressed a dilemma concerning her
position in the middle between institutional strategies of teaching and learning and her teaching practice as an
academic. Bretag described two contrasting learning settings in her practice: one involved small groups where faceto-face communication and email dialogue were combined and resulted in successful deep learning outcomes; the
other involved coordinating large international student cohorts and resulted in a heavy workload:
We’ve got, over the year, about 700 in our course now, you know, because we teach in Malaysia and
Hong Kong and here, and just maintaining the standard service to those students is as tough as it gets.
(Bretag)
113

This dilemma was expressed in terms of the difficulty of negotiating with an online strategy that “frames what we
do”, that is, determines the context of teaching practice:
It seems to me we’ve gone too much that way, economic, economic, economic, and I’m worried about the
online strategy being that way. I’m not saying there aren’t elements of it that would actually make a lot of
sense, but if we’re constantly thinking about hey, we’ll be more efficient… (Bretag)
Bretag located her practice at two points, with the “we” that thinks about efficiency, as well as the “I” that attempts
to adapt “elements” of the economically driven strategy. In her discussion of this issue, the two points are too far
apart for Bretag to find a balance or negotiate her practice satisfactorily with either.
The rhetoric invoking the economic determinants of the contexts of teaching was associated with the institutional
application of online learning technologies. Hannon (Interview, April 2007) described an encounter with an
academic manager for whom he was conducting staff development in using online discussion, who equated learning
technologies with a future institutional shift to wholly online courses, stating that, “this is the model we’re going
towards, where the university has no option.” In Hannon’s account he was concerned that the online strategy of the
institution was interpreted unproblematically with a technology-led shift to online courses.
Hannon then identified an institutional effect in which learning technology became first separated from teaching
practice, then implemented by organisational units removed from teaching and learning priorities:
In the organisational sense you’ve got IT departments and they have their own interests and they put up
technologies to use, and in the sense the technologies become the end themselves. (Hannon)
A consequence of the separate interests of those who implement learning technologies (“IT departments”) from those
who teach with them, then, created the conditions for a technology-led approach to emerge to a teaching setting,
which functions “like a default pedagogy”. Hannon acknowledged Goodyear and Jones (2003, p. 40) for this term:
I think it's wrong to promote people to use a particular technology, it's got to be the use to which it's put,
and almost always in an academic issue there's no simple one solution. … so the technology always tries
to take centre stage, I've realised, and if you leave it alone it will do that, and if it does that it starts
corroding or corrupting pedagogy. (Hannon)
In expressing this strong view, Hannon indicates a conflict in his practice of academic development in online
learning, between “opening people’s eyes” to the possibilities of learning technologies and institutional constraints
on teaching practice.
Bretag identified a value conflict in her practice, where online strategy was “largely based on an economic
rationale”, and doesn’t fit “with an educational rationale”. Bretag identified efficiency as a value in the underpinning
of the online strategy, with implied time saving. In fact, the use of learning technologies came at a cost to her and her
colleagues:
It’s very time intensive to really use it to its full capacity, so then that takes me back to that question
about our online strategy. I don’t believe it saves you time; this didn’t save me time, this just changed my
life. I’m sorry, I’ve got two different priorities going there; this changed my student’s life, it didn’t save
anybody time, it took a lot of time. (Bretag)
Bretag was concerned that her style of teaching practice, which had produced profound effects on student learning,
had no place in the institutional online strategy exemplified in the “delivery” repertoire.

Technologies as communication and building relationships for learning
Hannon contrasted the open nature of the Internet with learning management systems, which he described as
emphasising “managing content”, despite some interactive software:
there's an assumption that doing a course is accessing content... As well they have a discussion facility but
in a sense it's an add on, it's really about accessing content and behind that there's this vision of distance
education which is the postal one. (Hannon)

114

Hannon equates the learning management systems with the “default pedagogy” of the model of content access, and
draws a contrasts to the “new social software” which places communication as the centre of its model. This clash of
models – learning as communication versus learning as access – has limited the extent to which innovative and
interactive software can be brought into institutional teaching and learning practice.
Bretag described as central to her teaching approach the process of building a relationship with a student in an
intercultural context, and she had developed a research informed practice which was founded on achieving
successful intercultural or “third space” communication. This approach is closely tied to a perception of student
needs in the learning environment:
They’re actually here for a much broader and deeper cultural experience, and you only get that cultural
experience with interacting with people, and sharing culture and sharing of yourself. (Bretag)
For Bretag, a dilemma arose when strategy documents promoted the use of online technologies to bridge dispersed
locations and conduct totally online education, yet she perceived the needs of students as primarily met by classroom
contact:
Why would students be wanting to come to university and actually be in a classroom environment?
People could just go and do all their learning sitting in front of a computer, but they don’t. Why don’t
they? Because they actually want human contact. (Bretag)
However, Hannon frames this as a need for contact, rather than a need for being in a particular physical location:
When it comes down to contact, ways of contacting people, whether you do this in a group form or by
email I think, well, it comes down to that contact, that’s what students tend to like. (Hannon)
Bretag attempted to accommodate her practice to the institutional online strategy, and despite professing scepticism
and ambivalence, suggested that online materials and online learning, “can complement the face-to-face, but I just
don’t think it should replace it.” Moreover, “something about the medium”, in her pedagogical use of email, afforded
greater sharing and disclosure than the face-to-face class setting. For Bretag, modelling “complementarity” was part
of her role of a teaching academic, and in the intercultural context, this involved sharing experiences mutually. Such
modelling gave students new to Australia an “insider view”, which she described as part of the reason they are
studying in Australia. The student need for contact, particularly contact of an intercultural nature, underpinned her
discourse of relationship building as central to her teaching approach, and student-teacher relationships was the code
that attracted the highest level of coding in both Bretag’s and Hannon’s interview transcripts at 39 units (see Table
2).
The keywords most frequently used in the Bretag and Hannon interview transcripts were “flexible/flexibility”, and
“technology/technological/technical” (Table 1). These tended to be used in a descriptive manner, reflecting the
pervasiveness of these words in descriptions of teaching and learning contexts and practice. By contrast, the ATN
policy documents reflected a strategic, goal-oriented purpose, and technology related words were softened by
positive associations, and closely positioned with terms such as, “opportunities”, “resource-rich”, “excellence”,
“enable access”, “effective use”.
Bretag invoked two distinct but opposing ways in which online technologies impacted and shaped her practice. In the
first, online modes were described as offering greater opportunity for blended learning: they were able to liberate
students’ expression when informal language was used online, and enable the formation of deeper relationships via
one-to one online communication, particularly in the intercultural context. The second way her practice was shaped
was by the pressures of teaching large cohorts afforded by online learning systems, with the result that teaching
became “maintaining the standard service”, and the economic rationale underpinning the online teaching and
learning strategy which marginalised the “educational rationale”.
For Hannon, central to his academic development practice was innovative online learning approaches that were
situated and contextualised in their use, enabled reflective and deep learning, and were responsive to workload issues
for teaching academics. A dilemma arose for Hannon when online teaching approaches were derived from an ideal
of all online, off-campus, mass learning contexts, the technology use overshadowed the pedagogy, and technology
became a platform rather than a learning space.

115

Summary of the three repertoires
The first repertoire, deployed exclusively by the ATN policy documents, was technology as a bridge to globalised
opportunity. We have shown that these documents metaphorically connect the reader, via the institution, to the global
community, with learning technologies taking the role as that bridge. In contrast to this strategist’s perspective, the
second repertoire, technology as delivery of learning was drawn from Hannon and Bretag’s perspective on using
learning technology systems scaled to large, distributed teaching contexts, in a similar globalised higher education
scenario. Both repertoires related to technology enabled education with the capacity for global reach, but with a
crucial distinction: the former reflected a strategic and visionary perspective which was unifying and community
oriented, whereas the latter used a rhetoric which was economically oriented, institutionally focussed and deployed
the vocabulary of the IT help-desk.
The third repertoire described the deployment of technologies as communication and building relationships for
learning, where technology augmented the learning context, extended pedagogical scope by offering more modes of
interaction and means of “sharing culture”. This repertoire was not used in the ATN policy documents. The
perspectives and goals of each repertoire are schematised in Table 4: Contrasting repertoires of learning
technologies.

Interpretative
repertoire
Source
Perspective

Goal or
accomplishment of
repertoire
Rhetoric

Table 4: Contrasting repertoires of learning technologies
bridge to global
delivery of learning
building learning
opportunity
relationships
ATN policy documents
Author interview transcripts Author interview transcripts
Strategic: visionary, a global, Implementation: access
Practice: situated contexts,
technologised world.
to learning, reach, scalable.
interactive, communicative style
Outward looking.
Inward looking.
positions institution as
provides access to large-scale, augments face-to-face settings
distributed cost-effective
and enables deep and effective
global player
education
learning
improve, engage, response,
economic inevitability,
relationships, interaction,
flexible, community, global, normative positioning
sharing cultural experiences,
appropriate, innovate
of large-scale online
learning as change
learning

Both the bridging and delivery metaphors of the first two repertoires have the effect of diverting attention from the
actual conditions of teaching and learning practice, from the concrete considerations of organising and coordinating
time, place, people, interactions and technologies, and especially maintaining quality of teaching and learning in
mass or spatially dislocated settings. In the second repertoire, of technology as delivery, learning technologies are
presented as neutral, a platform or system which is inherently separate from pedagogy. This repertoire deflected
attention from the work involved with learning technology systems, in making a delivery platform work in the
peopled settings of teacher-learner engagement. The work of organising the teaching and learning environment and
engagement with colleagues and students is left to the third repertoire of communication and building relationships.
The two repertoires of technology as “bridging” and as “delivery” are institutionally oriented, and project an
idealised world where the work of building relationships is invisible.

Conclusion
This research aimed to explore the intersections and disjunctures between our own situated practice and the rhetoric
expressed in policy documents relating to the use of learning technologies. An autoethnographic approach provided
the starting point from which to reflexively engage in this process, as it allowed us as both practitioners and
researchers to draw on our personal experience. Using autoethnography in combination with discourse analysis,
interviews by and of the authors were contrasted with the institutional policies from the five Australian Technology
Network (ATN) Universities to identify “interpretative repertoires” or recognisable themes, commonplaces and
tropes.

116

It was apparent from the findings that there was a separation of discourses around teaching and learning with online
technologies that was not easily reconcilable. The author interview transcripts revealed teaching practice that
embodied a strong commitment to using online technologies to develop relationships for deep learning. This
repertoire, which emerged from the context of practice, stood in stark contrast to the strategic discourse exemplified
in the university strategy documents, and the functional discourse of implementation. The question raised was
whether any shared ground exists between the “social worlds” (Wetherell & Potter, 1988, p. 171) depicted by these
discourses.
Our findings confirmed research that found contested discourses around learning technologies in institutions, where
technologies intensified the tension between technological management functions and teaching practice (Lewis et al.,
2005; Siemens, 2006). Our analysis also explored the resources which we personally drew upon to respond to the
resulting dilemmas in our own practice. While we clearly recognised that learning technologies provide access to
large-scale, distributed cost-effective education, the autoethnographic interview data also demonstrated that our
commitment to learning technologies went well beyond this functional stance, despite the challenges in our day to
day practice. As noted previously (Pollock & Cornford, 2002; Roberts, 2004; Hannon, 2008), the actual work of
organising people, technology, and resources, is complex and does not match the abstracted model of the delivery
platform and the futuristic e-learning promise. The dilemmas of practice experienced in online learning settings were
glossed over by the repertoire of technology as bridging, and occluded by the repertoire of technology as delivery.
However, the repertoire of technology as building relationships for learning, identified in the autoethnographic
element of this research, offers an alternative discourse to academics who wish to define, maintain and restore the
centrality of their practice, while simultaneously working within a mandated policy framework.

References
Adams, V. (2007). Laughing It Off: Uncovering the Everyday Work Experience of Nurses. International Journal of Qualitative
Methods, 6(1), 1–15.
Australian Technology Network of Universities. Retrieved February 29, 2008, from http://www.atn.edu.au/.
Austin, J. (1962). How to do things with words, Oxford: Clarendon Press.
Barnett, R. (2000). Realizing the University in an age of supercomplexity, Buckingham: Open University Press.
Bretag, T. & Hannon, J. (2008). Online Close and Personal: Developing a Community of Inquiry Using Computer Mediated
Communciation. In Hellsten, M. & Reid, A. (Eds.) Researching international pedagogies: Sustainable practice for teaching and
learning in higher education (pp. 221–239). Netherlands: Springer.
Bruni, N. (2000). The Crisis of Vulnerability: Ethical Dilemmas of Autoethnographic Research. Qualitative Research Journal,
2(1), 24–33.
Castells, M. Giroux, H., Freire, P.,Willis, P. & Macedo, D. (1999). Critical Education in the New Information Age, London:
Rowman & Littlefield.
Castells, M. (1996). The Rise of the Network Society, (1). Oxford: Blackwell.
Churchman, D. (2006). Institutional commitments, individual compromises: Identity-related responses to compromise in an
Australian university. Journal of Higher Education Policy and Management, 28(1), 3–15.
Conole, G., de Laat, M., Dillon, T., & Darby, J. (2008). ‘Disruptive technologies’, ‘pedagogical innovation’: What’s new?
Findings from an in-depth study of students’ use and perception of technology. Computers & Education, 50, 511–524.
Cresswell, J.W. (1998). Qualitative inquiry and research design: Choosing among the five traditions, Thousand Oaks: Sage
Publications.
Curtin University of Technology (2006). Teaching and learning enabling plan. Retrieved July 10, 2007, from
http://lsn.curtin.edu.au/ssu/tlplan.html.
Edley, N. (2001). Analysing masculinity: Interpretative repertoires, ideological dilemmas and subject positions. In Wetherell, M.,
Taylor, S., & Yates, S. (Eds.) Discourse and data: A guide for analysis (pp. 189-228). Milton Keynes: The Open University.
Edwards, B. (2002). Deep insider research. Qualitative Research Journal, 2(1), 71–84.
Ellis, C. & Bochner, A.P. (2000). Autoethnography, personal narrative, reflexivity. Chapter 28 in N. Denzin & Y. Lincoln (Eds.),
Handbook of qualitative research (pp. 733–768). 2nd Ed. Thousand Oaks: Sage Publications.
117

Gaita, R. (2000). Truth and the university. In Coady, T. (Ed.) Why universities matter (pp. 26–48). Sydney: Allen & Unwin,.
Gibbs, D., Gosper, M. (2006). The upside–down-world of e-learning. Journal of Learning Design, 1(2), 46–54. Retrieved
September 12, 2008 from http://www.jld.qut.edu.au/.
Goodyear, P. & Jones, C. (2003). Implicit theories of learning and change: Their role in the development of e-learning
environments in higher education. In Naidu, S. (Ed.) Learning and teaching with technology: Principles and practice (pp. 29–41).
London & New York: Routledge Farmer.
Hannon, J. (2008). Doing Staff development: Dilemmas, practices and technologies, Australasian Journal of Educational
Technology, 24(1). Retrieved February 29, 2008, from http://www.ascilite.org.au/ajet/ajet24/ajet24.html
Hotrum, M. (2005). Breaking down the LMS walls. The International Review of Research in Open and Distance Learning, 6(1).
Retrieved February 21, 2008, from http://www.irrodl.org/index.php/irrodl/article/view/212/295.
Kalaja, P. (2006). Research on students’ beliefs about SLA within a discursive approach. In Kalaja, P. & Ferreira Barcelos, A.M.
(Eds.) Beliefs about SLA: New research approaches. Netherlands: Springer.
Kellner , D. (2003). Technological transformation, multiple literacies, and the re-visioning of education. Graduate School of
Education
&
Information
Studies,
UCLA.
Retrieved
February
21,
2008,
from
from http://www.gseis.ucla.edu/faculty/kellner/essays.html.
Kemmis, S. & McTaggart, R. (2008). Participatory action research: Communicative action in the public sphere. In N. Denzin &
Y. Lincoln (eds.). Strategies for qualitative inquiry (pp. 271–330). Thousand Oaks, CA: Sage.
Lewis, T. Marginson, S. & Snyder, I. (2005). The network university? Technology, culture and organisational complexity in
contemporary higher education. Higher Education Quarterly, 59(1), 56–75.
Mann, S. (2004). A personal inquiry into an experience of adult learning online. In S. B. P. Goodyear, V. Hodgson, D. McConnell
(Eds.), Advances in research in networked learning (pp. 205–220). Boston: Kluwer Academic Publishers.
McLoughlin, C., & Lee, M. (2008). Future learning landscapes: Transforming pedagogy through social software. Innovate, 4(5).
McMullin, B. (2005). Putting the learning back into learning technology. In O’Neill, G., Moore, S., & McMullin, B. (Eds.)
Emerging issues in the practice of university learning and teaching, Dublin: AISHE. Retrieved February 21, 2008, from
http://www.aishe.org/readings/2005–1/
McWilliam, E. (2005). Unlearning pedagogy. Journal of Learning Design, 1 (1). Retrieved February 21, 2008, from
http://www.jld.qut.edu.au/Vol 1 No 1.
Marginson, S. & Considine, M. (2001). Enterprise university in Australia. Governance, strategy and reinvention, Cambridge:
Cambridge University Press.
Miller, R. (2001). The social life of information. Briefing paper from the Greater Expectations National Panel of the Association
of American Colleges and Universities. Retrieved May 15, 2007, from http://www.greaterexpectations.org/
briefing_papers/SocialLifeOfInformation.html.
Moloney, J. (2000). Australian universities today. In Coady, T. (Ed.) Why universities matter (pp.72–84). Sydney: Allen &
Unwin,.
Patton, M.Q. (2002). Qualitative research and evaluation methods, Thousand Oaks, CA: Sage.
Pollock, N. & Cornford, J. (2002). The theory and practice of the virtual university: Working through the work of making work
mobile. Minerva 40, 359–373.
Potter, J. & Wetherell, M. (1987). Discourse and social psychology, London: Sage.
Queensland University of Technology. (2007). Approach to and context of teaching and learning. Retrieved August 1, 2007, from
http://www.mopp.qut.edu.au/C/C_01_01.jsp.
RMIT University. (2007). RMIT 2010: Designing the Future. Retrieved July 3, 2007, from http://www.rmit.edu.au/
browse;ID=if1c83z42eua1.
Roberts, G. (2004). The new covert curriculum: A critical actor-network approach to learning technology policy, Proceedings of
the fourth international conference on networked learning, Lancaster University and the University of Sheffield, pp. 637–644.
Seddon, T. (1998). Steering futures: Practices and possibilities of institutional redesign in Australian education and training.
Retrieved July 13, 2005, from http://www.aare.edu.au/98pap/sed98354.htm.
Siemens, G. (2006). Learning or management system? A review of learning. Management System Reviews, University of
Manitoba, elearnspace. Retrieved February 21, 2008, from http://www.elearnspace.org/Articles/index.htm.
118

Smyth, J. & Hattam, R. (1998). Intellectual as hustler: Researching against the grain of marketisation. In I. Hunt & J. Smyth
(Eds.). The ethos of the university: West and beyond. Adelaide: Flinders University Press, pp. 145–175.
Soucek, V. (1994). Flexible education and new standards of communicative competence. In Soucek. V. (Ed.) Economising
education: The post-Fordist debates (pp. 43–103). Geelong: Deakin University,.
University of Technology, Sydney. (2007). Setting the pace 2006–2009: Strategic directions for the current decade. Retrieved
August 1, 2007, from http://www.planning.uts.edu.au/pdfs/settingthepace.pdf.
University of South Australia. (2007). Teaching and Learning Strategy 2006–8. Retrieved July 3, 2007, from
http://www.unisa.edu.au/staff/teachlearn/default.asp.
University of South Australia. (2004). Towards an online strategy. Internal policy document no longer available online as at 22
February 2008.
Wetherell, M. (1998). Positioning and interpretative repertoires: Conversation analysis and post-structuralism in dialogue.
Discourse & Society, 9, 387–413.
Wetherell, M. & Potter, J. (1988). Discourse analysis and the identification of interpretative repertoires. In Antaki, C. (Ed.)
Analysing everyday explanation: A casebook of methods (pp. 168–183). London: Sage.
Zemsky, R., & Massy, W. (2004). Thwarted innovation: What happened to e-learning and why. A final report for the
Weatherstation Project of the Learning Alliance at the University of Pennsylvania in cooperation with the Thomson Corporation.
Retrieved March 14, 2005, from http://www.irhe.upenn.edu/Docs/Jun2004/ThwartedInnovation.pdf.

119

APPENDIX 1: Interview Questions
1.

Your role as educator:
Can you describe your role – it may be multiple - your area of expertise, your teaching and research areas
2. First use:
Describe your first use(s) of e-learning technologies. What was it, when, was it successful and so on. How did
you discover it?
3. Changes in use:
Have you discarded or shifted away from any uses of elearning approaches or technologies? Why?
4. A specific current use:
Can your describe a current use of e-learning technologies or computer mediated communication that is
significant for you? Briefly, how did this project arise, and what do you hope to achieve?
5. How does this project fit in with the organisation framework and IT system.
6. What has worked well in this project or related areas? What has been opened up by this engagement for you or
others?
7. What hasn’t or doesn’t work well in this project or related areas? Has anything or anyone been constrained,
excluded or foreclosed?
8. Can you describe any unexpected consequences of in your use of networked communication technologies in this
project?
9. Can you describe any innovative uses, adaptations, or workarounds involving technologies for this project that
you have discovered or used with some success?
– Innovative uses include “official” uses, which are supported and presented by your organisation, and
“unofficial” uses, which are those discovered through your own research and contacts.
10. What concerns you about where e-learning is heading? Can you comment on the direction of your work with elearning.

120

Hoon, T. S., Chong, T. S., & Binti Ngah, N. A. (2010). Effect of an Interactive Courseware in the Learning of Matrices.
Educational Technology & Society, 13 (1), 121–132.

Effect of an Interactive Courseware in the Learning of Matrices
Teoh Sian Hoon1, Toh Seong Chong2 and Nor Azilah Binti Ngah2
1

Information Technology & Quantitative Sciences, Universiti Teknologi MARA, Penang, Malaysia // 2Centre for
Instructional Technology and Multimedia, Universiti Sains Malaysia, Penang, Malaysia // sianhoon02@yahoo.com //
tohsc@usm.my // azilah@usm.my
ABSTRACT
The main aim of this study is to integrate cooperative learning strategies, mastery learning and interactive
multimedia to improve students’ performance in Mathematics, specifically in the topic of matrices. It involved a
quasi-experimental design with gain scores and time-on-task as dependent variables. The independent variables
were three instructional strategies (CCL, CML and CCML) with academic abilities as the moderator variable.
The sample for the study was 262 Form Four Malaysian students. A courseware entitled "Matrices" was
developed using Macromedia Authorware as the authoring tool. In this study, the collected data was used to
investigate the effects of the three learning strategies on the gain scores and time-on-task. Based on the gain
scores and time-on-task, the effectiveness of the three learning strategies was discussed. This study showed that
the CCML and CML strategies were superior compared to the CCL strategy; CCML strategy produced the
highest gain score. For students with low academic ability, the CML strategy was found to be the most effective
strategy. The findings of this study also suggested that high academic ability students would obtain high gain
scores regardless of the instructional strategies. In terms of time-on-task, students in CCL and CML strategies
demonstrated significant lower time-on-task than CCML strategy.

Keywords
Computer-assisted cooperative learning (CCL), Computer-assisted mastery learning (CML) and Computer-assisted
cooperative mastery learning (CCML)

Background of the study
One of the major problems among mainstream secondary school students is the performance difference between the
low achievers and the high achievers. To overcome this problem, various interventions had been offered including
curriculum-based assessment (Fuchs, Fuchs and Tindal, 1986), direct instruction curriculum design (Engelman and
Camine, 1982), mastery learning (Bloom, 1984), tutoring (Sleeman and Brown, 1982), learning strategies (Mason,
Burton and Stacey, 1982), and so forth. Unfortunately, most of these interventions required additional resources such
as teachers’ efforts and time needed to use them. However, the advent of Information and Communication
Technology (ICT) in the last few years has eased the burden on the resources needed for the teaching and learning
process. The use of computer as an ubiquitous teaching tool has become prevalent in Malaysian schools. As a result,
the use of computer in conjunction with effective teaching strategies has tremendous potential in the teaching and
learning process.
With the use of computer, mastery learning has a high potential to become an effective and extensive teaching and
learning tool (Guskey, 1997; Guskey and Gates, 1986; Kulik, Kulik, and Bangert-Downs, 1990). The mastery
learning method divides subject matter into units and each unit has predetermined objectives. Students should
achieve mastery on unit tests, typically 80%, before moving on to the following units. Students who do
not
achieve mastery receive remediation and students who achieve mastery have the opportunity to participate in
enrichment activities. Mastery learning fits well with other strategies and complements cooperative learning
(Guskey, 1997). Researchers, such as Dansereau (1988), Gunderson and Johnson (1980), Hooper, Temiyakarn, and
Williams (1993), had strongly recommended that cooperative learning should be used in the teaching and learning
process. As suggested by Guskey (1997), it needed a comprehensive framework to incorporate other instructional
strategies into mastery learning. Guskey (1997) had suggested cooperative learning as one of the instructional
strategy. Cooperative learning as part of collaborative learning has gained educators’ attention to include it into
process of learning (Wells and Brook, 2004). A meta-analysis based on 39 rigorous studies on a common basis in
science, mathematics, engineering and technology showed that generally, cooperative learning significantly
increased academic performance, decreased dropout rates and increased student self-confidence (Springer, Stanne &
Donovan, 1999).
Cooperative learning was preferable to be incorporated into mastery learning since the goal of using cooperative
learning was to accomplish a specific learning task through people working together in groups (Panitz, 1997). The
ISSN 1436-4522 (online) and 1176-3647 (print). © International Forum of Educational Technology & Society (IFETS). The authors and the forum jointly retain the
copyright of the articles. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that copies bear the full citation on the first page. Copyrights for components of this work owned by
others than IFETS must be honoured. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior
specific permission and/or a fee. Request permissions from the editors at kinshuk@ieee.org.

121

learners were more concerned with mastery of a pre-determined body of knowledge. In other words, mastery
learning complemented and fitted well with cooperative learning (Guskey, 1997).
Over the years, studies by Guskey and other researchers (Atkinsola, 1996; Mevarech, 1985) had found that
cooperative learning could be incorporated into mastery learning to present a conducive learning environment for
students. Students who focused on specific instructional goals were actively engaged in cooperative learning
activities, thus, effective study time was increased. Students were properly guided with mastery learning materials in
a cooperative learning environment in order to strengthen their skills of self-awareness and personal controls in
learning. Hence, cooperative learning was considered an efficient way of increasing effective study in mastery
learning. Results from the above studies had shown that the combination of mastery learning and cooperative
learning were found to be superior to the traditional lecture teaching format. Specifically, these studies indicated that
mastery learning and cooperative learning had an impact on affective and academic outcomes of students.
The combination of systematic design and integration of cooperative learning strategies, mastery learning and
interactive multimedia might have a great impact on the teaching and learning of subjects, such as Mathematics,
where hierarchical knowledge is the requirement of the field. Mathematics learning skills could easily be learned in a
cooperative setting. Cooperative learning provided opportunities to students with low academic abilities to model
their study skills and work habits as compared with high academic abilities. With the help given by students with
high academic abilities in explaining in detail the steps in the worked-out examples, weaker students were then
convinced to use these skills in order to obtain a mathematical solution. Besides, students with high abilities often
developed greater mastery during discussions by obtaining a deeper understanding of the task (Becker, Silver,
Kantowski, Travers, and Wilson, 1990; Stigler, Lee, Lucker, and Stevenson, 1982). With mastery learning,
cooperation needs were structured and guided through systematic instruction and feedback.
In learning mathematics, a diagnostic test (Teoh, 2003) had shown that students with difficulties in matrices were
also weak in the basic skills of mathematics, such as solving equations. Specifically, students who experienced
difficulties in matrices would find doing multiplication of two matrices confusing. It showed that the basic skills in
mathematics had become a necessity in solving problems and understanding other concepts in mathematics (Wu,
1999). Furthermore, to avoid omission of important processing skills, students had to be trained to master the basic
skills in the early stages of the learning experience with enough time and quality instruction (Bloom, 1968). If the
students were not provided with enough time, they might find difficulty to proceed to a higher stage of learning
(Harrell, Walker, Hildreth and Tayler-Wood, 2004). Without the awareness, weak students were found with no
improvement in their skills in mathematics. In contrast, if teachers tended to focus only on weak students, then
students with good performance would not be able to get the teachers’ attention in the learning process.
Subsequently, mastery learning played an important role in providing an environment for students to be involved in
their study, whereby students with high and low abilities were able to learn at their own pace, with the help of
feedback corrective and enrichment activities. However, each component of mastery learning involved a great
amount of work which made it inapplicable to the manageability and constraints relating to time (Anderson and
Jones, 1981; Levine, 1985). As an example, excessive amount of testing, corrective and enrichment activities were
needed during ‘feedback’, an important component in mastery learning. The time allocation for subjects in the
normal school curriculum was evidently not sufficient for mastery learning to be applied. Currently, the use of the elearning platform to teach developmental mathematics in a mastery learning format had been promoted to overcome
this obstacle (Boggs, Shore and Shore, 2004).
With the advent of ICT as a teaching tool and the availability of computer hardware in the schools, the problem in
applying mastery learning could be improved by using interactive courseware. Feedback activities could also be
easily conducted by using computers. In addition, for recording purposes of students’ performance, the technology
could also reduce time and effort required to implement comprehensive interventions needed in mastery learning
materials.

Objectives of the study
This study is mainly aimed to integrate cooperative learning strategies, mastery learning and interactive multimedia
to improve the students’ performance in Mathematics, specifically in the topic of matrices. The integration of
cooperative learning, mastery learning, and interactive multimedia environment would provide a comprehensive
122

framework needed for an effective and efficient teaching and learning of mathematical concepts. A computer-based
systematically designed interactive courseware was created to test the hypotheses of this study. The effects on the
gain scores and time-on-task would be investigated to determine the effectiveness of using the courseware in three
different strategies, namely, Computer-assisted Mastery Learning (CML), Computer-assisted Cooperative Learning
(CCL), and Computer-assisted Cooperative Mastery Learning (CCML). Students in the three learning strategies used
the same instructional materials. The CML strategy was based on individual learning, while the CCML and CCL
strategies were based on cooperative learning. Certain elements of mastery learning were added to the courseware,
which were used in the CML and CCML strategies. The CCL strategy was based on cooperative learning and used
the version of the courseware without the elements of mastery learning. The effects of the three learning strategies on
the gain scores and time-on-task were investigated.

Research framework
This study examined the effects of the three learning strategies, which were measured using gain scores and time-ontask. The moderator variable was the academic ability. The dependent variables were the gain score and time-ontask. The relationship among the variables is depicted in Figure 1, describes the research framework of this study. As
evidenced from the past social and cognitive psychology researches, academic achievement outcomes (gain scores)
had been a significant variable in learning success within education classroom (Schwarz, 1998). In addition, many
researchers (Schremmer, Hortz and Fries, 2001; Toh, 1998) used gain scores to investigate the effectiveness of
treatment in instruction.
Samples

Students
who
participate
d in this
study

Moderator
variables

Independent Variables

CML
Academic
abilities

Dependent
variables
Gain
scores

CCL
CCML

Time-ontask

Figure 1. Research framework

Carroll (1989) emphasized that if a student really spent time in learning as needed, then he would achieve
competence in learning. Bloom (1984) described that mastery learning would take more time than the normal
teaching. With high quality instruction, a variety of methods were included that made learning easier for students to
understand and remember. Thus, all students could learn differently with their individual abilities and work at their
own pace through the planned sequence of lessons. It helped to motivate them to learn important concepts in order to
proceed to the subsequent learning units. In this situation, the different academic abilities among learners would not
affect their learning to a great extent. Mastery learning could be easily adapted to reduce achievement differences
among students. Besides, time-on-task for a student could be shorter if an opportunity is given to a student to learn a
learning unit through a series of quality instruction. With quality instruction, students would be more persistent in
learning and increase their ability to understand the learning unit.
In mastery learning, students were grouped into high and low academic abilities. In this study, a standardized
Mathematics examination results from the PMR examination (Penilaian Menengah Rendah, a standardized
examination in Malaysia) were used to classify the students into high and low academic abilities. The examination
gauges students’ abilities after nine years of education in Malaysia. Therefore, it would be an accurate representation
of students’ mathematical abilities vis-à-vis the national norm as students could be classified into high and low
academic abilities.
The followings are the hypotheses of this study.
H1
There are no significant differences in the dependent variables among students in the CCL, CML and
CCML strategies.
123

H1.1

H2

H3

There is no significant difference in the gain scores among students in the CCL, CML and CCML
strategies.
H1.2
There is no significant difference in the time-on-task among students in the CCL, CML and CCML
strategies.
There are no significant differences in the dependent variables among students with high academic abilities
in the CCL, CML and CCML strategies.
H2.1
There is no significant difference in the gain scores among students with high academic abilities in
the CCL, CML and CCML strategies.
H2.2
There is no significant difference in the time-on-task among students with high academic abilities
in the CCL, CML and CCML strategies.
There are no significant differences in the dependent variables among students with low academic abilities
in the CCL, CML and CCML strategies.
H3.1
There is no significant difference in the gain scores among students with low academic abilities in
the CCL, CML and CCML strategies.
H3.2
There is no significant difference in the time-on-task among students with low academic abilities in
the CCL, CML and CCML strategies.

Methodology
The design of this research is a quasi-experimental design. This study involved 262 students from four different
secondary schools, namely schools A, B, C and D. Randomly, school A was assigned to the CCL treatment, school B
and D was assigned to the CML treatment and school C was assigned to the CCML treatment. The number of
students in CCL, CML and CCML were 77, 81 and 104 respectively.
The researcher had developed the courseware entitled "Matrices". Two sets of courseware were used in this study.
The first courseware was designed with mastery learning elements, used in the CML and CCML strategies. The
second courseware was designed without mastery learning elements, used in the CCL strategy. Before conducting
the experiments, the courseware was field-tested for revision purposes.
The courseware was used as the instruction in those three groups, which were CCL, CML and CCML. Gain scores
and time-on-task were used to investigate the effectiveness of the mentioned strategies. Before using the courseware,
an entry test was conducted to filter students’ basic knowledge in Matrices and determine whether they possessed the
requisite prior knowledge on arithmetic, which involved addition, subtraction, multiplication and division of
numbers (integers, fraction and decimal), and solution of one linear equation as well as two linear equations. A
student’s prior knowledge was considered high if he or she scored 80% and above. If a student scored less than 80%,
then the student had to complete an interactive courseware program on arithmetic. The interactive courseware was
specially created to strengthen those students’ prior knowledge in matrices. Students had to obtain the required level
of mastery before they could be given the treatment. Thus, before the actual commencement of the experiment, all
samples in each group would have achieved the required prior knowledge.
The PMR Mathematics result was used to classify the students into different academic abilities. Students with Grade
A and Grade B were grouped in the high academic ability category. Students with Grade C and Grade D were
grouped in the low academic ability category.
The pretest and posttest questions were developed to determine students’ understanding of important concepts
related to Matrices. These tests, that consisting of 51 questions, had the reliability of 0.7051 based on the KuderRichardson Formula (KR20).
On the first day of the data collection, students were given a briefing on the learning strategies. Next, students were
given a pretest on matrices and followed by a lesson on Matrices and Equal Matrices on the second day. After the
lesson, students were given the first formative test using the computer. The subtopics covered in the whole process
were:
(1)
Matrices and Equal Matrices
(2)
Addition and Subtraction on Matrices
(3)
Multiplication of a matrix by a number; Multiplication of two matrices
(4)
Identity Matrix, Inverse Matrix and solution of simultaneous linear equations by using Matrices.
124

The whole lesson took four to six hours to finish. Students took a test after each subtopic. The differences of three
treatment groups in terms of presentation of the lessons, team function and individual improvement were as depicted
in the following discussion. Students in the CML, completed all formative tests or quizzes independently. Students
who failed to meet the required performance level received supplementary instruction and corrective activities
immediately after each question until the requirement was met. At the end of a test, extra corrective activities were
given to those who could not achieve the success level of 80% as evaluated by the computer.
Students in the CCL completed all activities in cooperative groups and they completed all tests independently.
Students in the CCML completed all activities in cooperative groups and all tests were carried out independently for
this group. Students who failed to meet the required performance level received supplementary instruction and
correction activities immediately after each question until the requirement was met. At the end of a test, extra
corrective activities were given to those who could not achieve the success level of 80% as evaluated by the
computer. Each student needed to wait until all members in the group had achieved the level of 80%. Those who had
finished and achieved 80% of the score were able to help other students who had not achieved 80% of the score. All
the cooperative work was examined using checklist for the approach named Student Team Achievement Division
(STAD).
The design of the courseware was based on a macro and micro design. Alessi and Trollip’s instructional design
model (Alessi and Trollip, 2001) was used for the macro design. Gagné’s nine events of instruction (1985) was used
for the micro design of the courseware. Motivational elements were incorporated into the courseware which was
created based on Gagné’s Motivational elements were incorporated into the courseware which was created based on
Gagné’s nine events of instruction.

Results
MANOVA was used with gain scores and time-on-task as the two dependent variables and the learning strategies
(CCML, CML and CCL) as the group factor. Follow-up analyses would be conducted if the test on MANOVA was
significant.

Gain scores and time-on-task for the three learning strategies
The descriptive statistics on the gain scores and time on task for CCL (GainCCL), CML (GainCML) and CCML
(GainCCML) are shown in Table 1, where GainCCML > GainCML > GainCCL and TimeCML < TimeCCL < TimeCCML.
Table 1. Descriptive statistics on gain score for CCL, CML and CCML

Mean
N
Std Dev

CCL
Gain
Time-onscores
task
31.47
3.90
77
77
19.206
0.771

CML
Gain
Time-onscores
task
42.79
3.70
81
81
19.678
1.030

CCML
Gain Time-onscores
task
49.40
4.71
104
104
17.849
0.784

TOTAL
Gain Time-onscores
task
42.09
4.16
262
262
20.164
0.973

The results of the MANOVA test (Table 2) showed that the Wilk’s lambda of 0.549 was significant, F = 45.032, p <
0.05. Thus, Hypothesis One, which stated that the population means on dependent variables (i.e., gain scores and
time-on-task) were the same for the three groups, was rejected. The multivariate Eta Squared indicated that 25.9% of
multivariate variance of the dependent variables was associated with the group factor.
Table 2. Multivariate tests of the effect of learning strategies on the dependent variables
Effect
strategies

Value
Wilks'
Lambda

.549

F
45.032

Hypothesis df
4.000

Error df
516.000

Sig.
.000

Partial Eta
Squared
.259
125

Using multiple univariate ANOVAs, a follow-up approach was conducted. Two ANOVAs were conducted, one for
each dependent variable (i.e., gain scores and time-on-task). The results of the univariate ANOVAs are shown in
Table 3. The univariate ANOVA for gain scores was significant, F = 20.155, p < 0.025. Likewise, the univariate
ANOVA for time-on-task was significant, F= 36.066, p < 0.025. Both results showed that there were significant
differences of gain scores and time-on-task among the groups. Therefore, Hypothesis 1.1 and Hypothesis 1.2 were
rejected.
Table 3. Univariate tests of the effect of learning strategies on the dependent variables
Source
Corrected
Model
Intercept
Strategies
Error
Total
Corrected
Total

Dependent
Variable
Gain Scores

Type III Sum of
Squares

df

Mean Square

F

Sig.

Partial Eta
Squared

14291.342

2

7145.671

20.155

.000

.135

Time-on-task
Gain Scores
Time-on-task
Gain Scores
Time-on-task
Gain Scores
Time-on-task
Gain Scores
Time-on-task
Gain Scores

53.863
437568.203
4336.984
14291.342
53.863
91825.639
193.404
570219.000
4782.000

2
1
1
2
2
259
259
262
262

26.932
437568.203
4336.984
7145.671
26.932
354.539
.747

36.066
1234.189
5807.944
20.155
36.066

.000
.000
.000
.000
.000

.218
.827
.957
.135
.218

106116.981

261

Time-on-task

247.267

261

The analyses revealed that there were significant differences in gain scores for the two pairs- CCML with CCL and
CML with CCL. Also, there were significant differences in time-on-task for the two pairs- CCML with CCL and
CCML with CML.

The effect sizes of learning strategies on the gain score
Effect sizes of CML and CCML towards CCL were studied because there were significant differences on gains
scores between CML and CCL as well as between CCML and CCL. Calculations of the effect size (ES) of CML and
CCML towards CCL are illustrated in Table 4. The results showed that the effect size of CML towards CCL was
0.5603, which was moderate. This indicated that an individual learner in CML had a 0.5603 standard deviation
increase. The effect size of CCML towards CCL was 0.8778. Therefore, effect size of CCML towards CCL was
stronger if compared to the effect size of CML towards CCL.
Table 4. The effect size of CCML and CML towards CCL
Learning Strategies

CML
CCL
CCML
CCL

Difference of Means

Pooled Standard
Deviation

11.32

20.202

17.93

20.424

Effect Size, ES
The
Difference of Means
=
Pooled Standard Deviation
11.32
 0.5603
20.202
17.93
ES 
 0.8778
20.424

ES 

The learning strategies effects on the dependent variables among students with high academic ability
The descriptive statistics on gain scores and time-on-task for CCL, CML and CCML of students with high academic
abilities are illustrated in Table 5. For gain scores, the mean of CML (GainCML) and CCML (GainCCML) were close to
each other, with the respective values of 53.49 and 52.52. Subsequently, CCL showed a lesser value in the gain
126

scores (GainCCL) of 46.03. Thus, students with high academic ability obtained gain scores in the following sequence,
GainCML > GainCCML > GainCCL.
Table 5. Descriptive statistics on gain scores and time-on-task among students with high academic ability
Gain Scores

Time-on-task

Strategy
Cooperative (CCL)
Mastery(CML)
Cooperative mastery(CCML)
Total
Cooperative(CCL)
Mastery(CML)
Cooperative mastery(CCML)
Total

Mean
46.03
53.49
52.52
51.45
3.46
3.27
4.57
4.00

Std. Deviation
18.128
18.505
16.254
17.372
.657
.780
.684
.930

N
35
45
92
172
35
45
92
172

It can be seen in Table 5 that the time-on-task of CML and CCL are close to each other which were 3.27 hours
(TimeCML) and 3.46 hours (TimeCCL) respectively. Subsequently, students in CCML spent longer time-on-task
(TimeCCML), that was 4.57 hours.
Results of the MANOVA test (Table 6) showed that Wilk’s lambda of 0.52 was significant, F = 32.542, p < 0.05.
Thus, Hypothesis Two, which stated population means on the dependent variables among students with high
academic ability were the same for the three groups, was rejected. The multivariate Eta Squared showed 27.9% of
multivariate variance of the dependent variables was associated with the group factor.
Table 6. Multivariate tests of effect of learning strategies on the dependent variables among students with high
academic ability
Effect
Strategy

Wilks' Lambda

Value
.520

F
32.542

Hypothesis df
4.000

Error df
336.000

Sig.
.000

Partial Eta Squared
.279

A follow-up approach was conducted by using multiple univariate ANOVAs. Two ANOVAs were conducted, one
for each dependent variable (i.e., gain scores and time-on-task). Results of the univariate ANOVAs are shown in
Table 7. The univariate ANOVA for gain scores was not significant, F = 2.221, p > 0.025, but the univariate
ANOVA for time-on-task was significant, F = 64.214, p < 0.025 and the associated Eta Squared was 43.2%. The
results showed that there were significant differences in time-on-task for the two pairs- CCML with CCL and CCML
with CML. Thus, Hypothesis 2.1 was not rejected and Hypothesis 2.2 was rejected.
Table 7. Univariate test of the effect of learning strategies on the dependent variables among students with high
academic ability
Source
Corrected
Model

Dependent
Variable
Gain Scores

Time-on-task
Gain Scores
Time-on-task
Strategy
Gain Scores
Time-on-task
Error
Gain Scores
Time-on-task
Total
Gain Scores
Time-on-task
Corrected Total Gain Scores
Time-on-task
Intercept

Type III Sum
of Squares

Df

Mean Square

F

Sig.

Partial Eta
Squared

1321.456

2

660.728

2.221

.112

.026

63.906
374873.704
2066.745
1321.456
63.906
50283.172
84.094
506968.000
2900.000
51604.628
148.000

2
1
1
2
2
169
169
172
172
171
171

31.953
374873.704
2066.745
660.728
31.953
297.534
.498

64.214
1259.938
4153.425
2.221
64.214

.000
.000
.000
.112
.000

.432
.882
.961
.026
.432

127

The learning strategies effects on the dependent variables among students with low academic ability
The descriptive statistics on gain scores and time-on-task for CCL, CML and CCML of students with low academic
ability are shown in Table 8. For the gain scores, it can be seen that the mean of CML (GainCML) was the highest,
which was 29.42, and followed by CCML (GainCCML) with the mean of 25.50. Mean of gain scores among students
in CCL (GainCCL) were the lowest, which was 19.33. Thus, GainCML > GainCCML > GainCCL.
Table 8. Descriptive statistics on gain scores and time-on-task among students with low academic ability
Strategy
Cooperative (CCL)
Mastery (CML)
cooperative mastery (CCML)
Total
Cooperative (CCL)
Mastery (CML)
cooperative mastery (CCML)
Total

Gain Scores

Time-on-task

Mean
19.33
29.42
25.50
24.19
4.26
4.25
5.83
4.47

Std. Deviation
8.913
11.111
9.625
10.909
.665
1.052
.577
.985

N
42
36
12
90
42
36
12
90

As shown in Table 8, time-on-task for CML and CCL are approximately equal which were 4.25 hours (TimeCML) and
4.26 hours (TimeCCL ) respectively. It was observed that the mean of the time-on-task in CCML (TimeCCML ) was the
highest, which was 5.83 hours.
The results of the MANOVA test (Table 9) showed that Wilk’s lambda of 0.527 was significant, F= 16.239, p <
0.05. Thus, Hypothesis Three, which stated that the population means on the dependent variables (i.e., gain score and
time-on-task) among students with low academic ability were the same for the three groups, was rejected. The
multivariate Eta Squared indicated that 27.4% of multivariate variance of dependent variables was associated with
the group factor.
Table 9. Multivariate tests of the effect of learning strategies on the dependent variable among students with low
academic ability
Effect
Strategy

Wilks' Lambda

Value
.527

F
16.239

Hypothesis df
4.000

Error df
172.000

Sig.
.000

Partial Eta Squared
.274

A follow-up approach was conducted by using multiple univariate ANOVAs. Two ANOVAs were conducted, one
for each dependent variable (i.e., gain scores and time-on-task). Results of the univariate ANOVAs are shown in
Table 10. The univariate ANOVA for gain scores was significant, F = 10.093, p < 0.025 and the associated Eta
Squared was 18.8%. Also, the univariate ANOVA for time-on-task was significant, F = 18.586, p < 0.025 and the
associated Eta Squared was 29.9%. Thus, Hypothesis 3.1 and Hypothesis 3.2 were rejected.
Table 10. Univariate test of the effect of learning strategies on the dependent variable among students with low
academic ability
Source
Corrected Model
Intercept
Strategy
Error
Total
Corrected Total

Dependent
Variable
Gain Scores
Time-on-task
Gain Scores
Time-on-task
Gain Scores
Time-on-task
Gain Scores
Time-on-task
Gain Scores
Time-on-task
Gain Scores
Time-on-task

Type III Sum of
Squares
1994.706
25.864
40861.522
1525.236
1994.706
25.864
8597.083
60.536
63251.000
1882.000
10591.789
86.400

df
2
2
1
1
2
2
87
87
90
90
89
89

Mean
Square
997.353
12.932
40861.522
1525.236
997.353
12.932
98.817
.696

F
10.093
18.586
413.507
2192.021
10.093
18.586

Sig.
.000
.000
.000
.000
.000
.000

Partial Eta
Squared
.188
.299
.826
.962
.188
.299

128

Discussion
Generally, there were significant differences in gain scores between the learning strategies. The effect size in gain
scores suggested that the CCML strategy had a more positive effect than the CML strategy. These results support the
findings from past research that cooperative mastery learning produced better results (Akinsola, 1996; Krank and
Moon, 2001; Laney, Frarich, Farich & Luke, 1996). Furthermore, these results are consistent with Mevarech’s
(1985) and Okebukola’s (1985) findings that discovered positive effects of cooperative learning in the application of
STAD approach and even better effects if it was combined with mastery learning.
Although the CCML strategy had better results in the gain scores, it showed no significant difference in the gain
scores between the CCML and CML strategies. In this case, the contribution of the CML and CCML strategies are
equally important in terms of the gain scores in which both learning strategies had mastery learning. In other words,
mastery learning plays an important role in organizing a systematic and more structured instruction to guide students
for gaining high scores. In addition, incorporating cooperative learning could strengthen the role of mastery learning.
This study also found that students in cooperative mastery learning groups were guided through well-designed
instruction. Hence, better effect size was seen when the CCML strategy was used. These were consistent with
Okebukola’s (1985) findings that cooperative learning could strengthen students' performance. This study shows
that the effect size of cooperative mastery learning was the highest in the gain scores and the effect was mainly
contributed by mastery learning. The findings also support Mevarech’s (1991) view that mastery learning had been
successful in producing gains in achievement. There was a tendency to incorporate such programmes with
cooperative learning strategies, which was called cooperative mastery learning (Mevarech, 1985).
Although mastery learning (systematic work) was the most important instructional method to make students succeed,
it is better if supported by cooperative learning. This finding suggests that the advantages in cooperative learning
were not obviously shown in the gain scores without mastery learning. This study shows that mastery learning plays
a primary role and when incorporated with cooperative learning, students will learn more and systematically in the
cooperative environment. Some students might be weak in the socialization and interaction skills and might need
guidelines in mastery learning. Likewise, some students needed peer-guidance during the learning process. Thus, the
CCML strategy was found to be the most effective learning strategy in this study.
In terms of time-on-task, the major finding is that students in the CML and CCL strategies spent shorter time-on-task
compared to the CCML strategy. The results are consistent with past researches (Mortimore, and Sammons, 1987).
According to Zimmerman (1998), there was an apparent correlation between time and achievement. The more timeon-task was made available to the student, the more activities and learning processes were involved. However, past
literature suggested that even though time was certainly a critical factor but it had little direct impact on students’
performance (Zimmerman, 1998). Simply adding time would not really produce large gains in student achievement.
Rather, quality was the key to making time matter (Funkhouser, Humphrey, Panton and Rosenthal, 1995; Levin,
1985). Essentially, students should be provided with activities and instructions that catered to their needs and
abilities, engaging them so they would continue to build on what they had learnt. What matters most were the
valuable experience when students were absorbed in instructional activities that were adequately challenging, yet
allowed them to experience success. This study has shown that the CML and CCML strategies could provide these
experiences for the students.
For students with high academic abilities, the analysis showed that there were no significant differences in the gain
scores among students with high academic ability in the CCL, CML and CCML strategies. The finding suggests that
high academic ability learners could obtain high gain scores regardless of learning strategies. In many ways, students
with high academic abilities were more alike in terms of the way they managed themselves in studies (Monaco,
2003). Students with high academic abilities were able to learn under any condition in the school, for example, small
groups, alone and in informal settings. Thus, teachers could use any grouping pattern and instructional method as
long as the method used could accommodate their teaching objectives.
This study also shows that there were significant differences in time-on-task across the learning strategies among
students with high academic abilities. Students with high academic abilities in the CML and CCL strategies
significantly spent shorter time-on-task compared to the CCML strategy. Therefore, the CCML students who were
involved in using components of mastery learning and cooperative learning spent more time compared to those who
were involved in either the mastery or the cooperative learning only. These findings indicated that there were no
effects resulting from learning strategies among students with high academic ability. However, students with high
129

academic abilities spent significant higher time-on-task on CCML without showing higher gain scores significantly.
This result shows that cooperative learning provided a conducive structure for learning because students with high
academic abilities were fully engaged to help other counterparts by clarifying misunderstandings and correcting
learning errors to attain a criterion-referenced standard through a well-designed mastery learning instruction as
revealed by Bork (1999). Therefore, students with high academic abilities in the CCML strategy showed significant
higher time-on-task without showing higher gain scores.
For students with low academic abilities, the analyses reveal that there were significant differences on gain scores for
CML and CCL. Also, there were significant differences on time-on-task for the two pairs which were CCML with
CCL and CCML with CML.
The above results show that students with low academic abilities did not achieve significantly higher gain scores in
the CCL and CCML strategies if compared to the CML strategy. This indicates that cooperative learning which was
used in the CCL and CCML strategies did not give assistance to students with low academic ability. Additionally,
within cooperative learning groups, students with low academic abilities did not influence each other’s learning.
Nevertheless, with the well-designed mastery learning instruction within a mastery learning environment, they could
attain significantly higher gain scores compared to students in the CCL and CCML strategies. This could be
interpreted within the context of the group processing concept in cooperative learning. According to Yager, Johnson,
and Johnson (1985), the achievement of low academic ability students in cooperative learning environment also
depended on how well their group was functioning.
In terms of time-on-task, the results were consistent with the results of students with high academic abilities, where
students in CML and CCL significantly spent shorter time-on-task compared to CCML. Therefore, in terms of
getting a higher gain score and shorter time-on-task, CML was the best learning strategies among students with low
academic abilities. With the CML strategy, students’ result increased within a shorter time compared to the students
who used the CCML strategy.

Summary and conclusion
In conclusion, this study has led to the following key findings. Firstly, this study shows that the CCML and CML
strategies are superior compared to the CCL strategy. Secondly, the CCML strategy is the best choice among the
three learning strategies to obtain a higher gain score. However, for students with low academic ability, the CML
strategy is found to be the best choice. This finding also suggests that high academic ability learners could obtain
high gain scores regardless of learning strategies. Thirdly, in terms of time-on-task, students in CCL and CML
strategies demonstrated significant lower time-on-task than CCML strategy.
The findings of this study propose a simple yet powerful approach to facilitate the learning process of students
through the use of a multimedia integrated learning system with a series of high quality instructions in cooperative
mastery learning and mastery learning.

References
Alessi, S. M., & Trollip, S. R. (2001). Multimedia for learning. NJ: Allyn and Bacon.
Anderson, L. W., & Jones, B. F. (1981). Designing instructional strategies which facilitate learning for mastery. Educational
Psychologist, 16(3), 121-137.
Atkinsola, M. K. (1996). Combined mastery learning and cooperative learning strategy was found to be more suitable in
facilitating achievement in integrated science in the Junior Secondary School. Retrieved September 24, 2001, from
http://www.ipn.uni-kiel.de/projekte/esera/book/b132-aki.pdf.
Becker, J.P., Silver, E.A., Kantowski, M.G., Travers, K.J., & Wilson, J.W. (1990). Some observations of mathematics teaching in
Japanese elementary and junior high schools. Arithmetic Teacher, 38(2), 12-21.
Bloom, B. S. (1968). Learning for mastery. Evaluation Comment, 1(2), 1-5.
Bloom, B. S. (1984). The search for methods of group instruction as effective as one-to-one tutoring. Educational Leadership,
130

41(8), 4-18.
Boggs, S., Shore, M., & Shore, J.A. (2004). Using e-Learning platforms for mastery learning in developmental mathematics
courses. Mathematics and Computer Education. 38(2), 213-220.
Bork, A. (1999). The future of learning. Educom Review. 34(4).
Carroll, J. B. (1989). The Carroll model: A 25-year retrospective and prospective view. Educational Researcher, 18(1), 26-31.
Dansereau, D. F. (1988). Cooperative learning strategies. In C. E. Weinstein, E. T. Goetz, & P.A. Alexander (Eds.), Learning and
study strategies: Issues in assessment, instruction, and evaluation (p. 103-120). New York: Academic Press.
Engelmann, S., & Camine, D. (1982). Theory of instruction: Principles and application. New York: Irvington.
Fuchs, L. S., Fuchs, D., & Tindal, G. (1986). Effects of mastery learning procedures on student achievement. Journal of Chemical
Education, 79(5), 286-291.
Funkhouser, J. E., Humphrey, D. C., Panton, K. L. M., & Rosenthal, E. D. (1995). A research review: The educational uses of
time. (Vol. IV). Washington, DC: Policy Studies Associates, Inc.
Gagné, R. M. (1985). The conditions of learning and theory of instruction. Fort Worth: Holt, Rinehart and Winston.
Gunderson, B., & Johnson, D. W. (1980). Building positive attitudes by using cooperative learning groups. Foreign Language
Annals, 13, 39-46.
Guskey, T. R. (1997). Implementing mastery learning. New York: Wadsworth.
Guskey, T. R., & Gates, S. L. (1986). Synthesis of research on the effects of mastery learning in elementary and secondary
classroom. Educational Leadership, 43(8), 73-80.
Harrell, P. E., Walker, M., Hildreth, B., & Tayler-Wood, T. (2004). Mentoring BUGS: An integrated science and technology
curriculum. The Journal of Computers in Mathematics and Science Teaching. Austin. 23(4). 367-378.
Hooper, S., Temiyakarn, C., & Williams, M. D. (1993). The effects of cooperative learning and learner control on high- and
average-ability students. Educational Technology Research and Development, 41(2), 5-18.
Krank, H. M., & Moon, C. E. (2001). Can a combined mastery/cooperative learning environment positively impact undergraduate
academic and affective outcomes?. Journal of College Reading and Learning, 31(2), 195-208.
Kulik, J. A., Kulik, C. C., & Bangert-Drowns, R. L. (1990). Effectiveness of mastery learning programs: A meta-analysis. Review
of Educational Research, 60(2), 265-299.
Laney, J. D., Frarich, D. K., Frarich, L. P., & Luke, K. P. (1996). The effect of cooperative and mastery learning methods on
primary grade students' learning and retention of economic concepts. Early Education and Development, 7(3), 253-74.
Levine, D. (1985). Improving student achievement through mastery learning programs. San Francisco: Jossey-Bass.
Mason J., Burton L., & Stacey K. (1982). Thinking mathematically. London: Addison-Wesley.
Mevarech, Z. R. (1985). The effects of cooperative mastery learning strategies on mathematical achievement. Journal of
Educational Research, 78(6), 372-377.
Mevarech, Z. R. (1991). Learning mathematics in different mastery environments. Journal of Educational Research, 84(4), 225231.
Monaco, T. (2003). Gifted education research practices recommended in Doctoral Dissertations. Academic Exchange- EXTRA.
Retrieved June 23, 2009, from http://asstudents.unco.edu/students/AE-Extra/2003/11/Art-3.html.
Mortimore, P., & Sammons, P. (1987). New Evidence on effective elementary schools. Educational Leadership, 45 (September):
4-8.
Okebukola, P. A. (1985). The relative effectiveness of cooperative and competitive interaction techniques in strengthening
students’ performance in science classes. Science Education, 69, 501-509.
Panitz, T. (1997). Collaborative versus cooperative learning: comparing the two definitions helps understand the nature of
interactive learning. Cooperative Learning and College Teaching, 8(2), 1-13.
Schremmer, Hortz, H., & Fries, S. (2001). Testing the knowledge gained in multimedia-enhanced learning. Retrieved September
4, 2009, from http://www.informatik.uni-mannheim.de/pi4/publications/Schremmer2001i.pdf.
Schwarz, N. (1998). Accessible content and accessibility experiences: The interplay of declarative and experiential information in
judgment. Personality and Social Psychology Review, 2(2), 87-99.
Sleeman, D., & Brown, J. S. (1982). Introduction: Intelligent tutoring systems. In D. Sleeman & J. S. Brown (Eds.), Intelligent
131

tutoring systems (p. 1-11). London: Academic Press.
Springer, L., Stanne, M.E. & Donovan, S. (1999). Effects of small-group learning on undergraduates in Science, Mathematics.
Engineering and Technology: A meta-analysis. Review of Educational Research, 69(1), 21-51.
Stigler, J.W., Lee, S.Y., Lucker, G.W.. & Stevenson. H.W. (1982). Curriculum and achievement in mathematics: A study of
elementary school children in Japan, Taiwan, and the United States. Journal of Educational Psychology, 74, 315-322.
Teoh, S. H. (2003). Mistake made in the topic of Matrices among secondary school students. Presented at
Scientific and Social Research (CSSR) 2003, the Golden Horses, Seri Kembangan, Malaysia.

Conference on

Toh, S. C. (1998). Cognitive and motivational effects of two multimedia simulation presentation modes science learning.
Unpublished Ph.D. thesis, University of Science Malaysia.
Wells, M.A., & Brook, P.W. (2004). Conversational KM - Student Driven Learning. Sixth Australasian Computing Education
Conference (ACE2004), Dunedin, New Zealand.
Wu, H. (1999). Basic skills versus conceptual understanding: a bogus dichotomy in mathematics education. American Educator,
23(3), 14-19.
Yager, S., Johnson, D. W., & Johnson, R. (1985). Oral discussion groups-to-individual transfer, and achievement in cooperative
learning groups. Journal of Educational Psychology, 77(1), 60-66.
Zimmerman, J. (1998). Improving student achievement by extending school: Is it just a matter of time? The PACE Media/
Education Writers Seminar.

132

Chen, C.-H., & Howard, B. (2010). Effect of Live Simulation on Middle School Students' Attitudes and Learning toward Science.
Educational Technology & Society, 13 (1), 133–139.

Effect of Live Simulation on Middle School Students’ Attitudes and Learning
toward Science
Ching-Huei Chen and Bruce Howard*
Graduate Institute of e-Learning, National Changhua University of Education, Changhua City, Taiwan //
chhchen@cc.ncue.edu.tw
*
Center for Educational Technologies®, NASA-Sponsored Classroom of the Future, Wheeling Jesuit University,
Wheeling WV, USA // howard@cet.edu
ABSTRACT
This study examined the effect of live simulation on students’ science learning and attitude. A total of 311
middle school students participated in the simulation, which allowed them to access and interpret satellite data
and images and to design investigations. A pre/post design was employed to compare students’ science learning
and attitude before and after the simulation. The findings revealed positive changes in students’ attitudes and
perceptions toward scientists, while male students had more positive adoption toward scientific attitudes than
females. The study also found that the change in student’s science learning was significantly influenced by the
teacher. Hence, teacher classroom preparation for the simulation experience proved vital to students’ attitudes
toward science as well as their scientific understanding. Implications for effective use of simulation to increase
science-related career awareness and inform effective teaching practice are shared and discussed.

Keywords
Simulation, Science attitude, Videoconferencing, and Teaching practice

Introduction
There has been a prolonged discussion on the use of technology as cognitive tools for teaching and learning.
Jonassen and Reeves (1996) characterized cognitive tools as “technologies that enhance the cognitive powers of
human beings during thinking, problem solving, and learning” (p.693). Technology holds great potential for students
to develop deeper knowledge and execute reflective thoughts by the specific tasks that they otherwise will not have
access to. Technology also provides capabilities to complement students’ learning styles and multiple intelligences.
Accordingly, simulation has emerged as one of the most popular instructional tools for delivering quality instruction.
The use of realistic simulation often requires students to apply newly acquired skills while motivating them toward
advanced learning (Hsu & Thomas, 2002; Lewis, Stern, & Linn, 1993; Moreno & Mayer, 2007; Weller, 2004).
Frequently, students participating in a simulation perceived the experience as helpful in providing a clear context for
the application of learned knowledge and in being a motivating experience (Spinello & Fischbach, 2004). Previously,
Jarvis and Pell (2005) investigated the effect of live simulation experience on middle school students in the United
Kingdom. They found such experience engaged students in performing expert-like thinking and acting as real
scientists would to analyze and assess real-time data. Although the development of technology-rich learning
environments has progressed greatly in recent years, researchers have just begun to signify the application of
technology for science learning and its impact on students’ achievement (Jonassen, 2003; Kim & Reeves, 2007).
In this paper, we sought to review previous research on scientific inquiry-based learning and its impact on students’
attitudes toward science, evaluate how simulation would be an ideal way to support inquiry learning and promote
positive attitudes toward science, and describe potential underpinning factors to optimize a successful simulation
experience for students.

Theoretical Perspectives
Inquiry science learning
Scientific concepts are complex, highly technical, and often considered among the most difficult subjects to teach K12 students. Recently, many science educators have advocated an inquiry-based approach to learning science in
which students are given opportunities to actively build scientific knowledge by asking overarching questions,
planning strategies, exploring solutions, constructing new knowledge, and reflecting on their own inquiry process
ISSN 1436-4522 (online) and 1176-3647 (print). © International Forum of Educational Technology & Society (IFETS). The authors and the forum jointly retain the
copyright of the articles. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that copies bear the full citation on the first page. Copyrights for components of this work owned by
others than IFETS must be honoured. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior
specific permission and/or a fee. Request permissions from the editors at kinshuk@ieee.org.

133

(Linn, 2000). The learning sciences community agrees that deep and effective learning is best promoted by situating
learning in purposeful and engaging activity (Cognition and Technology Group at Vanderbilt, 1993). The goal of this
study is to use the technology to mimic the real-world scientific activities and make the inquiry processes become
salient to the students.

Technological Support for Science Learning and Attitudes
Several longitudinal investigations into the use of technology in students’ science, technology, engineering, and
mathematics (STEM) learning are ongoing, but very little attention has been given to discovering the outcomes of
such endeavors (Boyle, Lamprianou, & Boyle, 2005). Technology can help in the scientific learning process because
of its potential to support activities such as data collection, visualization, meaningful thinking, problem solving, and
reflection. In fact, much of our current educational practice grows out of curriculum reform efforts that have
emphasized the teaching of process skills involved in the construction of scientific knowledge—diverse skills such as
observing, classifying, measuring, conducting controlled experiments, and constructing data tables and graphs of
experimental results (Linn & Hsi, 2000). Various strategies to promote better science learning have been explored.
For example, the Web-based Inquiry Science Environment (WISE) is one of the curriculum projects that Linn and
her colleagues created to help students develop more cohesive, coherent, and thoughtful accounts of scientific
phenomena (Linn, Clark, & Slotta, 2003). WISE is guided by an instructional framework called scaffolded
knowledge integration (SKI) that requires students to reflect on their deliberately developed repertoire of models for
complex phenomena, and to work toward expanding, refining, reconciling, and linking these models (Bell, 2002;
Linn, 1995). In WISE, students who engage in various inquiry activities such as compare ideas, distinguish cases,
identify the links and connections among ideas, seek evidence to resolve uncertainty and categorize valid
relationships show better understandings of complex scientific concepts (Davis & Linn, 2000). Further research into
how the effects of using technology-mediated tools to facilitate science practices, such as applying various real data
to empower students to understand the scientific enterprise itself, are worth further discussion.
Researchers have long discussed whether students’ positive attitudes toward science can influence whether students
consider science as a career (Papanastasiou & Zembylas, 2004). Several studies have found that students’ attitudes
toward science correlated with science achievement and participation in advanced science courses (e.g., Lee &
Burkam, 1996; Simpson & Oliver, 1990). It is also well known that students’ attitudes toward a subject as well as
their learning environment impact school achievement. In this study we sought to understand whether a technologysupported simulation learning environment can improve students’ positive attitudes toward science subjects and
careers.

Teaching Practice on Science Learning and Attitude
High-quality teachers are essential to improving the teaching and learning (Darling-Hammond, 1997). Teaching
practice and instructional decisions influence the quality of students’ academic performance and their motivation,
effort, and attitudes toward school and academic pursuits (Hidi & Harackiewicz, 2000). They also promote or reduce
students’ learning and achievement (Hardre & Chen, 2005). Research involving both secondary and older students
appears to indicate a relationship between teacher behaviors and students’ attitudes toward science (Haladyna, Olsen,
& Shaughnessy, 1982; Myers & Fouts, 1992). Children with positive attitudes toward science are more likely to be
found in classrooms that have high levels of involvement, teacher support, and use of innovative teaching strategies
(Myers & Fouts, 1992). Teachers who lack ability, confidence, and enthusiasm for the subject tend to use less
stimulating, more didactic methods and do not respond effectively to students’ questions (Osborne & Simon, 1996).
Those teachers also are more likely to have students with poor attitudes toward science. Effective teachers adapt
learners’ needs and evaluate how information should be presented. To meet these demands, teachers need to adjust
instruction to student ability levels and background. In fact, one study showed that teachers’ teaching style and
instructional decisions are the most noticeable factors in students’ attitude toward science (Jarvis & Pell, 2005).
Therefore, we surveyed teachers about their teaching practices to understand how their approaches might affect
students’ knowledge and attitude toward science as the result of a simulation learning experience.

134

The Challenger Mission: Emergency Responsive Learning Experience
The live simulation learning experience conducted for this study originates from the Challenger Learning Center at
Wheeling Jesuit University in Wheeling, WV, one of more than 50 Challenger Learning Centers in the United States,
Canada, and the United Kingdom. These centers for space science were created in memory of the space shuttle
Challenger. More than 25,000 students fly missions each year through the Wheeling facility, and it has been honored
nine times for having served the most children of all the centers.
The live simulation, or e-Mission™, is a real-world adventure delivered into the classroom via distance learning
technology. With the use of the internet and video conferencing equipment, these live simulations take place in the
classroom by a flight director at mission control from the Challenger Learning Center at Wheeling Jesuit University.
The learning approach is a student-centered, team-based, interactive educational experience that encourages students
to use scientifically accurate data to solve problems. Before the live simulation, teachers conduct a pre-mission
preparation for their students, which covers all the mission materials needed for the culminating “live” event. On the
mission day, students are assembled into emergency response teams. Via the Internet and videoconferencing
equipment, teams connect to a flight director at mission control in Wheeling. The emergency response teams work
together and with mission control to handle a “live” problem while the scenario unfolds. Every few minutes new data
is delivered to the classroom. Students perform calculations, create graphs, assess the situation, and make decisions
based upon their analysis of the data.
The Challenger Learning Center offers a number of e-Missions for all age groups covering mathematics, Earth
science, and biology. In this study we researched teachers and students who participated in the Operation Montserrat
e-Mission, which uses actual data collected during a 1996 volcanic eruption on the Caribbean island of Montserrat
and during a hurricane that hit the island a few years earlier.

Methods
Subjects/Procedures
The participants were 311 (186 males; 125 females) middle school students and 7 teachers from West Virginia, Ohio,
Pennsylvania, and New York. Before the e-Mission teachers attended a one-day workshop at the Challenger
Learning Center, where they covered Earth science curriculum and learned about the study procedure. Teachers and
students completed surveys before any mission-related activity. Teachers spent a week preparing students for the
mission. Although all of the participating teachers received one-day professional training on the e-Mission, the actual
classroom implementation and time allocation for the mission preparation was left up to each teacher. On the mission
day classrooms connected with Challenger Learning Center via videoconferencing to interact and solve problems
with a flight director. The scenario unfolds as the Soufriere Hills volcano is ready to erupt while at the same time a
Category 3 hurricane is approaching Montserrat from the east. Students worked in teams to take charge of different
tasks. The volcano team calculated rock fall and volcanic tectonic data to predict what would happen with the
volcano. The hurricane team tracked the approaching hurricane and estimated when it would arrive on the island.
The evacuation team used population maps and available transportation options to move residents out of danger
zones to shelters on the island. The communications team informed mission control about the situation brewing on
the island and relayed recommendations from all teams. All the data were real and sent from the satellite every 5-6
minutes. The length of the live simulation activity was approximately two hours. A week after the mission, students
and teachers took the post-surveys.

Surveys/Instruments
Attitudes toward Science
Osborne, Simon, and Collin (2003) suggested that attitudes toward science is not a unitary construct, but “rather of a
large number of subconstructs, all of which contribute in varying proportions towards an individual’s attitudes
towards science” (p. 1054). Our study used the Test of Scientific-related Attitudes (TOSRA), designed by Fraser
(1978) from Klopfer’s Classification (Klopfer, 1971) because this instrument contains more focused scales to
135

measure the sub-constructs of attitude toward science among middle school students. The instruction for completing
the TOSRA survey was given in the beginning: This test contains a number of statements about science. You will be
asked what you think about these statements. There are no “right” or “wrong” answers. Your opinion is what is
wanted. For each statement, draw a circle around the specific numeric value corresponding to how you feel about
each statement. 5 as strongly agree (SA), 4 as agree (A), 3 as uncertain (U), 2 as disagree (D), and 1 as strongly
disagree (SD). TOSRA with a total of 70 items includes seven distinct science-related attitudes. The first factor is
social implications of science. Here’s a sample statement measuring it: “Public money spent on science in the last
few years has been used wisely.” The second subscale is normality of scientists: “Scientists do not have enough time
to spend with their families.” Attitude toward scientific inquiry is the third subscale: “I would prefer to do
experiments than to read about them.” The fourth subscale is adoption of scientific attitudes: “Finding out about new
things is unimportant.” Enjoyment of science lessons is the fifth subscale: “Science lessons are a waste of time.” The
sixth subscale is leisure interest in science: “I would like to belong to a science club.” The last subscale is career
interest in science: “Working in a science laboratory would be an interesting way to earn a living.”

Teacher Content Knowledge Covered
A content knowledge covered sheet was given to the teachers asking for the information on the kinds of topics that
teachers covered during their classes and the information on the level of coverage for each science vocabulary. The
ratings are: 1 as not at all mentioned; 2 as mentioned briefly in class; 3 as discussed in class or observed in
homework, and 4 as covered thoroughly in class or through homework.

Student Content Knowledge Pre- and Post-Tests
The pretest and posttest each consisted of 40 items. The items represented the following categories: near transfer, far
transfer, and selected items from standardized testing. In this study, the tests include two forms and contain
respectable reliabilities (A=0.79; B=0.86) on the previous study (Howard, 2004). Tests were administered by
participating teachers. Choice of test form and order of test form administration was left up to each teacher (i.e., AA; A-B; B-A, B-B).

Data Analysis
A one-way ANOVA (analysis of variance) was performed to examine the changes on students’ science attitudes and
learning before and after participating in the live simulation. A Pearson coefficient correlation was then conducted to
examine whether there is a relationship between students’ content knowledge tests, TORSA, and teachers content
knowledge covered.

Results
The one-way ANOVA showed a significant difference on the normality of scientists before and after the live
simulation experience, F (1, 557) =5.00, p<.05. Table 1 presents their mean scores and standard deviations for the
TOSRA and its scales. Students’ perception toward normality of scientists increased from 3.14 to 3.22.
The results showed significant gender differences on adoption of scientific attitudes and career interest in science.
Male students (Mean=3.08, SD=0.42) showed significantly higher adoption of scientific attitudes than did female
students (Mean=3.01, SD=0.41), F (1,550) =4.13, p<.05. Male students (Mean=3.00, SD=0.39) also showed
significant higher interest in science careers than did female students (Mean=2.88, SD=0.37), F (1, 541) =5.10,
p<.05.
Additionally, the results showed that there was a significant difference on the students content knowledge tests, F (1,
510) =8.13, p=0.005. A further analysis showed that there is a significant difference on students content knowledge
among different teachers, F (4, 369) =7.94, p<.001, effect size=0.81, power=.99. Table 2 shows overall students
content knowledge test scores by teachers. Teacher B and F students showed significant higher students test results
136

than Teachers A, C, D, and E after the live simulation. Furthermore, the Pearson test revealed significant correlation
resided between teachers’ content knowledge covered on students’ content knowledge tests and attitudes toward
science. Specifically, teachers who covered most of science vocabulary thoroughly in class or through homework
had major impact on students’ content knowledge and positive attitudes toward science.
Table 1. Descriptive Statistics of Students’ TOSRA Before and After the Live Simulation
0-pre/
1-post
N
Mean
Std. Deviation
Social implications of science
0
279
3.09
0.46
1
287
3.09
0.43
Normality of scientists*
0
277
3.14
0.40
1
282
3.22
0.38
Attitude toward scientific inquiry
0
283
3.20
0.46
1
286
3.18
0.44
Adoption of scientific attitudes
0
279
3.03
0.43
1
288
3.05
0.41
Enjoyment of science lessons
0
275
2.93
0.48
1
284
2.92
0.42
Leisure interest in science
0
270
3.05
0.50
1
281
3.00
0.45
Career interest in science
0
273
2.91
0.38
1
285
2.92
0.38
*p<.05
Table 2. Descriptive Statistics of Students’ Content Knowledge Tests Before and After the Live Simulation by
Teachers
Teachers
0-pre/
N (of Students)
Mean
Std. Deviation
1-post
A
0
21
11.57
4.59
1
19
11.58
2.67
B*
0
14
11.86
3.98
1
12
15.75
4.05
C
0
51
10.14
2.69
1
38
10.61
3.23
D
0
77
10.51
3.05
1
91
13.99
2.51
E
0
22
11.92
3.44
1
24
12.05
4.13
F*
0
24
10.98
3.79
1
21
15.46
2.14
G
0
27
11.49
4.07
1
25
12.36
2.36
*p<.05

Discussion/Implications
The usefulness of technology simulation as a method for learning has been applied sporadically within education.
This study supports the significant role of simulation in transmission of knowledge for educational purposes. We
found that participation in the live simulation may have influenced students’ attitude toward science over time.
Students’ normality of scientists was one of the science-related attitudes that showed significant change. This result
is confirmed by prior research into realistic simulations showing the relevance of science and change in how students
perceive scientists (Jarvis & Pell, 2005). This has significant implication for promoting STEM-related career
awareness. As the nation strives to increase students’ knowledge of STEM-related careers, the live simulation
learning environment has potential for changing students’ self-perception and goal orientation. Such high-tech,
137

computer-assisted cooperative simulation of a real-life situation helped to trigger application learning as well as
professional identity/awareness/interest/construction.
Additionally, we found there is a difference in gender toward science attitudes and interest in science careers. Boys
tended to be more adoptable and receptive to the notion of science attitudes, such as discovering new things, and
more interest in careers toward science. Gender has been characterized as the most significant variable towards
students’ attitude to science (Gardner, 1975). Previous research has shown that boys have a consistently more
positive attitude to school science than girls (Becker, 1989; Breakwell & Beardsell, 1992; Hendley, Stables, &
Stables, 1996; Weinburgh, 1995). With regards to the career choices, Whitehead (1996) discussed gender stereotype
may influence career choices. For example, boys are more likely to choose sex-stereotype careers than girls. Despite
of gender stereotype, the findings of this study showed that the live simulation may have increased boys’ interests in
science or science careers. Therefore, future research should continue to study how the instructional content or
technology-enhanced learning environment will lead to a significant increase in the choices of science-related careers
by girls.
We also recognized that the potential gain depends on the quality of teacher preparation as well as teacher’s
instructional strategy. From this study, we learn that teachers who spent more time on science vocabulary had
impacted students’ content knowledge and attitudes toward science. Such support seems to build up students’
knowledge about the subject matter and support self-confidence so that students can participate effectively and make
real gains in the complex simulated learning environment. The study reaffirms the value of a synergy between
effective teaching practice and use of simulation in optimizing students’ learning of science.

Future Research
Our next step is to extend current findings by conducting an experimental study comparing students who experience
live simulation to those who do not. We also can further our research on the live simulation by investigating what
kinds of teaching strategies (e.g., coaching, inquiry, or mentoring) best align with a simulated learning environment.

Note
An earlier version of this article was presented at the 2008 annual meeting of the American Educational Research
Association in New York. We thank the Challenger Learning Center at Wheeling, WV for their support of data
collection, and teachers and students who participated in the simulation and the study.

References
Becker, B. J. (1989). Gender and science achievement: a re-analysis of studies from two metaanalyses. Journal of Research in
Science Teaching, 26, 141-169.
Bell, P. (2002). Using argument map representations to make thinking visible for individuals and groups. In T. Koschmann, R.
Hall & N. Miyake (Eds.), CSCL 2: Carrying forward the conversation. (Vol. 2, pp. 449-505). Mahwah, NJ: Lawrence Erlbaum
Associates.
Boyle, B., Lamprianou, I., & Boyle, T. (2005). A longitudinal study of teacher change: What makes professional development
effective. School Effectiveness & School Improvement, 16(1), 1-27.
Breakwell, G. M., & Beardsell, S. (1992). Gender, parental and peer influences upon science attitudes and activities. Public
Understanding of Science, 1, 183-197.
Cognition and Technology Group at Vanderbilt (1993). Anchored instruction and situated cognition revisited. Educational
Technology, 52-71.
Darling-Hammond, L. (1997). Doing what matters most: Investing in quality teaching. NY: National Commission on Teaching
and America's Future.
Davis, E. A., & Linn, M. C. (2000). Scaffolding students' knowledge integration: prompts for reflection in KIE. International
Journal of Science Education, 22(8), 819-837.
Fraser, B., J. (1978). Development of a test of science-related attitudes. Science Education, 62(4), 509-515.
138

Gardner, P. L. (1975). Attitudes to science. Studies in Science Education, 2, 1-41.
Haladyna, T., Olsen, R., & Shaughnessy, J. (1982). Relation of student, teacher, and learning environment variables to attitudes
toward science. Science Education, 66(5), 671-687.
Hardre, P., & Chen, C. H. (2005). A case study analysis of the role of instructional design in the development of teaching
expertise. Performance Improvement Quarterly, 18(1), 34-58.
Hendley, D., Stables, S., & Stables, A. (1996). Pupils’ subject preferences at Key Stage 3 in South Wales. Educational Studies, 22,
177-187.
Hidi, S., & Harackiewicz, J. M. (2000). Motivating the academically unmotivated: A critical issue for the twenty-first century.
Review of Educational Research, 70(2), 151-179.
Hsu, Y.-S., & Thomas, R. A. (2002). The impacts of a web-aided instructional simulation on science learning. International
Journal of Science Education, 24(9), 955-979.
Jarvis, T., & Pell, A. (2005). Secondary pupils of different abilities response to an e-Mission simulation of the Montserrat volcanic
eruption. Paper presented at the American Education Research Association, Montreal, CA.
Jonassen, D. H. (2003). Using cognitive tools to represent problems. Journal of Research on Technology in Education, 35(3), 362379.
Jonassen, D. H., & Reeves, T. C. (1996). Learning with technology: using computers as cognitive tools. In D. H. Jonassen (Ed.),
Handbook of research for educational communications and technology (pp. 693-719). New York: Macmillan.
Kim, B., & Reeves, T. C. (2007). Reframing research on learning with technology: In search of the meaning of cognitive tools.
Instructional Science, 35(3), 207-256.
Klopfer, B. J. (1971). Evaluation of learning in science. In B. S. Bloom, J. T. Hastings & G. F. Madaus (Eds.), Handbook on
summative and formative evaluation of student learning. New York: McGraw-Hill.
Lee, V. E., & Burkam, D. T. (1996). Gender differences in middle grade science achievement: Subject domain, ability Level, and
course emphasis. . Science Education, 80(6), 613-650.
Lewis, E., Stern, J., & Linn, M. (1993). The effect of computer simulations on introductory thermodynamics understanding.
Educational Technology, Jan. , 45-58.
Linn, M. C. (1995). Designing computer learning environments for engineering and computer science: The scaffolded knowledge
integration framework. Journal of Science Education and Technology, 4(2), 103-126.
Linn, M. C. (2000). Designing the knowledge integration environment. International Journal of Science Education, 22(8), 781796.
Linn, M. C., Clark, D., & Slotta, J. D. (2003). WISE design for knowledge integration. Science Education, 87(4), 517-538.
Linn, M. C., & Hsi, S. (2000). Computers, teachers, peers: Science learning partners. Mahwah, NJ: Lawrence Erlbaum
Associates.
Moreno, R., & Mayer, R. (2007). Interactive multimodal learning environments. Educational Psychology Review, 19(309-326).
Myers, R. E., & Fouts, J. T. (1992). A cluster analysis of high school science classroom environments and attitude toward science.
Journal of Research in Science Teaching, 29(9), 929-937.
Osborne, J., & Simon, S. (1996). Primary science: past and future directions. Studies in Science Education, 26, 99-147.
Osborne, J., Simon, S., & Collin, S. (2003). Attitudes towards science: a review of the literature and its implications. International
Journal of Science Education, 25(9), 1049-1079.
Papanastasiou, E. C., & Zembylas, M. (2004). Differential effects of science attitudes and science achievement in Australia,
Cyprus, and the USA. International Journal of Science Education, 26(3), 259-280.
Simpson, R. D., & Oliver, J. S. (1990). A summary of major influences on attitude toward science and achievement in science
among adolescent students. Science Education, 74(1), 1-18.
Spinello, E. F., & Fischbach, R. (2004). Problem-based learning in public health instruction: A pilot study of an online simulation
as a problem-based learning approach. Education for Health, 17(3), 365-373.
Weinburgh, M. (1995). Gender differences in student attitudes toward science: a meta-analysis of the literature from 1970 to 1991.
Journal of Research in Science Teaching, 32, 387-398.
Weller, J. M. (2004). Simulation in undergraduate medical education: Bridging the gap between theory and practice. Medical
Education, 38, 32-38.
Whitehead, J. M. (1996). Sex stereotypes, gender identity and subject choice at A level. Educational Research, 38, 147-160.

139

Yeh, Y.-C. (2010). Analyzing Online Behaviors, Roles, and Learning Communities via Online Discussions. Educational
Technology & Society, 13 (1), 140–151.

Analyzing Online Behaviors, Roles, and Learning Communities via Online
Discussions
Yu-Chu Yeh
Institute of Teacher Education; Research Center for Mind, Brain & Learning; Center for Creativity and Innovation
Studies; National Chengchi University, Taipei 116, Taiwan // ycyeh@nccu.edu.tw
ABSTRACT
Online learning communities are an important means of sharing and creating knowledge. Online behaviors and
online roles can reveal how online learning communities function. However, no study has elucidated the
relationships among online behaviors, online roles, and online learning communities. In this study, 32 preservice
teachers participated in an 18-week instruction program. Analyses of online group discussions revealed the
following: (a) of thirteen identified online behaviors, the most common were constructing a positive atmosphere,
providing opinions for group assignments, and providing reminders of assignment-related work; (b) of eight
online roles identified within a group, the most common roles were information providers, opinion providers,
and troublemakers; (c) four online learning communities based on “collaboration” and “participation” were
identified. The evolution of these online learning communities indicates the interrelationships among online
behaviors, roles, and learning communities.

Keyword
Behavior, Learning community, Preservice teacher, Online discussion, Roles

Introduction
The emergence of learning communities is an interesting and recent pedagogical development in higher education.
Various strategies have been developed to foster learning communities in an online setting. The objectives of these
strategies include communicating effectively, strengthening social ties, collaborating in small teams, establishing
social networks, and collaborating in knowledge construction (e.g., Chang, Chen & Li, 2006; Jones & Issroff, 2005;
Wang & Poole, 2004; Yang, Wang, Shen & Han, 2007). Online behaviors and roles that are fundamental to the
functioning of online learning communities, however, have seldom been compared (Yang et al., 2007). Moreover,
although a few studies (e.g., Cho, Gay, Davidson & Ingraffea, 2007; Lin, Lin & Huang, 2007) have attempted to
define online learning community styles or types, none has developed clear criteria for defining online learning
community types from a holistic viewpoint. For instance, Lin et al. (2007) classified the products and processes of
knowledge sharing and creation in a professional virtual community into six types. However, they did not define
these groups according to their characteristics. Although another study by Cho et al. (2007) defined the styles of
online learning communities, it only focused on willingness to communicate in learning communities.
Online learning communities are a collaborative means of achieving “shared creation” and “shared understanding,”
in which mutual exchange between community members are encouraged to support individual and collective
learning (Ludwig-Hardman & Woolley, 2000). Some studies indicate that online learning communities promote
active participation, increase academic achievement, contribute to knowledge creation, and improve learner cognitive
abilities (e.g., Lin et al., 2007; Ludwig-Hardman & Woolley, 2000; Moller, 1998; Waltonen-Moore, Stuart, Newton,
Oswald & Varonis, 2006). However, the question of how these benefits are obtained remains unanswered. Lin et al.
(2007) found that for any group to perform well via an online setting, group members must recognize their functional
roles in knowledge-related activities, and each functional role requires a corresponding behavior in the processes of
knowledge sharing and creation. Therefore, identifying the important online roles and their corresponding behaviors
should elucidate how online learning communities function, what online learning communities can be formed, and
which online learning communities best benefit learners. This information can help teachers improve their e-learning
instruction methods. Briefly, this study examines online roles and corresponding behaviors exhibited in an online
learning community and further, based on these analyses, develops objective criteria for categorizing online learning
communities.

ISSN 1436-4522 (online) and 1176-3647 (print). © International Forum of Educational Technology & Society (IFETS). The authors and the forum jointly retain the
copyright of the articles. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that copies bear the full citation on the first page. Copyrights for components of this work owned by
others than IFETS must be honoured. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior
specific permission and/or a fee. Request permissions from the editors at kinshuk@ieee.org.

140

Online behaviors, roles, and learning communities
Online behaviors and roles
Chang, Cheng, Deng, and Chan (2007) identified the following ten basic elements of structured network learning
societies: participants, shared visions, devices, services, rules, relations, manners, learning domains, learning goals,
and learning activities. They argued that because participants are the lifeblood of online learning groups, identifying
participants by analyzing “roles” is crucial for identifying the interpersonal behaviors of network community
members. Similarly, Yang et al. (2007) determined that learning communities inevitably include learners with similar
approaches and different interests and that each learning behavior reflects learner interests as well as their resource
and information needs. Accordingly, analyzing behaviors and roles in an online learning community is essential for
understanding online learning communities.
According to the Dictionary of Psychology (Corsini, 2002), a role is “the set of behaviors expected of a person
possessing a certain social status” (p. 850). Accordingly, a role is an upper-level concept of a behavior and can
comprise a set of behaviors. To date, few studies have analyzed online roles and behaviors or clarified their
corresponding relationships. In a study of online learner roles, Lin et al. (2007) compared widely varying examples
of inferior and superior consequences of special-interest groups at several group levels. Their analysis of group roles
revealed that inferior group roles comprised information/opinion seekers or givers, encouragers, and followers
whereas superior group roles included initiators, orienters, encouragers, recorders, gatekeepers, information/opinion
seekers or givers, coordinators, and clowns. Lin et al. also found that, for knowledge-creation roles, the inferior
group is primarily comprised of idea providers whereas the superior group consists of task performers followed by
idea providers and integrators. Similarly, Agre (1998) noted the critical importance of “thought leader” for building
trust within a community; thought leaders are individuals who foresee issues, gather positions and arguments,
network with people relevant to the issue, and articulate the issue in a manner that provokes thinking by individual
community members.
In their further analysis of online behaviors or strategies, Lin et al. (2007) classified collaborative strategies into two
categories: task performance and team maintenance. Task performance strategies are related to coordination tasks for
problem solving or goal attainment, such as initiating, seeking information/opinions, providing information/opinions,
coordinating, orienting, evaluating and recording. On the other hand, team maintenance strategies such as
encouraging, gate-keeping, following and clowning build friendly relationships among group members and maintain
team coherence. Lin et al. concluded that each functional role requires a corresponding behavior in knowledgesharing and -creation processes. However, the relationship between roles and behaviors has not been clarified. The
present study therefore attempts to elucidate these relationships.

Indices for online learning communities
Cooperation and motivation to participate are two crucial indices for distinguishing between the achievement of
online groups (Guzdial & Turns, 2000; Lin et al., 2007). Lin et al. (2007) found that while over 50% of participants
in the superior group habitually cooperated, few participants in the inferior group did so. They also indicated that
participants in the superior group were more enthusiastic about sharing knowledge than those in the inferior group.
Similarly, Ligoria (2001) proposed that when communities are organized into groups consisting of members with
different abilities, the overall purpose of the community must be kept in mind along with a sense of collaboration.
According to Ligorio (2001), the collaborative dimension of knowledge building comprises the community of
learners model and the community of practices model. In the learners model, each learner is invited to formulate
problems and hypotheses, search for solutions, share knowledge, explore new fields, learn about new topics, and
adopt new perspectives. In the practices model, learning is a function of an activity, context, culture and social
interaction between people with different competencies. Consequently, peripheral participation is legitimate; that is,
even when not directly participating in an activity, learners can still benefit from observation, analysis, and
discussion of that activity (Ligorio, 2001). Clearly, both the community of learners and the community of practices
models assume the interdependence of participants during cognitive learning (Salomon, 1993; 1998). Therefore,
collaboration is needed to build knowledge in online learning communities.
141

The second index of online learning communities, participation, is considered a general measure of successful online
discussions (Guzdial & Turns, 2000). Notably, although online learning communities have considerable potential for
encouraging students to construct and share knowledge, in most online discussions, only a few key students actively
do so (Chang, Chen & Li, 2006). Moreover, frequent messaging does not constitute a genuine community (Guzdial,
& Turns, 2000). However, from a meaning-making perspective, the content and context of messages are critically
important. In support of this perspective, Havelock (2004) suggested that, although the number and frequency of
connections provide a sense of community activity, they say little about how these interactions impact identity
formation, meaning-making, and the professional practices of participants. To determine which messages are
meaningful, Baym (1998) proposed that message content in a group should contribute to the development of intracommunity trust, and such messages are typically characterized by optimism, excitement, clear task orientation, and
shared leadership duties. In the same vein, Agre (1998) advocated the importance of facilitating a sense of group
trust and participation. Thus, collaboration and meaningful participation are clearly two important indices for
measuring the success of online learning communities. These two indices must also be utilized when categorizing
learning communities.

Research questions
Because this study is exploratory, only research questions rather than hypotheses are proposed. The principal
research questions are as follows.
1. What online behaviors are exhibited during interactive online discussions?
2. What online roles evolve from online behaviors?
3. What are the relationships among online behaviors, online roles, and online learning communities? Specifically,
how do online behaviors and online roles evolve into objective criteria for categorizing online learning
communities?

Group1-Group
discussion
board

The 1st level structure:
curricular content, curricular information, curricular interaction, individual area, and system area.
The 2nd level structure: The structure under curricular interaction
group discussion, thematic discussion, reflection of group learning, video conference setting,
online video conference, and video conference list

Figure 1. An example screen of group discussion board

142

Method
Participants
The study participants were 32 preservice teachers (6 males and 26 females) enrolled in the Instruction in Critical
Thinking class in a teacher-training program for secondary school teachers. Among the participants, 14 (43.75 %)
were undergraduates, and 18 were graduates (56.25 %). Mean subject age was 23.00 years (SD = 2.54 years).

Instruments
The research instrument employed in this study was the e-learning website developed by National Chengchi
University. The e-learning interface consists of three levels. The first level includes the functions of curricular
content, curricular information, curricular interaction, an individual area, and a system area. The instructional design
of this study required that participants complete several group assignments and engage in online discussions.
Consequently, “curricular interaction,” particularly the “Group Discussion Board” under this function, became the
most frequently used interface. Figure 1 presents an example screen of this Group Discussion Board. Since the
interface is written in Mandarin, the main functions of the menu bars are translated.

Procedures and instructional design
An 18-week experimental instruction program based on teaching critical thinking was developed to encourage the
formation and use of online learning communities. To achieve these two goals, collaborative problem-based learning
(PBL) was incorporated into the experimental program. According to Lee and Kim (2005), collaborative PBL is a
method in which learners share a common goal, perform given tasks at the same level, and interact with each other
during problem solving. Accordingly, collaborative PBL, which emphasizes the importance of interactive
discussions, is ideal for analyzing online learning communities. Specifically, the instructional design had two phases:
formation of online learning communities (weeks 1–7) and use of online learning communities (weeks 8–16).
In the first phase, participants were divided into six groups; each participant was allowed to select the group of his or
her own choice. Each group consisted of five to six members. However, one participant in Group 4 dropped out
during the semester; therefore, Group 4 was comprised of only four participants. During the second week, they
started preparing for their group project, which employed collaborative PBL. During this instruction period, the
researcher encouraged the formation of online learning communities by assigning group work on the following
topics: (1) develop test items for five critical-thinking skills—assumption identification, induction, deduction,
interpretation, and argument evaluation; (2) develop a situation-based problem; and (3) apply strategic thinking to
everyday problems.
In the second phase, participants were scaffolded to complete the collaborative PBL assignment via online learning
communities. The primary group tasks in this instructional period were as follows: (1) find an authentic case for
collaborative PBL; (2) define problems in the case; (3) decide roles in the case; (4) develop arguments for each role;
(5) present all arguments and a consensus of solutions via concept maps; and, (6) present arguments for each role and
role-play the problem-solving process.
The researcher assumed that the instructional design would prompt participants to take advantage of the online
discussion board, especially during group discussions. First, participants were from various departments, and each
participant was enrolled in several classes. Thus, face-to-face discussions were difficult to organize. Second,
participants were asked to discuss group assignments online and then upload their files to the e-learning website
during both phases of the study.

Analyses
The Group Discussion Board content was analyzed. As mentioned, an online role can comprise several online
behaviors, and not all online behaviors contribute to the formation and function of an online community (Corsini,
143

2002; Guzdial & Turns, 2000). Restated, among the many online behaviors, only some can be further combined into
online roles that contribute to group trust and participation. Further, identifying online roles is critical for
understanding an online learning community (Agre, 1998; Lin et al., 2007). Consequently, to achieve the goals of
this study, the online behaviors of participants were determined first. These behaviors were then used to determine a
set of online roles likely to influence the formation of an online learning community. When determining the number
of online roles, the researcher took the related findings in the applicable studies (Agre, 1998; Lin et al., 2007) as a
referenced framework and then tried to propose more elaborate categories based on the data obtained in this study.
Finally, based on the analyzed roles, different online learning communities were identified. The online behaviors and
online roles in this study were identified via discussions of the researcher and two trained graduate students.

Results
Analyses of online behaviors
Table 1 lists the frequencies of discussions on the Group Discussion Board. The analytical results indicate that most
discussions were conducted during weeks 8–16 via asynchronous discussions when participants started working on
their PBL projects. The mean number of asynchronous and synchronous discussions for each participant was 36.00
and 4.72, respectively.

Group
G1
G2
G3
G4
G5
G6
Total

N

Table 1. The frequencies and means of online discussions
Asynchronous discussions
Synchronous discussions
Count
M
Count
M
5
110
22.00
0
0
6
202
33.67
25
4.17
6
176
29.33
30
5.00
4
170
42.50
0
0
5
193
38.60
31
6.20
6
301
50.17
65
10.83
32
1152
36.00
151
4.72

Analyses of interactions in the asynchronous and synchronous discussions identified the following 13 online
discussion behaviors.
1.

2.

3.

4.

5.
6.

7.

Providing opinions for group functioning: Such behaviors helped the group function effectively and efficiently.
For example, “We should upload personal assignments before Sunday night to ensure efficient discussion on
Tuesday.”
Providing opinions for group assignments: Such behaviors referred to personal responses to member opinions or
ideas related to group assignments. For example, “The suggested story is good, but it’s kind of hard to discuss a
gang leader.”
Encouraging opinions about/responses to group assignments: Such behaviors were observed when the deadline
was approaching, but no one had posted any opinions about the assignments. These behaviors were also
observed when personal opinions had been posted, but no one had responded to these opinions. For example,
“Everyone posts your opinions on the discussion board; the deadline is Monday.”
Sharing information: Such behaviors were related to the sharing of information obtained from the teacher,
media, magazines, websites or other sources. For example, “I recently read a magazine article that discussed
bullies in schools. Maybe ‘bullies in schools’ can be a topic of our project. What do you think?”
Clarifying concepts: Such behaviors were performed to clarify misconceptions about an issue. For example,
“The test item you proposed is not about ‘assumption identification’; it is about ‘explanation’.”
Constructing a positive atmosphere: Such behaviors included giving encouragement and blessings as well as
expressing gratefulness, caring, and forgiveness. For example, “Two groups have decided to take ‘bird flu’ as
their project topic. You should continue to exercise to help prevent infection of a bird flu.”
Answering questions: Such behaviors occurred when a group member had questions about the assignment or
distributed work and asked for help on the discussion board. For example, “Julie: ‘I don’t know what I should
do.’ Albert: ‘You need to write a learning contract and then upload it to the Web site by Monday.’”
144

8.

9.

10.

11.

12.
13.

Providing reminders of assignment-related work: Such reminders were related to meeting times, assignment
content and progress, and distribution of assignments. For example, “When uploading the assignment, don’t
forget to list the filename as ‘Group 2’.”
Explaining personal problems: Students explaining personal problems typically posted excuses or reasons for
being unable to participate in group discussions or unable to finish assigned work on time. For example, “Sorry,
I was sleepy last night, so I forgot to upload the file.”
Explaining the problems of others: Such behaviors were performed to tell group members why someone could
not participate in group discussions or complete their assignment on time. For example, “Teresa has a class until
1:00 p.m., so, she will come later.”
Solving problems: Such behaviors were performed to work out problems that could hinder group progress. The
most frequently encountered problems were a group member forgetting to upload an assignment or not
distributing an assignment to all group members. For example, “The deadline is coming, but John has not
uploaded his file. I have just uploaded the file by myself.”
Setting schedules: Such behaviors occurred when no group members had proposed a specific time for
discussions. For example, “We should discuss our project topic this Friday.”
Assigning work: Such behaviors included asking group members to be responsible for certain work or asking for
volunteers to complete work. For example, “Is there any volunteer to complete the learning contract?”

These online behaviors were counted (Table 2) to determine their frequency. Of these 13 behaviors, B6 (constructing
a positive atmosphere) was the most frequent, followed by B2 (providing opinions for group assignments) and B8
(providing reminders of assignment-related work). The B10 behavior (explaining the problems of others) was the
least common.
Table 2. The employed 13 online behaviors
Type of online behavior
B1
B2
B3
B4
B5
B6
B7
B8
B9
Count
19
80
16
36
18
90
10
80
48
Note. B1 to B13 represent type 1 to type 13 online behaviors, respectively.

B10
3

B11
8

B12
8

B13
10

Analyses of online roles
Based on the data obtained in this study, the findings in related studies (Agre, 1998; Lin et al., 2007), and the
definition that a role may comprise a set of behaviors (Corsini, 2002), the researcher tried to combine the
aforementioned 13 behaviors into some ‘meaning-making’ roles. Finally, the following eight roles were identified.
1.

2.
3.
4.
5.
6.
7.

8.

Supervisors (R1): This role comprises B1 and B3. This role was the key to good group functioning. Supervisors
gave suggestions about creating high-quality work, requested opinions from group members, set discussion
schedules and assigned work to group members.
Information providers (R2): This role consists of B4. These group members typically provided and shared
information related to assigned work.
Group instructors (R3): This role consists of B5. These group members attempted to clarify misconceptions.
Atmosphere constructors (R4): This role consists of B6. These group members attempted to construct a positive
and harmonious atmosphere of support, caring, and cooperation.
Opinion providers (R5): This role is composed of B2. These group members provided opinions that contributed
to group work.
Reminders (R6): This role is composed of B8. These group members were responsible for reminding others
about discussion times, assignment deadlines, and the details for completing group work.
Trouble-makers (R7): This role is composed of B9. These group members frequently caused problems that
hindered the completion of group work via their absence from group discussions or inability to finish assigned
work on time.
Problem solvers (R8): This role comprises B7, B10, and B11. These group members attempted to answer
questions posed by group members as well as to correct and explain problems caused by group members.

To determine which online roles were employed most frequently, the roles of each participant were counted and the
outstanding roles were analyzed from within-group and across-group perspectives. Table 3 presents the details
concerning role counts for all participants.
145

Participant

R1

G1-1
G1-2
G1-3
G1-4
G1-5
Mean

3
1
1
0
1
1.20

G2-1
G2-2
G2-3
G2-4
G2-5
G2-6
Mean

3
4
1
0
1
2
1.83

G3-1
G3-2
G3-3
G3-4
G3-5
G3-6
Mean

2
0
1
1
4
7
2.50

G4-1
G4-2
G4-3
G4-4
Mean

1
0
2
0
0.75

G5-1
G5-2
G5-3
G5-4
G5-5
Mean

2
3
1
0
1
1.40

G6-1
G6-2
G6-3
G6-4
G6-5
G6-6
Mean

5
0
4
2
0
0
1.83

Total
Mean

53
1.66

Table 3. Counts and means for online roles
Type of online role
R2
R3
R4
R5
Group 1
2
0
1
2
0
0
0
0
1
0
1
0
0
0
0
1
0
0
0
0
0.60
0.00
0.40
0.60
Group 2
1
0
5
4
2
0
9
2
1
0
5
0
0
0
1
0
0
0
1
0
0
0
3
0
0.67
0.00
4.00
1.00
Group 3
0
0
1
3
0
0
0
2
4
0
2
1
2
0
5
1
0
0
1
1
3
0
4
0
1.80
0.00
2.17
1.33
Group 4
1
1
3
0
0
0
1
0
2
2
0
2
0
0
4
1
0.75
0.75
2.00
0.75
Group 5
0
0
6
4
1
2
0
2
2
0
4
4
1
0
6
6
0
1
0
7
0.80
0.60
3.20
4.60
Group 6
4
5
8
8
1
0
3
7
4
4
8
8
1
0
2
4
2
1
5
8
1
1
2
1
2.17
1.83
4.67
6.00
Class
36
17
91
79
1.16
0.53
2.84
2.47

R6

R7

R8

3
1
0
0
0
0.80

1
0
0
0
2
0.60

0
0
0
0
0
0.00

8
16
5
0
0
3
5.33

4
0
5
0
0
0
1.50

3
2
1
0
0
0
1.00

0
3
1
2
1
8
2.50

5
0
3
2
3
0
2.17

0
0
0
0
2
4
1.00

1
1
3
1
1.50

3
1
1
1
1.50

1
0
2
0
0.75

1
3
2
5
1
2.40

0
1
2
1
0
0.80

0
1
2
1
1
1.00

3
0
4
2
1
1
1.83

3
4
0
2
3
1
2.17

1
0
0
0
0
0
0.17

80
2.50

48
1.5

21
0.66

An outstanding role within a group was observed when the count of a participant for a certain role exceeded the
group mean for that role, and an outstanding role across groups was observed when the count of a participant for a
certain role exceeded the class mean for that role. For instance, when analyzed within the group, two participants in
Group 3 (i.e., G3-5 and G3-6) were outstanding for R1 (supervisors). Their total number of participation for this role
were, respectively, 4 and 7, which were higher than the group mean for this role (M = 2.50). However, when
146

analyzed across groups, three participants in Group 3 (i.e., G3-1, G3-5, and G3-6) were outstanding for R1; their
total numbers of participation for this role were 2, 4, and 7, respectively, and these numbers were higher than the
class mean for this role (M = 1.66). Restated, in Group 3, participants G3-5 and G3-6 were supervisors, and, when
examined in the context of the whole class, G3-1, G3-5, and G3-6 were supervisors. Following these calculations,
Table 4 presents the numbers and distributions of outstanding roles for each group.
When analyzed within the group, the roles of information providers, opinion providers, and troublemakers were the
most numerous (15, 14, and 14, respectively), and the number of group instructors was the lowest (Table 4). When
analyzed across groups, the roles of supervisors and troublemakers were the most numerous (both 13), followed by
positive atmosphere constructors, reminders, and problem solvers. Group instructor was the least common.
Table 4. Counts for outstanding roles in each group
Type of online role
Group
R1
R2
R3
R4
R5
Within the group
G1
1
2
0
0
2
G2
3
3
0
3
2
G3
2
3
0
2
2
G4
2
2
2
2
2
G5
2
3
2
3
2
G6
3
2
2
3
4
Total
13
15
6
13
14
Across groups
G1
1
1
0
0
0
G2
3
1
0
1
1
G3
3
3
0
2
1
G4
1
1
2
2
0
G5
2
1
2
3
4
G6
3
3
4
4
5
Total
13
10
8
12
11
Note. R1 to R8 represent type 1 to type 8 online roles, respectively.

R6

R7

R8

0
2
2
1
2
3
10

2
2
3
1
3
3
14

0
3
2
2
4
1
12

1
4
2
1
2
2
12

1
2
4
1
1
4
13

0
3
2
2
4
1
12

Online learning communities
This study attempted to define online learning community types based on two indices: collaboration and
participation. These indices were evaluated based on the aforementioned roles. In terms of collaboration, the number
of roles was counted to represent a group member’s discussion frequency. Specifically, if most group members had
similar discussion frequencies, the group was considered “high collaboration,” whereas if the discussion frequencies
of group members varied significantly, the group was considered “low collaboration.” Based on this central idea, this
study first summed up the roles contributing to collaboration within each group. Means and standard deviations
were then calculated for each group. According to Baym (1998), sense-making message content contributes to the
development of intracommunity trust. Troublemakers are clearly harmful to group collaboration; this role was
therefore eliminated when summing up collaborative roles in each group. Specifically, the sum of cooperative roles
= the sum of all roles − the sum of troublemakers (Table 5).
Table 5. The counts of online roles and collaborative roles
Type of online role
Group
R1
R2
R3
R4
R5
R6
R7
R8
G1
6
3
0
2
3
4
3
0
G2
11
4
0
24
6
32
9
6
G3
15
9
0
13
8
15
13
6
G4
3
3
3
8
3
6
6
3
G5
7
4
3
16
23
12
4
5
G6
11
13
11
28
36
11
13
1
Note. R1 to R8 represent type 1 to type 8 online roles, respectively.

Sum
Roles Collaborative roles
21
18
92
83
79
66
35
29
74
70
124
111
147

Since the total online discussion count varied greatly between groups, directly comparing SDs between groups would
have been inappropriate. Therefore, the coefficient of relative variability (CV) rather than SD was employed to
compare individual differences within a group and further helped determine the degree of collaboration for each
group. Notably, CV represents the ratio of SD to mean (CV = SD*100/M). The analytical results indicated that
Group 1 and Group 2 had comparatively large CVs, 117.19 and 96.49, respectively. Thus, these groups were
regarded as “low collaboration.” The other groups were regarded as “high collaboration.” (Table 6)

Group
G1
G2
G3
G4
G5
G6

n
5
6
6
4
5
6

Table 6. The Ms, SDs, and CVs for collaborative roles
Minimum
Maximum
Total
M
1
11
18
3.60
1
35
83
13.83
5
26
66
11.00
2
13
29
7.25
11
19
70
14.00
6
34
111
18.50

SD
4.22
13.35
7.67
4.57
3.16
11.78

CV
117.19
96.49
69.71
63.08
22.59
63.66

As collaboration was measured according to a within-group perspective, the degree of participation was defined from
an across-group perspective. The rationale for this difference is that a participant may have had high participation in
comparison with other group members but low participation in comparison with the entire class. This typically
occurred when the entire group had low participation. On the other hand, a participant may have had low
participation compared with that of his/her group but high participation compared with that of the entire class; this
generally occurred when the entire group had high participation. Additionally, since being a trouble-maker is a
participation type, this role was included when determining the participation for each group. Restated, the sum of all
roles for each group was considered indicative of its participation. Thus, “high participation” in this study was
defined as a mean role of a group higher than that of the class, and “low participation” was defined as a mean role of
a group lower than that of the class. The analytical results demonstrated that Groups 2, 5, and 6 were classified as
having “high participation”, and Groups 1, 3, and 4 were classified as having “low participation” (Table 7).
Table 7. Group means and class means for online roles
Type of online role
Group
R1
R2
R3
R4
R5
R6
R7
G1
6
3
0
2
3
4
3
G2
11
4
0
24
6
32
9
G3
15
9
0
13
8
15
13
G4
3
3
3
8
3
6
6
G5
7
4
3
16
23
12
4
G6
11
13
11
28
36
11
13
Class
53
36
17
91
79
80
48
Note. R1 to R8 represent type 1 to type 8 online roles, respectively.

R8
0
6
6
3
5
1
21

Total
21
92
79
35
74
124
425

n
5
6
6
4
5
6
32

Mean
4.20
15.33
13.17
8.75
14.80
20.67
13.28

Based on the above analysis, a two (collaboration vs. participation) by two (high vs. low) model was proposed.
Specifically, four online learning communities were identified: active collaboration, passive collaboration,
individualized participation, and indifference (Figure 2). The four online learning community types and their
distributions are as follows.
1.
2.
3.
4.

Active collaboration (high cooperation and high participation): Groups 5 and 6.
Passive collaboration (high cooperation and low participation): Groups 3 and 4.
Individualized participation (low collaboration and high participation): Group 2.
Indifference (low cooperation and low participation): Group 1.

As Table 7 shows, the active collaboration communities were typically high on R4 and R5; the passive collaboration
communities were common in using R4, R6, and R7; the individualized participation community was high on R4 and
R6; and the indifference community was high on R1.

148

Collaboration

High

High

Low

Active

Individualized

collaboration

participation

Participation
Low

Passive
collaboration

Indifference

Figure 2. Types of online learning communities

Discussion
This study examines three questions concerning online behaviors, online roles, online learning communities, and
their interrelationships. The analytical findings in this study indicate that the three questions are satisfactorily
answered. Although this is an exploratory study, it evolves from a pilot study (Yeh, 2005) of 48 preservice teachers
enrolled in the same course a year before this study was conducted. The identification of seven online roles in that
pilot study provides a framework for this study. To generate a more comprehensive list of online roles than what is
presented in the pilot study, this study deliberately starts by analyzing online behaviors. Moreover, to further clarify
the relationships among online behaviors, online roles, and online learning communities, an elaborate instructional
design is utilized and objective analyses based on online discussions are applied. As expected, the high frequencies
of discussions (Table 1) suggest that the instructional design in this study successfully motivates participants to take
advantage of online discussions, especially asynchronous discussions. Such participation is essential for objectively
analyzing online behaviors and roles of participants as well as for learning community types.
Formation of an online learning community depends on the effectiveness of online learning behaviors (Palloff &
Pratt, 1999) and the meaningfulness of exchanged messages (Baym, 1998; Havelock, 2004). Based on these
rationales, this study only considers meaningful messages when analyzing online behaviors. The messages not
focused on the discussed topics or issues and those not representing personal thoughts (e.g., a simple answer, “Yes”)
are screened out. Further, rather than focusing on a specific perspective such as collaborative strategies (Lin et al.,
2007), this study analyzes participant behaviors from a holistic perspective. Accordingly, 13 online behaviors are
identified. The most frequently utilized behaviors are constructing a positive atmosphere, providing opinions for
group assignments, and providing reminders of assignment-related work.
Lin et al. (2007) found that group members recognize their functional roles in knowledge-related activities.
Accordingly, Lin concluded that each functional role requires a corresponding behavior to act during the knowledge
sharing and creation processes. However, Lin et al. did not further analyze the corresponding relationships between
roles and behaviors. Analytical findings in this study provide empirical and descriptive evidence supporting the
conclusions obtained by Lin et al. By further integrating the 13 behaviors, the empirical evidence in this study shows
that 8 important roles exist in online learning communities, and all participants play multiple roles during online
discussions. The analytical results also demonstrate that, although some roles are composed of multiple behaviors,
some comprise only one behavior. Moreover, this study analyzes the outstanding roles from different perspectives.
From the within–group perspective, the most frequently utilized roles are information providers, opinion providers,
and trouble-makers; on the other hand, the most frequently used roles determined using the across-group perspective
are supervisors, trouble-makers, positive atmosphere constructors, reminders, and problem solvers. Among these
roles, trouble-makers clearly hinder the formation and functioning of online learning communities. Unfortunately,
this role typically exists in online learning communities, as the analytical findings in this study suggest. In the
within–group context, “group instructor” is the least common role. This analytical finding is expected. Group
instructors assist in resolving misconceptions and organizing gathered information. Although such a role is critical
for knowledge construction in online settings, not everyone can play this role (Ludwig-Hardman & Woolley, 2000;
Waltonen-Moore et al., 2006). As Chang et al. (2007) suggested, identifying participants by analyzing “roles” is
149

essential for understanding the interpersonal behaviors of network community members. The findings obtained in
this study are valuable for further analyses of online learning communities.
To define online learning community types, this study employs two indices—collaboration and participation—which
have been suggested by many researchers (Agre, 1998; Baym, 1998; Collison, Elbaum, Haavind & Tinker 2000;
Havelock, 2004; Ligoria, 2001; Lin et al., 2007). The index of collaboration is derived from the sum of collaborative
roles (the sum of all roles − the sum of troublemakers) and CVs, while the index of participation is evaluated based
on the mean of total online roles. The analytical results reveal the following four online learning communities: active
collaboration, passive collaboration, individualized participation, and indifference. It is also found that while R4
(atmosphere constructors) is commonly found in active collaboration, passive collaboration, and individualized
participation communities, R5 (opinion providers) seems to be the key role for distinguishing the active collaboration
communities from the other communities. Moreover, R1 (instructors) is exclusively eminent in the indifference
community. More specifically, the behavior of constructing a positive atmosphere is commonly used in the active
collaboration, passive collaboration, and individualized participation communities; the behavior of providing
opinions for group assignment is critical for establishing the active collaboration communities; and the behavior of
providing opinions for group functioning and that for encouraging opinions about/responses to group assignments
are eminent in the indifference community. It is also found in this study that the active collaboration communities
(Group 5 and Group 6) have best performance in the assigned tasks while the indifference community (Group 1) has
the worst performance when evaluated by their final grades. When examining the online roles in Table 7, it is
determined that the active collaboration communities have all the eight types of roles although the frequencies of
these roles are high on R4 (atmosphere constructors) and R5 (opinion providers) and low on R8 (problem solvers).
On the other hand, the indifference community is high on R1 (supervisors) and R6 (reminders), but is missing on R3
(group instructors) and R8 (problem solvers). These findings are in line with findings of Lin et al: ‘encouragers’ exist
in both the inferior and superior group, the superior group consists of a greater variety of roles than the inferior
group, and the superior group habitually cooperates while the inferior group does not. Accordingly, the relationships
among online behaviors, online roles, and types of online learning communities are closely related.
Moreover, the analytical findings in this study suggest that collaborative PBL is a useful tool for exploring online
learning communities when instructional activities are well designed. The finding that most groups have frequent
online discussions also supports the conclusion obtained by Hann, Glowacki-Dudka, and Conceicao-Runlee (2000),
who advocated that cooperative PBL contributes to the formation of online learning communities.

Conclusion and suggestions
To date, no study has clearly identified the important online roles and their corresponding behaviors, nor has a study
defined the online learning community types from a holistic perspective. Moreover, objective indices have not been
proposed for categorizing online learning communities. This study therefore attempts to pioneer an examination of
these areas. To achieve this goal, an 18-week instructional program is employed and the findings are inspiring. The
principal findings are as follows.
First, 13 important online behaviors and 3 commonly used online behaviors (constructing a positive atmosphere,
providing opinions for group assignments, and providing reminders of assignment-related work) are identified.
Second, eight online roles and three common online roles (information providers, opinion providers, and
troublemakers) are identified; moreover, the eight roles and their corresponding relationships with online behaviors
are elucidated. Third, a two (collaboration vs. participation) by two (high vs. low) model is proposed and four online
learning community types (active collaboration, passive collaboration, individualized participation, and indifference)
are recognized. These types of online learning communities should be representative, for they carefully evolve from
a pilot study, an elaborate instructional design, and, most importantly, specific objective criteria based on online
behaviors and online roles. Based on this elaborate evolving process, it is strongly believed that online behaviors,
online roles, and online learning communities are closely related.
To conclude, the analytical results of this study are valuable since the instructional design and analyses in this study
are deliberately constructed and applied; however, the number of online behaviors and online roles may vary with
different discussion content and different participants. Therefore, in addition to replicating the analytical results of
this study in a different context, future studies may compare and contrast online behaviors, roles, and communities
150

across various contexts. Additionally, realizing what online learning community type is most beneficial to learners
would enhance the effectiveness of online learning. Consequently, further study can verify the relationship between
learning effects and the online learning community types identified in this study.

Acknowledgments
The author would like to thank the National Science Council of the Republic of China, Taiwan for
financially/partially supporting this research under Contract No. NSC-94-2520-S-004-001.

References
Agre, P. E. (1998). Designing genres for new media: Social, economic, and political contexts. In S. G. Jones (Ed.), Cybersociety
2.0: Revisiting computer-mediated communication and community (pp. 69-99). Thousand Oaks, CA: Sage.
Baym, N. K. (1998). The emergence of online community. In S. G. Jones (Ed.), Cybersociety 2.0: Revisiting computer-mediated
communication and community (pp. 69-99). Thousand Oaks, CA: Sage.
Chang, B., Cheng, N. H., Deng, Y. C. & Chan, T. W. (2007). Environmental design for a structured network learning society.
Computers & Education, 48, 234-249.
Chang, C. K., Chen, G. D. & Li, L. Y. (2006). Constructing a community of practice to improve coursework activity. Computers
& Education, 50, 235-247.
Cho, H, Gay, G., Davidson, B, & Ingraffea, A. (2007). Social networks, communication styles, and learning performance in a
CSCL community. Computers & Education, 49, 309-329.
Collison, G., Elbaum, B., Haavind, S. & Tinker, R. (2000). Facilitating online learning: Effective strategies for moderators.
Madison, WI: Atwood Publishing.
Corsini, R. (2002). The dictionary of psychology. New York, NY: Brunner-Routledge.
Guzdial, M. & Turns, J. (2000). Effective discussion though a computer-mediated anchored forum. Journal of the Learning
Sciences, 9, 437-469.
Hann, D., Glowacki-Dudka, M. & Conceicao-Runlee, S. (2000). 147 Practical tips for teaching online groups: Essentials of webbased education. Madison, WI: Atwood Publishing.
Havelock, B. (2004). Online Community and Professional Learning in Education: Research-Based Keys to Sustainability. AACE
Journal, 12(1), 56-84.
Jones, A. and Issroff, K. (2005). Learning technologies: Affective and social issues in computer-supported collaborative learning.
Computers & Education, 44, 395-408.
Ligoria, M. B. (2001). Integrating communication formats: Synchronous versus asynchronous and text-based versus visual.
Computers & Education, 37, 103-125.
Lin, F., Lin, S. & Huang, T. (2007). Knowledge sharing and creation in a teachers’ professional virtual community. Computers &
Education, 50, 742-756.
Ludwig-Hardman, S. & Woolley, S. (2000). Online learning communities: Vehicles for collaboration and learning in online
learning environments. Proceedings of World Conference on Educational Multimedia, Hypermedia and Telecommunication, 2000,
1556-1558.
Moller, L. (1998). Designing communities of learners for synchronous distance education. Educational Technology Research and
Development, 46(4), 115-122.
Palloff, R. & Pratt, K. (1999). Building learning communities in cyberspace. San Francisco, CA: Jossey-Bass Publishers.
Salomon, G. (1993). Distributed cognitions: Psychological and educational consideration. Cambridge: Cambridge University
Press.
Salomon, G. (1998). Novel constructivist learning environments and novel technologies: some issues to be concerned with.
Research Dialogues in Learning and Instruction, 1(1), 3-12.
Waltonen-Moore, S., Stuart, D., Newton, E., Oswald, R. & Varonis, E. (2006). From virtual strangers to a cohesive online
learning community: The evolution of online group development in a professional development course. Journal of Technology
and Teacher Education, 14, 287-311.
Wang, M. J. & Poole, M. (2004). Nurturing a dynamic online learning community among teens. The International Journal of
Learning, 9, 859–870.
Yang, F., Wang, M., Shen, R. & Han, P. (2007). Community-organizing agent: An artificial intelligent system for building
learning communities among large numbers of learners. Computers & Education, 49, 131–147.
151

Lee, J., Cerreto, F. A., & Lee, J. (2010). Theory of Planned Behavior and Teachers' Decisions Regarding Use of Educational
Technology. Educational Technology & Society, 13 (1), 152–164.

Theory of Planned Behavior and Teachers’ Decisions Regarding Use of
Educational Technology
Jung Lee, Frank A. Cerreto and Jihyun Lee*
The Richard Stockton College of New Jersey, Pomona, NJ, USA // *College of Education, Seoul National
University, Seoul, Korea // leej@stockton.edu // Frank.Cerreto@stockton.edu // leeji1@snu.ac.kr
ABSTRACT
According to Ajzen’s Theory of Planned Behavior (TPB), behavioral intention (BI) is predicted by attitude
toward the behavior (AB), subjective norm (SN), and perceived behavioral control (PBC). Previous studies
using the TPB to explain teachers’ intentions to use technology have resulted in inconsistent findings. This
inconsistency might be due to overly broad definitions of the target behavior. To investigate this potential
weakness, we defined a specific target behavior, using computers only to create and deliver lessons, and then
used the TPB to investigate teachers’ decisions. An elicitation study was used to identify teachers’ salient beliefs
and develop a closed-ended questionnaire. Results of the closed-ended questionnaire revealed that AB, SN, and
PBC all were significant predictors of teachers’ intentions. However, AB had twice the influence of SN and
three times that of PBC. This finding suggests that teachers must have positive attitudes about using computers
to create and deliver lessons. They are less concerned about what others think of this practice, and far less
bothered by internal or external constraints. Results provide specific information that can be used to design
effective teacher development programs and remind TPB researchers of the importance of using specific
definitions of the target behavior.

Keywords
Theory of Planned Behavior, Behavioral intention, Technology usage, Teacher beliefs

Problem Statement and Theoretical Foundation
It is generally accepted that the use of technology in schools has altered, and continues to transform the educational
landscape dramatically, fueling changes in content, pedagogy, and assessment (US DOE, 2004). In order to
capitalize on the potential benefits of technology in the classroom, governments throughout the world have instituted
initiatives intended to increase its use (e.g., Rha & Yoshida, 2005; US Web-based Education Commission, 2000).
These initiatives generally recognize the need for effective, continuous teacher development programs designed to
help teachers integrate technology into their teaching. However, most recommendations focus mainly on teacher
competence with technology. For example, according to the US Web-based Education Commission (2000), teachers
must be, “able to apply it [technology] appropriately, and conversant with new technological tools, resources, and
approaches (p. 39).”
We will argue that teachers’ competence is only one of several factors determining their decisions regarding the use
of educational technology. Other influences might include the value they attribute to the use of technology.
Regardless of their perceived self competence, teachers may not use technology if they do not value it in their
teaching. Another possible influential factor is the opinions of significant others. If, for example, a teachers’
supervisor strongly promotes the use of technology in the classroom, this teacher might be inclined to please the
supervisor by using technology, despite any perceived personal incompetence or uncertainly of the value.
As a result, designing professional development programs without taking into account other factors limits their
potential impact. Moreover, monetary decisions regarding support for technology initiatives must be based on
consideration of all factors that determine teachers’ decisions to use educational technology. Finally, from a research
perspective, it is important to establish the extent to which empirical findings support intuition or conventional
wisdom. What then are the primary factors that underlie teachers’ intentions to utilize technology in their classrooms,
and what are their relative strengths?
Icek Ajzen’s (1985) Theory of Planned Behavior (TPB), an explanatory model for a wide variety of behavioral
intention, can be used to address this question. According to the TPB, volitional human behavior is immediately
preceded by intention to engage in this behavior (see Figure 1). Behavioral intention is predicted, in turn, by three
main determinants: attitude toward the behavior (AB), subjective norm (SN), and perceived behavioral control
ISSN 1436-4522 (online) and 1176-3647 (print). © International Forum of Educational Technology & Society (IFETS). The authors and the forum jointly retain the
copyright of the articles. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that copies bear the full citation on the first page. Copyrights for components of this work owned by
others than IFETS must be honoured. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior
specific permission and/or a fee. Request permissions from the editors at kinshuk@ieee.org.

152

(PBC). The extent to which individuals view a particular behavior positively (attitude), think that significant others
want them to engage in the behavior (subjective norm), and believe that they are able to perform the behavior
(perceived behavioral control), serve as direct determinants of the strength of their intention to carry out the
behavior.
Each of these three direct determinants of behavioral intention is influenced, in turn, by an indirect determinant.
Indirect determinants are based on a set of salient beliefs and evaluations of these beliefs. Measures of the indirect
determinants embody expectancy-value theory (Fishbein & Ajzen, 1975). This theory posits that attitudes are
developed and revised according to assessments about beliefs and values. This idea was applied to the calculation of
the three indirect determinants of the TPB as follows (Ajzen, 1985):

Figure 1. Theory of Planned Behavior (Adapted from Ajzen (1985))
Salient behavioral beliefs (BB) about the outcomes of a particular behavior, weighted by their outcome evaluations
(oe), form an indirect measure of an individual’s attitude toward the behavior (ABI). Salient normative beliefs (NB)
about whether important others approve of the behavior, weighted by the motivation to comply (mc) with these
perceived norms, constitute an indirect measure of subjective norm (SNI). Salient control beliefs (CB) about
facilitators of or obstacles to performing the behavior, weighted by their control power (cp), comprise an indirect
measure of perceived behavioral control (PBCI). The three indirect measures are given by:

ABI  i (oe) i ( BB) i

SNI   j (mc) j ( NB ) j

PBCI  k (cp) k (CB ) k

The TPB has been used successfully to understand a wide variety of human behaviors, such as weight loss behavior
(Schifter & Ajzen, 1985), and smoking cessation (Godin, Valois, Lepage, & Desharnais, 1992). Results of these
studies have provided specific information used to design effective programs aimed at these behaviors (See Ajzen,
n.d., for a comprehensive list of TPB studies). For example, because Godin et al. (1992) found perceived behavioral
control to be a relatively strong predictor of both intention and behavior, they recommended programs that help
smokers develop their will-power and inform them of the effort required in order to modify smoking behavior.
The TPB has also been used to explain teachers’ intentions and behavior in the classroom (e.g., Crawley, 1990; Zint,
2002). In particular, the TPB has been utilized in predicting K-12 teachers’ intentions toward educational technology
usage (Czerniak, Lumpe, Haney, & Beck, 1999; Salleh & Albion 2004; Sugar, Crawley, & Fine, 2004). Results of
such studies have the potential to help guide approaches to fostering teacher technology use.
Besides the TPB, several other models have been used to predict intentions to use technology, including the Theory
of Reasoned Actions (Fishbein and Ajzen, 1975) and the Technology Acceptance Model (Davis, Bagozzi, and
Warshaw, 1989). The Theory of Reasoned Actions, a precursor to the TPB, includes only two of the three direct
determinants of behavioral intention, attitude toward the behavior and subjective norm. The Technology Acceptance
Model uses perceived usefulness, instead of subjective norm, as the second determinant of behavioral intention. All
three models postulate that behavior is predicted by behavioral intention.

153

Davis, Bagozzi, and Warshaw (1989) compared the relative effectiveness of the Theory of Reasoned Actions and the
Technology Acceptance Model on MBA students’ intention to use a word processing program and their subsequent
usage and found that, while both models predicted behavioral intention well, perceived usefulness was a relative
strong predictor, accounting for more than half of the variance in behavioral intention. Mathieson (1991) compared
Technology Acceptance Model and TPB in predicting undergraduate students’ intention to use an information
system. Both models were found to be effective, and although the Technology Acceptance Model was easier to use,
the TPB provided more specific guidance to developers. Because those who are interested in teachers’ intentions to
use technology are often searching for specific information to guide program development, the TPB is a wise choice.
Unfortunately, attempts to use the TPB to explain teachers’ intentions to use technology have resulted in inconsistent
findings. For example, Czerniak, Lumpe, Haney, and Beck (1999) concluded that subjective norm and perceived
behavioral control were the only two statistically significant predictors of behavioral intention. Sugar, Crawley, and
Fine (2004) found that the only significant predictor of behavioral intention was attitude toward the behavior. Salleh
and Albion (2004) reported that only attitude and subjective norm were significant predictors of intention.
There are many possible explanations for this inconsistency. Our argument is that these conflicting findings may
have resulted from insufficient granularity in the definitions used for the targeted behavior. These three studies
applied the TPB to describe teachers’ beliefs and intentions regarding the integration of electronic technology in
general terms only. They did not take into account the fact that many different technologies exist, and there are many
different ways for teachers to utilize a specific technology in the classroom. For example, Czerniak et al. (1999)
defined the target behavior as using a wide variety of technologies to foster student learning. Similarly, in their study,
Sugar et al. (2004) defined the behavior of interest as, “adopting at least one new technology into a lesson by the end
of the next school year (p. 204).” Salleh and Albion (2004) used the general term, Information and Communication
Technology, to describe the behavior. These definitions allow for many different technologies, ranging from
electronic computers to physical manipulatives, not to mention a broad array of possible uses for each one.
According to Ajzen (2006), when using the TPB, the action comprising the behavior must be defined at an
appropriate level of specificity to allow for useful generalization. Ajzen argued, for example, that using a definition
such as walking on a treadmill as opposed to exercising would yield more useful results because the reasons
individuals decide whether to exercise may depend on the specific form of exercise. Mathieson (1991) also pointed
out that the TPB focuses on “specific beliefs that are specific to each situation (p. 178),” providing specific
information and insight into an individual’s or a group’s predispositions.
In particular, teachers’ attitude, subjective norm, and perceived behavioral control, and the relative importance of
these three factors as predictors of behavioral intention might be very different for different technologies, such as the
use of educational sources on the World Wide Web as opposed to the use of online conferencing systems. Relatively
vague behavioral definitions may explain discrepancies in findings among previous studies of teacher’s intention to
utilize technology.
Moreover, from a practical perspective, many, if not most technology initiatives, focus on particular technological
solutions to particular educational problems. In practice, teachers do not make global decisions about the place of
technology in their classrooms, but rather, they make local decisions about whether or not they will adopt a
particular, often emerging technology. Therefore, in order to be fruitful, any study of teachers’ intentions, including
those that use the TPB, must focus on particular uses of technology.

Purpose and Research Questions
In order to address the aforementioned discrepancy in findings of previous studies, we applied the TPB to investigate
teachers’ intentions to utilize a specific technology in a specific way. By defining the target behavior at an
appropriate level of specificity, we expect to obtain more accurate insight into the factors that influence teachers’
intentions to integrate a particular technological approach into their classrooms.
The primary purpose of this study is to use the TPB to examine the underlying beliefs and the relative strengths of
the three direct determinants (AB, SN, and PBC) of teachers’ intentions to utilize technology in a specific way. For
154

this study, the target behavior is defined as using computers to create and deliver teachers’ lessons by using
presentation software, such as PowerPoint, during the next month.
In particular, we address the following research questions:
1. Underlying Beliefs: What salient teacher beliefs (behavioral, normative, and control) underlie each of the three
indirect determinants of teachers’ intention to use computers to create and deliver lessons?
2. Direct and Indirect Determinants of Intention:
a. Which of the three indirect determinants of intention are statistically significant predictors of their
corresponding direct determinants of teachers’ intentions to use computers to create and deliver
lessons?
b. To what extent does each statistically significant indirect determinant predict its associated direct
determinant of teachers’ intention to use computers to create and deliver lessons?
c. Which of the three direct determinants of intention are statistically significant predictors of teachers’
intentions to use computers to create and deliver lessons?
d. For the statistically significant direct determinants of intention, what are their relative strengths vis-àvis teachers’ intentions to use computers to create and deliver lessons?

Research Method
The Republic of Korea is one of the most technologically advanced societies in the world. According to the 2008
Organization for Economic Cooperation and Development (OECD) report, Korea’s broadband subscription rate is
32%, one of the highest rates among countries (OECD, 2008). Also according to the 2003 Program for International
Student Assessment (PISA), Korea ranked fourth among 40 nations in student access to computers, averaging fewer
than four students per computer (OECD, 2006). With such access, the potential impact of computer technology on
education is vast. Consequently, Korea serves as an ideal setting for examining teachers’ intentions to utilize
technology.
In order to insure construct validity, guidelines for conducting TPB studies given by Ajzen (2006), Ajzen and
Fishbein (1980), and Francis et al. (2004) were followed. First, a preliminary, elicitation study was conducted in
order to identify participants’ salient beliefs regarding the use of presentation software in classroom lessons. The
results of the elicitation study were then used to develop measures of behavioral, normative, and control beliefs that
were then included, along with direct measures of behavioral intention, attitude, subjective norm, and perceived
behavioral control, in the construction of a closed-ended questionnaire.
Because the participants are Korean, all research instruments were administered in Korean. However, following the
procedures described in a manual for developing TPB questionnaires (Francis et al., 2004), the documents were first
written in English and then translated by two of the authors, who are native Korean speakers. A third native Korean
speaker reviewed final drafts. Back-translation validation procedures, as described in Francis et al. (2004), were also
used.

Elicitation Study
Participants
The academic preparation of secondary teachers is the same for middle school and high school teachers, but differs
significantly from that of elementary school teachers. Because this difference may be related to differences regarding
the research questions, and because the TPB is intended for use with homogeneous groups, both the elicitation study
and the questionnaire study focused on secondary teachers alone.
The elicitation study was conducted with 34 middle and high school teachers in the Republic of Korea, in March,
2007, in order to identify teachers’ relevant salient beliefs. These teachers were purposely selected to represent
various subjects, grade levels, teaching experience, and expertise with technology. Table 1 contains participants’
demographic information.
155

Table 1. Elicitation Study Participant Information
Category

Number

Location
Seoul
Busan
Deajeon

26
3
5

Male
Female

16
18

25-29
30-34
35-39
40-44
45-49

6
9
7
6
6

High School (HS)
Middle School (MS)
Vocational HS
Science HS

24
3
3
4

Math
Language
Science
Social Science
Korean
Other

7
5
6
7
4
5

0-4
5-9
10-14
15-19
20-14

4
10
6
5
9

Low
Middle
High

4
22
8

Gender

Age

School Type

Subject

Teaching Experience

Technology Level

Procedure
Participants were asked to write answers to open-ended questions regarding their beliefs about the use of
presentation software to create and present classroom lessons. In order to educe behavioral beliefs, participants were
asked to specify advantages and disadvantages of using presentation software. They were asked to list individuals or
groups who would approve or disapprove their use of presentation software in order to provide data on their
normative beliefs. Finally, in order to elicit control beliefs, participants were asked to enumerate factors or
circumstances that would facilitate or hinder their use of presentation software.

Analysis
In order to answer Research Question 1, two of the authors analyzed the responses independently, grouping similar
responses into categories, labeling the categories, and noting their frequencies. All three authors met to finalize labels
and reach consensus on discrepant cases. Labels that occurred most often were selected for inclusion in the
subsequent closed-ended questionnaire.
156

Closed-ended Questionnaire Study
Instrument
The closed-ended questionnaire was developed following procedures described in Ajzen (2006) and Francis et al.
(2004). In addition to background questions, the questionnaire contained both direct measures of behavioral
intention, attitude, subjective norm, and perceived behavioral control, as well as indirect measures (behavioral
beliefs, normative beliefs, and control beliefs). Standard scaling procedures were used to construct measures. Except
for background questions, all items used a seven-point Likert scale, and items measuring various constructs were
interspersed.
As recommended in Ajzen (2006), in order to improve internal consistency of the direct measures, items were
constructed with the particular behavior and population in mind. Items on the attitude scale included two types: those
that are instrumental (e.g., valuable, beneficial), and those that are experiential (e.g., pleasant, enjoyable). Items on
the perceived behavioral control scale embody capability or controllability of performing the behavior.
As mentioned earlier, results of the elicitation study were used to develop each of the three indirect measures. As
recommended by Ajzen and Fishbein (1980), those beliefs constituting a majority of the beliefs obtained in the
elicitation study were selected for inclusion in the closed-ended questionnaire. Items were constructed for each
identified behavioral belief and its outcome evaluation, each normative believe and the motivation to comply with it,
and each control belief and its power.
Before administering the closed-ended questionnaire, it was piloted with 20 graduate students in the education
department at a large university in Korea, including 10 middle school or high school teachers, and reviewed by two
university faculty members. Based on the pilot study and review, minor changes were made to questionnaire
instructions and a few items, and subsets of scales that exhibited high internal consistency were selected for the final
version.
Reliability of each construct in the final questionnaire was calculated using Cronbach alpha procedures, and all
scales were found to have acceptable internal consistency (alpha > 0.6) based on guidelines provided by Francis et al.
(2004). Table 2 contains reliability data for each of the 10 constructs.
Table 2. Reliability Values from Closed-Ended Questionnaire
Variable
Behavioral Intention (BI)
Attitude Toward the Behavior (AB)
Subjective Norm (SN)
Perceived Behavioral Control (PBC)
Behavioral Beliefs (BB)
Outcome Evaluations (oe)
Normative Beliefs (NB)
Motivation to Comply (mc)
Control Beliefs (CB)
Control Power (cp)

Cronbach’s alpha
0.94
0.71
0.73
0.73
0.77
0.94
0.79
0.70
0.60
0.75

Procedure
Stratified sampling was used to select 11 schools for the questionnaire, based on the relative student population size
in each of several major geographic regions of Korea, and intended to represent urban, suburban, and rural areas.
Based on the number of teachers in each school, a predetermined number of questionnaires was sent via post. At
each school, a senior administrator was asked to distribute questionnaires to teachers and then collect and return all
completed and blank questionnaires via post. A detailed script and instructions for administering the surveys were
provided to the administrators. The script included a description of the purpose of the research study as well as
assurances of confidentiality and safety for participants.
157

Questionnaires were administered to 149 middle school and high school teachers in Korea, in May, 2007. A total of
137 questionnaires were returned, representing a return rate of 91.9 percent. Assuming a moderate effect size for
TPB studies (Francis et al., 2004), a sample size of 137 resulted in acceptable statistical power.
Data Analysis
In order to address Research Question 2, a two-stage regression procedure was used (Francis et al., 2004). After item
analysis was performed in order to establish internal consistency and appropriate diagnostics for linearity, normality
and homoscedasticity were performed, multiple regression was conducted using the direct determinants of attitude,
subjective norm, and perceived behavioral control as predictors of intention. Finally, regression was performed with
each of the indirect determinants and its associated direct determinant. As described earlier, the sum of the products
of each behavioral belief and its outcome evaluation was used as the predictor of attitude toward the behavior, and
similarly for subjective norm and perceived behavioral control. SPSS Version 12.0 for Windows was used to
compute all statistics for this report.

Results
Underlying Beliefs: Research Question #1
Middle and high school teachers who participated in the elicitation study expressed a variety of behavioral,
normative, and control beliefs regarding the use of computers to create and deliver lessons. Table 3 contains a
summary of the most commonly held beliefs.
Teachers’ behavioral beliefs regarding the use of computers to create and deliver lessons gravitate toward two areas:
better teaching, and improved student behaviors. Reactions were generally positive, with perceived advantages of
using computers outweighing disadvantages. Following are a few samples of specific teacher comments about
attitudes and, in parentheses, the categories to which they were assigned in Table 3:
“Because of reducing writing time on the blackboard, I can give more detailed explanations.” (quality of
teaching)
“When presenting computer graphics, it could deliver wrong information. For example, in the case of
y=x, if the graphic angle is not exactly 45 degree on PPT, students may get the wrong information.”
(student achievement)
“Visualized computer-based presentations make students pay attention to teacher’s instruction.”
(student attention)
The most important others whose opinions teachers consider include school administrators, students and their
parents. Teachers who cited school administrators generally reported that these administrators support the use of
computers to create and deliver lessons. However, those who cited students or their parents had conflicted views
reporting that, for example, students who are interested in computers encourage their use to create and deliver
lessons, but those who are not discourage it.
In order to use computers effectively to create and deliver lessons, teachers cite the importance of both external
(reliable hardware and software) and internal (skills) factors. For both types of factors, some teachers reported
inhibitory effects, whereas in others reported enabling effects.

Behavioral Beliefs
 quality of teaching
 student achievement
 student attention

Table 3. Summary of Salient Beliefs from Elicitation Study
Normative Beliefs
Control Beliefs
 reliable hardware and software
 school administrators
 skills
 students
 training and support
 students’ parents
 time to create
158

Direct and Indirect Determinants of Intention: Research Question #2
Summary statistics for the seven main constructs measured in the closed-ended questionnaire are provided in Table
4. For each of the four direct measures (BI, AB, SN, PBC), the theoretical minimum and maximum scores are -21
and 21, respectively. For the each of the three indirect measures (ABI, SNI, PBCI), the theoretical minimum and
maximum scores are -63 to 63, respectively. All intercorrelations are considerably less than 1, indicating that
discriminant validity was achieved. We also considered the assumption for TPB models that predictive factors in the
model are correlated. As shown in Table 4, pairwise correlations among all predictors in the model were statistically
significant at the 0.05 level.
Table 4. Descriptive Statistics and Correlations (N= 137)
ABI
AB
SNI
SN
PBCI
PBC
Indirect Attitude toward
-the Behavior (ABI)
Direct Attitude toward
-.534**
-the Behavior (AB)
Indirect Subjective
-.360*
.443**
Norm (SNI)
Direct Subjective Norm
-.240**
.657**
(SN)
Indirect Perceived
Behavioral Control
.474** -.708**
(PBCI)
Direct Perceived
Behavioral Control
-.402**
.583**
(PBC)
Behavioral Intention
-.434**
.799**
(BI)
Note. *p < .05, ** p < .01, ***p < .001.

BI

-.662**

--

-.330**

-.612**

--

.302**

.558**

-.687**

--

.479**

.661**

-.713**

.604**

--

Mean

SD

27.33

15.735

14.56

3.410

-1.12

17.852

12.68

3.306

19.33

18.454

15.17

3.392

13.21

4.752

All direct measures in the model are positive. Teachers in the study intended to use computers to create and deliver
lessons, they had positive attitudes about its use, significant others endorse this use, and teachers expressed
confidence they had the necessary capability.
The situation with regard to indirect measures is less consistent. Teachers in the study exhibited positive beliefs
about the outcomes associated with using computers to create and deliver lessons. In addition, they expressed the
belief that they possessed the internal and external resources needed to do so. However, teachers reported that
opinions of others regarding their use of computers were neutral, overall.
Data diagnostics were conducted in order to ascertain whether assumptions underlying the validity of conclusions
based on the regression analysis were met. A preliminary examination of histograms and normality plots suggested
that all seven variables were normally distributed. Subsequent analyses were conducted using the KolmogorovSmirnov test, with the Lilliefors correction (Lilliefors, 1967) and the Jarque-Bera test (Jarque and Bera, 1987). The
results of these tests confirmed that none of the variables differs from normality at the 0.05 significance level (Table
5).
Table 5. Normality Tests
Variables
ATTI
SNI
PBCI
ATTD
SND
PBCD
BI

Kolmogorov-Smirnov Statistic
0.074
0.074
0.053
0.076
0.075
0.072
0.076

Jarque-Bera
Statistic
1.283
1.171
1.720
0.835
1.031
1.676
4.262
159

An examination of scatter plots provided strong evidence of linearity and multivariate normality. Ramsey’s RESET
test (Ramsey, 1969) provided formal support for the assumption of linearity and the specification of the models with
all results failing to reject the null hypothesis at the 0.05 significance level (Tables 6-9). A scatterplot of the
standardized residuals versus the predicted values from the regression analysis confirmed the assumption of
homogeneity of variance-covariance. Formal tests of heteroscedasticity, using White’s test (White, 1980) with the
number of predictors as the degrees of freedom, were conducted. All results failed to reject the null hypothesis at the
0.05 significance level, and, therefore, supported the assumption of homoscedasticity for all regressions (Tables 6-9).
Tables 6-8 contain the results when each direct determinant was regressed on its indirect counterpart. The indirect
determinant of attitude toward the behavior (ABI) was a significant predictor of the direct determinant (AB),
F(1,135) = 48.610, p < 0.001, and accounted for 26.5 percent of its variance. The indirect determinant of subjective
norm (SNI) had a significant influence on the direct determinant (SN), F(1, 135) = 113.017, p < 0.001, and
accounted for 45.6 percent of its variance. The indirect determinant of perceived behavioral control (PBCI) was a
significant predictor of the direct determinant (PBC), F(1,135) = 114.281, p < 0.001, and accounted for 45.8 percent
of its variance.
Regarding Research Question 2a and b, each of the three indirect determinants of the theory constructs was
significantly and strongly related to its corresponding direct determinant, further supporting the model, and providing
additional support for the measures’ validity.
Table 6. Regression Analysis: Predicting Attitude Toward the Behavior, AB (N = 137)
R2

S.E.
Indirect Attitude toward
0.265
2.935
the Behavior (ABI)
Note. *p < .05, ** p < .01, ***p < .001.

F

B

48.610***

0.112



S. E. B
0.16

White’s
Statistic

0.515

1.781

Ramsey’s
RESET
Statistic
1.586

Table 7. Regression Analysis: Predicting Subjective Norm, SN (N = 137)
R2

S.E.
F
Indirect Subjective
0.456
2.448
113.017***
Norm (SNI)
Note. *p < .05, ** p < .01, ***p < .001.

B

S. E. B



White’s
Statistic

0.125

0.012

0.675

3.699

Ramsey’s
RESET
Statistic
1.724

Table 8. Regression Analysis: Predicting Perceived Behavioral Control, PBC (N = 137)

F

B

S. E. B



White’s
Statistic

114.281***

0.124

0.012

0.677

5.617

R2

S.E.
Indirect Perceived
Behavioral
0.458
2.505
Control (PBCI)
Note. *p < .05, ** p < .01, ***p < .001.

Ramsey’s
RESET
Statistic
0.123

Table 9. Regression Analysis: Predicting Behavioral Intention (N=137)
R2
0.700

S.E.
Model
2.630
Attitude toward the
Behavior (AB)
Subjective Norm (SN)
Perceived Behavioral
Control (PBC)
Note. *p < .05, ** p < .01, ***p < .001.

F
103.644***

B

S. E. B



0.793

0.094

0.569

.0329

0.096

0.229

0.201

0.084

0.144

White’s
Statistic
16.577

Ramsey’s
RESET
Statistic
0.211

160

Table 9 contains the results of behavioral intention regressed on the three direct predictors in the model, which show
that at least one of the direct determinants influenced behavioral intention, F(3,133) = 103.644, p < 0.001. In
response to Research Question 2c, the analysis revealed that all three direct determinants – attitude toward the
behavior, t(133) = 8.481, p < 0.001, subjective norm, t(133) = 3.446, p = 0.001, and perceived behavioral control,
t(133) = 2.386, p < 0.05 – were statistically significant predictors of teachers’ intentions to use computers to create
and deliver lessons. Together, the three determinants accounted for 70 percent of the variance in teachers’ intentions.
This finding is in contrast to those of Czerniak et al. (1999), Sugar et al. (2004), and Salleh and Albion (2004). In
none of those studies were all three direct determinants found to be significant predictors of behavioral intention.
Figure 2 represents the pathways, including beta values, found in the regression analysis. The betas from regression
model were used to determine the relative weights of each factor.
Regarding Research Question 2d, of the three direct determinants, attitude toward the behavior had the most
substantial impact ( = 0.569) on teachers’ intentions to use computers to create and deliver lessons, producing a
change of 0.569 units in behavioral intention for each unit change in attitude. This influence on intention is more
than twice that of subjective norm ( = 0.229) and more than three times that of perceived behavioral control ( =
0.144). This finding suggests that teachers’ decisions about using computers to create and deliver lessons are
influenced strongly by their view of its value, moderately by the opinions of significant others, and weakly by
teachers’ perceived ability to do so.

Figure 2. Path diagram of TPB model of teachers’ use of computers to create and deliver lessons

Discussion
Regarding Research Question 1, underlying salient beliefs, it is interesting to compare and contrast the results of this
study and those of Czerniak et al. (1999), Sugar et al. (2004), and Salleh and Albion (2004). All of the modal salient
beliefs in attitude, subjective norm, and perceived behavioral control identified in this study were also present in
Czerniak et al. (1999)and all but one in Sugar et al. (2004) (Salleh and Albion (2004) did not report salient beliefs.).
However, the first two studies identified several additional common beliefs. For example, under attitudes, both
Czerniak et al. (1999) and Sugar et al. (2004) reported preparing students for the future and helping them to acquire
new skills as common responses. These additional salient beliefs are undoubtedly an artifact of the relatively general
definition of the target behavior used in the other studies. Given that the current study focused specifically on
teachers’ use of computers to create and deliver lessons, it is not surprising that these other beliefs were not
observed.
Regarding Research Question 2, indirect and direct predictors of intention, recall the disagreement as to which
factors serve as significant predictors of teachers’ intention to use technology. Czerniak et al. (1999) found that SN
and PBC were the only two significant predictors of teachers’ intentions to use technology. Sugar et al. (2004)
identified AB as the only significant predictor of this behavior. Salleh and Albion (2004) found that only AB and SN
161

were significant predictors of the behavior. The main finding of this study, embodied in Research Question 2c,
provides a possible resolution to this paradox, by demonstrating that AB, SN, and PBC all served as significant
antecedents to teachers’ intentions to use computers to create and deliver lessons. This finding demonstrated our
claim that providing a clear and specific definition of the target behavior, as opposed to general definitions, could
lead to more meaningful conclusions that are consistent with the TPB.
Once the significance of all three direct determinants was established, we were able to examine their relative
strengths. In response to Research Question 2d, AB had more than twice the influence of SN and more than three
times the influence of PBC on teachers’ intentions to use computers to create and deliver lessons. This finding
suggests that teachers must believe positive educational outcomes will follow in order for them to intend to use
computers to create and deliver lessons. They are less concerned about what others think of this practice, and far less
bothered by any internal or external constraints that may exist.

Conclusion
This study has both theoretical and practical importance. With regard to the TPB, we refined the application of a
widely used social psychological theory by reemphasizing the importance of providing specific definitions of the
target behavior.
Findings provide practical information to two groups of individuals interested in the effective integration of
computer technology in the classrooms. First, these findings give specific guidance to individuals who design and
implement technology initiatives. In particular, findings from the elicitation study lead to specific recommendations
for developers of teacher development programs. For example, because several teachers expressed concern that
using computers to create and deliver lessons requires too much time, designers of teacher development programs
should emphasize methods to improve efficiency.
Second, the findings will aid decision makers in determining where resources should be targeted in order to optimize
their allocation. Attitude toward the behavior was found to have much greater influence on teachers’ intentions to use
computers to create and deliver lessons than either subjective norm or perceived behavioral control. According to
these findings, teachers base their decisions primarily on their evaluation of the potential benefits, with less regard
for the opinions of others and little concern over internal and external resources. Therefore, resources directed
toward teacher development programs should be allocated accordingly.
The concern for internal consistency for direct measures discussed earlier does not apply to indirect measures in the
TPB because individuals can (and often do) hold both positive and negative beliefs about any particular behavior.
Therefore, alternate measures of reliability, such as test-retest studies, are recommended for indirect measures
(Ajzen, 2006; Francis et al., 2004). Unfortunately, participant access constraints did not allow for completion of a
test-retest study of indirect (belief-based) measures in this study. As an alternative, reliability analysis was
conducted, with satisfactory Cronbach alpha values obtained for all three indirect measures, as was the case with the
direct measures. Although this approach provides strong evidence of reliability for the direct measures, it gives
weaker reliability support for indirect measures and thus may limit this study’s conclusions.
Our primary interest in this study is in the direct and indirect factors determining teachers’ intentions to utilize
technology. The ultimate goal of many, of course, is that teachers will actually use technology effectively in their
classrooms. However, for reasons outlined earlier, unless teachers “buy into” the idea, efforts to bring technology
into the schools will have limited effectiveness. Our decision to focus on behavioral intention and not the behavior
itself rests on solid theoretical and empirical ground. Like many other models of behavior, The TPB postulates that
behavioral intention is the immediate antecedent of volitional behavior. Empirical studies have validated the strength
of this intention-behavior link in the TPB model (e.g., Ajzen & Madden, 1986) as well as in other models of
behavior (Davis et al., 1989; Sheppard, Hartwick, & Warshaw, 1988).
This study demonstrated that precise definitions must be used in order to determine the predictors of teachers’
intentions to use technology in specific ways. A logical next step would be to replicate study with other specific uses
of technology in order to ascertain what differences exist among them when the TPB is used as an explanatory
model. The authors have completed a study comparing and contrasting the results among three different uses of
162

technology that reveals significant differences with respect to significance and influence of the three direct
determinants of teachers’ intentions to use different forms of educational technology (Lee, Cerreto, & Lee, in press).
Finally, in order to establish the generalizabilty of the results, the study should be replicated in other geographic
locations and with elementary school teachers. Findings of these follow-up studies would help us to identify which
findings can be applied to which populations.

References
Ajzen, I. (1985). From intentions to action: A theory of planned behavior. In J. Kuhl & J. Beckman (Eds.) Action-control: From
cognition to behavior, Heidelberg: Springer, 11-39.
Ajzen, I. (2006). Constructing a TpB questionnaire: Conceptual and methodological considerations, Retrieved March 22, 2007,
from http://www.people.mass.edu/aizen/tpb.html.
Ajzen, I. (n.d.). The Theory of Planned
http://people.umass.edu/aizen/tpbrefs.html.

Behavior:

A

Bibliography.

Retrieved

June

23,

2007,

from

Ajzen, I., & Fishbein, M. (1980). Understanding attitudes and predicting social behavior. Englewood Cliffs, NJ: Prentice-Hall.
Ajzen, I & Madden, T. J. (1986). Prediction of goal-directed behavior: Attitudes, intentions and perceived behavioral control.
Journal of Experimental Social Psychology, 22(5), 453-474.
Crawley, F. E. (1990). Intentions of science teachers to use investigative teaching methods: A test of the theory of planned
behavior. Journal of Research in Science Teaching. 27(7), 623-716.
Czerniak, C.M., Lumpe, A. T., Haney, J.J., & Beck, J. (1999). Teachers’ beliefs about using educational technology in the science
classroom. International Journal of Educational Technology, 1(2), Retrieved February 12, 2007, from
http://www.outreach.uiuc.edu/ijet.
Davis, F., Bagozzi, P., & Warshaw, P. (1989). User acceptance of computer technology: A comparison of two theoretical models.
Management Science, 35(8), 982-1003.
Fishbein, M., & Ajzen, I. (1975). Belief, attitude, intention, and behavior: An introduction to theory and research. Reading, MA:
Addison-Wesley.
Francis, J. J., Eccles, M. P., Johnston, M., Walker, A., Grimshaw, J., Foy, R., et al. (2004). Constructing questionnaires based on
the theory of planned behavior: A manual for health services researchers. Centre for Health Services Research: Newcastle Upon
Tyne, United Kingdom. Retrieved January 25, 2007, from http://www.rebeqi.org/ViewFile.aspx?itemID=212
Godin G., Valois P., Lepage L., & Desharnais R.(1992). Predictors of smoking behaviour: an application of Ajzen's theory of
planned behaviour. British Journal of Addiction, 87(9), 1335-1343.
Jarque, C. M., and Bera, A. K. (1987). A test for normality of observations and regression residuals. International Statistical
Review, 55(2), 163-172.
Lee, J., Cerreto, F. A., & Lee, J (in press). Teachers’ intentions toward technology usage: Do different uses lead to different
determinants? In C. Maddux (Ed.), Research Highlights in Information Technology and Teacher Education 2009. Chesapeake,
VA: Society for Information Technology and Teacher Education
Lilliefors, H. W. (1967). On the Kolmogorov-Smirnov tests for normality with mean and variance unknown. Journal of the
American Statistical Association, 62(318), 399-402.
Mathieson, K. (1991). Predicting user intentions: Comparing the technology acceptance model with the theory of planned
behavior. Information Systems Research, 2(3), 173-191.
OECD (2006). Are students ready for a technology-rich world?: What PISA studies tell us. OECD, Paris. Retrieved June 1, 2009,
from http://www.oecd.org/dataoecd/28/4/35995145.pdf
OECD (2009). Broadband subscribers per
http://www.oecd.org/dataoecd/21/35/39574709.xls

100

inhabitants

(Dec.

2008).

Retrieved

June

1,

2009,

from

Ramsey, J. B. (1969). Test for specification error in classical linear least squares regression analysis. Journal of the Royal
Statistical Society, Series B (Methodological), 31(2), 350-371.
Rha, I. & Yoshida, A. (2005). A comparative study on ICT policy in education in Korea and Japan. Educational Technology
International, 6(1), 3-39.
163

Salleh, S. & Albion, P. (2004). Using the theory of planned behaviour to predict Bruneian teachers' intentions to use ICT in
teaching. In C. Crawford et al. (Eds.), Proceedings of Society for Information Technology and Teacher Education International
Conference 2004 (pp. 1389-1396). Chesapeake, VA: Association for the Advancement of Computing in Education.
Schifter, D. E. & Ajzen, I. (1985). Intention, perceived control, and weight loss: An application of the theory of planned behavior.
Journal of Personality and Social Psychology, 49(3), 843-851.
Sheppard, B. H., Hartwick J., & Warshaw, P.R. (1988). The theory of reasoned action: A meta-analysis of past research with
recommendations for modifications and future research. Journal of Consumer Research. 15(3), 325-343.
Sugar, W., Crawley, F., & Fine, B. (2004). Examining teachers’ decisions to adopt new technology. Educational Technology and
Society, 7(4), 201-213.
US Department of Education (2004). Toward a new golden age in American education--How the Internet, the law and today's
students
are
revolutionizing
expectations.
Retrieved
September
21,
2007,
from
http://www.ed.gov/about/offices/list/os/technology/plan/2004/index.html.
US Web-based Education Commission (2000). The power of the Internet for learning: Final report of Web-Based Education
Commission. Retrieved April 1, 2007, from http://www.ed.gov/offices/AC/WBEC/FinalReport/index.html.
White, H. (1980). A heteroskedasticity-consistent covariance matrix estimator and a direct test for heteroskedasticity.
Econometrica, 48(4), 817–838.
Zint, M. (2002). Comparing three attitude-behavior theories for predicting science teachers' intentions. Journal of Research in
Science Teaching, 39(9), 819-844.

164

Almekhlafi, A. G., & Almeqdadi, F. A. (2010). Teachers' Perceptions of Technology Integration in the United Arab Emirates
School Classrooms. Educational Technology & Society, 13 (1), 165–175.

Teachers’ Perceptions of Technology Integration in the United Arab Emirates
School Classrooms
Abdurrahman Ghaleb Almekhlafi and Farouq Ahmad Almeqdadi*
College of Education, P.O Box 17551, Al-Ain, UAE // almekhlafi@uaeu.ac.ae
Emirates College for Advanced Education, P. O. Box 126662 Abu Dhabi, UAE // falmeqdadi@ecae.ac.ae

*

ABSTRACT
Technology is a growing part of any society today. Educational technology has become a cornerstone for any
country’s efforts to improve students’ performance at K-12 schools. It has become the focus of educators
worldwide. However, research studies investigating technology integration, particularly at the United Arab
Emirates (UAE) K-12 schools, focus on quantitative data collection methodology. This study investigated
technology integration at UAE Model schools using a mixed method of data collection consisting of focus group
interviews and a questionnaire. Study sample consisted of 40 female and 60 male teachers from two schools in
Al-Ain Educational Zone, Abu Dhabi. Study results showed that teachers at both schools are integrating
technology in their classes’ activities. They use a variety of technologies to promote students’ learning.
However, methods of integration by male teachers differed in some cases compared to their female colleagues.
Implications for technology integration in the UAE context are discussed.

Keywords
Technology Integration, Teachers’ Perceptions, UAE Schools

Introduction
Technology integration in the classroom has become an important aspect of successful teaching. It has triggered
many researchers to investigate different aspects of such integration (e.g., Kotrlik & Redmann, 2005; Bauer and
Kenton, 2005; Judson, 2006; Totter et al., 2006; ChanLin et al., 2006; Zhao, 2007; Gulbahar, 2007; Anderson and
Maninger, 2007; Abbit and Klett, 2007; & Wood and Ashfield, 2008). This is because it allows students to learn
more in less time and allows schools to focus on global learning environments if used appropriately. In addition, it
could be an effective teaching tool when used to engage all students in the learning process (Almekhlafi, 2006a,
2006b).
Research shows that there are increasing number of computers being used at home and an increasing number of
technological devices available to schools (Goddard, 2002). Research documented teachers’ use of computers for
different purposes and objectives (e.g., Guha, 2000; Yildirim, 2000; & Rowand, 2000). Some teachers use computers
for instructional purposes while others use them for both personal and instructional goals. This study investigates
teachers’ perceptions of tilizing of computers and other technologies for teaching and learning.

Literature Review
Technology use in education is becoming an increasingly important part of higher and professional education
(Wernet, Olliges, & Delicath, 2000; & Almekhlafi, 2006a, 2006b). Technology not only gives learners the
opportunity to control their own learning process, but also provides them with ready access to a vast amount of
information over which the teacher has no control (Lam & Lawrence, 2002).
According to Rowand (2000), a survey based on a National Center for Education Statistics (NCES, 2000), found
that 39% of teachers indicated that they used computers or the Internet to create instructional materials, 34% for
administrative record keeping, less than 10% reported to access model lesson plans or to access research and best
practices. Novice teachers were more likely to use computers or the Internet. Similarly and according to a report
released by the U. S. Department of Education, NCES (2000), novice teachers were more likely to use computers or
the Internet to accomplish various teaching objectives. Teachers with at most nine years of teaching experience were
more likely compared teachers with 20 or more years of experience to report using computers or the Internet to
communicate with colleagues.

ISSN 1436-4522 (online) and 1176-3647 (print). © International Forum of Educational Technology & Society (IFETS). The authors and the forum jointly retain the
copyright of the articles. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that copies bear the full citation on the first page. Copyrights for components of this work owned by
others than IFETS must be honoured. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior
specific permission and/or a fee. Request permissions from the editors at kinshuk@ieee.org.

165

Because technology integration is a very broad concept and has several aspects and implications, researchers
categorized the previous studies into four different categories:
(1) Technology Integration and its Impact on Students and Teachers
A number of researchers have explored technology integration projects worldwide and reported positive impact on
teaching and learning for teachers using technology (e.g., Holinga, 1999; Guha, 2000; Sandholtz, 2001; Manzo,
2001; Sherry et al., 2001; Hong and Koh, 2002; Zorfass and Rivero, 2005, & Almekhlafi, 2006a, 2006b). For
example, Guha (2000) reported significant differences and positive correlations between teachers' present computer
training, level of comfort, and computer usage in the classroom as compared to their previous training, comfort level,
and usage.
Manzo’s (2001) study found that many of the students who are drawn to Electronic Arts Class were struggling in
most of their other classes. Once they saw what they could do with technology, they began to appreciate the
importance of doing well in all subjects.
Similarly, Sherry et al. (2001) studied the WEB Project. Their findings
of a survey assessing the grant’s impact on student achievement suggest that teachers should emphasize the use of
meta-cognitive skills, application of skills, and inquiry of learning as they infuse technology into their academic
content areas.
(2) Factors Influencing Teachers’ Technology Integration in the Classroom
Technology integration at schools and factors affecting such integration has drawn the attention of many researchers
and has been of high interest to them. A number of studies and projects have been conducted to explore teachers’ use
of technology and factors hindering such use (e.g., Becker and Ravitz, 2001; Redmann and Kotrlik, 2004; Kotrlik
and Redmann, 2005; Bauer and Kenton, 2005; Judson, 2006; Totter et al., 2006; ChanLin et al., 2006; Zhao, 2007;
Gulbahar, 2007; & Anderson and Maninger, 2007).
Bauer and Kenton (2005) found that teachers, who were highly educated and skilled with technology, were
innovative and adept at overcoming obstacles, but they did not integrate technology on a consistent basis both as a
teaching and learning tool. Results suggest that schools have not yet achieved true technology integration. Gulbahar
(2007) concluded that teachers and administrative staff felt themselves competent in using ICT available at the
school; they reported a lack of guidelines that would lead them to successful integration. On the other hand, students
reported that ICT is not utilized sufficiently in their classes.
Zhao (2007) conducted a qualitative research to investigate the perspectives and experiences of 17 social studies
teachers following technology integration training. The research indicated that teachers held a variety of views
towards technology integration. These views influenced their use of technology in the classroom. Most teachers were
willing to use technology, expressed positive experiences with technology integration training, increased their use of
technology in the classroom, and used technology more creatively.
On the other hand, numerous studies have been carried out to identify factors facilitating or prohibiting technology
integration in the classroom, particularly computers. Some studies focus on the availability of computers in the
classroom, sharing of resources, a supportive administration, and a strong support staff as the primary influencing
factors. As an example, the Becker and Ravitz (2001) study showed that computer use among teachers is related to
more constructivist views and practices and to changes in practice in a more constructivist-compatible direction. In
addition, other research studies suggest that there is a relationship between a teacher’s student-centered beliefs about
instruction and the nature of teacher’s technology-integrated experiences (Judson, 2006; & Totter et al., 2006).
Similarly, ChanLin et al. (2006) conducted a study to identify the factors affecting eight teachers’ use of technology
in creative teaching practicies. The identified factors were classified into four categories: environmental, personal,
social and curricular issues. Besides Chanlin's study, Anderson and Maninger (2007) investigated the changes in and
factors related to students' technology-related abilities, beliefs, and intentions. Statistically significant changes were
found in students' perceived abilities, self-efficacy beliefs, value beliefs, and intentions to use software in their future
classrooms. Students' self-efficacy, value beliefs, and intentions were moderately correlated with each other.
Abilities were correlated with self-efficacy and computer access. The best predictors of intentions were self-efficacy
beliefs, gender, and value beliefs.
166

(3) Teachers' Perceptions of Technology Integration and Gender Differences
Teachers’ perspectives of their use of instructional technology, understanding of this technology, and feelings about
the support structure associated with this equipment have been examined with the findings suggesting that teachers
believe technology is an integral part of the process of educating their students. Pertaining to gender differences in
technology integration, the literature showed that there were some differences between male and female teachers in
technology use, while other studies did not (e.g., Shashaani, 1997; Bhargava et al., 1999; and Hong & Koh, 2002).
The results of Shashaani's study (1997) showed that female students were less interested in computers and less
confident than male students. The results also showed that males were more experienced than females and females'
attitudes improved after taking the course. Bhargava et al. (1999) studied gender discrepancy in both classroom
access and use. The findings showed that there were significant differences between males and females and these
differences were due to biased classroom practices, lack of female role models, and home computer gender gaps.
Following the same path, Hong and Koh (2002) found that female teachers were more anxious than male teachers
toward hardware. They also found that the overall computer anxiety levels of male teachers were not significantly
different from the anxiety levels of female teachers. Only for the hardware anxiety domain was significant
differences detected between male and female teachers.
(4) Technology Integration Barriers
A number of barriers that hinder technology integration have been documented (Flores, 2002; Earle, 2002; &
Brinkerhof, 2006). According to Flores (2002), teachers face many barriers in their quest to incorporate technology.
In addition to time scheduling for technology use and administrative support, equity is another important issue. The
introduction of technology is particularly hard when there are few resources.
Earle (2002) pointed out some barriers to the integration of technology in the classroom including both restraining
forces that are extrinsic to teachers such as access, time, support, resources, and training and forces that are intrinsic
such as attitudes, beliefs, practices, and resistance. More recently, Brinkerhof (2006) pointed out that barriers are
grouped into four main categories: resources, institutional and administrative support, training and experience, and
attitudinal or personality factors.

Statement of the problem
Due to the role of technology in the advancement of society in general and educational sector in particular, effective
technology integration into teaching and learning has become the focus of many educators. However, most research
studies conducted so far focus on quantitative data collection methodology such as surveys. This method of data
collection does not always give a true picture of technology integration in the classroom. This is particularly true if
teachers and students were not voluntarily participating in the study. In such a case, they may fill in questionnaires
without giving enough thought to content. Hence, study results are affected and do not reflect reality. Therefore, the
need to investigate technology integration using a mixed-methodology is a must. This study aims at investigating
technology integration at UAE K-12 schools using a mixed-method for data collection. Such research in the United
Arab Emirates has not been conducted as literature search did not result in any studies. This study aims at
investigating teachers’ perceptions of their technology integration competencies, barriers obstructing such
integration, and incentives to increase it, in addition to other related issues.

Methodology
Participants
The participants were 100 (Grades 6-9)) teachers from two model schools in Al-Ain educational zone, Abu Dhabi,
United Arab Emirates. Forty of the participants were female, while the rest were male teachers. All teachers at both
schools had between 5 and 15 years of teaching experience. All had experience using technology in their classes as it
is mandated by model schools. Both schools have good technology infrastructures available for teachers.
167

Research Questions
This study aimed at answering the following questions
1. How do teachers perceive their competencies to technology integration?
2. How do teachers perceive obstacles and incentives related to successful classroom technology integration?
3. How do teachers perceive their students’ classroom usage of technology?
4. To what extent do teachers perceive their classroom use of technology tools?
5. What is the difference in perception of male and female teachers in technology integration?

Data Collection
To answer these questions, the study used multiple research tools, a questionnaire and focus group interviews. The
aim of these tools is to investigate teachers’ perceptions of technology integration and actual classroom practices. In
addition, the use of these tools enables researchers to validate study results, and hence get more reliable findings.
a)

A questionnaire focusing on teachers’ perception of technology integration was developed. It consisted of a
number of subthemes that investigated teachers’ perceptions of their technology competencies and usage,
students’ usage of technology, problems hindering technology integration, and incentives that motivate teachers
to integrate technology. The face validity of the questionnaire was established by refereeing it by a panel of
university professors with different specializations, including educational technology. The questionnaire validity
using Cronbach's Alpha was 0.94. The questionnaire used a five-point Likert scale extending from 5 (very high
or strongly agree) to 1 (very low or strongly disagree). The questionnaire was distributed to all teachers at
participating schools. Response rate was around 75%.
b) Focus group interviews were conducted with the teachers at both schools. The aim of these interviews was to
collect detailed data on technology integration methods, problems hindering such integration, and incentives that
increase this integration in the class. Two focus group interviews were conducted with about 20 teachers from
the two schools representing different subjects, namely Islamic and Arabic Studies, Social Studies, Science,
Math, and English.

Data Analysis
Data gathered from questionnaire items were analyzed using SPSS 15.0. Descriptive statistics, a multivariate
analysis, and analysis of variance (ANOVA) were used. In addition, the researchers analyzed these items using “Item
Analysis” method in order to get a deep understanding of the results from the questionnaire. On the other hand, data
collected from focus group interviews were analyzed using the phenomenographic approach to data analysis, which
classified expressions used by participants according to similarities and differences (Levin and Wadmany, 2006).

Results and Discussion
To answer question 1 “How do teachers perceive their competencies to technology integration?”, results indicated
that teachers highly regard their competencies in technology integration. The mean scores ranged from 4.0 to 4.8 on
a 5-point scale (see Table 1). This high perception by teachers might be due to the fact that technology integration in
classrooms is a part of teacher evaluation, particularly at model schools. Investigating the items in details, the highest
mean scores were for items that are related to teachers’ ability to use hardware and software, using technology to
locate, evaluate, and collect information from a variety of sources, and content-specific tools.
These results conform to Bauer and Kenton (2005), where they found that teachers were highly skilled with
technology and had the competencies required from successful technology integration. In addition, they were also
supported by Zhao (2007) who investigated the perspectives and experiences of 17 social studies teachers following
technology integration training. Four major categories of technology-related activities were observed among
participants: (a) teacher-centered, (b) structured inquiry, (c) teacher-student negotiated, and (d) student-centered.
Most teachers were willing to use technology, expressed positive experiences with technology integration training,
increased their use of technology in the classroom, and used technology more creatively.
168

Enforcing these results, focus group interviews yielded some recommendations by teachers in order to enhance their
technology competencies and hence result in successful and effective technology integration in the classroom. Male
teachers recommend the following: (1) using computer labs as they give teachers the freedom and flexibility to
prepare class materials required for the whole course, (2) providing teachers with appropriate professional
development in the form of workshops on technology integration, (3) matching technology with curriculum goals so
that technology integration enhances teaching and learning, and (4) giving enough freedom for teachers in the
coverage and selection of materials to focus on quality rather than quantity.
ChanLin et al. (2006) supported the above findings, where teachers' perceptions about technology use were studied
in order to identify the factors affecting their use of technology in their teaching. Two major issues were explored.
First, the researchers studied how teachers integrated technology into creative teaching; they then identified the
factors that influenced teachers' use of technology in teaching. The identified factors were classified into four
categories: environmental, personal, social and curricular issues.
Table 1: Teachers’ Perceptions of their Competencies to Technology Integration
I am proficient in the use of common input and output devices; I can solve routine hardware and
software problems; I can make informed choices about technology systems, resources, and
services.
I can use technology to locate, evaluate, and collect information from a variety of sources.
I can use technology tools and information resources to increase productivity, promote
creativity, and facilitate academic learning.
I can use content-specific tools (e.g., software, simulation, environmental probes, graphing
calculators, exploratory environments, Web tools) to support learning and research.
I can collaborate in constructing technology-enhanced models, preparing publications, and
producing other creative works using productivity tools.
I can use technology tools to process data and report results.
I have a strong understanding of the nature and operation of technology systems.
I understand the legal, ethical, cultural, and societal issues related to technology.
I can choose learning and technology resources.
I can use technology resources to facilitate higher order and complex thinking skills, including
problem solving, critical thinking, informed decision-making, knowledge construction, and
creativity.
I can troubleshoot common computer problems.
I can use technology in the development of strategies for solving problems in the real world.
I have knowledge to discuss health and ethical issues related to technology.
I can use technology tools and resources for managing and communicating information (e.g.,
finances, schedules, addresses, purchases, correspondence).
I can evaluate and select new information resources and technological innovations based on
their appropriateness to specific tasks
I can use a variety of media and formats, including telecommunications, to collaborate, publish,
and interact with peers, experts, and other audiences.
I can discuss diversity issues related to electronic media.

M
4.8

SD
0.4

4.6
4.5

0.5
0.6

4.5

0.6

4.5

0.6

4.4
4.3
4.2
4.1
4.1

0.6
0.6
0.7
0.7
0.8

4.0
4.0
4.0
4.0

0.9
0.7
0.8
0.8

4.0

0.7

4.0

0.8

4.0

0.8

To answer question 2 “How do teachers perceive obstacles and incentives related to successful technology
integration in the classroom?, results showed that teachers perceive time and curriculum as two major obstacles that
hinder their technology integration in their classrooms (see Table 2).
Deep analysis of focus group interviews showed other barriers that hinder technology integration. Male teachers
indicated that there is a lack of training on how to integrate technology effectively. Most teachers depend on selflearning. They need to be involved in subjects that enable them to learn technology integration techniques and
strategies so they can use it successfully in their classes. Another barrier is parents’ and teachers’ negative attitudes
toward the importance and benefits of technology for learning and teaching.
Female teachers pointed out that a large number of students, technical problems, and expensive tools are the common
problems that negatively affect the effectiveness of technology. They suggested that schools should provide teachers
169

with affordable and/or free professional development subjects. Furthermore, there should be collaboration between
schools where teachers can exchange ideas and successful technology integration techniques. Finally, they suggested
that the curriculum should be accompanied with technology-enhanced materials such as CDs and videos. These
findings conform to Shelly et al. (2002).
Pertaining to incentives (see Table 2), results showed that participants consider having a free or discounted computer
as a major incentive for them. This might be because they need computers at home to enable them to work on
technology integration activities at their own pace and time. Other incentives that had high mean scores are
participation in workshops, having additional resources, positive evaluations, and recognition by school or school
zone. In fact, professional development for teachers and having enough technology resources are crucial for
successful technology integration in the classroom. These results are supported by the results of a number of research
studies such as Roberts and Ferris, 1994; Slough and Chamblee, 2000; Flores, 2002; Earle, 2002; Zorfass and
Rivero, 2005; & ChanLin et al., 2006.
Roberts and Ferris (1994) stated that barriers to technology integration included lack of knowledge of available
hardware and software, time commitment, and the risk of using technology. Similarly, Slough and Chamblee (2000)
argued that a view of technology as something unstable and always changing presents a major barrier to its use in the
classroom. Moreover, Flores (2002) concluded that teachers face many barriers in their quest to incorporate
technology such as time scheduling for technology use and administrative support, equity, and the lack of resources.
Earle (2002) pointed out to extrinsic barriers to technology integration such as access, time, support, resources, and
training and forces that are intrinsic to teachers such as attitudes, beliefs, practices, and resistance. On the other hand,
ChanLin et al. (2006) supported these findings by identifying the factors that influenced teachers' use of technology
in teaching. These factors were classified into four categories: environmental, personal, social and curricular issues.
Table 2: Teachers’ Perceptions of Obstacles and Incentives Related to Successful Technology Integration in
Classroom
Variable
Mean SD
Obstacles
The teacher does not have much time to prepare and implement them
3.4
1.2
Curricula are not ready to use such new technologies
3.0
1.3
Not enough encouragement to use them
2.7
1.3
Qualified staff for the labs are not available to help
2.5
1.3
Equipped labs are not available in schools
2.1
1.2
Technologies are not available in schools
2.0
1.1
Incentives
Free or discounted computers for their own use
3.6
1.6
Participation in special workshops
3.0
1.2
Additional resources for their classroom
3.0
1.2
Positive evaluations
3.0
1.3
School or educational zone recognition program
3.0
1.2
Free software.
2.8
1.6
Release time
2.7
1.4
Salary supplement
2.3
1.5
Mentor teacher designation (or similar designation)
2.3
1.3
To answer question 3 “How do teachers perceive their students’ usage of technology in the classroom?”, results
showed that teachers had high perception of students’ usage of technology (see Table 3). They reported high usage of
technology for interaction and communication, independent learning, engagement in learning, and understanding of
academic subjects. The mean score for each of these items was 4.0 on a 5-point scale. These results are supported by
Holinga (1999) who studied how Project LINCOLN in Springfield, Illinois, changed children’s education in an
important and meaningful way. The result of the project showed that student achievement has improved across all
grades.
To answer question 4 “To what extent do teachers perceive their usage of technology tools in the classroom?”,
results indicated that teachers use a number of technologies in their classrooms such as computers with different
software, transparencies, the Internet, maps, OHP, and Flyers & Folded Papers (see Table 4). Mean scores for the
170

usage of these tools were 3.3 or above. These results are supported by Ertmer et al. (1999) who found that teachers’
perceptions of the role of technology are closely linked to how technology is used. Another study conforming the
results of this study was conducted by Kotrlik and Redmann (2005), where results revealed that although teachers
feel some anxiety when it comes to technology integration, they perceived that they are effective in using
technology.
Table 3: Teachers’ Perceptions of their Students’ Usage of Technology in classroom
Variable
Mean
Students are interacting and communicating differently with the help of technology
4.0
Students become more independent learners as a result of technology.
4.0
Students are more engaged in learning due to technology.
4.0
Student understanding of academic subjects has deepened due to technology use
4.0
Students use technology to improve their basic skills with computer programs.
3.8
Students are developing online research expertise.
3.8
Students do more school work when not in school
3.8
The primary student-related use of technology is to teach students how to use the technology 3.7
itself.
Schools report that students have better grades and/or test scores since they began using 3.7
technology
Students use technology in at least some of their regular classrooms.
3.6
Schools report an increase in attendance on days that students
are scheduled to use 3.2
technology.
Students use computers only in a lab
3.1
Schools have reported decreases in the student dropout rate
attributed to the use of 3.1
technology.
Students actively participate in distance learning with other schools.
2.7

SD
0.9
0.8
0.7
0.8
0.8
0.9
0.6
0.9
1.0
1.1
1.0
1.2
1.2
1.3

Away from perceptions, data analysis for the first focus group interview with male teachers indicated that most male
teachers believe that using technology is important, but not all the time. On the other hand, they indicated that
technology has many advantages for the teaching-learning process. It saves class time, minimizes teachers’ efforts,
grasps students’ attention, and makes learning interesting. Students’ understanding is the most important factor that
teachers could use to evaluate the effectiveness of using technology in their classrooms.
Most female teachers highly regard technology and are using different types of applications in their classes such as
computers, visual projectors, and the Internet. Female teachers think that technology helps facilitate learning and
teaching, increases student participation, and provides visual support for students of different learning styles.
Table 4: Teachers’ Perceptions of their Usage of Technology Tools in Classroom
Variable
Computer
Transparencies
Different Computer Software
Geographic maps
Internet
Over Head Projector
Flyers & Folded Papers
Electronic Mail
Posters
Video
Wood Manipulatives
Drawing Tools
Tools for Creating Models
Raw Materials & Real Things ( e.g., Seeds, Buttons, Bean Pills,…)
Video
TV
Distance Learning Equipment and Infrastructure

M
4.6
4.6
4.5
4.2
4.0
4.0
4.0
3.8
3.8
3.7
3.7
3.6
3.6
3.6
3.5
3.4
3.3

SD
0.5
0.6
0.7
1.0
1.1
1.1
1.1
1.2
1.0
1.2
1.2
1.1
1.2
1.2
1.3
1.4
1.1
171

To answer question 5 “What is the difference in perception of male and female teachers in technology integration?”,
a multivariate analysis was run. Results indicated a significant difference between the two groups with a Hotelling’s
trace value of 9.3 with a significant f. of 10.75. To locate the significant differences within subscales, a one way
analysis of variance (ANOVA) was run. However, in order to control Type I error when conducting the analysis of
Variance, the researchers adjusted α level (0.05) using Benfaroni modification method. The adjusted value of α is ≤
0.005. Table 5 shows the items that yielded significant differences within the sub-themes.
As seen from the table, technology availability was a concern for female teachers more than it was for males. In
spite of this fact, results showed that female teachers use different types of technologies more than male teachers do.
The means scores for female teachers on technologies used are all above 4.4, while the mean scores for male teachers
ranged from 2.5 to 3.5. This might indicate that female teachers integrate technology in their classrooms more than
male teachers do.
On the other hand, Hong and Koh (2002) found that female teachers were more anxious than male teachers toward
hardware. They also found that the overall computer anxiety levels of male teachers were not significantly different
from the anxiety levels of female teachers. Only for the hardware anxiety domain were significant differences
detected between male and female teachers.
Table 5: Differences Between Male and Female Teachers in their Perception of Technology Integration
M
F
Teachers Perception of their Competencies in Technology Integration
I can use a variety of media and formats, including telecommunications, to collaborate, 3.9
4.4
publish, and interact with peers, experts, and other audiences.
creating multimedia presentations.
5.0
4.8
using computers for on-line communication (e.g., e-mail).
4.7
4.2
designing web pages.
3.3
4.2
Obstacles
Technologies are not available in schools
1.7
2.8
Qualified staff for the labs are not available to help
2.2
3.1
Technologies that Might be Used
Video
2.8
4.6
Over Head Projector
3.3
4.7
TV
2.5
4.7
Electronic Mail
3.7
4.6
Internet
3.7
4.7
Distance Learning Equipment and Infrastructure
3.0
4.1
Video
2.8
4.6
Over Head Projector
3.3
4.7
TV
2.5
4.7
Electronic Mail
3.4
4.6
Internet
3.7
4.7
Wood Manipulatives
2.9
3.9
Video
2.8
4.6
Models and 3D Pieces
3.8
4.7
Posters
3.4
4.4
Transparencies
3.5
4.6
Drawing Tools
3.5
4.6
Tools for Creating Models
3.0
4.4
Raw Materials & Real Things ( e.g., Seeds, Buttons, Bean Pills,…)
3.1
4.7
Flyers & Folded Papers
3.8
4.4
Results of Teachers' Beliefs about Technologies and Using them in Instruction
Most students have so many other needs that technology use is a low priority
3.3
4.0

f.
0.0
0.0
0.0
0.0
.00
.01
.00
.00
.00
.01
.01
.01
.00
.00
.00
.00
.01
.02
.00
.02
.00
.00
.00
.00
.00
.02
.04

From focus group interviews the following can be concluded: (1) female teachers have more experience, familiarity,
and knowledge of technology resources and applications than male teachers, (2) male teachers think that technology
should be a part of the curriculum plan and that they should receive rewards for their technology integration
172

performance, (3) most male and female teachers are mainly focusing on the use of computers and transparencies in
their classes, (4) both male and female teachers think technology should be used only when needed while teachers
should use a variety of teaching methods, and (5) all teachers agree that lesson goals and the nature of the subject are
the two factors that determine the type of technology the teacher should use.

Conclusion
Study results show that both male and female teachers at UAE Model Schools have high self perception of their
abilities and competencies to integrate technology successfully in their teaching. In addition, results revealed that
teachers integrate technology in their classes with different degrees and effectiveness in spite of the barriers that
hinder such integration (e.g., technical problems, large number of students, lack of professional development
training, lack of motivation and financial support, and negative teacher and parent attitudes toward the impact of
technology on teaching and learning).
In order to increase effective technology integration, both male and female teachers recommend the following: (1)
regular professional development workshops, (2) enhancing curriculum with technology-enhanced materials such as
CDs and videos, (3) increasing collaboration between schools across the country, and (4) giving enough freedom for
teachers in the selection and coverage of curriculum materials.
It is worth mentioning that when model schools at the UAE were inaugurated more than a decade ago, they had
advantages over typical schools, particularly in their infrastructure and teacher professional development activities.
Due to the success of these schools, most public schools around the country started to follow their path. As a result,
these days the gap between model schools and public schools almost vanished when it comes to technology
availability and teacher professional development. Most public schools, particularly in Abu Dhabi have more or less
the same advantages of model schools, particularly when it comes to technology equipment and teacher training.
Thus, the implication of this change is that the results obtained from this study can be easily generalized to other
UAE public schools covering the same grade levels as model schools.
These results were consistent with other studies investigating the same issues (e.g., Slough and Chamblee, 2000;
Guha, 2000; Flores, 2002; Earle, 2002; Shelly et al., 2002; Bauer and Kenton, 2005; Kotrlik and Redmann, 2005;
Zorfass and Rivero, 2005; and ChanLin et al., 2006).
Based on the above findings, the researchers recommend the following to enhance teachers’ skills and competencies
in technology integration regardless of country or gender:
1. Enhance teachers’ technology integration abilities and skills by delivering workshops about effective technology
integration.
2. Provide teachers with state-of-the-art technology including hardware and software.
3. Provide teachers with incentives and awards for outstanding technology integration in their classrooms.
4. Provide teachers with some release time so that they can plan effectively for technology integration in teaching
and learning.
5. Explore the use of technology in classrooms covering all school levels, including public and private schools.
6. Investigate the effect of technology integration on students’ achievement and attitude.
7. Investigate technology integration in relationship to curriculum goals and outcomes.

References
Abbit, J. T., & Klett, M. D. (2007). Identifying influences on attitudes and self –efficacy beliefs towards technology integration
among pre-service educators: Electronic Journal for the integration of technology in Education, 6, 28-42.
Almekhlafi, A.G. (2006a). The effect of computer assisted language learning (CALL) on United Arab Emirates English as a
foreign language (EFL) school students achievement and attitude. Journal of Interactive Learning Research, 17(2), 121-142.
Almekhlafi, A.G. (2006b). Effectiveness of interactive multimedia environment on language acquisition skills of 6th grade
students in the United Arab Emirates. International Journal of Instructional Media, 33 (4), 427, 241.

173

Anderson, S., & Maninger, R, (2007). Preservice teachers' abilities, beliefs, and intentions regarding technology integration.
Journal of Educational Computing Research, 37 (2), 151-172.
Bauer, J., & Kenton, J. (2005). Toward technology integration in the schools: Why it isn't happening. Journal of Technology and
Teacher Education, 13 (4), 519-546.
Becker, H. J., & Ravitz, J. L. (2001). Computer use by teachers: Are Cuban’s predictions correct? Paper presented at the
American
Educational
Research
Association,
Seattle.
Retrieved
February
15,
2009
from
http://www.crito.uci.edu/tlc/indings/confer-ences-pdf/aera_2001.pdf.
Bhargava, A., Kirova-Petrova, A., & McNair, S. (1999). Computers, gender bias, and young children. Information Technology in
Childhood Education Annual, January 1, 263-274.
Brinkerhof, J. (2006). Effects of a long-duration, professional development academy on technological skills, computer selfefficacy, and technology integration beliefs and practices. Journal of Research on Technology in Education, 39 (1), 22-44.
ChanLin, L., Hong, J., Horng, J., Chang, S., & Chu, H. (2006). Factors influencing technology integration in teaching: A
Taiwanese perspective. Innovations in Education and Teaching International, 43 (1), 57-68.
Earle, R. S. (2002). The Integration of instructional technology into public education: promises and challenges. Educational
Technology, 42 (1), 5-13.
Ertmer, P. A., Addison, P., Lane, M., Ross, E., & Woods, D. (1999). Examining teachers’ beliefs about the role of technology in
the elementary classroom. Journal of Research on Computing in Education, 32 (1), 54–72.
Flores, A. (2002). Learning and teaching mathematics with technology. Teaching Children Mathematics, 8 (6), 308-325.
Goddard, M. (2002). What do we do with these computers? Reflections on technology in the classroom. Journal of Research on
Technology in Education, 35 (1), 19-26.
Guha, S. (2000). A Comparative analysis of present and preferred situations of elementary grade teachers in using computers for
classroom instruction, ERIC Document Reproduction Service No. ED440089.
Gulbahar, Y. (2007). Technology planning: A Roadmap to successful technology integration in schools. Computers and
Education, 49 (4), 943-956.
Holinga, M. J. (1999). Project LINCOLN: improving and enhancing student learning. Learning and Leading with Technology, 26
(7), 54-80.
Hong, K., & Koh, C. (2002). Computer anxiety and attitudes toward computers among rural secondary school teachers: A
Malaysian perspective. Journal of Research on Technology in Education, 35 (1), 27-46.
Judson, E. (2006). How teachers integrate technology and their beliefs about learning: Is there a connection? Journal of
Technology and Teacher Education, 14 (3), 581-597.
Kotrlik, J., & Redmann, D. (2005). Extent of technology integration in instruction by adult basic education teachers. Adult
Education Quarterly: A Journal of Research and Theory, 55 (3), 200-219.
Lam, Y., & Lawrence, G. (2002). Teacher-student role redefinition during a computer-based second language project: Are
computers catalysts for empowering change? Computer Assisted Language Learning, 15 (3), 295-315.
Levin, T., & Wadmany, R. (2006). Teachers’ beliefs and practices in technology-based classrooms: A Developmental view.
Journal of Research on Technology in Education, 39 (2), 157–181.
Manzo, K. K. (2001). Academic record. Education Week, 20(35), 22-35. Washington.
NCES (2000). Internet access in U. S. public schools and classrooms: 1994–99. Washington, DC: NCES 2000–086.
Redmann, D., & Kotrlik, J. (2004). Technology integration into the teaching- learning process by business education teachers.
Delta Pi Epsilon Journal, 46 (2), 76-91.
Roberts, N., & Ferris, A. (1994). Integrating technology into a teacher education program. Journal of Technology and Teacher
Education, 2 (3), 215-225.
Rowand, C. (2000). Teacher use of computers and the internet in public schools. Stats in Brief, ERIC Document Reproduction
Service No. 442463.
Sandholtz, J. H. (2001). Learning to teach with technology: A Comparison of teacher development programs. Journal of
Technology and Teacher Education, 9, (3), 349.
Shashaani, L. (1997). Gender differences in computer attitudes and use among college students. Journal of Educational
Computing Research, 16 (1), 37-51.
174

Shelly, B. G., Cashman, T.J., Gunter, R. E., & Gunter, G. A. (2002). Teachers discovering computers: Integrating technology in
the classroom (2nd Ed.), Boston, MA: Course Technology.
Sherry, L., Bilig, S., Jesse, D., & Acosta, D. W. (2001). Assessing the impact of instructional technology on student achievement.
T.H.E. Journal, 28 (7), 40-43.
Slough, S. W., & Chamblee, G. E. (2000). Implementing technology in secondary science and mathematics classrooms: A
perspective on change. In D. A. Willis, J. D. Willis, & J. Willis (Eds.), Proceedings of the Society for Information Technology and
Teacher Education International Conference, 1021–1026, Charlottesville, VA: AACE.
Totter, A., Stutz, D., & Grote, G. (2006). ICT and schools: Identification of factors influencing the use of new media in vocational
training schools. The Electronic Journal of e-Learning, 4 (1), 95–102.
Wernet, S. P., Olliges, R. H., & Delicath, T.A. (2000). Postcourse evaluation of WebCT (Web Course Tools) classes by social
work students. Research on Social Work Practice, 10 (4), 487-504.
Wood, R.; & Ashfield, J. (2008). The use of the interactive whiteboard for creative teaching and learning in literacy and
mathematics: a case study. British Journal of Educational Technology, 39 (1), 84-96.
Yildirim, S. (2000). Effects of an educational computing course on preservice and inservice teachers: A discussion and analysis of
attitudes and use. Journal of Research on Computing in Education, 32 (4), 479-495.
Zhao, Y. (2007). Social studies teachers' perspectives of technology integration. Journal of Technology and Teacher Education,
15 (3), 311-333.
Zorfass, J., & Rivero, H. (2005). Collaboration is a key: How a community of practice promotes technology integration. Journal
of Special Education Technology, 20 (3), 51-60.

175

Johnson, G. M. (2010). Internet Use and Child Development: Validation of the Ecological Techno-Subsystem. Educational
Technology & Society, 13 (1), 176–185.

Internet Use and Child Development: Validation of the Ecological TechnoSubsystem
Genevieve Marie Johnson
Department of Psychology, Grant MacEwan College, Edmonton, Canada T5J 4S2 // gen.johnson@shaw.ca
ABSTRACT
Johnson and Puplampu recently proposed the ecological techno-subsystem, a refinement to Bronfenbrenner’s
theoretical organization of environmental influences on child development. The ecological techno-subsystem
includes child interaction with both living (e.g., peers) and nonliving (e.g., hardware) elements of
communication, information, and recreation technologies in immediate or direct environments. The theoretical
techno-subsystem requires empirical validation. Parents of 128 children in first through sixth grade consented to
cognitive developmental assessment of their children and completed questionnaires on children’s use of the
Internet at home and family socioeconomic characteristics. In general, indices of home Internet use accounted
for more of the variance in children’s cognitive development than did indices of socioeconomic status. The
ecological techno-subsystem furthers our understanding of environmental influences on child development by
emphasizing the impact of digital technologies on cognitive growth during childhood.

Keywords
Ecological techno-subsystem, Child development, Child cognition, Ecological theory

Introduction
According to the Corporation for Public Broadcasting (2002), the prevalence of Internet use among American 6 to 8
year old children doubled between 2000 and 2002 (from 27% to 60%, across all locations, at least one a week).
Approximately 20% of Canadian 9 year old children access the Internet through their own personal computer (Media
Awareness Network, 2006). The Office of Communication (2007) reported that 7% of British 10-year-olds have a
webcam. In Australia, nine in ten families have home Internet connectivity and 75% have broadband access
(Australian Communications and Media Authority, 2007). Trends indicate continued increase in the number of children
accessing the Internet, the amount of time they spend online, and the complexity of their online behavior (Livingstone &
Helpsper, 2007).
Historically, panic surrounds the introduction of new technologies, particularly in relation to children and youth
(Johnson, 2006). For example, in the 19th century, “the telegraph enabled a young woman, against her father’s
wishes, to maintain a flirtation with a number of men on the wire” (Quigley & Blashki, 2003, p. 311). In the 21st
century, there are two conflicting public anxieties surrounding children and the Internet; first, that the Internet may
harm children, for example, by exposure to inappropriate content (Media Awareness Network, 2008) and, second,
that children without Internet access are cognitively and socially disadvantaged (Jackson et al., 2006). Public anxiety
surrounding the digital divide (Burnett & Wilkinson, 2005; Livingstone & Helpsper, 2007), increasingly complex
school Internet literacy curriculum (Johnson, 2007a; Takahira, Ando, & Sakamoto, 2007), and social policy
initiatives directed at enhancing childhood Internet access (Sandvig, 2003) reveal the extent to which Internet use is
perceived as developmentally appropriate (if not required). Indeed, there is mounting evidence that using the Internet
provides children with cognitive and social benefits (Greenfield & Yan, 2006).

Internet Use and Child Development
Particularly during periods of rapid development associated with childhood, Internet use stimulates cognitive and
psychosocial development (Johnson, 2006; Young, 2007). Fish and colleagues (2008) investigated home computer
experience and cognitive development among preschool children in inner-city Head Start programs. Data was
collected from parents regarding the children's experience with computers in the home environment, including access
to a computer, time spent on a computer, and types of computer programs used. Two hundred participating children
were administered standardized tests of cognitive development. After controlling for parent's education and
household income, children who had home computer access had significantly higher scores of cognitive development
than did children who did not have home access. Frequency of children's computer use also related to cognitive

ISSN 1436-4522 (online) and 1176-3647 (print). © International Forum of Educational Technology & Society (IFETS). The authors and the forum jointly retain the
copyright of the articles. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that copies bear the full citation on the first page. Copyrights for components of this work owned by
others than IFETS must be honoured. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior
specific permission and/or a fee. Request permissions from the editors at kinshuk@ieee.org.

176

development. The investigators concluded that early computer use at home was a positive influence on young
children's cognitive development.
The Internet, although rich in graphic display, is primarily a text-based medium; “the more a child uses the Internet,
the more he/she reads” (Jackson et al., 2007, p. 188). Li and Atkins (2004) concluded that computer exposure during
the preschool years was associated with subsequent school readiness. Jackson and colleagues (2006) provided low
income children with home-based Internet access and continuously recorded time online. “Findings indicated that
children who used the Internet more had higher scores on standardized tests of reading achievement and higher grade
point averages 6 months, 1 year, and 16 months later than did children who used the Internet less” (p. 429). Fuchs
and Wößmann (2005) inferred, having controlled for socioeconomic status, “a negative relationship between home
computer availability and academic achievement, but a positive relationship between home computer use for Internet
communication” (p. 581). From a developmental perspective, Internet use stimulates cognitive processes involved in
interpreting text and images (Johnson, 2006). Metacognitive processes such as planning, search strategies, and
evaluation of information are exercised when navigating websites (Tarpley, 2001).
DeBell and Chapman (2006) concluded that Internet use promotes cognitive development in children, “specifically
in the area of visual intelligence, where certain computer activities -- particularly games -- may enhance the ability to
monitor several visual stimuli at once, to read diagrams, recognize icons, and visualize spatial relationships” (p. 3).
Van Deventer and White (2002) observed proficient 10- and 11-year-old video gamers and noted extremely high
levels of self-monitoring, pattern recognition, and visual memory. In a comprehensive review of the literature of the
time (when interactive digital games were relatively unsophisticated), Subrahmanyam, Kraut, Greenfield, and Gross
(2000) concluded that “children who play computer games can improve their visual intelligence” (p. 128). It should
be noted, however, that playing video games has also been linked to childhood distractibility, over-arousal, hostility,
and aggression (Anderson, Gentile, & Buckley, 2007; Funk, Chan, Brouwer, & Curtiss, 2006).
While Internet use during childhood has been associated with negative developmental outcomes, research
increasingly suggests that the Internet provides children with more developmental advantages than disadvantages
(Greenfield & Yan, 2006). Comprehensive theoretical description of the developmental impact of Internet use is
required. The recently proposed ecological techno-subsystem (Johnson & Puplampu, 2008) provides a conceptual
framework for understanding the effect of Internet use on child development.

Ecological Systems Theory and the Techno-Subsystem
Contemporary theories of child development assume that biological predispositions and environmental experiences,
to varying combined degrees, result in social, emotional, and cognitive growth. Cognitive-developmental theories
assume that neurological maturation and environmental experience result in individuals who are progressively more able
to function effectively in their environments (Luria, 1976). A socio-cultural orientation to child cognitive development
presupposes that “through participation in activities that require cognitive and communicative functions, children are
drawn into the use of these functions in ways that nurture and scaffold them" (Vygotsky, 1986, pp. 6-7). Ecological
systems theory (Bronfenbrenner, 1979) presents a particularly comprehensive view of environmental influences on
development by situating the child within a system of relationships affected by multiple levels of the surrounding
environment.
Bronfenbrenner (1979) organized the contexts of development into five nested environmental systems, with bidirectional influences within and among systems. The microsystem refers to the immediate environment and
includes, most notably, home and school interactions. The mesosystem is comprised of connections between
immediate environments (e.g., home-school interactions). The exosystem includes environmental settings that
indirectly affect child development (e.g., the parent's workplace). The macrosystem refers to overarching social
ideologies and cultural values. The chronosystem highlights the effect of time on all systems and all developmental
processes. As his theory evolved, Bronfenbrenner (2005) proposed a bioecological perspective which views the
child's own biology as part of the microsystem. Bronfenbrenner (1989) described human development as “the
progressive, mutual accommodation, throughout the life course, between an active, growing human being, and the
changing properties of the immediate settings in which the developing person lives, as this process is affected by the
relations between these settings, and by the larger contexts in which the settings are embedded” (p. 188).
177

Ecological systems theory (Bronfenbrenner, 1979) emerged prior to the Internet revolution and the developmental
impact of then available technology (e.g., television) was conceptually situated in the child’s microsystem. Johnson
and Puplampu (2008) recently proposed the ecological techno-subsystem, a dimension of the microsystem. As
illustrated in Figure 1, the techno-subsystem includes child interaction with both living (e.g., peers) and nonliving
(e.g., hardware) element of communication, information, and recreation technologies in immediate or direct
environments. Since tools, by definition, extend human capabilities, interaction with increasingly complex tools
requires increasingly complex cognitive processes (Johnson, 2008; Nickerson, 2005). The Internet extends human
access to information and communication and provides cognitive scaffolding (e.g., search engines and e-directories)
which allows for higher-order processes such as evaluation and application of information to solve real problems.

Research Issues and Questions: Validation of the Ecological Techno-Subsystem
The utility of the recently proposed ecological techno-subsystem in explaining child development has not been
established nor investigated. From an ecological perspective, the techno-subsystem mediates bidirectional interaction
between the child (i.e., bioecology) and the family (i.e., microsystem). Does the techno-subsystem contribute to
increased understanding of the mechanisms of cognitive development during childhood? Which is the better
predictor of cognitive development during childhood, -- indices of home Internet use (elements of the technosubsystem) or family socioeconomic characteristics (elements of the microsystem)?

Figure 1. The Ecological Techno-Subsystem (Johnson & Puplumpu, 2008)

Methods
Participants
Parents of children in first through sixth grade (N = 151) attending an elementary school in suburban Western
Canada were invited to participate in the study. Parents completed a questionnaire and consented to cognitive
developmental assessment of their children. One hundred twenty-eight signed consent forms and completed parent
questionnaires were returned to the school. Participating children (62 males and 66 females) ranged in age from 6
years, 4 months to 12 years, 5 months; 14.8% of the children were in first grade, 12.5% were in second grade, 15.6%
178

were in third grade, 25.0% were in fourth grade, 16.4% were in fifth grade, and 15.6% were in sixth grade. Twelve
of the 128 children were funded for special needs (e.g., communication disorder, learning disability, behavior
disorder, medical condition).

Measures
Three constructs, corresponding to three ecological systems/subsystems, were measured: child cognitive
development (bioecology), indices of child use of the Internet at home (techno-subsystem), and family
socioeconomic characteristics (microsystem).

Child Cognitive Development
Children’s cognitive development was measured with one subtest from the fourth edition of Wechsler Intelligence
Scale for Children (WISC-IV; Wechsler, 2003) and three subtests from the Cognitive Assessment System (CAS; Das
& Naglieri, 2001). Subtests were selected to ensure comprehensive representation of child cognitive development
(i.e., language, metacognition, perception, and memory). Expressive language was assessed with the WISC-IV
vocabulary subtest (child provides verbal definitions to orally presented words). WISC-IV subtest scoring criteria
was maintained; norms were not required because all comparisons occurred within the group of 128 children. With
respect to the CAS, the matching numbers subtest measured metacognitive planning (find the two numbers that are
the same in a series of numbers), the nonverbal matrices subtest assessed visual perception (select an option that best
completes a visual matrix), and the word series subtest measured short-term auditory memory (repeat a string of
words presented orally); CAS scoring criteria was maintained.
Each of the 128 children was individually administered the four cognitive subtests by one of two examiners (an
educational psychologist and a trained research assistant). Rapport was initiated by in-class introduction of the
examiners, explanation of testing procedures, and response to class questions. Rapport was further established by
individual child-examiner interaction walking from the classroom to the testing room and, as required, upon entry
into the testing room. Each individual assessment was complete in approximately 15 minutes. Table 1 presents a
summary of cognitive developmental measures and description of children’s cognitive scores.
Table 1. Summary of Cognitive Developmental Measures and Description of Children’s Scores

Cognitive Skill
Expressive Language
Metacognitive Planning
Visual Perception
Auditory Memory

Test and Subtest
WISC Vocabulary
CAS Matching Numbers
CAS Nonverbal Matrices
CAS Word Series

N
127
126
128
128

Children’s Raw Scores
Range
Mean
14 – 54
30.7
1 – 11
7.1
4 – 33
14.3
2 – 16
9.7

SD
8.38
1.84
4.64
2.51

Indices of Child Home Internet Use
The parent questionnaire included two yes/no response items: Do you have the Internet in your home? Does your
child use the Internet at home? Approximately 83% of families (106/128) reported home Internet access; 71.9 %
(92/128) indicated that their child used the Internet at home. For purposes of the current investigation, five indices of
child home Internet use were obtained from parental response to questionnaire items. First, parents reported the
number of years of home Internet access (range 0.2 to 12 years, mean 5.2 years, standard deviation 2.96 years).
Additionally, parents who reported that their child used the Internet at home were asked to respond to the open-ended
questionnaire item, what does your child do when he/she uses the Internet at home? Thematic analysis of 90 parental
responses to the open-ended item revealed four categories or types of child home online behavior: learn (e.g.,
schoolwork, math practice, research for assignments), play (e.g., play games, have fun with friends), browse (e.g.,
visit websites, find things of interest), and communicate (e.g., email, chat). Approximately 17% of parents responded
to the open-ended questionnaire item with description that suggested one type of child online behavior, 35.9%
described two, 14.1% described three, and 3.1% described four types of online behavior. Using the Internet at home
179

to learn was reported in 65 cases, to play was reported in 57 cases, to browse in 35 cases, and to communicate in 27
cases. Thus, the five indices of child home Internet use included: 1) the continuous variable years of home Internet
access and the dichotomous (reported-unreported) variables of child home Internet use to 2) learn, 3) play, 4)
browse, and 5) communicate.

Family Socioeconomic Characteristics
The parent questionnaire assessed five family characteristics commonly used to determine socioeconomic status
(Bradley & Corwyn, 2002; Sirin, 2005). Two items queried father’s and mother’s employment status. Approximately
70% of mothers and 96% of fathers were employed, full-time or part-time. Two questionnaire items requested
father’s and mother’s level of education, coded as: elementary = 1, junior high school = 2, high school incomplete =
3, high school complete = 4, technical school/college (complete or incomplete) = 5 and university (complete or
incomplete) = 6. The mean educational level of mothers was 4.79 (SD = 0.95) suggesting that many mothers had
post-secondary education; the mean educational level of fathers was 4.45 (SD = 1.02) suggesting that some fathers
had post-secondary education. The final socioeconomic item on the questionnaire asked parents to indicate annual
family income by selecting one of the following options: < $20 000 = 1, $20 000 to $40 000 = 2, $40 000 to $60 000
= 3, $60 000 to $80 000 = 4, $80 000 to $100 000 = 5, > $100 000 = 6. Annual income for participating families was
approximately $60,000 CD (M = 4.07, SD = 1.48).
Table 2 presents a summary of measured constructs which includes: four tests of children’s cognitive development,
five indices of children’s home Internet use, and five family socioeconomic characteristics. Which are the better
predictors of cognitive development during childhood, -- elements of the microsystem or elements of the technosubsystem? Two series of stepwise regression analysis were conducted with the four cognitive development scores as
the dependant variables. In the first regression analyses, family socioeconomic characteristics (elements of the
microsystem) were the independent variables. In the second analyses, indices of home Internet use (elements of the
techno-subsystem) were the independent variables.
Table 2. Description of Constructs and Measures
Ecological System
Bioecology

System Elements
Cognitive Development

Specific Measures
Expressive Language
Metacognitive Planning
Visual Perception
Auditory Memory

Techno-Subsystem

Home Internet Use

Years of Internet Access
Online Learning
Online Playing
Online Browsing
Online Communication

Microsystem

Family Characteristics

Father Employment
Mother Employment
Father Education
Mother Education
Annual Family Income

Results
Results of analyses revealed that family socioeconomic characteristics (elements of the microsystem) explained a
modest (but significant) amount of the variation in children’s cognitive development scores. As presented in Table 3,
adjusted R2 values indicated that father’s level of education accounted for approximately 7% of the variation in
children’s level of expressive language (as measured by the WISC-IV vocabulary subtest), 5% of the variation in
children’s visual perception and auditory memory (as measured by the CAS nonverbal matrices subtest and CAS
180

word series subtest, respectively). Whether or not mothers were employed, part-time or full-time, accounted for
approximately 6% of the differences in children’s capacity to execute metacognitive functions such as planning (as
measured by the CAS matching numbers subtest). While the other measures of familial socioeconomic status (e.g.,
mother’s education and family income) explained some of the variance in children’s cognitive development, such
measures did not improve upon the predictive utility of father’s education or maternal employment; variation is
prerequisite to prediction. Almost all fathers were employed and almost all mothers had finished high school. For
participating middle-class families, father’s education and mother’s employment were more sensitive to children’s
cognitive development scores than were family income, father’s employment, and mother’s education.

Table 3. Stepwise Regression Analysis: Family Characteristics Predicting Child Cognitive Development
Cognitive Score
Expressive Language
Metacognitive Planning
Visual Perception
Auditory Memory
*p < .05; **p < .01

Predictor
Father Education
Mother Employed
Father Education
Father Education

Beta Weight
.292
.270
.244
.258

t value
2.70**
2.46*
2.22*
2.36*

R2(adj)
.074
.061
.047
.054

F value
(1, 78) = 7.29**
(1, 77) = 6.05*
(1, 78) = 4.93*
(1, 78) = 5.55*

Results of analyses further revealed that indices of home Internet use (elements of the techno-subsystem), in general,
explained more of the variation in children’s cognitive development than did family socioeconomic characteristics
(elements of the microsystem). Summarized in Table 4, specific types on online behavior (i.e., learning,
communicating, and playing) and years of home Internet access combined to predicted child cognitive
developmental outcomes. Indicated by adjusted R2, children’s online communication, years of home Internet access,
and online learning (as reported by parents) accounted for approximately 29% of the variation in children’s level of
expressive language as measured by the WISC-IV vocabulary subtest. Online learning and communicating (reportedunreported) combined to explain 13.5% of the variation in children’s metacognitive planning. Online learning and
playing (reported-unreported) combined to explain 10.9% of the variation in children’s auditory memory. Years of
home Internet access explained approximately 3% of the differences in children’s visual perception scores. With the
exception of visual perception, indices of home Internet use (elements of the techno-subsystem) were better
predictors of children’s cognitive development than were family socioeconomic characteristics (elements of the
microsystem).

Table 4. Stepwise Regression Analysis: Home Internet Use Predicting Child Cognitive Development
Cognitive Score
Expressive Language

Metacognitive Planning

Visual Perception
Auditory Memory

Predictor/s
Online Communication
Years of Internet Access
Online Learning

Beta Weight
.344
.263
.256

t value
4.00***
3.12 **
2.99**

R2(adj)

F value

.287

(3, 101) = 14.97***

Online Learning
Online Communication

.287
.201

3.03**
2.12*

.135

(2, 101) = 9.06***

Years of Internet Access

.192

1.99*

.028

(1, 104) = 3.98*

.242
.228

2.60*
2.46*

.109

(3, 101) = 14.97***

Online Learning
Online Playing
*p < .05; **p < .01; ***p < .001

Discussion
A variety of mechanisms linking family socioeconomic status to child cognitive development have been proposed
including parenting (Petrill, Pike, Price, & Plomin, 2004; Mistry, Biesanz, Chien, Howes, & Benner, 2008) and
181

resources (Bradley & Corwyn, 2002). For the current sample of middle class children, paternal education and
maternal employment were associated with measures of child cognitive development. More educated fathers tended
to have offspring who scored high on three of the four cognitive measures (expressive language, visual perception,
and auditory memory). Mothers who were employed tended to have children who scored high on the measure of
metacognitive planning. Educated fathers and employed mothers may genetically transmit to their offspring some
neurological processing advantage (bioecology). Simultaneously, educated fathers may provide enhanced language
models and stimulating environments that facilitate the cognitive development of their children (microsystemic
influence). Employed mothers may provide models of organization and place increased demands on children to selfregulate thereby enhancing the metacognitive planning abilities of their offspring (microsystemic influence).
Family socioeconomic status (as measured and for the current sample) accounted for 5% to 7% of differences in
child cognitive development scores. In contrast, indices of home Internet use (as measured and for the current
sample) accounted for 3% to 29% of differences in child cognitive development scores. Meta-analysis confirms that
the impact of socioeconomic status on academic achievement is eroding over time (Sirin, 2005). Increasingly
effective structures of social equalization (e.g., public education, quality daycare, preschool intervention, and
prenatal programs) and the expanding middle class create the need for more precise description of home
environments. Current results suggest that indices of home Internet use (i.e., elements of the ecological technosubsystem) provide more useful information regarding cognitive development than do family socioeconomic
characteristics (elements of the microsystem).
Only two of five family socioeconomic characteristics added to the regression equation, suggesting that some
measures (i.e., family income, father employment, and mother education) did not differ in relation to children’s
cognitive development. In contrast, four of the five indices of home Internet use during childhood added to the
regression equation, suggesting that these measures differed in relation to children’s cognitive development. In the
context of the current investigation, socioeconomic status is a crude construct relative to home Internet use. Internet
use includes both organized (e.g., search) and disorganized (e.g., browse) interactions with both human (e.g., chat)
and nonhuman (e.g., database) elements in online environments (Johnson & Kulpa, 2007). Internet use is a complex
set of behaviors that vary widely across individuals and that is influenced by cognitive and personality characteristics
(Joinson, 2003). For the current sample of children, patterns of home Internet use explained more of the variation in
cognitive development than did family socioeconomic characteristics.
In the context of middle class families, elements in the techno-subsystem (e.g., Internet access) may not necessarily
facilitate child cognitive development; effective use of those elements, highly dependent upon parent behavior, may
promote development. For example, Cho and Cheon (2005) surveyed families and found that parents’ perceived
control, obtained through shared web activities and family cohesion, reduced children’s exposure to negative Internet
content. Lee and Chae (2007) reported a positive relationship between parental mediation techniques (website
recommendation and Internet co-use) and children’s educational attainment. In the current investigation, the
cognitive experiences provided to children by employed mothers may include Internet skills instruction (e.g., sending
email) and models of information management (e.g., accessing websites for information). Such experiences, over
time, may provide children with enhanced opportunities to direct their own cognitive development via increasingly
sophisticated uses of the Internet. According to Livingston and Bober (2005), “a new divide is opening up between
those for whom the internet is an increasingly rich, diverse, engaging and stimulating resource and those for whom it
remains a narrow, unengaging, if occasionally useful, resource of rather less significance” (p. 2).
Bruner (2005) recently reiterated that “our minds appropriate ways of representing the world from using and relating
to the codes or rules of available technology” (p. x). Cognitive abilities prerequisite to utilization of Internet
applications constitute an implicit component of contemporary notions of intelligence (Maynard, Subrahmanyam, &
Greenfield, 2005). The ecological techno-subsystem furthers our understanding of environmental influences on child
development by emphasizing the impact of digital technologies on cognitive growth during childhood. The technosubsystem provides precise description of microsystemic mechanisms of developmental influence which lead to
intervention strategies. According to Livingston and Bober (2005), many parents lack the skills to guide and support
their children’s Internet use and Internet-literate parents have Internet-literate children. Subsequent research may
evaluate the effectiveness of techno-subsystem interventions for elementary school children at-risk, for example, the
provision of home Internet access and parent Internet literacy training. As stated elsewhere, “current anxiety
surrounding children’s Internet use should be for those whose cognitive processes are not influenced by the cultural
tool” (Johnson, 2006, p. 570).
182

Limitations and Future Research
In the current investigation, children’s use of the Internet at home was determined by parent-report (common in the
literature, e.g., Livingston & Bober, 2005; Rideout, Vandewater, & Wartella, 2003). The validity of such approaches,
however, has been questioned and alternatives suggested including asking the child directly (Media Awareness
Network, 2006; Roberts, Foehr, & Rideout, 2005) and standardized measures such as the Internet Vocabulary Test
for Children (Johnson, 2007b). In the current investigation, indices of children’s use of the Internet at home were
obtained with objective (i.e., years of home Internet access) and subjective (what does your child do when he/she
uses the Internet at home) parental response to questionnaire items. Alternative indices of children’s use of the
Internet may not replicate current findings.
Type of child online behavior (learn, play, browse, and communicate) emerged from thematic analysis of parent
response to an open-ended questionnaire item. Alternative abstraction is apparent. For example, parental response to
the open-ended item, what does your child do when he/she uses the Internet at home, may be dichotomized into
directed versus undirected or focused versus unfocused use of the Internet. Responses such as schoolwork, math
practice, research for assignments, email and chat may be interpreted as reflecting goal-directed and focused
behavior; responses such as play games, have fun with friends, visit websites, and find things of interest refer to
behavior that is unfocused and undirected. As opposed to online learning and communication, it may be that focused
and goal-directed Internet use contributes to cognitive development during childhood.
Childhood use of the Internet occurs in three contexts: home, school, and community. From an ecological
perspective, Internet use in one environment influences Internet use in other environments. Because all children in
the sample attended the same elementary school, school-based Internet experience was assumed equivalent.
However, Gibson and Oberg (2004) noted that the quality of school-based Internet experience varies widely across
classrooms. Subsequent theoretical and empirical research may expand techno-subsystem description to include
child-peer interactions during home, school, and community Internet use.

References
Anderson, C. A., Gentile, D. A., & Buckley, K. E. (2007). Violent video game effects on children and adolescents, New York;
Oxford University Press.
Australian Communications and Media Authority. (2007). Media and communications in Australian families 2007, Retrieved
February 22, 2009, from http://www.acma.gov.au/WEB/STANDARD/pc=PC_310893
Bradley, R. H., & Corwyn, R. F. (2002). Socioeconomic status and child development. Annual Review of Psychology, 53, 371399.
Bronfenbrenner, U. (1979). The ecology of human development: Experiments by nature and design, Cambridge, MA: Harvard
University Press.
Bronfenbrenner, U. (1989). Ecological systems theory. Annals of Child Development, 6, 187-24.
Bronfenbrenner, U. (2005). Making human beings human: Bioecological perspectives of human development, Thousand Oaks,
CA: Sage.
Bruner, J. (2005). Forward. In R. J. Sternberg & D. D. Preiss (Eds.), Intelligence and technology: The impact of tools on the
nature and development of human abilities (pp. ix-xi), Mahwah, NJ: Lawrence Erlbaum.
Burnett, C., & Wilkinson, J. (2005). Holy lemons! Learning from children’s uses of the Internet in out-of-school contexts.
Literacy, 39, 158-164.
Cho, C. H., & Cheon, H. J., (2005) Children’s exposure to negative Internet content: Effects of family context. Journal of
Broadcasting and Electronic Media, 49, 488-509.
Corporation for Public Broadcasting. (2002). Connected to the future: A report on children’s Internet use. Washington, DC.
Retrieved February 22, 2009, from http://www.cpb.org/stations/reports/connected/.
Das, J. P., & Naglieri, J. A. (2001). The Das-Naglieri cognitive assessment system in theory and practice. In J. J. W. Andrews, D.
H. Sakolfske, & H. L. Janzen (Eds.), Handbook of psychoeducational assessment: Ability, achievement, and behavior in children
(pp. 34-64). San Diego, CA: Academic Press.
183

DeBell, M., & Chapman, C. (2006). Computer and Internet use by students in 2003. National Center for Educational Statistics.
U.S. Department of Education, Washington, DC. Retrieved February 22, 2009, from http://nces.ed.gov/pubs2006/2006065.pdf.
Fish, A. M., Li, X., McCarrick, K., Butler, S. T., Stanton, B., Brumitt, G. A., et al. (2008). Early childhood computer experience
and cognitive development among urban low-income preschoolers. Journal of Educational Computing Research, 38, 97-113.
Fuchs, T., & Wößmann, L. (2005). Computers and student learning: Bivariate and multivariate evidence on the availability and
use of computers at home and school. Brussels Economic Review, 47, 359-385.
Funk, J. B., Chan, M., Brouwer, J., & Curtiss, K. (2006). A biopsychosocial analysis of the video-game-playing experience of
children and adults in the United States. Studies in Media and Information Literacy education, 6(3), 1-15.
Gibson, S., & Oberg, D. (2004). Visions and realities of Internet use in schools: Canadian perspectives. British Journal of
Educational Technology, 35, 569-585.
Greenfield, P., & Yan, Z. (2006). Children, adolescents, and the Internet: A new field of inquiry in developmental psychology.
Developmental Psychology, 42, 391-394.
Jackson, L. A., Samona, R., Moomaw, J., Ramsay, L., Murray, C., Smith, A., & Murray, L. (2007). What children do on the
Internet: Domains visited and their relationship to socio-demographic characteristics and academic performance?
CyberPsychology and Behavior, 10, 182-190.
Jackson, L. A., Von Eye, A., Biocca, F. A., Barbatsis, G., Zhao, Y., & Fitzgerald, H. E. (2006). Does home Internet use Influence
the academic performance of low income children? Developmental Psychology, 42, 429-435.
Johnson, G. M. (2006). Internet use and cognitive development: A theoretical framework. E-Learning, 4, 565-573.
Johnson, G. M. (2007a). Functional Internet literacy: Required cognitive skills with implications for instruction. E-Learning, 4,
433-441.
Johnson, G. M. (2007b). The Internet Vocabulary Test for Children: Preliminary development. Internet Research, 17, 235-248.
Johnson, G. M. (2008). Cognitive processing differences between frequent and infrequent Internet users. Computers and Human
Behavior, 24, 2094-2106.
Johnson, G. M., & Kulpa, A. (2007). Dimensions of online behavior: Toward a user typology. CyberPsychology and Behavior, 10,
773-780.
Johnson, G. M., & Puplampu, P. (2008). A conceptual framework for understanding the effect of the Internet on child
development: The ecological techno-subsystem. Canadian Journal of Learning and Technology, 34, 19-28
Joinson, A. N. (2003). Understanding the psychology of Internet behaviour: Virtual worlds, real lives, New York: Palgrave
MacMillan.
Lee, S. J., & Chae, Y. G. (2007). Children’s Internet use in a family context: Influence on family relationships and parental
mediation. CyberPsychology and Behavior, 10, 640-644.
Li, X., & Atkins, M. S. (2004). Early childhood computer experience and cognitive and motor development. Pediatrics, 113,
1715-1722.
Livingstone, S., & Bober, M. (2005). UK children go online: Emerging opportunities and dangers. London, UK: London School
of Economics. Retrieved February 22, 2009, from http://www.lse.ac.uk/collections/children-go-online/UKCGO_Final_report.pdf.
Livingstone, S., & Helpsper, E. (2007). Gradations in digital inclusion: Children, young people and the digital divide. New Media
& Society, 9, 671-696.
Luria, A. R. (1976). Cognitive development: Its cultural and social foundations, Cambridge MA: Harvard University Press.
Maynard, A. E., Subrahmanyam, K., & Greenfield, P. M. (2005). Technology and the development of intelligence: From the loom
to the computer. In R. J. Sternberg & D. D. Preiss (Eds.), Intelligence and technology: The impact of tools on the nature and
development of human abilities (pp. 29-54), Mahwah, NJ: Lawrence Erlbaum.
Media Awareness Network. (2006). Young Canadians in a wired world. Ottawa, ON. Media and Internet Education Resources.
Retrieved February 22, 2009, from http://www.media-awareness.ca/english/research/YCWW/phaseII/key_findings.cfm.
Media Awareness Network. (2008). Web awareness Canada – An Overview. Ottawa, ON. Media and Internet Education
Resources. Retrieved February 22, 2009, from http://www.media-awareness.ca/english/special_initiatives/web_awareness/.
Mistry, R., Biesanz, J., Chien, N., Howes, C., & Benner, A. (2008). Socioeconomic status, parental investments, and the cognitive
and behavioral outcomes of low-income children from immigrant and native households. Early Childhood Research Quarterly,
23, 193-212.
184

Nickerson, R. S. (2005). Technology and cognitive amplification. In R. J. Sternberg & D. D. Preiss (Eds.), Intelligence and
technology: The impact of tools on the nature and development of human abilities (pp. 3-27). Mahwah, NJ: Lawrence Erlbaum.
Office of Communications. (2007). The Communications Market, 2007. London, UK. Retrieved February 22, 2009, from
http://www.ofcom.org.uk/research/cm/cmr07/.
Petrill, S. A., Pike, A., Price, T., and Plomin, R. (2004). Chaos in the home and socioeconomic status are associated with cognitive
development in early childhood: Environmental mediators identified in a genetic design. Intelligence, 32, 445-460.
Quigley, M., & Blashki, K. (2003). Beyond the boundaries of the sacred garden: Children and the Internet. Educational
Technology Review, 11, 70-77.
Rideout, V. J., Vandewater, E. A., & Wartella, E. A. (2003). Zero to six: Electronic media in the lives of infants, toddlers and
preschoolers. Menlo Park, CA: The Henry J. Kaiser Family Foundation. Retrieved February 22, 2009, from
http://www.kaisernetwork.org/health_cast/uploaded_files/102803_kff_kids_report.pdf.
Roberts, D. F., Foehr, U. G., & Rideout, V. (2005). Generation M: Media in the lives of 8 – 18 year olds. Menlo Park, CA: The
Henry J. Kaiser Family Foundation. Retrieved February 22, 2009, from http://www.kff.org/entmedia/7251.cfm.
Sandvig, C. (2003). Public Internet access for young children in the inner city: Evidence to inform access subsidies and content
regulation, The Information Society, 19, 171-183.
Sirin, S. (2005). Socioeconomic status and academic achievement: A meta-analytic review of research. Review of Educational
Research, 75, 417-453.
Subrahmanyam, K., Kraut, R., Greenfield, P., & Gross, E. (2000). The impact of home computer use on children’s activities and
development. Future of Children, 10, 123-144.
Takahira, M., Ando, R., & Sakamoto, A. (2007). Effect of Internet use on development of information literacy: A panel study with
Japanese elementary school children. Computers in the Schools, 24, 65-82.
Tarpley, T. (2001). Children, the Internet, and other new technologies. In D. G. Singer & J. L. Singer (Eds.), Handbook of children and
the media (pp. 547-556). Thousand Oaks, CA: Sage.
Van Deventer, S. S., & White, J. A. (2002). Expert behavior in children’s video game playing. Simulation and Games, 33, 28-48.
Vygotsky, L. (1986). Thought and language, Cambridge, MA: The MIT Press.
Wechsler, D. (2003). Wechsler Intelligence Scale for Children (4th Ed.), San Antonio, TX: Harcourt Assessment.
Young, K. (2007). Toward a model for the study of children’s informal Internet use. Computers in Human Behavior, 24, 173-184.

185

Uzunboylu, H., & Tuncay, N. (2010). Divergence of Digital World of Teachers. Educational Technology & Society, 13 (1), 186–
194.

Divergence of Digital World of Teachers
Huseyin Uzunboylu and Nazime Tuncay
Department of Instructional Technology, Near East University, Nicosia, North Cyprus // huzunboylu@neu.edu.tr,
nazime.tuncay@gmail.com
ABSTRACT
There exists great diversity in the teachers’ digital world. Teachers are being discriminated based on numerous
educational gaps. This paper seeks to assess the extent of the digital divide among the North Cyprus vocational
teachers along the four axes: age, Internet access, computer access, and performance (computing
knowledge/experience). A research was carried out through a questionnaire, which was then analyzed
statistically. According to the experts’ views, the questionnaire was divided into three factors: technology-based
e-learning applications; web-based e-learning applications, and administrative e-learning applications. There
was a significant digital divergence among the teachers surveyed, which may adversely affect their ability to
prepare the students to become a part of the knowledge society. To bridge these gaps in the world, action plans
should be prepared, collaboratively with the instructional technologist and ICT (Information and
Communication Technology) experts. This unique research study is the first to investigate the divergence of the
digital world of teachers.

Keywords
Digital world, gaps, teachers, e-learning

Introduction
Gaps in the Digital World

Gaps, which are defined as, have and have-nots; know and know-nots, are widening [United Nations Development
Programme (UNDP), 1999; Malloch, 2000; James, 2000; Main, 2001; Dalsgaard, 2001; Cobb, 2002]. The obstacles
that are defined as gaps significantly affect education. However, day-by-day, the effects of gaps in education has
become clearer, and the easiness of e-learning applications in education has become more common among teachers.
Unequal opportunities among countries in access to technology and Internet host have also been of interest for
researchers for many decades. Correspondingly, UNDP (1999), OECD (2001, 2007), Hargittai (2003), Piskurich
(2003), and Papastergiou and Solomonidou (2005) studied the existence of access gaps. In fact, all the divides and
gaps have been observed to be interrelated in one way or the other. Using different approaches toward teaching and
learning, both pedagogical (e.g., literacy teaching) and organizational (e.g., class size, ratio of teaching assistant to
pupils) can help achieve positive outcomes and narrow the gap (Demiralay & Karadeniz, 2008; DCSF, 2007a;
Cassen and Kingdon, 2007; Younger et al., 2005). Effective literacy interventions are an important element to
narrowing the gap in the outcomes, as poor literacy at primary-school age is strongly and significantly associated
with future low achievement (Cassen and Kingdon, 2007). With respect to these factors, earlier studies (Clarke et al,
2008; Ahmed, 2007; Souter, 2007; Bhanji, 2008) cited that the most-accepted gaps are Internet gaps, age gaps,
digital gaps, knowledge gaps, access gaps, economic gaps, and performance gaps. Although there have been several
studies (Tezer, & Bicen, 2008; Gunga & Ricketts, 2007; Cole, 2005, Manette, 2004) in the literature about gaps
among students, there has been no study on the gaps among the teachers. Thus, this study attempts to examine the
digital divide among the teachers along the four axes: age, Internet access, computer access, and performance
(computing knowledge/experience) (see Figure 1).

Divides in the Digital World
Digital divide is used to describe the increasing gap between computer users and non-users (Becker, 2000). Digital
divide is the gap between individuals, households, business, and geographic areas at different socioeconomic levels
with regard to both their opportunities to access Information and Communication Technology (ICT) and the use of
Internet for a wide variety of purposes (OECD, 2001). It is the division of the world between those who have access
to new ICT and those who do not (Asian Development Bank, 2002). Digital divide is a term increasingly used to
describe the social implications of unequal access to ICT by some sectors of the community, and to the acquisition of
ISSN 1436-4522 (online) and 1176-3647 (print). © International Forum of Educational Technology & Society (IFETS). The authors and the forum jointly retain the
copyright of the articles. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that copies bear the full citation on the first page. Copyrights for components of this work owned by
others than IFETS must be honoured. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior
specific permission and/or a fee. Request permissions from the editors at kinshuk@ieee.org.

186

necessary skills (National Office for the Information Economy, 2001). It is the concrete and symbolic distance
between those who enjoy the access and familiarity of the immense potential of technology, and those who do not
(Munoz, 2002). Furthermore, it is the separation between those who have access to and can effectively use
technology and those who could not (Pearson, 2002). Occurrence of a digital divide may be owing to several reasons,
such as income, educational level, class, gender, race, and geographical location (Norris, 2001). As a result of access
gaps, there also exist digital divides between (and within) rich and poor countries [Chinn & Fairlie, 2004; Hargittai,
2003]. Does this divide exist among teachers? Does age gap, Internet-access gap, computer-access gap, and
performance gap result in divide in the digital world of teachers? Is there a convergence or a divergence? These are
the questions that arise during the literature reviews, which must be addressed.

Figure 1. Divergence of teachers’ digital world
Researches about Gaps in Education
There have been numerous researches on gaps and divides in education. However, most of them focused on digital
divide among students. The European Centre for Development of Vocational Training (2001) carried out a survey
among 446 individuals from several European countries, where one of the principle observations of this survey was
that the trainers and vocational teachers undertake inadequate ICT skills development to improve the expertise in
pedagogy and management issues, resulting in divergence in education. Uzunboylu (2006) observed technological,
pedagogical, social, economic, and cultural barriers in education. According to Park and Ertmer (2008), barriers in
education exist owing to lack of knowledge and skills, unclear expectations, and insufficient feedbacks. Although
most of the researchers explained these factors in different terms, their research results revealed the existence of gaps
in education. There exists a research gap with regard to studies focusing on digital world of teachers. Is there a
divergence in the digital world of teachers which affects their ability to prepare students for the knowledge society?

Purpose of the Study
The purpose of this study is to determine the divergence of teachers in the digital world. In addition, the four subquestions of this research are as follows:
1. Is there an Internet gap among teachers?
2. Is there a digital gap among teachers?
3. Is there an age gap among teachers?
4. Is there a performance gap among teachers?
187

Method
Population
The population of this research consisted of all the teaching staffs in 12 vocational schools in North Cyprus: teacher
assistants, headmasters, headmaster assistants, area heads, curriculum managers, electric/electronics teachers, ICT
teachers, motor teachers, machine technology teachers, accounting teachers, mathematics teachers, science teachers,
history teachers, geography teachers, Turkish teachers, social science teachers, English teachers, metal teachers, etc.
(throughout this paper, teaching staffs are simply referred as teachers). There were about 490 teachers in the
vocational high schools in North Cyprus, and the questionnaires were given to all the teachers in the vocational
schools. The response rate was 81.2%, with 396 valid and 2 incompletely answered questionnaires.

Instrument
A questionnaire was developed to examine the e-learning training needs of the vocational high-school teachers. To
evaluate the items in the questionnaire, experts’ evaluation (n = 17) was employed. An expert group of instructional
technologists evaluated the data-gathering scale, both individually and collaboratively. Under the suggestions of the
experts, necessary corrections were made to the draft form of the questionnaire. Thus, the content validity was
maintained with the help of the educational technologist experts. After making the necessary corrections on the draft
form of the questionnaire based on the experts’ suggestions, the questionnaire was given to the teachers to fill, using
a five-point Likert scale. In total, the questionnaire consisted of two parts: Part 1 contained the questions like:
- How long have you been working in the school?
- Where can you access to internet?
- What is your specification?
Part 2 consisted of 32 items, where the teachers were asked to choose the suitable scales for themselves, in each item
of the questionnaire The scales were arranged as: needs to be improved, basic, good, very good, and excellent..
Among the main subjects measured in the Part 2 of the questionnaire were:
- Skill of publishing/visual tools and technology;
- Skill of using wireless technologies (phone, laptop);
- Ability to join video-conferences through internet.
After considering the experts’ suggestions, the questionnaire was divided into three factors: technology-based elearning applications (r=0.96), web-based e-learning applications (r=0.97), and administrative e-learning
applications (r=0.97).

Process of Data Collection
The researchers visited all the vocational schools in North Cyprus and the questionnaires were given to all the
teachers working in these schools, which was not an easy job, and demanded lots of traveling and explanations. As
there were many part-time teachers in the vocational schools who teach on different days, each school was visited
more than once, on different days. There were also other obstacles in the beginning of the research—some teachers
did not want to fill the questionnaire as they were afraid of revealing their lack of necessary e-learning
skill/knowledge for teaching, while some did not know the meanings of the terms, such as e-learning, e-TV, e-board,
and e-discussion. The researchers explained each of the questionnaire items to the teachers and provided the
necessary feedback needed for analyzing their own skills or knowledge as excellent, very good, good, basic, and
needs to be improved. Thus, the maximum confidential atmosphere was provided to the participants.

Data Analysis
For data analysis, SPSS 16.0 was used. The questionnaire items were arranged according to the Internet gap,
performance gap, digital gap, and age gap. The access to the Internet by teachers formed the basis of Internet gap.
The total score of the questionnaire items about e-learning skills and ability was used as the performance level. The
188

total score of the questions that were about ICT knowledge were used as the digital level. On the other hand, to
reveal the age gap, the teachers who have more than 20 years of teaching experience were classified as older and
those having less than 20 years of teaching experience were classified as younger. The teachers’ accesses were
classified as: “1” indicating limited and “2” indicating unlimited. Subsequently, the Internet score was calculated as
the sum of the total score of the web-based applications and access score. Thus, the scores above this sum were
considered to indicate narrow Internet gap and those below this total were considered to signify wide Internet gap.
Furthermore, the gap score was considered to be the total of the four gap levels (Internet gap, digital gap,
performance gap, and age gap). The score below the gap score mean was considered as “narrow gap” and the gap
score above the mean was considered as “wide gap.” The marks above the mean of the total score obtained from the
questionnaire were regarded as high, and these teachers were accepted to be “e-literate.” On the other hand, the
marks below the mean of the total score obtained from the questionnaire were considered as low, and these teachers
were accepted to be “not e-literate.” The independent sample test was used to analyze all the variables, with 0.05 as
the significant level. However, in all the tests, the resulting significant level was < 0.01.

Results and Discussions
The results and discussions will be explained in five sub-sections. The results obtained show the existence of digital
gap, access gap, performance gap, and age gap among the teachers. In addition, they also demonstrate the effects of
these gaps on the e-learning applications of the teachers, and are really important results showing the divergence of
teachers’ digital world.

Tech-based
Web-based
Admin-based

Table 1. Descriptive statistics results of gap’s effect on e-learning applications
Gap Levels
N
M
SD
Std. Error Mean
Wide
269
22.96
8.61
0.53
Narrow
129
40.66
10.44
0.92
Wide
269
25.87
9.03
0.55
Narrow
129
48.10
12.67
1.12
Wide
269
11.29
4.75
0.29
Narrow
129
22.49
7.41
0.65

Furthermore, the effects of the gap score levels were also investigated (see Table 1). In technology-based e-learning
applications, the number of teachers who were observed to have wide gaps (M = 22.96, SD = 8.61) were 269, and the
number of teachers who were observed to have narrow gaps (M =40.66, SD = 10.44) were 129. Although the number
of teachers who had wide gaps were more than those who had narrow gaps, the difference in the means was really
significant. The narrow the gap’s total was, the higher was the technology-based e-learning application scores. With
regard to the web-based e-learning applications, the number of teachers who had wide gaps (M = 25.87, SD = 9.03)
were 269 and those demonstrating narrow gaps (M =48.10, SD = 11.29) were 129. Although the number of teachers
who had wide gaps were more than those with narrow gaps, the difference in the means was really significant. The
narrow the gap’s total was, the higher were the web-based e-learning application scores. On the other hand, with
respect to the administrative e-learning applications, the number of teachers who had wide gaps (M = 11.29, SD =
4.75) were 269 and those who had narrow gaps (M =22.49, SD = 7.41) were 129. Although the numbers of teachers
who had wide gaps were more than those who showed narrow gaps, the difference in the means was again high. The
result demonstrates that if the gap’s total is narrow, the administrative e-learning application scores are higher.
Above all, the number of wide gaps and narrow gaps were found to be the same in all the three e-learning
applications. These also prove the consistent existence of the effect of these gaps in e-learning application scores.
Furthermore, the divergence of the e-learning applications according to the gaps can be seen in Figure 2. Here, the
teachers who have high performance, unlimited Internet access, who are computer users, and are younger are
observed to have an upper hand with regard to e-learning applications. Furthermore, an independent sample t-test
also demonstrated a significant difference between the groups (p<0.01). These results show that the gaps between the
teachers are really an important problem in the education world. It is an urgent problem that needs the attention of
the educators.
According to the research results (see Table 2), the number of teachers who are: both “e-literate” and have “low
performance” is 14; both “e-literate” and have “high performance” is 286; both “e-literate” and have “limited
189

Internet access” is 99; both “e-literate” and have “unlimited Internet” access is 22; both “e-literate” and are “noncomputer users” is 259; both “e-literate” and “older” is 100; and both “e-literate” and “younger” is 52. On the other
hand; the number of teachers who are: both “not e-literate” and have “low performance” is 0; both “ not e-literate”
and have “high performance” is 98; both “ not e-literate” and have “limited Internet access” is 165; both “not eliterate” and have “unlimited Internet” access is 29; both “ not-literate” and are “non-computer users” is 99; both “
not e-literate” and “older” is 27; and both “ not e-literate” and “younger” is 10.

Low
performance
e-Literate
Not eliterate

14
0

Table 2. Descriptive statistics of Axes of Divergence
High
Limited Unlimited
NonComputer
performance Internet
Internet
Computer
Users
Access
Access
Users
286
70
99
22
259
98
165
29
99
27

Older

Younger

100
222

52
10

Figure 2. Discriminating Teachers
The teachers who have “high performance,” who are “computer users,” who have “limited Internet access,” and who
are “older,” are those “discriminating digital world.” This case is illustrated in Figure 2.
It can be seen from Figure 2 that the low e-learning application score line is higher on the points of low performance,
limited Internet access, and not users of computer, and older. However, the high e-learning application score line is
higher on the points of high performance, unlimited Internet access, computer users, and younger. This proves the
divergence of the digital world of teachers.

Internet Gaps of Teachers
The digital gap between low limited Internet access (M = 66.51, SD = 32.55) and unlimited Internet access (M =
87.50, SD = 31.25) is very much obvious in Table 3. Furthermore, an independent sample t-test also demonstrated a
significant difference between the groups (p<0.01). This shows the divergence among the teachers, with respect to
the limited and unlimited Internet access. Access to knowledge is clearly a fundamental requirement for development

190

(Cockerill & Knols, 2008). Hence, these results must be taken into consideration if a country wants to keep up with
the speed of the digital world.

Internet access
Limited
Unlimited

Table 3. Descriptive statistics of teachers’ Internet access
N
M
SD
Std. Error Mean
205
66.51
66.51
32.55
193
87.50
87.5
31.25

Std. Error Mean
2.27
2.25

Digital Gaps of Teachers
The digital gap between low ICT knowledge (M = 60.74, SD = 21.69) and high ICT knowledge (M = 115.47, SD =
24.52) is evident in Table 4. Furthermore, an independent sample t-test also showed a significant difference between
the groups (p<0.01). This result demonstrates that the digital gap between the teachers is really an important problem
in e-learning applications.
Table 4. Descriptive statistics of ICT-knowledge of teachers
ICT Knowledge Level
N
M
SD
Low
282
60.74
21.69
High
116
115.47
24.52

Std. Error Mean
1.29
2.28

Age Gaps of Teachers
The digital gap between the younger teachers (M = 80.60, SD = 33.37) and high ICT knowledge (M = 59.29, SD =
28.78) is evident in Table 5. Furthermore, an independent sample t-test also demonstrated a significant difference
between the groups (p<0.01). These results show that if the teachers are more experienced, then their digital skills are
limited. This may be because the younger teachers are more motivated, more in favor of ICT, or using more new
technologies in their life.

Age
Younger
Older

Table 5. Descriptive statistics of teachers’ ages in teaching
N
M
SD
325
80.60
33.37
73
59.29
38.78

Std. Error Mean
1.85
3.37

Performance Gaps of Teachers
The digital gap between low (M = 61.29, SD = 20.58) and high ICT knowledge (M = 123.84, SD = 18.13) is evident
in Table 6. In addition, an independent sample t-test also demonstrated a significant difference between the groups
(p<0.01), showing that a divergence exists in the teachers’ world with respect to their low or high performance.
Solutions to problems in the developing world depend on complete and effective collaboration between those
working in the developed and the developing worlds.

Performance Level
Low
High

Table 6. Descriptive statistics of performance levels of teachers
N
M
SD
300
61.29
20.58
98
123.84
18.13

Std. Error Mean
1.19
1.83

Percentages of the Effects of gaps on Teachers’ e-Learning Application Results
When the overall effect of these gaps on the e-learning application scores was calculated (see Figure 3), it was
observed that the access gap (31%) had a greater effect on the e-learning scores. On the other hand, the age gap
(26%) had the second greater effect on the e-learning scores. The third was the digital gap (19%) that had the lowest
effect. Furthermore, several factors at the teachers’ level were observed to influence the implementation of
191

innovative ICT-use in education (Drent & Meelissen, 2008), and the existence of performance gaps, access gaps,
digital gaps, and age gaps cannot be ignored in the digital world.

Figure 3. Percentages of gaps

Conclusions and Suggestions
The aim of this study was to find the divergence of teachers’ digital world in North Cyprus. As the key point of
education is teachers, their divergence in the digital world was explored. For this purpose, the four pathways
(Internet gaps, age gaps, digital gaps, and performance gaps) were explored to determine their occurrence. It was
observed that teachers who have low ICT skills also have low e-learning skills, which were proven to cause low
teacher performance in digital technologies, in which teachers failures in that divergence in the digital world may be
the most possible result (See Figure 5). The teachers must be able to prepare young people for the knowledge society
in which the competency to use ICT to acquire and process information is very important (Ministry of Education,
Culture and Science (MECS, 1999; Plomp et al., 1996).

Figure 4. Circuit diagram of divergence in the world

Teachers are the key personnel in the integration of computers in instructional situations and in the adoption of all
other innovations in schools. They are the key point in narrowing the gaps in education. There was a significant
digital divergence observed among the teachers surveyed, which would adversely affect their ability to prepare
192

students for the knowledge society. This study not only produces significant practical implications on the vocational
high-school teachers’ education in North Cyprus, but also contributes to the current literature related to ICT, elearning, and digital divide. Teacher’s digital world gaps should be taken seriously, not only in North Cyprus, but all
over the world. As there has been no research on the divergence of the digital world of teachers, the current study is a
unique attempt to address its limitations. It is found that there exists divergence among e-learning competences of
teachers. It is found that if a teacher has high ICT performance, than he is e-literate. If a person is e-literate, his ICT
performance level has not been investigated: This is another limitation of this study. Furthermore, an action plan
could be devised for taking precautions in the digital education, comprising strategies for narrowing the Internet gap,
age gap, digital gap, and performance gap. In addition, further investigations may be needed to determine the impact
of gender gaps and economic gaps on the teachers’ digital world.

References
Ahmed, A. (2007). Open access towards bridging the digital divide–policies and strategies for developing countries. Information
Technology for Development, 13(4), 337-361.
Asian Development Bank. (2002). Digital Divide: Determinants and policies with special reference to Asia, ERD Working Paper
Series No 27, Economic and Research Dept., Asian Development Bank, Philippines.
Attewell, P. (2001). The first and second digital divides. Sociology of Education, 74, 252–259.
Becker, H. J., & Ravitz, J. L. (2001). Computer use by teachers: Are Cuban’s predictions correct? Paper presented at the 2001
annual meeting of the American Educational Research Association, Seattle, WA, USA.
Becker, H. J. (2000). Who’s wired and who’s not: Children’s access to and use of computer technology. The Future of Children,
10, 44-75.
Bhanji, Z. (2008). Transnational corporations in education: filling the governance gap through new social norms and market
multilateralism? Globalisation, Societies and Education, 6(1), 55–73.
Chinn, M. & Fairlie, R. (2004). The determinants of the global digital divide: A cross- country analysis of computer and internet
penetration. National Bureau of Economic Research Working Paper, No. 10686, Cambridge: Cambridge University Press.
Clarke, A., Milner, H., Killer, T. and Dixon, G. (2008). Bridging the digital divide. Adults Learning, 20(3), 20-22.
Cobb, L. (2002). Ethics and Reconciliation to Scarcity. The 26th Annual Congress on Metropolisation in a Global Economy, 23-26
June 2002, The Hague,The Netherlands.
Cockerill,M.J., & Knols,B.G.J.(2008). Open Access to research for the developing world. Science and Technology, 24(2), 65-69.
Cole, G. (2005). Bridge over the digital divide. Times Educational Supplement, 9/16/2005, 4652, 4-4.
Dalsgaard, S. (2001). Digital Discourses: An Essay on ICT in Development. Retrieved January 15, 2010, from
http://www.allfreeessays.com/essays/Ict-Essay/4166.html.
Demiralay, R., & Karadeniz, S. (2008). Developing Information Literacy Skills for Lifelong Learning in Elementary Education.
Cypriot Journal of Educational Sciences, 2(6), 89-119
Drent, M., & Meelissen, M.(2008). Which factors obstruct or stimulate teacher educators to use ICT innovatively? Computers &
Education, 51, 187–199
Engelbrecht, J. H.(2008) Internet-based ‘social sharing’ as a new form of global production: The case of ETI@home. Telematics
and Informatics, 25(3), 156-168.
European Centre for Development of Vocational Training (2001). E-Learning and the professional development of Trainers and
Vocational Teachers, Thessaloniki, Greece, Eric Documents: ED473658.
Gunga, S.O. & Ricketts, I.W. (2007). Facing the challenges of e-learning initiatives in African universities. British Journal of
Educational Technology, 38(5), 896-906.
Hargittai, E. (2003). The digital divide and what to do about it. In Jones D. (Ed.), New Economy Handbook (pp. 821-839), San
Diego: Elsevier/Academic Press.
Hawkins, B. L., & Oblinger, D. G. (2006). The myth about the digital divide. Educause Review, 41(4), 12–13.
James, J. (2000). Pro-Poor Modes of Technical Integration into the Global Economy. Development and Change, 31, 765-783.
Kim, M. C., & Kim, J. K. (2001). Digital divide: Conceptual discussions and prospect. Human Society and the Internet. Lecture
Notes in Computer Science, 2105, 78–91.
Kong, S.C. (2008). A curriculum framework for implementing information technology in school education to foster information
literacy. Computers and Education, 51(1), 129-141.
193

Korat, O., Bachar, E., & Snapir, M. (2003), Functional, social and cognitive aspects in emergent literacy: Relations to SES and to
reading-writing acquisition in first grade, Megamoth, 42, 195–218.
Kozma, R., McGhee, R., Quellmalz, E., & Zalles, D. (2004). Closing the digital divide: Evaluation of the World Links Program.
International Journal of Educational Development 24, 361–381
Lim, C. (2005). Development and effects of a Learning Management System for supporting self-regulated learning. Journal of
Educational Technology, 21(4), 77-100.
Main, L. (2001). The Global Information Infrastructure: Empowerment or Imperialism? Through new social norms and market
multilateralism? Globalisation, Societies and Education, 6(1), p.55–73.
Mairtin, O.F. (2001) E-learning and Access: Some Issues and Implications. Retrieved 15 January, 2010, from
http://www.bath.ac.uk/iohm/fathaighr.rft.
Malloch, B. (2000).Commentary: The Internet and Development Choices. The Third World Quarterly, 22(1), 83-97.
Manette, G. (2004). Leadership, money could carry college across the digitaldivide. Tribal College Journal, 15(4), p.26-27.
Ministry of Education & Culture and Science (1999). Maatwerk voor Morgen: het perspectief van een open arbeidsmarkt
[Adjustments for tomorrow, perspectives on an open job market]. Retrieved 15 January, 2010, from
http://www.minocw.nl/werkinonderwijs/publicaties.html.
Munoz, J. S. (2002). [Dis] Integrating Multiculturalism with Technology. Multicultural Education, 10, 19-24.
National Office for the Information Economy (NOIE) (2000) Access and Equity, Commonwealth of Australia. Retrieved January
20, 2010 from http://www.noie.gov.au.
Noce, A.A., & McKeown, L. (2008). A new benchmark for Internet use: A logistic modelling of factors influencing Internet use in
Canada, 2005. Government Information Quarterly, 25(3), 462-476.
Norris, P. (2001). Digital divide: Civic engagement, information poverty, and the Internet worldwide. New York: Cambridge
University Press.
OECD (2007). Broadband and ICT access and use by households and individuals. Paris: OECD Publications.
OECD (2001). Understanding the digital divide, Paris: OECD Publications.
Papastergiou, M., & Solomonidou, C. (2005). Gender issues in Internet access and favorite Internet activities among Greek high
school pupils inside and outside school. Computers and Education, 44, 377–393.
Park, S.H., & Ertmer, P.A. (2008) Examining barriers in technology-enhanced problem-based learning: Using a performance
support systems approach. British Journal of Educational Technology, 39(4), p.631-643.
Pearson, T. (2002). Falling behind: A technology crisis facing minority students. Tech Trends, 46, 15-20.
Peterson, M. (2008). Maps and the Internet. What a mess it is and how to fix it. Cartographic Perspectives 59, 4-11.
Piskurich, G. M. (2003). Preparing Learners for e-LearnING, Danvers, CA: Pfeiffer.
Plomp, Tj., ten Brummelhuis, A. C. A., & Rapmund, R. (1996). Teaching and learning for the future. Report of the Committee on
MultiMedia in Teacher Training (COMMITT), Den Haag: SDU.
Prevost, A.K., & Schaffner, B.F. (2008). Digital Divide or just another absentee ballot?: Evaluating internet voting in the 2004
Michigan democratic primary. American Politics, 36(4), 510-529.
Redd, T.M. (2003). Tryin to make a dolla out a fifteen cent: Teaching Composition with the Internet at an HBCU. Computers and
Composition, 20, 359–373.
Rice, R.E., & Katz, J.E (2008). Assessing new cell phone text and video services. Telecommunications Policy, 32(7), 455-467.
Singh, A.K., & Sahu, R.(2008). Integrating Internet, telephones, and all centres fro delivering better quality e-governance for all
citizens. Government Information Quarterly, 25(3), 477-490.
Souter, D. (2007). Internet governance and development: Another digital divide? Information Policy: The International Journal of
Government & Democracy in the Information Age, 12(1/2), 29-38.
Tezer, M., & Bicen, H. (2008). The Preparations University Teachers towards E-Education Systems. Cypriot Journal of
Educational Sciences, 3(5), 16-27.
UNDP (1999). New technologies and the global race for knowledge. In Human Development Report 1999, UNDP, 57-76.
Uzunboylu, H. (2006). A Review of Two Mainline E-Learning Projects in the European. Educational Technology Research and
Development, 54(2), 201-209.
Vogelwiesche, U., Grob, A., &Winkler, B. (2006). Improving computer skills of socially disadvantaged adolescents: Same-age
versus cross-age tutoring. Learning and Instruction, 16(3), 241-255.
Youssef, A.B. (2004). Four dimensions of the digital divide (in French), Réseaux, 128, 181-209.

194

Yang, J. C., & Lin, Y. L. (2010). Development and Evaluation of an Interactive Mobile Learning Environment with Shared
Display Groupware. Educational Technology & Society, 13 (1), 195–207.

Development and Evaluation of an Interactive Mobile Learning Environment
with Shared Display Groupware
Jie Chi Yang and Yi Lung Lin
Graduate Institute of Network Learning Technology, National Central University, Jhongli City 320, Taiwan //
yang@cl.ncu.edu.tw // nigel@cl.ncu.edu.tw
ABSTRACT
When using mobile devices in support of learning activities, students gain mobility, but problems arise when
group members share information. The small size of the mobile device screen becomes problematic when it is
being used by two or more students to share and exchange information. This problem affects interactions among
group members. To overcome the information sharing problem, the concept of Shared Display Groupware
(SDG) has been proposed to support face-to-face collaboration using a shared display. However, little attention
had been paid on the integration of the shared display with mobile devices in order to design a learning activity
in the mobile learning environment. In this study, a learning activity was designed and a mobile learning
environment was developed with the integration of the SDG to permit students to share information from
individual and public spaces. During the learning activity, each student performed individual tasks using a PDA.
Group tasks, following the individual tasks, were performed using a shared display, thus facilitating the sharing
of information and group discussions. Each group was given their own display to share. To evaluate students’
perceptions and learning effectiveness regarding the use of the SDG in supporting mobile learning, an empirical
study was conducted. The study included a survey questionnaire as well as a learning achievement test. The
participants in the experiment included thirty-four fourth-grade students and followed a one-group pretestposttest design. The results show that the participants evaluated high scores in every category of the
questionnaire. Significant differences were found between pretest and posttest in most aspects of the learning
achievement test on the creation of conditions for classifying plants.

Keywords
Shared Display Groupware, Single Display Groupware, Mobile learning, One-to-one technology enhanced learning,
Mobile devices, Science learning, Plant classification, Interactive learning environments

Introduction
Various mobile devices have been used in mobile learning, such as wrist-worn devices, mobile phones, handheld
computers, web pads, pen tablet computers and laptop computers (Sharples & Beale, 2003). Many studies have
reported achievements in the investigation of learning interests and the effectiveness of mobile learning (Rieger &
Gay, 1997; Roschelle, 2003; Tatar, Roschelle, Vahey & Penuel, 2003; Zurita & Nussbaum, 2004). For an effective
integration of mobile learning into a digital classroom environment, it is important for all students in a group to have
their own computing device equipped with wireless communication capability to conduct learning tasks (Chan et al.,
2006; Liang et al., 2005; Soloway et al., 2001). However, through the observation of learning activities with students
using mobile devices for collaborative learning, some problems exist. For example, the experience of using Personal
Digital Assistants (PDAs) as learning tools shows that it was difficult while sharing information with group members
using mobile devices during the learning activity. When learners discuss and share information in a collaborative
learning setting using mobile devices such as PDAs, screen size may have a negative influence on the learning
activity due to the limitation of the screen size of the PDA (Magerkurth & Tandler, 2002). It is difficult to let all
group members look at the same screen on a PDA simultaneously. To help other group members get access to the
information simultaneously during the discussion, the learner may interrupt the task which still in progress, wait for
other group members until they finished watching, and then continue the learner’s own original task. Therefore, a
shared display is needed, which group members can access, thus creating a common focus, facilitating group
discussion and sharing of information without interruption.
The concept for supporting collaborative work between people via a shared computer with a single shared display
was proposed by Stewart, Bederson, & Druin (1999). Subsequently, many studies followed this concept to develop
environments or tools which support face-to-face collaboration, working with a shared display, in which all
participants have their own input device (Ryall, Forlines, Shen, & Morris, 2004; Tse & Greenberg, 2004; Zanella &
Greenberg, 2001). Most of the above-mentioned studies focused on the issues of the system interface design, such as
the design of the screen size, transparent interface components, or support of multiple input devices. However, little

ISSN 1436-4522 (online) and 1176-3647 (print). © International Forum of Educational Technology & Society (IFETS). The authors and the forum jointly retain the
copyright of the articles. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that copies bear the full citation on the first page. Copyrights for components of this work owned by
others than IFETS must be honoured. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior
specific permission and/or a fee. Request permissions from the editors at kinshuk@ieee.org.

195

attention focused on the integration of the shared display with mobile devices in order to design a learning activity in
the mobile learning environment.
The aim of this study, based on the above considerations, was three-fold. Firstly, a learning activity was designed
with the integration of the concept of Shared Display Groupware (SDG) to support mobile learning. Secondly, a
mobile learning environment was developed under the design of a learning activity. The learning environment
permits students to share and exchange information from individual and public spaces and allows each group
member to continue with individual tasks in this environment without interruption of other simultaneous group tasks.
The learning activity was integrated into a course based on the observation of plants which includes descriptions of
plants and the creation of conditions for the classification of plants through the support of the SDG together with
mobile devices. Finally, an empirical study was conducted to evaluate students’ overall perceptions and the learning
effectiveness on the use of the SDG in the support of mobile learning.

Literature review
Mobile learning
With the rapid development of network communication technologies, more and more wireless and mobile
technology applications are integrated into classrooms to support teaching and learning. A study integrated wireless
network technology with mobile devices including an e-whiteboard to build a Wireless Technology Enhanced
Classroom (WiTEC) which provides various features for supporting teaching and learning in the classroom (Liu et
al., 2003). For example, the feature reducing the time for tedious work allows the teacher to readily select materials
and present or broadcast them to students, as well as mark and revise students’ tasks through the use of e-whiteboard
and mobile devices that enable numerous tedious tasks to be completed instantly. Another feature is engaging
students in learning activities which indicates students engaging in learning activities such as exploring and
organizing online course-related resources, as well as answering quizzes by means of group discussion using their
mobile devices. In addition, students are not only able to discuss course work with each other face-to-face, but are
also able to exchange personal materials through the mobile devices and the process of interaction can be recorded,
thus facilitating group collaborative learning. Another feature is empowering the teacher to monitor students’
learning progress which means a number-signal is provided to each mobile device that represents different statuses
in using a mobile device such as disconnected to the server or request for help, therefore the teacher can monitor
students’ learning progress and determine how to implement the subsequent activities. Moreover, the recording
teaching and learning processes as portfolios feature means the establishment of teaching records and learning
portfolios and then promotes teachers’ reflection on teaching as well as learners’ portfolio assessment. Finally, the
user-friendly interface by providing a handwriting function and the interactive classroom server effectively
coordinate all system works to allow the teacher and students benefit from technology easily for implementing
technology-supported activities smoothly.
In a Mobile Computer Supported Collaborative Learning activity, students engaged in collaborative learning through
face-to-face communication on a social network with the support of handheld devices by a wireless network (Cortez
et al., 2004; Zurita & Nussbaum, 2004). Many studies have demonstrated successful experiments which help
students exchange information through PDAs as well as providing opportunities to interact with each other by using
the PDAs as handheld devices for supporting learning. For example, ad hoc networks and mobile classrooms using
PDAs in a wireless environment (Chang, Sheu & Chan, 2003), mobile learning systems for supporting outdoor
learning about bird and butterfly watching (Chen, Kao & Sheu, 2003; Chen et al., 2004), and improving knowledge
creation during experiential learning by mobile technologies (Lai, Yang, Chen, Ho & Chan, 2007).
Although the above-mentioned studies demonstrate positive results on the integration of mobile devices in support of
collaborative learning, it is difficult to share information through mobile devices for group discussion. When two
learners discuss and share information in a collaborative learning setting, they may not encounter any problems as
they can transmit data using the built-in infrared light. However, this does not apply to multi-user transmissions
among groups with three or more members (Danesh, Inkpen, Lau, Shu & Booth, 2001). In addition, students usually
adopt an attitude of watching information displayed on the horizontal PDA which held by sharing the screen with
other students who stand shoulder to shoulder or stand facing from the opposite direction (Chang, Sheu & Chan,
2003; Chen et al., 2004; Danesh et al., 2001). Students can also reverse the PDA screen to point the screen toward to
196

the people who wants to see. Although the above-mentioned solutions can solve the problem of difficulties in sharing
information on mobile devices, the action of information sharing may interfere with the learner’s original task. While
the learner is sharing information via his mobile device, showing data to group members for discussion for example,
he is unable to use his own mobile device to continue the task and has to wait until the process has been completed.
Limitations due to the small screen size of mobile devices, make it necessary to provide group members with a
shared display to facilitate group discussion with a common focus.

Shared Display Groupware
In the development of SDG, originally SDG meant Single Display Groupware, where the purpose was to design a
computer system that can support face-to-face collaborative interactions on a single display (Stewart, Bederson &
Druin, 1999). Afterwards, the definition of SDG was extended to denote Shared Display Groupware (Ryall et al.,
2004). Because of the popularization of the hardware, a class can be divided into several groups, each of which has
its own SDG. The SDG can be seen as a groupware system, where one screen receives input from multiple devices,
such as the mouse and keyboard, or adopts a touch screen environment to permit multi-user operation concurrently
(Nicol & MacLeod, 2005; Scott, Mandryk & Inkpen, 2003). Another extended application is a multi-screen
environment, which is called Distributed Display Environments (DDEs) (Inkpen & Mandryk, 2005). DDEs can be
defined as one computer system which can have more than one physical display, such as applications of multiplemonitor desktop systems, rooms with networked projectors and displays, or campus-wide connected information
display systems (Hutchings, Stasko, & Czerwinski, 2005).
In previous studies on the SDG, task arrangement was only available for the group’s task, thus that all group
members are working at the same time through the SDG to complete the common task of the group (Myers, Stiel &
Gargiulo, 1998; Scott, Grant & Mandryk, 2003). Even though both the individual task and the group task were taken
into consideration, the design was that after the individual task was completed in the private space of the SDG, the
learner then worked in the public space of the SDG on the group task (Baudisch, Good, Bellotti, & Schraedley,
2002). This design is limited because the individual task affects the group task, as they are working on the same
display simultaneously. Thus, the individual task was not genuinely performed on a personal device. The mobile
device is better complemented with the SDG, thus the individual tasks should be completed on the mobile device,
and the related group tasks should be completed on the SDG. Solutions to this problem were investigated to find out
how people move from individual to group work through the use of both mobile devices and a shared public display
(Greenberg, Boyle, & LaBerge, 1999; Kitamura, Osawa, Yamaguchi, Takemura, & Kishino, 2005; Liu & Kao,
2007). It indicated that users not only use shared display space as public space, but also they have their own personal
display space as individual space. Therefore, such a system supports both group tasks such as discussions and
individual tasks such as personal operations. Consequently, the individual tasks should not interfere with the group
tasks and then could co-exist.

Phase
1

2

Step
1
2
3
4
5
6
7

Table 1. Learning activity design
Learning activity
Grouping and login
Get clues
Investigate and collect
Compare and integrate clues
Second phase grouping
Create conditions for classification
Demonstrate results of classification

Learning activity design and system implementation
Design of learning activity
This study adopted a two-phase learning activity design. The first phase focuses on the observation of plants, in
which students have to investigate various plants through characteristics of plants that assigned as clues to students.
The second phase focuses on the classification of plants, in which students have to create specific conditions and
197

then use those conditions to classify the plants. There are seven steps in the two phases, as shown in Table 1. The
group members in the second phase differ from those in the first phase, which were formed by dividing original
group members into newly formed groups.

Individual
login message

Login messages
pertaining to all
students

Figure 1. Screenshots of the login on the PDA (left), and the login messages on the SDG (right)
During the learning activity, there are two types of tasks – individual tasks and group tasks. The individual task is
mainly performed on each student’s PDA. Students work on their own PDA to investigate plants that matched the
assigned clues. The group task is performed on the SDG. Group members work together sharing information or
discussing assigned tasks. The individual task is also performed on the SDG because the SDG is designed as a tool to
promote information sharing and discussion but not limited as to interrupt individual tasks. Detailed explanations for
each step of the learning activity are described as follows:
 Step 1: Grouping and login
Individual task: Students login to the system using their PDAs (left side of Figure 1).
Group task: Students find out their login messages on the SDG to confirm which group they belong to (right
side of Figure 1), and meet all members in their group.

Recording number
Location of the plant
Name of the plant
A clue is assigned
to each student

Photo of the
observed plant
Note on the
observed plant

Figure 2. Screenshots of received clues (left), and the taking of photos and notes on the PDA (right)




Step 2: Get clues
Individual task: All students get individual clues on their PDAs (left side of Figure 2). Clues were created from
different parts of plants for representing the characteristics of plants, such as “spiral shaped leaves” or “white
flowers”.
Group task: Every clue is different for all group members. The information of the clues of the group is shown on
the SDG.
Step 3: Investigate and collect

198



Individual task: Students use their clues to investigate and observe different characteristics of plants. During the
process, students take photos of the plants that fit the description found in the clues. They also take notes on
their PDAs about the plants, as well as the locations and names of the plants observed (right side of Figure 2).
Group task: Group members collect information about various plants according to their clues. The task is
achieved by either individually or collaboratively working with group members.
Steps 4: Compare and integrate clues
Individual task: Students use their PDAs to interact with the group’s SDG to share individual collected data on
the shared display through the wireless network.
Group task: The left side of Figure 3 shows a screenshot of the SDG for comparing clues. The six blocks on the
two sides represent individual spaces of six students in a group, whereas the two blocks in the middle represent
public spaces of the group. Students can look all clues of their group members on the SDG. They can compare
these clues with their collected data (annotation 1 and 2), and select one answer for uploading to the public space
(annotation 3).Group members can explain their views on their answers to other group members for further
discussion. They can also refer to materials on various plants (annotation 4). Group members should agree upon
the final answer of the group by using the “vote function” which is provided in the system (right side of Figure
3, annotation 5).

Figure 3. Screenshots of clue comparisons (left), and voting (right) on the SDG
(1) The individual space displays collected data with enlarged photo
(2) The individual space displays collected data with photo and notation on the plant
(3) The public space displays students’ answers with clues and photos
(4) Referred data of plants
(5) The vote function for group decisions






Steps 5: Second phase grouping
Individual task: During the second phase, new groups were formed, different from the group of the first phase.
Thus, each group member has the all clues of the plants from the first phase.
Group task: The task in the second phase is prompted in the SDG environment on completion of the second
phase grouping.
Step 6: Create conditions for classification
Individual task: Group members explain details of the plants that they are familiar with to their new group
members. On completion of the tasks in the first phase, the students have a certain degree of understanding of all
clues that they possess. Therefore, students can filter and choose clues that help the classification of plants
which are worth sharing to all group members in creating conditions for classifying plants.
Group task: Group members should use the characteristics of plants to collaboratively create conditions for the
classification of plants (left side of Figure 4), and use these created conditions to classify plants by using the
method of Linnaeus’ Binominal Nomenclature (Linnaeus, 1753) (middle and right sides of Figure 4, annotation
1 and 2). These plants are the target plants from the first phase. Group members should also vote on the created
conditions, and determine which condition is better for classifying plants.
Step 7: Demonstrate results of classification
Individual task: All students explain why they created the conditions for classifying the plants.
199

Group task: Students view the achievements made by other groups (Figure 5), as well as listening to teacher
comments.
Create conditions
for
classifying
plants

Classify plants

Figure 4. Screenshots of creating conditions for the classification of plants on the PDA (left), classifying plants on
the PDA (middle), and classifying plants on the SDG (right)
(1) Individual space displays created conditions for classifying plants
(2) Public space displays the progress of classifying plants using the created conditions

The public space displays the
achievement of classifying plants
using the created conditions
Figure 5. Completion of classifying plants
System implementation
The system can be divided into three parts from the viewpoint of the different hardware used: the server, the SDG,
and the client (PDA). A system diagram is shown in Figure 6. The system functional modules are developed
according to the three parts of the system. The server consists of a login module and a learning portfolio module. The
SDG consists of a command receiving module and a transfer module. The client consists of a transfer module and a
learning flow control module.
200

Figure 6. System diagram
Login module
The login module includes two functions: the login judgment and user information response functions. When the
server receives a login request from a client, it checks the authorization for login to the system. If the authorization is
approved, then user information containing the students’ clues, group information, and IP address of the shared
display will be transferred to the client. The user information can be used for indentifying groups for transmitting
data among the server, clients and SDGs.

Learning portfolio module
The learning portfolio module includes two functions: an individual learning portfolio, and a group learning
portfolio. For example, the students’ notes in the first phase and created conditions for classifying plants in the
second phase are recorded as an individual learning portfolio, whereas the answers of group decision in the first
phase and the results of plant classifications in the second phase are recorded as a group learning portfolio.

Command receiving module
The command receiving module includes three functions: command receiving and responding, command
interpretation, and command execution. The command receiving and responding function is responsible for receiving
commands from clients, such as commands for enlarging photos, displaying notes, and voting for group decisions.
201

The types of commands are judged by the command interpretation function. Corresponding functions are executed
on the SDG after receiving commands from clients by the command execution function.

Transfer module
The transfer module belongs to both the SDG and the client, and includes three functions: command transfer, user
information receiving, and learning portfolio transfer functions. The command transfer function is responsible for the
transmission of commands to the SDG from clients, such as uploading answers to the SDG. The user information
receiving functions are responsible for receiving information from the server to the client, and from the client to the
SDG, such as students’ clues and group information. The learning portfolio transfer function is responsible for
transferring the learning portfolio from SDGs to the server which contains individual and group learning portfolios.

Learning flow control module
The Learning flow control module includes two functions: prompting instructions for the learning activity, and
judging the completeness of tasks. Clients are prompted with instructions on the tasks for each step of the learning
activity. Students can refer to these instructions to complete the tasks. When a student has completed a task, this
module is responsible for judging whether the student has completed the task well enough to begin the next step. If
the task has not been satisfactorily completed, the student may not proceed to the next step.

The study
Based on the learning activity design with the support of the SDG and mobile devices, the objective of the empirical
study was conducted to examine students’ learning effectiveness on plant observations in terms of descriptions of
plants and the creation of conditions for the classification of plants. Students’ perceptions on the use of the SDG in
the learning activity were also evaluated through a questionnaire survey.

Methods
The subjects of this study were fourth-grade students from an elementary school located in northern Taiwan. There
were 34 subjects – 18 boys and 16 girls who participated in the study. Equipment used in the study included the
PDAs, embedded mobile cameras and laptop computers as SDGs. The elementary school classroom was not
equipped with any large screens for the participant groups to use, thus the screen of a laptop computer was used as
the SDG.
Taking the subjects’ former experience on PDA, mobile camera, and SDG environment usage into account, some
training courses were given to the participants before the main experiment took place to avoid the “novelty effect”
while facing a new form of media (Clark, 1983, 1994). The subjects were trained on the basic system functions for
operating the PDA, mobile camera, as well as how to operate the PDA within the SDG environment. They were also
familiarized with collaboration skills in the SDG environment. After the training courses, the main experiment took
place and was comprised of four sessions, in total 180 minutes of the learning activity.
The instruments used in this study included a questionnaire and a learning achievement test. The questionnaire was
designed to comprise nine categories, which included SDG function, reference materials, system message response,
SDG equipment, collaboration, clue design, plant classification, learning attitude, and impression of the learning
activity. The questionnaire consisted of 33 questions. Each question was evaluated on a five-point Likert scale (5
indicating strong agreement and 1 indicating strong disagreement).
The learning achievement test was conducted to examine the student’s understanding of the description of plants and
the creation of conditions for classifying plants through the support of the SDG with mobile devices. The test
questions were designed by referring to the teacher’s manual for the “Natural and Living Technology” course. The
type of test questions consisted of two parts, being the descriptions of the plants and the creation of conditions for
202

classifying the plants. The test on descriptions of the plants asked students to describe plants which cannot be found
on the campus. Students had five minutes to answer the test. The test on the creation of conditions for classifying
plants asked students to create conditions for classifying plants and classify the plants by Linnaean Binomial
Nomenclature. Students had ten minutes to answer the test. The learning achievement test was designed as pretest
and posttest, and the test questions used in the posttest were the same as in the pretest.
In the test on the descriptions of plants, the students answers were evaluated from four aspects, which included the
use of the plant’s six parts (roots, stems, leaves, flowers, fruits, and seeds), adjective words (for describing the
appearance of plants such as color, size, quantity, shape), perceptual words (depending on personal affinity, for
example, beautiful, ugly, good and bad), and the number of answered words (the total number of words in the
answers). In the test on the creation of conditions for classifying plants, students answers were evaluated on five
aspects, which included the use of the plant’s parts, adjective words, perceptual words, successful classification
(clearly classifying the plant into the A class and the non-A class successfully), and the number of created conditions
for classifying plants.

Results of the questionnaire
Thirty-one valid copies of the questionnaire were collected after the experiment. Table 2 shows the results of the
questionnaire with the mean scores for all questions in the nine categories. The results show that students evaluated
high scores in each category of the questionnaire.
Table 2. Results of the questionnaire
Questions
SDG function
Sharing information with group members through the SDG was convenient.
Sharing photos and notes on the SDG facilitated group discussion.
The enlarged photos were clearly seen on the SDG.
Group discussions were aided through the individual and public spaces on the SDG.
The vote function on the SDG was useful for forming group decisions and agreements.
Reference materials
It was helpful to find the target plant by referring the materials on the SDG.
I referred to materials on the SDG to look up plants information during group discussion.
I referred to materials on the SDG for proof of my explanation to group members.
I referred to materials on the SDG to contradict group members’ ideas.
System message response
I knew how to get to the next step by looking the system messages.
I understood the meaning of the system messages.
I carefully read the system messages.
SDG equipment
I think the screen size of the SDG was large enough for viewing information.
I can clearly watch information on the SDG.
Collaboration
I understood the group members’ explanation during the process of group discussion.
I could work on my individual task without interrupting the process of the group task.
Group discussions were facilitated with a common focus on the SDG.
I collaborated with group members to observe plants using different clues.
I collaborated with group members to create and discuss conditions for classifying plants.
Clue design
I found many plants by using the assigned clue.
I collected plant information which matched the assigned clue.
The clue was clearly described for finding plants that matched the clue.
Plant classification
The clue used in the first phase was helpful for creating conditions for the classification of plants.
Group members working together to create conditions for classifying plants was meaningful.

M
4.32
4.32
4.29
4.16
4.29
4.52
4.23
4.65
4.42
4.26
3.58
4.37
4.10
4.48
4.52
3.80
4.23
3.36
4.19
4.16
4.10
4.42
4.03
4.26
4.20
4.42
4.32
3.87
4.08
4.48
4.00
203

Classifying plants with group members was an easy task.
Learning attitude
Group discussions were interesting through the use of the SDG.
Controlling both the PDA and the SDG were convenient.
The learning activity was fun like a game with clues to find plants.
I carefully read group members’ data on the SDG.
Impression of the learning activity
I am willing to participate in the learning activity again.
I am willing to use the SDG again to share information with group members.
I am willing to use the SDG again to discuss information with group members.
I am willing to use the SDG again to create more conditions for classifying plants.

3.77
4.38
4.52
4.25
4.51
4.25
4.29
4.28
4.23
4.42
4.23

The results of the questionnaire show that SDG functions, such as information sharing, photos enlargement,
individual and public spaces, and vote function were highly rated by the students. Students also positively rated the
use of reference materials as well as the system message responses. Although the screen size of the laptop computer
was small, most of the students stated that the screen size of the laptop computer was large enough for viewing
information. In regards to collaboration supported by the SDG, students agreed that the SDG supported information
sharing and group discussion, which they shared on the SDG including items such as photos and notes on the
description of plants, as well as conditions for classifying plants from other group members. During the discussion,
students did not interrupt the process of sharing information with group members while they completed individual
tasks. In addition, students also highly rated the design of the clues and plant classifications, even though some
students did not match plants with the clues or correctly classify plants with their group members. Moreover, the
findings on students’ learning attitude and impressions of the learning activity show that most students agreed that
the learning activity is designed with the support of the SDG, and were willing to participate in the learning activity
again.

Results of learning achievement test
Description of plants
In comparing the difference between the pretest and posttest, only a valid sample of twenty-seven students, who
participated in both the pretest and posttest were used in the final analysis. A paired-samples t test was the statistical
method adopted for use in the study. Table 3 shows the results of the test for the descriptions of plants. The results
show that there was no significant difference between the pretest and posttest in all aspects of the test on the
descriptions of plants.
Table 3. Results of the test on the descriptions of plants
Aspect
Average
pretest
posttest
Number of answered words
22.46
23.44
Use of plant’s parts
2.83
2.74
Adjective words
3.42
3.56
Perceptual words
0.70
0.52

p-value
.721
.183
.185
.449

Creation of conditions for classifying plants
To compare the difference between the pretest and the posttest, only the valid sample of twenty-five students whose
answers appeared in both the pretest and posttest were used in the analysis. A paired-samples t test was the statistical
method adopted for use in the study. Table 4 shows the results of the test on the creation of conditions for classifying
plants.
The results show that there were significant differences between pretest and posttest in most aspects of the test on the
creation of conditions for classifying plants, except for the successful classification aspect. The number of created
conditions improved from 4.20 (84%) in the pretest to 4.92 (98%) in the posttest, showing a significant difference
204

(p=0.023). The number of plant’s parts used in creating conditions had also greatly improved, from 1.80 (43%) in the
pretest to 3.12 (63%) in the posttest, showing a significant difference (p=0.000). Regarding the number of adjective
words used to create conditions for classifying plants, the number had increased from 1.76 (42%) in the pretest to
2.60 (53%) in the posttest, showing a significant difference (p=0.002). In contrast, the number of perceptual words
used decreased from 1.04 (25%) in the pretest to 0.72 (13%) in the posttest, showing a significant difference
(p=0.043).
Table 4. Results of the test on creation of conditions for classifying plants
Aspect
Average
pretest
posttest
Created conditions
4.20
4.92
Use of plant’s parts
1.80
3.12
Adjective words
1.76
2.60
Perceptual words
1.04
0.72
Successful classification
1.12
1.48
*: p<0.05; **: p<0.01; ***: p<0.001

p-value
0.023*
0.000***
0.002**
0.043*
0.095

Discussions
The results of the questionnaire show that most students rated high scores in each category. Although some questions
of the questionnaire were rated a little lower than the rest of the questions, they were still higher than the average
score. The question that got the lowest score is the question asking students if they can clearly look at the
information on the SDG in the category of “SDG equipment”. A reasonable explanation for this result could be
found from the observation of students’ behaviors during the learning activity. Some students answered that they
cannot clearly see information from certain viewpoints due to light reflections. However, it was found that students
could easily adjust the position of the laptop to give them a better view of the screen. This indicates that the screen of
laptop computer could be adapted as the SDG in the absence of a large-scale screen. This also demonstrates the
feasibility of integrating the SDG into the classroom.
From the results of the test on the descriptions of plants, there was no significant difference between pretest and
posttest in all aspects of the test. One of the possible reasons may be that the students only observed three kinds of
parts of plants, such as the stems, leaves, and flowers. The other three parts of the plants (roots, fruits and seeds)
were not observed due to limited time in the experiment. Therefore, students answered with few words in the test
because they did not observe many parts of plants, causing no significant difference between pretest and posttest.
From the results of the test on the creation of conditions for classifying plants, there were significant differences
between pretest and posttest in most aspects of the test. This indicates that students had improved their abilities to
create conditions for classifying plants, and they became familiar with the plant’s parts during the learning activity
for observing plants and creating conditions for classifying plants. From a comparison of the pretest and the posttest,
the increase in the amount of adjective words used and the reduction of subjective perceptual words used, reveal that
students have learned a better way of classifying plants.

Conclusion
In this study, the authors integrated the Shared Display Groupware (SDG) concept with the use of mobile devices to
design a learning activity and develop a mobile learning environment. The two-phase seven-step learning activity
was designed to examine the characteristics of plants by means of observation and classification of plants. The SDG
supports the simultaneous sharing and discussing of information among group members within the mobile learning
environment. The system also supports the concurrent use of all individual and group tasks. Each task may be carried
out without the interruption to any other ongoing tasks.
The results of the empirical study demonstrated that students positively evaluated on the questionnaire. For example,
students reacted positively when surveyed on the convenience of the SDG functions for sharing information and
creating a common focus during group discussions. Most students answered that they were willing to use the SDG
205

again. In addition, the results of the learning achievement test demonstrated that students had improved their abilities
to create conditions for classifying plants through the support of the SDG together with mobile devices under the
learning activity design.
The empirical study described in this paper was only a small-scale study with sample consisting of one class and a
one-group pretest-posttest design. This limitation may have influenced the validity of the results. Therefore, it is
suggested that further studies be undertaken with a large sample, under a two-group experimental design to verify the
results described in this paper. Another limitation is that the period of the study was short, which caused a lack time
to observe plants and create sufficient conditions for the classification of plants. A significant amount of time was
needed for students to familiarize themselves with operations of the SDG in group discussions. Therefore, a longterm experiment is required for examining students’ learning effectiveness by using the SDG for the support of the
learning activity. In addition, students’ comments may also be gathered and analyzed for the improvement of the
interface design of the SDG creating a more flexible design for use.

Acknowledgements
The authors would like to thank Mr. Aubrey Neil Leveridge for proofreading on an earlier version of this paper. The
authors would also like to thank Mr. Ting Yen Lin for assisting in the system development. In addition, the authors
wish to thank all the subjects who participated and cooperated in the experiment. This study was partially supported
by grants (NSC 97-2628-S-008-001-MY3, NSC 97-2631-S-008-002) from the National Science Council of Taiwan.

References
Baudisch, P., Good, N., Bellotti, V., & Schraedley, P. (2002). Keeping things in context: A comparative evaluation of focus plus
context screens, overviews, and zooming. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems.
Minneapolis, Minnesota, USA. 259-266.
Chan, T. W., Roschelle, J., His, S., Kinshuk, Sharples, M., Brown, T., Patton, C., Cherniavsky, J., Pea, R., Norris, C., Soloway,
E., Balacheff, N., Scardamalia, M., Dillenbourg, P., Looi, C. K., Milrad, M., & Hoope, U. (2006). One-to-one technology
enhanced learning: an opportunity for global research collaboration. Research and Practice in Technology Enhanced Learning,
1(1), 3-29.
Chang, C. Y., Sheu, J. P., & Chan, T. W. (2003). Concept and design of Ad Hoc and Mobile classrooms. Journal of Computer
Assisted Learning, 19(3), 336-346.
Chen Y. S., Kao T. C., & Sheu J. P. (2003). A mobile learning system for scaffolding bird watching learning. Journal of
Computer Assisted Learning, 19(3), 347-359.
Chen, Y. S., Kao, T. C., Yu, G. J., & Sheu, J. P. (2004). A Mobile Butterfly-Watching Learning System for Supporting
Independent Learning. Proceedings of IEEE International Workshop on Wireless and Mobile Technologies in Education (WMTE
2004). Jhongli, Taiwan. 11-18.
Clark, R. (1983). Reconsidering research on learning from media. Review of Educational Research, 53(4), 445-459.
Clark, R. (1994). Media will never influence learning. Educational Technology Research and Development, 42(2), 21-29.
Cortez, C., Nussbaum, M., Santelices, R. A., Rodríguez, P., Zurita, G., Correa, M., Cautivo, R. (2004). Teaching Science with
Mobile Computer Supported Collaborative Learning (MCSCL). Proceedings of IEEE International Workshop on Wireless and
Mobile Technologies in Education (WMTE 2004). Jhongli, Taiwan. 67-74.
Danesh, A., Inkpen, K., Lau, F., Shu, K., & Booth. K. S. (2001). Geney: Designing a collaborative activity for the Palm handheld
computer. Proceedings of the ACM CHI 2001 Conference on Human Factors in Computing Systems. Seattle, Washington, USA.
388-395.
Greenberg, S., Boyle, M., & LaBerge, J. (1999). PDAs and Shared Public Displays: Making Personal Information Public, and
Public Information Personal. Personal Technologies, 3(1), 54-64.
Hutchings, D. R., Stasko, J., & Czerwinski, M. (2005). Distributed display environments. Interactions, 12(6), 50-53.
Inkpen, K., & Mandryk, R. L. (2005). Multi-Display Environment for Co-located Collaboration. Proceedings of the ACM CHI
2005 Workshop on Distributed Display Environments. Portland, Oregon, USA.
206

Kitamura, Y., Osawa, W., Yamaguchi, T., Takemura, H., & Kishino, F. (2005). A display table for strategic collaboration
preserving private and public information. Lecture Notes in Computer Science: Entertainment Computing, 3711, 167-179.
Lai, C. H., Yang, J. C., Chen, F. C., Ho, C. W., & Chan, T. W. (2007). Affordances of Mobile Technologies for Experiential
Learning: The Interplay of Technology and Pedagogical Practices. Journal of Computer Assisted Learning, 23(4), 326-337.
Liang, J. K., Liu, T. C., Wang, H. Y., Chang, L. J., Deng, Y. C., Yang, J. C., Chou, C. Y., Ko, H. W., Yang, S., & Chan, T. W.
(2005). A Few Design Perspectives on One-on-one Digital Classroom Environment. Journal of Computer Assisted Learning,
21(3), 181-189.
Linnaeus, C. (1753). Species Plantarum. Stockholm, Sweden.
Liu, C. C., & Kao, L. C. (2007). Do handheld devices facilitate face-to-face collaboration? Handheld devices with large shared
display groupware to facilitate group interactions. Journal of Computer Assisted Learning, 23(4), 285-299.
Liu, T. C., Wang, H. Y., Liang, J. K., Chan, T. W., Ko, H. W., & Yang, J. C. (2003). Wireless and mobile technologies to enhance
teaching and learning. Journal of Computer Assisted Learning, 19(3), 371-382.
Magerkurth, C., & Tandler, P. (2002). Interactive Walls and Handheld Devices - Applications for a Smart Environment.
Proceedings of the UbiComp'02 Workshop on Collaboration with Interactive Walls and Tables. Göteborg, Sweden.
Myers, B. A., Stiel, H., & Gargiulo, R. (1998). Collaboration Using Multiple PDAs Connected to a PC. Proceedings of the ACM
Conference on Computer-Supported Cooperative Work (CSCW'98). Seattle, Washington, USA. 285-294.
Nicol, D. J., & MacLeod, I. A. (2005).Using a Shared Workspace and Wireless Laptops to Improve Collaborative Project
Learning in an Engineering Design Class. Computers & Education, 44(4), 459-475.
Rieger, R., & Gay, G. (1997). Using Mobile Computing to Enhance Field Study. Proceedings of the Computer-Supported
Collaborative Learning Conference (CSCL'97). Toronto, Canada, 215-223.
Roschelle J. (2003) Unlocking the learning value of wireless mobile devices. Journal of Computer Assisted Learning, 19(3), 260272.
Ryall, K., Forlines, C., Shen, C., Morris, M. R. (2004). Exploring the Effects of Group Size and Table Size on Interactions with
Tabletop Shared-Display Groupware. Proceedings of the 2004 ACM Conference on Computer Supported Cooperative Work.
Chicago, Illinois, USA. 284-293.
Scott, S. D., Mandryk, R. L., & Inkpen, K. (2003). Understanding Children’s Collaborative Interactions in Shared Environments.
Journal of Computer Assisted Learning, 19(2), 220-228.
Scott, S. D., Grant, K., & Mandryk, R. L. (2003). System Guidelines for Co-located Collaborative Work on a Tabletop Display.
Proceedings of Eighth Conference on European Conference on Computer Supported Cooperative Work (ECSCW 2003). Helsinki,
Finland. 159-178.
Sharples, M., & Beale, R. (2003). A Technical Review of Mobile Computational Devices. Journal of Computer Assisted
Learning, 19(3), 392-395.
Soloway, E., Norris, C., Blumenfeld, P., Fishman, B., Krajcik, J., & Marx, R. (2001). Log on Education: Handheld devices are
ready-at-hand. Communication of the ACM, 44(6), 15-20.
Stewart, J., Bederson, B. B., & Druin, A. (1999). Single Display Groupware: A model for co-present collaboration. Proceedings
of the SIGCHI Conference on Human Factors in Computing Systems: the CHI is the limit. Pittsburgh, Pennsylvania, USA. 286293.
Tatar, D., Roschelle, J., Vahey, P., & Penuel, W. R. (2003). Handhelds Go to School. IEEE Computer, 36(9), 30-37.
Tse, E., & Greenberg, E. (2004). Rapidly prototyping Single Display Groupware through the SDGToolkit. Proceedings of the fifth
conference on Australasian user interface. Dunedin, New Zealand. 101-110.
Zanella, A., & Greenberg, S. (2001). Reducing interference in single display groupware through transparency. Proceedings of the
seventh conference on European Conference on Computer Supported Cooperative Work. Bonn, Germany. 339-358.
Zurita, G., & Nussbaum, M. (2004). Computer Supported Collaborative Learning Using Wirelessly Interconnected Handheld
Computers. Computers & Education, 42(3), 289-314.

207

Vernadakis, N., Giannousi, M., Derri, V., Kellis, I., & Kioumourtzoglou, E. (2010). Athens 2004 Team Leaders' Attitudes toward
the Educational Multimedia Application "Leonidas". Educational Technology & Society, 13 (1), 208–219.

Athens 2004 Team Leaders’ Attitudes toward the Educational Multimedia
Application “Leonidas”
Nikolaos Vernadakis, Maria Giannousi, Vassiliki Derri, Iraklis Kellis and Efthimis
Kioumourtzoglou
Department of Physical Education and Sport Science, Democritus University of Thrace, Greece // nvps@otenet.gr //
mgiannou@phyed.duth.gr // vaderri@phyed.duth.gr // opkellis@yahoo.com // kioumour@phyed.duth.gr
ABSTRACT
The purpose of this study was to adapt the questionnaire Multimedia Attitude Survey (MAS; Garcia, 2001) to
the Greek population in order to evaluate the educational multimedia application “Leonidas” considering the
attitudes of ATHENS 2004 team leaders. In addition, the differences among the sex were also investigated.
Participants were 232 team leaders, between the ages from 33-44 years old. One hundred twenty two (52.6%) of
the participants were men and one hundred ten were women (47.4%). Data was collected using an on-line
survey at the end of this study. Results from the factor analysis yielded eight factors accounting for 89.98% of
the variance. Reliability analysis indicated a satisfactory internal consistency estimate of reliability for the
attitude questionnaire. Independent-samples t test analysis revealed significant differences between the two sex
groups, in the case of one factor: “general experience”. In the factor above the women reported better results. In
conclusion the team leaders’ feedback from the questionnaires indicated a general level of satisfaction and
contentment with this particular multimedia application. The scale adapted in the present study can be a useful
tool for the evaluation of other relative multimedia applications by multimedia developers. Nevertheless, further
examination is warranted in order to obtain additional information concerning the difficulties of multimedia
experience on employees’ attitudes toward multimedia applications.

Keywords
Multimedia application, Attitude, Olympic Games, Gender, Technology

Introduction
Technology offers a promising resource (via computer networks, distance learning systems, multimedia software,
and video materials) for training staff and volunteers, sharing information about promising practices, and reducing
the isolation of many programs. The new technologies offer ways of individualizing instruction to meet the needs of
types of learners and potentially to reach all types of learners in ways they learn best.
So, is there any value added by using technology in adult education? As with so many other innovations, the value is
not intrinsic, but rather depends on how and for what purposes one uses the innovation. Simply adding technology
without challenging ourselves to do things we could not do before, or to do them differently, is meaningless at best,
and very expensive at worst. On the other hand, technology applications and activities that lead to expanded
opportunities for learning can only help adult learners acquire the skills and mastery of tools to support independent,
lifelong learning.
Multimedia computer-assisted instruction (MCAI) is increasingly being used as a means of delivering educational
content in organizations training. Efficiency, portability, consistency, and effectiveness have all been cited as reasons
for employing this technology in the company's educational environment. These visual learning symbols, pictures,
and other representative techniques allow students to go deeper into ideas and concepts (Chandler, 2003).
However, the rapid growth of multimedia implementation in learning settings does not guarantee participation and
acceptance on the part of employees. Negative attitudes towards multimedia-based instruction could be a deterrent to
using multimedia technology as a learning tool. Therefore, the thoughts, tendencies and attitudes of the learners’
towards these tools are needed to be determined (Becker and Maunsaiyat, 2002; Christensen and Knezek, 2000;
İşman and Dabaj, 2004; Selwyn, 1997).
Awareness of employees’ attitudes toward MCAI is a critical criterion in the evaluation of multimedia courses and in
the development of multimedia computer-assisted curricula. Attitudes toward multimedia-enhanced instruction are
considered to influence not only the acceptance of this medium of instruction, but also future behaviors in the
learning process. For this reason, the promotion and maintenance of positive attitudes toward MCAI is of paramount
ISSN 1436-4522 (online) and 1176-3647 (print). © International Forum of Educational Technology & Society (IFETS). The authors and the forum jointly retain the
copyright of the articles. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that copies bear the full citation on the first page. Copyrights for components of this work owned by
others than IFETS must be honoured. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior
specific permission and/or a fee. Request permissions from the editors at kinshuk@ieee.org.

208

importance. Negative attitudes must not be allowed to limit the knowledge and creativity of learners, nor anxiety to
interfere with the learning process. If the utilization of multimedia teaching/learning environments is to be
maximized, attitudes toward these learning settings must be continuously monitored. Fast, effective instruments to
assess attitudes toward multimedia instruction are crucial to this process.
There is a wealth of computer attitude scales available in the literature. Many instruments have been developed with
the purpose of measuring computer anxiety, computer usage, computer appreciation, and other computer-related
attitudes (Jones and Clarke, 1994; Kay, 1993; Selwyn, 1997). There are a number of studies which provide useful
empirical comparisons of available computer attitude scales (Gardner, Discenza & Dukes, 1993; Woodrow, 1991).
All the previous references to existing surveys focus on general attitudinal parameters rather than on in-depth
attitude- related dimensions. Besides, all the surveys reviewed explore learners' attitudes towards computers, and
none of them elicit students' perceptions toward multimedia instruction as such.
Garcia (2001) reported a practical, multi-dimensional, easy-to-administer research tool specifically intended to assess
the attitude of learners towards multimedia-enhanced instruction. The specificity of this 25-item instrument
constituted a powerful tool for the assessment of student attitudes towards multimedia technology when this was
used for educational purposes. This instrument was tested by Garcia on 40 subjects. The internal reliability
coefficient for each of the attitude sub-scales making up the survey - student attitudes on individualized instruction;
student attitudes toward self-paced instruction; student attitudes toward the user-friendliness of the learningenvironment; student levels of anxiety when working with multimedia; and the general opinion of the students
toward their experience with the instructional material - showed a high degree of internal consistency. The
independence of these subscales has allowed practitioners, evaluators and researchers to make their own selection of
factors in order to adapt the survey to meet their own needs with an eye toward evaluating and predicting the
performance of learners in a multimedia-enhanced learning setting.
Cardoso, Peralta & Costa (2005) examined the attitudes and the perceptions of the Portuguese students about
educational multimedia software’s criteria of quality. Their study was part of an international project supported by
EU (PEDACTICE - Educational Multimedia in Compulsory School: From Pedagogical Assessment to Product
Assessment), and the sample of interviewed pupils can be considered as representative of the Lisbon schools,
attended by teachers and pupils very much interested in multimedia materials. The results indicated the confirmation
of the success of computers and multimedia among the young Portuguese student population, being manifest either
in their attitudes or in the diversity of their experiences, including the technical mastery of informatics.
Teoh and Neo (2007) investigated students’ learning impact and attitudes towards independent learning and selfpaced discovery. A set of multimedia tools were employed to create the student–centred learning environment and
were designed using Gagne’s Nine Events of Instructions which provides a proper theoretical framework of a good
instructional lesson plan. In general, this study has found that interactive learning using this Multimedia-based
environment is feasible and is a viable alternative to the traditional classroom which has proved to be limited in
achieving the necessary needs of the students in the modern learning context. Students were positive towards active
learning and were confident in enforcing self-paced strategy. This is a viable learning strategy and should be
encouraged by educationists.
Gandole, Khandewale & Mishra (2006) examined the effect of multimedia software support on the attitude towards
electronics subject of students while working in laboratory of electronics science. The investigator developed an
attitude scale having 25 items by covering various aspects related to electronics experiments and laboratory
communication. There were 21 positive and four negative items on five-point scale (Likert type). The difference of
points in pre test and posttest decided the change in Attitude. The findings showed that the multimedia software
support used for laboratory communication was much effective in bringing an attitudinal change among the students.
There was a remarkable enhancement in attitude for all items.
Numerous studies have indicated sex differences in computer attitudes whereby males hold more positive and less
negative attitudes and females vice versa (e.g. Bebetsos, Kouli & Antoniou, 2007; Ho & Lee, 2001; Schumacher and
Moharan-Martin, 2001). Equally there is evidence that denies a difference (e.g. Antoniou, Patsi, Bebetsos &
Ifantidou, 2006) or conversely finding that females liked computers more than males (Keasar, Baruch & GrobgeldDahan, 2005). North and Noyes (2002) found that the impact of psychological gender (sex and sex-role), does not
influence significantly attitudes or cognitions towards computers. This does not support the notion that a
209

technological gender gap is developing, nor the literature that suggests males hold more positive attitudes and
cognitions than females. However, in this instance it appears that the computer is viewed positively.
In general, the relationship between computer attitudes and gender is not straightforward. Although research implies
that males hold more global positive computer attitudes, there is evidence that on certain aspects females view
computers more favourably than males. This is in relation to elements of computer culture, on certain new
technology items or when computers are presented as useful tools. In addition, it has been suggested that whilst
males view computers as more appropriate for them, females regard computers as more gender neutral than males
and do not regard mathematics ability as a prerequisite. This supports the finding that males hold more genderstereotypes of computers than females (Sanders, 2006; Whitely, 1997).
Learners’ attitudes have contributed to our understanding of why MCAI have enhanced achievement and
performance and motivation. Multimedia applications are profit tools for individual and student-centered learning,
so, in order to be informative, effective and attractive on their use, they should have a clear view of the students’
attitudes on the use of multimedia. Learners are no doubt the most important stakeholders in what concerns the use
of educational multimedia software. However they are seldom questioned about their interests, difficulties or
suggestions on this matter (Cardoso, Peralta & Costa, 2005). That’s why this study takes the learners’ point of view
about quality of educational multimedia software as its main concern.
The leading aim of this study was to adapt the Multimedia Attitude Survey (MAS; Garcia, 2001) to the Greek
population in order to evaluate the educational multimedia application “Leonidas” considering the attitudes of
ATHENS 2004 team leaders. In addition, the differences among the sex were also investigated. Some more specific
objectives come out from the following three research questions:
1. Is there a single dimension or are multiple dimensions underlying the 25 attitude items toward the multimedia
application?
2. How reliable is our 25-item measure of attitude control?
3. Does the average amount of students’ attitude differ between males and females?

Methods
Participants
Participants in this study consisted of two hundred thirty two (n=232) employees who were enrolled in “Team
Leaders” Training Program at Organizing Committee for the Olympic Games ATHENS 2004 during Spring 2004.
One hundred twenty two (52.6%) of the participants were male and one hundred ten were female (47.4%), between
the ages from 33-44 years old.

Instrumentation
Software instrument
The multimedia application “Leonidas” was developed by the education and training department to support the
"Team Leader" Training Program of the Organizing Committee for the Olympic Games ATHENS 2004. The
material was constructed using the following programs: a) Macromedia Flash MX 2004, b) Adobe in Design 2.1, c)
Adobe Photoshop 7.0 and d) Adobe Premiere Pro. The multimedia application run under Windows and Mac personal
computer systems and was divided into five theme groupings: 1. Introduction, 2. Leadership, 3. Leader & Team, 4.
Team Leader Skills and 5. Audit & Evaluation (see figure 1).
In order to help the team leader implementing the required policies, providing members of his/her challenge of
pressing Games-time conditions and contribute to their success, “Leonidas” included the use of a simple language, a
host of interactive applications such as audio flash movies and video, a wealth of photographic and other high quality
illustrative material.

210

Figure 1. Example screen from the multimedia application “Leonidas”
The multimedia application consisted of 905 pages; 10 pages were introductory, 7 were main menus, 486 were
information, 124 were practice, 243 were feedback and 35 were help. At the end of each topic and sometimes in
certain sub-topics, a quiz was provided which contained 10 multiple-choice questions on the material. In this quiz
learners were asked to resolve various readiness situations and giving responses to scenarios of potential problems.
The users had the ability to navigate through the path structured by the programmer via the site map or from the
menu appearing on each page.

Attitude instrument
The MAS questionnaire (Garcia, 2001) consisting of 25 items and one open-response item was adapted in order to
elicit relevant information on the participants' attitude towards using the multimedia application “Leonidas”.
The repeated forward–backward translation procedure was adopted as it is most commonly quoted in the adaptation
and translation process (Carlson, 2001) and was considered to be the best within the strategies which were
pragmatically possible. In this procedure a forward translation is made from the source original language to the target
new language. The target language version is then translated back into the source language and compared to the
original version. Errors in the target language version are identified through changes in meaning that arise in the back
translation.
The procedure was broadly divided into four phases. Phase 1 was to make four Greek translated versions of the
original survey and unify these four. Phase 2 was to produce a back-translated version. Phase 3 was to check the
equivalence between the original survey and the back-translated version. Phase 4 was to continue forward and
backward translation until satisfactory equivalence was agreed.
In Phase 1, two pairs of two bilingual and bicultural colleagues of the ATHENS 2004 translation department were
separately asked to translate the original scale into Greek while discussing among in pairs about content, semantic
and conceptual equivalence between the original and their translation. All the translators were fully informed of the
objectives of their role in the whole procedure.
One of the authors (whose first language was Greek) unified the two Greek translations created by this process into a
single translated version. Selection among alternative Greek translations was based upon perceived ‘‘naturalness’’ of
the linguistic expression in the Greek language version.

211

In Phase 2, a further pair with a native English speaker and a native Greek speaker, both unaware of the original
scale, was identified. They were asked to back-translate the Greek version produced in Phase 1. Again, they were
professional translators, and were required to discuss content, semantic and conceptual equivalence and to emphasise
meaning rather than word-to-word translation.
In Phase 3, a panel of two language experts, two software industry professionals, and two educational leadership
content experts compared the original scale and the back-translation brought about by Phase 2, and checked for
semantic discrepancies. In Phase 4, the author altered the Greek expression of the parts found to be problematic in
Phase 3 with reference to any alternatives rejected in Phase 1. The pair used in Phase 2 re-translated them into
English. The panel of experts used in Phase 3 checked discrepancies between the original scale and the re-translation.
Detailed discussion of cultural difference and nuance ensured semantic equivalence and aimed to overcome
conceptual differences by identifying parallel concepts that might be perceived as stressful. More specifically, the
language experts helped eliminate unintended complexity and imprecision in wording. They remarks also helped
ensure cultural neutrality and detect wording that might bias responses. Software industry professionals and
educational leadership content experts suggested ancillary constructs and operationalization techniques suitable to
the goals of the study, in addition to critiquing the instrument for clarity. These experts reviewed questions in such
detail that in some cases they identified individual words that 'didn't feel right'. This process was repeated until
problems were resolved.
According to expert's recommendation, a few of the items were modified to meet the team leaders Olympic Games
milieu. Moreover, the open-ended question was also excluded from the selected parts, since there were only six
participants that wrote some comments about the multimedia application.
The modified questionnaire contained twenty five (25) items on a format that used a 5-point Likert-type scale
(1=strongly disagree, 2=disagree, 3=neither disagree nor agree, 4=agree, 5=strongly agree). This format allowed
participants to select a response from “1,” to “5,” representing their disagreement or agreement on the particular item
respectively where “3” stood for a neutral response (see Table 2).

Procedure
Students enrolled in “Team Leaders” Training Program of the Organizing Committee for the Olympic Games
ATHENS 2004, were invited to participate in a study designed to understand the user attitude levels of a multimedia
application used as a supportive tool for a course in a traditional classroom. The researchers administered the
questionnaire during the last class session on the ninth week of the training program due to the nature of the
questionnaire, since the questionnaire is typically offered to users after they have completed a session of work with
the particular multimedia application. Students were informed verbally and briefly on the research topic and the
questionnaire. Participation of the students was voluntary since confidentiality was guaranteed (i.e., students did not
place their name on any of the materials in the study). Participants were presented with a letter of informed consent
and provided the URL to the online survey. No technical errors were encountered during the completion of the online
survey. Data were analyzed using SPSS 13 statistical software.

Design
Due to practical limitations, a field experiment, instead of a laboratory experiment was conducted to test the
hypotheses. The experiment was a factorial design with sex groups (males and females) as independent variables,
and attitude performance as dependent variable.
Factor analysis was conducted to identify underlying clusters or relationships concerning the learners' attitude
towards the multimedia application “Leonidas”. In determining the internal consistency of the attitude scale, the
alpha reliability method was used. Independent-samples t test analysis was conducted to investigate the differences
of this attitude among the sex of the participants.
The hypotheses of this study were:
H1: There are multiple dimensions underlying the 25 attitude items toward the multimedia application.
212

H2: The 25-item measure of attitude control is reliable.
H3: The males will have more positive attitudes than females toward the multimedia application.

Results
Means and standard deviations for each factor in this study are presented on Table 1, while the means and standard
deviations for the sex groups are presented on Table 3. The results of each analysis are given separately below.
Table 1. Means1 and standard deviations for each factor
Factors
N
Mean
S.D.
Individualized instruction
232
4.25
.70
Self-paced instruction
232
4.22
.66
Involvement
232
3.48
.63
General experience
232
3.73
.88
Interaction
232
3.63
.97
Learner’s control
232
3.64
.75
Anxiety
232
3.73
.43
User-friendliness
232
4.47
.63
1
Scale: 1=strongly disagree, 2=disagree, 3=neither disagree nor agree, 4=agree, 5=strongly agree

Factor analysis
A principal component analysis of the 25-item scale was performed in order to investigate the underlying dimensions
of the educational web site’s evaluation, using the SPSS Factor Analysis program. Prior to performing principal
component analysis the suitability of data for factor analysis was assessed. Inspection of the correlation matrix
revealed the presence of many coefficients of .35 and above. The Kaiser-Meyer-Oklin values was .769, exceeding
the recommended value of .6 and the Bartlett’s Test of Sphericity =5304.243, reached statistical significance
(p<.001), supporting the factorability of the correlation matrix (Tabachnick, & Fidell, 2001).
Results indicated that our initial hypothesis of multidimensionality was correct. The principal components analysis
revealed the presence of eight components with eigenvalue exceeding 1. An inspection of the screen plot revealed a
clear break after the eighth component. Based on screen plot and the eigenvalues, it was decided to retain eight
components for further investigation. To aid in the interpretation of these eight components, Varimax rotation was
performed (Stevens, 1996). The rotated solution (presented in Table 2) revealed the presence of simple structure,
with eight components showing a number of strong loadings, and all variables loading substantially on only one
component. The eight factors solution explained a total of 89.98 per cent of the variance, with component 1
contributing 15.98 per cent, component 2 contributing 12.45 per cent, component 3 contributing 12.26 per cent,
component 4 contributing 11.92 per cent component 5 contributing 10.87 per cent, component 6 contributing 10.85
per cent, component 7 contributing 7.89 per cent and component 8 contributing 7.74 per cent. The interpretation of
the eight components was defined as follows:
(1) Individualized instruction, (4 items)
(2) Self-paced instruction (3 items)
(3) Involvement (3 items)
(4) General experience (4 items)
(5) Interaction (3 items)
(6) Learner’s control (3 items)
(7) Anxiety (3 items) and
(8) User-friendliness (2 items).
Table 2. The rotated loading matrix from the factor analysis1
Items
1
2
3
4
5
6
I enjoyed doing this exercise by myself.
.984
I would have liked to have had a partner to .972
work within these multimedia lessons.

7

8

H2
.995
.976
213

The multimedia exercises turned out to be
efficient thanks to the fact that there was
only one student per session.
I liked working with the application
without having to share it with other
students.
To be able to work at my pace resulted in a
more effective instruction.
I feel more motivated when I am allowed
to work at my own pace.
I did not like to be left working at my own
pace
The time flew while I was working with
the multimedia lessons.
I had the feeling that the time to finish
with the multimedia sessions never got to
its end.
As soon as I start to work with the
multimedia lessons I feel immersed in the
activity.
The software I have worked with looks
good to me.
The multimedia lessons are well designed.
The lessons have been planned out well.
In general. it has been a good experience
to work with the interactive lessons.
The interaction with the instructional
material through the computer was
pleasant
The interactions with the computer were
more positive.
The interactions with the computer made
me be more attentive all the time
When exploring the program I was not
happy when I found out that it was me
who had to decide what needed to be done
at every step.
I was grateful for the freedom I was given
to explore the activity in my own way.
I did not like to be able to navigate freely
throughout the program
The lessons with interactive multimedia
make me feel tense.
I get nervous when I think that I am going
to study lessons with multimedia
technology.
When I examine material with interactive
multimedia I feel comfortable.
I did not find the program confusing to
use.
The program was user-friendly.
% of variance
Total variance
Eigenvalue
1
H2 = communalities

.937

.921

.947

.929

.917

.957

.948

.953

.938

.943
.939

.955

.922

.920

.952

.932

.855

.879

.910
.749
.751

.900
.872
.762
.762

.817

.852

.799

.718

.620
.816

.862

.892

.939

.848

.932
.853

.964

.818

.856

.616

.856
.932

.976
.982

15.98

12.45

12.26

11.92

10.87

10.85

7.89

.888
7.74

3.997

3.112

3.066

2.980

2.719

2.715

1.973

1.935

89.98

214

Reliability analysis
Coefficient alpha is the statistic mostly used to assess the internal consistency. The Cronbach-alpha coefficient was
calculated for each of the sub-scales. The “Individualized instruction” factor had an a =.83, the “Self-paced
instruction” had an a =.89, the “Involvement” factor had an α =.86, the “General experience” factor had an α =.86,
the “Interaction” factor had an α =.75, the “Learner’s control” factor had an α =.90, the “Anxiety” factor had an α
=.82 and the “User-friendliness” factor had an a =.79. Although statistical texts (DeVellis, 1991) suggest that scale
with reliabilities more than 0.70 should normally be considered as acceptable, in practice lower limits have been set
up as acceptable by researchers.

Independent-Samples t Test analysis
An independent-samples t test was conducted to evaluate the hypothesis that males have more positive attitudes than
females toward the multimedia application. There was significant difference in scores for males (M=3.46, SD=.77)
and females (M=4.00, SD=.52) in the factor “General experience” t(230)=9.452, p<.01. As shown in Table 3, the
females scored significantly higher in the above factor, counter to the research hypothesis. No significant difference
was found between the two sexes groups in any case of the remaining seven factors of the MAS.
Table 3. Means1 and standard deviations for the sex groups in each factor
Factors
Males
Females
N
Mean
S.D.
N
Mean
Individualized instruction
122
4.28
.70
110
4.21
Self-paced instruction
122
4.19
.63
110
4.23
Involvement
122
3.49
.74
110
3.47
General experience
122
3.46
.77
110
4.00
Interaction
122
3.59
.87
110
3.67
Learner’s control
122
3.63
.78
110
3.66
Anxiety
122
3.72
.59
110
3.74
User-friendliness
122
4.44
.68
110
4.50
1
Scale: 1=strongly disagree, 2=disagree, 3=neither disagree nor agree, 4=agree, 5=strongly agree.

S.D.
.66
.68
.74
.52
.84
.73
.53
.62

Discussion
This study adapted a questionnaire in order to evaluate the multimedia application “Leonidas” considering the
attitudes of Athens 2004 employees. The study also sought to investigate differences among the sex of the
participants. Results indicated that the evaluation on a pedagogic multimedia application was a multidimensional
concept. This fact has been proved from other studies that have examined the role of the multimedia application as
an educational tool (Garcia 2001; Selwyn, 1997). As a result of the factor analysis conducted in each of the predefined subscales, all items agree with the attitudinal dimensions of MAS proposed by Garcia (2001). The reaction
of learners to the multimedia application “Leonidas” was encouraging. Analysis of the survey revealed a generally
strong positive attitude towards this particular multimedia application.
The finding was not a surprise given the learners’ positive attitude toward the multimedia application but the level of
positive reaction was higher than expected. The explanation in this phenomenon could be that participants in this
study already had increased interest in Olympic issues. Factors that could have contributed in this were the
multimedia experience of the participants with Olympic applications and their Greek origin. If this were the cases, it
was also likely that some other group of learners were less favourable toward the multimedia application “Leonidas”
of Athens 2004 training department. Also, the use of volunteers clearly had predisposed the learners towards more
positive attitudes.
Further analysis of the survey showed that the first factor of the questionnaire “Individualized instruction” had
positive ranging from “agree” to “strongly agree” for the majority of learners (76%). This reveals that participants
found the multimedia application “Leonidas” as an individual and self-paced learning tool that allows them to work
privately, in an enjoyable environment on their own. The factor “Self-paced instruction” had positive ranging from
215

“agree” to “strongly agree” in 78% of the learners. This indicates that the multimedia application “Leonidas”
contained materials that allowed the learners to learn at their own pace, giving them a sense of control over learning.
The third factor “Involvement” had the smallest positive impact on attitude of the multimedia application “Leonidas”
ranging from “agree” to “strongly agree” in 53% of the learners. The explanation to this phenomenon could be that
learners between the ages from 33-44 years old may need more sophisticated and complicated applications to have
their work done. Another consideration could be that the learners were not satisfied with the amount and the clarity
of information received. Also, participants found the learning experience in general worthwhile since the 90/% of the
respondents rated the “General experience” questions by answering, from “agree” to “strongly agree”. The fifth
factor “Interaction” had positive ranging from “agree” to “strongly agree” in 68% of the learners. This means that the
particular multimedia application contained interactive features that would empower the learners to control the
content and the flow of information and encouraged them to be responsible for their own learning. Moreover, the
factor “Learner’s control” had positive ranging from “agree” to “strongly agree” in 73% of the learners. This
indicates that the participants felt happy when they explored the multimedia application and they found out that they
have to decide by themselves what needs to be done at every step, by exploring the activity in their own way. The
seventh factor “Anxiety” had positive ranging from “agree” to “strongly agree” in 72% of the learners. This reveals
that participants felt nerveless and comfortable when they studied lessons by browsing the material via multimedia
application. Finally, the strong positive responses on the last factor “User-friendliness” made it the most dominant in
increasing “Leonidas” attitude. This shows that participants found the multimedia application easy to use, all
necessary special commands were clear and the user interface issues such as menu design and readability of screens
had been addressed.
The research on how sex changes attitudes of the multimedia application “Leonidas” showed no significant
differences. Males and females answered the questions of the survey the same way, indicating similar attitude. This
suggests that using the multimedia application “Leonidas” has a positive effect for both sexes. Similar results have
been reported by Antoniou, Patsi, Bebetsos & Ifantidou, (2006) and North and Noyes (2002), who found that the
impact of psychological gender, does not influence significantly attitudes towards computers. Other researchers
report that males have more positive attitudes than females (Bebetsos, Kouli & Antoniou, 2007; Ho and Lee, 2001;
Schumacher and Moharan-Martin, 2001) or conversely finding that females liked computers more than males
(Keasar, Baruch & Grobgeld-Dahan, 2005). Thus, the subsequent psychological gender theories of human–computer
interaction (namely the socialization theory as applied by Whitely, 1997) are unsupported.
The fact that the gender differentiation has not occurred may be viewed on two levels. First, there may be a general
cohort effect or second, there may be confounding factors exclusive to the sample group. In relation to the first point,
the positive attitudes found in both males and females may be associated with changes in societal values and the
socialisation processes in today’s computer generation (Whitely, 1997). These are perhaps mediated by the impact of
increased use of multimedia applications in organizations, at home and software developments improving
multimedia applications interfaces. In other words the MCAI may no longer cultivate gender differences in
multimedia applications attitudes per se. The second possible explanation for an absence of gender differences is that
there may be some factors intrinsic to this sample group that were responsible. This overlaps with the previous idea
of a cohort effect, e.g. it may be that the managers and the trainers at ATHENS 2004 organization were particularly
keen to ensure all employees viewed multimedia applications positively and did not convey a gender bias.
The more detailed component analysis found that the attitude towards the factor “General experience” had a
significant sex effect. This in part supports the suggestion by Whitely (1997) that gender effect exist on some attitude
components but not others, which is fundamentally based on the assumption that adults hold bi-directional views
about computers and especially multimedia applications. The fact that the females in this study viewed multimedia
computing ability more impartially than males, may explain why they displayed positive attitudes in general about
multimedia experience and why a significant difference was not found on other attitude components. In other words
females did not accept the belief that multimedia computing was related to gender, mathematics background or
nationality and viewed multimedia ability perhaps as an open option (Sanders, 2006). According to the socialization
hypothesis, a greater acceptance of the belief that multimedia computing is inappropriate should be associated with
more negative attitudes (Whitely, 1997). Since the females in this sample did not endorse the views that multimedia
computing ability was related to sex then the absence of a sex difference on attitudes is not surprising.

216

Implications for practice
The use of multimedia technology in traditional classrooms has been growing at a rapid pace. Though many
instructors are using various modes of multimedia technology to communicate with and instruct their learners, it is
important to understand that these various modes affect not only learner acceptance and performance but also future
behaviors in the learning process. This is a major concern because the cost of multimedia technology infrastructures
continues to absorb an increasing percentage of organizations’ budgets. Therefore, this study is timely and has
several practical implications.
First, the findings reveal that the use of multimedia resources provides complementary learning activities that aid the
learning process. There is great interest and potential in MCAI flexible learning with many instructors incorporating
some form of multimedia technology as a part of their instruction in organizations training (Chandler, 2003). While
instructors are the focal point in most course settings, it should be noted that complementary learning activities are
just as important for practice, if not more so.
Second, the use of multimedia technology in organization training is a matter that not only the Organizing
Committee for the Olympic Games ATHENS 2004 should be interested in, but also all the training organizations
should benefit from the results of this study for further corporate planning. Furthermore, they should not only
provide the opportunities of multimedia technology and MCAI for this organization but also they should take into
consideration the experience other training organizations had in this area of study. They should take into
consideration the attitudes of the employees on the uses of multimedia technology in organization and should prepare
the courses required multimedia using for their learners. Because as the employees’ success increases the success of
the organization increases.
Third, using multimedia applications is time-consuming and labor-intensive, if productive outcomes are to be
derived. Learners and instructors may find valuable resources and increased opportunities in communication through
the multimedia technology, but at the expense of continuous effort and time consumption. Establishing an interactive
and dynamic MCAI course like the learning environment of this study can help overcome time consumption
difficulty, while providing learners with quick and convenient ways to find useful information. Multimedia
applications create a much more interactive learning environment thereby increasing the effectiveness of learning.
Finally, a well-designed MCAI course provides the balance of real and virtual classrooms and class sessions. This
ideally makes the class a more continuous environment rather than an environment, which is done in one or two
hours and then set aside for the remainder of the week. Continuing education of employees can offer competitive
advantages; field experiments within real-world organizations would be very useful to organizations already using
interactive multimedia training to identify if their programs are effective and acceptable. For others, a field
experiment might suggest if migration to self-paced instruction via interactive multimedia would be relevant and
how it should be designed.

Limitations
As with all investigations, this study is not without limitations. First, the data used in this study were drawn from a
single corporation sample. The organization is best described as a large, organizing committee for the Olympic
Games located in Greece. Thus, the findings should be interpreted with caution and generalizations may only be
relevant to organizations similar in size, control status, and corporation emphasis. The present study used self-report
data and this may be another possible limitation. To the extent that respondents did not know the information being
requested or found survey questions to be ambiguous and unclear, the generalizability of these findings may be
limited.
Perhaps another limitation relates to the dataset used in the study — the MAS questionnaire. Perhaps, the attitude
instrument of this study was limited to factors that could be defined or operationalized using items drawn from the
database. It is highly possible that the MAS did not measure all of the variables needed to explain the variance in
student self-reported attitude toward multimedia applications. Likewise, it is plausible that the MAS items have a
marginal relationship with the constructs (e.g., Interaction, Learner’s control, Anxiety, etc.) that they are purported to
measure (Garcia, 2001).
217

Despite these limitations, this study contributes to our understanding of the potential effect of various uses of
multimedia technology on learners’ attitudes in training organizations. Specifically, it provided information about the
association between employees’ use of multimedia technology and self-reported attitude toward multimedia
application “Leonidas”. In addition, this research provides a foray into group differences that exist between males
and females of multimedia technology.

Conclusion
In conclusion the learner’s feedback from the questionnaires indicated a general level of satisfaction and contentment
with this particular multimedia application. Yet, in order to have the learners make constructive and flexible use of
the educational multimedia technologies, the “Individualized instruction”, the “Self-paced instruction”, the
“Involvement”, the “General experience”, the “Interaction”, the “Learner’s control”, the “Anxiety” and the “Userfriendliness” seem to be crucial considerations. Perhaps, adherence to these basic principles will not only improve
overall multimedia impressions, but also will increase use frequency to the multimedia application concerned. The
scale adapted in the present study can be a useful tool for the evaluation of other relative multimedia applications by
multimedia developers. Nevertheless, further examination is warranted in order to obtain additional information
concerning the difficulties of multimedia experience on employees’ attitudes toward multimedia applications.
When using multimedia technology in organization, it is strongly recommended that trainers take some time in
assessing employees' attitudes toward multimedia technology prior to the structuring of instruction and its
implementation in the training sessions. This approach is appropriate in that it ensures that the learners will have
maximum gains in utilizing multimedia applications as a tool for learning. Furthermore, education managers will be
given the chance to create an environment that can be conducive to the learners.
Research and development in this area will be continued with the view to refining any kind of multimedia
educational environment so that it meets and full fills all expectations for supporting and enhancing employees
learning process. More studies should be conducted to investigate the effect of multimedia experience on learner’s
attitudes toward the multimedia applications, especially when its effect is linked to gender. Also, one can reasonably
assume that most people – regardless of gender, age, or other demographic factors – access multimedia application
credibility in similar ways. Although real differences do exist, it’s more striking to see how many things were not
different, suggesting that the various demographic groups shared similar approaches to evaluating multimedia
applications.

References
Antoniou, P., Patsi, H., Bebetsos, E., & Ifantidou, G. (2006). Validity of scale and evaluation of students’ attitudes toward
computers. Compare with students’ attitudes toward physical education and physical activity. Inquiries in Sport & Physical
Education, 4(1), 114–124.
Bebetsos, E., Kouli, O., & Antoniou, P. (2007). Attitudes and Behaviors of University PE Students Towards the Use of
Computers. International Journal of Computer Science in Sport, 6(1), 55–63.
Becker, K. H., & Maunsaiyat, S. (2002). Thai Students’ Attitudes and Concepts of Technology. Journal of Technology Education,
13(2), 6–19.
Cardoso, A., Peralta, H., & Costa, F. (2005). The students’ point of view about quality of educational multimedia software.
Interactive Educational Multimedia, 11, 38–59.
Carlson, E. D. (2001). A Case Study in Translation Methodology Using the Health-Promotion Lifestyle Profile II. Public Health
Nursing, 17(1), 61–70.
Chandler, H. (2003). Concept mapping: WebQuests in Social Studies. Media & Methods, 39(3), 38–39.
Christensen, R., & Knezek, G. (2000). Internal Consistency Reliabilities for 14 Computer Attitude Scales. Journal of Technology
and Teacher Education, 8(4), 327–336.
DeVellis, R. F. (1991). Scale Development: Theory and Applications. Newbury Park, CA: Sage Publications.

218

Gandole, Y. B., Khandewale, S. S., & Mishra R. A. (2006). A Comparison of Students Attitudes between Computer Software
Support and Traditional Laboratory Practical Learning Environments in Undergraduate Electronics Science. E–Journal of
Instructional Science and Technology, 9(1). Retrieved July 10, 2008, from http://www.usq.edu.au/electpub/ejist/docs/vol9_no1/papers/current_practice/gandole_khandewale_mishra.pdf.
Garcia, J. C. (2001). An instrument to help teachers assess learners’ attitudes towards multimedia instruction. Education, 122(1),
94–101.
Gardner, D. G., Discenza, R., & Dukes, R. (1993). The measurement of computer attitudes: An empirical comparison of available
scales. Journal of Educational Computing Research, 9(4), 487–507.
Ho, S. M. Y., & Lee, T. M. C. (2001). Computer usage and its relationship with adolescent lifestyle in Hong Kong. Journal of
Adolescent Health, 29(4), 258–266.
İşman, A., & Dabaj, F. (2004). Attitudes of Students towards Internet. Turkish Online Journal of Distance Education, 5(4).
Retrieved July 11, 2008, from http://tojde.anadolu.edu.tr/tojde16/pdf/dabaj.pdf.
Jones, T., & Clarke, V. A. (1994). A computer attitude scale for secondary students. Computers and Education. 22(4), 315–318.
Kay, R. H. (1993). A practical research tool for assessing ability to use computers: The computer ability survey (CAS). Journal of
Research on Computing in Education, 26(1), 16–25.
Keasar, T., Baruch, R., & Grobgeld-Dahan, E. (2005). An evaluation of web enhanced instruction in college level biology
Courses. Australasian Journal of Educational Technology, 21(4), 533–545.
North, A. S., & Noyes J. M. (2002). Gender influences on children’s computer attitudes and cognitions. Computers in Human
Behavior, 18, 135–150.
Sanders, J. (2006). Gender and technology: A research review. In Skelton, C., Francis, B. & Smulyan L. (Eds.). Handbook of
Gender and Education (pp. 1–40). London: Sage Publications.
Schumacher, P., & Moharan-Martin, T. (2001). Gender, Internet and computer experiences. Computers in Human Behavior, 17,
95–110.
Selwyn, N. (1997). Students’ Attitudes toward Computers: Validation of a Computer Attitude Scale for 16–19 Education.
Computers and Education, 28(1), 35–41.
Stevens, J. (1996). Applied multivariate statistics for the social sciences (3rd Ed.). Mahwah, NJ: Lawrence Erlbaum.
Tabachnick, B. G., & Fidell, L. S. (2001). Using multivariate statistics (4th Ed.). Needham Heights, MA: Allyn & Bacon.
Teoh, B. S. P., & Neo, T. K. (2007). Interactive Multimedia Learning: Students’ Attitudes and Learning Impact in an Animation
Course. The Turkish Online Journal of Educational Technology, 6(4). Retrieved July 12, 2008, from
http://www.tojet.net/articles/643.doc.
Whitely, B. (1997). Gender differences in computer related attitudes and behavior; a meta-analysis. Computers in Human
Behavior, 13(1), 1–22.
Woodrow, J.E. (1991). A comparison of four computer attitude scales. Journal of Educational Computing Research, 7(2), 165–
187.

219

Lu, E. J.-L., Horng, G., Yu, C.-S., & Chou, L.Y. (2010). Extended Relation Metadata for SCORM-based Learning Content
Management Systems. Educational Technology & Society, 13 (1), 220–235.

Extended Relation Metadata for SCORM-based Learning Content
Management Systems
2

Eric Jui-Lin Lu1, Gwoboa Horng2, Chia-Ssu Yu and Ling-Ying Chou

3

1

Department of Management Information Systems, National Chung Hsing University, Taichung, Taiwan //
jllu@nchu.edu.tw
2
Department of Computer Science and Engineering, National Chung Hsing University, Taichung, Taiwan //
gbhorng@cs.nchu.edu.tw // s9456010@cs.nchu.edu.tw
3
Department of Applied English, Diwan University, Tainan, Taiwan // annechou@cc.kuas.edu.tw
ABSTRACT
To increase the interoperability and reusability of learning objects, Advanced Distributed Learning Initiative
developed a model called Content Aggregation Model (CAM) to describe learning objects and express
relationships between learning objects. However, the suggested relations defined in the CAM can only describe
structure-oriented relationships and cannot express semantic relationships between learning objects. Although
extended relations were proposed in the past, some of the proposed relations are redundant and even
inappropriate. In addition, the usefulness of these relations has never been formally studied. To solve the
problems, we systematically studied these relations from authors’ perspective and proposed an extension to
CAM. The extension was tested by 30 authors using a web-based learning content management system that was
developed by us.

Keywords
Learning content management systems, SCORM, Reusability, CAM, Metadata

Introduction
Due to the emergence and flourishing of the Internet, the development of e-Learning systems has become an
important research topic in both academia and industries. As a result, many learning systems and learning objects
(LOs) were developed. One major problem of these LOs is that they cannot be reused among different learning
systems. To resolve the problem, Advanced Distributed Learning Initiative (ADL) developed a reference model
called Sharable Content Object Reference Model (SCORM).
There are two kinds of LOs defined in SCORM. One is asset, and the other is sharable content object (SCO). Assets
are digital media such as text, images, sound, assessment objects, or any other piece of data. Each SCO is composed
of assets or other SCOs. To increase reusability and interoperability of LOs, metadata can be defined for each LO.
ADL developed a metadata model called Content Aggregation Model (CAM). CAM, adapted from IEEE Learning
Object Metadata (LOM), classifies all metadata into nine categories, and one of the categories is “RELATION”. A
relation in the “RELATION” category is mainly used to describe a LO and express relationships between learning
resources. When used skillfully, a relation is a very useful metadata that can enhance learning effectiveness as well
as increase the reusability of LOs. For example, as shown in Figure 1, LOA describes how bubble sort works. At the
bottom of LOA, there is a figure illustrating how bubble sort works in steps. Using the proposed relation metadata, we
can define the figure as a learning object of type “Illustration”. When the figure is stored in a repository, it can be
easily searched and reused by other users. Additionally, the application of relations can be further extended. If the
author of LOA wishes to provide more illustrations to help learners, she can easily provide links to more illustrations
such as LOB and LOC. LOs such as LOB and LOC can be created by the author or other authors as long as they can be
accessed. Also, these LOs can be searched and retrieved from repositories.
As defined in CAM, there are twelve suggested values as shown in Table 1 for the “RELATION” category.
However, these suggested values can only describe structure-oriented relationships and cannot express semantic
relationships between learning resources (Steinacker et al., 1999). In the past, many relations were proposed (Karger
et al., 2006; Ullrich, 2004, 2003, 2005; Loser et al., 2002; Steinacker et al., 1999, 2001; Sddik et al., 2001; Fischer,
2001; Fischer et al., 1999). These relations are mainly based on two major theories. One is instructional design
theory (IDT), and the other is rhetorical structure theory (RST). Although these relations can express meaningful
relationships between LOs, they are limited in the following ways.

ISSN 1436-4522 (online) and 1176-3647 (print). © International Forum of Educational Technology & Society (IFETS). The authors and the forum jointly retain the
copyright of the articles. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that copies bear the full citation on the first page. Copyrights for components of this work owned by
others than IFETS must be honoured. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior
specific permission and/or a fee. Request permissions from the editors at kinshuk@ieee.org.

220

Figure 1: An Example Application of RELATION Category

ispartof
isformatof
isbasedon

Table 1: The Suggested Values for RELATION Category
haspart
isversionof
hasformat
references
isbasisfor
requires

hasversion
isreferencedby
isrequiredby

First of all, some of the relations are redundant. For example, both “Example” and “Illustration” relations are not
only defined in IDT, but also defined in RST. Secondly, several relations have already been defined in CAM. For
example, the “Interactivity” relation defined in IDT has been defined in the “EDUCATION” category of CAM.
Thirdly, although the relations can express semantic relationships between LOs, the usefulness of the relations has
never been formally studied. Fourthly, there is no learning content management system supporting new relations.
Finally, after studying the relations, we found that some relations are confusing and may not be easy for authors to
clearly specify relation for each LO.
In this paper, we studied all 32 relations proposed in IDT and RST, removed 8 duplicated relations, and analyzed the
rest 24 relations. A web-based learning content management system supporting these 24 relations was designed and
implemented. By using the system, authors are allowed to assign a specific relation to a LO. Also, the relationships
between LOs can be easily defined. When learners read LOs (for example, LOA as shown in Figure 1), “more
illustration” will be shown along with LOA.
The developed system was also used as the platform to survey 30 authors. The survey was divided into two parts:
one was to study whether or not they would use these relations in LOs to increase learning effectiveness; the other
was to investigate, when specifying a relation for a LO, whether or not they might possibly use the other 23 relations
for the same LO. Based on the results of the survey, extended relation metadata was proposed.
The rest of the paper is organized as follows. First, we briefly discussed the relationships between LOs and described
all 32 relations in Section 2. The design and implementation of the prototype is described in Section 3. In Section 4,
we described how the survey was conducted and showed the survey results. Additionally, the discussions of the
results and suggestions were presented. Finally, we drew our conclusions and future works in Section 5.

221

Literature Review
Currently, existing relations are mainly based on two theories: one is instruction design theory (IDT) (Reigeluth,
1999), and the other is rhetorical structure theory (RST) (Mann and Thompson, 1988).

Instruction Design Theory
IDT encourages teachers to search for related learning resources and exploit them to satisfy all possible learning
needs. Based on IDT, Ullrich (Ullrich, 2003, 2004; Karger et al., 2006) proposed twenty-three relations, and they are
summarized in Table 2. The descriptions and examples for these relations are provided in Appendix A for reference.

Definition
Process
Example
Explanation

Table 2: The Relations Defined in IDT
Fact
Law
Law of Nature
Policy
Procedure
Interactivity
Counterexample
Evidence
Proof
Introduction
Conclusion
Remark

Theorem
Illustration
Demonstration

The Rhetorical Structure Theory
In the past, RST is an approach to analyze the rhetorical structure of article contents. Recently, researchers
(Steinacker et al., 1999; Seeberg et al., 1999; Sddik et al., 2001; Steinacker et al., 2001; Fischer, 2001; Loser et al.,
2002) extended the concept to express the relationships between LOs and proposed rhetorical-didactic relations. The
rhetorical-didactic relations include 9 relations, and they are summarized in Table 3. The descriptions and examples
for these relations are provided in Appendix B for reference.

Example
Continues

Table 3: The Relations Defined in RST
Illustration
Instance
Restriction
Deepen
Opposition
Alternative

Amplify

Discussions
After careful study of these relations, it is found that both “Example” and “Illustration” were redundantly defined in
both IDT and RST. “Interactivity” and its 4 subclasses have already been defined in the “EDUCATION” category of
CAM. Additionally, “Alternative” can be replaced with “hasformat” which is defined in the “RELATION” category
of CAM. As a result, out of 32 relations defined in IDT and RST, only 24 relations need to be further investigated.

The Design and Implementation of a Learning Content Management System
Design
Like IEEE LOM, SCORM CAM classifies metadata into nine categories. “RELATION” is one of the nine
categories. To define that LOA references LOB, one must define a <relation> element in the <lom> of LOA as shown
in Figure 2. Additionally, one has to define a <kind> element in <relation> and define both <source> and <value>
elements in <kind>. The values for the <source> and <value> elements are LOMv1.0 and references, respectively, to
express that it is a references relation based on LOM version 1.0. LOB is defined in a <resource> element which is a
subelement of <relation>. In the <resource> element, <identifier> is used to identify the location of LOB, and
<description> is used to describe LOB.

222

Figure 2: An Example SCORM CAM Relation

When designing the learning content management system, we attempted to revise CAM as less as possible. We also
used the concept of RDF triple to design our metadata. First of all, the name of the extended relation metadata is
called NCHUMISv0.1. In other words, if the extended relation metadata were used, the value for the <source>
element should be NCHUMISv0.1. The suggested values for the <value> element in NCHUMISv0.1 are the 24
relations which include “Conclusion”, “Continues”, “Counterexample”, “Deepen”, “Definition”, “Demonstration”,
“Evidence”, “Example”, “Explanation”, “Extension”, “Fact”, “Guideline”, “Illustration”, “Instance”, “Introduction”,
“Law”, “Law of Nature”, “Opposition”, “Procedure”, “Process”, “Proof”, “Remark”, “Restriction”, and “Theorem”.

Figure 3: An example Relation Architecture
To describe how to use the relations in applications such as the one shown in Figure 1, Figure 3 is used as an
example. In Figure 3, it shows that LOA, identified as http://www.example.org/courses/A, has an example (identified
as
http://www.example.org/courses/A#id(ex.1))
and
a
definition
(identified
as
http://www.example.org/courses/A#id(def.1)). The example was described by a <relation> element as shown in lines
02-13 of Figure 4, while the definition was defined in lines 15-26. Note that, as shown in lines 04 and 17, the value
for both <source> elements is NCHUMISv0.1.
223

The author of LOA may also provide more examples for A#id(ex.1) or more definitions for A#id(def.1). These
examples or definitions are generally stored in repositories and can be authored by her or other authors as long as
they are accessible. To be able to express such relationships, we defined extra attributes id and idref for <relation>
elements. With XPointer, we can clearly define the relations between LOs by using id and idref. For example, to
define one extra example link for A#id(ex.1), we first defined an id="ex.1" for the <relation> in line 02. Then, we
added another <relation> element in lines 27-38. As shown in line 27, an idref="ex.1" attribute has to be defined to
indicate that this relation is for the relation identified as id="ex.1". The newly created
<relation>
element
represents an example located at URIB#id(ex.1). With the proposed design, one can easily define a resource (which
can be all or part of a resource) and link other resources to the resource.

Figure 4: The Definition for The Example Relation Architecture

Figure 5: An Example Metadata That Supports Two Relations
224

Sometimes, a LO can be of more than two relation types. For example, as shown in Figure 5, A#id(multi.1) is an
example in LOA. However, A#id(multi.1) may be a counterexample for other LOs. To support this, multiple relations
are allowed to define in <value> element as long as each relation is separated by a blank space (“ ”). Therefore, for
the example shown in Figure 5, <value> example restriction </value> is defined in line 5. It is noted that we think
that the relation “restriction” defined in RST is more appropriate to be used in this example than the relation
“counterexample” defined in IDT. The possible confusion caused by these two relations would be further
investigated in Section 4.

Implementation
Sun’s Java Standard Edition 1.5 and Apache’s Tomcat were used to implement the prototype of the learning content
management system. Although all metadata defined in CAM are optional, there are some restrictions imposed when
using the extended relation metadata. For example, to be able to reuse existing LOs, it is required for authors to enter
values for <keyword> element which is in the “GENERAL” category of CAM.
Figure 6 is a screenshot that demonstrates the prototype allows authors to create or edit a LO (called LOA). Similar to
many personal Blog systems, the prototype allows authors to click on buttons above the text area to insert a relation.
Each button represents a relation in the metadata. It is also possible to upload a file which can be an image, a sound
clip, or any other multimedia file. The IMG and A buttons can be used to create images or hyper-links in LOs. As
shown in Figure 6, two for loop examples were defined. Also, authors are allowed to enter other relation types of LOs.
After finishing editing LOA, an author can click on the OK button. LOA will be saved in XML format, an id attribute
will be inserted for each relation, and the value for each id attribute is automatically generated to ensure uniqueness.
The resulting page is shown in Figure 7(a). Note that, because two EXAMPLE relations were defined in Figure 6, a
drop down menu with these two relations was created as shown in Figure 7(a).
For each relation in the drop down menu, authors are allowed to create more links for the selected relation. For
example, if an author wished to provide more example links for the first example, she first selects the first example
from the drop down menu and clicks on the GO button. The system will then search for all possible relations in
repositories. Currently, the search mechanism used is quite simple, although it can be further enhanced in the future.
The search engine embedded in the prototype will first look up LOs with the same relation name. From the matched
LOs, the search engine will then check their keyword values. LOs with more matched keywords will be ranked
higher than those with less matched keywords. Using LOA as an example, all LOs in repositories with the relation
“Example” and the keyword loop will be matched and displayed which are shown in Figure 7(b). Authors can select
zero or more links for each relation in the drop down menu. As shown in Figure 7(b), the author selected two
example links for the first example of LOA. When learners study LOA, they will see a page similar to Figure 1. After
reading LOA, a learner may choose to read more examples by selecting an entry in the drop down menu.

Analysis and Discussions
As mentioned earlier, the usefulness of relations based on IDT and RST has never been formally studied. In addition,
it is sometimes really difficult for authors to clearly specify one relation for a LO without thinking maybe another
relation is more appropriate for the LO. For example, it can be very difficult for authors to choose either “Process”,
“Procedure”, or “Continue” for a “sorting algorithm”.
To investigate the above issues, we used questionnaire to survey 30 graduate students at the Department of Computer
Science and Engineering, National Chung Hsing University, Taiwan. Out of 30 students, 17 of them have teaching
experience, while the other 13 have no teaching experience. The questionnaire was divided into two parts. In the first
part, authors were asked whether or not the 24 relations will be helpful for their learners. There are 24 questions (one
for each relation) in the questionnaire. Each question was rated on a discrete scale from 5 to 1, with corresponding
verbal descriptions ranging from ‘highly useful’ through ‘useful’, ‘no-opinion’, ‘useless’, to ‘highly useless’; respectively.

225

Figure 6: A Screenshot for Creating/Editing Los

Figure 7: Example screenshots.
226

In the second part of the questionnaire, authors were asked, when specifying a relation for a LO, whether or not she
thought other relations might also be adequate for the LO. There are also 24 questions because there are 24 relations.
For each relation, authors were allowed to select zero or more than one relation out of the other 23 relations.
The survey was conducted as follows: Firstly, we explained the meanings of the 24 relations. Then, the prototype is
used as the platform to show them how authors can use the prototype to assign a relation for a LO as well as to
search and link other LOs to the LO. In the prototype, several LOs had been created using the prototype. Lastly, we
let authors to use the prototype and answer the questions. To avoid possible confusions, every author was surveyed
individually and each session took about 30 to 60 minutes.

Analysis: Part I
To study the usefulness of all 24 relations, we employed One-Sample T Test on each relation. The null hypothesis
and the alternative hypothesis were defined as below where µ represents the population mean:
H0: µ1≤ µ
H1: µ1> µ
With the level of significance (α) set as 0.05, the confidence interval of the difference between µ and the sample
mean for each relation is summarized in Table 4.
Table 4: The Results of One-Sample T Tests

227

In Table 4, the second column is the sample mean; the third and fourth columns are the lower and upper bounds,
respectively, of the confidence interval when µ is 3; and the fifth and sixth columns are the lower and upper bounds,
respectively, of the confidence interval when µ is 4.
Take “Demonstration” as an example. Because its confidence interval is [1.18, 1.69], H0 should be rejected. In other
words, the usefulness of the relation “Demonstration” is significant. We can conclude that authors believed that
using the relation “Demonstration” in LOs will increase learning effectiveness. Therefore, as shown in Table 4, the
usefulness of all relations, except for “Law of Nature” and “Opposition”, is significant when µ is 3. Furthermore,
when µ is 4, only “Demonstration”, “Example”, and “Illustration” are significant.
In addition, we like to study if there is a significant difference between authors who have teaching experience and
those who have no teaching experience. Independent-Samples T Test was employed to study the difference between
two groups. The means, t-values, and p-values are calculated for each relation and summarized in Table 5. In Table
5, the second and third columns represent the numbers of the surveyed graduate students who have teaching
experience and those who have no teaching experience, respectively; the fourth and fifth columns are the sample
means for the graduate students who have teaching experience and those who have no teaching experience,
respectively; and the sixth and seventh columns are t-values and p-values, respectively, of two groups.
Table 5: The Results of Independent-Sample T Tests

228

From the table, it shows that there is no significant difference between two groups if the level of significance is 0.05.
However, if the level of significance is 0.1, two groups are significantly different on the relations “Continues”,
“Guideline”, “Law of Nature”, and “Theorem”. Because “Law of Nature” is not considered as useful from the
previous analysis, only the relations “Continues”, “Guideline”, and “Theorem” were further studied. The ratios
shown in Figure 8 are computed by dividing the number of authors in a group who believed a relation is either
‘highly useful’ or ‘useful’ by the total number of authors in the group. Take the relation “Theorem” as an example.
The number of authors who have teaching experience and believed the relation is either ‘highly useful’ or ‘useful’ is
9. When divided by the total number of authors in the group which is 17, the ratio is 0.53. From the figure, it is clear
that, authors who have teaching experience preferred both “Continues” and “Guide-line”, while authors who have no
teaching experience preferred “Theorem”.

Figure 8: Group Analysis

(1)

(2)

(3)

(4)
(5)

Table 6: Association Analysis (The Minimum Support Value=3%)
correlative relation
support %
example instance
8.5
example illustration
4.9
example illustration instance
4.9
illustration instance
4.9
law lawofnatural
7.2
law theorem
5.4
lawofnatural theorem
4.7
law lawofnatural theorem
4.7
procedure process
6.8
continues process
4.2
continues procedure
3.8
continues procedure process
3.8
counterexample restriction
3.6
deepen extension
3.2
total of transactions = 720

Analysis: Part II
It is believed that, when specifying a relation for a LO, authors may have difficulty in choosing one relation over
another. Therefore, the main purpose of the second part of the questionnaire was to examine whether or not authors
would be confused when selecting an appropriate relation for LOs. Association rule mining (Agrawal and Srikant,
1994; Agrawal et al., 1993), a popular technique used in data mining, was used to examine associations or
correlation relationships among the 24 relations. In particular, Apriori algorithm (Agrawal and Srikant, 1994) was
employed.

229

When specifying a relation for a LO, authors were asked whether or not they might use other relations for the LO.
Thus, the answer for one relation and the relation itself can be treated as a transaction (or called itemset). For
example, if an author selected nothing for “Conclusion” and selected both “Procedure” and “Process” for
“Continues”, there were two transactions which were shown as follows:
Conclusion
Continues Procedure Process
Because there were 24 questions for each author, there were 720 transactions in total. A program developed by
Borgelt (Borgelt, 2007) was used to analyze transactions. With the support value as 3%, the results are shown in
Table 6.
From the results, it is clear that relations which have high associations can be categorized into five groups, and they
are (1) “Example”, “Illustration”, and “Instance”; (2) “Law”, “Law of Nature”, and “Theorem”; (3) “Continues”,
“Procedure”, and “Process”; (4) “Counterexample” and “Restriction”; and (5) “Deepen” and “Extension”.
Discussions
Based on the previous analysis, the following suggestions were made. However, please bear in mind that both the
surveyed authors and the studied learning materials are closely related to information technology. The suggestions
below may not be applied to other domains.
First of all, both “Law of Nature” and “Opposition” were considered useless, while “Demonstration”, “Example”,
and “Illustration” were considered the most important relations. It is, therefore, suggested that both “Law of Nature”
and “Opposition” can be removed from the relation metadata.
Secondly, in the analysis of correlation relationships among relations, it is shown that some relations are highly
correlated and are clustered into five groups. In the followings, each group will be discussed further.
Example, Illustration, and Instance
Based on its original definition in RST, an instance is an example of a learning object. In addition, because
“Example” and “Instance” are highly correlated, either one can be removed from the relation metadata. It is
suggested that “Instance” is removed from the metadata.
The meanings of “Illustration” and “Example” are somewhat overlapped and sometimes cannot be clearly
distinguished one from the others. According to OxFord Advanced Learner’s Dictionary, an illustration is either “a
drawing or picture in a book, magazine, etc. especially one that explains something”, “the process of illustrating
something”, or “a story, an event or an example that clearly shows the truth about something”. For the second case,
we can use the relation “Process” to describe LOs. For the third case, we can use the relation “Example” to describe
LOs. Therefore, in the relation metadata, an illustration is re-defined as “a drawing or picture that explains
something”.
Law, Law Of Nature, and Theorem
Because “Law of Nature” was removed from the metadata, it will not be discussed again. As shown in Table 4, the
surveyed authors believed the usefulness of “Theorem” is higher than the usefulness of “Law”. As a result, it is
suggested that “Law” can be removed from the metadata. However, it is noted that we believed the main reason why
both “Law” and “Law of Nature” are considered less useful is simply because they are rarely used in IT domain. It
may not be true in other domains.
Continues, Procedure, and Process
All three relations are highly correlated. From Table 4, it showed that “Process” is considered more useful than
“Continues” and “Procedure”. Therefore, it is suggested that “Continues” and “Procedure” can be removed from the
metadata.
230

Counterexample and Restriction
Although we clearly explained the definitions of “Counterexample” and “Restriction” before the surveyed authors
answered questions, these two relations still confused many authors which was shown in Table 4. In order to avoid
confusions, it is suggested that “Restriction” is removed from the metadata and “Counterexample” is re-defined as
the unsuccessful or exceptional situation of a LO.

Deepen and Extension
A LO of type “Deepen” is to explain another LO in depth. An extension object is a LO that is extended from another
LO. Because they were considered useful (with the average values higher than 3.53), and because their support value
was only slightly higher than 3%, it is suggested that both relations should be kept in the metadata. Also, they can be
considered as “optional”.
From the above discussions, it was concluded that 7 relations (including “Law of Nature”, “Law”, “Opposition”,
“Instance”, “Continues”, “Procedure”, and “Restriction”) were removed and 2 relations (including “Illustration” and
“Counterexample”) were re-defined. The proposed extended relation metadata consists of 17 relations and are
summarized in Table 7. Out of 17 relations, it is highly recommended to define “Demonstration”, “Example”, and
“Illustration” LOs. As shown in Table 7, redefined relations were italicized and highly recommended relations were
bolded.
Finally, although the survey was carefully and systematically conducted, it was limited by several factors. For one,
the sample size is small because the survey had to be conducted in one-to-one manners. Also, as mentioned earlier,
the results were focus on IT domain. Other factors may include the proficiency of English of the surveyed authors.

Conclusion
Evidence
Guideline
Remark

Table 7: The Proposed Relation Metadata
Counterexample
Deepen
Definition
Explanation
Extension
Example
Introduction
Process
Illustration
Theorem

Demonstration
Fact
Proof

Conclusions and Future Works
It is believed that the reusability of learning objects can be significantly increased if metadata is employed properly.
In this paper, relation metadata were thoroughly studied and extended relation metadata were proposed. Additionally,
a prototype of learning content management system was designed and implemented. Using the learning content
system that support the proposed metadata, many authors believed the learning effectiveness can be increased.
In SCORM CAM, metadata are divided into 9 categories. Currently, only one category (i.e., relation) was studied.
The utilization of other 8 categories has not been fully explored and may worth further investigation. In addition, the
current search mechanism used in the prototype is primitive. It is worth further investigation in the design of search
mechanisms by utilizing metadata (Lu and Jung, 2003) to increase both performance and search precision.

Acknowledgements
This research was partially supported by the National Science Council, Taiwan, R.O.C., under contract no.: NSC942213-E-005-037 and NSC95-2221-E-005-138.

References
Agrawal, R., Imielinski, T., & Swami, A. (1993). Mining association rules between sets of items in large databases. Paper
presented at the ACM SIG-MOD Conference on Management of Data, June 9-12, San Diego, California, USA.
231

Agrawal, R., & Srikant, R. (1994). Fast algorithms for mining association rules. Paper presented at the 20th International
Conference on Very Large Databases, September 12-15, Santiago, Chile.
Borgelt, C. (2007). Apriori - Association rule induction / frequent item set mining. Retrieved October 19, 2009, from
http://www.borgelt.net//apriori.html.
Cover, R. (2006). SGML/XML: Using element and attributes. Retrieved October 19, 2009, from http://www.oasisopen.org/cover/elementsAndAttrs.html.
Fischer, S. (2001). Course and exercise sequencing using metadata in adaptive hypermedia learning systems. ACM Journal of
Education Resource in Computing, 1(1), 1-21.
Fischer, S., Steinacker, A., Seberg, C., & Steinmetz, R. (1999). Multibook: Metadata for web-based learning systems. Paper
presented at the 2nd International Conference on New Learning Technologies, August 30-31, Bern, Switzerland.
Karger, P., Ullrich, C., & Melis, E. (2006). Integrating learning object repositories using a mediator architecture. Paper presented
at the First European Conference on Technology Enhanced Learning, October 1-4, Crete, Greece.
Loser, A., Grune, C., & HoRmann, M. (2002). A didactical model, definition of learning objects and selection of metadata for an
online curriculum, Retrieved January 2, 2010, from http://www.ibi.tu-berlin.de/diskurs/veranst/online_educa/oeb_02/
Talk_Online_Educa__02_Loeser_TU_berlin.pdf.
Lu, E. J.-L. & Jung, Y.-M. (2003). XDSearch: An effcient search engine for XML document schemata. Expert Systems with
Applications, 24(2), 213-224.
Lu, E. J.-L., Wu, B.-C., & Chuang, P.-Y. (2006). An empirical study of XML data management in business information systems.
Journal of Systems and Software, 79(7), 984-1000.
Mann, W. C. & Thompson, S. A. (1988). Rhetorical structure theory: Toward a functional theory of text organization. Text, 8(3),
243-281.
Reigeluth, C. (1999). Instructional-design theories and models, Volume II: A new paradigm of instructional theory. Mahwah, NJ:
Lawrence Erlbaum.
Sddik, A. E., Fischer, S., & Steinmetz, R. (2001). Reusable multimedia content in web-based learning systems. IEEE Multimedia,
8(3), 30-38.
Seeberg, C., Saddik, A. E., Steinacker, A., Reichenberger, K., Fischer, S., & Steinmetz, R. (1999). From the users’ needs to
adaptive documents. Paper presented at the 4th World Conference on the Integrated Design and Process Technology, June 27 July 2, Kusadasi, Turkey.
Steinacker, A., Faatz, A., Seeberg, C., Rimac, L., Hormann, S., Saddik, A. E., & Steinmetz, R. (2001). Medibook: Combining
semantic networks metadata for learning resources to build a web based learning system. Paper presented at the Ed-Media
Conference, June 25-30, Tampere, Finland.
Steinacker, A., Seberg, C., Fischer, S., & Steinmetz, R. (1999). Multibook: Metadata for webbased learning systems. Paper
presented at the 2nd International Conference on New Learning Technologies, August 30-31, Bern, Switzerland.
Ullrich, C. (2003). An instructional component for dynamic course generation and delivery. Paper presented at the Berliner XML
Tage, October 13-15, Berlin, Germany.
Ullrich, C. (2004). Description of an instructional ontology and its application in web services for education. Paper presented at
the Workshop on Application of Semantic Web Technologies for E-learning, November 8, Hiroshima, Japan.
Ullrich, C. (2005). The learning-resource-type is dead, long live the learning-resource-type. Learning Objects and Learning
Designs, 1(1), 7-15.
Ullrich, C. (2006). Discuss forum for the ontology of instructional objects, From http://forum.activemath.org/viewtopic.php, this
discuss forum was closed.

232

Appendix A. The Relations Defined in Instruction Design Theory
The twenty-three relations defined in IDT are described as follows:
Definition
A definition is used to describe the meaning of a word, a phrase, a symbol, or a proper noun which appears in LOs.
For example, e-learning by definition is to engage in learning activities by utilizing information technology and the
Internet.
Fact
A fact is an event that happened. For instance, “Tim Berners-Lee is the creator of HTML” is a fact.
Law
A law describes a general principle that can be found in natural phenomena or statements that have been proven to be
true. The “Law” class also consists of two sub-classes: “Law of Nature” and “Theorem”. For example, Moore’s Law
stated that the number of transistors on an integrated circuit doubles approximately every eighteen months, but the
price reduces by one half.
Law of Nature
A learning object of type “Law of Nature” describes a general rule observed in nature. For example, Newton’s Laws
of Motion is a law of nature in the physics.
Theorem
A theorem is a concept that has been shown to be true. Bayes theorem, for instance, is a theorem.
Process
The “Process” class includes two sub-classes: “Policy” and “Procedure”. A process is a flow of events that describes
how a task can be accomplished in steps. For example, software development life cycle (SDLC) describes the steps
how information systems can be developed.
Policy
A policy describes a set of predefined principles of actions. It is usually composed of informal suggestions or
guidelines for specific activities. For example, interview is a method for system analysts to obtain system
requirements. A good policy for interview is to confirm time with the interviewer and then make sure the she
comprehends the subject in advance. Based on the definition, a policy is similar to a guideline. Ullrich, the creator of
the instructional ontology, also said that the definition of policy is not crystal clear and thus suggested one can make
changes if necessary (Ullrich, 2006). Therefore, we renamed “Policy” to “Guideline”.
Procedure
A procedure is a sequence of steps that can accomplish a goal. An algorithm of, for example, bubble sort is a
procedure.
Interactivity
A learning object of type “Interactivity” is some kind of activities that allow learners to practise or develop a skill
interactively. The “Interactivity” class consists of four sub-classes: “Exploration”, “Real World Problem”,
“Invitation”, and “Exercise”. They have been defined in the “EDUCATION” category in SCORM.
Illustration
An illustration is to illustrate a concept or parts of a concept of a learning object. For example, LOB and LOC shown
in Figure 1 are two illustration objects for the concept of bubble sort.
Example
An example is an auxiliary learning object that is used to further explain parts or the whole of a fundamental learning
object.

233

Counterexample
In IDT, a counterexample is not an example of a fundamental learning object, but it is often mistakenly thought of as
one. For example, a parallelogram is often mistakenly treated as a rectangle. Therefore, parallelogram can be used as
a counterexample for a learning object that describes rectangle.
Evidence
An evidence is a learning object that supports the claims made for a law or any learning object of its subclasses. The
“Evidence” class includes two sub-classes: “Proof” and “Demonstration”. For instance, the time complexity for
bubble sort is O(n2), and the time complexity for quick sort is O(n log n). Thus, this is an evidence that quick sort is
faster than bubble sort.
Proof
A proof is an evidence that is derived formally or mathematically to support a law.
Demonstration
A demonstration is used to demonstrate, in general through experiments, that a law holds under a certain condition.
For example, Galileo’s experiment showed that falling objects of different weights landed at the same time.
Explanation
A learning object of type “Explanation” provides extra information for a fundamental learning object so as to
highlight its important properties. The “Explanation” class includes three sub-classes: “Introduction”, “Conclusion”,
and “Remark”.
Introduction
A learning object of type “Introduction” provides the bird’s-eye view of a fundamental learning object so that
learners have a rough idea what will be covered in a learning resource.
Conclusion
A learning object of type “Conclusion” summarizes key points covered in a fundamental learning object.
Remark
A remark provides extra but inessential information for a fundamental learning object. For instance, when a learning
topic is about entity relationship diagram (ERD), an example remark can be “ERD is similar to the class diagram in
Unified Model Language (UML). ERD is mainly used in structured analysis and design, while UML is used in
object-oriented analysis and design.”

234

Appendix B. The Relations Defined in Instruction Design Theory
The nine relations defined in RST are described as follows:
Example and Illustration
The definitions of “Example” and “Illustration” are identical to the “Example” and “Illustration”, respectively,
defined in the previous section.
Instance
If a learning topic LTA is sorting, it is then said that bubble sort is an instance of LTA.
Restriction
A restriction describes cases where a certain theory fails. For example, in 1640s, Fermat stated that all Fermat
numbers are prime numbers. The formula of Fermat numbers is:
(1)
Fn = 22n + 1
However, Euler found that Fermat number is not a prime number when n is 5 in 1732.
Amplify/Extension
An amplify or extension object is a learning object that is extended from another learning object. For example,
semantic web has grown out of the traditional web.
Continues
A learning object of type “Continues” describes the sequence relationship between two learning objects where one is
performed after the other. For example, LOA represents data before sorting, and LOB represents the data after sorting.
Then, LOB continues LOA.
Deepen/Intensification
A deepen object provides information for another learning object in depth. For example, LOA describes how greatest
common divisor (GCD) is obtained, and LOB describes in details the reasons why the process described in LOA can
obtain GCD. Then, LOB deepens LOA.
Opposition
An opposition describes a statement proposed by a specialist that is contradicting another statement made by another
specialist. For example, when designing XML documents, some experts suggested avoid using attributes can reduce
processing time (Cover, 2006). Still, some experts stated that it can shorten processing time by using attributes (Lu et
al., 2006).
Alternative
A learning object of type “Alternative” describes a thing that has been explained in another learning object but in
different format. For example, LOA describes bubble sort in text, and LOB describes bubble sort in animation. Then,
LOA. However, as stated earlier, the relation “hasformat” is already defined in the
LOB is an alternative to
“RELATION” category. Thus, the relationship “LOA Alternative LOB” can be replaced with the relationship “LOA
hasformat ANIMATION in LOB”.

235

Eskrootchi, R., & Oskrochi, G. R. (2010). A Study of the Efficacy of Project-based Learning Integrated with Computer-based
Simulation - STELLA. Educational Technology & Society, 13 (1), 236–245.

A Study of the Efficacy of Project-based Learning Integrated with Computerbased Simulation - STELLA
Rogheyeh Eskrootchi and G. Reza Oskrochi1*
School of Management and medical Information Sciences, Iran University of Medical Sciences & Health services,
Tehran, Iran // Eskrootchi@iums.ac.ir
1
School of Technology, Oxford Brookes University, Oxford, UK // roskrochi@brookes.ac.uk
*
Contact author
ABSTRACT
Incorporating computer-simulation modelling into project-based learning may be effective but requires careful
planning and implementation. Teachers, especially, need pedagogical content knowledge which refers to
knowledge about how students learn from materials infused with technology. This study suggests that students
learn best by actively constructing knowledge from a combination of experience, interpretation and structured
interactions with peers and teachers when using technology. Simulations do not work on their own, there needs
to be some structuring of the students' interactions with the simulation to increase effectiveness. The purpose of
this study was to investigate the effectiveness of project-based learning in a technology-rich environment. A
science project, Land-use in Watershed, that takes advantage of Internet facilities was developed and integrated
with a simulation software package, Structural Thinking and Experiential Learning Laboratory, with Animation,
(STELLA) developed to promote deeper understanding of Land-use by students. The Participants in the study
were 72 students in a quasi-experimental research design. Statistical analyses showed that students who
participated in the manipulation of the experimental model of the watershed experiment and the STELLA
simulation performed best on understanding the watershed concept.

Keywords
STELLA, computer-assisted simulation, learning technology, watershed concepts and modelling, project based
learning

Introduction
An often-stated belief is that transferring skills is the main job of education. However, an increasing body of research
shows that the way knowledge is presented to students in school and the kinds of operations they are asked to
perform often result in students knowing something but failing to use it when relevant. Brown, Collins and Duguid
(1989) believed that classroom activities lack the contextual features of real-life problem-solving situations and
therefore weaken the ability of students to transfer and apply their knowledge from the school setting to the outside
world. The challenge as Santos-Trigo and Camacho-Machín, (2009) purposed is to question; “Can routine problems
be transformed into problem solving activities that promote students' mathematical reflection?”
Studies suggest that in order to facilitate transfer, promote effective learning and encourage a high degree of
ownership and personal relevance, educators should provide training on real tasks. Similarly, researchers believe that
cases and examples must be studied as they really occur, in their natural contexts, not as stripped-down ‘textbook
examples’ that conveniently illustrate some principle (Blumenfeld et al., 1991). The extent at which the process of
solving textbook problems can help students develop a way of thinking to be consistent with mathematical practice is
still under an investigation (Santos-Trigo & Camacho-Machín, 2009).
The National Research Council’s (NRC, 1999) standards for science education suggest long-term inquiry activities
including argumentation and explanation, communicating ideas to others and using a wide range of manipulative,
cognitive and procedural skills will promote learning. The standards suggest that to develop their understanding,
students need to relate new information to existing knowledge and build connected networks of concepts. In
addition, NRC (1999) called for the teaching of “fluency with information technology” and strongly recommended
the use of technology to promote understanding of science and mathematics.
Theorists and educators are promoting reality-centred projects and other reality-centred activities as ways to engage
students in meaningful learning. Experienced educators tend to agree that students learn best through a project-based
approach in which they are able to discover things for themselves and take advantage of technological tools
(Blumenfeld et al., 1991; Clinchy, 1989; Linn, et al., 2000; Lebow, & Wager, 1994).
ISSN 1436-4522 (online) and 1176-3647 (print). © International Forum of Educational Technology & Society (IFETS). The authors and the forum jointly retain the
copyright of the articles. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that copies bear the full citation on the first page. Copyrights for components of this work owned by
others than IFETS must be honoured. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior
specific permission and/or a fee. Request permissions from the editors at kinshuk@ieee.org.

236

While technology can be valuable in supporting students and teachers in projects requiring higher level thinking
(Blumenfeld et al., 1991), it is not the kind of technology that matters most, but rather how it is used (Dyrli &
Kinnaman, 1994; Ehrmann, 1995; Green & Gilbert, 1995). The intention of the researcher in this study was to
examine and discuss ways in which learners engage in an intentional learning process and analyze the effectiveness
of such approach in two areas: a) an authentic learning activity in the content of project-based learning; and b)
educational simulations.

Project-based learning
Project-based learning (PBL) blends traditional subject-matter goals and objectives with authentic learning
environments. The primary rationale for using authentic activity as the model for appropriate learning activities is the
enhanced understanding that develops through application and manipulation of knowledge within context. Finding
solutions to a problem whether posed by the teacher or a new social environment, more likely develop generic, as
well as subject specific skills when using project-based curriculum. In other words, PBL provides productive
environments for the development of meta-cognition (Downing et al., 2009). In addition, in another study conducted
to identify dental students' self-reported sources of stress, the findings revealed that PBL when compared to
traditional curricula was inversely associated with perceived stress and that in turn had a strong impact on learning
(Polychronopoulou & Divans, 2009). Nevertheless, transformation of the conventional classroom into an authentic
learning environment involves much more than incorporating features of real-life situations into school work.
Furthermore, curriculum innovations are never easy to implement or to examine systematically. Balasooriya et al.,
(2009) carried out a study on the impact of a new integrated medical educational design on students' approaches to
learning. Although, the program was based on curriculum features identified in the research literature to promote
deeper approaches to learning, the results indicate shifting students towards deeper approaches to learning may be a
more complex task than previously understood.
Krajcik et al., (1994) suggest that there are five features of PBL that help communicate the complexity of the
innovation in terms that are familiar to teachers. These are driving questions, investigations, artefacts, collaboration
and technological tools. This approach can be supported by multimedia and network technologies such as the
Internet. The introduction of microcomputers into classrooms has generated innumerable instances of such
innovations which involve considerable change in classroom management, lesson structure and student assessment.
Powerful hardware and sophisticated software tools are enabling people to become more active learners about their
environment (Jackson et al., 1997).
Research has shown that students can make significant gains when computers are incorporated in the learning
process. Computer-based technologies integrated in project-based learning are particularly useful for constructive
learning (Roschelle et al., 2000). Students instantaneously can see the results of their experiment. Computer
technology supports learning; it is especially useful in developing the higher-order skills of critical thinking, analysis
and scientific inquiry (Roschelle et al., 2000). But the mere presence of computers in the classroom does not ensure
their effective use. Many factors influence how and who learns in the classroom: (1) active engagement, (2)
participation in groups, (3) frequent interaction and feedback and (4) connections to real-world contexts. Omale et
al., (2009) performed a study to investigate how the attributes of 3-D technology affect participants' social, cognitive
and teaching presences in a PBL environment. The results indicated that although the attributes of 3-D technology
promoted participants' social presence, additional technical and instructional features of the 3-D environment were
required to further enhance cognitive and teaching presence leading to overall learning experience. Some of the
pioneers in learning research believe computer simulation when used effectively has the potential to address these
related factors (Blake & Scanlon, 2007).

Educational Simulations
Simulations as defined by Alessi and Trollip (2001) are a representation of some phenomenon or activity that users
learn about through interaction with the simulation. Simulations offer an easy way of controlling experimental
variables, opening up the possibility of exploration and hypothesizing. Simulation is also valuable in presenting
many types of representational formats including diagrams, graphics, animations, sound and video that can facilitate
understanding.
237

Pea’s framework of distributed intelligence suggests that computer-assisted simulations have the potential to
reorganize mental processes by closing the temporal gaps between thought and action and between hypothesis and
experiment. Pea has proposed that by allowing the user to engage in "what-if thinking" through a partnership
between user and technology, deep qualitative effects are made possible on how problem solving occurs (Lebow &
Wager, 1994). From an instructional design perspective, educational simulations support predetermined learning
outcomes by providing participants with opportunities to deal with the consequences of their actions and to respond
to feedback. In other words, the simulation construction kit is a laboratory for scientific inquiry, for exploration,
explanation and testing. STELLA, which stands for Structural Thinking and Experiential Learning Laboratory, with
Animation, seems to facilitate this disciplined approach to inquiry (Steed, 1992). Understanding how a simulation
construction kit, like STELLA, can be used to refine thinking is important. Simulation models are simplified
representations of real-world systems over hypothetical time. Using simulation software, characteristics of selected
variables can be altered and their effects on other variables and the entire system assessed (Steed, 1992). STELLA is
a program designed to assist users in creating their own simulations using system dynamics. One needs to think in
terms of dynamic processes, positive and negative causal loops, flows, accumulation and converters. STELLA is one
technology that can enable individuals to enhance their understanding of and appreciation for the complex web of interrelationships that govern environmental behaviour (Peterson, 1985). The real value of the STELLA modelling
package is the cognitive processing that goes on in the creation and development of its model. Good science is good
questions. Through creating simulations one has to generate good questions and as the simulation evolves interesting
inquires are naturally pursued.
The simulation modelling generally takes two forms. Depending on the courses, students are (a) required to develop
their own models of scientific phenomena, or (b) given existing models and are asked to alter particular parameters
to examine the subsequent effects on the entire system. These two distinct approaches to modelling are likely to
produce different cognitive outcomes in terms of content knowledge and general problem-solving skills. In this
paper, we are considering interactive simulations (Blake & Scanlon, 2007) that allow students to change some of the
parameters in the program and observe what happens as a result. Although theory supports using technology to
engage students in project-based learning, and the literature provides descriptions of suitable classroom technology
to engage students, we have few case studies of middle-school teachers describing their development and effective
use of simulation in educational learning (Blake & Scanlon, 2007 and Steed, 1992). In fact, with the re-emergence of
experiential learning as a dominant model of learning in education and the recent research on infusing information
technologies into classrooms, it is a good time to examine the effectiveness of project-based learning integrated with
computer-based simulation.
The study reported here explores the effectiveness of the project-based learning which takes advantage of simulation
experiment. The following research question was proposed: How effective is the reality-based project in engaging
students in meaningful learning and enhancing their motivational attitude, especially when integrated with computerbased simulation?
A science project, Land-use in Watershed, funded by the National Science Foundation (NSF) was developed to
investigate this research question. The project was integrated with STELLA simulation software to enhance deeper
understanding for students. Land-use in Watershed was a collaborative science project which was developed by the
researcher for the Kansas Collaborative Research Network (KanCRN).

Method & Aims
Procedure & Sample
The participants of this study were 72 sixth to eighth graders, (32 males and 40 females). All students attended
Northwest Middle-school, Kansas city, Kansas at the time of the study. Three separate multi-age classrooms were
included in the study. The multi-age classrooms were randomly allocated to one of the three treatment groups.
Students from all groups read the project materials on-line through KanCRN and benefited from on-line learning
features such as collaboration with peers, finding definitions of related terminologies and using hyperlinks to
additional information. Teaching material was delivered by the same teacher. Students were pre-tested with respect
to their understanding of the watershed concept and their content knowledge. The first treatment group (n1=19) the
Project-Based group (PB), were taught the subject by receiving a traditional lecture. The second treatment group
(n2=33), the Project-Based Experimental Simulation group (PBES), were taught the subject by performing an
238

experimental model and a simulation model. The third group n3=20, the Project-Based Simulation group (PBS),
performed a simulation model but not an experimental model. Students were further divided into sub-groups of three
or four within each classroom based on their pre-test score in order to improve learning while obtaining homogenous
groups.
The researcher prepared a lesson on the “effect of land-use on the watershed” in which she designed an experimental
model of the watershed using sponge and cardboard. Two simulation applications using STELLA software were also
designed by the researcher to further emphasize the concept of watershed and in particular the effect of land-use on
runoff. The first application represented the experimental model and was created using experimental data obtained
from the sponge-cardboard experiment (STELLA1). The second application was more advanced and was created by
real data from a watershed (STELLA2). The simulation models played a role as a supplemental tool for practicing
“what if scenarios” by manipulating interacting factors and understanding the impact of important parameters on the
watershed.

Hypotheses
The first null hypothesis (H01) is: gain in students’ content knowledge is the same between groups. The second null
hypothesis (H02) is: students’ comprehension knowledge is the same between groups.
The third null hypothesis (H03) is: students’ attitude towards the project is the same between groups.
Instruments
A 58-question student survey was used to collect four types of information:
I.
Eight true/false questions; used in statistical analysis to test gain in content knowledge.
II.
Seven open-ended questions to test students’ understanding of the watershed concept.
III.
Twenty-three multiple-choice questions that tests the students’ attitude toward the project.
IV.
Five true/false questions that gathered information about students’ computer background knowledge.
In addition, information about the amount of time spent on reading the project on-line, type and amount of smallgroup interaction and number of requests for help in different parts of the project were collected by observation.

Results
The design of the study included three independent factors: gender (male or female), grade (sixth, seventh or eighth),
treatment (PB, PBES or PBS) and computer background as a categorical covariate on the 5-points Likert scale.
The dependent variables consisted of three measures: content knowledge; comprehension of the subject; students’
attitudes towards the project.
The results of a one way ANOVA test failed to reject H01 (Pvalue >0.05) and therefore conclude that the gain in
students’ content knowledge is not significantly different between groups (Table 1). In contrast H02 was rejected at
5% significance level, hence students’ comprehension knowledge is significantly different between groups (Pvalue =
0.002). A Post Hoc test result indicated that students in the PBES (mean = 8.61, SD = 2.42) treatment groups
outperformed the other two groups (PBS; mean=5.45 SD=3.78 & PB; mean=4.16 SD=2.52) on subject
comprehension.
Table 1: Mean change and significance level of comprehension knowledge between treatments
Treatment (J)
Mean (I-J)
Treatment (I)
Project-Based Experimental Simulation
Project-Based Simulation
3.162
Traditional
4.450
Project-Based Simulation
Traditional
1.294
Based on observed means. The mean difference is significant at the .05 level.

Sig.
.002
.000
.692
239

Second, analysis of interaction between gender and treatment showed that there were no significant interaction
effects between the PB and PBES groups within the male category while the females in the PBES group performed
significantly higher than the females in the PB group. This interaction between gender and treatment variable
indicated that the project had a stronger effect on females and led to the higher mean score for the PBES group in
comparison to the PB group.
Further analysis suggested that the gain (value added) in students’ comprehension of the watershed concept was
independent of the students’ grade-level.
Students’ attitudes toward the project, especially toward STELLA simulation, were promising (85%). Students who
experienced STELLA units and enhanced their understanding about the watershed through this activity found the
reading part of the project that they encountered prior to the STELLA lesson less difficult.
The analysis of data on students’ computer background indicated that only 2% of the students have never used a
computer before, 61% of students use a computer at home, 76% of students had previous experience with computer
simulation, but 80% of students had never worked with STELLA.

Figure 1: Students’ comprehension gains by treatments for male and female

Discussion
This study emphasizes on two important and interesting findings. First result indicated that although the PBES
performed significantly higher than the PB class, the PBS group which used computer-based simulation but did not
perform the experimental part of the project (Sponge-Cardboard model) and consequently could not benefit from the
project to its full potential did not score higher than the PB class. According to NCTM, (Linn et al., 2000, pp. 25)
240

this would further emphasize that “technology should not be used as a replacement for basic understandings and
intuitions; rather, it can and should be used to foster those understandings and intuitions”. Furthermore, the PBES
groups outperformed the PBS as well. However there was no significant difference between the two classrooms
treated with PBES.

Simulation and its Role
An explanation of the high performance of the PBES group could be due to the experience they gained first through
the experimental model of the watershed which generated data for modelling the first STELLA application
(STELLA1), and enhanced deeper appreciation of a more advanced and complicated model (STELLA2). Many
studies have identified the benefits of combining technology and other materials, such as construction kits or handson experiments for projects to make projects successful (Linn et al., 2000). For example modelling environments
such as Model-It combine the strengths of technology with opportunities for personal data collection. Technology
can support and encourage those projects and activities that are more meaningful to students (Roschelle et al., 2000).
Research indicates that simulations can play an important role in science learning, however their use is not as
straight-forward as it seems. Many researchers emphasize the importance of a good instructional plan when using
simulations. They argue that despite their popularity in instruction, the lack of support for learners in some
simulations is the reason for finding no conclusive evidence for effectiveness and efficiency of simulations (Blake &
Scanlon, 2007).

PBL and Interactions
In the PBES treatment group there is more opportunity for collaboration during the experiment and simulation, hence
more interaction between students, and also with the teacher and the model compared to the other two methods. This
finding also supports Schutte’s (1997) suggestion that the enhanced levels of interaction with other students and the
teacher results in greater efficacy of computer-mediated communications. Another study demonstrates that all
learning activities such as constructive, self-directed and collaborative learning occur as a result of verbal
interactions through PBL environment (Yew & Schmidt, 2009).
Two factors explain reasons for improvement of problem-solving skills in a group work (Teaching Professor, 2009).
One is the knowledge that students acquire from other students' explanations and another is the students' involvement
to solve the problem causes them to think more deeply about the problem and its solution. Other findings report that
successful innovative programs improve instruction for all students and have the greatest impact on those who are at
greatest risk for learning less (Linn et al., 2000). Chu et al., (2009) indicate that in spite of most e-learning platforms
that offer theoretical knowledge content, a problem-based e-learning (PBeL) model which incorporates the PBL
theory, social constructivism, and situated learning theories assist teachers in effectively developing practical
knowledge for mathematics teaching for students with mild disabilities.
On the other hand, according to Linn et al., (2000), adding technology to science instruction has the danger of
increasing biased stereotypes and promoting the idea that these are male domains; however, if used effectively,
technology can connect science to problems that interest individuals who have been assumed under-represented in
science careers. As Linn et al., (2000) indicate, although there is often a belief that male students may make greater
usage of information technology, some studies suggest that females and members of various cultural groups who
have fewer opportunities for learning science and mathematics in the elementary and middle school in traditional
practice can benefit from new technological innovations where students work in groups, and therefore males and
females have equal contributions to the discussion. Likewise, these reports support the second finding of this study.
The interaction between gender and treatment variable indicated that the project had a stronger effect on females and
led to the higher mean score for the PBES group in comparison to the PB group. In contrast, the PBES treatment had
no significant effect on the male students (Figure 1). Basically, this interesting finding supports a few researchers’
views and requires additional investigation. Figure 1 shows the average score gained for males and females by
treatment group.

241

PBL and Critical Thinking
Overall, the findings of this study supported the use of the project-based experimental simulation on learning
outcome. The result of this study indicated that the students’ comprehension of the watershed concept had improved
as a result of the innovative approach, although it did not improve students’ content knowledge (multiple-choice
questions) significantly. One possible explanation for this finding is the lack of sufficient time to cover multiple
concepts in the PBES groups. While the PBES groups spent their time on various activities to comprehend a few
concepts deeply, the PB group focused on direct instruction for receiving and memorizing multiple concepts. This in
turn will explain why PBES students performed better in comprehension. Examination of these students’ responses
indicated that they achieved better and deeper understanding of the watershed concept. They were able to interpret
the graphs of runoff, absorbed water in the ground and inflow correctly and in more detail. Similar results were
obtained in the study by Şendağ and Ferhan (2009). This study investigated how the online PBL approach employed
in an online learning environment influenced undergraduate students’ critical thinking skills and content knowledge
acquisition. The results indicated that learning in the online PBL group did not have a significant effect on the
content knowledge acquisition scores but it had a significant effect on increasing the critical thinking skills.
Furthermore, this finding is consistent with the report provided by NSF (Linn et al., 2000) on The Middle School
Math through Applications Project (MMAP), a series of project-based units that offers the most technology-intensive
middle school curriculum. MMAP uses the HabiTech application, a simplified version of STELLA. According to
NSF, the evaluation of this unit of MMAP was very positive, indicating that the unit met the visions of the 1989
NCTM standards, engaged students and enhanced mathematical communication in classrooms. Roschelle et al.,
(2000) also agreed that innovative computer-based simulation demonstrated significantly higher gains compared to
those receiving only traditional instruction.
Moreover, it has been concluded that the gain in students’ comprehension of the watershed concept was independent
of the students’ grade-level. This finding further emphasized that the multi-age computer-mediated group as a whole
performed better in comprehension than the multi-age traditional group due to the treatment effect.
The promising students’ attitudes toward the project, especially toward STELLA simulation, might be due to their
positive perception of the whole project afterwards. The NSF’s evaluation report also indicated that MMAP has
positive effects on students’ attitudes as well as on their performance (Linn et al., 2000).

Research Role
Research is moving ahead to introduce the computer into the classroom to ensure this technology is used effectively
to enhance learning. Research should also help teachers to teach with technology rather than to use computers for
personal productivity. Teachers, especially, need pedagogical content knowledge which refers to knowledge about
how students learn from materials infused with technology. Successful technology use and effective learning for
science teaching is dependent on the teachers’ knowledge of the technology itself, and how a particular tool is best
utilized for particular purposes, classroom or laboratory settings, and students themselves (Hennessy, 2006).
Simulation encourages the student to interact with the variables, understand their sensitivities and appreciate how a
change in one variable results in changes in other variables. However, we have shown here that the success of
simulations as effective learning tools is dependent on how simulations are used. Finding ideal uses of technology in
science instruction remains an active research area, and the technology itself is a “moving target,” as new projects
emerge on a regular basis. As Chiocchio and Lafrenière (2009) recommend, teamwork and technology are becoming
important components of PBL in academic settings but fostering computer-assisted teamwork is complex and time
consuming. Knowing how and when to intervene would prove useful. Finally, research demonstrates that
technological tools can enhance learning in science and mathematics, in a PBL setting, since they allow more
personalized and project-oriented commitments (Linn et al., 2000). According to Hakkarainen (2009), PBL offers a
good model to support students’ knowledge and skills, and students will benefit from learning with and about
technology such as computer-based simulations in science and mathematics instruction. Nevertheless, effective
incorporation of these technologies into the curriculum has been controversial, difficult and demanding.

242

Conclusion
In conclusion, the method by which features of project-based learning should be implemented to have its full impact
on learning requires further investigation. Also as it was noted, many studies suggest that project-based simulations
for visualization and modelling have transformed some fields of science and seem promising for elementary and
middle school instruction. However, research indicates that incorporating computer-simulation modelling into
project-based learning requires careful planning and implementation. Recent research demonstrates simulations of
complex relationships, such as graphs of change over time, connections between components of a geometric
construction, and location of a local minimum in a three dimensional surface can help students learn these difficult
topics. At the same time, considerable research suggests that all simulations are not successful. Often simulations fail
because they are too complex or too difficult to understand. In fact, in replicated studies, researchers have noted
significant improvements in students’ understanding of scientific concepts and motivation when using simulation
software, but few studies elaborate on reasons of its failure or underling success.
In this study, the researcher administered the same simulation model to both groups PBS and PBES and conduct an
experimental model only to one group (PBES) in order to demonstrate that although the structure and the design of
the STELLA models were the same and presented similarly to both groups, the PBES group significantly
outperformed PBS. Nevertheless, the number of interactions of students with the simulation models and themselves
also increased as a result of deeper insight gained with experimental model in PBES.
This research suggests that once the learners learn the bases of using simulation, in this case through experimental
model, it enables them to interact efficiently and effectively with the more complicated models. In other words,
experimental model in this study helps the students to understand how primary data and flows operate in the first
simulation model (STELLA1) because the model was built upon those data, therefore allowing them to build up
deeper intuitions into the simulation model and its role, which in turn pursue them to appreciate more advanced
subjects that is impossible to perform in natural settings, through a more complicated simulation model (STELLA2).
This is an analogy for learning a new language by learning bases of vocabularies and grammatical rules which results
in writing more sophisticated sentences, in comparison with learning through environment. This study also suggests
that students learn best by actively constructing knowledge from a combination of experience, interpretation and
structured interactions with peers when using simulation in a PBL setting. This is only possible by a careful step by
step instructional design in which simulation model should solely be considered as one component of whole
pedagogical structure of the PBL. The challenge is to ensure that simulation is used effectively to enhance how and
what children learn. Careful planning is required for constructing a project to be purposeful and uses technology
specially computer simulations such as STELLA in a constructive, real-world manner. Simulations do not work on
their own, there needs to be some structuring of the students' interactions with the simulation to increase
effectiveness, as it was initiated through experimental model causing to raise the quality and quantity of
interactions with the STELLA models.
Hence, the first important finding of this research indicates that simulation models such as STELLA are used to
expand students' experience of experimental science and should not be used as a substitution for basic
understandings and intuitions; one can promote students to higher level STELLA model only when they learned the
basic role of STELLA. . The second important finding advocates that in spite of common belief that the use of
technology is male dominant activity, a few studies such as this paper suggest females and those who are at risk for
learning less in traditional practice can benefit from new technological innovations. This study strongly suggests
further research in this area.
In addition, it should be noted that the experimental model may only be one way to promote students’ understanding
of and interactions with STELLA model in this PBL. As an enhancement to this project, development and evaluation
of further PBL in which students are required to create their own STELLA models (integrating developing
simulations not interactive simulations) instead of using experimental models is highly recommended. Computer
model developments are mirrors of one’s own mental development. Model building is an interactive process; moving
from identification of causal loops to computer simulation provides deep involvement in the topic and consequently
deep understanding of the subject as well as STELLA mission.

243

Furthermore, it is recommended that further research be conducted with randomly selected participants to be truly
representative for the study. Also, as this study suggests, it would be desirable to have a clear measure of
effectiveness before committing to continual investment in technology.
Nevertheless, it is important to note that the results of this study are limited to the integration of simulation model, in
particular STELLA, in a PBL settings and cannot be extended to all kind of educational technologies nor to other
learning strategies.

Acknowledgment
We would like to acknowledge and extend our gratitude to Dr. V. Simonite and Dr. R. Long for their comments and
corrections.

References
Alessi, S.M., & Trollip, S.R. (2001). Multimedia for Learning, Boston, MA: Allyn and Bacon.
Balasooriya, C. D., Hughes C., & Toohey S. (2009). Impact of a new integrated medicine program on students' approaches to
learning. Higher Education Research & Development, 28(3), 289-302.
Blake, C., & Scanlon, E. (2007). Reconsidering simulations in science education at a distance: features of effective use. Journal of
Computer Assisted Learning, 23(6), 491-502.
Brown, J. S., Collins, A., & Duguid, P. (1989). Situated cognition and the culture of learning. Educational Researcher, 18(l), 3242.
Blumenfeld, P. C., Soloway, E., Marx, R. W., Krajcik, J. S., Guzdial, M., & Palincsar, A. (1991). Motivating project-based
learning: Sustaining the doing, supporting the learning. Educational Psychologist, 26 (3&4), 369-398.
Chiocchio, F., & Lafrenière, A. (2009). A project management perspective on student's declarative commitments to goals
established within asynchronous communication. Journal of Computer Assisted Learning, 25(3), 294-305.
Chu, H., Chen, T., Lin, C., Liao, M., & Chen, Y. (2009). Development of an adaptive learning case recommendation approach for
problem-based e-learning on mathematics teaching for students with mild disabilities. Expert Systems with Applications, 36(3),
5456-5468.
Clinchy, E. (1989). Education in and about the real world. Equity and Choice, 3, 19-29.
Downing, K., Kwong, T., Chan, S., Lam, T., & Downing, W. (2009). Problem-based learning and the development of
metacognition. Higher Education, 57(5), 609-621.
Dyrli, O.E. & Kinnaman, D.E. (1994). Integrating technology into your classroom curriculum. Technology and Learning, 14(5),
38-44.
Ehrmann, S.C. (1995). Asking the right questions. Change, 27(2), 20-27.
Green, K., & Gilbert, S. (1995). Great expectations: Content, communications, productivity and the role of information
technology in higher education. Change, 27(2), 8-18.
Hakkarainen, P. (2009). Designing and implementing a PBL course on educational digital video production: lessons learned from
a design-based research. Educational Technology Research & Development, 57(2), 211-228.
Hennessy, S. (2006). Integrating technology into teaching and learning of school science: a situated perspective on pedagogical
issues in research. Studies in Science Education, 42, 1-48.
Jackson, D., Bourdeau, G., Sampson, A., & Hagen, T. J. (1997). Internet Resources for Middle School science: Golden
Opportunity or “Silicon Snake Oil”? Journal of Science Education and Technology, 6(1), 49-57.
Krajcik, J. S., Blumenfeld, P. C., Marx, R. W., & Soloway, E. (1994). A collaborative model for helping teachers learn projectbased instruction. Elementary School journal, 94, 483-497.
Lebow, D. G., & Wager, W. W. (1994). Authentic activity as a model for appropriate learning activity: Implication for design of
computer-based simulations. Paper presented at the 1994 National Convention of the Association for educational
Communications Technology Sponsored by the research and Theory Division, February 16-20, Nashville, TN, USA.
244

Linn, M. C., Kessel, C., Lee, K., Levenson, J., Spitulnik, M., & Slotta, J. D. (2000). Teaching and learning k-8 mathematics and
science through inquiry: Program reviews and recommendations. Retrieved January 10, 2010, from http://www.metiri.com/
Solutions/k8ReportSubmitted.doc.
National Research Council (1999). Being fluent with information technology, Washington, DC: National Academy Press.
Omale N., Hung W., Luetkehans L., & Cooke-Plagwitz J. (2009). Learning in 3-D multiuser virtual environments: Exploring the
use of unique 3-D attributes for online problem-based learning. British Journal of Educational Technology, 40(3), 480-495.
Penrose, R. (1989). The emperor’s new mind: Concerning computers, minds, and the laws of physics, Oxford: Oxford University
Press.
Peterson, S. (1985). Using STELLA in environmental Education. Environmental education report and newsletter, 14(2), 13-18.
Polychronopoulou, A., & Divans, K. (2009). Dental Students' Perceived Sources of Stress: A Multi-Country Study. Journal of
Dental Education, 73(5), 631-639.
Roschelle, J. M., Pea, R. D., Hoadley, C. M., Gordin, D. N., & Means, B. M. (2000). Changing How and What Children Learn in
School with Computer-Based Technologies: The Future of Children. Children and Computer Technology, 10(2), 76-101.
Santos-Trigo, M., & Camacho-Machín, M. (2009). Towards the Construction of a Framework to Deal with Routine Problems to
Foster Mathematical Inquiry. Problems, Resources & Issues in Mathematics Undergraduate Studies, 19(3), 260-279.
Şendağ, S., & Ferhan, O. H. (2009). Effects of an online problem based learning course on content knowledge acquisition and
critical thinking skills. Computers & Education, 53(1), 132-141.
Steed, M. (1992). Stella, A simulation construction kit: Cognitive process and educational implications. Journal of Computers in
Mathematics and Science Teaching, 11(1), 39-52.
Schutte, J. G. (1997). Virtual Teaching in Higher Education: the new intellectual superhighway or just another traffic jam?
Retrieved May 1, 2009, from http://www.csun.edu/sociology/virexp.htm.
Teaching Professor (2009). Why Group Work Improves Problem-Solving Abilities. Retrieved May 1, 2009, from
http://www.magnapubs.com/issues/magnapubs_tp/23_5/.
Yew, E., & Schmidt, H. (2009). Evidence for constructive, self-regulatory, and collaborative processes in problem-based learning.
Advances in Health Sciences Education, 14(2), 251-273.

245

Chen, L.-J., Ho, R.-G., & Yen, Y.-C. (2010). Marking Strategies in Metacognition-Evaluated Computer-Based Testing.
Educational Technology & Society, 13 (1), 246–259.

Marking Strategies in Metacognition-Evaluated Computer-Based Testing
Li-Ju Chen, Rong-Guey Ho* and Yung-Chin Yen
Graduate Institute of Information and Computer Education, National Taiwan Normal University, Taipei Taiwan //
ljchen@ice.ntnu.edu.tw // hrg@ntnu.edu.tw // scorpio@ice.ntnu.edu.tw
*Contact author
ABSTRACT
This study aimed to explore the effects of marking and metacognition-evaluated feedback (MEF) in computerbased testing (CBT) on student performance and review behavior. Marking is a strategy, in which students place
a question mark next to a test item to indicate an uncertain answer. The MEF provided students with feedback
on test results classified as correct answers with and without marking or incorrect answers with and without
marking. The study analyzed 454 ninth graders randomly assigned to three groups: Gmm (marking + MEF), Gmu
(marking), and Guu (none). Each group was further categorized into three subgroups based on their English
ability. Results showed that marking improved medium-ability examinees’ test scores. This was a promising
finding because the medium-ability students were the very target group that had the most potential for
improvement. Additionally, MEF was found to be beneficial as well in that it encouraged students to use
marking skills more frequently and to review answer-explanations of the test items. The follow-up interviews
indicated that providing adaptive and detailed AEs for low-ability students were necessary. The present study
reveals the potential of integrating marking and adaptive feedbacks into the design of learning functions that are
worth implementing in CBT systems.

Keywords
Computer-based testing (CBT), Test-taking behavior, Marking behavior, Metacognition evaluation, Confidence
rating technique

Introduction
Computer-based testing (CBT) has been widely used since information technology became popularity. Such tests are
easily administrated by computer or an equivalent electronic device, and students can immediately access their test
results. Many researchers claimed that CBT systems were valuable self-evaluation tools for self-managed learning
(Croft, Danson, Dawson, & Ward, 2001; Peat & Franklin, 2002). However, studies indicated that, for effective and
efficient use as self-managed learning tools, CBT systems must provide adaptive feedback for future learning
(Souvignier & Mokhlesgerami, 2006; Thelwall, 2000; Wong, Wong, & Yeung, 2001). They must also provide
information that enables students to control their own pace during the test (Parshall, Kalhn, & Davey, 2002, p. 41).
Adaptive feedback enabled students to learn according to provided instructional strategies (Collis & Messing, 2000;
Collis & Nijhuis, 2000). According to Collis, De Boer and Slotman (2001), giving adaptive feedback after a test was
one strategy for helping students learn effectively. It could help underachievers extend their learning. For example,
giving answer-explanations (AEs) related to key knowledge concepts of test items after a CBT could help students to
understand what they have learned and to identify their mistakes (Wang, Wang, Huang, & Chen, 2004); that is, AEs
was a metacognitive strategy (Rasekh & Ranjbary, 2003). Answer-explanations offered via automatic evaluation
tools could correct student mistakes, reinforce their memories, and support their learning as well as reduce teacher
workload so that individual students could receive adaptive compensatory instruction in a forty-student class.
Therefore, if CBT systems only displayed scores without feedback, the “teachable moment”, or the moment of
educational opportunity when students were disposed to learn, might not be used effectively (Collis et al., 2001;
Ram, Cox, & Narayanan, 1995).
To help students control their own pace, CBT systems could provide the information needed to navigate a test, such
as reminders of unanswered items. Gibson, Brewer, Dholakia, Vouk and Bitzer (1995) showed that such information
could help students complete the CBT efficiently and reduce their frustration and anxiety. Another mechanism for
controlling the testing process within the CBT environment was the marking function. Marking was a skill used to
increase the efficiency and effectiveness of self-managed learning (Parshall et al., 2002, p34). In the present study,
marking referred to a test-taking behavior, in which the student placed a question mark next to a test item to indicate
an uncertain answer, and it also served as a reminder to review, check or revise the answer. According to Higgins
and Hoffmann (2005), students rarely marked test items when they were sure of their answers. Therefore, marking
could be considered one alternative to the confidence rating technique conventionally used to measure the
ISSN 1436-4522 (online) and 1176-3647 (print). © International Forum of Educational Technology & Society (IFETS). The authors and the forum jointly retain the
copyright of the articles. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that copies bear the full citation on the first page. Copyrights for components of this work owned by
others than IFETS must be honoured. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior
specific permission and/or a fee. Request permissions from the editors at kinshuk@ieee.org.

246

metacognition monitoring ability of students. Students applying confidence rating technique were required to check
the confidence degree of their answers. Their metacognition monitoring ability was then evaluated by the matching
the confidence degree with the test results (Baker & Brown, 1984; Vu, Hanley, Desoete, & Roeyers, 2006; Strybel,
& Proctor, 2000). For example, choosing a correct answer and marking it high on confidence level suggested good
metacognition monitoring ability whereas choosing a wrong answer and marking it high on confidence level
indicated poor metacognition monitoring ability.
This study proposed metacognition-evaluated feedback (MEF), a new feedback mode for CBT systems displaying
AEs integrating student answer responses and marking records. This study had two purposes. First, it explored
whether marking could improve the test scores of examinees. Second, it investigated how MEF affected the review
behavior of students after completing a CBT. To achieve these two purposes, an experiment was designed to address
the following questions:
1. Does marking improve student scores?
2. Does MEF increase use of marking skills and review behavior?

Related research
Test-taking behavior and marking
Test-taking behavior varies among students. Researchers generally classified test-taking behaviors into nine types:
(1) browsing items, (2) clarifying meanings of item body and options, (3) knowing the answer, (4) not knowing the
answer and guessing, (5) omitting, (6) abandoning, (7) not reaching, (8) having partial knowledge that might be right
or wrong, and (9) changing answers (Brown, 1980; Lazarte, 1999; Lord, 1975; McMorris & Leonard, 1976).
Examinees usually used marking skills under type (8) and (9) conditions (Burton, 2002; Parshall et al., 2002, p.34)
because marking was a helpful test-taking technique for checking answers. However, most CBT systems described in
the literature did not incorporate the marking function (Gibson et al., 1995; Parshall et al., 2002, p.34).
Marking was a direct test-taking strategy used by students. It helped examinees remember the test items they skipped
or wanted to recheck. The marked test items could then be changed according to partial knowledge or the test-taking
skills of the examinees (Burton, 2002). Rogers and Bateson (1991) concluded that good test-taking skills and
knowledge of a certain subject could help examinees improve their scores by identifying clues embedded in the test
items. Therefore, marking was likely to enhance student performance because it could make them focus on specific
items. However, current CBT systems such as Mklesson, Tutorial Gateway, Eval and Open Learning Agency of
Australia (Gibson et al., 1995), LON-CAPA (http://www.lon-capa.org), and TopClass (http://www.websystems.com)
(Bonham, Beichnen, Titus, & Martin, 2000; Wang et al., 2004) did not analyze marking behavior. Briefly, a
noticeable problem of the current CBT systems was that they did not incorporate the marking function. In CBT
systems without the marking function, examinees might not focus on the items they needed to reconsider. Therefore,
this study attempted to overcome this problem by designing a CBT system with marking function.

Confidence rating technique
Marking indicated student confidence as well as a remainder to recheck test items (Parshall et al., 2002, p.34;
Higgins et al., 2005). For example, students might put a check mark beside a test item to indicate that they were not
sure of the answer. Restated, marking was an alternative approach for judging the confidence level of examinees,
which was traditionally measured by using confidence rating technique to estimate metacognition monitoring ability
(Baker & Brown, 1984). Other measurement methods, such as interview, observation, thinking aloud, self-reporting
and questionnaire survey, have also been used in past studies (Desoete & Roeyers, 2006; Elshout-Mohr, Meijer, van
Daalen-Kapteijns, & Meeus, 2003; Garner, 1988, p61). However, each had drawbacks. The analytic results of
interview, observation and thinking-aloud were accurate but time-consuming. Moreover, coherent results were
difficult to obtain because these measurement methods often involved subjective evaluations (Veenman, 2003). Also,
the results of self-reporting and questionnaire survey might induce ‘response set’ problems such as careless
answering or acquiescence and social expectations (Garner, 1988, p.61; Linn & Gronlund, 2000, p.182). Therefore,
this study employed marking as a confidence rating technique for the benefits of its stability, efficiency and
practicality.
247

Confidence rating technique was performed as follows. Examinees estimated their confidence in their answers by
ticking one of the three levels: ‘sure correct’, ‘not sure’, or ‘sure incorrect’. Their metacognition monitoring ability
was then measured by matching their confidence degree (‘sure correct’ or ‘sure incorrect’) with their test results
(‘correct’ or ‘incorrect’). For example, students who chose a correct answer but marked ‘sure incorrect’ on the
confidence level suggested that they had poor metacognition monitoring ability. Conversely, students choosing a
wrong answer and marking ‘sure incorrect’ on the confidence level showed that they had good metacognition
monitoring ability. However, noted that students who marked ‘not sure’ were excluded from the analysis of
metacognition monitoring ability regardless of whether their answers were correct. This approach provided simple
and quick measures, which were expected in computer-based adaptive learning environments (Kalyuga, 2006).
However, the problem with this confidence rating technique was that low-ability examinees were most likely to
choose ‘sure incorrect’ in tests, and most indeed ended up having incorrect answers. Therefore, they were mistakenly
interpreted as students with high metacognition monitoring abilities. The method applied in this study avoided this
problem since MEF can clearly identify this particular group.
In short, using marking as an alternative confidence rating technique was not only a good way to measure the
metacognition monitoring abilities of examinees; it was also rather easy to incorporate into CBT systems (Parshall et
al., 2002, p.34). Therefore, if the CBT was designed to employ marking, the confidence rating technique could be
applied, and data for metacognition monitoring abilities could be attained.

Design of metacognition-evaluation feedback
This study proposed metacognition-evaluated feedback (MEF), a new feedback mode integrating student marking
records and responses. Before the CBT starts, students were instructed to place a mark on the test item where they
were unsure of the answer. As soon as students completed the CBT, they obtained the MEF. The marking and
correctness of their answers were the criteria used to classify their test results into four categories: Category I, II, III,
and IV (Chen, Ho, & Yen, 2006). Category I represented correct answers with marking while Category II denoted
correct answers without marking. Category III included incorrect answers with marking whereas Category IV was
incorrect answers without marking. Since the presence of marking indicated whether or not students were sure of
their answers, Categories I and III could therefore be defined as unsure-correct and unsure-incorrect. Further,
Category II and IV were defined as expected-correct and unexpected-incorrect according to failure-driven learning
theory (Pacifici & Garrison, 2004; Schank, 1995). This learning theory claimed that mistakes, including unsuccessful
results and unmet expectations, were failures that could promote advanced learning. For instance, students made
predictions about their test results and then observed what happened to check their predictions. If their predictions
failed, they tried to determine how these mistakes occurred and then solved their problems. In MEF, further
classification of incorrect responses as either unsure or unexpected might motivate students to practice further.
In this study, MEF adopted marking as an indicator of student confidence level. Compared with traditional
confidence rating technique, marking was more straightforward, and it reduced interference because it did not require
students to check confidence level on each test item during the test (Jacobs & Chase, 1992; Wise & Plake, 1989).
Also, by excluding Category III (incorrect answers with marking) from the score for metacognition monitoring
ability, MEF avoided a common problem in traditional confidence rating technique: misinterpreting low-ability
students as having high metacognition monitoring abilities.
As Wang et al. (2004) indicated, CBT systems that collected and analyzed student responses and answering
processes could identify student learning outcomes and subject matter misconceptions. Therefore, the AEs in the
MEF were designed to incorporate the above information to provide useful adaptive feedback so that students could
understand their performance, clarify their mistakes, and increase their learning motivation.

Methodology
Participants
A total of 454 ninth-graders participated in this experiment. All participants had over five years of formal computer
literacy instruction (more than 180 hours), which confirmed that they had basic computer skills required to take a
248

CBT. One reason they volunteered to take part in the experiment was because they wanted to prepare for the English
Basic Competence Test (EBCT) given by the Committee of the Basic Competence Test for Junior High School
Students three months later. The participants were randomly assigned to Guu (four classes, 145 students), Gmu (four
classes, 139 students), and Gmm (five classes, 170 students).
Three versions of CBT system
Three versions of CBT system, labeled Gmm, Gmu, and Guu, were designed based on two factors, marking and MEF.
The Gmm (See Figure 1) adopted marking; MEF; Gmu (See Figure 2) only adopted marking, and Guu adopted neither
of them (See Figure 3). Figure 1 shows an example of MEF test results in Gmm: examinee X had twenty correct
responses and ten incorrect responses, i.e., 67% of responses by X were correct. The test results were then
categorized as follows: the 2nd, 5th, 12th, 15th, 21st, and 25th test items were marked and correct (Category I); the
3rd, 6th, 8th, 9th, 11th, 14th, 17th, 18th, 22nd, 24th, 26th, 28th, 29th, and 30th test items were unmarked and correct
(Category II); the 7th and 27th test items were marked and incorrect (Category III), and the 1st, 4th, 10th, 13th, 16th,
19th, 20th, and 23rd were unmarked and incorrect (Category IV). However, Figure 2 shows an example of feedback
for the test results of examinee X in Gmu. The displayed information was identical to that in Gmm but not sorted into
four categories. Figure 3 shows an example of feedback for the test results of examinee X in Guu. The displayed
information was similar to that in Gmu, except that the summary did not include marking records. Three versions of
CBT system recorded examinee scores, answer responses, time consumed and review records in each action. The
examinee test results and responses were examined for effects of marking on student scores, test-taking time, and
MEF on marking skills and review behavior.
Briefly, the three versions of CBT system were as follows:
1. Gmm: Examinees could place or remove a question mark on any items, indicating they were ‘unsure’. The
examinee responses, results, marks, and scores for each item were shown on the screen and sorted into four
categories after the test was completed.
2. Gmu: The marking method was the same as that in Gmm, and so was the displayed information. However, the
information was not sorted into four categories in this version.
3. Guu: Examinees could not mark any items. Except for marking, the displayed information was the same as that in
Gmu.

Figure 1. Example of MEF screen in Gmm

249

Figure 2. Example of feedback screen in Gmu

Figure 3. Example of feedback screen in Guu
Test items
Three versions of CBT system were adopted in a test comprised of thirty multiple-choice items selected from the
vocabulary and reading comprehension sections of the EBCT in Taiwan. Because less than 500 participants were
sampled, analyzing the parameters of test items based on three-parameter model in item response theory was
unsuitable (Hambleton & Swaminathan, 1985, p.227, p.308; Mason, Patry, & Bernstein, 2001). Therefore, classical
test theory was used; the item difficulty index and item discrimination index of the test items were calculated. As
Table 1 shows, both indices had means above .5, and item discrimination indices were above .4. The reliability of
internal consistency (KR-20) was .926. These three figures indicated that the quality of the test items was acceptable
(Ahmanan & Glock, 1981, p163; Ebel & Frisbie, 1991, p.231-32; Noll, Scannell, & Craig, 1979, p.109). The
following is an example item in the reading comprehension section:
250

In 1999, there were about 2,482 traffic accidents in Taiwan. Most of the accidents happened because
_________1_________. For example, some drivers drove too fast. Some drivers drank too much wine
or beer before they got into the car. And some drivers never tried to stop when the traffic lights went
from yellow to red.
Most of the accidents happened because _________1_________.
(1) motorcyclists were riding too fast
(2) the MRT system was not built yet
(3) drivers didn't follow the traffic rules
(4) there were too many traffic lights on the road
Table 1. Statistical properties of the test items (number of items=30, number of examinees =454)
Parameter
Mean
Std Dev
Minimum
Maximum
Item difficulty index
.56
.098
.37
.71
Item discrimination index
.70
.095
.44
.89
Start

Step1: Introductions
N
Next Step?
Y
Step2: Practice
N
Next Step?
Y

Step3: Test

Select First Item

Display item
Response & Record

Item Selection
(sequence/self-selection)
Display Messages

Stop?

N

Y
Step4: Review
N
Quit?
Y
End

Figure 4. Testing procedure of CBT system
251

Procedure
Design three versions of CBT system
As Figure 4 shows, three versions of CBT system first displayed instructions and demo clips page by page in the
Instruction session. During the Practice session, examinees could answer the four sample items repeatedly to
familiarize themselves with the CBT system interface. In the Test session, examinees could change their answer(s)
by selecting items from a pop-up menu. The participants in Gmm and Gmu were also instructed to put a question mark
next to items in which they were unsure of the answer. They could also remove question marks if they later felt
certain that their answers were correct. The stopping conditions of the test were activated when the test time was
finished or when all the items were completed and the ‘Finish’ button was clicked. The scores were calculated as
soon as the test ended.
In the Review session, the test results were shown and examinees could review the AEs including supplementary
materials related to major knowledge concepts written by several junior-high-school English teachers. Also,
according to suggestions from these teachers, the important concepts, words, and keys in the CBT system were
highlighted with bright colors. Figure 5 illustrates an example of the AEs for test items in the Review session.

Figure 5. Screenshot of an AE for a test item
Tryout
Six ninth-graders and three eighth-graders from another school in the same district were recruited for system testing
and tryout. The student with the lowest EA in the tryout group was able to complete a thirty-item test in 30 minutes.
Therefore, the test-taking time in the main study was set to 30 minutes to ensure that all participants could finish the
test within the time limit. Problems such as unclear instructions, blurred pictures, and misspelled items were
corrected after the tryout.
Main study
All participants were volunteers and were randomly assigned to the Gmm, Gmu, or Guu group. They were coached
briefly on the testing procedure, answering method, and how to get AE feedback on each CBT systems before taking
the EBCT. Students in Gmm and Gmu were instructed to place a question mark next to any item when they were not
252

sure of the answer. The participants in the Gmm were told that the Review session would display their test results and
sorted AEs according to their marking records and answer responses while those in Gmu and Guu were told that their
test results and AEs given in the Review session would not be sorted. The participants took the ECBT in the
computer classroom in their school to control for the anxiety associated with testing in an unfamiliar environment.
The three versions of CBT system recorded the responses and the time administered. After the experiment, the
participants received their test results and reviewed their record reports.

Results and Discussion
To investigate the effects of marking and MEF on examinees with different levels of English ability, the examinees
in Gmm, Gmu, and Guu were further classified by their test scores. The top 25% scorers were labeled as high English
ability (H-EA) group; the bottom 25% scorers were labeled as low English ability (L-EA) group, and the scorers
from 37.5% to 62.5% were labeled as middle English ability (M-EA) group. The sampling procedure skipped the
scorers from 25% to 37.5% and from 62.5% to 75% (totaling 25% of the whole sample, i.e., 112 participants). This
was to reduce possible influence between two successive EA groups on test scores and review behavior. Restated,
the number of sampling participants was 342: 114 examinees in H-EA, M-EA, and L-EA, respectively. For H-EA
examinees, thirty-two, forty-four, and thirty-eight were in Gmm, Gmu, and Guu, respectively. Forty-four, thirty-two, and
thirty eight students in the M-EA were classified as Gmm, Gmu, and Guu, respectively. For L-EA examinees, forty-five,
thirty-nine, and thirty were in Gmm, Gmu, and Guu, respectively. Compared with the total number of participants, the
number of sample in each subgroup was relatively small, and the statistical reliability of the analysis would decrease.
However, the sample size in each subgroup was still more than 30 which satisfied the criteria of minimum numbers
(15 subjects in each subgroup) in an experimental study recommended by Gall, Borg, and Gall (1996, p.229).
Therefore, the level of reliability to explore the effects of marking and MEF on test scores and review behavior for
examinees with different EA levels was acceptable. Table 2 shows the descriptive statistics for test scores in each
subgroup. The average test scores of H-EA examinees in three groups were, from highest to lowest, Gmm, Guu, and
Gmu. However, the average test scores of M-EA and L-EA examinees in three groups were, from highest to lowest,
Gmu, Gmm, and Guu. Additionally, the average test scores of examinees in three groups were, from highest to lowest,
Gmu, Guu, and Gmm.

Groups
Gmm
Gmu
Guu
Total

Table 2. Descriptive statistics of test scores in each subgroup (N=342)
H-EA
M-EA
L-EA
Total
n Mean Std Dev
n
Mean Std Dev
n
Mean Std Dev
n
Mean Std Dev
32 27.97
1.58
44
16.52
3.35
45
7.11
1.42
121 16.05
8.56
44 27.16
4.14
32
17.25
5.13
39
7.62
3.99
115 17.77
9.40
38 27.53
1.59
38
14.95
3.42
30
6.27
1.39
106 17.00
8.95
114 27.97
1.61
114 15.79
3.31
114 6.81
1.62
342 16.92
8.97

The following section presents the results and related discussions to answer the two research issues: the effect of
marking on three levels of EA examinee performance in Gmm, Gmu, and Guu and the effects of MEF on their marking
frequency and review behavior.

Effects of marking on examinee test scores
To investigate the effects of marking on examinee test scores, the t-tests were conducted for three successive EA
levels. As Table 3 shows, marking significantly affected the test scores of M-EA examinees (t.05 (112) =2.4, p<.05), but
had no significant effect on those of H-EA (t.05 (112) =–.05, p>.05) or L-EA (t.05 (112) =1.95, p>.05) examinees. The test
results indicated that middle EA examinees benefited significantly from the marking function, which suggested that
CBT systems incorporating marking function could improve average student performance. This finding was rather
promising because classroom intervention was typically aimed at average level students, since this target group had
the most potential for improvement compared to their high ability and low ability counterparts. In the present case,
the high ability students were confident of their own answers or they had already understood the important concepts
prior to the test; therefore, marking skill was not an immediate need for them. Similarly, marking did not improve the
performance of low ability students. They lacked enough knowledge to answer correctly even though they had good
marking skills. However, marking might have encouraged average ability students to seek the clues among test items
253

and assist them in answering correctly, which would thus have improved their performance. Therefore, marking
should not be neglected in CBT systems design.
Table 3. Descriptive statistics of examinee test scores and results of three t-tests (N=342)
Marking
With
Without
Examinees’ English ability
df
n
Mean
Std Dev
n
Mean
Std Dev
H-EA
76
27.50
3.32
38
27.53
1.59
112
M-EA
76
16.83
4.18
38
14.95
3.42
112
L-EA
84
7.35
2.90
30
6.27
1.39
112
*
p<.05

t value
-0.05
2.4*
1.95

Effects of MEF on examinee marking skill and review behavior
To evaluate how MEF affected the marking skills and review behavior of examinees, the ANOVAs used MEF (with
and without) and examinee EA levels (H-EA, M-EA and L-EA) as independent variables. Dependent variables were
the marking frequency, number of reviewed AEs, and time (in seconds) spent reviewing AEs. The sampling
participants in Gmm used CBT with MEF function, in which test results were sorted into four categories (See Tab. 2).
Thirty-two H-EA, forty-four M-EA, and forty-five L-EA examinees were included. However, the sampling
participants in Gmu received the same information, but it was not sorted into the same four categories as that in Gmm
(See Tab. 2). Forty-four H-EA, thirty-two M-EA, and thirty-night L-EA examinees were included in this group.

Marking frequency
Tables 4 and 5 present the descriptive statistics for the marking frequency of examinees and the ANOVA summary,
respectively. The statistical results showed that the interaction of MEF and examinee EA was significantly associated
with marking frequency (F(2, 230) =4.06, p<.05). Post hoc analysis further showed that the marking frequency in MEA (t.05 (73) =2.13, p<.05) and L-EA (t.05 (74) =3.55, p<.001) examinees was significantly higher than that in H-EA
examinees when MEF was adopted (F(2, 230) =6.31, p<.05) (See Tab. 6). However, marking frequency did not
significantly differ among the three levels of examinee EA without MEF (F(2, 230) =.55, p>.05). More importantly, as
Figure 6 shows, MEF significantly increased the marking frequency in M-EA (F(1, 230) =4.10, p<.05) and L-EA
examinees (F(1, 230) =20.14, p<.05). That is, MEF motivated low and middle examinees to use marking skills when
they knew in advance that the test results would be sorted into four categories. However, MEF did not significantly
affect H-EA examinees, possibly due to their mastery of the subject matter, as stated above.

Examinees’ EA
H-EA
M-EA
L-EA

Source
MEF
Examinees’ EA
MEF × Examinees’ EA
w. cell (error)
*
p<.05

Table 4. Descriptive statistics of marking frequency (N=236)
MEF
With
N
Mean
Std Dev
n
32
4.00
4.72
44
44
6.95
6.84
32
45
9.24
12.04
39

Table 5. Summary of ANOVA for marking frequency (N=236)
SS
df
MS
718.21
1
718.21
169.97
2
84.98
417.23
2
208.62
11810.15
230
51.35

Without
Mean
3.57
3.72
2.33

Std Dev
5.17
4.11
5.27

F
13.99*
1.66
4.06*

254

Mean
( Marking Frequency )

Table 6. Summary of ANOVA of simple main effects for marking frequency (N=236)
Source
SS
df
MS
F
Post hoc
Examinees’ EA
With MEF
648.46
2
324.22
6.31*
H-EA,L-EA>H-EA
Without MEF
56.54
2
28.27
0.55
MEF
H-EA
12.57
1
12.57
0.24
MEF> without MEF
M-EA
210.37
1
210.37
4.10*
MEF> without MEF
L-EA
1033.96
1
1033.96
20.14*
w. cell (error)
11810.15
230
51.35
*
p<.05
10
9
8
7
6
5
4
3
2
1
0

MEF
Without
With

H-EA

M-EA
Examinees' EA

L-EA

Figure 6. Marking frequency (MEF x Examinees’ EA)
Number of reviewed AEs and review time
Table 7 presents the descriptive statistics of the number of reviewed AEs and review time. As Tables 8 and 9 show,
no significant interactions were detected between MEF and examinee EA for number of reviewed AEs (F(2, 230) =.88,
p>.05) and review time (F(2, 230) =.57, p>.05). In addition, no significant main effect of examinee EA was found in
these two dependent variables (number of reviewed AEs: F(2, 230) =1.63, p>.05; review time: F(2, 230) =1.75, p>.05).
This suggested that low ability students might have reviewed fewer AEs while making far more mistakes might
because the design of the AEs did not meet their needs. Further, the significance of the main effects of MEF on both
dependent variables were examined (number of reviewed AEs: F(1, 230) =4.87, p<.05; review time: F(1, 230) =8.33,
p<.05) (See Figs. 7 and 8). In short, the CBT incorporating MEF increased the frequency and length of time spent
reviewing AEs.
Table 7. Descriptive statistics of number of reviewed AEs and review time (N=236)
Number of reviewed AEs
Review time
MEF
n
Mean
Std Dev
Mean
Std Dev
H-EA
With
Without

32
44

2.44
2.07

2.91
2.11

63.12
35.34

89.62
34.69

With
Without

44
32

4.32
2.84

5.88
3.88

86.82
51.38

111.00
60.39

With
Without

45
39

3.89
1.51

7.51
2.68

61.09
33.23

83.82
75.69

M-EA

L-EA

255

Table 8. Summary of ANOVA for number of reviewed AEs (N=236)
Source
SS
Df
MS
MEF
109.26
1
109.26
Examinees’ EA
73.13
2
36.56
39.58
2
19.79
MEF x Examinees’ EA
w. cell (error)
5162.28
230
22.44
*
p<.05

Source
MEF
Examinees’ EA
MEF x Examinees’ EA
w. cell (error)
**
p<.01

Table 9. Summary of ANOVA for review time (N=236)
SS
Df
MS
53245.19
1
53245.19
22321.18
2
11160.56
731.82
2
365.91
1470344.00
230
6392.40

F
4.87*
1.63
0.88

F
8.33**
1.75
0.57

5

Mean
(Number of Reviewed AEs)

4.5
4

3.5
3

2.5
2

MEF

1.5
1

Without

0.5

With

0
H-EA

M-EA
Examinees' EA

L-EA

Mean (Review Time)
(second)

Figure 7. Number of reviewed AEs (MEF x Examinees’ EA)
100
90
80
70
60
50
40
30
20
10
0

MEF
Without
With

H-EA

M-EA

L-EA

Exa minees' EA

Figure 8. Review time (MEF x Examinees’ EA)
256

In summary, the statistical results showed that marking and MEF had different effects on the scores and review
behavior for different levels of EA examinees (See Tab.10). First, for H-EA examinees, marking did not significantly
increase test scores. Additionally, MEF did not encourage them to apply marking skills more frequently, but it
improved their review behavior, including the number of reviewed AEs and review time. This suggested that the
high ability students might have understood important concepts of the subject matter prior to the experiment; thus,
they did not need marking skills to help them to recheck their answers. However, the new MEF feedback mode
effectively prompted students to review supplementary knowledge both quantitatively and qualitatively. Second, for
M-EA examinees, marking significantly increased test scores, possibly because the students tended to spend more
time deliberating on the marked items during the test. For this group of students, marking therefore was a reminder to
further ponder unsure test items. Besides, MEF motivated them to use marking skills more frequently and promoted
their review behavior. This suggested that both marking and MEF were effective functions for the M-EA students.
Third, for L-EA students, marking did not significantly increase their test scores. This suggested that low ability
students had insufficient knowledge to correct their answers despite the help of marking. Further, the effects of MEF
on their marking frequency and review behavior were the same as those of M-EA students. In short, student ability
was a critical threshold for marking behavior to improve test performance.
Table 10. Summary results for three levels of examinees’ EA
Independent variable
Results
Test scores
+ (only M-EA examinees)
Marking frequency
MEF + (under M-EA and L-EA conditions)
M-EA, L-EA > H-EA (under MEF condition)
MEF and
Number of reviewed AEs
MEF (Main effect) +
Examinees’ EA
Review time
MEF (Main effect) +
+ : significant increase
Factor (s)
Marking

Interview with examinees
The qualitative results of interviews in this study also revealed important implications. Ten examinees who ranked in
the bottom 25% of scores with low AE review rates were selected and interviewed to explore why they reviewed
fewer AEs and made far more mistakes. The students stated that the AEs were too brief to understand and what they
needed were more detailed explanations. However, interviews with another ten examinees with average scores
indicated that the AEs for test items were sufficiently clear. This suggested that the AEs should be revised for
underachievers to encourage review behavior. For example, the content of AEs could be made more detailed, or the
illustrations for supplementary materials could be increased. Besides, interviews with six examinees who correctly
answered all test items with 100% review rate in Guu indicated that they had to review all AEs in order to find the
explanations for the questions they had guessed. This suggested that marking function in CBT systems could be
helpful to increase the efficiency of compensatory instruction.

Conclusion
In this study, a metacognition-evaluated CBT system integrating marking and MEF was employed to evaluate their
effects on self-managed learning for junior high school students. The MEF was designed using confidence rating
technique, failure-driven learning theory, and suggestions from the participating English teachers, which served to
support teachers’ role of giving individual students adaptive compensatory instruction. In metacognition-evaluated
CBT system, marking might also help students navigate lengthy tests. The present study, therefore, investigated the
following two issues: (1) whether marking affects students’ test scores and (2) whether MEF affects student marking
skills and encourage student in review behavior.
Findings were summarized as follows. First, marking enhanced test scores in middle level EA students, but not in
high and low level EA students. In other words, of the three groups, the middle level EA students benefited the most
from using marking skills. Some low EA students, for example, demonstrated good marking skills but did not
significantly improve their test scores. Generally, low level EA students spent less time on testing than high or
middle level EA students, which revealed that low level EA students tried to complete items that were far above their
ability. Thus, they had little patience to complete difficult test items. Secondly, MEF not only effectively encouraged
three levels of EA students to apply marking skills more frequently, but it also motivated their review behavior. Also,
257

follow-up interview indicated that providing adaptive and detailed AEs in CBT systems for low level EA students
were necessary.
In sum, the findings in the present study indicate that, marking should be incorporated in CBT systems because it
may encourage average students to deliberate upon marked test items, which can then increase their scores. Also,
MEF is effective for improving students’ review behavior. Additionally, providing AEs of test items after completing
a CBT helps students learn and review knowledge concepts related to test items and can also lighten the workload of
teachers by minimizing the need for compensatory instruction. To design AEs, future studies should engage students
to review their mistakes.
Some limitations are presented in this study as well. The conclusions are based only on the analysis of junior high
school students taking EBCT. Experimental results may differ from students in other age groups or with different
knowledge domains. These issues should be considered in further studies to obtain more comprehensive results.
Future research should investigate how MEF influences student performance and learning motivations as well as
how AEs affect student performance.

Acknowledgements
This work was supported in part by the Science Education Division of Taiwan National Science Council under the
Grant No. NSC 93-2520-S-003-010-. The project leader, Rong-Guey Ho, is also the corresponding author of this
article. The authors would like to thank the NSC of Taiwan for financial support and the anonymous reviewers for
constructive suggestions.

References
Ahmanan, J. S., & Glock, M. D. (1981). Evaluating Student Process: Principles of Tests and Measurement (6th Ed.), Boston:
Allyn & Bacon.
Baker, L., & Brown, A. L. (1984). Metacognitive skills in reading. In P. D. Pearson, R. Barr, M. L. Kamil, & P. Mosenthal (Eds.),
Handbook of Reading Research (pp.353-394), New York: Longman.
Bleuer, J. C., & Walz, G. R. (2002). New perspectives on Counseling Underachievers, ERIC Document Reproduction Service No.
ED 470602.
Bonham, S. W., Beichner, R. J., Titus, A., & Martin, L. (2002). Education research using web-based assessment systems. Journal
of Research on Computing in Education, 33(1), 28-43.
Brown, A. L. (1980). Metacognitive development and reading. In R. J. Spiro, B. C. Bruce, & W. F. Brewer (Eds.), Theoretical
Issues in Reading Comprehension (pp. 454-481), Hillsdale, NJ: Lawrence Erlbaum.
Brown, A. L. (1987). Metacognition, executive control, selfregulation and other more mysterious mechanisms. In F. E. Weinert,
& R. H. Kluwe (Eds.), Metacognition, Motivation, and Understanding (pp. 65-116), London: Lawrence Erlbaum.
Burton, R. F. (2002). Misinformation, partial knowledge and guessing in true/false tests. Medical Education, 36, 805-811.
Chen, L. J., Ho, R. G., & Yen, Y. C. (2006, Octber). Effects of metacognition evaluated strategy on computer-based test system.
Paper presented at the E-Learn 2006 Conference, October 13-17, Honolulu, Hawaii, USA.
Collentine, J. (2000). Insights into the construction of grammatical knowledge provided by user-behaviour tracking technologies,
Language Learning and Technology, 3(2), 44-57.
Collis, B., De Boer, W., & Slotman, K. (2001). Feedback for web-based assignments, Journal of Computer Assisted Learning, 17,
306-313.
Collis, B., & Nijhuis, G. G. (2000). The instructor as manager: Time and task. The Internet in Higher Education, 3, 75-97.
Collis, B., & Messing, J. (2000). Usage, attitudes and workload implications for a Web-based learning environment. Journal of
Advanced Learning Technologies, 9(1), 17–25.
Croft, A. C., Danson, M., Dawson, B. R., & Ward, J. P. (2001). Experiences of Using Computer Assisted Assessment in
Engineering Mathematics. Computers & Education, 37, 53-66.
Desoete, A., & Roeyers, H. (2006). Metacognitive macroevaluations in mathematical problem solving. Learning and Instruction,
16(1), 12-25.
Ebel, R. L., & Frisbie, D. A. (1991). Essentials of educational measurement (5th Ed.), Englewood, NJ: Prentice Hall.

258

Elshout-Mohr, M., Meijer, J., van Daalen-Kapteijns, M., & Meeus, W. (2003). A self-report inventory for metacognition related
to academic tasks. Paper presented at the 10th Conference of the European Association for Research on Learning and Instruction
(EARLI), August 26-30, Padova, Italy.
Gall, M. D., Borg, W. R., & Gall, J. P. (1996). Educational Research: An introduction (6th Ed.), White Plains, NY: Longman.
Garner, R. (1988). Metacognition and Reading Comprehension, Norwood, NJ: Ablex.
Gibson, E. J., Brewer, P. W., Dholakia, A., Vouk, M. A., & Bitzer, D. L. (1995). A comparative analysis of web-based testing and
evaluation systems. Proceedings of the 4th WWW conference. Retrieved September 3, 2009, from
http://renoir.csc.ncsu.edu/MRA/Reports/WebBasedTesting.html.
Hambleton, R. K., & Swaminathan, H. (1985). Item Response Theory: Principles and Applications, Boston: Kluwer.
Higgins, J., Russell, M., & Hoffmann, T. (2005). Examining the effect of computer-based passage presentation on reading test
performance. The Journal of Technology, Learning, and Assessment, 3(4), 1-35
Jacobs, L. C., & Chase, C. I. (1992). Developing and Using Tests Effectively: A Guide for Faculty, San Francisco, CA: JosseyBass.
Kalyuga, S. (2006). Rapid cognitive assessment of learners' knowledge structures. Learning and Instruction, 16(1), 1-11.
Lazarte, A. A. (1999). Modeling time to respond and probability of correct answer in a simulated computerized test-taking
situation. Paper presented at the Annual Meeting of the American Educational Research Assoc, April 19-23, Montreal, Canada.
Linn, R. L., & Gronlund, N. E. (2000). Measurement and Assessment in Teaching (8th Ed.), Upper Saddle River, NJ: PrenticeHall.
Lord, F. M. (1975). Formula-scoring and number-right scoring. Journal of Educational Measurement, 12(1), 7-12.
Mason B.J., Patry M., & Bernstein D.J. (2001) An examination of the equivalence between non-adaptive computer-based test and
traditional testing. Journal of Educational Computing Research, 24, 29–39.
McMorris, R. F., & Leonard, G. (1976). Item response changes and cognitive style. Paper presented at the 22nd Annual Meeting
of the National Council on Measurement in Education, San Francisco (ERIC Document Reproduction Service ED 12991839).
Miller, T. W., & Weiss, D. J. (1976). Effect of Time Limits on Test-taking Behavior, Research Report 76-2, Minneapolis:
Minnesota University, Dept. of Psychology (ERIC Document Reproduction Service No. ED126123).
Noll, V. H., Scannell, D. P., & Craig, R. C. (1979). Introduction to Educational Measurement (4th Ed.), Boston: Houghton
Mifflin.
Pacifici, L., & Garrison, J. (2004). Imagination, emotion and inquiry: The teachable moment. Contemporary Pragmatism, 1(1),
119-132.
Parshall, C. G., Kalhn, J. C., & Davey, T. (Eds.). (2002). Practical Considerations in Computer Based Testing, New York:
Springer-Verlag.
Peat, M., & Franklin, S. (2002). Supporting student learning: The use of computer-based formative assessment modules. British
Journal of Educational Technology, 33(5), 515-23.
Ram, A., Cox, M. T., & Narayanan, S. (1995). Goal-driven learning in multistrategy reasoning and learning system. In A. Ram &
D. B. Leake (Eds.), Goal-driven Learning (pp421-437), Cambridge, MA: MIT Press/Bradford Books.
Rasekh, Z., & Ranjbary, R. (2003). Metacognitive strategy training for vocabulary learning, TESL-EJ, 7(2), 1-18.
Rogers, W. T., & Bateson, D. J. (1991). Verification of a model of test-taking behavior of high school seniors. Journal of
Experimental Education, 59(4), 331-350.
Schank, R. C. (1995). End run to the goal line. EDU- COM Review, 30(1), 14-17.
Souvignier, H., & Mokhlesgerami, J. (2006). Using self-regulation as a framework for implementing strategy instruction to foster
reading comprehension. Learning and Instruction, 16(1), 57-71.
Thelwall, M. (2000). Computer-based assessment: A versatile educational tool. Computers & Education, 34, 37-49.
Veenman, M. V. J. (2003). The assessment of metacognitive skills: what can be learned from multi-method designs? Paper
presented at the 10th Conference of the European Association for Research on Learning and Instruction (EARLI), August 26-30,
Padova, Italy.
Vu, K. L., Hanley, G. L., Strybel, T. Z., & Proctor, R. W. (2000). Metacognitive processes in human-computer interaction: Selfassessments of knowledge as predictors of computer expertise. International Journal of Human-Computer Interaction, 12(1). 4371.
Wang, T. H., Wang, K. H., Wang, W. L., Huang, S. C., & Chen, S. Y. (2004). Web-based assessment and test analyses (WATA)
system: Development and evaluation. Journal of Computer Assisted Learning, 20(1), 59-71.
Wise, S. L., & Plake, B. S. (1989). Research on the effects of administering test via computers. Educational Measurement: Issues
and Practice, 8(3), 5-10.
Wong, C. K., Wong, W., & Yeung, C. H. (2001). Student behavior and performance in using a Web-based assessment system.
Innovations in Education and Teaching International, 38(4), 339-346.
259

Liu, G.-Z. (2010). Book review: Handbook of Research on Educational Communications and Technology (Eds. J. M. Spector, M.
D. Merrill, J. van Merrienboer, & M. P Driscoll). Educational Technology & Society, 13 (1), 260-263.

Handbook of Research on Educational Communications and Technology
(Book Review)

Reviewer:
Gi-Zen Liu
Faculty of ET & CALL, Foreign Language & Literature Department
National Cheng Kung University, Tainan City, Taiwan
gizen@mail.ncku.edu.tw

Textbook Details:
Handbook of Research on Educational Communications and Technology (3rd Ed.)
Editors: J. Michael Spector, M. David Merrill, Jeroen van Merrienboer, and Marcy P Driscoll
2008, Lawrence Erlbaum, ISBN: 978-0-415-96338-1
Chapters: 56; Pages: 928

As a variety of information and communication technologies (ICT) have been emerging and evolving in different
contexts and fields, it is estimated that ICT integrated education will become normal in entirely online learning
environments and in blended courses over the next five to ten years (Mayadas et al, 2009). In terms of research and
development in educational technology (ET), the pioneers and some practitioners have already been experiencing it
for several decades ; however, in terms of the paradigm shift in ET research and development, we have travelled only
a short distance down the path of a thorough educational and conceptual reconfiguration (Ely, 2008). Hence, research
concerning how to choose, use, design, develop, implement, manage, and evaluate appropriate ICT with cutting-edge
methodology and theory—in learning, instruction, training, and beyond—has become necessary and crucial in the
diverse and broad field of ET (Liu, 2008).
This is the third edition of a Handbook, with the first appearing in 1996, and the second in 2004. The latest edition
reflects the fact there that there have been a number of new technological developments and innovative educational
utilizations of emerging ICT over the last few years. There are 56 chapters in the six major parts of the Handbook,
with a total of almost one thousand pages. The wide-ranging contributions in this third edition show that it has met
the needs of the nearly 200 members of the Association for Educational Communications and Technology (AECT),
who provided (a) their feedback on the second edition, and (b) details of what they would like to see in this new
edition, which were collected via an online survey. This strategy for updating the work effectively illustrates the
thoughtful efforts of the four co-editors, all of whom are well-established ET scholars.
This Handbook may be viewed as one of the most thought-provoking works of the current ET research paradigm. It
is not only a collaborative and insightful academic handbook comprised of numerous research inputs, but is also a
professional association-led (AECT) collection of many research outputs. Obviously, the aim of this comprehensive
Handbook is to provide state-of-the-art analyses, syntheses, and summaries of the theory and practice of ICT use in
education and educational research in the U.S. and other countries. It systematically introduces and discusses the
relevant (a) foundations, (b) strategies, (c) technologies, (d) models, (e) design and development, and (f)
methodological issues of ET in education and educational research, with a special focus on each of these six major
areas. Therefore, novice and experienced practitioners and researchers, as well as other interested faculty members,
graduate students and readers, may find much of value in the way that this work highlights the meaningful
interrelationships and relevant forms among users (including instructors and learners), ICT applications, and situated
learning contexts that promote and enhance traditional and innovative technology-enhanced learning and instruction.
What makes this edition especially valuable is that the theoretical focus of the Handbook is provided in a series of
chapters on the historical and theoretical foundations and evolution of this broad and engaging topic. As a transdisciplinary field, ET has been criticized by some for a lack of solid theoretical foundations (Mishra & Koehler,
2006). Fortunately, the seven chapters in “Part I: Foundations” provide a series of convincing historical, theoretical,
and philosophical concepts, along with their background and development, in order to offer multiple perspectives on
ET. The main purpose of the these chapters is to help this professional field establish a fundamental knowledge
base—as drawn from the past and present academic, educational, and inquiring experiences in ET and other relevant
disciplines. Consequently, interested researchers and practitioners can utilize this knowledge to meaningfully
ISSN 1436-4522 (online) and 1176-3647 (print). © International Forum of Educational Technology & Society (IFETS). The authors and the forum jointly retain the
copyright of the articles. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that copies bear the full citation on the first page. Copyrights for components of this work owned by
others than IFETS must be honoured. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior
specific permission and/or a fee. Request permissions from the editors at kinshuk@ieee.org.

260

construct their own views on ET research and development. For instance, Chapter 1 reviews the historical
foundations of this field, taking a broad view of educational media; Chapter 2 analytically explores the theoretical
foundations of this field in four relevant areas (including the psychology of learning, communications theory,
human-computer interaction, and instructional design and development); Chapter 7 meaningfully constructs the
philosophical foundations for the ET field with the seven philosophical perspectives (including Objectivism,
Realism, Empiricism, Rationalism, Idealism, Relativism, and Pragmatism) and the five psychological perspectives
(including Behaviorism, Cognitivism, Cognitive contructivism, Sociocultural/historicism, and Situativity theory).
Readers will benefit from the clearly structured overview of the milestones and key players in the foundation,
development and evolution of ET that is presented in this part of the work.
Most of us realize that the aim of ET is making effective and efficient use of appropriate ICT in various situated
learning contexts for educational purposes. In “Part II: Strategies”, readers may learn a variety of helpful
instructional or learning strategies that can be used in specifically defined learning environments. The authors do a
great job in these seven chapters of clearly demonstrating the use of various strategies, with vivid and inspiring
descriptions, explanations, and examples. For example, Chapter 8 uses illustrations and figures to demonstrate how
to apply research-based guidelines to support learning with various types of media. These guidelines are derived
from four related learning theories: (a) information-processing theory, (b) dual-coding theory, (c) cognitive load
theory, and (d) Baddeley’s model of memory. In addition, Chapter 14 discusses comparisons of Merrill’s first
principles of instructions and other recently developed instructional design principles, in order to stimulate more
rigorous research to evaluate the validity of such principles. These research-driven strategies and guidelines for real
practices consciously and concisely offer critical thinking with regard to the design and development of ET, all
embedded in the form of the tactics, models, figures, and tables that are used throughout this part of the book.
The sixteen chapters in “Part III: Technologies,” which constitute the largest part of the Handbook, identify,
introduce and discuss the so-called “upstream technologies” (referring to analysis, planning, and design) as well as
“downstream technologies” (referring to development, deployment, and evaluation), by considering various ICT uses
in education. Some prominent topics in this part include programmed technologies, computer-mediated technologies,
knowledge-based technologies, blended learning, adaptive technologies, learning objects, and open source and open
standards. These upstream and downstream technologies can be treated as what Heinich et al. (1999) term “soft
technologies” (which refers to well-designed instructional processes, models and techniques that are developed with
behavioral and social methods and theories in mind in order to bring about desired outcomes with the use of hard
technologies, which are composed of hardware and software). In these chapters, readers will learn from the clear
descriptions and commentary on the current use of various ICT tools with upstream and downstream technologies, as
well as their impact on education.
In recent years, autonomous learning has received growing emphasis, so models that can guide, promote, and
enhance effective and efficient learning are desired. Interested readers will find “Part IV: Models” valuable and
important, because it introduces various new approaches to facilitate learning that are designed to be used in schools,
universities, workplaces and beyond. These new models and approaches are well-designed and -developed in terms
of soft technologies, with or without the use of ICT. Readers will explore the current prevailing research topics,
including cooperative learning models, cognitive apprenticeship approaches, adaptive instructional systems,
problem-based learning, performance improvement approaches, resource-based training, and domain specific
approaches in this profession-oriented part of the work.
Practitioners and researchers may be curious as to why some stakeholders are doing better than others in similar
contexts and situations, what the best practices are for these professional activities, and how they can develop
themselves to become professional educational technologists or instructional designers of ET. The eleven chapters in
“Part V: Design and Development” focus on the research towards professional practice and development in this field.
Readers will realize the high-quality know-how from several significant dimensions of instructional design and
technology, including instructional design competencies, task analysis, performance assessment, evaluation models
and methodology, system design for changes, and others.
One of the apparent advantages of this Handbook, and of “Part VI: Methodological Issues,” is that the research
paradigm in technology-enhanced learning and teaching, and the relevant research on it, are consciously and
explicitly introduced and discussed. Readers will benefit from the four chapters on theory development, research
designs, data collection and analysis, and foundations for the future in this part of the work. Chapter 54 especially
261

focuses on the recognized research paradigms and the paradigm shift in research with regard to instructional design
and methodological approaches of ET in this ever-changing professional field. This chapter also provides help on
how to identify, design, and investigate research questions—in order to choose the appropriate method with regard to
quantitative, qualitative, or other inquiry research. Just as Shih et al. (2008) try to identify the current research trends
and possible new research directions for e-learning studies, the eight co-authors of Chapter 54 discuss the research
papers published in the Educational Technology Research & Development between 1994 and 2005 to provide new
research directions and research topics, along with the relevant research methodologies and issues. Moreover, readers
will learn much important know-how with regard to how to collect and analyze formative data in various phases of
their inquiries in Chapter 55.
In terms of structure in this Handbook, the four co-editors arrange related topics in the same part in order for readers
to systematically capture the overview and the main ideas of the related chapters. In terms of format within each
chapter, the co-authors thoughtfully provide an abstract, keyword definitions, an introduction, the main texts, and
references, so that readers can better understand the content and be able to continue reading further on their own.
Theoretically speaking, this well-chunked food for thought will certainly inspire readers’ reflections on the
information it contains. Technically speaking, the add-on information in each chapter means that they are all clearly
organized texts that provide the key terms and main concepts in each of the well-specified domains and areas that
they cover.
One minor shortcoming of this Handbook is the lack of an introduction to cutting-edge technologies and their
possible applications in education, as well as how we can learn from them. For example, context-aware ubiquitous
computing technologies (Hwang et al., 2009; Liu & Hwang, 2009) foster immediate learning by using sensors and
RFID readers and tags, or by using the Global Positioning System (GPS; Ogata et al., 2008). But educational
applications of these two types of new ICT are lacking in the Handbook. Although such technologies are not yet in
common use, possibly due to issues of cost or inaccessibility, I believe that most readers would enjoy the opportunity
to learn more about possible directions in formal and informal learning with the most advanced technologies, and
hope that the next edition of the Handbook will provide such a chapter.
I would also like to suggest that a more comprehensive survey should be conducted in order to receive a broader
perspective concerning the research methodologies and topics that should be included in the next edition, as well as
new ways to develop or identify such research tools and directions. Maybe the editor of the next edition of the
Handbook or other interested authors should conduct an online survey of AECT members and non-members, and of
researchers and practitioners in developed, developing, and under-developed countries, in order to uncover
unexplored or unidentified research issues and directions in ET and ICT. This would then make the next edition even
more useful in enabling readers to conduct novel or innovative studies based on emerging ICT in a broad range of
contexts, learning situations and educational settings.
However, these are minor criticisms, and overall I feel that the Handbook of Research on Educational
Communications and Technology (third ed.) could well be Educational Technology & Society’s Best Research
Handbook for 2009, if such an award existed. I highly recommend this collection of so many excellent works at such
an affordable price to all novice and experienced stakeholders, graduate programs, and university libraries in our
field and beyond, as I am convinced that it will enable readers to conduct innovative and beneficial research in
educational communications and technology.

Acknowledgements
This work was supported by the NCKU Project of Promoting Academic Excellence (R021) and the National Science
Council (NSC 98-2631-S-024-001) in Taiwan.

References
Ely, D. (2008). Frameworks of educational technology. British Journal of Educational Technology, 39(2), 244-250.

262

Heinich, R, Molenda, M, Russell, J. D. & Smaldino, S. E. (1999). Instructional media and technologies for learning (6th edn.).
Prentice-Hall, Upper Saddle River, NJ.
Hwang, G. J., Yang, T. C., Tsai, C.C. & Yang, S. J. H. (2009). A context-aware ubiquitous learning environment for conducting
complex science experiments. Computers & Education, 53(2), 402-413.
Liu, G. Z. (2008). Innovating research topics in learning technology: Where are the new blue oceans?. British Journal of
Educational Technology, 39(4), 738-747.
Liu, G. Z. & Hwang, G. J. (2009). A key step to understanding paradigm shifts in e-learning: Towards context-aware ubiquitous
learning. British Journal of Educational Technology, 40(6), doi: 10.1111/j.1467-8535.2009.00976.x
Mayadas, A. F., Bourne, J. & Bacsich, P. (2009). Online education today. Science, 323(5910), 85-89.
Mishra, P. & Koehler, M. J. (2006). Technological pedagogical content knowledge: A framework for teacher knowledge.
Teachers College Record, 108(6), 1017-1054.
Ogata, H., Saito, N. A., Paredes J. R. G., San Martin, G. A., & Yano, Y. (2008). Supporting Classroom Activities with the BSUL
System. Educational Technology & Society, 11 (1), 1–16.
Shih, M., Feng, J. & Tsai, C. C. (2008). Research and trends in the field of e-learning from 2001 to 2005: A content analysis of
cognitive studies in selected journals. Computers & Education, 51(2), 955-976.

263

AIED 2013 Workshops Proceedings
Volume 4

AIED Workshop on Simulated Learners
Workshop Co-Chairs:
Gord McCalla
Department of Computer Science, University of Saskatchewan
John Champaign
RELATE, Massachusetts Institute of Technology

https://sites.google.com/site/aiedwsl/

ii

Preface
This workshop is intended to bring together researchers who are interested in
simulated learners, whatever their role in the design, development, deployment, or
evaluation of learning systems. Its novel aspect is that it isn’t just a workshop about
pedagogical agents, but is also concerned about other roles for simulated learners in
helping system designers, teachers, instructional designers, etc.
As learning
environments become increasingly complex and are used by growing numbers of
learners (sometimes in the hundreds of thousands) and apply to a larger range of
domains, the need for simulated learners (and simulation more generally) is compelling,
not only to enhance these environments with artificial agents, but also to explore design
issues using simulation that would be otherwise too expensive, too time consuming, or
even impossible using human subjects. The workshop aims to be broadly integrative
across all possible roles for simulated learners.

July, 2013
Gord McCalla & John Champaign.

iii

Program Committee
Co-Chair: Gord McCalla, University of Saskatchewan (mccalla@cs.usask.ca)
Co-Chair: John Champaign, Massachusetts Institute of Technology (jchampai@mit.edu)

Esma Aimeur, Université de Montréal
Roger Azvedo, McGill University
Gautam Biswas, Vanderbilt University
Tak-Wai Chan, National Central University of Taiwan
Robin Cohen, University of Waterloo
Ricardo Conejo, Universidad de Málaga
Evandro Costa, Federal University of Alagoas
Michel C. Desmarais, Ecole Polytechnique de Montréal
Sylvie Girard, Arizona State University
Lewis Johnson, Alelo
Yang Hee Kim, Utah State University
James Lester, North Carolina State University
Noboru Matsuda, Carnegie Mellon University
Kurt VanLehn, Arizona State University
Beverly Park Woolf, University of Massachusetts, Amherst

iv

Table of Contents
Using Simulated Learners and Simulated Learning Environments
within a Special Education Context
Carrie Demmans Epp and Alexandra Makos

1

Simulated Students, Mastery Learning, and Improved Learning Curves
for Real-World Cognitive Tutors
Stephen Fancsali, Tristan Nixon, Annalies Vuong and Steven Ritter

11

Exploring through Simulation the Effects of Peer Impact on Learning
Stephanie Frost and Gordon McCalla

21

Using HCI Task Modeling Techniques to Measure How Deeply Students Model
Sylvie Girard, Lishan Zhang, Yoalli Hidalgo-Pontet, Kurt Vanlehn,
Winslow Burleson, Maria Elena Chavez-Echeagary
and Javier Gonzalez-Sanchez

31

Validating Item Response Theory Models in Simulated Environments
Manuel Hernando, Eduardo Guzmán and Ricardo Conejo

41

Toward a reflective SimStudent:
Using experience to avoid generalization errors
Christopher MacLellan, Noboru Matsuda and Kenneth Koedinger

51

Towards Moment of Learning Accuracy
Zachary Pardos and Michael Yudelson

61

Impact of Prior Knowledge and Teaching Strategies on Learning by Teaching
Ma. Mercedes T. Rodrigo, Aaron Ong, Rex P. Bringula, Roselle S. Basa,
Cecilio Dela Cruz and Noboru Matsuda

71

1

Using Simulated Learners and Simulated Learning
Environments within a Special Education Context
Carrie Demmans Epp1 and Alexandra Makos2
1

Technologies for Aging Gracefully Laboratory (TAGlab), Dept. of Computer Science
University of Toronto, Toronto, Canada
carrie@taglab.ca
2
Ontario Institute for Studies in Education (OISE)
University of Toronto, Toronto, Canada
alexandra.makos@mail.utoronto.ca

Abstract. The needs of special education populations require specific support
to scaffold learning. The design and use of intelligent tutoring systems (ITS)
has the potential to meet these needs. Difficulty in the development of these
systems lies in their validation due to the ethics associated in studying learners
from this population as well as the difficulty associated with accessing members
of this learner group. This paper explores the use of simulated learners as a potential avenue for validating ITS designed for a special education population.
The needs of special education learners are discussed. Potential avenues for
employing simulated learners and simulated learning environments to test ITS,
instructional materials, and instructional methods are presented. Lastly, the expansion of an educational game designed to develop emotion recognition skills
in children with autism spectrum disorder is used to illustrate how simulated
learning environments can be used to support the learning of these students.
Keywords: Special Education, Ethics, Simulated Learners, Simulated Learning
Environments

1

Introduction

Many intelligent learning environments have been shown to help learners who belong
to the general population, but few existing systems have been shown to meet the
needs of those who fall under the umbrella of special education [1]. Learners in this
category have highly differentiated needs that are specified in an individual education
plan (IEP) [2]. Their increased need for personalization and continuous reinforcement
makes the argument for augmenting their education with intelligent tutoring systems
(ITS) even stronger. However, this has not been done widely.
Several factors may contribute to the lack of ITS use within special education. The
lack of validation that has been performed on the systems for special education populations [1], the difficulty of integrating ITS into special education settings [3], and the
difficulty of designing activities that ensure deep understanding may contribute to the

2

lack of ITS that support this population. The variability of learner needs presents additional challenges for system designers with respect to content development [3].
Furthermore, challenges that relate to the motivation, attitude, and social vulnerability
of members of this population make it more difficult to design and validate systems.
Developing systems for the special education population as a whole is difficult [4].
In addition to the above challenges, it may be difficult for designers to obtain access to a sufficiently large sample of the population to ensure that their ITS is beneficial in special education contexts. This is where the use of simulated learners and
simulated learning environments can be advantageous since their use can mitigate the
challenges presented by limited access to this vulnerable population and reduce the
negative ethical implications of testing these systems on members of this population.
It is important to look at the research on situated learning in order to understand the
achievements in best practices and lessons from research on simulated learning. Critical to this research is the combination of immersion and well-designed guidance that
supports the situated understanding of learners whereby they not only have a deep
understanding of the particular concepts that are being targeted, but the learners are
able to then generalize and apply these learned concepts to other contexts [5]. Research shows that game-like learning through digital technologies is a viable tool
across disciplines [6] and suggests that elements of game-like learning scaffold and
guide learners towards a deep understanding of concepts. The on demand instruction
of information that is vital to progress in the game is also important [5] and can be
exploited to encourage learning. Simulations can include these elements and use
stimuli to which special education populations react positively. Some stimuli that
have been shown to increase student engagement include music, visual cues, and
social stories [7]. Not only do these “strategies…help teachers increase engagement
[but they] are vital for promoting positive outcomes for students” [7].
To support the argument for the use of simulated learners in this educational context, we first describe the characteristics and needs of this population as well as the
learning environments in which they can be found. Following this, we discuss the use
of ITS by special education students, which includes student interactions with agents.
After laying this groundwork, we discuss the ethical implications and potential benefits to using simulated learners for validating ITS for use by special education populations. We then describe the potential uses of simulated learners and learning environments. This includes the description of an educational game, called EYEdentify,
which was designed to develop emotion recognition skills in children with autism
spectrum disorder (ASD). A discussion of how gaming principles and simulated environments can be further employed to expand EYEdentify for the purposes of helping
scaffold learners’ social interactions is provided.

2

Special Education

An introduction to the learning environments that exist in schools and the needs of
learners who are classified as special education is presented. The use of agents and
other forms of intelligent tutoring, within special education contexts, is then provided.

3

2.1

Learners and Learning Environments

These learners are either segregated into dedicated special education classrooms or
integrated into classrooms whose majority population consists of learners from the
general student body. Research has explored the design and integration of ubiquitous
technology into special education classrooms [8], but few e-learning environments
have been created to specifically support these students.
The needs and abilities of this population are highly variable, which can make generalizability hard [9]. This variability can be used to argue for the importance of personalizing students’ learning materials, environments, and experiences, which is evidenced by the existence of IEP that detail the learner’s specific needs and the accommodations that can be used to help the learner succeed [2]. Some of these accommodations include providing learners with additional time in order to complete tasks [1]
or allowing learners to perform tasks using different modalities (e.g., oral responses
rather than written ones) [2]. While these accommodations are necessary to ensuring
the learner’s success, it can be difficult to provide the necessary support, especially in
integrated classrooms. The use of ITS that better support the individual needs of these
learners could help alleviate the teacher’s need to provide these supports.
2.2

Simulated Learner and Agent Use

While the use of agents within ITS used by special education populations has been
studied, it appears that researchers and system developers are not simulating learners
who have special needs. Nilsson and Pareto have instead used teachable agents within
a special education context to help learners improve their math skills [3]. However,
they experienced difficulty integrating the ITS into the classroom. Whereas, Woolf et
al. were able to integrate their ITS into a classroom that had a mixed demographic:
the class consisted of both low and high performing students, and of those who were
low-performing, one third had a learning disability [10]. In this case, students interacted with an agent who played the role of a learning companion in order to support
the learner’s affective needs. It was found that this approach was especially beneficial to the low-performing students in the study, which may indicate the potential that
this system holds for helping many of the learners who fall under the special education umbrella. Other work has also shown that interactions with agents within an ITS
can improve or maintain learner interest and motivation [1].

3

Ethics

Given the vulnerable nature of this population, it is important that we not increase the
risk that they are exposed to by introducing them to ITS or other learning techniques
that have not been properly vetted since these could threaten the emotional well-being
of learners or their learning success [11]. The use of simulated learners can help ensure that these systems are properly tested before we expose special education learners to them. Simulated learners can help teachers, instructional designers, and system
developers meet the ethical guidelines of professional bodies by providing evidence

4

of the limitations and appropriateness of the instructional methods used by systems or
of the system itself [12].

4

Potential for Simulated Learner Use

We foresee two potential uses for simulated learners within a special education context both of which have been explored within other contexts. The first is during the
development and testing of ITS [13, 14], and the second is for teacher training [13].
Using simulated learners in these ways provides developers and instructors with access to learners in this population and prevents any potential harm that could result
from experimenting with members of this population. However, it may create a false
sense of the validity and usefulness of different systems and instructional techniques,
especially when we lack a full understanding of the abilities and symptomology of
some members of this population (e.g., those with Phelan-McDermid Syndrome).
Generalizability is difficult to perform with this population [9], but some level of
generalizability is required if a system is to be used by many people. Unfortunately,
current design methods, such as participatory design, fail to address how the system's
use and design should change over time. Furthermore, most users are unable to predict how they will use a system until they have integrated that system into their environment [15]. Carrying these challenges into the special education domain increases
their severity because of the additional communication barriers that may exist between system designers and learners with special needs [4]. While observation is a
component of many design methods, the lack of access to this population when combined with the communication challenges that exist reduces the feasibility of employing many of the more traditional user-centered design techniques.
Using simulated learners could benefit system designers and developers by allowing them to evaluate a system with various members of the special education population. This could reduce demands on a vulnerable population while allowing for some
level of system validation to be performed. Furthermore, the use of simulated learners
would allow systems to be tested with a far greater variety of learner types in order to
identify where the system may or may not be beneficial. If the system were webbased, the simulated learners could be implemented using a Selenium test suite based
on behavioural models of the system's target learners.
To effectively use simulated learners in this context, it is important to create these
learners using different and competing theoretical models of their behaviours and
abilities. This also alleviates some of the concerns that have been expressed over the
use of simulated users when testing adaptive systems [16]. The source of these models can be teachers or special education experts since their mental models might inform good stereotype-based models of learners that capture general behaviours which
are grounded in the expert's classroom experience. For example, haptic feedback can
be used to reinforce certain behaviours (e.g., pressing a button) in children with ASD.
However, we would argue for also including models from other sources since the
above experts are in short supply and cannot provide sufficient diversity in the models
to ensure that systems are adequately tested for a general special education popula-

5

tion. Simulated learners can be created from the cognitive models that are currently
described in the educational psychology literature or through the application of educational data mining and learning analytics techniques to the logs of ITS usage where
low performing and special education students were included in the classroom intervention. An example from the educational psychology literature could consider models of attention deficit hyperactivity disorder (ADHD), which include the amount of
hyperactivity and inattention that a learner has, to create simulated students that behave in a way that is consistent with both the inattention that is known to affect individual outcomes and the hyperactivity that can affect the classroom environment for
all students. Thus, allowing teachers to explore strategies that minimize the impact of
both of the behaviours that characterize students with ADHD [17].
The diversity of models on which the simulated learners are based may help compensate for the inaccuracies that are inherent to modeling techniques, therefore, reducing the need for simulated learners to have high-fidelity cognitive models. Especially,
since there is an incomplete understanding of the cognitive processes of all those who
fall under the umbrella of special education, as is demonstrated by research in mathematics and learning disabilities [18].
That said, simulated learners that are based on these models could be used to validate the design of learning materials and to ensure their effectiveness or comprehension [13, 14]. Teachers could use simulated learners to test learning materials for their
ability to increase learner engagement across a variety of contexts [7] before trying
the materials on learners in their class. This would give teachers the opportunity to
refine their teaching materials and confirm their suitability for students in the class.
Simulated learners can also be used to help prepare teachers either during preservice training or before a new school year begins when the teacher is preparing for
his/her incoming students [13]. The use of agents who play different types of special
education learners reduces the need to worry about the possible negative consequences that mistakes would have on learners [19]. This use of simulated learners also holds
the potential to reduce teacher errors since teachers can try new techniques with the
simulated learners and learn from those experiences, which may reduce the risk of
their committing errors with live learners.

5

Potential for Simulated Learning Environment Use

While simulated learning environments can pose a threat to learning because of the
complexity of the learning experience [20], they still hold the potential to benefit
learners with special needs. Simulated environments allow learners to take risks in
order to develop a deeper understanding of the situations they encounter [5]. This can
increase learner awareness of potential situations that could be encountered when
interacting with others. Ideally, simulated learning environments would be used to
help the learner develop and transfer skills into the real world by gradually increasing
the external validity of the tasks being performed.
Simulations allow system designers to ensure that the problems or activities being
studied resemble those that learners experience outside of the simulation [1] and they

6

allow for the gradual increase in the complexity and ecological validity of tasks [21].
This means that learners can begin their learning activities in a simpler environment
that is safe and progress towards more realistic situations, enabling the use of van
Dam's spiral approach, where learners encounter a topic multiple times at increasing
levels of sophistication [22]. This can help learners transfer their developing skills
into the real world. Additionally, the use of simulations accessible on different technologies can shift learner dependence on experts to technology whereby learner use of
the technology can help learners gain a sense of independence and begin to develop
the skills required to expand and extend their interactions to the real world [23]. We
illustrate this trajectory through a discussion of a mobile game that was designed to
help children with autism spectrum disorder learn to recognize emotions.
5.1

EYEdentify: An Educational Game for Emotion Recognition

EYEdentify is a mobile application for the Android platform that is designed to develop the emotion recognition skills of children with ASD since these are lacking.
Previous technologies that have tried to teach this skill to children with ASD have
primarily focused on the use of videos to model emotions for the learner [24]. Current
research focuses on social skill development through the use of interventions that use
a video series to develop social skills by exploiting the relationship between facial
expressions and emotion [4, 25]. Emotion recognition research suggests the most
important features of the face necessary to correctly identify emotions are the eyes
and the mouth [26]. Considering research on social skill development and advancements in portable technology, a mobile application that can support anytimeanywhere support to children with this deficit is timely.
EYEdentify is a game that uses a basic learner model to provide a flexible intervention in the form of an engaging game. It has an open learner model that can show
the child's progress to parents, caregivers, teachers, and specialists. The first version
of this application incorporates four emotions (i.e., happy, sad, frustrated, and confused) into a matching game that progresses through different levels (Fig. 1). There
are three types of images that are used in this game to help scaffold the child’s learning: cartoon robot faces, real faces that are superimposed on robot faces, and photographs of actual faces. The cartoon robot faces are designed to emphasize the eyes
and mouth. The superimposed faces are designed to activate the child’s knowledge of
focusing on the eyes and mouth to correctly identify the displayed emotions while
maintaining the scaffold of the robot head. The photograph of an individual making a
particular expression is used to activate the knowledge from the previously superimposed images to correctly identify the emotions. Difficulty increases with respect to
the type of emotion that is incorporated into game play and the types of images that
are used. Positive feedback is provided to the child throughout the game to encourage
continuous play. The game also has a calming event that is triggered by the accelerometer when the mobile device is shaken aggressively. The calming event increases
the volume of the music that is being played and prompts the child to count to ten.
The child is then asked whether or not s/he wants to continue playing the game.

7

Fig. 1. The gameplay screen with the correct responses identified (surrounded in green).

The mobile application provides the ability to customize game play by incorporating personalized feedback and images. Users can customize feedback by typing a
comment and recording an audio message before adding this feedback to the schedule. Image customization uses the front camera of the device to capture individuals
parroting the facial expression represented on the robot prompt. As children progress
through the levels, they are rewarded with parts to assemble their own robot.
The current version focuses on developing emotion recognition skills for four of
the fifteen basic emotions identified by Golan et al. [25]. The addition of the remaining eleven emotions could be used to extend game play. Currently, the mobile application is functional; however, more emotions are being incorporated and iOS versions
are being developed before releasing EYEdentify on Google Play and the App Store.
5.2

Expanding EYEdentify to Include a Simulated Learning Environment

The expansion of EYEdentify to include a simulated learning environment draws on
Csikszentmihalyi's definition of flow and research on gaming. Flow is described as
the experience of being fully engaged in an activity where an individual is “so involved…that nothing else seems to matter” [27]. This is derived from activities where
a person’s skills are matched to the challenges encountered [27]. For learners, this
means that they will be in a mental state that keeps them motivated to stay involved in
a particular activity. Research in gaming and game design incorporates these psychological underpinnings whereby elements of a game seek to cultivate and support the
player’s active engagement and enhanced motivation [28]. In educational games,
these elements are employed to scaffold learning just-in-time and provide instructors
with the ability to adapt the system to the specific needs of the learner [29].
EYEdentify currently provides a matching game with rewards that are selfcontained within the mobile application. Preliminary trials indicate that it keeps learners involved in the activity of identifying emotions for long periods of time. These

8

trials parallel the findings of research that used a video intervention program known
as “The Transporters” to develop the social skills of children with ASD [30].
EYEdentify’s game play can be expanded into simulated learning environments to
move players beyond the acquisition of emotion recognition skills toward the development of social skills. In creating game-based simulations for learners to use, the
capacity to scaffold their learning within game play and support the development of
transferable skills to the real-world increases.
There are several ways to expand game play into a simulated learning environment. All possibilities would require the mastery of basic emotion recognition and
could involve levels of progressive difficulty that incorporates these emotions into
depictions of social situations. The front camera of the mobile device could be used to
scaffold the recognition of emotions by way of augmented reality, as could the recent
introduction of Google glass. Avatars that represent individuals from the learner’s
day-to-day life could be used by learners to practice particular social situations. Additionally, game play could incorporate depictions of situations that model different
social interactions. This could then be incorporated with a Sims-like environment
where learners would have to identify the emotion of the character that they are interacting with and demonstrate the appropriate behaviour or emotional response. Specific to keeping learners engaged, the addition of an emotion recognition system that can
detect the learner’s emotion from the front camera and keep track of their emotion
when playing the game to determine that learner’s level of engagement would be
useful. Through the development of these possibilities, EYEdentify has the potential
to enhance learners’ emotion recognition and social skill development in a way that
enables the learner to transfer these skills to their day-to-day encounters.

6

Conclusion

The use of simulated learners and learning environments within special education
contexts holds great potential for improving the quality and applicability of ITS use
by members of this population. Simulated learners can be used to test learning materials, learning methods, and ITS to ensure their appropriateness for the members of this
population, who have highly variable needs. The use of simulated learners and learning environments can be further exploited for teacher training. In addition to this use,
simulated learning environments can be used to help learners who have been classified as having special needs to transfer their knowledge and skills to their everyday
lives. The potential for members of this population to use simulated learning environments was illustrated through an example of an educational game, EYEdentify,
that is used to help children with autism spectrum disorder improve their ability to
recognize emotions. The described potential expansions of this game show how different approaches to simulated learning environments and the use of augmented reality can be used to help learners transition between the simulated world and the one
they encounter every day.

9

References
1.

2.
3.

4.

5.
6.

7.

8.

9.

10.

11.

12.
13.

14.

Bruno, A., Gonzalez, C., Moreno, L., Noda, M., Aguilar, R., Munoz, V.: Teaching mathematics in children with Down’s syndrome. In: Artificial Intelligence in Education
(AIED). Sydney, Australia (2003).
Government of Ontario: Individual Education Plans Standards for Development, Program
Planning, and Implementation. Ontario Ministry of Education (2000).
Nilsson, A., Pareto, L.: The complexity of integrating technology enhanced learning in
special math education - a case study. In: 5th European Conference on Technology Enhanced Learning on Sustaining TEL: from Innovation to Learning and Practice. pp. 638–
643. Springer-Verlag, Berlin, Heidelberg (2010).
Wainer, A.L., Ingersoll, B.R.: The use of innovative computer technology for teaching
social communication to individuals with autism spectrum disorder. Research in Autism
Spectrum Disorders. 5, 96–107 (2011).
Assessment, equity, and opportunity to learn. Cambridge University Press, Cambridge ;
New York (2008).
Jackson, L.A., Witt, E.A., Games, A.I., Fitzgerald, H.E., von Eye, A., Zhao, Y.: Information technology use and creativity: Findings from the Children and Technology Project.
Computers in Human Behavior. 28, 370–376 (2012).
Carnahan, C., Basham, J., Musti-Rao, S.: A Low-Technology Strategy for Increasing
Engagement of Students with Autism and Significant Learning Needs. Exceptionality. 17,
76–87 (2009).
Tentori, M., Hayes, G.: Designing for Interaction Immediacy to Enhance Social Skills of
Children with Autism. Ubiquitous Computing (Ubicomp). pp. 51–60. ACM, Copenhagen,
Denmark (2010).
Moffatt, K., Findlater, L., Allen, M.: Generalizability in Research with Cognitively Impaired Individuals. In: Workshop on Designing for People with Cognitive Impairments,
ACM Conference on Human Factors in Computing Systems (CHI). ACM, Montreal, Canada (2006).
Woolf, B.P., Arroyo, I., Muldner, K., Burleson, W., Cooper, D.G., Dolan, R., Christopherson, R.M.: The Effect of Motivational Learning Companions on Low Achieving
Students and Students with Disabilities. In: Aleven, V., Kay, J., and Mostow, J. (eds.) Intelligent Tutoring Systems (ITS). pp. 327–337. Springer Berlin Heidelberg, Berlin, Heidelberg (2010).
Cardon, T.A., Wilcox, M.J., Campbell, P.H.: Caregiver Perspectives About Assistive
Technology Use With Their Young Children With Autism Spectrum Disorders. Infants &
Young Children. 24, 153–173 (2011).
Code of Fair Testing Practices in Education. Washington, D.C.: Joint Committee on Testing Practices, American Psychological Association (1988).
VanLehn, K., Ohlsson, S., Nason, R.: Applications of Simulated Students: An Exploration. International Journal of Artificial Intelligence in Education (IJAIED). 5, 135–175
(1996).
Mertz, J.S.: Using A Simulated Student for Instructional Design. International Journal of
Artificial Intelligence in Education (IJAIED). 8, 116–141 (1997).

10

15. Dawe, M.: Design Methods to Engage Individuals with Cognitive Disabilities and their
Families. In: the Science of Design Workshop, ACM Conference on Human Factors in
Computing Systems (CHI) (2007).
16. Paramythis, A., Weibelzahl, S., Masthoff, J.: Layered Evaluation of Interactive Adaptive
Systems: Framework and Formative Methods. User Modeling and User-Adapted Interaction (UMUAI). 20, 383–453 (2010).
17. Rogers, M., Hwang, H., Toplak, M., Weiss, M., Tannock, R.: Inattention, working
memory, and academic achievement in adolescents referred for attention deficit/hyperactivity disorder (ADHD). Child Neuropsychology. 17, 444–458 (2011).
18. Geary, D.C.: Mathematics and Learning Disabilities. J Learn Disabil. 37, 4–15 (2004).
19. Ogan, A., Finkelstein, S., Mayfield, E., D’Adamo, C., Matsuda, N., Cassell, J.: “Oh dear
stacy!”: social interaction, elaboration, and learning with teachable agents. In: ACM Conference on Human Factors in Computing Systems (CHI). pp. 39–48. ACM, New York,
NY, USA (2012).
20. Moreno, R., Mayer, R., Lester, J.: Life-Like Pedagogical Agents in Constructivist Multimedia Environments: Cognitive Consequences of their Interaction. In: World Conference
on Educational Multimedia, Hypermedia and Telecommunications (EDMEDIA). pp. 776–
781 (2000).
21. Henderson-Summet, V., Clawson, J.: Usability at the Edges: Bringing the Lab into the
Real World and the Real World into the Lab. In: Workshop on Usability in the Wild, International Conference on Human-Computer Interaction (INTERACT) (2007).
22. Van Dam, A., Becker, S., Simpson, R.M.: Next-generation educational software: why we
need it & a research agenda for getting it. EDUCAUSE Review. 40, 26–43 (2007).
23. Stromer, R., Kimball, J.W., Kinney, E.M., Taylor, B.A.: Activity schedules, computer
technology, and teaching children with autism spectrum disorders. Focus on Autism and
Other Developmental Disabilities. 21, 14–24 (2006).
24. DiGennaro Reed, F.D., Hyman, S.R., Hirst, J.M.: Applications of technology to teach
social skills to children with autism. Research in Autism Spectrum Disorders. 5, 1003–
1010 (2011).
25. Golan, O., Ashwin, E., Granader, Y., McClintock, S., Day, K., Leggett, V., Baron-Cohen,
S.: Enhancing Emotion Recognition in Children with Autism Spectrum Conditions: An
Intervention Using Animated Vehicles with Real Emotional Faces. Journal of Autism and
Developmental Disorders. 40, 269–279 (2009).
26. Erickson, K., Schulkin, J.: Facial expressions of emotion: A cognitive neuroscience perspective. Brain and Cognition. 52, 52–60 (2003).
27. Csikszentmihalyi, M.: Flow: The Psychology of Optimal Experience. Harper Perennial
Modern Classics (2008).
28. Tom Chatfield: 7 ways games reward the brain | Video on TED.com. (2010).
29. Fernández López, Á., Rodríguez Fórtiz, M.J., Noguera García, M.: Designing and Supporting Cooperative and Ubiquitous Learning Systems for People with Special Needs. In:
Confederated International Workshops and Posters on the Move to Meaningful Internet
Systems: ADI, CAMS, EI2N, ISDE, IWSSA, MONET, OnToContent, ODIS, ORM,
OTM Academy, SWWS, SEMELS, Beyond SAWSDL, and COMBEK. pp. 423–432.
Springer-Verlag, Berlin, Heidelberg (2009).
30. The Transporters. Changing Media Development Ltd (2006).

11

Simulated Students, Mastery Learning, and Improved
Learning Curves for Real-World Cognitive Tutors
Stephen E. Fancsali, Tristan Nixon, Annalies Vuong, and Steven Ritter
Carnegie Learning, Inc.
Frick Building, Suite 918
437 Grant Street, Pittsburgh, PA 15219

{sfancsali, tnixon, avuong, sritter}
@carnegielearning.com

Abstract. We briefly describe three approaches to simulating students to develop and improve intelligent tutoring systems. We review recent work with simulated student data based on simple probabilistic models that provides important
insight into practical decisions made in the deployment of Cognitive Tutor
software, focusing specifically on aspects of mastery learning in Bayesian
Knowledge Tracing and learning curve analysis to improve cognitive (skill)
models. We provide a new simulation approach that builds on earlier efforts to
better visualize aggregate learning curves.
Keywords: Knowledge tracing, learning curves, student modeling, Cognitive
Tutor, simulation, simulated students, mastery learning

1

Introduction

There are at least three general approaches to simulating students for the purposes
of improving cognitive (skill) models and other features of intelligent tutoring systems (ITSs). One approach, generally connoted in discussions of “simulated” students or learners, employs aspects of cognitive theory to simulate students’ learning
and progression through ITS problems (e.g., via machine learning or computational
agents like SimStudent [2]). Another class of simulations makes use of relatively
simple probabilistic models to generate response data (i.e., Bayesian Knowledge
Tracing [BKT] [1]) intended to represent a (simulated) student’s evolving performance over many practice attempts. Third, there are data-driven approaches that do
not easily fit into either of the first two categories.
In this work, we explicate and provide examples of each approach and briefly describe Carnegie Learning’s Cognitive Tutors (CTs) [3]. We then focus on the second
approach and review recent work on simulations of student learning with simple
probabilistic models. These simulation studies provide novel insights into a variety of
features of CTs and their practical deployment.

12

CTs implement mastery learning; mathematics content is adaptively presented to
students based upon whether the tutor has judged that a student has mastered particular skills. Mastery is assessed according to whether the tutor judges that the probability that a student has mastered a particular skill exceeds a set threshold. We review a
simulation study that provides for best and worst-case analyses (when “ground truth”
characteristics of simulated learner populations are known) of tutor skill mastery
judgment and efficient student practice (i.e., adaptively providing students with opportunities to practice only those skills they have not mastered). This study not only
provides justification for the traditionally used 95% probability threshold, but it also
illuminates how the threshold for skill mastery can function as a “tunable” parameter,
demonstrating the practical import of such simulation studies.
Finally, learning curves provide a visual representation of student performance on
opportunities to practice purported skills in an ITS. These representations can be used
to analyze whether a domain has been appropriately atomized into skills. If opportunities correspond to practice for a single skill, we expect to see a gradual increase in
the proportion of correct responses as students get more practice opportunities. If, for
example, the proportion of students responding correctly to an opportunity drastically
decreases after three practice opportunities, it seems unlikely that the opportunities
genuinely correspond to one particular skill. Turning to the third, data-driven approach to simulating students, we provide a new method to visualize aggregate learning curves to better drive improvements in cognitive (skill) models used in CTs, This
approach extends recent work that explores several problems for utilizing learning
curves aggregated over many students to determine whether practice opportunities
correspond to a single skill.

2

Cognitive Tutors

CTs are ITSs for mathematics curricula used by hundreds of thousands of K-12
and undergraduate students every year. Based on cognitive models that decompose
problem solving into constituent knowledge components (KCs) or skills, CT implements BKT to track student skill knowledge. When the system’s estimate of a student’s knowledge of any particular skill exceeds a set threshold, the student is judged
to have mastered that skill. Based on the CT’s judgment of skill mastery, problems
that emphasize different skills are adaptively presented so that the student may focus
on those skills most in need of practice.

3

Three Approaches to Simulating Learners

There are at least three general simulation methods used to model student or learner performance. One simulation strategy, based on cognitive theories such as ACT-R
[4], explicitly models cognitive problem-solving processes to produce rich agentbased simulated students. The SimStudent project ([2], [5]), for example, has been
developed as a part of a suite of authoring tools to develop curricula for CTs, called
Cognitive Tutor Authoring Tools (CTAT) [6]. SimStudent learns production rules

13

from problem-solving demonstrations (e.g., an author providing simple demonstrations of problem solutions or via ITS log data). These human-interpretable production
rules correspond to KCs that comprise cognitive models vital to CTs. SimStudent
aims to simplify development of new CT material by automating the discovery of KC
models in new domains via a bottom-up search for skills that potentially explain the
demonstrations.
Second, there are numerous probabilistic methods that model task performance as a
function of practice, according to various task and learner-specific parameters. One
may instantiate numerous such models, with varying parameters, and sample from the
resulting probability distributions to obtain simulated performance data for an entire
hypothetical learner population.
One common example is a Hidden Markov Model (HMM) with two latent and two
observable states, that can serve as a generative BKT model, using parameters specified according to expert knowledge or inferred by a data-driven estimation procedure.
Two hidden nodes in the HMM represent “known” and “unknown” student
knowledge states. In practice, of course, student knowledge is latent. Simulated students are assigned to a knowledge state according to BKT’s parameter for the probability of initial knowledge, P(L0), and those in the “unknown” state transition to the
“known” state according to the BKT parameter for the probability of learning or
transfer, P(T). Simulated, observed responses are then sampled according to BKT
parameters that represent the probability of student guessing, P(G) (i.e., responding
correctly when in the unknown state) and slipping, P(S) (i.e., responding incorrectly
when in the known state), depending upon the state of student knowledge at each
practice opportunity.
Contrary to her real-world epistemological position, simulations generally allow an
investigator to access the student’s knowledge state at each simulated practice opportunity. This allows for comparisons between the “ground truth” of skill mastery and
any estimate derived from resulting simulated behavior. Clearly, richer cognitive
agents, such as SimStudent, provide a more complete picture of the student’s cognitive state at any point.
Simpler probabilistic models represent student knowledge of a skill with a single
state variable, so they correspondingly scale better to larger scale simulations of
whole populations. While a probabilistic model only requires a reasonable distribution
over initial parameters, richer cognitive models may require training on a great deal of
detailed, behavioral or demonstration data. Nevertheless, cognitive model-based
simulations allow us to investigate issues like timing (i.e., response latency), sensitivity to input characteristics, and error patterns in learner responses.
There are many cases in which a relatively simple probabilistic model may be of
utility, despite its impoverished nature. A simplistic representation of student
knowledge provides an ideal situation to test the performance and characteristics of
inference methods using data from a known generating process and parameters. One
might, for example, compare the point at which simulated students acquire knowledge
of a skill to the point at which the CT judges the student to have mastered the skill.
The approach thus allows for students of “best” and “worst” case scenarios with respect to the relationship between how the CT models students and the actual make up

14

of (simulated) student populations. We can better understand the dynamics of the
student sub-populations we inevitably face in practice by simulating data from diverse
sub-populations, the make up of which we can specify or randomize in various ways.
Furthermore, we can simulate student performance (sometimes augmenting available
empirical data) both with and without mastery learning (i.e., students being removed
from a population because they have mastered a skill) on learning curves constructed
from aggregate data.
Previous work [7] explored a third, data-driven simulation method that “replays”
empirical student performance data through CT in order to estimate the impact of a
change in BKT parameters in a more substantive way. For each KC that occurred in a
given problem, we sampled the next observed response on that KC from the sequence
actually observed from a real student. These responses would then drive updates to
CT’s cognitive model, knowledge tracing, and the problem-selection mechanism. If
more data were required than were observed for a given student, further observations
were sampled from a BKT model initialized to the state inferred from the student’s
actions thus far. By repeating this process for all students in the observed data set, we
could obtain estimates of the number of problems students would be expected to
complete if a change to the cognitive model were implemented.
This method has the advantage of preserving characteristics of real student data rather than resorting to a theoretical model of student performance. However, it does
make several assumptions about the reproducibility of that behavior under the hypothesized changes. Specifically, it assumes that the observed sequence of correct/incorrect responses would not change even given a different selection of problems, potentially emphasizing different KCs. This assumption may be justified if we
believe we have complete coverage of all KCs relevant to the task in question in the
cognitive model and that all KCs are truly independent of each other.
While simulation methods based on rich cognitive theory and data-driven re-play
of empirical data provide many opportunities for future research, we focus in this
paper on simple, probabilistic simulations in the context of the BKT framework.

4

Substantive Measures of Efficient Student Practice

Before we discuss how the BKT mastery threshold probability functions as a “tunable” parameter in an ITS like the CT, we provide “substantive” quantification of
goodness of fit of cognitive/skill models for CTs beyond mere RMSE of prediction
(i.e., beyond the extent to which models can predict whether students will respond
correctly to particular practice opportunities) [8-11]. New error or goodness of fit
measures are countenanced in terms of efficient student practice, based on the number
of practice opportunities (i.e., “over-practice” or “under-practice”) we might expect a
student to experience in a CT. Over-practice refers to the continued presentation of
new practice opportunities, despite the student’s mastery or knowledge of the relevant
KC.1 Student “under-practice” instances are those in which a student has yet to
1

One exception is an experimental study [11] that reports increased efficiency by deploying
parameters estimated using a data mining method called Learning Factors Analysis (LFA).

15

achieve knowledge of a KC, and yet the mastery learning system has judged the student as having mastered it, ending the presentation of further learning opportunities.
From estimates of expected under- and over-practice, one can calculate other meaningful measures of students gains and losses, such as time saved or wasted.
Some of this work [8, 9] uses empirical data to estimate the extent of underpractice and over-practice we might expect students to experience. Specifically, the
expected numbers of practice opportunities it takes a student to reach mastery when
parameters are individualized per student are compared to the expected practice when
a single (population) set of parameters is used to assess all students. One individualization scheme used to study under and over-practice estimates all four BKT parameters, per student, from response data over all relevant skills (i.e., each student receives
one individualized quadruple of BKT parameters for all KCs) [8]. Another approach
[9] only individualizes P(T) for each student based on both per-student and per-skill
components estimated from observed data [12]. Both individualization schemes provide for substantive gains (compared to using a set of population parameters to assess
all students’ progress to mastery) in the efficiency of practice (i.e., fewer expected
under and over-practice opportunities) as well as better prediction performance
judged, in the standard way, by a metric like RMSE.

5

Idealized Performance of Mastery Learning Assessment

Now we address how BKT performs with respect to efficiency of practice in idealized cases in which the composition of student (sub-) populations is known. Simulation studies can shed light on how BKT performs when mastery learning parameters
used by the CT run-time system exactly match those of the generating model (i.e., the
best case), and in worst cases in which student parameters either maximally differ
from mastery learning parameters or vary at random for each student.
Recent work addresses these issues by adopting a probabilistic simulation regime
[10]. Since we can track the point at which a simulated student acquires knowledge
of a skill, we are able to compare this to the opportunity at which the mastery learning
system first judges it to be acquired. Simulations were run for fourteen skills, a subset
of those found by [13] to be representative of a substantial portion of skills in deployed CT curricula, across thousands of virtual students.
Even in idealized, best case scenarios (i.e., when parameters used to assess skill
mastery perfectly match simulated student data-generating parameters), for most
skills and a large number of students, we expect there to be one to four “lagged” practice opportunities between the point at which simulated students transition to mastery
and the point at which the BKT run-time system judges mastery. That is, in general,
even when a student population is modeled “perfectly,” and given the traditional setting of the probability threshold for mastery at 95%, most students should be expected
to see at least a few opportunities beyond the point of skill acquisition. That some
“over-practice” may be inevitable provides a relevant context within which to considEfficiency is operationalized as decreased time required to work through material in the Geometry CT without decreasing overall learning.

16

er empirically driven results of [8, 9]. Although a certain amount of lag may be inherent in the nature of BKT, we seek to establish a range for the “acceptable” lag, and to
better appraise efficiency of practice [10].

6

Mastery Learning Threshold as a “Tunable” Parameter

In addition to lagged opportunities and over-practice, situations in which students
under-practice skills are important to consider. Given the possibly inevitable lag between skill acquisition and mastery judgment, simulations [10] have also been used to
explore how the mastery probability threshold might be “tuned” to influence the
trade-off of over-practice and under-practice experienced by students in mastery
learning systems like CTs.
Pre-mature mastery judgments can lead, for example, to students being moved
along by the CT to problems that emphasize new KCs without having mastered prerequisite KCs. Other things held equal, simulations in [10] provide that pre-mature
mastery judgment is more likely to occur in worst-case scenarios, when masterylearning parameters do not match parameters for sub-populations of simulated students.
Simulations in [10] also establish that the mastery-learning threshold can function
as a tuning parameter, partially governing the trade-off between the expected proportion of students pre-maturely judged to have reached skill mastery and the number of
over-practice opportunities they are likely to experience. As the threshold probability
is increased, the proportion of students assessed as having pre-maturely mastered
skills decreases while the proportion of those that are exposed to practice opportunities after skill acquisition increases (along with the number of lagged and overpractice opportunities, i.e., those beyond a calculated acceptable lag they experience).
The results of [10] show that the traditionally used 95% threshold seems to provide
for a “conservative” tutor that is more likely to present opportunities after skill acquisition rather than under-practice. Depending upon course design and practice regimes, the mastery-learning threshold might be manipulated to important, practical
effect. For example, pre-mature mastery judgments might be acceptable in larger
numbers when there is a mixed-practice regime that will allow students to practice
KCs later in the curriculum.

7

Using Simulations to Illuminate Learning in Learning Curves

Learning curves provide a visual representation of student performance over opportunities to practice skills. For each (purported) skill, we construct a learning curve
by plotting opportunities (i.e., 1st, opportunity, 2nd opportunity, and so on) on the xaxis and the proportion of students that provide correct responses at each opportunity
on the y-axis. Aggregated over real-world student practice opportunity data, such

17

curves provide means by which to visually2 inspect whether opportunities genuinely
correspond to practice of one particular skill. If opportunities correspond to one particular skill, we expect a gradual increase in the proportion of students that respond
correctly with increasing practice. Generally, for well-modeled skills (and a variety
of other cognitive tasks), it is thought that such a plot should correspond roughly to a
power law function (i.e., the power law of practice [14]), though this point is not
without controversy [15]. Recent research [16-17] demonstrates how some aggregate
learning curves can distort the picture of student learning. Aggregate learning curves
may, for example, appear to show no learning, when, in fact all students are learning
at different rates. Others may provide for a small rise in probability of correct response initially but then “drop,” as if students were forgetting, even when individual
students are consistently mastering their skills.
The learning curve of Fig. 1 illustrates aspects of both problems, with a relatively
flat portion, followed by a drop, after a small increase in probability correct from its
initial value. The red line, representing the size of the student population at each opportunity, illustrates that BKT is determining that students are mastering the skill
relatively quickly.

70

900 1000

80

1200

90

100

1400

Learning Curve
Skill: select form of one with numerator of one−1

●
●

●

●
●

800

% correct
50
60

●

●
●

●

●

●
●

●

●
●

●
●

●

●
●

●

●

700

●

●

600

●

500

●

10

200

300

20

400

30

40

●

# students

●
●
●

0

100

% correct = 100 − 45.2 * opportunity^−0.0421
R^2 = 0.0571

1

2

3

4

5

6

7

8

9 10

12

14

16

18

20

22

24

26

28

30

opportunities

Fig. 1. Empirical Learning Curve for Skill “Select form of one with numerator of one”; the blue
line represents empirical data plotted as percentage of correct responses, and the black line
represents a fitted power function. The red line provides the size of the student population.

Two ways to re-visualize problematic, aggregated learning curves have been suggested [16]. One is to provide multiple learning curves (on the same plot) for individual
2

Developers at Carnegie Learning also deploy several data-driven heuristics (that correspond to
various visual features of learning curves) to analyze our large portfolio of KCs (i.e., several
thousand KCs over several mathematics CT curricula) and observed student data to draw attention to those KCs that may require revision in our deployed cognitive models.

18

“segments” of students based upon how many opportunities students, in observed
data, take to reach the mastery learning threshold for a skill. Such segmented learning
curves are provided with the same x-axis and y-axis as standard learning curves (i.e.,
practice opportunity count on the x-axis and, e.g., percentage of student correct response on the y-axis).
The second approach suggested by [16] has the analyst plot “mastery-aligned”
learning curves. In such learning curves, students are also segmented according to the
number of opportunities required to reach mastery, but the end-point of the x-axis
corresponds to the opportunity at which students’ reach mastery (m) and moving left
along the x-axis corresponds to the opportunity before mastery (m-1), the second to
last opportunity before mastery (m-2), and so on.
Further work [17] provides a mathematical explanation, along with proof-ofconcept simulation studies based on HMMs, for the dynamics of aggregate learning
curves to explain how both mastery learning itself and differing student subpopulations, when aggregated, can contribute to learning curves that do not show
learning (or manifest other peculiar, possible deceptive, phenomena like “negative”
learning).
We illustrate an alternative to [16] by providing a method that relies on probabilistic simulation to construct aggregate learning curves that better represent learning in
empirical student data. Specifically, we “pad” empirical data for student skill opportunities with simulated data to mask the effects of attrition due to mastery learning
and possibly “reveal” student learning. Student opportunity data are generated with
the same parameters used to track student progress and the probability of student
knowledge estimated at the point at which the student crossed the mastery threshold.
Such simulations provide us data after a student no longer receives practice opportunities for a particular skill because they have been judged as having achieved mastery.
For the aggregate learning curve of Fig. 1, the “padded” learning curve is Fig. 2.
The fitted power-law slope parameter decreases from -0.042 to -0.363 (indicating
more learning), and the goodness-of-fit of the power law function (R2) increases from
0.0571 to 0.875. We apply the method to 166 skills identified3 by [16] as possibly
problematic in the Cognitive Tutor Algebra I (CTAI) curriculum. We find an improvement (i.e., power-fit parameter decreases from above -0.1 to below -0.1, a criterion deployed by [16]) for 98 skills (59%). While this method provides an improved
visualization and understanding of fewer skills than the disaggregation procedures
suggested by [16], this seems to provide evidence of the great extent to which mastery
learning attrition obfuscates evidence for student learning.
Importantly, our simulation method does not eliminate the early dip in the learning
curve at opportunity 3 when little attrition has yet to take place, but only masks the
effects of attrition due to mastery learning. Such an approach focuses largely on a
better representation or visualization of the “tail” of aggregate learning curves. This
3

These skills were chosen because the over-whelming majority of students are judged to
eventually master them (i.e., CT “thinks” the students are learning); they are not premastered (i.e., P(L0) < 0.95); they do not show learning in their aggregate learning curve
(i.e., power-law fit parameter > -0.1); aggregate learning curves for these skills do not have
multiple maxima; and we have data for at least 250 students for these skills [16].

19

allows us to focus on other features of the learning curve that may indicate illmodeled KCs in a cognitive model, software bugs, and other possible problems.

90

100

Simulation−Padded Learning Curve
Skill: select form of one with numerator of one−1

80

●

●

70

●
●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●
●

●

●

●

10

20

30

40

% correct
50
60

●

0

% correct = 100 − 67.5 * opportunity^−0.363
R^2 = 0.875

1

2

3

4

5

6

7

8

9

10

12

14

16

18

20

22

24

26

28

30

opportunities

Fig. 2. Simulation-Padded Learning Curve for Skill “Select form of one with numerator of one”

8

Summary

We briefly reviewed several methods for simulating learners. We focused on ways
in which simple probabilistic models, in contrast to methods that rely on rich cognitive theory, can be used to generate student performance data to help drive practical
decision-making about CT deployment, focusing first on the mastery threshold probability of BKT as a tunable parameter to determine aspects of efficient practice. Then
we introduced a new method for visualizing aggregate learning curves that relies on
both empirical and simulated data that helps to mask the bias introduced by mastery
learning attrition. Future work will further explore these methods, new simulation
regimes, and their practical import.

References
1.

2.

Corbett, A.T., Anderson, J.R.: Knowledge Tracing: Modeling the Acquisition of
Procedural Knowledge. User-Modeling and User-Adapted Interaction 4, 253–278
(1995)
Matsuda, N., Cohen, W.W., Sewall, J., Koedinger, K.R.: Applying Machine Learning
to Cognitive Modeling for Cognitive Tutors. Human-Computer Interaction Institute,
Carnegie Mellon University. Paper 248 (CMU-ML-06-105) (2006)

20

3.

4.
5.

6.

7.

8.

9.

10.

11.

12.

13.

14.

15.
16.

17.

Ritter, S., Anderson, J.R., Koedinger, K.R., Corbett, A.T.: Cognitive Tutors: Applied
Research in Mathematics Education. Psychonomic Bulletin & Review 14, 249–255
(2007)
Anderson, J.R.: Rules of the Mind. Erlbuam, Hillsdale, NJ (1993)
Matsuda, N., Cohen, W.W., Sewall, J., Lacerda, G., Koedinger, K.R.: Evaluating a
Simulated Student Using Real Students Data for Training and Testing. In: Proceedings
of the International Conference on User Modeling (LNAI 4511), pp. 107–116 (2007)
Aleven, V., McLaren, B.M., Sewall, J., Koedinger, K.R.: The Cognitive Tutor Authoring Tool (CTAT): Preliminary Evaluation of Efficiency Gains. In: Proceedings of the
8th International Conference on Intelligent Tutoring Systems, pp. 61-70 (2006)
Dickison, D., Ritter, S., Nixon, T., Harris, T., Towle, B., Murray, R.C., Hausmann,
R.G.M.: Predicting the Effects of Skill Model Changes on Student Progress. In: Proceedings of the 10th International Conference on Intelligent Tutoring Systems (Part II),
pp. 300-302 (2010)
Lee, J.I., Brunskill, E.: The Impact of Individualizing Student Models on Necessary
Practice Opportunities. In: Proceedings of the 5th International Conference on Educational Data Mining, pp. 118–125 (2012)
Yudelson, M.V., Koedinger, K.R.: Estimating the Benefits of Student Model Improvements on a Substantive Scale. In: Proceedings of the 6th International Conference on
Educational Data Mining (2013)
Fancsali, S., Nixon, T., Ritter, S.: Optimal and Worst-Case Performance of Mastery
Learning Assessment with Bayesian Knowledge Tracing. In: Proceedings of the 6th International Conference on Educational Data Mining (2013)
Cen, H., Koedinger, K., Junker, B.: Is Over-Practice Necessary? – Improving Learning
Efficiency with the Cognitive Tutor through Educational Data Mining. In: Proceedings
of the 13th International Conference on Artificial Intelligence in Education, pp. 511–518
(2007)
Yudelson, M.V., Koedinger, K.R., Gordon, G.J.: Individualized Bayesian Knowledge
Tracing Models. In: Proceedings of the 16th International Conference on Artificial Intelligence in Education (2013)
Ritter, S., Harris, T.K., Nixon, T., Dickison, D., Murray, R.C., Towle, B.: Reducing the
Knowledge Tracing Space. In: Proceedings of the 2nd International Conference on Educational Data Mining, pp. 151-160 (2009)
Newell, A., Rosenbloom, P.S.: Mechanisms of Skill Acquisition and the Law of Practice. In: Anderson, J.R. (ed.) Cognitive Skills and Their Acquisition, pp. 1-55. Erlbaum,
Hillsdale, NJ (1981)
Heathcote, A., Brown, S.: The Power Law Repealed: The Case for an Exponential Law
of Practice. Psychonomic Bulletin & Review 7, 185-207 (2000)
Murray, R.C., Ritter, S., Nixon, T., Schwiebert, R., Hausmann, R.G.M., Towle, B.,
Fancsali, S., Vuong, A.: Revealing the Learning in Learning Curves. In: Proceedings of
the 16th International Conference on Artificial Intelligence in Education, pp. 473-482
(2013)
Nixon, T., Fancsali, S., Ritter, S.: The Complex Dynamics of Aggregate Learning
Curves. In: Proceedings of the 6th International Conference on Educational Data Mining
(2013)

21

Exploring through Simulation the Effects of Peer
Impact on Learning
Stephanie Frost1 and Gord McCalla1
ARIES Lab, Dept. of Computer Science, U. of Saskatchewan, Saskatoon, Canada
stephanie.frost@usask.ca, mccalla@cs.usask.ca

Abstract. Simulation modelling helps designers to keep track of many
possible behaviours in a complex environment. Having a technique to
simulate the effect of peer impact on learning allows designers to test the
social effects of their educational software. We implement an agent-based
simulation model based on the ecological approach (EA) architecture [9].
The model considers learner attributes, learning object attributes and
two styles of peer impact to explore the effects when learners are either
positively or negatively impacted by high achieving peers. In this study,
we observe different patterns of behaviour based on the style of peer
impact and by limiting simulated learners’ access to information (the
EA metadata). Gaining understanding of these patterns will inform our
future work on recommending sequences of learning objects (LOs).
Keywords: simulated learning environments, simulated learners, ecological approach, instructional planning

1

Introduction

Before taking an action in a learning environment, it is important for an intelligent tutoring system (ITS) to have some way of estimating the likelihood that
the action will be successful, i.e. that it will benefit the learner(s) involved. To
compute such an estimate, there are many dimensions to consider such as: the
nature of the content being learned, the pedagogical style of the environment,
learning goals, individual learner characteristics, and social factors such as how a
learner’s own performance can be influenced by knowledge of peer performance.
Such complexity is often managed through the use of models.
Simulation modelling can be used by instructional developers for testing their
systems; this was identified by VanLehn, Ohlsson and Nason [11] in a survey of
possible uses of simulated students. One example is SimStudent by Matsuda et
al. [8] which can be used by designers to explore through simulation the effects of
various decisions on cognitive tutor design. Whether a model is used “internally”
(by an ITS to compute the next action) or “externally” (to evaluate a system design), a challenge remains: How does the model estimate the amount of learning
that occurs when a learner interacts with a Learning Object (LO)? In particular, we wanted to explore the impact on learning when learner performance is
influenced by the performance of peers. Some learners may become encouraged

22

when observing high peer achievement and perform even better than they would
have otherwise. Other learners might become discouraged in the same situation and perform even worse. Having a technique to simulate the effects of peer
performance would allow instructional developers to test social effects of their
designs. In this paper, we use simulation to explore the behaviours exhibited by
two different reactions to peer impact.
We describe our approach in Section 2, followed by the simulation study in
Section 3. It is possible to simulate many different kinds of educational software
in the ecological approach (EA) architecture [5], and then test the simulation
under various conditions to get insight into issues the designer is interested in.
Because our model is implemented in the EA architecture, our approach for modelling peer impact can be used across many different styles of learning systems.
The data to feed our simulation is synthetic, but could, itself, be modelled on
data extracted from actual learner behaviour [5]. We follow with a description
of ongoing research that uses simulation for testing and developing a method for
recommending sequences of LOs, and conclude with a discussion of our findings.

2

Model Structure

In another paper [5], we have argued that it is not necessary to model every
detail of the learning process, but that systems can be tested in a simulation
that captures only the most relevant characteristics for a given purpose. Therefore, we take an approach that lets an instructional developer choose different
dimensions – such as attributes of the learning objects, aspects of the pedagogical environment, attributes of the learner – and assign weights to each dimension
according to the priorities of the developer. This section describes the structure
of the simulation model so as to provide background for the experiment around
peer impact, described in Section 3.
The EA architecture [9] provides a way to record metadata about learner
interactions with LOs. As learners interact with LOs, any information that is
known about the learner at the time of the interaction can be saved as metadata
and associated with the LO. The EA assumes that each learner is represented by
a learner model that contains static attributes (characteristics) as well as other
data gathered as they interact with the LOs (episodic).
We developed an agent-based simulation model with very simple abstractions
of learners and LOs. Each learner agent has an attribute, aptitude-of-learner, a
number between (0,1), which we use to model the range of aptitudes (low to high)
different learners have for a given subject matter. In our model, this attribute is
assigned at the start of the simulation and does not change, but in future work we
plan to create more sophisticated simulations where this attribute is not static.
The simulated LOs have an attribute to represent difficulty level, which is also a
number between (0,1) where higher values represent more difficult material. The
simulated LOs are arranged into a random directed acyclic graph to represent
prerequisite relationships between the LOs.

23

The model execution revolves around an atomic action: the learner’s interaction with a LO. This action might occur hundreds or thousands of times during
a simulation run, thus creating a multitude of EA metadata from which measurements can be taken. In related work [5], we introduce the term evaluation
function to describe the function that computes the degree of success as result
of an interaction between a learner and a LO. We will use the term P[learned] to
describe the value that is generated by the evaluation function, i.e. the “probability that the learner learned the LO”, or the “system’s belief that the learner
knows the LO”. The P[learned] value is included as part of the EA metadata
that is associated with LOs after learners interact with them.
Our evaluation function is a weighted sum, where each term deals with a
dimension of learning to be considered. Each dimension of learning is calculated
with a mini function. For example, suppose LearnerA were a novice with aptitudeof-learner=0.1. Next, suppose LOX were a fairly easy LO, which implies a high
probability of success. We use a mini function, difficulty-of-LO, to translate the
LO difficulty attribute into a high probability value, giving difficulty-of-LO=0.8.
Suppose we also wish to take into account that the likelihood of the learner
learning the LO is higher if the learner has already viewed prerequisite LOs.
Prerequisite information is given in the LO attributes. Our simulation model
has a function for hasPrerequisites which searches through the EA metadata to
discover whether the learner has indeed viewed the prerequisites and returns
1.0 if the answer is yes and 0.0 otherwise. If we want these dimensions to have
approximately equal weights, then we can define the evaluation function below
and obtain P[learned] as follows:

(w)(aptitude-of-learner) + (w)(difficulty-of-LO) + (w)(hasPrerequisites)
= (0.33)(0.1) + (0.33)(0.8) + (0.34)(1.0) = 0.637

If, on the other hand, we wish to give the aptitude a higher weight, such
as 60%, then the new value could be (0.6)(0.1) + (0.2)(0.8) + (0.2)(1.0), or
0.42. As expected, giving greater weight to this learner’s low aptitude decreases
the P[learned] somewhat. More dimensions can be incorporated so long as the
weights sum to 1.0. The evaluation function, implemented as a weighted sum,
will provide an estimated likelihood the LO has been learned between (0,1),
making it easy to compare averages of such P[learned] values between simulation runs. However, we caution against comparing two simulation runs with
different evaluation functions (i.e. different weights or dimensions) because that
would be like comparing two numbers with different units of measure.
The independent variables in our experiment are the aptitude-of-learner values, the difficulty level values, the directed acyclic graph giving prerequisite relationships between LOs, as well as a dimension called peer-impact, which is
explained in the next section.

24

3

Experiment

Our experiment is intended to explore through simulation the effects of peer
impact on learning. We motivate the experiment by visiting literature around
how peers can impact each other’s scores.
Students are impacted by their peers even in their ordinary lives. A study
was performed by Hanushek et al. [6] to clarify the impacts of peer group characteristics on achievement in the context of family and school factors, race and
socio-economic status. Results suggested that students benefitted from higher
achieving schoolmates. In contrast, the American Academy of Pediatrics warned
that Facebook pages can make some children feel badly because they see themselves as being inferior to their peers [10]. This effect is due to the nature of
Facebook, where most users will censor their posts and only share the most positive information about themselves, skewing the view of reality. Along the same
lines, Daniel et al. [3] found in a study that learners will usually only participate
in online learning activities if they have trust in their peers or some degree of
self confidence.
Others have used simulations to study peer effects. Mao et al. [7] used a
simulation model to study the impact of social factors in a course where students
shared learning materials with each other. The output of Mao et al.’s model was
a comparison of the amount of sharing connected to status levels: gold, silver,
bronze, common. Populations fluctuated as users began at the common status
and gradually transitioned between levels. The paper concluded that simulation
models can be useful for developing and improving incentive mechanisms in
virtual communities. In a different study, Zhang et al. [12] studied the fluctuation
of a population of learners through various activities: registration, activation,
action and adaptation. The authors found that learners who participated the
most were also the ones most sensitive to changes in the community and had
the most fluctuations.
This research, and other research, shows that a learner’s score can be impacted by peer performance. We decided to explore this issue by creating a
notion of “peer impact”, where learners respond differently from one another
according to how well other learners are doing in mastering the LOs. This takes
the form of a new dimension in our evaluation function called peer-impact. Like
the other dimensions we discussed in Section 2 (aptitude-of-learner, difficulty-ofLO, hasPrerequisites), this is a function that produces a value between (0,1) to
represent a positive or negative impact on P[learned]. In our experiment, we use
the following Equation 1 to compute P[learned] each time a learner visits a LO.

.25(apt-of-learner) + .25(diff-of-LO) + .25(hasPrereq) + .25(peer-impact) (1)
We created two styles of peer impact called reinforcing and balancing which
refer to a comparison between an individual learner’s average P[learned] on
the LOs they have viewed so far, compared to the average P[learned] of all
learner agents, which we call “class average”. The information to compute these

25

P[learned] averages is obtained from the EA metadata. Each learner is given
one of these styles at the start of the simulation and it remains fixed. Future
work could explore more sophisticated learner agents where this attribute is not
static.
The reinforcing style means that the learner’s score is “attracted” to the class
average P[learned]. That is, when the class average is higher than their own, the
peer impact function for a reinforcing learner produces a value close to 1; thus
the learner will perform even better than they would have otherwise. This is a
positive feedback loop, because as the learner performs better so does the class
average thus further encouraging the learner to do better. If the class average is
lower than their own, then the peer-impact function gives a value close to zero;
thus the learner will do even worse than they would have otherwise.
Balancing is the opposite. In this case, a learner’s score is “repelled” from
the class average P[learned]. That is, when the class average is higher than the
individual’s average P[learned], then their score will be pulled down lower than
it would have been otherwise. This is a negative feedback loop because when
the class average is high, the learner’s average goes in the other direction. When
the class average is low, then the learner’s score will be boosted higher than it
would have otherwise. In Figure 1, we show the peer-impact function (the values
0.2 and 0.8 were chosen as thresholds to allow clear effects of the two types of
learner to emerge).
if currentLearner BALANCING
if class average is HIGHER than mine
set peerImpact == randomNumBetween(0.0,0.2)
if class average is LOWER than mine
set peerImpact == randomNumBetween(0.8,1.0)
if currentLearner REINFORCING
if class average is HIGHER than mine
set peerImpact == randomNumBetween(0.8,1.0)
if class average is LOWER than mine
set peerImpact == randomNumBetween(0.0,0.2)

Fig. 1. Function to generate peer-impact for a given learner at a given time in the
simulation

The dependent variable in our experiment is the P[learned] values generated
by the simulation; we gain insight into whether the peer impact has a positive or negative effect by observing the relative P[learned] values. We varied
this experiment under six conditions. We varied the proportions of balancing
and reinforcing styles: mostly balancing, mostly reinforcing, and fifty-fifty. For
instance, if the model is set to mostly balancing, when new learners are initialized, they have a high chance of being assigned the balancing personality and
a low chance of being assigned the reinforcing personality. These three propor-

26

tions were each run under two difficulty levels: one with mostly easy LOs and
high aptitude learners, and the other with mostly difficult LOs and low aptitude
learners. These six conditions were hand picked to be representative samples on
a curve of possible population mixes that should provide some insight about the
effect of these two kinds of personality on the learning environment. We ran each
of the six conditions 5 times because our model is stochastic; it produces slightly
different results each time even under the same starting conditions.
A typical result is shown in Figure 2 (fifty-fifty, high difficulty with low aptitude learners). Each line represents the average P[learned] of different portions
of the simulated learner population: the lightest thin line for all learners, black
thin line for the learners who were assigned the reinforcing personality, and the
dark grey thin line for the learners who were assigned the balancing personality.
Normally, our simulation model would be used to evaluate a particular instructional planning technique, but because this experiment is intended to illuminate
peer impact, the order in which LOs are consumed isn’t important. Therefore,
the simulated learners, of which there are 80, visited random LOs, of which there
are 100.

Fig. 2. Typical result

At the start of the simulations, the class average starts at zero. The balancing
simulated learners had higher scores in this state because this is the behaviour
defined in the evaluation function – that balancing learners do well when the class
average is lower than their individual average. The learning gradually increases
for both groups as the simulated learners visit more and more LOs. Although
the results seem low overall – P[learned] only reaching short of 0.3 – this is due
to the number of LOs (100) created in the simulation and the time it would
take for learners to visit them all. We ran the simulation again with only 30
LOs and observed the same patterns, but with a steeper slope; the average
P[learned] reached around 0.5. This raises interesting questions about whether
the amount of time required to learn a set of LOs should actually be represented
with a linear function. In reality, learners would get tired or lose interest or
change their learning goals. Future work could compare instructional plans with
learners having different levels of stamina.

27

The thick lines in Figures 2 and 3 represent subsets of the balancing and
reinforcing personalities whose behaviour we wish to discuss in this experiment.
Simulated learners do not have access to the actual class average, but compute the average based on what other simulated learners have allowed them to
perceive about their performance. Based on Daniel et al.’s [3] results that confident learners are more likely to share their success, simulated learners with high
P[learned] values shared their EA metadata, while those with lower P[learned]
values did not. This creates a suppression effect, where each simulated learner
has access to different information in the computation of how others are doing,
depending on which other learners have suppressed information at the time they
are computing the average.
The thick grey line shows only the balancing learners with low aptitudes while
the thick black line shows only the reinforcing learners with high aptitudes. At
the start of the simulation, the thick black line is below the thick grey line: it is
perhaps surprising that a group of simulated learners with high aptitudes would
have overall lower scores than a group of simulated learners with low aptitudes.
We highlight this because it shows that different parts of the evaluation function
– peer-impact, aptitude-of-learner etc. – can dominate at different times. In this
case, high aptitude can be dominated by peer impact for reinforcing personalities
when the class average is low.
In Figure 3, we observe another interesting phenomenon by injecting 80 more
simulated learners halfway though the experiment, a somewhat contrived situation, although one that might happen in the real world if, say, two classes merged
partway through a course, or if two study groups in an online course were mashed
together, or due to the openness of many online courses (e.g. MOOCs) when new
learners can join any time. Under most of the experimental conditions we tried,
such as the typical result in Figure 2, although the influx of new learners caused
the class average to drop (as expected, because each new learner starts with an
average P[learned] of zero), there was no apparent change in the relative ranking of the groups of learners being measured. That is, if the balancing learners
had the highest average before the influx, this continued afterward. However, in
about a third of the runs with low difficulty LOs and high aptitude learners, the
influx of learners caused a phase shift: now the thick black line jumps above the
thick grey line (see Figure 3). This makes sense: the balancing learners who tend
to do more poorly when the class average drops, do just that. The influx also
creates a situation where there are now learners with high averages intermingled with learners with zero averages; this creates a different environment than
the starting condition where everyone started at zero. Different environmental
conditions cause the model to exhibit different behaviour. With the suppression
effect deactivated, all learners have access to the same information. In this condition, we observed that the thick grey line overlapped with the thick black line
and there was no apparent phase shift (i.e. no lines crossing over).
Even though the observed patterns are merely a result of the evaluation
function implementation – that is, the model is simply doing what it was programmed to do – it helps system designers to keep track of the different possible

28

Fig. 3. Condition showing phase shift

behaviours as they try to design systems to support learning in all of these
conditions: low or high aptitude learners, easy or difficult material, peer effects,
prerequisites and many other possible dimensions, with each behaving differently
in different situations. Without simulation, it is unlikely we would have made
our observations about the phase shift as well as the observation about the high
aptitude reinforcing learners having lower scores than low aptitude balancing
learners. These observations reveal the specific circumstances that instructional
developers should address in order to maximize the expected learning. For example, the system could be programmed to intervene when it detects that the
current class average will push a learner’s expected outcome in an undesirable
direction. When the class average is higher than an individual’s average, the
scores of other learners should be displayed more prominently for the balancing
learners but not for the reinforcing learners.
Through this experiment, we have also shown that simulations can be used to
test unexpected situations. Future experiments could test for influxes of new LOs
instead of new learners. Other variations could look at adding or removing LOs to
impact the difficulty level of the course or the level of expertise of peer learners.
When we injected a herd of simulated learners, we observed some surprising
results. But, by examining the underlying dynamic behaviour as the simulation
proceeded, we could actually explain why these results happened, thus gaining
more intuition about learning that would help to better inform an experiment
that might be carried out with real learners.

4

Other Research Directions

In ongoing work, we are also developing a technique for recommending sequences
of LOs. Instructional planners have been built that explore different kinds of sequencing such as sequencing things of the same type, like “lessons” or even
sequencing several types of activities, like presentations and assessments [1].
Our method involves using the EA metadata to identify “trails” of LOs. We are
investigating the use of user-based and item-based approaches to generate recom-

29

mendations of these trails using Apache Mahout 1 . Using information captured
in the EA metadata, we create metrics for giving sequences a score to reflect the
quality of the sequence, for example does P[learned] increase or decrease over the
sequence. We are also exploring changes to the evaluation function to favour sequences that suggest coherence, such as trails that give learners a view of the big
picture before going into the details. Sequences with high scores are then used
as a basis for recommending sequences to other learners. Our study will examine
whether learners receiving sequence recommendations see any improvement over
learners receiving one LO recommendation at a time.
Other work in simulating recommender systems for learning systems has
been done by Drachsler et al. [4]; but the main difference is that this work did
not involve sequences, peer impact or the EA architecture. Champaign [2] uses
the ecological approach architecture to use the experiences of past learners to
suggest sequences of LOs for future learners while also studying the impact of
peer ratings, which are not the same as our peer impact because our peer impact
is linked to the evaluation function.
Even with the simplistic models of learners and LOs we have presented so
far, the peer impact experiment demonstrates the combinatorics of the various
features is already becoming too complex to rely on human intuition; this is one
of the main reasons for simulation modelling.

5

Conclusion

We created simulated learners whose overall learning was influenced by one
of two styles of peer impact. Our study demonstrated that different patterns
emerge when when simulated learners change their own behaviour based on the
behaviour of the group and when these learners have limited access to information due to others’ ability to suppress their EA metadata. In some conditions, a
phase shift occurred from the initial situation where the class average is zero to a
new situation with some learners having relatively high averages. The simulated
learners prior to the influx had higher averages because they had the opportunity to visit LOs before the arrival of the new simulated learners. One style of
peer impact is not universally better or worse than another, but each has advantages in different circumstances. It is important for instructional developers
to understand such patterns. In future work, the use of simulations with the EA
architecture will shed more light on peer impact and will allow us to also factor
in the effects of different kinds of sequence recommendations.
The EA metadata make it easy to look deeply into the underlying dynamics
and identify the conditions that create such behaviours. The EA metadata also
allow us to change the inputs of the simulation and take measurements, as we
did to compare the P[learned] averages between learners with different styles of
peer impact. By using the EA architecture for the simulation studies, the later
construction of a real learning system is made easier if the real system also uses
1

http://mahout.apache.org/

30

the EA architecture. That is, if the real system also stores information about a
learner’s interaction with a LO as metadata associated with the LO, then estimating the likelihood of success for a real learner follows the same methods used
by developers to estimate the success of simulated learners.
Acknowledgements
We would like to thank Dr. Julita Vassileva for discussions with the first author
about using simulations for instructional planning and for insights on applying
social concepts in the simulation model; Graham Erickson for his ideas about the
evaluation function; and Dr. Nathaniel Osgood for discussions about cause loop
diagrams and agent-based modelling in AnyLogic. We also wish to acknowledge
the Natural Sciences and Engineering Research Council of Canada for funding
some aspects of this research through a Discovery Grant to the last author.

References
[1] Brusilovsky, P. and Vassileva, J.: Course sequencing techniques for large-scale webbased education. International Journal of Continuing Engineering Education and
Lifelong Learning, 13, 75-94 (2003).
[2] Champaign, J.: Peer-based intelligent tutoring systems: a corpus-oriented approach.
Ph.D. Thesis, University of Waterloo, Waterloo, Canada (2012)
[3] Daniel, B., McCalla, G., Schwier, R.: Social Network Analysis techniques: implications for information and knowledge sharing in virtual learning communities. Int. J.
of Interactive Media in Education, 2(1), 20-34 (2008)
[4] Drachsler, H., Hummel, H., and Koper, R.: Using simulations to evaluate the effects
of recommender systems for learners in informal learning networks. In Learning, 3
CEUR Workshop Proc., 404-423 (2008).
[5] Erickson, G., Frost, S., Bateman, S., and McCalla, G.: Using the ecological approach
to create simulations of learning environments. To appear in Lane, H.C., Yacef, K.,
Graesser, A., Mostow, J. (eds.), Proc. 16th Int. Conf. on AIED, Memphis (2013)
[6] Hanushek, E., Kain, J., Markman, J., Rivkin, S.: Does peer ability affect student
achievement? Journal of Applied Economics, 18, 527-544 (2003)
[7] Mao, Y., Vassileva, J., and Grassmann, W.: A system dynamics approach to study
virtual communities. In Proc. 40th Annual Hawaii Int. Conf. on System Sciences,
HICSS ’07, pp.178a-, Washington, DC, USA, IEEE Computer Society (2007)
[8] Matsuda, N., Cohen, W.W., Sewall, J., Lacerda, G. and Koedinger, K.R.: Predicting
students performance with SimStudent that learns cognitive skills from observation.
In Luckin, R., Koedinger, K.R., and Greer, J. (eds.), Proc. 12th Int. Conf. on AIED,
Marina del Rey, 467-476 (2007)
[9] McCalla, G: The ecological approach to the design of e-learning environments:
purpose-based capture and use of information about learners. Journal of Interactive
Media in Education.
[10] Tanner, L: Docs warn about teens and ’Facebook depression’. Associated Press.
http://www.msnbc.msn.com/id/42298789/ns/health-mental health/t/docs-warnabout-teens-facebook-depression/ Accessed April 13, 2013.
[11] VanLehn, K., Ohlsson, S., and Nason, R.: Applications of simulated students: an
exploration. Int. J. Artificial Intelligence in Education, 5, 135-175 (1996)
[12] Zhang, Y., and Tanniru, M.: An agent-based approach to study virtual learning
communities. Hawaii International Conference on System Sciences, 1(11c) (2005)

31

Using HCI Task Modeling Techniques to Measure How
Deeply Students Model

Sylvie Girard, Lishan Zhang, Yoalli Hidalgo-Pontet, Kurt VanLehn,
Winslow Burleson, Maria Elena Chavez-Echeagary, Javier Gonzalez-Sanchez
Arizona State University, Computing, Informatics, and Decision Systems Engineering, Tempe,
AZ, 85281, U.S.A.

{sylvie.girard, lzhang90, yhidalgo, kurt.vanlehn, winslow.burleson, helenchavez, javiergs}@asu.edu

Abstract: User modeling in AIED has been extended in the past decades to
include affective and motivational aspects of learner’s interaction in intelligent
tutoring systems. An issue in such systems is researchers’ ability to understand
and detect students’ cognitive and meta-cognitive processes while they learn. In
order to study those factors, various detectors have been created that classify
episodes in log data as gaming, high/low effort on task, robust learning, etc.
When simulating students’ learning processes in an ITS, a question remains as
to how to create those detectors, and how reliable their simulation of the user’s
learning processes can be. In this article, we present our method for creating a
detector of shallow modeling practices within a meta-tutor instructional system.
The detector was defined using HCI (human-computer interaction) task modeling as well as a coding scheme defined by human coders from past users’
screen recordings of software use. The detector produced classifications of student behavior that were highly similar to classifications produced by human
coders with a kappa of .925.
Keywords: intelligent tutoring system, shallow learning, robust learning, human-computer interaction, task modeling

1

Introduction

Advances in student modeling in the past two decades enabled the detection of
various cognitive [3, 4, 8, 11, 13, 16, 17], meta-cognitive [1,6], and affective [2, 9]
processes during learning based on classification of episodes in log data. Steps have
been taken toward detecting when learning occurs [4] and to predict how much of the
acquired knowledge students can apply to other situations [5, 6]. However, an obstacle in such research is how to gain an understanding of the user’s cognitive or metacognitive processes while learning. While some of the indicators used in the literature

32

are common to any intelligent tutoring system, others are closely linked to the activities and pedagogical goals of a specific application. The adaptation of such indicators
to the design of a new system often necessitates a detailed analysis of the new domain
and how the tutoring system guides learners to acquire its skills and knowledge. In
particular, an issue within this process is the ability to reach common ground between
learner scientists that perform an analysis of learners (meta-)cognitive actions at a
high level - via video or log analysis of student’s past actions for example – and the
definition of the indicators by software engineers, related to how the system was implemented, that can be used to simulate such processes in agreement with the constraints and functionalities of software. We view the specificity of detectors as unavoidable, so the best solution is to develop good methods for analyzing the new tutoring system and designing the detectors. This short article describes our method
and its application to out project, AMT. In the AMT project, a choice was made to use
HCI (human computer interaction) task modeling - a method for formally representing human activity, and by extension, the behavior of an interactive system -, as well
as video coding schemes from human coders, to develop the detectors. The detectors
aim to evaluate student’s use of shallow and deep modeling practices with and without being guided by a meta-tutor, on the domain of dynamic systems modeling.
In Section 2, the AMT learning environment, for which the detectors were created,
is introduced. In a third section, the task model of the user’s activity in AMT is described. Next, the process of defining a coding scheme for the detector with human
coders is presented, followed by the definition of the different classifications that
define the value, the implementation and empirical evaluation of the detector. The
final section summarizes the uses of task modeling within this work, and how it could
be applied in future to other applications.

2

AMT software: a meta-tutor to teach deep modeling of
dynamic systems.

AMT software teaches students how to create and test a model of a dynamic system. In our modeling language, a model is a directed graph with one type of link, as
illustrated in Figure 1. Each node represents both a variable and the computation that
determines the variable’s value. There are three types of nodes.
• A fixed value node represents a constant value that is directly specified in the problem. A fixed value node has a diamond shape and never contains incoming links.
• An accumulator node accumulates the values of its inputs. That is, its current
value is the sum of its previous value plus or minus its inputs. An accumulator
node has a rectangular shape and always has at least one incoming link.
• A function node’s value is an algebraic function of its inputs. A function node has
a circular shape and at least one incoming link.

33

The students’ learning objective is to draw a model representing a situation that is
described in the form of a relatively short text. In the example of Figure 1, the description of the problem was “ Rust destroys steel and can spread quickly. Suppose
you take a large sheet of steel, such as one that might be used as the roof of the boxcar on a train, and you put it outside in the weather. Suppose it starts with a spot of
rust that is 10 square inches in area. However, each week the rust spot gets bigger, as
it grows by 30%. Therefore at the end of the first week, the rust spot is 13 square
inches in area.” and the objective of the problem was to “Graph the size of the rust
spot over 10 weeks.”

Fig. 1. The left image is the example of model, with gray callouts added to explain the
contents of nodes. The right image is the example of a node editor.

The student constructs the model node by node, by filling in all information within
each node in the form of four interactive tabs (description, plan, inputs, and calculations). During construction, students can use the Check button to evaluate the correctness of the current tab, or the Solve it for me button to ask the system to fill out the tab
automatically.
The instruction is divided into three phases: (1) an introduction phase where students learn basic concepts of dynamic system model construction and how to use the
interface; (2) a training phase where students are guided by a tutor and a meta-tutor to
create several models; and (3) a transfer phase where all scaffolding is removed from
soft-ware and students are free to model as they wish. The tutor gives feedback and
corrections on domain mistakes.
The meta-tutor requires students to follow a goal-reduction problem solving strategy, the Target Node Strategy [18]. The basic idea is to focus on one node at a time
(the target node) and completely define it before working on any other node. This
process decomposes the whole problem of modeling a system into a series of atomic
modeling problems, one per node. Like Pyrenees [2], it teaches students that if they
just master this one difficult but small skill, then the rest of the problem solving will
be straight-forward. In addition, the meta-tutor complains if students appear to be
guessing too much or giving up too early, just as the Help Tutor did [3].
While students learn, their motivation, attention to details, and modeling depth can
fluctuate. To assess students, the project needed detectors that detect shallow and
deep modeling practices both with and without the meta-tutor. The measure should be
usable in the transfer phase of the experiment as a dependent variable, because deep

34

modeling is the skill/knowledge that AMT teaches. The depth measure should also
apply to student’s behavior during the training phase so that we can check whether the
instructional manipulations done during that phase have their intended effects (i.e.,
the measure serves as a manipulation check). The detector should further operate in
real time (i.e., it doesn’t require to know future actions or states in order to interpret
the current action) so that it can be eventually be used by the system itself to condition its behavior.

3

Task Modeling: analysis of user’s actions on software

A task model is a formal representation of the user’s activity. It is represented by a
hierarchical task tree to express all sub-activity that enables the user to perform the
planned activity. The tasks need to be achieved in a specific order, defined in the task
tree by the ordering operators. In AMT, every modeling activity follows the same
procedure involving the same help features, task flow, and meta-tutor interventions.
With a single task model of a prototypical modeling task, it is therefore possible to
account for all of the user’s activity in software. Due to the complexity of the final
model, only one sub-activity will be described in this paper, illustrated in Figure 2.
Only part of the model is deployed in the figure, and some subtasks will not be detailed here. In this part of the model the sub-activity the learner wishes to perform is
to create a new node for the dynamic system s/he is currently modeling. We will first
describe the task tree, and then insert the iterations and conditions that enable a formal
verification of the flow of the task within the task model.
Figure 2: Sub-task “Creating a Node” in the AMT activity task model using K-MADe

35

Short description of the sub-task to model:
In order for a node to be created, the description tab of the node editor needs to be
completed by selecting a node description, which corresponds to a valid quantity in
the system to model. Each node is unique and cannot be created more than once. The
user can engage in the task only if at least one node still needs to be created for the
model to be complete.
Task tree and order of the tasks:
At the top level of the task tree “Creating a node”, the learner can either attempt to
create the node (task 1) or give up on the creation (task 2). The second task is represented in software by the user closing the node editor window, and can be done at any
time during the task. The task “Creating a node” is over when a good description has
been found and validated. The system can then try to initialize the selection and create
the node.
In the first level of the task “Attempting”, the learner first needs to select a node
description (task 1.1), i.e.: what quantity the node will represent. S/he is then allowed
to finish the creation of the node by validating the selection (task 1.2).
In order to select a node description, the user first needs to choose a node description (task 1.1.1) among the set of node descriptions offered by the system. This process involves the user choosing mentally one description (task 1.1.1.1), exploring the
help features offered by software (task 1.1.1.2) and exploring the set of node descriptions displayed (task 1.1.1.3). S/he can then select the node (task 1.1.2). This subtask
is not described in Figure 1 for a lack of space.
In order to validate the selection, the learner can choose to go back to the description of the problem to verify the correctness of his solution according to the problem
to be simulated (task 1.2.1), and then has to validate the selection (task 1.2.1.2). When
the user checks the validity of the selection, it can either be performed by checking
the solution against the set of nodes still remaining to be modeled (task 1.2.1.2.1) or
asking software to produce the solution (task 1.2.1.2.2). The user is allowed to ask for
the solution only when a description has been checked at least once.
Now that the different actions of the learner are defined, the iterations and conditions will help represent the flow of the activity on the subtask “Selecting a node description” (task 1.1).
Iterative and Optional tasks
• Task 1.1 is iterative: it is possible to make several selections before trying
to finish the description by validating.
• Task 1.1.1.2 is optional: The learner is not forced to explore the help features to choose a description, this is merely a choice on the learner’s part.
• The main task, “creating a node”, is iterative until the node is created or
the activity is abandoned. The later is represented in the task model by an
interruptible task: the learner can stop his/her creation of node activity any
time by choosing to close the node editor window.
Conditions on tasks:
• Main task 1 has a pre-condition attached to it: the software only allows the
user to engage in a creation of a new node if there is at least one node re-

36

lated to the modeling of the dynamic system that still remains to be created.
A first task model was created to represent learner’s activity on software without
the presence of the meta-tutor. This corresponds to the first version of software, which
was evaluated against the interface including the meta-tutor in [18]. This second software interface includes a text-based agent that intervenes as the students engage in
modeling to help them achieve deeper modeling behaviors, by applying constraints to
the user’s actions and giving meta-cognitive feedback. The meta-tutor was therefore
added to the task model under the type “system” and the model was completed to
include the constraints and interventions of the meta-tutor.
The final task model produced represented all possible actions of the learner on
software in order to model a dynamic system. Next, a study of these actions, which
led to the definition of the depth detectors, is detailed.

4

Detecting when students are modeling using shallow practices

The task model developed with K-MADe was used to define the episode structure.
The first step in creating a coding scheme is to define a unit of measurement for the
user’s modeling actions. The task model clearly highlighted the different subactivities the learner could engage in, referred to as goals. All goals are interruptible
tasks in favor to accessing the help features1 or abandoning the completion of the
current goal for a new one. After a brainstorming session where researchers studied
how students’ actions fell in line with those goals, the following unit of depth, called
“segment”, was defined. This established the unit of coding to be used in the next
phase.
Screen videos representing the learners’ use of the AMT software with and without
the meta-tutor were recorded during an experimental study described in [6]. These
videos were studied to determine how much shallow vs. deep modeling occurred and
the contexts, which tended to produce each type. A coding system was then created
for video recordings of the learners’ behavior. Three iterations of design for this coding scheme were performed, ending with a coding scheme that reached a multi-rater
pairwise kappa of .902. The final coding scheme mapped learners’ behavior to six
classifications, which were implemented as the following depth detectors[AIED short
paper]
• GOOD_METHOD: The students followed a deep method in their modeling. They used the help tools appropriately, including the one for planning
each part of the model.
• VERIFY_INFO: Before checking their step for correctness, students
looked back at the problem description, the information provided by the instruction slides, or the meta-tutor agent.
1

It is to be noted that two help systems are available to users: (1) referring back to the instructions always available for viewing, and (2) looking at the problem situation where all details
of the dynamic system to model are described.

37

• SINGLE_ANSWER: The student’s initial response for this step was correct, and the student did not change it.
• SEVERAL_ANSWERS: The student made more than one attempt at
completing the step. This includes guessing and gaming the system:
o The user guessed the answer, either by clicking on the correct answer by mistake or luck, or by entering a loop of click and guessing to find
the answer.
o The user “games the system” by using the immediate feedback
given to guess the answer: series of checks on wrong answers that help deduce the right answer.
• UNDO_GOOD_WORK: This action suggests a modeling misconception
on the students’ part. One example is when students try to run the model
when not all of the nodes are fully defined.
• GIVEUP: The student gave up on finding how to do a step and clicked on
the “give up” button.
Another detector was defined as a linear function of the six episode detectors. It
was intended to measure the overall depth of the students’ modeling, therefore providing an outcome measure in the transfer phase in future experimental studies. It considered two measures (GOOD_ANSWER, VERIFY_INFO) to indicate deep modeling, one measure (SINGLE_ANSWER) to be neutral, and three measures
(SEVERAL_ANSWERS, UNDO_GOOD_WORK, and GIVE_UP) to indicate shallow modeling.
Once the coding scheme reached a sufficient level of agreement between coders,
the task model was used to adapt the coding to students’ actions on the software. The
episodes that were coded for depth by human analysts in the sample video were analyzed by creating scenarios from the task model within K-MADe. The validation of
six detectors’ implementation involved three human coders, who watched a sample of
50 episodes, paying attention to the depth of modeling exhibited by the student’s actions, and chose the classification that best represented the depth of the learner modeling at the time of the detected value. A multi-rater and pairwise kappa was then performed, reaching a level of inter-reliance of .925.

5

The different uses of the Task Model

The task modeling language K-MAD and its task model creation and simulation
environment, K-MADe [7] were chosen for the following reasons: the environment
enables the creation and replay of scenarios of student’s actions, a set of functionalities not described here enable a formal verification of the model. Additionally the
associated simulation environment ProtoTask [14] allows non-specialists in task modeling to visualize the flow of the task model, via scenarios in a clear and simple manner.
The use of K-MAD helped in the creation of the detectors and are a first step in offering an alternative technique to simulated learners, by tackling the following problems:

38

Breaching the gap between learner scientists’ understanding of how the
learning process works and programmers’ definition of the application
flow, functionalities, and indicators.
• Enabling a formal validation of software flow, understandable by all.
• Using simulated learners scenarios to define the detectors.
A researcher in educational technology - expert in teaching modeling and part of the
AMT project - and an HCI practitioner, realized the task model. The former was an
expert on how AMT software was designed in terms of pedagogical content and task
flow. His expertise focused in particular on the actions the students were allowed/incited/forbidden to do within software at each moment of the modeling task.
The HCI practitioner was not familiar with intelligent tutoring systems or meta-tutors.
She was involved in the creation of the task model in a consulting capacity, in regards
to her expertise in task modeling of interactive systems.
The task model could be defined at the level of the user’s planning of actions and
system flow, with iterations and conditions alone. However, the objects in K-MADe
enable us to represent the constraints of the learner’s actions concretely and to apply a
formal verification of task flow. It was therefore possible to represent the set of descriptions as either valid or invalid, to detect when a node has been checked and the
result of that check, and to add constraints on the checking procedure such as to avoid
node duplication. This enabled a formal verification of software flow prior to validate
its fidelity to learner scientists’ ideas about possible actions on software and the underlying processes involved.
Once the model was constructed, the use of ProtoTask to visualize software flow
and follow learners’ possible sets of actions allowed by software enabled the ability to
simulate learners by creating scenarios of use that could be played and replayed at
will, focusing on the cognitive and meta-cognitive levels of learner’s experience on
software. In the process of creating our detectors, a video analysis of learner’s past
actions was performed. The model could be used to check the possible actions of
users with what the designer of the system wanted to offer as functionalities and software flow. During this analysis, the task model could be used once again to define
scenarios that simulated learner’s pertinent behaviors using ProtoTask. Once those
scenarios were formed, the task analyst came back to the original K-MAD modeling
language and studied the similarities and contrasts between scenarios to define the
rules that govern the detection of shallow and deep modeling practices within AMT.
Once the task model identified points of detection of such practices, it became easy
for programmers to go back to software and implement the rules.
•

6

Conclusion and Future Work

In this paper, a method to create a detector of deep modeling within a meta-tutor
using HCI task modeling and video coding schemes was described. The main outcome of this process was the creation of detectors inferring the depth of students’
modeling practices while they learn on a meta-tutoring system, reaching a multi-rater
and pairwise kappa score of .925. We believe the use of the task model to define shal-

39

low and deep modeling practices by helping to create the detectors to be of value for
any simulated learning environments, in particular for indicators that a common to all
learning tasks present in a tutoring system.
In interdisciplinary teams, the design of indicators can lead to communication issues due to misunderstandings and a lack of common ground between analysis made
at a high level of learners’ cognitive and meta-cognitive processes, and the representation of those behaviors within software. In particular, video-coding processes can
become costly when the coders’ understanding of the details of how the system works
differs from how the system actually works. Our experience using K-MADe and ProtoTask highlighted an ease in this project in gaining a better view of the tutoring system and the detection of deep modeling within the interface. In particular, the use of
ProtoTask by the non-specialists in task modeling helped clarify issues of task flow
and the definition of the set of user’s actions at each moment of interaction.
A limitation of the method is the applicability to different types of tutoring systems. In AMT, a single task model was able to represent the entirety of a users’ learning activity. In tutoring systems that teach a set of skills through different pedagogical
approaches for diverse types of learning tasks, the creation of such task models might
prove more costly and may not be completely adapted to the creation of detectors that
need to be adapted to each task specifically.

Acknowledgements
This material is based upon work supported by the National Science Foundation
under Grant No. 0910221. We would like to thank Sybille Caffiau for consulting in
the project and sharing her expertise in task modeling of interactive systems.

References
1. Aleven, V., McLaren, B.M., Roll, I., Koedinger, K.R.(2006): Toward meta-cognitive tutoring: A model of help seeking with a Cognitive Tutor. International Journal of Artificial Intelligence and Education 16, 101–128
2. Arroyo, I., and Woolf, B.P., 2005. Inferring learning and attitudes from a Bayesian Network of log file data. In Proceedings of the 2005 conference on Artificial Intelligence in
Education: Supporting Learning through Intelligent and Socially Informed Technology,
Chee-Kit Looi, Gord McCalla, Bert Bredeweg, and Joost Breuker (Eds.). IOS Press, Amsterdam, The Netherlands, The Netherlands, 33-40.
3. Baker, R. S. J. d., Corbett, A. T., Koedinger, K. R., Evenson, S., Roll, I., Wagner, A. Z., …
Beck, J. E. (2006). Adapting to when students game an intelligent tutoring system, Proceedings of the 8th international conference on Intelligent Tutoring Systems, Jhongli, Taiwan Berlin, Heidelberg.
4. Baker, R.S.J.d., Goldstein, A.B., Heffernan, N.T.: Detecting the Moment of Learning. In:
Aleven, V., Kay, J., Mostow, J. (eds.) ITS 2010. LNCS, vol. 6094, pp. 25–34. Springer,
Heidelberg (2010)
5. Baker, R. S. J. D., Gowda, S. M., & Corbett, A. T. (2011). Towards predicting future
transfer of learning, Proceedings of the 15th international conference on Artificial intelli-

40

6.

7.

8.

9.

10.

11.

12.

13.
14.

15.

16.

17.

18.

gence in education. Proceedings from AIED’11, Auckland, New Zealand Berlin, Heidelberg.
Baker, R. S. J. D., Gowda, S. M., Corbett, A. T., & Ocumpaugh, J. (2012). Towards automatically detecting whether student learning is shallow., Proceedings of the 11th international conference on Intelligent Tutoring Systems, Chania, Crete, Greece Berlin, Heidelberg.
Caffiau, S., Scapin, D., Girard, P., Baron, M., & Jambon, F. (2010). Increasing the expressive power of task analysis: Systematic comparison and empirical assessment of toolsupported
task
models.
Interacting
with
Computers,
22(6),
569–593.
doi:10.1016/j.intcom.2010.06.003
Corbett, A.T., MacLaren, B., Kauffman, L., Wagner, A., Jones, E.A.: Cognitive Tutor for
Genetics Problem Solving: Learning Gains and Student Modeling. Journal of Educational
Computing Research 42(2), 219–239 (2010)
D’Mello, S. K., Lehman, B., & Person, N. (2010). Monitoring affect states during effortful
problem solving activities. International Journal of Artificial Intelligence in Education,
20(4), 361–389., doi:10.3233/JAI-2010-012
Girard, S., Chavez-Echeagary, H., Gonzalez-Sanchez, J., Hildalgo-Pontet, Y., Zhang, L.,
Burleson, W., and VanLehn, K., (2013), Defining the behavior of an affective learning
companion in the affective meta-tutor project, in K. Yacef et al. (Eds.): Proceedings of the
16th international conference on Artificial Intelligence in EDucation (AIED’13), LNAI
7926, pp. 21--30. Springer-Verlag, Berlin, Heidelberg.
Gowda, S.M., Pardos, Z.A., and Baker, R. S. J. D. 2012. Content learning analysis using
the moment-by-moment learning detector. In Proceedings of the 11th international conference on Intelligent Tutoring Systems (ITS'12), Stefano A. Cerri, William J. Clancey, Giorgos Papadourakis, and Kitty Panourgia (Eds.). Springer-Verlag, Berlin, Heidelberg, 434443. DOI=10.1007/978-3-642-30950-2_56
Koedinger, K.R., Corbett, A.T., Perfetti, C. (2010): The Knowledge-Learning-Instruction
(KLI) Framework: Toward Bridging the Science-Practice Chasm to Enhance Robust Student Learning. Carnegie Mellon University Technical Report, June, 2010
Martin, J., VanLehn, K.: Student assessment using Bayesian nets. International Journal of
Human-Computer Studies 42, 575–591 (1995)
Lachaume, T., Girard, P., Guittet, L., & Fousse, A. (2012). ProtoTask, new task model
simulator. In M. Winckler, P. Forbrig, & R. Bernhaupt (Eds.), Human-Centered Software
Engineering (Vol. 7623, pp. 323– 330). Berlin, Heidelberg: Springer Berlin Heidelberg.
doi:10.1007/978-3-642-34347-6
Muldner, K., Burleson, W., Van, D. S., Brett, & Vanlehn, K. (2011). An analysis of students’ gaming behaviors in an intelligent tutoring system: predictors and impacts. User
Modeling and User-Adapted Interaction, April 2011, 21(1-2), 99–135m
doi:10.1007/s11257-010-9086-0
Shih, B., Koedinger, K.R., Scheines, R.: A response time model for bottom-out hints as
worked examples. In: Proceedings of the 1st International Conference on Educational Data
Mining, pp. 117–126 (2008)
Walonoski, J. A., & Heffernan, N. T. (2006). Prevention of off-task gaming behavior in intelligent tutoring systems Proceedings of the 8th international conference on Intelligent
Tutoring Systems. Jhongli, Taiwan Berlin, Heidelberg.
Zhang, L., Burleson, W., Chavez-Echeagary, H., Girard, S., Gonzalez-Sanchez, J., Hildalgo-Pontet, Y., and VanLehn, K., (2013), Evaluation of a meta-tutor for constructing
models of dynamic systems, in K. Yacef et al. (Eds.): AIED’13, LNAI 7926, pp. 666--669.
Springer-Verlag, Berlin, Heidelberg.

41

w  	
	
			
 	

	
¹

¹
  ¹ 
¹
 ¹  
!
"
#
$¹%
&' ¹  ¹¹
&¹(')¹'$*+,-
¹¹
¹

 	
 	


 	
	

È   ".#$'
'"./'¹'''$.
0
¹/' 

$'$
'/'$'#$'¹¹$

'&

&

$'0'$.
$
  ¹ ¹
$  ¹$¹ ".¹$ '' ' '$''  1$.  
  
¹
'
1..$.¹$
'$.¹$¹'$ 
$ ¹'.¹ $$
$2
1
#" ' 
$¹'¹¹$¹'$' ' $$.¹$¹
¹$
¹
 1.¹&$3
1¹$
¹$&'4
$.'¹$
¹$&''¹

1¹/ '¹ 0¹'/$' '#"".' '¹
.¹1¹/ $1
 .$' ¹
  /$'  ' ¹
  0 '' ¹$¹
$.¹
/$' '

¹$&$.¹
 .$'
'¹
¹$
 $.' 
1  ' ' ¹ &/ $ '' 
 ¹ ¹ 
&

$ '

'$ 
$ 5
1  & ' 
$ '&¹ 6 '¹$
 
&

$  
. '$ &/ 
1  '  #" ' ' 1$.¹'¹$1 ¹
'$ /

$'
¹'¹
 '&.1 .¹&'
$.

	)
&

&

$'
$ 
$ 
) ¹
2
1 '$¹$#$'
'"./






$ 
$ 

'&

&

$'¹
$¹
$''
$.6#7
  
'$¹
$¹'   
 8 9 :; ¹
  
$& "$' 8 "9 :; ¹ $.
$'$¹
 
¹¹.'
$.¹$¹$$  '¹¹'$
'$¹
$'¹''¹$ 
$ 
'
$. ¹
$.¹$    $. &¹$  
$$.' 
'$¹
$'¹
¹$ $ ¹¹$&
'$. ¹
 "
  ¹'$ 
$5
1
  $/ '$ 
$
$¹$
'$..¹$.
0¹ 2
1 "¹

 :; 1.. ' ¹'  
 ¹/'¹
  ' ¹
  '$¹$' $. ¹$/ $.¹$ ¹
'$ 
$.¹'¹
 ¹$¹
$. ¹
&
.'<.¹$
'
) ¹ 5
1     ¹' 
  / $. $.
0' '. ¹' $.#$
'
'"./8#"9:*;1..'
$.'$$¹
$'$¹$' ¹¹
$&5
1 ¹''''
$
$'$
'/'$'4'¹¹/
#"$
'&

&

$'$'¹

$
$1
'&

&

$¹
 

42

$'$
$.¹$'1¹5¹¹$.
$1
$.
$''&
¹
 
$.
$'$'$
1¹
 $/¹/#"$
 ¹5
1 #

$.''
'1 $.'$
¹.¹'¹ $ ¹.1.
 '¹
'$¹$'  $. '$
 ¹$.  $.  ¹
   ' ¹ $¹
'$
' $1
 '$¹$'
%'
$.¹$'
$¹$
¹.
  
 '$ ¹'¹0'$
¹
 ¹. 
¹'¹
$

$.0'$

4.¹
'¹'$ &¹
¹$¹$8'¹$¹$9 


$.'&
¹$.$.''$ 
$'=¹
1.'&
$
".'

''1 ¹ '¹'1$
$.#"

$'1..1
'  ¹
'
$. ¹5
1 >$.1¹'1¹
$$.'
 $  /
¹ $.¹$ ' $. '$
 ¹$. ¹
  $. 

  #" 

$'
.¹& $   ¹$  /
¹¹/ 1.
 
1 ' 1  ¹&¹¹ 6 
/ $.
¹.1
 ¹'''$ 
$¹$
''1.
¹'$ 
$$'
¹
 ¹$
 $.¹$ 
& 1¹' $  / ¹
$. '$ 
$ $ .¹' $  
   
 $.
¹.".'
1¹$
'.¹&$$¹5

$¹
$1.
$. ¹
5
1  ' ¹''''  $.¹$ ' $. ¹¹$
    $. #" .¹' $  

1.

1¹$
'¹
¹$ $$.¹.
".¹¹
/$1¹'#"¹'  '¹ 
$.1$./ ¹$
$.'$¹$ '$ 
$5
1 
$'$.'$ 
$=''
'? .$'¹
 
/$' '7.$' '
' ¹.'
'¹'$.$

$ 1.¹' /$'  ' 
'  ¹. '
' 
 & ¹/ ".'
$¹ $
¹#" ' 
$¹' ¹'$'3$  ¹
1$.$.'$/
¹¹$
''
 .$' '¹
$¹'
¹$&¹'1
 
$.'5
 
'¹
 $.1¹$
'1$.$$& 
8¹/¹$
'1 /
'$ 
$'9$/$'¹¹$
'¹/
 
  ¹
'1.$.
'$''¹$
''&/¹
#
 $3
1#" '$.¹$$$$1$..¹
'1.¹& 
& ¹'¹$

&

$
1..&$¹'$ 
$'81$.¹5
1
¹
¹ 5
1   $. ¹
9 '& &$¹ ' 8'¹$
 $. .¹&
¹ 
$$.5
1 9¹
 1.¹&¹ $.'$¹$ 5
1 
1$. $. ¹ 5
1  #
 $.' '
' 1 .¹& &  ¹ 
1 ¹/  #"
¹'  '¹ ß 	 
	1..¹.¹1¹/ .$'¹
 
/$' '
' 

$¹'''
''$¹''$$.



	
	
			

".#"'
$.'$'''¹
 1
  '$¹$'5
1 



$'$
'/'$':*;#"
'¹
  ''$ 
$¹
/¹
'
 ' ¹'$ 
$
' ¹  .¹¹$'$ &' $. ¹ '$.¹$ '$ 
$='
'$' 3¹
 /¹'$

'&¹¹$'8
'$¹
$.5
1
 &9
".¹¹$#" '¹' 
.1$. ' ¹$$.'$¹$ '$

$5
1 
$'.'<.'
'$./ :@;?

43

	 7.$'  '? ¹. '
' ' 
'   ¹' $. $  
$
A.
¹'$ 
$'$'¹
$

¹0'$
$'$.'<.'$¹$ 5
1 '
 ¹$ ¹ 
$1.$.$.$
'$ '$.$
$.¹$'$.
 ".'  ' 0 
/ ¹ .¹¹$'$ &  $ $.¹$ '
$' $.
¹$/$.¹$¹'$ 
$1$.¹$¹
5
1 &¹
'1'$$/".'
.¹¹$'$&'¹ $.¹¹$'$&8# 9
	 )/$' '?¹.'''
'.¹'$'1
.¹¹$'$&¹ 
¹$
 .¹¹$'$ & 84 9 :+; 1.. 3''' $. ¹$/ $.¹$ ¹
'$ 
$ '$ $.¹$ ¹
'1 :B; ". '$ 
$ '$¹$  5
1  '  ¹$  /
¹
'$.4 ¹$ $$.'$ '
'



8 .¹¹$'$&'¹
$
  .$'¹
 /$' '


)/$'  ' ¹  
¹$& $.¹
 .$' 
' '
 $./ $¹5

$¹
$¹.'''
'
 
 
$/
'$¹ 
' 
¹.¹
'1
¹'$
$
>  '.1' $. &'  ¹
 $  $. $. .$' ¹
  /$'
 '#"". .$' ''.1

>8¹9¹
 .¹'
/$.# 
1.. ' $. ¹$/  ¹
'1
 $/ $.' $ 8/¹3'9 &
 ¹ $¹

5
1  & 83¹3'9 ". /$'   ' '
$  
 > 89¹.
$..¹'$'1
.¹¹$'$&1..'$.¹$/.'
$.'
.8/¹3'9&
¹$¹
5
1 &83¹3'9
".'$¹'¹' 
$. .$'.¹¹$'$&'¹
$. '$  ' 1.. ' '$ 
$
' ".'  '    ¹'' 

' 
$.
¹¹$'$.¹$$.
$
.¹'6 
$$.'¹''
¹$
 $. ¹ * 5
 '  '$  '? )( )( ¹
  *)( 1$. 
 $1 ¹
 
$.¹¹$''$&/6
*)(# ¹
$

' 
 ¹'1'?


     	 
   




  

89

44

A.'¹¹¹$
$  $$$.&'¹$$.
¹&¹¹
$ '$. '
¹$
¹¹$'$. $/¹¹$
'$.''

¹¹$'$.$
¹
 
'$.5
1 &
".¹¹$/$' '#"#
151.¹&
'  $.
#"  '  / ".''
¹
  
$
 $. $':-; #
 $.'
  ' ' $. '&¹ ¹$' 8'$¹ .'9 $. ' ¹
$. 


'&¹¹
 ¹$
$¹$/¹ C
D8729$.¹$3'''$.¹
$/$.¹$¹'$ 
$ '
$5
1.1$¹
'1$.$¹.'&¹¹$/.¹'
¹$
$.¹$/72
  '
'$ 
$'1. 
$5
11'$¹

'&¹ ¹$/ ". ¹  ¹. '&¹ ¹$/ ' 3'  1 


'
$'$.'
'$$.$¹
 '$.¹$/'$ ?


    

  

    

	 



  


    

89



¹$/  ' $.

'&¹ ¹$/ 72 ¹
  ' $. $
  $.¹$ ¹$/

   
 ¹. '&¹ ¹$/  ". ¹¹$' 
$  /  $' $.
 ¹' 1¹' '
¹$
 $. ¹$'¹
  $.¹¹$' 
$  / 
$$.¹$&0
/$.'$
¹.¹$
¹$&

ß
 	

". ¹¹$
  #" $  '&
 
&

$' 0' ¹ /$'
  '
 ¹. '$ 
$¹$
 '.   $¹5
 
$¹
$E ¹ .$'  

/ 1   ¹ $ '$¹'.  ¹
 ¹$
 ' $  
$ 1& '
¹$
' .¹&$$& 
¹
 #"¹¹$
 
$¹'¹¹$¹$$'
3$ >$.¹$¹'
1.¹& & ¹
1¹/ '#"¹ 
0¹'/$' '#"1..¹
$..¹1¹/$1
 .$'¹
 
/$'
'
F¹'/$'  ' 
'  
$ ¹ .' ¹' 
 
 
$ $ 
/ $.'
$.¹$ .¹& 
. & 
 > 
'$¹
 $ ' 
'  ¹
 $ 1$.  .'
81.¹$''¹
'&

&

$1
 ¹'$ 
$¹$
'9G
$..¹&
'$ 
/ /'$ 
$'$./ 
$'
.&

$ ¹/$'¹¹$
#
'$¹  
$1
' $.'G.'
¹'¹'. 
$.
4 '8
.9$*8

 & ¹.'¹
 ¹
3$¹.$.¹$$.$.G9$.¹$.¹&¹$.
& 

.$ ¹
#"¹¹$

#
 $'$
 '/'$' $.    $' 1$. ¹ $  .' $ $ ' 
' 

1..$.
.'''
1.¹&$$¹5
$¹
$$.
$¹$
$1
$.'+
$'


45


86
$1$..'

ß


 	
	

#
 $&/0¹'/$' '1.¹& & ¹'¹$
1..
&$¹ '$ 
$'.¹& ¹¹ 89 5
1  ¹''
  ¹
  $./ .¹& $ '& '
&$¹'¹ 
$$.5
1 &4
$.'$ 
$''&$.
' '$.5
1 ''$¹$ /¹
'¹0¹'/$' 
#"¹
 $.
$.''$¹$
'¹¹ 1$.$.'$ 
$'=¹5
1 
ß

w 
 	


#
$.''¹$

&

$¹&$¹''
$ ¹'¹$

$'E¹.$'¹'$¹$'&
¹$.1$.¹$¹

''
$¹
'$
'$$.'$¹$'".'$¹
'$
'¹$..'$.$
A. '$ 
$' ¹ '&
 ' 
1 '$ 
$'= ¹$
'   ¹¹ 
 $.
'/'$ ¹
  $./ 
  $  
  
 $.   > $.¹$¹'
 &$¹ '
¹
$'$¹$
$$'$$./¹
.¹
 
'$ 
$'=
$¹$
'
6$ $. 


 ¹.  .¹' 
/ $.  ¹ '$
 ¹$. $.¹$ ' $.'

 ' $.¹$ ¹ ¹$  $.  ¹ '$
 ".' 
 ' ¹    / .$'
$'¹ 
$$.0¹$
6.¹¹$'$&$.# '
.$ 
$.' 5
   $' 
$ 
$' 1.  
$ ¹
'1 $/ $. $   1$. '
¹$/ ¹5 ¹ 
1 ¹$
 ¹$ $.' '$   '&
 ".
 $. '$
&8# 9'¹
. 
$$1&'.¹

$.
¹ .$'$
$ ¹ /$' 
 6 
 $ $.  3¹
¹$
 '$ 
$'= ¹$
'  
&5$.¹ $

1
 '$$.¹.



8ß6 $

1&'$¹
$

46

>*'.1'.1
1&'¹¹  $¹
$>'$/1.¹&$.'$
#  & 1.. 3''' $. ¹$/ $.¹$ ¹ '$ 
$ &
 .'<. 5
1 
&1¹
'1$.$
$/".''&¹5 ¹'
$. &

'¹
. 
$&'
¹
 1.
$.¹$
'
$ /&'¹  $
$. E
¹/&'¹
. ¹¹
¹
 $.
&'
¹
 ¹¹  $$.
$ ". $ 1¹' ¹$ $. 


 ¹ .$' $ 1$. $1 '' '

''E$.
¹
1¹$
1¹'
  
$¹
 ¹'¹
'0
$.1
&
1¹' 
&$ 
 $.$1 &' 6¹
 ¹
1¹$
 1¹' ¹  $$. ¹
 
$.$.¹
'$¹@.'$
ß

w 	

6 &$¹ '$ 
$ ' ¹
 
$$/ $.¹$ .¹' ¹ ¹ 89 5
1  ¹''¹$  ¹
  '
¹$'&'¹ 
$$". ¹'$.¹$ 
 

$'5
1 
&$.'$ 
$ 
$..$.¹./$
¹
¹$

$.>$.$./ ¹'.
1¹$
'1$.¹$¹
¹$/
6'$ 
$1'$¹
¹$
$.'$¹$¹ 
$.'<.5
1 &¹
 
$. .¹¹$'$ & '
 $.' &' ¹ ¹'$ 
$
'  $. 5
1
 &>@'.1'¹
3¹¹&$¹'$ 
$'$
¹.
¹@
.$".¹$'$ 
$1'$¹..1$.¹¹$/*GG
¹
 @'$&/A.
¹

$¹$
''$ $.1¹¹
$/ $.¹$ $. '$ 
$ 1 ¹'. ¹ 
1 ¹$
 ¹
  $. $.' & 1 
¹
. 
$$.$1


 ¹ 5
1   &$¹ '$ 
$' ' ¹''
   '¹$
 1 ¹

.' 1.¹$ ¹$
 '$$
 1 1¹
$ 
 ¹. 3
$ #$ ' ¹
 $¹
$
¹$$.'¹$
'
1¹
'$ /$.¹$ 
$5
 ¹$


 $&¹ ¹$
1 '
¹¹''
/
'$.


8$¹'$ 
$'$
¹.

47




	
	 

#
 $&/ 
¹'¹$ 
&

$1.¹&
 $ '
3
$' 1$. &$¹ '$ 
$' ¹
  &$¹ ' ". 3
$' .¹& 

¹'.  $ &/ $.   1$. 
$ ¹$
 '$$
' ¹' 1 ¹'

$ '#"¹
  
$
'



	
		


".3
$'¹'. 1$.'¹$1¹$¹''
 /0¹'/$' '#"
 
$'$¹$
'¹.3
$.¹'

 ¹'.  * $' 
   $   $. ¹$  ¹
¹' ¹$¹ ".

&$¹'$ 
$'
¹.3
$1¹'
#
 $$
 
'$'1.¹&
$ 
$.¹¹$
#".¹'

'$¹ $.¹$1.¹&'$¹$ 5
1 &1$.$.5
1
&'¹. 
)/$' ' 
$¹'¹¹$¹'3$ 
 
$.¹¹$

'$¹'
'$.' .¹&
$
.& 
$¹¹$ 
/ ". '$ 
$ '$¹$  5
1  & ' ¹¹$  '
 $. ¹  $.
0¹$
 * $. ¹$/  .¹&
 ¹ ' 5
1  & &
 $. '$' 
1 /$.'$ 
$'&
$.'¹¹$ $/
$.¹$/
'$
¹.'$&
$.5
1 &


   !

"! # ! $ 

 %$& 





8*9


#
3
$'&$¹'$ 
$'.¹&'& $1'1$.$'A
.¹&¹'. ¹
/$.3
$'&¹/
$.¹$
 '$$
$.
¹$/
¹$

1¹$
'¹
 $.$ $/'$&/#
3

$' 1 .¹& ¹  ¹¹/  5
1  & '$¹$
 $¹
  / ¹
0¹'/$' 1$.$.'$¹
 //$'¹
  .$' '
1$.$.'¹ ¹$¹
".¹¹/5
1 &'$¹$
1¹'¹¹$ '
$.
3$¹
1. 
 '$.¹ 5
1   '$. '$¹$  5
1  ¹
   $.
 
'$ 
$'?


 ! ' (  '  '

( +
,
- )*) 

.



8@9

A .¹& .'
  $/'  ¹$
 '$$
 $ ¹'. $. 3

$>'$¹
¹ '$$
¹$
1¹''$ E$.¹$/.¹&

¹5
1 & 1¹'
$

$. 5
1 ¹
'
$.¹$¹

1¹':**;E$. '$$
1¹'
$ 
".'
 ¹$
1¹'¹



1..$.¹$/.¹&
¹5
1 &'0¹/ '$$ ".
$.$1¹$
'1¹1&¹
 ¹..&¹$
1..1

¹ '$$
' 
$  
 '¹ ¹
 .. &¹''$&/ > + '.1' $.

48

5
1 & '$$
'' 
3
$'>$$.$$./¹$.

¹$.
$.1&¹
 $...& '$$
'$&/



8
$ 
$'=5
1 & '$$
'' 
$.3
$'

".¹$$.
¹$

1¹$
'1¹'¹''$  
3
$'
"$.'
 1.¹&
 $ 3
$'&¹/
$.
$¹¹ 
¹
1
¹$
 &
 $ &¹'  H +H H¹
  +H  $.¹
+H  ¹ 

1
¹$
' ¹ $¹&/¹
&'
$. 
>
¹/1.¹&¹'
'  $. $/$.$'A.¹&
 $ ¹

3
$ .¹

 $. &¹  $.' ¹¹$ ". $/ &¹ ' ¹¹$ 
¹ 
$¹
¹ '$$

$ 
¹$¹
&¹1..'$. $/
¹&¹A.¹& 
3
$'1$.&¹'¹
 '$&/



	
		

3
$'
 $ ''$$.¹$/$' ' #"¹
¹$/$.¹
 .$'
'".''$'
$''
'
/$' 
'¹
¹$&3
$'¹''.1$.¹$0¹'/$' '#"
¹ 
$ ¹' ¹¹$ ¹' /$' 
' $  ¹¹$ $.¹
 .$' 
'
' ' '$' $¹
  / 0¹'/$'  ' ¹ &/ '¹ $ $.' 
$¹
 //$'
'
"¹  '.1' '$' $¹
  1$. 
$ ¹$
 '$$
#
¹ ¹''
¹¹/ $¹
  / /$'  ' ¹ $$ $.¹
 $¹
  / .$'
¹
 0¹'/$' '".' 
'¹..
$.
¹ '$$

¹
 11$.¹1&¹$



	6¹/#" '.¹

$.¹$
 '$$


 


! 





"
 
@+*G
-,+-B*,
B-@*+
#$

*G@@**
BG@@+
+*,--+-
%&		
,,G
@GB-@
-*+G*
'&		
*-G@-*G
GB-BG
GG+-B

A¹
'
$.$¹$.¹$ 
'$1
'$'$¹
 /0¹'
/$' '¹
 /$'
'¹
$'
¹
$
4'
 3
$
 $ '$  .1¹$'$.¹$/¹ 
¹

1¹$
"$.'
 1.¹
 $.
$¹¹ 
¹
1¹$
H

49

$ +H "¹  '.1' $.' '$' 1.
 1 
¹' $. ¹$/  ¹ 
 ¹

1 ¹$
 $. ¹¹/  .$' ¹
  0¹'/$'  ' $' 1'
'
 $ $.¹¹/  $. /$'  ' $' $$ '
 $./ $¹
 
'
¹$



	6¹/#" '.¹

$.
$¹¹ 
¹
1¹$


(	 


! 





)(
*,*G
BB,GG,
BBB@B
(
*@G,,+B
BB,++-B
+G@+,-
)(
*B*@B@
B--GG
++,****
(
*B-@+GG
-B,G
+*-,@B

>
¹/1.¹&¹ '$'$¹
 /$.' '&¹/
$8'&

¹$.'$9 $/¹&¹".'$ 
$5
1 & ' 
3
$'

$.¹
 :* *; A .¹& 
'  ¹&¹ $ $/   ¹
   6

$1$. $/ 1¹
'1 $//¹'$ 
$1$.5
1 &
1$.¹¹$/+"¹*'.1'$.'$'$.'3
$$. .
$' '.¹&¹$$.¹&1.
$. $/'1.1&0¹'
/$' ¹
  /$'  ' $ $$ '$' 1.
 $. $/ 
¹''
)/$' ¹
  0¹'/$'  ' ¹¹$ $$ 1.
$. $ $/ '
.. '
 $¹1' $.   $ .¹&  &' $ '$¹$ $. '$ 
$ 5
1
 &


	ß6¹/#" '.¹

$.$ $/¹&¹

$$
	 ¶&
	 ¶)
	 ¶







*B@*
**G
++B-*+

! 


,
+@G-
,G




++G@@G
@@++G,
,,*



#
 $.' ¹ 1 .¹& &¹ ¹$  ¹ 
1 ¹¹. $.¹$ '' $. #$ '
'
"./¹1
  $./
¹/'  ¹¹$&5
1 '$¹$


$'$
 '/'$' $ 
  ¹ '5' 
  '&
 
&

$' " 
$.¹$1.¹& & ¹
1 #"$.0¹'/$' ".' 
' .¹1¹/ $1
 .$' ¹
  /$'  ' 
  
¹$&
$.¹
 .$' '¹
 
 
''¹
$ ¹$¹$.¹
/$'
'
".'&¹$
  $/¹'. 
¹¹
&

$'
1

 $5
1$.5
1 &'$ 
$'$¹'$.'$¹$
¹¹
/".'5
1 &.1&'¹¹$
$$¹$$.¹$'
$'&¹#
¹ $


50

1
  ¹
$ 
&

$1.$.'$ 
$'=¹
1¹'
$¹' 
/ 3$
¹ ¹$' > ¹ $.' ¹'
' 1 .¹& &  ¹ '¹$
 
&


$
%'
¹'¹$

&

$1¹
.'$.
¹$$.¹$


 $ '$ / .1 1 $.   ' 
 
$ '$¹$
' ¹
  1$. 
$
'$ 
$'='¹'A¹'¹
  $. $/$.'$'$.8
$'9?  ¹
/ ¹¹$
 $. '¹$ ' ¹ $   1.¹$ $' ¹ 
$¹
 1.¹$¹¹'
4$.¹ &¹
$¹'
¹'¹$

&

$'$.¹$1¹
¹$¹.3

$ 
   $   $. ¹$  ¹
¹' ¹$¹ >$. 1 ¹

'
$$.'$¹¹
'$ 
$'1..  $
¹¹

&

$
¹ 
$.0¹'/$' #"3
$''.1$.¹$$'¹
¹$
''
'&

&

$8' '¹
/5
  ¹$¹'5
¹
  
 ¹¹$& ¹
'  

 ¹¹$& 5
1 9 '¹/  1
15 1$.  
  ¹
' 
 1.. $. ¹
$  '' 
1 ¹$
' ' &/
¹F¹'/$' '#"'¹'$¹$
'¹'/$' 
'$
 '' ¹$¹' '0¹'/$' '#"¹
¹$&
$.¹
 .$'
''
$./$ ¹$¹$¹
 
$'
''

È*		
	?".'15'¹$776(4!$1..'
¹
 /
$. 6
 ¹'¹
 
¹ 
'$/  

 #

&¹$
 ¹
  
$' 8),"# 
++9

+

	$			

	 $&6¹$

¹1¹)?#
$
$$$'¹?".
'$¹
$¹' ¹
¹.##
$
$
/'$'8-9*G@+
	 6
 '
 I $$ 6" 2 
 2 )$ ? 
$& "$'? (''
'
¹
 ".!
¹$.(¹



'@8,,+9B--
*	 $'

'
)?#$'
'$./'/.'$'(¹1
¹
¹.1¹.89
@	 
 
! ¹ I()?6 ¹$&$'$
.¹.¹'$ 
$ 
'%' %'6 ¹$#
$¹$-8-9,+-
+	 7 76/¹¹I2.A? $ ¹ ¹$&$'$
1$./$'
$'6 )'/.¹¹'
$,8,,+9+
B	 
 
!?6 '$ 
$5
1  ¹
''$..¹ ¹$&$'$

 #
? #
 ) 
'  -$. #
$
¹$
¹ 

 #
$
$ "$
 
/'$'
#"
@¹

¹8@9
-	 ".''
7
$
(?6'
' $.$')'/.$5¹@,
8,G@9++,


51

Toward a reflective SimStudent: Using
experience to avoid generalization errors
Christopher J. MacLellan, Noboru Matsuda, and Kenneth R. Koedinger
Human-Computer Interaction Institute
Carnegie Mellon University
Pittsburgh PA 15213, USA
cmaclell@cs.cmu.edu, mazda@cs.cmu.edu, and koedinger@cmu.edu

Abstract. Simulated learner systems are used for many purposes ranging from computational models of learning to teachable agents. To support these varying applications, some simulated learner systems have
relied heavily on machine learning to achieve the necessary generality.
However, these efforts have resulted in simulated learners that sometimes
make generalization errors that the humans they model never make. In
this paper, we discuss an approach to reducing these kinds of generalization errors by having the simulated learner system reflect before acting.
During these reflections, the system uses background knowledge to recognize implausible actions as incorrect without having to receive external
feedback. The result of this metacognitive approach is a system that
avoids implausible errors and requires less instruction. We discuss this
approach in the context of SimStudent, a computational model of human
learning that acquires a production rule model from demonstrations.
Keywords: simulated learners, metacognition, cognitive modeling, representation learning, grammar induction, generalization error

1

Introduction

Simulated learning systems can be used for a wide range of tasks, such as modeling how humans learn, as teachable agents, and as a means to automate the
construction of models that can be used in cognitive tutors. In an effort to reduce the amount of developer effort needed to deploy simulated learners for these
tasks, researchers have been relying increasingly on the use of machine learning
algorithms. However, by increasing the generality of these systems through machine learning approaches, these systems become more susceptible to making
unrealistic generalization errors.
When using simulated learners to model human learning, we desire systems
that predict student’s errors as well as their correct behavior. Unrealistic generalization errors, in the context of these systems, are errors that the system predicts
humans will make, but that they never actually make. If a system is prone to
making these kinds of errors, then it becomes difficult to draw conclusions from
the predictions the simulated learners makes for novel tasks.

52

These generalization errors also complicate the use of simulated learners as
teachable agents because they result in a system that produces non-human behavior. When human students are teaching a simulated learner in a peer-tutoring
scenario and it makes errors that humans never make, then it decreases the authenticity of the experience. This inauthenticity might effect the social dynamics
of the learning-by-teaching scenario possibly making the teachable agent less effective.
Finally, generalization errors also have negative effects when using simulated
learners to automatically build cognitive tutors. For this purpose, simulated
learners have been used to author production rule models via interactive demonstrations of the solutions to the problems the system will tutor. This approach
may decrease the amount of work required to build a cognitive tutor and allow
subject-matter experts to author tutors directly, without an AI developer. In
this paradigm, SimStudent’s errors are useful to the extent that they correspond
with typical student errors; in these cases, the resulting production rules can
be added to the tutor’s bug library. However, if the errors are unrealistic, the
author must waste time identifying and deleting these nonsensical production
rules.
In this paper, we propose an approach that uses background knowledge to
mitigate unrealistic generalization errors with no changes to the underlying algorithms and which should increase the effectiveness of the underlying learning
mechanisms. Before presenting this approach in section 4, we first review SimStudent, the simulated learning system that provides the context for this work
(section 2) and introduce a motivating example of a nonsensical generalization
error SimStudent currently makes (section 3). After presenting this approach,
we present some initial results and discuss conclusions and future work.

2

The SimStudent Architecture

The simulated learner system that we focus on in this paper is SimStudent, a
system that induces production rule models from demonstration and problem
solving. The SimStudent system is used primarily for three tasks: to model and
predict human learning, to author cognitive tutors, and to function as a teachable
peer-agent.
In order to understand how SimStudent works and the situations in which
it makes unrealistic generalization errors, we will review the types of knowledge used by SimStudent, how this knowledge is represented, and the learning
mechanisms SimStudent uses to acquire this knowledge from experience.
2.1

Knowledge and Representation

There are three kinds of knowledge in SimStudent: primitive operator function
knowledge, conceptual knowledge, and procedural knowledge. The first kind of
knowledge is hand-constructed and consists of the low-level functions for manipulating data available to the system (i.e., adding two values, appending two

53

strings together, etc.). One example of a low-level function is SkillAdd, which
accepts two arguments, each of type arithmetic expression, and returns the sum
of these two expressions as a single arithmetic expression. These functions constitute SimStudent’s background knowledge. Depending on the task SimStudent
is being used for, different kinds of background knowledge may be appropriate.

Head
Expression
Expression
Variable
Minus
Number
Number

←
←
←
←
←
...
←

Body
Number Variable
Minus Variable
x
0

Prob
0.95
0.05
1.0
1.0
0.1

9

0.1

Fig. 1. A simple probabilistic context-free grammar and example parses of two expressions using this grammar.

The second kind of knowledge is conceptual, or representational, knowledge,
which is encoded as a probabilistic context-free grammar. It is automatically
acquired by SimStudent and is used to interpret the interface and information
in it. Figure 1 shows a simple example of the conceptual knowledge SimStudent
might possess about expressions for an algebra domain. This knowledge enables
SimStudent to automatically extract plausible “chunks” from the input, such as
the coefficient or term in an equation, which can subsequently be manipulated
by primitive operator functions or procedural rules. Furthermore, this knowledge
can be used to determine the likelihood that a given example was produced by
the grammar.

If (current-row ’output-cell ’row)
then (write-text ’output-cell
(cell-in-row ’row 1 ’left-side)
→ (append “divide” ’coefficient)).
(is-left-child-of ’left-side ’coefficient)
Fig. 2. An example production rule for division.

The final kind of knowledge is procedural knowledge, which represents the
skills that we desire students to learn. This knowledge is encoded as production
rules, which contain conditions under which the rules apply and what to do
under those conditions. Figure 2 shows an example of a production rule signifying
that when the left side of the equation’s parse tree has a left child (here called
coefficient), then enter “divide <the coefficient>” into the output cell.

54

2.2

Learning Mechanisms

Fig. 3. A diagram of the SimStudent learning mechanisms and how they interact.

Of the three kinds of knowledge manipulated by the SimStudent system, two
are learned automatically: the conceptual and procedural knowledge. To acquire
these two kinds of knowledge the system employs four learning mechanisms: what
learning, where learning, when learning, and how learning. The what learning
is used to acquire the conceptual knowledge whereas the where, when, and how
learning are used to acquire the procedural knowledge. Figure 3 shows how these
four learning mechanisms interact. Before SimStudent is used, the what learning
is run to acquire the conceptual knowledge. When SimStudent encounters a
situation where it does not know how to act, which is common initially, it requests
a demonstration from the author (the tutor developer or student tutor). This
demonstration is comprised of four parts:
• Focus of attention: the set of relevant interface elements (e.g., the left and
right hand sides of an equation);
• Selection: the interface element to manipulate (e.g., the output cell);
• Action: the action taken in the selection (e.g., update the text value); and,
• Input: the argument to the action (e.g, the text string used to update the
selection).
Every time the system sees a new demonstration or gets corrective feedback on
its performance, it learns or modifies a production rule. Production rule learning
is done in three parts: 1) how learning attempts to explain the demonstration
and produce the shortest sequence of primitive operator functions that replicates
the demonstrated steps and ones like it, 2) where learning identifies a generalized
path to relevant elements in the tutor interface that can be used as arguments
to the function sequence, and 3) when learning identifies the conditions under
which the learned production rule produces correct actions. We will now review
each of these learning mechanisms.

55

What This mechanism operates off-line to acquire a probabilistic context-free
grammar from only positive examples. This task can be defined as:
• Given: a set of examples of correct input;
• Find: a probabilistic context-free grammar with the maximal likelihood of
producing the examples.
This task is performed using a grammar induction approach outlined by Li et
al. [1], which uses a greedy approach to hypothesize the grammar structure and
Expectation Maximization to estimate the grammar parameters.
Whenever a demonstration is given to SimStudent, it augments the provided
information with the most likely parse trees of the content of each element in
the focus of attention. This additional information is used by SimStudent in
the subsequent learning mechanisms to extract deep feature knowledge from the
content (e.g., to recognize and extract the coefficient of a term in an equation).
The parse trees make this deep feature information directly accessible to SimStudent through the nodes in the parse tree (e.g., the left child of the parse tree
for “3x” in Figure 1 corresponds to the coefficient).
How This is the first of three mechanisms executed in response to a demonstration. The how learning task can be defined as:
• Given: a set of demonstrations consisting of the state of the relevant interface elements and the parse trees of the contents of these elements as well
as the resulting input for each state;
• Find: a sequence of primitive functions that when applied to each state
produces the corresponding input.
This task is performed by exhaustively applying the primitive operator functions
over all nodes in the focus of attention parse trees until the input is produced.
The iterative-deepening depth-first search strategy is used to find the shortest
sequence of functions that explains the data [1]. If no sequence exists, then a
special functions is created that takes the states and produces the corresponding
inputs.
Where This learning mechanism identifies the path to the relevant tutor interface elements. The tutor interface elements are specified by a hierarchical tree
structure (a table is comprised of rows which each contain cells). During interactive instruction, the relevant interface elements are specified by the author
teaching SimStudent. For each relevant element, SimStudent generates a parse
tree for the contents. The relevant portions of these parse trees are defined as
those that are utilized by the operator function sequence acquired through the
how learning. The task of learning a general path to this relevant information
can be defined as:
• Given: a hierarchical representations of the interface elements and their
parse trees, the function sequence from the how learner, and a set of elements
that have been identified as relevant;

56

• Find: a list of paths through the representation hierarchy to all of the relevant elements and the relevant portions of their parse trees.
The SimStudent approach to this task is to conduct specific-to-general learning
over the set of relevant interface elements and parse trees [1]. Returning to the
table examples, if the first cell in the first row of the table is always relevant,
then a path to that specific cell will be returned. However, if all of the elements
in the row are specified as relevant, then the entire row will be returned. After
the location to the relevant elements has been identified, the system utilizes
the function sequence to identify the relevant portions of the parse trees for
each element. This same specific-to-general learning is then conducted over these
relevant parse trees (within each element).
When This final mechanism identifies the conditions when the learned production rule is applicable. This task is defined as:
• Given: a set of positive and negative examples, each consisting of a set of
features and their associated label;
• Find: a set of conditions over the features that separate the positive and
negative examples.
As specified, this is a supervised learning task. The features used by SimStudent
to represent each example are predicates that are automatically generated from
the relevant portions of the parse trees. For example, there exists an “is-leftchild-of” predicate, which says that a particular argument is the left child of a
given node in one of the parse trees. This type of feature enables the retrieval
of equations, terms, coefficients, and variables. Given the feature descriptions of
each example, the positive and negative labels come from the user instructing
the SimStudent system. The first positive example is the initial demonstration.
Subsequent examples are generated when SimStudent tries to use the learned
rules to solve novel problems and receives yes/no feedback from the author.
To derive the set of conditions given the examples, SimStudent uses the FOIL
algorithm [2], which uses information theory to perform a general-to-specific
exploration of the space of hypothetical conditions.
These four learning mechanisms result in a simulated learning system that
accepts user demonstrations and feedback and automatically acquires probabilistic context-free grammar rules and production rules. The system requires little
background knowledge; for each task only the primitive functions need to be
defined by the developer. However, the cost of this generality is a system that
sometimes makes unrealistic generalization errors.

3

An example of an unrealistic generalization error

To explore the types of generalization errors that SimStudent makes, we turn
to the algebra domain. One of the skills that students learn in this domain is
how to proceed when given a problem of the form < Symbol >< V ariable >=<

57

Symbol > (e.g., 3x = 6). The skill that we desire the student to learn in this
situation is to specify that their next step is to divide both sides by the coefficient
of the term on the left side of the equation (the production rule from Figure 2).

Fig. 4. SimStudent requesting a demonstration in an algebra tutor interface after the
author has just entered “divide 3.”

When SimStudent is first presented with a problem of this form, such as
3x = 6, it will inform the author that it does not know how to proceed and
ask for a demonstration. The author might demonstrate to SimStudent that the
cells containing the left and right hand sides of the equation are relevant to the
problem (by double-clicking on these cells) and update the next step interface
element with “divide 3” (see Figure 4).
After receiving this demonstration, SimStudent parses the contents of the
focus of attention (The first parse tree in Figure 1 shows an example of what
the left hand of the equation might look like). Next, it employs the how learning
mechanism, which searches for a sequence of functions that when applied to
the nodes in the parse tree produce the input. In this example, it might learn
to append the left child of the parse tree (for the left side of the equation) to
the word “divide” and place it into the tutor interface (the then part of the
production rule in Figure 2). Using the locations of the relevant elements (the
left child of the parse tree), SimStudent then learns a general path through
the representation hierarchy to the relevant elements and the relevant portions
of the parse trees for these elements. Finally, SimStudent runs FOIL over the
relevant information to learn the conditions under which the learned behavior is
applicable. This results in the if portion of the production rule in Figure 2.
The learned production rule is more general than the single demonstration it
was learned from; it is applicable for many equations, such as 4x = 12 or 2x = 8.
However, when SimStudent is presented with a subtly different example that
utilizes the same skill, −x = 2, it results in the mistaken generation of the input
“divide -” (instead of “divide -1”). This is because in this situation the left child
of the parse tree on the left hand side of the equation is a minus sign instead of
the coefficient (see the second parse tree in Figure 1). In a review of problems
of the form −x =< Constant > in the ‘Self Explanation CWCTC Winter 2008
(CL)’ dataset accessed via DataShop [3], none of the human student made this
error– therefore it is an example of unrealistic generalization error.

58

4

Reflecting before Acting

One reason that humans do not make this error is that they have a “sense” for
what are reasonable output actions and they (subconsciously) reflect on actions
before taking them. When a student is faced with the problem −x = 2 they may
mentally produce the output “divide -,” but realize that a “-” by itself is not
mathematically grammatical because they have never seen an instance where
this has occurred. This might lead them to consider a different action or to ask
for help.
To reproduce this type of behavior, we modified SimStudent to utilize its
conceptual knowledge, the probabilistic context-free grammar trained on example inputs (described as “what” learning in section 2). The acquired grammar is
used to recognize when a potential output is not grammatical (when it cannot
be parsed) and automatically flag the situation as a negative example. In other
words, the system supervises itself and provides negative feedback (which the
when learner uses) to improve its learning.
Now, when SimStudent is presented with a problem and finds an applicable
rule, it simulates the execution of the rule and constructs a probabilistic parse
of the value generated by the rule. If the value cannot be parsed by the current
grammar (there is a 0% probability that the grammar produced the value),
then SimStudent flags the trace as a negative instance and re-runs the when
learning, which refines the conditions of the rule so that it no longer applies
in the erroneous situation. If SimStudent has no other applicable rules, then it
request a demonstration from the author, exactly like a human student.

5

Initial Results

To evaluate the effectiveness of this metacognitive loop, we have tested the probabilistic parser’s ability to separate correct from incorrect actions based on the
parse probability defined by the probabilistic context-free grammar. Table 1
shows five problems where SimStudent might make unrealistic errors. The first
three are problems where SimStudent might induce a rule for dividing by the
symbol before the variable instead of the coefficient. The last two problems correspond to inducing a rule retrieving the symbol after the variable and division
sign instead of the entire denominator. On all five problems, the probabilistic
grammar was capable of identifying the correct from the incorrect actions.
These results suggest that this approach is capable of identifying these kinds
of errors. In general, this approach will be effective at identifying errors that result in non-grammatical output, where grammatical is defined by the probabilistic context-free grammar. This is effective because the rules are learned specificto-general on a substantial amount of positive example inputs. By bringing this
previous experience to bare, SimStudent can avoid nonsensical generalization
errors and produce its own negative feedback, which enhances the effectiveness
of its other learning mechanisms (more self-labeled examples for the when learning). Furthermore, this requires no additional work from an author and should
reduce the amount of required author feedback.

59

Table 1. Five examples of problems where SimStudent might make the generalization
error of retrieving the character before the variable or after the variable and the division
sign, the corresponding correct and incorrect actions, the validity of these actions, and
the parse probability of the actions.
Example

Possible Action
divide −
−x = 2
divide −1
divide )
(−2)x = 6
divide (−2)
divide (
3(x + 1) = 6
divide 3
multiply (
x/(−3) = 3
multiply (−3)
multiply −
x/ − 5 = 1
multiply −5

Valid Parse Probability
No
0.00%
Yes
19.64%
No
0.00%
Yes
0.09%
No
0.00%
Yes
27.90%
No
0.00%
Yes
0.09%
No
0.00%
Yes
19.64%

This task of verifying the output could alternatively be viewed as applying constraints to SimStudent’s output and learning from constraint violations.
Viewed this way, our work is related to the work on constraint-based tutoring systems [4]. In our case, there is only one constraint, “the output must be
grammatical” where grammatical is defined as the probability of the output being produced by the grammar must be greater than 0%. We use a threshold of
greater than 0% to signify grammatical, but one could imagine using a different
threshold (e.g., greater than 0.05%). Thus, this constraint could be viewed as
a probabilistic constraint that is automatically acquired from positive training
examples.

6

Conclusion and Future work

In this paper, we outlined a novel approach to detecting and learning from unrealistic generalization errors that can be employed by simulated learner systems.
The implications of this approach are threefold: (1) its use will result in models of learning that more closely aligns with human data, (2) teachable agents
using this approach will be more realistic for the students using them, and (3)
developers can produce cognitive tutor models with less work.
While this approach shows promise, it clearly has some shortcomings that
should be remedied in future work. First, a more in-depth analysis of the alignment between SimStudent and human students is necessary. Previous work [5, 6]
has looked at the human errors that SimStudent is capable of predicting, but a
more detailed analysis of the unrealistic generalization errors, or errors that SimStudent makes that human students do not, would be useful. This would serve
as a baseline to evaluate the SimStudent model and to evaluate the effectiveness
of this approach.

60

A second direction for future work is to compare this approach to other approaches that might reduce these errors. We could imagine a system that has
additional condition knowledge for the operator functions so that it would not
generalize to situations where the function sequence would not be applicable
(such as trying to divide by a symbol instead of a number). It would also be
interesting to explore how reflection might facilitate the acquisition of this additional condition knowledge for the operator functions.
Finally, we are interested in applying this approach in other more complex
and open-ended domains such as in RumbleBlocks, an educational game that
teaches K-3 children about the relationships between the concepts of stability,
low center of mass, wide base, and symmetry. We have been exploring how probabilistic grammars can be used to learn conceptual knowledge in RumbleBlocks
[7] and we believe that this approach should scale up to this more complex
domain.

References
1. Li, N., Schreiber, A.J., Cohen, W.W., Koedinger, K.R.: Efficient Complex Skill
Acquisition Through Representation Learning. Advances in intelligent tutoring systems 2 (2012) 149–166
2. Quinlan, J.R.: Learning Logical Definitions from Relations. Machine Learning 5
(1990) 239–266
3. Koedinger, K.R., Baker, R.S.J.d., Cunningham, K., Skogsholm, A., Leber, B., Stamper, J.: A Data Repository for the EDM community: The PSLC DataShop. In
Romero, C., Ventura, S., Pechenizkiy, M., Baker, R.S.J.d., eds.: Handbook of Educational Data Mining. CRC Press (2010)
4. Mitrovic, A., Ohlsson, S.: Evaluation of a constraint-based tutor for a database
language. International journal of artificial intelligence in Education 10 (1999)
238–256
5. Lee, A., Cohen, W.W., Koedinger, K.R.: A Computational Model of How Learner
Errors Arise from Weak Prior Knowledge. In Taatgen, N., van Rijn, H., eds.: Proceedings of the Annual Conference of the Cognitive Science Society, Austin, TX
(2009) 1288–1293
6. Matsuda, N., Cohen, W., Sewall, J., Lacerda, G., Koedinger, K.R.: Evaluating a
Simulated Student using Real Students Data for Training and Testing. In Conati,
C., McCoy, K., Paliouras, G., eds.: Proceedings of the International Conference on
User Modeling. (2007) 107–116
7. Harpstead, E., MacLellan, C., Koedinger, K.R., Aleven, V., Dow, S.P., Myers, B.:
Investigating the Solution Space of an Open-Ended Educational Game Using Conceptual Feature Extraction. In: Proceedings of the International Conference on
Educational Data Mining. (2013)

61

Towards Moment of Learning Accuracy
Zachary A. Pardos† and Michael V. Yudelson‡
†Massachusetts Institute of Technology
77 Massachusetts Ave., Cambridge, MA 02139
‡Carnegie Learning, Inc.
437 Grant St., Pittsburgh, PA 15219, USA
zp@csail.mit.edu,yudelson@carnegielearning.com

1

Introduction

Models of student knowledge have occupied a significant portion of the literature in the area of Educational Data Mining1 . In the context of Intelligent
Tutoring Systems, these models are designed for the purpose of improving prediction of student knowledge and improving prediction of skill mastery. New
models or model modifications need to be justified by marked improvement
in evaluation results compared to prior-art. The standard evaluation has been
to forecast student responses with an N-fold student level cross-validation and
compare the results of prediction to the prior-art model using a chosen error
or accuracy metric. The hypothesis of this often employed methodology is that
improved performance prediction, given a chosen evaluation metric, translates
to improved knowledge and mastery prediction. Since knowledge is a latent, the
estimation of knowledge cannot be validated directly. If knowledge were directly
observable, would we find that models with better prediction of performance also
estimate knowledge more accurately? Which evaluation metrics of performance
would best correlate with improvements in knowledge estimation? In this paper
we investigate the relationship between performance prediction and knowledge
estimation with a series of simulation studies. The studies allow for observation
of the ground truth knowledge states of simulated students. With this information we correlate the accuracy of estimating the moment of learning (mastery)
with a host of error metrics calculated based on performance.

2

Bayesian Knowledge Tracing

Among the various models of knowledge, a model called Bayesian Knowledge
Tracing [2] has been a central focus among many investigators. The focus on
this model has been in part motivated by its use in practice in the Cognitive Tutors [4], used by over 600,000 students, and by its grounding in widely adopted
cognitive science frameworks for knowledge acquisition. For our experiments we
will be employing the most frequently used basic Bayesian Knowledge Tracing
1

A session during the main proceedings of EDM 2012 was dedicated to papers on
Knowledge Tracing, a frequently used approach to modeling student knowledge.

62

model for both simulation and evaluation; however, there are implications beyond BKT models. Knowledge Tracing is a simple Hidden Markov Model of
Knowledge defined by four parameters; two performance parameters and two
knowledge parameters. The performance parameters, guess and slip, are the
emission parameters in an HMM which respectively correspond to the probability that a student answers correct even if she is in the negative knowledge
state (guess) and the probability that she answers incorrectly even if she is in
the positive knowledge state (slip). The knowledge parameters, prior and learn
rate, are the probability that a student knows the skill before answering any
questions and the probability that, if the student is in the negative knowledge
state, she will transition to the positive state at any given opportunity.

3

Related Work

There has been a limited amount of prior work focusing on detecting the moment
of learning. We were able to track one relevant publication by Baker and colleagues [1]. They investigated detection of moment of learning in student data by
modifying BKT structure. Another relevant result was published by [5]. They
looked at scoring student model fits on simulated data and found a disparity
between rankings of two frequently used metrics: root mean squared error and
area under ROC curve. In this work we would like to address the question of
the quality of detecting the moment of learning and investigate the problem of
choosing a goodness-of-fit metric for that purpose.

4

Data

Our simulation dataset consisted of 1,000 simulated students and 100 skills with
30 questions per skill. Every student answered all 30 questions for each of the
100 skills. In the BKT simulation model we included no dependencies between
skills and also no student specific parameters; therefore, the data can be thought
of as either being produced by 1,000 students total or a new 1,000 students for
every skill. Programmatically, data for each skill is stored in a separate file. Each
row in each file represents one students data for that skill. The data stored from
the simulation contains the students ground truth binary state of knowledge
(mastered or not) at each of the 30 opportunities to answer (first 30 columns)
and also the students correctness of responses to the 30 questions (stored in the
second set of 30 columns).
In addition to the simulated data files containing student knowledge states
and observed responses, we had corresponding files containing inferences of
knowledge states and predictions of responses made with 16 different parameter sets resulting in 1,600 prediction files. Details of the parameter selection for
simulation and prediction are discussed in the next section.

63

5
5.1

Methodology
Simulation

We generated 1,000 students knowledge and performance for 100 skills. Skills are
defined by a set of four knowledge tracing parameters which the skill data is generated from. The 100 sets of four parameters were selected at random, uniformly
sampling from the following constrained ranges for the parameters; prior between
0.01-0.80, learn rate between 0.01-0.60, and guess and slip between 0.05-0.40. After the 100 sets of parameters were selected, simulated data was produced by
specifying a Dynamic Bayesian Network representation of Knowledge Tracing
with a time slice length of 30. This representation, defined in Kevin Murphys
Bayes Net Toolbox, with a particular parameter set fixed in the conditional
probability tables, was then sampled 1,000 times, representing each simulated
student. The sample Dynamic Belief Network function in BNT for simulation is
a simple one; a random number between 0 and 1 is generated, if the number is
equal to or lower than the prior parameter, the simulated student begins in the
negative (not learned) state at time slice 1. To generate the observed response at
this time slice, another random number is generated, if that number is greater
than the guess parameter, the observed response is incorrect. To determine if
the students knowledge state is positive (learned) at the next time slice; a random number is generated, if that number is less than or equal to the learning
rate, then the students state is positive. With a positive state, the new random
number needs to be greater than the slip parameter in order to produce a correct response. This is repeated for 30 times to simulate 30 knowledge states and
observed responses per student.
5.2

Prediction

Typically, to predict student data, a hold-out strategy is used whereby a fraction
of the students and their data is used to find a good fitting set of parameters.
That good fitting set is then used to predict the fraction of students not used in
training. The research question of this paper did not involve parameter fitting
but rather required us to evaluate various models and observe how the models
prediction of performance corresponded to its inference of knowledge. To do this
we needed variation in models which we accomplished by choosing 16 candidate
parameter sets with which to predict student data from each of the 100 skills.
Since no training was involved, all data served as the test set. The top five sets
of parameters used in the Cognitive Tutors was used, as well as 10 randomly
generated parameters sets using the the same parameter constraints as the simulation, and, lastly, the ground truth parameter set for the skill was used to
predict. The the same 15 parameter sets were used to predict the 100 skills, only
the ground truth parameter set changed.
The prediction procedure is the same one used in all papers that use Knowledge Tracing; the prior, guess and slip parameters dictate the probability of
correct on the first question. After the prediction is made, the correctness of

64

Table 1: Confusion Table
Actual
Correct
Incorrect
Correct True Topisitve(TP) False Positive (FP)
Predicted
Incorrect False Negative (FN) True Negative (TN)

the first question is revealed to the KT algorithm, which incorporates this observation using Bayes Theorem to infer the likelihood that the knowledge was
known at that time. A learning rate transition function is applied and the processes is repeated 30 times in total to create 30 predictions of knowledge and 30
predictions of correctness per student for a skill.

6

Metrics

The most common metrics used to evaluate prediction performance in the EDM
literature has been Area Under the Receiver Operator Curve (AUC) and Root
Mean Squared Error (RMSE). One of the goals of our experiment is to reveal
how indicative these measures are of the models accuracy in inferring knowledge.
While these are the most common metrics, many others have been used in machine learning to evaluate predictions. We utilize a suite of metrics to investigate
which metric is best at forecasting knowledge inference accuracy.

6.1

Model Performance

We selected a set of metrics in wide use today to score models when predicting
student performance and knowledge state. Below is a short description of them.

Confusion Table Metrics Confusion table (rf. Table 1) is a table widely used
in information retrieval and is a basis for a set of metrics capturing correctness
of a retrieval or classification algorithm. Rows and columns of the confusion
table denote the predicted and actual classes respectively and the cells in the
intersection contain the counts of cases. Refer to Table 1 for an illustration. Here
we illustrate a case for binary classification akin to the problem of binary classification of student performance (correct or incorrect) and state of knowledge
(known or unknown).
If prediction is not categorical, say a probability from [0, 1], it is customary
to round it: probabilities of 0.5 and greater become 1. For example, the cases
when prediction matches the reality are captured in True Positive cell and the
cases when the actually incorrect responses are marked as correct are captured
in False Positive cell. We will use the confusion table metrics below.

65

TP + TN
TP + FP + TN + FN
TP
precision =
TP + FP
TP
recall =
TP + FN
precisionṙecall
F − measure = 2
precision + recall
accuracy =

(1a)
(1b)
(1c)
(1d)

As opposed to the so-called point measures described above, there is also
a frequently used Area Under Receiver Operating Characteristic curve (AUROC), which is a curve measure. The curve is produced by varying the rounding
threshold (0.5 for point measures) from 0 to 1 and computing and plotting False
Positive Rate (FPR) vs. True Positive Rate (TPR) (see below).
TP
TP + FN
FP
FPR =
FP + FN
TPR =

(2a)
(2b)

An area under resulting curve is the sought metric. An area of 0.5 is equivalent
to random chance for a binary classifier. An area greater than 0.5 is, thus, better
than chance. An exact AUC calculation can also be derived by enumerating
all possible pairs of predictions. The percentage of the pairs in which the true
positive prediction is higher is the AUC. This is the ability of the predictor to
discriminate between true and false.
Pseudo R2 R2 or percent variance explained is often used as a goodness of
fit metric in linear regression analysis. For with binary classification, there exist
several versions of R2 called pseudo R2 . Applicable to our situation is Efrons
pseudo R2 (refer to Equation below).
PN
yi − yˆi
R2 = 1 − Pi=1
N
i=1 yi − ȳ

(3)

Where N is the number of data points, yi is the i-th component of the
observed variable, y¯i is the mean observed value, and yˆi the prediction of i-th
component of the observed variable.
Metrics Based on Log-Likelihood Likelihood functions are widely used in
machine learning and classification. Likelihood captures the probability of the
observing data given parameters of the model. In binary classification a natural log transformation of the likelihood function is often used (see below). Here

66

N is the total number of datapoints, yi is the i-th component of the dependent variable, yˆi is the predicted value of the i-th component of the dependent
variable.

loglikelihood =

N
X

yi ln(yˆi ) + (1 − yi ) ln(1 − yˆi )

(4)

i=1

In addition to log-likelihood itself, there are several metrics that use loglikelihood as kernel component. For example, Akaike Information Criterion (AIC),
Akaike Information Criterion with correction for finite sample size (AICc), Bayesian
Information Criterion (BIC), and several others. These metrics introduce various
forms of penalty for the size of the model (number of parameters) and number
of datapoints in the sample in order to put overfitting models at disadvantage
when performing model selection. Here k is the number of model parameters, N
is the number of datapoints.

AIC = −2loglikelihood + 2k
2k(k + 1)
AICc = AIC +
N −k−1
BIC = −2loglikelihood + k ln(N )

(5a)
(5b)
(5c)

Since we are comparing models that are only different in the parameter values
and are doing so on the same dataset, we will not see difference in ranks assigned
by log-likelihood, AIC, AICc, and BIC metrics.

Capped Binomial Deviance In addition to log-likelihood and log-likelihoodbased metrics, we include the Capped Binomial Deviance (CBD). Capped binomial deviance is a version of the log-likelihood where prediction values are
mandated to be at least away from 0 and 1 values and uses a logarithm with
base 10 instead of natural logarithm. The is usually set to a small value of 0.001.

6.2

Moment of Learning

To capture the quality of detecting the moment of learning we devised a metric
based on mean absolute deviation (MAD). Namely, moment of learning MAD is
the average absolute difference of number of skill application opportunities between the moment when the internal state of the generating skill model switched
to learned state and the moment when the probability of the skill being in a
learned state reaches 0.95 (a traditionally used threshold in the area of intelligent tutoring systems). A perfect model would have a moment of learning MAD
of 0. The larger the moment of learning MAD is the worse the model prediction
of model of learning is.

67

7
7.1

Experiments and Results
Experiment 1

Research question: Among accuracy metrics used for ranking various parameter
sets (models), which ones correlate best with accuracy of moment of learning
prediction?
7.2

Results

The Table 2 below contains the correlations of performance prediction value,
knowledge prediction value for all metrics, and moment of learning mean absolute error. Since prediction of performance is most widely adopted as a standard
approach and the fact that we are trying to contrast it to the moment of learning
mean absolute error, we sorted the rows corresponding to various statistical metrics by the respective column. The first column lists the metric used to evaluate
the goodness of performance and knowledge prediction. The second column is the
correlation between knowledge and performance prediction using the particular
metric on both (this is the column the table is sorted by). The third column is
the correlation between the particular metric used to evaluate performance and
Mean Absolute Deviation (MAD) of Moment of Learning prediction. This is the
column which tells us if the metrics used to evaluate performance are correlated
with error in mastery / Moment of Learning prediction. The fourth column gives
correlations of Moment of Learning MAD and metric values for predicting internal knowledge state. This correlation captures agreement between identifying the
moment student learned a skill (this happens once per student-skill tuple) and
the correctness of identifying the skills knowledge state for the student across
all skill attempts.
7.3

Experiment 2

Hypothetically, the ground truth parameter sets should be the best at both
making predictions of performance and estimating knowledge. A good metric
should favor the ground truth parameters, therefore we ask: How often is the
ground truth model the best at prediction performance according to the various
metrics?
7.4

Results

The correlations of the performance and knowledge state prediction metrics from
prior section targeted the 15 model parameter combinations that were different
from the generating ground truth model parameters. Now, let us look at how
the ground truth model compares to the other 15 we tested with respect to the
statistical metrics we chose. Table 3, for each metric, gives the number of times
a ground truth model parameter set is the best with respect to a given metric,
and an average rank of the ground model parameter set as compared to the

68

Table 2: Metric correlations
Metric

Correlation of per- Correlation of per- Correlation of knowlformance and knowl- formance metric and edge metric and
edge metric
Moment of Learning Moment of Learning
MAD
MAD
recall
0.878 ***
-0.954 ***
-0.819 ***
F-measure
0.561 ***
-0.839 ***
-0.792 ***
accuracy
0.522 ***
-0.802 ***
-0.822 ***
precision
0.334 ***
-0.797 ***
-0.628 ***
RMSE
0.470 ***
0.754 ***
0.828 ***
AIC
0.375 ***
0.751 ***
0.702 ***
AICc
0.375 ***
0.751 ***
0.702 ***
BIC
0.375 ***
0.751 ***
0.702 ***
CBD
0.409 ***
0.751 ***
0.762 ***
log-likelihood 0.375 ***
0.751 ***
0.702 ***
pseudo R2
0.592 ***
-0.236 *
-0.296 **
AU ROC
0.335 ***
-0.119
-0.652 ***
Note: with respect to correlations with moment of learning MAD, in some cases a
negative correlation is desirable (e.g., for accuracy), and for some cases a positive
correlation is desirable (e.g., for RMSE). This is due to the fact that the smaller the
moment of learning MAD the better, which is true for some metrics and the inverse
is true for others. The table is sorted while observing this phenomenon (effectively
sorting by the absolute value of the correlation coefficient).

Table 3: ground truth model rank vs. the other 15 models
Metric

Ground truth model Mean rank of ground
has rank of 1
truth model
AIC
88/100
1.82/16
AICc
88/100
1.82/16
BIC
88/100
1.82/16
CBD
88/100
1.82/16
log-likelihood 88/100
1.82/16
RMSE
88/100
1.82/16
pseudo R2
88/100
1.83/16
accuracy
33/100
2.52/16
F-measure
12/100
4.27/16
AU ROC
26/100
4.35/16
recall
0/100
6.65/16
precision
5/100
9.71/16

other 15 model. In each case we are aggregating across 100 different sets of 15
models plus one ground truth model. As we can see log-likelihood based models
and RMSE form a group of metrics that gives ground truth models a large edge
over the 15 reference models. Confusion table metrics, Area under ROC curve
and the pseudo R2 gibe a drastically smaller support for it.

69

7.5

Experiment 3

Ground truth parameters do not always predict the data the best, but often do
when using metrics like RMSE or log-likelihood. Do the parameter sets that are
not predicted well by ground truth share a common pattern? Does the relative
performance of ground truth correlate with high or low values of prior, learn,
guess or slip in the generating parameters?
7.6

Results

Seeing log-likelihood based and RMSE metrics score the ground truth model
at the same level of mean rank, we are wondering whether, across all 100 of
generating parameter sets, the data produced by the same sets of parameters is
equally hard to predict with ground truth model. For that we looked at whether
the BKT parameter values correlate with ranks ground truth model receives on
the moment of learning MAD metric.
First of all, moment of learning MAD metric ranked ground truth as best
only 33/100 times with an average rank of 2.53/16. Correlations of moment
of learning MAD ranks for ground truth models showed that theres a small
marginally significant effect of pInit probability on the moment of learning MAD
score (r = 0.18, p − val = 0.07). Guessing probability does not correlates with
moment of learning MAD (r = −.06, p − val = 0.55).
Probability of learning and slip probability, however, are very strongly related
to the moment of learning metric. The larger the learning rate of a simulated skill
is, the higher the rank of the ground truth model is (r = 0.68, p − val < 0.001).
Namely, the faster the skill is learned, the worse job ground truth model is doing.
In the case of pSlip, the relation is the opposite: the higher the guess rate is,
the higher rank moment of learning MAD assigns to the ground truth model
(r = −0.52, p − val < 0.001).
Both the pLearn and pSlip parameters are controlling the process of skills
transitioning into the learned state. Strong negative correlation of moment of
learning MAD and pSlip is quite logical. Higher pSlip results in more errors even
when the skill is mastered, as a result the transition to the learned state becomes
more blurred. In this situation the ground truth model has an edge over other
models. However, it is high to explain a high positive correlation of moment of
learning MAD and pLearn. Higher pLearn means more correct responses overall,
this should put ground truth model at an advantage. Additional investigation is
necessary to address this phenomenon.

8

Discussion

In our first experiment we found that three less commonly used accuracy metrics
showed the best correspondence to accuracy of moment of learning estimation.
These metrics were: recall, F-measure, and accuracy, with recall giving a very
high correlation of 0.954. Also noteworthy was the poor performance of AUC

70

with a correlation of -0.119. This was the worst correlation and suggests that
AUC should not be used to determine the relative goodness of models based
on prediction performance if the underlying goal is to rank models based on
knowledge estimation goodness. Metrics like recall and F-measure ought to be
adopted in place of AUC for these purposes.
We also found that ground truth model parameters did not always perform
the best and that RMSE and log-likelihood based metrics tended to predicted
ground truth being the best parameter set more than the others. AUC, recall,
F-measure, and precision, however, were among the worst. Therefore, if the underlying goal of an analysis is to recover ground truth parameters (such as with
inferring pedagogical efficacy), RMSE and log-likelihood measures should be
used and the aforementioned accuracy metrics should be avoided. The experiments 2 raised the question of why ground truth may not always predict the
best experiment 3 indicated that high learning rate and low slip in the generating
parameters can prove difficult for mastery prediction.
Overall detecting the moment of learning in the generated data by observing
a switch from a string of all 0s (unknown state) to the string of all 1s (known
state) is often not easy even when ground truth parameters are used. Especially
if guess and slip parameters are larger, several back-and-forths between known
and unknown state are common. In the area of ITS it is customary to wait till
three correct attempts in a row to be sure student has mastered the underlying
skill. In our case, when we assumed the moment of learning is the first time
when probability of knowing the skill crosses the 0.95 threshold. Following from
recent results on the lag with detecting the moment of learning that occurs in the
Bayesian Knowledge Tracing [3], in future, we will experiment with adjustments
to our computation of the moment of learning to compensate for this.

References
1. Baker, R.S.J.d., Goldstein, A.B., Heffernan, N.T. (2010) Detecting the Moment
of Learning. Proceedings of the 10th Annual Conference on Intelligent Tutoring
Systems, 25-34.
2. Corbett, A. T. and Anderson, J. R.: Knowledge tracing: Modeling the acquisition of
procedural knowledge. User Modeling and User-Adapted Interaction, 4(4), 253-278.
(1995)
3. Fancsali, S.E., Nixon, T., Ritter, S. (2013) Optimal and Worst-Case Performance of
Mastery Learning Assessment with Bayesian Knowledge Tracing. In: Proceedings
of the 6th International Conference on Educational Data Mining.
4. Koedinger, K. R., Anderson, J. R., Hadley, W. H., and Mark, M. A. (1997). Intelligent tutoring goes to school in the big city. International Journal of Artificial
Intelligence in Education, 8, 3043.
5. Pardos, Z. A., Wang, Q. Y., Trivedi, S. (2012) The real world significance of performance prediction. In Proceedings of the 5th International Conference on Educational Data Mining. Crete, Greece. pp 192-195.

71

Impact of Prior Knowledge and Teaching
Strategies on Learning by Teaching
Ma. Mercedes T. RODRIGO1, Aaron ONG1, Rex BRINGULA2, Roselle S.
BASA2, Cecilo DELA CRUZ2, Noboru MATSUDA3
1

Ateneo Laboratory for the Learning Sciences, Department of Information Systems
and Computer Science, Ateneo de Manila University, Loyola Heights, Quezon City,
Philippines
2
University of the East, Manila, Philippines
3
Human-Computer Interaction Institute, Carnegie Mellon University, Pittsburgh, PA,
USA
mrodrigo@ateneo.edu, icemanfresh@yahoo.com
rexbringula@gmail.com, roselle_basa@yahoo.com
noboru.matsuda@cs.cmu.edu

Abstract. We investigate cognitive factors that are predictive of
learning gains when students learn to solve equations by teaching a
synthetic peer, called SimStudent. Previous empirical studies
showed that prior knowledge is strongly predictive of post-test
scores. However, in a recent study in the Philippines that replicated
our previous study in the USA, there were students with low priorknowledge who tutored their SimStudent better than other equally
low prior students. In this paper, we analyze both process data (tutoring interactions) and outcome data (test scores) to understand
what makes learning by teaching more effective. The results imply a
presence of individual behavioral differences beyond the difference
in the prior knowledge that might have affected SimStudent’s learning, which in turn had non-trivial influence on tutor learning.
Keywords. Learning by teaching, teachable agent, SimStudent, Algebra equations, prior knowledge

1.

Introduction

Since the late 1990s, researchers have investigated intelligent tutoring
systems with intelligent pedagogical agents (often called teachable agents)
to study a promising type of learning where students learn by teaching [1-3].
These technologies allow researchers to conduct tightly controlled experiments and to collect detailed process data representing interactions between
students and teachable agents that together provide empirical evidence for
the benefit of learning by teaching [4].
Matsuda et al. (in print), for example, showed that students’ learning significantly correlated with the learning of teachable agents. Biswas et al. [5]

72

studied whether students could learn to self-regulate their teaching activities
and how the ability of self-regulation affects the tutor learning. It is therefore of intellectual interest to uncover how the tutoring interaction affects
students’ learning by teaching.
In the current study, we use SimStudent, which is a teachable agent that
helps students learn problem-solving skills by teaching [6]. It has been tested and redesigned several times, resulting in insights regarding the effects of
learning by teaching and related cognitive theories to explain when and how
students learn by teaching. Previous studies showed that pre-test score were
highly predictive of post-test scores when students learn equation solving by
teaching SimStudent [7]. In general, when students do not have sufficient
prior knowledge on the subject to teach, they are not able to teach correctly
and appropriately hence the benefit of learning by teaching would be arguably decreased.
Nonetheless, there are some students with low prior knowledge who
learned more than others by teaching SimStudent. Among equally low-prior
students, those who showed better performance on the post-test actually
tutored their SimStudent better as well. The difference in the learning gain
among students with comparable prior-knowledge indicates a presence of
effective interaction for learning by teaching that might bootstrap tutor
learning even with insufficient prior knowledge.
The goal of this paper is to investigate cognitive factors that affect tutor
learning. The central research question is why some students (even with low
prior knowledge) learned more than other students with comparable prior
knowledge. To address this research question, the current paper analyzes
data from two classroom (in-vivo) studies conducted in the USA and the
Philippines. The Philippines study was a replication of the USA study reported earlier [8].
In the rest of the paper, we first introduce a learning environment in
which students learn to solve linear equations by teaching SimStudent. We
will then introduce two classroom studies conducted in the USA and the
Philippines followed by the results and discussions.

2.

Online Learning Environment with SimStudent

This section provides a brief overview of SimStudent and the online
learning environment, Artificial Peer Learning environment using
SimStudent (APLUS), in which students learn to solve algebra equations by
interactively teach SimStudent. Technical details about SimStudent and
APLUS can be found elsewhere [7]
2.1.

SimStudent

SimStudent is a synthetic pedagogical agent that acts as a peer learner. It
learns procedural skills from examples. That is, a student gives SimStudent

73

a problem to solve. SimStudent then attempts to solve the problem one step
at a time, occasionally asking the student about the correctness of each step.
If SimStudent cannot perform a step correctly, it asks the student for a hint.
To respond to this request, the student has to demonstrate the step.
Students may not be able to provide the correct feedback and hints. As
SimStudent is unable to distinguish correct from incorrect feedback, it continues to try to generalize examples and generate production rules that represent the skills learned. SimStudent is also capable of making incorrect
inductions that would allow SimStudent to learn incorrect productions even
when students teach SimStudent correctly. SimStudent’s ability to model
students’ incorrect learning is one of the unique characteristics of
SimStudent as a teachable agent.
2.2.

APLUS: Artificial Peer Learning Environment using SimStudent

Figure 1 shows an example screen shot of APLUS. In APLUS, students
act as a tutor to teach SimStudent how to solve equations. SimStudent is
named Stacy and visualized at the lower left corner of APLUS. The tutoring
interface allows the student and Stacy to solve problems collaboratively. In
the figure, a student poses the problem 3x+6=15 for Stacy to solve. Stacy
enters “divide 3” and asks the student whether this is correct. The student
responds by clicking on the [Yes/No] button. If the student gets stuck, she
can consult the examples tabbed at the top of the screen.
The student has the option of gauging how much Stacy has learned with
the use of a quiz. The student chooses when and how often to administer

Fig 1. A screen shot of APLUS. SimStudent is visualzed with an avatar
image and names Stacy.

74

the quiz by clicking a button at the bottom of the interface. The quiz interface looks like the tutoring interface, however, when Stacy takes the quiz,
she does so independently, without any feedback or intervention from the
student. At the end of the quiz, the student is presented with a quiz result.
The quiz is divided into 4 sections, each with two equation problems.
The quiz items were created from the mix of one-step, two-step, and target
equations (i.e., the equations with variables on both sides).
Stacy cannot progress to a section until she passes the previous section.
The students were asked to tutor Stacy to be able to solve equations with
variables on both sides. In the classroom studies, the students were informed
that their goal was to help Stacy pass all four (4) sections of the quiz.

3.
3.1.

Methods
Participants

The USA study took place in one high school in Pittsburgh, PA, under
the supervision of the Pittsburgh Science of Learning Center [8]. There were
eight Algebra I classes with an average of 20 students per class. A total of
160 students with ages ranging from 14 to 15 participated in the study.
The Philippines study took place in one high school in Manila, Philippines, under the supervision of the co-authors from the University of the
East and the Ateneo de Manila University. We enlisted participation from
five first year high school sections with an average of 40 students per class.
There were 201 study participants in all with ages ranging from 11 to 15.
The average age of the participants was 12.5 years.
3.2.

Structure of the study

In both the USA and the Philippine studies, each participant was randomly assigned to one of two versions of SimStudent: an experimental condition
in which Stacy prompted the participants to self-explain their tutoring decisions and a control condition with no self-explanation prompts. The study
was designed this way to investigate a particular research question on the
effect of self-explanation for tutor learning [8], which is beyond the scope of
the current paper. For three consecutive days, participants used their assigned version of SimStudent for one classroom period per day (42 minutes
for the USA and 60 minutes for the Philippines study).
3.3.

Measures

Students took pre- and post-test before and after the intervention. The
students also took a delayed-test two weeks after the post-test was administered. Three versions of isomorphic tests were randomly used for pre-, post-,
and delayed-tests to counterbalance the test differences. Students had the
entire class period to finish the tests.

75

The tests are divided into five parts. Of these five parts, three parts are to
test procedural knowledge on how to solve equations (the Procedural Skill
Test, or PST), whereas other two parts are to test conceptual knowledge
about algebra equations (the Conceptual Knowledge Test, or CKT). 102 out
of 160 USA participants took all three tests, whereas in the Philippines 146
out of 201 participants took all three tests. In the following analyses, unless
otherwise indicated, only those students who took all three tests are included.
The system automatically logged all of the participants’ activities including problems tutored, feedback provided, steps performed, examples reviewed, hints requested, and quiz attempts. In the following analysis, we
use these factors as process data.

4.
4.1.

Results
Overall Test Scores

Table 1 shows mean test scores plus or minus SD for the pre, post, and
delayed Procedural Skill Tests from two studies. To see how students’ test
scores varied before and after teaching SimStudent, we conducted a twoway repeated-measures ANOVA with condition as a between-subjects variable and test-time (pre, post, and delayed) as a within-subjects variable. For
the USA study, the repeated measure analysis revealed a weak trend for the
main effect for test-time. A post-hoc analysis detected a difference from pretest to post-test [8]. In the Philippines study, the test-time was also the main
effect, and the post-hoc analysis detected that delayed-test was significantly
higher than pre-test; t(247.1) = 2.457, p < 0.05. This difference, however,
was likely due to the classroom instruction that students were taking during
the two-week interval between the intervention and the delayed test.
Both in the USA and the Philippine studies, condition was not the main
effect—the presence of self-explanation did not affect tutor learning with
the version of APLUS and SimStudent used in two studies.
Table 1: Mean test scores ± SD for pre, post, delayed procedural skill test for
each study.
Pre-test
Post-test
Delayed-test
Philippines (PH)
0.21±0.01
0.22±0.02
0.25±0.03
USA (US)
0.68±0.04
0.71±0.05
0.69±0.06

4.2.

Impact of prior knowledge

As shown in Table 1, there was a notable difference in the pre-test scores
suggesting that USA students had higher level prior knowledge than Philippine students; t(142.4) = -22.25, p < 0.001.

76

To see how prior knowledge affected learning and if the impact of prior
knowledge differ between two studies, we ran a regression analysis with
post-test score as a dependent variable and study (US vs. PH) as a fixed
factor using pre-test score as a covariate. The results showed that pre-test is
a strong predictor of post-test; t(244) = 2.80, p < 0.01. There was also a
strong interaction between pre-test and study; the regression coefficient
(slope) differed significantly between two studies; bPH = 0.32 vs. bUS = 0.76;
F(1,244) = 11.24, p < 0.001—suggesting that, in general, USA students
gained (from pre- to post-test) more than Philippine students. Figure 2
shows the scatter plot for pre-test (x-axis) and post-test (y-axis) scores.
USA students (red triangles) had steeper regression line than Philippine
students.
4.3.

Quiz Results

0.0

0.2

0.4

Post.

0.6

0.8

1.0

In the USA study, 36 out of 102(35%) students made their SimStudents
pass all four quiz sections. In the Philippines study, no students passed all
four sections. At the best, only 7 out of 146 (5%) of Philippine students had
their SimStudents pass quiz section 2.
In the Philippines study, there were 73 students who solved quiz item #1
correctly. Of those 73 students, 68 students solved quiz item #2 correctly
(hence by definition passing quiz section 1). Of those 68, only 11 students
passed quiz section 2 (i.e., solving the first four quiz items correctly).
One possible explanation for the Philippine students’ poor performance
on the quiz is that Philippine students have insufficient prior knowledge, as
indicated by the
factor(Country)
low pre-test scores
PH
and the weak reUS
gression slope. A
number of factors
may account for the
difference
prior
knowledge, including curricular and
age differences.
Still, some Philippine
students
managed to solve
the first four quiz
items (i.e., passing
the quiz section 2),
0.0
0.2
0.4
0.6
0.8
1.0
while others did
Pre
not. Why might
this be so? The Fig. 2: Scatter plot of pre-test (x-axis) and post-test
next section ad- (y-axis) scores. US students had larger regression
dresses this issue.
slope (0.76) than the PH students (0.32).

77

4.4.

What makes learning by teaching more effective?

To understand why some SimStudents performed better on the quiz than
others, we have analyzed the process data. In this analysis, we grouped students depending on the quiz sections their SimStudents passed. We call
students whose SimStudents passed and failed quiz section x the “passing
Sx” and “failing Sx” students, respectively. By definition, there were no
passing S3 students in the Philippines study.
Our focus in this particular analysis is to understand how some students
managed to pass quiz sections in the Philippines study. Therefore, we only
included Philippine students for this analysis unless otherwise noted.
4.4.1. Accuracy of tutoring
One cognitive factor that had a significant contribution to tutor learning
in the past studies is the accuracy of tutoring—i.e., the accuracy of recognizing correct and incorrect steps made by SimStudent as well as the accuracy
the steps demonstrated as hint.
We thus compared the mean accuracy of passing/failing S1 and S2 students. The result suggested that the accuracy of tutoring is a key for success
on the quiz in the Philippines study as well. For S1: MPassing = .70 (SD = .14)
vs. MFailing = .52 (SD = 0.16); t(119.3)=-6.89, p < 0.001. For S2: MPassing =
.75 (SD = 0.09) vs. MFailing = .59 (SD = 0.18); t(8.7)=-4.39, p < 0.01.
Students’ prior knowledge should have affected tutoring accuracy. There
was actually a strong correlation between the prior knowledge (measured as
the pre-test score on the Procedural Skill Test) and the accuracy of tutoring.
There was also a study difference—USA students tutored more accurately
than Philippine students. The centered polynomial regression with the centered pre-test score (i.e., the difference from the mean) as the covariate
(C.Pre) and the study (US vs. PH) as a fixed factor predicting the accuracy
of tutoring (AT) revealed the following regression coefficients: AT = 0.62 +
0.16*C.Pre + 0.18[if US]; r2=0.42, F(2, 235)=88.31, p<0.001; meaning that
Philippine students at the average procedural skill pre-test tutored with a
62% accuracy rate. USA students tutored 18% more accurately than Philippine students in general. There was no study difference for the regression
slope—suggesting that the prior knowledge affected the accuracy of tutoring
equally in two studies.
A further analysis that compared passing and failing S1 students revealed
that the prior knowledge was not the dominant factor that affected the accuracy of tutoring. In the Philippines study, the average pre-test score of the
Procedural Skill Test for passing S1 students (M=.21, SD=0.10) was not
higher than failing S1 students (M=.20, SD=0.09). However, the average
accuracy of tutoring was higher for passing S1 students (M=.70, SD=.14)
than failing S1 students (M=.52, SD=0.17).
As for the students’ learning, there was a weak trend on the average normalized gain from pre- to post- favorable to passing S1 students (M=.05,
SD=0.22) than failing S1 students (M=.01, SD=0.18); t(92.3)=-0.46, p=0.65.

78

This indicates that the passing S1 students in the Philippines study learned
more by teaching than the failing S1 students although where was no significant difference of the prior knowledge among them. There might have been
difference in the way passing and failing S1 students tutored SimStudent.
The next section shows the results on analyzing process data.
4.4.2. Tutoring strategies
Since quiz items were fixed, using quiz items for tutoring could be a
good strategy to help SimStudent pass the quiz. Actually, in the USA study,
passing S4 students showed a higher percentage of using quiz problems for
tutoring (MUS = .95, SD = .11) than failing S4 students (MPH = .59, SD =
.42); t(28) = -4.08, p < 0.001.
Thus, we first investigated whether passing S1 and S2 students in the
Philippines study used more quiz items for tutoring than failing S2 students.
We found that only 47% (1826 out of 3898) problems tutored in the Philippines study were the quiz items. Philippine students did not copy quiz items
for tutoring as often as the successful (i.e., passing S4) USA students.
If time on task were a crucial factor for learning by teaching, then students who tutored on more problems should have learn more than those who
tutored on fewer problems. To test this hypothesis, we first analyzed if passing S1 students simply tutored more problems than failing S1 students. The
average number of problems tutored was 28.9±14.6 for passing S1 students
and 20.9±12.2 for failing S1 students. The difference was not statistically
significant. There was no notable difference in the number of problems tutored between passing and failing S1 students.
4.4.3. Resource usage
Did passing S1 students self-learn the materials by using resources more
than failing S1 students? When counting the number of times students referred to worked-out examples, there was actually a notable difference. The
passing S1 students referred to worked-out examples more than failing S1
students; MPassing S1 (N=52) = 164±116 vs. MFailing S1 (N=79) = 106±94;
t(93.19) = -3.00, p < 0.01.
Furthermore, passing S1 students copied more example problems for tutoring than failing S1 students; MPassing S1 = 2.2 vs. MFailing S1 = 1.4; t(111.16)
= -3.62, p < 0.001. Even when students did not actually understand how to
solve equations, they could simply copy worked-out examples line by line to
tutor SimStudent, which should have certainly affected SimStudent’s ability
to pass the quiz.
There was also a significant correlation between the number of example
problems tutored and number of times example tab were clicked; r2=0.36,
t(133)=8.67, p < 0.001—suggesting that Philippine students were actually
switching between tutoring interface and example tabs frequently when they
were copying example problems and their solutions for tutoring.
4.4.4.

Predictor of learning

79

Since there were several factors that contributed SimStudent’s and students’ learning found in the data, we conducted a regression analysis to see
how certain factors contributed to the post-test score on the procedural skill
test. The following variables were entered in the regression model: pre-test
score on the Procedural Skill Test, total number of problems tutored, total
number of quiz items tutored, total number of examples viewed, total number of example problems tutored, accuracy of tutoring, and study.
The result showed that pre-test score, accuracy of tutoring (AT), and
study were significant predictors of post-test score (PTS) on the Procedural
Skill Test. When pre-test score was centered (C.Pre), the following regression coefficients were revealed: PST = 0.21 + 0.61*C.Pre + 0.23*AT +
0.14[if US]; r2 = 0.77, F(3, 234)=267.7, p < 0.001. Since pre-test and accuracy of tutoring are highly correlated, dropping accuracy of tutoring from
the model also showed an equally good fit: PST = 0.34 + 0.63*C.Pre +
0.34[if US]; r2 = 0.76, F(2, 245) = 399.3, p < 0.001.

5.

Discussions and Concluding Remarks

We found that the prior knowledge had a strong influence on tutor learning—if students do not have sufficient prior knowledge for tutoring, they
would not benefit from tutoring as much as students who have appropriate
prior knowledge. The regression model mentioned in the results section
shows that prior knowledge is the dominating predictor of post-test score for
the Procedural Skill Test.
Nonetheless, in the Philippines study, students who managed to have
their SimStudent pass the first quiz section (i.e., the first two quiz problems)
outperformed those who failed to do so on the post-test of the Procedural
Skill Test (albeit the small effect size) even when there was no pre-test difference between passing and failing students. Students who tutored
SimStudent better learned more. The same correlation between
SimStudent’s and students’ learning was observed in previous studies [7].
These results indicate that some students had actually learned how to tutor better SimStudent via the actual tutoring interaction. We found that, in
the Philippines study, students who managed their SimStudent to pass the
first two sections of the quiz copied worked-out examples more often than
those who failed to pass the quiz. Furthermore, those passing students reviewed the worked-out examples more often than failing students. Further
investigation would be necessary to understand how to better assist students
with low prior knowledge to learn by teaching.
Learning by teaching is a promising type of learning especially when
combined with an advanced agent technologies. Yet, there are many to understand when and how students learn by teaching and how to best facilitate
their learning with various individual differences.

80

6.

Acknowledgements

The authors thank the Ateneo Laboratory for the Learning Sciences,
Marc Lester Armenta, Regina Ira Antonette M. Geli, Victoria Keiser, Gabriel Jose G. Vitug, and Evelyn Yarzebinski. We thank the Department of Science and Technology Philippine Council for Industry, Energy, and Emerging Technology Research and Development (PCIEERD) for the grant entitled, "Development of Affect-Sensitive Interfaces" and the Engineering
Research and Development for Technology (ERDT) program for the grant
entitled, "Development of an Educational Data Mining Workbench."

7.

References

1. Chin, D., et al., Preparing students for future learning with Teachable
Agents. Educational Technology Research and Development, 2010.
58(6): p. 649-669.
2. Pareto, L., et al., A Teachable-Agent Arithmetic Game's Effects on
Mathematics Understanding, Attitude and Self-efficacy, in Proceedings
of the International Conference on Artificial Intelligence in Education,
G. Biswas, et al., Editors. 2011, Springer: Heidelberg, Berlin. p. 247255.
3. Uresti, J.A.R. and B. du Boulay, Expertise, Motivation and Teaching in
Learning Companion Systems. International Journal of Artificial
Intelligence in Education, 2004. 14(2): p. 193-231.
4. Roscoe, R.D. and M.T.H. Chi, Understanding tutor learning:
Knowledge-building and knowledge-telling in peer tutors' explanations
and questions. Review of Educational Research, 2007. 77(4): p. 534574.
5. Biswas, G., et al., Measuring Self-Regulated Learning Skills through
Social Interactions in a teachable Agent Environment. Research and
Practice in Technology Enhanced Learning, 2010: p. 123-152.
6. Matsuda, N., et al., Learning by Teaching SimStudent – An Initial
Classroom Baseline Study comparing with Cognitive Tutor, in
Proceedings of the International Conference on Artificial Intelligence in
Education, G. Biswas and S. Bull, Editors. 2011, Springer: Berlin,
Heidelberg. p. 213-221.
7. Matsuda, N., et al., Cognitive anatomy of tutor learning: Lessons
learned with SimStudent. Journal of Educational Psychology, in print.
8. Matsuda, N., et al., Studying the Effect of Tutor Learning using a
Teachable Agent that asks the Student Tutor for Explanations, in
Proceedings of the International Conference on Digital Game and
Intelligent Toy Enhanced Learning (DIGITEL 2012), M. Sugimoto, et
al., Editors. 2012, IEEE Computer Society: Los Alamitos, CA. p. 25-32.

Chi, M., VanLehn, K., Litman, D., & Jordan, P. (2010). Inducing effective pedagogical strategies using learning
context features. In P. De Bra, A. Kobsa & D. Chin (Eds.), User Modeling, Adaptation and Personalization: 18th
International Conference, UMAP 2010 (pp. 147-158). Heidelberg, Germany: Springer.

Inducing Effective Pedagogical Strategies Using
Learning Context Features
Min Chi1 , Kurt VanLehn2 , Diane Litman3 and Pamela Jordan4
1

2

4

Machine Learning Department, Carnegie Mellon University, PA, 15213 USA
minchi@cs.cmu.edu
School of Computing and Informatics, Arizona State University, AZ, 85287 USA
Kurt.Vanlehn@asu.edu
3
Department of Computer Science, University of Pittsburgh, PA, 15260 USA
litman@cs.pitt.edu
Department of Biomedical Informatics, University of Pittsburgh, Pittsburgh, PA
15260 pjordan@pitt.edu

Abstract. Effective pedagogical strategies are important for e-learning
environments. While it is assumed that an effective learning environment should craft and adapt its actions to the user’s needs, it is often
not clear how to do so. In this paper, we used a Natural Language Tutoring System named Cordillera and applied Reinforcement Learning
(RL) to induce pedagogical strategies directly from pre-existing human
user interaction corpora. 50 features were explored to model the learning
context. Of these features, domain-oriented and system performance features were the most influential while user performance and background
features were rarely selected. The induced pedagogical strategies were
then evaluated on real users and results were compared with pre-existing
human user interaction corpora. Overall, our results show that RL is a
feasible approach to induce effective, adaptive pedagogical strategies by
using a relatively small training corpus. Moreover, we believe that our
approach can be used to develop other adaptive and personalized learning environments.

1

Introduction

Natural Language (NL) Tutoring Systems are a form of Intelligent Tutoring Systems (ITSs) that use natural dialogue for instructional purposes such as helping
students to learn a subject by engaging in a natural language conversation.
Why2-Atlas and Why2-AutoTutor [17], for example, are NL tutoring systems
that teach students conceptual physics. One central component of NL Tutoring
Systems is the dialogue manager, which uses dialogue strategies to decide what
action to take at each point during the tutorial dialogue. For tutoring systems,
dialogue strategies are also referred to as pedagogical strategies.
It is commonly believed that an effective tutoring system would craft and
adapt its actions to the students’ needs based upon their current knowledge
level, general aptitude, and other salient features [4]. However, most pedagogical strategies for ITSs are encoded as hand-coded rules that seek to implement

cognitive and/or pedagogical theories. Typically, the theories are considerably
more general than the specific decisions that designers must make, which makes
it difficult to tell if a specific pedagogical strategy is consistent with the theory.
Moreover, it is often not easy to empirically evaluate these decisions because the
overall effectiveness of the system depends on many factors, such as the usability of the system, how easily the dialogues are understood, and so on. Ideally,
several versions of a system are created, each employing a different pedagogical
strategy. Data is then collected with human subjects interacting with these different versions of the system and the results are compared. Due to the high cost
of experiments, only a handful of strategies are typically explored. Yet, many
such other reasonable ones are still possible.
In recent years, work on the design of NL non-tutoring Dialogue Systems
has involved an increasing number of data-driven methodologies. Among these,
Reinforcement Learning (RL) has been widely applied [13]. RL is a machine
learning method that centers on the maximization of expected rewards. It has
many features well-suited to the problem of designing the dialogue manager
such as unobservable states, delayed rewards, and so on. Its primary advantage
is its ability to compute an optimal policy within a much larger search space,
using a relatively small training corpus. In this work, rather than implementing
pedagogical strategies drawn from human experts or theories, we applied RL to
derive pedagogical strategies using pre-existing interactivity data.
While most previous work on using RL to train non-tutoring dialogue systems
has been successful [13], whether it can be used to improve the effectiveness of NL
tutoring systems is still an open question. One major source of uncertainty comes
from the fact that the rewards used in RL are much more delayed in NL tutoring
systems than those in non-tutoring dialogue systems. Much of this work in NL
non-tutoring Dialogue Systems is focused on systems that obtain information
or search databases such as querying bus schedules [11]. For example, in nontutoring Systems like the train scheduler, the interaction time is often less than 20
minutes, and the number of interactions within user-dialogue systems is generally
less than 20 turns [13]. In the training corpora reported here, the time is roughly
4-9 hours and the number of interactions is about 280 turns. More immediate
rewards are more effective than more delayed rewards for RL induction. This is
because the issue of assigning credit for a decision, attributing responsibility to
the relevant decision is substantially easier in the former case. The more we delay
rewards, the more difficult it becomes to identify the decision(s) responsible for
our success or failure. Additionally, to train an RL model, a large amount of
data is generally needed. In this work, we use human data only instead of data
from simulators as in applying RL in non-tutoring dialogue systems. This is
because the cause of human learning is still an open question and thus it would
be difficult to accurately simulate students’ responses to the tutor and simulate
how students would learn. Given the high cost of collecting human data, we were
more likely to encounter the issue of data sparsity.
For RL, as with all machine learning tasks, success is dependent upon an
effective state representation or state model. An effective state representation

should be an accurate and compact model of the learning context. Compared
with non-tutoring Dialogue Systems, where success is primarily a function of
communication efficiency, communication efficiency is only one of the factors determining whether a student learns well from an NL tutoring system. Moreover,
the other factors are not well understood, so to be conservative, states need to
contain features for anything that is likely to affect learning. Hence, state models
for RL applications to tutoring systems tend to be much larger than state models
for non-tutoring applications. Unfortunately, as states increase in size and complexity, we risk making the learning problem intractable or the decision space
too large to sample effectively. In order to obtain an effective state model that
both minimizes state size while retaining sufficient relevant information about
the learning context, we began with a large set of features to which we applied a
series of feature-selection methods in order to reduce them to a tractable subset.
Before describing our approach in detail, we will briefly describe the two types
of tutorial decisions covered by the induced pedagogical policies.

2

Two Types of Tutorial Decisions

Among the many tutorial decisions that must be made, we focus on two types of
decisions, Elicit/Tell (ET) and Justify/Skip-Justify (JS). The ET decision asks
“should the tutor elicit the next problem-solving step from the student, or should
he or she tell the student the next step directly?”. For example, when the next
step is to select a principle to apply and the target principle is the “definition of
Kinetic Energy’, the tutor can choose to elicit this from the student by asking
the question, “Which principle will help you calculate the rock’s kinetic energy
at T0?” By contrast, the tutor can elect to tell the student the step by stating,
“To calculate the rock’s kinetic energy at T0, let’s apply the definition of Kinetic
Energy.” The JS decision asks “should the tutor include a justify for a step just
taken or or not”. For example, after deciding to use the “definition of Kinetic
Energy”, the tutor can choose to ask the student why the principle is applicable
or to skip to ask. There is no widespread consensus on how or when any of these
actions should be taken [5, 3, 6, 8]. This is why our research objective is to derive
policies for them from empirical data.

3

Applying RL to Induce Pedagogical Strategies

Previous research on using RL to improve non-tutoring dialogue systems (e.g.
[12]) has typically used Markov Decision Processes (MDPs) [14] to model dialogue data. The central idea behind this approach is to transform the problem
of inducing effective pedagogical strategies into computing an optimal policy
for an agent that is choosing actions in an MDP. An MDP formally corresponds to a 4-tuple (S, A, T, R), in which: S = {S1 , · · · , Sn } is a state space;
A = {A1 , · · · , Am } is an action space represented by a set of action variables;
T : S × A × S → [0, 1] is a set of transition probabilities P (Sj |Si , Ak ), which is
the probability that the model would transition from state Si to state Sj after

the agent takes action Ak ; R : S × A × S → R assigns rewards to state transitions. Finally, π : S → A is defined as a policy, which determines which action
the agent should take in each state in order to maximize the expected reward.
The set of possible actions, A, is small and well-defined. In our application,
we have A = {Elicit, T ell} for inducing pedagogical strategies on ET decisions
and A = {Justif y, Skip − Justif y} for inducing those on JS decisions. The set
of possible states, S, however is not well-defined in advance and can potentially
be astronomically large if we include everything that could possibly influence
the effectiveness of a tutorial action. In this study, we assumed that S is the
Cartesian product of a set of state features F = {F1 , · · · , Fp } and our challenge
now becomes finding a set of features F to model the state or learning context
compactly and yet effectively. Features must be operational, in that there is
some way to determine their value prior to just before each tutor action in the
dialogue. For instance, one operational feature would be a count of the number
of words uttered by the student since the last tutor turn.
Each student-system interaction dialogue d can be viewed as a trajectory in
the chosen state space determined by the system actions and student responses:

A1 ,R1

A2 ,R2

An ,Rn

S1 −−−−→ S2 −−−−→ · · · Sn −−−−→
Ai ,Ri

Here Si −−−−→ Si+1 indicated that at the ith turn in the tutorial dialogue
d, the system was in state Si , executed action Ai , received reward Ri , and then
transferred into state Si+1 . Because our primary interest is to improve students’
learning, we used Normalized Learning Gain (NLG) as the reward because it
measures students’ gain irrespective of their incoming competence. The NLG
. Here posttest and pretest refer to the
is defined as: N LG = posttest−pretest
1−pretest
students’ test scores before and after the training respectively; and 1 is the
maximum score. Given that a student’s NLG will not be available until the
entire tutorial dialogue is completed, only terminal dialogue states have nonzero rewards. Thus for a tutorial dialogue d, R1 · · · , Rn−1 are all equal to 0 and
only the final reward equal to the student’s N LG × 100, which is in the range
of (-∞, 100].
Once the MDP structure {S, A, R} has been defined, the transition probabilities T are estimated from the training corpus, which is the collection of
,m
dialogues, as: T = {p(Sj |Si , Ak )}k=1,···
i,j=1,··· ,n . More specifically, p(Sj |Si , Ak ) is calculated by taking the number of times that the dialogue is in state Si , the tutor
took action Ak , and the dialogue was next in state Sj divided by the number
of times the dialogue was in Si and the tutor took Ak . The reliability of these
estimates clearly depends upon the size and structure of the training data. Once
a complete MDP is constructed, a dynamic programming approach can be used
to learn the optimal control policy π ∗ and here we used the toolkit developed by
Tetreault and Litman [15]. The rest of this section presents a few critical details
of the process, but many others must be omitted to save space.

3.1

Knowledge Component (KC) Based Pedagogical Strategies

In the learning literature, it is commonly assumed that relevant knowledge in
domains such as math and science is structured as a set of independent but
co-occurring Knowledge Components (KCs) and that KC’s are learned independently. A KC is “a generalization of everyday terms like concept, principle, fact,
or skill, and cognitive science terms like schema, production rule, misconception,
or facet” [16]. For the purposes of ITSs, these are the atomic units of knowledge.
The domain selected for this project is a subset of the physics work-energy
domain, which is characterized by eight primary KCs. For instance, one KC is
the definition of kinetic energy (KE = 12 ∗ m ∗ v 2 ) and another is the definition
of gravitational potential energy (GP E = m ∗ g ∗ h). It is assumed that a tutorial
dialogue about one KC (e.g., kinetic energy) will have no impact on the student’s
understanding of any other KC (e.g, of potential energy). This is an idealization,
but it has served ITS developers well for many decades, and is a fundamental
assumption of many cognitive models [1, 10]
When dealing with a specific KC, the expectation is that the tutor’s best
policy for teaching that KC (e.g., when to Elicit vs, when to Tell) would be based
upon the student’s mastery of the KC in question, its intrinsic difficulty, and
other relevant, but not necessarily known, factors specific to that KC. In other
words, an optimal policy for one KC might not be optimal for another. Therefore,
one assumption made in this paper is that inducing pedagogical policies specific
to each KC would be more effective than inducing an overall KC-general policy.
In order to learn a policy for each KC, we annotated our tutoring dialogues
and action decisions with the KCs covered by each action. For each KC, the
final kappa was ≥ 0.77, which is fairly high given the complexity of the task.
Additionally, a domain expert also mapped the pre-/post test problems to the
sets of relevant KCs. This resulted in a KC-specific NLG score for each student.
Thus, for the decision of when to Elicit vs. Tell about the definition of kinetic
energy KC20 , we consider all and only the dialogue about that KC and consider
only the learning gains on that KC.
Given these independence assumptions, the overall problem of inducing a
policy for ET decisions and a policy for JS decisions is decomposed into 8 subproblems of each kind, one per KC. Among the eight KCs, KC 1 does not arise in
any JS decisions and thus only an ET policy was induced for it. For each of the
remaining seven KCs, a pairs of policies, one ET policy and one JS policy, were
induced. So we induced 15 KC-based NormGain policies. During the tutoring
process, there were some decision steps that did not involve any of the eight
primary KCs. For them, two KC-general policies, an ET policy and a JS policy,
were induced. To sum, a total of 17 NormGain policies were induced in this
study.
3.2

Training Corpora

In order to apply RL to induce pedagogical strategies and evaluate the induced
strategies, we used Cordillera. Cordillera is a NL tutoring system that teaches

introductory physics[16]. To reduce confounds due to imperfect NL understanding, the NL understanding module was replaced with human wizards whose only
task is to match students’ answers to the closest response from a list of potential
responses and they cannot make the tutorial decisions. As the first step, we developed an initial version of Cordillera, called random-Cordillera on which both
ET and JS decisions on it were made randomly. 64 college students were then
trained on random-Cordillera in 2007 and the collected training data is called
the Exploratory corpus.
From the Exploratory corpus, we tried our first round of policy induction. It
is done by first defining 17 state features and then used some sort of greedy-like
procedure to search for a small subset of it as the state representation. For the
reward functions, we had dichotomized the NLGs scores so that there were only
two levels of reward and thus the derived policies were named DichGain policies.
We next tested our hypothesis that these RL-induced policies would improve the
effectiveness of a tutoring system. The version of Cordillera that implemented
the DichGain policies was named DichGain-Cordillera. Except following the policies (random vs. DichGain), the remaining components of Cordillera, including
the GUI interface, the same training problems, and the tutorial scripts, were
left untouched. DichGain-Cordillera’s effectiveness was tested by training a new
group of 37 college students in 2008. Results showed that although the DichGain
policies generated significantly different patterns of tutorial decisions than the
random policy, no significant difference was found between the two groups on
the pretest, posttest, or the NLGs.
3.3

Inducing NormGain Strategies

Although the previous experiment seemingly failed to confirm our hypothesis,
it did generate more training data. We now have three training corpora: the
Exploratory corpus in 2007, the DichGain corpus in 2008, and a combined training corpus dataset consisting of the 101 dialogues from both the Exploratory
and the DichGain corpora. This time we started with a larger set of possible
state features. We included 50 features based upon six categories of features
considered by previous research [9, 2, 7] to be relevant. They include not only
student’s performance and background related features such as student’s overall performance but also domain-oriented and system behavior related features.
Moreover, we explored more domain -general methods of searching the power
set of the 50 features and instead of dichotomizing learning gains as rewards, we
used the N LG × 100 directly. Based on the reward function, the induced policies
are named normalized Gain (NormGain) policies in the following.
Fig. 1 shows an example of a learned NormGain policy on KC20 , “Definition
of Kinetic Enegy”, for JS decisions. The policy involves five features. They are:
TimeInSession: The total time spent in the current session. This feature reflects a
student’s fatigue level.
nKCs: The number of times the present KC has occurred in the current dialogue.
This feature reflects the students’ familiarity with the current KC.

pctElicit: The percentage of ET decisions turned out to be elicit during the dialogue.
This feature reflects how active a student is overall.
stuAverageWords: The average number of words per student turn. This reflects the
student’s level of activity and verbosity.
stuAverageConceptSession: The ratio of the number of the student’s turns which
involves at least one physics concept to all the student turns in this session. This
feature reflects how often the student’s answers involved at least one physics concepts since the start of the training.

[Feature:]
TimeInSession:
[0, 3040.80) → 0; [3040.8, ∞] → 1
nKCs:
[0, 66) → 0;
[66, ∞] → 1
pctElicit:
[0, 0.49) → 0;
[0.49, 1) → 1
stuAverageWords:
[0, 4.18) → 0;
[4.18, ∞] → 1
stuAverageConceptSession: [0, 0.29) → 0;
[0.29, 1] → 1
[Policy:]
Justify:
0:0:0:0:0 0:0:1:1:0 0:1:0:0:1 0:0:1:0:0 0:1:0:1:1 0:1:1:0:0 0:1:1:0:1 0:1:1:1:0
0:1:1:1:1 1:0:0:0:0 1:0:0:1:0 1:0:1:0:0 1:0:1:0:1 1:0:1:1:0 1:0:1:1:1 1:1:0:0:1
1:1:1:0:0 1:1:1:0:1 1:1:1:1:0 1:1:1:1:1
Skip-Justify:
0:0:0:0:1 0:0:0:1:0 0:0:0:1:1 0:0:1:0:1 0:0:1:1:1 0:1:0:0:0 0:1:0:1:0 1:0:0:0:1
1:0:0:1:1 1:1:0:0:0 1:1:0:1:0 1:1:0:1:1
Fig. 1. An NormGain Policy on KC20 For JS Decisions

MDP generally requires discrete features and thus all the continous features
need to be discretized. Fig. 1 describes how each of the five features was discretized. For example, for TimeInSession, if its value is above 3040.80 sec (50.68
min), it is 1 otherwise, it is 0. There were a total of 32 rules learned: in 20 situations the tutor should execute the justification step, in the other 12 situations
the tutor should skip. For example, 0:0:0:0:0 is listed as the first situation under
the [Justify], it means that when the student has spend less than 50.68 min in
this session, the occurrence of KC20 in the student’s dialogue history is less than
66, the student has got less than 49% of elicit in the past, the average number
of words in student’s entries is less than 4.18 words, and the percentage of times
times that the student mention a physics concept in his/her turn is less than
29%, then the tutor should execute the justification. As you can see, the RL
induced policies are very subtle and adaptive to the learning context and they
are not like most of the tutorial tactics derived from analyzing human tutorial
dialogues.
The resulting 17 NormGain policies were implemented back into Cordillera
yielding a new version of the system, named NormGain-Cordillera. In order to

test our hypothesis that RL can be used to improve tutoring systems, we tested
the effectiveness of NormGain-Cordillera on a new group of students as described
in the next section. The section is written as if one large experiment was done
with 3 conditions, when in fact the 3 groups of students were run sequentially,
as described above.

4

Methods

The purpose of this experiment is to compare the learning gains of students using
random-Cordillera, DichGain-Cordillera and NormGain-Cordillera respectively.
All participants were required to have basic knowledge of high-school algebra,
no experience with college-level physics, and were paid for their time. Each
participant took between six and fourteen hours (3-7 sessions) to finish the study
in a period of two to three weeks. Each session typically lasted about two hours.
The domain selected here is Physics work-energy domain as covered in a firstyear college physics course. The eight primary KCs were: the weight law (KC1),
definition of work (KC14), Definition of Kinetic Energy (KC20), Gravitational
Potential Energy (KC21), Spring Potential Energy (KC22), Total Mechanical
Energy (KC24), Conservation of Total Mechanical Energy (KC27), and Change
of Total Mechanical Energy (KC28).
All three groups experienced the identical procedure and materials. More
specifically, participants all completed a background survey; read a textbook
covering the target domain knowledge; took a pretest; solved the same seven
training problems in the same order on Cordillera; and finally took a posttest.
The pretest and posttest were identical.
Only three salient differences existed across the three groups:
1. The Exploratory group with a population of 64 was recruited in 2007; the
DichGain group with a population of 37 was recruited in 2008; and the
NormGain group with a population of 29 was recruited in 2009.
2. Random-Cordillera made random decisions and the DichGain-Cordillera and
NormGain-Cordillera followed the induced DichGain and NormGain policies
respectively.
3. A group of six human wizards were used by the Exploratory and DichGain
groups; but only one of six wizards were involved in the NormGain group.
4.1

Grading

All tests were graded by a single experienced grader who did not know which
student belonged to which group. For all identified relevant KCs in a test question, a KC-based score for each KC application was given. We assigned an overall
competence to a student by the sum of these KC-based scores and normalizing
to a [0,1] interval. We also tried other methods of computing an overall score,
and this did not affect the pattern of results discussed below.

5

Results

The primary goal reported below is twofold: first, to test whether our improved
RL methodology and software produced more effective pedagogical strategies
than either random policies or the policies used by the DichGain group; and
second, to determine the features selected in the state models in the NormGain
policies.
5.1

Learning Results

A one-way ANOVA showed that there were no significant differences among the
three groups on overall training time: F (2, 122) = 1.831, p = .17. After solving
seven training problems on Cordillera, all three groups scored significantly higher
in the posttest than pretest: F (1, 126) = 10.40, p = 0.002 for the Exploratory
group, F (1, 72) = 7.20, p = 0.009 for the DichGain group, and F (1, 56) = 32.62,
p = 0.000 for the NormGain group respectively. The results suggested that the
basic practices and problems, domain exposure, and interactivity of Cordillera
might cause students to learn even from tutors with non-optimal pedagogical
skills.
A one-way ANOVA was used for comparing the learning performance differences among the three groups. While no significant pre-test score differences were
found: F (2, 127) = 0.53, p = 0.59, there were significant differences among the
three groups on both post-test scores and NLG scores: F (2, 127) = 5.16, p = .007
and F (2, 127) = 7.57, p = 0.001 respectively. Figure 2 compares the three groups
on the pre-test, post-test, and NLG scores. Moreover, a t-test comparison showed
that the NormGain group out-performed the DichGain on both post-test scores
and NLG scores: t(64) = 3.28, p = .002, d 5 = 0.82 and t(64) = 3.68, p =
0.000, d = 0.95 respectively. Similar results were found between the NormGain
and Exploratory groups: t(91) = 2.76, p = .007, d = 0.63 on post-test, and
t(91) = 3.61, p = 0.000, d = 0.84 on NLG scores respectively.
To summarize, the comparison among the three groups shows that the NormGain group significantly outperformed both the Exploratory and DichGain
groups. These results were consistent both for the post-test scores and the NLGs
and the effect sizes were large by Cohen’s d criteria.
5.2

Feature Choices in INDUCED POLICIES

Only 30 out of 50 defined features occurred among the 17 NormGain policies.
Among them, the most frequent feature appeared seven times. Four features
appeared in more than three induced policies and they are:
StepDifficulty (7 Occurrences): which encodes a step’s difficulty level and its value
is roughly estimated from the Combined Corpus based on the percentage of answers
that were correct on the step.
5

Cohen’s d, which is defined as the mean learning gain of the experimental group
minus the mean learning gain of the control group, divided by the groups’ pooled
standard deviation.

Fig. 2. Compare Three Groups Learning Performance under Overall Grading

ConceptToWordRatio (5 Occurences): which represents the ratio of the physics
concepts to words in the tutor’s dialogue.
NumberTellsSinceElicit (5 Occurences): which represents the number of tells the
student has received since the last elicit.
TimeBetweenDecisions (4 Occurences): which represents the time since the last
tutorial decision was made on the current KC.

While StepDifficulty can be seen as domain-oriented feature, the remaining
three features are all the system-behavior related features. The high occurrence
of StepDifficulty in the NormGain policies is not very surprising because it has
been widely believed that difficulty level is an important factor for the system to
behave adaptively and effectively. The frequent involvement of System-behavior
related features in the induced policy maybe because these features might reflect
student’s general aptitude, the activeness of their knowledge on a specific KC,
and so on. For example, NumberTellsSinceElicit reflects how interactive a student has been recently and TimeBetweenDecisions reflect how active a student’s
knowledge on the current KC is. When TimeBetweenDecisions is high, it means
that the tutor has not mentioned the KC recently so the student’s knowledge on
the current KC may be still or forgotten.
Much to our surprise, the features related to the students’ overall or recent performance (e.g., error rate) and background (e.g., MSAT, VSAT, gender,
pretest score) appeared the least or none in the NormGain policies. Although
space does not permit a detailed discussion of the prevalence of features, it appears to be a mixture of easily anticipated dependencies (e.g., step difficulty)
and a few surprises (why doesn’t error rate matter?).

6

Conclusions

We presented a general data-driven method that can be used to improve NL
tutoring system over time. We built and improved a large NL tutoring system
using our methodology, and showed that RL is able to effectively search a very
large continous space of dialogue policies (After discretized, the space is ≥ 250 in
size) using a relatively small amount of training dialogue data (64 subjects in Exploratory group and 37 in the DichGain group). A post-hoc comparison showed
that our learned policy outperformed both sets of training policies in terms of
learning performance. This success supports the hypothesis that RL-induced
rules are effective and that the approach taken in this project was a feasible
one. However, inducing effective tutorial tactics was not trivial. The DichGain
tutorial tactics did not seem to be more effective than the random decisions in
Random-Cordillera. A number of factors were changed in deriving NormGain
policies from the process of inducing DichGain policies. These included the feature choices, the choice of training corpora, feature selection methods, and so
on. So it is still not clear which factor or factors caused a change in effectiveness.
Although the discussion of induced features has been cursory, it nonetheless
appears that the learning context features that make the most difference for
determining when to Tell vs. Elicit and when to Justify vs. Skip-Justify are not
always the ones that one would first think of given current theories of learning
and tutoring. For instance, it is widely believed that effective tutors adapt their
behavior to the individual student knowledge level. However, such feature did not
appear in the NormGain policies. Indeed, individualized tutoring is considered
a Grand Challenge by the National Academy of Engineering. However, such
features appeared to play little role in the effective tutorial policies induced from
our data. Overall, our results suggested that when building an accurate learning
context model, adding domain-oriented and the system behavior related features
would be beneficial.

7

Acknowledgments

NSF (#0325054) supported this work and NSF (#SBE-0836012) supported its
publication. We thank Collin Lynch and the reviewers for helpful comments.

References
1. John R. Anderson. The architecture of cognition. Cambridge, Mass. : Harvard
University Press, 1983.
2. Joseph Beck, Beverly Park Woolf, and Carole R. Beal. Advisor: A machine learning
architecture for intelligent tutor construction. In AAAI/IAAI, pages 552–557.
AAAI Press / The MIT Press, 2000.
3. Michelene T. H. Chi, Nicholas de Leeuw, Mei-Hung Chiu, and Christian LaVancher.
Eliciting self-explanations improves understanding. Cognitive Science, 18(3):439–
477, 1994.

4. Michelene T. H. Chi, Stephanie A. Siler, Heisawn Jeong, Takashi Yamauchi, and
Robert G. Hausmann. Learning from human tutoring. Cognitive Science, 25:471–
533, 2001.
5. Allan Collins, John Seely Brown, and Susan E. Newman. Cognitive apprenticeship:
Teaching the craft of reading, writing and mathematics. In L. B. Resnick, editor,
Knowing, learning and instruction: Essays in honor of Robert Glaser, chapter 14,
pages 453–494. Lawrence Erlbaum Associates: Hillsdale New Jersey, 1989.
6. Christina Conati and Kurt VanLehn. Toward computer-based support of metacognitive skills: a computational framework to coach self-explanation. International
Journal of Artificial Intelligence in Education, 11:398–415, 2000.
7. Katherine Forbes-Riley, Diane J. Litman, Amruta Purandare, Mihai Rotaru, and
Joel R. Tetreault. Comparing linguistic features for modeling learning in computer
tutoring. In Rosemary Luckin, Kenneth R. Koedinger, and Jim E. Greer, editors,
AIED, volume 158 of Frontiers in Artificial Intelligence and Applications, pages
270–277. IOS Press, 2007.
8. Sandra Katz, Gabriel ODonnell, and Heather Kay. An approach to analyzing
the role and structure of reflective dialogue. International Journal of Artificial
Intelligence and Education, 11:320–343, 2000.
9. Johanna D. Moore, Kaska Porayska-Pomsta, Sebastian Varges, and Claus Zinn.
Generating tutorial feedback with affect. In Valerie Barr and Zdravko Markov,
editors, FLAIRS Conference. AAAI Press, 2004.
10. Allen Newell, editor. Unified Theories of Cognition. Harvard University Press;
Reprint edition, 1994.
11. Antoine Raux, Brian Langner, DDan Bohus, Alan W Black, and Maxine Eskenazi.
Let’s go public! taking a spoken dialog system to the real world. In Proceedings of
Interspeech (Eurospeech), 2005.
12. Satinder P. Singh, Michael J. Kearns, Diane J. Litman, and Marilyn A. Walker.
Reinforcement learning for spoken dialogue systems. In Sara A. Solla, Todd K.
Leen, and Klaus-Robert Müller, editors, NIPS, pages 956–962. The MIT Press,
1999.
13. Satinder P. Singh, Diane J. Litman, Michael J. Kearns, and Marilyn A. Walker.
Optimizing dialogue management with reinforcement learning: Experiments with
the njfun system. J. Artif. Intell. Res. (JAIR), 16:105–133, 2002.
14. Richard S. Sutton and Andrew G. Barto. Reinforcement Learning. MIT Press
Bradford Books, 1998.
15. Joel R. Tetreault and Diane J. Litman. A reinforcement learning approach to evaluating state representations in spoken dialogue systems. Speech Communication,
50(8-9):683–696, 2008.
16. Kurt VanLehn, Pamela W. Jordan, and Diane Litman. Developing pedagogically
effective tutorial dialogue tactics: Experiments and a testbed. In Proceedings of
SLaTE Workshop on Speech and Language Technology in Education ISCA Tutorial
and Research Workshop., 2007.
17. Kurt VanLehn, Pamela W. Jordan, Carolyn Penstein Rosé, Dumisizwe Bhembe,
and et al. The architecture of why2-atlas: A coach for qualitative physics essay
writing. In Stefano A. Cerri, Guy Gouardères, and Fábio Paraguaçu, editors,
Intelligent Tutoring Systems, volume 2363 of Lecture Notes in Computer Science,
pages 158–167. Springer, 2002.

Computers & Education 75 (2014) 196–217

Contents lists available at ScienceDirect

Computers & Education
journal homepage: www.elsevier.com/locate/compedu

Evaluation of a meta-tutor for constructing models of
dynamic systems
Lishan Zhang*, Kurt VanLehn, Sylvie Girard, Winslow Burleson,
Maria Elena Chavez-Echeagaray, Javier Gonzalez-Sanchez, Yoalli Hidalgo-Pontet
Arizona State University, Computing, Informatics, and Decision Systems Engineering, Tempe, AZ 85281, USA

a r t i c l e i n f o

a b s t r a c t

Article history:
Received 5 September 2013
Received in revised form
17 February 2014
Accepted 25 February 2014
Available online 13 March 2014

Modelling is an important skill to acquire, but it is not an easy one for students to learn. Existing
instructional technology has had limited success in teaching modelling. We have applied a recently
developed technology, meta-tutoring, to address the important problem of teaching model construction.
More speciﬁcally, we have developed and evaluated a system that has two parts, a tutor and a meta-tutor.
The tutor is a simple step-based tutoring system that can give correct/incorrect feedback on student’s
steps and can demonstrate steps for students when asked. Because deep modelling requires difﬁcult
analyses of the quantitative relationships in a given system, we expected, and found, that students
tended to avoid deep modelling by abusing the tutor’s help. In order to increase the frequency of deep
modelling, we added a meta-tutor that coached students to follow a learning strategy that decomposed
the overall modelling problem into a series of “atomic” modelling problems. We conducted three experiments to test the effectiveness of the meta-tutor. The results indicate that students who studied with
meta-tutor did indeed engage in more deep modelling practices. However, when the meta-tutor and
tutor were turned off, students tended to revert to shallow modelling. Thus, the next stage of the
research is to add an affective agent that will try to persuade students to persist in using the taught
strategies even when the meta-tutoring and tutoring have ceased.
Ó 2014 Elsevier Ltd. All rights reserved.

Keywords:
Meta-tutor
Gaming the system
Intelligent tutoring systems
Modelling
Learning strategies

1. Introduction
This paper reports progress on two research problems: (1) teaching students how to construct mathematical models of dynamic systems,
and (2) teaching students to use effective learning strategies. Both research problems have long histories, which are covered in the next few
sections.
1.1. A brief history of educational uses of system dynamics modelling
There are two distinct reasons why students should learn model construction. First, modelling is an important cognitive skill in itself. The
Common Core State Standards for Mathematics (CCSSO, 2011) considers modelling to be one of 7 essential mathematical practices that
should be taught at all grade levels. The Next Gen standards for science instruction (National, 2012) also have 7 strands that are threaded
throughout the standards, and modelling is one of them.
Second, modelling is widely believed to be an important method for learning domain knowledge. For instance, modelling has been
claimed to help in achieving a deep understanding of scientiﬁc systems, economic systems and other systems (Chin et al., 2010; Metcalf,
Krajcik, & Soloway, 2000; Stratford, 1997), removing misconceptions and making of conceptual changes (Booth Sweeney & Sterman,
2000; Bredeweg & Forbus, 2003; Hestenes, 2007; Lee, Jonassen, & Teo, 2011; Mandinach & Cline, 1994b; Wilensky, 2003; Wilensky &
* Corresponding author.
E-mail addresses: lishan.zhang@asu.edu (L. Zhang), kurt.vanlehn@asu.edu (K. VanLehn), sylvie.girard@asu.edu (S. Girard), winslow.burleson@asu.edu (W. Burleson),
mchaveze@asu.edu (M.E. Chavez-Echeagaray), javiergs@asu.edu (J. Gonzalez-Sanchez), yhidalgo@asu.edu (Y. Hidalgo-Pontet).
http://dx.doi.org/10.1016/j.compedu.2014.02.015
0360-1315/Ó 2014 Elsevier Ltd. All rights reserved.

L. Zhang et al. / Computers & Education 75 (2014) 196–217

197

Reisman, 2006), understanding the epistemology of models in science (Treagust, Chittleborough, & Mamiala, 2002), and developing intuitions, predilections and skills at understanding complex phenomena in general (Hogan & Thomas, 2001; Mandinach & Cline, 1994a;
Schecker, 1993; Steed, 1992).
In short, modelling is both an important cognitive skill and a potentially powerful means of learning many topics. That is, it is both an end
goal and a means to other end goals.
The modelling activity addressed here is traditionally called system dynamics modelling (see Collins and Ferguson (1993) for a particularly comprehensive taxonomy of modelling). The model is comprised of real-valued variables that are constrained by temporal differential
equations. The variables denote quantities in the system whose values change over time. The job of the model is to predict those changing
values as accurately as parsimony allows.
There is a long history of using system dynamics model construction as in instructional activity. According to the oral history of the
System Dynamics Society (http://www.systemdynamics.org/oral-history/), system dynamics began to be used for university instruction
around 1957 with Jay Forrester’s formulation of system dynamics for teaching management. When a graphical language, Stella, became
available (Richmond, 1985), instructional usage dramatically increased and extended to high school. Many early Stella projects trained
teachers in modelling and let them invent activities (Mandinach & Cline, 1994a; Zaraza & Fisher, 1997).
After years of experience by hundreds of teachers, observers began to report that getting students to actually construct models took so
much class time that most teachers used Stella only for model exploration activities, wherein students were given a model and were asked
to observe how graphs of the variables values changed as the students manipulated parameters of the model (Alessi, 2000; Doerr, 1996;
Mandinach & Cline, 1994b; Stratford, 1997).
Laboratory studies conﬁrmed the observers’ reports about both the length of time required for model construction and the importance of
model construction. For example, Hashem and Mioduser (2011) found that students who constructed NetLogo models learned more about
emergence, self-organization and other complex system concepts than students who explored NetLogo models that were given to them. The
comparison took place during two 90-min lessons on complex systems, which were ﬂanked by a pre-test and a post-test. However, prior to
the pre-test, it took only 2 h to train the model exploration group whereas it took 48 h to train the model construction group. In a review of
the modelling literature, VanLehn (2013) found that the only experiments that produced reliable positive results for model construction also
devoted at least 5 h to training the students before the main lessons.
This history motivates the speciﬁc research problem addressed here: How can we speed up students’ acquisition of skill in constructing
system dynamics models?
Many methods for accelerating the acquisition of skill in model construction have been implemented, but only a few have been
compared to baseline versions of the model construction activity in order to test their effectiveness (see VanLehn, 2013, for a review). Of
those that have been evaluated, one form of scaffolding has shown considerable promise: The use of feedback and hints on student’s
steps in constructing the model. A whole model is usually composed of many parts (e.g., nodes, links, equations, labels, icons, numbers,
etc.) which the student enters one at a time. Entering such a part is called a “step”. Systems that give feedback and hints on steps are
called step-based tutoring systems (VanLehn, 2006). Step-based tutoring systems have been used for a wide variety of tasks besides
model construction, and appear to be almost as effective as human tutors (VanLehn et al., 2011). Thus, this project decided early on to
build a step-based tutoring system for system dynamics modelling in the hope that it would accelerate students’ acquisition of
modelling skill.
1.2. Learning strategies research
A learning strategy is a process, procedure or method that meets two criteria (Donker, de Boer, Kostons, Dignath van Ewijk, & van der
Werf, 2014): (1) Students can use the strategy when studying, but it is not required by the material that they are studying. (2) Using the
learning strategy is believed to affect the student’s learning. A good learning strategy is thought to improve students’ learning, while a poor
learning strategy is thought to harm the students’ learning. When used without modiﬁcation, “learning strategy” generally means a good
learning strategy. Some examples are:
 When memorizing facts, a good learning strategy is to construct a mental image and associate each fact with a part of the image.
 When studying an example, a good learning strategy (called self-explanation) is to explain each step in the example to yourself, asking
“Why is this true? Why did the author include this step?”
 When reading a text, a good learning strategy is to reﬂect afterwards on what you have learned.
 When reading a text, a poor learning strategy is to ignore words or passages that you don’t understand.
Learning strategies have been studied for decades, and comprehensive meta-analytic reviews exist (Donker et al., 2014; Hattie, Biggs, &
Purdie, 1996). Some of the main ﬁndings are:
A. Students often exhibit poor learning strategies.
B. Good learning strategies can be taught, often with little difﬁculty.
C. When students use the taught learning strategies, their domain learning often increases compared to students who are not taught to use
the learning strategies.
D. When instruction in the learning strategy includes meta-cognitive and motivational components, students can often be prevented from
reverting to poor learning strategies when the instruction ceases.
E. Speciﬁc learning strategies often have larger effect sizes than general purpose ones.
These ﬁndings mean that research on learning strategies is intimately linked to advances in instruction. Whenever a new instructional
method or subject matter is developed, there are likely to be poor learning strategies that are speciﬁc to it (ﬁnding A) as well as speciﬁc
good learning strategies that are likely to be effective (ﬁnding E) and easily taught (B). The key question is whether they are effective for

198

L. Zhang et al. / Computers & Education 75 (2014) 196–217

domain learning (C) and students can be persuaded to continue using them after being taught (D). We have developed a step-based
tutoring system for teaching students how to construct models, so it is likely that for this new form of instruction, students tend to
exhibit poor learning strategies, but that good strategies can be easily taught; the main questions are whether the good strategies really
are effective at increasing domain learning and whether students can be taught to persist in using them. These are the main research
questions addressed here.
The next introductory section focuses on reviewing projects that are similar to the one described here. However, their relationship to
learning strategies can be a bit hard to see, so a few prefatory remarks may be helpful.
Because this paper is concerned with teaching students how to construct models, and constructing a model is a kind of problem solving,
it is worth clarifying how learning strategies differ from problem solving strategies. When students are taught an effective strategy for
solving problems, the strategy qualiﬁes as either a learning strategy, a problem solving strategy or both depending on the instructional
objectives.
 If the objective is for students to solve problems well, then a strategy for solving problems counts as domain knowledge and is called a
problem solving strategy. In the jargon of students, it’s on the test.
 On the other hand, when problem solving is done only to give student practice in applying certain concepts and principles and the
problem solving itself is not an instructional objective, then the strategy counts as a learning strategy because it is optional and yet it
probably impacts the students’ learning of the concepts and principles. In the jargon of students, it is not on the test.
 As pointed out earlier, the process of constructing models is both an instructional objective (especially in math classes) and a means for
acquiring domain knowledge (especially in science classes). Thus, an effective strategy for constructing models counts as both a problem
solving strategy in some cases (it’s on the test) and a learning strategy in other cases (it’s not on the test).
For example, suppose the instructional objective is to learn which botanical features go with which plants, and the instruction involves
arranging cards labelled with plants and features. If this matching activity does not appear on the exams, then teaching students a
methodical method for arranging the cards counts as a learning strategy even though it is also a problem solving strategy. From the students’
point of view, what matters is whether the strategy is “on the test.” That is, if a problem solving strategy is an instructional objective and is
part of a test or other assessment, then students have a different attitude towards it than a strategy that is merely helpful for learning the
material that will be on the test. As we discuss various methods for teaching model construction, it is important to note whether students
believe a taught strategy is required or merely helpful.
Interactive instructional systems, such as the tutoring system described herein, have their own unique learning strategies. Here are some
examples:
1. Teachers often complain that their students would rather click than think. Actuating buttons, menus, etc. without thinking or even
reading the rest of the screen is a poor learning strategy.
2. When a tutoring system gives immediate feedback on the correctness of a student’s entry, then students often guess instead of think.
That is, they rapidly revise and resubmit an incorrect entry until the tutoring system says that it is correct.
3. If the tutoring system gives a sequence of hints that get gradually mores speciﬁc until they tell the student exactly what to do,
then students abuse the hint sequences by clicking rapidly on the Hint button until they get the ﬁnal hint that tells them what
to do.
4. A good learning strategy for tutoring systems is to ask for a hint only when you need one (Aleven, McLaren, Roll, & Koedinger, 2004).
5. After asking for hints from a tutoring system and ﬁnally making a correct entry, a good learning strategy is to reﬂect on why it is correct
(self-explanation) and whether the hint makes sense (Shih, Koedinger, & Scheines, 2008).
6. Examples 2, 3 and 4 are help-seeking strategies (Aleven, Stahl, Schworm, Fischer, & Wallace, 2003). Examples 2 and 3 are often referred
to as “gaming the system” (Baker, Corbett, Koedinger, & Wagner, 2004).
Feedback and hints are the signature methods of tutoring, but several projects have used feedback and hints for two distinct purposes, so
let us distinguish them as follows:
 Let domain knowledge refer to what students are supposed to learn. It is typically measured with a post-test and sometimes a pre-test.
 When the hints address the domain and the feedback indicates whether the domain knowledge has been correctly applied, the system
is said to be tutoring and the module responsible for it is called the tutor.
 A learning strategy is an optional method for using the system (i.e., it is not domain knowledge) that is believed to increase student’s
learning of domain knowledge.
 When the feedback and hints refer only to the learning strategy and whether it is being applied correctly, the system is said to be metatutoring and the module responsible for it is called the meta-tutor.
In this paper, meta-tutoring will only means a method for teaching students a learning strategy. However, in the tutoring literature,
“meta-tutoring” is used more broadly to mean using feedback and hints to teach anything other than domain knowledge. For instance,
meta-tutoring is sometimes used to increase motivation or change beliefs about self-efﬁcacy. Du Boulay, Avramides, Luckin, MartinezMiron, and Rebolledo-Mendez (2010) propose a framework that includes many types of meta-tutoring.
Now we can turn to reviewing prior work on using learning strategies to help students learn how to construct models.
1.3. Prior work on learning strategies for model construction
Betty’s Brain (Leelawong & Biswas, 2008) was a step-based tutoring system for constructing models that also taught a learning strategy. It
could give feedback and hints on the student’s model (tutoring) or it could give feedback and hints on the way that the student was using the

L. Zhang et al. / Computers & Education 75 (2014) 196–217

199

system to create the model (meta-tutoring).1 For instance, sometimes it would not permit the student to evaluate the model with an
instructor-provide test suite (called a “quiz”) until the student had ﬁrst examined speciﬁc predictions of their model (e.g., if air temperature
goes down, what does body temperature do?). The system included some multimedia resources on the task domain, so another part of the
learning strategy was encouraging students to read them. As these examples indicate, the learning strategy was speciﬁc to the particular
instructional features provided by system. This is consistent with the ﬁndings in the learning strategies literature, which suggest that such
speciﬁcity provides better results than general learning strategies.
Three evaluations of the effectiveness of Betty’s meta-tutor were conducted (Biswas, Leelawong, Schwartz, & Vye, 2005, study 2;
Leelawong & Biswas, 2008; Tan, Biswas, & Schwartz, 2006). Their methods will be described fully here, as the experiments to be reported
later used similar methods. The Betty’s Brain experiments all had two phases, called the training phase and the transfer phase here. During
the training phase, ﬁfth-grade students worked with Betty for approximately seven 45-min sessions on constructing a model of a river
system. One group of students used Betty’s Brain with the meta-tutor turned on, and another group used the same system with the metatutor turned off. Two months later the transfer phase occurred, where all the students used Betty’s Brain with the meta-tutor turned off to
create models for the nitrogen cycle. This transfer phase was used to assess their modelling skill.
The results were roughly the same in all three studies. Using the general results on learning strategy mentioned above as a framework,
the results from the Betty’s Brain studies were:
A. Students often exhibit poor learning strategies. True of the control conditions in all three studies.
B. Good learning strategies can be taught, often with little difﬁculty. In the training phase of all three studies, meta-tutored students’
behaviour was consistent with the learning strategies.
C. When students use the taught learning strategies, their domain learning increases compared to students who are not taught to use the
learning strategies. Unfortunately, using conventional science tests of ecology concepts, there were few signiﬁcant differences between
the meta-tutored students and the students who used Betty’s Brain without meta-tutoring.2 Using four measures of model quality,
meta-tutored students’ models were better than control students’ models on only one measure.
D. When instruction in the learning strategy includes meta-cognitive and motivational components, students can often be prevented from
reverting to poor learning strategies when the instruction ceases. Students who received meta-tutoring during the training phase tended to
continue using the learning strategy in the transfer phase when the meta-tutoring was turned off.
E. Speciﬁc learning strategies often have larger effect sizes than general purpose ones. Not tested.
In short, although the meta-tutor in Betty’s Brain was successful at teaching the learning strategy and persuading students to continue
using it, the learning strategy had little impact on domain learning. More recent work has focused on ﬁnding out what behaviours
distinguish good learners from poor learners (Segedy, Kinnebrew, & Biswas, 2012a, 2012b).
The Help Tutor (Aleven et al., 2004; Roll, Aleven, McLaren, & Koedinger, 2007a, 2007b; Roll, Aleven, McLaren, & Koedinger,
2011; Roll et al., 2006) was a meta-tutor that augmented an existing step-based tutoring system, the Cognitive Geometry Tutor
(www.carnegielearning.com). Although the tutoring system did not teach students to construct models, it is included in this review of
past work because it is an often-cited representative of using meta-tutoring to improve students’ learning from step-based tutoring.
The Geometry Cognitive Tutor allowed students to ask for a hint. The ﬁrst hint was rather general, but if the student kept asking for hints,
then the last hint told the student exactly what to do. This was called the “bottom-out” hint. Students sometimes clicked rapidly on the Help
button so that they could get to the bottom-out hint. As mentioned earlier, this abuse of the help system is a kind of “gaming the system”
(Baker, Corbett, Koedinger, et al., 2004). In order to reduce the frequency of gaming the system, the Help Tutor gave students feedback and
hints on their use of the help system.
Two studies evaluated the Help Tutor. As in the Betty’s Brain studies, the Help Tutor studies had both a training phase where the metatutor was used by half the students and a transfer phase where none of the students used the meta-tutor. Using the general ﬁndings
mentioned above as a framework, the results were:
A. Students often exhibit poor learning strategies. True of the control conditions in all both studies.
B. Good learning strategies can be taught, often with little difﬁculty. In the training phase of both studies, meta-tutored students’ behaviour
was consistent with the taught strategies.
C. When students use the taught learning strategies, their domain learning increases compared to students who are not taught to use the
learning strategies. Unfortunately, students taught the learning strategy did not differ from the control group in their acquisition of
geometry knowledge.
D. When instruction in the learning strategy includes meta-cognitive and motivational components, students can often be prevented from
reverting to poor learning strategies when the instruction ceases. Students who received meta-tutoring during the training phase tended to
continue using the learning strategy in the transfer phase when the meta-tutoring was turned off, albeit with less frequency.
E. Speciﬁc learning strategies often have larger effect sizes than general purpose ones. Not tested.
Betty’s Brain and the Help Tutor provided most of their instruction via feedback and hints. A somewhat more didactic approach is to
provide forms and phases that constrain students’ modelling behaviour. For instance, the ﬁnal version of Model-It (Metcalf et al., 2000) had

1
Those familiar with Betty’s Brain might be surprised to see it described as a tutoring system because the students see the system as two agents: Betty (a teachable agent)
and Mr. Davis (a mentor). Students were told that the model they were constructing comprised the knowledge of Betty, so editing the model comprised “teaching Betty.”
When they ask Betty take a quiz, Mr. Davis gives the student feedback on the correctness of the model’s predictions which would sometimes include unsolicited hints about
ﬁsh, algae, carbon dioxide and other domain entities. Mr. Davis personiﬁes the system’s tutoring. On the other hand, meta-tutoring was done by both agents. For instance,
Betty would sometimes refuse to take a quiz, and Mr. Davis would discourage students from using trial-and-error methods.
2
In a fourth study, the meta-tutored students produced slightly better river system models than the control students (Tan, Wagster, Wu, & Biswas, 2007; Wagster, Tan,
Biswas, & Schwartz, 2007; Wagster, Tan, Wu, Biswas, & Schwartz, 2007). However, results on the other measures were not reported.

200

L. Zhang et al. / Computers & Education 75 (2014) 196–217

4 phases, which were selected by clicking on one of four buttons labelled Plan, Build, Test and Evaluate. The Build mode was the actual model
editor. The other phases presented forms to be ﬁlled in by the student. Fig. 1 shows the form for the Test phase. Students were given feedback
and hints, which they could suppress if they desired, when they attempted to bypass a suggested activity. The Carnegie Learning’s Algebra
Cognitive Tutor (www.carnegielearning.com) and the Word Problem Solving Tutor (Wheeler & Regian, 1999) also scaffolded problem
solving strategies via lightweight constraints, implemented with phases and forms. None of these meta-tutors were evaluated separately
from the rest of the system.
On the other hand, Mulder, Lazonder, de Jong, Anjewierden, and Bollen (2011) did assess the effects of a phase-based learning strategy.
The experiments used Co-Lab, a system that taught system dynamics modelling. Co-Lab was not a step-based tutoring. Instead, students
received feedback only on the accuracy of the model’s predictions. The learning strategy was to encourage students to do their model
construction in three stages. In Stage 1, they deﬁned variables and drew undirected links between variables that directly affected each other
somehow. In Stage 2, they added qualitative labels to the links indicating whether an increase in one variable caused an increase or decrease
in the other variable. In Stage 3, they added equations to the model that further speciﬁed the relationships between variables. Three versions
of the learning strategy were compared to a control version of Co-Lab that lacked phases. The three versions differed in how strictly they
enforced the learning strategy. The restricted strategy required students to achieve a certain level of success in one stage before moving to
the next. The semi-restricted version of the strategy allowed students to move from one stage to the next at will, but they were not allowed
to move backwards. The unrestricted version allowed students to move at will between stages. Compared to the control version, all three
versions of the learning strategy increased the number of correct model elements generated by students as they used the system. The three
versions were not signiﬁcantly different in productivity from each other, but there was a trend for the semi-restricted version to be better
than the other two. However, this experiment included only a training phase and not a transfer phase, and it did not assess students’ domain
knowledge with pre- and post-testing. Thus, its consistency with the 5 general ﬁndings mentioned above (A through F) cannot be determined. Nonetheless, this work is interesting because the learning strategy was taught without using a meta-tutor, and the system was not a
step-based tutoring system.
The meta-tutoring of Betty’s Brain and the Help Tutor placed only weak constraints on students’ behaviour. The phases and forms of CoLab, Model-It, the Cognitive Tutors and the Algebra Word Problem tutor placed somewhat stronger constraints on students’ behaviour. At
the far end of this progression is procedural scaffolding, which places very strong constraints on student behaviour. The basic idea of procedural scaffolding is to require students to temporarily follow a speciﬁc procedure for constructing a model. Although the procedure is not
required by the task and there are many other ways to successfully construct models, the procedure is used as a temporary scaffolding to
guide students who might otherwise be quite lost.

Fig. 1. Scaffolding for the Test mode of Model-It (Metcalf, 1999).

L. Zhang et al. / Computers & Education 75 (2014) 196–217

201

Although procedural scaffolding was ﬁrst used by (Marshall, Barthuli, Brewer, & Rose, 1989) to scaffold arithmetic story problem solving,
its beneﬁts were not evaluated. Procedural scaffolding was ﬁrst evaluated with Pyrenees (Chi & VanLehn, 2010; Vanlehn & Chi, 2012), which
required students to construct models using a version of goal reduction, which is a well-known general purpose reasoning strategy used in
artiﬁcial intelligence applications (Russell & Norvig, 2009).
Pyrenees’ procedural scaffolding was evaluated in a two-phase experiment. In the training phase, students learned to construct model of
probabilistic systems. In the transfer phase, student learned to construct models of mechanical energy systems. Using the framework
mentioned earlier, the ﬁndings were:
A. Students often exhibit poor learning strategies. True of the control conditions in both phases.
B. Good learning strategies can be taught, often with little difﬁculty. This could not be determined, because Pyrenees required students in the
experimental condition to follow the learning strategy during the training phase.
C. When students use the taught learning strategies, their domain learning increases compared to students who are not taught to use the
learning strategies. In both the training phase and the transfer phase, the experimental group acquired more domain knowledge than
control group. The effect sizes were large (d z 1.0).
D. When instruction in the learning strategy includes meta-cognitive and motivational components, students can often be prevented from
reverting to poor learning strategies when the instruction ceases. The experimental group tended to use the learning strategy during the
transfer phase on difﬁcult problems, but not on simple problems. However, even on simple problems, they did not use poor learning
strategies.
E. Speciﬁc learning strategies often have larger effect sizes than general purpose ones. Not tested.
In summary, Betty’s Brain and the Help Tutor used the standard methods of hints and feedback and their meta-tutoring succeeded in
improving students’ behaviour, but there were only weak improvements at best in their learning of the domain. Co-Lab’s learning strategy
increased performance, but the effect on learning was not measured. Pyrenees used procedural scaffolding, and its meta-tutoring caused
large improvements in both behaviour and domain learning.
1.4. Our research questions
Our research questions are the same 4 questions that our predecessors have focused one:
A. Without instruction in good learning strategies, do students often exhibit poor learning strategies?
B. Can good learning strategies be easily taught?
C. When students use the taught learning strategies, does their domain learning increase compared to students who are not taught to use
the learning strategies?
D. When instruction in the learning strategy ceases, do students revert to poor learning strategies?
Although these 4 questions about learning strategies are the central focus of the research, we are also interested in seeing if the time
required to achieve adequate competence in model construction can be reduced from 5 or more hours to 2 for fewer hours.
Because Pyrenees was arguably more successful than the other methods, we chose to use procedural scaffolding as the basic method of
teaching learning strategies. However, there is a major difference between our research problem and the one addressed by Pyrenees.
Pyrenees assumed that students had already been taught all the relevant domain principles. Indeed, the students repeatedly selected
principles from a menu. In contrast, when students construct system dynamics models, they are seldom taught domain principles in
advance. They are instead asked to infer domain relationships from multimedia resources, common sense and/or experimentation. They do
this while constructing the model. This allows modelling to be used as part of enquiry-based instruction where students construct the
domain knowledge themselves. Having to infer domain relationships while learning to model may be one of the reasons why it takes
students so long to master system dynamics model construction. Nonetheless, inferring quantities and their relationships is exactly the skill
we want students to learn.
This suggests that if we require students to follow a procedure, as Pyrenees did, then they will learn that the key to modelling is a
single non-mechanical step: ﬁguring out the mathematical relationship among a set of directly related quantities. Once they have
mastered that step, we expect that they will be able to construct models well, even if they don’t follow the procedure. Thus, our research
question is whether applying the Pyrenees approach of heavy-handed procedural scaffolding to the cognitive skill of model construction
will cause deeper, more effective modelling skills to develop during the training phase. If the tutor does work in the training phase, our
next question is whether the beneﬁts will persist in the transfer phase. To answer these two questions, we developed an instructional
system, called AMT, and conducted experiments to evaluate it. The following sections described the system, how it was implemented, and
the evaluation at last.
The name “AMT” is an acronym for the overall project, which is called the Affective Meta-Tutoring project. The ﬁrst objective of the
project is to develop and test a meta-tutor that improves student’s learning of modelling by using procedural scaffolding as well as the
traditional feedback and hints. The results of that phase of the project are reported here. The second phase of the project will be to add an
affective learning companion to the AMT system; hence the term “affective” in the project name. The second phase will be described further
in the discussion section.
2. The AMT system’s design and behaviour
This section has three parts. The ﬁrst describes the student’s task, and in particular, the graphical language in which they write models.
The second section describes the tutoring system. The third section describes the meta-tutor and the strategy that it teaches students to use.

202

L. Zhang et al. / Computers & Education 75 (2014) 196–217

2.1. Constructing models in the AMT modelling language
In order to decrease the difﬁculty of learning how to construct system dynamics models, most prior work has used graphical modelling
languages where a model consists of several types of nodes and links. These graphical languages are easier to learn than text-based languages (Löhner, Van Joolingen, & Savelsbergh, 2003). The traditional “stock and ﬂow” language has two types of links, and one of them (the
ﬂow links) acts somewhat like nodes. In pilot studies, our high school students found this confusing, so we removed the confusing type of
link.
In our modelling language, a model is a directed graph with one type of link. Each node represents both a variable and the computation
that determines the variable’s value. The inputs of that computation, which are themselves variables, are indicated by incoming links. In
Fig. 2 for example, the computation for “births” requires the values of “growth rate” and “population.” The value of a variable is a real
number that can change over time, where time is represented discretely.
There are three types of nodes:
 A ﬁxed value node represents a constant value that is directly speciﬁed in the problem. A ﬁxed value node has a diamond shape. It never
has incoming links.
 An accumulator node accumulates the values of its inputs. That is, its current value is the sum of its previous value plus its inputs. An
accumulator node has a rectangular shape and always has at least one incoming link.
 A function node’s value is an algebraic function of its inputs. A function node has a circular shape and at least one incoming link.
The students’ task is to draw a model that represents a situation that is completely described by a relatively short text. For instance, Fig. 2
is a correct model for the following problem:
Rust destroys steel and can spread quickly. Suppose that you take a large sheet of steel, such as one that might be used as the roof of the boxcar
on a train, and you put it outside in the weather. Suppose it starts with a spot of rust that is 10 square inches in area. However, each week the
rust spot gets bigger, as it grows by 30%. Therefore at the end of the ﬁrst week, the rust spot is 13 square inches in area. Graph the size of the rust
spot over 10 weeks.
Such a text contains all the information that students need. However, it sometimes contains extra quantities (e.g., the rust spot at the end
of the ﬁrst week) that are not needed in the model. Students are asked to draw only the necessary nodes, so drawing a node for the rust spot
at the end of the ﬁrst week is an indication of shallow modelling.
2.2. The tutoring system
Many tutoring systems have a sequence of feedback and hints (VanLehn, 2006). When the student makes an entry, the tutoring system
ﬁrst just tells the student whether the entry is correct or incorrect. If the student asks for help again, the system gives a general hint.
Subsequent requests for help on the same entry generate increasingly speciﬁc hints. Eventually, the ﬁnal hint (called the “bottom-out hint”)
tells the student what the correct entry is. There is some evidence that the mid-level hints are not pedagogically useful (Muldner, Burleson,
van de Sande, & VanLehn, 2011; Timms, 2007), so our tutoring system does not use them. Instead, it generates just the ﬁrst and last members
of the typical hint sequence. More speciﬁcally, it has a Check and a Give-up button. Clicking on the Check button causes the tutoring system
to give minimal feedback: It colours entries red if they are incorrect and green if they are correct. Clicking on the Give-up button causes the
tutoring system to ﬁll in entries correctly, that is, to give a bottom-out hint. This section describes the AMT model editor, pointing out where
the Check and Give-up buttons appear.
The system presents itself to the student as a large tabbed window (Fig. 3). The Model tab (shown in the ﬁgure) is for drawing models.
The Situation tab shows a textual representation of the problem, and is illustrated by a static picture. The Instructions tab contains a slide
deck that teaches students the basics of the modelling language, the user interface, the modelling process and the learning strategy. Students can access the Introduction and Situation tab at any moment during the construction of the model (cf. Section 4.4).
The Model tab has three buttons: Create Node, Run Model and Done. The Done button is disabled (grey) until the student has completed a
problem successfully (i.e. created an accurate model of the system), at which time clicking on the Done button advances the system to the
next problem.

Fig. 2. A model. The grey bubbles have been added to this ﬁgure in order to show how the value of each variable is calculated.

L. Zhang et al. / Computers & Education 75 (2014) 196–217

203

Fig. 3. The tutoring system’s screen.

The Create Node button creates a new node symbol in the model area and opens the Node Editor on it. The Node Editor (Fig. 4) is divided
into ﬁve tabs: Description, Plan, Inputs, Calculations and Graph. Students can edit all tabs with the exception of the Graph tab, where a graph
showing the evolution of the node’s value over time is generated by the tutoring system once the model is run. To create a node, the student
ﬁlls out the four tabs in order, left to right. Students can also change the information within a node by double clicking on the node shape
within the model area, which opens the Node Editor, and then selecting the tab they wish to modify.
The Description tab was engineered to facilitate grounding, where “grounding” is the process of negotiating mutual understanding of the
meaning of a term between two participants in a conversation (Clark & Brennan, 1991). In system dynamics model editors that are not used
for tutoring, such as Stella, Vensim and Powersim, users merely type in the name of the node that they want. This allows them to use names
such as “x” that do not match anything in the problem. While this is unproblematic for such editors, it makes it difﬁcult or impossible for a

Fig. 4. The node editor, showing the Description tab.

204

L. Zhang et al. / Computers & Education 75 (2014) 196–217

tutoring system to determine what quantity the student’s node denotes, and hence the tutoring system cannot determine whether the node
is deﬁned correctly. Urging the students to choose adequately precise names is only partially successful. In one study, only 78% of the node
names could be identiﬁed by the tutoring system (Bravo, van Joolingen, & de Jong, 2009). This is a case of bad grounding: the tutoring system
did not understand the meaning of 22% of the students’ terms.
On the other hand, some tutoring systems for system dynamics modelling, including the ﬁrst version of this system, provide students
with nodes that already have names but are otherwise undeﬁned (VanLehn et al., 2011). Unfortunately, students often do not pay enough
attention to the names, which can be rather similar. Students sometimes create models that are correct except that the names of two nodes,
such as “rust growth factor” and “new rust area per week”, have been switched. This is also a case of bad grounding: the student did not
understand the meaning of the system’s terms.
The Description tab of AMT, illustrated in Fig. 4, is intended to prevent bad grounding. First, the student walks through the tree in the
Description tab located in the large box at the top of the window. Clicking on a leaf in the displayed tree selects both a description and a
name for the node. Each problem has a different tree, and the tree’s contents are engineered to make the student select among subtly
different descriptions. After selecting a leaf, the student clicks on the Check button. If the selected description denotes a quantity that the
system understands, and that quantity does not already have a node deﬁned for it, then clicking on the Check button turns some boxes
green, as shown in Fig. 4. Otherwise, the boxes turn red. Students may also click on the Give-up button that ﬁlls out the tab correctly but
colours the boxes yellow. When the student exits the node editor, the node’s name, which is shown beneath the node, is highlighted by the
colour of the Description tab’s boxes. Thus, a student who had given up on the Description tab would forever see yellow highlighting on the
node’s name. This feature is intended to discourage giving up.
When the Description tab is completed correctly and its boxes are either green or yellow, then students can go on to the Plan tab. The Plan
tab lets students choose among 7 different plans (Fig. 5). The Instruction tab describes the meaning of these plans and gives an example of
each plan. After a selection is made, students may click on the Check button and see their selection coloured green (correct) or red
(incorrect). Clicking on the Give-up button causes the correct plan to be selected and coloured yellow. Unlike the other tabs, ﬁlling out the
Plan tab correctly is optional. Students can go to the Inputs tab regardless of how their plan selection is coloured, and they can skip the Plan
tab entirely if they want.
The Inputs tab (Fig. 6) allows students to indicate whether the node’s value is a ﬁxed, given constant or if it is computed from other nodes’
values. In the latter case, they click on “Inputs:” and choose some of the existing nodes as inputs, which causes links are drawn between
those nodes and the current node. When students see that the input they want is not in the list because they have not yet created a node for
it, they can click on the convenient “Create a new node” button, deﬁne the desired node using a pop-up version of the Description tab, then

Fig. 5. The plan tab.

L. Zhang et al. / Computers & Education 75 (2014) 196–217

205

Fig. 6. The Inputs tab.

return to this Input tab by closing the pop-up. As always, the Check and Give-up buttons colour the student’s choices in either red (incorrect),
green (correct) or yellow (gave up). While red choices can be revised later on, green or yellow choices cannot be changed.
When the Inputs tab is ﬁlled out correctly, students can go to the Calculation tab (Fig. 7). If they had selected “Fixed Value” on the Inputs
tab, then “has a ﬁxed value” is checked here on the Calculation tab, and so all they need to do here is enter the number that is the value of the
Fixed Value node. On the other hand, if they had selected “Inputs:” on the Inputs tab, then they must click either the “accumulates the values
of its inputs” button or the “is a function of its inputs” button. This selection determines which form appears in the lower part of the tab.
Fig. 7 shows the tab to ﬁll for the Accumulator node type on the left, and the Function node type on the right. For both the Function and
Accumulator nodes, the inputs appear initially in the “Available inputs:” box, and then move rightward into the calculation box as they are
clicked. In this fashion, students enter a calculation.
As a model is being constructed, the state of the node’s deﬁnition is indicated by little circular indicators (“i” for input, “c” for calculation
and “g” for graph) and the node’s outline (see Fig. 8). A dotted border indicates that the type of the node hasn’t been deﬁned yet. A blue
border means that the node’s deﬁnition is complete.
When all the nodes have blue borders, the student can click on the Run Model button. If there are syntactic errors in the model that
prevent running it, a pop-up window describes them. Otherwise, Run Model colours the “g” indicators on every node either green or red,
depending on whether the node’s graph is correct or not. Opening the Graph tab of a node displays both the user’s model’s graph and the
expected graph (see Fig. 9). Students can use the difference in the graphs as a clue to where the bug in the model could be found.
A model is considered completely correct when all the graphs match, in which case all the “g” indicators are green. At this point, students
can click on the Done button and go to the next problem.
This is a step-based tutoring system (VanLehn, 2006) because it can give feedback on every step taken by the student. It only gives such
feedback when the student clicks on the Check button. The feedback is minimal: just correct vs. incorrect. Absent are the usual sequences of
hints that many step-based tutoring systems have. However, the “Give-up” button implements the bottom-out hint in that it gives away
exactly what step the student should do at this point.
The AMT system has a mode, called test mode, which is used to assess students’ skill at modelling. Although students can still debug their
model by running it and seeing which graphs are correct, they cannot use the Check and Give-up buttons on any of the tabs that they ﬁll out,
with one exception. The Check button is always enabled on the Description tab. This is because the system and the student must agree on the
meaning of nodes. If the system doesn’t know which quantity is denoted by the student’s node, then it can’t know which graph is correct and
can’t colour the “g” indicators. In test mode, the Check button is enabled only on the Description tab, whereas in training mode, it is enabled
everywhere. In test mode, the Give-up button is never enabled.

206

L. Zhang et al. / Computers & Education 75 (2014) 196–217

Fig. 7. The Calculations tab.

2.3. The meta-tutor
So far, only the tutoring system has been described. It is essentially just a model editor with Check and Give-up buttons. This section
described the meta-tutor, which is the remaining module of the AMT system. This section begins by describing and motivating the learning
strategy that the meta-tutor teaches, then describes how it teaches that strategy.
Like Pyrenees (Chi & VanLehn, 2010), the meta-tutor teaches students a simple, goal reduction procedure for constructing models called
the Target Node Strategy. The basic idea is to focus on one node at a time (the target node) and complete all the tabs for this node before
working on other nodes. As students complete the target node, they may create new nodes as a side effect, but the new nodes will only be
named and not fully deﬁned. These nodes are displayed with dotted borders. Thus, when students have ﬁnished the target node, they can
pick any node that has a dotted border as the next target node, and begin working on deﬁning it. When there are no more nodes with dotted
borders, the model is complete.
There are sound reasons to think that the Target Node Strategy might help students learn more effectively, but it will take several
paragraphs to describe them. These paragraphs also illustrate the distinction between deep and shallow modelling practices.
The key steps in the Target Node Strategy are ﬁlling out the Inputs tab and the Calculation tab. These steps correspond to the moment
where students must analyze the given system information and determine the quantitative relationships between the target node’s
quantity and other quantities in the problem. That is, given a particular target quantity, students must ﬁnd a set of quantities such that the

Fig. 8. An incomplete model.

L. Zhang et al. / Computers & Education 75 (2014) 196–217

207

Fig. 9. The Graphs tab.

target quantity’s value can be computed as a simple function of their values. The whole problem of constructing a model of a system can be
decomposed into making a series of decisions like this one. One might call these decisions “atomic modelling problems”, as they cannot be
easily decomposed further.
For example, consider the system described earlier:
“Rust destroys steel and can spread quickly. Suppose that you take a large sheet of steel, such as one that might be used as the roof of the boxcar
on a train, and you put it outside in the weather. Suppose it starts with a spot of rust that is 10 square inches in area. However, each week the
rust spot gets bigger, as it grows by 30%. Therefore at the end of the ﬁrst week, the rust spot is 13 square inches in area. Graph the size of the rust
spot over 10 weeks.”
Suppose students are following the Target Node Strategy and have decided that the ﬁrst target node is the size of the rust spot. They
create a node and select “rust spot area” as its description. Now they face an atomic modelling problem. How to determine the value of rust
spot area? What we want students to think is something like, “I know the rust spot area is changing, and the value increases every week”.
This reasoning sufﬁces for ﬁlling out the Plan tab correctly. Next the students need to ﬁll out the Input tab, so they should think, “The rust
spot area is increased by the rust produced during the week”. There are no other nodes at the moment, so the student must click on “Create a
new node.” The student browses the available descriptions, and chooses the one that comes closest to the student’s idea of “rust produced
during the week.” Clicking on this description deﬁnes the node new rust area per week. The student can then select this node as an input for
the node rust spot area. Next comes the Calculation tab, which requires for the student to turn his/her qualitative understanding of the
relationship into a formal, mathematical one: the student should decide that the next value of rust spot area is its current value plus new rust
per week. This illustrates how the user interface decomposes the key reasoning into three major steps, corresponding to the Plan, Input and
Calculation tabs. These correspond roughly to the phases of Co-Lab (Mulder et al., 2011) and Model-It (Metcalf et al., 2000). This paragraph
also illustrates how the key reasoning should be done: by thinking hard about the system, its quantities and their interrelationships, and
then abstracting the mathematical relationships from them. This is the “deep” way to solve an atomic modelling problem.
However, there are shallow ways to solve the atomic modelling problem as well. When students need to create nodes in order to ﬁll out
the Inputs tab, there are only a ﬁnite number of choices: the leaves in the tree of descriptions on the Description tab. Thus, they can work
through all combinations until they have found the appropriate inputs. When the Check button is available and the problem is simple, this
can be done rapidly. For instance, the rust spot area problem’s Description tab has 7 possible descriptions, of which only 4 are legal
quantities in the problem (1 is an extra node; 3 are necessary for the model). It will not take the student long to create all the legal nodes, and

208

L. Zhang et al. / Computers & Education 75 (2014) 196–217

then test each one to see if it turns the Inputs tab green. Such extensive guessing is one “shallow” method for solving atomic modelling
problems. The Give-up button is another. Other methods involve looking for keywords (e.g., “initially”) or patterns.
Many of our students’ shallow modelling methods involve abusing the Check button or the Run Model button, so their behaviour is a
form of gaming the system (Baker et al., 2006; Baker, Corbett, & Koedinger, 2004), which is what the Help Tutor (Roll et al., 2011), Scooter the
Tutor (Baker et al., 2006), and other tutoring systems have tried to address. However, studies of modelling that did not use tutoring systems
have also noted this propensity of students to do shallow modelling (Alessi, 2000; Booth Sweeney & Sterman, 2000; Doerr, 1996; Mandinach
& Cline, 1994b; Nathan, 1998; Zaraza & Fisher, 1999). Thus, we prefer to refer to the phenomenon as “shallow modelling” rather than
“gaming the system.”
Teaching the Target Node Strategy does not require that student do deep modelling, so one might wonder why we think it could help
students model better. However, it does decompose the whole problem of modelling a system into a series of atomic modelling problems,
and even decomposes an atomic modelling problem into three major steps. Like Pyrenees, it teaches students that if they just master this
one difﬁcult but small skill (deep modelling applied to atomic modelling problems), then the rest of the problem solving is purely
mechanical.
Having described and motivated the Target Node Strategy, it is ﬁnally time to describe how the meta-tutor teaches it. First, the Introduction slides describe the strategy brieﬂy. Even students who use AMT with the meta-tutor turned off see this presentation of the strategy.
Also when the meta-tutor is turned off, students can edit any node at any time. Moreover, they can ﬁll out some of a node’s tab, then quit
editing the node and come back to it later. This freedom is removed when the meta-tutor is turned on. Students are required to correctly ﬁll
out each tab before moving on to the next, and they must ﬁll out all tabs before closing the node editor. When they are on the Description tab
and choosing which quantity to create a node for, if they choose one that would not be selected by Target Node Strategy and yet it will
eventually be include in the model, then the selection turns blue and a pop-up window says, “That quantity is in the correct model, but it is
too early to deﬁne it now.” This message is typical of others that pop up when students stray from the Target Node Strategy. In short, the
main job of the meta-tutor is simply to keep students on the one of the paths that are consistent with the Target Node Strategy.
In addition to requiring students to follow the Target Node Strategy, the meta-tutor enacts a bit more scaffolding. For completeness, this
section describes the remaining forms of help.
When the meta-tutor is turned on, it complains if students appear to be guessing too much or giving up too early, just as the Help Tutor
did. When a student clicks on the Check button on the same tab twice within 3 s and gets red both times, the meta-tutor complains that the
student is guessing. If the student clicks on the Give-up button without ﬁlling out anything and checking it, the meta-tutor complains that
the student is giving up too early.
Some of the training problems let students practice debugging an incorrect model. They present a model that will run but generates
incorrect graphs. This kind of situation occurs frequently when the system is in test model (the tutor and meta-tutor are turned off). When
the model has been run, students see that some nodes have red “g” indicators and some have green ones. They face a decision of where to
start looking for an error in the model. The Introduction slides teach all students two heuristics:
 When possible, start by examining a node that has a red “g” indicator and no incoming links from nodes with red “g” indicators. Such a
node is guaranteed to have an error inside it.
 Avoid editing nodes that have green “g” indicators, because if the graph is correct, the node’s deﬁnition is probably correct.
When the meta-tutor is on and students are working on a debugging problem, it constrains them to obey these two heuristics.
In summary, the meta-tutor actually uses three types of scaffolding: (1) it requires students to follow the Target Variable Strategy; (2) it
complains when they abuse the Check or Give-up buttons; and (3) it teaches some heuristics for locating errors in buggy models. Although
(1) is procedural scaffolding, for convenience, we refer to this collection as “the meta-strategy.”

3. System architecture and implementation
The AMT system can be divided into two parts: the tutor and the meta-tutor. The tutor’s main functionalities included drawing a model,
checking correctness and running the model. It is an example-tracing tutor (VanLehn, 2006) in that an author must provide a correct model
with each problem; the students’ work is checked against that model. On the other hand, the meta-tutor is driven by algorithms (e.g., the
Target Node Strategy) that work for any problem. The student’s actions are check against the actions selected by the meta-strategy.
Although the meta-tutor and the tutor belong to the same Java project, they don’t share objects in the memory. This made it possible to
develop the tutor and meta-tutor programme independently. It also facilitates data analysis as will be explained later. More details about the
tutor’s architecture and implementation are presented in (Gonzalez-Sanchez, Chavez-Echeagaray, VanLehn, & Burleson, 2011). This section
describes the meta-tutor only.
Between the tutor and meta-tutor, there are three kinds of communication, each with its own channel as shown in Fig. 10:
(1) Every action made by student is sent to the meta-tutor via the activity channel.
(2) Before executing certain user actions, such as closing a node, the tutor sends a message to the meta-tutor via the block channel and
meta-tutor responds to the tutor, indicating whether the action should be blocked or not.
(3) Whenever the meta-tutor detects that the student needs an unsolicited hint, it sends a command to the tutor via the message channel to
tell it to initialize the tutorial dialogue. The tutor sends back the student’s response. Based on the response, the meta-tutor tells the tutor
either to display another dialogue box or to close the dialogue.
The meta-tutor is a production system, implemented in Java using Drools Expert (http://www.jboss.org/drools/drools-expert.html). The
production system implements two major functionalities: requiring the student to follow the Target Node Strategy and discouraging
gaming. The following sections describe the implementations of each function.

L. Zhang et al. / Computers & Education 75 (2014) 196–217

Tutor

Activity
channel

Student’s actions
Query at student’s actions

Message Block
channel channel

Response[block or not]

Activity
channel

209

Metatutor

Block Message
channel channel

Tutorial dialog
Student’s answer to dialog
Fig. 10. How the tutor and meta-tutor communicate with each other.

3.1. Constraining students to follow the Target Node Strategy
One main function of the meta-tutor is to constrain the student to follow the Target Node Strategy. In order to do so, it implements the
algorithm shown in Fig. 11. Aside from a little bookkeeping, the procedure’s actions are either “Prompt and constrain the student to do
<step>,” or “Constrain the student to do <step>”.
Prompting means that as soon as the problem state allows a step to be done, the meta-tutor gives the student an unsolicited hint about
the step. Here are examples of production rules that implement prompting:
 IF the activity channel reports that the student’s action is “closed a node”
AND there are at least two nodes that can become the next target node
THEN
send a command via the message channel to show a dialogue box that lists the nodes and ask which one the student wants to choose
as the target node.
 IF the context is that the node editor is open and the current tab is the plan tab,
AND the activity channel reports that the student’s action is “clicked on the Check”
AND the correct plan of this node is “a ﬁxed, given number”
AND the student has selected this plan
THEN
send a command via the message channel to show a message that says that the next step is to ﬁll in the Inputs tab by clicking on
“Value is ﬁxed, so no inputs.”
Constraining the student to do a step means that if the problem state is such that the student should do a certain action, and the student
tries to do another action instead, then the meta-tutor blocks the student’s action from occurring and pops up a message indicating the right
action to do. Some examples of production rules that implement constraining are:
 IF the context is that the student is in the model tab and the node editor is not open
AND the student’s action is clicking on a node in order to open it in the node editor
AND the node that student is trying to open is not the target node
THEN
send via the block channel the message “Please focus on your target node, <target node name>” and block opening the clicked-on
node.
 IF the context is that the node editor is open
AND the student’s action is closing the node
AND the node’s calculation tab is not ﬁnished yet
THEN
Send via the block channel the message “Please ﬁnish the target node, <target node name>, before leaving the editor” and block
closing of the node editor.
In order for the meta-tutor to track the student’s progress and respond appropriately, its rules need to have up-to-date information about
the state of the problem. Using the activity channel, the tutoring system sends all student actions to the meta-tutor, which converts some of
them (e.g., not node movements) into elements in the production system’s working memory. The working memory is also used to store the
identity of the target node, the contents of the sought set and other bookkeeping information that is kept updated via production rules.
3.2. Discouraging gaming
As mentioned earlier, the Give-up and Check buttons can be easily misused by students to construct models without thinking deeply or
learning, e.g., “gaming the system” (Baker et al., 2006; Baker, Corbett, Koedinger, et al., 2004). Although constraining students to follow the
Target Node Strategy may discourage gaming, the meta-tutor also looks for patterns of student actions that indicate gaming is occurring.
When it sees such a pattern, it pops up a warning message but does not block the student’s actions.
Detecting and responding to gaming of the Check button is implemented by the ﬁnite state machine shown in Fig. 12. The students are
always in one of 5 states. When they start working on a problem, they start in state S1 (the node editor is not open). Clicking on a node opens
the node editor and moves to state S2 (no check yet). Edits to the contents of the open tab can occur in any of the states except S1, and are not
shown in Fig. 12, as they merely add a loop from a state back to itself. The arc label “Wrong check” means that the student clicked on the
Check button and got red, indicating the tab was not ﬁlled out correctly. The arc label “Got the answer right” means the student either

210

L. Zhang et al. / Computers & Education 75 (2014) 196–217

1. Initialize the target node to the top level goal (i.e., the quantity the problem wants graphed).
Initialize the sought set to null.
2. Constrain the student to create a node and fill out its Description tab to correspond to the top level
goal quantity.
3. Constrain the student to fill out the Plan tab correctly.
4. Prompt and constrain the student to fill out the Input tab correctly. Add any newly created nodes
to the sought set.
5. Prompt and constrain the student to fill out the Calculation tab correctly.
6. Constrain the student to close the node editor.
7. If there are no nodes in the sought set, then prompt and constrain student to run the model.
If there is just one node in the sought set, then set it as the target node and tell the student.
If there are two or more nodes in the sought set, ask the student which one should be the target
node and set it to be the target node.
8. Remove the target node from the sought set.
9. Prompt and constrain the student to create a node and fill out its Description tab to correspond to
the target node.
10. Go to step 3 above.
Fig. 11. The Target Node Strategy implemented by the meta-tutor.

clicked on the Check button and got green, or clicked on the Give-up button. Whenever the student enters state S4 (gaming detected), the
meta-tutor sends a randomly chosen message such as, “Guessing so quickly wastes the opportunity to learn.”
In short, because the main job of the meta-tutor was to teach students to stay on a certain path deﬁned by the meta-strategy by blocking
off-path actions, and the meta-strategy was simple and easily deﬁned, the only signiﬁcant technical challenge was integrating the metatutor with the tutor. The tutor was implemented with a standard model-view-controller architecture, and the meta-tutor essentially was
given a chance to intervene after the user’s actions had actuated the control and before the model was updated.
At this point, the design and implementation of the AMT system have been deﬁned, so it is time to consider whether it works. That is,
does meta-tutoring improve students’ learning compared to tutoring without meta-tutoring?
4. Evaluating the meta-tutor
So far, we have conducted 5 studies of the AMT system. In the ﬁrst two studies, which were conducted in the summer of 2010, students
used the tutoring system without the meta-tutor. This led to signiﬁcant changes in the design of the tutoring system (VanLehn et al., 2011),
the materials and the experimental procedure (VanLehn et al., 2011). The two studies also produced data revealing students’ deep and
shallow modelling practices, and thus allowed us to design the meta-tutor. This article presents the results of the next three studies. There
were slight changes to the tutor and the experimental procedure in between the studies 3 and 4, which were conducted in summer 2011.
Analysis of the data from these two studies over the next year led to signiﬁcant changes to the AMT system, such as adding the Plan tab,
which led to study 5, which was conducted in summer 2012.
All the three studies had two phases, training and transfer, as did most of the studies reviewed earlier. During the training phase, a metatutor taught students both to use a good learning strategy (adapted from Pyrenee’s) and to avoid using poor learning strategies. During the
transfer phase, the meta-tutor and the tutor were turned off, thus allowing us to measure both domain learning and spontaneous use of the
learning strategy.
4.1. Research hypotheses
Recall that our framework, adopted from the literature on learning strategies, poses four research questions:
A. Without instruction in good learning strategies, do students often exhibit poor learning strategies?
B. Can good learning strategies be easily taught?
C. When students use the taught learning strategies, does their domain learning increase compared to students who are not taught to use
the learning strategies?
Close the node
/leave the tab

Close the node
/leave the tab

S1

Wrong check
& interval > 3s
Wrong check

Open a node
Get to the input tab

S2

S3

Got the answer Got the answer right
right

Interval
>3s

Wrong check
& interval < 3 sec

Close the node
/leave the tab

S5

Got the answer right

Close the node
/leave the tab

S4
Wrong check
& interval < 3s

S1: no opened node editor
S2: no check
S3: one wrong check
S4: gaming
S5: got right

Fig. 12. Finite state machine for gaming the Check button.

L. Zhang et al. / Computers & Education 75 (2014) 196–217

211

D. After instruction in the learning strategy ceases, do students revert to poor learning strategies?
Our observations during studies 1 and 2 answered question A by demonstrating that our students often use poor learning strategies
when they are not taught good ones. Moreover, because the meta-tutor requires students to follow the Target Node Strategy during the
training phase, question B cannot be addressed by our studies. This leaves us to focus on C and D.
Addressing question C requires that we clearly deﬁne the expected domain learning. As mentioned earlier, two entirely different
instructional objectives are associated with model construction: Using model construction to learn domain concepts and principles (usually
in a science classes) and learning how to construct models (usually in math classes). In these studies, we focus only on the second objective,
so “domain learning” in question C means learning how to do deep model construction. Moreover, question C can be asked of both the
training phase, where the learning strategy is taught, and the transfer phase, when the meta-tutor and tutor are turned off. Thus, the speciﬁc
hypotheses we tested in studies 3, 4 and 5 are:
1. In the transfer phase, do meta-tutored students display deep modelling more frequently than students who were not meta-tutored?
2. In the training phase, do meta-tutored students display deep modelling more frequently than students who do not receive metatutoring?
3. In the transfer phase, do meta-tutored students follow the Target Node Strategy more frequently than students who were not metatutored?
Although we predict positive answers for all three questions, there is a caveat concerning the third hypothesis. The instruction used in
studies 3, 4 and 5 did not provide much meta-cognitive and motivational encouragement to use the learning strategy, because we are
developing that part of the instruction for use in later studies. Thus, we have low expectations for hypothesis 3.
4.2. Method
Students were randomly assigned to one of two conditions: with and without meta-tutoring. The difference between the conditions
occurred only during a training phase where students learned how to solve model construction problems. The meta-tutor group solved
problems with the meta-tutor turned on, while the control group solved the same problems with the meta-tutor turned off. During the
training phase, all students could use the Check and Give-up buttons at any time.
In order to assess how much students learned, a transfer phase followed the training phase. During the transfer phase, all students solved
model construction problems with almost no help. That is, the meta-tutor and the Give-up button were turned off, and the Check button was
turned off everywhere except on the Description tab where it remained enabled in order to facilitate grounding, as mentioned earlier.
Because system dynamics is rarely taught in high school, the procedure did not include a pre-test in modelling dynamic systems.
In order to provide a motivation similar to that which occurs in school, we told students that prizes would be awarded to students who
solved the most problems during the transfer phase. In particular, we repeatedly told them to use the Check and Give-up buttons judiciously
during the training, trying always to learn as much as possible, so that they could rapidly solve problems during the transfer phase when the
Check and Give-up buttons would not be available.
4.3. Participants
There were 34 student participants in the ﬁrst experiment, 44 students participated in the second experiment, and 34 students in the
third experiment. No person participated in more than one experiment. All were high school students who were participating in summer
camps at our university.
4.4. Procedure
The three experiments followed the same procedure. Each lasted two and a half hours and had two main phases, training and transfer.
Sensors recorded students’ physiological states throughout both the training and transfer phases. The sensors were: wireless skin
conductance bracelets, facial expression cameras, posture-sensing chairs, and pressure sensitive mice. Periodically, a window would pop up
and ask students to report their affective state. These data are being used to develop algorithms for affect detection, and will not be discussed further here. Also in the ﬁrst two experiments, but not the third, students were asked to speak their thoughts aloud into a headset
microphone, and their verbal protocols were recorded by screen-capture software.
The summer camp students were available for only a ﬁxed period of time and all needed to be kept occupied productively during that
period. Thus, each phase of the experiment lasted a ﬁxed period of time, and students solved as many problems as they could during that time.
We obtained parental consent for minors prior to the experiment. During the experiment, student gave informed consent, ﬁlled out a
background questionnaire, donned sensors and started the training phase. The training phase lasted a total of 75 min. It consisted of ﬁrst
studying a sequence of PowerPoint slides that introduced them to the user interface, model construction and the Target Node Strategy, and
then solving a sequence of model construction problems. The introduction slides remained available while students were solving problems.
During pilot testing, we noticed that some students spent a long time studying the introduction then rarely referred back to it, whereas
other studied the introduction brieﬂy and referred back to it frequently as they solved problems. Thus, we let students decide how to allocate
their 75 min between the studying introduction and solving the training problems. The training phase was followed by a 15-min break
where students went to another room and had a snack. After the break, all the students began the transfer phase, where both conditions
were identical. The transfer phase lasted for 30 min, and a debrieﬁng of the students followed.
Students in both conditions solved the same problems in the same order. Except for a few debugging problems during the training phase,
where student were asked to correct a given faulty model, both training and transfer problems required students to construct a correct

212

L. Zhang et al. / Computers & Education 75 (2014) 196–217

model, which is deﬁned as a model whose graphs match graphs of correct values. When the model was correct, students were allowed to go
on to the next problem. This procedure was followed for both the training and transfer phases.
The only procedural difference between the training and transfer phases was the amount of scaffolding available. During the training
phase, the Check and Give-up buttons were enabled, and the meta-tutor was active for students in the meta-tutor condition. During the
transfer phase, the Check button was enabled only on the Description tab because it was necessary for grounding, as discussed earlier. None
of the other scaffolding was available during the transfer phase.
4.5. Measures
To test the hypotheses and answer the research questions, 7 measures were deﬁned by extracting information about student’s interaction with software from students’ log ﬁles. The measures are described in this section.
Hypothesis 1 is that the meta-tutored students will use deep modelling more frequently than the control students during the transfer
phase. Deep modelling is not easy to measure directly, so we used three indirect indicators:
 The number of the “Run model” button presses for completing one problem in the transfer phase indicates how much help the student needs
from the system. Deep modellers should be able to complete problems using the Run model button only a couple of times per model.
 Another measure was the number of extra nodes created during the transfer phase, where extra nodes are deﬁned as the nodes that can
be legally created for the problem but are not required for solving the problem. Deep modellers should realize that these quantities are
irrelevant and therefore avoid modelling them.
 The number of problems completed during the 30 min transfer period was considered to be an indicator of deep modelling with the
assumption that it would be faster for students to solve atomic modelling problems deeply than shallowly.
Hypothesis 2 is that meta-tutored students should use deep modelling more frequently than the control group students during the
training phase. The three dependent measures used to evaluate this hypothesis are described next.
 Help button usage: Ideally, a deep modeller would ﬁll out a tab, click on the Check button and the tab’s ﬁelds would turn green (correct).
A student who is trying to do deep modelling but hasn’t mastered the skill might have to click the Check button twice, thinking hard
before each attempt, in order to get them right. On the other hand, shallow modellers might click on the Check button repeatedly as they
guess or use the Give-up button. To measure usage of the help buttons, the following measure was calculated:

Help usage ¼ nwc þ 3ngu



nrn

where nwc is the number of Check button presses that yielded red, ngu is the number of Give-up buttons the student clicked, and nrn is the
number of nodes required by the problem. The “3” here is a weight to make ngu be roughly comparable to nwc. The denominator represents
the number of nodes required for a correct, minimal solution rather than the actual number of nodes the student completed. Should the
prediction for Hypothesis 2 hold true, meta-tutored students’ usage of the help button should be lower than the control students’ one.
 Correct on ﬁrst Check: This measure for Hypothesis 2 is a traditional one in tutoring system research. It represents how frequently
students succeed on their ﬁrst attempt at ﬁlling out a tab. That is, what proportion of the tabs turned green when students ﬁrst clicked
on the Check button? Deep modellers should get almost everything right the ﬁrst time, whereas a student who is confused or guessing
might rarely get elements right on the ﬁrst try. Unfortunately, this measure could only be applied to experiment 5, as insufﬁcient log
data was kept during the 2011 experiments.
 Training efﬁciency: This measure is based on the (now dubious) assumption that deep modelling is faster than shallow modelling. Speed
is often measured by counting the number of problems solved during a ﬁxed training period. However, our problems’ solutions varied in
their complexity. Some had many nodes and some had few nodes. Moreover, students could always use the Give-up button, which does
part of the problem solving for them. There was one Give-up button per tab, and completing a node requires ﬁlling out three tabs, so
clicking on three Give-up buttons per node would solve the problem. Thus, a better measure of the amount of problem solving
accomplished than “problems completed” is the number of tabs the student completed without using the Give-up button, which is
given by:

Training efficiency ¼ 3ncn  ngu
where ncn is the number of nodes that the student completed correctly (so 3ncn is the number of tabs), and ngu is the number of Give-up
buttons the student clicked. The prediction for Hypothesis 2 is that the training efﬁciency of meta-tutored students should be higher than
the training efﬁciency of control students.
Hypothesis 3 is that the experimental group, which was required to follow the Target Node Strategy during training, would continue to
use it during the transfer phase. To evaluate this hypothesis, we calculated the proportion of student steps consistent with the target node
strategy. The algorithm of Fig. 12 describes how it is calculated.
5. Results
Experiments 1 and 2 found, as expected, that students often exhibit shallow learning strategies (VanLehn et al., 2011a). They also led to
many revisions in the software and the experimental procedure (VanLehn et al., 2011b). However, both experiments had only one condition,
and it did not include the meta-tutor. This section reports on comparisons of the system with the meta-tutor turned on and turned off.

L. Zhang et al. / Computers & Education 75 (2014) 196–217

213

5.1. Experiment 3 results
Experiment 3 was conducted in June 2011. Of the 34 participants, some students’ data needed to be omitted. Probably because there were
too many introduction slides (101 in total), 11 out of 34 students were still in the introduction phase at the break, and thus had no time in the
training phase where the manipulation occurred. These 11 students were omitted from the analyses. Two students, one in each condition,
performed much better than others. Both students’ training measures and test measures were greater than three standard deviations from
the means. These two students were also excluded from the analyses. The ﬁnal number of students left for each group was 11 (control) and
12 (meta-tutor). All the analyses below were computed based on these 23 students.
5.1.1. Hypothesis 1 (transfer phase deep modelling)
Run Model button usage: Of the 23 students, there were only 9 Meta-tutored students and 4 control students that ﬁnished the ﬁrst
problem in the transfer phase. Among the 13 students, meta-tutored students used the Run Model button 4.88 times per problem, while
students in control group used it 6.67 times. Due to the limited number of subjects, it is not surprising that the difference is not signiﬁcant
(p ¼ 0.72, d ¼ 0.12). This T-test, and all other tests reported here, are two-tailed.
Extra nodes: The ﬁrst problem of the transfer phase allowed 2 extra nodes, thus allowing us to measure the number of extra nodes created
by students who completed that problem. Meta-tutor students deﬁned 0.44 (SD ¼ 0.8) extra nodes vs. 0.75 (SD ¼ 0.96) extra nodes for the
control students. The difference was not reliable (p ¼ 0.61, d ¼ 0.58) although it was in the expected direction.
Number of problems completed: The average number of problems completed during the transfer phase was 0.82 (SD ¼ 0.60) for the metatutor students vs. 0.35 (SD ¼ 0.52) for the control students. The difference was marginally signiﬁcant (p ¼ 0.057) even though the effect size
was large (d ¼ 0.88) and in the expected direction. These ﬁgures show that on average, students completed less than one problem. More
speciﬁcally, 9 meta-tutored and 4 control students ﬁnished one or more problems, which was a marginally reliable difference (c2 ¼ 3.486,
p ¼ 0.06).
In short, trends in the data support Hypothesis 1 but the differences were not reliable, probably due to the small sample size.
5.1.2. Hypothesis 2 (training phase deep modelling)
Help button usage: This measure is a weighted sum of help button uses divided by the total number of nodes completed. On this measure,
the meta-tutor students averaged 4.78 (SD ¼ 2.25) vs. 7.03 (SD ¼ 3.44) for the control students. The difference was marginally signiﬁcant
according with a large effect size (p ¼ 0.08, d ¼ 0.82).
Training efﬁciency: The average training efﬁciency measure the amount of correct work done by the student during the ﬁxed-length
training period. For the meta-tutor group, training efﬁciency was 13.00 (SD ¼ 6.94) vs. 11.45 (SD ¼ 6.77) for the control group. The difference was not signiﬁcant (p ¼ 0.30; d ¼ 0.23).
Thus, although there was a trend in the data supporting Hypothesis 2, the reliability was poor, perhaps due to the small sample size.
5.1.3. Hypothesis 3 (meta-strategy usage)
Hypothesis 3 was that meta-tutored students would voluntary the Target Node Strategy during the transfer phase more frequently than
the student who was not meta-tutored. During the transfer phase, 0.39 (SD ¼ 0.35) of the meta-tutor students’ steps matched the Target
Node Strategy’s steps, vs. 0.34 (SD ¼ 0.30) for the control students. This difference was quite small (p ¼ 0.70, d ¼ 0.16), suggesting that
hypothesis 3 may be false for this study.
5.1.4. Summary of results and next step forward
Although there were some trends in the expected directions, the students in both conditions performed quite poorly. This was probably
due to the large number of slides in the introduction phase as well as the difﬁculty of the tasks themselves.
In order to increase the number of training and transfer problems solved by students, thus allowing us to differentiate their performance
statistically, we reduced the number of slides from 101 to 64, and simpliﬁed some of the tasks. As the second experiment conﬁrmed, these
changes led to an improvement on the performance students in both phases.

5.2. Experiment 4 results
Experiment 4 took place in July 2011. Data from all 44 participants (22 in each condition) were used in the analyses here.
5.2.1. Hypothesis 1 (transfer phase deep modelling)
Run Model button usage: To complete the ﬁrst problem in the transfer phase, meta-tutored students used the Run model button 3.05
times on average. In comparison, students in the control group used the Run model button 5.13 times. However, the difference was not
signiﬁcant (p ¼ 0.31, d ¼ 0.32). Because the standard deviation was high (SD ¼ 6.77) due to extreme values, we divided all the students into
two types with the threshold being 2, which is the median. The students who used the run model button once or twice were considered
deep modellers. The rest of the students were considered shallow modellers. Chi-square test is then used to compare the number of deep
modellers in meta-tutored group to the number in control group. There was a trend in the expected direction, but the signiﬁcance was
marginal (c2 ¼ 3.36, p ¼ 0.067).
Extra nodes: Because most of the students ﬁnished at least two tasks in the transfer phase (only 3 students in the control group did not)
and the second task allowed up to two extra nodes, we used the second task to count extra nodes. As predicted by Hypothesis 1, metatutored students produced fewer extra nodes (0.27, SD ¼ 0.70) than control students (0.95, SD ¼ 1.03). The difference was signiﬁcant
with a large effect size (p ¼ 0.02, d ¼ 0.80).
Problems completed: The meta-tutor students solved 3.27 (SD ¼ 1.03) transfer problems vs. 3.23 (SD ¼ 1.57) for the control students. The
difference was small and not signiﬁcant (p ¼ 0.65, d ¼ 0.04).

214

L. Zhang et al. / Computers & Education 75 (2014) 196–217

5.2.2. Hypothesis 2 (training phase deep modelling)
Help button usage: Consistent with Hypothesis 2, meta-tutor students’ help button usage averaged 2.35 (SD ¼ 1.75) vs. 3.55 (SD ¼ 1.85)
for the control students. The difference was signiﬁcant (p ¼ 0.04, d ¼ 0.68).
Training efﬁciency: Contrary to Hypothesis 2, the control students had higher training efﬁciency 72.77 (SD ¼ 32.12) than the meta-tutor
students, 54.36 (20.17), and the difference was marginally signiﬁcant (p ¼ 0.05, d ¼ 0.70).
These results suggest that meta-tutor students did increase deep modelling in the training phase than the control students, but they also
moved slower than control students.
5.2.3. Hypothesis 3 (use of Target Node Strategy)
Missing data precluded testing this hypothesis in experiment 4.
5.2.4. Summary of results, revisions and the next iteration
Both hypothesis 1 (transfer) and hypothesis 2 (training) were supported, albeit by one measure each. We were not satisﬁed with the pace
of the students in the training phase, especially the meta-tutored students. Watching the screen-capture video suggested that meta-tutored
students spent a lot of time in reading and answering the tutorial pop-ups. Students also became “stuck” (Burleson & Picard, 2007) when
ﬁlling out the current Inputs tab and needing to create new nodes. In order to do so, students had to close the node editor and click on the
Create Node button on the canvas in the Model tab. However, many students could not realize that they had to close the node editor. They
complained that no button in the Inputs tab could help them get out of the stuck state. Thus, we spent several months reﬁning both the
tutoring system and the meta-tutor. We installed the Plan tab in order to reduce the time spent reading the meta-tutor’s pop-ups. We also
improved the interface, adding a “create a new node,” button on the Input tab with the goal of reducing students’ state of stuck in this stage.
The transfer phase proved to be extremely challenging for the students. This was evident in the relatively small number of problems
solved as well as direct observation of the students. The null result on our productivity measure, number of problems solved in the transfer
phase, might be due to students in both conditions becoming discouraged and ceasing to try hard. Thus, we modiﬁed the transfer phase so
that besides colouring the “g” indicator, the system coloured the “i” indicator and “c” indicator as well to show the correctness of the
corresponding tabs. This was intended to make it easier to locate errors while leaving unchanged the logic required for ﬁxing the errors, and
thus allowing students to make faster progress through the test problems while still allowing us to assess their skill.
5.3. Experiment 5 results
Experiment 5 was conducted in June 2012. Of the 34 participants, data from 33 were used in the analyses below (16 in the control group
and 17 in meta-tutor group). One student was excluded due to his extraordinary performance. He ﬁnished all 7 problems in the transfer
phase well before the end of the transfer phase, while the second fastest person only completed 4 problems.
5.3.1. Hypothesis 1 (transfer phase deep modelling)
Run Model button usage: On average, students in the control group used the Run model button 7.76 times, while meta-tutored students
used the Run model button 7.82 times. So they almost had the same performance (p ¼ 0.98, d ¼ 0.0093).
Extra nodes: Most of the students ﬁnished at least two tasks in the transfer phase (one in each group didn’t), so we again used the second
task to measure extra nodes. As predicted by Hypothesis 1, the meta-tutor students produced fewer extra nodes (0.88, SD ¼ 0.96) than the
control students (1.13, SD ¼ 0.99). However, the difference was not reliable (p ¼ 0.47, d ¼ 0.26).
Problems solved: Contrary to Hypothesis 1, the meta-tutor students solved 2.18 (SD ¼ 0.53) transfer problems vs. 2.56 (SD ¼ 0.78) for the
control students. Students in the control group outperformed meta-tutored student with marginal signiﬁcance (p ¼ 0.094, d ¼ 0.57).
Once again, the meta-tutor students tended to work more slowly than the control students. This time, there was only a trend to show that
they might be doing more deep modelling than the control students.
5.3.2. Hypothesis 2 (training phase deep modelling)
Help button usage: As expected, meta-tutored students’ help button usage averaged 3.92 (SD ¼ 2.19) vs. 6.13 (SD ¼ 2.73) for the control
students. The difference was signiﬁcant with a large effect size (p ¼ 0.016, d ¼ 0.89).
Correct on ﬁrst attempt: To provide an additional test of Hypothesis 2, we calculated the percentage of time that the ﬁrst Check on a tab
was correct. Meta-tutored students achieved a higher percentage (0.77, SD ¼ 0.068) than control students (0.68, SD ¼ 0.11), and the difference was signiﬁcant with a large effect size (p ¼ 0.015, d ¼ 0.98).
Training efﬁciency: Meta-tutor students scored 73.18 (SD ¼ 27.53), a little bit higher in training efﬁciency than control students, 68.88
(SD ¼ 17.16), but the difference was not reliable (p ¼ 0.59, d ¼ 0.19).
So students in both groups kept the same pace in the training session this time, and there was strong evidence that the meta-tutor
students were engaged in deeper modelling than the control students.
5.3.3. Hypothesis 3 (use of Target Node Strategy)
Contrary to hypothesis 3, the meta-tutored students had nearly the same level of Target Node Strategy usage (0.66, SD ¼ 0.23) as control
students (0.70, SD ¼ 0.19), and the difference is not reliable (p ¼ 0.59, d ¼ 0.19).
6. Discussion
Our results are summarized in Table 1. This section discusses our interpretation of them.
As mentioned in the introduction, whenever a new kind of instruction is developed, there are four classic questions to ask about learning
strategies that are speciﬁc to it:

L. Zhang et al. / Computers & Education 75 (2014) 196–217

215

Table 1
Summary of the results.
Measure (predicted dir.)

Experiment 3 (N ¼ 23)

Experiment 4 (N ¼ 44)

Experiment 5 (N ¼ 33)

Transfer phase (Hypothesis 1)
Run model button usage (E < C)
Extra nodes (E < C)
Probs completed (E > C)

Not available
E < C (p ¼ 0.61, d ¼ 0.58)
E > C (p ¼ 0.06, d ¼ 0.88)

E < C (p ¼ 0.31, d ¼ 0.32)
E < C (p [ 0.02, d [ 0.80)
E z C (p ¼ 0.65, d ¼ 0.04)

E z C (p ¼ 0.98, d ¼ 0.0093)
E < C (p ¼ 0.47, d ¼ 0.26)
E < C (p ¼ 0.09, d ¼ 0.57)

Training phase (Hypothesis 2)
Help button usage (E < C)
Correct on 1st Chk (E > C)
Efﬁciency (E > C)

E < C (p ¼ 0.08, d ¼ 0.82)
Missing data
E > C (p ¼ 0.30, d ¼ 0.23)

E < C (p [ 0.04, d [ 0.68)
Missing data
E < C (p ¼ 0.05, d ¼ 0.70)

E < C (p [ 0.02, d [ 0.89)
E > C (p [ 0.015, d [ 0.98)
E > C (p ¼ 0.59, d ¼ 0.19)

Missing data

E z C (p ¼ 0.59, d ¼ 0.19).

Transfer phase use of Target Node Strategy (Hypothesis 3)
Usage (E ¼ C)
E z C (p ¼ 0.70, d ¼ 0.16)

E stands for the meta-tutor group, and C stands for the control group. Reliable results are bold.

A. Without instruction in good learning strategies, do students often exhibit poor learning strategies?
B. Can good learning strategies be easily taught?
C. When students use the taught learning strategies, does their domain learning increase compared to students who are not taught to use
the learning strategies?
D. When instruction in the learning strategy ceases, do students revert to poor learning strategies?
After we developed a step-based tutoring system for model construction, experiments 1 and 2 answered question A by ﬁnding that
student did indeed exhibit poor learning strategies when using it. This led us to develop a meta-tutor that taught students a meta-strategy
that we hoped would increase their acquisition of skill in model construction.
Answering question B requires that even during the training phase, students have the freedom to choose between following or not
following the learning strategy. This freedom allows experimenters to measure the growth in compliance during the training phase. Of the
four systems reviewed earlier (Betty’s Brain, the Help Tutor, Co-Lab and Pyrenees; see Table 2), Betty’s Brain and the Help Tutor provide such
freedom, and thus were able to show that while students were being meta-tutored, their behaviours were more often consistent with the
learning strategy than the behaviours of students who were not being meta-tutored. This suggests that their meta-tutoring was effective at
getting students to use the taught learning strategy. AMT, Co-Lab and Pyrenees all required students to follow the taught learning strategy
during the training phase, so they could not answer question B.
Given a learning strategy that designers think is good, question C asks whether it really does increase domain learning. Of the four
systems reviewed earlier, only Pyrenees’ learning strategy increased domain learning. Thus, we adapted its learning strategy for use in AMT.
Because the goal of our system is teach students to construct models properly, most of the measures addressed the frequency of deep
modelling. During the training phase, on the measures “help button usage” and “correct on ﬁrst check” meta-tutored students scored
reliably higher than students who were not meta-tutored, except on experiment 3, which appears to have been underpowered. Moreover,
the effect sizes were large (d ¼ 0.89; d ¼ 0.98) or moderately larger (d ¼ 0.68). Thus, the AMT results for domain learning in the training
phase were nearly as good as those from Pyrenees, and substantially better than the other three meta-tutoring systems.
Unfortunately, neither the Target Node Strategy nor the domain learning advantages transferred. During the transfer phase, on the
measures “Run model button usage”, “extra nodes” and “Target Node Strategy usage”, the meta-tutored students were no better than the
students who had not been meta-tutored earlier, with one exception. On the measure “extra nodes” in experiment 4, meta-tutored students
outscored the control students. It is always difﬁcult to get learning to transfer from a supported context to an unsupported one, so this lack of
transfer should perhaps not be surprising. However, it does appear to be a bit weaker than the transfer obtained by Pyrenees, Betty’s Brain
and the Help Tutor.
In both the training and transfer phases, we attempted to measure deep modelling using efﬁciency: the amount of modelling done in a
ﬁxed period of time. This measurement made the tacit assumption that deep modelling is faster than shallow modelling. That is, thinking
hard to solve an atomic modelling problem should take less time than guessing, overusing the Give-up button, overusing the Run button,
scanning the problem statement for keywords and other shallow model construction tactics. However, the efﬁciency measures all produced
null results. If anything, there was trend for non-meta-tutored students to get more done than the meta-tutored students. This is consistent
with our informal analyses of the log data. It appears that guessing was actually quite a bit faster than thinking, especially in the training
phase when the Check button was enabled. Even when students could only use the Run Model button for feedback in the transfer phase,
guessing was fast because the models were so small for the ﬁrst few problems. After that, guessing became must less efﬁcient, but few
students got that far.
As the experience with Betty’s Brain and the Help Tutor shows, not every supposedly good learning strategy actually turns out to increase
domain learning. In the case of AMT, we have found a learning strategy that does increase domain learning, and thus deserves to be taught.
On the other hand, our method of teaching the learning strategy appears not to have had enough meta-cognitive or motivational impact
on students, because their gains while being meta-tutor did not persist when the meta-tutoring was turned off.
Thus, the next stage of the AMT project is to augment the instruction with an affective learning companion. Its job will be to persuade
students of the beneﬁts of using the Target Node Strategy and of not abusing the Check and Give-up buttons. The agent cannot use the
argument that the learning strategy and deep modelling will speed up the students’ work, because we have found that shallow modelling
strategies are actually faster at least on these simple problems. Thus, we plan on having the agent use Dweck’s well-known argument
(Dweck & Leggett, 1988) that the “mind is a muscle; the harder you exercise it, the stronger it becomes.” Our summer school students
presumably want stronger minds, so if they believe the agent, they should more often use both the learning strategy and deep modelling. In
order to make the agent easier to believe, we plan to use attribution shifting, empathy, rapport-building chit-chat, and other non-cognitive

216

L. Zhang et al. / Computers & Education 75 (2014) 196–217

Table 2
Comparison of four meta-tutors of model construction.
Meta-tutor

AMT
Pyrenees
Help Tutor
Betty’s Brain
Co-Lab

Training phase

Transfer phase

Knowledge/skill

Meta-strategy

Knowledge/skill

Meta-strategy

þ
þþ
NS
NS
NS

Required
Required
þ
þ
Required

þ
þþ
NS
þ

NS
NS
þ
þ

NS ¼ non-signiﬁcant difference, two-tailed. þ ¼ Signiﬁcant but weak. þþ ¼ Signiﬁcant and strong. Required ¼ meta-tutor required student to follow the learning strategy.

techniques. In order to optimize the timing and selection of these non-cognitive interventions, we plan to monitor the students’ affective
state using physiological sensors.
Lastly, by experiment 5, students seem to be acquiring decent amounts of competence in system dynamics modelling in only 1 h and
15 min total. This is a considerable reduction from the 5 or more hours required of Model-It students and others. Although we have no way
to actually compare our results to the early ones, because the systems, students, domains and almost everything else were quite different,
we nonetheless are quite encouraged. The combination of tutoring and meta-tutoring may be the key to getting model construction out into
the classrooms at last.
Acknowledgements
This material is based upon work supported by the National Science Foundation, United States (_100000001) under Grant No. 0910221.
References
Alessi, S. M.. (December 2000). The application of system dynamics modeling in elementary and secondary school curricula. In Paper presented at the RIBIE 2000 – The ﬁfth
Iberoamerican conference on informatics in education. Viña del Mar, Chile.
Aleven, V., McLaren, B., Roll, I., & Koedinger, K. (2004). Toward tutoring help seeking (applying cognitive modeling to help-seeking skills). In J. C. Lester, R. M. Vicari, &
F. Paraguacu (Eds.), Intelligent tutoring systems: Seventh international conference: ITS 2005 (pp. 227–239). Berlin: Springer.
Aleven, V., Stahl, E., Schworm, S., Fischer, F., & Wallace, R. M. (2003). Help seeking and help design in interactive learning environments. Review of Educational Research, 73(2),
277–320.
Baker, R. S. J. d., Corbett, A., & Koedinger, K. R. (2004). Detecting student misuse of intelligent tutoring systems. In Proceedings of the 7th international conference on intelligent
tutoring systems (pp. 531–540).
Baker, R. S., Corbett, A., Koedinger, K. R., Evenson, S., Roll, I., Wagner, A. Z., et al. (2006). Adapting to when students game an intelligent tutoring system. In Intelligent tutoring
systems (pp. 392–401). Berlin: Springer.
Baker, R. S. J. d., Corbett, A., Koedinger, K. R., & Wagner, A. Z. (2004). Off-task behavior in the cognitive tutor classroom: when students “game the system”. In E. DykstraErickson, & M. Tscheligi (Eds.), Proceedings of the SIGCHI conference on human factors in computing systems (pp. 383–390). New York, NY: ACM.
Biswas, G., Leelawong, K., Schwartz, D. L., & Vye, N. J. (2005). Learning by teaching: a new agent paradigm for educational software. Applied Artiﬁcial Intelligence, 19, 363–392.
Booth Sweeney, L., & Sterman, J. D. (2000). Bathtub dynamics: initial results of a systems thinking inventory. System Dynamics Review, 16(4), 249–286.
Bravo, C., van Joolingen, W. R., & de Jong, T. (2009). Using Co-Lab to build system dynamics models: students’ actions and on-line tutorial advice. Computer and Education, 53,
243–251.
Bredeweg, B., & Forbus, K. D. (2003). Qualitative modeling in education. AI Magazine, 24(4), 35–46.
Burleson, W., & Picard, R. W. (2007). Affective learning companions. Educational Technology, Special Issue on Pedagogical Agents, Saddle Brook, N.J., 47(1), 28–32.
CCSSO. (2011). The Common Core State Standards for mathematics. Downloaded from www.corestandards.org. on 31.10.11.
Chi, M., & VanLehn, K. (2010). Meta-cognitive strategy instruction in intelligent tutoring systems: how, when and why. Journal of Educational Technology and Society, 13(1), 25–
39.
Chin, D., Dohmen, I. I. M., Cheng, B. H., Oppezzo, M., Chase, C. C., & Schwartz, D. L. (2010). Preparing students for future learning with teachable agents. Educational Technology
Research and Development, 58, 649–669.
Clark, H. H., & Brennan, S. E. (1991). Grounding in communication. In L. B. Resnick, J. M. Levine, & S. D. Teasley (Eds.), Perspectives on socially shared cognition (pp. 127–149).
Washington, DC: American Psychological Association.
Collins, A., & Ferguson, W. (1993). Epistemic forms and epistemic games: structures and strategies to guide inquiry. Educational Psychologist, 28(1), 25–42.
Doerr, H. M. (1996). Stella ten-years later: a review of the literature. International Journal of Computers for Mathematical Learning, 1, 201–224.
Donker, A. S., de Boer, H., Kostons, D., Dignath van Ewijk, C. C., & van der Werf, M. P. C. (2014). Effectiveness of learning strategy instruction on academic performance: a metaanalysis. Educational Research Review, 11, 1–26.
Du Boulay, B., Avramides, K., Luckin, R., Martínez-Mirón, E., & Rebolledo-Méndez, G. (2010). Towards systems that care: a conceptual framework based on motivation,
metacognition and affect. International Journal of Artiﬁcial Intelligence in Education, 20(3), 197–229.
Dweck, C. S., & Leggett, E. L. (1988). A social-cognitive approach to motivation and personality. Psychological Review, 95(2), 256–273.
Gonzalez-Sanchez, J., Chavez-Echeagaray, M.-E., VanLehn, K., & Burleson, W. (2011). From behavioral description to a pattern-based model for intelligent tutoring systems. In
Paper presented at the Proceedings of the 18th international conference on pattern languages of programs (PLoP). Portland, OR.
Hashem, K., & Mioduser, D. (2011). The contribution of learning by modeling (LbM) to students’ understanding of complexity concepts. International Journal of e-Education, eBusiness, e-Management and e-Learning, 1(2), 151–157.
Hattie, J., Biggs, J., & Purdie, N. (1996). Effects of learning skills interventions on student learning: a meta-analysis of ﬁndings. Review of Educational Research, 66, 99–136.
Hestenes, D. (2007). Modeling theory for math and science education. In Paper presented at the ICTMA-13: The international community of teachers of mathematical modelling
and applications. Indiana, IL.
Hogan, K., & Thomas, D. (2001). Cognitive comparisons of students’ systems modeling in ecology. Journal of Science Education and Technology, 10(4), 319–345.
Lee, C. B., Jonassen, D., & Teo, T. (2011). The role of model building in problem solving and conceptual change. Interactive Learning Environments, 19(3), 247–265.
Leelawong, K., & Biswas, G. (2008). Designing learning by teaching agents: the Betty’s brain system. International Journal of Artiﬁcial Intelligence and Education, 18(3), 181–208.
Löhner, S., Van Joolingen, W. R., & Savelsbergh, E. R. (2003). The effect of external representation on constructing computer models of complex phenomena. Instructional
Science, 31, 395–418.
Mandinach, E. B., & Cline, H. F. (1994a). Classroom dynamics: Implementing a technology-based learning environment. Mahwah, NJ: Erlbaum.
Mandinach, E. B., & Cline, H. F. (1994b). Modeling and simulation in the secondary school curriculum: the impact on teachers. Interactive Learning Environments, 4(3), 271–289.
Marshall, S. P., Barthuli, K. E., Brewer, M. A., & Rose, F. E. (1989). Story problem solver: A schema-based system of instruction. San Diego, CA: Center for Research in Mathematics
and Science Education, San Diego State University.
Metcalf, S. J. (1999). The design of guided learner-adaptable scaffolding in interactive learning environment (Doctoral Dissertation). University of Michigan.
Metcalf, S. J., Krajcik, J., & Soloway, E. (2000). Model-It: a design retrospective. In M. J. Jacobson, & R. B. Kozma (Eds.), Innovations in science and mathematics education:
Advanced designs for technologies of learning (pp. 77–115).

L. Zhang et al. / Computers & Education 75 (2014) 196–217

217

Mulder, Y. G., Lazonder, A. W., de Jong, T., Anjewierden, A., & Bollen, L. (2011). Validating and optimizing the effects of model progression in simulation-based inquiry learning.
Journal of Science Education and Technology, 21, 722–729.
Muldner, K., Burleson, W., van de Sande, B., & VanLehn, K. (2011). An analysis of students’ gaming behaviors in an intelligent tutoring system: predictors and impacts. User
Modeling and User-Adapted Interaction, 21(1–2), 99–135.
Nathan, M. J. (1998). Knowledge and situational feedback in a learning environment for algebra story problem solving. Interactive Learning Environments, 5, 135–159.
National Research Council. (2012). A framework for K–12 science education: Practices, crosscutting concepts, and core ideas. Washington, DC: National Academies Press.
Richmond, B. M. (1985). STELLA: software for bringing system dynamics modeling to the other 98%. In Paper presented at the Proceedings of the 1985 international conference of
the System Dynamics Society: 1985 International system dynamics conference.
Roll, I., Aleven, V., McLaren, B., & Koedinger, K. R. (2007a). Can help seeking be tutored? Searching for the secret sauce of metacognitive tutoring. In Proceedings of the international conference on artiﬁcial intelligence in education (pp. 203–210). Amsterdam: IOS Press.
Roll, I., Aleven, V., McLaren, B., & Koedinger, K. R. (2007b). Designing for metacognition – applying cognitive tutor principles to the tutoring of help seeking. Metacognition and
Learning, 2(2).
Roll, I., Aleven, V., McLaren, B., & Koedinger, K. R. (2011). Improving students’ help-seeking skills using metacognitive feedback in an intelligent tutoring system. Learning and
Instruction, 267–280.
Roll, I., Aleven, V., McLaren, B., Ryu, E., Baker, R. S. J. d., & Koedinger, K. R. (2006). The Help Tutor: does metacognitive feedback improve student’s help-seeking actions, skills
and learning. In M. Ikeda, K. Ashley, & T.-W. Chan (Eds.), Intelligent tutoring systems: 8th International conference, its 2006 (pp. 360–369). Berlin: Springer.
Russell, S., & Norvig, P. (2009). Artiﬁcial intelligence: A modern approach (2nd ed.). Upper Saddle River, NJ: Prentice Hall.
Schecker, H. (1993). Learning physics by making models. Physics Education, 28, 102–106.
Segedy, J. R., Kinnebrew, J. S., & Biswas, G. (2012a). Relating student performance to action outcomes and context in a choice-rich learning environment. In S. A. Cerri,
W. J. Clancey, & G. Papadourakis (Eds.), Intelligent tutoring systems: 11th International conference its 2012 (pp. 505–510). Berlin: Springer-Verlag.
Segedy, J. R., Kinnebrew, J. S., & Biswas, G. (2012b). Supporting student learning using conversational agents in a teachable agent environment. In Paper presented at the
Proceedings of the 10th international conference of the learning sciences. Sydney, Australia.
Shih, B., Koedinger, K. R., & Scheines, R. (2008). A response time model for bottom-out hints as worked examples. In C. Romero, S. Ventura, M. Pechenizkiy, & R. S. J. d Baker
(Eds.), Handbook of educational data mining (pp. 201–211). Boca Raton, FL: Taylor & Francis.
Steed, M. (1992). Stella, a simulation construction kit: cognitive process and educational implications. Journal of Computers in Mathematics and Science Teaching, 11, 39–52.
Stratford, S. J. (1997). A review of computer-based model research in precollege science classroom. Journal of Computers in Mathematics and Science Teaching, 16(1), 3–23.
Tan, J., Biswas, G., & Schwartz, D. L. (2006). Feedback for metacognitive support in learning by teaching environments. In Proceedings of the twenty-eighth annual meeting of the
Cognitive Science Society. Mahwah, NJ: Erlbaum.
Tan, J., Wagster, J., Wu, Y., & Biswas, G. (2007). Effect of metacognitive support on student behaviors in learning by teaching environments. In R. Luckin, K. R. Koedinger, &
J. Greer (Eds.), Proceedings of the 13th international conference on artiﬁcial intelligence in education (pp. 650–652). Amsterdam: IOS Press.
Timms, M. J. (2007). Using item response theory (IRT) to select hints in an ITS. In R. Luckin, K. R. Koedinger, & J. Greer (Eds.), Artiﬁcial intelligence in education (pp. 213–221).
Amsterdam: IOS Press.
Treagust, D. F., Chittleborough, G., & Mamiala, T. (2002). Students’ understanding of the role of scientiﬁc models in learning science. International Journal of Science Education,
24(4), 357–368.
VanLehn, K. (2006). The behavior of tutoring systems. International Journal of Artiﬁcial Intelligence and Education, 16, 227–265.
VanLehn, K. (2013). Model construction as a learning activity: a design space and review. Interactive Learning Environments, 21(4), 371–413.
VanLehn, K., Burleson, W., Chavez-Echeagaray, M.-E., Christopherson, R., Gonzalez-Sanchez, J., Hastings, J., et al. (2011a). The level up procedure: how to measure learning
gains without pre- and post-testing. In T. Hirashima (Ed.), Proceedings of the 19th international conference on computers in education (pp. 96–100). Chiang-Mai, Thailand:
Asia-Paciﬁc Society for Computers in Education.
VanLehn, K., Burleson, W., Chavez-Echeagaray, M.-E., Christopherson, R., Gonzalez-Sanchez, J., Hastings, J., et al. (2011b). The affective meta-tutoring project: how to motivate
students to use effective meta-cognitive strategies. In Paper presented at the 19th International conference on computers in education. Chiang Mai, Thailand.
Vanlehn, K., & Chi, M. (2012). Adaptive expertise as acceleration of future learning: a case study. In P. J. Durlach, & A. Lesgold (Eds.), Adaptive technologies for training and
education. Cambridge: Cambridge University Press.
Wagster, J., Tan, J., Biswas, G., & Schwartz, D. L. (2007). How metacognitive feedback affects behavior in learning and transfer. In Paper presented at the 13th International
conference on artiﬁcial intelligence in education: Workshop on metacognition and self-regulated learning in ITSs. Marina del Rey, CA.
Wagster, J., Tan, J., Wu, Y., Biswas, G., & Schwartz, D. L. (2007). Do learning by teaching environments with metacognitive support help students develop better learning
behaviors?. In Proceedings of the twenty-sixth annual meeting of the Cognitive Science Society. Mahwah, NJ: Erlbaum.
Wheeler, J. L., & Regian, J. W. (1999). The use of a cognitive tutoring system in the improvement of the abstract reasoning component of word problem solving. Computers in
Human Behavior, 15, 243–254.
Wilensky, U. (2003). Statistical mechanics for secondary school: the GasLab multi-agent modeling toolkit. International Journal of Computers for Mathematical Learning, 8(1),
1–41.
Wilensky, U., & Reisman, K. (2006). Thinking like a wolf, a sheep, or a ﬁreﬂy: learning biology through constructing and testing computational theories – an embodied
modeling approach. Cognition and Instruction, 24(2), 171–209.
Zaraza, R., & Fisher, D. (1997). Introducing system dynamics into the traditional secondary curriculum: the CC-STADUS project’s search for leverage points. In Paper presented
at the 15th International system dynamics conference. Istanbul, Turkey.
Zaraza, R., & Fisher, D. (1999). Training system modelers: the NSF CC-STADUS and CC-SUSTAIN projects. In W. Feurzeig, & N. Roberts (Eds.), Modeling and simulation in science and
mathematics education (Vol. 1); (pp. 38–69). New York, NY: Springer.

User Model User-Adap Inter (2011) 21:99–135
DOI 10.1007/s11257-010-9086-0
ORIGINAL PAPER

An analysis of students’ gaming behaviors
in an intelligent tutoring system: predictors
and impacts
Kasia Muldner · Winslow Burleson ·
Brett Van de Sande · Kurt VanLehn

Received: 30 April 2010 / Accepted in revised form: 17 November 2010 /
Published online: 4 January 2011
© Springer Science+Business Media B.V. 2010

Abstract Students who exploit properties of an instructional system to make
progress while avoiding learning are said to be “gaming” the system. In order to investigate what causes gaming and how it impacts students, we analyzed log data from
two Intelligent Tutoring Systems (ITS). The primary analyses focused on six college
physics classes using the Andes ITS for homework and test preparation, starting with
the research question: What is a better predictor of gaming, problem or student? To
address this question, we developed a computational gaming detector for automatically labeling the Andes data, and applied several data mining techniques, including
machine learning of Bayesian network parameters. Contrary to some prior findings,
the analyses indicated that student was a better predictor of gaming than problem.
This result was surprising, so we tested and confirmed it with log data from a second
ITS (the Algebra Cognitive Tutor) and population (high school students). Given that
student was more predictive of gaming than problem, subsequent analyses focused
on how students gamed and in turn benefited (or not) from instructional features of
the environment, as well as how gaming in general influenced problem solving and
learning outcomes.

K. Muldner (B)
Department of Psychology, Arizona State University, Tempe, AZ, USA
e-mail: katarzyna.muldner@asu.edu
W. Burleson · B. Van de Sande · K. VanLehn
School of Computing, Informatics and Decision Systems Engineering, Arizona State University,
Tempe, AZ, USA
e-mail: winslow.burleson@asu.edu
B. Van de Sande
e-mail: bvds@asu.edu
K. VanLehn
e-mail: kurt.vanlehn@asu.edu

123

100

K. Muldner et al.

Keywords Educational data mining · Gaming · Utility of hints ·
Bayesian network parameter learning

1 Introduction
Students have long found ways to avoid reasoning about instructional materials, e.g.,
by copying from examples to generate problem solutions (VanLehn 1998), by paraphrasing text instead of self-explaining it more deeply (Chi et al. 1989), or by passively
listening to tutors’ didactic explanations without providing substantial contributions,
even though active participation is needed for effective learning (Chi et al. 2001). A
name given to shallow reasoning in the context of an Intelligent Tutoring System (ITS)
is gaming: “attempting to succeed by exploiting properties of the system rather than by
learning the material” (Baker et al. 2009). Not surprisingly, gaming is associated with
reduced learning (Baker et al. 2004b), and so there have been efforts to detect gaming
(Baker et al. 2006b, 2008a), understand gaming (Baker et al. 2008b; Rodrigo et al.
2008; Baker et al. 2009) and reduce gaming (Murray and VanLehn 2005; Walonoski
and Heffernan 2006b). While progress has been made on each front, challenges still
remain.
A key challenge pertains to understanding the causes of gaming. Several projects
have tackled this challenge by looking for statistical associations with gaming (Arroyo
and Woolf 2005; Baker 2007; Rodrigo et al. 2008; Baker et al. 2009). Early work
focused on student features and how they correlate with gaming, such as students’
goal orientation (Baker et al. 2005), attitudes (Baker et al. 2008b), and affect (Rodrigo
et al. 2008). Of the studies that reported statistical relationships between student features and gaming, the maximum variance accounted for by student features was about
9% (Arroyo and Woolf 2005). In contrast, Baker et al. (2009) found that 56% of the variance in gaming was associated with lesson features, such as confusing hints and poor
interface design. Because the lesson features explained more of the gaming variance
than prior work involving student features, the argument was made that instructional
defects are a key predictor of gaming. These findings are logical: if the student is at
an impasse that appears to be caused by poor system design rather than by a lack of
knowledge, then the student will game in order to work around the impasse. On the
other hand, non-ITS research has shown that even when students use exactly the same
instructional materials, they vary dramatically in how they choose to process them
(e.g., Chi et al. 1989; Renkl 1997; VanLehn 1998). These non-ITS studies suggest that
some kind of student features (e.g., knowledge, motivation) might be more important
than instructional features in determining gaming.
To investigate the predictors of gaming further, we conducted an in-depth analysis
of log data corresponding to several years worth of students interacting with Andes,
an ITS for introductory physics (VanLehn et al. 2005). To identify gaming episodes
in this data, we applied a computational gaming detector that we calibrated with a
hand-analysis of the data. This detector provided the input to a series of data mining
analyses that we conducted to better understand gaming. The initial research question
was: What is a better predictor of gaming, problem or student?

123

An analysis of students’ gaming behaviors

101

Contrary to some prior work (e.g., Baker et al. 2009), we found that gaming is best
predicted by student, and that by accounting for both student and problem features
our approach explains 61% of the variance in the Andes data set. Four separate analyses of the Andes data support this conclusion. To extend and validate these findings,
we applied our approach to a different ITS (the Algebra Cognitive Tutor, Carnegie
Learning Inc. 2010) and population (high school students); we found that student was
again a better predictor than problem for the majority of the analyses.
Having established that frequency of gaming depends more on who rather than what
is being gamed, we next sought to characterize where students were gaming, and how
gaming behaviors and subsequent learning and problem-solving outcomes depended
on students who gamed frequently (high gamers) versus infrequently (low gamers).
While we found that low gamers did indeed game differently than high gamers, in
general a frequently gamed feature was high-level hints. Thus, to see if poor hint
usability was driving gaming, the subsequent analysis focused on how hints scaffolded problem solving and learning. The findings show that when students actually try
to use high-level hints, they eventually obtain the correct solution, although this may
require several attempts, particularly for high gamers. However, neither high-level nor
bottom-out hints elicited as much learning as we anticipated. As follow-up analysis
demonstrated, this result was likely mediated by gaming. In particular, high gamers
did not appear to benefit from high-level hints and in general learned less from hints
than low gamers. Altogether, these analyses highlight the need for further research to
obtain a better understanding of the utility of various kinds of help in ITSs and how
to best promote learning for different types of students from it.
One of the techniques we relied on corresponded to Dynamic Bayesian Network
(DBN) parameter learning. Bayesian networks, of which DBNs are a specific class, are
established in the user modeling community as a tool for representing and reasoning
about user states of interest (e.g., Mayo and Mitrovic 2001; Conati et al. 2002; Reye
2004); Bayesian networks have also been used in a data mining context outside of the
educational community (e.g., for understanding tuberculosis epidemiology (Getoor
et al. 2004)). However, within educational data mining (EDM), the application of
Bayesian networks is more recent (Beck et al. 2008), although it shows high promise:
DBN parameter mining obtained comparable results to other, arguably more established, techniques (Zhang et al. 2008).
To summarize, this paper includes three key analyses: (1) whether student or problem better predicts gaming, (2) how and where students are gaming and (3) the utility
of ITS help features, including the impact of gaming and various kinds of help on
problem solving and learning. Each analysis is motivated by its predecessor. In particular, because we found that student was such a strong predictor of gaming (analysis 1),
we wanted to better understand how and what students were gaming, and whether certain groups of students (e.g., low gamers) were gaming differently than other groups
(analysis 2). Since we found that students in general and high gamers in particular had
a very high rate of help abuse, this led us to explore how ITS help scaffolded students,
and how gaming influenced this process (analysis 3).
Our work brings the following contributions: (1) Through a variety of EDM techniques, we show that student is a better predictor of gaming than problem; (2) We
identify individual differences in terms of how students game and in turn benefit (or

123

102

K. Muldner et al.

not) from instructional features of the environment; (3) We extend the work in (Beck
et al. 2008) on Bayesian network parameter learning, both through its application to
a novel data set and by subsequent analysis exploring the impact of gaming and various types of help on problem solving and learning for low versus high gamers. We
presented a subset of this work in (Muldner et al. 2010), namely the gaming detector
and its application to the Andes data for exploring problem versus student predictors
of gaming, as well as a basic analysis on the short-term impact of ITS help. Here,
we extend this work by applying our approach to a second data set, namely from the
Cognitive Tutor, as well as presenting substantial new results from Bayesian parameter
learning not in (Muldner et al. 2010).
We begin with a survey of related work (Sect. 2). We then present the gaming detector (Sect. 3) and the log data analysis to investigate predictors of gaming (Sect. 4);
we validate this analysis by applying it to data from a different tutor and population
(Sect. 5). Next, we present students’ gaming profiles (Sect. 6). Since we found that
high-level hints were one of the most abused features, we performed an in-depth analysis of how students used hints (Sect. 7). We then present findings on the impact of
gaming on hint utility, problem solving and learning (Sect. 8). We conclude with a
discussion of the findings and future work.

2 Related work
Students who frequently game tend to score lower on a post-test than students who
never game (Baker et al. 2004b; Walonoski and Heffernan 2006a); likewise, a specific
form of gaming, namely help abuse corresponding to skipping hints, is negatively
correlated with learning (Aleven et al. 2006). Research also shows that some students
are less likely to be hurt by gaming, such as ones with high initial domain knowledge
(Baker et al. 2004a)—this makes sense in that if a concept is already known, gaming
is not likely to erode that knowledge.
Given that some work suggests gaming hurts learning, the next question is: How
wide-spread is this behavior? One study found that only about 10% of students ever
gamed (Baker et al. 2004b), while in another study, all students gamed, albeit only some
were frequent gamers (i.e., 15% were high gamers and another 25% were above-average gamers) (Walonoski and Heffernan 2006a). This discrepancy in gaming frequency
may be due to the type of data collection method employed in each study. Baker et al.
(2004b) used human observers to identify and record gaming episodes in real-time as
students interacted with an ITS; it is possible that these observers missed some episodes. In contrast, Walonoski and Heffernan (2006a) employed a post-hoc rule-based
detection algorithm to identify gaming frequencies (more on gaming detection below).
For the remainder of this section, we first survey work on detecting and understanding gaming, using EDM and other techniques, respectively. To discern between EDM
versus other work, we rely on the following definition: “educational data mining . . .
is defined as the area of scientific inquiry centered around the development of methods
for making discoveries within the unique kinds of data that come from educational
settings, and using those methods to better understand students and the settings which

123

An analysis of students’ gaming behaviors

103

they learn in” (Baker 2010). We then provide an overview of interventions designed
to discourage gaming, and wrap up with EDM research on ITS help functionality.
2.1 EDM work on detecting and understanding the causes of gaming
Some of the EDM gaming-related work has focused on building machine detectors
for gaming identification (Baker et al. 2004a, 2006b; Walonoski and Heffernan 2006a;
Baker et al. 2008a). For instance, Baker et al. (2004a) used a density estimator with a
set of Latent Response Models to predict how often a student would game the system.
The gold standard to evaluate this detector was classroom observation of students’
gaming behaviors. In this study, only some students’ learning was hurt by gaming,
and the detector accurately identified these students, but it was not accurate in recognizing the “gamed-not hurt” students (i.e., ones whose learning was unaffected by
gaming). Another detector relied on various algorithms in WEKA, a machine learning
package, to identify gaming (Walonoski and Heffernan 2006a)—while this detector
correctly identified lack of gaming (98%), it’s ability to recognize when gaming was
occurring was quite low (19%).
There is also EDM work on understanding why gaming occurs. Some researchers
propose that gaming is due to features of the instructional materials, including poor
ITS design (Baker et al. 2009). This conjecture is based on analysis that involved (1)
identifying salient lesson features, such as the total number of skills in a lesson and/or
clarity of lesson interface icons; (2) clustering the features using Principal Component
Analysis, and (3) checking for significant correlations between the clusters and a students’ frequency of gaming. The full model explained 56% of the data variance, which
was much higher than prior attempts, suggesting that lesson features are a strong predictor of gaming; likewise, Baker (2007) reported that lesson features were a stronger
predictor of gaming than student features.
As far as student characteristics that drive gaming are concerned, Baker et al.
(2004a) use the Latent Response Model described above to show that students’ domain
knowledge influences gaming frequency. In that model, relevant features such as number of errors and/or prior knowledge were included only if they reduced prediction
error. Thus, the features remaining provided insight into factors influencing gaming.
One of the remaining features encapsulated that if a student knew a skill then s/he had
a low probability of gaming. Furthermore, the model predicted that students who were
most hurt by gaming gamed on steps they did not know—this lead to the conjecture
that students game on difficult steps (i.e., an unknown step is a difficult step). Other
work has explored the relationship between student affect and gaming, and found that
boredom is the most frequent emotion to precede gaming (Rodrigo et al. 2008).
2.2 Other work on detecting and understanding the causes of gaming
Some work does not rely on EDM for gaming detection, using instead other methods
for labeling the data as gamed or not, but does use EDM for subsequent analysis of the
labeled data (Baker et al. 2009; Cohen and Beal 2009)—we describe a representative
sample of such work here.

123

104

K. Muldner et al.

One gaming detection approach entails human observation (Baker et al. 2004b;
Walonoski and Heffernan 2006a). For instance, in the seminal work on gaming, Baker
et al. (2004b) used several human observers in the classroom, as students interacted
with an ITS. The observers identified gaming by visual inspection. This approach
is challenging for several reasons. First, to avoid making students uncomfortable,
observers only watched students out of the corner of their eye, and so may have missed
nuances in a fast-paced classroom environment. Second, it’s challenging for a human
observer to be consistent in terms of ensuring all students are observed equally, and
so the results may be biased towards certain students. Given these challenges related
to human observation, there has subsequently been work on post hoc gaming identification. One method entails hand labeling of log data by a human coder (Baker
et al. 2009). This approach affords the coder time to consider all student actions and
attend to context-specific details that may be hard for a machine algorithm to recognize. However, hand-coding of data is very labor intensive and human coders may
be inconsistent, particularly given the copious amounts of data that typically need to
be coded. Other approaches have focused on machine-based gaming detection. For
instance, the Help Tutor (Roll et al. 2006) relied on if-then rules that used latency
between actions to recognize when students were gaming hints by skipping them. In
contrast to the EDM approaches described above, these rules were designed by hand,
using an expert-centric approach of relying on existing psychology work to guide the
rule design. Likewise, latency between successive attempts and/or hint requests was
used to identify gaming in the Wayang Outpost tutor (Cohen and Beal 2009).
In addition to detection, there is also work outside of the EDM community on
understanding how student characteristics impact gaming. For instance, surveys were
administered to students to study how attitudes, e.g., towards computers, influence
gaming (Baker et al. 2008b). Survey methods were also used to understand how goal
orientation influences gaming frequency (Baker et al. 2005), and in particular whether
students with performance goals game more than those without. Contrary to expectations, no link was found between gaming and performance goal orientation; in fact,
students with this orientation solved problems more slowly than other students.

2.3 Interventions for discouraging gaming
Various interventions have been developed to discourage gaming and so foster learning—the design of these interventions is often informed by EDM findings, and so for
the sake of completeness, here we review a sample of this work. One strategy involved
the introduction of a mandatory delay before a student could ask for a hint, to discourage students from rushing through hints (Aleven 2001; Murray and VanLehn 2005).
Murray and VanLehn (2005) found that this design significantly reduced help requests,
but overall did not impact learning (although students with low initial expertise did
marginally learn more when given the intervention). Interestingly, the introduction of
a mandatory delay resulted in a new form of gaming, where students would repeatedly
cancel a step until they received proactive help from the tutor.
Other strategies for discouraging gaming include the incorporation of (1) supplementary exercises and (2) an animated agent that shows disapproval upon detection of

123

An analysis of students’ gaming behaviors

105

inappropriate student behaviors (Baker et al. 2006a). These interventions had a marginal impact on reducing overall gaming, as compared to a control condition. However,
while fewer students gamed, those students who did game did not game less. Furthermore, the interventions did not affect overall student learning. Yet another approach
to discourage gaming relies on visualizations of various student behaviors (Walonoski
and Heffernan 2006b; Arroyo et al. 2007). Walonoski and Heffernan (2006b) found
that a visualization of student behaviors did significantly reduce gaming in an ITS;
impact on learning was not assessed. This was done by Arroyo et al. (2007), who found
that visualizations of student progress fostered learning, while reducing some types of
gaming. There is also work on reducing help abuse, a form of gaming, through written
messages delivered upon its detection (Roll et al. 2006). This intervention significantly
reduced help abuse, but did not impact learning. In summary, while progress has been
made in discouraging gaming, more work is needed to understand how do so for all
students and how to improve learning outcomes in the process.

2.4 EDM work on understanding the utility of its help-related functionality
As we mentioned above, our analysis of gaming prompted us to investigate the utility
of tutor help. Therefore, here we review a representative sample of EDM work related
to understanding and/or modeling ITS help functionality.
Typically, skipping through high-level hints to reach the bottom-out hint that essentially provides the solution step is labeled as gaming. In contrast, Shih et al. (2008)
conjecture that bottom-out hints could be considered analogous to worked-out examples, and so viewing only the bottom-out hint might not be harmful to learning. To
test this conjecture, a model was used that took into account the time a student thinks
about the bottom-out hint. The model embedded the assumption that the time before
an answer is produced corresponds to a student thinking about applying the hint, while
the time after an answer is produced corresponds to a student reflecting about the hint.
Analysis of log file data shows that bottom-out hint usage is indeed correlated with
learning, as are the model time-related parameters. While this study suggests there
is utility to bottom-out hints, it does not tell us whether skipping high-level hints is
harmful, or what added value (if any) there is to those kinds of hints.
In general, high-level hints are intended to elicit information from students, while
bottom-out hints tell information. Human tutors use a variety of elicit versus tell strategies during one-on-one tutoring sessions; these strategies are much more flexible than
the ITS standard of including several high-level hints always followed by a bottom-out
hint. Some researchers have investigated whether Reinforcement Learning (RL) could
be applied to data from a tutoring corpus to learn elicit versus tell rules that would
enable an ITS to more flexibly provide help (Chi et al. 2010a,b). While an earlier
study did not show the RL approach to be significantly better than a random strategy, a subsequent model did generate better learning gains over a random approach.
In this latter model, a larger set of features was used than in the original model,
which included student, domain and system features. Since these features were not
always ones predicted by current theories of learning, this work highlights how EDM

123

106

K. Muldner et al.

can extend existing theories by providing fine-grained insights about the pedagogical
process.
Yet another example of EDM work investigating instructional interventions related
to ITS help provision used learning decomposition (Feng et al. 2009). Three pedagogical approaches, embedded in an ITS, were compared, including: (1) scaffolding
questions that appeared automatically when students generated an incorrect response
and that students had to answer correctly before proceeding; (2) scaffolding questions
that were instead available on demand, and that students were not required to answer;
(3) delayed feedback that was provided in the form of worked-out examples after
all problem solving was done. While the results are only marginally reliable, they
suggest that providing delayed feedback might be better than providing scaffolding
questions that students must answer. There was no difference, however, between scaffolding questions and delayed feedback when students were not forced to answer the
scaffolding questions.
The work described above relied on pre and post test data to help train EDM models. Such data, however is not always available, and so alternatively, students’ actions
during their interaction with an ITS may be used to infer variables of interest. For
instance, Beck et al. (Beck et al. 2008) relied on the BNT-SM toolkit (Chang et al.
2006) to learn the parameters of a Bayesian network designed to assess the impact of
tutor help on students’ problem solving and learning. The resulting parameters suggest that help is slightly more “helpful” in terms of fostering learning than no help
(we provide more details on this work in Sect. 7.3). There is also work on biasing
the parameter learning process through Dirichlet wavelets, initialized to represent the
domain’s parameters (Rai et al. 2009)—to date, however, this method has not been
shown to result in better model prediction. Alternatively to using Dirichlet priors,
others propose using a k-means clustering approach to find parameters for clusters of
skills rather than individual skills, thereby reducing the space of parameters that need
to be learned (Ritter et al. 2008).
3 The primary data and gaming detector
We now describe the primary data source and gaming detector.
3.1 The primary data
Our primary data, obtained from the Pittsburgh Learning Center DataShop, corresponds to logs of students using the Andes ITS (VanLehn et al. 2005) for assigned
class homework and test preparation. This data was collected from a total of six different college physics classes over the span of about three years.
Andes tutors Newtonian physics by providing corrective feedback and hints on the
target domain. Students solve problems in the Andes interface by drawing diagrams
via the provided tools and typing equations—we refer to such interface actions as
entries. Andes does not constrain students’ solution generation, in that students are
free to produce entries in any order they wish and/or to skip entries. Andes provides
immediate feedback for correctness on students’ entries, by coloring the entry red

123

An analysis of students’ gaming behaviors

107

Table 1 Tutor-Student turn pairs
(a) Student: hint request

(b) Student: entry

Fast

Fast

Slow

Slow

(i) Tutor: bottom-out hint

(S)Ski p hint

–

(C)Copy hint

–

(ii) Tutor: high-level hint

(S)Ski p hint

–

–

–

(iii) Tutor: incorrect (red)

–

–

(G)Guess (r ed only)

(iv) Tutor: correct (green)

(P)N o planning

–

–

–

–
–

gamed cells italici zed

(incorrect) or green (correct). As students solve problems, they can ask Andes for
hints. The Andes hint sequence starts out with general information (e.g., “Why don’t
you continue with the solution by working on setting the pressure at a point open to
the atmosphere”) and ends with a bottom-out hint that indicates the step to enter (e.g.,
“Write the equation Pa = Pr0”). To discourage students from always clicking through
to the bottom-out hint, Andes assigns a score to each problem, which is decremented
slightly every time a bottom-out hint is requested. Full details on the system may be
found in (VanLehn et al. 2005).
3.2 The gaming detector
We now describe the gaming detector (Muldner et al. 2010). After irrelevant actions
are removed from the log data, a log consists of a time-stamped sequence of tutor-student turn pairs (e.g., tutor indicates an entry is incorrect, student responds by asking
for a hint). To address our research questions, we needed to know which of these turn
pairs corresponded to gaming. Given that the data comprised about 900,000 pairs,
manual analysis was not feasible. Therefore, we first hand-analyzed a fragment of the
log data to identify patterns indicative of gamed turn pairs (described below). Next,
we encoded these patterns into a computational rule-based gaming detector that could
automatically label the data. We then applied the detector to the data, hand-checking
its output on a new data fragment, and revising as necessary.
For purposes of the analyses reported here, we classified the tutor turns as follows:
(i) coloring an entry red (incorrect), (ii) coloring an entry green (correct), (iii) providing a bottom-out hint, or (iv) providing a high-level hint (we did not further subdivide
the high-level hints since the number and characteristics of such hints varied considerably). We classified a student’s turn as either (a) asking for a hint or (b) generating
an entry. Thus, there are 4 × 2 = 8 types of turn pairs (see Table 1).
Each turn pair has a time duration associated with it, which is how long the student
paused between seeing the tutor’s turn and starting to take action. We assume that turn
pairs with long durations are not gaming. Of the eight possible turn pairs with short
durations (see Table 1), we consider the following five to be gaming:
(1–2) Skipping a hint: the tutor presents a hint and the student skips the hint by quickly
asking for another hint (see ‘S  cells in Table 1). This suggests the student is
trying to reach the bottom-out hint without even reading the preceding hints.

123

108

K. Muldner et al.

(3) Copying a hint: the tutor presents a bottom-out hint and the student quickly
generates a solution entry, suggesting a shallow copy of the hint instead of
learning of the underlying domain principle1 (see ‘C’ cell, Table 1).
(4) Guessing: the tutor signals an incorrect entry, and the student quickly generates
another incorrect entry, suggesting s/he is guessing instead of reasoning about
why the entry is incorrect (see ‘G’ cell in Table 1). This is the only student entry
for which we take into account correctness, as not doing so might incorrectly
classify fixing slips as gaming (i.e., the other half of the fast cell next to the ‘G’
cell, Table 1).
(5) Lack of planning: the tutor signals a correct entry and the student quickly asks
for a hint, suggesting reliance on hints for planning the solution (see ‘P’ cell,
Table 1).
Other gaming detectors also consider hint abuse (i.e., skipping, lack of planning) and
guessing as gaming (e.g., Walonoski and Heffernan 2006a; Cohen and Beal 2009). We
also labeled copying hints as a gaming behavior because we wanted to capture instances
when students were not reasoning about the bottom-out hint content (although as
Sect. 6 describes, the copy category was gamed very infrequently). Note that copying
a hint does not take into account the possibility that a student may copy the hint and
then reason about it. This was explored in (Shih et al. 2008), by analyzing time spent
after a hint was copied. However, a time lag after a hint could also correspond to a
student reasoning about the next solution entry. How students reason after copying a
bottom-out hint could be verified by obtaining verbal protocols of students interacting
with an ITS. Since at this point we do not have such data, for the time being we only
consider time before an entry is generated, as we felt this was more likely to correspond
to reasoning about the entry.
Setting the time thresholds. Gaming detection relies on having reasonable time
thresholds, one for each of the five gamed turn pairs. To set the thresholds, we obtained
a value for each turn pair through a visual inspection of (1) the log file data and (2)
the time distribution for each turn-pair under consideration (e.g., see Fig. 1). We were
conservative when setting the thresholds. For instance, we set the skipping hint threshold T < 3 s 2 . The Andes log files only afford precision rounded to a full second, so
skipping a hint means a student spent zero, one or two seconds on the hint before
asking for another hint. While this range likely does not afford sufficient time to read
all of a hint, the threshold captures instances when students are skipping most of the
hint.
Unless otherwise stated, the results reported throughout this paper are based on
applying the above-described gaming detector to the full Andes data set, corresponding to a set of 318 unique problems and 286 students.

1 A high-level hint followed by a fast entry is not gaming since you can’t copy high-level hints.
2 The other thresholds were as follows: copying <4 s; guessing related to equations that were typed <4 s;

guessing related to free-body diagrams that were drawn with interface tools <6 s. lack of planning<4 s.

123

An analysis of students’ gaming behaviors

109

Fig. 1 Viewing time distribution for turn pairs corresponding to cells (i–ii)-(a) in Table 1

4 What is a better predictor of gaming: student or problem?
A primary research question we wanted to explore is whether student or problem
features better predict gaming. To do so, we first obtained measures on gaming, as
follows:
PerGamingsp
 p=N
p=1 perGaming p /N
s=M
s=1

perGaming p /M

percentage of gaming by a student s on a problem p

(1)

average gaming by a student s across all N
problems p solved by that student
(2)
average gaming on a problem p across all M students s (3)

We use problem as the unit of analysis (see Eq. 1; Eq. 2 and 3 rely on it). Some research
has used lesson as the primary unit of analysis (Baker et al. 2009). In fact, the ideal
unit would correspond to tutor-student turn pairs, as these are where a student makes
a game versus no-game decision. However, we need a unit of analysis that can be
compared across students, so that we can determine whether all students tend to game
at “the same” place. It would be difficult to determine if turn-pairs from one student are
“the same” as turn-pairs from another student. The smallest unit of analysis that allows
simple equivalence across students is the problem. Thus, we chose problem as the unit
of analysis instead of lesson (too large) or tutor-student turn pairs (not equatable; too
small). In these calculations, we use percentage of gaming (see Eq. 1) instead of raw
values to avoid biasing the analysis towards, for instance, short problems.
We did not consider a series of rapid hint requests within a given problem as a single gamed action, for several reasons. First, lumping together a series of hint requests
abstracts information on how many hints and which hints are skipped, and so fails to
provide information on students’ hint usage. Second, it does not make sense to lump
certain gamed actions if other actions are not lumped (e.g., a series of non-gamed
correct entries, or a series of slow hint requests), since this could skew the results.
An alternative approach includes using a step as the base measure within a problem
instead of individual actions, where a step includes all the actions corresponding to a

123

110

K. Muldner et al.

single entry (which includes asking for hints about that entry, incorrect attempts, and
the final correct entry). In this scheme, a step would be labeled as gamed or not, and
the overall percentage of gamed steps within a problem would be used in the various
models. This approach suffers from its own drawbacks—a prominent one being that
it is not clear when a step should be labeled as gamed (e.g., when a student games
a single action while generating a step? Multiple actions—if so how many?). Given
these considerations, for the analysis reported here we label each action as gamed
or not, and then obtain the overall percentage of gamed actions for a given problem,
although we may revisit the step analysis in the future.
To investigate predictors of gaming we conducted four different analyses, enabling
us to triangulate results across them. The first three analyses were presented in (Muldner et al. 2010), extended here with analysis on mined Bayesian parameters in Sect. 4.4.
4.1 Linear regression analysis
One way of investigating predictors of gaming is through a linear regression analysis.
In the linear regression, we used Per Gamingsp (see Eq. 1 above) as the dependent
variable, and two independent variables: (1) student, the average gaming by a student
s across all N problems p solved by that student (see Eq. 2 above), and (2) problem,
the average gaming on a problem p across all M students s who solved that problem (see Eq. 3 above). The model we obtained is significant (F=16915, p < 0.001),
and accounts for 60.8% of the variance (R2 = .608). In this model, both student
and problem yield a significant correlation with the dependent variable, but student is
more strongly associated with gaming (student: standardized coefficient = .658, t =
152.7, p < 0.001; problem: standardized coefficient = .325, t = 74.23, p < 0.001).
Thus, the overall regression equation is as follows:
perGameds,p = const + .658students + .325problemp
If we enter the independent variables separately to analyze the variance explained
by each, the student variable accounts for 49.6% of the variance (standardized
coefficient = .713), while the problem variable accounts for 18.6% of the variance
(standardized coefficient = .434).
To explore the impact of a given data set (i.e., class or course section), we re-ran the
regression analysis with a third independent variable, namely data set id. This variable
explained only an additional 1% of the variance, showing that data set had at best a
weak effect on gaming, and so we did not consider it in subsequent analysis.
4.2 Self-correlation analysis
Another way to verify whether students are more consistently gaming across problems
or if instead problems are more consistent across students is to randomly sub-divide
students (or problems) into buckets and then check for correlation between the buckets.
To this end, to check how consistent students were in terms of gaming across problems
(student self-correlation), we created two buckets by (1) randomly splitting problems

123

An analysis of students’ gaming behaviors

111

Fig. 2 Scatter plot for student (le f t) and problem (right) self correlations from the Andes data (percentage
of gaming shown on X and Y axes)

solved by a given student across the two buckets and (2) storing in each bucket the
average gaming across that bucket’s problem subset, obtained using Eq. 2 above but
applied to the bucket subset. A correlation analysis yielded a high degree of association between the two buckets (r = .963, p < 0.001). That is, if a student tended to
game problems in the A bucket, then that student also tended to game problems in
bucket B.
We used an analogous technique to check how consistent problems were across students (problem self-correlation). Specifically, we created two buckets by (1) randomly
splitting students who solved a given problem across the two buckets and (2) storing
in each bucket the average gaming across all students for that problem, obtained using
Eq. 3 above. We also found a high degree of association between the two buckets
(r = .89, p < 0.001). That is, if a problem was often gamed by students in bucket A,
then it was also often gamed by students in bucket B (Fig. 2 shows the scatter plots
for the two analyses). However, the correlation coefficient for the latter analysis was
lower than for the former, suggesting that students are more consistent than problems.
Specifically, if a student is a high gamer on half of the problems, then the student is also
likely to be a high gamer on the other half. In contrast, if a problem is a high-gaming
problem for half the students, then it is less likely to be a high-gaming problem for
the other half. We verified the difference between the two correlation coefficients was
reliable using Z scores (Lowry 2010); this was indeed the case (Z = 6.9, p < 0.05).

4.3 Gaming frequency distributions
Yet another way to investigate predictors of gaming is to examine histograms of gaming
frequency. That is, we can look at how many students are high frequency gamers versus
middle versus low frequency gamers. If differences among students are completely
unimportant, and all students tend to solve roughly the same set of problems, then
gaming frequency should be normally distributed (i.e., most of the gaming frequencies should cluster around the average). In fact, the student distribution is significantly
different from the normal (Shapiro-Wilks test of normality W = .89, p = 0.02), and

123

112

K. Muldner et al.

Fig. 3 Andes data on student (top) and problem (bottom) gaming distributions. Each bucket contains
students (or problems) with a 3% gaming range (e.g., bucket 6 has 3% <gaming <6%)

appears bimodal (see Fig. 3, top). As the graph shows, there is one group of students
who frequently game, and another group who seldom game. This again suggests that
differences between students play an important role in gaming frequency.
Likewise, if the characteristics of problems are completely unimportant, then a
histogram of the number of problems (y-axis) gamed at a certain range of frequencies (x-axis) should be normally distributed (Fig. 3, bottom). This is the case: the
Shapiro-Wilks test of normality showed that the problem distribution is not significantly different from normal (W = .92, p > 0.05). Thus, it appears once again
that characteristics of students are more important than characteristics of problems in
predicting gaming.

4.4 Parameter learning for predicting gaming
We also rely on Bayesian network parameter learning to investigate how student versus problem predicts gaming, using the Bayesian network shown in Fig. 4 (top). In
this network, all nodes are observable and binary, and have the following semantics:

123

An analysis of students’ gaming behaviors

113

Fig. 4 Predictive Bayesian network (top) and corresponding parameters obtained from the Andes log data
via the Netica counting algorithm (bottom). The parameters for P(gamed = false |. . .), not shown here, are
simply 1− P (gamed = true |. . .)

• ‘studentClass’: whether a student is a low or high gamer
• ‘problemClass’: whether a problem is a low or high gamed problem
• ‘gamed’: true if a tutor-student turn pair was gamed and false otherwise (see Table 1
for a listing of the turn pairs)
Thus, the network models how student and problem predict gaming. To obtain data for
setting the values of the student and problem class nodes, we classified: (i) each student
as a low or high gamer, based on a median split of average gaming obtained using
Eq. 2 presented above and (ii) each problem as a low or high gamed problem based on
a median split of average gaming obtained using Eq. 3. The data for setting the value
of the ‘gamed’ node was provided by the gaming detector described in Sect. 3.2.
Since all nodes in the network are observable, we used Netica (Netica Reference
Manual 2010), a Bayesian network toolkit, and specifically its counting algorithm to
learn the network parameters from the Andes data. The counting algorithm uses a traditional one-pass method to determine the probabilities, which essentially amounts to
counting the number of times a node takes on a certain value given each configuration
of the parents (for details, see Russell and Norvig 2009). The resulting parameters
are shown in Fig. 4 (bottom) and support the above findings that student is a stronger
predictor of gaming than problem. In particular, the probability of gaming is higher
when the ‘studentClass’ node is high and the ‘problemClass’ node is low, as compared
to when the ‘problemClass’ node is high and the ‘studentClass’ node is low. Another
way of looking at this is to consider the fact that when studentClass is low, indicating
a student is not a gamer, then the probability of gaming is below 20%, regardless of
whether a low or high gaming problem is solved by that student. In contrast, when
studentClass is high, indicating that a student is a gamer, the probability of gaming
is much higher, both when a low and a high gamed problem is solved. Conversely,
when problem class is low, then the probability of gaming more depends on the type
of student solving that problem (alternating between about 10% if the student is a low
gamer and 37% if the student is a high gamer); likewise, when the problemClass is
high. All together, this suggests that while problem does have an impact, it is less than
the type of student solving that problem.

123

114

K. Muldner et al.

Fig. 5 Naïve Bayes classifier (top) and corresponding parameters obtained from the Andes log data via the
Netica counting algorithm (bottom). The parameters for P([problem,student]Class = low |. . .), not shown
here, are simply 1− P ([problem, student]Class = high |. . .)

An alternative network structure to the one in Fig. 4 is a naïve Bayes classifier, such
as the one shown in Fig. 5. The disadvantage of this network, however, is that by definition its structure assumes that student class and problem class are independent given
evidence of gaming, which is not necessarily the case (in fact, the predictive network
in Fig. 4 suggests that the two variables are related). For the sake of completeness,
we used Netica to learn the parameters for this network. As Fig. 5 (bottom) shows,
the results confirm the above analysis, in that the probability of gaming is higher if
‘studentClass’ is high than when ‘problemClass’ is high (see, Fig. 5, bottom; in fact,
if we set the value of ‘studentClass’ = high and ‘problemClass’ = low, the probability of gaming is 0.48, higher than the p = 0.28 obtained when we set the value of
‘studentClass’ = low and ‘problemClass’ = high).
5 How do Gaming predictors in other tutors compare to those in Andes?
Past research has shown that both student and instructional aspects influence gaming, but to date there does not exist agreement as to which is the stronger predictor.
Work suggesting that instructional features drive gaming (Baker et al. 2009) relied on
data from the Algebra Cognitive Tutor (Koedinger et al. 1995; Carnegie Learning Inc.
2010). To see whether the particular data we used influenced the finding that student
was the strongest predictor of gaming, we applied the gaming detector and analyses
to the Cognitive Tutor data used in (Baker et al. 2009).
Before presenting the results, we provide a brief overview of the Cognitive Tutor.
This tutor provides problems for students to solve, which they do by typing and/or
drawing objects in the interface, much like in Andes. Also as in Andes, the Cognitive
Tutor allows students to ask for hints, which start out general and end in a bottom-out
hint that informs students of the step needed to generate the solution. Given this, the
general gaming detection framework presented in Sect. 3.2 is also appropriate for
gaming detection in the Cognitive Tutor data. Prior to applying it to the new data
set we refined the gaming thresholds to make them appropriate in the context of the
Cognitive tutor, using the method described in Sect. 3.2 to set the thresholds.

123

An analysis of students’ gaming behaviors

115

Once the gaming detector labeled the Cognitive Tutor data, we obtained information for PerGaming, problem and student using Eqs. 1, 2, 3 (i.e., as for the Andes
data set, see Sect. 4), and analyzed the relationship between these variables using the
methods presented in Sect. 4. The results, presented below, are based on data from 53
individual students 775 unique problems.
Linear Regression Analysis. A linear regression with Per Gamingsp (Eq. 1, Sect. 4)
as the dependent variable, and as the independent variables, student (Eq. 2, Sect. 4)
and problem (Eq. 3, Sect. 4) produced a significant model (F = 4588, p < 0.001)
that accounted for 42% of the variance (R 2 = .420). In this model, both student and
problem are correlated with the dependent variable, although student yields a higher
correlation than problem (student: standardized coefficient = .52, t = 76, p < 0.001;
problem: standardized coefficient = .35, t = 51, p < 0.001).
If we enter the independent variables separately to analyze the variance explained
by each, the student variable accounts for 30% of the variance, while the problem
variable accounts for 15% of the variance.
Self-correlation Analysis. We used the method in Sect. 4.2 to analyze whether students were consistently gaming on problems, i.e., by randomly assigning problems
a given student solved to two buckets and obtaining an overall problem average. We
found a high degree of association (r. = .973, p < 0.001). In contrast, when we
analyzed that problems were consistently gamed on by randomly assigning students
to two buckets and obtaining an overall average, the analysis yielded a lower degree
of association (r = .55, p < 0.001); the difference between the two correlation
coefficients is reliable (Z = 10.5, p < 0.05). Thus, if a student is a high gamer on
half of the problems, then the student is also likely to be a high gamer on the other
half. In contrast, if a problem is a high-gaming problem for half the students, then it
is less likely to be a high-gaming problem for the other half. This analysis confirms
the Andes findings that students are more consistent and thus better predictors than
problems when it comes to gaming.
Gaming frequency distributions. As we proposed in Sect. 4.3, distributions of gaming frequency across students (or problems) should be normally distributed if differences between students (or problems) do not impact gaming. The histograms for the
student and problem gaming distributions from the Cognitive Tutor data are shown in
Fig. 6.3 Based on a visual inspection, the student gaming distribution is not as cleanly
separated into low and high gamers as the Andes data. This distribution is not normally
distributed according to the Shapiro-Wilks test of normality (W = .64, p < 0.05).
However, if we remove the outlier (see bucket 38, Fig. 6, top), then the Shapiro-Wilks
test reports a normal distribution (W = .912, p > 0.05).
The problem gaming distribution appears normally distributed in the middle of the
graph, but not on the extreme ends. Specifically, there is a large number of problems
in the Cognitive Tutor that receive very little gaming (bucket 1, Fig. 6, bottom), and
there is a long tail in the upper end of the distribution, suggesting that a few Cognitive
3 The distributions are divided into smaller buckets in Fig. 6 than in Fig. 2, because the Cognitive Tutor
data had an overall lower gaming frequency, and so bigger buckets would have collapsed the data into
very few buckets. The previously reported Andes results hold if we subdivide the Andes buckets into the
Cognitive Tutor-sized buckets.

123

116

K. Muldner et al.

Fig. 6 Cognitive Tutor data on student (top) and problem (bottom) gaming distributions. Each bucket
contains students (or problems) with a 1% gaming range (e.g., bucket 1 has 0% <gaming <1%)

Fig. 7 Parameters for the Bayesian network shown in Fig. 4 obtained with the Netica counting algorithm
for the Cognitive Tutor data. The parameters for P(gamed = false | . . .), not shown here, are simply 1− P
(gamed = true | . . .)

Tutor problems are highly gamed. These two factors likely contribute to the result
that this distribution is not normally distributed according to the Shapiro-Wilks test
(W = 0.71, p < 0.05), which is not consistent with our hypothesis.
Parameter learning for predicting gaming. We used the Cognitive Tutor data to
learn the parameters for the Bayesian network shown in Fig. 4—the results are shown
in Fig. 7 (see Sect. 4.4 for a complete description of this technique). As was the case
with the Andes log data, in the Cognitive Tutor the probability of gaming is higher

123

An analysis of students’ gaming behaviors

117

Table 2 Gaming opportunities for each Tutor–student turn pair

(a) Student: Hint Request
fast

slow

(1) Tutor: B-O Hint

S: 0.02 (.3)

(2) Tutor: H-L Hint

S:18.4 (58.6)

5.8 (18.5)

3.0 (12.4 )

2.5 (10.3 )

P: 5.3 (14.5)

3.6 (10.0)

(3) Tutor: Incorrect
(4) Tutor: Correct

.2 (2.1)

(b) Student: Entry
fast

slow

C: 1.8 (23.6)
.7 (2.3)
G: 5.4 (22.1)
(RED)

5.7 (73.9)
6.4 (20.6)

4.8 (20)
(GREEN)

14.1 (38.9)

8.7 (35.8)
13.3 (36.6)

Shown in each cell: (1) the mean % of a student responses overall (i.e., the 17 numbers in the table sum to
100%), (2) in parentheses: mean % of a student response for that row’s tutor action
Fast: student action <gaming threshold; slow: student action >gaming threshold; B-O: Bottom-out, H-L:
High-level

when student is true/problem is false than vice versa, again suggesting that student is
a stronger predictor of gaming than problem.
In summary, the majority of the analyses with the Cognitive Tutor data confirms the
Andes-related findings in Sect. 4 that student is a stronger predictor of gaming than
problem. The ‘gaming distribution’ analysis was the only one that did not support this
conjecture. This latter analysis suggested that compared to the Andes problems, some
problems in the Cognitive Tutor appeared to be less consistently gamed; likewise that
differences between students were not as pronounced as in the Andes data set.
6 Gaming profiles: how much and where are student gaming?
The analyses above established that frequency of gaming depends more on who rather
than what is being gamed, but do not provide any insight into how gaming is occurring, nor its consequences. Thus, for the remainder of the paper, we will focus on
obtaining details on students’ gaming and related behaviors and how these impact
outcomes of interest, including problem solving and learning. For these analyses, we
will use the Andes data (while it would be interesting to compare whether differences
exist between the Andes and Cognitive Tutors, we leave doing so for future work).
We begin with a descriptive analysis of the Andes data, which will set the stage for
guiding subsequent analysis.
On average, 22.5% of tutor-student turn pairs were gamed. We first analyzed where
the gaming was occurring—the results are shown in Table 2, which extends Table 1
with numeric data (as in Table 1, the italicized cells indicate gamed turn pairs). For
this analysis, because we were interested in general in how students gamed, we collapsed across problems (i.e., obtained for each type of turn pair the total number of
corresponding actions across all problems for a given student); Table 2 reports the
average percentage for each type of turn pair.
As shown in Table 2, students most frequently took advantage of the opportunity
to game when the tutor presented a high-level hint: on average, 18.4% of all student actions corresponded to gaming on these hints; when given such a hint, students
gamed on it 58.6% of the time. In contrast, bottom-out hint turn-pairs were rarely

123

118

K. Muldner et al.

Table 3 Gaming opportunities for each tutor–student turn pair for low and high gamers

(a) Student: Hint Request

(1) Tutor: B-O Hint

(2) Tutor: H-L Hint

(3) Tutor: Incorrect

(4) Tutor: Correct

(b) Student: Entry

fast

slow

fast

slow
3.2

LG

S: 0.02

0.07

C: 0.34

HG

S: .03

.25

C: 3.3

8.3

LG

S: 7.1

6.4

0.53

7.3

HG

S: 29.7

5.1

LG

3.2

3.2

G: 5.5 (RED)

5.3 (GREEN)

HG

2.8

1.8

G: 5.2 (RED)

4.2 (GREEN)

LG

P: 1.5

3.4

20.3

21.1

HG

P: 9.1

3.9

8.1

5.5

0.92

5.7
11.4
6.0

Shown in each cell is the mean % of a student response given a tutor action over all 17 possible combinations
for the low gamers (LG upper half cell) and high gamers (H G lower half cell)
Fast: student action <gaming threshold; slow: student action >gaming threshold; B-O: Bottom-out, H-L:
High-level

gamed. After skipping of high-level hints, guessing and lack of planning (see ‘G’ and
‘P’ cells, Table 2) were the next most frequently gamed turn-pairs (5.4% and 5.2%,
respectively). Gaming by copying bottom-out hints was rare (see ‘C’ cells, Table 2).
In order to compare the gaming patterns of students who frequently gamed with
those who infrequently gamed, we divided students into low gamers and high gamers
based on a median split. The gaming profiles for each group are shown in Table 3. As
the table highlights, high-level hints are a highly-gamed feature by both low and high
gamers. Furthermore, low gamers are more likely to game by guessing than high gamers; this is the only gaming turn pair that low gamers abuse more than high-gamers.
Another way to investigate differences between low and high gamers is to subdivide
the data slightly differently: instead of considering at all the possible tutor-student
turn pairs, as we do in Tables 2 and 3, we can instead analyze how much students
game on a certain turn-pair in proportion only to the total five gaming opportunities
(e.g., skipping high-level hints/total gamed events). The results of doing so are shown
graphically in Fig. 8. On average, low gamers were significantly more likely than high
gamers to game by guessing (46% vs. 13.2%; F(1, 283) = 126, p < .01). On the
other hand, in contrast to low gamers, high gamers had a significantly higher proportion of skipped high-level hints (61.6% vs. 43.4%; F(1, 283) = 64, p < .01), lack of
planning (18.5% vs. 8.9%; F(1, 283) = 159, p < .01) and bottom-out hint copying
(6.5% vs. 1.7%; F(1, 283) = 215, p < .01).

7 Exploring the utility of hints for supporting problem solving and learning
Over all students’ gaming opportunities, as well as proportion of gaming for high
gamers, high-level hints elicited the most gaming. Some investigators propose that
gaming is in part due to the fact that reading hints does not influence solution entry
success, i.e., that hints are not helpful (Baker et al. 2009). We wanted to see if this was

123

An analysis of students’ gaming behaviors

119

Fig. 8 Proportion of gamed-turn pairs for low and high gamers

the case for the Andes data—if we found evidence that hints were not helpful, then this
would provide insight into why the high-level hints were gamed as much as they were.
7.1 Hint viewing
The most basic analysis we started with was to calculate the time students spent on
hints. To do so, we obtained the latency between the provision of a hint and the next
student action. On average, students spent 9.2 s vs. 5.7 s. on bottom-out versus highlevel hints. High gamers spent significantly less time on hints than low gamers, both
on bottom-out hints (7.5 s vs. 10.9 s; F(1, 277) = 71, p < .01) and high-level hints
(3.2 s vs. 8.1 s; F(1, 286) = 246, p < .01). Thus, both low and high gamers devoted
more time to bottom-out than high-level hints. High gamers’ average viewing time
for high-level hints was quite low, suggesting that in contrast to low gamers, these
students did not pay much attention to high-level hints.
7.2 Hint utility: basic analyses
In this section, we report Andes hints’ impact on short-term performance, i.e., does
the hint scaffold the student to generate the entry after seeing a hint (Muldner et al.
2010). In the subsequent section, we describe hints’ impact on long-term outcomes,
namely learning.
If for a moment we don’t consider entry correctness, high gamers tried to generate
an entry only about 16% of the time after receiving a high-level hint, asking for another
hint the other 84% of the time. Low gamers, on the other hand, responded to a highlevel hint with an entry about 36% of the time. This is in contrast to bottom-out hints,
when both low and high gamers responded to the hint with an entry about 97% of the
time. The latter is probably due to the fact that asking for a hint after the bottom-out
hint merely repeats the hint, so students only do so by accident.
For each student, we obtained the percentage of time s/he was successful at (eventually) generating the correct entry after receiving each type of hint (bottom out, high
level). Note that (1) students may require several attempts in order to generate a correct
entry and (2) if hint B is requested after hint A but prior to generating a correct entry,
then hint A is not counted as “successful” for helping the student. We acknowledge
that in some situations, hint information might be additive, i.e., seeing hint A might

123

120

K. Muldner et al.

add information to hint B. If this is the case, then high-level hints are at a disadvantage
in this analysis.
If students did generate a correct entry (or entries) after seeing a bottom-out hint,
on average, they were successful in 90% of instances (i.e., the entry was correct, albeit
possibly after several tries, as we show below). There was little difference between
low and high gamers for this analysis (89% vs. 92%, respectively, NS difference).
After high-level hints, students (eventually) generated a correct entry 73% of the time
(again, this may have required several tires, as we analyze below). There was also little
difference between low and high gamers (72% vs. 73%, respectively, NS difference).
This suggests that high-level hints scaffolded students to generate the solution entry
in about three out of four instances, albeit doing so may have taken a number of tries,
as we now show.
After bottom-out hints, students required 1.1 attempts on average (1.23 for low
gamers versus 1.19 for high gamers, NS), and took 29 sec. to answer correctly (34 s for
low gamers versus 23 s for high gamers, F(284, 1) = 4, p = .052). This makes sense,
assuming that most students copied entries from the bottom out hints, but the entries
were complex enough that it took about a half-minute to do the copying, and occasionally generated typos and other minor errors. After high-level hints, on average students
required 1.83 attempts to generate the correct entry; here low gamers needed significantly fewer attempts than high gamers (1.66 vs. 2.01, F(1, 284) = 17, p < 0.001),
suggesting that perhaps the low gamers were more diligent about applying high-level
hints. This conjecture is supported by the fact that low-gamers spent significantly
longer than high-gamers to generate a correct entry after seeing a high-level hint (37 s
vs. 28 s; F(1, 286) = 9, p < 0.01).

7.3 Hint utility: machine learning analyses
Above, we analyzed the short-term impact of hints with a relatively simple technique,
i.e., by counting the number of times a student was able to (eventually) generate a
correct response after seeing a hint. A more complex alternative is to rely on Bayesian
network parameter learning to explore the impact of help on both short-term performance and long-term learning. This was originally implemented in (Beck et al. 2008),
where it was applied to data from the Reading Tutor, an ITS designed to help children
learn to read (Beck et al. 2004). Here, we apply this method to the Andes data. In
this section, we explore the impact of hints in general for all students, while in the
subsequent section we tease apart hints’ influence on low versus high gamers.
For parameter learning, we relied on the BNT-SM Bayesian network parameter
learning toolkit (Chang et al. 2006), which was also used in (Beck et al. 2008) and
is based on the established MatLab Bayesian toolkit BNT (Murphy 2004). Learning
in BNT-SM is accomplished using Expectation Maximization (Dempster et al. 1977),
since as we shall see below, some of the network variables are latent, i.e., not observable. The toolkit is especially useful for parameter learning of dynamic Bayesian
networks, DBNs (Dean and Kanazawa 1989). DBNs model the evolution of states of
interest over time, by including so called time slices to represent particular points in
time (e.g., a basic dynamic Bayesian network for modeling the evolution of knowledge

123

An analysis of students’ gaming behaviors

121

Fig. 9 Basic knowledge-tracking DBN model (top) & corresponding parameters mined from Andes data
(bottom)

is shown in Fig. 9, top). BNT-SM implements parameter tying for dynamic Bayesian
networks (Murphy 2002), meaning that expected statistics are pooled for all nodes that
share the same parameters (e.g., ‘knowledge’ nodes in Fig. 9). Consequently, when
machine learning is applied, the same parameter for a given node class is obtained,
which is the desirable outcome (although not all Bayesian toolkits yet implement
parameter tying, e.g., Netica does not).
7.3.1 The basic knowledge-tracing model
We begin by presenting a simple knowledge-tracing model, one that does not take
into account the impact of hints at all, shown in Fig. 9 (referred to as the Basic model
below). This model is advocated as one appropriate for inferring student learning from
problem-solving actions (Corbett and Anderson 1995; Reye 2004); Beck et al. (2008)
use it as the baseline model to compare against a “Help” model, something we also
do here. The Basic model, a dynamic Bayesian network, encodes two variables: (1)
‘knowledge’, i.e., the probability that a student knows a particular domain principle,
and (2) ‘solution entry’, i.e., the probability that a student will generate a correct
problem-solving entry as a result of applying the corresponding domain principle. In
this model, all nodes have binary values, namely correct/incorrect for solution entries
and mastered/unmastered for knowledge. Note that ‘knowledge’ is not observable, but
‘solution entry’ is observable (i.e., its value can be set given evidence corresponding
to a student’s correct or incorrect solution entry).
The Basic model captures four key variables relevant to student modeling of
problem solving and learning in instructional contexts (Conati et al. 2002; Reye 2004;
Beck et al. 2008):
– guess: the probability the student will generate a correct entry (captured by the
‘solution entry’ node in the network in Fig. 9) by guessing when the corresponding knowledge is in the unmastered state;

123

122

K. Muldner et al.

– slip: the probability the student will not generate a correct entry when the corresponding knowledge is in the mastered state;
– learn: the probability the student will learn the domain principle, which was unmastered in the past and which is needed to generate the corresponding entry;
– forget: the probability the student will forget a domain principle that was mastered
in the past, needed to generate the corresponding entry.
Figure 9, bottom, operationalizes these four variables in terms of Bayesian network
parameters (probabilities shown are learned from the Andes data, as we describe
below). BNT-SM requires that initial conditional parameter table (CPT) values are
provided: to do so, we rely on the ones specified in (Beck et al. 2008), taking into
account parameters advocated for Andes-specific Bayesian networks (Conati et al.
2002). These, however, are only starting values, as they are subsequently refined by
the learning algorithm.
When we used BNT-SM to learn the four parameters (guess, slip, learn, forget) for
the Andes data in the Basic model shown in Fig. 9, top, we obtained the values shown
in Fig. 9, bottom. Note that BNT-SM learns parameters for individual skills—in (Beck
et al. 2008), a single value was reported, suggesting that an average over the whole
set of parameters learned was obtained; that is what we report here. The parameters
learned from the data for the Basic model suggest that in Andes, there is a moderate
probability of guessing and learning (interestingly, the probability of learning is quite
a bit higher than that reported in the Reading Tutor network, i.e., Andes learn = 0.25
versus Reading Tutor learn = .08).

7.3.2 The help model
The network in Fig. 9 does not represent the influence of hints on problem solving
or learning. Thus, as the next step we used BNT-SM to learn parameters for a DBN
that does account for hints, referred to as the Help model below. The Help model was
proposed in (Beck et al. 2008), and is shown in Fig. 10, top. This model extends the
one in Fig. 9 with an additional observable node, ‘hint(s)’. The Help model in (Beck
et al. 2008) used a binary ‘hint(s)’ node, with values true/false to indicate the presence/absence of tutor help, respectively. While we are also interested in investigating
the impact of various kinds of hints, something not captured with a binary-valued
node, we begin with this binary ‘hint(s)’ node network.
The ‘hint(s)’ node has the following semantics: If a student asked for one or more
hints related to (described below) a given entry, then hint(s)=true, and hint(s)=false
otherwise. We say a hint could be related to an entry because when Andes displays a
hint, it is shown and remains visible in a side panel, so a student could, for instance,
ask for a hint, generate an incorrect entry, look back at the hint, and generate a correct
entry. Thus, once a student asks for a hint, the value of ‘hint(s)’ node will be true until
that entry concludes (i.e., student generates a correct entry for the current solution step
or gives up and moves on). The ‘hint(s)’ node includes links to both the ‘knowledge’
and ‘solution entry’ nodes in the DBN, which model several hint-related influences,
shown in Fig. 10, bottom; key ones include:

123

An analysis of students’ gaming behaviors

123

Fig. 10 Help DBN model (top) and corresponding parameters mined from Andes data (bottom)

– scaffolding, the short-term influence of a hint(s): the probability of a correct entry
given that a hint(s) was requested and the relevant knowledge was unmastered;
– learnWithHint(s): learning of a domain principle via a requested hint(s);
– learnNoHint(s): learning of a domain principle without a hint request.
Beck et al. (Beck et al. 2008) used BNT-SM to learn the parameters for the Help network using data from the Reading Tutor, and reported that tutor help was useful both
in terms of scaffolding problem solving and learning—albeit its influence on learning
was very minor: learning with a hint obtained a learnHint parameter of 0.088, while
the learnNoHint parameter was only very slightly lower at 0.083.
To explore the impact of hints in Andes, we used BNT-SM to learn the Help model
parameters with the Andes log data; the results are shown in Fig. 10, bottom. We
found, as did (Beck et al. 2008), that when students did not posses the necessary
domain knowledge, a hint(s) increased the probability of a correct entry, i.e., scaffolded problem solving, as compared to episodes where students did not ask for a
hint(s) (and thus probably guessed, see scaffold versus guess, p = .38 vs. p = .31,
Fig. 10, bottom), confirming the analysis in Sect. 7.2. In contrast to the results in (Beck
et al. 2008), we found that overall, students learned slightly worse when they received
hint(s), as compared to whey they did not receive hints (i.e., p = .21 vs. p = 23,
learnWithHint versus learnNoHint, Fig. 10, bottom). We will return to why this may
have been the case shortly.
To explore the utility of different types of hints we refined the domain of the ‘hint(s)’
node to include three values: high-level (HL), bottom-out (BO), none. To set the value
of this node, we followed the approach above by looking for hints related to an entry,

123

124

K. Muldner et al.

Fig. 11 Parameters for the Help DBN model shown in Fig. 10 but revised with a three-valued hint node:
high-level (HL, high-level hint obtained), bottom-out (BO, bottom-out hint obtained), none (step generated
without requesting a hint)

but set the value of ‘hint(s)’ to: (1) bottom-out if a student requested a bottom-out hint
related to the current entry, (2) high-level if a student requested a high-level hint, and
no bottom-out hint(s), related to the current entry, and (3) none otherwise. When we
applied BNT-SM to this revised network, we obtained the parameters shown in Fig. 11.
The results highlight that bottom-out hints provide more short-term scaffolding for
entry generation than high-level hints, something we also found with the basic analysis
in Sect. 7.2. In fact, the guess parameter is slightly higher than the scaffold-HL parameter (see Fig. 11, p = .25 vs. p = .21), suggesting that high-level hints are not very helpful for immediate correct entry generation. This doesn’t contradict the finding obtained
via the basic analysis in Sect. 7.2, because in the latter, we analyzed whether a student would eventually generate a correct entry after seeing a high-level hint (possibly
requiring several attempts), while here, the network assesses hints’ immediate impact.
As was the case for the scaffold parameter, the learn parameter is also higher for
bottom-out hint(s), as compared to high-level hint(s). However, when the network
includes a three-valued ‘hint(s)’ node, learning without help obtains the lowest score,
i.e., learning with hint(s) is better than without (see Fig. 11, bottom). Thus, one possibility for why the original Help network with a two-valued ‘hint(s)’ node did not
suggest hints fostered learning is that it did not consider the impact of different types
of hints. There are, however, other possibilities that we explore in the next section.
8 Impact of gaming on hint utility and learning
The initial analysis on the impact of help did not show that hint(s) fostered learning.
While the subsequent analysis that teased apart the impact of high-level and bottom-out
hints did provide indications of learning with both types of hints, learning was not as
high as we hoped, particularly for high-level hints. One possibility for this finding is
related to gaming. Past research suggests that gaming in general can be harmful to
learning (Baker et al. 2004b; Walonoski and Heffernan 2006a), as well as help abuse
in particular (Aleven et al. 2006). Furthermore, help abuse has been associated with
poor learning gains in cognitive science research. For instance, VanLehn (1998) found

123

An analysis of students’ gaming behaviors

125

that when students copy from examples instead of trying to generate the solution on
their own, a form of help abuse, their learning is diminished. Altogether these findings
suggest that at least some of the Andes students may be learning less precisely because
they obtain hints without trying to generate the solution on their own and/or trying to
learn from the hints provided.
A way to test this hypothesis is to re-run the parameter learning with the original
Help network, but with a refined ‘hint(s)’ node that includes information on gaming, as follows: ‘hint(s)’=gamed, ‘hint(s)’=not-gamed and ‘hint(s)’=none. That is, the
‘hint(s)’=true category would be replaced by ‘hint(s)’=gamed if one or more hints
related to an entry were gamed (e.g., skipped, see “fast” in Table 2; latency below
threshold), and ‘hint(s)’=not-gamed if all hints were not gamed (“slow” in Table 2;
latency above threshold). Unfortunately, when we applied this method, there were
very few values for the ‘hint(s)’=not-gamed parameter: when students asked for a hint
(or series of hints), they tended to game on at least one of these. Note that the data in
Table 2 shows that overall, 18.1% of all student hint requests were slow (not gamed)
after receiving a high-level hint—this does not contradict the previous statement on
lack of data, because in Table 2, only single tutor-student pairs were considered (e.g.,
tutor provides a hint, student asks for another hint). In contrast, for the Help model,
we consider all the hints related to an entry, and also consider a hint as requested if it is
related to a entry (but not necessarily requested directly prior to generating the entry,
since as we mention above, hints remain visible once requested). While students don’t
always game on a single turn pair related to hints, if they ask for hints then they tend
to game on at least one of these. An alternative is to consider only the last hint request
prior to a solution entry attempt; unfortunately, this also results in sparse data, since
as shown in Table 2, students rarely copy in a shallow manner from hints (and this is
the only type of gaming possible from the hint-provided/solution-entry turn pair).
An alternative is to investigate the utility of hints for two groups of students: the
low and the high gamers. As the analysis in Sect. 7.2 suggests, low gamers are more
diligent than high gamers in terms of using hints, both in terms of their willingness
to use high-level hints and time spent on all types of hints. To see if the BNT-SM
parameter learning also showed differences between low and high gamers and hint
impact, we re-ran the learning algorithms separately for the low and the high gamers,
for each of the Help networks (original 2-valued ‘hint(s)’ node and subsequent 3valued ‘hint(s)’ node that differentiated between the two types of hints). The results,
shown in Figs. 12 and 13, suggest that low gamers benefit from hints in terms of
learning, while high-gamers’ learning is hurt by at least some of the hints (for the
sake of brevity, the forget parameter is not shown in the figures). Specifically, for the
low gamers, the mined learnWithHint(s) parameter is higher than learnNoHint(s), i.e.,
p = .26 vs. p = .22 (see Fig. 12, bolded items, top). The opposite is true for the high
gamers (learnWithHint(s) = .17 versus learnNoHint(s) = .19, see Fig. 12, bolded items,
bottom). If the impact the two types of hints (bottom out, high-level) is considered,
then the learn parameter for both types of hints is higher for low gamers than high
gamers (see Fig. 13, bolded items). In fact, according to this network’s parameters,
low gamers’ learning is not affected by high-level hints.
The above analysis suggests that gaming can be harmful to learning, since it shows
that high gaming students who abused hints more also learned less from them. It

123

126

K. Muldner et al.

Fig. 12 Parameters for the DBN Help network shown in Fig. 10 with a 2-valued ‘hint(s)’ node for low and
high gamers

Fig. 13 Parameters for the DBN Help network shown in Fig. 10 revised with a 3-valued ‘hint(s)’ node for
low and high gamers

does not, however, directly tell us about the impact of gaming on learning. To explore
this, we applied BNT-SM parameter learning to the network shown in Fig. 14. This
network is designed to capture the impact of gaming on both problem solving and
learning, by including a ‘gamed’ node that represents gaming behaviors. In this network, all nodes have binary values, and all nodes except knowledge are observable.
The ‘gamed’ node is set to true if the gaming detector indicates that gaming occurred

123

An analysis of students’ gaming behaviors

127

Fig. 14 Gaming DBN model (top) and corresponding parameters mined from the Andes data (bottom)

for the corresponding solution entry, either (1) because students abused hints prior to
generating the entry (here, only hints between two successive entries are considered,
since gamed hints corresponding to previous entries are accounted for by the corresponding ‘gamed’ node in a previous DBN time slice), or (2) because they generated
the entry by guessing or with a lack of planning. Note that in contrast to the earlier
attempt to learn parameters for gamed help requests that we did not have sufficient
data to do, here there is sufficient data, because in the former, we only considered
gamed/ungamed help requests, while in the current Game model, all types of actions
are included (solution entries, help requests, copying etc).
As the parameters obtained from BNT-SM show (Fig. 14, bottom), the probability of learning is indeed higher in the absence of gaming (the mined learnNoGaming
parameter is higher than learnWithGaming, p = .33 vs. p = .19, see Fig. 14, bottom).
This network also suggests that the probability of making a slip is higher when gaming
occurs than without gaming (the slipWithGaming parameter is higher than slipNoGaming, p = .26 vs. p = .15, see Fig. 14, bottom). That is, when students has gamed, for
instance by speeding through the hints, their next entry is more likely to be incorrect
than if they had processed the hints slowly.

9 Discussion and future work
A prerequisite for reducing gaming in ITSs is understanding when and why it occurs.
Past research has shown that both student and instructional aspects influence gaming,

123

128

K. Muldner et al.

but to date there does not exist agreement as to which is the stronger predictor, making
it difficult to understand where to focus research efforts. Some researchers argue that
it is the latter, i.e., instructional aspects, that drive gaming (Baker et al. 2009). This
argument stems from data mining related to instructional differences between ITS
lessons that resulted in a model explaining 56% of the gaming variance (we refer to
the model as the Baker model below). Since 56% was considerably higher than other
models at that time considering student characteristics, the conclusion was made that
instructional features are the better gaming predictors.
To analyze predictors of gaming, we took a slightly different approach than the one
in (Baker et al. 2009). Rather than extracting fine-grained features, such as number of
hints in a lesson, we obtained an overall gaming statistic for a given student or problem. While this means that we did not have fine-grained information on exactly which
aspects of student or problem are driving gaming, our approach does very cleanly
separate problem versus student (more on this below), as well as is lightweight to
implement. One of the EDM techniques we used to analyze gaming predictors corresponded to a linear regression model that took into account the average percentage
of gaming by a student over all the problems s/he solved, and the average percentage
of gaming on a problem over all the students. We initially applied this model to data
from the Andes tutor, and found that it explained 61% of the variance, and that in this
model, student was a stronger predictor of gaming than problem. We re-analyzed the
data using a range of techniques: each analysis confirmed that student was a stronger
predictor of gaming.
Given the above-stated prior findings on gaming predictors, to shed more light on
the issue, we applied the gaming detector to the same data set as that used in (Baker
et al. 2009). The full model explained 43% of the variance, somewhat less than the
56% explained by the Baker model. In this model, student was again a better predictor
than problem, confirmed by all but one analyses. One could argue that in the Baker
model only instructional features (i.e., lesson) are considered, and so for a fair comparison, we should use a subset of the model, i.e., one that includes only problem (and
excludes student). However, a closer analysis of the Cognitive Tutor features used
in the Baker model, which are characterized as “lesson features”, suggests that these
features may sometimes blur the instructional/student boundary. For instance, the category ‘Difficulty, Complexity of Material, and Time Consumingness’ includes aspects
such as ‘student already knows the skill’ and ‘average probability the student will
learn the skill at each application’. The category of ‘Help Features’ includes ‘average
amount that reading hints improves future performance’—this is in fact one of the
features included in the final full model. These features, however, are not only lesson
specific, since they clearly take into account student characteristics. For instance, the
amount that reading hints improves performance should be influenced by a students’
willingness and/or ability to learn from the hint. Thus, it seems that even in the Baker
model, some aspects of student were included in the gaming predictor analysis.
As far as the cause of the difference between our findings and ones in (Baker et al.
2009), there are a number of possibilities. First and foremost, the argument in (Baker
et al. 2009) that instructional features better predict gaming was based on the fact that
the corresponding model predicted more of the variance in the data than prior work
with models incorporating only student features. However, as we suggest above, even

123

An analysis of students’ gaming behaviors

129

the Baker model may have relied on some student features. This makes it more difficult to compare the output of the two models, since ours is the only one that explicitly
divides student and problem features. Second, it is possible that the labeling of the data
influenced the findings. While we relied on a computational gaming detector, in (Baker
et al. 2009) a human observer coded the data, and only coded a representative sample.
There are trade offs with either approach: while a computational detector might miss
some nuances that a human could pick up, the former is also more consistent than a
human who is tasked with the rather arduous job of labeling thousands of episodes.
In general, our model does quite well in explaining the variance in the Andes data,
surpassing existing approaches. The amount of variance explained by the model does
decrease somewhat when applied to the Cognitive Tutor data. There are a number of
possible reasons for this. First, the Andes ITS might have less instructional variability than the Cognitive Tutor, as suggested by the ‘gaming distribution analysis’, and
the gaming detector may need to take into account more fine-grained features than
problem to capture that variability. Second, we found that in the Andes model student
was a very strong predictor of gaming, more so than in the Cognitive Tutor model;
this helped to increase the Andes model’s ability to explain the gaming variance. The
two sets of data came from very different contexts and populations: the Andes data
corresponds to college students working at home while the Cognitive Tutor data came
from high school students working in classrooms. When we recently did a preliminary
analysis on a set of high school honors students using Andes mostly in their classroom, we found that their gaming levels were much lower (in the 5% range) than
those of the college students reported here; thus it is possible that gaming behaviors
differ between these two populations and contexts, and in particular are more predictable in older populations/non-classroom settings where students are not observed by
researchers/teachers, something that warrants further analysis and validation.
In addition to exploring predictors of gaming, we also analyzed differences between
students in terms of gaming behaviors. We found that when we looked at gaming opportunities over the tutor-student turn pairs, students most often seized the opportunity to
game after the tutor presented them with a high-level hint. However, when we analyzed
the proportion of each type of gaming over the total gaming events for each class of
gamers, in contrast to high-gamers (who primarily skipped hints), the low gamers had
a slightly higher incidence of guessing on entries. One possible explanation for this
difference, supported by literature on individual differences in help seeking behaviors
(Nelson-Le Gall 1985), is that the low gamers preferred to obtain the solution on their
own, without the tutor’s help. Another possibility relates to Andes’ scoring system.
Recall that students were penalized for asking for a bottom out hint but were not
penalized for guessing, and so perhaps the low gamers were simply more concerned
about their Andes score than high gamers. The analysis also showed, however, that
low gamers spent more time with hints and took longer to generate a solution entry
after seeing a hint. Since no points were awarded for taking time, this suggests that
obtaining a higher score was not the only incentive for the high-gamers, indicating that
perhaps these students were motivated and/or diligent in the problem-solving process.
Prior research suggests that poor hint usability is associated with gaming (Baker
et al. 2009). To see if this was the case, we investigated how students used hints in the
Andes data; doing so was particularly pertinent since certain hints were a highly-gamed

123

130

K. Muldner et al.

feature of the Andes tutor. The basic analysis consisted of obtaining statistics on students’ hint usage and utility. This analysis showed that when Andes presented a highlevel hint, high gamers were quite unlikely to even try generating a corresponding
solution entry, as compared to low gamers. If students did try to generate a solution
entry, both low and high gamers were moderately successful when given high-level
hints. This conclusion was based on determining if students could (eventually) generate a correct entry after being presented with a hint, prior to seeing another hint. An
alternative possibility, however, is that students gave up using the high-level hint and
generated the solution entry on their own.
While the basic analysis provided some indication for the utility of hints for scaffolding short term performance, it did not provide information on whether hints fostered
long-term shifts, i.e., learning. To investigate this aspect, we applied the data mining
techniques advocated in (Beck et al. 2008), by learning Bayesian network parameters
related to the utility of hints.
Bayesian networks have been long been used in the user modeling community for
representing and inferring various user states of interest, such as knowledge (Mayo
and Mitrovic 2001; Conati et al. 2002), affect (Conati and Maclaren 2009) and metacognitive tendencies (Bunt et al. 2004; Muldner and Conati 2005). Another application
of Bayesian networks, however, pertains to data mining. Parameter mining has been
used in the data mining community outside of educational applications, for instance
to analyze tuberculosis epidemiology (Getoor et al. 2004) and/or understand gene
variation (Rodin et al. 2005). As discussed in (Heckerman 1997), there are a number
of advantages of the graphical model afforded by a Bayesian network for traditional
data mining, including: (1) the ability to gracefully handle missing data points, (2)
during exploratory analysis, support for insight into a particular domain, including
causal relationships in that domain, (3) affordance for the integration of prior knowledge about a domain into the learning process and (4) Bayesian statistical methods
afford a principled approach to avoid over fitting the data. Although EDM has some
unique needs as compared to traditional mining (Baker 2010), these advantages clearly
also apply to educational domains. Bayesian networks offer other advantages as well.
First, they can be easily extended to provide support for decision tasks, enabling
them to be used for risk analysis. While traditionally this approach has not been used
in educational applications, one can imagine doing so could be highly informative;
for instance, “risk” could correspond to analyzing when a student looses motivation
and therefore disengages from the learning task. Second, Bayesian networks afford
a clear semantic interpretation of the model parameters. This is especially beneficial
for EDM applications since it means that potentially novel relationships encoded in
the network are directly visible. Thus, for data mining tasks in general, and EDM
tasks in particular, Bayesian networks are well suited in helping to identify and/or
model relationships among a large variety of variables. One disadvantage of Bayesian
learning is the computational expense, although this is becoming less of an issue with
the increasingly-available processor resources.
Our results using Bayesian parameter learning confirmed the basic analysis that
hints scaffolded short-term problem solving, both in the original DBN Help model
that only considered whether a hint was requested or not, and in a refined version that
took into account the type of hint (high-level versus bottom-out). However, the former

123

An analysis of students’ gaming behaviors

131

analysis also showed that students learned slightly better in the absence of hint(s)
as compared to when given hint(s), while in prior work students did very slightly
better when given help (Beck et al. 2008). When we dug deeper into this issue, we
found that learning from hints was influenced by several factors. First, the type of
hint had an impact on learning: the network parameters indicated that students learned
the most from bottom-out hints, as opposed to high-level hints. Second, gaming also
had an impact. In particular, low gamers appeared to benefit from both high-level and
bottom-out hints, while the high gamers only (slightly) learned from bottom-out hints.
This discussion highlights how data mining can guide subsequent research by pointing to the need to more closely investigate (1) the utility of high-level hints and (2) how
student characteristics interact to influence learning and problem-solving outcomes
with various kinds of hints. One way this could be achieved is through talk-aloud
protocol, where students are asked to verbally express their thoughts as they interact
with an ITS. While this technique is established as a means of obtaining rich data on
users’ cognitive processes (e.g., Chi et al. 2008), surprisingly it is not often applied
in the context of ITS evaluation (although exceptions exist, e.g., D’Mello et al. 2006;
Muldner and Conati 2010). We should point out that there already is substantial work
on the utility of help, both in the cognitive science and the ITS communities (e.g., Phye
and Sanders 1994; Chi et al. 2008; Hays et al. 2009; Razzaq and Heffernan 2009).
However, there is a lack of agreement across studies as to which type of feedback is
best, as is pointed out by various meta-analyses (e.g., Shute 2008). Some researchers
suggest that bottom-out hints can be useful for learning (Shih et al. 2008), but do
not compare the utility of various types of hints. Work directly comparing various
levels of feedback found that specific feedback, i.e., providing the answer, is better
than general high-level advice (Phye and Sanders 1994). In contrast, however, there
is also evidence that high-level scaffolding, which is at least somewhat comparable
to general hints, fosters learning better than simply providing didactic explanations
(e.g., the answer) (Chi et al. 2008; Chi 2010). Furthermore, student characteristics,
such as learning orientation, may interact with the utility of various types of help.
For instance, Davis et al. (Davis et al. 2005) found that students with low and high
learning orientation perform better with more specific help, as compared to general
help. We also found that low and high gamers, who could arguably be labeled as high
and low achievers, respectively, had better problem-solving performance and slightly
better learning with bottom-out, specific hints, but that only low gamers benefited in
terms of learning from high-level hints.
In addition to using it for analyzing hint impact, we also relied on Bayesian network parameter learning to analyze impact of gaming on various outcomes. Our results
confirm other findings that gaming in general can be harmful to learning (Baker et al.
2004b), but to the best of our knowledge, ours is the first work to show this in the
absence of pre/post test data. The analysis also highlights another reason why gaming
is not an effective strategy: in the Gaming model, the probability of making a slip
when gaming is present is higher than in the absence of gaming. One way to apply this
particular result is through the design of tutorial interventions. For instance, students
are willing to shift to strategies that foster learning if they understand that it is more
cost effective to do so, for instance in terms of time (Chi and VanLehn 2007b,a). Thus,
perhaps we can convince students, for instance via visualizations extending existing

123

132

K. Muldner et al.

ones (Walonoski and Heffernan 2006b), that gaming is not effective strategy because
it results in more errors (slips).
As the next steps, we plan to follow up on some of the avenues highlighted by
the data mining analyses, including evaluating further the usage of high-level hints
through verbal protocols. We are also in the process of designing tailored interventions to reduce gaming in Andes. As we described in Sect. 2, interventions for reducing
gaming have been successful in reducing gaming overall, but only in the less-frequent
gamers, and often without influencing learning outcomes. Since findings suggest that
student features are a key predictor of gaming, we believe that to be successful on all
these fronts related to discouraging gaming, the system needs to tailor its interventions according to a user model that takes into account user traits relevant to gaming.
To this end, we are planning on extending the gaming model we proposed here to
include user traits of interest, e.g., gaming tendency, and using it in Andes to inform
the interventions to discourage gaming.
Acknowledgements The authors thank the anonymous reviewers for their helpful suggestions. This research was funded the National Science Foundation, including the following grants: (1) IIS/HCC Affective
Learning Companions: Modeling and supporting emotion during learning(#0705883); (2) Deeper Modeling
via Affective Meta-tutoring(DRL-0910221) and (3) Pittsburgh Science of Learning Center(SBE-0836012).

References
Aleven, V.: Helping students to become better help seekers: towards supporting metacognition in a cognitive
tutor. In: Paper Presented at German-USA Early Career Research Exchange Program: Research on
Learning Technologies and Technology-Supported Education (2001)
Aleven, V., McLaren, B., Roll, I., Koedinger, K.: Toward meta-cognitive tutoring: a model of help seeking
with a cognitive tutor. Int. J. Artif. Intell. Educ. 16, 101–128 (2006)
Arroyo I., Woolf B.: Inferring learning and attitudes from a Bayesian network of log file data. In: Proceedings
of the 12th International Conference on Artificial Intelligence in Education (AIED’05), Amsterdam,
Netherlands, pp. 33–40 (2005)
Arroyo, I., Ferguson, K., Johns, J., Dragon, T., Meheranian, H., Fisher, D., Barto, A., Mahadevan, S., Woolf,
B.: Repairing disengagement with non-invasive interventions. In: Proceedings of the 13th International
Conference on Artificial Intelligence in Education (AIED’07), Los Angeles, United States, pp. 195–
202 (2007)
Baker, R.: Is gaming the system state-or-trait? educational data mining through the multi-contextual application of a validated behavioral model. In: Proceedings of the Workshop on Data Mining for User
Modeling, Corfu, Greece, pp. 76–80 (2007)
Baker, R., Corbett, A., Koedinger, K.: Detecting student misuse of intelligent tutoring systems. In: Proceedings of the 7th International Conference on Intelligent Tutoring Systems (ITS’04), Maceio, Brazil, pp.
531–540 (2004a)
Baker, R., Corbett, A., Koedinger, K., Wagner, A.: Off-task behavior in the cognitive tutor classroom: when
students “game the system”. In: Proceedings of the ACM CHI 2004: Computer-Human Interaction
(CHI’04), Vienna, Austria, pp. 383–390 (2004b)
Baker, R.S., Roll, I., Corbett, A., Koedinger, K.: Do performance goals lead students to game the system. In: Proceedings of the 12th International Conference on Artificial Intelligence and Education
(AIED2005), Amsterdam, Netherlands, pp. 57–64 (2005)
Baker, R., Corbett, A., Koedinger, K., Evenson, E., Roll, I., Wagner, A., Naim, M., Raspat, J., Baker, D.,
Beck, J.: Adapting to when students game an intelligent tutoring system. In: Proceedings of the 8th
International Conference on Intelligent Tutoring Systems (ITS’06), Jhongli, Taiwan, pp. 392–401
(2006a)
Baker, R.S.J.d., Corbett, A.T., Koedinger, K., Roll, I.: Generalizing detection of gaming the system across
a tutoring curriculum. In: Proceedings of the 11th International Conference on Intelligent Tutoring
Systems (ITS’06), Jhongli, Taiwan, pp. 402–411 (2006b)

123

An analysis of students’ gaming behaviors

133

Baker, R., Corbett, A., Roll, I., Koedinger, K.: Developing a generalizable detector of when students game
the system. User Model. User-Adap. Inter. 18(3), 287–314 (2008a)
Baker, R., Walonoski, J., Heffernan, N., Roll, I., Corbett, A., Koedinger, K.: Why students engage in “gaming the system”. Behavior in interactive learning environments. J. Interact. Learn. Res. 19(2), 185–
224 (2008b)
Baker, R., Corbett, A., Koedinger, K.: Educational software features that encourage and discourage “gaming
the system”. In: Proceedings of the 14th international conference on artificial intelligence in education
(AIED’09), Brighton, UK, pp. 475–482 (2009)
Baker, R.: Data mining for education. In: McGaw, B., Peterson, P., Baker, E. (eds.) International Encyclopedia of Education, vol. 7, 3rd edn, pp. 112–118. Elsevier Oxford, UK (2010)
Beck, J.E., Jia, P., Mostow, J.: Using automated questions to assess reading comprehension, vocabulary,
and effects of tutorial interventions. Technol. Instr. Cogn. Learn. (TICL) 2, 97–134 (2004)
Beck, J., Chang, K., Mostow, J., Corbett, A.: Does help? Introducing the Bayesian evaluation and assessment methodology. In: Proceedings of the 9th International Conference on Intelligent Tutoring Systems
(ITS’08), Montreal, Canada, pp. 383–394 (2008)
Bunt, A., Conati, C., Muldner, K.: Scaffolding self-explanation to improve learning in exploratory learning
environments. In: Proceedings of the 7th International Conference on Intelligent Tutoring Systems
(ITS’04), Maceio, Brazil, pp. 656–667 (2004)
Carnegie Learning Inc.: Retrieved April 1, 2010, from http://www.carnegielearning.com/ (2010)
Chang, K., Beck, J., Mostow, J., Corbett, A.: A Bayes net toolkit for student modeling in intelligent tutoring systems. In: Proceedings of the 11th International Conference on Intelligent Tutoring Systems
(ITS’06), Jhongli, Taiwan, pp. 104–113 (2006)
Chi, M.T.H.: How adaptive is an expert human tutor? In: Proceedings of the 10th International Conference
on Intelligent Tutoring Systems (ITS’10), Pittsburgh, United States, pp. 401–113 (2010)
Chi, M., VanLehn, K.: The impact of explicit strategy instruction on problem-solving behaviors across
intelligent tutoring systems. In: Proceedings of the 29th annual conference of the cognitive science
society, Nashville, Tennessee, pp. 167–172 (2007a)
Chi, M., VanLehn, K.: Accelerated future learning via explicit instruction of a problem solving strategy.
In: Proceedings of the 13th international conference on artificial intelligence in education (AIED’07),
Los Angeles, United States, pp. 409–416 (2007b)
Chi, M.T.H., Bassok, M., Lewis, M., Reimann, P., Glaser, R.: Self-explanations: how students study and
use examples in learning to solve problems. Cogn. Sci. 15, 145–182 (1989)
Chi, M.T.H., Siler, S.A., Jeong, H., Yamauchi, T., Hausmann, R.G.: Learning from human tutoring. Cogn.
Sci. 25, 471–533 (2001)
Chi, M.T.H., Roy, M., Hausmann, R.: Observing tutoring collaboratively: insights about tutoring effectiveness from vicarious learning. Cogn. Sci. 32(2), 301–341 (2008)
Chi, M., VanLehn, K., Litman, D.: Do micro-level tutorial decisions matter: applying reinforcement learning to induce pedagogical tutorial tactics. In: Proceedings of the 10th International Conference on
Intelligent Tutoring Systems (ITS’10), Pittsburgh, United States, pp. 184–193 (2010a)
Chi, M., VanLehn, K., Litman, D., Jordan, P.: Inducing effective pedagogical strategies using learning context features. In: Proceedings of the 18th International Conference on User Modeling, Adaptation and
Personalization (UMAP’10), Kona, Hawaii, pp. 147–158 (2010b)
Chi, M., VanLehn, K., Litman, D., Jordan P.: Empirically evaluating the Application of reinforcement learning to the induction of effective and adaptive pedagogical strategies. User model User-Adap. Inter.
(to appear). Special issue on educational data mining for personalized educational systems
Cohen, P., Beal, C.: Temporal data mining for educational applications. Int. J. Softw. Inform. 3(1),
31–46 (2009)
Conati, C., Gertner, A., VanLehn, K.: Using Bayesian networks to manage uncertainty in student modeling. User Model. User-Adap. Inter. 12(4), 371–417 (2002)
Conati, C., Maclaren, H.: Empirically building and evaluating a probabilistic model of user affect. User
Model. User-Adap. Inter. 19(3), 267–303 (2009)
Corbett, A.T., Anderson, J.R.: Knowledge tracing: modeling the acquisition of procedural knowledge. User
Model. User-Adap. Inter. 4(4), 253–278 (1995)
D’Mello, S., Craig, S., Sullins, J., Graesser, A.: Predicting affective states expressed through an emote-aloud
procedure from autotutor’s mixed-initiative dialogue. Int. J. Artif. Intell. Educ. 16(1), 3–28 (2006)
Davis, W., Carson, C., Ammeter, A., Treadway, D.: The interactive effects of goal orientation and feedback
specificity on task performance. Human Perform. 18, 409–426 (2005)

123

134

K. Muldner et al.

Dean, T., Kanazawa, K.: A model for reasoning about persistence and causation. Comput. Intell. 5(3), 142–
150 (1989)
Dempster, L., Laird, N., Rubin, D.: Maximum likelihood from incomplete data via the EM algorithm. J. R.
Stat. Soc. 39(1), 1–38 (1977)
Feng, M., Beck, J., Heffernan, N.T.: Using learning decomposition and bootstrapping with randomization
to compare the impact of different educational interventions on learning. In: Proceedings of the 2nd
International Conference on Educational Data Mining (EDM’09), Cordoba, Spain, pp. 51–60 (2009)
Getoor, L., Rhee, J., Koller, D., Small, P.: Understanding tuberculosis epidemiology using probabilistic
relational models. J. Artif. Intell. Med. 30, 233–256 (2004)
Hays, M., Lane, C., Auerbach, D., Core, M., Gomboc, D., Rosenberg, M.: Feedback specificity and the
learning of intercultural communication skills. In: Proceedings of the 14th International Conference
on Artificial Intelligence in Education (AIED’09), Brighton, UK, pp. 391–398 (2009)
Heckerman, D.: Bayesian networks for data mining. Data Min. Knowl. Discov. 1, 79–119 (1997)
Koedinger, K.R., Anderson, Hadley, W.H., Mark, M.A.: Intelligent tutoring goes to school in the big city. In:
Proceedings of the 3rd International Conference on Artificial Intelligence and Education (AIED’95),
Washington, DC, United States, pp. 421–428 (1995)
Mayo, M., Mitrovic, A.: Optimizing ITS behaviour with Bayesian networks and decision theory. Int. J.
Artif. Intell. Educ. 12, 124–153 (2001)
Muldner, K., Conati, C.: Using similarity to infer meta-cognitive behaviors during analogical problem solving. In: Proceedings of the 10th International Conference on User Modeling (UM’05), Edinborough,
Scottland, pp. 134–143 (2005)
Muldner, K., Burleson, W., de Sande, B., VanLehn, K.: An analysis of gaming behaviors in an intelligent
tutoring system. In: Proceedings of the 10th International Conference on Intelligent Tutoring Systems
(ITS’10), Pittsburgh, United States, pp. 195–205 (2010)
Muldner, K., Conati, C.: Scaffolding meta-cognitive skills for effective analogical problem solving via
tailored example selection. Int. J. Artif. Intell. Educ. 20(2), 99–136 (2010)
Murphy, K.: Dynamic Bayesian Networks: Representation, Inference and Learning. Ph.D. Thesis, UC,
Berkeley (2002)
Murphy, K.: Bayes Net Toolbox for Matlab. http://bnt.sourceforge.net. Accessed 2010 April 1 (2004)
Murray, R.C., VanLehn, K.: Effects of dissuading unnecessary help requests while providing proactive
help. In: Proceedings of the 12th International Conference on Artificial Intelligence in Education
(AIED’05), Amsterdam, Netherlands, pp. 887–889 (2005)
Nelson-Le Gall, S.: Help-seeking behavior in learning review of research in education. Rev. Res.
Educ. 12, 55–90 (1985)
Netica Reference Manual. Retrieved March 1, 2010, from www.norsys.com/netica-j/docs/NeticaJ_Man.
pdf (2010)
Phye, G.D., Sanders, C.: Advice and feedback: elements of practice for problem solving. Contemp. Educ.
Psychol. 19(3), 286–301 (1994)
Rai, D., Gong, Y., Beck, J.: Using Dirichlet priors to improve model parameter plausibility. In: Proceedings
of the international conference on educational data mining (EDM’09), pp. 141–150 (2009)
Razzaq, L., Heffernan, N.: To tutor or not to tutor: that is the question. In: Proceedings of th 2nd International
Conference on Artificial Intelligence in Education (AIED’09), Cordoba, Spain, pp. 457–464 (2009)
Renkl, A.: Learning from worked-examples: a study on individual differences. Cogn. Sci. 21(1), 1–30 (1997)
Reye, J.: Student modeling based on belief networks. Int. J. Artif. Intell. Educ. 14, 1–33 (2004)
Ritter, S., Harris, T., Nixon, T., Dickison, D., Murray, R.C., Towle, B.: Reducing the knowledge tracing
space. In: Proceedings of the 1st International Conference On Educational Data Mining (EDM’08),
Montreal, Canada, pp. 151–160 (2008)
Rodin, A., Mosley, T.H., Clark, A.G., Sing, C.F., Boerwinkle, E.: Mining genetic epidemiology data
with Bayesian networks application to APOE gene variation and plasma lipid levels. J. Comput.
Biol. 12(1), 1–11 (2005)
Rodrigo, M., Baker, R., d’Mello, S., Gonzalez, M.C.T., Lagud, M., Lim, S., Macapanpan, A., Pascua, S.,
Santillano, J., Sugay, J., Tep, S., Viehland, N.: Comparing learners affect while using an intelligent
tutoring systems and a simulation problem solving game. In: Proceedings of the 9th International
Conference on Intelligent Tutoring Systems (ITS’08), Montreal, Canada, pp. 40–49 (2008)
Roll, I., Aleven, V., McLaren, B., Ryu, E., Baker, R.S.J.d., Koedinger, K.: The help tutor: does metacognitive feedback improve students’ help-seeking actions, skills and learning? In: Proceedings

123

An analysis of students’ gaming behaviors

135

of the Eight International Conference on Intelligent Tutoring Systems (ITS’06), Jhongli, Taiwan,
pp. 360–369 (2006)
Russell, S., Norvig, P.: Artificial Intelligence: A Modern Approach. Los Altos, CA, Morgan-Kaufman
(2009)
Shih, B., Koedinger, K., Scheines, R.: A response time model for bottom-out hints as worked examples.
In: Proceedings of the 1st International Conference on Educational Data Mining (EDM’08), Montreal
Canada, pp. 117–126 (2008)
Shute, V.: Focus on formative feedback. Rev. Educ. Res. 78(1), 153–189 (2008)
VanLehn, K.: Analogy events: how examples are used during problem solving. Cogn. Sci. 22(3), 347–
388 (1998)
VanLehn, K., Lynch, C., Schulze, K., Shapiro, J.A., Shelby, R., Taylor, L., Treacy, D., Weinstein, A.,
WintersgillM.TheAndesphysics tutoring, system: Lessons learned. Int. J. Artif. Intell. Educ. 15(3), 1–
47 (2005)
VassarStats: Website for Statistical Computation. Retrieved April 1, 2010, from http://faculty.vassar.edu/
lowry/VassarStats.html
Walonoski, J.A., Heffernan, N.T.: Detection and analysis of off-task gaming behavior in intelligent tutoring
systems. In: Proceedings of the 8th International Conference on Intelligent Tutoring Systems (ITS’06),
Jhongli, Taiwan, pp. 382–391 (2006a)
Walonoski, J.A., Heffernan, N.T.: Prevention of off-task gaming behavior in intelligent tutoring systems.
In: Proceedings of the 8th International Conference on Intelligent Tutoring Systems (ITS’06), Jhongli,
Taiwan, pp. 722–724 (2006b)
Zhang, X., Mostow, J., Beck, J.: A case study empirical comparison of three methods to evaluate tutorial behaviors. In: Proceedings of the 9th International Conference on Intelligent Tutoring Systems
(ITS’08), Montral, Canada, pp. 122–131 (2008)

Author Biographies
Kasia Muldner is currently a Post-Doctoral Scholar in the department of Psychology at Arizona State University (ASU); prior to this appointment, she worked as a Post-Doctoral Scholar in the School of Computing,
Informatics, and Decision Systems Engineering, also at ASU. She received her Ph.D. from the University
of British Columbia, Canada, from the Computer Science Department at the Laboratory for Computational
Intelligence. Her research interests span Artificial Intelligence, Cognitive Science and Human Computer
Interaction, with a particular focus on pedagogical applications, including the design and evaluation of
intelligent tutoring systems, educational data mining, and investigating ways to foster constructive learning, for instance during analogical problem solving and learning from observing.
Winslow Burleson is an Assistant Professor of Human-Computer Interaction in the School of Computing,
Informatics, and Decision Systems Engineering at Arizona State University, and directs the Motivational
Environments research group (http://hci.asu.edu). Author of over 60 scientific publications, including “best
paper” at AIED 2009, and 10 patents, he received a Ph.D. in Affective Computing from the MIT Media
Lab, a BA from Rice, and MSE from Stanford. He worked with the Life Long Kindergarten group and
Entrepreneurial Management Unit at Harvard Business School, IBM Research, NASA-SETI Institute, the
Space Telescope Science Institute and UNICEF.
Brett Van de Sande is an Assistant Research Professional in the School of Computing, Informatics, and
Decision Systems Engineering at Arizona State University, working in the area of intelligent tutoring systems. He completed his Ph.D. at Ohio State University in theoretical physics, and has since taught physics
and conducted research in physics education, including the development of adaptive physics tutors.
Kurt VanLehn is a Professor of Computer Science at Arizona State University. He completed his Ph.D.
at MIT, did post-doctoral research at Xerox PARC, joined the faculty of Carnegie-Mellon University in
1985, moved to the University of Pittsburgh in 1990 and joined ASU in 2008. His work involves applications of artificial intelligence to education, including intelligent tutoring systems, cognitive modeling and
educational data mining.

123

Carnegie Mellon University

Research Showcase @ CMU
Machine Learning Department

School of Computer Science

7-2011

Instructional Factors Analysis: A Cognitive Model
For Multiple Instructional Interventions
Min Chi
Stanford University

Kenneth R. Koedinger
Carnegie Mellon University, koedinger@cmu.edu

Geoffrey J. Gordon
Carnegie Mellon University, ggordon@cs.cmu.edu

Pamela Jordon
University of Pittsburgh

Kurt VanLahn
Arizona State University

Follow this and additional works at: http://repository.cmu.edu/machine_learning
Part of the Theory and Algorithms Commons
Published In
Proceedings of the 4th International Conference on Educational Data Mining, 61-70.

This Conference Proceeding is brought to you for free and open access by the School of Computer Science at Research Showcase @ CMU. It has been
accepted for inclusion in Machine Learning Department by an authorized administrator of Research Showcase @ CMU. For more information, please
contact research-showcase@andrew.cmu.edu.

Instructional Factors Analysis: A Cognitive Model For Multiple
Instructional Interventions
Min Chi, Stanford University, CA USA
Kenneth Koedinger, Carnegie Mellon University, PA USA
Geoff Gordon, Carnegie Mellon University, PA USA
Pamela Jordan, University of Pittsburgh, PA USA
Kurt VanLehn, Arizona State University, AZ USA

In this paper, we proposed a new cognitive modeling approach: Instructional Factors Analysis Model (IFM). It belongs to a
class of Knowledge-Component-based cognitive models. More specifically, IFM is targeted for modeling student’s performance
when multiple types of instructional interventions are involved and some of them may not generate a direct observation of
students’ performance. We compared IFM to two other pre-existing cognitive models: Additive Factor Models (AFMs) and
Performance Factor Models (PFMs). The three methods differ mainly on how a student’s previous experience on a Knowledge
Component is counted into multiple categories. Among the three models, instructional interventions without immediate direct
observations can be easily incorporate into the AFM and IFM models. Therefore, they are further compared on two important
tasks—unseen student prediction and unseen step prediction—and to determine whether the extra flexibility afforded by additional parameters leads to better models, or just to over fitting. Our results suggested that, for datasets involving multiple types
of learning interventions, dividing student learning opportunities into multiple categories is beneficial in that IFM out-performed
both AFM and PFM models on various tasks. However, the relative performance of the IFM models depends on the specific
prediction task; so, experimenters facing a novel task should engage in some measure of model selection.

1.

INTRODUCTION

For many existing Intelligent Tutoring Systems (ITSs), the system-student interactions can be viewed as a
sequence of steps [VanLehn 2006]. Most ITSs are student-driven. That is, at each time point the system elicits
the next step from students, sometimes with a prompt, but often without any prompting (e.g., in a free form
equation entry window where each equation is a step). When a student enters an attempt on a step, the ITS
records whether it is a success or failure without the tutor’s assistance and may give feedbacks and/or hints
based on the entry. Students’ first attempt records on each step are then collected for student modeling.
Often times in ITSs, completion of a single step requires students to apply multiple Knowledge Components.
A Knowledge Component (KC) is: “a generalization of everyday terms like concept, principle, fact, or skill,
and cognitive science terms like schema, production rule, misconception, or facet” [VanLehn et al. 2007].
They are the atomic units of knowledge. Generally speaking, students’ modeling on conjunctive-KC steps
are more difficult than that on steps that require a single KC.
The three most common student modeling methods are: Knowledge Tracing (KT) [Corbett and Anderson 1995], Additive Factor Models (AFM) [Cen et al. 2006; 2008], and Performance Factor Models
(PFM) [Pavlik et al. 2009]. When performing student modeling we seek to construct a cognitive model
based upon these observed behaviors and to apply the model to make predictions. Generally speaking, we
are interested in three types of predictions: type 1 is about how unseen students will perform on the observed
steps same as those in the observed dataset; type 2 is about how the same students seen in the observed data
will perform on unseen steps; and type 3 is about how unseen students will perform on unseen steps, that
is, both. For the present purposes we classifie students or steps that appear in the observed training data

as seen and those that appear only in the unobserved test data as unseen. In this paper we will examine
prediction types 1 and 2 and leave type 3 for future work.
Previously KT and PFM have been directly compared both on datasets involved single-KC steps [Pavlik
et al. 2009] and those involved conjunctive-KC steps[Gong et al. 2010]. Results have shown that PFM is as
good or better than KT for prediction tasks under Bayesian Information Criteria (BIC) [Schwarz 1978] in
[Pavlik et al. 2009] or using Mean Squared Error (MSE) as criteria in [Gong et al. 2010]. For both BIC and
MSE, the lower the value, the better.
While PFM and KT have been compared on datasets involved conjunctive-KC step, prior applications of
AFM and PFM have mainly been with single-KC steps and indicated no clear winner. More specifically, while
AFM is marginally superior to PFM in that the former has lower BIC and cross-validation Mean Absolute
Deviance (MAD) scores in [Pavlik et al. 2009], PFM performed better than AFM under MAD scores in
[Pavlik et al. 2011]. For MAD, same as MSE, the lower the value, the better. On the other hand, previous
research have shown that AFM can, at least in some cases, do a fine job in modeling conjunctive KCs [Cen
et al. 2008]. Therefore, in this paper we will compare AFM and PFM directly on a dataset involving many
conjunctive-KC steps.
Moreover, most prior research on cognitive modelings was conducted on datasets collected from classical
student-driven ITSs. Some ITSs, however, are not always student-driven in that they may involve other
instructional interventions that do not generate direct observations on student’s performance. The dataset
used in this paper, for example, was collected from a tutor that, at each step chose to elicit the next step
information from students or to tell them the next step. In our view these tell steps should also be counted
as a type of Learning Opportunity (LO) as they do provide some guidance to students. Yet on the other
hand, these steps do not allow us to directly observe students’ performance. KT model is designed mainly for
student-driven ITSs in that its parameters are directly learned from the sequences of student’s performance
(right or wrong) on each step. When there are multiple instructional interventions and some of them do
not generate direct observations, it is not very clear how to incorporate these interventions directly into
conventional KT models. Therefore, in this paper we are mainly interested in comparing AFM and PFM.
Our dataset was collected from an ITS that can either elicit the next step from the student or tell them
directly. Incorporating tell steps into AFM model is relatively easy in that tells can be directly added to
total LO counts. The PFM, however, uses student’s prior performance counts, the success or failure, in the
equation. Since tells do not generate any observed performance, it is hard to include them in the PFM.
Therefore, we elected to add a new feature to represent instructional interventions such as tells. As shown
later, the new model can be easily modified for modeling datasets with multiple instructional interventions
and thus it is named as Instructional Factors Analysis Model (IFM).
To summarize, in this paper we will compare three models, AFM, PFM and IFM, on a dataset involving
many conjunctive-KC steps and multiple instructional interventions. Previous research has typically focused
on how well the models fit the observed data. In the following, we also investigated how well they perform at
making the predictions of unseen students’ performance on seen steps (type 1) and seen students’ performance
on unseen steps (type 2). Before describing our general methods in details we will first describe the three
models.

2.

THREE MODELS: AFM, PFM, AND IFM

All three models, AFM, PFM, and IFM, use a Q-matrix to represent the relationship between individual
steps and KCs. Q-matrices are typically encoded as a binary 2-dimensional matrix with rows representing
KCs and columns representing Steps. If a given cell Qkj = 1, then step j is an application of KC k. Previous
researchers have focused on the task of generating or tuning Q-matrices based upon a dataset [Barnes 2005;
Tatsuoka 1983]. For the present work we employed a static Q-matrix for all our experiments. Equations 1,

2, and 3 present the core of each model. Below the equations are the detailed descriptions of each term used
in the three equations.
The central idea of AFM was originally proposed by [Draney et al. 1995] and introduced into ITS field by
[Cen et al. 2006; 2008]. Equation 1 shows that AFM defines the log-odds of a student i completing a step j
correctly to be a linear function of several covariates. Here pij is a student i’s probability of completing a step
j correctly, Nik is the prior LO counts. AFM models contain three types of parameters: student parameters
θi , KC (or skill) parameters βk , and learning rates γk . While AFM is sensitive to the frequency of prior
practice, it assumes that all students accumulate knowledge in the same manner and ignores the correctness
of their individual responses.
PFM, by contrast, was proposed by [Pavlik et al. 2009] by taking the correctness of individual responses
into account. It can be seen as a combination of learning decomposition [Beck and Mostow 2008] and AFM.
Equation 2 expresses a student i’s log-odds of completing a step j correctly based upon performance features
such as Sik (the number of times student i has previously practiced successfully relevant KC k) and Fik (the
number of times student i has previously practiced unsuccessfully relevant KC k). PFM may also include
student parameters such as θi and skill parameters, such as βk . Additionally, PFM employs parameters to
represent the benefit of students’ prior successful applications of the skill µk and the benefit of prior previous
failures ρk .
While PFM was originally proposed without a θi , it is possible to include or exclude these student parameters from either PFM or AFM. In prior work, Corbett et al. noted that models which tracked learning
variability on a per-subject basis, such as with θ outperform models that do not [Corbett and Anderson
1995]. Pavlik [Pavlik et al. 2009] further noted that the full AFM model seemed to outperform PFM without θ which in turn outperformed AFM without θ. Pavlik et al. also hypothesized that PFM with θ would
outperform the other models and they investigated it in their recent work. In this study, our analysis showed
that prediction is better with student parameters, especially for AFM models, thus we include θi in our
versions of both AFM and PFM.
From PFM, IFM can be seen as adding a new feature to represent the tells together with the success
or failure counts, shown in Equation 3. Equation 3 expresses a student i’s log-odds of completing a step
j correctly based upon performance features including Sik , Fik , Tik (the number of times student i has
previously got told on relevant KC k). IFM also includes student parameters θi , skill parameters βk , µk , ρk ,
and the benefit of prior previous tells νk .
AFM:
PFM:
IFM:

X
X
pij
= θi +
βk Qkj +
Qkj (γk Nik )
1 − pij
k
k
X
X
pij
ln
= θi +
βk Qkj +
Qkj (µk Sik + ρk Fik )
1 − pij
k
k
X
X
pij
= θi +
βk Qkj +
Qkj (µk Sik + ρk Fik + νk Tik )
ln
1 − pij
ln

k

k

Where:
i. represents a student i.
j. represents a step j.
k. represents a skill or KC k.
pij . is the probability that student i would be correct on step j.
θi . is the coefficient for proficiency of student i.
βj . is coefficient for difficulty of the skill or KC k.

(1)
(2)
(3)

Qkj . is the Q-matrix cell for step j using skill k.
γk . is the coefficient for the learning rate of skill k (AFM only);
Nik . is the number of practice opportunities student i has had on the skill k (AFM only);
µk . is the coefficient for the benefit of previous successes on skill k (PFM & IFM);
Sik . is the number of prior successes student i has had on the skill k (PFM & IFM);
ρk . is the coefficient for the benefit of previous failures on skill k (PFM & IFM);
Fik . is the number of prior failures student i has had on the skill k (PFM & IFM);
νk . the coefficient for the benefit of previous tells on skill k (IFM only);
Tik . the # of prior Tells student i has had on the skill k (IFM only);
3.

TRAINING DATASET AND EIGHT LEARNING OPPORTUNITY MODES

The original dataset was collected by training 64 students on a natural-language physics tutoring system
named Cordillera [VanLehn et al. 2007; Jordan et al. 2007] over a period of four months in 2007. The physics
domain contains eight primary KCs including the weight law (KC1 ), Definition of Kinetic Energy (KC20 ),
Gravitational Potential Energy (KC21 ), and so on. All participants began with a standard pretest followed
by training 7 physics problems on Cordillera and then a post-test. The pre- and post-tests are identical in
that they both have the same 33 test items. The tests were given online and consisted of both multiple-choice
and open-ended questions. Open-ended questions required the students to derive an answer by applying one
or multiple KCs.
In this study, our training dataset comprises 19301 data points resulted from 64 students solving 7 training
problems on Cordillera. Each student completed around 300 training problem steps. Note that the training
dataset does not include the pre- or posttest. In other words, a data point in our training dataset is either
the first attempt by a student on an elicit step or a system tell during his/her training on Cordillera only.
There are two types of steps in Cordillera. The primary steps are necessary problem-solving and conceptual
discussion steps. The justification steps, on the other hand, are optional steps that occur when students are
asked to justify the primary step they have just completed. The primary steps are designed to move the
solution process forward while the justification steps are designed to help the students engage with the
domain knowledge in a deeper way. When collecting our dataset the Cordillera system decided whether to
elicit or tell each step randomly. Thus, we have two types of LOs: elicit and tell for the primary steps; and
self-explain or explain for the justifications.
Figure 1 shows a pair of sample dialogues taken from the cordillera system for the same series of primary
steps with the same domain content. In dialogue (1.a) the system elects to elicit the students’ answer (steps
2- 3), while in dialogue (1.b) the system chooses to tell the student the answer (steps 2). Similarly in Figure 2
we present a similar comparison between a pair of self-explain and explain dialogues. As before both dialogues
cover the same domain content. In dialogue (2.a) the system asks the student to self-explain their answer
to the question in qualitative terms (steps 3-4). In dialogue (2.b) they are provided with a short qualitative
explanation (step 3).
For the primary steps, the average number of decisions ranges from 2.5 for KC1 to 31 for KC20 for elicit
steps and ranges from 4 for KC1 to 47 for KC20 for the tell steps. Compared with primary steps, justification
steps are significantly less frequent in that the average number ranges from 0 for KC1 to 3.5 for KC21 for
self-explain steps and ranges from 0 for KC1 to 2.5 for KC20 for explain steps.
In most of the prior research that we have surveyed the authors mainly used datasets involving quantitative
problem-solving steps similar to the primary steps in Cordillera. So when counting LOs in this study, we
first need to decide whether or not to include justification steps. Additionally, most previous research did not
include students’ performance on the pretest when counting LOs. While this makes some sense as students

(1.a) Elicit Version

(1)
(2)
(3)
(4)

T: So let’s start with determining the value of v1.
T: Which principle will help you calculate the rock’s instantaneous magnitude of velocity at T1? {ELICIT}
S: definition of kinetic energy
T: Let me just write the equation for you: KE1 = 0.5*m*v1ˆ2.
(1.b) Tell Version

(1) T: So let’s start with determining the value of v1.
(2) T: To calculate the rockś instantaneous magnitude of velocity at T1, we will apply the definition of kinetic
energy. {TELL}
(3) T: Let me just write the equation for you: KE1 = 0.5*m*v1ˆ2.
Fig. 1. Learning Opportunities on Problem-Solving Steps: Elicit vs. Tell

(2.a) Self-Explain
(1)
(2)
(3)
(4)
(5)

T:Can we infer the direction of the velocity of the rock at T1 from the rock’s kinetic energy at T1? {ELICIT}
S:nope.
T:Excellent! Please explain why. {Self-Explain}
S:Only the magnitude of the velocity and not the direction of it is part of the definition of kinetic energy.
T:Excellent! Now that we know v1, · · ·

(2.b) Explain
(1) T:Can we infer the direction of the velocity of the rock at T1 from the rock’s kinetic energy at T1? {ELICIT}
(2) S:nope.
(3) T:Excellent! This is because the kinetic energy only depends on mass and the magnitude of velocity, not the direction
of velocity.{Explain}
(4) T:Now that we know v1, · · ·

Fig. 2. SelfExplain vs. Explain

receive no feedback indicating their successes or failures during the test, it is still the case that they do practice
their skills. Therefore, secondly we need to decide whether or not to include student’s pretest performance
in the LO counts.
In order to explore how different choices of LOs would impact different cognitive models, we defined four
ways to count the LOs. In the primary mode we count only the primary steps within the ITS. In pretestprimary we count the primary mode steps plus the pretest (each test item is treated as one step for training).
Primary-Justify mode counts the primary and justification steps within the ITS alone. And finally the overall
mode counts all steps in both the pretest and ITS training.
Note that using different modes of LOs neither changes the size of the training dataset which is generated
along students’ logs when training on Cordillera nor changes the number of parameters to be fit. Using
pretest in the LO count means that various LOs do not start with 0 for the pretest-primary and overall

modes but are based on the frequency of KC appearances (and, in the case of PFM, the accuracy) in the
pretest. For example, if a KC20 is tested 20 times in the pretest and a student was correct 5 times and
wrong 15 times, then the student’s LOs on KC20 for pretest-primary and overall mode would start with
LO = 20, Success = 5, F ail = 15, T ell = 0. For Primary and Primary-Justify modes, all LOs start with 0.
Coupled with this variation we can also count LOs additively or logarithmically. Using logarithmic count
is inspired by the power law relationship between measures of performance (reaction time or error rate) and
the amount of practice [Newell and Rosenbloom 1981]. But others [Heathcote et al. 2000] have argued that
the relationship is an exponential, which corresponds to additive counting. To summarize, we have {Primary,
Pretest-Primary, Primary-Justify, Overall} × {count, ln(count)}, a total of eight LO modes.
4.

RESULTS

Two measures of quality, the Bayesian Information Criteria (BIC) and the cross-validation Root Mean
Squared Error (RMSE), are used to evaluate how well various instantiated models perform. For both BIC
and cross-validation RMSE, the lower the value, the better. BIC [Schwarz 1978] is a criterion for model
selection among a class of parametric models with different numbers of parameters. In prior research on the
evaluation and comparison of different cognitive models [Cen et al. 2006; Pavlik et al. 2009; Gong et al. 2010]
the authors used BIC as a measure of success. In machine learning, however, it is conventional to use the
cross-validation RMSE, which is a more interpretable metric and, we believe, a more robust measure. For
the purposes of this paper, we will report both BIC and RMSE.
4.1

AFM, PFM, vs. IFM.

First, we will investigate whether considering Tell and Explains into the LOs is beneficial. In traditional
cognitive modeling the focus is solely on steps where the student’s performance is observed. In the context
of Cordillera that means counting only the elicit and self-explain steps as both require students to apply
their knowledge without support and their performance can be directly evaluated. For AFM models, we thus
compared the AFM algorithms shown in equation 1 by either including Tells and Explains into Nik or by
excluding them out of Nik . The two resulted models are referred as AFM-Tell and AFM+Tell respectively.
Therefore, in this section we compared four models: AFM-Tell, AFM+Tell, PFM and IFM across eight LO
modes.
For each of the four models, its corresponding count LOs on corresponding {Primary, Pretest-Primary,
Primary-Justify, Overall} modes are defined in Table I. For example, the IFM has three LO counts: prior
success Sik , prior failures Fik , and prior tells Tik . Under the Primary-Justify mode (shown in the left bottom
of the table), Sik = Success in (Elicit + Self-Explain) on the KC k, Fik = prior failure in (Elicit + SelfExplain) on the KC k, and Tik = prior tells and explains on the KC k. Once the count mode is defined, the
corresponding Ln(Count) mode is simply taking each count logarithmically. For example, under {PrimaryJustify, Ln(Count)} mode, we have Sik = ln[Success in (Elicit + Self-Explain) on KC k], Fik = ln[prior
failure in (Elicit + Self-Explain) on KC k], and Tik = ln[prior tells and explains on the KC k].
For each model on each mode, we carried out a 10-fold cross-validation. Such procedure resulted in 8
(modes) × 4 (models) = 32 BIC values and CV RMSE values. Table II shows the comparisons among the
four models when using {Primary-Justify, Count} and {Primary-Justify, Ln(Count)} LO modes respectively.
It shows that across both modes, the IFM is more accurate (both lower BIC and RMSE) than the PFM;
similarly, the latter is more accurate than AFM+Tell and AFM-Tell. However, it is harder to compare
AFM-Tell and AFM+Tell. For example, on {Primary-Justify, Count} mode, although AFM-Tell has lower
BIC than AFM+Tell 9037 vs. 9058, the latter has lower RMSE than the former: 4.456E-01 vs. 4.459E-01.
So on both {Primary-Justify, Count} and {Primary-Justify, Ln(Count)} modes, we have IFM > PFM >
AFM+Tell, AFM-Tell. Such pattern is consistence across all eight modes.

Table I. {Primary, Pretest-Primary, Primary-Justify, Overall} Learning Opportunity Modes
AFM-Tell
AFM+Tell
PFM
IFM

AFM-Tell
AFM+Tell
PFM
IFM

Primary

Pretest-Primary

Nik
Nik
Sik
Fik
Sik
Fik
Tik

Elicit
Elicit+Tell
Success(Elicit)
Failure(Elicit)
Success(Elicit)
Failure(Elicit)
Tell

Pretest+Elicit
Pretest+Elicit+Tell
Success in (Pretest + Elicit)
Failure in (Pretest + Elicit)
Success in (Pretest + Elicit)
Failure in (Pretest + Elicit)
Tell

Primary-Justify

Overall

Nik
Nik
Sik
Fik
Sik
Fik
Tik

Elicit + SelfExplain
Elicit+Tell + SelfExplain +Explain
Success in (Elicit + Self-Explain)
Failure in (Elicit + Self-Explain)
Success in (Elicit + Self-Explain)
Failure in (Elicit + Self-Explain)
Tell+ Explain

Pretest+ Elicit+SelfExplain
Pretest+ Elicit+Tell + SelfExplain+Explain
Success in (Pretest+ Elicit + Self-Explain)
Failure in (Pretest+ Elicit + Self-Explain)
Success in (Pretest+ Elicit + Self-Explain)
Failure in (Pretest+ Elicit + Self-Explain)
Tell+ Explain

Table II. Compare AFM-Tell, AFM+Tell, PFM and IFM on
{Primary-Justify, Count} and {Primary-Justify, Ln(Count)} mode
Model

{Primary-Justify, Count}
BIC
10-fold RMSE

{Primary-Justify, Ln(Count)}
BIC
10-fold RMSE

AFM-Tell
AFM+Tell
PFM
IFM

9037
9117
8474
8347

9037
9058
8461
8321

4.460E-01
4.470E-01
4.235E-01
4.217E-01

4.459E-01
4.456E-01
4.236E-01
4.211E-01

In order to compare the performance among four models, Wilcoxon Signed Ranks Tests were conducted
on resulted BICs and RMSEs. Results showed that IFM significantly outperformed the PFMs across eight
modes: Z = −2.52, p = 0.012 for both BIC and cross-validation RMSE. Similarly, it was shown that
across all eight modes IFM beat corresponding AFM-Tell across eight modes significantly on both BIC and
RMSE: Z = −2.52, p = 0.012. Similar results were found between IFM and AFM+Tell in that the former
out-performed the latter across eight modes significantly on both BIC and RMSE: Z = −2.52, p = 0.012.
Comparisons between PFM and AFM-Tell and AFM+Tell showed that PFM beats corresponding AFMTell across eight modes significantly on both BIC and RMSE: Z = −2.52, p = 0.012; and PFM also beat
AFM+Tell significantly on both BIC and RMSE: Z = −2.52, p = 0.012. Finally, comparisons between AFMTell and AFM+Tell showed that adding Tells and Explains into LOs did not statistically significantly improve
the BIC and RMSE of the corresponding AFM model: Z = −0.28, p = 0.78 for BIC and Z = −1.35, p = 0.18
for RMSE respectively. Therefore, our overall results suggested: IFM > PFM > AFM-Tell, AFM+Tell.
Next, we investigated which way of counting LOs is better, using logarithmic or additive tabulation?
Wilcoxon Signed Ranks Tests were conducted on comparing the BIC and RMSE of the performances when
using Count versus using Ln(Count) on the same model and mode. Results showed using Ln(Count) performed significantly better than using Count: Z = −2.27, p = 0.008 for BIC and Z = −2.33, p = 0.02
for RMSE respectively. This analysis is interesting in relation to a long-standing debate about whether the
learning curve is exponential (like additive tabulation) or a power law (logarithmic tabulation) [Heathcote
et al. 2000]. Our results appear to favor the power law.
Next, we investigated the impact of four LO modes. The BICs and RMSEs were compared among the
{Primary, Pretest-Primary, Primary-Justify, Overall} modes regardless of Count and Ln(Count). A pairwise
comparisons on Wilcoxon Signed Ranks Tests showed that the {Primary-Justify} modes generated signifi-

cantly better models than using {Primary} modes Z = −2.1, p = 0.036; the {Primary} modes generated
better models than using {Pretest-Primary} and {Overall} Z = −2.27, p = 0.018 and Z = −2.521, p = 0.012
respectively. While no significant difference was found between {Pretest-Primary} and {Overall} modes. Similar results was found on RMSE. Therefore, it suggested that adding justification steps into LOs is beneficial
in that Primary-Justify mode beats Primary; however, adding pretest into the LOs did not produce better
models and it may even have resulted worse models: the benefit of adding justification steps into LOs was
seemingly washed out by including pretest in the LOs in that {Overall} modes generate worse models than
{Primary-Justify} and {Primary}.
To summarize, for modeling the training data, applying IFM model and using {Primary-Justify, Ln(Count)}
as LOs generated the best fitting model. Additionally, comparisons among the IFM, PFM, AFM-Tell,and
AFM+Tell showed that IFM > PFM > AFM-Tell, AFM+Tell. In this paper, our goal is to compare cognitive
models on datasets involving multiple types of instructional interventions. As shown above, for AFM the tell
steps can be directly added into existing opportunity count Nik ; For the PFM model, however, there is no
direct way how tells should be incorporated. Therefore, in the following we will mainly compare IFM and
AFM+Tell. For the convenient reasons, we will refer to AFM+Tell as AFM.
4.2

IFM vs. AFM for Unseen Student Prediction (Type 1)

Next we compared the AFM and IFM models on the task of unseen student prediction. In order to predict
unseen student’s performance, Student ID was treated as a random factor in both AFM and IFM models.
Here we conducted Leave-one-student-out cross-validation. In other words, 64 students resulted in a 64-fold
cross validation. Thus, we have 8 (modes) × 2 (AFM vs.IFM) BIC values and Cross-Validation RMSE values.
Table III shows the correpsonding BIC and RMSE values of AFM and IFM models using {Primary-Justify,
Ln(Count)} mode. Table III shows that IFM generates better prediction models (both lower BIC and RMSE)
than AFM and the difference is large. Such pattern is consistence across all eight modes.
Table III. AFM vs. IFM On Unseen Students
with Random Effect Student Parameters
Model

BIC

64-fold Cross-Validation RMSE

AFM
IFM

8724
7952

4.6144E-01
4.1661E-01

To compare IFM and AFM across eight modes, Wilcoxon Signed Ranks Tests were conducted on both
BICs and cross-validation RMSEs. Consistent with the patterns shown in Table III, results showed that
IFM is significant better than AFM across eight modes: Z = −2.52, p = 0.012 for both BIC and crossvalidation RMSE. To summarize, IFM with random student parameter is a better model for predicting
unseens students’ performances on seen steps than AFM model with random student parameter. The best
performance was generated IFM model using {Primary-Justify, Ln(Count)} as LOs.
4.3

AFM vs. IFM for Unseen Step prediction (Type 2).

Finally we compared AFM and IFM models on the task of unseen step prediction. Here we used training
dataset and tested each models’ prediction using students’ post-test performance. For each model on each
mode, we carried out a 10-fold cross-validation. Such procedure again resulted in 8 × 2 BIC values and CV
RMSE values.
Table IV shows the results on comparisons for the AFM and IFM models on both {Primary-Justify,
Ln(Count)} and {Overall, Ln(Count)} modes. Across the eight LO modes, the performance of AFM reaches
its best when using {Primary-Justify, Ln(Count)} mode and IFM reaches its best when using {Overall,
Ln(Count)} mode. Table III shows that when using {Primary-Justify, Ln(Count)} mode, the AFM is even

more accurate (both lower BIC and RMSE) than the corresponding IFM model; while when using {Overall,
Ln(Count)} LO mode, the IFM is more accurate (both lower BIC and RMSE) than the corresponding AFM.
Moreover, the best IFM model, using {Overall, Ln(Count)} LO mode, is still better than the best AFM
which using {Primary-Justify, Ln(Count)} LO mode. Thus, cross 8 modes on both AFM and IFM, the best
prediction model is still generated by IFM but using {Overall, Ln(Count)} LO mode.
Table IV. AFM vs. IFM On Predicting Post-test
Performance by {Primary-Justify, Ln(Count)} and {Overall,
Ln(Count)} modes
Mode

Model

BIC

10-fold RMSE

{Primary-Justify, Ln(Count)}

AFM
IFM
AFM
IFM

2414
2428
2443
2252

4.6632E-01
4.6791
4.7027E-01
4.4529E-01

{Overall, Ln(Count)}

In order to compare AFM and IFM across eight modes, Wilcoxon Signed Ranks Tests were again conducted
on resulted 8 × 2 BIC and RMSE results. Result showed that IFM is marginally significant better than AFM
across eight modes: Z = −1.68, p = 0.093 for BIC and Z = −1.82, p = 0.069 for 10-fold CV RMSE
respectively. Previously, the best model for fitting the training dataset and type 1 predictions are generated
by IFM using {Primary-Justify, Ln(Count)} LOs; on the task of predicting students’ posttest performance
(type 2), however, the best model is still IFM but using {Overall, Ln(Count)} LO counts. To summarize, the
best performance of IFM is better than the best AFM and across the eight LO modes and IFM is marginally
better than AFM model on type 2 prediction.
5.

CONCLUSION

In this paper we investigated student modeling on a dataset involving multiple instructional interventions. We
proposed a cognitive model named IFM. We compared IFM with AFM and PFM on the training dataset.
We determined that including non-standard LOs such as tells and explains as a separated parameter is
effective in that the IFM models’ out-performance PFM, AFM-Tell, and AFM+Tell across all modes; but
for AFM modes, simply adding tells into AFM LO counts did not seemingly significantly improved the AFM
model’s performance. This is probably because AFM gives a same learning rate for different instructional
interventions. For example, under the {Primary, Count} mode, the Nik in AFM+Tell model is Elicit + T ell.
On one KC, KC20 , the AFM had: the learning rate γk = 0.011462. By contrast, the corresponding IFM
has three parameters: µk for benefit of previous successes on skill k; ρk is the coefficient for the benefit of
previous failures, and νk the coefficient for the benefit of previous tells on skill k. For the same KC, the
IFM resulted µk = 0.083397; ρk = −0.213746, νk = 0.031982. The values of the three parameters are quite
different from each other, which suggested the the benefit of tells is in the middle of the benefit of success
and failure. Such patterns on learned parameters between AFM and IFM showed throughout our analysis.
It suggested that rather than using one learning rate parameters for different instructional interventions, it
is better to break them into categories and learn seperated parameters.
In order to fully exploring the effectiveness of three models, we further compared them on two prediction
tasks – unseen student prediction (type 1) and unseen step prediction (type 2). Our results indicate that the
IFM model is significantly better than the AFM model on predicting unseen student’s performance on seen
steps (type 1) and marginal significant better on predicting seen students’ performance on posttest (type 2).
Additionally, we examined the impact of including pretest performance in the LOs as well as qualitative
justification steps in the LOs. We found that the Primary-Justify mode seems to be most effective. Generally
speaking, models trained with logarithmic tabulation outperformed those trained with additive tabulation

probably because the number of prior LOs counts in this study can be ralatively large. For example, the
average number of primary steps (including both elicits and tells) in the training data varies from 6 for KC1
to 83 for KC20 .
Even though IFM model performed the best on modeling the training data on both type 1 and type 2
predictions, its performance is heavily dependent upon the specific prediction task being performed and the
way in which the specific LOs were counted. For modeling the training data and type 1 prediction, it is the
best to using (Primary-Justify,Ln(Count)) mode; but for type 2 predictions, it was best to include the pretest
data as well and thus using(Overall,Ln(Count)) mode for LO counts. Thus we conclude that, for datasets
involving multiple learning interventions, IFM is a more robust choice for student and cognitive modeling.
However the performance of IFM is heavily dependent upon the specific prediction task being performed and
the way in which the specific LOs were counted. Experimenters facing a novel task should engage in some
measure of parameter-fitting to determine the best fit.
ACKNOWLEDGMENTS

NSF (#SBE-0836012) and NSF (#0325054) supported this work.
REFERENCES
Barnes, T. 2005. The q-matrix method: Mining student response data for knowledge.
Beck, J. E. and Mostow, J. 2008. How who should practice: Using learning decomposition to evaluate the efficacy of different
types of practice for different types of students. See Woolf et al. [2008], 353–362.
Cen, H., Koedinger, K. R., and Junker, B. 2006. Learning factors analysis - a general method for cognitive model evaluation
and improvement. In Intelligent Tutoring Systems, M. Ikeda, K. D. Ashley, and T.-W. Chan, Eds. Springer, 164–175.
Cen, H., Koedinger, K. R., and Junker, B. 2008. Comparing two irt models for conjunctive skills. See Woolf et al. [2008],
796–798.
Corbett, A. T. and Anderson, J. R. 1995. Knowledge tracing: Modelling the acquisition of procedural knowledge. User
Model. User-Adapt. Interact. 4, 4, 253–278.
Draney, K., Pirolli, P., and Wilson, M. 1995. A Measurement Model for a Complex Cognitive Skill. Erlbaum, Hillsdale,
NJ.
Gong, Y., Beck, J., and Heffernan, N. 2010. Comparing knowledge tracing and performance factor analysis by using multiple
model fitting procedures. In Intelligent Tutoring Systems, V. Aleven, J. Kay, and J. Mostow, Eds. Lecture Notes in Computer
Science Series, vol. 6094. Springer Berlin / Heidelberg, 35–44. 10.1007/978-3-642-13388-6 8.
Heathcote, A., Brown, S., and D.J.K., M. 2000. The power law repealed: The case for an exponential law of practice.
Psychonomic Bulletin and Review 7, 2, 185207.
Jordan, P. W., Hall, B., Ringenberg, M., Cue, Y., and Rosé, C. 2007. Tools for authoring a dialogue agent that participates
in learning studies. In AIED, R. Luckin, K. R. Koedinger, and J. E. Greer, Eds. Frontiers in Artificial Intelligence and
Applications Series, vol. 158. IOS Press, Los Angeles, California, USA, 43–50.
Newell, A. and Rosenbloom, P. 1981. Mechanisms of Skill Acquisition and the Law of Practice. Erlbaum Hillsdale NJ.
Pavlik, P. I., Cen, H., and Koedinger, K. R. 2009. Performance factors analysis –a new alternative to knowledge tracing. In
Proceeding of the 2009 conference on Artificial Intelligence in Education. IOS Press, 531–538.
Pavlik, P. I., Yudelson, M., and Koedinger, K. 2011. Using contextual factors analysis to explain transfer of least common
multiple skills.
Schwarz, G. E. 1978. Estimating the dimension of a model. Annals of Statistics. 6, 2, 461464.
Tatsuoka, K. 1983. Rule space: An approach for dealing with misconceptions based on item response theory. Journal of
Educational Measurement. 20, 4, 345–354.
VanLehn, K. 2006. The behavior of tutoring systems. International Journal Artificial Intelligence in Education 16, 3, 227–265.
VanLehn, K., Jordan, P., and Litman, D. 2007. Developing pedagogically effective tutorial dialogue tactics: Experiments
and a testbed. In Proceedings of SLaTE Workshop on Speech and Language Technology in Education ISCA Tutorial and
Research Workshop. 17–20.
Woolf, B. P., Aı̈meur, E., Nkambou, R., and Lajoie, S. P., Eds. 2008. Intelligent Tutoring Systems, 9th International
Conference, ITS 2008, Montreal, Canada, June 23-27, 2008, Proceedings. Lecture Notes in Computer Science Series, vol.
5091. Springer.

